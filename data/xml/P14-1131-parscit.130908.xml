<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.653901">
CoSimRank: A Flexible &amp; Efficient Graph-Theoretic Similarity Measure
</title>
<author confidence="0.957339">
Sascha Rothe and Hinrich Sch¨utze
</author>
<affiliation confidence="0.982479">
Center for Information &amp; Language Processing
University of Munich
</affiliation>
<email confidence="0.976369">
sascha@cis.lmu.de
</email>
<sectionHeader confidence="0.994281" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974">
We present CoSimRank, a graph-theoretic
similarity measure that is efficient because
it can compute a single node similarity
without having to compute the similarities
of the entire graph. We present equivalent
formalizations that show CoSimRank’s
close relationship to Personalized Page-
Rank and SimRank and also show how
we can take advantage of fast matrix mul-
tiplication algorithms to compute CoSim-
Rank. Another advantage of CoSimRank
is that it can be flexibly extended from ba-
sic node-node similarity to several other
graph-theoretic similarity measures. In an
experimental evaluation on the tasks of
synonym extraction and bilingual lexicon
extraction, CoSimRank is faster or more
accurate than previous approaches.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999696777777778">
Graph-theoretic algorithms have been successfully
applied to many problems in NLP (Mihalcea and
Radev, 2011). These algorithms are often based on
PageRank (Brin and Page, 1998) and other central-
ity measures (e.g., (Erkan and Radev, 2004)). An
alternative for tasks involving similarity is Sim-
Rank (Jeh and Widom, 2002). SimRank is based
on the simple intuition that nodes in a graph should
be considered as similar to the extent that their
neighbors are similar. Unfortunately, SimRank
has time complexity O(n3) (where n is the num-
ber of nodes in the graph) and therefore does not
scale to the large graphs that are typical of NLP.
This paper introduces CoSimRank,1 a new
graph-theoretic algorithm for computing node
similarity that combines features of SimRank and
PageRank. Our key observation is that to compute
the similarity of two nodes, we need not consider
</bodyText>
<footnote confidence="0.565946">
1Code available at code.google.com/p/cistern
</footnote>
<bodyText confidence="0.999843081081081">
all other nodes in the graph as SimRank does; in-
stead, CoSimRank starts random walks from the
two nodes and computes their similarity at each
time step. This offers large savings in computa-
tion time if we only need the similarities of a small
subset of all n2 node similarities.
These two cases – computing a few similari-
ties and computing many similarities – correspond
to two different representations we can compute
CoSimRank on: a vector representation, which is
fast for only a few similarities, and a matrix repre-
sentation, which can take advantage of fast matrix
multiplication algorithms.
CoSimRank can be used to compute many vari-
ations of basic node similarity – including similar-
ity for graphs with weighted and typed edges and
similarity for sets of nodes. Thus, CoSimRank has
the added advantage of being a flexible tool for dif-
ferent types of applications.
The extension of CoSimRank to similarity
across graphs is important for the application of
bilingual lexicon extraction: given a set of corre-
spondences between nodes in two graphs A and B
(corresponding to two different languages), a pair
of nodes (a ∈ A, b ∈ B) is a good candidate for a
translation pair if their node similarity is high. In
an experimental evaluation, we show that CoSim-
Rank is more efficient and more accurate than both
SimRank and PageRank-based algorithms.
This paper is structured as follows. Section 2
discusses related work. Section 3 introduces
CoSimRank. In Section 4, we compare CoSim-
Rank and SimRank. By providing some useful
extensions, we demonstrate the great flexibility of
CoSimRank (Section 5). We perform an exper-
imental evaluation of CoSimRank in Section 6.
Section 7 summarizes the paper.
</bodyText>
<sectionHeader confidence="0.999215" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.824528">
Our work is unsupervised. We therefore do not
review graph-based methods that make extensive
</bodyText>
<page confidence="0.938248">
1392
</page>
<note confidence="0.8313865">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1392–1402,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999579145833333">
use of supervised learning (e.g., de Melo and
Weikum (2012)).
Since the original version of SimRank (Jeh and
Widom, 2002) has complexity 0(n4), many ex-
tensions have been proposed to speed up its calcu-
lation. A Monte Carlo algorithm, which is scalable
to the whole web, was suggested by Fogaras and
R´acz (2005). However, in an evaluation of this al-
gorithm we found that it does not give competitive
results (see Section 6). A matrix representation of
SimRank called SimFusion (Xi et al., 2005) im-
proves the computational complexity from 0(n4)
to 0(n3). Lizorkin et al. (2010) also reduce com-
plexity to 0(n3) by selecting essential node pairs
and using partial sums. They also give a useful
overview of SimRank, SimFusion and the Monte
Carlo methods of Fogaras and R´acz (2005). A
non-iterative computation for SimRank was intro-
duced by Li et al. (2010). This is especially useful
for dynamic graphs. However, all of these meth-
ods have to run SimRank on the entire graph and
are not efficient enough for very large graphs. We
are interested in applications that only need a frac-
tion of all 0(n2) pairwise similarities. The algo-
rithm we propose below is an order of magnitude
faster in such applications because it is based on a
local formulation of the similarity measure.2
Apart from SimRank, many other similarity
measures have been proposed. Leicht et al. (2006)
introduce a similarity measure that is also based on
the idea that nodes are similar when their neigh-
bors are, but that is designed for bipartite graphs.
However, most graphs in NLP are not bipartite and
Jeh and Widom (2002) also proposed a SimRank
variant for bipartite graphs.
Another important similarity measure is cosine
similarity of Personalized PageRank (PPR) vec-
tors. We will refer to this measure as PPR+cos.
Hughes and Ramage (2007) find that PPR+cos
has high correlation with human similarity judg-
ments on WordNet-based graphs. Agirre et al.
(2009) use PPR+cos for WordNet and for cross-
lingual studies. Like CoSimRank, PPR+cos is
efficient when computing single node pair simi-
larities; we therefore use it as one of our base-
lines below. This method is also used by Chang
et al. (2013) for semantic relatedness. They also
experimented with Euclidean distance and KL-
</bodyText>
<footnote confidence="0.6071665">
2A reviewer suggests that CoSimRank is an efficient ver-
sion of SimRank in a way analogous to SALSA’s (Lempel
and Moran, 2000) relationship to HITS (Kleinberg, 1999) in
that different aspects of similarity are decoupled.
</footnote>
<bodyText confidence="0.999831745098039">
divergence. Interestingly, a simpler method per-
formed best when comparing with human simi-
larity judgments. In this method only the entries
corresponding to the compared nodes are used for
a similarity score. Rao et al. (2008) compared
PPR+cos to other graph based similarity mea-
sures like shortest-path and bounded-length ran-
dom walks. PPR+cos performed best except for
a new similarity measure based on commute time.
We do not compare against this new measure as it
uses the graph Laplacian and so cannot be com-
puted for a single node pair.
One reason CoSimRank is efficient is that we
need only compute a few iterations of the random
walk. This is often true of this type of algorithm;
cf. (Sch¨utze and Walsh, 2008).
LexRank (Erkan and Radev, 2004) is similar to
PPR+cos in that it combines PageRank and cosine;
it initializes the sentence similarity matrix of a
document using cosine and then applies PageRank
to compute lexical centrality. Despite this superfi-
cial relatedness, applications like lexicon extrac-
tion that look for similar entities and applications
that look for central entities are quite different.
In addition to faster versions of SimRank, there
has also been work on extensions of SimRank.
Dorow et al. (2009) and Laws et al. (2010) ex-
tend SimRank to edge weights, edge labels and
multiple graphs. We use their Multi-Edge Extrac-
tion (MEE) algorithm as one of our baselines be-
low. A similar graph of dependency structures was
built by Minkov and Cohen (2008). They applied
different similarity measures, e.g., cosine of de-
pendency vectors or a new algorithm called path-
constrained graph walk, on synonym extraction
(Minkov and Cohen, 2012). We compare CoSim-
Rank with their results in our experiments (see
Section 6).
Some other applications of SimRank or other
graph based similarity measures in NLP include
work on document similarity (Li et al., 2009),
the transfer of sentiment information between lan-
guages (Scheible et al., 2010) and named entity
disambiguation (Han and Zhao, 2010). Hoang and
Kan (2010) use SimRank for related work sum-
marization. Muthukrishnan et al. (2010) combine
link based similarity and content based similarity
for document clustering and classification.
These approaches use at least one of cosine sim-
ilarity, PageRank and SimRank. CoSimRank can
either be interpreted as an efficient version of Sim-
</bodyText>
<page confidence="0.976122">
1393
</page>
<bodyText confidence="0.999932">
Rank or as a version of Personalized PageRank
for similarity measurement. The novelty is that
we compute similarity for vectors that are induced
using a new algorithm, so that the similarity mea-
surement is much more efficient when an applica-
tion only needs a fraction of all O(n2) pairwise
similarities.
</bodyText>
<sectionHeader confidence="0.994557" genericHeader="method">
3 CoSimRank
</sectionHeader>
<bodyText confidence="0.9997095">
We first first give an intuitive introduction of
CoSimRank as a Personalized PageRank (PPR)
derivative. Later on, we will give a matrix formu-
lation to compare CoSimRank with SimRank.
</bodyText>
<subsectionHeader confidence="0.995588">
3.1 Personalized PageRank
</subsectionHeader>
<bodyText confidence="0.992231125">
Haveliwala (2002) introduced Personalized Page-
Rank – or topic-sensitive PageRank – based on the
idea that the uniform damping vector p(0) can be
replaced by a personalized vector, which depends
on node i. We usually set p(0)(i) = ez, with ez be-
ing a vector of the standard basis, i.e., the ith entry
is 1 and all other entries are 0. The PPR vector of
node i is given by:
</bodyText>
<equation confidence="0.999859">
p(k)(i) = dAp(k−1)(i) + (1 − d)p(0)(i) (1)
</equation>
<bodyText confidence="0.99986675">
where A is the stochastic matrix of the Markov
chain, i.e., the row normalized adjacency matrix.
The damping factor d E (0, 1) ensures that the
computation converges. The PPR vector after k
iterations is given by p(k).
To visualize this formula, one can imagine a
random surfer starting at node i and following one
of the links with probability d or jumping back to
the starting node i with probability (1 − d). Entry
i of the converged PPR vector represents the prob-
ability that the random surfer is on node i after an
unlimited number of steps.
To simulate the behavior of SimRank we will
simplify this equation and set the damping factor
d = 1. We will re-add a damping factor later in
the calculation.
</bodyText>
<equation confidence="0.999704">
p(k) = Ap(k−1) (2)
</equation>
<bodyText confidence="0.999533333333333">
Note that the personalization vector p(0) was elim-
inated, but is still present as the starting vector of
the iteration.
</bodyText>
<subsectionHeader confidence="0.999892">
3.2 Similarity of vectors
</subsectionHeader>
<bodyText confidence="0.9795334">
Let p(i) be the PPR vector of node i. The cosine
of two vectors u and v is computed by dividing
Figure 1: Graph motivating CoSimRank algo-
rithm. Whereas PPR gives relatively high similar-
ity to the pair (law,suit), CoSimRank assigns the
pair similarity 0.
the inner product (u, v) by the lengths of the vec-
tors. The cosine of two PPR vectors can be used as
a similarity measure for the corresponding nodes
(Hughes and Ramage, 2007; Agirre et al., 2009):
</bodyText>
<equation confidence="0.993609">
s(i,j) = (p(i),p(j)) (3)
|p(i)||p(j)|
</equation>
<bodyText confidence="0.999130461538462">
This measure s(i, j) looks at the probability that
a random walker is on a certain edge after an un-
limited number of steps. This is potentially prob-
lematic as the example in Figure 1 shows. The
PPR vectors of suit and dress will have some
weight on tailor, which is good. However, the
PPR vector of law will also have a non-zero weight
for tailor. So law and dress are similar because of
the node tailor. This is undesirable.
We can prevent this type of spurious similarity
by taking into account the path the random surfer
took to get to a particular node. We formalize this
by defining CoSimRank s(i, j) as follows:
</bodyText>
<equation confidence="0.9922875">
s(i, j) = �∞ ck(p(k)(i),p(k)(j)) (4)
k=0
</equation>
<bodyText confidence="0.972872692307693">
where p(k)(i) is the PPR vector of node i from
Eq. 2 after k iterations. We compare the PPR vec-
tors at each time step k. The sum of all similarities
is the value of CoSimRank, i.e., the final similar-
ity. We add a damping factor c, so that early meet-
ings are more valuable than later meetings.
To compute the similarity of two vectors u and
v we use the inner product (·,·) in Eq. 4 for two
reasons:
1. This is similar to cosine similarity except that
the 1-norm is used instead of the 2-norm.
Since our vectors are probability vectors, we
have
</bodyText>
<equation confidence="0.824895666666667">
(p(i),p(j))
|p(i)||p(j)|
= (p(i),p(j))
</equation>
<page confidence="0.887237">
1394
</page>
<bodyText confidence="0.872016444444444">
for the 1-norm.3
2. Without expensive normalization, we can
give a simple matrix formalization of CoSim-
Rank and compute it efficiently using fast
matrix multiplication algorithms.
Later on, the following iterative computation of
CoSimRank will prove useful:
s(k)(i, j) = ck(p(k)(i), p(k)(j)) + s(k−1)(i, j)
(5)
</bodyText>
<subsectionHeader confidence="0.992365">
3.3 Matrix formulation
</subsectionHeader>
<bodyText confidence="0.991991">
The matrix formulation of CoSimRank is:
</bodyText>
<equation confidence="0.9999544">
S(0) = E
S(1) = cAAT + S(0)
S(2) = c2A2(AT )2 + S(1)
. . .
S(k) = ckAk(AT)k + S(k−1) (6)
</equation>
<bodyText confidence="0.999961">
We will see in Section 5 that this formulation is the
basis for a very efficient version of CoSimRank.
</bodyText>
<subsectionHeader confidence="0.999047">
3.4 Convergence properties
</subsectionHeader>
<bodyText confidence="0.962068785714286">
As the PPR vectors have only positive values, we
can easily see in Eq. 4 that the CoSimRank of
one node pair is monotonically non-decreasing.
For the dot product of two vectors, the Cauchy-
Schwarz inequality gives the upper bound:
(u,v) &lt; IluIl IlvIl
where IlxIl is the norm of x. From Eq. 2 we get
IIp(k)II1 = 1, where Il&apos;Il1 is the 1-norm. We also
know from elementary functional analysis that the
1-norm is the biggest of all p-norms and so one
�
has �p(k)� � &lt; 1. It follows that CoSimRank grows
more slowly than a geometric series and converges
if |c |&lt; 1:
</bodyText>
<equation confidence="0.758816333333333">
c=
k 1
1 − c
If an upper bound of 1 is desired for s(i, j) (in-
stead of 1/(1 − c)), then we can use s0:
s0(i, j) = (1 − c)s(i, j)
</equation>
<footnote confidence="0.94843675">
3This type of similarity measure has also been used and
investigated by O´ S´eaghdha and Copestake (2008), Cha
(2007), Jebara et al. (2004) (probability product kernel) and
(Jaakkola et al., 1999) (Fisher kernel) among others.
</footnote>
<sectionHeader confidence="0.734832" genericHeader="method">
4 Comparison to SimRank
</sectionHeader>
<bodyText confidence="0.9985925">
The original SimRank equation can be written as
follows (Jeh and Widom, 2002):
</bodyText>
<equation confidence="0.998553833333333">
1, if i = j
P
c
|N(i)||N(j)|
k∈N(i)
l∈N(j)
</equation>
<bodyText confidence="0.99975425">
where N(i) denotes the nodes connected to i.
SimRank is computed iteratively. With A be-
ing the normalized adjacency matrix we can write
SimRank in matrix formulation:
</bodyText>
<equation confidence="0.999984">
R(0) = E
R(k) = max{cAR(k−1)AT,R(0)} (7)
</equation>
<bodyText confidence="0.999906666666667">
where the maximum of two matrices refers to the
element-wise maximum. We will now prove by in-
duction that the matrix formulation of CoSimRank
</bodyText>
<equation confidence="0.759853">
(Eq. 6) is equivalent to:
S0(k) = cAS0(k−1)AT + S(0) (8)
</equation>
<bodyText confidence="0.994860666666667">
and thus very similar to SimRank (Eq. 7).
The base case S(1) = S0(1) is trivial. Inductive
step:
</bodyText>
<equation confidence="0.99856825">
S0(k) (8)= cAS0(k−1)AT + S(0)
= cA(ck−1Ak−1(AT)k−1 + S(k−2))AT + S(0)
= ckAk(AT )k + cAS(k−2)AT + S(0)
= ckAk(AT)k + S(k−1) (6)= S(k)
</equation>
<bodyText confidence="0.998700722222222">
Comparing Eqs. 7 and 8, we see that SimRank
and CoSimRank are very similar except that they
initialize the similarities on the diagonal differ-
ently. Whereas SimRank sets each of these en-
tries back to one at each iteration, CoSimRank
adds one. Thus, when computing the two similar-
ity measures iteratively, the diagonal element (i, i)
will be set to 1 by both methods for those initial it-
erations for which this entry is 0 for cAS(k−1)AT
(i.e., before applying either max or add). The
methods diverge when the entry is =� 0 for the first
time.
Complexity of computing all n2 similarities.
The matrix formulas of both SimRank (Eq. 7)
and CoSimRank (Eq. 8) have time complexity
O(n3) or – if we want to take the higher efficiency
of computation for sparse graphs into account –
O(dn2) where n is the number of nodes and d the
</bodyText>
<equation confidence="0.889337125">
∞
s(i, j) &lt; X
k=0
⎧
⎨⎪
⎪⎩
r(i, j) =
r(k, l), else
</equation>
<page confidence="0.897767">
1395
</page>
<bodyText confidence="0.998915105263158">
average degree. Space complexity is O(n2) for
both algorithms.
Complexity of computing k2 « n2 similar-
ities. In most cases, we only want to compute
k2 similarities for k nodes. For CoSimRank, we
compute the k PPR vectors in O(kdn) (Eq. 2) and
compute the k2 similarities in O(k2n) (Eq. 5). If
d &lt; k, then the time complexity of CoSimRank
is O(k2n). If we only compute a single similar-
ity, then the complexity is O(dn). In contrast, the
complexity of SimRank is the same as in the all-
similarities case: O(dn2). It is not obvious how to
design a lower-complexity version of SimRank for
this case. Thus, we have reduced SimRank’s cu-
bic time complexity to a quadratic time complex-
ity for CoSimRank or – assuming that the aver-
age degree d does not depend on n – SimRank’s
quadratic time complexity to linear time complex-
ity for the case of computing few similarities.
Space complexity for computing k2 similarities
is O(kn) since we need only store k vectors, not
the complete similarity matrix. This complexity
can be exploited even for the all similarities appli-
cation: If the matrix formulation cannot be used
because the O(n2) similarity matrix is too big for
available memory, then we can compute all sim-
ilarities in batches – and if desired in parallel –
whose size is chosen such that the vectors of each
batch still fit in memory.
In summary, CoSimRank and SimRank have
similar space and time complexities for comput-
ing all n2 similarities. For the more typical case
that we only want to compute a fraction of all sim-
ilarities, we have recast the global SimRank for-
mulation as a local CoSimRank formulation. As a
result, time and space complexities are much im-
proved. In Section 6, we will show that this is also
true in practice.
</bodyText>
<sectionHeader confidence="0.998183" genericHeader="method">
5 Extensions
</sectionHeader>
<bodyText confidence="0.999885">
We will show now that the basic CoSimRank algo-
rithm can be extended in a number of ways and is
thus a flexible tool for different NLP applications.
</bodyText>
<subsectionHeader confidence="0.957056">
5.1 Weighted edges
</subsectionHeader>
<bodyText confidence="0.998372">
The use of weighted edges was first proposed in
the PageRank patent. It is straightforward and
easy to implement by replacing the row normal-
ized adjacency matrix A with an arbitrary stochas-
tic matrix P. We can use this edge weighted Page-
Rank for CoSimRank.
</bodyText>
<subsectionHeader confidence="0.997749">
5.2 CoSimRank across graphs
</subsectionHeader>
<bodyText confidence="0.977539714285714">
We often want to compute the similarity of nodes
in two different graphs with a known node-node
correspondence; this is the scenario we are faced
with in the lexicon extraction task (see Section 6).
A variant of SimRank for this task was presented
by Dorow et al. (2009). We will now present an
equivalent method for CoSimRank. We denote the
number of nodes in the two graphs U and V by
|U |and |V |, respectively. We compute PPR vec-
tors p ∈ R|U |and q ∈ R|V  |for each graph. Let
S(0) ∈ R|U|&amp;quot;|V  |be the known node-node corre-
spondences. The analog of CoSimRank (Eq. 4)
for two graphs is then:
The matrix formulation (cf. Eq. 6) is:
</bodyText>
<equation confidence="0.999687">
S(k) = ckAkS(0)(BT)k + S(k−1) (10)
</equation>
<bodyText confidence="0.999804">
where A and B are row-normalized adjacency ma-
trices. We can interpret S(0) as a change of basis.
A similar approach for word embeddings was pub-
lished by Mikolov et al. (2013). They call S(0) the
translation matrix.
</bodyText>
<subsectionHeader confidence="0.982904">
5.3 Typed edges
</subsectionHeader>
<bodyText confidence="0.999992666666667">
To be able to directly compare to prior work in our
experiments, we also present a method to integrate
a set of typed edges T in the CoSimRank calcula-
tion. For this we will compute a similarity matrix
for each edge type T and merge them into one ma-
trix for the next iteration:
</bodyText>
<equation confidence="0.991842">
S(k) = |c|TτETX AτS(k−1)BT + S(0) (11)
τ !
</equation>
<bodyText confidence="0.997913">
This formula is identical to the random surfer
model where two surfers only meet iff they are
on the same node and used the same edge type to
get there. A more strict claim would be to use the
same edge type at any time of their journey:
</bodyText>
<equation confidence="0.9917175">
! !
k−1Y
S(0)
Aτi BT τk−i
i=0
+ S(k−1) (12)
</equation>
<bodyText confidence="0.566638">
We will not use Eq. 12 due to its space complexity.
</bodyText>
<equation confidence="0.998923714285714">
s(i,j) = X00 ck X p(k)
k=0 (u,v)ES(0) u (i)q(k)
v (j) (9)
S(k) = ck X
|T |k τETk
Yk
i=1
</equation>
<page confidence="0.959002">
1396
</page>
<subsectionHeader confidence="0.992711">
5.4 Similarity of sets of nodes
</subsectionHeader>
<bodyText confidence="0.9982865">
CoSimRank can also be used to compute the sim-
ilarity s(V, W) of two sets V and W of nodes,
e.g., short text snippets. We are not including this
method in our experiments, but we will give the
equation here, as traditional document similarity
measures (e.g., cosine similarity) perform poorly
on this task although there also are known alter-
natives with good results (Sahami and Heilman,
2006). For a set V , the initial PPR vector is given
by:
</bodyText>
<figure confidence="0.366815">
� pi
(V)= |v�, if i E V
(0)
0, else
We then reuse Eq. 4 to compute s(V, W):
Edge types
</figure>
<table confidence="0.929430571428572">
relation entities description example
amod a, v adjective-noun a fast car
dobj v, n verb-object drive a car
ncrd n, n noun-noun cars and busses
Graph statistics
nodes nouns adjectives verbs
de 34,544 10,067 2,828
en 22,258 12,878 4,866
edges ncrd amod dobj
de 65,299 417,151 143,905
en 288,878 686,069 510,351
s(V, W) = ∞ ck(p(k)(V ),p(k)(W)) Table 1: Edge types (above) and number of nodes
E and edges (below)
k=0
</table>
<bodyText confidence="0.999695666666667">
In summary, modifications proposed for Sim-
Rank (weighted and typed edges, similarity across
graphs) as well as modifications proposed for
PageRank (sets of nodes) can also be applied to
CoSimRank. This makes CoSimRank a very flex-
ible similarity measure.
We will test the first three extensions experi-
mentally in the next section and leave similarity
of node sets for future work.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999929318181818">
We evaluate CoSimRank for the tasks of syn-
onym extraction and bilingual lexicon extraction.
We use the basic version of CoSimRank (Eq. 4)
for synonym extraction and the two-graph version
(Eq. 9) for lexicon extraction, both with weighted
edges. Our motivation for this application is that
two words that are synonyms of each other should
have similar lexical neighbors and that two words
that are translations of each other should have
neighbors that correspond to each other; thus, in
each case the nodes should be similar in the graph-
theoretic sense and CoSimRank should be able to
identify this similarity.
We use the English and German graphs pub-
lished by Laws et al. (2010), including edge
weighting and normalization. Nodes are nouns,
adjectives and verbs occurring in Wikipedia.
There are three types of edges, corresponding to
three types of syntactic configurations extracted
from the parsed Wikipedias: adjective-noun, verb-
object and noun-noun coordination. Table 1 gives
examples and number of nodes and edges.
</bodyText>
<subsectionHeader confidence="0.990554">
6.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999868777777778">
We propose CoSimRank as an efficient algorithm
for computing the similarity of nodes in a graph.
Consequently, we compare against the two main
methods for this task in NLP: SimRank and exten-
sions of PageRank.
We also compare against the MEE (Multi-Edge
Extraction) variant of SimRank (Dorow et al.,
2009), which handles labeled edges more effi-
ciently than SimRank:
</bodyText>
<equation confidence="0.99922375">
SI(k) = c
|T  |τYET
_
S(k) = max{SI(k), S(0)}
</equation>
<bodyText confidence="0.999985461538462">
where Aτ is the row-normalized adjacency matrix
for edge type τ (see edge types in Table 1).
Apart from SimRank, extensions of PageRank
are the main methods for computing the similar-
ity of nodes in graphs in NLP (e.g., Hughes and
Ramage (2007), Agirre et al. (2009) and other pa-
pers discussed in related work). Generally, these
methods compute the Personalized PageRank for
each node (see Eq. 1). When the computation has
converged, the similarity of two nodes is given by
the cosine similarity of the Personalized PageRank
vectors. We implemented this method for our ex-
periments and call it PPR+cos.
</bodyText>
<subsectionHeader confidence="0.999161">
6.2 Synonym Extraction
</subsectionHeader>
<bodyText confidence="0.996991666666667">
We use TS68, a test set of 68 synonym pairs pub-
lished by Minkov and Cohen (2012) for evalua-
tion. This gold standard lists a single word as the
</bodyText>
<figure confidence="0.499501">
AτS(k−1)BT
τ
</figure>
<page confidence="0.864048">
1397
</page>
<table confidence="0.998199454545455">
P@1 P@10 MRR
one-synonym
PPR+cos 20.6% 52.9% 0.32
SimRank 25.0% 61.8% 0.37
CoSimRank 25.0% 61.8% 0.37
Typed CoSimRank 23.5% 63.2% 0.37
extended
PPR+cos 32.6% 73.5% 0.48
SimRank 45.6% 83.8% 0.59
CoSimRank 45.6% 83.8% 0.59
Typed CoSimRank 44.1% 83.8% 0.59
</table>
<tableCaption confidence="0.8641725">
Table 2: Results for synonym extraction on TS68.
Best result in each column in bold.
</tableCaption>
<bodyText confidence="0.9905602">
correct synonym even if there are several equally
acceptable near-synonyms (see Table 3 for exam-
ples). We call this the one-synonym evaluation.
Three native English speakers were asked to mark
synonyms, that were proposed by a baseline or by
CoSimRank, i.e. ranked in the top 10. If all three
of them agreed on one word as being a synonym
in at least one meaning, we added this as a correct
answer to the test set. We call this the “extended”
evaluation (see Table 2).
Synonym extraction is run on the English graph.
To calculate PPR+cos, we computed 20 iterations
with a decay factor of 0.8 and used the cosine sim-
ilarity with the 2-norm in the denominator to com-
pare two vectors. For the other three methods, we
also used a decay factor of 0.8 and computed 5 it-
erations. Recall that CoSimRank uses the simple
inner product (·, ·) to compare vectors.
Our evaluation measures are proportion of
words correctly translated by word in the top
position (P@1), proportion of words correctly
translated by a word in one of the top 10 posi-
tions (P@10) and Mean Reciprocal Rank (MRR).
CoSimRank’s MRR scores of 0.37 (one-synonym)
and 0.59 (extended) are the same or better than all
baselines (see Table 2). CoSimRank and SimRank
have the same P@1 and P@10 accuracy (although
they differed on some decisions). CoSimRank is
better than PPR+cos on both evaluations, but as
this test set is very small, the results are not signif-
icant. Table 3 shows a sample of synonyms pro-
posed by CoSimRank.
Minkov and Cohen (2012) tested cosine and
random-walk measures on grammatical relation-
keyword expected extracted
movie film film
modern contemporary contemporary
demonstrate protest show
attractive appealing beautiful
economic profitable financial
close shut open
Table 3: Examples for extracted synonyms. Cor-
rect synonyms according to extended evaluation in
bold.
ships (similar to our setup) as well as on cooccur-
rence statistics. The MRR scores for these meth-
ods range from 0.29 to 0.59. (MRR is equivalent
to MAP as reported by Minkov and Cohen (2012)
when there is only one correct answer.) Their
best number (0.59) is better than our one-synonym
result; however, they performed manual postpro-
cessing of results – e.g., discarding words that are
morphologically or semantically related to other
words in the list – so our fully automatic results
cannot be directly compared.
</bodyText>
<subsectionHeader confidence="0.999838">
6.3 Lexicon Extraction
</subsectionHeader>
<bodyText confidence="0.999938923076923">
We evaluate lexicon extraction on TS1000, a test
set of 1000 items, (Laws et al., 2010) each con-
sisting of an English word and its German transla-
tions. For lexicon extraction, we use the same pa-
rameters as in the synonym extraction task for all
four similarity measures. We use a seed dictionary
of 12,630 word pairs to establish node-node corre-
spondences between the two graphs. We remove
a search keyword from the seed dictionary before
calculating similarities for it, something that the
architecture of CoSimRank makes easy because
we can use a different seed dictionary S(0) for ev-
ery keyword.
Both CoSimRank methods outperform Sim-
Rank significantly (see Table 4). The differ-
ence between CoSimRank with and without typed
edges is not significant. (This observation was also
made for SimRank on a smaller graph and test set
(Laws et al., 2010).)
PPR+cos’s performance at 14.8% correct trans-
lations is much lower than SimRank and CoSim-
Rank. The disadvantage of this similarity mea-
sure is significant and even more visible on bilin-
gual lexicon extraction than on synonym extrac-
tion (see Table 2). The reason might be that we
are not comparing the whole PPR vector anymore,
</bodyText>
<page confidence="0.969628">
1398
</page>
<table confidence="0.9970858">
P@1 P@10
PPR+cos 14.8%† 45.7%†
SimRank MEE 48.0%† 76.0%†
CoSimRank 61.1% 84.0%
Typed CoSimRank 61.4% 83.9%
</table>
<tableCaption confidence="0.910848333333333">
Table 4: Results for bilingual lexicon extraction
(TS1000 EN → DE). Best result in each column
in bold.
</tableCaption>
<bodyText confidence="0.9996555">
but only entries which occur in the seed dictionary
(see Eq. 9). As the seed dictionary contains 12,630
word pairs, this means that only every fourth entry
of the PPR vector (the German graph has 47,439
nodes) is used for similarity calculation. This is
also true for CoSimRank, but it seems that CoSim-
Rank is more stable because we compare more
than one vector.
We also experimented with the method of Fog-
aras and R´acz (2005). We tried a number of differ-
ent ways of modifying it for weighted graphs: (i)
running the random walks with the weighted ad-
jacency matrix as Markov matrix, (ii) storing the
weight (product of each edge weight) of a random
walk and using it as a factor if two walks meet
and (iii) a combination of both. We needed about
10,000 random walks in all three conditions. As a
result, the computational time was approximately
30 minutes per test word, so this method is even
slower than SimRank for our application. The ac-
curacies P@1 and P@10 were worse in all experi-
ments than those of CoSimRank.
</bodyText>
<subsectionHeader confidence="0.99976">
6.4 Run time performance
</subsectionHeader>
<bodyText confidence="0.999194066666667">
Table 5 compares the run time performance of
CoSimRank with the baselines. We ran all exper-
iments on a 64-bit Linux machine with 64 Intel
Xenon X7560 2.27Ghz CPUs and 1TB RAM. The
calculated time is the sum of the time spent in user
mode and the time spent in kernel mode. The ac-
tual wall clock time was significantly lower as we
used up to 64 CPUs.
Compared to SimRank, CoSimRank is more
than 40 times faster on synonym extraction and six
times faster on lexicon extraction. SimRank is at
a disadvantage because it computes all similarities
in the graph regardless of the size of the test set;
it is particularly inefficient on synonym extraction
because the English graph contains a large number
</bodyText>
<footnote confidence="0.953861">
†significantly worse than CoSimRank (α = 0.05, one-
tailed Z-Test)
</footnote>
<table confidence="0.9998925">
synonym extraction lexicon extraction
(68 word pairs) (1000 word pairs)
PPR+cos 2,228 2,195
SimRank 23,423 14,418
CoSimRank 524 2,342
Typed CoSimRank 615 6,108
</table>
<tableCaption confidence="0.847753">
Table 5: Execution times in minutes for CoSim-
Rank and the baselines. Best result in each column
in bold.
</tableCaption>
<bodyText confidence="0.992537619047619">
of edges (see Table 1).
Compared to PPR+cos, CoSimRank is roughly
four times faster on synonym extraction and has
comparable performance on lexicon extraction.
We compute 20 iterations of PPR+cos to reach
convergence and then calculate a single cosine
similarity. For CoSimRank, we need only com-
pute five iterations to reach convergence, but we
have to compute a vector similarity in each itera-
tion. The counteracting effects of fewer iterations
and more vector similarity computations can give
either CoSimRank or PPR+cos an advantage, as
is the case for synonym extraction and lexicon ex-
traction, respectively.
CoSimRank should generally be three times
faster than typed CoSimRank since the typed ver-
sion has to repeat the computation for each of
the three types. This effect is only visible on the
larger test set (lexicon extraction) because the gen-
eral computation overhead is about the same on a
smaller test set.
</bodyText>
<subsectionHeader confidence="0.99961">
6.5 Comparison with WINTIAN
</subsectionHeader>
<bodyText confidence="0.999785588235294">
Here we address inducing a bilingual lexicon from
a seed set based on grammatical relations found
by a parser. An alternative approach is to in-
duce a bilingual lexicon from Wikipedia’s inter-
wiki links (Rapp et al., 2012). These two ap-
proaches have different strengths and weaknesses;
e.g., the interwiki-link-based approach does not
require a seed set, but it can only be applied to
comparable corpora that consist of corresponding
– although not necessarily “parallel” – documents.
Despite these differences it is still interesting to
compare the two algorithms. Rapp et al. (2012)
kindly provided their test set to us. It contains
1000 English words and a single correct German
translation for each. We evaluate on a subset we
call TS774 that consists of the 774 test word pairs
that are in the intersection of words covered by the
</bodyText>
<page confidence="0.985629">
1399
</page>
<table confidence="0.96932">
P@1 P@10
Wintian 43.8% 55.4%†
CoSimRank 43.0% 73.6%
</table>
<tableCaption confidence="0.721738666666667">
Table 6: Results for bilingual lexicon extraction
(TS774 DE → EN). Best result in each column in
bold.
</tableCaption>
<bodyText confidence="0.995874933333333">
WINTIAN Wikipedia data (Rapp et al., 2012) and
words covered by our data. Most of the 226 miss-
ing word pairs are adverbs, prepositions and plural
forms that are not covered by our graphs due to the
construction algorithm we use: lemmatization, re-
striction to adjectives, nouns and verbs etc.
Table 6 shows that CoSimRank is slightly, but
not significantly worse than WINTIAN on P@1
(43.0 vs 43.8), but significantly better on P@10
(73.6 vs 55.4).4 The reason could be that CoSim-
Rank is a more effective algorithm than WIN-
TIAN; but the different initializations (seed set vs
interwiki links) or the different linguistic represen-
tations (grammatical relations vs bag-of-words)
could also be responsible.
</bodyText>
<subsectionHeader confidence="0.820478">
6.6 Error Analysis
</subsectionHeader>
<bodyText confidence="0.845000981481482">
The results on TS774 can be considered conserva-
tive since only one translation is accepted as being
correct. In reality other translations might also be
acceptable (e.g., both street and road for Straße).
In contrast, TS1000 accepts more than one cor-
rect translation. Additionally, TS774 was created
by translating English words into German (using
Google translate). We are now testing the reverse
direction. So we are doomed to fail if the original
English word is a less common translation of an
ambiguous German word. For example, the En-
glish word gulf was translated by Google to Golf,
but the most common sense of Golf is the sport.
Hence our algorithm will incorrectly translate it
back to golf.
As we can see in Table 7, we also face the prob-
lems discussed by Laws et al. (2010): the algo-
rithm sometimes picks cohyponyms (which can
still be seen as reasonable) and antonyms (which
are clear errors).
Contrary to our intuition, the edge-typed vari-
ant of CoSimRank did not perform significantly
better than the non-edge-typed version. Looking
4We achieved better results for CoSimRank by optimizing
the damping factor, but in this paper, we only present results
for a fixed damping factor of 0.8.
keyword gold standard CoSimRank
arm poor impoverished
erreichen reach achieve
gehen go walk
direkt directly direct
weit far further
breit wide narrow
reduzieren reduce increase
Stunde hour second
Westen west southwest
Junge boy child
Table 7: Examples for CoSimRank translation er-
rors on TS774. We counted translations as incor-
rect if they were not listed in the gold standard
even if they were correct translations according to
www.dict.cc (in bold).
at Table 1, we see that there is only one edge type
connecting adjectives. The same is true for verbs.
The random surfer only has a real choice between
different edge types when she is on a noun node.
Combined with the fact that only the last edge type
is important this has absolutely no effect for a ran-
dom surfer meeting at adjectives or verbs.
Two possible solutions would be (i) to use more
fine-grained edge types, (ii) to apply Eq. 12, in
which the edge type of each step is important.
However, this will increase the memory needed for
calculation.
</bodyText>
<sectionHeader confidence="0.997939" genericHeader="conclusions">
7 Summary
</sectionHeader>
<bodyText confidence="0.986903888888889">
We have presented CoSimRank, a new similar-
ity measure that can be computed for a single
node pair without relying on the similarities in the
whole graph. We gave two different formaliza-
tions of CoSimRank: (i) a derivation from Person-
alized PageRank and (ii) a matrix representation
that can take advantage of fast matrix multipli-
cation algorithms. We also presented extensions
of CoSimRank for a number of applications, thus
demonstrating the flexibility of CoSimRank as a
similarity measure.
We showed that CoSimRank is superior to
SimRank in time and space complexity; and
we demonstrated that CoSimRank performs bet-
ter than PPR+cos on two similarity computation
tasks.
Acknowledgments. This work was supported
by DFG (SCHU 2246/2-2).
</bodyText>
<page confidence="0.986301">
1400
</page>
<sectionHeader confidence="0.981437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997231112149532">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ’09, pages 19–27.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. In
WWW, pages 107–117.
Sung-Hyuk Cha. 2007. Comprehensive survey on dis-
tance/similarity measures between probability den-
sity functions. Mathematical Models and Methods
in Applied Sciences, 1(4):300–307.
Ching-Yun Chang, Stephen Clark, and Brian Harring-
ton. 2013. Getting creative with semantic similarity.
In Semantic Computing (ICSC), 2013 IEEE Seventh
International Conference on, pages 330–333.
Gerard de Melo and Gerhard Weikum. 2012. Uwn: A
large multilingual lexical knowledge base. In ACL
(System Demonstrations), pages 151–156.
Beate Dorow, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In Proceedings of the Workshop on
Geometrical Models of Natural Language Seman-
tics, GEMS ’09, pages 91–95.
G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. J. Artif. Intell. Res. (JAIR), 22:457–
479.
D´aniel Fogaras and Bal´azs R´acz. 2005. Scaling
link-based similarity search. In Proceedings of the
14th international conference on World Wide Web,
WWW ’05, pages 641–650.
Xianpei Han and Jun Zhao. 2010. Structural semantic
relatedness: a knowledge-based method to named
entity disambiguation. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, ACL ’10, pages 50–59.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank.
In Proceedings of the 11th international conference
on World Wide Web, WWW ’02, pages 517–526.
Cong Duy Vu Hoang and Min-Yen Kan. 2010. To-
wards automated related work summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ’10,
pages 427–435.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic relatedness with random graph walks. In
EMNLP-CoNLL, pages 581–589.
Tommi Jaakkola, David Haussler, et al. 1999. Exploit-
ing generative models in discriminative classifiers.
Advances in neural information processing systems,
pages 487–493.
Tony Jebara, Risi Kondor, and Andrew Howard. 2004.
Probability product kernels. The Journal ofMachine
Learning Research, 5:819–844.
Glen Jeh and Jennifer Widom. 2002. Simrank: a
measure of structural-context similarity. In Proceed-
ings of the eighth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
KDD ’02, pages 538–543.
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604–632.
Florian Laws, Lukas ichelbacher, Beate Dorow, Chris-
tian Scheible, Ulrich Heid, and Hinrich Sch¨utze.
2010. A linguistically grounded graph model
for bilingual lexicon extraction. In Coling 2010:
Posters, pages 614–622.
Elizabeth Leicht, Petter Holme, and Mark Newman.
2006. Vertex similarity in networks. Physical Re-
view E, 73(2):026120.
Ronny Lempel and Shlomo Moran. 2000. The
stochastic approach for link-structure analysis
(salsa) and the tkc effect. Computer Networks,
33(1):387–401.
Pei Li, Zhixu Li, Hongyan Liu, Jun He, and Xiaoy-
ong Du. 2009. Using link-based content analy-
sis to measure document similarity effectively. In
Proceedings of the Joint International Conferences
on Advances in Data and Web Management, AP-
Web/WAIM ’09, pages 455–467.
Cuiping Li, Jiawei Han, Guoming He, Xin Jin, Yizhou
Sun, Yintao Yu, and Tianyi Wu. 2010. Fast com-
putation of simrank for static and dynamic informa-
tion networks. In Proceedings of the 13th Interna-
tional Conference on Extending Database Technol-
ogy, EDBT ’10, pages 465–476.
Dmitry Lizorkin, Pavel Velikhov, Maxim Grinev, and
Denis Turdakov. 2010. Accuracy estimate and op-
timization techniques for simrank computation. The
VLDB Journal—The International Journal on Very
Large Data Bases, 19(1):45–66.
Rada Mihalcea and Dragomir Radev. 2011. Graph-
based natural language processing and information
retrieval. Cambridge University Press.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168.
Einat Minkov and William W. Cohen. 2008. Learn-
ing graph walk based similarity measures for parsed
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’08, pages 907–916.
</reference>
<page confidence="0.806513">
1401
</page>
<reference confidence="0.998878543478261">
Einat Minkov and William W. Cohen. 2012. Graph
based similarity measures for synonym extraction
from parsed text. In Workshop Proceedings of
TextGraphs-7 on Graph-based Methods for Natural
Language Processing, TextGraphs-7 ’12, pages 20–
24.
Pradeep Muthukrishnan, Dragomir Radev, and
Qiaozhu Mei. 2010. Edge weight regularization
over multiple graphs for similarity learning. In Data
Mining (ICDM), 2010 IEEE 10th International
Conference on, pages 374–383.
Diarmuid O´ S´eaghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proceedings of the 22nd International Conference
on Computational Linguistics-Volume 1, pages 649–
656.
Delip Rao, David Yarowsky, and Chris Callison-Burch.
2008. Affinity measures based on the graph Lapla-
cian. In Proceedings of the 3rd Textgraphs Work-
shop on Graph-Based Algorithms for Natural Lan-
guage Processing, TextGraphs-3, pages 41–48.
Reinhard Rapp, Serge Sharoff, and Bogdan Babych.
2012. Identifying word translations from compa-
rable documents without a seed lexicon. In LREC,
pages 460–466.
Mehran Sahami and Timothy D. Heilman. 2006. A
web-based kernel function for measuring the simi-
larity of short text snippets. In Proceedings of the
15th international conference on World Wide Web,
WWW ’06, pages 377–386.
Christian Scheible, Florian Laws, Lukas Michelbacher,
and Hinrich Sch¨utze. 2010. Sentiment transla-
tion through multi-edge graphs. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ’10, pages
1104–1112.
Hinrich Sch¨utze and Michael Walsh. 2008. A graph-
theoretic model of lexical syntactic acquisition. In
EMNLP, pages 917–926.
Wensi Xi, Edward A. Fox, Weiguo Fan, Benyu Zhang,
Zheng Chen, Jun Yan, and Dong Zhuang. 2005.
Simfusion: measuring similarity using unified re-
lationship matrix. In Proceedings of the 28th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ’05, pages 130–137.
</reference>
<page confidence="0.996455">
1402
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950387">
<title confidence="0.999449">CoSimRank: A Flexible &amp; Efficient Graph-Theoretic Similarity Measure</title>
<author confidence="0.997074">Sascha Rothe</author>
<author confidence="0.997074">Hinrich</author>
<affiliation confidence="0.9990685">Center for Information &amp; Language University of</affiliation>
<email confidence="0.962585">sascha@cis.lmu.de</email>
<abstract confidence="0.999506684210526">present a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph. We present equivalent formalizations that show CoSimRank’s close relationship to Personalized Page- Rank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSim- Rank. Another advantage of CoSimRank is that it can be flexibly extended from basic node-node similarity to several other graph-theoretic similarity measures. In an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction, CoSimRank is faster or more accurate than previous approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>In WWW,</booktitle>
<pages>107--117</pages>
<contexts>
<context position="1109" citStr="Brin and Page, 1998" startWordPosition="154" endWordPosition="157">nd SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank. Another advantage of CoSimRank is that it can be flexibly extended from basic node-node similarity to several other graph-theoretic similarity measures. In an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction, CoSimRank is faster or more accurate than previous approaches. 1 Introduction Graph-theoretic algorithms have been successfully applied to many problems in NLP (Mihalcea and Radev, 2011). These algorithms are often based on PageRank (Brin and Page, 1998) and other centrality measures (e.g., (Erkan and Radev, 2004)). An alternative for tasks involving similarity is SimRank (Jeh and Widom, 2002). SimRank is based on the simple intuition that nodes in a graph should be considered as similar to the extent that their neighbors are similar. Unfortunately, SimRank has time complexity O(n3) (where n is the number of nodes in the graph) and therefore does not scale to the large graphs that are typical of NLP. This paper introduces CoSimRank,1 a new graph-theoretic algorithm for computing node similarity that combines features of SimRank and PageRank. </context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. In WWW, pages 107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung-Hyuk Cha</author>
</authors>
<title>Comprehensive survey on distance/similarity measures between probability density functions.</title>
<date>2007</date>
<booktitle>Mathematical Models and Methods in Applied Sciences,</booktitle>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="13618" citStr="Cha (2007)" startWordPosition="2285" endWordPosition="2286">nequality gives the upper bound: (u,v) &lt; IluIl IlvIl where IlxIl is the norm of x. From Eq. 2 we get IIp(k)II1 = 1, where Il&apos;Il1 is the 1-norm. We also know from elementary functional analysis that the 1-norm is the biggest of all p-norms and so one � has �p(k)� � &lt; 1. It follows that CoSimRank grows more slowly than a geometric series and converges if |c |&lt; 1: c= k 1 1 − c If an upper bound of 1 is desired for s(i, j) (instead of 1/(1 − c)), then we can use s0: s0(i, j) = (1 − c)s(i, j) 3This type of similarity measure has also been used and investigated by O´ S´eaghdha and Copestake (2008), Cha (2007), Jebara et al. (2004) (probability product kernel) and (Jaakkola et al., 1999) (Fisher kernel) among others. 4 Comparison to SimRank The original SimRank equation can be written as follows (Jeh and Widom, 2002): 1, if i = j P c |N(i)||N(j)| k∈N(i) l∈N(j) where N(i) denotes the nodes connected to i. SimRank is computed iteratively. With A being the normalized adjacency matrix we can write SimRank in matrix formulation: R(0) = E R(k) = max{cAR(k−1)AT,R(0)} (7) where the maximum of two matrices refers to the element-wise maximum. We will now prove by induction that the matrix formulation of CoSi</context>
</contexts>
<marker>Cha, 2007</marker>
<rawString>Sung-Hyuk Cha. 2007. Comprehensive survey on distance/similarity measures between probability density functions. Mathematical Models and Methods in Applied Sciences, 1(4):300–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Yun Chang</author>
<author>Stephen Clark</author>
<author>Brian Harrington</author>
</authors>
<title>Getting creative with semantic similarity.</title>
<date>2013</date>
<booktitle>In Semantic Computing (ICSC), 2013 IEEE Seventh International Conference on,</booktitle>
<pages>330--333</pages>
<contexts>
<context position="6013" citStr="Chang et al. (2013)" startWordPosition="960" endWordPosition="963"> NLP are not bipartite and Jeh and Widom (2002) also proposed a SimRank variant for bipartite graphs. Another important similarity measure is cosine similarity of Personalized PageRank (PPR) vectors. We will refer to this measure as PPR+cos. Hughes and Ramage (2007) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs. Agirre et al. (2009) use PPR+cos for WordNet and for crosslingual studies. Like CoSimRank, PPR+cos is efficient when computing single node pair similarities; we therefore use it as one of our baselines below. This method is also used by Chang et al. (2013) for semantic relatedness. They also experimented with Euclidean distance and KL2A reviewer suggests that CoSimRank is an efficient version of SimRank in a way analogous to SALSA’s (Lempel and Moran, 2000) relationship to HITS (Kleinberg, 1999) in that different aspects of similarity are decoupled. divergence. Interestingly, a simpler method performed best when comparing with human similarity judgments. In this method only the entries corresponding to the compared nodes are used for a similarity score. Rao et al. (2008) compared PPR+cos to other graph based similarity measures like shortest-pa</context>
</contexts>
<marker>Chang, Clark, Harrington, 2013</marker>
<rawString>Ching-Yun Chang, Stephen Clark, and Brian Harrington. 2013. Getting creative with semantic similarity. In Semantic Computing (ICSC), 2013 IEEE Seventh International Conference on, pages 330–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard de Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>Uwn: A large multilingual lexical knowledge base.</title>
<date>2012</date>
<booktitle>In ACL (System Demonstrations),</booktitle>
<pages>151--156</pages>
<marker>de Melo, Weikum, 2012</marker>
<rawString>Gerard de Melo and Gerhard Weikum. 2012. Uwn: A large multilingual lexical knowledge base. In ACL (System Demonstrations), pages 151–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Dorow</author>
<author>Florian Laws</author>
<author>Lukas Michelbacher</author>
<author>Christian Scheible</author>
<author>Jason Utt</author>
</authors>
<title>A graphtheoretic algorithm for automatic extension of translation lexicons.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, GEMS ’09,</booktitle>
<pages>91--95</pages>
<contexts>
<context position="7546" citStr="Dorow et al. (2009)" startWordPosition="1208" endWordPosition="1211">rations of the random walk. This is often true of this type of algorithm; cf. (Sch¨utze and Walsh, 2008). LexRank (Erkan and Radev, 2004) is similar to PPR+cos in that it combines PageRank and cosine; it initializes the sentence similarity matrix of a document using cosine and then applies PageRank to compute lexical centrality. Despite this superficial relatedness, applications like lexicon extraction that look for similar entities and applications that look for central entities are quite different. In addition to faster versions of SimRank, there has also been work on extensions of SimRank. Dorow et al. (2009) and Laws et al. (2010) extend SimRank to edge weights, edge labels and multiple graphs. We use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below. A similar graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include wo</context>
<context position="17858" citStr="Dorow et al. (2009)" startWordPosition="3038" endWordPosition="3041">lexible tool for different NLP applications. 5.1 Weighted edges The use of weighted edges was first proposed in the PageRank patent. It is straightforward and easy to implement by replacing the row normalized adjacency matrix A with an arbitrary stochastic matrix P. We can use this edge weighted PageRank for CoSimRank. 5.2 CoSimRank across graphs We often want to compute the similarity of nodes in two different graphs with a known node-node correspondence; this is the scenario we are faced with in the lexicon extraction task (see Section 6). A variant of SimRank for this task was presented by Dorow et al. (2009). We will now present an equivalent method for CoSimRank. We denote the number of nodes in the two graphs U and V by |U |and |V |, respectively. We compute PPR vectors p ∈ R|U |and q ∈ R|V |for each graph. Let S(0) ∈ R|U|&amp;quot;|V |be the known node-node correspondences. The analog of CoSimRank (Eq. 4) for two graphs is then: The matrix formulation (cf. Eq. 6) is: S(k) = ckAkS(0)(BT)k + S(k−1) (10) where A and B are row-normalized adjacency matrices. We can interpret S(0) as a change of basis. A similar approach for word embeddings was published by Mikolov et al. (2013). They call S(0) the translati</context>
<context position="21944" citStr="Dorow et al., 2009" startWordPosition="3767" endWordPosition="3770">n. Nodes are nouns, adjectives and verbs occurring in Wikipedia. There are three types of edges, corresponding to three types of syntactic configurations extracted from the parsed Wikipedias: adjective-noun, verbobject and noun-noun coordination. Table 1 gives examples and number of nodes and edges. 6.1 Baselines We propose CoSimRank as an efficient algorithm for computing the similarity of nodes in a graph. Consequently, we compare against the two main methods for this task in NLP: SimRank and extensions of PageRank. We also compare against the MEE (Multi-Edge Extraction) variant of SimRank (Dorow et al., 2009), which handles labeled edges more efficiently than SimRank: SI(k) = c |T |τYET _ S(k) = max{SI(k), S(0)} where Aτ is the row-normalized adjacency matrix for edge type τ (see edge types in Table 1). Apart from SimRank, extensions of PageRank are the main methods for computing the similarity of nodes in graphs in NLP (e.g., Hughes and Ramage (2007), Agirre et al. (2009) and other papers discussed in related work). Generally, these methods compute the Personalized PageRank for each node (see Eq. 1). When the computation has converged, the similarity of two nodes is given by the cosine similarity</context>
</contexts>
<marker>Dorow, Laws, Michelbacher, Scheible, Utt, 2009</marker>
<rawString>Beate Dorow, Florian Laws, Lukas Michelbacher, Christian Scheible, and Jason Utt. 2009. A graphtheoretic algorithm for automatic extension of translation lexicons. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, GEMS ’09, pages 91–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Intell. Res. (JAIR),</journal>
<pages>22--457</pages>
<contexts>
<context position="1170" citStr="Erkan and Radev, 2004" startWordPosition="164" endWordPosition="167">matrix multiplication algorithms to compute CoSimRank. Another advantage of CoSimRank is that it can be flexibly extended from basic node-node similarity to several other graph-theoretic similarity measures. In an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction, CoSimRank is faster or more accurate than previous approaches. 1 Introduction Graph-theoretic algorithms have been successfully applied to many problems in NLP (Mihalcea and Radev, 2011). These algorithms are often based on PageRank (Brin and Page, 1998) and other centrality measures (e.g., (Erkan and Radev, 2004)). An alternative for tasks involving similarity is SimRank (Jeh and Widom, 2002). SimRank is based on the simple intuition that nodes in a graph should be considered as similar to the extent that their neighbors are similar. Unfortunately, SimRank has time complexity O(n3) (where n is the number of nodes in the graph) and therefore does not scale to the large graphs that are typical of NLP. This paper introduces CoSimRank,1 a new graph-theoretic algorithm for computing node similarity that combines features of SimRank and PageRank. Our key observation is that to compute the similarity of two </context>
<context position="7064" citStr="Erkan and Radev, 2004" startWordPosition="1133" endWordPosition="1136">e entries corresponding to the compared nodes are used for a similarity score. Rao et al. (2008) compared PPR+cos to other graph based similarity measures like shortest-path and bounded-length random walks. PPR+cos performed best except for a new similarity measure based on commute time. We do not compare against this new measure as it uses the graph Laplacian and so cannot be computed for a single node pair. One reason CoSimRank is efficient is that we need only compute a few iterations of the random walk. This is often true of this type of algorithm; cf. (Sch¨utze and Walsh, 2008). LexRank (Erkan and Radev, 2004) is similar to PPR+cos in that it combines PageRank and cosine; it initializes the sentence similarity matrix of a document using cosine and then applies PageRank to compute lexical centrality. Despite this superficial relatedness, applications like lexicon extraction that look for similar entities and applications that look for central entities are quite different. In addition to faster versions of SimRank, there has also been work on extensions of SimRank. Dorow et al. (2009) and Laws et al. (2010) extend SimRank to edge weights, edge labels and multiple graphs. We use their Multi-Edge Extra</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Intell. Res. (JAIR), 22:457– 479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D´aniel Fogaras</author>
<author>Bal´azs R´acz</author>
</authors>
<title>Scaling link-based similarity search.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th international conference on World Wide Web, WWW ’05,</booktitle>
<pages>641--650</pages>
<marker>Fogaras, R´acz, 2005</marker>
<rawString>D´aniel Fogaras and Bal´azs R´acz. 2005. Scaling link-based similarity search. In Proceedings of the 14th international conference on World Wide Web, WWW ’05, pages 641–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Jun Zhao</author>
</authors>
<title>Structural semantic relatedness: a knowledge-based method to named entity disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>50--59</pages>
<contexts>
<context position="8323" citStr="Han and Zhao, 2010" startWordPosition="1334" endWordPosition="1337">lines below. A similar graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include work on document similarity (Li et al., 2009), the transfer of sentiment information between languages (Scheible et al., 2010) and named entity disambiguation (Han and Zhao, 2010). Hoang and Kan (2010) use SimRank for related work summarization. Muthukrishnan et al. (2010) combine link based similarity and content based similarity for document clustering and classification. These approaches use at least one of cosine similarity, PageRank and SimRank. CoSimRank can either be interpreted as an efficient version of Sim1393 Rank or as a version of Personalized PageRank for similarity measurement. The novelty is that we compute similarity for vectors that are induced using a new algorithm, so that the similarity measurement is much more efficient when an application only ne</context>
</contexts>
<marker>Han, Zhao, 2010</marker>
<rawString>Xianpei Han and Jun Zhao. 2010. Structural semantic relatedness: a knowledge-based method to named entity disambiguation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 50–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 11th international conference on World Wide Web, WWW ’02,</booktitle>
<pages>517--526</pages>
<contexts>
<context position="9212" citStr="Haveliwala (2002)" startWordPosition="1475" endWordPosition="1476">nk. CoSimRank can either be interpreted as an efficient version of Sim1393 Rank or as a version of Personalized PageRank for similarity measurement. The novelty is that we compute similarity for vectors that are induced using a new algorithm, so that the similarity measurement is much more efficient when an application only needs a fraction of all O(n2) pairwise similarities. 3 CoSimRank We first first give an intuitive introduction of CoSimRank as a Personalized PageRank (PPR) derivative. Later on, we will give a matrix formulation to compare CoSimRank with SimRank. 3.1 Personalized PageRank Haveliwala (2002) introduced Personalized PageRank – or topic-sensitive PageRank – based on the idea that the uniform damping vector p(0) can be replaced by a personalized vector, which depends on node i. We usually set p(0)(i) = ez, with ez being a vector of the standard basis, i.e., the ith entry is 1 and all other entries are 0. The PPR vector of node i is given by: p(k)(i) = dAp(k−1)(i) + (1 − d)p(0)(i) (1) where A is the stochastic matrix of the Markov chain, i.e., the row normalized adjacency matrix. The damping factor d E (0, 1) ensures that the computation converges. The PPR vector after k iterations i</context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In Proceedings of the 11th international conference on World Wide Web, WWW ’02, pages 517–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cong Duy Vu Hoang</author>
<author>Min-Yen Kan</author>
</authors>
<title>Towards automated related work summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>427--435</pages>
<contexts>
<context position="8345" citStr="Hoang and Kan (2010)" startWordPosition="1338" endWordPosition="1341">r graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include work on document similarity (Li et al., 2009), the transfer of sentiment information between languages (Scheible et al., 2010) and named entity disambiguation (Han and Zhao, 2010). Hoang and Kan (2010) use SimRank for related work summarization. Muthukrishnan et al. (2010) combine link based similarity and content based similarity for document clustering and classification. These approaches use at least one of cosine similarity, PageRank and SimRank. CoSimRank can either be interpreted as an efficient version of Sim1393 Rank or as a version of Personalized PageRank for similarity measurement. The novelty is that we compute similarity for vectors that are induced using a new algorithm, so that the similarity measurement is much more efficient when an application only needs a fraction of all </context>
</contexts>
<marker>Hoang, Kan, 2010</marker>
<rawString>Cong Duy Vu Hoang and Min-Yen Kan. 2010. Towards automated related work summarization. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 427–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>581--589</pages>
<contexts>
<context position="5660" citStr="Hughes and Ramage (2007)" startWordPosition="900" endWordPosition="903">such applications because it is based on a local formulation of the similarity measure.2 Apart from SimRank, many other similarity measures have been proposed. Leicht et al. (2006) introduce a similarity measure that is also based on the idea that nodes are similar when their neighbors are, but that is designed for bipartite graphs. However, most graphs in NLP are not bipartite and Jeh and Widom (2002) also proposed a SimRank variant for bipartite graphs. Another important similarity measure is cosine similarity of Personalized PageRank (PPR) vectors. We will refer to this measure as PPR+cos. Hughes and Ramage (2007) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs. Agirre et al. (2009) use PPR+cos for WordNet and for crosslingual studies. Like CoSimRank, PPR+cos is efficient when computing single node pair similarities; we therefore use it as one of our baselines below. This method is also used by Chang et al. (2013) for semantic relatedness. They also experimented with Euclidean distance and KL2A reviewer suggests that CoSimRank is an efficient version of SimRank in a way analogous to SALSA’s (Lempel and Moran, 2000) relationship to HITS (Kleinberg, 1999) in</context>
<context position="10909" citStr="Hughes and Ramage, 2007" startWordPosition="1783" endWordPosition="1786">tor later in the calculation. p(k) = Ap(k−1) (2) Note that the personalization vector p(0) was eliminated, but is still present as the starting vector of the iteration. 3.2 Similarity of vectors Let p(i) be the PPR vector of node i. The cosine of two vectors u and v is computed by dividing Figure 1: Graph motivating CoSimRank algorithm. Whereas PPR gives relatively high similarity to the pair (law,suit), CoSimRank assigns the pair similarity 0. the inner product (u, v) by the lengths of the vectors. The cosine of two PPR vectors can be used as a similarity measure for the corresponding nodes (Hughes and Ramage, 2007; Agirre et al., 2009): s(i,j) = (p(i),p(j)) (3) |p(i)||p(j)| This measure s(i, j) looks at the probability that a random walker is on a certain edge after an unlimited number of steps. This is potentially problematic as the example in Figure 1 shows. The PPR vectors of suit and dress will have some weight on tailor, which is good. However, the PPR vector of law will also have a non-zero weight for tailor. So law and dress are similar because of the node tailor. This is undesirable. We can prevent this type of spurious similarity by taking into account the path the random surfer took to get to</context>
<context position="22293" citStr="Hughes and Ramage (2007)" startWordPosition="3829" endWordPosition="3832">ficient algorithm for computing the similarity of nodes in a graph. Consequently, we compare against the two main methods for this task in NLP: SimRank and extensions of PageRank. We also compare against the MEE (Multi-Edge Extraction) variant of SimRank (Dorow et al., 2009), which handles labeled edges more efficiently than SimRank: SI(k) = c |T |τYET _ S(k) = max{SI(k), S(0)} where Aτ is the row-normalized adjacency matrix for edge type τ (see edge types in Table 1). Apart from SimRank, extensions of PageRank are the main methods for computing the similarity of nodes in graphs in NLP (e.g., Hughes and Ramage (2007), Agirre et al. (2009) and other papers discussed in related work). Generally, these methods compute the Personalized PageRank for each node (see Eq. 1). When the computation has converged, the similarity of two nodes is given by the cosine similarity of the Personalized PageRank vectors. We implemented this method for our experiments and call it PPR+cos. 6.2 Synonym Extraction We use TS68, a test set of 68 synonym pairs published by Minkov and Cohen (2012) for evaluation. This gold standard lists a single word as the AτS(k−1)BT τ 1397 P@1 P@10 MRR one-synonym PPR+cos 20.6% 52.9% 0.32 SimRank </context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Thad Hughes and Daniel Ramage. 2007. Lexical semantic relatedness with random graph walks. In EMNLP-CoNLL, pages 581–589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommi Jaakkola</author>
<author>David Haussler</author>
</authors>
<title>Exploiting generative models in discriminative classifiers. Advances in neural information processing systems,</title>
<date>1999</date>
<pages>487--493</pages>
<marker>Jaakkola, Haussler, 1999</marker>
<rawString>Tommi Jaakkola, David Haussler, et al. 1999. Exploiting generative models in discriminative classifiers. Advances in neural information processing systems, pages 487–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Jebara</author>
<author>Risi Kondor</author>
<author>Andrew Howard</author>
</authors>
<title>Probability product kernels.</title>
<date>2004</date>
<journal>The Journal ofMachine Learning Research,</journal>
<pages>5--819</pages>
<contexts>
<context position="13640" citStr="Jebara et al. (2004)" startWordPosition="2287" endWordPosition="2290">ves the upper bound: (u,v) &lt; IluIl IlvIl where IlxIl is the norm of x. From Eq. 2 we get IIp(k)II1 = 1, where Il&apos;Il1 is the 1-norm. We also know from elementary functional analysis that the 1-norm is the biggest of all p-norms and so one � has �p(k)� � &lt; 1. It follows that CoSimRank grows more slowly than a geometric series and converges if |c |&lt; 1: c= k 1 1 − c If an upper bound of 1 is desired for s(i, j) (instead of 1/(1 − c)), then we can use s0: s0(i, j) = (1 − c)s(i, j) 3This type of similarity measure has also been used and investigated by O´ S´eaghdha and Copestake (2008), Cha (2007), Jebara et al. (2004) (probability product kernel) and (Jaakkola et al., 1999) (Fisher kernel) among others. 4 Comparison to SimRank The original SimRank equation can be written as follows (Jeh and Widom, 2002): 1, if i = j P c |N(i)||N(j)| k∈N(i) l∈N(j) where N(i) denotes the nodes connected to i. SimRank is computed iteratively. With A being the normalized adjacency matrix we can write SimRank in matrix formulation: R(0) = E R(k) = max{cAR(k−1)AT,R(0)} (7) where the maximum of two matrices refers to the element-wise maximum. We will now prove by induction that the matrix formulation of CoSimRank (Eq. 6) is equiv</context>
</contexts>
<marker>Jebara, Kondor, Howard, 2004</marker>
<rawString>Tony Jebara, Risi Kondor, and Andrew Howard. 2004. Probability product kernels. The Journal ofMachine Learning Research, 5:819–844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glen Jeh</author>
<author>Jennifer Widom</author>
</authors>
<title>Simrank: a measure of structural-context similarity.</title>
<date>2002</date>
<booktitle>In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02,</booktitle>
<pages>538--543</pages>
<contexts>
<context position="1251" citStr="Jeh and Widom, 2002" startWordPosition="177" endWordPosition="180">nk is that it can be flexibly extended from basic node-node similarity to several other graph-theoretic similarity measures. In an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction, CoSimRank is faster or more accurate than previous approaches. 1 Introduction Graph-theoretic algorithms have been successfully applied to many problems in NLP (Mihalcea and Radev, 2011). These algorithms are often based on PageRank (Brin and Page, 1998) and other centrality measures (e.g., (Erkan and Radev, 2004)). An alternative for tasks involving similarity is SimRank (Jeh and Widom, 2002). SimRank is based on the simple intuition that nodes in a graph should be considered as similar to the extent that their neighbors are similar. Unfortunately, SimRank has time complexity O(n3) (where n is the number of nodes in the graph) and therefore does not scale to the large graphs that are typical of NLP. This paper introduces CoSimRank,1 a new graph-theoretic algorithm for computing node similarity that combines features of SimRank and PageRank. Our key observation is that to compute the similarity of two nodes, we need not consider 1Code available at code.google.com/p/cistern all othe</context>
<context position="3973" citStr="Jeh and Widom, 2002" startWordPosition="616" endWordPosition="619">ing some useful extensions, we demonstrate the great flexibility of CoSimRank (Section 5). We perform an experimental evaluation of CoSimRank in Section 6. Section 7 summarizes the paper. 2 Related Work Our work is unsupervised. We therefore do not review graph-based methods that make extensive 1392 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1392–1402, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics use of supervised learning (e.g., de Melo and Weikum (2012)). Since the original version of SimRank (Jeh and Widom, 2002) has complexity 0(n4), many extensions have been proposed to speed up its calculation. A Monte Carlo algorithm, which is scalable to the whole web, was suggested by Fogaras and R´acz (2005). However, in an evaluation of this algorithm we found that it does not give competitive results (see Section 6). A matrix representation of SimRank called SimFusion (Xi et al., 2005) improves the computational complexity from 0(n4) to 0(n3). Lizorkin et al. (2010) also reduce complexity to 0(n3) by selecting essential node pairs and using partial sums. They also give a useful overview of SimRank, SimFusion </context>
<context position="5441" citStr="Jeh and Widom (2002)" startWordPosition="867" endWordPosition="870"> and are not efficient enough for very large graphs. We are interested in applications that only need a fraction of all 0(n2) pairwise similarities. The algorithm we propose below is an order of magnitude faster in such applications because it is based on a local formulation of the similarity measure.2 Apart from SimRank, many other similarity measures have been proposed. Leicht et al. (2006) introduce a similarity measure that is also based on the idea that nodes are similar when their neighbors are, but that is designed for bipartite graphs. However, most graphs in NLP are not bipartite and Jeh and Widom (2002) also proposed a SimRank variant for bipartite graphs. Another important similarity measure is cosine similarity of Personalized PageRank (PPR) vectors. We will refer to this measure as PPR+cos. Hughes and Ramage (2007) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs. Agirre et al. (2009) use PPR+cos for WordNet and for crosslingual studies. Like CoSimRank, PPR+cos is efficient when computing single node pair similarities; we therefore use it as one of our baselines below. This method is also used by Chang et al. (2013) for semantic relatedness. T</context>
<context position="13829" citStr="Jeh and Widom, 2002" startWordPosition="2316" endWordPosition="2319"> 1-norm is the biggest of all p-norms and so one � has �p(k)� � &lt; 1. It follows that CoSimRank grows more slowly than a geometric series and converges if |c |&lt; 1: c= k 1 1 − c If an upper bound of 1 is desired for s(i, j) (instead of 1/(1 − c)), then we can use s0: s0(i, j) = (1 − c)s(i, j) 3This type of similarity measure has also been used and investigated by O´ S´eaghdha and Copestake (2008), Cha (2007), Jebara et al. (2004) (probability product kernel) and (Jaakkola et al., 1999) (Fisher kernel) among others. 4 Comparison to SimRank The original SimRank equation can be written as follows (Jeh and Widom, 2002): 1, if i = j P c |N(i)||N(j)| k∈N(i) l∈N(j) where N(i) denotes the nodes connected to i. SimRank is computed iteratively. With A being the normalized adjacency matrix we can write SimRank in matrix formulation: R(0) = E R(k) = max{cAR(k−1)AT,R(0)} (7) where the maximum of two matrices refers to the element-wise maximum. We will now prove by induction that the matrix formulation of CoSimRank (Eq. 6) is equivalent to: S0(k) = cAS0(k−1)AT + S(0) (8) and thus very similar to SimRank (Eq. 7). The base case S(1) = S0(1) is trivial. Inductive step: S0(k) (8)= cAS0(k−1)AT + S(0) = cA(ck−1Ak−1(AT)k−1 </context>
</contexts>
<marker>Jeh, Widom, 2002</marker>
<rawString>Glen Jeh and Jennifer Widom. 2002. Simrank: a measure of structural-context similarity. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02, pages 538–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>Journal of the ACM,</journal>
<volume>46</volume>
<issue>5</issue>
<contexts>
<context position="6257" citStr="Kleinberg, 1999" startWordPosition="1000" endWordPosition="1001">s and Ramage (2007) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs. Agirre et al. (2009) use PPR+cos for WordNet and for crosslingual studies. Like CoSimRank, PPR+cos is efficient when computing single node pair similarities; we therefore use it as one of our baselines below. This method is also used by Chang et al. (2013) for semantic relatedness. They also experimented with Euclidean distance and KL2A reviewer suggests that CoSimRank is an efficient version of SimRank in a way analogous to SALSA’s (Lempel and Moran, 2000) relationship to HITS (Kleinberg, 1999) in that different aspects of similarity are decoupled. divergence. Interestingly, a simpler method performed best when comparing with human similarity judgments. In this method only the entries corresponding to the compared nodes are used for a similarity score. Rao et al. (2008) compared PPR+cos to other graph based similarity measures like shortest-path and bounded-length random walks. PPR+cos performed best except for a new similarity measure based on commute time. We do not compare against this new measure as it uses the graph Laplacian and so cannot be computed for a single node pair. On</context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Laws</author>
<author>Lukas ichelbacher</author>
<author>Beate Dorow</author>
<author>Christian Scheible</author>
<author>Ulrich Heid</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>A linguistically grounded graph model for bilingual lexicon extraction. In Coling 2010: Posters,</title>
<date>2010</date>
<pages>614--622</pages>
<marker>Laws, ichelbacher, Dorow, Scheible, Heid, Sch¨utze, 2010</marker>
<rawString>Florian Laws, Lukas ichelbacher, Beate Dorow, Christian Scheible, Ulrich Heid, and Hinrich Sch¨utze. 2010. A linguistically grounded graph model for bilingual lexicon extraction. In Coling 2010: Posters, pages 614–622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Leicht</author>
<author>Petter Holme</author>
<author>Mark Newman</author>
</authors>
<title>Vertex similarity in networks. Physical Review E,</title>
<date>2006</date>
<contexts>
<context position="5216" citStr="Leicht et al. (2006)" startWordPosition="827" endWordPosition="830">hods of Fogaras and R´acz (2005). A non-iterative computation for SimRank was introduced by Li et al. (2010). This is especially useful for dynamic graphs. However, all of these methods have to run SimRank on the entire graph and are not efficient enough for very large graphs. We are interested in applications that only need a fraction of all 0(n2) pairwise similarities. The algorithm we propose below is an order of magnitude faster in such applications because it is based on a local formulation of the similarity measure.2 Apart from SimRank, many other similarity measures have been proposed. Leicht et al. (2006) introduce a similarity measure that is also based on the idea that nodes are similar when their neighbors are, but that is designed for bipartite graphs. However, most graphs in NLP are not bipartite and Jeh and Widom (2002) also proposed a SimRank variant for bipartite graphs. Another important similarity measure is cosine similarity of Personalized PageRank (PPR) vectors. We will refer to this measure as PPR+cos. Hughes and Ramage (2007) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs. Agirre et al. (2009) use PPR+cos for WordNet and for crossl</context>
</contexts>
<marker>Leicht, Holme, Newman, 2006</marker>
<rawString>Elizabeth Leicht, Petter Holme, and Mark Newman. 2006. Vertex similarity in networks. Physical Review E, 73(2):026120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronny Lempel</author>
<author>Shlomo Moran</author>
</authors>
<title>The stochastic approach for link-structure analysis (salsa) and the tkc effect.</title>
<date>2000</date>
<journal>Computer Networks,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="6218" citStr="Lempel and Moran, 2000" startWordPosition="993" endWordPosition="996">e will refer to this measure as PPR+cos. Hughes and Ramage (2007) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs. Agirre et al. (2009) use PPR+cos for WordNet and for crosslingual studies. Like CoSimRank, PPR+cos is efficient when computing single node pair similarities; we therefore use it as one of our baselines below. This method is also used by Chang et al. (2013) for semantic relatedness. They also experimented with Euclidean distance and KL2A reviewer suggests that CoSimRank is an efficient version of SimRank in a way analogous to SALSA’s (Lempel and Moran, 2000) relationship to HITS (Kleinberg, 1999) in that different aspects of similarity are decoupled. divergence. Interestingly, a simpler method performed best when comparing with human similarity judgments. In this method only the entries corresponding to the compared nodes are used for a similarity score. Rao et al. (2008) compared PPR+cos to other graph based similarity measures like shortest-path and bounded-length random walks. PPR+cos performed best except for a new similarity measure based on commute time. We do not compare against this new measure as it uses the graph Laplacian and so cannot</context>
</contexts>
<marker>Lempel, Moran, 2000</marker>
<rawString>Ronny Lempel and Shlomo Moran. 2000. The stochastic approach for link-structure analysis (salsa) and the tkc effect. Computer Networks, 33(1):387–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pei Li</author>
<author>Zhixu Li</author>
<author>Hongyan Liu</author>
<author>Jun He</author>
<author>Xiaoyong Du</author>
</authors>
<title>Using link-based content analysis to measure document similarity effectively.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint International Conferences on Advances in Data and Web Management, APWeb/WAIM ’09,</booktitle>
<pages>455--467</pages>
<contexts>
<context position="8189" citStr="Li et al., 2009" startWordPosition="1314" endWordPosition="1317">end SimRank to edge weights, edge labels and multiple graphs. We use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below. A similar graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include work on document similarity (Li et al., 2009), the transfer of sentiment information between languages (Scheible et al., 2010) and named entity disambiguation (Han and Zhao, 2010). Hoang and Kan (2010) use SimRank for related work summarization. Muthukrishnan et al. (2010) combine link based similarity and content based similarity for document clustering and classification. These approaches use at least one of cosine similarity, PageRank and SimRank. CoSimRank can either be interpreted as an efficient version of Sim1393 Rank or as a version of Personalized PageRank for similarity measurement. The novelty is that we compute similarity for</context>
</contexts>
<marker>Li, Li, Liu, He, Du, 2009</marker>
<rawString>Pei Li, Zhixu Li, Hongyan Liu, Jun He, and Xiaoyong Du. 2009. Using link-based content analysis to measure document similarity effectively. In Proceedings of the Joint International Conferences on Advances in Data and Web Management, APWeb/WAIM ’09, pages 455–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cuiping Li</author>
<author>Jiawei Han</author>
<author>Guoming He</author>
<author>Xin Jin</author>
<author>Yizhou Sun</author>
<author>Yintao Yu</author>
<author>Tianyi Wu</author>
</authors>
<title>Fast computation of simrank for static and dynamic information networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 13th International Conference on Extending Database Technology, EDBT ’10,</booktitle>
<pages>465--476</pages>
<contexts>
<context position="4704" citStr="Li et al. (2010)" startWordPosition="740" endWordPosition="743">ch is scalable to the whole web, was suggested by Fogaras and R´acz (2005). However, in an evaluation of this algorithm we found that it does not give competitive results (see Section 6). A matrix representation of SimRank called SimFusion (Xi et al., 2005) improves the computational complexity from 0(n4) to 0(n3). Lizorkin et al. (2010) also reduce complexity to 0(n3) by selecting essential node pairs and using partial sums. They also give a useful overview of SimRank, SimFusion and the Monte Carlo methods of Fogaras and R´acz (2005). A non-iterative computation for SimRank was introduced by Li et al. (2010). This is especially useful for dynamic graphs. However, all of these methods have to run SimRank on the entire graph and are not efficient enough for very large graphs. We are interested in applications that only need a fraction of all 0(n2) pairwise similarities. The algorithm we propose below is an order of magnitude faster in such applications because it is based on a local formulation of the similarity measure.2 Apart from SimRank, many other similarity measures have been proposed. Leicht et al. (2006) introduce a similarity measure that is also based on the idea that nodes are similar wh</context>
</contexts>
<marker>Li, Han, He, Jin, Sun, Yu, Wu, 2010</marker>
<rawString>Cuiping Li, Jiawei Han, Guoming He, Xin Jin, Yizhou Sun, Yintao Yu, and Tianyi Wu. 2010. Fast computation of simrank for static and dynamic information networks. In Proceedings of the 13th International Conference on Extending Database Technology, EDBT ’10, pages 465–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Lizorkin</author>
<author>Pavel Velikhov</author>
<author>Maxim Grinev</author>
<author>Denis Turdakov</author>
</authors>
<title>Accuracy estimate and optimization techniques for simrank computation.</title>
<date>2010</date>
<booktitle>The VLDB Journal—The International Journal on Very Large Data Bases,</booktitle>
<contexts>
<context position="4427" citStr="Lizorkin et al. (2010)" startWordPosition="693" endWordPosition="696">2014 Association for Computational Linguistics use of supervised learning (e.g., de Melo and Weikum (2012)). Since the original version of SimRank (Jeh and Widom, 2002) has complexity 0(n4), many extensions have been proposed to speed up its calculation. A Monte Carlo algorithm, which is scalable to the whole web, was suggested by Fogaras and R´acz (2005). However, in an evaluation of this algorithm we found that it does not give competitive results (see Section 6). A matrix representation of SimRank called SimFusion (Xi et al., 2005) improves the computational complexity from 0(n4) to 0(n3). Lizorkin et al. (2010) also reduce complexity to 0(n3) by selecting essential node pairs and using partial sums. They also give a useful overview of SimRank, SimFusion and the Monte Carlo methods of Fogaras and R´acz (2005). A non-iterative computation for SimRank was introduced by Li et al. (2010). This is especially useful for dynamic graphs. However, all of these methods have to run SimRank on the entire graph and are not efficient enough for very large graphs. We are interested in applications that only need a fraction of all 0(n2) pairwise similarities. The algorithm we propose below is an order of magnitude f</context>
</contexts>
<marker>Lizorkin, Velikhov, Grinev, Turdakov, 2010</marker>
<rawString>Dmitry Lizorkin, Pavel Velikhov, Maxim Grinev, and Denis Turdakov. 2010. Accuracy estimate and optimization techniques for simrank computation. The VLDB Journal—The International Journal on Very Large Data Bases, 19(1):45–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dragomir Radev</author>
</authors>
<title>Graphbased natural language processing and information retrieval.</title>
<date>2011</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1041" citStr="Mihalcea and Radev, 2011" startWordPosition="143" endWordPosition="146">tions that show CoSimRank’s close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank. Another advantage of CoSimRank is that it can be flexibly extended from basic node-node similarity to several other graph-theoretic similarity measures. In an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction, CoSimRank is faster or more accurate than previous approaches. 1 Introduction Graph-theoretic algorithms have been successfully applied to many problems in NLP (Mihalcea and Radev, 2011). These algorithms are often based on PageRank (Brin and Page, 1998) and other centrality measures (e.g., (Erkan and Radev, 2004)). An alternative for tasks involving similarity is SimRank (Jeh and Widom, 2002). SimRank is based on the simple intuition that nodes in a graph should be considered as similar to the extent that their neighbors are similar. Unfortunately, SimRank has time complexity O(n3) (where n is the number of nodes in the graph) and therefore does not scale to the large graphs that are typical of NLP. This paper introduces CoSimRank,1 a new graph-theoretic algorithm for comput</context>
</contexts>
<marker>Mihalcea, Radev, 2011</marker>
<rawString>Rada Mihalcea and Dragomir Radev. 2011. Graphbased natural language processing and information retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</title>
<date>2013</date>
<contexts>
<context position="18428" citStr="Mikolov et al. (2013)" startWordPosition="3147" endWordPosition="3150"> for this task was presented by Dorow et al. (2009). We will now present an equivalent method for CoSimRank. We denote the number of nodes in the two graphs U and V by |U |and |V |, respectively. We compute PPR vectors p ∈ R|U |and q ∈ R|V |for each graph. Let S(0) ∈ R|U|&amp;quot;|V |be the known node-node correspondences. The analog of CoSimRank (Eq. 4) for two graphs is then: The matrix formulation (cf. Eq. 6) is: S(k) = ckAkS(0)(BT)k + S(k−1) (10) where A and B are row-normalized adjacency matrices. We can interpret S(0) as a change of basis. A similar approach for word embeddings was published by Mikolov et al. (2013). They call S(0) the translation matrix. 5.3 Typed edges To be able to directly compare to prior work in our experiments, we also present a method to integrate a set of typed edges T in the CoSimRank calculation. For this we will compute a similarity matrix for each edge type T and merge them into one matrix for the next iteration: S(k) = |c|TτETX AτS(k−1)BT + S(0) (11) τ ! This formula is identical to the random surfer model where two surfers only meet iff they are on the same node and used the same edge type to get there. A more strict claim would be to use the same edge type at any time of </context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>William W Cohen</author>
</authors>
<title>Learning graph walk based similarity measures for parsed text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>907--916</pages>
<contexts>
<context position="7794" citStr="Minkov and Cohen (2008)" startWordPosition="1252" endWordPosition="1255">ix of a document using cosine and then applies PageRank to compute lexical centrality. Despite this superficial relatedness, applications like lexicon extraction that look for similar entities and applications that look for central entities are quite different. In addition to faster versions of SimRank, there has also been work on extensions of SimRank. Dorow et al. (2009) and Laws et al. (2010) extend SimRank to edge weights, edge labels and multiple graphs. We use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below. A similar graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include work on document similarity (Li et al., 2009), the transfer of sentiment information between languages (Scheible et al., 2010) and named entity disambiguation (Han and Zhao, 2010). Hoang and Kan (2010) use SimRank for related work summarization. Muth</context>
</contexts>
<marker>Minkov, Cohen, 2008</marker>
<rawString>Einat Minkov and William W. Cohen. 2008. Learning graph walk based similarity measures for parsed text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 907–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>William W Cohen</author>
</authors>
<title>Graph based similarity measures for synonym extraction from parsed text.</title>
<date>2012</date>
<booktitle>In Workshop Proceedings of TextGraphs-7 on Graph-based Methods for Natural Language Processing, TextGraphs-7 ’12,</booktitle>
<pages>20--24</pages>
<contexts>
<context position="7975" citStr="Minkov and Cohen, 2012" startWordPosition="1279" endWordPosition="1282">ar entities and applications that look for central entities are quite different. In addition to faster versions of SimRank, there has also been work on extensions of SimRank. Dorow et al. (2009) and Laws et al. (2010) extend SimRank to edge weights, edge labels and multiple graphs. We use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below. A similar graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include work on document similarity (Li et al., 2009), the transfer of sentiment information between languages (Scheible et al., 2010) and named entity disambiguation (Han and Zhao, 2010). Hoang and Kan (2010) use SimRank for related work summarization. Muthukrishnan et al. (2010) combine link based similarity and content based similarity for document clustering and classification. These approaches use at least one of cosine similarity</context>
<context position="22754" citStr="Minkov and Cohen (2012)" startWordPosition="3907" endWordPosition="3910">n Table 1). Apart from SimRank, extensions of PageRank are the main methods for computing the similarity of nodes in graphs in NLP (e.g., Hughes and Ramage (2007), Agirre et al. (2009) and other papers discussed in related work). Generally, these methods compute the Personalized PageRank for each node (see Eq. 1). When the computation has converged, the similarity of two nodes is given by the cosine similarity of the Personalized PageRank vectors. We implemented this method for our experiments and call it PPR+cos. 6.2 Synonym Extraction We use TS68, a test set of 68 synonym pairs published by Minkov and Cohen (2012) for evaluation. This gold standard lists a single word as the AτS(k−1)BT τ 1397 P@1 P@10 MRR one-synonym PPR+cos 20.6% 52.9% 0.32 SimRank 25.0% 61.8% 0.37 CoSimRank 25.0% 61.8% 0.37 Typed CoSimRank 23.5% 63.2% 0.37 extended PPR+cos 32.6% 73.5% 0.48 SimRank 45.6% 83.8% 0.59 CoSimRank 45.6% 83.8% 0.59 Typed CoSimRank 44.1% 83.8% 0.59 Table 2: Results for synonym extraction on TS68. Best result in each column in bold. correct synonym even if there are several equally acceptable near-synonyms (see Table 3 for examples). We call this the one-synonym evaluation. Three native English speakers were a</context>
<context position="24679" citStr="Minkov and Cohen (2012)" startWordPosition="4240" endWordPosition="4243">oportion of words correctly translated by word in the top position (P@1), proportion of words correctly translated by a word in one of the top 10 positions (P@10) and Mean Reciprocal Rank (MRR). CoSimRank’s MRR scores of 0.37 (one-synonym) and 0.59 (extended) are the same or better than all baselines (see Table 2). CoSimRank and SimRank have the same P@1 and P@10 accuracy (although they differed on some decisions). CoSimRank is better than PPR+cos on both evaluations, but as this test set is very small, the results are not significant. Table 3 shows a sample of synonyms proposed by CoSimRank. Minkov and Cohen (2012) tested cosine and random-walk measures on grammatical relationkeyword expected extracted movie film film modern contemporary contemporary demonstrate protest show attractive appealing beautiful economic profitable financial close shut open Table 3: Examples for extracted synonyms. Correct synonyms according to extended evaluation in bold. ships (similar to our setup) as well as on cooccurrence statistics. The MRR scores for these methods range from 0.29 to 0.59. (MRR is equivalent to MAP as reported by Minkov and Cohen (2012) when there is only one correct answer.) Their best number (0.59) is</context>
</contexts>
<marker>Minkov, Cohen, 2012</marker>
<rawString>Einat Minkov and William W. Cohen. 2012. Graph based similarity measures for synonym extraction from parsed text. In Workshop Proceedings of TextGraphs-7 on Graph-based Methods for Natural Language Processing, TextGraphs-7 ’12, pages 20– 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pradeep Muthukrishnan</author>
<author>Dragomir Radev</author>
<author>Qiaozhu Mei</author>
</authors>
<title>Edge weight regularization over multiple graphs for similarity learning.</title>
<date>2010</date>
<booktitle>In Data Mining (ICDM), 2010 IEEE 10th International Conference on,</booktitle>
<pages>374--383</pages>
<contexts>
<context position="8417" citStr="Muthukrishnan et al. (2010)" startWordPosition="1349" endWordPosition="1352">008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include work on document similarity (Li et al., 2009), the transfer of sentiment information between languages (Scheible et al., 2010) and named entity disambiguation (Han and Zhao, 2010). Hoang and Kan (2010) use SimRank for related work summarization. Muthukrishnan et al. (2010) combine link based similarity and content based similarity for document clustering and classification. These approaches use at least one of cosine similarity, PageRank and SimRank. CoSimRank can either be interpreted as an efficient version of Sim1393 Rank or as a version of Personalized PageRank for similarity measurement. The novelty is that we compute similarity for vectors that are induced using a new algorithm, so that the similarity measurement is much more efficient when an application only needs a fraction of all O(n2) pairwise similarities. 3 CoSimRank We first first give an intuitiv</context>
</contexts>
<marker>Muthukrishnan, Radev, Mei, 2010</marker>
<rawString>Pradeep Muthukrishnan, Dragomir Radev, and Qiaozhu Mei. 2010. Edge weight regularization over multiple graphs for similarity learning. In Data Mining (ICDM), 2010 IEEE 10th International Conference on, pages 374–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Ann Copestake</author>
</authors>
<title>Semantic classification with distributional kernels.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>649--656</pages>
<marker>S´eaghdha, Copestake, 2008</marker>
<rawString>Diarmuid O´ S´eaghdha and Ann Copestake. 2008. Semantic classification with distributional kernels. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 649– 656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Affinity measures based on the graph Laplacian.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd Textgraphs Workshop on Graph-Based Algorithms for Natural Language Processing, TextGraphs-3,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="6538" citStr="Rao et al. (2008)" startWordPosition="1042" endWordPosition="1045">refore use it as one of our baselines below. This method is also used by Chang et al. (2013) for semantic relatedness. They also experimented with Euclidean distance and KL2A reviewer suggests that CoSimRank is an efficient version of SimRank in a way analogous to SALSA’s (Lempel and Moran, 2000) relationship to HITS (Kleinberg, 1999) in that different aspects of similarity are decoupled. divergence. Interestingly, a simpler method performed best when comparing with human similarity judgments. In this method only the entries corresponding to the compared nodes are used for a similarity score. Rao et al. (2008) compared PPR+cos to other graph based similarity measures like shortest-path and bounded-length random walks. PPR+cos performed best except for a new similarity measure based on commute time. We do not compare against this new measure as it uses the graph Laplacian and so cannot be computed for a single node pair. One reason CoSimRank is efficient is that we need only compute a few iterations of the random walk. This is often true of this type of algorithm; cf. (Sch¨utze and Walsh, 2008). LexRank (Erkan and Radev, 2004) is similar to PPR+cos in that it combines PageRank and cosine; it initial</context>
</contexts>
<marker>Rao, Yarowsky, Callison-Burch, 2008</marker>
<rawString>Delip Rao, David Yarowsky, and Chris Callison-Burch. 2008. Affinity measures based on the graph Laplacian. In Proceedings of the 3rd Textgraphs Workshop on Graph-Based Algorithms for Natural Language Processing, TextGraphs-3, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
<author>Serge Sharoff</author>
<author>Bogdan Babych</author>
</authors>
<title>Identifying word translations from comparable documents without a seed lexicon. In</title>
<date>2012</date>
<booktitle>LREC,</booktitle>
<pages>460--466</pages>
<contexts>
<context position="30190" citStr="Rapp et al., 2012" startWordPosition="5159" endWordPosition="5162">e for synonym extraction and lexicon extraction, respectively. CoSimRank should generally be three times faster than typed CoSimRank since the typed version has to repeat the computation for each of the three types. This effect is only visible on the larger test set (lexicon extraction) because the general computation overhead is about the same on a smaller test set. 6.5 Comparison with WINTIAN Here we address inducing a bilingual lexicon from a seed set based on grammatical relations found by a parser. An alternative approach is to induce a bilingual lexicon from Wikipedia’s interwiki links (Rapp et al., 2012). These two approaches have different strengths and weaknesses; e.g., the interwiki-link-based approach does not require a seed set, but it can only be applied to comparable corpora that consist of corresponding – although not necessarily “parallel” – documents. Despite these differences it is still interesting to compare the two algorithms. Rapp et al. (2012) kindly provided their test set to us. It contains 1000 English words and a single correct German translation for each. We evaluate on a subset we call TS774 that consists of the 774 test word pairs that are in the intersection of words c</context>
</contexts>
<marker>Rapp, Sharoff, Babych, 2012</marker>
<rawString>Reinhard Rapp, Serge Sharoff, and Bogdan Babych. 2012. Identifying word translations from comparable documents without a seed lexicon. In LREC, pages 460–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Timothy D Heilman</author>
</authors>
<title>A web-based kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web, WWW ’06,</booktitle>
<pages>377--386</pages>
<contexts>
<context position="19656" citStr="Sahami and Heilman, 2006" startWordPosition="3385" endWordPosition="3388">ir journey: ! ! k−1Y S(0) Aτi BT τk−i i=0 + S(k−1) (12) We will not use Eq. 12 due to its space complexity. s(i,j) = X00 ck X p(k) k=0 (u,v)ES(0) u (i)q(k) v (j) (9) S(k) = ck X |T |k τETk Yk i=1 1396 5.4 Similarity of sets of nodes CoSimRank can also be used to compute the similarity s(V, W) of two sets V and W of nodes, e.g., short text snippets. We are not including this method in our experiments, but we will give the equation here, as traditional document similarity measures (e.g., cosine similarity) perform poorly on this task although there also are known alternatives with good results (Sahami and Heilman, 2006). For a set V , the initial PPR vector is given by: � pi (V)= |v�, if i E V (0) 0, else We then reuse Eq. 4 to compute s(V, W): Edge types relation entities description example amod a, v adjective-noun a fast car dobj v, n verb-object drive a car ncrd n, n noun-noun cars and busses Graph statistics nodes nouns adjectives verbs de 34,544 10,067 2,828 en 22,258 12,878 4,866 edges ncrd amod dobj de 65,299 417,151 143,905 en 288,878 686,069 510,351 s(V, W) = ∞ ck(p(k)(V ),p(k)(W)) Table 1: Edge types (above) and number of nodes E and edges (below) k=0 In summary, modifications proposed for SimRank</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Mehran Sahami and Timothy D. Heilman. 2006. A web-based kernel function for measuring the similarity of short text snippets. In Proceedings of the 15th international conference on World Wide Web, WWW ’06, pages 377–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Scheible</author>
<author>Florian Laws</author>
<author>Lukas Michelbacher</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Sentiment translation through multi-edge graphs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>1104--1112</pages>
<marker>Scheible, Laws, Michelbacher, Sch¨utze, 2010</marker>
<rawString>Christian Scheible, Florian Laws, Lukas Michelbacher, and Hinrich Sch¨utze. 2010. Sentiment translation through multi-edge graphs. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 1104–1112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
<author>Michael Walsh</author>
</authors>
<title>A graphtheoretic model of lexical syntactic acquisition.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>917--926</pages>
<marker>Sch¨utze, Walsh, 2008</marker>
<rawString>Hinrich Sch¨utze and Michael Walsh. 2008. A graphtheoretic model of lexical syntactic acquisition. In EMNLP, pages 917–926.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wensi Xi</author>
<author>Edward A Fox</author>
<author>Weiguo Fan</author>
<author>Benyu Zhang</author>
<author>Zheng Chen</author>
<author>Jun Yan</author>
<author>Dong Zhuang</author>
</authors>
<title>Simfusion: measuring similarity using unified relationship matrix.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’05,</booktitle>
<pages>130--137</pages>
<contexts>
<context position="4345" citStr="Xi et al., 2005" startWordPosition="680" endWordPosition="683"> Linguistics, pages 1392–1402, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics use of supervised learning (e.g., de Melo and Weikum (2012)). Since the original version of SimRank (Jeh and Widom, 2002) has complexity 0(n4), many extensions have been proposed to speed up its calculation. A Monte Carlo algorithm, which is scalable to the whole web, was suggested by Fogaras and R´acz (2005). However, in an evaluation of this algorithm we found that it does not give competitive results (see Section 6). A matrix representation of SimRank called SimFusion (Xi et al., 2005) improves the computational complexity from 0(n4) to 0(n3). Lizorkin et al. (2010) also reduce complexity to 0(n3) by selecting essential node pairs and using partial sums. They also give a useful overview of SimRank, SimFusion and the Monte Carlo methods of Fogaras and R´acz (2005). A non-iterative computation for SimRank was introduced by Li et al. (2010). This is especially useful for dynamic graphs. However, all of these methods have to run SimRank on the entire graph and are not efficient enough for very large graphs. We are interested in applications that only need a fraction of all 0(n2</context>
</contexts>
<marker>Xi, Fox, Fan, Zhang, Chen, Yan, Zhuang, 2005</marker>
<rawString>Wensi Xi, Edward A. Fox, Weiguo Fan, Benyu Zhang, Zheng Chen, Jun Yan, and Dong Zhuang. 2005. Simfusion: measuring similarity using unified relationship matrix. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’05, pages 130–137.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>