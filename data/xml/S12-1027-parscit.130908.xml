<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.58631">
An Evaluation of Graded Sense Disambiguation using Word Sense Induction
</title>
<author confidence="0.709737">
David Jurgens1,2
</author>
<affiliation confidence="0.75785925">
1HRL Laboratories, LLC
Malibu, California, USA
2Department of Computer Science
University of California, Los Angeles
</affiliation>
<email confidence="0.998642">
jurgens@cs.ucla.edu
</email>
<sectionHeader confidence="0.996872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.963583947368421">
Word Sense Disambiguation aims to label the
sense of a word that best applies in a given
context. Graded word sense disambiguation
relaxes the single label assumption, allow-
ing for multiple sense labels with varying de-
grees of applicability. Training multi-label
classifiers for such a task requires substan-
tial amounts of annotated data, which is cur-
rently not available. We consider an alter-
nate method of annotating graded senses using
Word Sense Induction, which automatically
learns the senses and their features from cor-
pus properties. Our work proposes three ob-
jective to evaluate performance on the graded
sense annotation task, and two new methods
for mapping between sense inventories using
parallel graded sense annotations. We demon-
strate that sense induction offers significant
promise for accurate graded sense annotation.
</bodyText>
<sectionHeader confidence="0.998758" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.962248217391304">
Word Sense Disambiguation (WSD) aims to identify
the sense of a word in a given context, using a pre-
defined sense inventory containing the word’s differ-
ent meanings (Navigli, 2009). Traditionally, WSD
approaches have assumed that each occurrence of
a word is best labeled with a single sense. How-
ever, human annotators often disagree about which
sense is present (Passonneau et al., 2010), espe-
cially in cases where some of the possible senses
are closely related (Chugur et al., 2002; McCarthy,
2006; Palmer et al., 2007).
Recently, Erk et al. (2009) have shown that in
cases of sense ambiguity, a graded notion of sense
labeling may be most appropriate and help reduce
the ambiguity. Specifically, within a given context,
multiple senses of a word may be salient to the
reader, with different levels of applicability. For ex-
ample, in the sentence
• The athlete won the gold metal due to her hard
work and dedication.
multiple senses could be considered applicable for
“won” according to the WordNet 3.0 sense inventory
(Fellbaum, 1998):
</bodyText>
<listItem confidence="0.758909428571429">
1. win (be the winner in a contest or competition; be victo-
rious)
2. acquire, win, gain (win something through one’s efforts)
3. gain, advance, win, pull ahead, make headway, get ahead,
gain ground (obtain advantages, such as points, etc.)
4. succeed, win, come through, bring home the bacon, de-
liver the goods (attain success or reach a desired goal)
</listItem>
<bodyText confidence="0.998296176470588">
In this context, many annotators would agree that the
athlete has both won an object (the gold metal itself)
and won a competition (signified by the gold medal).
Although contexts can be constructed to elicit only
one of these senses, in the example above, a graded
annotation best matches human perception.
Graded word sense (GWS) annotation offers sig-
nificant advantages for sense annotation with a fine-
grained sense inventory. However, creating a suf-
ficiently large annotated corpus for training super-
vised GWS disambiguation models presents a sig-
nificant challenge, i.e., the laborious task of gath-
ering annotations for all combinations of a word’s
senses, along with variation in those senses appli-
cabilities. To our knowledge, Erk et al. (2009) have
provided the only data set with GWS annotations for
11 terms.
</bodyText>
<page confidence="0.977404">
189
</page>
<note confidence="0.97279">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 189–198,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999940214285714">
Therefore, we consider the use of Word Sense In-
duction (WSI) for GWS annotation. WSI removes
the need for substantial training data by automati-
cally deriving a word’s senses and associated sense
features through examining its contextual uses. Fur-
thermore, the data-driven sense discovery defines
senses as they are present in the corpus, which may
identify usages not present in traditional sense in-
ventories (Lau et al., 2012). Last, many WSI models
represent senses loosely as abstractions over usages,
which potentially may transfer well to expressing
GWS annotations as a blend of their sense usages.
In this paper, we consider the performance of WSI
models on a GWS task. The contributions of this
paper are as follows. First, in Sec. 2, we motivate
three GWS annotation objectives and propose corre-
sponding measures that provide fine-grained analy-
sis of the capabilities of different WSI models. Sec-
ond, in Sec. 4, we propose two new sense mapping
procedures for converting an induced sense inven-
tory to a reference sense inventory when GWS an-
notations are present, and demonstrate significant
performance improvement using these procedures
on GWS annotation. Last, in Sec. 5, we demon-
strate a complete evaluation framework using three
graph-based WSI models as examples, generating
several insights for how to better evaluate GWS dis-
ambiguation systems.
</bodyText>
<sectionHeader confidence="0.992026" genericHeader="method">
2 Evaluating GWS Annotations
</sectionHeader>
<bodyText confidence="0.995468857142857">
Graded word sense annotation conveys multiple lev-
els of information, both in which senses are present
and their relative levels of applicability; and so, no
single evaluation measure alone is appropriate for
assessing GWS annotation capability. Therefore, we
propose three objectives for the evaluating the sense
labeling: (1) Detection of which senses are present,
</bodyText>
<listItem confidence="0.9810165">
(2) Ranking senses according to applicability, and
(3) Perception of the graded presence of each sense.
</listItem>
<bodyText confidence="0.998782185185185">
We separate the three objectives as a way to evaluate
how well different techniques perform on each as-
pect individually, which may encourage future work
in ensemble WSD methods that use combinations of
the techniques. Figure 1 illustrates each evaluation
on example annotations. We note that Erk and Mc-
Carthy (2009) have also proposed an alternate set of
evaluation measures for GWS annotations. Where
applicable, we describe and compare their measures
to ours for the three objectives.
In the following definitions, let 5iG refer to the set
of senses {si, ... , s,,,} present in context i according
to the gold standard, and similarly, let 5iL refer to
the set of senses for context i as labeled by a WSD
system using the same sense inventory. Let peri(sj)
refer to the perceived numeric applicability rating of
sense sj in context i.
Detection measures the ability to accurately iden-
tify which senses are applicable in a given context,
independent of their applicability. While the most
basic of the evaluations, systems that are highly ac-
curate at multi-sense detection could be used for rec-
ognizing ambiguous contexts where multiple senses
are applicable or for evaluating the granularity of
sense ontologies by testing for correlations between
senses in a multi-sense labeling. Detection is mea-
sured using the Jaccard Index between 5iG and 5iL
</bodyText>
<equation confidence="0.480093666666667">
L
for a given context i: Si G�Si
SiGUSiL
</equation>
<bodyText confidence="0.99995675">
Ranking measures the ability to order the senses
present in context i according to their applicabil-
ity but independent of their quantitative applicabil-
ity scores. Even though multiple senses are present,
a context may have a clear primary senses. By pro-
viding a ranking in agreement with human judge-
ments, systems create a primary sense label for each
context. When the induced senses are mapped to a
sense inventory, selecting the primary sense is analo-
gous to non-graded WSD where a context is labeled
with its most applicable sense.
To compare sense rankings, we use Goodman and
Kruskal’s &apos;y, which is related to Kendall’s T rank cor-
relation. When the data has many tied ranks, &apos;y is
preferable to both Kendall’s T as well as Spearman’s
p rank correlation (Siegel and Castellan Jr., 1988),
the latter of which is used by Erk and McCarthy
(2009) for evaluating sense rankings. The use of &apos;y
was motivated by our observation that in the GWS
dataset (described later in Section 5.1), roughly 65%
of the instances contained at least one tied ranking
between senses.
To compute &apos;y, we examine all pair-wise combi-
nations of senses (si, sj) of the target word. Let
rG(si) and rL(si) denote the ranks of sense si in
the gold standard and provided annotations. In the
event that a ranking does not include senses, all
of the inapplicable senses are assigned a tied rank
</bodyText>
<page confidence="0.994535">
190
</page>
<subsectionHeader confidence="0.957224">
Instance Gold Standard Annotation
</subsectionHeader>
<bodyText confidence="0.984093">
The athlete won the gold metal due to her win.v.1: 0.6, win.v.2: 0.4
hard work and dedication. (not applicable: win.v.3, win.v.4)
</bodyText>
<subsectionHeader confidence="0.795276">
Test Annotation Detection Ranking Perception
</subsectionHeader>
<table confidence="0.455895625">
win.v.1: 0.7, win.v.2: 0.3
win.v.1:1.0
win.v.2: 1.0
win.v.3: 0.5, win.v.1: 0.3, win.v.4: 0.2
1.0 1.0 0.983
0.5 1.0 0.832
0.5 0.333 0.554
0.25 -0.2 0.405
</table>
<figureCaption confidence="0.9012925">
Figure 1: Example annotations of the same context compared with the gold standard according to Detection,
Ranking, and Perception.
</figureCaption>
<bodyText confidence="0.996470846153846">
lower than the least applicable sense; i.e., for m
applicable senses, all inapplicable senses have rank
m+1. A pair of senses, (si, sj) is said to be con-
cordant if rG(si) &lt; rG(sj) and rL(si) &lt; rL(sj) or
rG(si) &gt; rG(sj) and rL(si) &gt; rL(sj), and discor-
dant otherwise. γ is defined as c�d
c�d where c is the
number of concordant pairs and d is the number of
discordant.
Perception measures the ability to equal human
judgements on the levels of applicability for each
sense in a context. Unlike ranking, this evaluation
quantifies the difference in sense applicability. As a
potential application, these differences can be used
to quantify the contextual ambiguity. For example,
the relative applicability differences can be used to
distinguish between ambiguous contexts where mul-
tiple highly-applicable senses exist and unambigu-
ous contexts where a single main sense exists but
other senses are still minimally applicable.
To quantify Perception, we compare sense label-
ings using the cosine similarity. Each labeling is rep-
resented as a vector with a separate component for
each sense, whose value is the applicability of that
sense. The Perception for two annotations of con-
text j is then calculated as
</bodyText>
<equation confidence="0.900861">
Ei perj(sGi ) x perj(sLi )
�E �E
i perj(sG i )2 x i perj(sL i )2
</equation>
<bodyText confidence="0.999534571428571">
Note that because all sense perceptibilities are non-
negative, the cosine similarity is bounded to [0, 1].
Erk and McCarthy (2009) propose an alternate
measure for comparing the applicability values us-
ing the Jensen-Shannon divergence. The sense an-
notations are normalized to probability distributions,
denoted G and L, and the divergence is computed as:
</bodyText>
<equation confidence="0.9980325">
1 1
JSD(G||L) = �DKL(G||M) + �DKL(L||M)
</equation>
<bodyText confidence="0.999983375">
where M is the average of the distributions G and L
and DKL denotes the Kullback-Leibler divergence.
While both approaches are similar in intent, we find
that the cosine similarity better matches the expected
difference in Perception for cases where two anno-
tations use different numbers of senses. For exam-
ple, the fourth test annotation in Fig. 1 has a JSS1
of 0.593, despite its significant differences in order-
ing and the omission of a sense. Indeed, in cases
where the set of senses in a test annotation is com-
pletely disjoint from the set of gold standard senses,
the JSS will be positive due to comparing the two
distributions against their average; In contrast, the
cosine similarity in such cases will be zero, which
we argue better matches the expectation that such an
annotation does not meet the Perception objective.
</bodyText>
<sectionHeader confidence="0.998534" genericHeader="method">
3 WSI Models
</sectionHeader>
<bodyText confidence="0.999847083333333">
For evaluation we adapt three recent graph-based
WSI methods for the task of graded-sense annota-
tion: Navigli and Crisafulli (2010), referred to as
Squares, Jurgens (2011), referred to as Link, and
UoY (Korkontzelos and Manandhar, 2010). At an
abstract level, these methods operate in two stages.
First, a graph is built, using either words or word
pairs as vertices, and edges are added denoting some
form of association between the vertices. Second,
senses are derived by clustering or partitioning the
graph. We selected these methods based on their su-
perior performance on recent benchmarks and also
</bodyText>
<footnote confidence="0.951481">
1The JSD is a distance measure in [0, 1], which we convert
to a similarity JSS = 1 − JSD for easier comparison.
</footnote>
<page confidence="0.997329">
191
</page>
<bodyText confidence="0.994740888888889">
for their significant differences in approach. Follow-
ing, we briefly summarize each method to highlight
its key parameters and then describe its adaptation
to GWS annotation.
Squares Navigli and Crisafulli (2010) propose a
method that builds a separate graph for each term
for sense induction. First, a large corpus is used to
identify associated terms using the Dice coefficient:
For two terms w1, w2, Dice(w1, w2) = 2c(w1,w )
</bodyText>
<equation confidence="0.595933">
c(w1)+c(w)
</equation>
<bodyText confidence="0.999967744680851">
where c(w) is the frequency of occurrence. Next,
for a given term w the initial graph, G, is con-
structed by adding edges to every term w2 where
Dice(w, w2) &gt; S, and then the step is repeated for
the neighbors of each term w2 that was added.
Once the initial graph is constructed, edges are
pruned to separate the graph into components. Nav-
igli and Crisafulli (2010) found improved perfor-
mance on their target application using a pruning
method based on the number of squares (closed
paths of length 4) in which an edge participates. Let
s denote the number of squares that an edge e par-
ticipates in and p denote the number of squares that
would be possible from the set of neighbors of e.
Edges with ps &lt; Q are removed. The remaining con-
nected components in G denote the senses of w.
Sense disambiguation on a context of w is per-
formed by computing the intersection of the con-
text’s terms with the terms in each of the connected
components. As originally specified, the component
with the largest overlap is labeled as the sense of w.
We adapt this to graded senses by returning all inter-
secting components with applicability proportional
to their overlap. Furthermore, for efficiency, we use
only noun, verb, and adjective lemmas in the graphs.
Link Jurgens (2011) use an all-words method
where a single graph is built in order to derive the
senses of all words in it. Here, the graph’s clus-
ters do not correspond to a specific word’s senses
but rather to contextual features that can be used to
disambiguate any of the words in the cluster.
In its original specification, the graph is built with
edges between co-occurring words and edge weights
corresponding to co-occurrence frequency. Edges
below a specified threshold T are removed, and then
link community detection (Ahn et al., 2010) is ap-
plied to discover sense-disambiguating word com-
munities, which are overlapping cluster of vertices
in the graph, rather than hard partitions. Once the set
of communities is produced, communities with three
or fewer vertices are removed, under the assumption
that these communities contain too few features to
reliably disambiguate.
Senses are disambiguated by finding the commu-
nity with the largest overlap score, computed as the
weighted Jaccard Index. For a context with the set
of features Fi and a community with features Fj, the
</bodyText>
<equation confidence="0.3801215">
overlap is measured as |Fj ||Fi∩Fj|
|Fi∪Fj|�
</equation>
<bodyText confidence="0.995942078947368">
We adapt this algorithm in three ways. First,
rather than use co-occurrence frequency to weight
edges between terms, we weight edges accord to
their statistical association with the G-test (Dunning,
1993). The G-test weighting helps remove edges
whose large edge weights are due to high corpus fre-
quency but provide no disambiguating information,
and the weighting also allows the T parameter to
be more consistently set across corpora of different
sizes. Second, while Jurgens (2011) used only nouns
as vertices in the graph, we include both verbs and
adjectives due to needing to identify senses for both.
Third, for graded senses, we disambiguate a context
by reporting all overlapping communities, weighted
by their overlap score.
UoY Korkontzelos and Manandhar (2010) pro-
pose a WSI model that builds a graph for each term
for disambiguation. The graph is built in four stages,
with four main tuning parameters, summarized next.
First, using a reference corpus, all contexts of the
target word w are selected to build a list of co-
occurring noun lemmas, retaining all those with fre-
quency above P1. Second, the Log-Likelihood ratio
(Dunning, 1993) is computed between all selected
nouns and w, retaining only those with an associa-
tion above P2. Third, all remaining nouns are used
to create all (n ) noun pairs. Next, each term and
2
pair is mapped to the set of contexts in the reference
corpus in which it is present. A pair (wi, wj) is re-
tained only if its set of contexts is dissimilar to the
sets of contexts of both its member terms, using the
Dice coefficient to measure the similarity of the sets.
Pairs with a Dice coefficient above P4 with either of
its constituent terms are removed. Last, edges are
added between nouns and noun pairs according to
their conditional probabilities of occurring with each
other. Edges with a conditional probability less than
</bodyText>
<page confidence="0.992555">
192
</page>
<bodyText confidence="0.991895058823529">
P3 are not included.
Once the graph has been constructed, the Chi-
nese Whispers graph partitioning algorithm (Bie-
mann, 2006) is used to identify word senses. Each
graph partition is assigned a separate sense of w.
Next, each partition is mapped to the set of contexts
in the reference corpus in which at least one of its
vertices occurs. Partitions whose context sets are a
strict subset of another are merged with the subsum-
ing partition.
Word sense disambiguation occurs by counting
the number of overlapping vertices for each parti-
tion and selecting the partition with the highest over-
lap as the sense of w. We extend this to graded an-
notation by selecting all partitions with at least one
vertex present and set the applicability equal to the
degree of overlap.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="method">
4 Evaluation Across Sense Inventories
</sectionHeader>
<bodyText confidence="0.986873830769231">
Directly comparing GWS annotations from the in-
duced and gold standard sense inventories requires
first creating a mapping from the induced senses to
the gold standard inventory. Agirre et al. (2006) pro-
pose a sense-mapping procedure, which was used in
the previous two SemEval WSI Tasks (Agirre and
Soroa, 2007; Manandhar et al., 2010). We consider
this procedure and two extensions of it to support
learning a mapping from graded sense annotations.
The procedure of Agirre et al. (2006) uses three
corpora: (1) a base corpus from which the senses
are derived, (2) a mapping corpus annotated with
both gold standard senses, denoted gs, and induced
senses, denoted is, and (3) a test corpus annotated
with is senses that will be converted to gs senses.
Once the senses are induced from the base cor-
pus, the mapping corpus is annotated with is senses
and a matrix M is built where cell i, j initially con-
tains the counts of each time gsj and isi were used
to label the same instance. The rows of this matrix
are then normalized such that each cell now repre-
sents p(gsj isi). The final mapping selects the most
probable gs sense for each is sense.
To label the test corpus, each instance that is
labeled with isi is relabeled with the gs sense
with the highest conditional probability given isi.
When a context c is annotated by a set of labels
L = {isi, ... , isj}, the final sense labeling con-
tains the set of all gs to which the is senses were
mapped, weighted by their mapping frequencies:
perc(gsj) = |L |&amp; EL S(isi,gsj) where 6 returns
1 if isi is mapped to gsj and 0 otherwise.
The original algorithm of Agirre et al. (2006) does
not consider the role of applicability in evaluating
whether an is sense should be mapped to a gs sense;
is senses with different levels of applicability in the
same context are treated equivalently in updating
M. Therefore, as a first extension, referred to as
Graded, we revise the update rule for constructing
M where for the set of contexts C labeled by both
isi and gsj, Mi,j = EcEC perc(isi)xperc(gsj). As
in (Agirre et al., 2006), M is normalized and each is
sense is mapped to its most probable gs sense.
To label the test corpus using the Graded method,
the applicability of the is sense is also included.
For a context c is annotated with senses L =
{isi, ... , isj}, the final sense labeling contains the
set of all gs senses to which the is senses were
mapped, weighted by their mapping frequencies:
perc(gsj) = EisiEL [�(isi,gsj) x perc(isi)] . The
applicabilities are then normalized to sum to 1.
The prior two methods restrict an is sense to map-
ping to only a single gs sense. However, an is sense
may potentially correspond to multiple gs senses,
each with different levels of applicability. There-
fore, we consider a second extension, referred to as
Distribution, that uses the same matrix construc-
tion as the Graded procedure, but rather than map-
ping each is to a single sense, maps it to a distribu-
tion over all gs senses for which it was co-annotated,
which is the normalized row vector in M for an is
sense. Labeling in the test corpus is then done by
summing the distributions of the is senses annotated
in the context and normalizing to create a probability
distribution over the union of their gs senses.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999715">
We adapt the supervised WSD setting used in prior
SemEval WSI Tasks (Agirre and Soroa, 2007; Man-
andhar et al., 2010) to evaluation the models accord-
ing to the three proposed objectives. In the super-
vised setting, WSI systems provide GWS annotation
of their induced senses for the test corpus, which
is already labeled with the gold-standard GWS an-
notations. Then, a portion of the test corpus with
gold standard annotations is used to build a mapping
from induced senses to the reference sense inven-
</bodyText>
<page confidence="0.997776">
193
</page>
<table confidence="0.999932">
Term PoS # senses Avg. # Senses
per Instance
add verb 6 4.18
ask verb 7 5.98
win verb 4 3.98
argument noun 7 5.18
interest noun 7 5.12
paper noun 7 5.54
different adj. 5 4.98
important adj. 5 4.82
</table>
<tableCaption confidence="0.9968305">
Table 1: The terms from the GWS dataset (Erk et
al., 2009) used in this evaluation
</tableCaption>
<bodyText confidence="0.9972847">
tory using one of the three algorithms described in
Section 4. The remaining, held-out test corpus in-
stances have their induced senses converted to the
gold standard sense inventory and the sense label-
ings are evaluated for the three objectives from Sec-
tion 2. In our experiments we divide the reference
corpus into five evenly-sized segments and then use
four segments (80% of the test corpus) for construct-
ing the mapping and then evaluate the converted
GWS annotations of the remaining segment.
</bodyText>
<subsectionHeader confidence="0.98374">
5.1 Graded Annotation Data
</subsectionHeader>
<bodyText confidence="0.999983181818182">
The gold standard GWS annotations are derived
from a subset of the GWS data provided by Erk et
al. (2009). Here, three annotators rated the applica-
bility of all WordNet 3.0 senses of a word in a single
sentence context. Ratings were done using a 5-point
ordinal ranking according to the judgements from 1
– this sense is not applicable to 5 – this usage exactly
reflects this sense. Annotators used a wide-range of
responses, leading to many applicable senses per in-
stance. We selected the subset of the GWS dataset
where each term has 50 annotated contexts, which
were distributed evenly between SemCor (Miller et
al., 1993) and the SENSEVAL-3 lexical substitution
corpus (Mihalcea et al., 2004). Table 1 summarizes
the target terms in this context.
To prepare the data for evaluation, we constructed
the gold standard GWS annotations using the mean
applicability ratings of all three annotators for each
context. Senses that received a mean rating of 1 (not
applicable) were not listed in gold standard labeling
for that instance. All remaining responses were nor-
malized to sum to 1.
</bodyText>
<subsectionHeader confidence="0.998164">
5.2 Model Configuration
</subsectionHeader>
<bodyText confidence="0.9994815">
For consistency, all three WSI models were trained
using the same reference corpus. We used a 2009
snapshot of Wikipedia,2 which was PoS tagged and
lemmatized using the TreeTagger (Schmid, 1994).
All of target terms occurred over 12,000 times. The
G-test between terms was computed using a three-
sentence sliding window within each article in the
corpus. The Dice coefficient was calculated using a
single sentence as context.
For all three models, we performed a limited grid
search to find the best performing system param-
eters, within reasonable computational limits. We
summarize the parameters and models, selecting the
configuration with the highest average Perception
score. For all models, the applicability ratings for
each instance are normalized to sum to 1.
</bodyText>
<table confidence="0.99858325">
Model Parameter Range Selected
Squares �={0.008, 0.009, ... , 0.092} 0.037
v={0.25, 0.30, ... , 0.50, 0.55} 0.55
Link T={400, 500,..., 900, 1000} 500
UoY P1={10, 20} 20
P2={10, 20, 30} 20
P3={0.2, 0.3, 0.4} 0.3
P4={0.4, 0.6, 0.8} 0.4
</table>
<subsectionHeader confidence="0.9874">
5.3 Baselines
</subsectionHeader>
<bodyText confidence="0.999619428571429">
Prior WSI evaluations have used the Most Frequent
Sense (MFS) labeling a strong baseline in the super-
vised WSD task. For the GWS setting, we consider
five other baselines that select one, some, or all of
the sense of the target word, with different ordering
strategies. In the six baselines, each instance is la-
beled as follows:
</bodyText>
<table confidence="0.911201714285714">
MFS: the most frequent sense of the word
RS: a single, randomly-selected sense
ASF: all senses, ranked in order of frequency starting
with the most frequent
ASR: all senses, randomly ranked
ASE: all senses, ranked equally
RSM: a random number of senses, ranked arbitrarily
</table>
<bodyText confidence="0.995281666666667">
To establish applicability values from a ranking of n
senses, we set applicability to the ith ranked sense of
(n nZ)+1 , where rank 1 is the highest ranked sense.
</bodyText>
<footnote confidence="0.978505">
�k_1 k
2http://wacky.sslmit.unibo.it/
</footnote>
<page confidence="0.991794">
194
</page>
<table confidence="0.9189356">
Model Agirre et al. (2006) Mapping Graded Mapping Distribution Mapping Recall
D R P D R P D R P
Squares 0.192 -0.024 0.382 0.198 0.555 0.504 0.879 0.562 0.925 0.560
Link 0.282 0.081 0.454 0.335 0.436 0.528 0.854 0.503 0.907 0.800
UoY 0.238 0.116 0.445 0.244 0.486 0.528 0.848 0.528 0.907 0.940
</table>
<tableCaption confidence="0.982732">
Table 2: Average performance of the three WSI models according to Detection, Ranking, and Percetion
</tableCaption>
<table confidence="0.999972285714286">
Baseline Detection Ranking Perception
MFS 0.204 0.334 0.469
RS 0.167 -0.036 0.363
ASF 0.846 0.218 0.830
ASR 0.846 0.006 0.776
ASE 0.846 0.000 0.862
RSM 0.546 0.005 0.632
</table>
<tableCaption confidence="0.999923">
Table 3: Average performance of the six baselines
</tableCaption>
<subsectionHeader confidence="0.752148">
5.4 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999983378787879">
Each WSI model was trained and then used to la-
bel the sense of each target term in the GWS corpus.
The three sense-mapping procedures were then ap-
plied to the induced sense labels on the held-out in-
stances to perform a comparison in the graded sense
annotations. Table 2 reports the performance for the
three evaluation measures for each model and map-
ping configuration on all instances where the sense
mapping is defined. The sense mapping is unde-
fined when (1) a WSI model cannot match an in-
stance’s features to any of its senses therefore leaves
the instance unannotated or (2) when an instance is
labeled with an is sense not seen in the training data.
Therefore, we report the additional statistic, Recall,
that indicates the percentage of instances that were
both labeled by the WSI model and mapped to gs
senses. Table 3 summarizes the baselines’ perfor-
mance.
The results show three main trends. First, intro-
ducing applicability into the sense mapping process
noticeably improves performance. For almost all
models and scores, using the Graded Mapping im-
proves performance a small amount. However, the
largest increase comes from using the Distribution
mapping where induced senses are represented as
distributions over the gold standard senses.
Second, performance was well ahead of the base-
lines across the three evaluations, when consider-
ing the models’ best performances. The Squares
and Link models were able to outperform the base-
lines that list all senses on the Detection objec-
tive, which the UoY model only improves slightly
from this baseline. For the Ranking objective, all
models substantially outperform the best baseline,
MFS; and similarly, for the Perception objective,
all models outperform the best performing baseline,
ASE. Overall, these performance suggest that in-
duce senses can be successfully used to produce
quality GWS annotations.
Third, the WSI models themselves show signif-
icant differences in their recall and multi-labeling
frequencies. The Squares model is only able to la-
bel approximately 56% of the GWS instances due to
sparseness in its sense representation. Indeed, only
12 of its 237 annotated instances received more than
one sense label, revealing that the model’s perfor-
mance is mostly based on correctly identifying the
primary sense in a context and not on identifying
the less applicable senses. The UoY model shows a
similar trend, with most instances being assigned a
median of 2 senses. However, its sense representa-
tion is sufficiently dense to have the highest recall of
any of the models. In contrast to the other two mod-
els, the Link model varies significantly in the num-
ber of induced senses assigned: “argument,” “ask,”
“different,” and “win” were assigned over 60 senses
on average to each of their instances, with “differ-
ent” having an average of 238, while the remaining
terms were assigned under two senses on average.
Furthermore, the results also revealed two unex-
pected findings. First, the ASE baseline performed
unexpectedly high in Perception, despite its assign-
ment of uniform applicability to all senses. We hy-
pothesize this is due to the majority of instances in
the GWS dataset being labeled with most of a word’s
senses, as indicated by Table 1, which results in their
</bodyText>
<page confidence="0.997856">
195
</page>
<bodyText confidence="0.999936357142857">
perceptibilities becoming normalized to small val-
ues. Because the ASE solution has applicability rat-
ings for all senses, normalization brings the ratings
close to those of the gold standard solution, and fur-
thermore, the difference in score between applicable
and inapplicable senses become too small to signifi-
cantly affect the resulting cosine similarity. As an al-
ternate model, we reevaluated the baselines against
the gold standard using the Jensen-Shannon diver-
gence as proposed by Erk and McCarthy (2009).
Again, ASE is still the highest performing baseline
on Perception. The high performance for both eval-
uation measures suggests that an alternate measure
may be better suited for quantifying the difference
in solutions’ GWS applicabilities.
Second, performance was higher on the Percep-
tion task than on Ranking, the former of which was
anticipated being more difficult. We attribute the
lower Ranking performance to two factors. First,
the GWS data contains main tied rank senses; how-
ever, ties in sense ranks after the mapping process
are relatively rare, which reduces &apos;y. Second, in-
stances in the GWS often have senses within close
applicability ranges. When scoring an induced an-
notation that swaps the applicability, the Perception
is less affected by the small change in applicability
magnitude, whereas Ranking is more affected due to
the change in ordering.
</bodyText>
<sectionHeader confidence="0.998415" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99990659375">
GWS annotations offer great potential for reli-
ably annotating using fine-grained sense invento-
ries, where word instance may elicit several concur-
rent meanings. Given the expense of creating an-
notated training corpora with sufficient examples of
the graded senses, WSI offers significant promise for
learning senses automatically while needing only a
small amount GWS annotated data to learn the sense
mapping for a WSD task.
In this paper, we have carried out an initial study
on the performance of WSI systems on a GWS an-
notation task. Our primary contribution is an end-
to-end framework for mapping and evaluating in-
duced GWS data. We first proposed three objectives
for graded sense annotation along with correspond-
ing evaluation measures that reliably convey the ef-
fectiveness given the nature of GWS annotations.
Second, we proposed two new mapping procedures
that use graded sense applicability for converting in-
duced senses into a reference sense inventory. Using
three graph-based WSI models, we demonstrated
that incorporating graded sense applicability into the
sense mapping significantly improves GWS perfor-
mance over the commonly used method of Agirre et
al. (2006). Furthermore, our study demonstrated the
potential of WSI systems, showing that all the mod-
els were able to outperform all six of the proposed
baseline on the Ranking and Perception objectives.
Our findings raise several avenues for future
work. First, our study only considered three graph-
based WSI models; future work is needed to as-
sess the capabilities other WSI approaches, such as
vector-based or Bayesian. We are also interested in
comparing the performance of the Link model with
other recently developed all-words WSI approaches
such as Van de Cruys and Apidianaki (2011).
Second, the proposed evaluation relies on a su-
pervised mapping to the gold standard sense inven-
tory, which has potential to lose information and in-
correctly map new senses not in the gold standard.
While unsupervised clustering evaluations such as
the V-measure (Rosenberg and Hirschberg, 2007)
and paired Fscore (Artiles et al., 2009) are capable
of evaluating without such a mapping, future work
is needed to test extrinsic soft clustering evaluations
such as BCubed (Amig´o et al., 2009) or develop
analogous techniques that take into account graded
class membership used in GWS annotations.
Last, we note that our setup normalized the GWS
ratings into probability distribution, which is stan-
dard in the SemEval evaluation setup. However, this
normalization incorrectly transforms GWS annota-
tions where no predominant sense was rated at the
highest value, e.g., an annotation of only two senses
rated as 3 on a scale of 1 to 5. While these percepti-
bilities may be left unnormalized, it is not clear how
to compare the induced GWS annotations with such
mid-interval values, or when the rating scale of the
WSI system is potentially unbounded. Future work
is needed both in GWS evaluation and in quantify-
ing applicability along a range in GWS-based WSI
systems to address this issue.
All models and data will be released as a part of
the S-Space Package (Jurgens and Stevens, 2010).3
</bodyText>
<footnote confidence="0.880693">
3https://github.com/fozziethebeat/S-Space
</footnote>
<page confidence="0.997758">
196
</page>
<sectionHeader confidence="0.995324" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999104125">
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations, pages 7–12.
ACL, June.
Eneko Agirre, David Martinez, Oier o´ de Lacalle, and
Aitor Soroa. 2006. Evaluating and optimizing the pa-
rameters of an unsupervised graph-based WSD algo-
rithm. In Proceedings of TextGraphs: the First Work-
shop on Graph Based Methods for Natural Language
Processing, pages 89–96. Association for Computa-
tional Linguistics.
Yong-Yeol Ahn, James P. Bagrow, and Sune Lehmann.
2010. Link communities reveal multiscale complexity
in networks. Nature, (466):761–764, August.
Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4):461–486.
Javier Artiles, Enrique Amig´o, and Julio Gonzalo. 2009.
The role of named entities in web people search. In
Proceedings of EMNLP, pages 534–542. Association
for Computational Linguistics.
Chris Biemann. 2006. Chinese whispers: an efficient
graph clustering algorithm and its application to natu-
ral language processing problems. In Proceedings of
the First Workshop on Graph Based Methods for Nat-
ural Language Processing, pages 73–80. Association
for Computational Linguistics.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the ACL-02 workshop on
Word sense disambiguation: recent successes and fu-
ture directions - Volume 8, WSD ’02, pages 32–39,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61–74.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1, pages 440–449. Association for
Computational Linguistics.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of theAFNLP: Volume 1, pages 10–18. Association
for Computational Linguistics.
Christine Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations. Association for Computational Lin-
guistics.
David Jurgens. 2011. Word sense induction by com-
munity detection. In Proceedings of Sixth ACL Work-
shop on Graph-based Methods for Natural Language
Processing (TextGraphs-6). Association for Computa-
tional Linguistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010.
Uoy: Graphs of unambiguous vertices for word sense
induction and disambiguation. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 355–358. Association for Computational Lin-
guistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proceedings of the
13th Conference of the European Chapter of the Asso-
ciation for computational Linguistics (EACL 2012).
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-
2010 task 14: Word sense induction &amp; disambigua-
tion. In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63–68. Association for
Computational Linguistics.
Diana McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. Making Sense of Sense:
Bringing Psycholinguistics and Computational Lin-
guistics Together, page 17.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The senseval-3 english lexical sample task.
In Senseval-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text, pages 25–28. Barcelona, Spain, Association for
Computational Linguistics.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance.
In Proceedings of the workshop on Human Language
Technology, pages 303–308. Association for Compu-
tational Linguistics.
Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing
word senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 116–
126. Association for Computational Linguistics.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
</reference>
<page confidence="0.980095">
197
</page>
<reference confidence="0.998980038461538">
sense distinctions, both manually and automatically.
Natural Language Engineering, 13(02):137–163.
Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense anno-
tation of polysemous words by multiple annotators. In
Proceedings of Seventh International Conference on
Language Resources and Evaluation (LREC-7).
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL). ACL, June.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing.
Sidney Siegel and N. John Castellan Jr. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, second edition.
Tim Van de Cruys and Marianna Apidianaki. 2011. La-
tent Semantic Word Sense Induction and Disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies (ACL/HLT), pages 1476–
1485.
</reference>
<page confidence="0.997701">
198
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.862936">
<title confidence="0.956672">An Evaluation of Graded Sense Disambiguation using Word Sense Induction</title>
<affiliation confidence="0.92722">Laboratories,</affiliation>
<address confidence="0.999234">Malibu, California, USA</address>
<affiliation confidence="0.9939395">of Computer University of California, Los</affiliation>
<email confidence="0.999424">jurgens@cs.ucla.edu</email>
<abstract confidence="0.99929775">Word Sense Disambiguation aims to label the sense of a word that best applies in a given context. Graded word sense disambiguation relaxes the single label assumption, allowing for multiple sense labels with varying degrees of applicability. Training multi-label classifiers for such a task requires substantial amounts of annotated data, which is currently not available. We consider an alternate method of annotating graded senses using Word Sense Induction, which automatically learns the senses and their features from corpus properties. Our work proposes three objective to evaluate performance on the graded sense annotation task, and two new methods for mapping between sense inventories using parallel graded sense annotations. We demonstrate that sense induction offers significant promise for accurate graded sense annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations,</booktitle>
<pages>7--12</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="17608" citStr="Agirre and Soroa, 2007" startWordPosition="2887" endWordPosition="2890">ber of overlapping vertices for each partition and selecting the partition with the highest overlap as the sense of w. We extend this to graded annotation by selecting all partitions with at least one vertex present and set the applicability equal to the degree of overlap. 4 Evaluation Across Sense Inventories Directly comparing GWS annotations from the induced and gold standard sense inventories requires first creating a mapping from the induced senses to the gold standard inventory. Agirre et al. (2006) propose a sense-mapping procedure, which was used in the previous two SemEval WSI Tasks (Agirre and Soroa, 2007; Manandhar et al., 2010). We consider this procedure and two extensions of it to support learning a mapping from graded sense annotations. The procedure of Agirre et al. (2006) uses three corpora: (1) a base corpus from which the senses are derived, (2) a mapping corpus annotated with both gold standard senses, denoted gs, and induced senses, denoted is, and (3) a test corpus annotated with is senses that will be converted to gs senses. Once the senses are induced from the base corpus, the mapping corpus is annotated with is senses and a matrix M is built where cell i, j initially contains th</context>
<context position="20665" citStr="Agirre and Soroa, 2007" startWordPosition="3439" endWordPosition="3442">refore, we consider a second extension, referred to as Distribution, that uses the same matrix construction as the Graded procedure, but rather than mapping each is to a single sense, maps it to a distribution over all gs senses for which it was co-annotated, which is the normalized row vector in M for an is sense. Labeling in the test corpus is then done by summing the distributions of the is senses annotated in the context and normalizing to create a probability distribution over the union of their gs senses. 5 Experiments We adapt the supervised WSD setting used in prior SemEval WSI Tasks (Agirre and Soroa, 2007; Manandhar et al., 2010) to evaluation the models according to the three proposed objectives. In the supervised setting, WSI systems provide GWS annotation of their induced senses for the test corpus, which is already labeled with the gold-standard GWS annotations. Then, a portion of the test corpus with gold standard annotations is used to build a mapping from induced senses to the reference sense inven193 Term PoS # senses Avg. # Senses per Instance add verb 6 4.18 ask verb 7 5.98 win verb 4 3.98 argument noun 7 5.18 interest noun 7 5.12 paper noun 7 5.54 different adj. 5 4.98 important adj</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the Fourth International Workshop on Semantic Evaluations, pages 7–12. ACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
<author>Oier o´ de Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Evaluating and optimizing the parameters of an unsupervised graph-based WSD algorithm.</title>
<date>2006</date>
<booktitle>In Proceedings of TextGraphs: the First Workshop on Graph Based Methods for Natural Language Processing,</booktitle>
<pages>89--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Agirre, Martinez, de Lacalle, Soroa, 2006</marker>
<rawString>Eneko Agirre, David Martinez, Oier o´ de Lacalle, and Aitor Soroa. 2006. Evaluating and optimizing the parameters of an unsupervised graph-based WSD algorithm. In Proceedings of TextGraphs: the First Workshop on Graph Based Methods for Natural Language Processing, pages 89–96. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yong-Yeol Ahn</author>
<author>James P Bagrow</author>
<author>Sune Lehmann</author>
</authors>
<title>Link communities reveal multiscale complexity in networks.</title>
<date>2010</date>
<journal>Nature,</journal>
<volume>466</volume>
<contexts>
<context position="14035" citStr="Ahn et al., 2010" startWordPosition="2293" endWordPosition="2296">efficiency, we use only noun, verb, and adjective lemmas in the graphs. Link Jurgens (2011) use an all-words method where a single graph is built in order to derive the senses of all words in it. Here, the graph’s clusters do not correspond to a specific word’s senses but rather to contextual features that can be used to disambiguate any of the words in the cluster. In its original specification, the graph is built with edges between co-occurring words and edge weights corresponding to co-occurrence frequency. Edges below a specified threshold T are removed, and then link community detection (Ahn et al., 2010) is applied to discover sense-disambiguating word communities, which are overlapping cluster of vertices in the graph, rather than hard partitions. Once the set of communities is produced, communities with three or fewer vertices are removed, under the assumption that these communities contain too few features to reliably disambiguate. Senses are disambiguated by finding the community with the largest overlap score, computed as the weighted Jaccard Index. For a context with the set of features Fi and a community with features Fj, the overlap is measured as |Fj ||Fi∩Fj| |Fi∪Fj|� We adapt this a</context>
</contexts>
<marker>Ahn, Bagrow, Lehmann, 2010</marker>
<rawString>Yong-Yeol Ahn, James P. Bagrow, and Sune Lehmann. 2010. Link communities reveal multiscale complexity in networks. Nature, (466):761–764, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Javier Artiles</author>
<author>Felisa Verdejo</author>
</authors>
<title>A comparison of extrinsic clustering evaluation metrics based on formal constraints.</title>
<date>2009</date>
<journal>Information Retrieval,</journal>
<volume>12</volume>
<issue>4</issue>
<marker>Amig´o, Gonzalo, Artiles, Verdejo, 2009</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval, 12(4):461–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
</authors>
<title>The role of named entities in web people search.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>534--542</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Artiles, Amig´o, Gonzalo, 2009</marker>
<rawString>Javier Artiles, Enrique Amig´o, and Julio Gonzalo. 2009. The role of named entities in web people search. In Proceedings of EMNLP, pages 534–542. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese whispers: an efficient graph clustering algorithm and its application to natural language processing problems.</title>
<date>2006</date>
<booktitle>In Proceedings of the First Workshop on Graph Based Methods for Natural Language Processing,</booktitle>
<pages>73--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16618" citStr="Biemann, 2006" startWordPosition="2721" endWordPosition="2723">ference corpus in which it is present. A pair (wi, wj) is retained only if its set of contexts is dissimilar to the sets of contexts of both its member terms, using the Dice coefficient to measure the similarity of the sets. Pairs with a Dice coefficient above P4 with either of its constituent terms are removed. Last, edges are added between nouns and noun pairs according to their conditional probabilities of occurring with each other. Edges with a conditional probability less than 192 P3 are not included. Once the graph has been constructed, the Chinese Whispers graph partitioning algorithm (Biemann, 2006) is used to identify word senses. Each graph partition is assigned a separate sense of w. Next, each partition is mapped to the set of contexts in the reference corpus in which at least one of its vertices occurs. Partitions whose context sets are a strict subset of another are merged with the subsuming partition. Word sense disambiguation occurs by counting the number of overlapping vertices for each partition and selecting the partition with the highest overlap as the sense of w. We extend this to graded annotation by selecting all partitions with at least one vertex present and set the appl</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Chinese whispers: an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of the First Workshop on Graph Based Methods for Natural Language Processing, pages 73–80. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irina Chugur</author>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
</authors>
<title>Polysemy and sense proximity in the senseval-2 test suite.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Word sense disambiguation: recent successes and future directions - Volume 8, WSD ’02,</booktitle>
<pages>32--39</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1571" citStr="Chugur et al., 2002" startWordPosition="235" endWordPosition="238">sense annotations. We demonstrate that sense induction offers significant promise for accurate graded sense annotation. 1 Introduction Word Sense Disambiguation (WSD) aims to identify the sense of a word in a given context, using a predefined sense inventory containing the word’s different meanings (Navigli, 2009). Traditionally, WSD approaches have assumed that each occurrence of a word is best labeled with a single sense. However, human annotators often disagree about which sense is present (Passonneau et al., 2010), especially in cases where some of the possible senses are closely related (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007). Recently, Erk et al. (2009) have shown that in cases of sense ambiguity, a graded notion of sense labeling may be most appropriate and help reduce the ambiguity. Specifically, within a given context, multiple senses of a word may be salient to the reader, with different levels of applicability. For example, in the sentence • The athlete won the gold metal due to her hard work and dedication. multiple senses could be considered applicable for “won” according to the WordNet 3.0 sense inventory (Fellbaum, 1998): 1. win (be the winner in a contest or competi</context>
</contexts>
<marker>Chugur, Gonzalo, Verdejo, 2002</marker>
<rawString>Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002. Polysemy and sense proximity in the senseval-2 test suite. In Proceedings of the ACL-02 workshop on Word sense disambiguation: recent successes and future directions - Volume 8, WSD ’02, pages 32–39, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--1</pages>
<contexts>
<context position="14824" citStr="Dunning, 1993" startWordPosition="2418" endWordPosition="2419"> is produced, communities with three or fewer vertices are removed, under the assumption that these communities contain too few features to reliably disambiguate. Senses are disambiguated by finding the community with the largest overlap score, computed as the weighted Jaccard Index. For a context with the set of features Fi and a community with features Fj, the overlap is measured as |Fj ||Fi∩Fj| |Fi∪Fj|� We adapt this algorithm in three ways. First, rather than use co-occurrence frequency to weight edges between terms, we weight edges accord to their statistical association with the G-test (Dunning, 1993). The G-test weighting helps remove edges whose large edge weights are due to high corpus frequency but provide no disambiguating information, and the weighting also allows the T parameter to be more consistently set across corpora of different sizes. Second, while Jurgens (2011) used only nouns as vertices in the graph, we include both verbs and adjectives due to needing to identify senses for both. Third, for graded senses, we disambiguate a context by reporting all overlapping communities, weighted by their overlap score. UoY Korkontzelos and Manandhar (2010) propose a WSI model that builds</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
</authors>
<title>Graded word sense assignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing:</booktitle>
<volume>1</volume>
<pages>440--449</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5644" citStr="Erk and McCarthy (2009)" startWordPosition="879" endWordPosition="883">e evaluation measure alone is appropriate for assessing GWS annotation capability. Therefore, we propose three objectives for the evaluating the sense labeling: (1) Detection of which senses are present, (2) Ranking senses according to applicability, and (3) Perception of the graded presence of each sense. We separate the three objectives as a way to evaluate how well different techniques perform on each aspect individually, which may encourage future work in ensemble WSD methods that use combinations of the techniques. Figure 1 illustrates each evaluation on example annotations. We note that Erk and McCarthy (2009) have also proposed an alternate set of evaluation measures for GWS annotations. Where applicable, we describe and compare their measures to ours for the three objectives. In the following definitions, let 5iG refer to the set of senses {si, ... , s,,,} present in context i according to the gold standard, and similarly, let 5iL refer to the set of senses for context i as labeled by a WSD system using the same sense inventory. Let peri(sj) refer to the perceived numeric applicability rating of sense sj in context i. Detection measures the ability to accurately identify which senses are applicab</context>
<context position="7566" citStr="Erk and McCarthy (2009)" startWordPosition="1203" endWordPosition="1206"> clear primary senses. By providing a ranking in agreement with human judgements, systems create a primary sense label for each context. When the induced senses are mapped to a sense inventory, selecting the primary sense is analogous to non-graded WSD where a context is labeled with its most applicable sense. To compare sense rankings, we use Goodman and Kruskal’s &apos;y, which is related to Kendall’s T rank correlation. When the data has many tied ranks, &apos;y is preferable to both Kendall’s T as well as Spearman’s p rank correlation (Siegel and Castellan Jr., 1988), the latter of which is used by Erk and McCarthy (2009) for evaluating sense rankings. The use of &apos;y was motivated by our observation that in the GWS dataset (described later in Section 5.1), roughly 65% of the instances contained at least one tied ranking between senses. To compute &apos;y, we examine all pair-wise combinations of senses (si, sj) of the target word. Let rG(si) and rL(si) denote the ranks of sense si in the gold standard and provided annotations. In the event that a ranking does not include senses, all of the inapplicable senses are assigned a tied rank 190 Instance Gold Standard Annotation The athlete won the gold metal due to her win</context>
<context position="9977" citStr="Erk and McCarthy (2009)" startWordPosition="1604" endWordPosition="1607"> multiple highly-applicable senses exist and unambiguous contexts where a single main sense exists but other senses are still minimally applicable. To quantify Perception, we compare sense labelings using the cosine similarity. Each labeling is represented as a vector with a separate component for each sense, whose value is the applicability of that sense. The Perception for two annotations of context j is then calculated as Ei perj(sGi ) x perj(sLi ) �E �E i perj(sG i )2 x i perj(sL i )2 Note that because all sense perceptibilities are nonnegative, the cosine similarity is bounded to [0, 1]. Erk and McCarthy (2009) propose an alternate measure for comparing the applicability values using the Jensen-Shannon divergence. The sense annotations are normalized to probability distributions, denoted G and L, and the divergence is computed as: 1 1 JSD(G||L) = �DKL(G||M) + �DKL(L||M) where M is the average of the distributions G and L and DKL denotes the Kullback-Leibler divergence. While both approaches are similar in intent, we find that the cosine similarity better matches the expected difference in Perception for cases where two annotations use different numbers of senses. For example, the fourth test annotat</context>
<context position="29214" citStr="Erk and McCarthy (2009)" startWordPosition="4851" endWordPosition="4854"> the GWS dataset being labeled with most of a word’s senses, as indicated by Table 1, which results in their 195 perceptibilities becoming normalized to small values. Because the ASE solution has applicability ratings for all senses, normalization brings the ratings close to those of the gold standard solution, and furthermore, the difference in score between applicable and inapplicable senses become too small to significantly affect the resulting cosine similarity. As an alternate model, we reevaluated the baselines against the gold standard using the Jensen-Shannon divergence as proposed by Erk and McCarthy (2009). Again, ASE is still the highest performing baseline on Perception. The high performance for both evaluation measures suggests that an alternate measure may be better suited for quantifying the difference in solutions’ GWS applicabilities. Second, performance was higher on the Perception task than on Ranking, the former of which was anticipated being more difficult. We attribute the lower Ranking performance to two factors. First, the GWS data contains main tied rank senses; however, ties in sense ranks after the mapping process are relatively rare, which reduces &apos;y. Second, instances in the </context>
</contexts>
<marker>Erk, McCarthy, 2009</marker>
<rawString>Katrin Erk and Diana McCarthy. 2009. Graded word sense assignment. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1, pages 440–449. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
<author>Nicholas Gaylord</author>
</authors>
<title>Investigations on word senses and word usages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP:</booktitle>
<volume>1</volume>
<pages>10--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1638" citStr="Erk et al. (2009)" startWordPosition="246" endWordPosition="249">cant promise for accurate graded sense annotation. 1 Introduction Word Sense Disambiguation (WSD) aims to identify the sense of a word in a given context, using a predefined sense inventory containing the word’s different meanings (Navigli, 2009). Traditionally, WSD approaches have assumed that each occurrence of a word is best labeled with a single sense. However, human annotators often disagree about which sense is present (Passonneau et al., 2010), especially in cases where some of the possible senses are closely related (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007). Recently, Erk et al. (2009) have shown that in cases of sense ambiguity, a graded notion of sense labeling may be most appropriate and help reduce the ambiguity. Specifically, within a given context, multiple senses of a word may be salient to the reader, with different levels of applicability. For example, in the sentence • The athlete won the gold metal due to her hard work and dedication. multiple senses could be considered applicable for “won” according to the WordNet 3.0 sense inventory (Fellbaum, 1998): 1. win (be the winner in a contest or competition; be victorious) 2. acquire, win, gain (win something through o</context>
<context position="3228" citStr="Erk et al. (2009)" startWordPosition="506" endWordPosition="509">ignified by the gold medal). Although contexts can be constructed to elicit only one of these senses, in the example above, a graded annotation best matches human perception. Graded word sense (GWS) annotation offers significant advantages for sense annotation with a finegrained sense inventory. However, creating a sufficiently large annotated corpus for training supervised GWS disambiguation models presents a significant challenge, i.e., the laborious task of gathering annotations for all combinations of a word’s senses, along with variation in those senses applicabilities. To our knowledge, Erk et al. (2009) have provided the only data set with GWS annotations for 11 terms. 189 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 189–198, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics Therefore, we consider the use of Word Sense Induction (WSI) for GWS annotation. WSI removes the need for substantial training data by automatically deriving a word’s senses and associated sense features through examining its contextual uses. Furthermore, the data-driven sense discovery defines senses as they are present in the corpus, which may identify us</context>
<context position="21332" citStr="Erk et al., 2009" startWordPosition="3562" endWordPosition="3565"> according to the three proposed objectives. In the supervised setting, WSI systems provide GWS annotation of their induced senses for the test corpus, which is already labeled with the gold-standard GWS annotations. Then, a portion of the test corpus with gold standard annotations is used to build a mapping from induced senses to the reference sense inven193 Term PoS # senses Avg. # Senses per Instance add verb 6 4.18 ask verb 7 5.98 win verb 4 3.98 argument noun 7 5.18 interest noun 7 5.12 paper noun 7 5.54 different adj. 5 4.98 important adj. 5 4.82 Table 1: The terms from the GWS dataset (Erk et al., 2009) used in this evaluation tory using one of the three algorithms described in Section 4. The remaining, held-out test corpus instances have their induced senses converted to the gold standard sense inventory and the sense labelings are evaluated for the three objectives from Section 2. In our experiments we divide the reference corpus into five evenly-sized segments and then use four segments (80% of the test corpus) for constructing the mapping and then evaluate the converted GWS annotations of the remaining segment. 5.1 Graded Annotation Data The gold standard GWS annotations are derived from</context>
</contexts>
<marker>Erk, McCarthy, Gaylord, 2009</marker>
<rawString>Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009. Investigations on word senses and word usages. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP: Volume 1, pages 10–18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christine Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christine Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Keith Stevens</author>
</authors>
<title>The S-Space Package: An Open Source Package for Word Space Models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL</booktitle>
<marker>Jurgens, Stevens, 2010</marker>
<rawString>David Jurgens and Keith Stevens. 2010. The S-Space Package: An Open Source Package for Word Space Models. In Proceedings of the ACL 2010 System Demonstrations. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
</authors>
<title>Word sense induction by community detection.</title>
<date>2011</date>
<booktitle>In Proceedings of Sixth ACL Workshop on Graph-based Methods for Natural Language Processing (TextGraphs-6). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11256" citStr="Jurgens (2011)" startWordPosition="1816" endWordPosition="1817">ences in ordering and the omission of a sense. Indeed, in cases where the set of senses in a test annotation is completely disjoint from the set of gold standard senses, the JSS will be positive due to comparing the two distributions against their average; In contrast, the cosine similarity in such cases will be zero, which we argue better matches the expectation that such an annotation does not meet the Perception objective. 3 WSI Models For evaluation we adapt three recent graph-based WSI methods for the task of graded-sense annotation: Navigli and Crisafulli (2010), referred to as Squares, Jurgens (2011), referred to as Link, and UoY (Korkontzelos and Manandhar, 2010). At an abstract level, these methods operate in two stages. First, a graph is built, using either words or word pairs as vertices, and edges are added denoting some form of association between the vertices. Second, senses are derived by clustering or partitioning the graph. We selected these methods based on their superior performance on recent benchmarks and also 1The JSD is a distance measure in [0, 1], which we convert to a similarity JSS = 1 − JSD for easier comparison. 191 for their significant differences in approach. Foll</context>
<context position="13509" citStr="Jurgens (2011)" startWordPosition="2206" endWordPosition="2207">sible from the set of neighbors of e. Edges with ps &lt; Q are removed. The remaining connected components in G denote the senses of w. Sense disambiguation on a context of w is performed by computing the intersection of the context’s terms with the terms in each of the connected components. As originally specified, the component with the largest overlap is labeled as the sense of w. We adapt this to graded senses by returning all intersecting components with applicability proportional to their overlap. Furthermore, for efficiency, we use only noun, verb, and adjective lemmas in the graphs. Link Jurgens (2011) use an all-words method where a single graph is built in order to derive the senses of all words in it. Here, the graph’s clusters do not correspond to a specific word’s senses but rather to contextual features that can be used to disambiguate any of the words in the cluster. In its original specification, the graph is built with edges between co-occurring words and edge weights corresponding to co-occurrence frequency. Edges below a specified threshold T are removed, and then link community detection (Ahn et al., 2010) is applied to discover sense-disambiguating word communities, which are o</context>
<context position="15104" citStr="Jurgens (2011)" startWordPosition="2462" endWordPosition="2463">dex. For a context with the set of features Fi and a community with features Fj, the overlap is measured as |Fj ||Fi∩Fj| |Fi∪Fj|� We adapt this algorithm in three ways. First, rather than use co-occurrence frequency to weight edges between terms, we weight edges accord to their statistical association with the G-test (Dunning, 1993). The G-test weighting helps remove edges whose large edge weights are due to high corpus frequency but provide no disambiguating information, and the weighting also allows the T parameter to be more consistently set across corpora of different sizes. Second, while Jurgens (2011) used only nouns as vertices in the graph, we include both verbs and adjectives due to needing to identify senses for both. Third, for graded senses, we disambiguate a context by reporting all overlapping communities, weighted by their overlap score. UoY Korkontzelos and Manandhar (2010) propose a WSI model that builds a graph for each term for disambiguation. The graph is built in four stages, with four main tuning parameters, summarized next. First, using a reference corpus, all contexts of the target word w are selected to build a list of cooccurring noun lemmas, retaining all those with fr</context>
</contexts>
<marker>Jurgens, 2011</marker>
<rawString>David Jurgens. 2011. Word sense induction by community detection. In Proceedings of Sixth ACL Workshop on Graph-based Methods for Natural Language Processing (TextGraphs-6). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Korkontzelos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>355--358</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11321" citStr="Korkontzelos and Manandhar, 2010" startWordPosition="1824" endWordPosition="1827"> Indeed, in cases where the set of senses in a test annotation is completely disjoint from the set of gold standard senses, the JSS will be positive due to comparing the two distributions against their average; In contrast, the cosine similarity in such cases will be zero, which we argue better matches the expectation that such an annotation does not meet the Perception objective. 3 WSI Models For evaluation we adapt three recent graph-based WSI methods for the task of graded-sense annotation: Navigli and Crisafulli (2010), referred to as Squares, Jurgens (2011), referred to as Link, and UoY (Korkontzelos and Manandhar, 2010). At an abstract level, these methods operate in two stages. First, a graph is built, using either words or word pairs as vertices, and edges are added denoting some form of association between the vertices. Second, senses are derived by clustering or partitioning the graph. We selected these methods based on their superior performance on recent benchmarks and also 1The JSD is a distance measure in [0, 1], which we convert to a similarity JSS = 1 − JSD for easier comparison. 191 for their significant differences in approach. Following, we briefly summarize each method to highlight its key para</context>
<context position="15392" citStr="Korkontzelos and Manandhar (2010)" startWordPosition="2505" endWordPosition="2508"> to their statistical association with the G-test (Dunning, 1993). The G-test weighting helps remove edges whose large edge weights are due to high corpus frequency but provide no disambiguating information, and the weighting also allows the T parameter to be more consistently set across corpora of different sizes. Second, while Jurgens (2011) used only nouns as vertices in the graph, we include both verbs and adjectives due to needing to identify senses for both. Third, for graded senses, we disambiguate a context by reporting all overlapping communities, weighted by their overlap score. UoY Korkontzelos and Manandhar (2010) propose a WSI model that builds a graph for each term for disambiguation. The graph is built in four stages, with four main tuning parameters, summarized next. First, using a reference corpus, all contexts of the target word w are selected to build a list of cooccurring noun lemmas, retaining all those with frequency above P1. Second, the Log-Likelihood ratio (Dunning, 1993) is computed between all selected nouns and w, retaining only those with an association above P2. Third, all remaining nouns are used to create all (n ) noun pairs. Next, each term and 2 pair is mapped to the set of contex</context>
</contexts>
<marker>Korkontzelos, Manandhar, 2010</marker>
<rawString>Ioannis Korkontzelos and Suresh Manandhar. 2010. Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 355–358. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Diana McCarthy</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Word sense induction for novel sense detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for computational Linguistics (EACL</booktitle>
<contexts>
<context position="3896" citStr="Lau et al., 2012" startWordPosition="608" endWordPosition="611">ns for 11 terms. 189 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 189–198, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics Therefore, we consider the use of Word Sense Induction (WSI) for GWS annotation. WSI removes the need for substantial training data by automatically deriving a word’s senses and associated sense features through examining its contextual uses. Furthermore, the data-driven sense discovery defines senses as they are present in the corpus, which may identify usages not present in traditional sense inventories (Lau et al., 2012). Last, many WSI models represent senses loosely as abstractions over usages, which potentially may transfer well to expressing GWS annotations as a blend of their sense usages. In this paper, we consider the performance of WSI models on a GWS task. The contributions of this paper are as follows. First, in Sec. 2, we motivate three GWS annotation objectives and propose corresponding measures that provide fine-grained analysis of the capabilities of different WSI models. Second, in Sec. 4, we propose two new sense mapping procedures for converting an induced sense inventory to a reference sense</context>
</contexts>
<marker>Lau, Cook, McCarthy, Newman, Baldwin, 2012</marker>
<rawString>Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense induction for novel sense detection. In Proceedings of the 13th Conference of the European Chapter of the Association for computational Linguistics (EACL 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis P Klapaftis</author>
<author>Dmitriy Dligach</author>
<author>Sameer S Pradhan</author>
</authors>
<title>SemEval2010 task 14: Word sense induction &amp; disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>63--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17633" citStr="Manandhar et al., 2010" startWordPosition="2891" endWordPosition="2894">ces for each partition and selecting the partition with the highest overlap as the sense of w. We extend this to graded annotation by selecting all partitions with at least one vertex present and set the applicability equal to the degree of overlap. 4 Evaluation Across Sense Inventories Directly comparing GWS annotations from the induced and gold standard sense inventories requires first creating a mapping from the induced senses to the gold standard inventory. Agirre et al. (2006) propose a sense-mapping procedure, which was used in the previous two SemEval WSI Tasks (Agirre and Soroa, 2007; Manandhar et al., 2010). We consider this procedure and two extensions of it to support learning a mapping from graded sense annotations. The procedure of Agirre et al. (2006) uses three corpora: (1) a base corpus from which the senses are derived, (2) a mapping corpus annotated with both gold standard senses, denoted gs, and induced senses, denoted is, and (3) a test corpus annotated with is senses that will be converted to gs senses. Once the senses are induced from the base corpus, the mapping corpus is annotated with is senses and a matrix M is built where cell i, j initially contains the counts of each time gsj</context>
<context position="20690" citStr="Manandhar et al., 2010" startWordPosition="3443" endWordPosition="3447">cond extension, referred to as Distribution, that uses the same matrix construction as the Graded procedure, but rather than mapping each is to a single sense, maps it to a distribution over all gs senses for which it was co-annotated, which is the normalized row vector in M for an is sense. Labeling in the test corpus is then done by summing the distributions of the is senses annotated in the context and normalizing to create a probability distribution over the union of their gs senses. 5 Experiments We adapt the supervised WSD setting used in prior SemEval WSI Tasks (Agirre and Soroa, 2007; Manandhar et al., 2010) to evaluation the models according to the three proposed objectives. In the supervised setting, WSI systems provide GWS annotation of their induced senses for the test corpus, which is already labeled with the gold-standard GWS annotations. Then, a portion of the test corpus with gold standard annotations is used to build a mapping from induced senses to the reference sense inven193 Term PoS # senses Avg. # Senses per Instance add verb 6 4.18 ask verb 7 5.98 win verb 4 3.98 argument noun 7 5.18 interest noun 7 5.12 paper noun 7 5.54 different adj. 5 4.98 important adj. 5 4.82 Table 1: The ter</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dligach, and Sameer S. Pradhan. 2010. SemEval2010 task 14: Word sense induction &amp; disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Relating WordNet senses for word sense disambiguation. Making Sense of Sense: Bringing Psycholinguistics and Computational Linguistics Together,</title>
<date>2006</date>
<pages>17</pages>
<contexts>
<context position="1587" citStr="McCarthy, 2006" startWordPosition="239" endWordPosition="240"> demonstrate that sense induction offers significant promise for accurate graded sense annotation. 1 Introduction Word Sense Disambiguation (WSD) aims to identify the sense of a word in a given context, using a predefined sense inventory containing the word’s different meanings (Navigli, 2009). Traditionally, WSD approaches have assumed that each occurrence of a word is best labeled with a single sense. However, human annotators often disagree about which sense is present (Passonneau et al., 2010), especially in cases where some of the possible senses are closely related (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007). Recently, Erk et al. (2009) have shown that in cases of sense ambiguity, a graded notion of sense labeling may be most appropriate and help reduce the ambiguity. Specifically, within a given context, multiple senses of a word may be salient to the reader, with different levels of applicability. For example, in the sentence • The athlete won the gold metal due to her hard work and dedication. multiple senses could be considered applicable for “won” according to the WordNet 3.0 sense inventory (Fellbaum, 1998): 1. win (be the winner in a contest or competition; be victori</context>
</contexts>
<marker>McCarthy, 2006</marker>
<rawString>Diana McCarthy. 2006. Relating WordNet senses for word sense disambiguation. Making Sense of Sense: Bringing Psycholinguistics and Computational Linguistics Together, page 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The senseval-3 english lexical sample task. In</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>25--28</pages>
<institution>Association for Computational Linguistics.</institution>
<location>Barcelona, Spain,</location>
<contexts>
<context position="22578" citStr="Mihalcea et al., 2004" startWordPosition="3770" endWordPosition="3773">a provided by Erk et al. (2009). Here, three annotators rated the applicability of all WordNet 3.0 senses of a word in a single sentence context. Ratings were done using a 5-point ordinal ranking according to the judgements from 1 – this sense is not applicable to 5 – this usage exactly reflects this sense. Annotators used a wide-range of responses, leading to many applicable senses per instance. We selected the subset of the GWS dataset where each term has 50 annotated contexts, which were distributed evenly between SemCor (Miller et al., 1993) and the SENSEVAL-3 lexical substitution corpus (Mihalcea et al., 2004). Table 1 summarizes the target terms in this context. To prepare the data for evaluation, we constructed the gold standard GWS annotations using the mean applicability ratings of all three annotators for each context. Senses that received a mean rating of 1 (not applicable) were not listed in gold standard labeling for that instance. All remaining responses were normalized to sum to 1. 5.2 Model Configuration For consistency, all three WSI models were trained using the same reference corpus. We used a 2009 snapshot of Wikipedia,2 which was PoS tagged and lemmatized using the TreeTagger (Schmi</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. 2004. The senseval-3 english lexical sample task. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 25–28. Barcelona, Spain, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22507" citStr="Miller et al., 1993" startWordPosition="3760" endWordPosition="3763">old standard GWS annotations are derived from a subset of the GWS data provided by Erk et al. (2009). Here, three annotators rated the applicability of all WordNet 3.0 senses of a word in a single sentence context. Ratings were done using a 5-point ordinal ranking according to the judgements from 1 – this sense is not applicable to 5 – this usage exactly reflects this sense. Annotators used a wide-range of responses, leading to many applicable senses per instance. We selected the subset of the GWS dataset where each term has 50 annotated contexts, which were distributed evenly between SemCor (Miller et al., 1993) and the SENSEVAL-3 lexical substitution corpus (Mihalcea et al., 2004). Table 1 summarizes the target terms in this context. To prepare the data for evaluation, we constructed the gold standard GWS annotations using the mean applicability ratings of all three annotators for each context. Senses that received a mean rating of 1 (not applicable) were not listed in gold standard labeling for that instance. All remaining responses were normalized to sum to 1. 5.2 Model Configuration For consistency, all three WSI models were trained using the same reference corpus. We used a 2009 snapshot of Wiki</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Proceedings of the workshop on Human Language Technology, pages 303–308. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Giuseppe Crisafulli</author>
</authors>
<title>Inducing word senses to improve web search result clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>116--126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11216" citStr="Navigli and Crisafulli (2010)" startWordPosition="1808" endWordPosition="1811">. 1 has a JSS1 of 0.593, despite its significant differences in ordering and the omission of a sense. Indeed, in cases where the set of senses in a test annotation is completely disjoint from the set of gold standard senses, the JSS will be positive due to comparing the two distributions against their average; In contrast, the cosine similarity in such cases will be zero, which we argue better matches the expectation that such an annotation does not meet the Perception objective. 3 WSI Models For evaluation we adapt three recent graph-based WSI methods for the task of graded-sense annotation: Navigli and Crisafulli (2010), referred to as Squares, Jurgens (2011), referred to as Link, and UoY (Korkontzelos and Manandhar, 2010). At an abstract level, these methods operate in two stages. First, a graph is built, using either words or word pairs as vertices, and edges are added denoting some form of association between the vertices. Second, senses are derived by clustering or partitioning the graph. We selected these methods based on their superior performance on recent benchmarks and also 1The JSD is a distance measure in [0, 1], which we convert to a similarity JSS = 1 − JSD for easier comparison. 191 for their s</context>
<context position="12609" citStr="Navigli and Crisafulli (2010)" startWordPosition="2045" endWordPosition="2049">Squares Navigli and Crisafulli (2010) propose a method that builds a separate graph for each term for sense induction. First, a large corpus is used to identify associated terms using the Dice coefficient: For two terms w1, w2, Dice(w1, w2) = 2c(w1,w ) c(w1)+c(w) where c(w) is the frequency of occurrence. Next, for a given term w the initial graph, G, is constructed by adding edges to every term w2 where Dice(w, w2) &gt; S, and then the step is repeated for the neighbors of each term w2 that was added. Once the initial graph is constructed, edges are pruned to separate the graph into components. Navigli and Crisafulli (2010) found improved performance on their target application using a pruning method based on the number of squares (closed paths of length 4) in which an edge participates. Let s denote the number of squares that an edge e participates in and p denote the number of squares that would be possible from the set of neighbors of e. Edges with ps &lt; Q are removed. The remaining connected components in G denote the senses of w. Sense disambiguation on a context of w is performed by computing the intersection of the context’s terms with the terms in each of the connected components. As originally specified,</context>
</contexts>
<marker>Navigli, Crisafulli, 2010</marker>
<rawString>Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing word senses to improve web search result clustering. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 116– 126. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1267" citStr="Navigli, 2009" startWordPosition="187" endWordPosition="188"> graded senses using Word Sense Induction, which automatically learns the senses and their features from corpus properties. Our work proposes three objective to evaluate performance on the graded sense annotation task, and two new methods for mapping between sense inventories using parallel graded sense annotations. We demonstrate that sense induction offers significant promise for accurate graded sense annotation. 1 Introduction Word Sense Disambiguation (WSD) aims to identify the sense of a word in a given context, using a predefined sense inventory containing the word’s different meanings (Navigli, 2009). Traditionally, WSD approaches have assumed that each occurrence of a word is best labeled with a single sense. However, human annotators often disagree about which sense is present (Passonneau et al., 2010), especially in cases where some of the possible senses are closely related (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007). Recently, Erk et al. (2009) have shown that in cases of sense ambiguity, a graded notion of sense labeling may be most appropriate and help reduce the ambiguity. Specifically, within a given context, multiple senses of a word may be salient to the reader, </context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR), 41(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Hoa Trang Dang</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="1609" citStr="Palmer et al., 2007" startWordPosition="241" endWordPosition="244">t sense induction offers significant promise for accurate graded sense annotation. 1 Introduction Word Sense Disambiguation (WSD) aims to identify the sense of a word in a given context, using a predefined sense inventory containing the word’s different meanings (Navigli, 2009). Traditionally, WSD approaches have assumed that each occurrence of a word is best labeled with a single sense. However, human annotators often disagree about which sense is present (Passonneau et al., 2010), especially in cases where some of the possible senses are closely related (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007). Recently, Erk et al. (2009) have shown that in cases of sense ambiguity, a graded notion of sense labeling may be most appropriate and help reduce the ambiguity. Specifically, within a given context, multiple senses of a word may be salient to the reader, with different levels of applicability. For example, in the sentence • The athlete won the gold metal due to her hard work and dedication. multiple senses could be considered applicable for “won” according to the WordNet 3.0 sense inventory (Fellbaum, 1998): 1. win (be the winner in a contest or competition; be victorious) 2. acquire, win, </context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2007</marker>
<rawString>Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum. 2007. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Natural Language Engineering, 13(02):137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Ansaf Salleb-Aoussi</author>
<author>Vikas Bhardwaj</author>
<author>Nancy Ide</author>
</authors>
<title>Word sense annotation of polysemous words by multiple annotators.</title>
<date>2010</date>
<booktitle>In Proceedings of Seventh International Conference on Language Resources and Evaluation (LREC-7).</booktitle>
<contexts>
<context position="1475" citStr="Passonneau et al., 2010" startWordPosition="218" endWordPosition="221">nse annotation task, and two new methods for mapping between sense inventories using parallel graded sense annotations. We demonstrate that sense induction offers significant promise for accurate graded sense annotation. 1 Introduction Word Sense Disambiguation (WSD) aims to identify the sense of a word in a given context, using a predefined sense inventory containing the word’s different meanings (Navigli, 2009). Traditionally, WSD approaches have assumed that each occurrence of a word is best labeled with a single sense. However, human annotators often disagree about which sense is present (Passonneau et al., 2010), especially in cases where some of the possible senses are closely related (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007). Recently, Erk et al. (2009) have shown that in cases of sense ambiguity, a graded notion of sense labeling may be most appropriate and help reduce the ambiguity. Specifically, within a given context, multiple senses of a word may be salient to the reader, with different levels of applicability. For example, in the sentence • The athlete won the gold metal due to her hard work and dedication. multiple senses could be considered applicable for “won” according to</context>
</contexts>
<marker>Passonneau, Salleb-Aoussi, Bhardwaj, Ide, 2010</marker>
<rawString>Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas Bhardwaj, and Nancy Ide. 2010. Word sense annotation of polysemous words by multiple annotators. In Proceedings of Seventh International Conference on Language Resources and Evaluation (LREC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). ACL,</booktitle>
<contexts>
<context position="32163" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="5314" endWordPosition="5317"> First, our study only considered three graphbased WSI models; future work is needed to assess the capabilities other WSI approaches, such as vector-based or Bayesian. We are also interested in comparing the performance of the Link model with other recently developed all-words WSI approaches such as Van de Cruys and Apidianaki (2011). Second, the proposed evaluation relies on a supervised mapping to the gold standard sense inventory, which has potential to lose information and incorrectly map new senses not in the gold standard. While unsupervised clustering evaluations such as the V-measure (Rosenberg and Hirschberg, 2007) and paired Fscore (Artiles et al., 2009) are capable of evaluating without such a mapping, future work is needed to test extrinsic soft clustering evaluations such as BCubed (Amig´o et al., 2009) or develop analogous techniques that take into account graded class membership used in GWS annotations. Last, we note that our setup normalized the GWS ratings into probability distribution, which is standard in the SemEval evaluation setup. However, this normalization incorrectly transforms GWS annotations where no predominant sense was rated at the highest value, e.g., an annotation of only two sen</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). ACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="23186" citStr="Schmid, 1994" startWordPosition="3870" endWordPosition="3871">2004). Table 1 summarizes the target terms in this context. To prepare the data for evaluation, we constructed the gold standard GWS annotations using the mean applicability ratings of all three annotators for each context. Senses that received a mean rating of 1 (not applicable) were not listed in gold standard labeling for that instance. All remaining responses were normalized to sum to 1. 5.2 Model Configuration For consistency, all three WSI models were trained using the same reference corpus. We used a 2009 snapshot of Wikipedia,2 which was PoS tagged and lemmatized using the TreeTagger (Schmid, 1994). All of target terms occurred over 12,000 times. The G-test between terms was computed using a threesentence sliding window within each article in the corpus. The Dice coefficient was calculated using a single sentence as context. For all three models, we performed a limited grid search to find the best performing system parameters, within reasonable computational limits. We summarize the parameters and models, selecting the configuration with the highest average Perception score. For all models, the applicability ratings for each instance are normalized to sum to 1. Model Parameter Range Sel</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N John Castellan Jr</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill,</title>
<date>1988</date>
<note>second edition.</note>
<marker>Siegel, Jr, 1988</marker>
<rawString>Sidney Siegel and N. John Castellan Jr. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Marianna Apidianaki</author>
</authors>
<title>Latent Semantic Word Sense Induction and Disambiguation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT),</booktitle>
<pages>1476--1485</pages>
<marker>Van de Cruys, Apidianaki, 2011</marker>
<rawString>Tim Van de Cruys and Marianna Apidianaki. 2011. Latent Semantic Word Sense Induction and Disambiguation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT), pages 1476– 1485.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>