<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001899">
<title confidence="0.99845">
Biases in Predicting the Human Language Model
</title>
<author confidence="0.992739">
Alex B. Fine
</author>
<affiliation confidence="0.996763">
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.989862">
abfine@illinois.edu
</email>
<author confidence="0.996148">
T. Florian Jaeger
</author>
<affiliation confidence="0.997958">
University of Rochester
</affiliation>
<email confidence="0.997574">
fjaeger@bcs.rochester.edu
</email>
<author confidence="0.584142">
Austin F. Frank
</author>
<affiliation confidence="0.352379">
Riot Games
</affiliation>
<email confidence="0.91836">
aufrank@riotgames.com
</email>
<author confidence="0.98864">
Benjamin Van Durme
</author>
<affiliation confidence="0.962031">
Johns Hopkins University
</affiliation>
<email confidence="0.998268">
vandurme@cs.jhu.edu
</email>
<sectionHeader confidence="0.993885" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970736842105">
We consider the prediction of three hu-
man behavioral measures – lexical deci-
sion, word naming, and picture naming –
through the lens of domain bias in lan-
guage modeling. Contrasting the predic-
tive ability of statistics derived from 6 dif-
ferent corpora, we find intuitive results
showing that, e.g., a British corpus over-
predicts the speed with which an Amer-
ican will react to the words ward and
duke, and that the Google n-grams over-
predicts familiarity with technology terms.
This study aims to provoke increased con-
sideration of the human language model
by NLP practitioners: biases are not lim-
ited to differences between corpora (i.e.
“train” vs. “test”); they can exist as well
between corpora and the intended user of
the resultant technology.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998608">
Computational linguists build statistical language
models for aiding in natural language processing
(NLP) tasks. Computational psycholinguists build
such models to aid in their study of human lan-
guage processing. Errors in NLP are measured
with tools like precision and recall, while errors in
psycholinguistics are defined as failures to model
a target phenomenon.
In the current study, we exploit errors of the lat-
ter variety—failure of a language model to predict
human performance—to investigate bias across
several frequently used corpora in computational
linguistics. The human data is revealing because
it trades on the fact that human language process-
ing is probability-sensitive: language processing
reflects implicit knowledge of probabilities com-
puted over linguistic units (e.g., words). For ex-
ample, the amount of time required to read a word
varies as a function of how predictable that word is
(McDonald and Shillcock, 2003). Thus, failure of
a language model to predict human performance
reveals a mismatch between the language model
and the human language model, i.e., bias.
Psycholinguists have known for some time that
the ability of a corpus to explain behavior depends
on properties of the corpus and the subjects (cf.
Balota et al. (2004)). We extend that line of work
by directly analyzing and quantifying this bias,
and by linking the results to methodological con-
cerns in both NLP and psycholinguistics.
Specifically, we predict human data from
three widely used psycholinguistic experimental
paradigms—lexical decision, word naming, and
picture naming—using unigram frequency esti-
mates from Google n-grams (Brants and Franz,
2006), Switchboard (Godfrey et al., 1992), spoken
and written English portions of CELEX (Baayen
et al., 1995), and spoken and written portions
of the British National Corpus (BNC Consor-
tium, 2007). While we find comparable overall
fits of the behavioral data from all corpora un-
der consideration, our analyses also reveal spe-
cific domain biases. For example, Google n-
grams overestimates the ease with which humans
will process words related to the web (tech, code,
search, site), while the Switchboard corpus—a
collection of informal telephone conversations be-
tween strangers—overestimates how quickly hu-
mans will react to colloquialisms (heck, darn) and
backchannels (wow, right).
</bodyText>
<page confidence="0.992045">
7
</page>
<note confidence="0.494114">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–12,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.956033333333333">
Figure 1: Pairwise correlations between log frequency es-
timates from each corpus. Histograms show distribution over
frequency values from each corpus. Lower left panels give
Pearson (top) and Spearman (bottom) correlation coefficients
and associated p-values for each pair. Upper right panels plot
correlations
</figureCaption>
<sectionHeader confidence="0.532246" genericHeader="method">
2 Fitting Behavioral Data
</sectionHeader>
<subsectionHeader confidence="0.971526">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.981299">
Pairwise Pearson correlation coefficients for log
frequency were computed for all corpora under
consideration. Significant correlations were found
between log frequency estimates for all pairs (Fig-
ure 1). Intuitive biases are apparent in the corre-
lations, e.g.: BNCw correlates heavily with BNCs
(0.91), but less with SWBD (0.79), while BNCs
correlates more with SWBD (0.84).1
</bodyText>
<table confidence="0.943753285714286">
Corpus Size (tokens)
Google n-grams (web release) — 1 trillion
British National Corpus (written, BNCw) — 90 million
British National Corpus (spoken, BNCs) — 10 million
CELEX (written, CELEXw) — 16.6 million
CELEX (spoken, CELEXs) — 1.3 million
Switchboard (Penn Treebank subset 3) — 800,000
</table>
<tableCaption confidence="0.999424">
Table 1: Summary of the corpora under consideration.
</tableCaption>
<subsectionHeader confidence="0.997933">
2.2 Approach
</subsectionHeader>
<bodyText confidence="0.9998308">
We ask whether domain biases manifest as sys-
tematic errors in predicting human behavior. Log
unigram frequency estimates were derived from
each corpus and used to predict reaction times
(RTs) from three experiments employing lexical
</bodyText>
<footnote confidence="0.789915">
1BNCw and BNCs are both British, while BNCs and
SWBD are both spoken.
</footnote>
<bodyText confidence="0.999948235294118">
decision (time required by subjects to correctly
identify a string of letters as a word of English
(Balota et al., 1999)); word naming (time required
to read aloud a visually presented word (Spieler
and Balota, 1997); (Balota and Spieler, 1998));
and picture naming (time required to say a pic-
ture’s name (Bates et al., 2003)). Previous work
has shown that more frequent words lead to faster
RTs. These three measures provide a strong test
for the biases present in these corpora, as they
span written and spoken lexical comprehension
and production.
To compare the predictive strength of log fre-
quency estimates from each corpus, we fit mixed
effects regression models to the data from each
experiment. As controls, all models included (1)
mean log bigram frequency for each word, (2)
word category (noun, verb, etc.), (3) log mor-
phological family size (number of inflectional and
derivational morphological family members), (4)
number of synonyms, and (5) the first principal
component of a host of orthographic and phono-
logical features capturing neighborhood effects
(type and token counts of orthographic and phono-
logical neighbors as well as forward and backward
inconsistent words; (Baayen et al., 2006)). Mod-
els of lexical decision and word naming included
random intercepts of participant age to adjust for
differences in mean RTs between old (mean age
= 72) vs. young (mean age = 23) subjects, given
differences between younger vs. older adults’ pro-
cessing speed (cf. (Ramscar et al., 2014)). (All
participants in the picture naming study were col-
lege students.)
</bodyText>
<subsectionHeader confidence="0.906713">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.999982066666667">
For each of the six panels corresponding to fre-
quency estimates from a corpus A, Figure 2 gives
the χ2 value resulting from the log-likelihood ra-
tio of (1) a model containing A and an estimate
from one of the five remaining corpora (given on
the x axis) and (2) a model containing just the cor-
pus indicated on the x axis. Thus, for each panel,
each bar in Figure 2 shows the explanatory power
of estimates from the corpus given at the top of the
panel after controlling for estimates from each of
the other corpora.
Model fits reveal intuitive, previously undocu-
mented biases in the ability of each corpus to pre-
dict human data. For example, corpora of British
English tend to explain relatively little after con-
</bodyText>
<page confidence="0.987914">
8
</page>
<bodyText confidence="0.999659292307692">
trolling for other British corpora in modeling lexi-
cal decision RTs (yellow). Similarly, Switchboard
provides relatively little explanatory power over
the other corpora in predicting picture naming
RTs (blue bars), possibly because highly image-
able nouns and verbs frequent in everyday interac-
tions are underrepresented in telephone conversa-
tions between people with no common visual ex-
perience. In other words, idiosyncratic facts about
the topics, dialects, etc. represented in each cor-
pus lead to systematic patterns in how well each
corpus can predict human data relative to the oth-
ers. In some cases, the predictive value of one
corpus after controlling for another—apparently
for reasons related to genre, dialect—can be quite
large (cf. the χ2 difference between a model with
both Google and Switchboard frequency estimates
compared to one with only Switchboard [top right
yellow bar]).
In addition to comparing the overall predictive
power of the corpora, we examined the words
for which behavioral predictions derived from the
corpora deviated most from the observed behav-
ior (word frequencies strongly over- or under-
estimated by each corpora). First, in Table 2 we
give the ten words with the greatest relative differ-
ence in frequency for each corpus pair. For exam-
ple, fife is deemed more frequent according to the
BNC than to Google.2
These results suggest that particular corpora
may be genre-biased in systematic ways. For in-
stance, Google appears to be biased towards termi-
nology dealing with adult material and technology.
Similarly, BNCw is biased, relative to Google, to-
wards Britishisms. For these words in the BNC
and Google, we examined errors in predicted lexi-
cal decision times. Figure 3 plots errors in the lin-
ear model’s prediction of RTs for older (top) and
younger (bottom) subjects.
The figure shows a positive correlation between
how large the difference is between the lexical de-
cision RT predicted by the model and the actu-
ally observed RT, and how over-estimated the log
frequency of that word is in the BNC relative to
Google (left panel) or in Google relative to the
BNC (right panel). The left panel shows that BNC
produces a much greater estimate of the log fre-
2Surprisingly, fife was determined to be one of the words
with the largest frequency asymmetry between Switchboard
and the Google n-grams corpus. This was a result of lower-
casing all of the words in in the analyses, and the fact that
Barney Fife was mentioned several times in the BNC.
quency of the word lee relative to Google, which
leads the model to predict a lower RT for this word
than is observed (i.e., the error is positive; though
note that the error is less severe for older relative to
younger subjects). By contrast, the asymmetry be-
tween the two corpora in the estimated frequency
of sir is less severe, so the observed RT deviates
less from the predicted RT. In the right panel, we
see that Google assigns a much greater estimate
of log frequency to the word tech than the BNC,
which leads a model predicting RTs from Google-
derived frequency estimates to predict a far lower
RT for this word than observed.
</bodyText>
<sectionHeader confidence="0.999787" genericHeader="method">
3 Discussion
</sectionHeader>
<bodyText confidence="0.999958">
Researchers in computational linguistics often as-
sume that more data is always better than less
data (Banko and Brill, 2001). This is true in-
sofar as larger corpora allow computational lin-
guists to generate less noisy estimates of the av-
erage language experience of the users of compu-
tational linguistics applications. However, corpus
size does not necessarily eliminate certain types of
biases in estimates of human linguistic experience,
as demonstrated in Figure 3.
Our analyses reveal that 6 commonly used cor-
pora fail to reflect the human language model in
various ways related to dialect, modality, and other
properties of each corpus. Our results point to
a type of bias in commonly used language mod-
els that has been previously overlooked. This bias
may limit the effectiveness of NLP algorithms in-
tended to generalize to a linguistic domains whose
statistical properties are generated by humans.
For psycholinguists these results support an im-
portant methodological point: while each corpus
presents systematic biases in how well it predicts
human behavior, all six corpora are, on the whole,
of comparable predictive value and, specifically,
the results suggest that the web performs as well
as traditional instruments in predicting behavior.
This has two implications for psycholinguistic re-
search. First, as argued by researchers such as
Lew (2009), given the size of the Web compared to
other corpora, research focusing on low-frequency
linguistic events—or requiring knowledge of the
distributional characteristics of varied contexts—
is now more tractable. Second, the viability of
the web in predicting behavior opens up possibil-
ities for computational psycholinguistic research
in languages for which no corpora exist (i.e., most
</bodyText>
<page confidence="0.950782">
9
</page>
<figure confidence="0.997039842105263">
2
χΛ
Pairwise model comparisons
CELEX written
BNC written
Google
120
40
30
30
80
20
20
40
10
10
0
0
0
CELEX spoken
Switchboard
BNC spoken
40
30
task
lexical decision
picture naming
word naming
10
30
20
20
5
10
10
0
0
0
CELEX written
BNC written
BNC written
BNC written
Google
CELEX spoken
BNC spoken
Switchboard
Google
CELEX spoken
BNC spoken
CELEX written
Switchboard
Google
CELEX spoken
BNC spoken
CELEX written
Switchboard
Comparison
</figure>
<figureCaption confidence="0.9975775">
Figure 2: Results of log likelihood ratio model comparisons. Large values indicate that the reference predictor (panel title)
explained a large amount of variance over and above the predictor given on the x-axis.
</figureCaption>
<figure confidence="0.995701171052632">
Error in linear model
-0.1
-0.1
0.4
0.3
0.2
0.1
0.0
0.4
0.3
0.2
0.1
0.0
�� duke god tech teens click
fife march glen nick bin ass web
sir gulf princehank den prep
lord dole wow search
cent lee site
ward code suck
twain pee
link log mail
dia dike
hall arn
but cart
flip hck
dame skp tag
prime store
hop hunk
king file chat gay
slot
sex quote thread
zip
self
ranch printmap page
script
whiz
tire bug
toe
tech
fife nick dike bin
lee ward
sir gulf glen dame code slottwain gay web
cent darn thread
search pee
fil heck site
prep cart
log
dole
lord duke god hank hall den mail teens click
march prince prime link chat zip
king storepage
sex quote
pint ranch
script
map
dal
whiz
skip tire
elf
flip suck
hunk
hop tag bug ass
wow butt
toe
Google &lt; BNC written
Google and BNC written
Google &gt; BNC written
young
old
goog.f
-4
-2
0
2
-3.5 -3.0 -2.5 2.5 3.0 3.5 4.0 4.5 5.0 5.5
Standardized difference score
</figure>
<figureCaption confidence="0.9911105">
Figure 3: Errors in the linear model predicting lexical decision RTs from log frequency are plotted against the standardized
difference in log frequency in the Google n-grams corpus versus the written portion of the BNC. Top and bottom panels show
</figureCaption>
<bodyText confidence="0.990736833333333">
errors for older and younger subjects, respectively. The left panel plots words with much greater frequency in the written
portion of the BNC relative to Google; the right panel plots words occurring more frequently in Google. Errors in the linear
model are plotted against the standardized difference in log frequency across the corpora, and word color encodes the degree to
which each word is more (red) or less (blue) frequent in Google. That the fit line in each graph is above 0 in the y-axis means
that on average these biased words in each domain are being over-predicted, i.e., the corpus frequencies suggest humans will
react (sometimes much) faster than they actually did in the lab.
</bodyText>
<page confidence="0.991929">
10
</page>
<table confidence="0.605256">
Greater Lesser Top-10
</table>
<tableCaption confidence="0.99370837037037">
google bnc.s web, ass, gay, tire, text, tool, code, woe, site, zip
google bnc.w ass, teens, tech, gay, bug, suck, site, cart, log, search
google celex.s teens, cart, gay, zip, mail, bin, tech, click, pee, site
google celex.w web, full, gay, bin, mail, zip, site, sake, ass, log
google swbd gay, thread, text, search, site, link, teens, seek, post, sex
bnc.w google fife, lord, duke, march, dole, god, cent, nick, dame, draught
bnc.w bnc.s pact, corps, foe, tract, hike, ridge, dine, crest, aide, whim
bnc.w celex.s staff, nick, full, waist, ham, lap, knit, sheer, bail, march
bnc.w celex.w staff, lord, last, nick, fair, glen, low, march, should, west
bnc.w swbd rose, prince, seek, cent, text, clause, keen, breach, soul, rise
celex.s google art, yes, pound, spoke, think, mean, say, thing, go, drove
celex.s bnc.s art, hike, pact, howl, ski, corps, peer, spoke, jazz, are
celex.s bnc.w art, yes, dike, think, thing, sort, mean, write, pound, lot
celex.s celex.w yes, sort, thank, think, jazz, heck, tape, well, fife, get
celex.s swbd art, cell, rose, spoke, aim, seek, shall, seed, text, knight
celex.w google art, plod, pound, shake, spoke, dine, howl, sit, say, draught
celex.w bnc.s hunch, stare, strife, hike, woe, aide, rout, yell, glaze, flee
celex.w bnc.w dike, whiz, dine, shake, grind, jerk, whoop, say, are, cram
celex.w celex.s wrist, pill, lawn, clutch, stare, spray, jar, shark, plead, horn
celex.w swbd art, rose, seek, aim, rise, burst, seed, cheek, grin, lip
swbd google mow, kind, lot, think, fife, corps, right, cook, sort, do
swbd bnc.s creek, mow, guess, pact, strife, tract, hank, howl, foe, nap
swbd bnc.w stuff, whiz, tech, lot, kind, creek, darn, dike, bet, kid
swbd celex.s wow, sauce, mall, deck, full, spray, flute, rib, guy, bunch
swbd celex.w heck, guess, right, full, stuff, lot, last, well, guy, fair
Table 2: Examples of words with largest difference in z-transformed log frequencies (e.g., the relative frequencies of fife,
lord, and duke, in the BNC are far greater than in Google).
</tableCaption>
<bodyText confidence="0.999975580645161">
languages). This furthers the arguments of the “the
web as corpus” community (Kilgarriff and Grefen-
stette, 2003) with respect to psycholinguistics.
Finally, combining multiple sources of fre-
quency estimates is one way researchers may be
able to reduce the prediction bias from any sin-
gle corpus. This relates to work in automatically
building domain specific corpora (e.g., Moore and
Lewis (2010), Axelrod et al. (2011), Daum´e III
and Jagarlamudi (2011), Wang et al. (2014), Gao
et al. (2002), and Lin et al. (1997)). Those efforts
focus on building representative document collec-
tions for a target domain, usually based on a seed
set of initial documents. Our results prompt the
question: can one use human behavior as the tar-
get in the construction of such a corpus? Con-
cretely, can we build corpora by optimizing an ob-
jective measure that minimizes error in predicting
human reaction times? Prior work in building bal-
anced corpora used either rough estimates of the
ratio of genre styles a normal human is exposed to
daily (e.g., the Brown corpus (Kucera and Fran-
cis, 1967)), or simply sampled text evenly across
genres (e.g., COCA: the Corpus of Contemporary
American English (Davies, 2009)). Just as lan-
guage models have been used to predict reading
grade-level of documents (Collins-Thompson and
Callan, 2004), human language models could be
used to predict the appropriateness of a document
for inclusion in an “automatically balanced” cor-
pus.
</bodyText>
<sectionHeader confidence="0.999296" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999995615384616">
We have shown intuitive, domain-specific biases
in the prediction of human behavioral measures
via corpora of various genres. While some psy-
cholinguists have previously acknowledged that
different corpora carry different predictive power,
this is the first work to our knowledge to system-
atically document these biases across a range of
corpora, and to relate these predictive errors to do-
main bias, a pressing issue in the NLP community.
With these results in hand, future work may now
consider the automatic construction of a “prop-
erly” balanced text collection, such as originally
desired by the creators of the Brown corpus.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997492142857143">
The authors wish to thank three anonymous ACL
reviewers for helpful feedback. This research
was supported by a DARPA award (FA8750-13-2-
0017) and NSF grant IIS-0916599 to BVD, NSF
IIS-1150028 CAREER Award and Alfred P. Sloan
Fellowship to TFJ, and an NSF Graduate Research
Fellowship to ABF.
</bodyText>
<page confidence="0.99782">
11
</page>
<note confidence="0.5950107">
J. Gao, J. Goodman, M. Li, and K. F. Lee. 2002. To-
ward a unified approach to statistical language mod-
eling for chinese. In Proceedings of the ACM Trans-
actions on Asian Language Information Processing
(TALIP 02).
References
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-
tation via pseudo in-domain data selection. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 11).
</note>
<reference confidence="0.998411862068966">
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2). Linguis-
tic Data Consortium, Philadelphia.
R. H. Baayen, L. F. Feldman, and R. Schreuder.
2006. Morphological influences on the recognition
of monosyllabic monomorphemic words. Journal of
Memory and Language, 53:496–512.
D. A. Balota and D. H. Spieler. 1998. The utility of
item-level analyses in model evaluation: A reply to
Seidenberg &amp; Plaut (1998). Psychological Science.
D. A. Balota, M. J. Cortese, and M. Pilotti. 1999. Item-
level analyses of lexical decision performance: Re-
sults from a mega-study. In Abstracts of the 40th An-
nual Meeting of the Psychonomics Society, page 44.
D. Balota, M. Cortese, S. Sergent-Marshall, D. Spieler,
and M. Yap. 2004. Visual word recognition for
single-syllable words. Journal of Experimental Psy-
chology:General, (133):283316.
M. Banko and E. Brill. 2001. Mitigating the paucity of
data problem. Human Language Technology.
E. Bates, S. D’Amico, T. Jacobsen, A. Szkely, E. An-
donova, A. Devescovi, D. Herron, CC Lu, T. Pech-
mann, C. Plh, N. Wicha, K. Federmeier, I. Gerd-
jikova, G. Gutierrez, D. Hung, J. Hsu, G. Iyer,
K. Kohnert, T. Mehotcheva, A. Orozco-Figueroa,
A. Tzeng, and O. Tzeng. 2003. Timed picture nam-
ing in seven languages. Psychonomic Bulletin &amp; Re-
view, 10(2):344–380.
BNC Consortium. 2007. The British National Corpus,
version 3 (BNC XML Edition). Distributed by Ox-
ford University Computing Services on behalf of the
BNC Consortium.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium (LDC).
Kevyn Collins-Thompson and James P. Callan. 2004.
A language modeling approach to predicting reading
difficulty. In HLT-NAACL, pages 193–200.
H. Daum´e III and J. Jagarlamudi. 2011. Domain
adaptation for machine translation by mining unseen
words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT 11).
M. Davies. 2009. The 385+ million word corpus of
contemporary american english (19902008+): De-
sign, architecture, and linguistic insights. Inter-
national Journal of Corpus Linguistics, 14(2):159–
190.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone Speech Corpus for
Research and Development. In Proceedings of
ICASSP-92, pages 517–520.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29(3):333–348.
H. Kucera and W.N. Francis. 1967. Computational
analysis of present-day american english. provi-
dence, ri: Brown university press.
R. Lew, 2009. Contemporary Corpus Linguistics,
chapter The Web as corpus versus traditional cor-
pora: Their relative utility for linguists and language
learners, pages 289–300. London/New York: Con-
tinuum.
S. C. Lin, C. L. Tsai, L. F. Chien, K. J. Chen, and
L. S. Lee. 1997. Chinese language model adapta-
tion based on document classification and multiple
domain-specific language models. In Proceedings
of the 5th European Conference on Speech Commu-
nication and Technology.
S.A. McDonald and R.C. Shillcock. 2003. Eye
movements reveal the on-line computation of lexical
probabilities during reading. Psychological science,
14(6):648–52, November.
R. C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 10).
M. Ramscar, P. Hendrix, C. Shaoul, P. Milin, and R. H.
Baayen. 2014. The myth of cognitive decline: non-
linear dynamics of lifelong learning. Topics in Cog-
nitive Science, 32:5–42.
D. H. Spieler and D. A. Balota. 1997. Bringing com-
putational models of word naming down to the item
level. 6:411–416.
L. Wang, D.F. Wong, L.S. Chao, Y. Lu, and J. Xing.
2014. A systematic comparison of data selection
criteria for smt domain adaptation. The Scientific
World Journal.
</reference>
<page confidence="0.998407">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.858049">
<title confidence="0.997428">Biases in Predicting the Human Language Model</title>
<author confidence="0.99998">Alex B Fine</author>
<affiliation confidence="0.997747">University of Illinois at</affiliation>
<email confidence="0.991886">abfine@illinois.edu</email>
<author confidence="0.9995">T Florian</author>
<affiliation confidence="0.99944">University of</affiliation>
<email confidence="0.998394">fjaeger@bcs.rochester.edu</email>
<author confidence="0.9323155">Austin F Riot Games</author>
<email confidence="0.996651">aufrank@riotgames.com</email>
<author confidence="0.9971925">Benjamin Van_Johns Hopkins</author>
<email confidence="0.999447">vandurme@cs.jhu.edu</email>
<abstract confidence="0.99951765">We consider the prediction of three human behavioral measures – lexical decision, word naming, and picture naming – through the lens of domain bias in language modeling. Contrasting the predictive ability of statistics derived from 6 different corpora, we find intuitive results showing that, e.g., a British corpus overpredicts the speed with which an Amerwill react to the words and that the Google n-grams overpredicts familiarity with technology terms. This study aims to provoke increased consideration of the human language model by NLP practitioners: biases are not limited to differences between corpora (i.e. “train” vs. “test”); they can exist as well between corpora and the intended user of the resultant technology.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>L Gulikers</author>
</authors>
<date>1995</date>
<booktitle>The CELEX Lexical Database (Release 2). Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="2836" citStr="Baayen et al., 1995" startWordPosition="426" endWordPosition="429">e ability of a corpus to explain behavior depends on properties of the corpus and the subjects (cf. Balota et al. (2004)). We extend that line of work by directly analyzing and quantifying this bias, and by linking the results to methodological concerns in both NLP and psycholinguistics. Specifically, we predict human data from three widely used psycholinguistic experimental paradigms—lexical decision, word naming, and picture naming—using unigram frequency estimates from Google n-grams (Brants and Franz, 2006), Switchboard (Godfrey et al., 1992), spoken and written English portions of CELEX (Baayen et al., 1995), and spoken and written portions of the British National Corpus (BNC Consortium, 2007). While we find comparable overall fits of the behavioral data from all corpora under consideration, our analyses also reveal specific domain biases. For example, Google ngrams overestimates the ease with which humans will process words related to the web (tech, code, search, site), while the Switchboard corpus—a collection of informal telephone conversations between strangers—overestimates how quickly humans will react to colloquialisms (heck, darn) and backchannels (wow, right). 7 Proceedings of the 52nd A</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1995</marker>
<rawString>R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995. The CELEX Lexical Database (Release 2). Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>L F Feldman</author>
<author>R Schreuder</author>
</authors>
<title>Morphological influences on the recognition of monosyllabic monomorphemic words.</title>
<date>2006</date>
<journal>Journal of Memory and Language,</journal>
<pages>53--496</pages>
<contexts>
<context position="6212" citStr="Baayen et al., 2006" startWordPosition="940" endWordPosition="943">og frequency estimates from each corpus, we fit mixed effects regression models to the data from each experiment. As controls, all models included (1) mean log bigram frequency for each word, (2) word category (noun, verb, etc.), (3) log morphological family size (number of inflectional and derivational morphological family members), (4) number of synonyms, and (5) the first principal component of a host of orthographic and phonological features capturing neighborhood effects (type and token counts of orthographic and phonological neighbors as well as forward and backward inconsistent words; (Baayen et al., 2006)). Models of lexical decision and word naming included random intercepts of participant age to adjust for differences in mean RTs between old (mean age = 72) vs. young (mean age = 23) subjects, given differences between younger vs. older adults’ processing speed (cf. (Ramscar et al., 2014)). (All participants in the picture naming study were college students.) 2.3 Results For each of the six panels corresponding to frequency estimates from a corpus A, Figure 2 gives the χ2 value resulting from the log-likelihood ratio of (1) a model containing A and an estimate from one of the five remaining c</context>
</contexts>
<marker>Baayen, Feldman, Schreuder, 2006</marker>
<rawString>R. H. Baayen, L. F. Feldman, and R. Schreuder. 2006. Morphological influences on the recognition of monosyllabic monomorphemic words. Journal of Memory and Language, 53:496–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Balota</author>
<author>D H Spieler</author>
</authors>
<title>The utility of item-level analyses in model evaluation: A reply to Seidenberg &amp; Plaut</title>
<date>1998</date>
<publisher>Psychological Science.</publisher>
<contexts>
<context position="5246" citStr="Balota and Spieler, 1998" startWordPosition="787" endWordPosition="790">) — 800,000 Table 1: Summary of the corpora under consideration. 2.2 Approach We ask whether domain biases manifest as systematic errors in predicting human behavior. Log unigram frequency estimates were derived from each corpus and used to predict reaction times (RTs) from three experiments employing lexical 1BNCw and BNCs are both British, while BNCs and SWBD are both spoken. decision (time required by subjects to correctly identify a string of letters as a word of English (Balota et al., 1999)); word naming (time required to read aloud a visually presented word (Spieler and Balota, 1997); (Balota and Spieler, 1998)); and picture naming (time required to say a picture’s name (Bates et al., 2003)). Previous work has shown that more frequent words lead to faster RTs. These three measures provide a strong test for the biases present in these corpora, as they span written and spoken lexical comprehension and production. To compare the predictive strength of log frequency estimates from each corpus, we fit mixed effects regression models to the data from each experiment. As controls, all models included (1) mean log bigram frequency for each word, (2) word category (noun, verb, etc.), (3) log morphological fa</context>
</contexts>
<marker>Balota, Spieler, 1998</marker>
<rawString>D. A. Balota and D. H. Spieler. 1998. The utility of item-level analyses in model evaluation: A reply to Seidenberg &amp; Plaut (1998). Psychological Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Balota</author>
<author>M J Cortese</author>
<author>M Pilotti</author>
</authors>
<title>Itemlevel analyses of lexical decision performance: Results from a mega-study.</title>
<date>1999</date>
<booktitle>In Abstracts of the 40th Annual Meeting of the Psychonomics Society,</booktitle>
<pages>44</pages>
<contexts>
<context position="5122" citStr="Balota et al., 1999" startWordPosition="768" endWordPosition="771">million CELEX (written, CELEXw) — 16.6 million CELEX (spoken, CELEXs) — 1.3 million Switchboard (Penn Treebank subset 3) — 800,000 Table 1: Summary of the corpora under consideration. 2.2 Approach We ask whether domain biases manifest as systematic errors in predicting human behavior. Log unigram frequency estimates were derived from each corpus and used to predict reaction times (RTs) from three experiments employing lexical 1BNCw and BNCs are both British, while BNCs and SWBD are both spoken. decision (time required by subjects to correctly identify a string of letters as a word of English (Balota et al., 1999)); word naming (time required to read aloud a visually presented word (Spieler and Balota, 1997); (Balota and Spieler, 1998)); and picture naming (time required to say a picture’s name (Bates et al., 2003)). Previous work has shown that more frequent words lead to faster RTs. These three measures provide a strong test for the biases present in these corpora, as they span written and spoken lexical comprehension and production. To compare the predictive strength of log frequency estimates from each corpus, we fit mixed effects regression models to the data from each experiment. As controls, all</context>
</contexts>
<marker>Balota, Cortese, Pilotti, 1999</marker>
<rawString>D. A. Balota, M. J. Cortese, and M. Pilotti. 1999. Itemlevel analyses of lexical decision performance: Results from a mega-study. In Abstracts of the 40th Annual Meeting of the Psychonomics Society, page 44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Balota</author>
<author>M Cortese</author>
<author>S Sergent-Marshall</author>
<author>D Spieler</author>
<author>M Yap</author>
</authors>
<title>Visual word recognition for single-syllable words.</title>
<date>2004</date>
<journal>Journal of Experimental Psychology:General,</journal>
<contexts>
<context position="2336" citStr="Balota et al. (2004)" startWordPosition="354" endWordPosition="357">an language processing is probability-sensitive: language processing reflects implicit knowledge of probabilities computed over linguistic units (e.g., words). For example, the amount of time required to read a word varies as a function of how predictable that word is (McDonald and Shillcock, 2003). Thus, failure of a language model to predict human performance reveals a mismatch between the language model and the human language model, i.e., bias. Psycholinguists have known for some time that the ability of a corpus to explain behavior depends on properties of the corpus and the subjects (cf. Balota et al. (2004)). We extend that line of work by directly analyzing and quantifying this bias, and by linking the results to methodological concerns in both NLP and psycholinguistics. Specifically, we predict human data from three widely used psycholinguistic experimental paradigms—lexical decision, word naming, and picture naming—using unigram frequency estimates from Google n-grams (Brants and Franz, 2006), Switchboard (Godfrey et al., 1992), spoken and written English portions of CELEX (Baayen et al., 1995), and spoken and written portions of the British National Corpus (BNC Consortium, 2007). While we fi</context>
</contexts>
<marker>Balota, Cortese, Sergent-Marshall, Spieler, Yap, 2004</marker>
<rawString>D. Balota, M. Cortese, S. Sergent-Marshall, D. Spieler, and M. Yap. 2004. Visual word recognition for single-syllable words. Journal of Experimental Psychology:General, (133):283316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Mitigating the paucity of data problem. Human Language Technology.</title>
<date>2001</date>
<contexts>
<context position="10557" citStr="Banko and Brill, 2001" startWordPosition="1676" endWordPosition="1679">ugh note that the error is less severe for older relative to younger subjects). By contrast, the asymmetry between the two corpora in the estimated frequency of sir is less severe, so the observed RT deviates less from the predicted RT. In the right panel, we see that Google assigns a much greater estimate of log frequency to the word tech than the BNC, which leads a model predicting RTs from Googlederived frequency estimates to predict a far lower RT for this word than observed. 3 Discussion Researchers in computational linguistics often assume that more data is always better than less data (Banko and Brill, 2001). This is true insofar as larger corpora allow computational linguists to generate less noisy estimates of the average language experience of the users of computational linguistics applications. However, corpus size does not necessarily eliminate certain types of biases in estimates of human linguistic experience, as demonstrated in Figure 3. Our analyses reveal that 6 commonly used corpora fail to reflect the human language model in various ways related to dialect, modality, and other properties of each corpus. Our results point to a type of bias in commonly used language models that has been</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>M. Banko and E. Brill. 2001. Mitigating the paucity of data problem. Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bates</author>
<author>S D’Amico</author>
<author>T Jacobsen</author>
<author>A Szkely</author>
<author>E Andonova</author>
<author>A Devescovi</author>
<author>D Herron</author>
<author>CC Lu</author>
<author>T Pechmann</author>
<author>C Plh</author>
<author>N Wicha</author>
<author>K Federmeier</author>
<author>I Gerdjikova</author>
<author>G Gutierrez</author>
<author>D Hung</author>
<author>J Hsu</author>
<author>G Iyer</author>
<author>K Kohnert</author>
<author>T Mehotcheva</author>
<author>A Orozco-Figueroa</author>
<author>A Tzeng</author>
<author>O Tzeng</author>
</authors>
<title>Timed picture naming in seven languages.</title>
<date>2003</date>
<journal>Psychonomic Bulletin &amp; Review,</journal>
<volume>10</volume>
<issue>2</issue>
<marker>Bates, D’Amico, Jacobsen, Szkely, Andonova, Devescovi, Herron, Lu, Pechmann, Plh, Wicha, Federmeier, Gerdjikova, Gutierrez, Hung, Hsu, Iyer, Kohnert, Mehotcheva, Orozco-Figueroa, Tzeng, Tzeng, 2003</marker>
<rawString>E. Bates, S. D’Amico, T. Jacobsen, A. Szkely, E. Andonova, A. Devescovi, D. Herron, CC Lu, T. Pechmann, C. Plh, N. Wicha, K. Federmeier, I. Gerdjikova, G. Gutierrez, D. Hung, J. Hsu, G. Iyer, K. Kohnert, T. Mehotcheva, A. Orozco-Figueroa, A. Tzeng, and O. Tzeng. 2003. Timed picture naming in seven languages. Psychonomic Bulletin &amp; Review, 10(2):344–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BNC Consortium</author>
</authors>
<date>2007</date>
<booktitle>The British National Corpus, version 3 (BNC XML Edition). Distributed by Oxford University Computing Services on behalf of the BNC Consortium.</booktitle>
<contexts>
<context position="2923" citStr="Consortium, 2007" startWordPosition="441" endWordPosition="443">ects (cf. Balota et al. (2004)). We extend that line of work by directly analyzing and quantifying this bias, and by linking the results to methodological concerns in both NLP and psycholinguistics. Specifically, we predict human data from three widely used psycholinguistic experimental paradigms—lexical decision, word naming, and picture naming—using unigram frequency estimates from Google n-grams (Brants and Franz, 2006), Switchboard (Godfrey et al., 1992), spoken and written English portions of CELEX (Baayen et al., 1995), and spoken and written portions of the British National Corpus (BNC Consortium, 2007). While we find comparable overall fits of the behavioral data from all corpora under consideration, our analyses also reveal specific domain biases. For example, Google ngrams overestimates the ease with which humans will process words related to the web (tech, code, search, site), while the Switchboard corpus—a collection of informal telephone conversations between strangers—overestimates how quickly humans will react to colloquialisms (heck, darn) and backchannels (wow, right). 7 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–</context>
</contexts>
<marker>Consortium, 2007</marker>
<rawString>BNC Consortium. 2007. The British National Corpus, version 3 (BNC XML Edition). Distributed by Oxford University Computing Services on behalf of the BNC Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Version 1. Linguistic Data Consortium (LDC).</booktitle>
<contexts>
<context position="2732" citStr="Brants and Franz, 2006" startWordPosition="410" endWordPosition="413">e language model and the human language model, i.e., bias. Psycholinguists have known for some time that the ability of a corpus to explain behavior depends on properties of the corpus and the subjects (cf. Balota et al. (2004)). We extend that line of work by directly analyzing and quantifying this bias, and by linking the results to methodological concerns in both NLP and psycholinguistics. Specifically, we predict human data from three widely used psycholinguistic experimental paradigms—lexical decision, word naming, and picture naming—using unigram frequency estimates from Google n-grams (Brants and Franz, 2006), Switchboard (Godfrey et al., 1992), spoken and written English portions of CELEX (Baayen et al., 1995), and spoken and written portions of the British National Corpus (BNC Consortium, 2007). While we find comparable overall fits of the behavioral data from all corpora under consideration, our analyses also reveal specific domain biases. For example, Google ngrams overestimates the ease with which humans will process words related to the web (tech, code, search, site), while the Switchboard corpus—a collection of informal telephone conversations between strangers—overestimates how quickly hum</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium (LDC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>James P Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>193--200</pages>
<contexts>
<context position="18029" citStr="Collins-Thompson and Callan, 2004" startWordPosition="2919" endWordPosition="2922">uestion: can one use human behavior as the target in the construction of such a corpus? Concretely, can we build corpora by optimizing an objective measure that minimizes error in predicting human reaction times? Prior work in building balanced corpora used either rough estimates of the ratio of genre styles a normal human is exposed to daily (e.g., the Brown corpus (Kucera and Francis, 1967)), or simply sampled text evenly across genres (e.g., COCA: the Corpus of Contemporary American English (Davies, 2009)). Just as language models have been used to predict reading grade-level of documents (Collins-Thompson and Callan, 2004), human language models could be used to predict the appropriateness of a document for inclusion in an “automatically balanced” corpus. 4 Conclusion We have shown intuitive, domain-specific biases in the prediction of human behavioral measures via corpora of various genres. While some psycholinguists have previously acknowledged that different corpora carry different predictive power, this is the first work to our knowledge to systematically document these biases across a range of corpora, and to relate these predictive errors to domain bias, a pressing issue in the NLP community. With these r</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>Kevyn Collins-Thompson and James P. Callan. 2004. A language modeling approach to predicting reading difficulty. In HLT-NAACL, pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>J Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 11).</booktitle>
<marker>Daum´e, Jagarlamudi, 2011</marker>
<rawString>H. Daum´e III and J. Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Davies</author>
</authors>
<title>The 385+ million word corpus of contemporary american english (19902008+): Design, architecture, and linguistic insights.</title>
<date>2009</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>14</volume>
<issue>2</issue>
<pages>190</pages>
<contexts>
<context position="17908" citStr="Davies, 2009" startWordPosition="2903" endWordPosition="2904">ions for a target domain, usually based on a seed set of initial documents. Our results prompt the question: can one use human behavior as the target in the construction of such a corpus? Concretely, can we build corpora by optimizing an objective measure that minimizes error in predicting human reaction times? Prior work in building balanced corpora used either rough estimates of the ratio of genre styles a normal human is exposed to daily (e.g., the Brown corpus (Kucera and Francis, 1967)), or simply sampled text evenly across genres (e.g., COCA: the Corpus of Contemporary American English (Davies, 2009)). Just as language models have been used to predict reading grade-level of documents (Collins-Thompson and Callan, 2004), human language models could be used to predict the appropriateness of a document for inclusion in an “automatically balanced” corpus. 4 Conclusion We have shown intuitive, domain-specific biases in the prediction of human behavioral measures via corpora of various genres. While some psycholinguists have previously acknowledged that different corpora carry different predictive power, this is the first work to our knowledge to systematically document these biases across a ra</context>
</contexts>
<marker>Davies, 2009</marker>
<rawString>M. Davies. 2009. The 385+ million word corpus of contemporary american english (19902008+): Design, architecture, and linguistic insights. International Journal of Corpus Linguistics, 14(2):159– 190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone Speech Corpus for Research and Development.</title>
<date>1992</date>
<booktitle>In Proceedings of ICASSP-92,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="2768" citStr="Godfrey et al., 1992" startWordPosition="415" endWordPosition="418">e model, i.e., bias. Psycholinguists have known for some time that the ability of a corpus to explain behavior depends on properties of the corpus and the subjects (cf. Balota et al. (2004)). We extend that line of work by directly analyzing and quantifying this bias, and by linking the results to methodological concerns in both NLP and psycholinguistics. Specifically, we predict human data from three widely used psycholinguistic experimental paradigms—lexical decision, word naming, and picture naming—using unigram frequency estimates from Google n-grams (Brants and Franz, 2006), Switchboard (Godfrey et al., 1992), spoken and written English portions of CELEX (Baayen et al., 1995), and spoken and written portions of the British National Corpus (BNC Consortium, 2007). While we find comparable overall fits of the behavioral data from all corpora under consideration, our analyses also reveal specific domain biases. For example, Google ngrams overestimates the ease with which humans will process words related to the web (tech, code, search, site), while the Switchboard corpus—a collection of informal telephone conversations between strangers—overestimates how quickly humans will react to colloquialisms (he</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. Godfrey, E. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone Speech Corpus for Research and Development. In Proceedings of ICASSP-92, pages 517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>G Grefenstette</author>
</authors>
<title>Introduction to the special issue on the web as corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="16825" citStr="Kilgarriff and Grefenstette, 2003" startWordPosition="2721" endWordPosition="2725">d, lot, think, fife, corps, right, cook, sort, do swbd bnc.s creek, mow, guess, pact, strife, tract, hank, howl, foe, nap swbd bnc.w stuff, whiz, tech, lot, kind, creek, darn, dike, bet, kid swbd celex.s wow, sauce, mall, deck, full, spray, flute, rib, guy, bunch swbd celex.w heck, guess, right, full, stuff, lot, last, well, guy, fair Table 2: Examples of words with largest difference in z-transformed log frequencies (e.g., the relative frequencies of fife, lord, and duke, in the BNC are far greater than in Google). languages). This furthers the arguments of the “the web as corpus” community (Kilgarriff and Grefenstette, 2003) with respect to psycholinguistics. Finally, combining multiple sources of frequency estimates is one way researchers may be able to reduce the prediction bias from any single corpus. This relates to work in automatically building domain specific corpora (e.g., Moore and Lewis (2010), Axelrod et al. (2011), Daum´e III and Jagarlamudi (2011), Wang et al. (2014), Gao et al. (2002), and Lin et al. (1997)). Those efforts focus on building representative document collections for a target domain, usually based on a seed set of initial documents. Our results prompt the question: can one use human beh</context>
</contexts>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>A. Kilgarriff and G. Grefenstette. 2003. Introduction to the special issue on the web as corpus. Computational Linguistics, 29(3):333–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kucera</author>
<author>W N Francis</author>
</authors>
<title>Computational analysis of present-day american english. providence, ri: Brown university press.</title>
<date>1967</date>
<contexts>
<context position="17790" citStr="Kucera and Francis, 1967" startWordPosition="2883" endWordPosition="2887">1), Wang et al. (2014), Gao et al. (2002), and Lin et al. (1997)). Those efforts focus on building representative document collections for a target domain, usually based on a seed set of initial documents. Our results prompt the question: can one use human behavior as the target in the construction of such a corpus? Concretely, can we build corpora by optimizing an objective measure that minimizes error in predicting human reaction times? Prior work in building balanced corpora used either rough estimates of the ratio of genre styles a normal human is exposed to daily (e.g., the Brown corpus (Kucera and Francis, 1967)), or simply sampled text evenly across genres (e.g., COCA: the Corpus of Contemporary American English (Davies, 2009)). Just as language models have been used to predict reading grade-level of documents (Collins-Thompson and Callan, 2004), human language models could be used to predict the appropriateness of a document for inclusion in an “automatically balanced” corpus. 4 Conclusion We have shown intuitive, domain-specific biases in the prediction of human behavioral measures via corpora of various genres. While some psycholinguists have previously acknowledged that different corpora carry d</context>
</contexts>
<marker>Kucera, Francis, 1967</marker>
<rawString>H. Kucera and W.N. Francis. 1967. Computational analysis of present-day american english. providence, ri: Brown university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lew</author>
</authors>
<title>Contemporary Corpus Linguistics, chapter The Web as corpus versus traditional corpora: Their relative utility for linguists and language learners,</title>
<date>2009</date>
<pages>289--300</pages>
<publisher>Continuum.</publisher>
<location>London/New York:</location>
<contexts>
<context position="11793" citStr="Lew (2009)" startWordPosition="1871" endWordPosition="1872">is bias may limit the effectiveness of NLP algorithms intended to generalize to a linguistic domains whose statistical properties are generated by humans. For psycholinguists these results support an important methodological point: while each corpus presents systematic biases in how well it predicts human behavior, all six corpora are, on the whole, of comparable predictive value and, specifically, the results suggest that the web performs as well as traditional instruments in predicting behavior. This has two implications for psycholinguistic research. First, as argued by researchers such as Lew (2009), given the size of the Web compared to other corpora, research focusing on low-frequency linguistic events—or requiring knowledge of the distributional characteristics of varied contexts— is now more tractable. Second, the viability of the web in predicting behavior opens up possibilities for computational psycholinguistic research in languages for which no corpora exist (i.e., most 9 2 χΛ Pairwise model comparisons CELEX written BNC written Google 120 40 30 30 80 20 20 40 10 10 0 0 0 CELEX spoken Switchboard BNC spoken 40 30 task lexical decision picture naming word naming 10 30 20 20 5 10 1</context>
</contexts>
<marker>Lew, 2009</marker>
<rawString>R. Lew, 2009. Contemporary Corpus Linguistics, chapter The Web as corpus versus traditional corpora: Their relative utility for linguists and language learners, pages 289–300. London/New York: Continuum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Lin</author>
<author>C L Tsai</author>
<author>L F Chien</author>
<author>K J Chen</author>
<author>L S Lee</author>
</authors>
<title>Chinese language model adaptation based on document classification and multiple domain-specific language models.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="17229" citStr="Lin et al. (1997)" startWordPosition="2788" endWordPosition="2791">(e.g., the relative frequencies of fife, lord, and duke, in the BNC are far greater than in Google). languages). This furthers the arguments of the “the web as corpus” community (Kilgarriff and Grefenstette, 2003) with respect to psycholinguistics. Finally, combining multiple sources of frequency estimates is one way researchers may be able to reduce the prediction bias from any single corpus. This relates to work in automatically building domain specific corpora (e.g., Moore and Lewis (2010), Axelrod et al. (2011), Daum´e III and Jagarlamudi (2011), Wang et al. (2014), Gao et al. (2002), and Lin et al. (1997)). Those efforts focus on building representative document collections for a target domain, usually based on a seed set of initial documents. Our results prompt the question: can one use human behavior as the target in the construction of such a corpus? Concretely, can we build corpora by optimizing an objective measure that minimizes error in predicting human reaction times? Prior work in building balanced corpora used either rough estimates of the ratio of genre styles a normal human is exposed to daily (e.g., the Brown corpus (Kucera and Francis, 1967)), or simply sampled text evenly across</context>
</contexts>
<marker>Lin, Tsai, Chien, Chen, Lee, 1997</marker>
<rawString>S. C. Lin, C. L. Tsai, L. F. Chien, K. J. Chen, and L. S. Lee. 1997. Chinese language model adaptation based on document classification and multiple domain-specific language models. In Proceedings of the 5th European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A McDonald</author>
<author>R C Shillcock</author>
</authors>
<title>Eye movements reveal the on-line computation of lexical probabilities during reading.</title>
<date>2003</date>
<journal>Psychological science,</journal>
<volume>14</volume>
<issue>6</issue>
<contexts>
<context position="2015" citStr="McDonald and Shillcock, 2003" startWordPosition="301" endWordPosition="304">s are defined as failures to model a target phenomenon. In the current study, we exploit errors of the latter variety—failure of a language model to predict human performance—to investigate bias across several frequently used corpora in computational linguistics. The human data is revealing because it trades on the fact that human language processing is probability-sensitive: language processing reflects implicit knowledge of probabilities computed over linguistic units (e.g., words). For example, the amount of time required to read a word varies as a function of how predictable that word is (McDonald and Shillcock, 2003). Thus, failure of a language model to predict human performance reveals a mismatch between the language model and the human language model, i.e., bias. Psycholinguists have known for some time that the ability of a corpus to explain behavior depends on properties of the corpus and the subjects (cf. Balota et al. (2004)). We extend that line of work by directly analyzing and quantifying this bias, and by linking the results to methodological concerns in both NLP and psycholinguistics. Specifically, we predict human data from three widely used psycholinguistic experimental paradigms—lexical dec</context>
</contexts>
<marker>McDonald, Shillcock, 2003</marker>
<rawString>S.A. McDonald and R.C. Shillcock. 2003. Eye movements reveal the on-line computation of lexical probabilities during reading. Psychological science, 14(6):648–52, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>W Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 10).</booktitle>
<contexts>
<context position="17109" citStr="Moore and Lewis (2010)" startWordPosition="2766" endWordPosition="2769"> full, stuff, lot, last, well, guy, fair Table 2: Examples of words with largest difference in z-transformed log frequencies (e.g., the relative frequencies of fife, lord, and duke, in the BNC are far greater than in Google). languages). This furthers the arguments of the “the web as corpus” community (Kilgarriff and Grefenstette, 2003) with respect to psycholinguistics. Finally, combining multiple sources of frequency estimates is one way researchers may be able to reduce the prediction bias from any single corpus. This relates to work in automatically building domain specific corpora (e.g., Moore and Lewis (2010), Axelrod et al. (2011), Daum´e III and Jagarlamudi (2011), Wang et al. (2014), Gao et al. (2002), and Lin et al. (1997)). Those efforts focus on building representative document collections for a target domain, usually based on a seed set of initial documents. Our results prompt the question: can one use human behavior as the target in the construction of such a corpus? Concretely, can we build corpora by optimizing an objective measure that minimizes error in predicting human reaction times? Prior work in building balanced corpora used either rough estimates of the ratio of genre styles a no</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>R. C. Moore and W. Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ramscar</author>
<author>P Hendrix</author>
<author>C Shaoul</author>
<author>P Milin</author>
<author>R H Baayen</author>
</authors>
<title>The myth of cognitive decline: nonlinear dynamics of lifelong learning. Topics in Cognitive Science,</title>
<date>2014</date>
<pages>32--5</pages>
<contexts>
<context position="6502" citStr="Ramscar et al., 2014" startWordPosition="989" endWordPosition="992">nd derivational morphological family members), (4) number of synonyms, and (5) the first principal component of a host of orthographic and phonological features capturing neighborhood effects (type and token counts of orthographic and phonological neighbors as well as forward and backward inconsistent words; (Baayen et al., 2006)). Models of lexical decision and word naming included random intercepts of participant age to adjust for differences in mean RTs between old (mean age = 72) vs. young (mean age = 23) subjects, given differences between younger vs. older adults’ processing speed (cf. (Ramscar et al., 2014)). (All participants in the picture naming study were college students.) 2.3 Results For each of the six panels corresponding to frequency estimates from a corpus A, Figure 2 gives the χ2 value resulting from the log-likelihood ratio of (1) a model containing A and an estimate from one of the five remaining corpora (given on the x axis) and (2) a model containing just the corpus indicated on the x axis. Thus, for each panel, each bar in Figure 2 shows the explanatory power of estimates from the corpus given at the top of the panel after controlling for estimates from each of the other corpora.</context>
</contexts>
<marker>Ramscar, Hendrix, Shaoul, Milin, Baayen, 2014</marker>
<rawString>M. Ramscar, P. Hendrix, C. Shaoul, P. Milin, and R. H. Baayen. 2014. The myth of cognitive decline: nonlinear dynamics of lifelong learning. Topics in Cognitive Science, 32:5–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Spieler</author>
<author>D A Balota</author>
</authors>
<title>Bringing computational models of word naming down to the item level.</title>
<date>1997</date>
<pages>6--411</pages>
<contexts>
<context position="5218" citStr="Spieler and Balota, 1997" startWordPosition="783" endWordPosition="786">oard (Penn Treebank subset 3) — 800,000 Table 1: Summary of the corpora under consideration. 2.2 Approach We ask whether domain biases manifest as systematic errors in predicting human behavior. Log unigram frequency estimates were derived from each corpus and used to predict reaction times (RTs) from three experiments employing lexical 1BNCw and BNCs are both British, while BNCs and SWBD are both spoken. decision (time required by subjects to correctly identify a string of letters as a word of English (Balota et al., 1999)); word naming (time required to read aloud a visually presented word (Spieler and Balota, 1997); (Balota and Spieler, 1998)); and picture naming (time required to say a picture’s name (Bates et al., 2003)). Previous work has shown that more frequent words lead to faster RTs. These three measures provide a strong test for the biases present in these corpora, as they span written and spoken lexical comprehension and production. To compare the predictive strength of log frequency estimates from each corpus, we fit mixed effects regression models to the data from each experiment. As controls, all models included (1) mean log bigram frequency for each word, (2) word category (noun, verb, etc</context>
</contexts>
<marker>Spieler, Balota, 1997</marker>
<rawString>D. H. Spieler and D. A. Balota. 1997. Bringing computational models of word naming down to the item level. 6:411–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wang</author>
<author>D F Wong</author>
<author>L S Chao</author>
<author>Y Lu</author>
<author>J Xing</author>
</authors>
<title>A systematic comparison of data selection criteria for smt domain adaptation. The Scientific World Journal.</title>
<date>2014</date>
<contexts>
<context position="17187" citStr="Wang et al. (2014)" startWordPosition="2779" endWordPosition="2782">ifference in z-transformed log frequencies (e.g., the relative frequencies of fife, lord, and duke, in the BNC are far greater than in Google). languages). This furthers the arguments of the “the web as corpus” community (Kilgarriff and Grefenstette, 2003) with respect to psycholinguistics. Finally, combining multiple sources of frequency estimates is one way researchers may be able to reduce the prediction bias from any single corpus. This relates to work in automatically building domain specific corpora (e.g., Moore and Lewis (2010), Axelrod et al. (2011), Daum´e III and Jagarlamudi (2011), Wang et al. (2014), Gao et al. (2002), and Lin et al. (1997)). Those efforts focus on building representative document collections for a target domain, usually based on a seed set of initial documents. Our results prompt the question: can one use human behavior as the target in the construction of such a corpus? Concretely, can we build corpora by optimizing an objective measure that minimizes error in predicting human reaction times? Prior work in building balanced corpora used either rough estimates of the ratio of genre styles a normal human is exposed to daily (e.g., the Brown corpus (Kucera and Francis, 19</context>
</contexts>
<marker>Wang, Wong, Chao, Lu, Xing, 2014</marker>
<rawString>L. Wang, D.F. Wong, L.S. Chao, Y. Lu, and J. Xing. 2014. A systematic comparison of data selection criteria for smt domain adaptation. The Scientific World Journal.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>