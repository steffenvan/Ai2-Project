<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000129">
<title confidence="0.9905135">
WebAnno: A Flexible, Web-based and Visually Supported
System for Distributed Annotations
</title>
<author confidence="0.779118">
Seid Muhie Yimam1,3 Iryna Gurevych2,3 Richard Eckart de Castilho2 Chris Biemann1
</author>
<affiliation confidence="0.5829688">
(1) FG Language Technology, Dept. of Computer Science, Technische Universit¨at Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universit¨at Darmstadt
(3) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
</affiliation>
<email confidence="0.873386">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.990163" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998234">
We present WebAnno, a general pur-
pose web-based annotation tool for a wide
range of linguistic annotations. Web-
Anno offers annotation project manage-
ment, freely configurable tagsets and the
management of users in different roles.
WebAnno uses modern web technology
for visualizing and editing annotations in
a web browser. It supports arbitrarily
large documents, pluggable import/export
filters, the curation of annotations across
various users, and an interface to farming
out annotations to a crowdsourcing plat-
form. Currently WebAnno allows part-of-
speech, named entity, dependency parsing
and co-reference chain annotations. The
architecture design allows adding addi-
tional modes of visualization and editing,
when new kinds of annotations are to be
supported.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999768">
The creation of training data precedes any sta-
tistical approach to natural language processing
(NLP). Linguistic annotation is a process whereby
linguistic information is added to a document,
such as part-of-speech, lemmata, named entities,
or dependency relations. In the past, platforms
for linguistic annotations were mostly developed
ad-hoc for the given annotation task at hand, used
proprietary formats for data exchange, or required
local installation effort. We present WebAnno, a
browser-based tool that is immediately usable by
any annotator with internet access. It supports an-
notation on a variety of linguistic levels (called an-
notation layers in the remainder), is interoperable
with a variety of data formats, supports annotation
project management such as user management, of-
fers an adjudication interface, and provides qual-
ity management using inter-annotator agreement.
Furthermore, an interface to crowdsourcing plat-
forms enables scaling out simple annotation tasks
to a large numbers of micro-workers. The added
value of WebAnno, as compared to previous an-
notation tools, is on the one hand its web-based
interface targeted at skilled as well as unskilled
annotators, which unlocks a potentially very large
workforce. On the other hand, it is the support for
quality control, annotator management, and adju-
dication/curation, which lowers the entrance bar-
rier for new annotation projects. We created Web-
Anno to fulfill the following requirements:
</bodyText>
<listItem confidence="0.998299789473684">
• Web-based: Distributed work, no installation
effort, increased availability.
• Interface to crowdsourcing: unlocking a very
large distributed workforce.
• Quality and user management: Integrated
different user roles support (administra-
tor, annotator, and curator), inter-annotator
agreement measurement, data curation, and
progress monitoring.
• Flexibility: Support of multiple annotation
layers, pluggable import and export formats,
and extensibility to other front ends.
• Pre-annotated and un-annotated documents:
supporting new annotations, as well as man-
ual corrections of existing, possibly auto-
matic annotations.
• Permissive open source: Usability of our tool
in future projects without restrictions, under
the Apache 2.0 license.
</listItem>
<bodyText confidence="0.9931458">
In the following section, we revisit related work
on annotation tools, which only partially fulfill the
aforementioned requirements. In Section 3, the ar-
chitecture as well as usage aspects of our tool are
lined out. The scope and functionality summary
</bodyText>
<page confidence="0.797119">
1
</page>
<bodyText confidence="0.7662855">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1–6,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
of WebAnno is presented in Section 4. Section 5
elaborates on several use cases of WebAnno, and
Section 6 concludes and gives an outlook to fur-
ther directions.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999731634146341">
GATE Teamware (Bontcheva et al., 2010) is prob-
ably the tool that closely matches our requirements
regarding quality management, annotator manage-
ment, and support of a large set of annotation lay-
ers and formats. It is mostly web-based, but the
annotation is carried out with locally downloaded
software. An interface to crowdsourcing platforms
is missing. The GATE Teamware system is heav-
ily targeted towards template-based information
extraction. It sets a focus on the integration of au-
tomatic annotation components rather than on the
interface for manual annotation. Besides, the over-
all application is rather complex for average users,
requires considerable training and does not offer
an alternative simplified interface as it would be
required for crowdsourcing.
General-purpose annotation tools like MMAX2
(M¨uller and Strube, 2006) or WordFreak (Morton
and LaCivita, 2003) are not web-based and do not
provide annotation project management. They are
also not sufficiently flexible regarding different an-
notation layers. The same holds for specialized
tools for single annotation layers, which we can-
not list here for the sake of brevity.
With the brat rapid annotation tool (Stenetorp
et al., 2012), for the first time a web-based open-
source annotation tool was introduced, which sup-
ports collaborative annotation for multiple anno-
tation layers simultaneously on a single copy of
the document, and is based on a client-server ar-
chitecture. However, the current version of brat
has limitations such as: (i) slowness for docu-
ments of more than 100 sentences, (ii) limits re-
garding file formats, (iii) web-based configuration
of tagsets/tags is not possible and (iv) configuring
the display of multiple layers is not yet supported.
While we use brat’s excellent visualization front
end in WebAnno, we decided to replace the server
layer to support the user and quality management,
and monitoring tools as well as to add the interface
to crowdsourcing.
</bodyText>
<sectionHeader confidence="0.963458" genericHeader="method">
3 System Architecture of WebAnno
</sectionHeader>
<bodyText confidence="0.768277">
The overall architecture of WebAnno is depicted
in Figure 1. The modularity of the architecture,
</bodyText>
<figureCaption confidence="0.9804435">
Figure 1: System architecture, organized in user,
front end, back end and persistent data storage.
</figureCaption>
<bodyText confidence="0.996911090909091">
which is mirrored in its open-source implementa-
tion1, makes it possible to easily extend the tool or
add alternative user interfaces for annotation lay-
ers that brat is less suited for, e.g. for constituent
structure. In Section 3.1, we illustrate how differ-
ent user roles are provided with different graphical
user interfaces, and show the expressiveness of the
annotation model. Section 3.2 elaborates on the
functionality of the back end, and describes how
data is imported and exported, as well as our im-
plementation of the persistent data storage.
</bodyText>
<subsectionHeader confidence="0.999139">
3.1 Front End
</subsectionHeader>
<bodyText confidence="0.999981285714286">
All functionality of WebAnno is accessible via
a web browser. For annotation and visualiza-
tion of annotated documents, we adapted the brat
rapid annotation tool. Changes had to be made to
make brat interoperate with the Apache Wicket,
on which WebAnno is built, and to better integrate
into the WebAnno experience.
</bodyText>
<subsectionHeader confidence="0.769868">
3.1.1 Project Definition
</subsectionHeader>
<bodyText confidence="0.9999355">
The definition and the monitoring of an annota-
tion project is conducted by a project manager (cf.
Figure 1) in a project definition form. It supports
creating a project, loading un-annotated or pre-
annotated documents in different formats2, adding
annotator and curator users, defining tagsets, and
configuring the annotation layers. Only a project
manager can administer a project. Figure 2 illus-
trates the project definition page with the tagset
editor highlighted.
</bodyText>
<footnote confidence="0.9266325">
1Available for download at (this paper is based on v0.3.0):
webanno.googlecode.com/
2Formats: plain text, CoNLL (Nivre et al., 2007), TCF
(Heid et al., 2010), UIMA XMI (Ferrucci and Lally, 2004)
</footnote>
<page confidence="0.992935">
2
</page>
<figureCaption confidence="0.999445">
Figure 2: The tagset editor on the project definition page
</figureCaption>
<subsectionHeader confidence="0.910335">
3.1.2 Annotation
</subsectionHeader>
<bodyText confidence="0.999069142857143">
Annotation is carried out with an adapted ver-
sion of the brat editor, which communicates with
the server via Ajax (Wang et al., 2008) using the
JSON (Lin et al., 2012) format. Annotators only
see projects they are assigned to. The annotation
page presents the annotator different options to set
up the annotation environment, for customization:
</bodyText>
<listItem confidence="0.996507842105263">
• Paging: For heavily annotated documents or
very large documents, the original brat vi-
sualization is very slow, both for displaying
and annotating the document. We use a pag-
ing mechanism that limits the number of sen-
tences displayed at a time to make the perfor-
mance independent of the document size.
• Annotation layers: Annotators usually work
on one or two annotations layers, such as
part-of-speech and dependency or named en-
tity annotation. Overloading the annota-
tion page by displaying all annotation layers
makes the annotation and visualization pro-
cess slower. WebAnno provides an option to
configure visible/editable annotation layers.
• Immediate persistence: Every annotation is
sent to the back end immediately and per-
sisted there. An explicit interaction by the
user to save changes is not required.
</listItem>
<subsectionHeader confidence="0.577574">
3.1.3 Workflow
</subsectionHeader>
<bodyText confidence="0.999785818181818">
WebAnno implements a simple workflow to track
the state of a project. Every annotator works on a
separate version of the document, which is set to
the state in progress the first time a document is
opened by the annotator. The annotator can then
mark it as complete at the end of annotation at
which point it is locked for further annotation and
can be used for curation. Such a document cannot
be changed anymore by an annotator, but can be
used by a curator. A curator can mark a document
as adjudicated.
</bodyText>
<subsectionHeader confidence="0.690087">
3.1.4 Curation
</subsectionHeader>
<bodyText confidence="0.9999785">
The curation interface allows the curator to open a
document and compare annotations made by the
annotators that already marked the document as
complete. The curator reconciles the annotation
with disagreements. The curator can either decide
on one of the presented alternatives, or freely re-
annotate. Figure 3 illustrates how the curation in-
terface detects sentences with annotation disagree-
ment (left side of Figure 3) which can be used to
navigate to the sentences for curation.
</bodyText>
<subsectionHeader confidence="0.913627">
3.1.5 Monitoring
</subsectionHeader>
<bodyText confidence="0.9999763">
WebAnno provides a monitoring component, to
track the progress of a project. The project man-
ager can check the progress and compute agree-
ment with Kappa and Tau (Carletta, 1996) mea-
sures. The progress is visualized using a matrix of
annotators and documents displaying which docu-
ments the annotators have marked as complete and
which documents the curator adjudicated. Fig-
ure 4 shows the project progress, progress of in-
dividual annotator and completion statistics.
</bodyText>
<page confidence="0.997721">
3
</page>
<figureCaption confidence="0.999407">
Figure 3: Curation user interface (left: sentences
</figureCaption>
<bodyText confidence="0.820901">
with disagreement; right: merging editor) Figure 4: Project monitoring
</bodyText>
<subsectionHeader confidence="0.882966">
3.1.6 Crowdsourcing
</subsectionHeader>
<bodyText confidence="0.99997784">
Crowdsourcing is a way to quickly scale annota-
tion projects. Distributing a task that otherwise
will be performed by a controlled user group has
become much easier. Hence, if quality can be en-
sured, it is an alternative to high quality annotation
using a large number of arbitrary redundant anno-
tations (Wang et al., 2013). For WebAnno, we
have designed an approach where a source doc-
ument is split into small parts that get presented
to micro-workers in the CrowdFlower platform3.
The crowdsourcing component is a separate mod-
ule that handles the communication via Crowd-
Flower’s API, the definition of test items and job
parameters, and the aggregation of results. The
crowdsourced annotation appears as a virtual an-
notator in the tool.
Since it is not trivial to express complex anno-
tation tasks in comparably simple templates suit-
able for crowdsourcing (Biemann, 2013), we pro-
ceed by working out crowdsourcing templates and
strategies per annotation layer. We currently only
support named entity annotation with predefined
templates. However, the open and modular archi-
tecture allows to add more crowdsourced annota-
tion layers.
</bodyText>
<subsectionHeader confidence="0.999538">
3.2 Back End
</subsectionHeader>
<bodyText confidence="0.984832333333333">
WebAnno is a Java-based web application that
may run on any modern servlet container. In mem-
ory and on the file system, annotations are stored
</bodyText>
<footnote confidence="0.891455">
3www.crowdflower.com
</footnote>
<bodyText confidence="0.9927075">
as UIMA CAS objects (Ferrucci and Lally, 2004).
All other data is persisted in an SQL database.
</bodyText>
<subsectionHeader confidence="0.640618">
3.2.1 Data Conversion
</subsectionHeader>
<bodyText confidence="0.9999775">
WebAnno supports different data models that re-
flect the different communication of data between
the front end, back end, and the persistent data
storage. The brat data model serves exchanging
data between the front end and the back end.
The documents are stored in their original for-
mats. For annotations, we use the type system
from the DKPro Core collection of UIMA compo-
nents (Eckart de Castilho and Gurevych, 2009)4.
This is converted to the brat model for visualiza-
tion. Importing documents and exporting anno-
tations is implemented using UIMA reader and
writer components from DKPro Core as plug-ins.
Thus, support for new formats can easily be added.
To provide quick reaction times in the user inter-
face, WebAnno internally stores annotations in a
binary format, using the SerializedCasReader and
SerializedCasWriter components.
</bodyText>
<subsectionHeader confidence="0.947175">
3.2.2 Persistent Data Storage
</subsectionHeader>
<bodyText confidence="0.999930714285714">
Project definitions including project name and de-
scriptions, tagsets and tags, and user details are
kept in a database, whereas the documents and an-
notations are stored in the file system. WebAnno
supports limited versioning of annotations, to pro-
tect against the unforeseen loss of data. Figure 5
shows the database entity relation diagram.
</bodyText>
<footnote confidence="0.472525">
4code.google.com/p/dkpro-core-asl/
</footnote>
<page confidence="0.991652">
4
</page>
<figureCaption confidence="0.999291">
Figure 6: Parts-of-speech &amp; dependency relations
Figure 5: WebAnno database scheme Figure 7: Co-reference &amp; named entites
</figureCaption>
<sectionHeader confidence="0.92575" genericHeader="method">
4 Scope and Functionality Summary
</sectionHeader>
<bodyText confidence="0.9939952">
WebAnno supports the production of linguistically
annotated corpora for different natural language
processing applications. WebAnno implements
ease of usage and simplicity for untrained users,
and provides:
</bodyText>
<listItem confidence="0.999933">
• Annotation via a fast, and easy-to-use web-
based user interface.
• Project and user management.
• Progress and quality monitoring.
• Interactive curation by adjudicating disagree-
ing annotations from multiple users.
• Crowdsourcing of annotation tasks.
• Configurable annotation types and tag sets.
</listItem>
<sectionHeader confidence="0.953979" genericHeader="method">
5 Use Cases
</sectionHeader>
<bodyText confidence="0.9987085">
WebAnno currently allows to configure different
span and arc annotations. It comes pre-configured
with the following annotation layers from the
DKPro Core type system:
</bodyText>
<subsectionHeader confidence="0.435402">
Span annotations
</subsectionHeader>
<listItem confidence="0.9602005625">
• Part-of-Speech (POS) tags: an annotation
task on tokens. Currently, POS can be added
to a token, if not already present, and can be
modified. POS annotation is a prerequisite of
dependency annotation (Figure 6).
• Named entities: a multiple span annotation
task. Spans can cover multiple adjacent to-
kens, nest and overlap (Figure 7), but cannot
cross sentence boundaries.
Arc Annotations
• Dependency relations: This is an arc annota-
tion which connects two POS tag annotations
with a directed relation (Figure 6).
• Co-reference chains: The co-reference chain
is realized as a set of typed mention spans
linked by typed co-reference relation arcs.
</listItem>
<bodyText confidence="0.990272954545455">
The co-reference relation annotation can
cross multiple sentences and is represented in
co-reference chains (Figure 7).
The brat front end supports tokens and sub-
tokens as a span annotation. However, tokens are
currently the minimal annotation units in Web-
Anno, due to a requirement of supporting the TCF
file format (Heid et al., 2010). Part-of-speech an-
notation is limited to singles token, while named
entity and co-reference chain annotations may
span multiple tokens. Dependency relations are
implemented in such a way that the arc is drawn
from the governor to the dependent (or the other
way around, configurable), while co-reference
chains are unidirectional and a chain is formed by
referents that are transitively connected by arcs.
Based on common practice in manual annota-
tion, every user works on their own copy of the
same document so that no concurrent editing oc-
curs. We also found that displaying all annotation
layers at the same time is inconvenient for anno-
tators. This is why WebAnno supports showing
</bodyText>
<page confidence="0.969259">
5
</page>
<bodyText confidence="0.999938625">
and hiding of individual annotation layers. The
WebAnno curation component displays all anno-
tation documents from all users for a given source
document, enabling the curator to visualize all of
the annotations with differences at a time. Unlike
most of the annotation tools which rely on config-
uration files, WebAnno enables to freely configure
all parameters directly in the browser.
</bodyText>
<sectionHeader confidence="0.994241" genericHeader="conclusions">
6 Conclusion and Outlook
</sectionHeader>
<bodyText confidence="0.99993225">
WebAnno is a new web-based linguistic annota-
tion tool. The brat annotation and GUI front end
have been enhanced to support rapidly process-
ing large annotation documents, configuring the
annotation tag and tagsets in the browser, speci-
fying visible annotation layers, separating anno-
tation documents per user, just to name the most
important distinctions. Besides, WebAnno sup-
ports project definition, import/export of tag and
tagsets. Flexible support for importing and ex-
porting different data formats is handled through
UIMA components from the DKPro Core project.
The monitoring component of WebAnno helps the
administrator to control the progress of annotators.
The crowdsourcing component of WebAnno pro-
vides a unique functionality to distribute the an-
notation to a large workforce and automatically
integrate the results back into the tool via the
crowdsourcing server. The WebAnno annotation
tool supports curation of different annotation doc-
uments, displaying annotation documents created
by users in a given project with annotation dis-
agreements. In future work, WebAnno will be en-
hanced to support several other front ends to han-
dle even more annotation layers, and to provide
more crowdsourcing templates. Another planned
extension is a more seamless integration of lan-
guage processing tools for pre-annotation.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995467888888889">
We would like to thank Benjamin Milde and Andreas
Straninger, who assisted in implementing WebAnno, as well
as Marc Reznicek, Nils Reiter and the whole CLARIN-D F-
AG 7 for testing and providing valuable feedback. The work
presented in this paper was funded by a German BMBF grant
to the CLARIN-D project, the Hessian LOEWE research ex-
cellence program as part of the research center “Digital Hu-
manities” and by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant No. I/82806.
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999926616666666">
Chris Biemann. 2013. Creating a system for lexical substi-
tutions from scratch using crowdsourcing. Lang. Resour.
Eval., 47(1):97–122, March.
Kalina Bontcheva, Hamish Cunningham, Ian Roberts, and
Valentin Tablan. 2010. Web-based collaborative corpus
annotation: Requirements and a framework implementa-
tion. In New Challenges for NLP Frameworks workshop
at LREC-2010, Malta.
Jean Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. In Computational Linguistics,
Volume 22 Issue 2, pages 249–254.
Richard Eckart de Castilho and Iryna Gurevych. 2009.
DKPro-UGD: A Flexible Data-Cleansing Approach to
Processing User-Generated Discourse. In Online-
proceedings of the First French-speaking meeting around
the framework Apache UIMA, LINA CNRS UMR 6241 -
University of Nantes, France.
David Ferrucci and Adam Lally. 2004. UIMA: An Architec-
tural Approach to Unstructured Information Processing in
the Corporate Research Environment. In Journal of Natu-
ral Language Engineering 2004, pages 327–348.
Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Erhard
Hinrichs. 2010. A Corpus Representation Format for
Linguistic Web Services: the D-SPIN Text Corpus Format
and its Relationship with ISO Standards. In Proceedings
of LREC 2010, Malta.
Boci Lin, Yan Chen, Xu Chen, and Yingying Yu. 2012.
Comparison between JSON and XML in Applications
Based on AJAX. In Computer Science &amp; Service System
(CSSS), 2012, Nanjing, China.
Thomas Morton and Jeremy LaCivita. 2003. WordFreak:
an open tool for linguistic annotation. In Proceedings of
NAACL-2003, NAACL-Demonstrations ’03, pages 17–18,
Edmonton, Canada.
Christoph M¨uller and Michael Strube. 2006. Multi-level an-
notation of linguistic data with MMAX2. In S. Braun,
K. Kohn, and J. Mukherjee, editors, Corpus Technology
and Language Pedagogy: New Resources, New Tools,
New Methods, pages 197–214. Peter Lang, Frankfurt a.M.,
Germany.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 Shared Task on Dependency Parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915–932, Prague, Czech Re-
public.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c, Tomoko
Ohta, Sophia Ananiadou, and Jun’ichi Tsujii. 2012. brat:
a Web-based Tool for NLP-Assisted Text Annotation. In
Proceedings of the Demonstrations at EACL-2012, Avi-
gnon, France.
Qingling Wang, Qin Liu, Na Li, and Yan Liu. 2008. An
Automatic Approach to Reengineering Common Website
with AJAX. In 4th International Conference on Next Gen-
eration Web Services Practices, pages 185–190, Seoul,
South Korea.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2013.
Perspectives on Crowdsourcing Annotations for Natural
Language Processing. In Language Resources And Eval-
uation, pages 9–31. Springer Netherlands.
</reference>
<page confidence="0.998786">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818249">
<title confidence="0.997495">WebAnno: A Flexible, Web-based and Visually System for Distributed Annotations</title>
<author confidence="0.918011">Muhie Iryna Richard Eckart de_Chris</author>
<affiliation confidence="0.9861786">(1) FG Language Technology, Dept. of Computer Science, Technische Universit¨at Darmstadt (2) Ubiquitous Knowledge Processing Lab Dept. of Computer Science, Technische Universit¨at Darmstadt (3) Ubiquitous Knowledge Processing Lab German Institute for Educational Research and Educational</affiliation>
<web confidence="0.97899">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.997731714285714">We present WebAnno, a general purpose web-based annotation tool for a wide range of linguistic annotations. Web- Anno offers annotation project management, freely configurable tagsets and the management of users in different roles. WebAnno uses modern web technology for visualizing and editing annotations in a web browser. It supports arbitrarily large documents, pluggable import/export filters, the curation of annotations across various users, and an interface to farming out annotations to a crowdsourcing platform. Currently WebAnno allows part-ofspeech, named entity, dependency parsing and co-reference chain annotations. The architecture design allows adding additional modes of visualization and editing, when new kinds of annotations are to be supported.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Creating a system for lexical substitutions from scratch using crowdsourcing.</title>
<date>2013</date>
<journal>Lang. Resour. Eval.,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="11639" citStr="Biemann, 2013" startWordPosition="1777" endWordPosition="1778">rge number of arbitrary redundant annotations (Wang et al., 2013). For WebAnno, we have designed an approach where a source document is split into small parts that get presented to micro-workers in the CrowdFlower platform3. The crowdsourcing component is a separate module that handles the communication via CrowdFlower’s API, the definition of test items and job parameters, and the aggregation of results. The crowdsourced annotation appears as a virtual annotator in the tool. Since it is not trivial to express complex annotation tasks in comparably simple templates suitable for crowdsourcing (Biemann, 2013), we proceed by working out crowdsourcing templates and strategies per annotation layer. We currently only support named entity annotation with predefined templates. However, the open and modular architecture allows to add more crowdsourced annotation layers. 3.2 Back End WebAnno is a Java-based web application that may run on any modern servlet container. In memory and on the file system, annotations are stored 3www.crowdflower.com as UIMA CAS objects (Ferrucci and Lally, 2004). All other data is persisted in an SQL database. 3.2.1 Data Conversion WebAnno supports different data models that r</context>
</contexts>
<marker>Biemann, 2013</marker>
<rawString>Chris Biemann. 2013. Creating a system for lexical substitutions from scratch using crowdsourcing. Lang. Resour. Eval., 47(1):97–122, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalina Bontcheva</author>
<author>Hamish Cunningham</author>
<author>Ian Roberts</author>
<author>Valentin Tablan</author>
</authors>
<title>Web-based collaborative corpus annotation: Requirements and a framework implementation.</title>
<date>2010</date>
<booktitle>In New Challenges for NLP Frameworks workshop at LREC-2010,</booktitle>
<contexts>
<context position="4170" citStr="Bontcheva et al., 2010" startWordPosition="587" endWordPosition="590">e revisit related work on annotation tools, which only partially fulfill the aforementioned requirements. In Section 3, the architecture as well as usage aspects of our tool are lined out. The scope and functionality summary 1 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1–6, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics of WebAnno is presented in Section 4. Section 5 elaborates on several use cases of WebAnno, and Section 6 concludes and gives an outlook to further directions. 2 Related Work GATE Teamware (Bontcheva et al., 2010) is probably the tool that closely matches our requirements regarding quality management, annotator management, and support of a large set of annotation layers and formats. It is mostly web-based, but the annotation is carried out with locally downloaded software. An interface to crowdsourcing platforms is missing. The GATE Teamware system is heavily targeted towards template-based information extraction. It sets a focus on the integration of automatic annotation components rather than on the interface for manual annotation. Besides, the overall application is rather complex for average users,</context>
</contexts>
<marker>Bontcheva, Cunningham, Roberts, Tablan, 2010</marker>
<rawString>Kalina Bontcheva, Hamish Cunningham, Ian Roberts, and Valentin Tablan. 2010. Web-based collaborative corpus annotation: Requirements and a framework implementation. In New Challenges for NLP Frameworks workshop at LREC-2010, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>In Computational Linguistics, Volume</journal>
<volume>22</volume>
<pages>249--254</pages>
<contexts>
<context position="10335" citStr="Carletta, 1996" startWordPosition="1571" endWordPosition="1572">and compare annotations made by the annotators that already marked the document as complete. The curator reconciles the annotation with disagreements. The curator can either decide on one of the presented alternatives, or freely reannotate. Figure 3 illustrates how the curation interface detects sentences with annotation disagreement (left side of Figure 3) which can be used to navigate to the sentences for curation. 3.1.5 Monitoring WebAnno provides a monitoring component, to track the progress of a project. The project manager can check the progress and compute agreement with Kappa and Tau (Carletta, 1996) measures. The progress is visualized using a matrix of annotators and documents displaying which documents the annotators have marked as complete and which documents the curator adjudicated. Figure 4 shows the project progress, progress of individual annotator and completion statistics. 3 Figure 3: Curation user interface (left: sentences with disagreement; right: merging editor) Figure 4: Project monitoring 3.1.6 Crowdsourcing Crowdsourcing is a way to quickly scale annotation projects. Distributing a task that otherwise will be performed by a controlled user group has become much easier. He</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. In Computational Linguistics, Volume 22 Issue 2, pages 249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Eckart de Castilho</author>
<author>Iryna Gurevych</author>
</authors>
<title>DKPro-UGD: A Flexible Data-Cleansing Approach to Processing User-Generated Discourse.</title>
<date>2009</date>
<booktitle>In Onlineproceedings of the First French-speaking meeting around the framework Apache UIMA, LINA CNRS UMR 6241 -University of Nantes,</booktitle>
<marker>de Castilho, Gurevych, 2009</marker>
<rawString>Richard Eckart de Castilho and Iryna Gurevych. 2009. DKPro-UGD: A Flexible Data-Cleansing Approach to Processing User-Generated Discourse. In Onlineproceedings of the First French-speaking meeting around the framework Apache UIMA, LINA CNRS UMR 6241 -University of Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment.</title>
<date>2004</date>
<journal>In Journal of Natural Language Engineering</journal>
<pages>327--348</pages>
<contexts>
<context position="7884" citStr="Ferrucci and Lally, 2004" startWordPosition="1166" endWordPosition="1169"> an annotation project is conducted by a project manager (cf. Figure 1) in a project definition form. It supports creating a project, loading un-annotated or preannotated documents in different formats2, adding annotator and curator users, defining tagsets, and configuring the annotation layers. Only a project manager can administer a project. Figure 2 illustrates the project definition page with the tagset editor highlighted. 1Available for download at (this paper is based on v0.3.0): webanno.googlecode.com/ 2Formats: plain text, CoNLL (Nivre et al., 2007), TCF (Heid et al., 2010), UIMA XMI (Ferrucci and Lally, 2004) 2 Figure 2: The tagset editor on the project definition page 3.1.2 Annotation Annotation is carried out with an adapted version of the brat editor, which communicates with the server via Ajax (Wang et al., 2008) using the JSON (Lin et al., 2012) format. Annotators only see projects they are assigned to. The annotation page presents the annotator different options to set up the annotation environment, for customization: • Paging: For heavily annotated documents or very large documents, the original brat visualization is very slow, both for displaying and annotating the document. We use a pagin</context>
<context position="12122" citStr="Ferrucci and Lally, 2004" startWordPosition="1850" endWordPosition="1853">he tool. Since it is not trivial to express complex annotation tasks in comparably simple templates suitable for crowdsourcing (Biemann, 2013), we proceed by working out crowdsourcing templates and strategies per annotation layer. We currently only support named entity annotation with predefined templates. However, the open and modular architecture allows to add more crowdsourced annotation layers. 3.2 Back End WebAnno is a Java-based web application that may run on any modern servlet container. In memory and on the file system, annotations are stored 3www.crowdflower.com as UIMA CAS objects (Ferrucci and Lally, 2004). All other data is persisted in an SQL database. 3.2.1 Data Conversion WebAnno supports different data models that reflect the different communication of data between the front end, back end, and the persistent data storage. The brat data model serves exchanging data between the front end and the back end. The documents are stored in their original formats. For annotations, we use the type system from the DKPro Core collection of UIMA components (Eckart de Castilho and Gurevych, 2009)4. This is converted to the brat model for visualization. Importing documents and exporting annotations is imp</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment. In Journal of Natural Language Engineering 2004, pages 327–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Heid</author>
<author>Helmut Schmid</author>
<author>Kerstin Eckart</author>
<author>Erhard Hinrichs</author>
</authors>
<title>A Corpus Representation Format for Linguistic Web Services: the D-SPIN Text Corpus Format and its Relationship with ISO Standards.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="7847" citStr="Heid et al., 2010" startWordPosition="1160" endWordPosition="1163">finition and the monitoring of an annotation project is conducted by a project manager (cf. Figure 1) in a project definition form. It supports creating a project, loading un-annotated or preannotated documents in different formats2, adding annotator and curator users, defining tagsets, and configuring the annotation layers. Only a project manager can administer a project. Figure 2 illustrates the project definition page with the tagset editor highlighted. 1Available for download at (this paper is based on v0.3.0): webanno.googlecode.com/ 2Formats: plain text, CoNLL (Nivre et al., 2007), TCF (Heid et al., 2010), UIMA XMI (Ferrucci and Lally, 2004) 2 Figure 2: The tagset editor on the project definition page 3.1.2 Annotation Annotation is carried out with an adapted version of the brat editor, which communicates with the server via Ajax (Wang et al., 2008) using the JSON (Lin et al., 2012) format. Annotators only see projects they are assigned to. The annotation page presents the annotator different options to set up the annotation environment, for customization: • Paging: For heavily annotated documents or very large documents, the original brat visualization is very slow, both for displaying and an</context>
<context position="15284" citStr="Heid et al., 2010" startWordPosition="2333" endWordPosition="2336">ries. Arc Annotations • Dependency relations: This is an arc annotation which connects two POS tag annotations with a directed relation (Figure 6). • Co-reference chains: The co-reference chain is realized as a set of typed mention spans linked by typed co-reference relation arcs. The co-reference relation annotation can cross multiple sentences and is represented in co-reference chains (Figure 7). The brat front end supports tokens and subtokens as a span annotation. However, tokens are currently the minimal annotation units in WebAnno, due to a requirement of supporting the TCF file format (Heid et al., 2010). Part-of-speech annotation is limited to singles token, while named entity and co-reference chain annotations may span multiple tokens. Dependency relations are implemented in such a way that the arc is drawn from the governor to the dependent (or the other way around, configurable), while co-reference chains are unidirectional and a chain is formed by referents that are transitively connected by arcs. Based on common practice in manual annotation, every user works on their own copy of the same document so that no concurrent editing occurs. We also found that displaying all annotation layers </context>
</contexts>
<marker>Heid, Schmid, Eckart, Hinrichs, 2010</marker>
<rawString>Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Erhard Hinrichs. 2010. A Corpus Representation Format for Linguistic Web Services: the D-SPIN Text Corpus Format and its Relationship with ISO Standards. In Proceedings of LREC 2010, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boci Lin</author>
<author>Yan Chen</author>
<author>Xu Chen</author>
<author>Yingying Yu</author>
</authors>
<date>2012</date>
<booktitle>Comparison between JSON and XML in Applications Based on AJAX. In Computer Science &amp; Service System (CSSS),</booktitle>
<location>Nanjing, China.</location>
<contexts>
<context position="8130" citStr="Lin et al., 2012" startWordPosition="1210" endWordPosition="1213">ts, and configuring the annotation layers. Only a project manager can administer a project. Figure 2 illustrates the project definition page with the tagset editor highlighted. 1Available for download at (this paper is based on v0.3.0): webanno.googlecode.com/ 2Formats: plain text, CoNLL (Nivre et al., 2007), TCF (Heid et al., 2010), UIMA XMI (Ferrucci and Lally, 2004) 2 Figure 2: The tagset editor on the project definition page 3.1.2 Annotation Annotation is carried out with an adapted version of the brat editor, which communicates with the server via Ajax (Wang et al., 2008) using the JSON (Lin et al., 2012) format. Annotators only see projects they are assigned to. The annotation page presents the annotator different options to set up the annotation environment, for customization: • Paging: For heavily annotated documents or very large documents, the original brat visualization is very slow, both for displaying and annotating the document. We use a paging mechanism that limits the number of sentences displayed at a time to make the performance independent of the document size. • Annotation layers: Annotators usually work on one or two annotations layers, such as part-of-speech and dependency or </context>
</contexts>
<marker>Lin, Chen, Chen, Yu, 2012</marker>
<rawString>Boci Lin, Yan Chen, Xu Chen, and Yingying Yu. 2012. Comparison between JSON and XML in Applications Based on AJAX. In Computer Science &amp; Service System (CSSS), 2012, Nanjing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Morton</author>
<author>Jeremy LaCivita</author>
</authors>
<title>WordFreak: an open tool for linguistic annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-2003, NAACL-Demonstrations ’03,</booktitle>
<pages>17--18</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5011" citStr="Morton and LaCivita, 2003" startWordPosition="712" endWordPosition="715"> is carried out with locally downloaded software. An interface to crowdsourcing platforms is missing. The GATE Teamware system is heavily targeted towards template-based information extraction. It sets a focus on the integration of automatic annotation components rather than on the interface for manual annotation. Besides, the overall application is rather complex for average users, requires considerable training and does not offer an alternative simplified interface as it would be required for crowdsourcing. General-purpose annotation tools like MMAX2 (M¨uller and Strube, 2006) or WordFreak (Morton and LaCivita, 2003) are not web-based and do not provide annotation project management. They are also not sufficiently flexible regarding different annotation layers. The same holds for specialized tools for single annotation layers, which we cannot list here for the sake of brevity. With the brat rapid annotation tool (Stenetorp et al., 2012), for the first time a web-based opensource annotation tool was introduced, which supports collaborative annotation for multiple annotation layers simultaneously on a single copy of the document, and is based on a client-server architecture. However, the current version of </context>
</contexts>
<marker>Morton, LaCivita, 2003</marker>
<rawString>Thomas Morton and Jeremy LaCivita. 2003. WordFreak: an open tool for linguistic annotation. In Proceedings of NAACL-2003, NAACL-Demonstrations ’03, pages 17–18, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
<author>Michael Strube</author>
</authors>
<title>Multi-level annotation of linguistic data with MMAX2.</title>
<date>2006</date>
<booktitle>Corpus Technology and Language Pedagogy: New Resources,</booktitle>
<pages>197--214</pages>
<editor>In S. Braun, K. Kohn, and J. Mukherjee, editors,</editor>
<location>New Tools, New Methods,</location>
<marker>M¨uller, Strube, 2006</marker>
<rawString>Christoph M¨uller and Michael Strube. 2006. Multi-level annotation of linguistic data with MMAX2. In S. Braun, K. Kohn, and J. Mukherjee, editors, Corpus Technology and Language Pedagogy: New Resources, New Tools, New Methods, pages 197–214. Peter Lang, Frankfurt a.M., Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>915--932</pages>
<location>Prague, Czech Republic.</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
<author>Sampo Pyysalo</author>
<author>Goran Topi´c</author>
<author>Tomoko Ohta</author>
<author>Sophia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>brat: a Web-based Tool for NLP-Assisted Text Annotation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Demonstrations at EACL-2012,</booktitle>
<location>Avignon, France.</location>
<marker>Stenetorp, Pyysalo, Topi´c, Ohta, Ananiadou, Tsujii, 2012</marker>
<rawString>Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii. 2012. brat: a Web-based Tool for NLP-Assisted Text Annotation. In Proceedings of the Demonstrations at EACL-2012, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingling Wang</author>
<author>Qin Liu</author>
<author>Na Li</author>
<author>Yan Liu</author>
</authors>
<title>An Automatic Approach to Reengineering Common Website with AJAX.</title>
<date>2008</date>
<booktitle>In 4th International Conference on Next Generation Web Services Practices,</booktitle>
<pages>185--190</pages>
<location>Seoul, South</location>
<contexts>
<context position="8096" citStr="Wang et al., 2008" startWordPosition="1203" endWordPosition="1206">r and curator users, defining tagsets, and configuring the annotation layers. Only a project manager can administer a project. Figure 2 illustrates the project definition page with the tagset editor highlighted. 1Available for download at (this paper is based on v0.3.0): webanno.googlecode.com/ 2Formats: plain text, CoNLL (Nivre et al., 2007), TCF (Heid et al., 2010), UIMA XMI (Ferrucci and Lally, 2004) 2 Figure 2: The tagset editor on the project definition page 3.1.2 Annotation Annotation is carried out with an adapted version of the brat editor, which communicates with the server via Ajax (Wang et al., 2008) using the JSON (Lin et al., 2012) format. Annotators only see projects they are assigned to. The annotation page presents the annotator different options to set up the annotation environment, for customization: • Paging: For heavily annotated documents or very large documents, the original brat visualization is very slow, both for displaying and annotating the document. We use a paging mechanism that limits the number of sentences displayed at a time to make the performance independent of the document size. • Annotation layers: Annotators usually work on one or two annotations layers, such as</context>
</contexts>
<marker>Wang, Liu, Li, Liu, 2008</marker>
<rawString>Qingling Wang, Qin Liu, Na Li, and Yan Liu. 2008. An Automatic Approach to Reengineering Common Website with AJAX. In 4th International Conference on Next Generation Web Services Practices, pages 185–190, Seoul, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aobo Wang</author>
<author>Cong Duy Vu Hoang</author>
<author>Min-Yen Kan</author>
</authors>
<title>Perspectives on Crowdsourcing Annotations for Natural Language Processing. In Language Resources And Evaluation,</title>
<date>2013</date>
<pages>9--31</pages>
<publisher>Springer</publisher>
<contexts>
<context position="11090" citStr="Wang et al., 2013" startWordPosition="1687" endWordPosition="1690"> complete and which documents the curator adjudicated. Figure 4 shows the project progress, progress of individual annotator and completion statistics. 3 Figure 3: Curation user interface (left: sentences with disagreement; right: merging editor) Figure 4: Project monitoring 3.1.6 Crowdsourcing Crowdsourcing is a way to quickly scale annotation projects. Distributing a task that otherwise will be performed by a controlled user group has become much easier. Hence, if quality can be ensured, it is an alternative to high quality annotation using a large number of arbitrary redundant annotations (Wang et al., 2013). For WebAnno, we have designed an approach where a source document is split into small parts that get presented to micro-workers in the CrowdFlower platform3. The crowdsourcing component is a separate module that handles the communication via CrowdFlower’s API, the definition of test items and job parameters, and the aggregation of results. The crowdsourced annotation appears as a virtual annotator in the tool. Since it is not trivial to express complex annotation tasks in comparably simple templates suitable for crowdsourcing (Biemann, 2013), we proceed by working out crowdsourcing templates</context>
</contexts>
<marker>Wang, Hoang, Kan, 2013</marker>
<rawString>Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2013. Perspectives on Crowdsourcing Annotations for Natural Language Processing. In Language Resources And Evaluation, pages 9–31. Springer Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>