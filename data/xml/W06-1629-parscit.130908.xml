<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000917">
<title confidence="0.996117">
Modeling Impression in Probabilistic Transliteration into Chinese
</title>
<author confidence="0.998359">
LiLi Xu* Atsushi Fujii
</author>
<affiliation confidence="0.996575666666667">
Graduate School of Library,
Information and Media Studies
University of Tsukuba
</affiliation>
<address confidence="0.97649">
1-2 Kasuga, Tsukuba, 305-8550, Japan
</address>
<email confidence="0.998412">
fujii@slis.tsukuba.ac.jp
</email>
<author confidence="0.956517">
Tetsuya Ishikawa
</author>
<affiliation confidence="0.973947">
The Historiographical Institute
The University of Tokyo
</affiliation>
<address confidence="0.9532935">
3-1 Hongo 7-chome, Bunkyo-ku
Tokyo, 133-0033, Japan
</address>
<email confidence="0.998666">
ishikawa@hi.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946722222222">
For transliterating foreign words into Chi-
nese, the pronunciation of a source word
is spelled out with Kanji characters. Be-
cause Kanji comprises ideograms, an indi-
vidual pronunciation may be represented
by more than one character. However,
because different Kanji characters convey
different meanings and impressions, char-
acters must be selected carefully. In this
paper, we propose a transliteration method
that models both pronunciation and im-
pression, whereas existing methods do not
model impression. Given a source word
and impression keywords related to the
source word, our method derives possible
transliteration candidates and sorts them
according to their probability. We evalu-
ate our method experimentally.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.974298618181818">
Reflecting the rapid growth of science, technology,
and economies, new technical terms and product
names have progressively been created. These
new words have also been imported into different
languages. There are three fundamental methods
for importing foreign words into a language.
In the first method—translation—the meaning
of the source word in question is represented by
an existing or new word in the target language.
In the second method—transliteration—the
pronunciation of the source word is represented by
using the phonetic alphabet of the target language,
such as Katakana in Japanese and Hangul in Ko-
rean.
* This work was done when the first author was a grad-
uate student at University of Tsukuba, who currently works
for Hitachi Construction Machinery Co., Ltd.
In the third method, the source word is spelled
out as it is. However, the misuse of this method
decreases the understandability and readability of
the target language.
While translation is time-consuming, requiring
selection of an existing word or generation of a
new word that correctly represents the meaning of
the source word, transliteration can be performed
rapidly. However, the situation is complicated for
Chinese, where a phonetic alphabet is not used and
Kanji is used to spell out both conventional Chi-
nese words and foreign words.
Because Kanji comprises ideograms, an in-
dividual pronunciation can potentially be repre-
sented by more than one character. However, if
several Kanji strings are related to the same pro-
nunciation of the source word, their meanings will
be different and will therefore convey different im-
pressions.
For example, “Coca-Cola” can be represented
by different Kanji strings in Chinese. The offi-
cial transliteration is “ৃQৃФ”, which comprises
“ৃষ (tasty)” and “ৃФ (pleasant)”, and is there-
fore associated with a positive connotation.
However, there are a number of Kanji strings
that represent similar pronunciations to that of
“Coca-Cola”, but which are associated with in-
appropriate impressions for a beverage, such as
“QवQt”. This word includes “ষव”, which is
associated with choking.
Therefore, Kanji characters must be selected
carefully during transliteration into Chinese. This
is especially important when foreign companies
intend to introduce their names and products into
China.
In this paper, we propose a method that models
both impression and pronunciation for translitera-
tion into Chinese.
</bodyText>
<page confidence="0.95855">
242
</page>
<note confidence="0.85389">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 242–249,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9990346">
Section 2 surveys previous research into auto-
matic transliteration, in order to clarify the mean-
ing and contribution of our research. Section 3
elaborates on our transliteration method. Section 4
evaluates the effectiveness of our method.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999901696969697">
In a broad sense, the term “transliteration” has
been used to refer to two tasks.
The first task is transliteration in the strict
sense, which creates new words in a target lan-
guage (Haizhou et al., 2004; Wan and Verspoor,
1998).
The second task is back-transliteration (Fujii
and Ishikawa, 2001; Jeong et al., 1999; Knight
and Graehl, 1998; Qu et al., 2003), which iden-
tifies the source word corresponding to an exist-
ing transliterated word. Back-transliteration is in-
tended mainly for cross-lingual information re-
trieval and machine translation.
Both transliteration tasks require methods that
model pronunciation in the source and target lan-
guages.
However, by definition, in back-transliteration,
the word in question has already been transliter-
ated and the meaning or impression of the source
word does not have to be considered. Thus, back-
transliteration is outside the scope of this paper.
In the following, we use the term “translitera-
tion” to refer to transliteration in the strict sense.
Existing transliteration methods for Chi-
nese (Haizhou et al., 2004; Wan and Verspoor,
1998) aim to spell out foreign names of people
and places, and do not model impression.
However, as exemplified by “Coca-Cola” in
Section 1, the impression of words needs to be
modeled in the transliteration of proper names,
such as companies and products. The contribu-
tion of our research is to incorporate a model of
impression into automatic transliteration.
</bodyText>
<sectionHeader confidence="0.996662" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.981964">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.926819222222222">
Figure 1 shows our transliteration method, which
models both pronunciation and impression when
transliterating foreign words into Chinese. We
will explain the entire process of our translitera-
tion method in terms of Figure 1.
The input for our method is twofold. First, a
source word to be transliterated into Chinese is re-
quested. Second, one or more words that describe
ranked list of transliteration candidates
</bodyText>
<figureCaption confidence="0.9845775">
Figure 1: Overview of our transliteration method
for Chinese.
</figureCaption>
<bodyText confidence="0.981473771428571">
the impression of the source word, which we call
“impression keywords”, are requested. Currently,
impression keywords must be provided manually
in Chinese. The output of our method is one or
more Kanji strings.
In an example scenario using our method, a user
has a good command of Chinese and intends to
introduce something (e.g., a company or product)
into China. It is reasonable to assume that this user
can provide one or more Chinese impression key-
words to associate with the target object.
Using the pronunciation model, the source word
is converted into a set of Kanji strings whose pro-
nunciation is similar to that of the source word.
Each of these Kanji strings is a transliteration can-
didate.
Currently, we use Japanese Katakana words as
source words, because Katakana words can easily
be converted into pronunciations using the Latin
alphabet. However, in principle, any language that
uses phonetic script can be a source language for
our method. In Figure 1, the Katakana word “bita-
min (vitamin)” is used as an example source word.
Using the impression model, impression key-
words are converted into a set of Kanji characters.
A simple implementation is to segment each im-
pression keyword into characters.
However, because it is difficult for a user to pro-
vide an exhaustive list of appropriate keywords
and characters, our impression model derives char-
acters that are not included in the impression key-
words.
Because of the potentially large number of se-
lected candidates, we need to rank the candidates.
We model both pronunciation and impression in
</bodyText>
<figure confidence="0.982644941176471">
impression model
pronunciation model
Kanji characters
Transliteration candidates
source word
䊎䉺䊚䊮
(bitamin)
Impression keyword(s)
㓈ᡸ (safeguard)
ҪҎ (another person)
⫳ᄬ (live)
㧹ݏ (nutrition)
㓈ศੑ
㓈Ҫੑ
䶺Ҫੑ ...
ranking candidates
㗄, ᒋ, 㓈, 䅽, Ҫ, ੑ ...
</figure>
<page confidence="0.995507">
243
</page>
<bodyText confidence="0.999050923076923">
a probabilistic framework, so that transliteration
candidates are sorted according to their probabil-
ity score.
Transliteration candidates that include many
characters derived from the impression model are
preferred. In other words, the Kanji characters
derived via the impression model are used to re-
rank the candidates derived via the pronunciation
model.
We elaborate on our probabilistic transliteration
model in Section 3.2. We then discuss the pronun-
ciation and impression models in Sections 3.3 and
3.4, respectively.
</bodyText>
<subsectionHeader confidence="0.99855">
3.2 Probabilistic Transliteration Model
</subsectionHeader>
<bodyText confidence="0.999864">
Given a romanized Japanese Katakana word R
and a set of impression keywords W, our pur-
pose is to select the Kanji string K that maxi-
mizes P(K|R, W), which is evaluated as shown
in Equation (1), using Bayes’ theorem.
</bodyText>
<equation confidence="0.995643">
P(K|R, W) = P(R, W |K) · P(K)
P(R, W)
P(R|K) · P(W |K) · P(K)
P(R, W)
∝ P(R|K) · P(W |K) · P(K)
(1)
</equation>
<bodyText confidence="0.999824">
In the second line of Equation (1), we assume the
conditional independence of R and W given K.
In the third line, we omit P(R, W), which is in-
dependent of K. This does not affect the rela-
tive rank of Kanji strings, when ranked in terms
of P(K|R, W).
In Figure 1, R and W are “bitamin” and
“MO t k t# VV”, respectively, and a K
candidate is “㓈ศੑ&amp;quot;.
&amp;quot;.
If a user intends to select more than one Kanji
string, those Ks associated with higher probabili-
ties should be selected.
As shown in Equation (1), P(K|R,W) can
be approximated by the product of P(R|K),
P(W |K), and P(K). We call these three factors
the pronunciation, impression, and language mod-
els, respectively.
The language model, P(K), models the proba-
bility of K irrespective of R and W. In probabilis-
tic natural language processing, P(K) is usually
realized by a word or character N-gram model, and
therefore a K that appears frequently in a corpus
is assigned a high probability.
However, because our purpose is to generate
new words, the use of statistics obtained from ex-
isting corpora is not effective. Therefore, we con-
sider P(K) to be constant for every K.
In summary, P(K|R, W) is approximated by a
product of P(R|K) and P(W |K). The quality of
our transliteration method will depend on the im-
plementation of the pronunciation and impression
models.
</bodyText>
<subsectionHeader confidence="0.995133">
3.3 Pronunciation Model
</subsectionHeader>
<bodyText confidence="0.9995643">
The pronunciation model, P(R|K), models the
probability that a roman representation R is se-
lected, given a Kanji string K.
In Japanese, the Hepburn and Kunrei systems
are commonly used for romanization purposes.
We use the Hepburn system. We use Pinyin as
a representation for Kanji characters. We decom-
pose K into Kanji characters and associate K with
R on a character-by-character basis. We calculate
P(R|K) as shown in Equation (2).
</bodyText>
<equation confidence="0.999547">
P(R|K) ≈ P(R|Y ) · P(Y |K)
P(yj|kj)
</equation>
<bodyText confidence="0.999708714285714">
Y denotes the Pinyin strings representing the pro-
nunciation of K. ki denotes a single Kanji char-
acter. ri and yi denote substrings of R and Y ,
respectively. R, Y , and K are decomposed into
the same number of elements, namely N. We cal-
culate P(ri|yi) and P(yi|ki) as shown in Equa-
tion (3).
</bodyText>
<equation confidence="0.998121166666667">
P(ri|yi) = F(ri, yi)
E F(r, yi)
r
P(yi|ki) = F(yi, ki)
E F(y, ki)
y
</equation>
<bodyText confidence="0.998168214285714">
F(x, y) denotes the co-occurrence frequency of x
and y. We need the co-occurrence frequencies of
ri and yi and the co-occurrence frequencies of yi
and ki in order to calculate P(R|K).
We used a bilingual dictionary comprising 1 140
Katakana words, most of which are technical
terms and proper nouns, and their transliterations
into Chinese, which are annotated with Pinyin. We
manually corresponded 151 pairs of Katakana and
roman characters on a mora-by-mora basis, and
romanized Katakana characters in the dictionary
automatically.
We obtained 1140 tuples, of the form
&lt; R, Y, K &gt;. Because the number of tuples was
</bodyText>
<equation confidence="0.975916">
≈
N N
≈ P(ri|yi) · H
i=1 j=1
</equation>
<page confidence="0.989525">
244
</page>
<bodyText confidence="0.9982606">
manageable, we obtained the element-by-element
R, Y , and K correspondences manually. Finally,
we calculated F(ri, yi) and F(yi, ki).
If there are many tuples, and the process of man-
ual correspondence is expensive, we can automate
the process as performed in existing transliteration
methods, such as the EM algorithm (Knight and
Graehl, 1998) or DP matching (Fujii and Ishikawa,
2001).
The above calculations are performed off-line.
In the online process, we consider all possible seg-
mentations of a single Katakana word. For exam-
ple, the romanized Katakana word “bitamin (vi-
tamin)” corresponds to two Pinyin strings and is
segmented differently, as follows:
</bodyText>
<listItem confidence="0.9999445">
• bi-ta-min: wei-ta-ming,
• bi-ta-mi-n: wei-ta-mi-an.
</listItem>
<sectionHeader confidence="0.635097" genericHeader="method">
3.4 Impression Model
</sectionHeader>
<bodyText confidence="0.999503111111111">
The impression model, P(W|K), models the
probability that W is selected as a set of impres-
sion keywords, given Kanji string K. As in the
calculation of P(R|K) in Equation (2), we de-
compose W and K into elements, in calculating
P(W|K).
W is decomposed into a set of words, wi, and
K is decomposed into a set of Kanji characters, kj.
We calculate P(W |K) as a product of P(wi|kj),
which is the probability that wi is selected as an
impression keyword given kj.
However, unlike Equation (2), the numbers of
wi and kj derived from W and K are not always
the same, because users are allowed to provide an
arbitrary number of impression keywords. There-
fore, for each kj we select the wi that maximizes
P(wi|kj) and approximate P(W |K) as shown in
Equation (4).
</bodyText>
<equation confidence="0.978461">
P(wi|kj) (4)
</equation>
<figureCaption confidence="0.995849">
Figure 2 shows an example in which the four Chi-
nese words in the “wi” column are also used in
Figure 1.
</figureCaption>
<bodyText confidence="0.920904">
We calculate P(wi|kj) by Equation (5).
</bodyText>
<equation confidence="0.9936595">
F(wi, kj)
P(wi|kj) = (5)
E F(w, kj)
w
</equation>
<bodyText confidence="0.984864">
As in Equation (3), F(x, y) denotes the co-
occurrence frequency of x and y.
</bodyText>
<equation confidence="0.893411375">
wi 㓈 Ҫ ੑ
㓈ᡸ 0.5 — —
ҪҎ 0.3 0.4 —
⫳ᄬ — — 0.6
㧹ݏ 0.1 — —
3�㓈ᡸ ҪҎ ⫳ᄬ 㧹ݏ_㓈Ҫੑ�
3(㓈ᡸ_㓈�h3MҎ_Ҫ�h3(⫳ᄬ_ੑ�
0.5h0.4h0.6
</equation>
<figureCaption confidence="0.993969">
Figure 2: Example calculation of P(W |K).
</figureCaption>
<bodyText confidence="0.99964746875">
In summary, we need co-occurrences of each
word and character in Chinese.
These co-occurrences can potentially be col-
lected from existing language resources, such as
corpora in Chinese.
However, it is desirable to collect an association
between a word and a character, not simply their
co-occurrence in corpora. Therefore, we used
a dictionary of Kanji in Chinese, in which each
Kanji character entry is explained via sentences,
and often exemplified by one or more words that
include that character.
We selected 599 entry characters that are often
used to spell out foreign words. Then we collected
the frequencies with which each word is used to
explain each entry character.
Because Chinese sentences lack lexical seg-
mentation, we used SuperMorpho1 to perform a
morphological analysis of explanation sentences
and example words. As a result, 16 943 word types
were extracted. We used all of these words to cal-
culate the co-occurrence frequencies, irrespective
of the parts of speech.
Table 1 shows examples of Kanji characters,
Chinese words, and their co-occurrence frequen-
cies in the dictionary.
However, P (wi|kj) cannot be calculated for the
Kanji characters not modeled in our method (i.e.,
the Kanji characters not included in the 599 entry
characters). Thus, for smoothing purposes, we ex-
perimentally set P (wi|kj) at 0.001 for those kj not
modeled.
</bodyText>
<sectionHeader confidence="0.999712" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.987962">
4.1 Method
</subsectionHeader>
<bodyText confidence="0.9998124">
We evaluated our transliteration method experi-
mentally. Because the contribution of our research
is the incorporation of the impression model in a
transliteration method, we used a method that uses
only the pronunciation model as a control.
</bodyText>
<footnote confidence="0.596947">
1http://www.omronsoft.com/
</footnote>
<equation confidence="0.9563208">
�
P (W |K) �
j
max
wa
</equation>
<page confidence="0.999416">
245
</page>
<tableCaption confidence="0.978058">
Table 1: Example of characters, words, and their
co-occurrence frequencies.
</tableCaption>
<table confidence="0.62581825">
kj w, F(w,,kj) kj w, F(w,,kj) kj w, F(w,,k j)
催 催 39 ད 㕢 3 Ф ୰ᙺ 2
催 催໻ 8 ད 䉠㕢 2 Ф ᛝᖿ 1
催 䖰 4 ད ད 43 Ф ᖿФ 5
催 ϟ 4 ད དⳟ 2 Ф ᑌ⽣ 2
催 䎱⾏ 2 ད 㕢Б 2 Ф Ф 51
催 ໻ 1 ད དϡ 2 Ф ュ 5
催 ֫ 2 ད དৗ 2 Ф ୰ 3
催 ዛ催 2 ད 㸼⼎ 4 Ф ䷇Ф 11
催 催ᇮ 2 ད ৠᛣ 2 Ф Фᛣ 2
催 ࡴ催 3 ད ୰ད 1 Ф ᅝФ 7
催 ๲催 1 ད ୰⠅ 2 Ф ФѢ 5
</table>
<bodyText confidence="0.999676605263158">
From a Japanese–Chinese dictionary, we se-
lected 210 Katakana words that had been translit-
erated into Chinese, and used these Katakana
words as test words. Each test word can be clas-
sified into one of the following five categories:
products, companies, places, persons, or general
words. Details of the categories of test inputs are
shown in Table 2.
Three Chinese graduate students who had a
good command of Japanese served as assessors
and produced reference data. None of the asses-
sors was an author of this paper. The assessors
performed the same task for the same test words
independently, in order to enhance the objectivity
of the results.
We produced the reference data via the follow-
ing procedure.
First, for each test word, each assessor pro-
vided one or more impression keywords in Chi-
nese. We did not restrict the number of impression
keywords per test word, which was determined by
each assessor.
If an assessor provided more than one impres-
sion keyword for a single test word, he/she was
requested to sort them in order of preference, so
that we could investigate the effect of the number
of impression keywords on the evaluation results,
by changing the number of top keywords used for
transliteration purposes.
We provided the assessors with the descriptions
for the test words from the source dictionary, so
that the assessors could understand the meaning
of each test word.
Second, for each test word, we applied the con-
trol method and our method independently, which
produced two lists of ranked transliteration candi-
dates. Because the impression keywords provided
by the assessors were used only in our method, the
</bodyText>
<tableCaption confidence="0.992519">
Table 2: Categories of test words.
</tableCaption>
<table confidence="0.997431857142857">
Category # Words Example word
Japanese Chinese English
Product 63 7!l 4 AX Audi
Company 49 shy/ kat Epson
Place 36 ���� faA,fa Ohio
Person 21 �51 9% Chopin
General 41 s�/-:. � zVJL angel
</table>
<bodyText confidence="0.995730125">
ranked list produced by the control was the same
for all assessors.
Third, for each test word, each assessor identi-
fied one or more correct transliterations, according
to their impression of the test word. It was impor-
tant not to reveal to the assessors which method
produced which candidates.
By these means, we selected the top 100
transliteration candidates from the two ranked lists
for the control and our method. We merged these
candidates, removed duplications, and sorted the
remaining candidates by the character code.
As a result, the assessors judged the correctness
of up to 200 candidates for each test word. How-
ever, for some test words, assessors were not able
to find correct transliterations in the candidate list.
The resultant reference data was used to eval-
uate the accuracy of a test method in ranking
transliteration candidates. We used the average
rank of correct answers in the list as the evalua-
tion measure. If more than one correct answer was
found for a single test word, we first averaged the
ranks of these answers and then averaged the ranks
over the test words.
Although we used the top 100 candidates for
judgment purposes, the entire ranked list was used
to evaluate each method. Therefore, the average
rank of correct answers can potentially be over
100. The average number of candidates per test
word was 31779.
Because our method uses the impression model
to re-rank the candidates produced by the pronun-
ciation model, the lists for the control and our
method comprise the same candidates. Therefore,
it is fair to compare these two methods by the av-
erage rank of the correct answers.
For each test word, there is more than one type
of “correct answer”, as follows:
(a) transliteration candidates judged as correct
by the assessors independently (translitera-
</bodyText>
<page confidence="0.99568">
246
</page>
<bodyText confidence="0.873624">
tion candidates judged as correct by at least
one assessor);
</bodyText>
<listItem confidence="0.98120925">
(b) transliteration candidates judged as correct
by all assessors;
(c) transliterations defined in the source dictio-
nary.
</listItem>
<bodyText confidence="0.999931857142857">
In (a), the coverage of correct answers is the
largest, whereas the objectivity of the judgment is
the lowest.
In (c), the objectivity of the judgment is the
largest, whereas the coverage of correct answers
is the lowest. Although for each Katakana word
the source dictionary gives only one transliteration
that is commonly used, there are a number of ap-
propriate out-of-dictionary transliterations.
In (b), where the assessors did not disagree
about the correctness, the coverage of correctness
and the objectivity are both middle ranked.
Because none of the above answer types is per-
fect, we used all three types independently.
</bodyText>
<subsectionHeader confidence="0.963516">
4.2 Results and Analyses
</subsectionHeader>
<bodyText confidence="0.999987151898734">
Tables 3–5 show the results of comparative exper-
iments using the answer types (a)–(c) above, re-
spectively.
In Tables 3–5, the column “# of test words” de-
notes the number of test words for which at least
one correct answer exists. While the values in the
second column of Table 3 are different depending
on the assessor, in Tables 4 and 5 the values of the
second column are the same for all assessors.
The columns “Avg. # of KW” and “Avg. # of
answers” denote the number of impression key-
words and the number of correct answers per test
word, respectively. While the values in the fourth
column of Table 3 are different depending on the
assessor, in Tables 4 and 5 the values of the fourth
column are the same for all assessors.
In Tables 4 and 5, the average rank of correct an-
swers for the control is the same for all assessors.
However, the average rank of correct answers for
our method is different depending on the assessor,
because the impression keywords used depended
on the assessor.
The two columns in “Avg. rank” denote the av-
erage ranks of correct answers for the control and
for our method, respectively. Looking at Tables 3–
5, it can be seen that our method outperformed the
control in ranking transliteration candidates, irre-
spective of the assessor and the answer type.
The average rank of correct answers for our
method in Table 5 was lower than those in Tables 3
and 4. One reason is that the correct answers in the
source dictionary are not always related to the im-
pression keywords provided by the assessors.
Table 6 presents the results in Table 3 on a
category-by-category basis. Because the results
were similar for answer types (b) and (c), we show
only the answer type (a) results, for the sake of
conciseness. Looking at Table 6, it can be seen
that our method outperformed the control in rank-
ing transliteration candidates, irrespective of the
category of test words.
Our method was effective for transliterating
names of places and people, although these types
of words are usually transliterated independently
of their impressions, compared with the names of
products and companies.
One reason is that, in the dictionary of Kanji
used to produce the impression model, the expla-
nation of an entry sometimes includes a phrase,
such as “this character is often used for a person’s
name”. Assessors provided the word “person” in
Chinese as an impression keyword for a number
of person names. As a result, transliteration can-
didates that included characters typically used for
a person’s name were highly ranked.
It may be argued that, because the impression
model was produced using Kanji characters that
are often used for transliteration purposes, the im-
pression model could possibly rank correct an-
swers better than the pronunciation model. How-
ever, the pronunciation model was also produced
from Kanji characters used for transliteration pur-
poses.
Figure 3 shows the distribution of correct an-
swers for different ranges of ranks, using answer
type (a). The number of correct answers in the top
10 for our method is approximately twice that of
the control. In addition, by our method, most of
the correct answers can be found in the top 100
candidates. Because the results were similar for
answer types (b) and (c), we show only the answer
type (a) results, for the sake of conciseness.
As explained in Section 4.1, for each test word,
the assessors were requested to sort the impression
keywords in order of preference. We analyzed the
relation between the number of impression key-
words used for the transliteration and the average
rank of correct answers, by varying the threshold
for the number of top impression keywords used.
</bodyText>
<page confidence="0.998288">
247
</page>
<tableCaption confidence="0.999843">
Table 3: Results obtained with answer type (a).
</tableCaption>
<table confidence="0.9869135">
Assessor # of test words Avg. # of KW Avg. # of answers Avg. rank
Control Our method
A 205 5.1 3.8 706 82
B 204 5.8 3.8 728 44
C 199 3.5 2.6 1130 28
Avg. 203 4.8 3.4 855 51
</table>
<tableCaption confidence="0.997721">
Table 4: Results obtained with answer type (b).
</tableCaption>
<table confidence="0.982181166666667">
Assessor # of test words Avg. # of KW Avg. # of answers Avg. rank
Control Our method
A 108 5.1 1.1 297 22
B 108 5.8 1.1 297 23
C 108 3.5 1.1 297 18
Avg. 108 4.8 1.1 297 21
</table>
<tableCaption confidence="0.996436">
Table 5: Results obtained with answer type (c).
</tableCaption>
<table confidence="0.980430166666666">
Assessor # of test words Avg. # of KW Avg. # of answers Avg. rank
Control Our method
A 210 5.1 1 1738 260
B 210 5.8 1 1738 249
C 210 3.5 1 1738 103
Avg. 210 4.8 1 1738 204
</table>
<tableCaption confidence="0.987228">
Table 6: Results obtained with answer type (a) on a category-by-category basis.
</tableCaption>
<table confidence="0.970214875">
Category # of test words Avg. # of KW Avg. # of answers Avg. rank
Control Our method
Product 144 4.8 3.5 1527 64
Company 186 4.7 3.6 742 54
Place 102 4.8 3.7 777 46
Person 61 5.0 3.4 766 51
General 115 4.7 2.6 280 38
Avg. 122 4.8 3.4 818 51
</table>
<figure confidence="0.886844142857143">
Rank
# of Correct answers
800
400
900
700
600
500
300
200
100
0
Control
Our method
</figure>
<figureCaption confidence="0.999949">
Figure 3: Distribution of average rank for correct answers.
</figureCaption>
<page confidence="0.988517">
248
</page>
<bodyText confidence="0.99898028125">
Table 7 shows the average rank of correct an-
swers for different numbers of impression key-
words, on an assessor-by-assessor basis. By com-
paring Tables 3 and 7, we see that even if a sin-
gle impression keyword was provided, the average
rank of correct answers was higher than that for
the control. In addition, the average rank of correct
answers was generally improved by increasing the
number of impression keywords.
Finally, we investigated changes in the rank of
correct answers caused by our method. Table 8
shows the results, in which “Higher” and “Lower”
denote the number of correct answers whose ranks
determined by our method were higher or lower,
respectively, than those determined by the control.
For approximately 30% of the correct answers,
our method decreased the control’s rank. Errors
were mainly caused by correct answers containing
Kanji characters that were not modeled in the im-
pression model. Although we used a smoothing
technique for characters not in the model, the re-
sult was not satisfactory. To resolve this problem,
the number of characters in the impression model
should be increased.
In summary, our method, which uses both the
impression and pronunciation models, ranked cor-
rect transliterations more highly than a method
that used only the pronunciation model. We con-
clude that the impression model is effective for
transliterating foreign words into Chinese. At the
same time, we concede that there is room for im-
provement in the impression model.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99341515">
For transliterating foreign words into Chinese, the
pronunciation of a source word is spelled out with
Kanji characters. Because Kanji characters are
ideograms, a single pronunciation can be repre-
sented by more than one character. However, be-
cause different Kanji characters convey different
meanings and impressions, characters must be se-
lected carefully.
In this paper, we proposed a transliteration
method that models both pronunciation and im-
pression, compared to existing methods that do
not model impression. Given a source word and
impression keywords related to the source word,
our method derives possible transliteration candi-
dates, and sorts them according to their probabil-
ity. We showed the effectiveness of our method
experimentally.
Table 7: Relation between the number of impres-
sion keywords and average rank of correct answers
with answer type (a).
</bodyText>
<table confidence="0.9975494">
# of KW
Assessor 1 2 3
A 103 94 92
B 64 60 52
C 113 73 34
</table>
<tableCaption confidence="0.993958">
Table 8: Changes in ranks of correct answers
caused by our method.
</tableCaption>
<table confidence="0.8108362">
Avg. rank
Answer type # of answers Higher Lower
2070 1431 639
360 250 110
630 422 208
</table>
<bodyText confidence="0.997367">
Future work will include collecting impression
keywords automatically, and adapting the lan-
guage model to the category of source words.
</bodyText>
<sectionHeader confidence="0.999307" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998960032258065">
Atsushi Fujii and Tetsuya Ishikawa. 2001.
Japanese/English cross-language information
retrieval: Exploration of query translation and
transliteration. Computers and the Humanities,
35(4):389–420.
Li Haizhou, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, pages
160–167.
Kil Soon Jeong, Sung Hyon Myaeng, Jae Sung Lee,
and Key-Sun Choi. 1999. Automatic identification
and back-transliteration of foreign words for infor-
mation retrieval. Information Processing &amp; Man-
agement, 35:523–540.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599–612.
Yan Qu, Gregory Grefenstette, and David A. Evans.
2003. Automatic transliteration for Japanese-to-
English text retrieval. In Proceedings of the 26th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 353–360.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and the 17th Inter-
national Conference on Computational Linguistics,
pages 1352–1356.
</reference>
<page confidence="0.998902">
249
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.304792">
<title confidence="0.999896">Modeling Impression in Probabilistic Transliteration into Chinese</title>
<author confidence="0.774363">Atsushi</author>
<affiliation confidence="0.931133">Graduate School of Information and Media University of</affiliation>
<address confidence="0.993112">1-2 Kasuga, Tsukuba, 305-8550,</address>
<email confidence="0.987117">fujii@slis.tsukuba.ac.jp</email>
<author confidence="0.739156">Tetsuya The Historiographical</author>
<affiliation confidence="0.992191">The University of</affiliation>
<address confidence="0.8833455">3-1 Hongo 7-chome, Tokyo, 133-0033,</address>
<email confidence="0.979978">ishikawa@hi.u-tokyo.ac.jp</email>
<abstract confidence="0.998157210526316">For transliterating foreign words into Chinese, the pronunciation of a source word is spelled out with Kanji characters. Because Kanji comprises ideograms, an individual pronunciation may be represented by more than one character. However, because different Kanji characters convey different meanings and impressions, characters must be selected carefully. In this paper, we propose a transliteration method that models both pronunciation and impression, whereas existing methods do not model impression. Given a source word and impression keywords related to the source word, our method derives possible transliteration candidates and sorts them according to their probability. We evaluate our method experimentally.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>Japanese/English cross-language information retrieval: Exploration of query translation and transliteration.</title>
<date>2001</date>
<journal>Computers and the Humanities,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="4280" citStr="Fujii and Ishikawa, 2001" startWordPosition="635" endWordPosition="638">2–249, Sydney, July 2006. c�2006 Association for Computational Linguistics Section 2 surveys previous research into automatic transliteration, in order to clarify the meaning and contribution of our research. Section 3 elaborates on our transliteration method. Section 4 evaluates the effectiveness of our method. 2 Related Work In a broad sense, the term “transliteration” has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998). The second task is back-transliteration (Fujii and Ishikawa, 2001; Jeong et al., 1999; Knight and Graehl, 1998; Qu et al., 2003), which identifies the source word corresponding to an existing transliterated word. Back-transliteration is intended mainly for cross-lingual information retrieval and machine translation. Both transliteration tasks require methods that model pronunciation in the source and target languages. However, by definition, in back-transliteration, the word in question has already been transliterated and the meaning or impression of the source word does not have to be considered. Thus, backtransliteration is outside the scope of this paper</context>
<context position="11832" citStr="Fujii and Ishikawa, 2001" startWordPosition="1896" endWordPosition="1899">f Katakana and roman characters on a mora-by-mora basis, and romanized Katakana characters in the dictionary automatically. We obtained 1140 tuples, of the form &lt; R, Y, K &gt;. Because the number of tuples was ≈ N N ≈ P(ri|yi) · H i=1 j=1 244 manageable, we obtained the element-by-element R, Y , and K correspondences manually. Finally, we calculated F(ri, yi) and F(yi, ki). If there are many tuples, and the process of manual correspondence is expensive, we can automate the process as performed in existing transliteration methods, such as the EM algorithm (Knight and Graehl, 1998) or DP matching (Fujii and Ishikawa, 2001). The above calculations are performed off-line. In the online process, we consider all possible segmentations of a single Katakana word. For example, the romanized Katakana word “bitamin (vitamin)” corresponds to two Pinyin strings and is segmented differently, as follows: • bi-ta-min: wei-ta-ming, • bi-ta-mi-n: wei-ta-mi-an. 3.4 Impression Model The impression model, P(W|K), models the probability that W is selected as a set of impression keywords, given Kanji string K. As in the calculation of P(R|K) in Equation (2), we decompose W and K into elements, in calculating P(W|K). W is decomposed</context>
</contexts>
<marker>Fujii, Ishikawa, 2001</marker>
<rawString>Atsushi Fujii and Tetsuya Ishikawa. 2001. Japanese/English cross-language information retrieval: Exploration of query translation and transliteration. Computers and the Humanities, 35(4):389–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Haizhou</author>
<author>Zhang Min</author>
<author>Su Jian</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="4188" citStr="Haizhou et al., 2004" startWordPosition="622" endWordPosition="625">06 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 242–249, Sydney, July 2006. c�2006 Association for Computational Linguistics Section 2 surveys previous research into automatic transliteration, in order to clarify the meaning and contribution of our research. Section 3 elaborates on our transliteration method. Section 4 evaluates the effectiveness of our method. 2 Related Work In a broad sense, the term “transliteration” has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998). The second task is back-transliteration (Fujii and Ishikawa, 2001; Jeong et al., 1999; Knight and Graehl, 1998; Qu et al., 2003), which identifies the source word corresponding to an existing transliterated word. Back-transliteration is intended mainly for cross-lingual information retrieval and machine translation. Both transliteration tasks require methods that model pronunciation in the source and target languages. However, by definition, in back-transliteration, the word in question has already been transliterated and the meaning or impression of the source word </context>
</contexts>
<marker>Haizhou, Min, Jian, 2004</marker>
<rawString>Li Haizhou, Zhang Min, and Su Jian. 2004. A joint source-channel model for machine transliteration. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kil Soon Jeong</author>
<author>Sung Hyon Myaeng</author>
<author>Jae Sung Lee</author>
<author>Key-Sun Choi</author>
</authors>
<title>Automatic identification and back-transliteration of foreign words for information retrieval.</title>
<date>1999</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<pages>35--523</pages>
<contexts>
<context position="4300" citStr="Jeong et al., 1999" startWordPosition="639" endWordPosition="642">c�2006 Association for Computational Linguistics Section 2 surveys previous research into automatic transliteration, in order to clarify the meaning and contribution of our research. Section 3 elaborates on our transliteration method. Section 4 evaluates the effectiveness of our method. 2 Related Work In a broad sense, the term “transliteration” has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998). The second task is back-transliteration (Fujii and Ishikawa, 2001; Jeong et al., 1999; Knight and Graehl, 1998; Qu et al., 2003), which identifies the source word corresponding to an existing transliterated word. Back-transliteration is intended mainly for cross-lingual information retrieval and machine translation. Both transliteration tasks require methods that model pronunciation in the source and target languages. However, by definition, in back-transliteration, the word in question has already been transliterated and the meaning or impression of the source word does not have to be considered. Thus, backtransliteration is outside the scope of this paper. In the following, </context>
</contexts>
<marker>Jeong, Myaeng, Lee, Choi, 1999</marker>
<rawString>Kil Soon Jeong, Sung Hyon Myaeng, Jae Sung Lee, and Key-Sun Choi. 1999. Automatic identification and back-transliteration of foreign words for information retrieval. Information Processing &amp; Management, 35:523–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="4325" citStr="Knight and Graehl, 1998" startWordPosition="643" endWordPosition="646">or Computational Linguistics Section 2 surveys previous research into automatic transliteration, in order to clarify the meaning and contribution of our research. Section 3 elaborates on our transliteration method. Section 4 evaluates the effectiveness of our method. 2 Related Work In a broad sense, the term “transliteration” has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998). The second task is back-transliteration (Fujii and Ishikawa, 2001; Jeong et al., 1999; Knight and Graehl, 1998; Qu et al., 2003), which identifies the source word corresponding to an existing transliterated word. Back-transliteration is intended mainly for cross-lingual information retrieval and machine translation. Both transliteration tasks require methods that model pronunciation in the source and target languages. However, by definition, in back-transliteration, the word in question has already been transliterated and the meaning or impression of the source word does not have to be considered. Thus, backtransliteration is outside the scope of this paper. In the following, we use the term “translit</context>
<context position="11790" citStr="Knight and Graehl, 1998" startWordPosition="1889" endWordPosition="1892">yin. We manually corresponded 151 pairs of Katakana and roman characters on a mora-by-mora basis, and romanized Katakana characters in the dictionary automatically. We obtained 1140 tuples, of the form &lt; R, Y, K &gt;. Because the number of tuples was ≈ N N ≈ P(ri|yi) · H i=1 j=1 244 manageable, we obtained the element-by-element R, Y , and K correspondences manually. Finally, we calculated F(ri, yi) and F(yi, ki). If there are many tuples, and the process of manual correspondence is expensive, we can automate the process as performed in existing transliteration methods, such as the EM algorithm (Knight and Graehl, 1998) or DP matching (Fujii and Ishikawa, 2001). The above calculations are performed off-line. In the online process, we consider all possible segmentations of a single Katakana word. For example, the romanized Katakana word “bitamin (vitamin)” corresponds to two Pinyin strings and is segmented differently, as follows: • bi-ta-min: wei-ta-ming, • bi-ta-mi-n: wei-ta-mi-an. 3.4 Impression Model The impression model, P(W|K), models the probability that W is selected as a set of impression keywords, given Kanji string K. As in the calculation of P(R|K) in Equation (2), we decompose W and K into elemen</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Qu</author>
<author>Gregory Grefenstette</author>
<author>David A Evans</author>
</authors>
<title>Automatic transliteration for Japanese-toEnglish text retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>353--360</pages>
<contexts>
<context position="4343" citStr="Qu et al., 2003" startWordPosition="647" endWordPosition="650">ics Section 2 surveys previous research into automatic transliteration, in order to clarify the meaning and contribution of our research. Section 3 elaborates on our transliteration method. Section 4 evaluates the effectiveness of our method. 2 Related Work In a broad sense, the term “transliteration” has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998). The second task is back-transliteration (Fujii and Ishikawa, 2001; Jeong et al., 1999; Knight and Graehl, 1998; Qu et al., 2003), which identifies the source word corresponding to an existing transliterated word. Back-transliteration is intended mainly for cross-lingual information retrieval and machine translation. Both transliteration tasks require methods that model pronunciation in the source and target languages. However, by definition, in back-transliteration, the word in question has already been transliterated and the meaning or impression of the source word does not have to be considered. Thus, backtransliteration is outside the scope of this paper. In the following, we use the term “transliteration” to refer </context>
</contexts>
<marker>Qu, Grefenstette, Evans, 2003</marker>
<rawString>Yan Qu, Gregory Grefenstette, and David A. Evans. 2003. Automatic transliteration for Japanese-toEnglish text retrieval. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 353–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Cornelia Maria Verspoor</author>
</authors>
<title>Automatic English-Chinese name transliteration for development of multilingual resources.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>1352--1356</pages>
<contexts>
<context position="4213" citStr="Wan and Verspoor, 1998" startWordPosition="626" endWordPosition="629">ical Methods in Natural Language Processing (EMNLP 2006), pages 242–249, Sydney, July 2006. c�2006 Association for Computational Linguistics Section 2 surveys previous research into automatic transliteration, in order to clarify the meaning and contribution of our research. Section 3 elaborates on our transliteration method. Section 4 evaluates the effectiveness of our method. 2 Related Work In a broad sense, the term “transliteration” has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998). The second task is back-transliteration (Fujii and Ishikawa, 2001; Jeong et al., 1999; Knight and Graehl, 1998; Qu et al., 2003), which identifies the source word corresponding to an existing transliterated word. Back-transliteration is intended mainly for cross-lingual information retrieval and machine translation. Both transliteration tasks require methods that model pronunciation in the source and target languages. However, by definition, in back-transliteration, the word in question has already been transliterated and the meaning or impression of the source word does not have to be consi</context>
</contexts>
<marker>Wan, Verspoor, 1998</marker>
<rawString>Stephen Wan and Cornelia Maria Verspoor. 1998. Automatic English-Chinese name transliteration for development of multilingual resources. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 1352–1356.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>