<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.5363705">
Multiple Many-to-Many Sequence Alignment for Combining
String-Valued Variables: A G2P Experiment
</title>
<author confidence="0.970783">
Steffen Eger
</author>
<affiliation confidence="0.951524666666667">
Text Technology Lab
Goethe University Frankfurt am Main
Frankfurt am Main, Germany
</affiliation>
<email confidence="0.988575">
steeger@em.uni-frankfurt.de
</email>
<sectionHeader confidence="0.994616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958583333333">
We investigate multiple many-to-many
alignments as a primary step in integrat-
ing supplemental information strings in
string transduction. Besides outlining DP
based solutions to the multiple alignment
problem, we detail an approximation of
the problem in terms of multiple sequence
segmentations satisfying a coupling con-
straint. We apply our approach to boosting
baseline G2P systems using homogeneous
as well as heterogeneous sources of sup-
plemental information.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990122291666667">
String-to-string translation (string transduction) is
the problem of converting one string x over an
alphabet E into another string y over a possi-
bly different alphabet Γ. The most prominent
applications of string-to-string translation in nat-
ural language processing (NLP) are grapheme-
to-phoneme conversion, in which x is a letter-
string and y is a string of phonemes, translit-
eration (Sherif and Kondrak, 2007), lemmatiza-
tion (Dreyer et al., 2008), and spelling error cor-
rection (Brill and Moore, 2000). The classi-
cal learning paradigm in each of these settings
is to train a model on pairs of strings {(x, y)}
and then to evaluate model performance on test
data. Thereby, all state-of-the-art modelings we
are aware of (e.g., (Jiampojamarn et al., 2007;
Bisani and Ney, 2008; Jiampojamarn et al., 2008;
Jiampojamarn et al., 2010; Novak et al., 2012))
proceed by first aligning the string pairs (x, y)
in the training data. Also, these modelings ac-
knowledge that alignments may typically be of a
rather complex nature in which several x sequence
ph oe n i x
f i n I ks
</bodyText>
<tableCaption confidence="0.698679">
Table 1: Sample monotone many-to-many align-
ment between x = phoenix and y = finIks.
</tableCaption>
<bodyText confidence="0.99839996875">
characters may be matched up with several y se-
quence characters; Table 1 illustrates. Once the
training data is aligned, since x and y sequences
are then segmented into equal number of seg-
ments, string-to-string translation may be seen as
a sequence labeling (tagging) problem in which x
(sub-)sequence characters are observed variables
and y (sub-)sequence characters are hidden states
(Jiampojamarn et al., 2007; Jiampojamarn et al.,
2010).
In this work, we extend the problem of classi-
cal string-to-string translation by assuming that, at
training time, we have available (M + 2)-tuples
of strings {(x, ˆy(1), ... , ˆy(M), y)}, where x is the
input string, ˆy(m), for 1 &lt; m &lt; M, are sup-
plemental information strings, and y is the de-
sired output string; at test time, we wish to pre-
dict y from (x, ˆy(1), ... , ˆy(M)). Generally, we
may think of ˆy(1), ... , ˆy(M) as arbitrary strings
over arbitrary alphabets E(m), for 1 &lt; m &lt; M.
For example, x might be a letter-string and ˆy(m)
might be a transliteration of x in language Lm (cf.
Bhargava and Kondrak (2012)). Alternatively, and
this is our model scenario in the current work, x
might be a letter input string and ˆy(m) might be
the predicted string of phonemes, given x, pro-
duced by an (offline) system Tm. This situation
is outlined in Table 3. In the table, we also illus-
trate a multiple (monotone) many-to-many align-
ment of (x, ˆy(1), ... , ˆy(M), y). By this, we mean
an alignment where (1) subsequences of all M +2
strings may be matched up with each other (many-
</bodyText>
<page confidence="0.972751">
909
</page>
<note confidence="0.978046666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 909–919,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.948153285714286">
to-many alignments), and where (2) the match-
ing up of subsequences obeys monotonicity. Note
that such a multiple alignment generalizes classi-
cal monotone many-to-many alignments between
pairs of strings, as shown in Table 1. Furthermore,
such an alignment may apparently be quite useful.
For instance, while none of the strings ˆy(&apos;) in the
table equals the true phonetic transcription y of x,
taking a position-wise majority vote of the multi-
ple alignment of (ˆy(1), ... , ˆy(M)) yields y. More-
over, analogously as in the case of pairs of aligned
strings, we may perceive the so extended string-
to-string translation problem as a sequence label-
ing task once (x, ˆy(1), ... , ˆy(M), y) are multiply
aligned, but now, with additional observed vari-
ables (or features), namely, (sub-)sequence char-
acters of each string ˆy(&apos;).
To further motivate our approach, consider the
situation of training a new G2P system on the ba-
sis of, e.g., Combilex (Richmond et al., 2009).
For each letter form in its database, Combilex
provides a corresponding phonetic transcription.
Now, suppose that, in addition, we can poll an
external knowledge source such as Wiktionary for
(its) phonetic transcriptions of the respective Com-
bilex letter words as outlined in Table 2. The cen-
Input form Wiktionary Combilex
neutrino nju:tôi:noU nutrinF
wooded wUdId wUd@d
wrench ôEnúS rEn&lt;
Table 2: Input letter words, Wiktionary and Com-
bilex transcriptions.
tral question we want to answer is: can we train
a system using this additional information which
performs better than the ‘baseline’ system that ig-
nores the extra information? Clearly, a system
with more information should not perform worse
than a system with less information (unless the ad-
ditional information is highly noisy), but it is a
priori not clear at all how the extra information
can be included, as Bhargava and Kondrak (2012)
note: output predictions may be in distinct alpha-
bets and/or follow different conventions, and sim-
ple rule-based conversions may even deteriorate
a baseline system’s performance. Their solution
to the problem is to let the baseline system out-
put its n-best phonetic transcriptions, and then to
re-rank these n-best predictions via an SVM re-
ranker trained on the supplemental representations
</bodyText>
<equation confidence="0.999348714285714">
x = schizo s ch i z o
ˆy(1) = skaIz@U s k aI z @U
ˆy(2) = saIz@U s - aI z @U
ˆy(3) = skIts@ s k I ts @
ˆy(4) = Sits@U S - i ts @U
ˆy(5) = skIts@ s k I ts @
y = skIts@U s k I ts @U
</equation>
<tableCaption confidence="0.81311075">
Table 3: Left: Input string x, predictions of 5
systems, and output string y. Right: A multiple
many-to-many alignment of (x, ˆy(1), . . . , ˆy(5), y).
Skips are marked by a dash (‘-’).
</tableCaption>
<bodyText confidence="0.999890625">
(see their figure 2). Our approach is much differ-
ent from this: we character (or substring) align
the supplemental information strings with the in-
put letter strings and then sequentially transduce
input character substrings as in the standard G2P
approach, but where the sequential transducer is
aware of the corresponding subsequences of the
supplemental information strings.
Our goals in the current work are first, in Sec-
tion 2, to formally introduce the multiple many-
to-many alignment problem, which, to our knowl-
edge, has not yet been formally considered, and
to indicate how it can be solved (by standard ex-
tensions of well-known DP recursions). Secondly,
we outline an ‘approximation algorithm’, also in
Section 2, with much better runtime complexity,
to solving the multiple many-to-many alignment
problem. This proceeds by optimally segmenting
individual strings to align under the global con-
straint that the number of segments must agree
across strings. Thirdly, we demonstrate exper-
imentally, in Section 5, that multiple many-to-
many alignments may be an extremely useful first
step in boosting the performance of a G2P model.
In particular, we show that by conjoining a base
system with additional systems very high perfor-
mance increases can be achieved. We also inves-
tigate the effects of using our introduced approxi-
mation algorithm instead of ‘exactly’ determining
alignments. We discuss related work in Section
3, present data and systems in Section 4 and con-
clude in Section 6.
</bodyText>
<sectionHeader confidence="0.889668" genericHeader="method">
2 Mult. Many-to-Many Alignm. Models
</sectionHeader>
<bodyText confidence="0.9997725">
We now formally define the problem of multiply
aligning several strings in a monotone and many-
to-many alignment manner. For notational conve-
nience, in this section, let the N strings to align be
</bodyText>
<page confidence="0.993173">
910
</page>
<bodyText confidence="0.99768725">
denoted by w1, ... , wN (rather than x, ˆy(m), y,
etc.). Let each wn, for 1 &lt; n &lt; N, be an arbitrary
string over some alphabet E(n). Let `n = |wn |de-
note the length of wn. Moreover, assume that a set
</bodyText>
<equation confidence="0.98358925">
S - IINn=1{0, ... , `n}\{0N} of allowable steps
is specified, where 0N = (0, . . . , 0 )
� Y � .1 We interpret
N times
</equation>
<bodyText confidence="0.999076857142857">
the elements of S as follows: if (s1, s2,... , sN) E
S, then subsequences of w1 of length s1, subse-
quences of w2 of length s2, ..., subsequences of
wN of length sN may be matched up with each
other. In other words, S defines the types of valid
‘many-to-many match-up operations’.2 While we
could drop S from consideration and simply al-
low every possible matching up of character sub-
sequences, it is convenient to introduce S because
algorithmic complexity may then be specified in
terms of S, and by choosing particular S, one may
retrieve special cases otherwise considered in the
literature (see next section).
As indicated, for us, a multiple alignment of
</bodyText>
<table confidence="0.536853333333333">
(w1, ... , wN) is any scheme
w1,1 w1,2 ··· w1,k
w2,1 w2,2 ··· w2,k
... ... .. ..
..
wN,1 wN,2 ··· wN,k
</table>
<bodyText confidence="0.997583">
such that (|w1,i|,..., |wN,i|) E S, for all i =
1, ... , k, and such that wn = wn,1 · · · wn,k, for
all 1 &lt; n &lt; N. Let AS = AS(w1, . . . , wN)
denote the set of all multiple alignments of
(w1, ... , wN). For an alignment a E AS, de-
note by score(a) = f(a) the score of align-
ment a under alignment model f, where f :
AS(w1, ... , wN) - R. We now investigate solu-
tions to the problem offinding the alignment with
maximal score under different choices of align-
ment models f, i.e., we search to efficiently solve
</bodyText>
<equation confidence="0.973627">
a∈AS(w1,...,wN) f(a). (1)
max
</equation>
<bodyText confidence="0.982499666666667">
Unigram alignment model For our first align-
ment model f, we assume that f(a), for a E AS,
is the score
</bodyText>
<equation confidence="0.94259225">
k
f(a) = sim1(w1,i, ... , wN,i) (2)
i=1
1Here, f j denotes the Cartesian product of sets.
</equation>
<footnote confidence="0.9969085">
2In the case of two strings, this is sometimes denoted in
the manner M-N (e.g., 3-2, 1-0), indicating that M charac-
ters of one string may be matched up with N characters of the
other string. Analogously, we could write here s1-s2-s3-· · · .
</footnote>
<equation confidence="0.676987">
for a real-valued similarity function sim1 :
IIN&apos;1 (E(
n= n))∗ R. We call the model f in
</equation>
<bodyText confidence="0.928887727272727">
(2) a unigram model because f(a) is the sum
of the similarity scores of the matched-up subse-
quences (w1,i, ... , wN,i), ignoring context. Due
to this independence assumption, solving max-
imization problem in Eq. (1) under specifica-
tion (2) is straightforward via a dynamic pro-
gramming (DP) recursion. To do so, define by
MS,sim1 (i1, i2, ... , iN) the score of the best align-
ment, under alignment model f = Esim1 and
set of steps S, of (w1(1 : i1), ... , wN(1 : iN)).3
Then, MS,sim1(i1, ... , iN) is equal to
</bodyText>
<equation confidence="0.999178333333333">
MS,sim1(i1 − j1, ... , iN − jN)
+ sim1 (w(i1 − j1 + 1 : i1), ... , w(iN − jN + 1 : jN)).
(3)
</equation>
<bodyText confidence="0.974130052631579">
This recurrence directly leads to a DP algorithm,
shown in Algorithm 1, for computing the score
of the best alignment of (w1, ... , wN); the ac-
tual alignment can be found by storing pointers to
the maximizing steps taken. If similarity evalua-
tions sim1(w1,i, . . . , wN,i) are thought of as tak-
ing constant time, this algorithm’s run time is
O(IINn=1 `n · |S|). When ` = `1 = ··· = `n and
|S |= `N - 1 (‘worst case’ size of S), then the al-
gorithm’s runtime is thus O(`2N), which quickly
becomes untractable as N, the number of strings
to align, increases.
Of course, the unigram alignment model could
be generalized to an m-gram alignment model.
An m-gram alignment model would exhibit worst-
case runtime complexity of O(`(m+1)N) under
analogous DP recursions as for the unigram
model.
Algorithm 1
</bodyText>
<listItem confidence="0.992771636363636">
1: procedure UNIGRAM-ALIGN(w1, ... , wN;
S, sim1)
2: M(i1, ... , iN) -oc for all
(i1, ... , iN) E ZN
3: M(0N) 0
4: for i1 = 0 ... `1 do
5: for ··· do
6: for iN = 0 ... `N do
7: if (i1, ... , iN) =� 0N then
8: M(i1, ... , iN) - Eq. (3)
9: return M(`1, . . . , `N)
</listItem>
<bodyText confidence="0.8613165">
Separable alignment models For our sec-
ond model class, assume that, for any a E
3We denote by x(a : b) the substring xaxa+1 · · · xb of
the string x1x2 · · · xt.
</bodyText>
<equation confidence="0.817826">
max
(j1,...,jN )∈S
911
AS(w1, ... , wN), f(a) decomposes into
( )
f(a) = Ψ fw1(w1,1 ··· w1,k), ..., fwN (wN,1 ··· wN,k)
(4)
</equation>
<bodyText confidence="0.997392520833333">
for some models fw1, ... , fwN and where Ψ :
RN → R is non-decreasing in its arguments (e.g.,
Ψ(fw1, . . . ,fwN) = ENn=1 fwn). If f(a) decom-
poses in such a manner, then f(a) is called sep-
arable.4 The advantage with separable models is
that we can solve the ‘subproblems’ fw1, ... , fwN
independently. Thus, in order to find optimal
multiple alignments of (w1, ... , wN) under such
a specification, we would only have to find the
best segmentations of sequences wn under mod-
els fwn, for 1 &lt; n &lt; N, subject to the constraint
that the segmentations must agree in their number
of segments (the coupling variable). Let Swn ⊆
{0, 1, ... , `n} denote the constraints on segment
lengths, similar to the interpretation of steps in
S. If fwn is a unigram segmentation model then
the problem of finding the best segmentation of
wn with exactly j segments can be solved in time
O(`n |Swn |j). Thus, if each fwn is a unigram
segmentation model, worst-case time complexity
for each subproblem would be O(`3n) (if string
wn can be segmented into at most `n segments)
and then the overall problem (1) under specifica-
tion (4) is solvable in worst-case time N · O(`3).
More generally, if each fwn is an m-gram seg-
mentation model, then worst-case time complexity
amounts to N · O(`m+2). Importantly, this scales
linearly with the number N of strings to align,
rather than exponentially as the O(`(m+1)N) un-
der the (non-separable) m-gram alignment model
discussed above.
Unsupervised alignments The algorithms pre-
sented may be applied iteratively in order to in-
duce multiple alignments in an unsupervised (EM-
like) fashion in which sim1 is gradually learnt
(e.g., starting from a uniform initialization of
sim1). We skip details of this, as we do not make
us of it in our current experiments. Rather, in our
experiments below, we directly specify sim1 as a
sum of pairwise similarity scores which we ex-
tract from alignments produced by an off-the-shelf
pairwise aligner.
4Note the difference between Eqs. (2) and (4). While each
fwn in (4) operates on a ‘row’ of an alignment scheme, sim1
in (2) acts on the ‘columns’. In other words, the unigram
alignment model correlates the multiply matched-up subse-
quences, while the separable alignment model assumes inde-
pendence here.
</bodyText>
<sectionHeader confidence="0.999749" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.999445217391305">
Monotone alignments have a long tradition, both
in NLP and bioinformatics. The classical
Needleman-Wunsch algorithm (Needleman and
Wunsch, 1970) computes the optimal alignment
between two sequences when only single charac-
ter matches, mismatches, and skips are allowed.
It is a special case of the unigram model (2)
in optimization problem (1) for which N = 2,
S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on val-
ues from {0, −1}, depending on whether com-
pared input subsequences match or not. As is
well-known, this alignment specification is equiv-
alent to the edit distance problem (Levenshtein,
1966) in which the minimal number of inser-
tions, deletions and substitutions is sought that
transforms one string into another. Substring-
to-substring edit operations — or equivalently,
(monotone) many-to-many alignments — have ap-
peared in the NLP context, e.g., in (Deligne et
al., 1995), (Brill and Moore, 2000), (Jiampoja-
marn et al., 2007), (Bisani and Ney, 2008), (Ji-
ampojamarn et al., 2010), or, significantly earlier,
in (Ukkonen, 1985), (V´eronis, 1988). Learning
edit distance/monotone alignments in an unsuper-
vised manner has been the topic of, e.g., (Ris-
tad and Yianilos, 1998), (Cotterell et al., 2014),
besides the works already mentioned. All of
these approaches are special cases of our uni-
gram model outlined in Section 2 — i.e., they
consider particular S (most prominently, S =
{(1, 0), (0, 1), (1, 1)}) and/or restrict attention to
only N = 2 strings.5
Alignments between multiple sequences, i.e.,
multiple sequence alignment, has also been an is-
sue both in NLP (e.g., Covington (1998), Bhar-
gava and Kondrak (2009)) and bioinformatics
(e.g., Durbin et al. (1998)). An interesting applica-
tion of alignments of multiple sequences is to de-
termine what has been called median string (Ko-
honen, 1985) or Steiner consensus string (Gus-
field, 1997), defined as the string s¯ that minimizes
the sum of distances, for a given distance function
d(x, y), to a list of strings s1, ... , sN (Jiang et al.,
2012); typically, d is the standard edit distance.
As Gusfield (1997) shows, the Steiner consen-
sus string may be retrieved from a multiple align-
</bodyText>
<footnote confidence="0.710178333333333">
5In Cotterell et al. (2014), context influences alignments,
so that the approach goes beyond the unigram model sketched
in (2), but there, too, the focus is on the situation N = 2 and
</footnote>
<equation confidence="0.51701">
S = {(1, 0), (0, 1), (1, 1)}.
</equation>
<page confidence="0.975089">
912
</page>
<bodyText confidence="0.999946448979592">
ment of s1, ... , sN by concatenating the column-
wise majority characters in the alignment, ignor-
ing skips. Since median string computation (and
hence also the multiple many-to-many alignment
problem, as we consider) is an NP-hard problem
(Sim and Park, 2003), designing approximations is
an active field of research. For example, Marti and
Bunke (2001) ignore part of the search space by
declaring matches-up of distant characters as un-
likely, and Jiang et al. (2012) apply an approxima-
tion based on string embeddings in vector spaces.
Paul and Eisner (2012) apply dual decomposition
to compute Steiner consensus strings. Via the ap-
proach taken in this paper, median strings may be
computed in case d is a (distance) function tak-
ing substring-to-substring edit operations into ac-
count, a seemingly straightforward, yet extremely
useful generalization in several NLP applications,
as indicated in the introduction.
Our approach may also be seen in the context of
classifier combination for string-valued variables.
While ensemble methods for structured prediction
have been considered in several works (see, e.g.,
Nguyen and Guo (2007), Cortes et al. (2014), and
references therein), a typical assumption in this
situation is that the sequences to be combined have
equal length, which clearly cannot be expected
to hold when, e.g., the outputs of several G2P,
transliteration, etc., systems must be combined. In
fact, the multiple many-to-many alignment models
investigated in this work could act as a preprocess-
ing step in this setup, since the alignment precisely
serves the functionality of segmenting the strings
into equal number of segments/substructures. Of
course, combining outputs with varying number
of elements is also an issue in machine transla-
tion (e.g., Macherey and Och (2007), Heafield et
al. (2009)), but, there, the problem is harder due to
the potential non-monotonicities in the ordering of
elements, which typically necessitates (additional)
heuristics. One approach for constructing multi-
ple alignments is here progressive multiple align-
ment (Feng and Doolittle, 1987) in which a multi-
ple (typically one-to-one) alignment is iteratively
constructed from successive pairwise alignments
(Bangalore et al., 2001). Matusov et al. (2006)
apply word reordering and subsequent pairwise
monotone one-to-one alignments for MT system
combination.
</bodyText>
<sectionHeader confidence="0.94446" genericHeader="method">
4 Data and systems
</sectionHeader>
<subsectionHeader confidence="0.965357">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999912125">
We conduct experiments on the General Ameri-
can (GA) variant of the Combilex data set (Rich-
mond et al., 2009). This contains about 144,000
grapheme-phoneme pairs as exemplarily illus-
trated in Table 2. In our experiments, we split
the data into two disjoint parts, one for test-
ing (about 28,000 word pairs) and one for train-
ing/development (the remainder).
</bodyText>
<subsectionHeader confidence="0.977691">
4.2 Systems
</subsectionHeader>
<bodyText confidence="0.9998791875">
BASELINE Our baseline system is a linear-chain
conditional random field model (CRF)6 (Lafferty
et al., 2001) which we apply in the manner in-
dicated in the introduction: after many-to-many
aligning the training data as in Table 1, at training
time, we use the CRF as a tagging model that is
trained to label each input character subsequence
with an output character subsequence. As fea-
tures for the CRF, we use all n-grams of subse-
quences of x that fit inside a window of size 5
centered around the current subsequence (context
features). We also include linear-chain features
which allow previously generated output character
subsequences to influence current output charac-
ter subsequences. In essence, our baseline model
is a standard discriminative approach to G2P. It is,
all in all, the same approach as described in Ji-
ampojamarn et al. (2010), except that we do not
include joint n-gram features. At test time, we first
segment a new input string x and then apply the
CRF. Thereby, we train the segmentation module
on the segmented x sequences, as available from
the aligned training data.7
BASELINE+X As competitors for the base-
line system, we introduce systems that rely on
the predictions of one or several additional (black
box/offline) systems. At training time, we first
multiply many-to-many align the input string x,
the predictions ˆy(1), ... , ˆy(M) and the true tran-
scription y as illustrated in Table 3 (see Section
4.3 for details). Then, as for the baseline sys-
tem, we train a CRF to label each input character
</bodyText>
<footnote confidence="0.975056875">
6We made use of the CRF++ package available at
https://code.google.com/p/crfpp/.
7To be more precise on the training of the segmentation
module, in an alignment as in Table 1, we consider the seg-
mented x string — ph-oe-n-i-x — and then encode this seg-
mentation in a binary string where 1’s indicate splits. Thus,
segmentation becomes, again, a sequence labling task; see,
e.g., Bartlett et al. (2008) or Eger (2013) for details.
</footnote>
<page confidence="0.997173">
913
</page>
<bodyText confidence="0.999615">
subsequence with the corresponding output char-
acter subsequence. However, this time, the CRF
has access to the subsequence suggestions (as the
alignments indicate) produced by the offline sys-
tems. As features for the extended models, we ad-
ditionally include context features for all predicted
strings ˆy(m) (all n-grams in a window of size 3
centered around the current subsequence predic-
tion). We also include a joint feature firing on
the tuple of the current subsequence value of x,
ˆy(1), . . . ,ˆy(M). To illustrate, when BASELINE+X
tags position 2 in the (split up) input string in Ta-
ble 3, it sees that its value is ch, that the previous
input position contains s, that the next contains
i, that the next two contain (i,z), that the predic-
tion of the first system at position 2 is k, that the
first system’s next prediction is ai, and so forth.
At test time, we first multiply many-to-many align
x, ˆy(1), ... , ˆy(M), and then apply the enhanced
CRF.
</bodyText>
<subsectionHeader confidence="0.98544">
4.3 Alignments
</subsectionHeader>
<bodyText confidence="0.9225194">
To induce multiple monotone many-to-many
alignments of input strings, offline system predic-
tions and output strings, we proceed in one of two
manners.
Exact alignments Firstly, we specify sim1 in
</bodyText>
<equation confidence="0.901719">
(
Eq. (2), as sim1(xi, ˆyi1) ,. . . , ˆy(M)
i , yi) =
(( M psi m(xi, + psim(xi, yi),
M
m=1
</equation>
<bodyText confidence="0.999238177777778">
where psim is a pair-similarity function. The ad-
vantage with this specification is that the similarity
of a tuple of subsequences is defined as the sum of
pairwise similarity scores, which we can directly
estimate from pairwise alignments of (x, ˆy(m))
that an off-the-shelf pairwise aligner can produce
(we use the Phonetisaurus aligner for this). We set
psim(u, v) as log-probability of observing the tu-
ple (u, v) in the training data of pairwise aligned
sequences. To illustrate, we define the similar-
ity of (o,@U,@U,@,@U,@,@U) in the example in Table
3 as the pairwise similarity of (o,@U) (as inferred
from pairwise alignments of x strings and sys-
tem 1 transcriptions) plus the pairwise similarity
of (o,@U) (as inferred from pairwise alignments of
x strings and system 2 transcriptions), etc. At test
time, we use the same procedure but drop the term
psim(xi, yi) when inducing alignments. For our
current purposes, we label the outlined modus as
exact (alignment) modus.
Approx. alignments Secondly, we derive the
optimal multiple many-to-many alignment of the
strings in question by choosing an alignment that
satisfies the condition that (1) each individual
string x, ˆy(1), ... , ˆy(M), y is optimally segmented
(e.g., ph-oe-n-i-x rather than pho-eni-x, f-i-n-I-ks
rather than f-inIk-s) subject to the global constraint
that (2) the number of segments must agree across
the strings to align. This constitutes a separa-
ble alignment model as discussed in Section 2,
and thus has much lower runtime complexity as
the first model. Segmentation models can be di-
rectly learned from the pairwise alignments that
Phonetisaurus produces by focusing on either the
segmented x or y/ˆy(m) sequences; we choose to
implement bigram individual segmentation mod-
els. This second model type may be considered an
approximation of the first, since in a good align-
ment, we would not only expect individually good
segmentations and agreement of segment numbers
but also that subsegments are likely correlations
of each other, precisely as our first model type
captures. Therefore, we shall call this alignment
modus approximate (alignment) modus, for our
present purposes.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999942863636364">
We now describe two sets of experiments, a con-
trolled experiment on the Combilex data set
where we can design our offline/black box sys-
tems ourselves and where the black box systems
are trained on a similar distribution as the base-
line and the extended baseline systems. In partic-
ular, the black box systems operate on the same
output alphabet as the extended baseline systems,
which constitutes an ‘ideal’ situation. Thereafter,
we investigate how our extended baseline system
performs in a ‘real-world’ scenario: we train a
system on Combilex that has as supplemental in-
formation corresponding Wiktionary (and PTE, as
explained below) transcriptions.
Throughout, we use as accuracy measures for
all our systems word accuray (WACC). Word ac-
curacy is defined as the number of correctly tran-
scribed strings among all transcribed strings in a
test sample. WACC is a strict measure that penal-
izes even tiny deviations from the gold-standard
transcriptions, but has nowadays become standard
in G2P.
</bodyText>
<page confidence="0.99563">
914
</page>
<subsectionHeader confidence="0.953054">
5.1 A controlled experiment
</subsectionHeader>
<bodyText confidence="0.999652777777778">
In our first set of experiments, we let our of-
fline/black box systems be the Sequitur G2P mod-
eling toolkit (Bisani and Ney, 2008) (S) and
the Phonetisaurus modeling toolkit (Novak et
al., 2012) (P). We train them on disjoint sets
of 20,000 grapheme-to-phoneme Combilex string
pairs each. The performance of these two sys-
tems, on the test set of size 28,000, is indicated
in Table 4. Next, we train BASELINE on dis-
</bodyText>
<table confidence="0.973945">
Phonetisaurus Sequitur
WACC 72.12 71.70
</table>
<tableCaption confidence="0.8801375">
Table 4: Word-accuracy (in %) on the test data, for
the two systems indicated.
</tableCaption>
<bodyText confidence="0.9993391">
joint sets (disjoint from both the training sets of
P and S) of size 2,000, 5,000, 10,000 and 20,000.
Making BASELINE’s training sets disjoint from
the training sets of the offline systems is both re-
alistic (since a black box system would typically
follow a partially distinct distribution from one’s
own training set distribution) and also prevents
the extended baseline systems from fully adapting
to the predictions of either P or S, whose train-
ing set accuracy is an upward biased representa-
tion of their true accuracy. As baseline extensions,
we consider the systems BASELINE+P (+P), and
BASELINE+P+S (+P+S).8
Results are shown in Figures 1 and 2. We
see that conjoining the base system with the
predictions of the offline Phonetisaurus and Se-
quitur models substantially increases the base-
line WACC, especially in the case of little train-
ing data. In fact, WACC increases here by al-
most 100% when the baseline system is comple-
mented by ˆy(P) and ˆy(S). As training set size
increases, differences become less and less pro-
nounced. Eventually, we would expect them to
drop to zero, since beyond some training set size,
the additional features may provide no new infor-
mation.9 We also note that conjoining the two sys-
tems is more valuable than conjoining only one
system, and, in Figure 2, that the models which are
based on exact multiple alignments outperform the
models based on approximate alignments, but not
</bodyText>
<footnote confidence="0.9931676">
8We omit BASELINE+S since it yielded similar results as
BASELINE+P.
9In fact, in follow-up work, we find that the additional
information may also confuse the base system when training
set sizes are large enough.
</footnote>
<figureCaption confidence="0.999927333333333">
Figure 1: WACC as a function of training set size
for the system indicated. Exact align. modus.
Figure 2: Comparison of models based on exact
</figureCaption>
<bodyText confidence="0.8244585">
and approximate alignments; WACC as a function
of training set size. APRX denotes the approxima-
tion alignment model.
Concerning differences in alignments between
the two alignment types, exact vs. approximate, an
illustrative example where the approximate model
fails and the exact model does not is (‘false’ align-
ment based on the approximate model indicated):
</bodyText>
<equation confidence="0.903842">
r ee n t e r e d
r i E n t @‘ r d
r i E n t @‘ r d
</equation>
<bodyText confidence="0.9996576">
which nicely captures the inability of the approx-
imate model to account for correlations between
the matched-up subsequences. That is, while the
segmentations of the three shown sequences ap-
pear acceptable, a matching of graphemic t with
</bodyText>
<figure confidence="0.995339657142857">
by a wide margin.
0 5T 10T 20T 30T
Training set size
BASELINE
+P
+P+S
0.8
0.75
0.7
0.65
Accuracy
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0 5T 10T 20T 30T
Training set size
+P
+P+S
+PAPRX
+P+SAPRX
0.76
0.75
0.74
Accuracy
0.73
0.72
0.71
0.7
0.69
0.68
0.67
</figure>
<page confidence="0.997076">
915
</page>
<bodyText confidence="0.999855214285714">
phonemic n, etc., seems quite unlikely. Still, it
is very promising to see that these differences in
alignment quality translate into very small differ-
ences in overall string-to-string translation model
performance, as Figure 2 outlines. Namely, dif-
ferences in WACC are typically on the level of
1% or less (always in favor of the exact alignment
model). This is a very important finding, as it in-
dicates that string-to-string translation need not be
(severely) negatively impacted by switching to the
approximate alignment model, a tractable alterna-
tive to the exact models, which quickly become
practically infeasible as the number of strings to
align increases.
</bodyText>
<subsectionHeader confidence="0.991255">
5.2 Real-world experiments
</subsectionHeader>
<bodyText confidence="0.999841736842105">
To test whether our approach may also succeed in
a ‘real-world setting’, we use as offline/black box
systems GA Wiktionary transcriptions of our in-
put forms as well as PhotoTransEdit (PTE) tran-
scriptions,10 a lexicon-based G2P system which
offers both GA and RP (received pronunciation)
transcription of English strings. We train and test
on input strings for which both Combilex and PTE
transcriptions are available, and for which both
Combilex and Wiktionary transcriptions are avail-
able.11 Test set sizes are about 1,500 in the case of
PTE and 3,500 in the case of Wiktionary. We only
test here the performance of the exact alignment
method, noting that, as before, approximate align-
ments produced slightly weaker results.
Clearly, Wiktionary and PTE differ from the
Combilex data. First, both Wiktionary and PTE
use different numbers of phonemic symbols than
Combilex, as Table 5 illustrates. Some differences
</bodyText>
<table confidence="0.998087166666667">
Dataset |Σ|
Combilex 54
WiktionaryGA 107
WiktionaryRP 116
PTEGA 44
PTERP 57
</table>
<tableCaption confidence="0.8410515">
Table 5: Sizes of phonetic inventaries of different
data sets.
</tableCaption>
<bodyText confidence="0.9637135">
arise from the fact that, e.g., lengthening of vowels
is indicated by two output letters in some data sets
</bodyText>
<footnote confidence="0.9086662">
10Downloadable from http://www.photransedit.com/.
11This yields a clear method of comparison. An alternative
would be to provide predictions for missing transcriptions. In
any case, by our task definition, all systems must provide a
hypothesis for an input string.
</footnote>
<bodyText confidence="0.998763928571429">
and only one in others. Also, phonemic transcrip-
tion conventions differ, as becomes most strikingly
evident in the case of RP vs. GA transcriptions —
Table 6 illustrates. Finally, Wiktionary has many
more phonetic symbols than the other datasets, a
finding that we attribute to its crowd-sourced na-
ture and lacking of normalization. Despite these
differences in phonemic annotation standards be-
tween Combilex, Wiktionary and PTE, we observe
that conjoining input strings with predicted Wik-
tionary or PTE transcriptions via multiple align-
ments leads to very good improvements in WACC
over only using the input string as information
source. Indeed, as shown in Table 7, for PTE,
WACC increases by as much as 80% in case of
small training sample (1,099 string pairs) and as
much as 37% in case of medium-sized training
sample (2,687 string pairs). Thus, comparing with
the previous situation of homogenous systems, we
also observe that the gain from including hetero-
geneous system is relatively weaker, as we would
expect due to distinct underlying assumptions, but
still impressive. Performance increases when in-
cluding Wiktionary are slightly lower, most likely
because it constitutes a very heterogenous source
of phonetic transcriptions with user-idiosyncratic
annotations (however, training set sizes are also
different).12
</bodyText>
<table confidence="0.997869857142857">
BASEL. BASEL.+PTEGA BASEL.+PTERP
1,099 31.34 56.47 50.22
2,687 45.75 60.80 62.80
BASEL. BASEL.+WikGA BASEL.+WikRP
2,000 38.44 60.71 62.18
5,000 51.69 65.81 65.96
10,000 58.97 67.30 68.66
</table>
<tableCaption confidence="0.54661825">
Table 7: Top: WACC in % for baseline CRF
model and the models that integrate PTE in the
GA versions and RP versions, respectively. Bot-
tom: BASELINE and BASELINE+Wiktionary.
</tableCaption>
<sectionHeader confidence="0.98951" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9972227">
We have generalized the task description of string
transduction to include supplemental information
strings. Moreover, we have suggested multiple
12To provide, for the interested reader, a comparison with
Phonetisaurus and Sequitur: for the Wiktionary GA data,
performance of Phonetisaurus is 41.80% (training set size
2,000), 55.70% (5,000) and 62.47% (10,000). Respective
numbers for Sequitur are 40.58%, 54.84%, and 61.58%. On
PTE, results are, similarly, slightly higher than our baseline,
but substantially lower than the extended baseline.
</bodyText>
<page confidence="0.992336">
916
</page>
<figure confidence="0.995564">
b o t ch i ng b a rr ed a s th m a t i c s
b o t f I N b a - d æ s - m æ t I k s
b A - tS I N b A r d a z 0 m a t I k s
</figure>
<tableCaption confidence="0.593343">
Table 6: Multiple alignments of input string, predicted PTE transcription and true (Combilex) transcrip-
</tableCaption>
<bodyText confidence="0.999260304347826">
tion. Differences may be due to alternative phonemic conventions (e.g., Combilex has a single phonemic
character representing the sound tf) and/or due to differences in pronunciation in GA and RP, resp.
many-to-many alignments — and a subsequent
standardly extended discriminative approach —
for solving string transduction (here, G2P) in this
generalized setup. We have shown that, in a real-
world setting, our approach may significantly beat
a standard discriminative baseline, e.g., when we
add Wiktionary transcriptions or predictions of
a rule-based system as additional information to
the input strings. The appeal of this approach
lies in the fact that almost any sort of external
knowledge source may be integrated to improve
the performance of a baseline system. For exam-
ple, supplemental information strings may appear
in the form of transliterations of an input string
in other languages; they may be predictions of
other G2P systems, whether carefully manually
crafted or learnt from data; they might even ap-
pear in the form of phonetic transcriptions of the
input string in other dialects or languages. What
distinguishes our solution to integrating supple-
mental information strings in string transduction
settings from other research (e.g., (Bhargava and
Kondrak, 2011; Bhargava and Kondrak, 2012)) is
that rather than integrating systems on the global
level of strings, we integrate them on the lo-
cal level of smaller units, namely, substrings ap-
propriated to the domain of application (e.g., in
our context, phonemes/grapheme substructures).
Both approaches may be considered complemen-
tary. Finally, another important contribution of our
work is to outline an ‘approximation algorithm’
to inducing multiple many-to-many alignments of
strings, which is otherwise an NP-hard problem
for which (most likely) no efficient exact solu-
tions exist, and to investigate its suitability for the
problem task. In particular, we have seen that ex-
act alignments lead to better overall model perfor-
mance, but that the margin over the approximation
is not wide.
The scope for future research of our modeling is
huge: multiple many-to-many alignments may be
useful in aligning cognates in linguistic research;
they may be the first necessary step for many other
ensemble techniques in string transduction as we
have considered (Cortes et al., 2014), and they
may allow, on a large scale, to boost G2P (translit-
eration, lemmatization, etc.) systems by inte-
grating them with many traditional (or modern)
knowledge resources such as rule- and dictionary-
based lemmatizers, crowd-sourced phonetic tran-
scriptions (e.g., based on Wiktionary), etc., with
the outlook of significantly outperforming current
state-of-the-art models which are based solely on
input string information.
Finally, we note that we have thus far shown
that supplemental information strings may be ben-
eficial in case of overall little training data and that
improvements decrease with data size. Further in-
vestigating this relationship will be of importance.
Morevoer, it will be insightful to compare the
exact and approximate alignment algorithms pre-
sented here with other (heuristic) alignment meth-
ods, such as iterative pairwise alignments as em-
ployed in machine translation, and to investigate
how alignment quality of multiple strings impacts
overall G2P performance in the setup of additional
information strings.
</bodyText>
<sectionHeader confidence="0.997774" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99198952631579">
S. Bangalore, G. Bodel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In In Proceedings of IEEE
Automatic Speech Recognition and Understanding
Workshop (ASRU-2001, pages 351–354.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2008. Automatic syllabification with structured
svms for letter-to-phoneme conversion. In Kath-
leen McKeown, Johanna D. Moore, Simone Teufel,
James Allan, and Sadaoki Furui, editors, ACL, pages
568–576. The Association for Computer Linguis-
tics.
Aditya Bhargava and Grzegorz Kondrak. 2009. Mul-
tiple word alignment with Profile Hidden Markov
Models. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Student Re-
search Workshop and Doctoral Consortium, pages
</reference>
<page confidence="0.987595">
917
</page>
<reference confidence="0.999850733944955">
43–48, Boulder, Colorado, June. Association for
Computational Linguistics.
Aditya Bhargava and Grzegorz Kondrak. 2011. How
do you pronounce your name?: Improving g2p with
transliterations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ’11, pages 399–408, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Aditya Bhargava and Grzegorz Kondrak. 2012. Lever-
aging supplemental representations for sequential
transduction. In HLT-NAACL, pages 396–406. The
Association for Computational Linguistics.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434–451.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction.
In Proceedings of the 38th Annual Meeting on As-
sociation for Computational Linguistics, ACL ’00,
pages 286–293, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Corinna Cortes, Vitaly Kuznetsov, and Mehryar Mohri.
2014. Ensemble methods for structured prediction.
In Proceedings of the 31th International Conference
on Machine Learning, ICML 2014, Beijing, China,
21-26 June 2014, pages 1134–1142.
Ryan Cotterell, Nanyun Peng, and Jason Eisner. 2014.
Stochastic contextual edit distance and probabilis-
tic FSTs. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Baltimore, June. 6 pages.
Michael A. Covington. 1998. Alignment of multiple
languages for historical comparison. In Proceedings
of the 36th Annual Meeting of the Association for
Computational Linguistics and 17th International
Conference on Computational Linguistics, Volume
1, pages 275–279, Montreal, Quebec, Canada, Au-
gust. Association for Computational Linguistics.
Sabine Deligne, Franois Yvon, and Fr´ed´eric Bimbot.
1995. Variable-length sequence matching for pho-
netic transcription using joint multigrams. In EU-
ROSPEECH. ISCA.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In EMNLP, pages 1080–
1089. ACL.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Steffen Eger. 2013. Sequence segmentation by enu-
meration: An exploration. Prague Bull. Math. Lin-
guistics, 100:113–132.
D. F. Feng and R. F. Doolittle. 1987. Progressive se-
quence alignment as a prerequisite to correct phy-
logenetic trees. Journal of molecular evolution,
25(4):351–360.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences - Computer Science and Computational
Biology. Cambridge University Press.
Kenneth Heafield, Greg Hanneman, and Alon Lavie.
2009. Machine translation system combination
with flexible word ordering. In Proceedings of the
EACL 2009 Fourth Workshop on Statistical Machine
Translation, pages 56–60, Athens, Greece, March.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme
conversion. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 372–
379, Rochester, New York, April. Association for
Computational Linguistics.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Pro-
ceedings of ACL-08: HLT, pages 905–913, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2010. Integrating joint n-gram features
into a discriminative training framework. In HLT-
NAACL, pages 697–700. The Association for Com-
putational Linguistics.
Xiaoyi Jiang, Jran Wentker, and Miquel Ferrer. 2012.
Generalized median string computation by means of
string embedding in vector spaces. Pattern Recog-
nition Letters, 33(7):842–852.
T. Kohonen. 1985. Median strings. Pattern Recogni-
tion Letters, 3:309–313.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282–289. Morgan Kauf-
mann, San Francisco, CA.
VI Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. Soviet
Physics Doklady, 10:707.
Wolfgang Macherey and Franz Josef Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems. In
EMNLP-CoNLL, pages 986–995. ACL.
Urs-Viktor Marti and Horst Bunke. 2001. Use of posi-
tional information in sequence alignment for multi-
ple classifier combination. In Josef Kittler and Fabio
Roli, editors, Multiple Classifier Systems, volume
</reference>
<page confidence="0.978971">
918
</page>
<reference confidence="0.99977584">
2096 of Lecture Notes in Computer Science, pages
388–398. Springer.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33–40, Trento, Italy, April.
Saul B. Needleman and Christian D. Wunsch. 1970.
A general method applicable to the search for sim-
ilarities in the amino acid sequence of two pro-
teins. Journal of Molecular Biology, 48(3):443–
453, March.
Nam Nguyen and Yunsong Guo. 2007. Comparisons
of sequence labeling algorithms and extensions. In
Zoubin Ghahramani, editor, ICML, volume 227 of
ACM International Conference Proceeding Series,
pages 681–688. ACM.
Josef R. Novak, Nobuaki Minematsu, and Keikichi Hi-
rose. 2012. WFST-based grapheme-to-phoneme
conversion: Open source tools for alignment,
model-building and decoding. In Proceedings of the
10th International Workshop on Finite State Meth-
ods and Natural Language Processing, pages 45–49,
Donostia–San Sebastin, July. Association for Com-
putational Linguistics.
Michael J. Paul and Jason Eisner. 2012. Implicitly in-
tersecting weighted automata using dual decompo-
sition. In HLT-NAACL, pages 232–242. The Associ-
ation for Computational Linguistics.
Korin Richmond, Robert A. J. Clark, and Susan Fitt.
2009. Robust LTS rules with the Combilex speech
technology lexicon. In INTERSPEECH, pages
1295–1298. ISCA.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Trans. Pattern Anal.
Mach. Intell., 20(5):522–532.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In John A. Carroll, Antal
van den Bosch, and Annie Zaenen, editors, ACL.
The Association for Computational Linguistics.
Jeong Seop Sim and Kunsoo Park. 2003. The consen-
sus string problem for a metric is np-complete. J. of
Discrete Algorithms, 1(1):111–117, February.
Esko Ukkonen. 1985. Algorithms for approximate
string matching. Information and Control, 64:100–
118.
Jean V´eronis. 1988. Computerized correction of
phonographic errors. Computers and the Humani-
ties, 22(1):43–56.
</reference>
<page confidence="0.998774">
919
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.409261">
<title confidence="0.998551">Multiple Many-to-Many Sequence Alignment for String-Valued Variables: A G2P Experiment</title>
<author confidence="0.990491">Steffen</author>
<affiliation confidence="0.7824">Text Technology Goethe University Frankfurt am Frankfurt am Main,</affiliation>
<email confidence="0.987836">steeger@em.uni-frankfurt.de</email>
<abstract confidence="0.98679">investigate many-to-many a primary step in integrating supplemental information strings in string transduction. Besides outlining DP based solutions to the multiple alignment problem, we detail an approximation of the problem in terms of multiple sequence segmentations satisfying a coupling constraint. We apply our approach to boosting baseline G2P systems using homogeneous as well as heterogeneous sources of supplemental information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Bodel</author>
<author>G Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems.</title>
<date>2001</date>
<booktitle>In In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU-2001,</booktitle>
<pages>351--354</pages>
<contexts>
<context position="18958" citStr="Bangalore et al., 2001" startWordPosition="3275" endWordPosition="3278">trings into equal number of segments/substructures. Of course, combining outputs with varying number of elements is also an issue in machine translation (e.g., Macherey and Och (2007), Heafield et al. (2009)), but, there, the problem is harder due to the potential non-monotonicities in the ordering of elements, which typically necessitates (additional) heuristics. One approach for constructing multiple alignments is here progressive multiple alignment (Feng and Doolittle, 1987) in which a multiple (typically one-to-one) alignment is iteratively constructed from successive pairwise alignments (Bangalore et al., 2001). Matusov et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 4 Data and systems 4.1 Data We conduct experiments on the General American (GA) variant of the Combilex data set (Richmond et al., 2009). This contains about 144,000 grapheme-phoneme pairs as exemplarily illustrated in Table 2. In our experiments, we split the data into two disjoint parts, one for testing (about 28,000 word pairs) and one for training/development (the remainder). 4.2 Systems BASELINE Our baseline system is a linear-chain conditional random field model</context>
</contexts>
<marker>Bangalore, Bodel, Riccardi, 2001</marker>
<rawString>S. Bangalore, G. Bodel, and G. Riccardi. 2001. Computing consensus translation from multiple machine translation systems. In In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU-2001, pages 351–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Bartlett</author>
<author>Grzegorz Kondrak</author>
<author>Colin Cherry</author>
</authors>
<title>Automatic syllabification with structured svms for letter-to-phoneme conversion.</title>
<date>2008</date>
<pages>568--576</pages>
<editor>In Kathleen McKeown, Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, editors, ACL,</editor>
<publisher>The Association for Computer Linguistics.</publisher>
<contexts>
<context position="21411" citStr="Bartlett et al. (2008)" startWordPosition="3681" endWordPosition="3684">n the input string x, the predictions ˆy(1), ... , ˆy(M) and the true transcription y as illustrated in Table 3 (see Section 4.3 for details). Then, as for the baseline system, we train a CRF to label each input character 6We made use of the CRF++ package available at https://code.google.com/p/crfpp/. 7To be more precise on the training of the segmentation module, in an alignment as in Table 1, we consider the segmented x string — ph-oe-n-i-x — and then encode this segmentation in a binary string where 1’s indicate splits. Thus, segmentation becomes, again, a sequence labling task; see, e.g., Bartlett et al. (2008) or Eger (2013) for details. 913 subsequence with the corresponding output character subsequence. However, this time, the CRF has access to the subsequence suggestions (as the alignments indicate) produced by the offline systems. As features for the extended models, we additionally include context features for all predicted strings ˆy(m) (all n-grams in a window of size 3 centered around the current subsequence prediction). We also include a joint feature firing on the tuple of the current subsequence value of x, ˆy(1), . . . ,ˆy(M). To illustrate, when BASELINE+X tags position 2 in the (split</context>
</contexts>
<marker>Bartlett, Kondrak, Cherry, 2008</marker>
<rawString>Susan Bartlett, Grzegorz Kondrak, and Colin Cherry. 2008. Automatic syllabification with structured svms for letter-to-phoneme conversion. In Kathleen McKeown, Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, editors, ACL, pages 568–576. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aditya Bhargava</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Multiple word alignment with Profile Hidden Markov Models.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium,</booktitle>
<pages>43--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="15987" citStr="Bhargava and Kondrak (2009)" startWordPosition="2803" endWordPosition="2807">ificantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called median string (Kohonen, 1985) or Steiner consensus string (Gusfield, 1997), defined as the string s¯ that minimizes the sum of distances, for a given distance function d(x, y), to a list of strings s1, ... , sN (Jiang et al., 2012); typically, d is the standard edit distance. As Gusfield (1997) shows, the Steiner consensus string may be retrieved from a multiple align5In Cotterell et al. (2014), context influences alignments, so that the approac</context>
</contexts>
<marker>Bhargava, Kondrak, 2009</marker>
<rawString>Aditya Bhargava and Grzegorz Kondrak. 2009. Multiple word alignment with Profile Hidden Markov Models. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium, pages 43–48, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Bhargava</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>How do you pronounce your name?: Improving g2p with transliterations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>399--408</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="34979" citStr="Bhargava and Kondrak, 2011" startWordPosition="5903" endWordPosition="5906">hat almost any sort of external knowledge source may be integrated to improve the performance of a baseline system. For example, supplemental information strings may appear in the form of transliterations of an input string in other languages; they may be predictions of other G2P systems, whether carefully manually crafted or learnt from data; they might even appear in the form of phonetic transcriptions of the input string in other dialects or languages. What distinguishes our solution to integrating supplemental information strings in string transduction settings from other research (e.g., (Bhargava and Kondrak, 2011; Bhargava and Kondrak, 2012)) is that rather than integrating systems on the global level of strings, we integrate them on the local level of smaller units, namely, substrings appropriated to the domain of application (e.g., in our context, phonemes/grapheme substructures). Both approaches may be considered complementary. Finally, another important contribution of our work is to outline an ‘approximation algorithm’ to inducing multiple many-to-many alignments of strings, which is otherwise an NP-hard problem for which (most likely) no efficient exact solutions exist, and to investigate its su</context>
</contexts>
<marker>Bhargava, Kondrak, 2011</marker>
<rawString>Aditya Bhargava and Grzegorz Kondrak. 2011. How do you pronounce your name?: Improving g2p with transliterations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 399–408, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Bhargava</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Leveraging supplemental representations for sequential transduction.</title>
<date>2012</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>396--406</pages>
<contexts>
<context position="2926" citStr="Bhargava and Kondrak (2012)" startWordPosition="473" endWordPosition="476">2010). In this work, we extend the problem of classical string-to-string translation by assuming that, at training time, we have available (M + 2)-tuples of strings {(x, ˆy(1), ... , ˆy(M), y)}, where x is the input string, ˆy(m), for 1 &lt; m &lt; M, are supplemental information strings, and y is the desired output string; at test time, we wish to predict y from (x, ˆy(1), ... , ˆy(M)). Generally, we may think of ˆy(1), ... , ˆy(M) as arbitrary strings over arbitrary alphabets E(m), for 1 &lt; m &lt; M. For example, x might be a letter-string and ˆy(m) might be a transliteration of x in language Lm (cf. Bhargava and Kondrak (2012)). Alternatively, and this is our model scenario in the current work, x might be a letter input string and ˆy(m) might be the predicted string of phonemes, given x, produced by an (offline) system Tm. This situation is outlined in Table 3. In the table, we also illustrate a multiple (monotone) many-to-many alignment of (x, ˆy(1), ... , ˆy(M), y). By this, we mean an alignment where (1) subsequences of all M +2 strings may be matched up with each other (many909 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on N</context>
<context position="5516" citStr="Bhargava and Kondrak (2012)" startWordPosition="895" endWordPosition="898">d in Table 2. The cenInput form Wiktionary Combilex neutrino nju:tôi:noU nutrinF wooded wUdId wUd@d wrench ôEnúS rEn&lt; Table 2: Input letter words, Wiktionary and Combilex transcriptions. tral question we want to answer is: can we train a system using this additional information which performs better than the ‘baseline’ system that ignores the extra information? Clearly, a system with more information should not perform worse than a system with less information (unless the additional information is highly noisy), but it is a priori not clear at all how the extra information can be included, as Bhargava and Kondrak (2012) note: output predictions may be in distinct alphabets and/or follow different conventions, and simple rule-based conversions may even deteriorate a baseline system’s performance. Their solution to the problem is to let the baseline system output its n-best phonetic transcriptions, and then to re-rank these n-best predictions via an SVM reranker trained on the supplemental representations x = schizo s ch i z o ˆy(1) = skaIz@U s k aI z @U ˆy(2) = saIz@U s - aI z @U ˆy(3) = skIts@ s k I ts @ ˆy(4) = Sits@U S - i ts @U ˆy(5) = skIts@ s k I ts @ y = skIts@U s k I ts @U Table 3: Left: Input string </context>
<context position="35008" citStr="Bhargava and Kondrak, 2012" startWordPosition="5907" endWordPosition="5910">nal knowledge source may be integrated to improve the performance of a baseline system. For example, supplemental information strings may appear in the form of transliterations of an input string in other languages; they may be predictions of other G2P systems, whether carefully manually crafted or learnt from data; they might even appear in the form of phonetic transcriptions of the input string in other dialects or languages. What distinguishes our solution to integrating supplemental information strings in string transduction settings from other research (e.g., (Bhargava and Kondrak, 2011; Bhargava and Kondrak, 2012)) is that rather than integrating systems on the global level of strings, we integrate them on the local level of smaller units, namely, substrings appropriated to the domain of application (e.g., in our context, phonemes/grapheme substructures). Both approaches may be considered complementary. Finally, another important contribution of our work is to outline an ‘approximation algorithm’ to inducing multiple many-to-many alignments of strings, which is otherwise an NP-hard problem for which (most likely) no efficient exact solutions exist, and to investigate its suitability for the problem tas</context>
</contexts>
<marker>Bhargava, Kondrak, 2012</marker>
<rawString>Aditya Bhargava and Grzegorz Kondrak. 2012. Leveraging supplemental representations for sequential transduction. In HLT-NAACL, pages 396–406. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Bisani</author>
<author>Hermann Ney</author>
</authors>
<title>Jointsequence models for grapheme-to-phoneme conversion.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>5</issue>
<contexts>
<context position="1485" citStr="Bisani and Ney, 2008" startWordPosition="217" endWordPosition="220">rent alphabet Γ. The most prominent applications of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence ph oe n i x f i n I ks Table 1: Sample monotone many-to-many alignment between x = phoenix and y = finIks. characters may be matched up with several y sequence characters; Table 1 illustrates. Once the training data is aligned, since x and y sequences are then segmented into equal number of segments, string-to-string translatio</context>
<context position="15321" citStr="Bisani and Ney, 2008" startWordPosition="2696" endWordPosition="2699">(1) for which N = 2, S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0, −1}, depending on whether compared input subsequences match or not. As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring edit operations — or equivalently, (monotone) many-to-many alignments — have appeared in the NLP context, e.g., in (Deligne et al., 1995), (Brill and Moore, 2000), (Jiampojamarn et al., 2007), (Bisani and Ney, 2008), (Jiampojamarn et al., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issu</context>
<context position="26036" citStr="Bisani and Ney, 2008" startWordPosition="4442" endWordPosition="4445"> Combilex that has as supplemental information corresponding Wiktionary (and PTE, as explained below) transcriptions. Throughout, we use as accuracy measures for all our systems word accuray (WACC). Word accuracy is defined as the number of correctly transcribed strings among all transcribed strings in a test sample. WACC is a strict measure that penalizes even tiny deviations from the gold-standard transcriptions, but has nowadays become standard in G2P. 914 5.1 A controlled experiment In our first set of experiments, we let our offline/black box systems be the Sequitur G2P modeling toolkit (Bisani and Ney, 2008) (S) and the Phonetisaurus modeling toolkit (Novak et al., 2012) (P). We train them on disjoint sets of 20,000 grapheme-to-phoneme Combilex string pairs each. The performance of these two systems, on the test set of size 28,000, is indicated in Table 4. Next, we train BASELINE on disPhonetisaurus Sequitur WACC 72.12 71.70 Table 4: Word-accuracy (in %) on the test data, for the two systems indicated. joint sets (disjoint from both the training sets of P and S) of size 2,000, 5,000, 10,000 and 20,000. Making BASELINE’s training sets disjoint from the training sets of the offline systems is both </context>
</contexts>
<marker>Bisani, Ney, 2008</marker>
<rawString>Maximilian Bisani and Hermann Ney. 2008. Jointsequence models for grapheme-to-phoneme conversion. Speech Communication, 50(5):434–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00,</booktitle>
<pages>286--293</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1212" citStr="Brill and Moore, 2000" startWordPosition="170" endWordPosition="173">ing baseline G2P systems using homogeneous as well as heterogeneous sources of supplemental information. 1 Introduction String-to-string translation (string transduction) is the problem of converting one string x over an alphabet E into another string y over a possibly different alphabet Γ. The most prominent applications of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence ph oe n i x f i n I ks Table 1: Sample monotone many-to-</context>
<context position="15268" citStr="Brill and Moore, 2000" startWordPosition="2687" endWordPosition="2690">case of the unigram model (2) in optimization problem (1) for which N = 2, S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0, −1}, depending on whether compared input subsequences match or not. As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring edit operations — or equivalently, (monotone) many-to-many alignments — have appeared in the NLP context, e.g., in (Deligne et al., 1995), (Brill and Moore, 2000), (Jiampojamarn et al., 2007), (Bisani and Ney, 2008), (Jiampojamarn et al., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, pages 286–293, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vitaly Kuznetsov</author>
<author>Mehryar Mohri</author>
</authors>
<title>Ensemble methods for structured prediction.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31th International Conference on Machine Learning, ICML 2014,</booktitle>
<pages>1134--1142</pages>
<location>Beijing,</location>
<contexts>
<context position="17882" citStr="Cortes et al. (2014)" startWordPosition="3116" endWordPosition="3119">Paul and Eisner (2012) apply dual decomposition to compute Steiner consensus strings. Via the approach taken in this paper, median strings may be computed in case d is a (distance) function taking substring-to-substring edit operations into account, a seemingly straightforward, yet extremely useful generalization in several NLP applications, as indicated in the introduction. Our approach may also be seen in the context of classifier combination for string-valued variables. While ensemble methods for structured prediction have been considered in several works (see, e.g., Nguyen and Guo (2007), Cortes et al. (2014), and references therein), a typical assumption in this situation is that the sequences to be combined have equal length, which clearly cannot be expected to hold when, e.g., the outputs of several G2P, transliteration, etc., systems must be combined. In fact, the multiple many-to-many alignment models investigated in this work could act as a preprocessing step in this setup, since the alignment precisely serves the functionality of segmenting the strings into equal number of segments/substructures. Of course, combining outputs with varying number of elements is also an issue in machine transl</context>
<context position="36044" citStr="Cortes et al., 2014" startWordPosition="6068" endWordPosition="6071">-to-many alignments of strings, which is otherwise an NP-hard problem for which (most likely) no efficient exact solutions exist, and to investigate its suitability for the problem task. In particular, we have seen that exact alignments lead to better overall model performance, but that the margin over the approximation is not wide. The scope for future research of our modeling is huge: multiple many-to-many alignments may be useful in aligning cognates in linguistic research; they may be the first necessary step for many other ensemble techniques in string transduction as we have considered (Cortes et al., 2014), and they may allow, on a large scale, to boost G2P (transliteration, lemmatization, etc.) systems by integrating them with many traditional (or modern) knowledge resources such as rule- and dictionarybased lemmatizers, crowd-sourced phonetic transcriptions (e.g., based on Wiktionary), etc., with the outlook of significantly outperforming current state-of-the-art models which are based solely on input string information. Finally, we note that we have thus far shown that supplemental information strings may be beneficial in case of overall little training data and that improvements decrease wi</context>
</contexts>
<marker>Cortes, Kuznetsov, Mohri, 2014</marker>
<rawString>Corinna Cortes, Vitaly Kuznetsov, and Mehryar Mohri. 2014. Ensemble methods for structured prediction. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1134–1142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Cotterell</author>
<author>Nanyun Peng</author>
<author>Jason Eisner</author>
</authors>
<title>Stochastic contextual edit distance and probabilistic FSTs.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<volume>6</volume>
<pages>pages.</pages>
<location>Baltimore,</location>
<contexts>
<context position="15568" citStr="Cotterell et al., 2014" startWordPosition="2733" endWordPosition="2736">venshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring edit operations — or equivalently, (monotone) many-to-many alignments — have appeared in the NLP context, e.g., in (Deligne et al., 1995), (Brill and Moore, 2000), (Jiampojamarn et al., 2007), (Bisani and Ney, 2008), (Jiampojamarn et al., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called median string (Kohonen, 1985) </context>
</contexts>
<marker>Cotterell, Peng, Eisner, 2014</marker>
<rawString>Ryan Cotterell, Nanyun Peng, and Jason Eisner. 2014. Stochastic contextual edit distance and probabilistic FSTs. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), Baltimore, June. 6 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>Alignment of multiple languages for historical comparison.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>275--279</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="15958" citStr="Covington (1998)" startWordPosition="2801" endWordPosition="2802">., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called median string (Kohonen, 1985) or Steiner consensus string (Gusfield, 1997), defined as the string s¯ that minimizes the sum of distances, for a given distance function d(x, y), to a list of strings s1, ... , sN (Jiang et al., 2012); typically, d is the standard edit distance. As Gusfield (1997) shows, the Steiner consensus string may be retrieved from a multiple align5In Cotterell et al. (2014), context influences al</context>
</contexts>
<marker>Covington, 1998</marker>
<rawString>Michael A. Covington. 1998. Alignment of multiple languages for historical comparison. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1, pages 275–279, Montreal, Quebec, Canada, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Deligne</author>
<author>Franois Yvon</author>
<author>Fr´ed´eric Bimbot</author>
</authors>
<title>Variable-length sequence matching for phonetic transcription using joint multigrams.</title>
<date>1995</date>
<booktitle>In EUROSPEECH. ISCA.</booktitle>
<contexts>
<context position="15243" citStr="Deligne et al., 1995" startWordPosition="2683" endWordPosition="2686">llowed. It is a special case of the unigram model (2) in optimization problem (1) for which N = 2, S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0, −1}, depending on whether compared input subsequences match or not. As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring edit operations — or equivalently, (monotone) many-to-many alignments — have appeared in the NLP context, e.g., in (Deligne et al., 1995), (Brill and Moore, 2000), (Jiampojamarn et al., 2007), (Bisani and Ney, 2008), (Jiampojamarn et al., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments betwee</context>
</contexts>
<marker>Deligne, Yvon, Bimbot, 1995</marker>
<rawString>Sabine Deligne, Franois Yvon, and Fr´ed´eric Bimbot. 1995. Variable-length sequence matching for phonetic transcription using joint multigrams. In EUROSPEECH. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Latent-variable modeling of string transductions with finite-state methods.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>1080--1089</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1157" citStr="Dreyer et al., 2008" startWordPosition="161" endWordPosition="164">a coupling constraint. We apply our approach to boosting baseline G2P systems using homogeneous as well as heterogeneous sources of supplemental information. 1 Introduction String-to-string translation (string transduction) is the problem of converting one string x over an alphabet E into another string y over a possibly different alphabet Γ. The most prominent applications of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence p</context>
</contexts>
<marker>Dreyer, Smith, Eisner, 2008</marker>
<rawString>Markus Dreyer, Jason Smith, and Jason Eisner. 2008. Latent-variable modeling of string transductions with finite-state methods. In EMNLP, pages 1080– 1089. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Durbin</author>
<author>Sean R Eddy</author>
<author>Anders Krogh</author>
<author>Graeme Mitchison</author>
</authors>
<title>Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.</title>
<date>1998</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16035" citStr="Durbin et al. (1998)" startWordPosition="2811" endWordPosition="2814">. Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called median string (Kohonen, 1985) or Steiner consensus string (Gusfield, 1997), defined as the string s¯ that minimizes the sum of distances, for a given distance function d(x, y), to a list of strings s1, ... , sN (Jiang et al., 2012); typically, d is the standard edit distance. As Gusfield (1997) shows, the Steiner consensus string may be retrieved from a multiple align5In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (2),</context>
</contexts>
<marker>Durbin, Eddy, Krogh, Mitchison, 1998</marker>
<rawString>Richard Durbin, Sean R. Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Eger</author>
</authors>
<title>Sequence segmentation by enumeration: An exploration.</title>
<date>2013</date>
<journal>Prague Bull. Math. Linguistics,</journal>
<pages>100--113</pages>
<contexts>
<context position="21426" citStr="Eger (2013)" startWordPosition="3686" endWordPosition="3687">predictions ˆy(1), ... , ˆy(M) and the true transcription y as illustrated in Table 3 (see Section 4.3 for details). Then, as for the baseline system, we train a CRF to label each input character 6We made use of the CRF++ package available at https://code.google.com/p/crfpp/. 7To be more precise on the training of the segmentation module, in an alignment as in Table 1, we consider the segmented x string — ph-oe-n-i-x — and then encode this segmentation in a binary string where 1’s indicate splits. Thus, segmentation becomes, again, a sequence labling task; see, e.g., Bartlett et al. (2008) or Eger (2013) for details. 913 subsequence with the corresponding output character subsequence. However, this time, the CRF has access to the subsequence suggestions (as the alignments indicate) produced by the offline systems. As features for the extended models, we additionally include context features for all predicted strings ˆy(m) (all n-grams in a window of size 3 centered around the current subsequence prediction). We also include a joint feature firing on the tuple of the current subsequence value of x, ˆy(1), . . . ,ˆy(M). To illustrate, when BASELINE+X tags position 2 in the (split up) input stri</context>
</contexts>
<marker>Eger, 2013</marker>
<rawString>Steffen Eger. 2013. Sequence segmentation by enumeration: An exploration. Prague Bull. Math. Linguistics, 100:113–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D F Feng</author>
<author>R F Doolittle</author>
</authors>
<title>Progressive sequence alignment as a prerequisite to correct phylogenetic trees.</title>
<date>1987</date>
<journal>Journal of molecular evolution,</journal>
<pages>25--4</pages>
<contexts>
<context position="18817" citStr="Feng and Doolittle, 1987" startWordPosition="3256" endWordPosition="3259">igated in this work could act as a preprocessing step in this setup, since the alignment precisely serves the functionality of segmenting the strings into equal number of segments/substructures. Of course, combining outputs with varying number of elements is also an issue in machine translation (e.g., Macherey and Och (2007), Heafield et al. (2009)), but, there, the problem is harder due to the potential non-monotonicities in the ordering of elements, which typically necessitates (additional) heuristics. One approach for constructing multiple alignments is here progressive multiple alignment (Feng and Doolittle, 1987) in which a multiple (typically one-to-one) alignment is iteratively constructed from successive pairwise alignments (Bangalore et al., 2001). Matusov et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 4 Data and systems 4.1 Data We conduct experiments on the General American (GA) variant of the Combilex data set (Richmond et al., 2009). This contains about 144,000 grapheme-phoneme pairs as exemplarily illustrated in Table 2. In our experiments, we split the data into two disjoint parts, one for testing (about 28,000 word pairs</context>
</contexts>
<marker>Feng, Doolittle, 1987</marker>
<rawString>D. F. Feng and R. F. Doolittle. 1987. Progressive sequence alignment as a prerequisite to correct phylogenetic trees. Journal of molecular evolution, 25(4):351–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<date>1997</date>
<booktitle>Algorithms on Strings, Trees, and Sequences - Computer Science and Computational Biology.</booktitle>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16212" citStr="Gusfield, 1997" startWordPosition="2841" endWordPosition="2843">entioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called median string (Kohonen, 1985) or Steiner consensus string (Gusfield, 1997), defined as the string s¯ that minimizes the sum of distances, for a given distance function d(x, y), to a list of strings s1, ... , sN (Jiang et al., 2012); typically, d is the standard edit distance. As Gusfield (1997) shows, the Steiner consensus string may be retrieved from a multiple align5In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (2), but there, too, the focus is on the situation N = 2 and S = {(1, 0), (0, 1), (1, 1)}. 912 ment of s1, ... , sN by concatenating the columnwise majority characters in the alignm</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on Strings, Trees, and Sequences - Computer Science and Computational Biology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Greg Hanneman</author>
<author>Alon Lavie</author>
</authors>
<title>Machine translation system combination with flexible word ordering.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>56--60</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="18542" citStr="Heafield et al. (2009)" startWordPosition="3218" endWordPosition="3221">ssumption in this situation is that the sequences to be combined have equal length, which clearly cannot be expected to hold when, e.g., the outputs of several G2P, transliteration, etc., systems must be combined. In fact, the multiple many-to-many alignment models investigated in this work could act as a preprocessing step in this setup, since the alignment precisely serves the functionality of segmenting the strings into equal number of segments/substructures. Of course, combining outputs with varying number of elements is also an issue in machine translation (e.g., Macherey and Och (2007), Heafield et al. (2009)), but, there, the problem is harder due to the potential non-monotonicities in the ordering of elements, which typically necessitates (additional) heuristics. One approach for constructing multiple alignments is here progressive multiple alignment (Feng and Doolittle, 1987) in which a multiple (typically one-to-one) alignment is iteratively constructed from successive pairwise alignments (Bangalore et al., 2001). Matusov et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 4 Data and systems 4.1 Data We conduct experiments on th</context>
</contexts>
<marker>Heafield, Hanneman, Lavie, 2009</marker>
<rawString>Kenneth Heafield, Greg Hanneman, and Alon Lavie. 2009. Machine translation system combination with flexible word ordering. In Proceedings of the EACL 2009 Fourth Workshop on Statistical Machine Translation, pages 56–60, Athens, Greece, March.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Grzegorz Kondrak</author>
<author>Tarek Sherif</author>
</authors>
<title>Applying many-to-many alignments and hidden markov models to letter-to-phoneme conversion.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>372--379</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="1463" citStr="Jiampojamarn et al., 2007" startWordPosition="213" endWordPosition="216">ing y over a possibly different alphabet Γ. The most prominent applications of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence ph oe n i x f i n I ks Table 1: Sample monotone many-to-many alignment between x = phoenix and y = finIks. characters may be matched up with several y sequence characters; Table 1 illustrates. Once the training data is aligned, since x and y sequences are then segmented into equal number of segments, strin</context>
<context position="15297" citStr="Jiampojamarn et al., 2007" startWordPosition="2691" endWordPosition="2695"> (2) in optimization problem (1) for which N = 2, S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0, −1}, depending on whether compared input subsequences match or not. As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring edit operations — or equivalently, (monotone) many-to-many alignments — have appeared in the NLP context, e.g., in (Deligne et al., 1995), (Brill and Moore, 2000), (Jiampojamarn et al., 2007), (Bisani and Ney, 2008), (Jiampojamarn et al., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignmen</context>
</contexts>
<marker>Jiampojamarn, Kondrak, Sherif, 2007</marker>
<rawString>Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif. 2007. Applying many-to-many alignments and hidden markov models to letter-to-phoneme conversion. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 372– 379, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Colin Cherry</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Joint processing and discriminative training for letter-to-phoneme conversion.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>905--913</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1512" citStr="Jiampojamarn et al., 2008" startWordPosition="221" endWordPosition="224">ost prominent applications of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence ph oe n i x f i n I ks Table 1: Sample monotone many-to-many alignment between x = phoenix and y = finIks. characters may be matched up with several y sequence characters; Table 1 illustrates. Once the training data is aligned, since x and y sequences are then segmented into equal number of segments, string-to-string translation may be seen as a sequence</context>
</contexts>
<marker>Jiampojamarn, Cherry, Kondrak, 2008</marker>
<rawString>Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kondrak. 2008. Joint processing and discriminative training for letter-to-phoneme conversion. In Proceedings of ACL-08: HLT, pages 905–913, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Colin Cherry</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Integrating joint n-gram features into a discriminative training framework.</title>
<date>2010</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>697--700</pages>
<contexts>
<context position="1539" citStr="Jiampojamarn et al., 2010" startWordPosition="225" endWordPosition="228">of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence ph oe n i x f i n I ks Table 1: Sample monotone many-to-many alignment between x = phoenix and y = finIks. characters may be matched up with several y sequence characters; Table 1 illustrates. Once the training data is aligned, since x and y sequences are then segmented into equal number of segments, string-to-string translation may be seen as a sequence labeling (tagging) problem</context>
<context position="15350" citStr="Jiampojamarn et al., 2010" startWordPosition="2700" endWordPosition="2704"> {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0, −1}, depending on whether compared input subsequences match or not. As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring edit operations — or equivalently, (monotone) many-to-many alignments — have appeared in the NLP context, e.g., in (Deligne et al., 1995), (Brill and Moore, 2000), (Jiampojamarn et al., 2007), (Bisani and Ney, 2008), (Jiampojamarn et al., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covingto</context>
<context position="20328" citStr="Jiampojamarn et al. (2010)" startWordPosition="3497" endWordPosition="3501">Table 1, at training time, we use the CRF as a tagging model that is trained to label each input character subsequence with an output character subsequence. As features for the CRF, we use all n-grams of subsequences of x that fit inside a window of size 5 centered around the current subsequence (context features). We also include linear-chain features which allow previously generated output character subsequences to influence current output character subsequences. In essence, our baseline model is a standard discriminative approach to G2P. It is, all in all, the same approach as described in Jiampojamarn et al. (2010), except that we do not include joint n-gram features. At test time, we first segment a new input string x and then apply the CRF. Thereby, we train the segmentation module on the segmented x sequences, as available from the aligned training data.7 BASELINE+X As competitors for the baseline system, we introduce systems that rely on the predictions of one or several additional (black box/offline) systems. At training time, we first multiply many-to-many align the input string x, the predictions ˆy(1), ... , ˆy(M) and the true transcription y as illustrated in Table 3 (see Section 4.3 for detail</context>
</contexts>
<marker>Jiampojamarn, Cherry, Kondrak, 2010</marker>
<rawString>Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kondrak. 2010. Integrating joint n-gram features into a discriminative training framework. In HLTNAACL, pages 697–700. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyi Jiang</author>
<author>Jran Wentker</author>
<author>Miquel Ferrer</author>
</authors>
<title>Generalized median string computation by means of string embedding in vector spaces.</title>
<date>2012</date>
<journal>Pattern Recognition Letters,</journal>
<volume>33</volume>
<issue>7</issue>
<contexts>
<context position="16369" citStr="Jiang et al., 2012" startWordPosition="2871" endWordPosition="2874">{(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called median string (Kohonen, 1985) or Steiner consensus string (Gusfield, 1997), defined as the string s¯ that minimizes the sum of distances, for a given distance function d(x, y), to a list of strings s1, ... , sN (Jiang et al., 2012); typically, d is the standard edit distance. As Gusfield (1997) shows, the Steiner consensus string may be retrieved from a multiple align5In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (2), but there, too, the focus is on the situation N = 2 and S = {(1, 0), (0, 1), (1, 1)}. 912 ment of s1, ... , sN by concatenating the columnwise majority characters in the alignment, ignoring skips. Since median string computation (and hence also the multiple many-to-many alignment problem, as we consider) is an NP-hard problem (Sim </context>
</contexts>
<marker>Jiang, Wentker, Ferrer, 2012</marker>
<rawString>Xiaoyi Jiang, Jran Wentker, and Miquel Ferrer. 2012. Generalized median string computation by means of string embedding in vector spaces. Pattern Recognition Letters, 33(7):842–852.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
</authors>
<title>Median strings.</title>
<date>1985</date>
<journal>Pattern Recognition Letters,</journal>
<pages>3--309</pages>
<contexts>
<context position="16167" citStr="Kohonen, 1985" startWordPosition="2834" endWordPosition="2836">l et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called median string (Kohonen, 1985) or Steiner consensus string (Gusfield, 1997), defined as the string s¯ that minimizes the sum of distances, for a given distance function d(x, y), to a list of strings s1, ... , sN (Jiang et al., 2012); typically, d is the standard edit distance. As Gusfield (1997) shows, the Steiner consensus string may be retrieved from a multiple align5In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (2), but there, too, the focus is on the situation N = 2 and S = {(1, 0), (0, 1), (1, 1)}. 912 ment of s1, ... , sN by concatenating the</context>
</contexts>
<marker>Kohonen, 1985</marker>
<rawString>T. Kohonen. 1985. Median strings. Pattern Recognition Letters, 3:309–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="19589" citStr="Lafferty et al., 2001" startWordPosition="3375" endWordPosition="3378"> et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 4 Data and systems 4.1 Data We conduct experiments on the General American (GA) variant of the Combilex data set (Richmond et al., 2009). This contains about 144,000 grapheme-phoneme pairs as exemplarily illustrated in Table 2. In our experiments, we split the data into two disjoint parts, one for testing (about 28,000 word pairs) and one for training/development (the remainder). 4.2 Systems BASELINE Our baseline system is a linear-chain conditional random field model (CRF)6 (Lafferty et al., 2001) which we apply in the manner indicated in the introduction: after many-to-many aligning the training data as in Table 1, at training time, we use the CRF as a tagging model that is trained to label each input character subsequence with an output character subsequence. As features for the CRF, we use all n-grams of subsequences of x that fit inside a window of size 5 centered around the current subsequence (context features). We also include linear-chain features which allow previously generated output character subsequences to influence current output character subsequences. In essence, our b</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282–289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levenshtein</author>
</authors>
<title>Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="14961" citStr="Levenshtein, 1966" startWordPosition="2642" endWordPosition="2643">e. 3 Related work Monotone alignments have a long tradition, both in NLP and bioinformatics. The classical Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) computes the optimal alignment between two sequences when only single character matches, mismatches, and skips are allowed. It is a special case of the unigram model (2) in optimization problem (1) for which N = 2, S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0, −1}, depending on whether compared input subsequences match or not. As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring edit operations — or equivalently, (monotone) many-to-many alignments — have appeared in the NLP context, e.g., in (Deligne et al., 1995), (Brill and Moore, 2000), (Jiampojamarn et al., 2007), (Bisani and Ney, 2008), (Jiampojamarn et al., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al.</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>VI Levenshtein. 1966. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady, 10:707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Josef Och</author>
</authors>
<title>An empirical study on computing consensus translations from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>986--995</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="18518" citStr="Macherey and Och (2007)" startWordPosition="3214" endWordPosition="3217">ces therein), a typical assumption in this situation is that the sequences to be combined have equal length, which clearly cannot be expected to hold when, e.g., the outputs of several G2P, transliteration, etc., systems must be combined. In fact, the multiple many-to-many alignment models investigated in this work could act as a preprocessing step in this setup, since the alignment precisely serves the functionality of segmenting the strings into equal number of segments/substructures. Of course, combining outputs with varying number of elements is also an issue in machine translation (e.g., Macherey and Och (2007), Heafield et al. (2009)), but, there, the problem is harder due to the potential non-monotonicities in the ordering of elements, which typically necessitates (additional) heuristics. One approach for constructing multiple alignments is here progressive multiple alignment (Feng and Doolittle, 1987) in which a multiple (typically one-to-one) alignment is iteratively constructed from successive pairwise alignments (Bangalore et al., 2001). Matusov et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 4 Data and systems 4.1 Data We c</context>
</contexts>
<marker>Macherey, Och, 2007</marker>
<rawString>Wolfgang Macherey and Franz Josef Och. 2007. An empirical study on computing consensus translations from multiple machine translation systems. In EMNLP-CoNLL, pages 986–995. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Urs-Viktor Marti</author>
<author>Horst Bunke</author>
</authors>
<title>Use of positional information in sequence alignment for multiple classifier combination.</title>
<date>2001</date>
<booktitle>In Josef Kittler and Fabio Roli, editors, Multiple Classifier Systems, volume 2096 of Lecture Notes in Computer Science,</booktitle>
<pages>388--398</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17078" citStr="Marti and Bunke (2001)" startWordPosition="2992" endWordPosition="2995">sensus string may be retrieved from a multiple align5In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (2), but there, too, the focus is on the situation N = 2 and S = {(1, 0), (0, 1), (1, 1)}. 912 ment of s1, ... , sN by concatenating the columnwise majority characters in the alignment, ignoring skips. Since median string computation (and hence also the multiple many-to-many alignment problem, as we consider) is an NP-hard problem (Sim and Park, 2003), designing approximations is an active field of research. For example, Marti and Bunke (2001) ignore part of the search space by declaring matches-up of distant characters as unlikely, and Jiang et al. (2012) apply an approximation based on string embeddings in vector spaces. Paul and Eisner (2012) apply dual decomposition to compute Steiner consensus strings. Via the approach taken in this paper, median strings may be computed in case d is a (distance) function taking substring-to-substring edit operations into account, a seemingly straightforward, yet extremely useful generalization in several NLP applications, as indicated in the introduction. Our approach may also be seen in the c</context>
</contexts>
<marker>Marti, Bunke, 2001</marker>
<rawString>Urs-Viktor Marti and Horst Bunke. 2001. Use of positional information in sequence alignment for multiple classifier combination. In Josef Kittler and Fabio Roli, editors, Multiple Classifier Systems, volume 2096 of Lecture Notes in Computer Science, pages 388–398. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="18981" citStr="Matusov et al. (2006)" startWordPosition="3279" endWordPosition="3282">of segments/substructures. Of course, combining outputs with varying number of elements is also an issue in machine translation (e.g., Macherey and Och (2007), Heafield et al. (2009)), but, there, the problem is harder due to the potential non-monotonicities in the ordering of elements, which typically necessitates (additional) heuristics. One approach for constructing multiple alignments is here progressive multiple alignment (Feng and Doolittle, 1987) in which a multiple (typically one-to-one) alignment is iteratively constructed from successive pairwise alignments (Bangalore et al., 2001). Matusov et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 4 Data and systems 4.1 Data We conduct experiments on the General American (GA) variant of the Combilex data set (Richmond et al., 2009). This contains about 144,000 grapheme-phoneme pairs as exemplarily illustrated in Table 2. In our experiments, we split the data into two disjoint parts, one for testing (about 28,000 word pairs) and one for training/development (the remainder). 4.2 Systems BASELINE Our baseline system is a linear-chain conditional random field model (CRF)6 (Lafferty et al</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Conference of the European Chapter of the Association for Computational Linguistics, pages 33–40, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saul B Needleman</author>
<author>Christian D Wunsch</author>
</authors>
<title>A general method applicable to the search for similarities in the amino acid sequence of two proteins.</title>
<date>1970</date>
<journal>Journal of Molecular Biology,</journal>
<volume>48</volume>
<issue>3</issue>
<pages>453</pages>
<contexts>
<context position="14505" citStr="Needleman and Wunsch, 1970" startWordPosition="2560" endWordPosition="2563"> experiments below, we directly specify sim1 as a sum of pairwise similarity scores which we extract from alignments produced by an off-the-shelf pairwise aligner. 4Note the difference between Eqs. (2) and (4). While each fwn in (4) operates on a ‘row’ of an alignment scheme, sim1 in (2) acts on the ‘columns’. In other words, the unigram alignment model correlates the multiply matched-up subsequences, while the separable alignment model assumes independence here. 3 Related work Monotone alignments have a long tradition, both in NLP and bioinformatics. The classical Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) computes the optimal alignment between two sequences when only single character matches, mismatches, and skips are allowed. It is a special case of the unigram model (2) in optimization problem (1) for which N = 2, S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0, −1}, depending on whether compared input subsequences match or not. As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring</context>
</contexts>
<marker>Needleman, Wunsch, 1970</marker>
<rawString>Saul B. Needleman and Christian D. Wunsch. 1970. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of Molecular Biology, 48(3):443– 453, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nam Nguyen</author>
<author>Yunsong Guo</author>
</authors>
<title>Comparisons of sequence labeling algorithms and extensions.</title>
<date>2007</date>
<booktitle>of ACM International Conference Proceeding Series,</booktitle>
<volume>227</volume>
<pages>681--688</pages>
<editor>In Zoubin Ghahramani, editor, ICML,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="17860" citStr="Nguyen and Guo (2007)" startWordPosition="3112" endWordPosition="3115">ings in vector spaces. Paul and Eisner (2012) apply dual decomposition to compute Steiner consensus strings. Via the approach taken in this paper, median strings may be computed in case d is a (distance) function taking substring-to-substring edit operations into account, a seemingly straightforward, yet extremely useful generalization in several NLP applications, as indicated in the introduction. Our approach may also be seen in the context of classifier combination for string-valued variables. While ensemble methods for structured prediction have been considered in several works (see, e.g., Nguyen and Guo (2007), Cortes et al. (2014), and references therein), a typical assumption in this situation is that the sequences to be combined have equal length, which clearly cannot be expected to hold when, e.g., the outputs of several G2P, transliteration, etc., systems must be combined. In fact, the multiple many-to-many alignment models investigated in this work could act as a preprocessing step in this setup, since the alignment precisely serves the functionality of segmenting the strings into equal number of segments/substructures. Of course, combining outputs with varying number of elements is also an i</context>
</contexts>
<marker>Nguyen, Guo, 2007</marker>
<rawString>Nam Nguyen and Yunsong Guo. 2007. Comparisons of sequence labeling algorithms and extensions. In Zoubin Ghahramani, editor, ICML, volume 227 of ACM International Conference Proceeding Series, pages 681–688. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef R Novak</author>
<author>Nobuaki Minematsu</author>
<author>Keikichi Hirose</author>
</authors>
<title>WFST-based grapheme-to-phoneme conversion: Open source tools for alignment, model-building and decoding.</title>
<date>2012</date>
<booktitle>In Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing,</booktitle>
<pages>45--49</pages>
<institution>Donostia–San Sebastin, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="1560" citStr="Novak et al., 2012" startWordPosition="229" endWordPosition="232">tion in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence ph oe n i x f i n I ks Table 1: Sample monotone many-to-many alignment between x = phoenix and y = finIks. characters may be matched up with several y sequence characters; Table 1 illustrates. Once the training data is aligned, since x and y sequences are then segmented into equal number of segments, string-to-string translation may be seen as a sequence labeling (tagging) problem in which x (sub-)seq</context>
<context position="26100" citStr="Novak et al., 2012" startWordPosition="4452" endWordPosition="4455">onary (and PTE, as explained below) transcriptions. Throughout, we use as accuracy measures for all our systems word accuray (WACC). Word accuracy is defined as the number of correctly transcribed strings among all transcribed strings in a test sample. WACC is a strict measure that penalizes even tiny deviations from the gold-standard transcriptions, but has nowadays become standard in G2P. 914 5.1 A controlled experiment In our first set of experiments, we let our offline/black box systems be the Sequitur G2P modeling toolkit (Bisani and Ney, 2008) (S) and the Phonetisaurus modeling toolkit (Novak et al., 2012) (P). We train them on disjoint sets of 20,000 grapheme-to-phoneme Combilex string pairs each. The performance of these two systems, on the test set of size 28,000, is indicated in Table 4. Next, we train BASELINE on disPhonetisaurus Sequitur WACC 72.12 71.70 Table 4: Word-accuracy (in %) on the test data, for the two systems indicated. joint sets (disjoint from both the training sets of P and S) of size 2,000, 5,000, 10,000 and 20,000. Making BASELINE’s training sets disjoint from the training sets of the offline systems is both realistic (since a black box system would typically follow a par</context>
</contexts>
<marker>Novak, Minematsu, Hirose, 2012</marker>
<rawString>Josef R. Novak, Nobuaki Minematsu, and Keikichi Hirose. 2012. WFST-based grapheme-to-phoneme conversion: Open source tools for alignment, model-building and decoding. In Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 45–49, Donostia–San Sebastin, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>Jason Eisner</author>
</authors>
<title>Implicitly intersecting weighted automata using dual decomposition.</title>
<date>2012</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>232--242</pages>
<contexts>
<context position="17284" citStr="Paul and Eisner (2012)" startWordPosition="3027" endWordPosition="3030">is on the situation N = 2 and S = {(1, 0), (0, 1), (1, 1)}. 912 ment of s1, ... , sN by concatenating the columnwise majority characters in the alignment, ignoring skips. Since median string computation (and hence also the multiple many-to-many alignment problem, as we consider) is an NP-hard problem (Sim and Park, 2003), designing approximations is an active field of research. For example, Marti and Bunke (2001) ignore part of the search space by declaring matches-up of distant characters as unlikely, and Jiang et al. (2012) apply an approximation based on string embeddings in vector spaces. Paul and Eisner (2012) apply dual decomposition to compute Steiner consensus strings. Via the approach taken in this paper, median strings may be computed in case d is a (distance) function taking substring-to-substring edit operations into account, a seemingly straightforward, yet extremely useful generalization in several NLP applications, as indicated in the introduction. Our approach may also be seen in the context of classifier combination for string-valued variables. While ensemble methods for structured prediction have been considered in several works (see, e.g., Nguyen and Guo (2007), Cortes et al. (2014), </context>
</contexts>
<marker>Paul, Eisner, 2012</marker>
<rawString>Michael J. Paul and Jason Eisner. 2012. Implicitly intersecting weighted automata using dual decomposition. In HLT-NAACL, pages 232–242. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Korin Richmond</author>
<author>Robert A J Clark</author>
<author>Susan Fitt</author>
</authors>
<date>2009</date>
<booktitle>Robust LTS rules with the Combilex speech technology lexicon. In INTERSPEECH,</booktitle>
<pages>1295--1298</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="4615" citStr="Richmond et al., 2009" startWordPosition="752" endWordPosition="755">table equals the true phonetic transcription y of x, taking a position-wise majority vote of the multiple alignment of (ˆy(1), ... , ˆy(M)) yields y. Moreover, analogously as in the case of pairs of aligned strings, we may perceive the so extended stringto-string translation problem as a sequence labeling task once (x, ˆy(1), ... , ˆy(M), y) are multiply aligned, but now, with additional observed variables (or features), namely, (sub-)sequence characters of each string ˆy(&apos;). To further motivate our approach, consider the situation of training a new G2P system on the basis of, e.g., Combilex (Richmond et al., 2009). For each letter form in its database, Combilex provides a corresponding phonetic transcription. Now, suppose that, in addition, we can poll an external knowledge source such as Wiktionary for (its) phonetic transcriptions of the respective Combilex letter words as outlined in Table 2. The cenInput form Wiktionary Combilex neutrino nju:tôi:noU nutrinF wooded wUdId wUd@d wrench ôEnúS rEn&lt; Table 2: Input letter words, Wiktionary and Combilex transcriptions. tral question we want to answer is: can we train a system using this additional information which performs better than the ‘baseline’ syste</context>
<context position="19222" citStr="Richmond et al., 2009" startWordPosition="3317" endWordPosition="3321">non-monotonicities in the ordering of elements, which typically necessitates (additional) heuristics. One approach for constructing multiple alignments is here progressive multiple alignment (Feng and Doolittle, 1987) in which a multiple (typically one-to-one) alignment is iteratively constructed from successive pairwise alignments (Bangalore et al., 2001). Matusov et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 4 Data and systems 4.1 Data We conduct experiments on the General American (GA) variant of the Combilex data set (Richmond et al., 2009). This contains about 144,000 grapheme-phoneme pairs as exemplarily illustrated in Table 2. In our experiments, we split the data into two disjoint parts, one for testing (about 28,000 word pairs) and one for training/development (the remainder). 4.2 Systems BASELINE Our baseline system is a linear-chain conditional random field model (CRF)6 (Lafferty et al., 2001) which we apply in the manner indicated in the introduction: after many-to-many aligning the training data as in Table 1, at training time, we use the CRF as a tagging model that is trained to label each input character subsequence w</context>
</contexts>
<marker>Richmond, Clark, Fitt, 2009</marker>
<rawString>Korin Richmond, Robert A. J. Clark, and Susan Fitt. 2009. Robust LTS rules with the Combilex speech technology lexicon. In INTERSPEECH, pages 1295–1298. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Peter N Yianilos</author>
</authors>
<title>Learning string-edit distance.</title>
<date>1998</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="15542" citStr="Ristad and Yianilos, 1998" startWordPosition="2728" endWordPosition="2732">the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring edit operations — or equivalently, (monotone) many-to-many alignments — have appeared in the NLP context, e.g., in (Deligne et al., 1995), (Brill and Moore, 2000), (Jiampojamarn et al., 2007), (Bisani and Ney, 2008), (Jiampojamarn et al., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called medi</context>
</contexts>
<marker>Ristad, Yianilos, 1998</marker>
<rawString>Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string-edit distance. IEEE Trans. Pattern Anal. Mach. Intell., 20(5):522–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tarek Sherif</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Substringbased transliteration. In</title>
<date>2007</date>
<editor>John A. Carroll, Antal van den Bosch, and Annie Zaenen, editors, ACL.</editor>
<publisher>The Association for Computational Linguistics.</publisher>
<contexts>
<context position="1120" citStr="Sherif and Kondrak, 2007" startWordPosition="155" endWordPosition="158">ultiple sequence segmentations satisfying a coupling constraint. We apply our approach to boosting baseline G2P systems using homogeneous as well as heterogeneous sources of supplemental information. 1 Introduction String-to-string translation (string transduction) is the problem of converting one string x over an alphabet E into another string y over a possibly different alphabet Γ. The most prominent applications of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex</context>
</contexts>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>Tarek Sherif and Grzegorz Kondrak. 2007. Substringbased transliteration. In John A. Carroll, Antal van den Bosch, and Annie Zaenen, editors, ACL. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeong Seop Sim</author>
<author>Kunsoo Park</author>
</authors>
<title>The consensus string problem for a metric is np-complete.</title>
<date>2003</date>
<journal>J. of Discrete Algorithms,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="16984" citStr="Sim and Park, 2003" startWordPosition="2978" endWordPosition="2981">012); typically, d is the standard edit distance. As Gusfield (1997) shows, the Steiner consensus string may be retrieved from a multiple align5In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (2), but there, too, the focus is on the situation N = 2 and S = {(1, 0), (0, 1), (1, 1)}. 912 ment of s1, ... , sN by concatenating the columnwise majority characters in the alignment, ignoring skips. Since median string computation (and hence also the multiple many-to-many alignment problem, as we consider) is an NP-hard problem (Sim and Park, 2003), designing approximations is an active field of research. For example, Marti and Bunke (2001) ignore part of the search space by declaring matches-up of distant characters as unlikely, and Jiang et al. (2012) apply an approximation based on string embeddings in vector spaces. Paul and Eisner (2012) apply dual decomposition to compute Steiner consensus strings. Via the approach taken in this paper, median strings may be computed in case d is a (distance) function taking substring-to-substring edit operations into account, a seemingly straightforward, yet extremely useful generalization in seve</context>
</contexts>
<marker>Sim, Park, 2003</marker>
<rawString>Jeong Seop Sim and Kunsoo Park. 2003. The consensus string problem for a metric is np-complete. J. of Discrete Algorithms, 1(1):111–117, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esko Ukkonen</author>
</authors>
<title>Algorithms for approximate string matching.</title>
<date>1985</date>
<journal>Information and Control,</journal>
<volume>64</volume>
<pages>118</pages>
<contexts>
<context position="15397" citStr="Ukkonen, 1985" startWordPosition="2709" endWordPosition="2710"> −1}, depending on whether compared input subsequences match or not. As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring edit operations — or equivalently, (monotone) many-to-many alignments — have appeared in the NLP context, e.g., in (Deligne et al., 1995), (Brill and Moore, 2000), (Jiampojamarn et al., 2007), (Bisani and Ney, 2008), (Jiampojamarn et al., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioi</context>
</contexts>
<marker>Ukkonen, 1985</marker>
<rawString>Esko Ukkonen. 1985. Algorithms for approximate string matching. Information and Control, 64:100– 118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>Computerized correction of phonographic errors.</title>
<date>1988</date>
<journal>Computers and the Humanities,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>V´eronis, 1988</marker>
<rawString>Jean V´eronis. 1988. Computerized correction of phonographic errors. Computers and the Humanities, 22(1):43–56.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>