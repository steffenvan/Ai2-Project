<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<note confidence="0.443767333333333">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 133-139.
Association for Computational Linguistics.
</note>
<title confidence="0.990515">
A Phrase-Based, Joint Probability Model for Statistical Machine Translation
</title>
<author confidence="0.993836">
Daniel Marcu William Wong
</author>
<affiliation confidence="0.8062274">
Information Sciences Institute and Language Weaver Inc.
Department of Computer Science 1639 11th St., Suite 100A
University of Southern California Santa Monica, CA 90404
4676 Admiralty Way, Suite 1001 wwong@languageweaver.com
Marina del Rey, CA, 90292
</affiliation>
<email confidence="0.997968">
marcu@isi.edu
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99980875">
We present a joint probability model for
statistical machine translation, which au-
tomatically learns word and phrase equiv-
alents from bilingual corpora. Transla-
tions produced with parameters estimated
using the joint model are more accu-
rate than translations produced using IBM
Model 4.
</bodyText>
<sectionHeader confidence="0.990022" genericHeader="introduction">
1 Motivation
</sectionHeader>
<bodyText confidence="0.99793345">
Most of the noisy-channel-based models used in
statistical machine translation (MT) (Brown et al.,
1993) are conditional probability models. In the
noisy-channel framework, each source sentence e in
a parallel corpus is assumed to “generate” a target
sentence f by means of a stochastic process, whose
parameters are estimated using traditional EM tech-
niques (Dempster et al., 1977). The generative
model explains how source words are mapped into
target words and how target words are re-ordered
to yield well-formed target sentences. A variety
of methods are used to account for the re-ordering
stage: word-based (Brown et al., 1993), template-
based (Och et al., 1999), and syntax-based (Yamada
and Knight, 2001), to name just a few. Although
these models use different generative processes to
explain how translated words are re-ordered in a tar-
get language, at the lexical level they are quite sim-
ilar; all these models assume that source words are
individually translated into target words.1
</bodyText>
<footnote confidence="0.9710705">
1The individual words may contain a non-existent element,
called NULL.
</footnote>
<bodyText confidence="0.99993290625">
We suspect that MT researchers have so far cho-
sen to automatically learn translation lexicons de-
fined only over words for primarily pragmatic rea-
sons. Large scale bilingual corpora with vocabu-
laries in the range of hundreds of thousands yield
very large translation lexicons. Tuning the probabil-
ities associated with these large lexicons is a difficult
enough task to deter one from trying to scale up to
learning phrase-based lexicons. Unfortunately, trad-
ing space requirements and efficiency for explana-
tory power often yields non-intuitive results.
Consider, for example, the parallel corpus of three
sentence pairs shown in Figure 1. Intuitively, if we
allow any Source words to be aligned to any Target
words, the best alignment that we can come up with
is the one in Figure 1.c. Sentence pair (S2, T2) of-
fers strong evidence that “b c” in language S means
the same thing as “x” in language T. On the basis
of this evidence, we expect the system to also learn
from sentence pair (S1, T1) that “a” in language S
means the same thing as “y” in language T. Unfortu-
nately, if one works with translation models that do
not allow Target words to be aligned to more than
one Source word — as it is the case in the IBM mod-
els (Brown et al., 1993) — it is impossible to learn
that the phrase “b c” in language S means the same
thing as word “x” in language T. The IBM Model
4 (Brown et al., 1993), for example, converges to the
word alignments shown in Figure 1.b and learns the
translation probabilities shown in Figure 1.a.2 Since
in the IBM model one cannot link a Target word
to more than a Source word, the training procedure
</bodyText>
<footnote confidence="0.916213">
2To train the IBM-4 model, we used Giza (Al-Onaizan et al.,
1999).
</footnote>
<equation confidence="0.9637438">
IBM−4 T−Table IBM−4 Intuitive Joint Joint T−Table
p(y  |a) = 1
p(x  |c) = 1
p(z  |b) = 0.98
p(x  |b) = 0.02
</equation>
<figure confidence="0.554336066666667">
S1: a b c
T1: x y
S2: b c
S1: a b c
T1: x y
S2: b c
S1: a b c
T1: x y
S2: b c
p(x y, a b c) = 0.32
p(x, b c) = 0.34
p(y, a) = 0.01
p(z, b) = 0.33
Corresponding
Conditional Table
</figure>
<equation confidence="0.998324">
T2: x T2: x T2: x
p(x y  |a b c ) = 1
p(x  |b c) = 1
p(y  |a) = 1
p(z  |b) = 1
S3: b S3: b S3: b
T3: z T3: z T3: z
a) b) c) d) e)
</equation>
<figureCaption confidence="0.999739">
Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model.
</figureCaption>
<bodyText confidence="0.994025411764706">
yields unintuitive translation probabilities. (Note
that another good word-for-word model is one that
assigns high probability to p(xb) and p(zb) and
low probability to p(xc).)
In this paper, we describe a translation model that
assumes that lexical correspondences can be estab-
lished not only at the word level, but at the phrase
level as well. In constrast with many previous ap-
proaches (Brown et al., 1993; Och et al., 1999; Ya-
mada and Knight, 2001), our model does not try to
capture how Source sentences can be mapped into
Target sentences, but rather how Source and Tar-
get sentences can be generated simultaneously. In
other words, in the style of Melamed (2001), we es-
timate a joint probability model that can be easily
marginalized in order to yield conditional probabil-
ity models for both source-to-target and target-to-
source machine translation applications. The main
difference between our work and that of Melamed
is that we learn joint probability models of trans-
lation equivalence not only between words but also
between phrases and we show that these models can
be used not only for the extraction of bilingual lexi-
cons but also for the automatic translation of unseen
sentences.
In the rest of the paper, we first describe our
model (Section 2) and explain how it can be imple-
mented/trained (Section 3). We briefly describe a
decoding algorithm that works in conjunction with
our model (Section 4) and evaluate the performance
of a translation system that uses the joint-probability
model (Section 5). We end with a discussion of the
strengths and weaknesses of our model as compared
to other models proposed in the literature.
</bodyText>
<sectionHeader confidence="0.958276" genericHeader="method">
2 A Phrase-Based Joint Probability Model
</sectionHeader>
<subsectionHeader confidence="0.977261">
2.1 Model 1
</subsectionHeader>
<bodyText confidence="0.999819">
In developing our joint probability model, we started
out with a very simple generative story. We assume
that each sentence pair in our corpus is generated by
the following stochastic process:
</bodyText>
<listItem confidence="0.9934465">
1. Generate a bag of concepts.
2. For each concept , generate a pair of
</listItem>
<bodyText confidence="0.751241666666667">
phrases , according to the distribution
, whereandeach contain at least
one word.
</bodyText>
<listItem confidence="0.86188475">
3. Order the phrases generated in each language
so as to create two linear sequences of phrases;
these sequences correspond to the sentence
pairs in a bilingual corpus.
</listItem>
<bodyText confidence="0.99722071875">
For simplicity, we initially assume that the bag of
concepts and the ordering of the generated phrases
are modeled by uniform distributions. We do not
assume that is a hidden variable that generates
the pair , but rather that . Un-
der these assumptions, it follows that the probability
of generating a sentence pair (E, F) using concepts
is given by the product of all phrase-to-
phrase translation probabilities, that
yield bags of phrases that can be ordered linearly so
as to obtain the sentences E and F. For example, the
sentence pair “a b c” — “x y” can be generated us-
ing two concepts, (“a b” : “y”) and (“c” : “x”); or
one concept, (“a b c” : “x y”), because in both cases
the phrases in each language can be arranged in a
sequence that would yield the original sentence pair.
However, the same sentence pair cannot be gener-
ated using the concepts (“a b” : “y”) and (“c” : “y”)
because the sequence “x y” cannot be recreated from
the two phrases “y” and “y”. Similarly, the pair can-
not be generated using concepts (“a c” : “x”) and
(“b” : “y”) because the sequence “a b c” cannot be
created by catenating the phrases “a c” and “b”.
We say that a set of concepts can be linearized
into a sentence pair (E, F) if E and F can be obtained
by permuting the phrasesandthat characterize
all concepts . We denote this property us-
ing the predicate . Under this model, the
probability of a given sentence pair (E, F) can then
be obtained by summing up over all possible ways
of generating bags of concepts that can be
linearized to (E, F).
</bodyText>
<equation confidence="0.758095">
(1)
</equation>
<subsectionHeader confidence="0.990442">
2.2 Model 2
</subsectionHeader>
<bodyText confidence="0.995393111111111">
Although Model 1 is fairly unsophisticated, we have
found that it produces in practice fairly good align-
ments. However, this model is clearly unsuited for
translating unseen sentences as it imposes no con-
straints on the ordering of the phrases associated
with a given concept. In order to account for this,
we modify slightly the generative process in Model
1 so as to account for distortions. The generative
story of Model 2 is this:
</bodyText>
<listItem confidence="0.999825">
1. Generate a bag of concepts.
2. Initialize E and F to empty sequences.
3. Randomly take a concept and generate
</listItem>
<bodyText confidence="0.701181333333333">
a pair of phrases , according to the dis-
tribution , whereandeach contain
at least one word. Remove then from.
</bodyText>
<listItem confidence="0.990349">
4. Append phraseat the end of F. Letbe the
start position ofin F.
5. Insert phraseat positionin E provided that
no other phrase occupies any of the positions
</listItem>
<bodyText confidence="0.9660885">
betweenand , wheregives the
length of the phrase. We hence create the
alignment between the two phrasesand
with probability
where is a position-based distortion dis-
tribution.
</bodyText>
<listItem confidence="0.549941285714286">
6. Repeat steps 3 to 5 untilis empty.
In Model 2, the probability to generate a sentence
pair (E, F) is given by formula (2), where
denotes the position of wordof phrasein sen-
tence F and denotes the position in sen-
tence E of the center of mass of phrase.
(2)
</listItem>
<bodyText confidence="0.999930090909091">
Model 2 implements an absolute position-based
distortion model, in the style of IBM Model 3. We
have tried many types of distortion models. We
eventually settled for the model discussed here be-
cause it produces better translations during decod-
ing. Since the number of factors involved in com-
puting the probability of an alignment does not vary
with the size of the Target phrases into which Source
phrases are translated, this model is not predis-
posed to produce translations that are shorter than
the Source sentences given as input.
</bodyText>
<sectionHeader confidence="0.9916" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.8357595">
Training the models described in Section 2 is com-
putationally challenging. Since there is an exponen-
tial number of alignments that can generate a sen-
tence pair (E, F), it is clear that we cannot apply the
</bodyText>
<listItem confidence="0.999354888888889">
1. Determine high-frequency n-
grams in the bilingual corpus.
2. Initialize the t-distribution
table.
3. Apply EM training on the
Viterbi alignments, while using
smoothing.
4. Generate conditional model
probabilities.
</listItem>
<figureCaption confidence="0.985028166666667">
Figure 2: Training algorithm for the phrase-based
joint probability model.
EM training algorithm exhaustively. To estimate the
parameters of our model, we apply the algorithm in
Figure 2, whose steps are motivated and described
below.
</figureCaption>
<subsectionHeader confidence="0.9438485">
3.1 Determine high-frequency n-grams in E
and F
</subsectionHeader>
<bodyText confidence="0.99945525">
If one assumes from the outset that any phrases
and can be generated from a con-
cept , one would need a supercomputer in order to
store in the memory a table that models the
distribution. Since we don’t have access to comput-
ers with unlimited memory, we initially learn t distri-
bution entries only for the phrases that occur often in
the corpus and for unigrams. Then, through smooth-
ing, we learn t distribution entries for the phrases
that occur rarely as well. In order to be considered
in step 2 of the algorithm, a phrase has to occur at
least five times in the corpus.
</bodyText>
<subsectionHeader confidence="0.994305">
3.2 Initialize the t-distribution table
</subsectionHeader>
<bodyText confidence="0.999081085106383">
Before the EM training procedure starts, one has no
idea what word/phrase pairs are likely to share the
same meaning. In other words, all alignments that
can generate a sentence pair (E, F) can be assumed to
have the same probability. Under these conditions,
the evidence that a sentence pair (E, F) contributes to
the fact that are generated by the same con-
cept is given by the number of alignments that can
be built between (E, F) that have a concept that
is linked to phrasein sentence E and phrase
in sentence F divided by the total number of align-
ments that can be built between the two sentences.
Both these numbers can be easily approximated.
Given a sentence E ofwords, there are
ways in which thewords can be partitioned into
non-empty sets/concepts, where is the Stir-
ling number of second kind.
There are also ways in which the words
of a sentence F can be partitioned into non-
empty sets. Given that any words in E can be
mapped to any words in F, it follows that there are
alignments that can be
built between two sentences (E, F) of lengthsand
, respectively. When a concept generates two
phrases of lengthand, respectively, there
are only and words left to link. Hence, in
the absence of any other information, the probabil-
ity that phrasesandare generated by the same
concept is given by formula (4).
Note that the fractional counts returned by equa-
tion (4) are only an approximation of the t distri-
bution that we are interested in because the Stir-
ling numbers of the second kind do not impose any
restriction on the words that are associated with a
given concept be consecutive. However, since for-
mula (4) overestimates the numerator and denomi-
nator equally, the approximation works well in prac-
tice.
In the second step of the algorithm, we apply
equation (4) to collect fractional counts for all un-
igram and high-frequency n-gram pairs in the carte-
sian product defined over the phrases in each sen-
tence pair (E, F) in a corpus. We sum over all these
t-counts and we normalize to obtain an initial joint
distribution. This step amounts to running the EM
algorithm for one step over all possible alignments
in the corpus.
</bodyText>
<subsectionHeader confidence="0.969259">
3.3 EM training on Viterbi alignments
</subsectionHeader>
<bodyText confidence="0.999987391304348">
Given a non-uniform t distribution, phrase-to-phrase
alignments have different weights and there are no
other tricks one can apply to collect fractional counts
over all possible alignments in polynomial time.
Starting with step 3 of the algorithm in Figure 2, for
each sentence pair in a corpus, we greedily produce
an initial alignment by linking together phrases so
as to create concepts that have high t probabilities.
We then hillclimb towards the Viterbi alignment of
highest probability by breaking and merging con-
cepts, swapping words between concepts, and mov-
ing words across concepts. We compute the prob-
abilities associated with all the alignments we gen-
erate during the hillclimbing process and collect t
counts over all concepts in these alignments.
We apply this Viterbi-based EM training proce-
dure for a few iterations. The first iterations estimate
the alignment probabilities using Model 1. The rest
of the iterations estimate the alignment probabilities
using Model 2.
During training, we apply smoothing so we can
associate non-zero values to phrase-pairs that do not
occur often in the corpus.
</bodyText>
<subsectionHeader confidence="0.999183">
3.4 Derivation of conditional probability model
</subsectionHeader>
<bodyText confidence="0.9985762">
At the end of the training procedure, we take
marginals on the joint probability distributionsand
. This yields conditional probability distributions
and which we use for
decoding.
</bodyText>
<subsectionHeader confidence="0.545017">
3.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999974681818182">
When we run the training procedure in Figure 2 on
the corpus in Figure 1, after four Model 1 iterations
we obtain the alignments in Figure 1.d and the joint
and conditional probability distributions shown in
Figure 1.e. At prima facie, the Viterbi alignment for
the first sentence pair appears incorrect because we,
as humans, have a natural tendency to build align-
ments between the smallest phrases possible. How-
ever, note that the choice made by our model is quite
reasonable. After all, in the absence of additional
information, the model can either assume that “a”
and “y” mean the same thing or that phrases “a b c”
and “x y” mean the same thing. The model chose
to give more weight to the second hypothesis, while
preserving some probability mass for the first one.
Also note that although the joint distribution puts
the second hypothesis at an advantage, the condi-
tional distribution does not. The conditional distri-
bution in Figure 1.e is consistent with our intuitions
that tell us that it is reasonable both to translate “a
b c” into “x y”, as well as “a” into “y”. The condi-
tional distribution mirrors perfectly our intuitions.
</bodyText>
<sectionHeader confidence="0.988905" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.999988375">
For decoding, we have implemented a greedy pro-
cedure similar to that proposed by Germann et
al. (2001). Given a Foreign sentence F, we first pro-
duce a gloss of it by selecting phrases inthat
maximize the probability . We then itera-
tively hillclimb by modifying E and the alignment
between E and F so as to maximize the formula
. We hillclimb by modifying an exist-
ing alignment/translation through a set of operations
that modify locally the aligment/translation built un-
til a given time. These operations replace the En-
glish side of an alignment with phrases of differ-
ent probabilities, merge and break existing concepts,
and swap words across concepts. The probability
p(E) is computed using a simple trigram language
model that was trained using the CMU Language
Modeling Toolkit (Clarkson and Rosenfeld, 1997).
The language model is estimated at the word (not
phrase) level. Figure 3 shows the steps taken by our
decoder in order to find the translation of sentence
“je vais me arrˆeter l`a .” Each intermediate transla-
tion in Figure 3 is preceded by its probability and
succeded by the operation that changes it to yield a
translation of higher probability.
</bodyText>
<sectionHeader confidence="0.995087" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.995476266666667">
To evaluate our system, we trained both Giza (IBM
Model 4) (Al-Onaizan et al., 1999) and our joint
probability model on a French-English parallel cor-
pus of 100,000 sentence pairs from the Hansard
corpus. The sentences in the corpus were at most
20 words long. The English side had a total
of 1,073,480 words (21,484 unique tokens). The
French side had a total of 1,177,143 words (28,132
unique tokens).
We translated 500 unseen sentences, which were
uniformly distributed across lengths 6, 8, 10, 15, and
20. For each group of 100 sentences, we manu-
ally determined the number of sentences translated
perfectly by the IBM model decoder of Germann et
al. (2001) and the decoder that uses the joint prob-
</bodyText>
<table confidence="0.9978438">
Model Percent perfect translations IBM Bleu score
Sentence length Sentence length
6 8 10 15 20 Avg. 6 8 10 15 20 Avg.
IBM 36 26 35 11 2 22 0.2076 0.2040 0.2414 0.2248 0.2011 0.2158
Phrase-based 43 37 33 19 6 28 0.2574 0.2181 0.2435 0.2407 0.2028 0.2325
</table>
<tableCaption confidence="0.999848">
Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task.
</tableCaption>
<bodyText confidence="0.356053">
je vais me arreter la .
je vais me arreter la .
9.46e−08
i am going to stop there .
</bodyText>
<figureCaption confidence="0.962472">
Figure 3: Example of phrase-based greedy decod-
ing.
</figureCaption>
<bodyText confidence="0.9996635">
ability model. We also evaluated the translations
automatically, using the IBM-Bleu metric (Papineni
et al., 2002). The results in Table 1 show that the
phrased-based translation model proposed in this pa-
per significantly outperforms IBM Model 4 on both
the subjective and objective metrics.
</bodyText>
<sectionHeader confidence="0.999762" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.963349">
6.1 Limitations
</subsectionHeader>
<bodyText confidence="0.999997571428571">
The main shortcoming of the phrase-based model in
this paper concerns the size of the t-table and the
cost of the training procedure we currently apply.
To keep the memory requirements manageable, we
arbitrarily restricted the system to learning phrase
translations of at most six words on each side. Also,
the swap, break, and merge operations used dur-
ing the Viterbi training are computationally expen-
sive. We are currently investigating the applicability
of dynamic programming techniques to increase the
speed of the training procedure.
Clearly, there are language pairs for which it
would be helpful to allow concepts to be realized as
non-contiguous phrases. The English word “not”,
for example, is often translated into two French
words, “ne” and “pas”. But “ne” and “pas” al-
most never occur in adjacent positions in French
texts. At the outset of this work, we attempted to de-
velop a translation model that enables concepts to be
mapped into non-contiguous phrases. But we were
not able to scale and train it on large amounts of data.
The model described in this paper cannot learn that
the English word “not” corresponds to the French
words “ne” and “pas”. However, our model learns
to deal with negation by memorizing longer phrase
translation equivalents, such as (“ne est pas”, “is
not”); (“est inadmissible”, “is not good enough”);
and (“ne est pas ici”, “is not here”).
</bodyText>
<subsectionHeader confidence="0.999857">
6.2 Comparison with other work
</subsectionHeader>
<bodyText confidence="0.997905625">
A number of researchers have already gone be-
yond word-level translations in various MT set-
tings. For example, Melamed (2001) uses word-
level alignments in order to learn translations of non-
compositional compounds. Och and Ney (1999)
learn phrase-to-phrase mappings involving word
classes, which they call “templates”, and exploit
them in a statistical machine translation system. And
Marcu (2001) extracts phrase translations from au-
tomatically aligned corpora and uses them in con-
junction with a word-for-word statistical translation
system. However, none of these approaches learn
simultaneously the translation of phrases/templates
and the translation of words. As a consequence,
there is a chance that the learning procedure will not
discover phrase-level patterns that occur often in the
</bodyText>
<figure confidence="0.873956277777778">
je vais me arreter la .
7.50e−11 FuseAndChangeTrans(&amp;quot;la .&amp;quot;, &amp;quot;there .&amp;quot;)
i want me to that .
je vais me arreter la .
2.97e−10 ChangeWordTrans(&amp;quot;arreter&amp;quot;,&amp;quot;stop&amp;quot;)
7.75e−10
1.09e−09
i want me to there .
je vais me arreter la .
i want me stop there .
je vais me arreter la .
let me to stop there .
FuseAndChange(&amp;quot;je vais&amp;quot;,&amp;quot;let me&amp;quot;)
FuseAndChange(&amp;quot;je vais me&amp;quot;,
&amp;quot;i am going to&amp;quot;)
1.28e−14
changeWordTrans(&amp;quot;vais&amp;quot;, &amp;quot;want&amp;quot;)
i . me to that .
</figure>
<bodyText confidence="0.999882">
data. In our approach, phrases are not treated differ-
ently from individual words, and as a consequence
the likelihood of the EM algorithm converging to a
better local maximum is increased.
Working with phrase translations that are learned
independent of a translation model can also affect
the decoder performance. For example, in our pre-
vious work (Marcu, 2001), we have used a statis-
tical translation memory of phrases in conjunction
with a statistical translation model (Brown et al.,
1993). The phrases in the translation memory were
automatically extracted from the Viterbi alignments
produced by Giza (Al-Onaizan et al., 1999) and re-
used in decoding. The decoder described in (Marcu,
2001) starts from a gloss that uses the translations
in the translation memory and then tries to improve
on the gloss translation by modifying it incremen-
tally, in the style described in Section 4. How-
ever, because the decoder hill-climbs on a word-for-
word translation model probability, it often discards
good phrasal translations in favour of word-for-word
translations of higher probability. The decoder in
Section 4 does not have this problem because it hill-
climbs on translation model probabilities in which
phrases play a crucial role.
Acknowledgments. This work was supported by
DARPA-ITO grant N66001-00-1-9814 and by NSF-
STTR grant 0128379.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999822127659574">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation. Fi-
nal Report, JHU Summer Workshop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge toolkit.
In Proceedings of Eurospeech, September.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(Ser B):1–38.
Ulrich Germann, Mike Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL’01), pages 228–235,
Toulouse, France, July 6–11. Decoder available at
http://www.isi.edu/natural-language/projects/rewrite/.
Daniel Marcu. 2001. Towards a unified approach to
memory- and statistical-based machine translation. In
Proceedings of the 39th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL’01), pages
378–385, Toulouse, France, July 6–11.
Dan Melamed. 2001. Empirical Methods for Exploiting
Parallel Texts. The MIT Press.
Franz Josef Och, Christoph Tillmann, and Herman Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Joint Work-
shop on Empirical Methods in NLP and Very Large
Corpora, pages 20–28, University of Maryland, Mary-
land.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish results. In Pro-
ceedings of the Human Language Technology Confer-
ence, pages 124–127, San Diego, CA, March 24-27.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the Association for Compu-
tational Linguistics (ACL’01), Toulouse, France, July
6–11.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.019877">
<note confidence="0.9897525">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 133-139.</note>
<title confidence="0.799985">Association for Computational Linguistics. A Phrase-Based, Joint Probability Model for Statistical Machine Translation</title>
<author confidence="0.9458905">Daniel William Information Sciences Institute Language Weaver</author>
<affiliation confidence="0.9860695">Department of Computer 1639 11th St., Suite University of Southern Santa Monica, CA</affiliation>
<address confidence="0.954136">4676 Admiralty Way, Suite wwong@languageweaver.com</address>
<author confidence="0.999199">Marina del Rey</author>
<author confidence="0.999199">CA</author>
<email confidence="0.999144">marcu@isi.edu</email>
<abstract confidence="0.996659449504951">We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4. 1 Motivation Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models. In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977). The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences. A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few. Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are into target individual words may contain a non-existent element, called NULL. We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons. Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons. Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons. Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results. Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1. Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c. Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the probabilities shown in Figure Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999). IBM−4 T−Table IBM−4 Intuitive Joint Joint T−Table p(y  |a) = 1 p(x  |c) = 1 p(z  |b) = 0.98 p(x  |b) = 0.02 S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c p(x y, a b c) = 0.32 p(x, b c) = 0.34 p(y, a) = 0.01 p(z, b) = 0.33 Corresponding Conditional Table T2: x T2: x T2: x p(x y  |a b c ) = 1 p(x  |b c) = 1 p(y  |a) = 1 p(z  |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities. (Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).) In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well. In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously. In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications. The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences. In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3). We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5). We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature. 2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story. We assume that each sentence pair in our corpus is generated by the following stochastic process: 1. Generate a bag of concepts. 2. For each concept , generate a pair of phrases , according to the distribution contain at least one word. 3. Order the phrases generated in each language so as to create two linear sequences of phrases; these sequences correspond to the sentence pairs in a bilingual corpus. For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions. We do not assume that is a hidden variable that generates pair , but rather that . Under these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts given by the product of all phrase-tophrase translation probabilities, yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair “a b c” — “x y” can be generated using two concepts, (“a b” : “y”) and (“c” : “x”); or one concept, (“a b c” : “x y”), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair. However, the same sentence pair cannot be generated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”. Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”. We say that a set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by permuting the phrasesandthat characterize concepts . We denote this property using the predicate . Under this model, the probability of a given sentence pair (E, F) can then be obtained by summing up over all possible ways of generating bags of concepts that can linearized to (E, F). (1) 2.2 Model 2 Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good alignments. However, this model is clearly unsuited for translating unseen sentences as it imposes no constraints on the ordering of the phrases associated with a given concept. In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions. The generative story of Model 2 is this: 1. Generate a bag of concepts. 2. Initialize E and F to empty sequences. 3. Randomly take a concept and generate a pair of phrases , according to the distribution , whereandeach contain at least one word. Remove then from. 4. Append phraseat the end of F. Letbe the start position ofin F. 5. Insert phraseat positionin E provided that no other phrase occupies any of the positions betweenand , wheregives length of the phrase. We hence create the alignment between the two phrasesand with probability is a position-based distortion distribution. 6. Repeat steps 3 to 5 untilis empty. In Model 2, the probability to generate a sentence pair (E, F) is given by formula (2), where the position of wordof phrasein sen- F and denotes the position in tence E of the center of mass of phrase. (2) Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3. We have tried many types of distortion models. We eventually settled for the model discussed here because it produces better translations during decoding. Since the number of factors involved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input. 3 Training Training the models described in Section 2 is computationally challenging. Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the 1. Determine high-frequency ngrams in the bilingual corpus. 2. Initialize the t-distribution table. 3. Apply EM training on the Viterbi alignments, while using smoothing. 4. Generate conditional model probabilities. Figure 2: Training algorithm for the phrase-based joint probability model. EM training algorithm exhaustively. To estimate the parameters of our model, we apply the algorithm in Figure 2, whose steps are motivated and described below. 3.1 Determine high-frequency n-grams in E and F If one assumes from the outset that any phrases can be generated from a cept , one would need a supercomputer in order to store in the memory a table that models the distribution. Since we don’t have access to computers with unlimited memory, we initially learn t distribution entries only for the phrases that occur often in the corpus and for unigrams. Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well. In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus. 3.2 Initialize the t-distribution table Before the EM training procedure starts, one has no idea what word/phrase pairs are likely to share the same meaning. In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability. Under these conditions, the evidence that a sentence pair (E, F) contributes to fact that are generated by the same cept is given by the number of alignments that can be built between (E, F) that have a concept that is linked to phrasein sentence E and phrase sentence F divided by the total number of alignments that can be built between the two sentences. Both these numbers can be easily approximated. Given a sentence E ofwords, there are ways in which thewords can be partitioned into sets/concepts, where is the ling number of second kind. There are also ways in which the words a sentence F can be partitioned into nonempty sets. Given that any words in E can be mapped to any words in F, it follows that there are alignments that can be built between two sentences (E, F) of lengthsand , respectively. When a concept generates two phrases of lengthand, respectively, there are only and words left to link. Hence, the absence of any other information, the probability that phrasesandare generated by the same concept is given by formula (4). Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive. However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice. In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus. We sum over all these t-counts and we normalize to obtain an initial joint distribution. This step amounts to running the EM algorithm for one step over all possible alignments in the corpus. 3.3 EM training on Viterbi alignments Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time. Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities. We then hillclimb towards the Viterbi alignment of highest probability by breaking and merging concepts, swapping words between concepts, and moving words across concepts. We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments. We apply this Viterbi-based EM training procedure for a few iterations. The first iterations estimate the alignment probabilities using Model 1. The rest of the iterations estimate the alignment probabilities using Model 2. During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus. 3.4 Derivation of conditional probability model At the end of the training procedure, we take marginals on the joint probability distributionsand . This yields conditional probability distributions and which we use for decoding. 3.5 Discussion When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e. At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible. However, note that the choice made by our model is quite reasonable. After all, in the absence of additional information, the model can either assume that “a” and “y” mean the same thing or that phrases “a b c” and “x y” mean the same thing. The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one. Also note that although the joint distribution puts the second hypothesis at an advantage, the conditional distribution does not. The conditional distribution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate “a b c” into “x y”, as well as “a” into “y”. The conditional distribution mirrors perfectly our intuitions. 4 Decoding For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001). Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat the probability . We then tively hillclimb by modifying E and the alignment between E and F so as to maximize the formula . We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time. These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts. The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). The language model is estimated at the word (not phrase) level. Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrˆeter l`a .” Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability. 5 Evaluation To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were at most 20 words long. The English side had a total of 1,073,480 words (21,484 unique tokens). The French side had a total of 1,177,143 words (28,132 unique tokens). We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20. For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et (2001) and the decoder that uses the joint prob- Model Percent perfect translations IBM Bleu score Sentence length Sentence length 6 8 10 15 20 Avg. 6 8 10 15 20 Avg. IBM 36 26 35 11 2 22 0.2076 0.2040 0.2414 0.2248 0.2011 0.2158 Phrase-based 43 37 33 19 6 28 0.2574 0.2181 0.2435 0.2407 0.2028 0.2325 Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task. je vais me arreter la . je vais me arreter la . 9.46e−08 i am going to stop there . Figure 3: Example of phrase-based greedy decoding. ability model. We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002). The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics. 6 Discussion 6.1 Limitations The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply. To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side. Also, the swap, break, and merge operations used during the Viterbi training are computationally expensive. We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure. Clearly, there are language pairs for which it would be helpful to allow concepts to be realized as non-contiguous phrases. The English word “not”, for example, is often translated into two French words, “ne” and “pas”. But “ne” and “pas” almost never occur in adjacent positions in French texts. At the outset of this work, we attempted to develop a translation model that enables concepts to be mapped into non-contiguous phrases. But we were not able to scale and train it on large amounts of data. The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”. However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”). 6.2 Comparison with other work A number of researchers have already gone beyond word-level translations in various MT settings. For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds. Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call “templates”, and exploit them in a statistical machine translation system. And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system. However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words. As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the je vais me arreter la . 7.50e−11 FuseAndChangeTrans(&amp;quot;la .&amp;quot;, &amp;quot;there .&amp;quot;) i want me to that . je vais me arreter la . 2.97e−10 ChangeWordTrans(&amp;quot;arreter&amp;quot;,&amp;quot;stop&amp;quot;) 7.75e−10 1.09e−09 i want me to there . je vais me arreter la . i want me stop there . je vais me arreter la . let me to stop there . FuseAndChange(&amp;quot;je vais&amp;quot;,&amp;quot;let me&amp;quot;) FuseAndChange(&amp;quot;je vais me&amp;quot;, &amp;quot;i am going to&amp;quot;) 1.28e−14 changeWordTrans(&amp;quot;vais&amp;quot;, &amp;quot;want&amp;quot;) i . me to that . data. In our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased. Working with phrase translations that are learned independent of a translation model can also affect the decoder performance. For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993). The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding. The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4. However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability. The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role.</abstract>
<note confidence="0.825439576923077">work was supported by DARPA-ITO grant N66001-00-1-9814 and by NSF- STTR grant 0128379. References Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation. Final Report, JHU Summer Workshop. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estima- 19(2):263–311. Philip Clarkson and Ronald Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge toolkit. of September. A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM of the Royal Statistical 39(Ser B):1–38. Ulrich Germann, Mike Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and decoding for machine translation. In Proceedings of the 39th Annual Meeting of the Association for Linguistics pages 228–235, Toulouse, France, July 6–11. Decoder available at</note>
<web confidence="0.821976">http://www.isi.edu/natural-language/projects/rewrite/.</web>
<note confidence="0.97978175">Daniel Marcu. 2001. Towards a unified approach to memoryand statistical-based machine translation. In Proceedings of the 39th Annual Meeting of the Associfor Computational Linguistics pages 378–385, Toulouse, France, July 6–11. Melamed. 2001. Methods for Exploiting The MIT Press. Franz Josef Och, Christoph Tillmann, and Herman Ney. 1999. Improved alignment models for statistical matranslation. In of the Joint Workshop on Empirical Methods in NLP and Very Large pages 20–28, University of Maryland, Maryland. Kishore Papineni, Salim Roukos, Todd Ward, John Henderson, and Florence Reeder. 2002. Corpus-based comprehensive and diagnostic MT evaluation: Initial Chinese, French, and Spanish results. In Proceedings of the Human Language Technology Conferpages 124–127, San Diego, CA, March 24-27. Kenji Yamada and Kevin Knight. 2001. A syntax-based translation model. In of the 39th Annual Meeting of the Association for Compu- Linguistics Toulouse, France, July 6–11.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation. Final Report, JHU Summer Workshop.</title>
<date>1999</date>
<contexts>
<context position="3607" citStr="Al-Onaizan et al., 1999" startWordPosition="583" endWordPosition="586">tely, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a.2 Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure 2To train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999). IBM−4 T−Table IBM−4 Intuitive Joint Joint T−Table p(y |a) = 1 p(x |c) = 1 p(z |b) = 0.98 p(x |b) = 0.02 S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c p(x y, a b c) = 0.32 p(x, b c) = 0.34 p(y, a) = 0.01 p(z, b) = 0.33 Corresponding Conditional Table T2: x T2: x T2: x p(x y |a b c ) = 1 p(x |b c) = 1 p(y |a) = 1 p(z |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities. (Note that another good word-for-word model </context>
<context position="16934" citStr="Al-Onaizan et al., 1999" startWordPosition="2919" endWordPosition="2922">s across concepts. The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). The language model is estimated at the word (not phrase) level. Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrˆeter l`a .” Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability. 5 Evaluation To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were at most 20 words long. The English side had a total of 1,073,480 words (21,484 unique tokens). The French side had a total of 1,177,143 words (28,132 unique tokens). We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20. For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et al. (2001) and the decoder that use</context>
<context position="21619" citStr="Al-Onaizan et al., 1999" startWordPosition="3687" endWordPosition="3690"> our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased. Working with phrase translations that are learned independent of a translation model can also affect the decoder performance. For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993). The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding. The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4. However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability. The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role. </context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation. Final Report, JHU Summer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="953" citStr="Brown et al., 1993" startWordPosition="127" endWordPosition="130">rtment of Computer Science 1639 11th St., Suite 100A University of Southern California Santa Monica, CA 90404 4676 Admiralty Way, Suite 1001 wwong@languageweaver.com Marina del Rey, CA, 90292 marcu@isi.edu Abstract We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4. 1 Motivation Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models. In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977). The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences. A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knigh</context>
<context position="3158" citStr="Brown et al., 1993" startWordPosition="499" endWordPosition="502"> in Figure 1. Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c. Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a.2 Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure 2To train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999). IBM−4 T−Table IBM−4 Intuitive Joint Joint T−Table p(y |a) = 1 p(x |c) = 1 p(z |b) = 0.98 p(x |b) = 0.02 S1: a b c T1: x y S2: b c S1: a b c T1: x y S</context>
<context position="4525" citStr="Brown et al., 1993" startWordPosition="784" endWordPosition="787">x p(x y |a b c ) = 1 p(x |b c) = 1 p(y |a) = 1 p(z |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities. (Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).) In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well. In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously. In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications. The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only betw</context>
<context position="21480" citStr="Brown et al., 1993" startWordPosition="3667" endWordPosition="3670">ge(&amp;quot;je vais&amp;quot;,&amp;quot;let me&amp;quot;) FuseAndChange(&amp;quot;je vais me&amp;quot;, &amp;quot;i am going to&amp;quot;) 1.28e−14 changeWordTrans(&amp;quot;vais&amp;quot;, &amp;quot;want&amp;quot;) i . me to that . data. In our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased. Working with phrase translations that are learned independent of a translation model can also affect the decoder performance. For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993). The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding. The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4. However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability. The deco</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<contexts>
<context position="16487" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="2841" endWordPosition="2844"> inthat maximize the probability . We then iteratively hillclimb by modifying E and the alignment between E and F so as to maximize the formula . We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time. These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts. The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). The language model is estimated at the word (not phrase) level. Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrˆeter l`a .” Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability. 5 Evaluation To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were </context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Philip Clarkson and Ronald Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge toolkit. In Proceedings of Eurospeech, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<pages>1--38</pages>
<contexts>
<context position="1231" citStr="Dempster et al., 1977" startWordPosition="170" endWordPosition="173"> translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4. 1 Motivation Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models. In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977). The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences. A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few. Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are individually translated into ta</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(Ser B):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Mike Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’01),</booktitle>
<pages>228--235</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="15777" citStr="Germann et al. (2001)" startWordPosition="2724" endWordPosition="2727">ean the same thing. The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one. Also note that although the joint distribution puts the second hypothesis at an advantage, the conditional distribution does not. The conditional distribution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate “a b c” into “x y”, as well as “a” into “y”. The conditional distribution mirrors perfectly our intuitions. 4 Decoding For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001). Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat maximize the probability . We then iteratively hillclimb by modifying E and the alignment between E and F so as to maximize the formula . We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time. These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts. The probability p(E) is computed using a simple </context>
<context position="17509" citStr="Germann et al. (2001)" startWordPosition="3016" endWordPosition="3019">h Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were at most 20 words long. The English side had a total of 1,073,480 words (21,484 unique tokens). The French side had a total of 1,177,143 words (28,132 unique tokens). We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20. For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et al. (2001) and the decoder that uses the joint probModel Percent perfect translations IBM Bleu score Sentence length Sentence length 6 8 10 15 20 Avg. 6 8 10 15 20 Avg. IBM 36 26 35 11 2 22 0.2076 0.2040 0.2414 0.2248 0.2011 0.2158 Phrase-based 43 37 33 19 6 28 0.2574 0.2181 0.2435 0.2407 0.2028 0.2325 Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task. je vais me arreter la . je vais me arreter la . 9.46e−08 i am going to stop there . Figure 3: Example of phrase-based greedy decoding. ability model. We also evaluated the translations automatically, using the IBM</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Mike Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’01), pages 228–235, Toulouse, France, July 6–11. Decoder available at http://www.isi.edu/natural-language/projects/rewrite/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>Towards a unified approach to memory- and statistical-based machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’01),</booktitle>
<pages>378--385</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="20162" citStr="Marcu (2001)" startWordPosition="3457" endWordPosition="3458">l with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”). 6.2 Comparison with other work A number of researchers have already gone beyond word-level translations in various MT settings. For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds. Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call “templates”, and exploit them in a statistical machine translation system. And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system. However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words. As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the je vais me arreter la . 7.50e−11 FuseAndChangeTrans(&amp;quot;la .&amp;quot;, &amp;quot;there .&amp;quot;) i want me to that . je vais me arreter la . 2.97e−10 ChangeWordTrans(&amp;quot;arreter&amp;quot;,&amp;quot;stop&amp;quot;) 7.75e−10 1.09e−09 i want me to there . je vai</context>
<context position="21682" citStr="Marcu, 2001" startWordPosition="3700" endWordPosition="3701">nd as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased. Working with phrase translations that are learned independent of a translation model can also affect the decoder performance. For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993). The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding. The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4. However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability. The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role. Acknowledgments. This work was supported by DARPA-ITO grant N66</context>
</contexts>
<marker>Marcu, 2001</marker>
<rawString>Daniel Marcu. 2001. Towards a unified approach to memory- and statistical-based machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’01), pages 378–385, Toulouse, France, July 6–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Empirical Methods for Exploiting Parallel Texts.</title>
<date>2001</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="4785" citStr="Melamed (2001)" startWordPosition="832" endWordPosition="833">at another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).) In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well. In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously. In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications. The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences. In the rest of the paper, we first describe our model (Section 2) and exp</context>
<context position="19891" citStr="Melamed (2001)" startWordPosition="3418" endWordPosition="3419">ncepts to be mapped into non-contiguous phrases. But we were not able to scale and train it on large amounts of data. The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”. However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”). 6.2 Comparison with other work A number of researchers have already gone beyond word-level translations in various MT settings. For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds. Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call “templates”, and exploit them in a statistical machine translation system. And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system. However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words. As a consequence, there is a chance that the learning proced</context>
</contexts>
<marker>Melamed, 2001</marker>
<rawString>Dan Melamed. 2001. Empirical Methods for Exploiting Parallel Texts. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Herman Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint Workshop on Empirical Methods in NLP and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<institution>University of Maryland,</institution>
<location>Maryland.</location>
<contexts>
<context position="1517" citStr="Och et al., 1999" startWordPosition="216" endWordPosition="219">istical machine translation (MT) (Brown et al., 1993) are conditional probability models. In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977). The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences. A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few. Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are individually translated into target words.1 1The individual words may contain a non-existent element, called NULL. We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons. Large scale bilingual corpora with vocabularies in </context>
<context position="4543" citStr="Och et al., 1999" startWordPosition="788" endWordPosition="791"> p(x |b c) = 1 p(y |a) = 1 p(z |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities. (Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).) In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well. In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously. In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications. The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Herman Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the Joint Workshop on Empirical Methods in NLP and Very Large Corpora, pages 20–28, University of Maryland, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>John Henderson</author>
<author>Florence Reeder</author>
</authors>
<title>Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<pages>124--127</pages>
<location>San Diego, CA,</location>
<contexts>
<context position="18145" citStr="Papineni et al., 2002" startWordPosition="3131" endWordPosition="3134">er that uses the joint probModel Percent perfect translations IBM Bleu score Sentence length Sentence length 6 8 10 15 20 Avg. 6 8 10 15 20 Avg. IBM 36 26 35 11 2 22 0.2076 0.2040 0.2414 0.2248 0.2011 0.2158 Phrase-based 43 37 33 19 6 28 0.2574 0.2181 0.2435 0.2407 0.2028 0.2325 Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task. je vais me arreter la . je vais me arreter la . 9.46e−08 i am going to stop there . Figure 3: Example of phrase-based greedy decoding. ability model. We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002). The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics. 6 Discussion 6.1 Limitations The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply. To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side. Also, the swap, break, and merge operations used during the Viterbi training are computation</context>
</contexts>
<marker>Papineni, Roukos, Ward, Henderson, Reeder, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, John Henderson, and Florence Reeder. 2002. Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results. In Proceedings of the Human Language Technology Conference, pages 124–127, San Diego, CA, March 24-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’01),</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="1561" citStr="Yamada and Knight, 2001" startWordPosition="222" endWordPosition="225">wn et al., 1993) are conditional probability models. In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977). The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences. A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few. Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are individually translated into target words.1 1The individual words may contain a non-existent element, called NULL. We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons. Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield ver</context>
<context position="4569" citStr="Yamada and Knight, 2001" startWordPosition="792" endWordPosition="796"> |a) = 1 p(z |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities. (Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).) In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well. In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously. In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications. The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we sh</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’01), Toulouse, France, July 6–11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>