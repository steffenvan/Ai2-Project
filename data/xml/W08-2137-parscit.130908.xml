<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013439">
<title confidence="0.9976345">
Dependency Tree-based SRL with Proper Pruning and Extensive
Feature Engineering
</title>
<author confidence="0.968301">
Hongling Wang Honglin Wang Guodong Zhou Qiaoming Zhu
</author>
<affiliation confidence="0.917389">
JiangSu Provincial Key Lab for Computer Information Processing Technology
School of Computer Science and Technology,
</affiliation>
<address confidence="0.941948">
Soochow University, Suzhou, China 215006
</address>
<email confidence="0.99907">
{redleaf, 064227065055,gdzhou, qmzhu}@suda.edu.cn
</email>
<sectionHeader confidence="0.993904" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999427086956522">
This paper proposes a dependency tree-
based SRL system with proper pruning and
extensive feature engineering. Official
evaluation on the CoNLL 2008 shared task
shows that our system achieves 76.19 in la-
beled macro F1 for the overall task, 84.56
in labeled attachment score for syntactic
dependencies, and 67.12 in labeled F1 for
semantic dependencies on combined test
set, using the standalone MaltParser. Be-
sides, this paper also presents our unofficial
system by 1) applying a new effective
pruning algorithm; 2) including additional
features; and 3) adopting a better depend-
ency parser, MSTParser. Unofficial evalua-
tion on the shared task shows that our sys-
tem achieves 82.53 in labeled macro F1,
86.39 in labeled attachment score, and
78.64 in labeled F1, using MSTParser on
combined test set. This suggests that proper
pruning and extensive feature engineering
contributes much in dependency tree-based
SRL.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996607875">
Although CoNLL 2008 shared task mainly
evaluates joint learning of syntactic and semantic
parsing, we focus on dependency tree-based se-
mantic role labeling (SRL). SRL refers to label
the semantic roles of predicates (either verbs or
nouns) in a sentence. Most of previous SRL sys-
tems (Gildea and Jurafsky, 2002; Gildea and
Palmer, 2002; Punyakanok et al., 2005; Pradhan
</bodyText>
<footnote confidence="0.930827">
© 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
</footnote>
<bodyText confidence="0.999686432432432">
et al., 2004, 2005) work on constituent structure
trees and has shown to achieve remarkable re-
sults. For example, Punyakanok et al. (2005)
achieved the best performance in the CoNLL
2005 shared task with 79.44 in F-measure on the
WSJ test set and 77.92 on the combined test set
(WSJ +Brown).
With rapid development of dependency pars-
ing in the last few years, more and more re-
searchers turn to dependency tree-based SRL
with hope to advance SRL from viewpoint of
dependency parsing. Hacioglu (2004) pioneered
this work by formulating SRL as a classification
problem of mapping various dependency rela-
tions into semantic roles. Compared with previ-
ous researches on constituent structure tree-based
SRL which adopts constituents as labeling units,
dependency tree-based SRL adopts dependency
relations as labeling units. Due to the difference
between constituent structure trees and depend-
ency trees, their feature spaces are expected to be
somewhat different.
In the CoNLL 2008 shared task, we extend the
framework by Hacioglu (2004) with maximum
entropy as our classifier. For evaluation, we will
mainly report our official SRL performance us-
ing MaltParser (Nivre and Nilsson, 2005). Be-
sides, we will also present our unofficial system
by 1) applying a new effective pruning algorithm;
2) including additional features; and 3) adopting
a better dependency parser, MSTParser (McDon-
ald, 2005).
In the remainder of this paper, we will briefly
describe our system architecture, present various
features used by our models and report the per-
formance on CoNLL 2008 shared task (both offi-
cial and unofficial).
</bodyText>
<page confidence="0.978845">
253
</page>
<note confidence="0.3745915">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 253–257
Manchester, August 2008
</note>
<sectionHeader confidence="0.931036" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999823777777778">
In CoNLL 2008 shared task, we adopt a standard
three-stage process for SRL: pruning, argument
identification and argument classification. To
model the difference between verb and noun
predicates, we carry out separate training and
testing for verb and noun predicates respectively.
In addition, we adopt OpenNLP maximum en-
tropy package1 in argument identification and
classification.
</bodyText>
<subsectionHeader confidence="0.991892">
2.1 Predicate identification
</subsectionHeader>
<bodyText confidence="0.908326">
Most of Previous SRL systems only consider
given predicates. However, predicates are not
given in CoNLL 2008 shared task and required
to be determined automatically by the system.
Therefore, the first step of the shared task is to
identify the verb and noun predicates in a sen-
tence. Due to time limitation, a simple algorithm
is developed to identify noun and verb predicates:
1) For the WSJ corpus, we simply adopt the
annotations provided by PropBank and
NomBank. That is, we only consider the verb
and noun predicates annotated in PropBank
and NomBank respectively.
2) For the Brown corpus, verb predicates are
identified simply according to its POS tag
and noun predicates are determined using a
simple method that only those nouns which
can also be used as verbs are identified. To
achieve this goal, an English lexicon of about
56K word is applied to identify noun predi-
cates.
Evaluation on the test set of CoNLL 2008
shared task shows that our simple predicate iden-
tification algorithm achieves the accuracies of
98.6% and 92.7 in the WSJ corpus for verb and
noun predicates respectively, with overall accu-
racy of 95.5%, while it achieves the accuracies of
73.5% and 43.1% in the Brown corpus for verb
and noun predicates respectively with overall
accuracy of 61.8%. This means that the perform-
ance of predicate identification in the Brown
corpus is much lower than the one in the WSJ
corpus. This further suggests that much work is
required to achieve reasonable predicate identifi-
cation performance in future work.
</bodyText>
<subsectionHeader confidence="0.998482">
2.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.998506">
Using the dependency relations returned by a
dependency parser (either MaltParser or
</bodyText>
<page confidence="0.8888515">
1https://sourceforge.net/project/showfiles.php?group_id=59
61
</page>
<bodyText confidence="0.99999003030303">
MSTParser in this paper), we can construct cor-
responding dependency tree for a given sentence.
For example, Figure 1 shows the dependency
tree of the sentence “Meanwhile, overall evi-
dence on the economy remains fairly clouded.”.
Here, W is composed of two parts: word and its
POS tag with “/” as a separator while R means a
dependency relation and ARG represents a se-
mantic role.
In Hacioglu (2004), a simple pruning algo-
rithm is applied to filter out unlikely dependency
relation nodes in a dependency tree by only
keeping the parent/children/grand-children of the
predicate, the siblings of the predicates, and the
children/grandchildren of the siblings. This paper
extends the algorithm a little bit by including the
nodes two more layers upward and downward
with regard to the predicate’s parent, such as the
predicate’s grandparent, the grandparent’s chil-
dren and the grandchildren’s children. For the
example as shown in Figure 1, all the nodes in
the entire tree are kept. Evaluation on the training
set shows that our pruning algorithm signifi-
cantly reduces the training instances by 76.9%.
This is at expanse of wrongly pruning 1.0% se-
mantic arguments for verb predicates. However,
this figure increases to 43.5% for noun predicates
due to our later observation that about half of
semantic arguments of noun predicates distrib-
utes over ancestor nodes out of our consideration.
This suggests that a specific pruning algorithm is
necessary for noun predicates to include more
ancestor nodes.
</bodyText>
<subsectionHeader confidence="0.961214">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.991296454545455">
Some of the features are borrowed from Ha-
cioglu (2004) with some additional features mo-
tivated by constituent structure tree-based SRL
(Pradhan et al 2005; Xue and Palmer, 2004). In
the following, we explain these features and give
examples with regard to the dependency tree as
shown in Figure 1. We take the word evidence in
Figure 1 as the predicate and the node “on” as
the node on focus.
The following eight basic features are moti-
vated from constituent structure tree-based SRL:
</bodyText>
<listItem confidence="0.990639571428572">
1) Predicate: predicate lemma. (evidence)
2) Predicate POS: POS of current predicate.
(NN)
3) Predicate Voice: Whether the predicate (verb)
is realized as an active or passive construc-
tion. If the predicate is a noun, the value is
null and presented as “_”. ( _ )
</listItem>
<page confidence="0.996826">
254
</page>
<figureCaption confidence="0.850524">
Figure 1. Example of a dependency tree augmented with semantic roles
for the given predicate evidence.
</figureCaption>
<listItem confidence="0.979523846153846">
4) Relation type: the dependency relation type
of the current node. (NMOD)
5) Path: the chain of relations from current rela-
tion node to the predicate. (NMOD-&gt;SBJ)
6) Sub-categorization: The relation type of
predicate and the left-to-right chain of the re-
lation label sequence of the predicate’s chil-
dren. (SBJ-&gt;NMOD-NMOD)
7) Head word: the head word in the relation,
that is, the headword of the parent of the cur-
rent node. (evidence)
8) Position: the position of the headword of the
current node with respect to the predicate po-
</listItem>
<bodyText confidence="0.83976">
sition in the sentence, which can be before,
after or equal. (equal)
Besides, we also include following additional
features borrowed from Hacioglu (2004):
</bodyText>
<listItem confidence="0.999148166666667">
1) Family membership: the relationship be-
tween current node and the predicate node in
the family tree, such as parent, child, sibling.
(child)
2) Dependent word: the modifying word in the
relation, that is, the word of current node. (on)
3) POS of headword: the POS tag of the head-
word of current word. (NN)
4) POS of dependent word: the POS tag of cur-
rent word. (IN)
5) POS pattern of predicate&apos;s children: the
left-to-right chain of the POS tag sequence of
the predicate’s children. (JJ-IN)
6) Relation pattern of predicate’s children:
the left-to-right chain of the relation label se-
quence of the predicate’s children. (NMOD-
NMOD)
7) POS pattern of predicate’s siblings: the
left-to-right chain of the POS tag sequence of
the predicate’s siblings. (RB-.-VBN-.)
8) Relation pattern of predicate’s siblings: the
left-to-right chain of the relation label se-
quence of the predicate’s siblings. (TMP-P-
PRD-P)
</listItem>
<sectionHeader confidence="0.978022" genericHeader="method">
3 System Performance
</sectionHeader>
<bodyText confidence="0.999766125">
All the training data are included in our system,
which costs 70 minutes in training and 5 seconds
on testing on a PC platform with a Pentium D
3.0G CPU and 2G Memory. In particular, the
argument identification stage filters out those
nodes whose probabilities of not being semantic
arguments are more than 0.98 for verb and noun
predicates.
</bodyText>
<table confidence="0.9984298">
Labeled Labeled LAS
Macro F1 F1
Test WSJ 78.39 70.41 85.50
Test Brown 59.89 42.67 77.06
Test WSJ+Brown 76.19 67.12 84.56
</table>
<tableCaption confidence="0.855289333333333">
Table 1: Official performance using MaltParser
(with the SRL model trained and tested on the
automatic output of MaltParser)
</tableCaption>
<bodyText confidence="0.999834909090909">
All the performance is returned on the test set
using the CoNLL 2008 evaluation script
eval08.pl provided by the organizers. Table 1
shows the official performance using MaltParser
(with the SRL model trained and tested on the
automatic output of MaltParser provided by the
task organizers) as the dependency parser. It
shows that our system performs well on the WSJ
corpus and badly on the Brown corpus largely
due to bad performance on predicate identifica-
tion.
</bodyText>
<sectionHeader confidence="0.994703" genericHeader="method">
4 Post-evaluation System
</sectionHeader>
<bodyText confidence="0.9996945">
To gain more insights into dependency tree-
based SRL, we improve the system with a new
</bodyText>
<page confidence="0.995392">
255
</page>
<bodyText confidence="0.9987955">
pruning algorithm and additional features, after
submitting our official results.
</bodyText>
<subsectionHeader confidence="0.996356">
4.1 Effective pruning
</subsectionHeader>
<bodyText confidence="0.99998162962963">
Our new pruning algorithm is motivated by the
one proposed by Xue and Palmer (2004), which
only keeps those siblings to a node on the path
from current predicate to the root are included,
for constituent structure tree-based SRL. Our
pruning algorithm further cuts off the nodes
which are not related with the predicate. Besides,
it filters out those nodes which are punctuations
or with “symbol” dependency relations. Evalua-
tion on the Brown corpus shows that our pruning
algorithm significantly reduces the training data
by 75.5% at the expense of wrongly filtering out
0.7% and 0.5% semantic arguments for verb and
noun predicates respectively. This suggests that
our new pruning algorithm significantly performs
better than the old one in our official system, es-
pecially for the identification of noun predicates.
Furthermore, the argument identification stage
filters out those nodes whose probabilities of not
being semantic arguments are more than 0.90
and 0.85 for verb and noun predicates respec-
tively, since we that our original threshold of
0.98 in the official system is too reserved.
Finally, those rarely-occurred semantic roles
which occur less than 200 in the training set are
filtered out and thus not considered in our system,
such as A5, AA, C-A0, C-AM-ADV, R-A2 and SU.
</bodyText>
<subsectionHeader confidence="0.993909">
4.2 Extensive Feature Engineering
</subsectionHeader>
<bodyText confidence="0.998934">
Motivated by constituent structure tree-based
SRL, two more combined features are considered
in our post-evaluation system:
</bodyText>
<listItem confidence="0.997978">
1) Predicate + Headword: (evidence + remain)
2) Headword + Relation: (remain + Root)
</listItem>
<bodyText confidence="0.999909315789474">
In order to better evaluate the contribution of
various additional feature, we build a baseline
system using hand-corrected dependency rela-
tions and the eight basic features, motivated by
constituent structure tree-based SRL, as de-
scribed in Section 2.3. Table 2 shows the effect
of various additional features by adding one in-
dividually to the baseline system. It shows that
the feature of dependent word is most useful,
which improves the labeled F1 score from
81.38% to 84.84%. It also shows that the two
features about predicate’s sibling deteriorate the
performance. Therefore, we delete these two fea-
tures from remaining experiments. Although the
combined feature of “predicate+head word” is
useful in constituent structure tree-based SRL, it
slightly decrease the performance in dependency
tree-based SRL. For convenience, we include it
in our system.
</bodyText>
<table confidence="0.999812722222222">
P R F1
Baseline 84.31 78.64 81.38
+ Family membership 84.70 78.87 81.68
+ Dependent word 86.74 83.01 84.84
+ POS of headword 84.44 78.55 81.38
+ POS of dependent 84.42 78.33 81.47
word
+ POS pattern of 84.35 78.73 81.47
predicate&apos;s children
+ Relation pattern of 84.75 78.97 81.76
predicate’s children
+ Relation pattern of 84.29 78.52 81.30
predicate’s siblings
+ POS pattern of 83.75 78.32 80.95
predicate’s siblings
+ Predicate + Head- 83.30 78.94 81.30
word
+Headword + Relation 84.66 79.37 81.93
</table>
<tableCaption confidence="0.99859">
Table 2: Effects of various additional features
</tableCaption>
<subsectionHeader confidence="0.998907">
4.3 Best performance
</subsectionHeader>
<bodyText confidence="0.99991575862069">
Table 3 shows our system performance after ap-
plying above effective pruning strategy and addi-
tional features using the default MaltParser. Ta-
ble 3 also reports our performance using the
state-of-the-art MSTParser. To show the impact
of predicate identification in dependency tree-
based SRL, Table 4 report the performance on
gold predicate identification, i.e. only using an-
notated predicates in the corpora.
Comparison of Table 1 and Table 3 using the
MaltParser shows that our new extension with
effective pruning and extensive engineering sig-
nificantly improves the performance. It also
shows that MSTParser-based SRL performs
slightly better than MaltParser-based one, much
less than the performance difference on depend-
ency parsing between them. This suggests that
such difference between these two state-of-the-
art dependency parsers does not much affect cor-
responding SRL systems. This is also confirmed
by the results in Table 4.
Comparison of Table 3 and Table 4 in labeled
F1 on the Brown test data shows that the system
with gold predicate identification significantly
outperforms the one with automatic predicate
identification using our simple algorithm by
about 22 in labeled F1. This suggests that the
performance of predicate identification is critical
to SRL.
</bodyText>
<page confidence="0.993769">
256
</page>
<table confidence="0.999139285714286">
MSTParser MaltParser
Labeled Macro Labeled F1 LAS Labeled Macro Labeled F1 LAS
F1 F1
Test WSJ 84.50 81.95 87.01 83.69 81.82 85.50
Test Brown 67.61 53.69 81.46 65.09 53.03 77.06
Test 82.53 78.64 86.39 81.52 78.45 84.56
WSJ+Brown
</table>
<tableCaption confidence="0.99396">
Table 3: Unofficial performance using MSTParser and MaltParser
with predicates automatically identified
</tableCaption>
<table confidence="0.999316857142857">
MSTParser MaltParser
Labeled Macro Labeled F1 LAS Labeled Macro Labeled F1 LAS
F1 F1
Test WSJ 84.75 82.45 87.01 84.04 82.52 85.50
Test Brown 78.31 75.07 81.46 75.72 74.28 77.06
Test 84.05 81.66 86.39 83.13 81.64 84.56
WSJ+Brown
</table>
<tableCaption confidence="0.999565">
Table 4: Unofficial performance using MSTParser and MaltParser with gold predicate identification
</tableCaption>
<sectionHeader confidence="0.99891" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999974">
This paper presents a dependency tree-based
SRL system by proper pruning and extensive
feature engineering. Evaluation on the CoNLL
2008 shared task shows that proper pruning and
extensive feature engineering contributes much.
It also shows that SRL heavily depends on the
performance of predicate identification.
In future work, we will explore better ways in
predicate identification. In addition, we will ex-
plore more on dependency parsing and further
joint learning on syntactic and semantic parsing.
</bodyText>
<sectionHeader confidence="0.973807" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9253716">
This research is supported by Project 60673041
under the National Natural Science Foundation
of China and Project 2006AA01Z147 under the
“863” National High-Tech Research and Devel-
opment of China.
</bodyText>
<sectionHeader confidence="0.99546" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999803333333333">
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28:3, pages 245-288.
Gildea, Daniel and Martha Palmer. 2002. The Neces-
sity of Syntactic Parsing for Predicate Argument
Recognition. In Proceedings of the 40th Associa-
tion for Computational Linguistics, 2002.
Hacioglu, Kadri. 2004. Semantic Role Labeling Using
Dependency Trees. In Proceedings of the Interna-
tional Conference on Computational Linguistics
(COLING). 2004.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov,
Jan Hajiˇ. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In the pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in
Natural Language Processing, 2005
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Lluís Màrquez, and Joakim Nivre. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings
of the 12th Conference on Computational Natural
Language Learning (CoNLL-2008).
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics, pp. 99-106, 2005
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu,
James H. Martin, Dan Jurafsky. 2004. Shallow
Semantic Parsing Using Support Vector Machines.
In Proceedings of (HLT-NAACL-2004), 2004.
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu,
James H. Martin, Dan Jurafsky. 2005. Semantic
role labeling using different syntactic views. In
Proceedings of the 43rd Association for Computa-
tional Linguistics (ACL-2005), 2005.
Punyakanok, Vasin, Peter Koomen, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Pro-
ceedings of 9th Conference on Computational
Natural Language Learning (CoNLL-2005).2005
Xue, Nianwen and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2004.
</reference>
<page confidence="0.997181">
257
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.289268">
<title confidence="0.999009">Dependency Tree-based SRL with Proper Pruning and Feature Engineering</title>
<author confidence="0.88852">Hongling Wang Honglin Wang Guodong Zhou Qiaoming</author>
<affiliation confidence="0.9457675">JiangSu Provincial Key Lab for Computer Information Processing School of Computer Science and</affiliation>
<address confidence="0.576447">Soochow University, Suzhou, China 215006</address>
<email confidence="0.694507">redleaf@suda.edu.cn</email>
<email confidence="0.694507">064227065055@suda.edu.cn</email>
<email confidence="0.694507">gdzhou@suda.edu.cn</email>
<email confidence="0.694507">qmzhu@suda.edu.cn</email>
<abstract confidence="0.986919666666667">This paper proposes a dependency treebased SRL system with proper pruning and extensive feature engineering. Official evaluation on the CoNLL 2008 shared task shows that our system achieves 76.19 in labeled macro F1 for the overall task, 84.56 in labeled attachment score for syntactic dependencies, and 67.12 in labeled F1 for semantic dependencies on combined test set, using the standalone MaltParser. Besides, this paper also presents our unofficial system by 1) applying a new effective pruning algorithm; 2) including additional features; and 3) adopting a better dependency parser, MSTParser. Unofficial evaluation on the shared task shows that our system achieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<pages>245--288</pages>
<contexts>
<context position="1587" citStr="Gildea and Jurafsky, 2002" startWordPosition="232" endWordPosition="235">valuation on the shared task shows that our system achieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL. 1 Introduction Although CoNLL 2008 shared task mainly evaluates joint learning of syntactic and semantic parsing, we focus on dependency tree-based semantic role labeling (SRL). SRL refers to label the semantic roles of predicates (either verbs or nouns) in a sentence. Most of previous SRL systems (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005; Pradhan © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, mor</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, Daniel and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28:3, pages 245-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The Necessity of Syntactic Parsing for Predicate Argument Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1612" citStr="Gildea and Palmer, 2002" startWordPosition="236" endWordPosition="239">k shows that our system achieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL. 1 Introduction Although CoNLL 2008 shared task mainly evaluates joint learning of syntactic and semantic parsing, we focus on dependency tree-based semantic role labeling (SRL). SRL refers to label the semantic roles of predicates (either verbs or nouns) in a sentence. Most of previous SRL systems (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005; Pradhan © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, more and more researchers tu</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Gildea, Daniel and Martha Palmer. 2002. The Necessity of Syntactic Parsing for Predicate Argument Recognition. In Proceedings of the 40th Association for Computational Linguistics, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
</authors>
<title>Semantic Role Labeling Using Dependency Trees.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="2322" citStr="Hacioglu (2004)" startWordPosition="345" endWordPosition="346">mmercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, more and more researchers turn to dependency tree-based SRL with hope to advance SRL from viewpoint of dependency parsing. Hacioglu (2004) pioneered this work by formulating SRL as a classification problem of mapping various dependency relations into semantic roles. Compared with previous researches on constituent structure tree-based SRL which adopts constituents as labeling units, dependency tree-based SRL adopts dependency relations as labeling units. Due to the difference between constituent structure trees and dependency trees, their feature spaces are expected to be somewhat different. In the CoNLL 2008 shared task, we extend the framework by Hacioglu (2004) with maximum entropy as our classifier. For evaluation, we will m</context>
<context position="6082" citStr="Hacioglu (2004)" startWordPosition="932" endWordPosition="933">redicate identification performance in future work. 2.2 Preprocessing Using the dependency relations returned by a dependency parser (either MaltParser or 1https://sourceforge.net/project/showfiles.php?group_id=59 61 MSTParser in this paper), we can construct corresponding dependency tree for a given sentence. For example, Figure 1 shows the dependency tree of the sentence “Meanwhile, overall evidence on the economy remains fairly clouded.”. Here, W is composed of two parts: word and its POS tag with “/” as a separator while R means a dependency relation and ARG represents a semantic role. In Hacioglu (2004), a simple pruning algorithm is applied to filter out unlikely dependency relation nodes in a dependency tree by only keeping the parent/children/grand-children of the predicate, the siblings of the predicates, and the children/grandchildren of the siblings. This paper extends the algorithm a little bit by including the nodes two more layers upward and downward with regard to the predicate’s parent, such as the predicate’s grandparent, the grandparent’s children and the grandchildren’s children. For the example as shown in Figure 1, all the nodes in the entire tree are kept. Evaluation on the </context>
<context position="8735" citStr="Hacioglu (2004)" startWordPosition="1364" endWordPosition="1365">(NMOD) 5) Path: the chain of relations from current relation node to the predicate. (NMOD-&gt;SBJ) 6) Sub-categorization: The relation type of predicate and the left-to-right chain of the relation label sequence of the predicate’s children. (SBJ-&gt;NMOD-NMOD) 7) Head word: the head word in the relation, that is, the headword of the parent of the current node. (evidence) 8) Position: the position of the headword of the current node with respect to the predicate position in the sentence, which can be before, after or equal. (equal) Besides, we also include following additional features borrowed from Hacioglu (2004): 1) Family membership: the relationship between current node and the predicate node in the family tree, such as parent, child, sibling. (child) 2) Dependent word: the modifying word in the relation, that is, the word of current node. (on) 3) POS of headword: the POS tag of the headword of current word. (NN) 4) POS of dependent word: the POS tag of current word. (IN) 5) POS pattern of predicate&apos;s children: the left-to-right chain of the POS tag sequence of the predicate’s children. (JJ-IN) 6) Relation pattern of predicate’s children: the left-to-right chain of the relation label sequence of th</context>
</contexts>
<marker>Hacioglu, 2004</marker>
<rawString>Hacioglu, Kadri. 2004. Semantic Role Labeling Using Dependency Trees. In Proceedings of the International Conference on Computational Linguistics (COLING). 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇ</author>
</authors>
<title>Non-Projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In the proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇ, 2005</marker>
<rawString>McDonald, Ryan, Fernando Pereira, Kiril Ribarov, Jan Hajiˇ. 2005. Non-Projective Dependency Parsing using Spanning Tree Algorithms. In the proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluís Màrquez</author>
<author>Joakim Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</booktitle>
<marker>Surdeanu, Johansson, Meyers, Màrquez, Nivre, 2008</marker>
<rawString>Surdeanu, Mihai, Richard Johansson, Adam Meyers, Lluís Màrquez, and Joakim Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>PseudoProjective Dependency Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>99--106</pages>
<contexts>
<context position="3006" citStr="Nivre and Nilsson, 2005" startWordPosition="444" endWordPosition="447">problem of mapping various dependency relations into semantic roles. Compared with previous researches on constituent structure tree-based SRL which adopts constituents as labeling units, dependency tree-based SRL adopts dependency relations as labeling units. Due to the difference between constituent structure trees and dependency trees, their feature spaces are expected to be somewhat different. In the CoNLL 2008 shared task, we extend the framework by Hacioglu (2004) with maximum entropy as our classifier. For evaluation, we will mainly report our official SRL performance using MaltParser (Nivre and Nilsson, 2005). Besides, we will also present our unofficial system by 1) applying a new effective pruning algorithm; 2) including additional features; and 3) adopting a better dependency parser, MSTParser (McDonald, 2005). In the remainder of this paper, we will briefly describe our system architecture, present various features used by our models and report the performance on CoNLL 2008 shared task (both official and unofficial). 253 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 253–257 Manchester, August 2008 2 System Description In CoNLL 2008 shared task</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Nivre, Joakim and Jens Nilsson. 2005. PseudoProjective Dependency Parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 99-106, 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow Semantic Parsing Using Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proceedings of (HLT-NAACL-2004),</booktitle>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, James H. Martin, Dan Jurafsky. 2004. Shallow Semantic Parsing Using Support Vector Machines. In Proceedings of (HLT-NAACL-2004), 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Semantic role labeling using different syntactic views.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Association for Computational Linguistics (ACL-2005),</booktitle>
<contexts>
<context position="7350" citStr="Pradhan et al 2005" startWordPosition="1129" endWordPosition="1132">ificantly reduces the training instances by 76.9%. This is at expanse of wrongly pruning 1.0% semantic arguments for verb predicates. However, this figure increases to 43.5% for noun predicates due to our later observation that about half of semantic arguments of noun predicates distributes over ancestor nodes out of our consideration. This suggests that a specific pruning algorithm is necessary for noun predicates to include more ancestor nodes. 2.3 Features Some of the features are borrowed from Hacioglu (2004) with some additional features motivated by constituent structure tree-based SRL (Pradhan et al 2005; Xue and Palmer, 2004). In the following, we explain these features and give examples with regard to the dependency tree as shown in Figure 1. We take the word evidence in Figure 1 as the predicate and the node “on” as the node on focus. The following eight basic features are motivated from constituent structure tree-based SRL: 1) Predicate: predicate lemma. (evidence) 2) Predicate POS: POS of current predicate. (NN) 3) Predicate Voice: Whether the predicate (verb) is realized as an active or passive construction. If the predicate is a noun, the value is null and presented as “_”. ( _ ) 254 F</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2005</marker>
<rawString>Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, James H. Martin, Dan Jurafsky. 2005. Semantic role labeling using different syntactic views. In Proceedings of the 43rd Association for Computational Linguistics (ACL-2005), 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Peter Koomen</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Generalized inference with multiple semantic role labeling systems.</title>
<date>2005</date>
<booktitle>In Proceedings of 9th Conference on Computational Natural Language Learning (CoNLL-2005).2005</booktitle>
<contexts>
<context position="1637" citStr="Punyakanok et al., 2005" startWordPosition="240" endWordPosition="243">chieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL. 1 Introduction Although CoNLL 2008 shared task mainly evaluates joint learning of syntactic and semantic parsing, we focus on dependency tree-based semantic role labeling (SRL). SRL refers to label the semantic roles of predicates (either verbs or nouns) in a sentence. Most of previous SRL systems (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005; Pradhan © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, more and more researchers turn to dependency tree-bas</context>
</contexts>
<marker>Punyakanok, Koomen, Roth, Yih, 2005</marker>
<rawString>Punyakanok, Vasin, Peter Koomen, Dan Roth, and Wen-tau Yih. 2005. Generalized inference with multiple semantic role labeling systems. In Proceedings of 9th Conference on Computational Natural Language Learning (CoNLL-2005).2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="7373" citStr="Xue and Palmer, 2004" startWordPosition="1133" endWordPosition="1136">e training instances by 76.9%. This is at expanse of wrongly pruning 1.0% semantic arguments for verb predicates. However, this figure increases to 43.5% for noun predicates due to our later observation that about half of semantic arguments of noun predicates distributes over ancestor nodes out of our consideration. This suggests that a specific pruning algorithm is necessary for noun predicates to include more ancestor nodes. 2.3 Features Some of the features are borrowed from Hacioglu (2004) with some additional features motivated by constituent structure tree-based SRL (Pradhan et al 2005; Xue and Palmer, 2004). In the following, we explain these features and give examples with regard to the dependency tree as shown in Figure 1. We take the word evidence in Figure 1 as the predicate and the node “on” as the node on focus. The following eight basic features are motivated from constituent structure tree-based SRL: 1) Predicate: predicate lemma. (evidence) 2) Predicate POS: POS of current predicate. (NN) 3) Predicate Voice: Whether the predicate (verb) is realized as an active or passive construction. If the predicate is a noun, the value is null and presented as “_”. ( _ ) 254 Figure 1. Example of a d</context>
<context position="11017" citStr="Xue and Palmer (2004)" startWordPosition="1736" endWordPosition="1739">ficial performance using MaltParser (with the SRL model trained and tested on the automatic output of MaltParser provided by the task organizers) as the dependency parser. It shows that our system performs well on the WSJ corpus and badly on the Brown corpus largely due to bad performance on predicate identification. 4 Post-evaluation System To gain more insights into dependency treebased SRL, we improve the system with a new 255 pruning algorithm and additional features, after submitting our official results. 4.1 Effective pruning Our new pruning algorithm is motivated by the one proposed by Xue and Palmer (2004), which only keeps those siblings to a node on the path from current predicate to the root are included, for constituent structure tree-based SRL. Our pruning algorithm further cuts off the nodes which are not related with the predicate. Besides, it filters out those nodes which are punctuations or with “symbol” dependency relations. Evaluation on the Brown corpus shows that our pruning algorithm significantly reduces the training data by 75.5% at the expense of wrongly filtering out 0.7% and 0.5% semantic arguments for verb and noun predicates respectively. This suggests that our new pruning </context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Xue, Nianwen and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>