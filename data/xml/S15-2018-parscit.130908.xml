<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002159">
<title confidence="0.966385">
FBK-HLT: A New Framework for Semantic Textual Similarity
</title>
<author confidence="0.837647">
Ngoc Phuoc An Vo
</author>
<affiliation confidence="0.783868333333333">
Fondazione Bruno Kessler,
University of Trento
Trento, Italy
</affiliation>
<email confidence="0.995115">
ngoc@fbk.eu
</email>
<author confidence="0.997596">
Simone Magnolini
</author>
<affiliation confidence="0.720393666666667">
University of Brescia,
Fondazione Bruno Kessler
Trento, Italy
</affiliation>
<email confidence="0.996336">
magnolini@fbk.eu
</email>
<note confidence="0.355515666666667">
Octavian Popescu
IBM Research, T.J. Watson
Yorktown, US
</note>
<email confidence="0.993584">
o.popescu@us.ibm.com
</email>
<sectionHeader confidence="0.995548" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999713133333333">
This paper reports the description and perfor-
mance of our system, FBK-HLT, participat-
ing in the SemEval 2015, Task #2 “Semantic
Textual Similarity”, English subtask. We sub-
mitted three runs with different hypothesis in
combining typical features (lexical similarity,
string similarity, word n-grams, etc) with syn-
tactic structure features, resulting in different
sets of features. The results evaluated on both
STS 2014 and 2015 datasets prove our hypoth-
esis of building a STS system taking into con-
sideration of syntactic information. We out-
perform the best system on STS 2014 datasets
and achieve a very competitive result to the
best system on STS 2015 datasets.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961054054054">
Semantic related tasks have been a noticed trend
in Natural Language Processing (NLP) community.
Particularly, the task Semantic Textual Similarity
(STS) has captured a huge attention in the NLP com-
munity despite being recently introduced since Se-
mEval 2012 (Agirre et al., 2012). Basically, the
task requires to build systems which can compute
the similarity degree between two given sentences.
The similarity degree is scaled as a real score from
0 (no relevance) to 5 (semantic equivalence). The
evaluation is done by computing the correlation be-
tween human judgment scores and system scores by
the mean of Pearson correlation method.
At SemEval 2015, Task #2 “Semantic Textual
Similarity (STS)”, English STS subtask (Agirre et
al., 2015) evaluates participating systems on five test
datasets: image description (image), news headlines
(headlines), student answers paired with reference
answers (answers-students), answers to questions
posted in stach exchange forums (answers-forum),
and English discussion forum data exhibiting com-
mited belief (belief). As being inspired by the UKP
system (Bär et al., 2012), which was the best system
in STS 2012, we build a supervised system on top of
it. Our system adopts some word and string similar-
ity features in UKP, such as string similarity, charac-
ter/word n-grams, and pairwise similarity; however,
we also add other distinguished features, like syn-
tactic structure information, word alignment and se-
mantic word similarity. As a result, our team, FBK-
HLT, submitted three runs and achieve very compet-
itive results in the top-tier systems of the task.
The remainder of this paper is organized as fol-
lows: Section 2 presents the System Description,
Section 3 describes our Experiment Settings, Sec-
tion 4 reports the Evaluations of our system. Finally,
Section 5 is Conclusions and Future Work.
</bodyText>
<sectionHeader confidence="0.973844" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999984">
We describe our system, which is built from differ-
ent linguistic features. We construct a pipeline sys-
tem, in which each component produces different
features independently and at the end, all features
are consolidated by a machine learning tool, which
learns a regression model for predicting the similar-
ity scores from given sentence-pairs. On top of this,
the system is expandable and scalable for adopting
more useful features aiming for improving the accu-
racy. The System Overview in Figure 1 shows the
logic and design processes in which different com-
</bodyText>
<page confidence="0.986102">
102
</page>
<note confidence="0.751198">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 102–106,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999892">
Figure 1: System Overview.
</figureCaption>
<bodyText confidence="0.923315">
ponents connect and work together.
</bodyText>
<subsectionHeader confidence="0.982562">
2.1 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.999932833333333">
The input data undergoes the data preprocessing
in which we use Tree Tagger (Schmid, 1994) to
perform tokenization, lemmatization, and Part-of-
Speech (POS) tagging. On the other hand, we use
Stanford Parser (Klein and Manning, 2003) to ob-
tain the dependency parsing from given sentences.
</bodyText>
<subsectionHeader confidence="0.999533">
2.2 Word and String Similarity Features
</subsectionHeader>
<bodyText confidence="0.999435">
We adopt some word and string similarity features
from the UKP system (Bär et al., 2012), which are
briefly described as follows:
</bodyText>
<listItem confidence="0.985441823529412">
• String Similarity: we use Longest Common
Substring (Gusfield, 1997), Longest Common
Subsequence (Allison and Dix, 1986) and
Greedy String Tiling (Wise, 1996) measures.
• Character/Word n-grams: we compare charac-
ter n-grams (Barrón-Cedeno et al., 2010) with
the variance n=2, 3, ..., 15. In contrast, we com-
pare the word n-grams using Jaccard coefficient
done by Lyon (Lyon et al., 2001) and contain-
ment measure (Broder, 1997) with the variance
of n=1, 2, 3, and 4.
• Semantic Word Similarity: we use the pairwise
similarity algorithm by Resnik (Resnik, 1995)
on WordNet (Fellbaum, 1998), and the vector
space model Explicit Semantic Analysis (ESA)
(Gabrilovich and Markovitch, 2007) which is
constructed by two lexical semantic resources
</listItem>
<bodyText confidence="0.933615">
Wikipedia 1 and Wiktionary 2.
</bodyText>
<subsectionHeader confidence="0.999538">
2.3 Syntactic Structure Features
</subsectionHeader>
<bodyText confidence="0.999994">
We exploit the syntactic structure information by the
mean of three different toolkits: Syntactic Tree Ker-
nel, Distributed Tree Kernel and Syntactic Gener-
alization. We describe how each toolkit is used to
learn and extract the syntactic structure information
from texts to be used in our STS system.
</bodyText>
<subsectionHeader confidence="0.939351">
2.3.1 Syntactic Tree Kernel
</subsectionHeader>
<bodyText confidence="0.999975571428571">
Syntactic Tree Kernel (Moschitti, 2006) is a tree
kernels approach to learn the syntactic structure
from syntactic parsing information, particularly, the
Partial Tree (PT) kernel is proposed as a new convo-
lution kernel to fully exploit dependency trees. We
use the open-source toolkit &amp;quot;Tree Kernel in SVM-
Light3&amp;quot; to learn this syntactic information.
Having assumed that paraphrased pairs would
share the same content and similar syntactic struc-
tures, we decide to choose the Microsoft Research
Paraphrasing Corpus (Dolan et al., 2005) which
contains 5,800 sentence pairs extracted from news
sources on the web, along with human annota-
tions indicating whether each pair captures a para-
phrase/semantic equivalence relationship. This cor-
pus is split into Training set (4,076 pairs) and Test-
ing set (1,725 pairs).
We use Stanford Parser (Klein and Manning,
2003) to obtain the dependency parsing from sen-
tence pairs. Then we use the machine learning tool
svm-light-tk 1.2 which uses Tree Kernel approach to
learn the similarity of syntactic structure to build a
binary classifying model on the Train dataset. The
output predictions are probability confidence scores
in [-1,1], corresponds to the probability of the label
to be positive. According to the assumption above,
we label paraphrased pairs as 1, -1 otherwise. We
obtain the Accuracy of 69.16% on the Test set.
</bodyText>
<subsectionHeader confidence="0.883205">
2.3.2 Distributed Tree Kernel
</subsectionHeader>
<bodyText confidence="0.99491225">
Distributed Tree Kernel (DTK) (Zanzotto and
Dell’Arciprete, 2012) is a tree kernels method using
a linear complexity algorithm to compute vectors for
trees by embedding feature spaces of tree fragments
</bodyText>
<footnote confidence="0.999802666666667">
1http://en.wikipedia.org/wiki/Main_Page
2http://en.wiktionary.org
3http://disi.unitn.it/moschitti/Tree-Kernel.htm
</footnote>
<page confidence="0.979447">
103
</page>
<table confidence="0.999892285714286">
Settings deft-forum deft-news headlines images OnWN tweet-news Mean
Baseline 0.353 0.596 0.510 0.513 0.406 0.654 0.507
DLS@CU (ranked 1st) 0.4828 0.7657 0.7646 0.8214 0.8589 0.7639 0.761
Word/String Sim (1) 0.4314 0.7089 0.6887 0.7671 0.8125 0.6932 0.7008
Syntactic Features (2) 0.2402 0.3886 0.3233 0.2419 0.4066 0.4489 0.3441
(1) &amp; (2) 0.4495 0.7032 0.6902 0.7627 0.8115 0.6974 0.7026
All Features 0.5076 0.7616 0.7647 0.8182 0.8953 0.7485 0.7672
</table>
<tableCaption confidence="0.997996">
Table 1: Evaluation Results on STS 2014 datasets.
</tableCaption>
<table confidence="0.999673833333333">
System ans-forums ans-students belief headlines images Mean
Baseline 0.4453 0.6647 0.6517 0.5312 0.6039 0.5871
DLS@CU-S1 (ranked 1st) 0.739 0.7725 0.7491 0.825 0.8644 0.8015
FBK-HLT Run1 0.7131 0.7442 0.7327 0.8079 0.8574 0.7831
FBK-HLT Run2 0.7101 0.7410 0.7377 0.8008 0.8545 0.7801
FBK-HLT Run3 0.6555 0.7362 0.7460 0.7083 0.8389 0.7461
</table>
<tableCaption confidence="0.999654">
Table 2: Evaluation Results on STS 2015 datasets.
</tableCaption>
<bodyText confidence="0.9997642">
in low-dimensional spaces. Then a recursive algo-
rithm is proposed with linear complexity to compute
reduced vectors for trees. The dot product among
reduced vectors is used to approximate the original
tree kernel when a vector composition function with
specific ideal properties is used.
Firstly, we use Stanford Parser (PCFG Parser)
trained on Penn TreeBank (Klein and Manning,
2003) to obtain the dependency parsing of sen-
tences, and feed them to the software &amp;quot;distributed-
tree-kernels&amp;quot; to produce the distributed trees.4 Then,
we compute the Cosine similarity between the vec-
tors of distributed trees of each sentence pair. This
cosine similarity score is converted to the scale of
STS and SR for evaluation.
</bodyText>
<subsectionHeader confidence="0.771784">
2.3.3 Syntactic Generalization
</subsectionHeader>
<bodyText confidence="0.999899">
Given a pair of parse trees, the Syntactic General-
ization (SG) (Galitsky, 2013) finds a set of maximal
common subtrees. The toolkit &amp;quot;relevance-based-on-
parse-trees&amp;quot; is an open-source project which eval-
uates text relevance by using syntactic parse tree-
based similarity measure.5 Given a pair of parse
trees, it measures the similarity between two sen-
tences by finding a set of maximal common subtrees,
using representation of constituency parse trees via
chunking. Each type of phrases (NP, VP, PRP etc.)
</bodyText>
<footnote confidence="0.999532">
4https://code.google.com/p/distributed-tree-kernels
5https://code.google.com/p/relevance-based-on-parse-trees
</footnote>
<bodyText confidence="0.997341125">
will be aligned and subject to generalization. It uses
the OpenNLP system to derive dependency trees for
generalization (chunker and parser).6 This tool is
made to give as a tool for text relevance which can
be used as a black box, no understanding of compu-
tational linguistics or machine learning is required.
We apply the tool on the STS datasets to compute the
similarity of syntactic structure of sentence pairs.
</bodyText>
<subsectionHeader confidence="0.985052">
2.4 Further Features
</subsectionHeader>
<bodyText confidence="0.999991">
We also deploy other features which also may help
in identifying the semantic similarity degree be-
tween two given sentences, such as word align-
ment in machine translation evaluation metric and
the vector space model Weighted Matrix Factoriza-
tion (WMF) for pairwise similarity.
</bodyText>
<sectionHeader confidence="0.9849995" genericHeader="method">
2.4.1 Machine Translation Evaluation Metric -
METEOR
</sectionHeader>
<bodyText confidence="0.994868222222222">
METEOR (Metric for Evaluation of Translation
with Explicit ORdering) (Banerjee and Lavie, 2005)
is an automatic metric for machine translation eval-
uation, which consists of two major components:
a flexible monolingual word aligner and a scorer.
For machine translation evaluation, hypothesis sen-
tences are aligned to reference sentences. Align-
ments are then scored to produce sentence and cor-
pus level scores. We use this word alignment feature
</bodyText>
<footnote confidence="0.993978">
6https://opennlp.apache.org
</footnote>
<page confidence="0.998718">
104
</page>
<bodyText confidence="0.9995625">
to learn the similarity between words, phrases in two
given texts in case of different orders.
</bodyText>
<subsectionHeader confidence="0.465227">
2.4.2 Weighted Matrix Factorization (WMF)
</subsectionHeader>
<bodyText confidence="0.99975525">
WMF (Guo and Diab, 2012) is a dimension re-
duction model to extract nuanced and robust latent
vectors for short texts/sentences. To overcome the
sparsity problem in short texts/sentences (e.g. 10
words on average), the missing words, a feature that
LSA/LDA typically overlooks, is explicitly mod-
eled. We use the pipeline to compute the similarity
score between texts.
</bodyText>
<sectionHeader confidence="0.993272" genericHeader="method">
3 Experiment Settings
</sectionHeader>
<bodyText confidence="0.958914111111111">
We generate and select 25 optimal features, rang-
ing from lexical level to string level and syntac-
tic level. We deploy the machine learning toolkit
WEKA (Hall et al., 2009) for learning a regression
model (GaussianProcesses) to predict the similarity
scores. We build three models based on three sets of
features to verify our hypothesis in which we aug-
ment that computing semantic similarity degree is
not only about lexical similarity and string similar-
ity, but also taking into consideration a deeper level
at syntactic structure where more semantic informa-
tion is embedded.
In the system development process, we train our
system on the given datasets of STS 2012, 2013 and
use the STS 2014 datasets for evaluating the system.
In Table 1, we also examine the contribution of dif-
ferent features to the overall accuracy of system, and
prove that syntactic structure information also has
some impact to the performance of our system. Our
model using all features described above outperform
the best system DLS@CU in STS 2014 evaluation.
We submitted three runs with different sets of fea-
tures as below:
- Run1: All features described in Section 2 used.
- Run2: The feature obtained by Distributed Tree
Kernel approach is excluded as sometimes it returns
negative correlation.
</bodyText>
<listItem confidence="0.704886">
- Run3: No syntactic features are included.
</listItem>
<sectionHeader confidence="0.994734" genericHeader="method">
4 Evaluations
</sectionHeader>
<bodyText confidence="0.999963416666667">
In Table 2 we report the performance of our three
runs achieved on the STS 2015 test datasets. Among
three submitted runs, Run1 has the best score, which
confirm that exploiting the syntactic structure infor-
mation benefits the overall performance of our sys-
tem. Besides, although occasionally the features
extracted by Distributed Tree Kernel approach re-
turns negative result, it still contributes a small pos-
itive portion in the final result, which is shown in
the Run2. In contrast, the Run3 which excludes all
syntactic structure features, eventually, returns 4%
lower than the other two runs.
In overall, our system achieves a very compet-
itive result compared to the best ranked system,
DLS@CU-S1. Specifically, the difference between
our Run1 and the DLS@CU-S1 on each test dataset
of STS 2015 varies slightly 1%-2%. However, this
difference is not statistically significant, as we can
understand that each system may perform slightly
different on different evaluation datasets. Generally,
by taking into account the results of our system and
DLS@CU on both STS 2014 and 2015 evaluation
datasets, we can consider that we are almost equiva-
lent in performance.
</bodyText>
<sectionHeader confidence="0.989308" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999887954545455">
In this paper, we describe the pipeline system FBK-
HLT participating in the SemEval 2015, Task #2
&amp;quot;Semantic Textual Similarity&amp;quot;, English subtask. We
present a supervised system which considers mul-
tiple linguistic features from low to high language
level, such as lexical, string and syntactic. We also
augment that looking into the syntactic structure of
text will more or less benefit the capability of pre-
dicting the semantic similarity. Among our three
submitted runs, our performance is much above the
baseline and very competitive to the best system; we
are ranked in the top-tier (12th, 13th, and 23nd) out of
total 73 systems.
For the time being, we can see that the contri-
bution of syntactic features is still limited (about
4%) to the overall performance. However, it does
not deny the significance of syntactic information
in semantic related tasks, especially, this STS task.
Hence, we expect to study to exploit more useful
features from the syntactic information, which in-
tuitively, is supposed to play a significant role in se-
mantic reasoning.
</bodyText>
<page confidence="0.998944">
105
</page>
<sectionHeader confidence="0.988328" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999266351648352">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385–393.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-
cea, German Rigau, Larraitz Uria, and Janyce Wiebe.
2015. SemEval-2015 Task 2: Semantic Textual Simi-
larity, English, Spanish and Pilot on Interpretability. In
Proceedings of the 9th International Workshop on Se-
mantic Evaluation (SemEval 2015), Denver, CO, June.
Lloyd Allison and Trevor I Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23(5):305–310.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65–72.
Daniel Bär, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics-Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 435–440.
Alberto Barrón-Cedeno, Paolo Rosso, Eneko Agirre, and
Gorka Labaka. 2010. Plagiarism detection across dis-
tant language pairs. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
pages 37–45.
Andrei Z Broder. 1997. On the resemblance and contain-
ment of documents. In Compression and Complexity
of Sequences 1997, pages 21–29. IEEE.
Bill Dolan, Chris Brockett, and Chris Quirk. 2005. Mi-
crosoft research paraphrase corpus. Retrieved March,
29:2008.
Christiane Fellbaum. 1998. WordNet. Wiley Online Li-
brary.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI, volume 7, pages
1606–1611.
Boris Galitsky. 2013. Machine learning of syntac-
tic parse trees for search and classification of text.
Engineering Applications of Artificial Intelligence,
26(3):1072–1091.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 864–872.
Dan Gusfield. 1997. Algorithms on strings, trees and
sequences: computer science and computational biol-
ogy. Cambridge University Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten. 2009.
The WEKA data mining software: an update. ACM
SIGKDD explorations newsletter, 11(1):10–18.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in large
document collections. In Proceedings of the 2001
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 118–125.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Machine Learning: ECML 2006, pages 318–329.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence - Volume 1, IJCAI’95, pages 448–453.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of interna-
tional conference on new methods in language pro-
cessing, volume 12, pages 44–49. Manchester, UK.
Michael J Wise. 1996. Yap3: Improved detection of sim-
ilarities in computer program and other texts. In ACM
SIGCSE Bulletin, volume 28, pages 130–134. ACM.
Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete.
2012. Distributed tree kernels. In Proceedings of the
29th International Conference on Machine Learning.
</reference>
<page confidence="0.997311">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.092137">
<title confidence="0.8502215">FBK-HLT: A New Framework for Semantic Textual Similarity Ngoc Phuoc An</title>
<author confidence="0.94098">Fondazione Bruno</author>
<affiliation confidence="0.7808125">University of Trento,</affiliation>
<email confidence="0.994999">ngoc@fbk.eu</email>
<author confidence="0.975486">Simone</author>
<affiliation confidence="0.837339666666667">University of Fondazione Bruno Trento,</affiliation>
<email confidence="0.993749">magnolini@fbk.eu</email>
<author confidence="0.713013">Octavian</author>
<affiliation confidence="0.77952">IBM Research, T.J. Yorktown,</affiliation>
<email confidence="0.999864">o.popescu@us.ibm.com</email>
<abstract confidence="0.9988580625">This paper reports the description and performance of our system, FBK-HLT, participating in the SemEval 2015, Task #2 “Semantic Textual Similarity”, English subtask. We submitted three runs with different hypothesis in combining typical features (lexical similarity, string similarity, word n-grams, etc) with syntactic structure features, resulting in different sets of features. The results evaluated on both STS 2014 and 2015 datasets prove our hypothesis of building a STS system taking into consideration of syntactic information. We outperform the best system on STS 2014 datasets and achieve a very competitive result to the best system on STS 2015 datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>385--393</pages>
<contexts>
<context position="1289" citStr="Agirre et al., 2012" startWordPosition="188" endWordPosition="191">resulting in different sets of features. The results evaluated on both STS 2014 and 2015 datasets prove our hypothesis of building a STS system taking into consideration of syntactic information. We outperform the best system on STS 2014 datasets and achieve a very competitive result to the best system on STS 2015 datasets. 1 Introduction Semantic related tasks have been a noticed trend in Natural Language Processing (NLP) community. Particularly, the task Semantic Textual Similarity (STS) has captured a huge attention in the NLP community despite being recently introduced since SemEval 2012 (Agirre et al., 2012). Basically, the task requires to build systems which can compute the similarity degree between two given sentences. The similarity degree is scaled as a real score from 0 (no relevance) to 5 (semantic equivalence). The evaluation is done by computing the correlation between human judgment scores and system scores by the mean of Pearson correlation method. At SemEval 2015, Task #2 “Semantic Textual Similarity (STS)”, English STS subtask (Agirre et al., 2015) evaluates participating systems on five test datasets: image description (image), news headlines (headlines), student answers paired with</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 385–393.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, CO,</location>
<contexts>
<context position="1751" citStr="Agirre et al., 2015" startWordPosition="261" endWordPosition="264">emantic Textual Similarity (STS) has captured a huge attention in the NLP community despite being recently introduced since SemEval 2012 (Agirre et al., 2012). Basically, the task requires to build systems which can compute the similarity degree between two given sentences. The similarity degree is scaled as a real score from 0 (no relevance) to 5 (semantic equivalence). The evaluation is done by computing the correlation between human judgment scores and system scores by the mean of Pearson correlation method. At SemEval 2015, Task #2 “Semantic Textual Similarity (STS)”, English STS subtask (Agirre et al., 2015) evaluates participating systems on five test datasets: image description (image), news headlines (headlines), student answers paired with reference answers (answers-students), answers to questions posted in stach exchange forums (answers-forum), and English discussion forum data exhibiting commited belief (belief). As being inspired by the UKP system (Bär et al., 2012), which was the best system in STS 2012, we build a supervised system on top of it. Our system adopts some word and string similarity features in UKP, such as string similarity, character/word n-grams, and pairwise similarity; h</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor I Dix</author>
</authors>
<title>A bit-string longest-common-subsequence algorithm.</title>
<date>1986</date>
<journal>Information Processing Letters,</journal>
<volume>23</volume>
<issue>5</issue>
<contexts>
<context position="4279" citStr="Allison and Dix, 1986" startWordPosition="650" endWordPosition="653">t and work together. 2.1 Data Preprocessing The input data undergoes the data preprocessing in which we use Tree Tagger (Schmid, 1994) to perform tokenization, lemmatization, and Part-ofSpeech (POS) tagging. On the other hand, we use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical </context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor I Dix. 1986. A bit-string longest-common-subsequence algorithm. Information Processing Letters, 23(5):305–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for mt evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="10109" citStr="Banerjee and Lavie, 2005" startWordPosition="1521" endWordPosition="1524">erstanding of computational linguistics or machine learning is required. We apply the tool on the STS datasets to compute the similarity of syntactic structure of sentence pairs. 2.4 Further Features We also deploy other features which also may help in identifying the semantic similarity degree between two given sentences, such as word alignment in machine translation evaluation metric and the vector space model Weighted Matrix Factorization (WMF) for pairwise similarity. 2.4.1 Machine Translation Evaluation Metric - METEOR METEOR (Metric for Evaluation of Translation with Explicit ORdering) (Banerjee and Lavie, 2005) is an automatic metric for machine translation evaluation, which consists of two major components: a flexible monolingual word aligner and a scorer. For machine translation evaluation, hypothesis sentences are aligned to reference sentences. Alignments are then scored to produce sentence and corpus level scores. We use this word alignment feature 6https://opennlp.apache.org 104 to learn the similarity between words, phrases in two given texts in case of different orders. 2.4.2 Weighted Matrix Factorization (WMF) WMF (Guo and Diab, 2012) is a dimension reduction model to extract nuanced and ro</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel Bär</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>435--440</pages>
<contexts>
<context position="2123" citStr="Bär et al., 2012" startWordPosition="311" endWordPosition="314">he evaluation is done by computing the correlation between human judgment scores and system scores by the mean of Pearson correlation method. At SemEval 2015, Task #2 “Semantic Textual Similarity (STS)”, English STS subtask (Agirre et al., 2015) evaluates participating systems on five test datasets: image description (image), news headlines (headlines), student answers paired with reference answers (answers-students), answers to questions posted in stach exchange forums (answers-forum), and English discussion forum data exhibiting commited belief (belief). As being inspired by the UKP system (Bär et al., 2012), which was the best system in STS 2012, we build a supervised system on top of it. Our system adopts some word and string similarity features in UKP, such as string similarity, character/word n-grams, and pairwise similarity; however, we also add other distinguished features, like syntactic structure information, word alignment and semantic word similarity. As a result, our team, FBKHLT, submitted three runs and achieve very competitive results in the top-tier systems of the task. The remainder of this paper is organized as follows: Section 2 presents the System Description, Section 3 describ</context>
<context position="4116" citStr="Bär et al., 2012" startWordPosition="627" endWordPosition="630">on (SemEval 2015), pages 102–106, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Figure 1: System Overview. ponents connect and work together. 2.1 Data Preprocessing The input data undergoes the data preprocessing in which we use Tree Tagger (Schmid, 1994) to perform tokenization, lemmatization, and Part-ofSpeech (POS) tagging. On the other hand, we use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 199</context>
</contexts>
<marker>Bär, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel Bär, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 435–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Barrón-Cedeno</author>
<author>Paolo Rosso</author>
<author>Eneko Agirre</author>
<author>Gorka Labaka</author>
</authors>
<title>Plagiarism detection across distant language pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>37--45</pages>
<contexts>
<context position="4411" citStr="Barrón-Cedeno et al., 2010" startWordPosition="669" endWordPosition="672">, 1994) to perform tokenization, lemmatization, and Part-ofSpeech (POS) tagging. On the other hand, we use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical semantic resources Wikipedia 1 and Wiktionary 2. 2.3 Syntactic Structure Features We exploit the syntactic structure information by </context>
</contexts>
<marker>Barrón-Cedeno, Rosso, Agirre, Labaka, 2010</marker>
<rawString>Alberto Barrón-Cedeno, Paolo Rosso, Eneko Agirre, and Gorka Labaka. 2010. Plagiarism detection across distant language pairs. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 37–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Z Broder</author>
</authors>
<title>On the resemblance and containment of documents.</title>
<date>1997</date>
<booktitle>In Compression and Complexity of Sequences</booktitle>
<pages>21--29</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4585" citStr="Broder, 1997" startWordPosition="702" endWordPosition="703">iven sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical semantic resources Wikipedia 1 and Wiktionary 2. 2.3 Syntactic Structure Features We exploit the syntactic structure information by the mean of three different toolkits: Syntactic Tree Kernel, Distributed Tree Kernel and Syntactic Generalization. We describe how each toolkit is used to learn and extract t</context>
</contexts>
<marker>Broder, 1997</marker>
<rawString>Andrei Z Broder. 1997. On the resemblance and containment of documents. In Compression and Complexity of Sequences 1997, pages 21–29. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Brockett</author>
<author>Chris Quirk</author>
</authors>
<title>Microsoft research paraphrase corpus. Retrieved</title>
<date>2005</date>
<contexts>
<context position="5822" citStr="Dolan et al., 2005" startWordPosition="890" endWordPosition="893">ture information from texts to be used in our STS system. 2.3.1 Syntactic Tree Kernel Syntactic Tree Kernel (Moschitti, 2006) is a tree kernels approach to learn the syntactic structure from syntactic parsing information, particularly, the Partial Tree (PT) kernel is proposed as a new convolution kernel to fully exploit dependency trees. We use the open-source toolkit &amp;quot;Tree Kernel in SVMLight3&amp;quot; to learn this syntactic information. Having assumed that paraphrased pairs would share the same content and similar syntactic structures, we decide to choose the Microsoft Research Paraphrasing Corpus (Dolan et al., 2005) which contains 5,800 sentence pairs extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship. This corpus is split into Training set (4,076 pairs) and Testing set (1,725 pairs). We use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from sentence pairs. Then we use the machine learning tool svm-light-tk 1.2 which uses Tree Kernel approach to learn the similarity of syntactic structure to build a binary classifying model on the Train dataset. The output predictions are pr</context>
</contexts>
<marker>Dolan, Brockett, Quirk, 2005</marker>
<rawString>Bill Dolan, Chris Brockett, and Chris Quirk. 2005. Microsoft research paraphrase corpus. Retrieved March, 29:2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1998</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="4746" citStr="Fellbaum, 1998" startWordPosition="729" endWordPosition="730">efly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical semantic resources Wikipedia 1 and Wiktionary 2. 2.3 Syntactic Structure Features We exploit the syntactic structure information by the mean of three different toolkits: Syntactic Tree Kernel, Distributed Tree Kernel and Syntactic Generalization. We describe how each toolkit is used to learn and extract the syntactic structure information from texts to be used in our STS system. 2.3.1 Syntactic Tree Kernel Syntactic Tree Kernel (Moschitti, 2006) is a tree kernels</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In IJCAI,</booktitle>
<volume>7</volume>
<pages>1606--1611</pages>
<contexts>
<context position="4842" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="740" endWordPosition="743">g (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical semantic resources Wikipedia 1 and Wiktionary 2. 2.3 Syntactic Structure Features We exploit the syntactic structure information by the mean of three different toolkits: Syntactic Tree Kernel, Distributed Tree Kernel and Syntactic Generalization. We describe how each toolkit is used to learn and extract the syntactic structure information from texts to be used in our STS system. 2.3.1 Syntactic Tree Kernel Syntactic Tree Kernel (Moschitti, 2006) is a tree kernels approach to learn the syntactic structure from syntactic parsing information, particularly, the</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In IJCAI, volume 7, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Galitsky</author>
</authors>
<title>Machine learning of syntactic parse trees for search and classification of text.</title>
<date>2013</date>
<journal>Engineering Applications of Artificial Intelligence,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="8712" citStr="Galitsky, 2013" startWordPosition="1319" endWordPosition="1320">kernel when a vector composition function with specific ideal properties is used. Firstly, we use Stanford Parser (PCFG Parser) trained on Penn TreeBank (Klein and Manning, 2003) to obtain the dependency parsing of sentences, and feed them to the software &amp;quot;distributedtree-kernels&amp;quot; to produce the distributed trees.4 Then, we compute the Cosine similarity between the vectors of distributed trees of each sentence pair. This cosine similarity score is converted to the scale of STS and SR for evaluation. 2.3.3 Syntactic Generalization Given a pair of parse trees, the Syntactic Generalization (SG) (Galitsky, 2013) finds a set of maximal common subtrees. The toolkit &amp;quot;relevance-based-onparse-trees&amp;quot; is an open-source project which evaluates text relevance by using syntactic parse treebased similarity measure.5 Given a pair of parse trees, it measures the similarity between two sentences by finding a set of maximal common subtrees, using representation of constituency parse trees via chunking. Each type of phrases (NP, VP, PRP etc.) 4https://code.google.com/p/distributed-tree-kernels 5https://code.google.com/p/relevance-based-on-parse-trees will be aligned and subject to generalization. It uses the OpenNLP</context>
</contexts>
<marker>Galitsky, 2013</marker>
<rawString>Boris Galitsky. 2013. Machine learning of syntactic parse trees for search and classification of text. Engineering Applications of Artificial Intelligence, 26(3):1072–1091.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>864--872</pages>
<contexts>
<context position="10652" citStr="Guo and Diab, 2012" startWordPosition="1603" endWordPosition="1606">Evaluation of Translation with Explicit ORdering) (Banerjee and Lavie, 2005) is an automatic metric for machine translation evaluation, which consists of two major components: a flexible monolingual word aligner and a scorer. For machine translation evaluation, hypothesis sentences are aligned to reference sentences. Alignments are then scored to produce sentence and corpus level scores. We use this word alignment feature 6https://opennlp.apache.org 104 to learn the similarity between words, phrases in two given texts in case of different orders. 2.4.2 Weighted Matrix Factorization (WMF) WMF (Guo and Diab, 2012) is a dimension reduction model to extract nuanced and robust latent vectors for short texts/sentences. To overcome the sparsity problem in short texts/sentences (e.g. 10 words on average), the missing words, a feature that LSA/LDA typically overlooks, is explicitly modeled. We use the pipeline to compute the similarity score between texts. 3 Experiment Settings We generate and select 25 optimal features, ranging from lexical level to string level and syntactic level. We deploy the machine learning toolkit WEKA (Hall et al., 2009) for learning a regression model (GaussianProcesses) to predict </context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 864–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on strings, trees and sequences: computer science and computational biology.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4227" citStr="Gusfield, 1997" startWordPosition="645" endWordPosition="646">ics Figure 1: System Overview. ponents connect and work together. 2.1 Data Preprocessing The input data undergoes the data preprocessing in which we use Tree Tagger (Schmid, 1994) to perform tokenization, lemmatization, and Part-ofSpeech (POS) tagging. On the other hand, we use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Ma</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on strings, trees and sequences: computer science and computational biology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="11188" citStr="Hall et al., 2009" startWordPosition="1689" endWordPosition="1692">fferent orders. 2.4.2 Weighted Matrix Factorization (WMF) WMF (Guo and Diab, 2012) is a dimension reduction model to extract nuanced and robust latent vectors for short texts/sentences. To overcome the sparsity problem in short texts/sentences (e.g. 10 words on average), the missing words, a feature that LSA/LDA typically overlooks, is explicitly modeled. We use the pipeline to compute the similarity score between texts. 3 Experiment Settings We generate and select 25 optimal features, ranging from lexical level to string level and syntactic level. We deploy the machine learning toolkit WEKA (Hall et al., 2009) for learning a regression model (GaussianProcesses) to predict the similarity scores. We build three models based on three sets of features to verify our hypothesis in which we augment that computing semantic similarity degree is not only about lexical similarity and string similarity, but also taking into consideration a deeper level at syntactic structure where more semantic information is embedded. In the system development process, we train our system on the given datasets of STS 2012, 2013 and use the STS 2014 datasets for evaluating the system. In Table 1, we also examine the contributi</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="3932" citStr="Klein and Manning, 2003" startWordPosition="596" endWordPosition="599">g for improving the accuracy. The System Overview in Figure 1 shows the logic and design processes in which different com102 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 102–106, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Figure 1: System Overview. ponents connect and work together. 2.1 Data Preprocessing The input data undergoes the data preprocessing in which we use Tree Tagger (Schmid, 1994) to perform tokenization, lemmatization, and Part-ofSpeech (POS) tagging. On the other hand, we use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon</context>
<context position="6146" citStr="Klein and Manning, 2003" startWordPosition="941" endWordPosition="944">it dependency trees. We use the open-source toolkit &amp;quot;Tree Kernel in SVMLight3&amp;quot; to learn this syntactic information. Having assumed that paraphrased pairs would share the same content and similar syntactic structures, we decide to choose the Microsoft Research Paraphrasing Corpus (Dolan et al., 2005) which contains 5,800 sentence pairs extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship. This corpus is split into Training set (4,076 pairs) and Testing set (1,725 pairs). We use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from sentence pairs. Then we use the machine learning tool svm-light-tk 1.2 which uses Tree Kernel approach to learn the similarity of syntactic structure to build a binary classifying model on the Train dataset. The output predictions are probability confidence scores in [-1,1], corresponds to the probability of the label to be positive. According to the assumption above, we label paraphrased pairs as 1, -1 otherwise. We obtain the Accuracy of 69.16% on the Test set. 2.3.2 Distributed Tree Kernel Distributed Tree Kernel (DTK) (Zanzotto and Dell’Arciprete, 201</context>
<context position="8275" citStr="Klein and Manning, 2003" startWordPosition="1248" endWordPosition="1251">7491 0.825 0.8644 0.8015 FBK-HLT Run1 0.7131 0.7442 0.7327 0.8079 0.8574 0.7831 FBK-HLT Run2 0.7101 0.7410 0.7377 0.8008 0.8545 0.7801 FBK-HLT Run3 0.6555 0.7362 0.7460 0.7083 0.8389 0.7461 Table 2: Evaluation Results on STS 2015 datasets. in low-dimensional spaces. Then a recursive algorithm is proposed with linear complexity to compute reduced vectors for trees. The dot product among reduced vectors is used to approximate the original tree kernel when a vector composition function with specific ideal properties is used. Firstly, we use Stanford Parser (PCFG Parser) trained on Penn TreeBank (Klein and Manning, 2003) to obtain the dependency parsing of sentences, and feed them to the software &amp;quot;distributedtree-kernels&amp;quot; to produce the distributed trees.4 Then, we compute the Cosine similarity between the vectors of distributed trees of each sentence pair. This cosine similarity score is converted to the scale of STS and SR for evaluation. 2.3.3 Syntactic Generalization Given a pair of parse trees, the Syntactic Generalization (SG) (Galitsky, 2013) finds a set of maximal common subtrees. The toolkit &amp;quot;relevance-based-onparse-trees&amp;quot; is an open-source project which evaluates text relevance by using syntactic pa</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>James Malcolm</author>
<author>Bob Dickerson</author>
</authors>
<title>Detecting short passages of similar text in large document collections.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>118--125</pages>
<contexts>
<context position="4546" citStr="Lyon et al., 2001" startWordPosition="694" endWordPosition="697">003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical semantic resources Wikipedia 1 and Wiktionary 2. 2.3 Syntactic Structure Features We exploit the syntactic structure information by the mean of three different toolkits: Syntactic Tree Kernel, Distributed Tree Kernel and Syntactic Generalization. We describe how each</context>
</contexts>
<marker>Lyon, Malcolm, Dickerson, 2001</marker>
<rawString>Caroline Lyon, James Malcolm, and Bob Dickerson. 2001. Detecting short passages of similar text in large document collections. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 118–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Machine Learning: ECML</booktitle>
<pages>318--329</pages>
<contexts>
<context position="5328" citStr="Moschitti, 2006" startWordPosition="816" endWordPosition="817"> 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical semantic resources Wikipedia 1 and Wiktionary 2. 2.3 Syntactic Structure Features We exploit the syntactic structure information by the mean of three different toolkits: Syntactic Tree Kernel, Distributed Tree Kernel and Syntactic Generalization. We describe how each toolkit is used to learn and extract the syntactic structure information from texts to be used in our STS system. 2.3.1 Syntactic Tree Kernel Syntactic Tree Kernel (Moschitti, 2006) is a tree kernels approach to learn the syntactic structure from syntactic parsing information, particularly, the Partial Tree (PT) kernel is proposed as a new convolution kernel to fully exploit dependency trees. We use the open-source toolkit &amp;quot;Tree Kernel in SVMLight3&amp;quot; to learn this syntactic information. Having assumed that paraphrased pairs would share the same content and similar syntactic structures, we decide to choose the Microsoft Research Paraphrasing Corpus (Dolan et al., 2005) which contains 5,800 sentence pairs extracted from news sources on the web, along with human annotations </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Machine Learning: ECML 2006, pages 318–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence -</booktitle>
<volume>1</volume>
<pages>448--453</pages>
<contexts>
<context position="4718" citStr="Resnik, 1995" startWordPosition="725" endWordPosition="726"> al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical semantic resources Wikipedia 1 and Wiktionary 2. 2.3 Syntactic Structure Features We exploit the syntactic structure information by the mean of three different toolkits: Syntactic Tree Kernel, Distributed Tree Kernel and Syntactic Generalization. We describe how each toolkit is used to learn and extract the syntactic structure information from texts to be used in our STS system. 2.3.1 Syntactic Tree Kernel Syntactic Tree Kernel (Moschi</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI’95, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of international conference on new methods in language processing,</booktitle>
<volume>12</volume>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="3791" citStr="Schmid, 1994" startWordPosition="577" endWordPosition="578">ty scores from given sentence-pairs. On top of this, the system is expandable and scalable for adopting more useful features aiming for improving the accuracy. The System Overview in Figure 1 shows the logic and design processes in which different com102 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 102–106, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Figure 1: System Overview. ponents connect and work together. 2.1 Data Preprocessing The input data undergoes the data preprocessing in which we use Tree Tagger (Schmid, 1994) to perform tokenization, lemmatization, and Part-ofSpeech (POS) tagging. On the other hand, we use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of international conference on new methods in language processing, volume 12, pages 44–49. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Wise</author>
</authors>
<title>Yap3: Improved detection of similarities in computer program and other texts.</title>
<date>1996</date>
<journal>In ACM SIGCSE Bulletin,</journal>
<volume>28</volume>
<pages>130--134</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4317" citStr="Wise, 1996" startWordPosition="658" endWordPosition="659">nput data undergoes the data preprocessing in which we use Tree Tagger (Schmid, 1994) to perform tokenization, lemmatization, and Part-ofSpeech (POS) tagging. On the other hand, we use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical semantic resources Wikipedia 1 and Wik</context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>Michael J Wise. 1996. Yap3: Improved detection of similarities in computer program and other texts. In ACM SIGCSE Bulletin, volume 28, pages 130–134. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Lorenzo Dell’Arciprete</author>
</authors>
<title>Distributed tree kernels.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning.</booktitle>
<marker>Zanzotto, Dell’Arciprete, 2012</marker>
<rawString>Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete. 2012. Distributed tree kernels. In Proceedings of the 29th International Conference on Machine Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>