<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.6819335">
Findings of the 2010 Joint Workshop on
Statistical Machine Translation and Metrics for Machine Translation
</title>
<author confidence="0.986813">
Chris Callison-Burch Philipp Koehn Christof Monz
</author>
<affiliation confidence="0.993654">
Johns Hopkins University University of Edinburgh University of Amsterdam
</affiliation>
<email confidence="0.978164">
ccb@cs.jhu.edu pkoehn@inf.ed.ac.uk c.monz@uva.nl
</email>
<author confidence="0.997031">
Kay Peterson and Mark Przybocki Omar F. Zaidan
</author>
<affiliation confidence="0.998661">
National Institute of Standards and Technology Johns Hopkins University
</affiliation>
<email confidence="0.99528">
kay.peterson,mark.przybocki@nist.gov ozaidan@cs.jhu.edu
</email>
<sectionHeader confidence="0.997348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999245466666667">
This paper presents the results of the
WMT10 and MetricsMATR10 shared
tasks,1 which included a translation task,
a system combination task, and an eval-
uation task. We conducted a large-scale
manual evaluation of 104 machine trans-
lation systems and 41 system combina-
tion entries. We used the ranking of these
systems to measure how strongly auto-
matic metrics correlate with human judg-
ments of translation quality for 26 metrics.
This year we also investigated increasing
the number of human judgments by hiring
non-expert annotators through Amazon’s
Mechanical Turk.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999638">
This paper presents the results of the shared
tasks of the joint Workshop on statistical Ma-
chine Translation (WMT) and Metrics for MA-
chine TRanslation (MetricsMATR), which was
held at ACL 2010. This builds on four previ-
ous WMT workshops (Koehn and Monz, 2006;
Callison-Burch et al., 2007; Callison-Burch et al.,
2008; Callison-Burch et al., 2009), and one pre-
vious MetricsMATR meeting (Przybocki et al.,
2008). There were three shared tasks this year:
a translation task between English and four other
European languages, a task to combine the out-
put of multiple machine translation systems, and
a task to predict human judgments of translation
quality using automatic evaluation metrics. The
</bodyText>
<footnote confidence="0.98854575">
1The MetricsMATR analysis was not complete in time for
the publication deadline. An updated version of paper will be
made available on http://statmt.org/wmt10/ prior
to July 15, 2010.
</footnote>
<bodyText confidence="0.99747175">
performance on each of these shared task was de-
termined after a comprehensive human evaluation.
There were a number of differences between
this year’s workshop and last year’s workshop:
</bodyText>
<listItem confidence="0.792462857142857">
• Non-expert judgments – In addition to hav-
ing shared task participants judge translation
quality, we also collected judgments from
non-expert annotators hired through Ama-
zon’s Mechanical Turk. By collecting a large
number of judgments we hope to reduce the
burden on shared task participants, and to in-
</listItem>
<bodyText confidence="0.791204461538461">
crease the statistical significance of our find-
ings. We discuss the feasibility of using non-
experts evaluators, by analyzing the cost, vol-
ume and quality of non-expert annotations.
• Clearer results for system combination –
This year we excluded Google translations
from the systems used in system combina-
tion. In last year’s evaluation, the large mar-
gin between Google and many of the other
systems meant that it was hard to improve on
when combining systems. This year, the sys-
tem combinations perform better than their
component systems more often than last year.
</bodyText>
<listItem confidence="0.6312668">
• Fewer rule-based systems – This year there
were fewer rule-based systems submitted. In
past years, University of Saarland compiled a
large set of outputs from rule-based machine
translation (RBMT) systems. The RBMT
systems were not submitted this year. This
is unfortunate, because they tended to outper-
form the statistical systems for German, and
they were often difficult to rank properly us-
ing automatic evaluation metrics.
</listItem>
<bodyText confidence="0.988114">
The primary objectives of this workshop are to
evaluate the state of the art in machine transla-
</bodyText>
<page confidence="0.991842">
17
</page>
<note confidence="0.9615465">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999771">
tion, to disseminate common test sets and pub-
lic training data with published performance num-
bers, and to refine evaluation methodologies for
machine translation. As with past years, all of the
data, translations, and human judgments produced
for our workshop are publicly available.2 We hope
they form a valuable resource for research into sta-
tistical machine translation, system combination,
and automatic evaluation of translation quality.
</bodyText>
<sectionHeader confidence="0.857088" genericHeader="introduction">
2 Overview of the shared translation and
system combination tasks
</sectionHeader>
<bodyText confidence="0.999823833333333">
The workshop examined translation between En-
glish and four other languages: German, Span-
ish, French, and Czech. We created a test set for
each language pair by translating newspaper arti-
cles. We additionally provided training data and
two baseline systems.
</bodyText>
<subsectionHeader confidence="0.998411">
2.1 Test data
</subsectionHeader>
<bodyText confidence="0.87768575">
The test data for this year’s task was created
by hiring people to translate news articles that
were drawn from a variety of sources from mid-
December 2009. A total of 119 articles were se-
lected, in roughly equal amounts from a variety
of Czech, English, French, German and Spanish
news sites:3
Czech: iDNES.cz (5), iHNed.cz (1), Lidov-
ky (16)
French: Les Echos (25)
Spanish: El Mundo (20), ABC.es (4), Cinco
Dias (11)
English: BBC (5), Economist (2), Washington
Post (12), Times of London (3)
German: Frankfurter Rundschau (11), Spie-
gel (4)
The translations were created by the profes-
sional translation agency CEET4. All of the trans-
lations were done directly, and not via an interme-
diate language.
</bodyText>
<subsectionHeader confidence="0.999141">
2.2 Training data
</subsectionHeader>
<bodyText confidence="0.9990685">
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
</bodyText>
<footnote confidence="0.992431333333333">
2http://statmt.org/wmt10/results.html
3For more details see the XML test files. The docid
tag gives the source and the date for each document in the
test set, and the origlang tag indicates the original source
language.
4http://www.ceet.eu/
</footnote>
<bodyText confidence="0.989171333333334">
train language models, and development sets to
tune parameters. Some statistics about the train-
ing materials are given in Figure 1.
</bodyText>
<subsectionHeader confidence="0.997406">
2.3 Baseline systems
</subsectionHeader>
<bodyText confidence="0.9999198">
To lower the barrier of entry for newcomers to
the field, we provided two open source toolkits
for phrase-based and parsing-based statistical ma-
chine translation (Koehn et al., 2007; Li et al.,
2009).
</bodyText>
<subsectionHeader confidence="0.996057">
2.4 Submitted systems
</subsectionHeader>
<bodyText confidence="0.999995307692308">
We received submissions from 33 groups from 29
institutions, as listed in Table 1, a 50% increase
over last year’s shared task.
We also evaluated 2 commercial off the shelf
MT systems, and two online statistical machine
translation systems. We note that these companies
did not submit entries themselves. The entries for
the online systems were done by translating the
test data via their web interfaces. The data used
to train the online systems is unconstrained. It is
possible that part of the reference translations that
were taken from online news sites could have been
included in the online systems’ language models.
</bodyText>
<subsectionHeader confidence="0.986801">
2.5 System combination
</subsectionHeader>
<bodyText confidence="0.999218714285714">
In total, we received 153 primary system submis-
sions along with 28 secondary submissions. These
were made available to participants in the sys-
tem combination shared task. Based on feedback
that we received on last year’s system combina-
tion task, we provided two additional resources to
participants:
</bodyText>
<listItem confidence="0.968017">
• Development set: We reserved 25 articles
</listItem>
<bodyText confidence="0.715577">
to use as a dev set for system combination.
These were translated by all participating
sites, and distributed to system combination
participants along with reference translations.
</bodyText>
<listItem confidence="0.86210475">
• n-best translations: We requested n-best
lists from sites whose systems could produce
them. We received 20 n-best lists accompa-
nying the system submissions.
</listItem>
<bodyText confidence="0.869159">
Table 2 lists the 9 participants in the system
combination task.
</bodyText>
<sectionHeader confidence="0.992238" genericHeader="method">
3 Human evaluation
</sectionHeader>
<bodyText confidence="0.994061666666667">
As with past workshops, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
</bodyText>
<page confidence="0.996404">
18
</page>
<table confidence="0.8403578">
Europarl Training Corpus
Spanish +-+ English French +-+ English German +-+ English
Sentences 1,650,152 1,683,156 1,540,549
Words 47,694,560 46,078,122 50,964,362 47,145,288 40,756,801 43,037,967
Distinct words 173,033 95,305 123,639 95,846 316,365 92,464
News Commentary Training Corpus
Spanish +-+ English French +-+ English German +-+ English Czech +-+ English
Sentences 98,598 84,624 100,269 94,742
Words 2,724,141 2,432,064 2,405,082 2,101,921 2,505,583 2,443,183 2,050,545 2,290,066
Distinct words 69,410 46,918 53,763 43,906 101,529 47,034 125,678 45,306
United Nations Training Corpus
Spanish +-+ English French +-+ English
Sentences 6,222,450 7,230,217
Words 213,877,170 190,978,737 243,465,100 216,052,412
Distinct words 441,517 361,734 402,491 412,815
109 Word Parallel Corpus
French +-+ English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech +-+ English
Sentences 7,227,409
Words 72,993,427 84,856,749
Distinct words 1,088,642 522,770
Europarl Language Model Data
English Spanish French German
Sentence 1,843,035 1,822,021 1,855,589 1,772,039
Words 50,132,615 51,223,902 54,273,514 43,781,217
Distinct words 99,206 178,934 127,689 328,628
News Language Model Data
English Spanish French German Czech
Sentence 48,653,884 3,857,414 15,670,745 17,474,133 13,042,040
Words 1,148,480,525 106,716,219 382,563,246 321,165,206 205,614,201
Distinct words 1,451,719 548,169 998,595 1,855,993 1,715,376
News Test Set
English Spanish French German Czech
Sentences 2489
Words 62,988 65,654 68,107 62,390 53,171
Distinct words 9,457 11,409 10,775 12,718 15,825
</table>
<figureCaption confidence="0.961704">
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and
the number of distinct words is based on the provided tokenizer.
</figureCaption>
<page confidence="0.997351">
19
</page>
<note confidence="0.96991">
ID Participant
AALTO Aalto University, Finland (Virpioja et al., 2010)
CAMBRIDGE Cambridge University (Pino et al., 2010)
CMU Carnegie Mellon University’s Cunei system (Phillips, 2010)
CMU-STATXFER Carnegie Mellon University’s statistical transfer system (Hanneman et al., 2010)
COLUMBIA Columbia University
CU-BOJAR Charles University Bojar (Bojar and Kos, 2010)
CU-TECTO Charles University Tectogramatical MT (ˇZabokrtsk´y et al., 2010)
CU-ZEMAN Charles University Zeman (Zeman, 2010)
DCU Dublin City University (Penkale et al., 2010)
DFKI Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz (Federmann et al., 2010)
EU European Parliament, Luxembourg (Jellinghaus et al., 2010)
EUROTRANS commercial MT provider from the Czech Republic
FBK Fondazione Bruno Kessler (Hardmeier et al., 2010)
GENEVA University of Geneva
HUICONG Shanghai Jiao Tong University (Cong et al., 2010)
JHU Johns Hopkins University (Schwartz, 2010)
KIT Karlsruhe Institute for Technology (Niehues et al., 2010)
KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al., 2010)
LIMSI LIMSI (Allauzen et al., 2010)
LIU Link¨oping University (Stymne et al., 2010)
LIUM University of Le Mans (Lambert et al., 2010)
NRC National Research Council Canada (Larkin et al., 2010)
ONLINEA an online machine translation system
ONLINEB an online machine translation system
PC-TRANS commercial MT provider from the Czech Republic
POTSDAM Potsdam University
RALI RALI - Universit´e de Montr´eal (Huet et al., 2010)
RWTH RWTH Aachen (Heger et al., 2010)
SFU Simon Fraser University (Sankaran et al., 2010)
UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010)
UEDIN University of Edinburgh (Koehn et al., 2010)
UMD University of Maryland (Eidelman et al., 2010)
UPC Universitat Polit`ecnica de Catalunya (Henriquez Q. et al., 2010)
UPPSALA Uppsala University (Tiedemann, 2010)
UPV Universidad Polit´ecnica de Valencia (Sanchis-Trilles et al., 2010)
UU-MS Uppsala University - Saers (Saers et al., 2010)
</note>
<tableCaption confidence="0.999299">
Table 1: Participants in the shared translation task. Not all groups participated in all language pairs.
</tableCaption>
<page confidence="0.896456">
20
</page>
<table confidence="0.7986451">
ID Participant
BBN-COMBO BBN system combination (Rosti et al., 2010)
CMU-COMBO-HEAFIELD CMU system combination (Heafield and Lavie, 2010)
CMU-COMBO-HYPOSEL CMU system combo with hyp. selection (Hildebrand and Vogel, 2010)
DCU-COMBO Dublin City University system combination (Du et al., 2010)
JHU-COMBO Johns Hopkins University system combination (Narsale, 2010)
KOC-COMBO Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIUM-COMBO University of Le Mans system combination (Barrault, 2010)
RWTH-COMBO RWTH Aachen system combination (Leusch and Ney, 2010)
UPV-COMBO Universidad Polit´ecnica de Valencia (Gonz´alez-Rubio et al., 2010)
</table>
<tableCaption confidence="0.996107">
Table 2: Participants in the system combination task.
</tableCaption>
<table confidence="0.9998255">
Language Pair Sentence Ranking Edited Translations Yes/No Judgments
German-English 5,212 830 824
English-German 6,847 755 751
Spanish-English 5,653 845 845
English-Spanish 2,587 920 690
French-English 4,147 925 921
English-French 3,981 1,325 1,223
Czech-English 2,688 490 488
English-Czech 6,769 1,165 1,163
Totals 37,884 7,255 6,905
</table>
<tableCaption confidence="0.995895">
Table 3: The number of items that were collected for each task during the manual evaluation. An item
</tableCaption>
<bodyText confidence="0.673754">
is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no
judgment in the judgment task.
</bodyText>
<page confidence="0.998605">
21
</page>
<bodyText confidence="0.999804705882353">
that automatic measures are an imperfect substi-
tute for human assessment of translation quality.
Therefore, we define the manual evaluation to be
primary, and use the human judgments to validate
automatic metrics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct it on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partic-
ipants, interested volunteers, and a small number
of paid annotators. More than 120 people partic-
ipated in the manual evaluation5, with 89 people
putting in more than an hour’s worth of effort, and
29 putting in more than four hours. A collective
total of 337 hours of labor was invested.6
We asked people to evaluate the systems’ output
in two different ways:
</bodyText>
<listItem confidence="0.854523285714286">
• Ranking translated sentences relative to each
other. This was our official determinant of
translation quality.
• Editing the output of systems without dis-
playing the source or a reference translation,
and then later judging whether edited transla-
tions were correct.
</listItem>
<bodyText confidence="0.999945">
The total number of judgments collected for the
different modes of annotation is given in Table 3.
In all cases, the output of the various translation
systems were judged on equal footing; the output
of system combinations was judged alongside that
of the individual system, and the constrained and
unconstrained systems were judged together.
</bodyText>
<subsectionHeader confidence="0.999499">
3.1 Ranking translations of sentences
</subsectionHeader>
<bodyText confidence="0.996431969696969">
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the in-
structions simple:
Rank translations from Best to Worst rel-
ative to the other choices (ties are al-
lowed).
5We excluded data from three errant annotators, identified
as follows. We considered annotators completing at least 3
screens, whose P(A) with others (see 3.2) is less than 0.33.
Out of seven such annotators, four were affiliated with shared
task teams. The other three had no apparent affiliation, and
so we discarded their data, less than 5% of the total data.
6Whenever an annotator appears to have spent more than
ten minutes on a single screen, we assume they left their sta-
tion and left the window open, rather than actually needing
more than ten minutes. In those cases, we assume the time
spent to be ten minutes.
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions. For each of the lan-
guage pairs, there were more than 5 submissions.
We did not attempt to get a complete ordering over
the systems, and instead relied on random selec-
tion and a reasonably large sample size to make
the comparisons fair.
Relative ranking is our official evaluation met-
ric. Individual systems and system combinations
are ranked based on how frequently they were
judged to be better than or equal to any other sys-
tem. The results of this are reported in Section 4.
Appendix A provides detailed tables that contain
pairwise comparisons between systems.
</bodyText>
<subsectionHeader confidence="0.953958">
3.2 Inter- and Intra-annotator agreement in
the ranking task
</subsectionHeader>
<bodyText confidence="0.999975965517242">
We were interested in determining the inter- and
intra-annotator agreement for the ranking task,
since a reasonable degree of agreement must ex-
ist to support our process as a valid evaluation
setup. To ensure we had enough data to measure
agreement, we purposely designed the sampling of
source segments shown to annotators so that items
were likely to be repeated, both within an annota-
tor’s assigned tasks and across annotators. We did
so by assigning an annotator a batch of 20 screens
(each with three ranking sets; see 3.1) that were to
be completed in full before generating new screens
for that annotator.
Within each batch, the source segments for nine
of the 20 screens (45%) were chosen from a small
pool of 60 source segments, instead of being sam-
pled from the larger pool of 1,000 source segments
designated for the ranking task.7 The larger pool
was used to choose source segments for nine other
screens (also 45%). As for the remaining two
screens (10%), they were chosen randomly from
the set of eighteen screens already chosen. Fur-
thermore, in the two “local repeat” screens, the
system choices were also preserved.
Heavily sampling from a small pool of source
segments ensured we had enough data to measure
inter-annotator agreement, while purposely mak-
ing 10% of each annotator’s screens repeats of pre-
viously seen sets in the same batch ensured we
</bodyText>
<footnote confidence="0.854448666666667">
7Each language pair had its own 60-sentence pool, dis-
joint from other language pairs’ pools, but ach of the 60-
sentence pools was a subset of the 1,000-sentence pool.
</footnote>
<page confidence="0.997946">
22
</page>
<sectionHeader confidence="0.556598" genericHeader="method">
INTER-ANNOTATOR AGREEMENT
</sectionHeader>
<table confidence="0.865951333333333">
P(A) K
With references 0.658 0.487
Without references 0.626 0.439
WMT ’09 0.549 0.323
INTRA-ANNOTATOR AGREEMENT
P(A) K
With references 0.755 0.633
Without references 0.734 0.601
WMT ’09 0.707 0.561
</table>
<tableCaption confidence="0.883251">
Table 4: Inter- and intra-annotator agreement for
</tableCaption>
<bodyText confidence="0.973934857142857">
the sentence ranking task. In this task, P(E) is
0.333.
had enough data to measure intra-annotator agree-
ment.
We measured pairwise agreement among anno-
tators using the kappa coefficient (K), which is de-
fined as
</bodyText>
<equation confidence="0.997290666666667">
P(A) − P(E)
K=
1 − P(E)
</equation>
<bodyText confidence="0.999915875">
where P(A) is the proportion of times that the an-
notators agree, and P(E) is the proportion of time
that they would agree by chance.
For inter-annotator agreement for the ranking
tasks we calculated P(A) by examining all pairs
of systems which had been judged by two or more
judges, and calculated the proportion of time that
they agreed that A &gt; B, A = B, or A &lt; B. Intra-
annotator agreement was computed similarly, but
we gathered items that were annotated on multiple
occasions by a single annotator.
Table 4 gives K values for inter-annotator and
intra-annotator agreement. These give an indi-
cation of how often different judges agree, and
how often single judges are consistent for repeated
judgments, respectively. The exact interpretation
of the kappa coefficient is difficult, but according
to Landis and Koch (1977), 0 −.2 is slight, .2 −.4
is fair, .4 − .6 is moderate, .6 − .8 is substantial
and the rest is almost perfect.
Based on these interpretations the agreement
for sentence-level ranking is moderate for inter-
annotator agreement and substantial for intra-
annotator agreement. These levels of agreement
are higher than in previous years, partially due to
the fact that that year we randomly included the
references along the system outputs. In general,
judges tend to rank the reference as the best trans-
lation, so people have stronger levels of agreement
when it is included. That said, even when compar-
isons involving reference are excluded, we still see
an improvement in agreement levels over last year.
</bodyText>
<subsectionHeader confidence="0.998973">
3.3 Editing machine translation output
</subsectionHeader>
<bodyText confidence="0.97333517948718">
In addition to simply ranking the output of sys-
tems, we also had people edit the output of MT
systems. We did not show them the reference
translation, which makes our edit-based evalu-
ation different from the Human-targeted Trans-
lation Edit Rate (HTER) measure used in the
DARPA GALE program (NIST, 2008). Rather
than asking people to make the minimum number
of changes to the MT output in order capture the
same meaning as the reference, we asked them to
edit the translation to be as fluent as possible with-
out seeing the reference. Our hope was that this
would reflect people’s understanding of the out-
put.
The instructions given to our judges were as fol-
lows:
Correct the translation displayed, mak-
ing it as fluent as possible. If no correc-
tions are needed, select “No corrections
needed.” If you cannot understand the
sentence well enough to correct it, select
“Unable to correct.”
A screenshot is shown in Figure 2. This year,
judges were shown the translations of 5 consec-
utive source sentences, all produced by the same
machine translation system. In last year’s WMT
evaluation they were shown only one sentence at a
time, which made the task more difficult because
the surrounding context could not be used as an
aid to understanding.
Since we wanted to prevent judges from see-
ing the reference before editing the translations,
we split the test set between the sentences used
in the ranking task and the editing task (because
they were being conducted concurrently). More-
over, annotators edited only a single system’s out-
put for one source sentence to ensure that their un-
derstanding of it would not be influenced by an-
other system’s output.
</bodyText>
<subsectionHeader confidence="0.998327">
3.4 Judging the acceptability of edited output
</subsectionHeader>
<bodyText confidence="0.999661666666667">
Halfway through the manual evaluation period, we
stopped collecting edited translations, and instead
asked annotators to do the following:
</bodyText>
<page confidence="0.997058">
23
</page>
<subsectionHeader confidence="0.264089">
Edit Machine Translation Outputs
</subsectionHeader>
<bodyText confidence="0.885337088235294">
Instructions:
You are shown several
machine translation outputs.
Your task is to edit each translation to make it as fluent as possible.
It is possible that the translation is already fluent. In that case, select No corrections needed.
If you cannot understand the sentence well enough to correct it, select Unable to correct.
The sentences are all from the same article. You can use the earlier and later sentences
to help understand a confusing sentence.
Your edited translations The machine translations
The shortage of snow in mountain
The shortage of snow in mountain worries the hoteliers
worries the hoteliers
correct Reset
Edited No corrections needed Unable to
The deserted tracks are not
The deserted tracks are not putting down problem only at the exploitants
of skilift.
putting down problem only at the
exploitants of skilift.
correct Reset
Edited No corrections needed Unable to
The lack of snow deters the people
The lack of snow deters the people to reserving their stays at the ski in
the hotels and pension.
to reserving their stays at the ski
in the hotels and pension.
correct Reset
Edited No corrections needed Unable to
Thereby, is always possible to
Thereby, is always possible to track free bedrooms for all the dates in
winter, including Christmas and Nouvel An.
track free bedrooms for all the
dates in winter, including
Christmas and Nouvel An.
</bodyText>
<figure confidence="0.8877165">
correct Reset
Edited No corrections needed Unable to
</figure>
<figureCaption confidence="0.9959095">
Figure 2: This screenshot shows what an annotator sees when beginning to edit the output of a machine
translation system.
</figureCaption>
<page confidence="0.997142">
24
</page>
<bodyText confidence="0.994909888888889">
Indicate whether the edited transla-
tions represent fully fluent and meaning-
equivalent alternatives to the reference
sentence. The reference is shown with
context, the actual sentence is bold.
In addition to edited translations, unedited items
that were either marked as acceptable or as incom-
prehensible were also shown. Judges gave a sim-
ple yes/no indication to each item.
</bodyText>
<sectionHeader confidence="0.978589" genericHeader="method">
4 Translation task results
</sectionHeader>
<bodyText confidence="0.9982364">
We used the results of the manual evaluation to
analyze the translation quality of the different sys-
tems that were submitted to the workshop. In our
analysis, we aimed to address the following ques-
tions:
</bodyText>
<listItem confidence="0.975793571428571">
• Which systems produced the best translation
quality for each language pair?
• Did the system combinations produce better
translations than individual systems?
• Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
</listItem>
<bodyText confidence="0.985201078947368">
Table 5 shows the best individual systems. We
define the best systems as those which had no
other system that was statistically significantly
better than them under the Sign Test at p &lt; 0.1.
Multiple systems are listed as the winners for
many language pairs because it was not possible to
draw a statistically significant difference between
the systems. There is no individual system clearly
outperforming all other systems across the differ-
ent language pairs. With the exception of French-
English and English-French one can observe that
top-performing constrained systems did as well as
the unconstrained system ONLINEB.
Table 6 shows the best combination systems.
For all language directions, except Spanish-
English, one can see that the system combina-
tion runs outperform the individual systems and
that in most cases the differences are statistically
significant. While this is to be expected, system
combination is not guaranteed to improve perfor-
mance as some of the lower ranked combination
runs show, which are outperformed by individual
systems. Also note that except for Czech-English
translation the online systems ONLINEA and ON-
LINEB where not included for the system combi-
nation runs
Understandability
Our hope is that judging the acceptability of edited
output as discussed in Section 3 gives some indi-
cation of how often a system’s output was under-
standable. Figure 3 gives the percentage of times
that each system’s edited output was judged to
be acceptable (the percentage also factors in in-
stances when judges were unable to improve the
output because it was incomprehensible).
This style of manual evaluation is experimental
and should not be taken to be authoritative. Some
caveats about this measure:
</bodyText>
<listItem confidence="0.947275">
• There are several sources of variance that are
difficult to control for: some people are better
at editing, and some sentences are more dif-
ficult to edit. Therefore, variance in the un-
derstandability of systems is difficult to pin
down.
• The acceptability measure does not strongly
correlate with the more established method of
ranking translations relative to each other for
all the language pairs.
</listItem>
<sectionHeader confidence="0.86371" genericHeader="method">
5 Shared evaluation task overview
</sectionHeader>
<bodyText confidence="0.9998998125">
In addition to allowing the analysis of subjective
translation quality measures for different systems,
the judgments gathered during the manual evalu-
ation may be used to evaluate how well the au-
tomatic evaluation metrics serve as a surrogate to
the manual evaluation processes. NIST began run-
ning a “Metrics for MAchine TRanslation” chal-
lenge (MetricsMATR), and presented their find-
ings at a workshop at AMTA (Przybocki et al.,
2008). This year we conducted a joint Metrics-
MATR and WMT workshop, with NIST running
the shared evaluation task and analyzing the re-
sults.
In this year’s shared evaluation task 14 different
research groups submitted a total of 26 different
automatic metrics for evaluation:
</bodyText>
<subsectionHeader confidence="0.527921">
Aalto University of Science and Technology
</subsectionHeader>
<bodyText confidence="0.502405">
(Dobrinkat et al., 2010)
</bodyText>
<listItem confidence="0.595096428571429">
• MT-NCD – A machine translation metric
based on normalized compression distance
(NCD), a general information-theoretic mea-
sure of string similarity. MT-NCD mea-
sures the surface level similarity between two
strings with a general compression algorithm.
More similar strings can be represented with
</listItem>
<page confidence="0.988379">
25
</page>
<figure confidence="0.991663090651558">
Czech-English
788–868 judgments per system
French-English
551–755 judgments per system
English-French
664–879 judgments per system
System C?
System
C?
&gt;others
System
C?
&gt;others
&gt;others
N
Y
Y
0.7
ONLINEB 9
0.71
0.70
LIUM 9?
UEDIN 9?
N
N
0.68
0.71
ONLINEB 9
ONLINEB 9
Y
Y
0.61
UEDIN ?
Y +GW
Y +GW
Y +GW
Y +GW
Y
Y
Y
Y +GW
Y +GW
Y
Y +GW
Y
Y +GW
Y
0.66
0.66
0.55
NRC 9?
RALI 9?
CMU
N
0.55
CU-BOJAR
0.66
0.66
CAMBRIDGE 9?
LIMSI 9?
0.65
0.63
Y
0.43
LIMSI ?
RWTH 9?
AALTO
N
0.37
ONLINEA
0.65
0.63
CAMBRIDGE ?
UEDIN
0.65
0.63
Y
0.22
RALI 9?
LIUM
CU-ZEMAN
0.62
0.59
NRC
JHU
N
0.55
ONLINEA
0.55
RWTH 9?
0.53
Y
0.53
LIG
JHU
N
N
0.52
0.40
DFKI
ONLINEA
Y
Y
0.51
Y
0.35
CMU-STATXFER
GENEVA
N
0.32
EU
0.51
HUICONG
N
0.42
DFKI
Y
0.26
CU-ZEMAN
Y
0.27
Y
0.26
GENEVA
KOC
0.21
Y
CU-ZEMAN
German-English
723–879 judgments per system
English-German
1284–1542 judgments per system
English-Czech
1375–1627 judgments per system
System C?
System
&gt;others
C?
&gt;others
System C?
&gt;others
N
0.73
N
N
ONLINEB 9
N
N
N
0.70
0.62
ONLINEB 9
DFKI 9
0.70
0.66
0.62
ONLINEB 9
CU-BOJAR 9
PC-TRANS 9
Y +GW
Y
Y
Y +GW
0.72
KIT 9?
Y
Y
0.62
0.68
UMD 9?
UEDIN 9?
Y
Y
0.66
0.60
0.62
UEDIN ?
KIT ?
UEDIN 9?
N
0.59
ONLINEA
0.66
0.60
FBK ?
CU-TECTO
N
N
0.54
0.63
ONLINEA 9
EUROTRANS
Y
0.56
FBK ?
Y
Y
Y +GW
Y
Y
Y
Y +GW
Y
0.62
Y
0.55
0.50
RWTH
CU-ZEMAN
LIU
0.51
0.59
Y
0.45
RWTH
SFU
LIU
N
0.44
ONLINEA
0.51
0.55
Y
UU-MS
LIMSI
0.53
Y
0.47
Y
0.44
UPPSALA
POTSDAM
JHU
N
0.38
DCU
0.52
Y
0.46
LIMSI
JHU
0.34
0.51
Y
Y
0.33
UPPSALA
SFU
KOC
N
0.50
DFKI
Y
0.30
KOC
Y
0.47
Y
0.28
HUICONG
CU-ZEMAN
Y
0.46
CMU
Y
0.42
AALTO
Y
0.36
CU-ZEMAN
Y
0.23
KOC
English-Spanish
540–722 judgments per system
Spanish-English
1448–1577 judgments per system
System
System C?
&gt;others
C? &gt;others
N
0.70
ONLINEB 9
N
N
0.71
0.69
ONLINEB 9
ONLINEA 9
Y
Y +GW
Y
0.69
UEDIN 9?
0.61
0.61
Y
CAMBRIDGE
UEDIN ?
N
N
0.61
0.55
DCU
DFKI ?
0.61
JHU
N
0.54
ONLINEA
Y
Y
0.51
Y
0.55
UPC ?
JHU ?
0.50
Y
0.55
HUICONG
UPV ?
N
0.45
DFKI
Y +GW
0.54
CAMBRIDGE ?
0.54
Y
0.45
Y
COLUMBIA
UHC-UPV ?
Y
0.27
Y
0.40
CU-ZEMAN
SFU
Y
0.23
CU-ZEMAN
Y
0.19
KOC
Systems are listed in the order of how often their translations were ranked higher than or equal to any other system. Ties are
broken by direct comparison.
C? indicates constrained condition, meaning only using the supplied training data, standard monolingual linguistic tools, and
optionally the LDC’s GigaWord, which was allowed this year (entries that used the GigaWord are marked +GW).
9 indicates a win in the category, meaning that no other system is statistically significantly better at p-level&lt;_0.1 in pairwise
comparison.
</figure>
<bodyText confidence="0.6411995">
? indicates a constrained win, no other constrained system is statistically better.
For all pairwise comparisons between systems, please check the appendix.
</bodyText>
<tableCaption confidence="0.7273585">
Table 5: Official results for the WMT10 translatiox��ask, based on the human evaluation (ranking trans-
lations relative to each other)
</tableCaption>
<figure confidence="0.998977697142857">
French-English
589–716 judgments per combo
System ≥others
0.71
LIUM ?
0.70
0.68
CMU-HEA-COMBO •
UPV-COMBO •
0.65
JHU-COMBO
0.65
RALI
0.64
0.64
LIUM-COMBO
BBN-COMBO
0.55
RWTH
DCU-COMBO •
NRC
CAMBRIDGE
UEDIN ?
LIMSI ?
0.66
0.66
0.65
0.65
RWTH-COMBO •
CMU-HYP-COMBO •
0.77
0.77
0.72
English-French
740–829 judgments per combo
0.70
UEDIN
0.68
0.66
KOC-COMBO •
UPV-COMBO
System
≥others
0.75
0.74
RALI ?
LIMSI
RWTH
CAMBRIDGE
0.66
0.66
0.63
0.63
RWTH-COMBO •
CMU-HEA-COMBO •
German-English English-German
743–835 judgments per combo 1340–1469 judgments per combo
System ≥others System ≥others
Spanish-English
1385–1535 judgments per combo
English-Spanish
516–673 judgments per combo
System ≥others
CMU-HEA-COMBO •
KOC-COMBO
UEDIN ?
UPV-COMBO
RWTH-COMBO
DFKI ?
JHU
UPV
CAMBRIDGE ?
UPV-NNLM ?
RWTH-COMBO •
DFKI ?
UEDIN ?
KIT ?
CMU-HEA-COMBO •
KOC-COMBO
FBK ?
UPV-COMBO
0.65
0.62
0.62
0.60
0.59
0.59
0.56
0.55
0.72
0.68
KIT ?
UMD ?
0.67
JHU-COMBO
0.66
0.66
UEDIN ?
FBK
0.65
0.64
CMU-HYP-COMBO
UPV-COMBO
0.62
RWTH
0.59
KOC-COMBO
BBN-COMBO •
RWTH-COMBO •
CMU-HEA-COMBO
0.77
0.75
0.73
System
≥others
0.69
UEDIN ?
0.51
UPC
CMU-HEA-COMBO •
UPV-COMBO •
BBN-COMBO
JHU-COMBO
0.66
0.66
0.62
0.55
0.68
0.62
0.61
0.60
0.59
0.55
0.55
0.55
0.54
0.54
0.75
DCU-COMBO •
0.70
ONLINEB ?
0.70
RWTH-COMBO
0.69
CMU-HEA-COMBO
0.68
UPV-COMBO
0.66
CU-BOJAR
0.66
KOC-COMBO
0.62
0.62
PC-TRANS
UEDIN
0.71
CMU-HEA-COMBO •
0.7
ONLINEB ?
0.61
UEDIN
Czech-English
766–843 judgments per combo
System ≥others
UPV-COMBO •
JHU-COMBO
0.70
0.65
0.63
0.62
BBN-COMBO •
RWTH-COMBO •
English-Czech
1405–1496 judgments per combo
System ≥others
</figure>
<bodyText confidence="0.984739666666667">
System combinations are listed in the order of how often their translations were ranked higher than or equal to any other system.
Ties are broken by direct comparison. We show the best individual systems alongside the system combinations, since the goal
of combination is to produce better quality translation than the component systems.
</bodyText>
<listItem confidence="0.874035">
• indicates a win for the system combination meaning that no other system or system combination is statistically signifi-
cantly better at p-level≤0.1 in pairwise comparison.
</listItem>
<bodyText confidence="0.8107965">
? indicates an individual system that none of the system combinations beat by a statistically significant margin at p-
level≤0.1.
</bodyText>
<subsectionHeader confidence="0.480695">
For all pairwise comparisons between systems, please check the appendix.
</subsectionHeader>
<bodyText confidence="0.73714125">
Note: ONLINEA and ONLINEB were not included among the systems being combined in the system combination shared tasks,
except in the Czech-English and English-Czech conditions, where ONLINEB was included.
Table 6: Official results for the WMT10 system combination task, based on the human evaluation (rank-
ing translations relative to each other)
</bodyText>
<page confidence="0.977636">
27
</page>
<figure confidence="0.554352">
French-English
</figure>
<figureCaption confidence="0.803473">
Figure 3: The percent of time that each system’s edited output was judged to be an acceptable translation.
</figureCaption>
<bodyText confidence="0.8318535">
These numbers also include judgments of the system’s output when it was marked either incomprehen-
sible or acceptable and left unedited. Note that the reference translation was edited alongside the system
outputs. Error bars show one positive and one negative standard deviation for the systems in that lan-
guage pair.
</bodyText>
<figure confidence="0.999674275862069">
English-Czech
English-German
English-Spanish
English-French
1
0.75
0.5
0.25
.91
.83 .58 .5 .46 .42 .4 .39 .38 .38 .38 .36 .32 .32 .3 .29 .29 .26
.54 .52 .51 .48 .48 .47 .45 .43 .42 .41 .37 .37 .36 .32 .31 .3 .28 .21 .17
0
German-English
Spanish-English
1
0.75
0.5
0.25
0
.98 .71 .64
.6
.54 .54 .52 .51 .51 .47 .46 .43 .4 .36 .29
Czech-English
1
0.75
0.5
0.25
0
1.0 .6 .43 .35 .32 .3 .28 .28 .27 .26 .2 .17 .09
1
1
0.75
0.75
0.5
0.5
0.25
0.25
0
0
.97 .58 .55 .49 .45 .43 .42 .4 .4 .34 .34 .29 .24 .24 .21 .21 .2 .19
.94 .8 .68 .65 .62 .62 .57 .52 .52 .51 .51 .51 .5 .49 .48 .47 .4 .31 .19
1
0.75
0.5
0.25
0
.98 .8 .67 .65 .52 .51 .51 .5 .5 .46 .45 .44 .44 .4 .4 .4 .37 .36 .36 .36 .31 .3 .26 .26 .24 .07
1
0.75
0.5
0.25
0
0.75
0.25
0.5
0
1
.91 .7 .58 .56 .53 .51 .5 .48 .46 .45 .45 .44 .43 .4 .4 .4 .4 .4 .35 .35 .33 .32 .24 .24 .2
</figure>
<page confidence="0.995532">
28
</page>
<bodyText confidence="0.99324925">
a shorter description when concatenated be-
fore compression than when concatenated af-
ter compression. MT-NCD does not require
any language specific resources.
</bodyText>
<listItem confidence="0.9141765">
• MT-mNCD – Enhances MT-NCD with flex-
ible word matching provided by stemming
and synonyms. It works analogously to
M-BLEU and M-TER and uses METEOR’s
aligner module to find relaxed word-to-word
alignments. MT-mNCD exploits English
WordNet data and increases correlation to hu-
man judgments for English over MT-NCD.
</listItem>
<bodyText confidence="0.929386142857143">
Due to a processing issue inherent to the metric,
the scores reported were generated excluding the
first segment of each document. Also, a separate
issue was found for the MT-mNCD metric, and ac-
cording to the developer the scores reported here
would like change with a correction of the issue.
BabbleQuest International8
</bodyText>
<listItem confidence="0.979458230769231">
• Badger 2.0 full – Uses the Smith-Waterman
alignment algorithm with Gotoh improve-
ments to measure segment similarity. The
full version uses a multilingual knowledge
base to assign a substitution cost which sup-
ports normalization of word infection and
similarity.
• Badger 2.0 lite – The lite version uses default
gap, gap extension and substitution costs.
City University of Hong Kong (Wong and Kit,
2010)
• ATEC 2.1 – This version of ATEC extends
the measurement of word choice and word or-
</listItem>
<bodyText confidence="0.8321575">
der by various means. The former is assessed
by matching word forms at linguistic levels,
including surface form, stem, sense and se-
mantic similarity, and further by weighting
the informativeness of both matched and un-
matched words. The latter is quantified in
term of the discordance of word position and
word sequence between an MT output and its
reference.
Due to a version discrepancy of the metric, final
scores for ATECD-2.1 differ from those reported
here, but only minimally.
</bodyText>
<footnote confidence="0.488878">
8http://www.babblequest.com/badger2
</footnote>
<bodyText confidence="0.7232245">
Carnegie Mellon University (Denkowski and
Lavie, 2010)
</bodyText>
<listItem confidence="0.988401666666667">
• METEOR-NEXT-adq – Evaluates a machine
translation hypothesis against one or more
reference translations by calculating a simi-
larity score based on an alignment between
the hypothesis and reference strings. Align-
ments are based on exact, stem, synonym,
and paraphrase matches between words and
phrases in the strings. Metric parameters are
tuned to maximize correlation with human
judgments of translation quality (adequacy
judgments).
• METEOR-NEXT-hter – METEOR-NEXT
tuned to HTER.
• METEOR-NEXT-rank – METEOR-NEXT
tuned to human judgments of rank.
Columbia University9
• SEPIA – A syntactically-aware machine
translation evaluation metric designed with
the goal of assigning bigger weight to gram-
matical structural bigrams with long surface
spans that cannot be captured with surface n-
gram metrics. SEPIA uses a dependency rep-
resentation produced for both hypothesis and
reference(s). SEPIA is configurable to allow
using different combinations of structural n-
grams, surface n-grams, POS tags, depen-
dency relations and lemmatization. SEPIA is
a precision-based metric and as such employs
clipping and length penalty to minimize met-
ric gaming.
Charles University Prague (Bojar and Kos,
2010)
• SemPOS – Computes overlapping of autose-
mantic (content-bearing) word lemmas in the
candidate and reference translations given a
fine-grained semantic part of speech (sem-
pos) and outputs average overlapping score
over all sempos types. The overlapping is de-
fined as the number of matched lemmas di-
vided by the total number of lemmas in the
candidate and reference translations having
the same sempos type.
</listItem>
<footnote confidence="0.949444">
9http://www1.ccls.columbia.edu/˜SEPIA/
</footnote>
<page confidence="0.99518">
29
</page>
<listItem confidence="0.982583761904762">
• SemPOS-BLEU – A linear combination of
SemPOS and BLEU with equal weights.
BLEU is computed on surface forms of au-
tosemantic words that are used by SemPOS,
i.e. auxiliary verbs or prepositions are not
taken into account.
Dublin City University (He et al., 2010)
• DCU-LFG – A combination of syntactic and
lexical information. It measures the similar-
ity of the hypothesis and reference in terms
of matches of Lexical Functional Grammar
(LFG) dependency triples. The matching
module can also access the WordNet syn-
onym dictionary and Snover’s paraphrase
database10.
University of Edinburgh (Birch and Osborne,
2010)
• LRKB4 – A novel metric which directly mea-
sures reordering success using Kendall’s tau
permutation distance metrics. The reordering
component is combined with a lexical metric,
</listItem>
<bodyText confidence="0.880317833333333">
capturing the two most important elements
of translation quality. This simple combined
metric only has one parameter, which makes
its scores easy to interpret. It is also fast
to run and language-independent. It uses
Kendall’s tau permutation.
</bodyText>
<listItem confidence="0.644645666666667">
• LRHB4 – LRKB4, replacing Kendall’s tau
permutation distance metric with the Ham-
ming distance permutation distance metric.
</listItem>
<bodyText confidence="0.97436625">
Due to installation issues, the reported submitted
scores for these two metrics have not been verified
to produce identical scores at NIST.
Harbin Institute of Technology, China
</bodyText>
<listItem confidence="0.9808575">
• I-letter-BLEU – Normal BLEU based on let-
ters. Moreover, the maximum length of N-
gram is decided by the average length for
each sentence, respectively.
• I-letter-recall – A geometric mean of N-gram
recall based on letters. Moreover, the maxi-
mum length of N-gram is decided by the av-
erage length for each sentence, respectively.
</listItem>
<footnote confidence="0.654987">
10Available at http://www.umiacs.umd.edu/
-snover/terp/.
</footnote>
<listItem confidence="0.945279222222222">
• SVM-RANK – Uses support vector ma-
chines rank models to predict an ordering
over a set of system translations with lin-
ear kernel. Features include Meteor-exact,
BLEU-cum-1, BLEU-cum-2, BLEU-cum-5,
BLEU-ind-1, BLEU-ind-2, ROUGE-L re-
call, letter-based TER, letter-based BLEU-
cum-5, letter-based ROUGE-L recall, and
letter-based ROUGE-S recall.
National University of Singapore (Liu et al.,
2010)
• TESLA-M – Based on matching of bags of
unigrams, bigrams, and trigrams, with con-
sideration of WordNet synonyms. The match
is done in the framework of real-valued lin-
ear programming to enable the discounting of
function words.
• TESLA – Built on TESLA-M, this metric
also considers bilingual phrase tables to dis-
cover phrase-level synonyms. The feature
weights are tuned on the development data
using SVMrank.
Stanford University
• Stanford – A discriminatively trained
string-edit distance metric with various
similarity-matching, synonym-matching, and
dependency-parse-tree-matching features.
</listItem>
<bodyText confidence="0.972783">
The model resembles a Conditional Random
Field, but performs regression instead of
classification. It is trained on Arabic, Chi-
nese, and Urdu data from the MT-Eval 2008
dataset.
Due to installation issues, the reported scores for
this metric have not been verified to produce iden-
tical scores at NIST.
</bodyText>
<subsectionHeader confidence="0.413655">
University of Maryland11
</subsectionHeader>
<listItem confidence="0.9140251">
• TER-plus (TERp) – An extension of the
Translation Edit Rate (TER) metric that mea-
sures the number of edits between a hypoth-
esized translation and a reference translation.
TERp extends TER by using stemming, syn-
onymy, and paraphrases as well as tunable
edit costs to better measure the distance be-
tween the two translations. This version
of TERp improves upon prior versions by
adding brevity and length penalties.
</listItem>
<footnote confidence="0.9850985">
11http://www.umiacs.umd.edu/-snover/
terp
</footnote>
<page confidence="0.996872">
30
</page>
<bodyText confidence="0.8760776">
Scores were not submitted along with this metric,
and due to installation issues were not produced at
NIST in time to be included in this report.
University Polit`ecnica de Catalunya/University
de Barcelona (Comelles et al., 2010)
</bodyText>
<listItem confidence="0.9873125">
• DR – An arithmetic mean over a set of
three metrics based on discourse representa-
tions, respectively computing lexical overlap,
morphosyntactic overlap, and semantic tree
matching.
• DRdoc – Is analogous to DR but, instead of
operating at the segment level, it analyzes
similarities over whole document discourse
representations.
• ULCh – An arithmetic mean over a
heuristically-defined set of metrics operat-
ing at different linguistic levels (ROUGE,
METEOR, and measures of overlap between
constituent parses, dependency parses, se-
mantic roles, and discourse representations).
University of Southern California, ISI
• BEwT-E – Basic Elements with Transfor-
mations for Evaluation, is a recall-oriented
metric that compares basic elements, small
portions of contents, between the two trans-
</listItem>
<bodyText confidence="0.584447090909091">
lations. The basic elements (BEs) consist
of content words and various combinations
of syntactically-related words. A variety of
transformations are performed to allow flexi-
ble matching so that words and syntactic con-
structions conveying similar content in dif-
ferent manners may be matched. The trans-
formations cover synonymy, preposition vs.
noun compounding, differences in tenses,
etc. BEwT-E was originally created for sum-
marization evaluation and is English-specific.
</bodyText>
<listItem confidence="0.998929">
• Bkars – Measures overlap between character
trigrams in the system and reference trans-
lations. It is heavily weighted toward recall
and contains a fragmentation penalty. Bkars
produces a score both with and without stem-
ming (using the Snowball package of stem-
mers) and averages the results together. It is
not English-specific.
</listItem>
<bodyText confidence="0.98989825">
Scores were not submitted for BEwT-E; the run-
time required for this metric to process the WMT-
10 data set prohibited the production of scores in
time for publication.
</bodyText>
<sectionHeader confidence="0.990714" genericHeader="method">
6 Evaluation task results
</sectionHeader>
<bodyText confidence="0.99997444">
The results reported here are preliminary; a final
release of results will be published on the WMT10
website before July 15, 2010. Metric developers
submitted metrics for installation at NIST; they
were also asked to submit metric scores on the
WMT10 test set along with their metrics. Not
all developers submitted scores, and not all met-
rics were verified to produce the same scores as
submitted at NIST in time for publication. Any
such caveats are reported with the description of
the metrics above.
The results reported here are limited to a com-
parison of metric scores on the full WMT10
test set with human assessments on the human-
assessed subset. An analysis comparing the hu-
man assessments with the automatic metrics run
only on the human-assessed subset will follow at
a later date.
The WMT10 system output used to generate
the reported metric scores was found to have im-
properly escaped characters for a small number of
segments. While we plan to regenerate the met-
ric scores with this issue resolved, we do not ex-
pect this to significantly alter the results, given the
small number of segments affected.
</bodyText>
<subsectionHeader confidence="0.99611">
6.1 System Level Metric Scores
</subsectionHeader>
<bodyText confidence="0.999598428571429">
The tables in Appendix B list the metric scores
for the language pairs processed by each metric.
These first four tables present scores for transla-
tions out of English into Czech, French, German
and Spanish. In addition to the metric scores of
the submitted metrics identified above, we also
present (1) the ranking of the system as deter-
mined by the human assessments; and (2) the
metrics scores for two popular baseline metrics,
BLEU as calculated by NIST’s mteval software12
and the NIST score. For each method of system
measurement the absolute highest score is identi-
fied by being outlined in a box.
Similarly, the remaining tables in Appendix B
list the metric scores for the submitted metrics and
the two baseline metrics, and the ranking based
on the human assessments for translations into En-
glish from Czech, French, German and Spanish.
As some metrics employ language-specific re-
sources, not all metrics produced scores for all lan-
guage pairs.
</bodyText>
<footnote confidence="0.957016">
12ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a-20091001.tar.gz
</footnote>
<page confidence="0.998777">
31
</page>
<table confidence="0.999787071428571">
cz- fr- de- es- avg
en en en en
SemPOS .78 .77 .60 .95 .77
IQmt-DRdoc .61 .79 .65 .98 .76
SemPOS-BLEU .75 .70 .61 .96 .75
i-letter-BLEU .71 .70 .60 .98 .75
NIST .85 .72 .55 .86 .74
TESLA .70 .70 .60 .97 .74
MT-NCD .71 .72 .58 .95 .74
Bkars .71 .67 .58 .98 .74
ATEC-2.1 .71 .67 .59 .96 .73
meteor-next-rank .69 .68 .60 .96 .73
IQmt-ULCh .70 .64 .60 .99 .73
IQmt-DR .68 .67 .60 .97 .73
meteor-next-hter .71 .66 .59 .95 .73
meteor-next-adq .69 .67 .60 .96 .73
badger-2.0-lite .70 .70 .56 .94 .73
DCU-LFG .69 .69 .58 .96 .73
badger-2.0-full .69 .70 .57 .94 .73
SEPIA .71 .70 .57 .92 .73
SVM-rank .66 .65 .61 .98 .73
i-letter-recall .65 .64 .61 .98 .72
TESLA-M .67 .67 .57 .95 .72
BLEU-4-v13a .69 .68 .52 .90 .70
LRKB4 .63 .62 .53 .89 .67
LRHB4 .62 .65 .50 .87 .66
MT-mNCD .69 .64 .52 .70 .64
Stanford .58 .19 .60 .46 .46
</table>
<tableCaption confidence="0.874005333333333">
Table 7: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation into English.
</tableCaption>
<bodyText confidence="0.9645175">
It is noticeable that system combinations are of-
ten among those achieving the highest scores.
</bodyText>
<subsectionHeader confidence="0.993649">
6.2 System-Level Correlations
</subsectionHeader>
<bodyText confidence="0.999829863636364">
To assess the performance of the automatic met-
rics, we correlated the metrics’ scores with the hu-
man rankings at the system level. We assigned a
consolidated human-assessment rank to each sys-
tem based on the number of times that the given
system’s translations were ranked higher than or
equal to the translations of any other system in
the manual evaluation of the given language pair.
We then compared the ranking of systems by the
human assessments to that provided by the au-
tomatic metric system level scores on the com-
plete WMT10 test set for each language pair, us-
ing Spearman’s p rank correlation coefficient. The
correlations are shown in Table 7 for translations
to English, and Table 8 out of English, with base-
line metrics listed at the bottom. The highest cor-
relation for each language pair and the highest
overall average are bolded.
Overall, correlations are higher for translations
to English than compared to translations from En-
glish. For all language pairs, there are a number
of new metrics that yield noticeably higher corre-
</bodyText>
<table confidence="0.999508857142857">
en- en- en- en- avg
cz fr de es
SVM-rank .29 .54 .68 .67 .55
TESLA-M .27 .49 .74 .66 .54
LRKB4 .39 .58 .47 .71 .54
i-letter-recall .28 .51 .61 .66 .52
LRHB4 .39 .59 .41 .63 .51
i-letter-BLEU .26 .49 .56 .65 .49
ATEC-2.1 .38 .52 .44 .62 .49
badger-2.0-full .37 .58 .41 .59 .49
Bkars .22 .54 .52 .66 .48
BLEU-4-v13a .35 .58 .39 .57 .47
badger-2.0-lite .32 .57 .41 .59 .47
TESLA .09 .62 .66 .50 .47
meteor-next-rank .34 .59 .39 .51 .46
Stanford .34 .48 .70 .32 .46
MT-NCD .17 .54 .51 .61 .46
NIST .30 .52 .41 .50 .43
MT-mNCD .26 .49 .17 .43 .34
SemPOS .31 n/a n/a n/a .31
SemPOS-BLEU .29 n/a n/a n/a .29
</table>
<tableCaption confidence="0.973477333333333">
Table 8: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation out of English.
</tableCaption>
<bodyText confidence="0.9795085">
lations with human assessments than either of the
two included baseline metrics. In particular, Bleu
performed in the bottom half of the into-English
and out-of-English directions.
</bodyText>
<subsectionHeader confidence="0.999349">
6.3 Segment-Level Metric Analysis
</subsectionHeader>
<bodyText confidence="0.999973916666666">
The method employed to collect human judgments
of rank preferences at the segment level produces
a sparse matrix of decision points. It is unclear
whether attempts to normalize the segment level
rankings to 0.0–1.0 values, representing the rela-
tive rank of a system per segment given the num-
ber of comparisons it is involved with, is proper.
An intuitive display of how well metrics mirror the
human judgments may be shown via a confusion
matrix. We compare the human ranks to the ranks
as determined by a metric. Below, we show an ex-
ample of the confusion matrix for the SVM-rank
metric which had the highest summed diagonal
(occurrences when a particular rank by the met-
ric’s score exactly matches the human judgments)
for all segments translated into English. The num-
bers provided are percentages of the total count.
The summed diagonal constitutes 39.01% of all
counts in this example matrix. The largest cell is
the 1/1 ranking cell (top left). We included the
reference translation as a system in this analysis,
which is likely to lead to a lot of agreement on the
highest rank between humans and automatic met-
rics.
</bodyText>
<page confidence="0.994728">
32
</page>
<table confidence="0.999613428571429">
Metric Human Rank
Rank 1 2 3 4 5
1 12.79 4.48 2.75 1.82 0.92
2 2.77 7.94 5.55 3.79 2.2
3 1.57 4.29 6.74 5.4 4.46
4 0.97 2.42 3.76 4.99 6.5
5 0.59 1.54 1.84 3.38 6.55
</table>
<bodyText confidence="0.99916125">
No allowances for ties were made in this analy-
sis. That is, if a human ranked two system transla-
tions the same, this analysis expects the metrics to
provide the same score in order to get them both
correct. Future analysis could relax this constraint.
As not all human rankings start with the highest
possible rank of “1” (due to ties and withholding
judgment on a particular system output being al-
lowed), we set the highest automatic metric rank
to the highest human rank and shifted the lower
metric ranks down accordingly.
Table 9 shows the summed diagonal percent-
ages of the total count of all datapoints for all met-
rics that WMT10 scores were available for, both
combined for all languages to English (X-English)
and separately for each language into English.
The results are ordered by the highest percent-
age for the summed diagonal on all languages
to English combined. There are quite noticeable
changes in ranking of the metrics for the separate
language pairs; further analysis into the reasons
for this will be necessary.
We plan to also analyze metric performance for
translation into English.
</bodyText>
<sectionHeader confidence="0.8139875" genericHeader="method">
7 Feasibility of Using Non-Expert
Annotators in Future WMTs
</sectionHeader>
<bodyText confidence="0.999993533333333">
In this section we analyze the data that we col-
lected data by posting the ranking task on Ama-
zon’s Mechanical Turk (MTurk). Although we did
not use this data when creating the official results,
our hope was that it may be useful in future work-
shops in two ways. First, if we find that it is pos-
sible to obtain a sufficient amount of data of good
quality, then we might be able to reduce the time
commitment expected from the system develop-
ers in future evaluations. Second, the additional
collected labels might enable us to detect signifi-
cant differences between systems that would oth-
erwise be insignificantly different using only the
data from the volunteers (which we will now refer
to as the “expert” data).
</bodyText>
<subsectionHeader confidence="0.994432">
7.1 Data collection
</subsectionHeader>
<bodyText confidence="0.985859604166667">
To that end, we prepared 600 ranking sets for each
of the eight language pairs, with each set con-
taining five MT outputs to be ranked, using the
same interface used by the volunteers. We posted
the data to MTurk and requested, for each one,
five redundant assignments, from different work-
ers. Had all the 5 x 8 x 600 = 24,000 assignments
been completed, we would have obtained 24,000
x 5 = 120,000 additional rank labels, compared
to the 37,884 labels we collected from the volun-
teers (Table 3). In actuality, we collected closer to
55,000 rank labels, as we discuss shortly.
To minimize the amount of data that is of poor
quality, we placed two requirements that must be
satisfied by any worker before completing any of
our tasks. First, we required that a worker have an
existing approval rating of at least 85%. Second,
we required a worker to reside in a country where
the target language of the task can be assumed to
be the spoken language. Finally, anticipating a
large pool of workers located in the United States,
we felt it possible for us to add a third restriction
for the *-to-English language pairs, which is that a
worker must have had at least five tasks previously
approved on MTurk.13 We organized the ranking
sets in groups of 3 per screen, with a monetary re-
ward of $0.05 per screen.
When we created our tasks, we had no expecta-
tion that all the assignments would be completed
over the tasks’ lifetime of 30 days. This was in-
deed the case (Table 10), especially for language
pairs with a non-English target language, due to
workers being in short supply outside the US.
Overall, we see that the amount of data collected
from non-US workers is relatively small (left half
of Table 10), whereas the pool of US-based work-
ers is much larger, leading to much higher com-
pletion rates for language pairs with English as the
target language (right half of Table 10). This is in
spite of the additional restriction we placed on US
workers.
13We suspect that newly registered workers on MTurk al-
ready start with an “approval rating” of 100%, and so requir-
ing a high approval rating alone might not guard against new
workers. It is not entirely clear if our suspicion is true, but our
past experiences with MTurk usually involved a noticeably
faster completion rate than what we experienced this time
around, indicating our suspicion might very well be correct.
</bodyText>
<page confidence="0.994916">
33
</page>
<table confidence="0.999980925925926">
Metric *-English Czech-English French-English German-English Spanish-English
SVM-rank 39.01 41.21 36.07 38.81 40.3
i-letter-recall 38.85 41.71 36.19 38.8 39.5
MT-NCD 38.77 42.55 35.31 38.7 39.48
i-letter-BLEU 38.69 40.54 36.05 38.82 39.64
meteor-next-rank 38.5 40.1 34.41 39.25 40.05
meteor-next-adq 38.27 39.58 34.41 39.5 39.35
meteor-next-hter 38.21 38.61 34.1 39.13 40.18
Bkars 37.98 40.1 35.08 38.6 38.52
Stanford 37.97 39.87 36.19 38.27 38.09
ATEC-2.1 37.95 40.06 34.96 38.6 38.53
TESLA 37.57 38.68 34.38 38.67 38.36
NIST 37.47 39.54 35.54 37.13 38.2
SemPOS 37.21 38.8 37.39 35.73 37.69
SemPOS-BLEU 37.16 38.05 36.57 37.11 37.21
badger-2.0-full 37.12 37.5 36 36.21 38.62
badger-2.0-lite 37.08 37.2 35.88 36.23 38.69
SEPIA 37.06 38.98 34.6 36.46 38.52
BLEU-4-v13a 36.71 37.83 34.84 36.44 37.81
LRHB4 36.14 38.35 34.65 34.24 37.93
TESLA-M 36.13 37.01 34 35.79 37.6
LRKB4 36.12 38.72 33.47 35.25 37.63
IQmt-ULCh 35.86 37.64 33.95 35.81 36.45
IQmt-DR 35.77 36.27 34.43 34.43 37.74
DCU-LFG 34.72 36.38 32.29 33.87 36.49
MT-mNCD 34.51 34.93 31.78 35.73 35.13
IQmt-DRdoc 31.9 33.85 28.99 32.9 32.18
</table>
<tableCaption confidence="0.995586">
Table 9: The segment-level performance for metrics for the into-English direction.
</tableCaption>
<table confidence="0.9996856">
en-de en-es en-fr en-cz de-en es-en fr-en cz-en
Location DE ES/MX FR CZ US US US US
Completed 1 time 37% 38% 29% 19% 3.5% 1.5% 14% 2.0%
Completed 2 times 18% 14% 12% 1.5% 6.0% 5.5% 19% 4.5%
Completed 3 times 2.5% 4.5% 0.5% 0.0% 8.5% 11% 20% 10%
Completed 4 times 1.5% 0.5% 0.5% 0.0% 22% 19% 23% 17%
Completed 5 times 0.0% 0.5% 0.0% 0.0% 60% 63% 22% 67%
Completed &gt; once 59% 57% 42% 21% 100% 99% 96% 100%
Label count 2,583 2,488 1,578 627 12,570 12,870 9,197 13,169
(% of expert data) (38%) (96%) (40%) (9%) (241%) (228%) (222%) (490%)
</table>
<bodyText confidence="0.937888888888889">
Table 10: Statistics for data collected on MTurk for the ranking task. In total, 55,082 rank labels were
collected across the eight language pairs (145% of expert data). Each language pair had 600 sets, and
we requested each set completed by 5 different workers. Since each set provides 5 labels, we could have
potentially obtained 600 x 5 x 5 = 15,000 labels for each language pair. The Label count row indicates
to what extent that potential was met (over the 30-day lifetime of our tasks), and the “Completed...” rows
give a breakdown of redundancy. For instance, the right-most column indicates that, in the cz-en group,
2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5
workers, with 100% of the sets completed at least once. The total cost of this data collection effort was
roughly $200.
</bodyText>
<page confidence="0.962366">
34
</page>
<sectionHeader confidence="0.495333" genericHeader="method">
INTRA-ANNOTATOR AGREEMENT
</sectionHeader>
<equation confidence="0.782716">
P(A) K
</equation>
<bodyText confidence="0.987934714285714">
With references 0.539 0.309
Without references 0.538 0.307
Table 11: Inter- and intra-annotator agreement for
the MTurk workers on the sentence ranking task.
(As before, P(E) is 0.333.) For comparison, we
repeat here the kappa coefficients of the experts
(K*), taken from Table 4.
</bodyText>
<subsectionHeader confidence="0.999367">
7.2 Quality of MTurk data
</subsectionHeader>
<bodyText confidence="0.999992433333334">
It is encouraging to see that we can collect a large
amount of rank labels from MTurk. That said, we
still need to guard against data from bad work-
ers, who are either not being faithful and click-
ing randomly, or who might simply not be compe-
tent enough. Case in point, if we examine inter-
and intra-annotator agreement on the MTurk data
(Table 11), we see that the agreement rates are
markedly lower than their expert counterparts.
Another indication of the presence of bad work-
ers is a low reference preference rate (RPR),
which we define as the proportion of time a ref-
erence translation wins (or ties in) a comparison
when it appears in one. Intuitively, the RPR
should be quite high, since it is quite rare that an
MT output ought to be judged better than the refer-
ence. This rate is 96.5% over the expert data, but
only 83.7% over the MTurk data. Compare this
to a randomly-clicking RPR of 66.67% (because
the two acceptable answers are that the reference
is either better than a system’s output or tied with
it).
Also telling would be the rate at which MTurk
workers agree with experts. To ensure that we ob-
tain enough overlapping data to calculate such a
rate, we purposely select one-sixth14 of our rank-
ing sets so that the five-system group is exactly one
that has been judged by an expert. This way, at
least one-sixth of the comparisons obtained from
an MTurk worker’s labels are comparisons for
</bodyText>
<footnote confidence="0.885970666666667">
14This means that on average Turkers ranked a set of sys-
tem outputs that had been ranked by experts on every other
screen, since each screen’s worth of work had three sets.
</footnote>
<bodyText confidence="0.999921777777778">
which we already have an expert judgment. When
we calculate the rate of agreement on this data,
we find that MTurk workers agree with the ex-
pert workers 53.2% of the time, or K = 0.297, and
when references are excluded, the agreement rate
is 50.0%, or K = 0.249. Ideally, we would want
those values to be in the 0.4–0.5 range, since that
is where the inter-annotator kappa coefficient lies
for the expert annotators.
</bodyText>
<subsectionHeader confidence="0.866747">
7.3 Filtering MTurk data by agreement with
experts
</subsectionHeader>
<bodyText confidence="0.986199725">
We can use the agreement rate with experts to
identify MTurk workers who are not performing
the task as required. For each worker w of the
669 workers for whom we have such data, we
compute the worker’s agreement rate with the ex-
perts, and from it a kappa coefficient Kexp(w) for
that worker. (Given that P(E) is 0.333, Kexp(w)
ranges between −0.5 and +1.0.) We sort the work-
ers based on Kexp(w) in ascending order, and ex-
amine properties of the MTurk data as we remove
the lowest-ranked workers one by one (Figure 4).
We first note that the amount of data we ob-
tained from MTurk is so large, that we could af-
ford to eliminate close to 30% of the labels, and
we would still have twice as much data than us-
ing the expert data alone. We also note that two
workers in particular (the 103rd and 130th to be
removed) are likely responsible for the majority
of the bad data, since removing their data leads to
noticeable jumps in the reference preference rate
and the inter-annotator agreement rate (right two
curves of Figure 4). Indeed, examining the data for
those two workers, we find that their RPR values
are 55.7% and 51.9%, which is a clear indication
of random clicking.15
Looking again at those two curves shows de-
grading values as we continue to remove workers
in large droves, indicating a form of “overfitting”
to agreement with experts (which, naturally, con-
tinues to increase until reaching 1.0; bottom left
curve). It is therefore important, if one were to fil-
ter out the MTurk data by removing workers this
way, to choose a cutoff carefully so that no crite-
rion is degraded dramatically.
In Appendix A, after reporting head-to-head
comparisons using only the expert data, we also
report head-to-head comparisons using the expert
15In retrospect, we should have performed this type of
analysis as the data was being collected, since such workers
could have been identified early on and blocked.
</bodyText>
<figure confidence="0.990114491525424">
INTER-ANNOTATOR AGREEMENT
P(A) K
With references 0.466 0.198
Without references 0.441 0.161
K*
0.487
0.439
K*
0.633
0.601
35
Agreement with Expert Data (kappa)
MTurk Data Remaining
(% of Expert Data)
�
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
160
140
120
100
20
�
80
40
60
0 100 200 300 400 500 600 700
# Workers Removed
0 100 200 300 400 500 600 700
# Workers Removed
Reference Preference Rate
Inter-Annotator Agreement (kappa)
100%
0.30
0.25
0.20
0.15
0.10
98%
96%
94%
92%
90%
88%
86%
84%
82%
0 100 200 300 400 500 600 700
# Workers Removed
0 100 200 300 400 500 600 700
# Workers Removed
</figure>
<figureCaption confidence="0.7708595">
Figure 4: The effect of removing an increasing number of MTurk workers. The order in which workers
are removed is by K p(w), the kappa agreement coefficient with expert data (excluding references).
</figureCaption>
<bodyText confidence="0.999864727272727">
data combined with the MTurk data, in order to
be able to detect more significant differences be-
tween the systems. We choose the 300-worker
point as a reasonable cutoff point before combin-
ing the MTurk data with the expert data, based
on the characteristics of the MTurk data at that
point: a high reference preference rate, high inter-
annotator agreement, and, critically, a kappa co-
efficient vs. expert data of 0.449, which is close
to the expert inter-annotator kappa coefficient of
0.439.
</bodyText>
<subsectionHeader confidence="0.998543">
7.4 Feasibility of using only MTurk data
</subsectionHeader>
<bodyText confidence="0.999991756756757">
In the previous subsection, we outlined an ap-
proach by which MTurk data can be filtered out
using expert data. Since we were to combine the
filtered MTurk data with the expert data to ob-
tain more significant differences, it was reason-
able to use agreement with experts to quantify the
MTurk workers’ competency. However, we also
would like to know whether it is feasible to use the
MTurk data alone. Our aim here is not to boost the
differences we see by examining expert data, but
to eliminate our reliance on obtaining expert data
in the first place.
We briefly examined some simple ways of fil-
tering/combining the MTurk data, and measured
the Spearman rank correlations obtained from the
MTurk data (alone), as compared to the rankings
obtained using the expert data (alone), and report
them in Table 12. (These correlations do not in-
clude the references.)
We first see that even when using the MTurk
data untouched, we already obtain relatively high
correlation with expert ranking (“Unfiltered”).
This is especially true for the *-to-English lan-
guage pairs, where we collected much more data
than English-to-*. In fact, the relationship be-
tween the amount of data and the correlation val-
ues is very strong, and it is reasonable to expect
the correlation numbers for English-to-* to catch
up had more data been collected.
We also measure rank correlations when apply-
ing some simple methods of cleaning/weighting
MTurk data. The first method (“Voting”) is per-
forming a simple vote whenever redundant com-
parisons (i.e. from different workers) are avail-
able. The second method (“K��p-filtered”) first re-
moves labels from the 300 worst workers accord-
ing to agreement with experts. The third method
</bodyText>
<page confidence="0.996075">
36
</page>
<bodyText confidence="0.999913153846154">
(“RPR-filtered”) first removes labels from the 62
worst workers according to their RPR. The num-
bers 300 and 62 were chosen since those are the
points at which the MTurk data reaches the level
of expert data in the inter-annotator agreement and
RPR of the experts.
The fourth and fifth methods (“Weighted by
Ke,,p” and “Weighted by K(RPR)”) do not re-
move any data, instead assigning weights to work-
ers based on their agreement with experts and their
RPR, respectively. Namely, for each worker, the
weight assigned by the fourth method is Ke,,p for
that worker, and the weight assigned by the fifth
method is K(RPR) for that worker.
Examining the correlation coefficients obtained
from those methods (Table 12), we see mixed re-
sults, and there is no clear winner among those
methods. It is also difficult to draw any conclusion
as to which method performs best when. However,
it is encouraging to see that the two RPR-based
methods perform well. This is noteworthy, since
there is no need to use expert data to weight work-
ers, which means that it is possible to evaluate a
worker using inherent, ‘built-in’ properties of that
worker’s own data, without resorting to making
comparisons with other workers or with experts.
</bodyText>
<sectionHeader confidence="0.996085" genericHeader="conclusions">
8 Summary
</sectionHeader>
<bodyText confidence="0.999866784313726">
As in previous editions of this workshop we car-
ried out an extensive manual and automatic eval-
uation of machine translation performance for
translating from European languages into English,
and vice versa.
The number of participants grew substantially
compared to previous editions of the WMT work-
shop, with 33 groups from 29 institutions partic-
ipating in WMT10. Most groups participated in
the translation task only, while the system combi-
nation task attracted a somewhat smaller number
of participants
Unfortunately, fewer rule-based systems partic-
ipated in this year’s edition of WMT, compared
to previous editions. We hope to attract more
rule-based systems in future editions as they in-
crease the variation of translation output and for
some language pairs, such as German-English,
tend to outperform statistical machine translation
systems.
This was the first time that the WMT workshop
was held as a joint workshop with NIST’s Metric-
sMATR evaluation initiative. This joint effort was
very productive as it allowed us to focus more on
the two evaluation dimensions: manual evaluation
of MT performance and the correlation between
manual metrics and automated metrics.
This year was also the first time we have in-
troduced quality assessments by non-experts. In
previous years all assessments were carried out
through peer evaluation exclusively consisting of
developers of machine translation systems, and
thereby people who are used to machine transla-
tion output. This year we have facilitated Ama-
zon’s Mechanical Turk to investigate two as-
pects of manual evaluation: How stable are man-
ual assessments across different assessor profiles
(experts vs. non-experts) and how reliable are
quality judgments of non-expert users? While
the intra- and inter-annotator agreements between
non-expert assessors are considerably lower than
for their expert counterparts, the overall rankings
of translation systems exhibit a high degree of cor-
relation between experts and non-experts. This
correlation can be further increased by applying
various filtering strategies reducing the impact of
unreliable non-expert annotators.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.16
</bodyText>
<sectionHeader confidence="0.997145" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999885428571429">
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-
0022, and the US National Science Foundation un-
der grant IIS-0713448.
</bodyText>
<sectionHeader confidence="0.942217" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.9180711">
Alexandre Allauzen, Josep M. Crego, lknur Durgar El-
Kahlout, and Francois Yvon. 2010. Limsi’s statisti-
cal translation systems for wmt’10. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 29–34, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Loic Barrault. 2010. Many: Open source mt system
combination at wmt’10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
</bodyText>
<footnote confidence="0.8974575">
16http://www.statmt.org/wmt09/results.
html
</footnote>
<page confidence="0.998468">
37
</page>
<table confidence="0.999474">
Label Unfiltered Voting Kexp-filtered RPR-filtered Weighted by Weighted by
count Kexp K(RPR)
en-de 2,583 0.862 0.779 0.818 0.862 0.868 0.862
en-es 2,488 0.759 0.785 0.797 0.797 0.768 0.806
en-fr 1,578 0.826 0.840 0.791 0.814 0.802 0.814
en-cz 627 0.833 0.818 0.354 0.833 0.851 0.828
de-en 12,570 0.914 0.925 0.920 0.931 0.933 0.926
es-en 12,870 0.934 0.969 0.965 0.987 0.978 0.987
fr-en 9,197 0.880 0.865 0.920 0.919 0.907 0.917
cz-en 13,169 0.951 0.909 0.965 0.944 0.930 0.944
</table>
<tableCaption confidence="0.8167115">
Table 12: Spearman rank coefficients for the MTurk data across the various language pairs, using differ-
ent methods to clean the data or weight workers. (These correlations were computed after excluding the
</tableCaption>
<bodyText confidence="0.851857789473684">
references.) Kexp is the kappa coefficient of the worker’s agreement rate with experts, with P(A) = 0.33.
K(RPR) is the kappa coefficient of the worker’s RPR (see 7.2), with P(A) = 0.66. In Kexp-filtering,
42% of labels remain, after removing 300 workers. In K(RPR)-filtering, 69% of labels remain, after
removing 62 workers.
and MetricsMATR, pages 252–256, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Ergun Bicici and S. Serdar Kozat. 2010. Adaptive
model weighting and transductive regression for pre-
dicting best system combinations. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 257–262, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Ergun Bicici and Deniz Yuret. 2010. L1 regularized
regression for reranking and system combination in
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 263–270, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
302–307, Uppsala, Sweden, July. Association for
Computational Linguistics.
Ondrej Bojar and Kamil Kos. 2010. 2010 failures
in english-czech phrase-based mt. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 35–41, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with amazons mechan-
ical turk. In Proceedings NAACL-2010 Workshop on
Creating Speech and Language Data With Amazons
Mechanical Turk, Los Angeles.
</bodyText>
<figureCaption confidence="0.962383318181818">
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07), Prague, Czech Repub-
lic.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, , Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the findings
of the 2009 workshop on statistical machine trans-
lation. In Proceedings of the Fourth Workshop on
Statistical Machine Translation (WMT09), Athens,
Greece.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using amazon’s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2009), Singapore.
</figureCaption>
<bodyText confidence="0.898396058823529">
Elisabet Comelles, Jesus Gimenez, Lluis Marquez,
Irene Castellon, and Victoria Arranz. 2010.
Document-level automatic mt evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 308–313, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Hui Cong, Zhao Hai, Lu Bao-Liang, and Song Yan.
2010. An empirical study on development set se-
lection strategy for machine translation learning.
In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 42–46, Uppsala, Sweden, July. Association
for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2010. Meteor-
next and the meteor paraphrase tables: Improved
</bodyText>
<page confidence="0.998443">
38
</page>
<bodyText confidence="0.275931">
evaluation support for five target languages. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 314–
317, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
</bodyText>
<figureCaption confidence="0.986815435897436">
Marcus Dobrinkat, Tero Tapiovaara, Jaakko V¨ayrynen,
and Kimmo Kettunen. 2010. Normalized compres-
sion distance based measures for metricsmatr 2010.
In Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
318–323, Uppsala, Sweden, July. Association for
Computational Linguistics.
Jinhua Du, Pavel Pecina, and Andy Way. 2010. An
augmented three-pass system combination frame-
work: Dcu combination system for wmt 2010. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
271–276, Uppsala, Sweden, July. Association for
Computational Linguistics.
Vladimir Eidelman, Chris Dyer, and Philip Resnik.
2010. The university of maryland statistical ma-
chine translation system for the fifth workshop on
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 47–51, Uppsala, Sweden,
July. Association for Computational Linguistics.
Christian Federmann, Andreas Eisele, Yu Chen, Sabine
Hunsicker, Jia Xu, and Hans Uszkoreit. 2010.
Further experiments with shallow hybrid mt sys-
tems. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 52–56, Uppsala, Sweden, July. Association
for Computational Linguistics.
Jes´us Gonz´alez-Rubio, Germ´an Sanchis-Trilles, Joan-
Andreu S´anchez, Jes´us Andr´es-Ferrer, Guillem
Gasc´o, Pascual Mart´ınez-G´omez, Martha-Alicia
Rocha, and Francisco Casacuberta. 2010. The upv-
prhlt combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 277–
281, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Greg Hanneman, Jonathan Clark, and Alon Lavie.
2010. Improved features and grammar selection for
syntax-based mt. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 57–62, Uppsala, Sweden, July.
Association for Computational Linguistics.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. Fbk at wmt 2010: Word lattices for
morphological reduction and chunk-based reorder-
ing. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 63–67, Uppsala, Sweden, July. Association
for Computational Linguistics.
Yifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in
wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 324–328, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Kenneth Heafield and Alon Lavie. 2010. Cmu multi-
engine machine translation for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 68–
73, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor
Leusch, Saab Mansour, Daniel Stein, and Hermann
Ney. 2010. The rwth aachen machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 74–78, Uppsala, Sweden,
July. Association for Computational Linguistics.
Carlos A. Henr´ıquez Q., Marta Ruiz Costa-juss`a, Vi-
das Daudaravicius, Rafael E. Banchs, and Jos´e B.
Mari˜no. 2010. Using collocation segmentation to
augment the phrase table. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 79–83, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
</figureCaption>
<bodyText confidence="0.668030227272727">
Almut Silja Hildebrand and Stephan Vogel. 2010.
Cmu system combination via hypothesis selection
for wmt’10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 282–285, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
St´ephane Huet, Julien Bourdaillet, Alexandre Patry,
and Philippe Langlais. 2010. The rali machine
translation system for wmt 2010. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 84–90, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Michael Jellinghaus, Alexandros Poulis, and David
Kolovratn´ık. 2010. Exodus - exploring smt for eu
institutions. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 91–95, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL
</bodyText>
<reference confidence="0.7278378">
2006 Workshop on Statistical Machine Translation,
New York, New York.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions, Prague, Czech Republic.
</reference>
<page confidence="0.999254">
39
</page>
<note confidence="0.866771285714286">
Philipp Koehn, Barry Haddow, Philip Williams, and
Hieu Hoang. 2010. More linguistic annotation for
statistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 96–101, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
</note>
<figureCaption confidence="0.932752048387097">
Patrik Lambert, Sadaf Abdul-Rauf, and Holger
Schwenk. 2010. Lium smt machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 102–107, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159–174.
Samuel Larkin, Boxing Chen, George Foster, Ulrich
Germann, Eric Joanis, Howard Johnson, and Roland
Kuhn. 2010. Lessons from nrcs portage system at
wmt 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 108–113, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Gregor Leusch and Hermann Ney. 2010. The rwth
system combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 290–
295, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 114–118, Uppsala, Sweden, July.
Association for Computational Linguistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. Tesla: Translation evaluation of sentences
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 329–
334, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Sushant Narsale. 2010. Jhu system combination
scheme for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 286–289, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Jan Niehues, Teresa Herrmann, Mohammed Mediani,
and Alex Waibel. 2010. The karlsruhe institute
for technology translation system for the acl-wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 119–123, Uppsala, Sweden, July. Association
for Computational Linguistics.
NIST. 2008. Evaluation plan for gale go/no-go phase
3 / phase 3.5 translation evaluations. June 18, 2008.
</figureCaption>
<reference confidence="0.903131666666667">
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat,
Pratyush Banerjee, Ankit K. Srivastava, Jinhua Du,
Pavel Pecina, Sudip Kumar Naskar, Mikel L. For-
cada, and Andy Way. 2010. Matrex: The dcu mt
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 124–129, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Aaron Phillips. 2010. The cunei machine translation
platform for wmt ’10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 130–135, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Juan Pino, Gonzalo Iglesias, Adri`a de Gispert, Graeme
Blackwood, Jamie Brunning, and William Byrne.
2010. The cued hifst system for the wmt10 trans-
lation shared task. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 136–141, Uppsala, Sweden,
July. Association for Computational Linguistics.
Marion Potet, Laurent Besacier, and Herv´e Blanchon.
2010. The lig machine translation system for wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 142–147, Uppsala, Sweden, July. Association
for Computational Linguistics.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 “Met-
rics for MAchine TRanslation” challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation, Honolulu, Hawaii.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. Bbn system descrip-
tion for wmt10 system combination task. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 296–
301, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Linear inversion transduction grammar alignments
as a second translation path. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 148–152, Uppsala,
</reference>
<page confidence="0.995331">
40
</page>
<reference confidence="0.934555347826087">
Sweden, July. Association for Computational Lin-
guistics.
Germ´an Sanchis-Trilles, Jes´us Andr´es-Ferrer, Guillem
Gasc´o, Jes´us Gonz´alez-Rubio, Pascual Martinez-
G´omez, Martha-Alicia Rocha, Joan-Andreu
S´anchez, and Francisco Casacuberta. 2010.
Upv-prhlt english–spanish system for wmt10. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
153–157, Uppsala, Sweden, July. Association for
Computational Linguistics.
Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.
2010. Incremental decoding for phrase-based sta-
tistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 197–204, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Lane Schwartz. 2010. Reproducible results in parsing-
based machine translation: The jhu shared task sub-
mission. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 158–163, Uppsala, Sweden, July.
Association for Computational Linguistics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and oovs: Two problems for translation
between german and english. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 164–169, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
J¨org Tiedemann. 2010. To cache or not to cache?
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 170–175, Uppsala, Sweden,
July. Association for Computational Linguistics.
Sami Virpioja, Jaakko V¨ayrynen, Andre Man-
sikkaniemi, and Mikko Kurimo. 2010. Apply-
ing morphological decompositions to statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 176–181, Uppsala, Sweden,
July. Association for Computational Linguistics.
Zdenˇek ˇZabokrtsk´y, Martin Popel, and David Mareˇcek.
2010. Maximum entropy translation model in
dependency-based mt framework. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 182–187, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Billy Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 335–
339, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Francisco Zamora-Martinez and Germ´an Sanchis-
Trilles. 2010. Uch-upv english–spanish system for
wmt10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 188–192, Uppsala, Sweden, July.
Association for Computational Linguistics.
Daniel Zeman. 2010. Hierarchical phrase-based mt
at the charles university for the wmt 2010 shared
task. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 193–196, Uppsala, Sweden, July. Association
for Computational Linguistics.
</reference>
<page confidence="0.999655">
41
</page>
<bodyText confidence="0.958281272727273">
A Pairwise system comparisons by human judges
Tables 13–20 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables’ cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row. Bolding indicates the winner of the two systems. The difference between
100 and the sum of the complimentary cells is the percent of time that the two systems were judged to
be equal.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables * indicates sta-
tistical significance at p G 0.10, t indicates statistical significance at p G 0.05, and t indicates statistical
significance at p G 0.01, according to the Sign Test.
</bodyText>
<subsectionHeader confidence="0.965241">
B Automatic scores
</subsectionHeader>
<bodyText confidence="0.989017">
The tables on pages 33–32 give the automatic scores for each of the systems.
</bodyText>
<subsectionHeader confidence="0.794984">
C Pairwise system comparisons for combined expert and non-expert data
</subsectionHeader>
<bodyText confidence="0.999670428571429">
Tables 21–20 show pairwise comparisons between systems for the into English direction when non-
expert judgments have been added.
The number of pairwise comparisons at the * level of significance increases from 48 to 50, and the
number at the t level of significants increases from 79 to 80 (basically same number). However, the
t level of significance went up considerably, from 280 to 369. That’s a 31% increase. 75 of t are
comparisons involving the reference, then the non-reference t count went up from 205 to 294, a 43%
increase.
</bodyText>
<table confidence="0.999917703703704">
REF – .00‡ .00‡ .00‡ .00‡ .00‡ .04‡ .03‡ .00‡ .00‡ .00‡ .00‡ .04‡ .00‡ .04‡ .00‡ .00‡ .00‡ .00‡ .05‡ .06‡ .03‡ .09‡ .04‡ .04‡
CAMBRIDGE .79‡ – .36 .16‡ .12‡ .23† .27 .43 .26† .38 .24 .3 .28 .51 .34 .23 .37 .24 .32 .46 .24 .29 .45 .59* .44
CMU-STATXFER .84‡ .58 – .16‡ .48 .14‡ .19 .39 .33 .54 .54* .50† .36 .50 .70‡ .55* .50 .46 .58† .67† .50 .56† .48 .58‡ .52†
CU-ZEMAN 1.00‡ .77‡ .72‡ – .76‡ .37 .73‡ .74‡ .79‡ .77‡ .77‡ .81‡ .75‡ .94‡ .86‡ .77‡ .89‡ .67 .77‡ .79‡ .81‡ .81‡ .77‡ .96‡ .86‡
DFKI 1.00‡ .72‡ .45 .12‡ – .32 .48 .50 .52 .53 .56 .65 .53 .62 .55 .43 .61* .50 .68† .73‡ .70† .60 .59* .72‡ .71‡
GENEVA 1.00‡ .69† .76‡ .48 .56 – .47 .71† .79‡ .72† .79‡ .71† .68† .76‡ .83‡ .57 .86‡ .72‡ .71† .69† .76† .65‡ .88‡ .96‡ .70
HUICONG .86‡ .54 .29 .12‡ .26 .37 – .48 .31 .43 .63‡ .62† .53 .55 .53‡ .44 .50 .55 .52 .68‡ .52* .51 .52* .57 .53
JHU .83‡ .39 .42 .13‡ .33 .19† .3 – .3 .36 .56† .56* .47 .52 .46 .29 .36 .42 .42 .59† .50 .31 .43 .29 .37
LIG .97‡ .63† .36 .15‡ .37 .18‡ .40 .60 – .62* .57‡ .39 .35 .54† .46 .33 .34 .38 .54† .48* .42 .44 .50 .61* .56
LIMSI .96‡ .41 .23 .19‡ .31 .17† .32 .50 .28* – .35 .42 .21 .62‡ .25 .21 .33 .22 .42 .35 .43 .32 .26 .35 .41
LIUM .83‡ .33 .21* .13‡ .41 .05‡ .13‡ .15† .09‡ .3 – .39 .19 .36 .43 .26 .23† .28 .29 .45 .28 .26 .28 .33 .28
NRC .96‡ .3 .10† .10‡ .32 .24† .15† .22* .22 .33 .43 – .26 .58 .26 .24 .3 .50 .36 .45 .47† .23 .38 .36† .35
ONLINEA .96‡ .55 .57 .14‡ .42 .16† .42 .4 .39 .53 .52 .47 – .52* .46 .36 .64 .57 .59 .50 .59 .42 .46 .43 .48
ONLINEB .87‡ .37 .33 .03‡ .29 .12‡ .31 .26 .16† .12‡ .39 .35 .20* – .33 .38 .17† .36 .29 .21 .33 .3 .3 .32 .21‡
RALI .89‡ .45 .15‡ .06‡ .35 .04‡ .12‡ .42 .35 .46 .32 .42 .39 .52 – .32 .31 .26 .43 .41 .27 .43 .40 .63* .26
RWTH .91‡ .46 .21* .05‡ .51 .36 .44 .46 .53 .39 .48 .48 .39 .48 .48 – .39 .38 .39 .52 .46 .53† .52 .50‡ .25
UEDIN .96‡ .40 .33 .03‡ .28* .03‡ .28 .29 .49 .38 .61† .3 .32 .50† .34 .24 – .42 .33 .43 .48 .18* .13 .27 .38
BBN-C .90‡ .48 .46 .29 .39 .22‡ .27 .27 .46 .43 .28 .35 .33 .39 .29 .34 .26 – .28 .44† .33 .26 .62* .36 .28
CMU-HEA-C .89‡ .50 .23† .14‡ .30† .21† .26 .25 .17† .33 .43 .16 .36 .43 .26 .29 .24 .24 – .48 .27 .13 .25 .30 .15
CMU-HYP-C .81‡ .17 .19† .11‡ .19‡ .19† .14‡ .14† .19* .40 .23 .18 .29 .46 .35 .29 .21 .15† .17 – .26 .18 .07‡ .32 .21
DCU-C .88‡ .27 .25 .11‡ .22† .24† .20* .28 .21 .35 .50 .10† .31 .44 .27 .29 .22 .21 .2 .30 – .12* .26 .26 .08
JHU-C .86‡ .48 .16† .16‡ .33 .21‡ .35 .41 .32 .44 .39 .35 .39 .37 .26 .19† .50* .23 .32 .43 .40* – .36 .27 .39
LIUM-C .87‡ .41 .36 .13‡ .31* .08‡ .21* .48 .31 .47 .44 .24 .39 .52 .28 .28 .33 .27* .25 .67‡ .26 .44 – .54‡ .48
RWTH-C .88‡ .18* .13‡ .04‡ .22‡ .04‡ .14 .24 .25* .3 .33 .05† .43 .50 .30* .13‡ .23 .14 .18 .21 .19 .23 .11‡ – .24
UPV-C .92‡ .25 .12† .10‡ .16‡ .3 .25 .34 .29 .31 .34 .29 .39 .65‡ .39 .36 .3 .45 .27 .36 .23 .16 .24 .28 –
&gt; others .90 .44 .31 .13 .33 .18 .29 .37 .34 .42 .44 .38 .37 .51 .41 .31 .38 .35 .38 .48 .39 .36 .40 .46 .37
&gt;= others .98 .66 .51 .21 .42 .27 .51 .59 .53 .65 .71 .66 .52 .71 .65 .55 .65 .64 .70 .77 .72 .65 .64 .77 .68
</table>
<tableCaption confidence="0.999379">
Table 13: Sentence-level ranking for the WMT10 French-English News Task
</tableCaption>
<figure confidence="0.9991122">
REF
CAMBRIDGE
CMU-STATXFER
CU-ZEMAN
DFKI
GENEVA
HUICONG
JHU
LIG
LIMSI
LIUM
NRC
ONLINEA
ONLINEB
RALI
RWTH
UEDIN
BBN-COMBO
CMU-HEAFIELD-COMBO
CMU-HYPOSEL-COMBO
DCU-COMBO
JHU-COMBO
LIUM-COMBO
RWTH-COMBO
UPV-COMBO
</figure>
<page confidence="0.994721">
42
</page>
<table confidence="0.999955409090909">
REF – .08‡ .02‡ .00‡ .04‡ .08‡ .13‡ .06‡ .09‡ .09‡ .07‡ .16‡ .11‡ .12‡ .12‡ .12‡ .05‡ .07‡ .08‡ .09‡
CAMBRIDGE .82‡ – .16‡ .24† .15‡ .07‡ .35 .10‡ .42 .36 .43 .27 .67‡ .46 .39 .44 .40 .46 .48? .40
CU-ZEMAN .98‡ .82‡ – .47 .54? .62‡ .71‡ .41 .79‡ .82‡ .70‡ .67‡ .85‡ .90‡ .75‡ .72‡ .92‡ .82‡ .88‡ .82‡
DFKI .95‡ .66† .31 – .46 .25? .78‡ .36 .59 .62? .75‡ .65† .45 .56? .75‡ .69‡ .71‡ .63? .57 .65†
EU .96‡ .78‡ .30? .41 – .55 .68‡ .16‡ .76‡ .72‡ .82‡ .67‡ .63‡ .86‡ .78‡ .78‡ .76‡ .76‡ .75‡ .71‡
GENEVA .86‡ .81‡ .23‡ .55? .34 – .65‡ .25‡ .65† .70‡ .69‡ .66‡ .77‡ .71‡ .70‡ .89‡ .75‡ .63† .84‡ .75‡
JHU .77‡ .42 .15‡ .22‡ .22‡ .22‡ – .06‡ .58? .47 .52† .49 .70‡ .61† .53 .64‡ .53? .65‡ .68‡ .50
KOC .85‡ .67‡ .4 .58 .55‡ .69‡ .82‡ – .76‡ .85‡ .81‡ .72‡ .86‡ .82‡ .86‡ .85‡ .77‡ .77‡ .74‡ .79‡
LIMSI .84‡ .23 .08‡ .29 .09‡ .30† .21? .08‡ – .33 .37 .17‡ .51 .40 .29 .45 .49 .40 .61‡ .28
LIUM .85‡ .39 .07‡ .32? .11‡ .21‡ .44 .07‡ .46 – .44 .4 .32 .44 .37 .64† .35 .40 .35 .42
NRC .91‡ .43 .15‡ .20‡ .11‡ .25‡ .21† .09‡ .31 .45 – .32 .48 .44 .49 .61† .52† .30 .58? .40
ONLINEA .80‡ .51 .21‡ .33† .23‡ .15‡ .41 .14‡ .60‡ .42 .54 – .52? .56? .36 .67‡ .61‡ .45 .50 .44
ONLINEB .87‡ .23‡ .08‡ .43 .23‡ .11‡ .12‡ .08‡ .27 .36 .43 .25? – .38 .31 .33 .52 .33? .46 .29
RALI .83‡ .38 .05‡ .27? .11‡ .15‡ .22† .10‡ .36 .44 .49 .31? .50 – .38 .44 .42 .37 .38 .34
RWTH .76‡ .33 .11‡ .12‡ .15‡ .17‡ .34 .05‡ .34 .44 .29 .42 .49 .40 – .56 .48 .44 .53‡ .50
UEDIN .84‡ .29 .20‡ .17‡ .12‡ .09‡ .19‡ .07‡ .33 .23† .24† .24‡ .56 .31 .3 – .36? .27 .51 .18†
CMU-HEAFIELD-COMBO .90‡ .23 .04‡ .23‡ .18‡ .12‡ .22? .11‡ .32 .41 .20† .23‡ .28 .31 .31 .11? – .29 .24 .3
KOC-COMBO .91‡ .26 .08‡ .31? .17‡ .28† .20‡ .07‡ .23 .26 .19 .36 .57? .37 .32 .32 .42 – .38 .34
RWTH-COMBO .85‡ .21? .02‡ .36 .16‡ .07‡ .12‡ .07‡ .16‡ .3 .30? .4 .34 .32 .06‡ .26 .35 .16 – .21?
UPV-COMBO .87‡ .38 .08‡ .30† .19‡ .19‡ .37 .11‡ .39 .24 .33 .37 .44 .27 .34 .46† .35 .28 .50? –
&gt; others .87 .43 .15 .30 .22 .25 .38 .13 .44 .45 .46 .41 .53 .49 .44 .52 .53 .45 .53 .45
&gt;=others .92 .63 .26 .40 .32 .35 .53 .26 .66 .63 .62 .55 .68 .66 .63 .70 .74 .68 .75 .66
</table>
<tableCaption confidence="0.998495">
Table 14: Sentence-level ranking for the WMT10 English-French News Task
</tableCaption>
<figure confidence="0.99832405">
REF
CAMBRIDGE
CU-ZEMAN
DFKI
EU
GENEVA
JHU
KOC
LIMSI
LIUM
NRC
ONLINEA
ONLINEB
RALI
RWTH
UEDIN
CMU-HEAFIELD-COMBO
KOC-COMBO
RWTH-COMBO
UPV-COMBO
</figure>
<page confidence="0.999113">
43
</page>
<table confidence="0.999975785714286">
REF – .00‡ .03‡ .00‡ .06‡ .03‡ .00‡ .00‡ .05‡ .00‡ .00‡ .03‡ .06‡ .09‡ .06‡ .00‡ .09‡ .03‡ .03‡ .14‡ .03‡ .06‡ .03‡ .03‡ .06‡ .00‡
AALTO 1.00‡ – .50 .31 .60 .69‡ .39 .41 .71† .31 .45 .60‡ .59† .65‡ .66‡ .64‡ .81‡ .45 .41 .69† .72‡ .75† .55 .55‡ .76‡ .57†
CMU .93‡ .31 – .29 .49 .57‡ .38 .50 .74‡ .13‡ .44 .59‡ .57† .59* .60† .67† .59‡ .41 .50 .68‡ .67‡ .46 .64‡ .55* .67‡ .54*
CU-ZEMAN 1.00‡ .44 .56 – .58 .64‡ .17 .44 .75‡ .38 .50 .54† .76† .79‡ .73‡ .72‡ .72‡ .50* .73‡ .78‡ .80‡ .68‡ .72† .62† .68* .73‡
DFKI .92‡ .25 .32 .27 – .53 .36 .46 .65* .07‡ .50 .47 .47 .69‡ .56 .35 .55 .58 .47 .67† .61* .52 .47 .38 .67† .51
FBK .97‡ .20‡ .16‡ .14‡ .38 – .11‡ .31 .45 .10‡ .22* .36 .50 .57† .37 .43 .40 .12‡ .17† .48* .43 .35 .38 .22 .38 .39
HUICONG .93‡ .35 .28 .46 .43 .75‡ – .52 .69† .16† .39 .42 .64† .79‡ .31 .51† .78‡ .27 .41 .49 .74‡ .68‡ .60* .37 .68‡ .56†
JHU .86‡ .34 .29 .16 .43 .31 .26 – .61‡ .15‡ .35 .36 .45 .69‡ .52* .56* .64† .27 .36 .70‡ .53 .47 .66‡ .52 .68‡ .44
KIT .89‡ .21† .10‡ .14‡ .29* .33 .19† .14‡ – .03‡ .27 .21† .36 .46 .17‡ .29 .24 .25‡ .25‡ .48 .23* .31 .38 .2 .36 .12‡
KOC .96‡ .58 .77‡ .48 .70‡ .77‡ .58† .71‡ .97‡ – .77‡ .90‡ .72‡ .82‡ .76‡ .84‡ .81‡ .84‡ .66‡ .83‡ .87‡ .79‡ .77‡ .75‡ .93‡ .71‡
LIMSI 1.00‡ .23 .28 .35 .35 .53* .33 .45 .41 .19‡ – .49 .48 .63† .49 .63‡ .52 .36 .29 .73‡ .53* .45 .59‡ .29 .56† .59†
LIU .88‡ .12‡ .15‡ .16† .39 .21 .46 .36 .61† .00‡ .27 – .44 .63† .49 .45 .53 .27* .33 .67‡ .55* .46 .44 .32 .37 .55
ONLINEA .92‡ .15† .23† .24† .42 .34 .21† .35 .50 .10‡ .32 .36 – .41 .4 .44 .37 .32 .34 .36 .4 .47 .3 .26 .48 .41
ONLINEB .68‡ .18‡ .29* .17‡ .26‡ .24† .18‡ .23‡ .33 .18‡ .23† .27† .34 – .3 .15‡ .29 .24† .15‡ .44 .28 .33* .20† .21‡ .38 .3
RWTH .88‡ .17‡ .20† .20‡ .37 .49 .41 .23* .61‡ .16‡ .4 .3 .43 .56 – .39 .50 .26 .49 .37 .29 .34 .41 .26 .44 .2
UEDIN .89‡ .14‡ .22† .13‡ .62 .34 .18† .22* .39 .03‡ .17‡ .3 .44 .67‡ .42 – .39 .15‡ .14‡ .52* .40 .36 .43 .26 .41 .38
UMD .91‡ .07‡ .14‡ .08‡ .36 .34 .11‡ .25† .48 .16‡ .24 .34 .52 .56 .41 .45 – .16‡ .21† .41 .28 .29 .43 .29 .25 .23
UPPSALA .97‡ .32 .34 .17* .36 .54‡ .23 .37 .70‡ .00‡ .41 .62* .56 .68† .57 .64‡ .59‡ – .2 .63‡ .69‡ .51‡ .60* .33 .69‡ .63‡
UU-MS .82‡ .22 .43 .14‡ .45 .51† .19 .21 .68‡ .14‡ .39 .52 .60 .64‡ .44 .53‡ .61† .28 – .36 .58‡ .52* .53* .30 .64‡ .44
BBN-C .86‡ .25† .10‡ .07‡ .27† .17* .23 .18‡ .35 .07‡ .15‡ .12‡ .32 .41 .3 .19* .22 .15‡ .27 – .39 .06† .23* .11‡ .21 .18†
CMU-HEA-C .87‡ .14‡ .15‡ .08‡ .29* .33 .04‡ .26 .53* .00‡ .20* .24* .44 .31 .46 .23 .53 .15‡ .13‡ .27 – .40 .2 .14‡ .22 .28
CMU-HYP-C .94‡ .25† .24 .14‡ .44 .3 .15‡ .26 .47 .08‡ .45 .31 .42 .67* .24 .36 .46 .14‡ .21* .50† .32 – .43 .28 .51* .42
JHU-C .97‡ .34 .11‡ .20† .29 .34 .29* .03‡ .38 .12‡ .07‡ .29 .55 .67† .34 .32 .23 .24* .24* .48* .40 .32 – .27 .37 .31
KOC-C .88‡ .00‡ .23* .21† .53 .44 .29 .22 .43 .08‡ .36 .50 .53 .63‡ .39 .37 .39 .28 .19 .64‡ .61‡ .38 .55 – .48* .46
RWTH-C .82‡ .09‡ .06‡ .29* .25† .25 .18‡ .18‡ .24 .03‡ .19† .26 .36 .54 .25 .26 .33 .06‡ .14‡ .29 .22 .23* .3 .17* – .13‡
UPV-C .97‡ .17† .21* .17‡ .36 .36 .23† .19 .67‡ .20‡ .18† .29 .41 .40 .40 .38 .48 .17‡ .31 .50† .43 .27 .27 .27 .65‡ –
&gt; others .91 .23 .25 .20 .39 .42 .24 .30 .53 .11 .31 .38 .47 .59 .42 .43 .48 .27 .30 .53 .49 .42 .44 .31 .51 .41
&gt;= others .96 .42 .46 .36 .50 .66 .47 .53 .72 .23 .52 .59 .63 .73 .62 .66 .68 .51 .55 .77 .73 .65 .67 .59 .75 .64
</table>
<tableCaption confidence="0.944281">
Table 15: Sentence-level ranking for the WMT10 German-English News Task
</tableCaption>
<table confidence="0.999980380952381">
REF – .03‡ .06‡ .01‡ .02‡ .05‡ .00‡ .00‡ .01‡ .04‡ .03‡ .01‡ .01‡ .01‡ .02‡ .01‡ .01‡ .05‡ .06‡
CU-ZEMAN .97‡ – .85‡ .67‡ .62‡ .78‡ .58* .70‡ .64‡ .80‡ .85‡ .64‡ .52 .80‡ .61† .79‡ .69‡ .76‡ .73‡
DFKI .89‡ .14‡ – .36† .24‡ .38 .30‡ .27‡ .36* .36* .55 .35† .21‡ .41 .39 .46 .38* .47 .37*
FBK .97‡ .30‡ .59† – .35† .42 .12‡ .36 .48 .48 .64‡ .39 .29‡ .46 .30† .44 .46 .48 .38
JHU .98‡ .27‡ .72‡ .57† – .59‡ .30‡ .51 .53 .56* .65‡ .43 .39 .66‡ .45 .56 .61‡ .52 .47
KIT .92‡ .18‡ .55 .42 .29‡ – .23‡ .32 .32† .43 .53* .41 .27‡ .43 .23‡ .41 .41 .42 .37
KOC 1.00‡ .37* .64‡ .82‡ .62‡ .70‡ – .74‡ .74‡ .74‡ .82‡ .63‡ .48 .62† .65‡ .73‡ .67‡ .81‡ .71‡
LIMSI .95‡ .27‡ .68‡ .39 .45 .49 .17‡ – .49 .74‡ .70‡ .51 .28‡ .58‡ .32 .51 .53* .52† .31
LIU .95‡ .32‡ .59* .4 .36 .58† .21‡ .37 – .39 .74‡ .33* .23‡ .55† .36* .49 .42 .46 .38
ONLINEA .95‡ .16‡ .55* .4 .36* .45 .21‡ .23‡ .50 – .56† .38 .23‡ .41 .23‡ .48 .4 .50 .33†
ONLINEB .92‡ .12‡ .42 .26‡ .27‡ .33* .14‡ .23‡ .21‡ .32† – .24‡ .14‡ .39 .19‡ .29‡ .27‡ .36 .32‡
RWTH .98‡ .33‡ .61† .51 .47 .46 .30‡ .33 .52* .55 .71‡ – .33† .57* .45 .40 .51† .47 .46
SFU .98‡ .42 .77‡ .66‡ .51 .69‡ .48 .68‡ .69‡ .72‡ .77‡ .56† – .82‡ .53 .65‡ .69‡ .73‡ .62‡
UEDIN .94‡ .17‡ .51 .4 .31‡ .49 .34† .25‡ .30† .52 .52 .36* .10‡ – .33* .31 .42 .38 .22‡
UPPSALA .97‡ .36† .55 .51† .47 .70‡ .25‡ .46 .57* .67‡ .71‡ .41 .38 .54* – .53† .42 .58‡ .40
CMU-HEAFIELD-COMBO .96‡ .17‡ .49 .36 .36 .37 .21‡ .35 .49 .42 .64‡ .38 .28‡ .48 .28† – .35 .46 .35
KOC-COMBO .99‡ .27‡ .56* .32 .27‡ .32 .23‡ .32* .41 .55 .64‡ .30† .21‡ .37 .36 .41 – .34 .36
RWTH-COMBO .92‡ .17‡ .50 .34 .35 .41 .09‡ .25† .38 .4 .54 .38 .20‡ .42 .19‡ .28 .35 – .16‡
UPV-COMBO .93‡ .23‡ .58* .38 .36 .51 .23‡ .50 .49 .57† .60‡ .42 .28‡ .51‡ .3 .38 .46 .48‡ –
&gt; others .95 .24 .57 .44 .37 .48 .24 .39 .45 .51 .63 .40 .27 .51 .34 .45 .44 .49 .39
&gt;=others .98 .28 .62 .56 .46 .60 .30 .51 .55 .59 .70 .51 .34 .62 .47 .59 .59 .65 .55
</table>
<tableCaption confidence="0.99939">
Table 16: Sentence-level ranking for the WMT10 English-German News Task
</tableCaption>
<figure confidence="0.999284422222222">
REF
AALTO
CMU
CU-ZEMAN
DFKI
FBK
HUICONG
JHU
KIT
KOC
LIMSI
LIU
ONLINEA
ONLINEB
RWTH
UEDIN
UMD
UPPSALA
UU-MS
BBN-COMBO
CMU-HEAFIELD-COMBO
CMU-HYPOSEL-COMBO
JHU-COMBO
KOC-COMBO
RWTH-COMBO
UPV-COMBO
REF
CU-ZEMAN
DFKI
FBK
JHU
KIT
KOC
LIMSI
LIU
ONLINEA
ONLINEB
RWTH
SFU
UEDIN
UPPSALA
CMU-HEAFIELD-COMBO
KOC-COMBO
RWTH-COMBO
UPV-COMBO
</figure>
<page confidence="0.995099">
44
</page>
<table confidence="0.999940823529412">
REF – .00‡ .01‡ .01‡ .01‡ .00‡ .00‡ .00‡ .00‡ .00‡ .01‡ .02‡ .05‡ .01‡ .04‡
CAMBRIDGE .95‡ – .23‡ .14‡ .34* .31† .41 .34 .62‡ .45* .35 .40* .42 .22† .44
COLUMBIA .97‡ .58‡ – .25‡ .52 .45 .59‡ .53* .65‡ .60‡ .47 .56‡ .55‡ .45 .58‡
CU-ZEMAN .96‡ .71‡ .59‡ – .60‡ .68‡ .79‡ .66‡ .75‡ .80‡ .66‡ .79‡ .78‡ .69‡ .75‡
DFKI .97‡ .51* .37 .23‡ – .43 .59‡ .52† .66‡ .62‡ .48 .53† .55† .55† .64‡
HUICONG .95‡ .50† .34 .21‡ .41 – .45 .50 .66‡ .61‡ .39 .50* .59‡ .40 .52‡
JHU .98‡ .39 .22‡ .12‡ .30‡ .33 – .37 .56‡ .51‡ .34 .39 .34† .22‡ .34
ONLINEA .96‡ .46 .37* .23‡ .32† .38 .44 – .59‡ .53† .4 .50 .36 .30† .54‡
ONLINEB .88‡ .25‡ .21‡ .16‡ .23‡ .21‡ .27‡ .23‡ – .35 .24‡ .28‡ .34† .22‡ .36
UEDIN .96‡ .31* .28‡ .10‡ .25‡ .19‡ .25‡ .31† .48 – .23‡ .27† .31 .23‡ .2
UPC .94‡ .47 .4 .20‡ .41 .33 .43 .46 .66‡ .56‡ – .50* .52† .48* .49†
BBN-COMBO .95‡ .26* .31‡ .09‡ .32† .34* .33 .37 .54‡ .44† .33* – .35 .24‡ .34
CMU-HEAFIELD-COMBO .91‡ .39 .21‡ .08‡ .34† .22‡ .16† .42 .57† .45 .31† .31 – .14‡ .27
JHU-COMBO .95‡ .40† .32 .15‡ .36† .31 .44‡ .50† .66‡ .50‡ .32* .47‡ .43‡ – .43†
UPV-COMBO .92‡ .35 .28‡ .16‡ .27‡ .23‡ .38 .28‡ .47 .30 .28† .26 .35 .25† –
&gt; others .95 .41 .30 .15 .33 .32 .39 .39 .56 .48 .34 .41 .43 .32 .43
&gt;=others .99 .61 .45 .27 .45 .50 .61 .54 .70 .69 .51 .62 .66 .55 .66
</table>
<tableCaption confidence="0.947482">
Table 17: Sentence-level ranking for the WMT10 Spanish-English News Task
</tableCaption>
<table confidence="0.999978789473684">
REF – .00‡ .02‡ .07‡ .15‡ .07‡ .02‡ .11‡ .14‡ .07‡ .07‡ .03‡ .06‡ .09‡ .06‡ .03‡ .07‡
CAMBRIDGE .91‡ – .28† .45 .38 .45 .11‡ .52 .61† .21* .52 .47 .35 .54 .51 .39 .49
CU-ZEMAN .95‡ .70† – .79‡ .75‡ .85‡ .49 .83‡ .82‡ .74‡ .87‡ .67‡ .85‡ .81‡ .80‡ .70‡ .74‡
DCU .93‡ .32 .21‡ – .45 .32 .09‡ .70† .59 .24‡ .48 .38 .29 .32 .36 .24 .14‡
DFKI .80‡ .41 .15‡ .45 – .38 .12‡ .64† .57 .4 .57 .31 .41 .59 .50 .48 .47
JHU .90‡ .37 .10‡ .52 .56 – .17‡ .67† .67‡ .26† .34 .3 .49 .54 .53† .47 .35
KOC .98‡ .87‡ .47 .88‡ .73‡ .76‡ – .76‡ .87‡ .67‡ .83‡ .86‡ .90‡ .87‡ .90‡ .86‡ .86‡
ONLINEA .82‡ .42 .08‡ .30† .18† .24† .20‡ – .49 .36 .25† .17‡ .25† .45 .30* .29 .18‡
ONLINEB .76‡ .26† .10‡ .32 .37 .22‡ .10‡ .34 – .21‡ .28 .24† .32 .33 .22‡ .19‡ .27*
SFU .91‡ .54* .19‡ .67‡ .51 .63† .27‡ .64 .72‡ – .74‡ .57* .68‡ .77‡ .71‡ .64‡ .46
UEDIN .91‡ .3 .08‡ .4 .38 .34 .14‡ .71† .49 .09‡ – .34 .4 .58 .33 .3 .31
UPV .94‡ .34 .07‡ .41 .53 .54 .07‡ .73‡ .61† .27* .45 – .37 .51 .44 .38 .48†
UCH-UPV .90‡ .55 .07‡ .58 .51 .41 .08‡ .69† .52 .24‡ .51 .46 – .47 .41 .49 .49
CMU-HEAFIELD-COMBO .83‡ .29 .13‡ .37 .38 .35 .07‡ .48 .54 .08‡ .29 .26 .28 – .17† .21* .21
KOC-COMBO .88‡ .27 .15‡ .40 .42 .24† .03‡ .62* .60‡ .15‡ .41 .27 .34 .53† – .3 .40
RWTH-COMBO .92‡ .36 .21‡ .52 .33 .31 .10‡ .55 .65‡ .14‡ .37 .22 .41 .52* .48 – .31
UPV-COMBO .91‡ .32 .13‡ .69‡ .4 .32 .09‡ .76‡ .52* .36 .38 .19† .31 .45 .35 .28 –
&gt; others .89 .39 .15 .48 .44 .41 .14 .61 .58 .29 .46 .36 .42 .51 .44 .39 .40
&gt;=others .93 .54 .23 .61 .55 .55 .19 .69 .71 .40 .61 .55 .54 .68 .62 .59 .60
</table>
<tableCaption confidence="0.999063">
Table 18: Sentence-level ranking for the WMT10 English-Spanish News Task
</tableCaption>
<figure confidence="0.99930946875">
REF
CAMBRIDGE
COLUMBIA
CU-ZEMAN
DFKI
HUICONG
JHU
ONLINEA
ONLINEB
UEDIN
UPC
BBN-COMBO
CMU-HEAFIELD-COMBO
JHU-COMBO
UPV-COMBO
REF
CAMBRIDGE
CU-ZEMAN
DCU
DFKI
JHU
KOC
ONLINEA
ONLINEB
SFU
UEDIN
UPV
UCH-UPV
CMU-HEAFIELD-COMBO
KOC-COMBO
RWTH-COMBO
UPV-COMBO
</figure>
<page confidence="0.995216">
45
</page>
<table confidence="0.999936933333333">
REF – .04‡ .02‡ .03‡ .00‡ .02‡ .00‡ .03‡ .03‡ .04‡ .01‡ .04‡ .02‡
AALTO .88‡ – .49 .51 .22‡ .38 .64‡ .55† .57* .71‡ .64‡ .65‡ .59‡
CMU .97‡ .35 – .4 .14‡ .18‡ .59‡ .49† .45† .57‡ .50‡ .34 .43
CU-BOJAR .90‡ .33 .43 – .12‡ .20‡ .64‡ .45 .45 .54‡ .42 .42 .41
CU-ZEMAN .99‡ .60‡ .77‡ .75‡ – .56† .81‡ .78‡ .88‡ .79‡ .84‡ .84‡ .76‡
ONLINEA .92‡ .46 .68‡ .59‡ .28† – .65‡ .54‡ .72‡ .75‡ .58‡ .57‡ .66‡
ONLINEB .97‡ .27‡ .28‡ .21‡ .10‡ .17‡ – .25† .32 .22 .21† .32 .28
UEDIN .95‡ .28† .26† .38 .07‡ .22‡ .49† – .60‡ .52‡ .33 .31 .32
BBN-COMBO .92‡ .31* .20† .39 .08‡ .15‡ .41 .16‡ – .27 .25 .3 .26
CMU-HEAFIELD-COMBO .90‡ .13‡ .23‡ .25‡ .07‡ .15‡ .31 .23‡ .34 – .18‡ .35 .28
JHU-COMBO .93‡ .20‡ .19‡ .33 .08‡ .25‡ .48† .39 .38 .52‡ – .37 .42
RWTH-COMBO .92‡ .18‡ .37 .38 .13‡ .25‡ .34 .28 .43 .40 .26 – .25
UPV-COMBO .96‡ .25‡ .36 .41 .11‡ .27‡ .45 .35 .37 .44 .31 .34 –
&gt; others .93 .28 .36 .38 .11 .23 .49 .38 .47 .48 .38 .40 .40
&gt;=others .98 .43 .55 .55 .22 .37 .70 .61 .70 .71 .62 .65 .63
</table>
<tableCaption confidence="0.96325">
Table 19: Sentence-level ranking for the WMT10 Czech-English News Task
</tableCaption>
<table confidence="0.99997715">
REF – .04‡ .04‡ .03‡ .01‡ .05‡ .03‡ .08‡ .04‡ .04‡ .03‡ .02‡ .02‡ .04‡ .08‡ .04‡ .07‡ .04‡
CU-BOJAR .87‡ – .46 .27‡ .12‡ .28‡ .16‡ .17‡ .44 .4 .11‡ .27‡ .41 .28 .52‡ .28 .42 .43
CU-TECTO .88‡ .36 – .30† .23‡ .38 .17‡ .28‡ .56† .44 .29† .27‡ .36 .45 .51† .4 .58† .35
CU-ZEMAN .91‡ .58‡ .51† – .38 .49 .19‡ .39 .62‡ .63‡ .36 .41 .48 .51‡ .58‡ .48† .54† .55‡
DCU .98‡ .73‡ .52‡ .43 – .59‡ .22‡ .47 .74‡ .63‡ .47† .53† .56‡ .77‡ .77‡ .62‡ .76‡ .71‡
EUROTRANS .88‡ .61‡ .47 .33 .30‡ – .10‡ .33 .51 .54† .25‡ .27‡ .49 .57‡ .59† .49 .57‡ .60‡
KOC .93‡ .69‡ .67‡ .54‡ .49‡ .77‡ – .54‡ .71‡ .70‡ .51‡ .55‡ .64‡ .72‡ .78‡ .65‡ .76‡ .78‡
ONLINEA .91‡ .62‡ .57‡ .51 .39 .44 .24‡ – .66‡ .62‡ .39 .43 .55‡ .60‡ .61‡ .59‡ .73‡ .61‡
ONLINEB .91‡ .31 .29† .27‡ .13‡ .33 .14‡ .19‡ – .44 .22‡ .09‡ .39 .19 .34 .24* .22† .39
PC-TRANS .88‡ .45 .43 .24‡ .26‡ .29† .21‡ .24‡ .49 – .22‡ .27‡ .37 .43 .55† .33† .49 .41
POTSDAM .88‡ .60‡ .51† .40 .27† .59‡ .25‡ .47 .63‡ .64‡ – .45 .52‡ .56‡ .69‡ .61‡ .70‡ .68‡
SFU .95‡ .52‡ .56‡ .4 .30† .61‡ .27‡ .39 .65‡ .64‡ .29 – .55‡ .54‡ .76‡ .53‡ .70‡ .60‡
UEDIN .94‡ .39 .44 .33 .23‡ .32 .20‡ .26‡ .32 .49 .25‡ .26‡ – .43 .57‡ .18 .46† .42
CMU-HEAFIELD-COMBO .91‡ .42 .39 .23‡ .10‡ .27‡ .14‡ .19‡ .23 .35 .24‡ .19‡ .28 – .48‡ .28 .34 .29
DCU-COMBO .84‡ .23‡ .27† .23‡ .03‡ .31† .10‡ .21‡ .42 .31† .15‡ .10‡ .16‡ .20‡ – .18‡ .27* .22‡
KOC-COMBO .91‡ .37 .49 .25† .10‡ .39 .17‡ .32‡ .42* .55† .17‡ .27‡ .26 .33 .41‡ – .32 .22
RWTH-COMBO .88‡ .29 .34† .28† .05‡ .26‡ .10‡ .17‡ .48† .43 .16‡ .15‡ .24† .33 .46* .36 – .29
UPV-COMBO .92‡ .37 .52 .22‡ .09‡ .25‡ .10‡ .19‡ .28 .47 .15‡ .25‡ .33 .24 .49‡ .34 .39 –
&gt; others .91 .45 .44 .32 .20 .39 .16 .29 .49 .49 .25 .28 .40 .43 .54 .39 .50 .45
&gt;=others .96 .66 .60 .50 .38 .54 .33 .44 .70 .62 .44 .45 .62 .69 .75 .66 .70 .68
</table>
<tableCaption confidence="0.998579">
Table 20: Sentence-level ranking for the WMT10 English-Czech News Task
</tableCaption>
<figure confidence="0.999472806451613">
REF
AALTO
CMU
CU-BOJAR
CU-ZEMAN
ONLINEA
ONLINEB
UEDIN
BBN-COMBO
CMU-HEAFIELD-COMBO
JHU-COMBO
RWTH-COMBO
UPV-COMBO
REF
CU-BOJAR
CU-TECTO
CU-ZEMAN
DCU
EUROTRANS
KOC
ONLINEA
ONLINEB
PC-TRANS
POTSDAM
SFU
UEDIN
CMU-HEAFIELD-COMBO
DCU-COMBO
KOC-COMBO
RWTH-COMBO
UPV-COMBO
</figure>
<page confidence="0.9965">
46
</page>
<table confidence="0.998304941176471">
=&amp;quot;5H ,-./012345670 66 R?)=A7% %&amp;&apos;8A)?E )* WOKM R?)=A7%RBMJ 2+3+X F#&amp;% )D* &apos;&amp;quot;%?#A5? 8?)=A7% !CSNY &amp;quot;5E WOKM$, K7*=?% L*= Z&amp;quot;##Z !?5[=? \RM3+ )?%)%?)$ &amp;quot;5E Z%&amp;&apos;Z !%&amp;&apos;%?) *L )@? @&amp;8&amp;quot;5#] &amp;quot;%%?%%?E E&amp;quot;)&amp;quot;$ &amp;quot;=? %@*D5, SJ^C-
RM6W_I RM68W_I C&amp;quot;E`?= L&amp;## C&amp;quot;E`?= #A)? BMN_ 2,3 RNMNTJ &amp;quot;EU RNMNTJ @)?= RNMNTJ =&amp;quot;5H KNVOB K?8VTK K?8VTK CSNY I_Y6S:a
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
+,-. +,/- +,/. +,-0 +,/+ +,-1 +,-0 +,-. +,-1 +,/+ +,-/ +,2- +,3/ +,24 +,./ +,/3
+,4+ +,/1 !&amp;quot;#$ !&amp;quot;#% !&amp;quot;#% !&amp;quot;#% !&amp;quot;#% !&amp;quot;#! !&amp;quot;## !&amp;quot;#&amp; !&amp;quot;#&apos; !&amp;quot;() !&amp;quot;&apos;&amp; !&amp;quot;*&amp; !&amp;quot;%% !&amp;quot;#$
!&amp;quot;&amp;&apos; +,/1 +,// +,/- +,/- +,/. +,/. +,/+ +,/- !&amp;quot;#&amp; +,/+ !&amp;quot;() !&amp;quot;&apos;&amp; +,.; !&amp;quot;%% !&amp;quot;#$
+,// +,// +,/. +,/2 +,/2 +,/3 +,/2 +,-; +,/2 +,/- +,-1 +,24 !&amp;quot;&apos;&amp; +,.. +,-3 +,/-
+,// +,/; +,/. +,/+ +,/3 +,-0 +,/+ +,-/ +,/3 +,/. +,-4 +,2/ +,3/ +,.+ +,.4 +,/.
+,22 +, 0 +,/2 +, / +, ; +, / +, / +,.0 +, / +, ; 2 +,23 +,3. +,2+ +,.+ +,/+
+,
+,;2 +,/4 !&amp;quot;#$ !&amp;quot;#% !&amp;quot;#% +,/. !&amp;quot;#% +,-1 +,/. +,// +,-0 +,21 !&amp;quot;&apos;&amp; +,./ +,-2 +,/.
+,.4 +,/ +,/2 +, 0 +,/+ +, 1 +,/+ +, 2 +, 4 +, 0 +, +,2 +,3 +,24 +, +, 0
+,4+ !&amp;quot;#) !&amp;quot;#$ !&amp;quot;#% +,/. +,/. +,/. +,-0 +,/- +,/; +,/+ !&amp;quot;() !&amp;quot;&apos;&amp; +,.; +,-. +,/.
+,;/ +,/1 +,// !&amp;quot;#% !&amp;quot;#% +,/. +,/. +, 0 +,/. +,/; +, 0 !&amp;quot;() !&amp;quot;&apos;&amp; +,./ 2 +,//
+,
+,;3 +,/4 +,/- +,/2 +,/. +,/3 +,/2 +,-1 +,/. +,// +,-0 +,21 +,3; +,.- +,-2 +,//
+,;. +,/1 +,// !&amp;quot;#% !&amp;quot;#% +,/. +,/. +, 0 +,/ +,/; +,/+ !&amp;quot;() !&amp;quot;&apos;&amp; +,./ +, +,//
</table>
<figure confidence="0.993129954022989">
&amp;quot;&amp;quot;#)*
&apos;&apos;567*8&apos;*
78&amp;69:67*8&apos;*
78&amp;
7&amp;6&apos;*&lt;&amp;quot;=
7&amp;6&gt;?8&amp;quot;5
&lt;@&amp;67*8&apos;*
*5#A5?B
*5#A5?C
=D)@67*8&apos;*
&amp;?EA5
&amp;FG67*8&apos;*
SJ9C-
!&amp;quot;##$ !%&amp;&apos;$
+,--
!&amp;quot;%)
+,-1
+,-;
+,-;
+, 3
+,-4
+, 2
+,-;
+, 1
+,-1
+, 1
O #?P?= CSNY O #?P?= =?7&amp;quot;## KQR J&amp;quot;5H MNKSB R MNKSB K)&amp;quot;5L*=E MNJF IJ I=E*7 YS_@ CNDM6N CH&amp;quot;=%
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
&amp;quot;&amp;quot;#)* +,-0 +,/2 -,04 !&amp;quot;%&apos; +,+2 +,+4 +,24 +,-2 +,2/ +,/2 +,/2
!&amp;quot;** !&amp;quot;%+ !&amp;quot;*! !&amp;quot;#+ !&amp;quot;#+
!&amp;quot;** +,-1 !&amp;quot;*! +,/4 +,/4
78&amp; +,/+ +,/. /,3+ +,-/ +,+2 +,+0 +,.3 +,-; +,21 +,/- +,/-
7&amp;6&apos;*&lt;&amp;quot;= +,/2 +,// /,2. +,-/ +,+2 +,+/ +,.+ +,-/ +,24 +,// +,//
7&amp;6&gt;?8&amp;quot;5 +,-4 +,-0 -,44 +,.; +,+2 +,+. +,2. +,.4 +,22 +,-0 +,-0
&lt;@&amp;67*8&apos;* +,/. +,// /,2/ +,-; +,+2 +,+. +,.2 +,-; +,20 +,/; +,/;
*5#A5?B +,-0 +,/3 -,14 +,-2 +,+2 +,+4 +,24 +, +,2/ +,/2 +,/2
*5#A5?C !&amp;quot;## !&amp;quot;#&amp; /,-3 !&amp;quot;%+ !&amp;quot;!* !&amp;quot;&apos;$ +,.2 +,-4 !&amp;quot;*! !&amp;quot;#+ !&amp;quot;#+
=D)@67*8&apos;* +,/-
+,/; /,-+ !&amp;quot;%+ !&amp;quot;!* +,+4 +,.2 +,-4 +,20 +,/4 +,/4
&amp;?EA5 +,/. !&amp;quot;#&amp;
/,-+ !&amp;quot;%+ !&amp;quot;!* +,3- +,.2 +,-4 +,20 +,/4 +,/4
&amp;FG67*8&apos;* +,/- !&amp;quot;#&amp; #&amp;quot;%&amp; !&amp;quot;%+ !&amp;quot;!* +,+4 !&amp;quot;** +,-4 +,20 !&amp;quot;#+ !&amp;quot;#+
!&amp;quot;#&amp; /,-; !&amp;quot;%+ !&amp;quot;!*
+,/4 /,-. !&amp;quot;%+ !&amp;quot;!*
+,33
+,+0
+,/-
+,/-
&apos;&apos;567*8&apos;*
78&amp;69:67*8&apos;*
CSNY G3.&amp;quot; WOKM
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
+,31 ;,2+
!&amp;quot;(# &amp;&amp;quot;&apos;!
!&amp;quot;(# ;,01
+,22 ;,;;
+,30 ;,2-
+,3- /,-/
+,2. 4,++
+,31 ;,3.
+,2- ;,3.
+,2- 4,+;
+,2. ;,;-
+,2- ;,00
&apos;&apos;)*+,-&apos;,
+&amp;quot; &apos;56789
+-&amp;*&lt;=*+,-&apos;,
+ &amp;*?@A*+, &apos;,
+-&amp;*%B&amp;quot;BCD95
+&amp;*F9 &amp;quot;)
7+&amp;*+,-&apos;,
7G6
89)9H&amp;quot;
?&amp;6+,)8
I?&amp;*+,-&apos;,
I?&amp;
#68
#6 %6
#6&amp;-*+,-&apos;,
#6&amp;
)5+
,)#6)9J
,)#6)9K
5&amp;quot;#6
5LB?*+,-&apos;,
5LB?
&amp;976)
</figure>
<table confidence="0.964143058823529">
&amp;AH*+, &apos;,
5&amp;quot;)[ 0123456738#9:5 ** M9B56+% %&amp;&apos;-6N97 B, OPQR M9B56+%MJRS E.4.T A#&amp;% BL, &apos;&amp;quot;%9#6)9 -9B56+% !KUVW &amp;quot;)7 OPQR$/ Q+,59% D,5 X&amp;quot;##X !9)Y59 ZMR4. B9%B%9B$ &amp;quot;)7 X%&amp;&apos;X !%&amp;&apos;%9B ,D B?9 ?&amp;-&amp;quot;)#@ &amp;quot;%%9%%97 7&amp;quot;B&amp;quot;$ &amp;quot;59 %?,L)/ US_K1 US&lt;K1
MR*O\] MR*-O\] K&amp;quot;7895 D&amp;## K&amp;quot;7895 #6B9 JRV\ E/4 MVRVaS &amp;quot;7c MVRVaS ?B95 MVRVaS 5&amp;quot;)[ QV`PJ Q9-`aQ Q9-`aQ KUVW ]\W*U=^
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
./01 ./0. %&amp;&apos;( %&amp;&apos;) %&amp;&apos;* ./23 %&amp;&apos;) %&amp;+, %&amp;&apos;- %&amp;.. %&amp;/* ./14 %&amp;&apos;% %&amp;&apos;* ./2.
./00 ./0. %&amp;&apos;( ./2: %&amp;&apos;* ./23 %&amp;&apos;) %&amp;+, %&amp;&apos; %&amp; %&amp;/* ./1 /1; %&amp;&apos;* ./2.
./&gt;. ./0. %&amp;&apos;( %&amp;&apos;) %&amp;&apos;* ./23 %&amp;&apos;) %&amp;+, %&amp;&apos;- %&amp;.. %&amp;/* ./14 ./1; %&amp;&apos;* ./2.
%&amp;(( ./0. %&amp;&apos;( ./2: %&amp;&apos;* ./23 %&amp;&apos;) %&amp;+, %&amp;&apos; %&amp; %&amp;/* %&amp; / %&amp; ) ./2&gt; ./2.
./24 ./2: ./22 ./22 ./21 ./1; ./20 ./2: ./24 ./E; %&amp;/( %&amp;.( %&amp;-. ./20 ./1:
./E4 ./2E ./22 ./24 ./2 /10 ./2E ./21 ./1: ./E1 %&amp;/&apos; %&amp;,. %&amp;.( ./23 ./11
./&gt;E %&amp;+/ %&amp;&apos;( %&amp;&apos;) %&amp;&apos;* %&amp;&apos;- %&amp;&apos;) ./0E %&amp;&apos;- %&amp;.. %&amp;/* %&amp;-. %&amp;&apos;% %&amp;&apos;* %&amp;&apos;/
./1E ./20 ./23 ./2E ./24 ./10 ./23 ./21 ./1; ./E2 ./4&gt; ./3E ./1 ./10
./E&gt; ./22 ./2E ./1; ./1; ./11 ./2. ./24 ./10 ./E3 ./41 ./E&gt; ./3&gt; /22 ./11
./23
./24 ./20 ./22 ./22 ./21 ./2 /20 ./2: ./24 ./E: ./4&gt; ./3E ./11 ./22 ./1&gt;
./02 ./0. %&amp;&apos;( ./2: %&amp;&apos;* ./23 ./2: ./04 ./23 ./3E %&amp;/* ./1. ./1: %&amp;&apos;* ./2.
./2; ./0. %&amp;&apos;( ./2&gt; ./20 ./2E ./2: ./0 /23 ./34 %&amp;/* ./3: ./1&gt; ./2&gt; ./1;
./23 ./2; ./20 ./22 ./22 ./24 ./20 ./2; ./2E ./3. ./4&gt; ./30 ./12 ./20 ./1:
./02 ./2; ./20 ./2&gt; ./20 ./2E ./2&gt; ./0 /23 ./34 ./4&gt; ./3: ./1&gt; ./2&gt; ./1:
./01 ./0. %&amp;&apos;( ./2: ./2&gt; ./23 ./2: ./04 ./23 %&amp;.. %&amp;/* ./14 ./1; ./2&gt; ./1;
/&gt;4 ./0. %&amp;&apos;( ./2: ./2&gt; ./23 %&amp;&apos;) ./04 %&amp;&apos; /3E %&amp;/* ./14 ./1; %&amp;&apos;* ./2.
./00 ./0. %&amp;&apos;( ./2&gt; ./20 ./2E ./2: ./04 ./23 ./3E %&amp;/* ./3; ./1: ./2&gt; ./1;
./2E ./2; ./20 ./22 ./22 ./2 /20 ./2: ./24 ./3 ./30 ./11 ./21 ./10
./&gt;4 ./0. %&amp;&apos;( ./2: %&amp;&apos;* ./23 ./2: ./04 %&amp;&apos;- ./3E /4&gt; ./1E ./1; ./2&gt; ./1;
%&amp;/*
./02 ./0 /20 ./2&gt; ./20 ./2E ./2: ./04 %&amp;&apos; /3E %&amp;/* ./1 /1: ./2&gt; ./1;
%&amp;(( ./0. %&amp;&apos;( ./2; %&amp;&apos;* %&amp;&apos;- %&amp;&apos;) %&amp;+, %&amp;&apos;- %&amp;.. %&amp;/* ./1E %&amp;&apos;% %&amp;&apos;* ./2.
./22 ./2; ./20 ./2&gt; ./20 ./2E ./2: ./0 /23 ./34 ./4&gt; ./3: ./10 %&amp;&apos;( ./1;
./02 ./0. ./20 ./2&gt; ./2&gt; ./23 ./2: ./04 %&amp;&apos;- ./3E %&amp;/* ./1. ./1; %&amp;&apos;* ./1;
./0: ./0. %&amp;&apos;( ./2: %&amp;&apos;* %&amp;&apos; %&amp;&apos;) %&amp;+, %&amp;&apos; %&amp; %&amp;/* ./1E %&amp;&apos;% %&amp;&apos;* %&amp;&apos;/
P #9N95 KUVW P #9N95 59+&amp;quot;## QbM S&amp;quot;)[ RVQUJ M RVQUJ QB&amp;quot;)D,57 RVSA ]S ]57,+ WU\? KVLR*V K[&amp;quot;5%
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
&apos;&apos;)*+,-&apos;, %&amp;&apos;* ./0. 2/:. ./24 %&amp;%. ./.E %&amp;.( ./2. %&amp;.- %&amp;+/
%&amp;+/
7+&amp;*+,-&apos;, %&amp;&apos;*
2/:&gt; ./2E %&amp;%. ./.3 %&amp;.( %&amp;&apos;/ %&amp;.- %&amp;+/
./0. 2/:. ./24 %&amp;%. ./.2 %&amp;.( ./2 /33 %&amp;+/
./0. 2/:4 %&amp;&apos;, %&amp;%. ./.3 %&amp;.( %&amp;&apos;/ ./31 %&amp;+/
+&amp;quot;-&apos;56789 ./2&gt;
+-&amp;*&lt;=*+,-&apos;, %&amp;&apos;*
+-&amp;*?@A*+,-&apos;, %&amp;&apos;* ./0. 2/:1 %&amp;&apos;, %&amp;%. ./.3 %&amp;.( %&amp;&apos;/ %&amp;.- %&amp;+/
+-&amp;*%B&amp;quot;BCD95 ./22 ./2&gt;
2/20 ./1: %&amp;%. ./.3 ./31 ./1; ./34 ./2:
+&amp;*F9-&amp;quot;) ./24 ./21 2/3. ./1. ./.E ./.E ./E&gt; ./1. ./E&gt; ./22
7G6 ./24 ./20 2/10 ./10 ./.E ./.1 ./34 ./12 ./E; ./22
89)9H&amp;quot; ./1; ./22 2/E; ./12 ./.E ./.2 ./E; ./13 ./E&gt; ./21
?&amp;6+,)8 ./22
./2: 2/0. ./1: %&amp;%. ./.3 ./33 ./1&gt; ./34 ./2&gt;
I?&amp;*+,-&apos;, ./2&gt; ./0. 2/&gt;&gt; ./24 %&amp;%. ./.4 ./30 ./2. ./33 ./0.
I?&amp; ./2&gt; ./2; 2/&gt;1 ./2. %&amp;%. ./.: ./32 ./2. ./3E ./0.
#68 ./22 ./2: 2/0E ./1: %&amp;%. ./.2 ./31 ./1; ./34 ./2:
</table>
<figure confidence="0.982748928571428">
#6-%6 ./20 ./2;
#6&amp;-*+,-&apos;, ./2&gt; ./0.
2/&gt;E ./24 %&amp;%. ./.2 ./30 ./2 /33 ./2;
2/&gt;&gt; ./24 %&amp;%. ./.3 %&amp;.( %&amp;&apos;/ ./33 ./0.
#6&amp;- %&amp;&apos;*
%&amp;+/
&apos;&amp;** %&amp;&apos;, %&amp;%.
./.0 %&amp;.( %&amp;&apos;/ %&amp;.- %&amp;+/
)5+ ./2&gt; ./0. 2/&gt;; ./2. %&amp;%. ./.1 ./30 ./2. ./33 ./0.
,)#6)9J ./22 ./2: 2/21 ./2. %&amp;%. ./.: ./33 ./1: ./34 ./2:
,)#6)9K %&amp;&apos;* ./0. 2/:. %&amp;&apos;, %&amp;%. %&amp;// %&amp;.( %&amp;&apos;/ %&amp;.- %&amp;+/
5&amp;quot;#6 ./2&gt; ./0. 2/:. %&amp;&apos;, %&amp;%. ./.0 ./30 ./2 /33 ./0.
5LB?*+,-&apos;, %&amp;&apos;* %&amp;+/ 2/:&gt; %&amp;&apos;, %&amp;%. ./.4 %&amp;.( %&amp;&apos;/ %&amp;.- %&amp;+/
5LB? ./20 ./2; 2/&gt;3 ./2. %&amp;%. ./.1 ./30 ./2. ./33 ./0.
&amp;976) ./2&gt; ./0. 2/:E ./24 %&amp;%. ./.1 ./30 ./2. ./33 %&amp;+/
&amp;AH*+,-&apos;, %&amp;&apos;* %&amp;+/ &apos;&amp;** %&amp;&apos;, %&amp;%. ./.3 %&amp;.( ./2. %&amp;.- %&amp;+/
KUVW 43&amp;quot; OPQR
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
./3. (&amp;*/
./3. &gt;/02
./3. &gt;/0;
./3. &gt;/0;
./E2 &gt;/40
./4; 0/ 4E
%&amp;./ &gt;/&gt;:
./4; 0/.4
./4&gt; 2/0&gt;
./E1 0/;;
./E; &gt;/04
./E: &gt;/11
./E0 &gt;/43
./E&gt; &gt;/34
./E; &gt;/&gt;.
./E; &gt;/2;
./E; &gt;/23
./E2 &gt;/E;
./E; &gt;/&gt;3
./E: &gt;/11
./3. &gt;/&gt;3
./E: &gt;/3;
./E: &gt;/11
./3. &gt;/01
</figure>
<page confidence="0.992185">
47
</page>
<table confidence="0.987008000000001">
,-./0123145678 66 M@)JC7% %&amp;&apos;8CN@A )* OPQR M@)JC7%MHRS .+3+T &gt;#&amp;% )K* &apos;&amp;quot;%@#C5@ 8@)JC7% !IUVW &amp;quot;5A OPQR$, Q7*J@% X*J Y&amp;quot;##Y !@5ZJ@ [MR3+ )@%)%@)$ &amp;quot;5A Y%&amp;&apos;Y !%&amp;&apos;%@) *X )&lt;@ &lt;&amp;8&amp;quot;5#= &amp;quot;%%@%%@A A&amp;quot;)&amp;quot;$ &amp;quot;J@ %&lt;*K5,
J&amp;quot;5E MR6O\] MR68O\] I&amp;quot;AF@J X&amp;## I&amp;quot;AF@J #C)@ HRV\ .,3 MVRVaS &amp;quot;Ac MVRVaS &lt;)@J MVRVaS J&amp;quot;5E QV`PH Q@8`aQ Q@8`aQ IUVW ]\W6U;^ US_I- US:I-
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
&amp;quot;&amp;quot;#)* +,-. +,/- +,/0 +,/+ +,-1
&apos;&apos;567*8&apos;* !&amp;quot;## !&amp;quot;$% +,// !&amp;quot;$$ !&amp;quot;$&amp;
78&amp;6:;67*8&apos;* +,90 +,/4 +,// !&amp;quot;$$ !&amp;quot;$&amp;
+,/4 +,// +,/ +,/0
78&amp;6&lt;=&gt;67*8&apos;* +,2/
+,/- +,/3 +,/+ +,/+
78&amp; +,-2
7&amp;6?@8&amp;quot;5 +,02 +,/3 +,/0 +, 9 +, 9
ABC +,/+ +,/- +,/. +,-1 +,-4
DE +,22 +,/4 +,/ +,/3 +,/3
&lt;&amp;C7*5F +,-9 +,/0 +,/3 +,-1 +,-4
G&lt;&amp;67*8&apos;* +,29 +,/4 +,// +,/ +,/0
G&lt;&amp; +,/0 +,// +,/. +,/. +,/3
EC) +,9. +,/4 +,/ +,/0 +,/.
E*767*8&apos;* +,/1 +,/2 +,/- +,/0 +,/.
E*7 +,.0 +, 1 +, 9 +, / +, /
#C8%C +,/. +,/9 +,/- +,/. +,/3
#C&amp; +,/1 +,/9 +,/ +,/3 +,/+
*5#C5@H +,20 +,/9 +,/- +,/. +,/3
*5#C5@I +,90 !&amp;quot;$% !&amp;quot;$* !&amp;quot;$$ !&amp;quot;$&amp;
JK)&lt;67*8&apos;* +,9/ +,/4 +,// +,/- !&amp;quot;$&amp;
JK)&lt; +,2. +,/4 +,// +,/0 +,/.
&amp;@AC5 +,22 +,/4 +,/- +,/0 +,/.
&amp;8A +,24 +,/4 +,// +,/0 +,/.
&amp;&gt;&gt;%&amp;quot;#&amp;quot; +,/3 +,// +,/0 +,/+ +,/+
&amp;&gt;L67*8&apos;* +,2 +,/4 +,// +,/ +,/0
&amp;&amp;68% +,// +,// +,/. +,/+ +,-1
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
+,-0 +,/+ +,/. +,-2 +,./ +,3/ +,./ +,0/ +,-4 +,02
+,/+ +,/2 +,/1 +,/. !&amp;quot;&apos;( +,39 +,02 +, 2 !&amp;quot;$&amp; !&amp;quot;&amp;)
+,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !&amp;quot;$&amp; !&amp;quot;&amp;)
+,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,0 +, +,/0 3
+,
+,-- +,/3 +,/0 +,-9 +,.2 +,32 +,0+ +,04 +,/3 +,04
+, +, 4 +,/+ +, / +,.0 +,3 +,.+ +,0. +,/+ +,02
+,-- +,/3 +,/. +,-9 +,.- +,32 +,.1 +,0/ +,/+ +,02
+, 1 +,// +,/9 +,/+ +,.9 +,32 +,03 +, +,/. +,01
+,-0 +,/+ +,/. +,-9 +,./ +,3/ +,./ +,02 +,/+ +,02
+,/+ +,// +,/4 +,/3 +,.1 +,32 +,0 +, +,/0 +, 3
+,-/ +,/3 +,/- +,-9 +,.9 +,32 +,.1 +,09 +,-1 +,09
+, 1 +,// +,/9 +,/3 +,.1 +,32 +,0 +, +,/0 +, +
+,-4 +,/0 +,/2 +,-1 +,.4 +,32 +,03 +,-3 +,/. +,-+
+,09 +, 0 +, / 3 +,.3 +,30 +,.3 +,.4 +, 1 +,0/
+,
+,-4 +,/- +,/2 +,/+ +,.4 +,32 +,0. +,-3 +,/. +,01
+, 9 +,/0 +,// +, 1 +,.9 +,32 +,0. +, 3 +,/. +,01
+,-4 +,/0 +,// +,-1 +,.9 +,32 +,03 +,-+ +,-1 +,02
!&amp;quot;$) +,/9 !&amp;quot;*! !&amp;quot;$&apos; !&amp;quot;&apos;( !&amp;quot;(+ !&amp;quot;&apos;# !&amp;quot;&amp;# !&amp;quot;$&amp; 3
+,
+,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !&amp;quot;$&amp; +,-3
+, 1 +,// +,/9 +,/3 +,.1 +,39 +,0 +, +,/0 +, 3
+,/+ +,// +,/9 +,/3 +,.1 +,32 +,00 +,-0 +,/0 +,01
+,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,00 +, +,/0 +, +
+,-/ +,/3 +,/- +,-4 +,.2 +,3/ +,.4 +,04 +,/3 +,04
+,/+ +,// +,/4 +,/3 +,.1 +,32 +,0 +, +,/0 +, 3
+,-/ +,/3 +,/0 +,-9 +,.2 +,3/ +,.1 +,09 +,/3 +,04
P #@N@J IUVW P #@N@J J@7&amp;quot;## QbM S&amp;quot;5E RVQUH M RVQUH Q)&amp;quot;5X*JA RVS&gt; ]S ]JA*7 WU\&lt; IVKR6V IE&amp;quot;J% IUVW 30&amp;quot; OPQR
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
&amp;quot;&amp;quot;#)* +,/+ +,/. /,+3 +,-3 +,+.
&apos;&apos;567*8&apos;* +,// +,/9 /,-2 +,-4 !&amp;quot;!&apos;
78&amp;6:;67*8&apos;* +,// +,/9 /,-9 +,-4 !&amp;quot;!&apos;
78&amp;6&lt;=&gt;67*8&apos;* +,/- +,/9 /,-2 +,-9 !&amp;quot;!&apos;
78&amp; +,/+ +,/0 /,+4 +,-. +,+.
7&amp;6?@8&amp;quot;5 +,-4 +,/3 -,14 +,09 +,+.
ABC +,-1 +,/0 /,3. +,-/ +,+.
DE +,/- +,/9 /,-0 +,-9 !&amp;quot;!&apos;
&lt;&amp;C7*5F +,/+ +,/0 /,33 +,-. +,+.
G&lt;&amp;67*8&apos;* +,/- +,/9 /,-0 +,-9 !&amp;quot;!&apos;
G&lt;&amp; +,/3 +,/0 /,3. +,-. +,+.
EC) +,/- +,/4 /,/3 +,-4 !&amp;quot;!&apos;
E*767*8&apos;* +,/. +,// /,03 +,-/ +,+.
E*7 +,-0 +,-4 -,9. +,02 +,+.
#C8%C +,/0 +,/2 /,0/ +,-9 +,+.
#C&amp; +,/0 +,/2 /,0- +,-2 +,+.
*5#C5@H +,/0 +,// /,.- +,-2 +,+.
*5#C5@I !&amp;quot;$* !&amp;quot;*! $&amp;quot;#( !&amp;quot;$) !&amp;quot;!&apos;
JK)&lt;67*8&apos;* +,// +,/4 /,/+ +,-4 !&amp;quot;!&apos;
JK)&lt; +,/- +,/9 /,-- +,-9 !&amp;quot;!&apos;
&amp;@AC5 +,/- +,/4 /,/0 +,-1 !&amp;quot;!&apos;
&amp;8A +,// +,/4 /,/+ +,-4 !&amp;quot;!&apos;
&amp;&gt;&gt;%&amp;quot;#&amp;quot; +,/3 +,/- /,.3 +,-0 +,+.
&amp;&gt;L67*8&apos;* +,/- +,/9 /,-/ +,-4 !&amp;quot;!&apos;
&amp;&amp;68% +,/+ +,/- /,34 +,-- +,+.
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
+,+3 +,.1 +,-. +,./
+,+- +,0/ +, 9 +,0+
+,/0 +,31 2,04
+,/4 !&amp;quot;)* #&amp;quot;)+
+,+0 +,0- +,-2 +,0+ +,/4 !&amp;quot;)* 9,31
+,+- +,00 +, 2 +,.1 +,/4 +,./ 9,+1
+,+0 +,0+ +,-- +,.2 +,/. +,.3 2,-9
+,+. +,.2 +,01 +,.0 +,/3 +,32 /,93
+,+2 +,03 +,-/ +,.4 +,/0 +,39 /,1/
+,+. +,00 +, 2 +,.4 +,/4 +, 2,29
+,+. +,0+ +,-0 +,.2 +,/. +,31 2,30
+,+. +,0 +, 2 +,.1 +,/4 +,./ 9,30
+,+/ +,03 +,-- +,.9 +,/- +,.3 2,90
+,+/ +,0 +, 9 +,0+ +,/4 +, 2,13
+,+. +,00 +,-/ +,.4 +,/2 +,.- 2,44
+,+3 +,.9 +, + +, +, 9 +,32 /,01
+,+- +,0. +,-2 +,.4 +,/9 +,.0 2,90
+,+0 +,0. +, 2 +,.4 +,/2 +, 2,2+
+,3+ +,00 +,-2 +,.1 +,/2 +,.3 2,44
!&amp;quot;(# !&amp;quot;&apos;* !&amp;quot;&amp;% !&amp;quot;&apos;) !&amp;quot;*( !&amp;quot;)* 9,34
+,+. +,0/ +,-9 +,0+ +,/1 !&amp;quot;)* 9,.+
+,+0 +,00 +, 2 +,.1 +,/4 +, 2,12
+,+2 +,0- +,-4 +,0+ +,/4 +,.0 2,4+
+,+9 +,00 +, 9 +,.1 +,/4 +,.0 2,99
+,+. +,03 +,-- +,.9 +,/- +,.3 2,/3
+,+0 +,0 +, / +,.1 +,/4 +,./ 9,3/
+,+. +,03 +,-/ +,.9 +,/- +,.+ 2,0.
8&amp;quot;)Z Sp.nish-En5lish ** H&lt;I89+% %&amp;&apos;-9J&lt;: I, KLMN H&lt;I89+%HDNO 1.2.P F#&amp;% IQ, &apos;&amp;quot;%&lt;#9)&lt; -&lt;I89+% !ERST &amp;quot;): KLMN$/ M+,8&lt;% U,8 V&amp;quot;##V !&lt;)W8&lt; XHN2. I&lt;%I%&lt;I$ &amp;quot;): V%&amp;&apos;V !%&amp;&apos;%&lt;I ,U IB&lt; B&amp;-&amp;quot;)#Y &amp;quot;%%&lt;%%&lt;: :&amp;quot;I&amp;quot;$ &amp;quot;8&lt; %B,Q)/ RO^E6 RO&gt;E6
!&amp;quot;##$ !%&amp;&apos;$ HN*K[\ HN*-K[\ E&amp;quot;:;&lt;8 U&amp;## E&amp;quot;:;&lt;8 #9I&lt; DNS[ 1/2 HSNS`O &amp;quot;:b HSNS`O BI&lt;8 HSNS`O 8&amp;quot;)Z MS_LD M&lt;-_`M M&lt;-_`M ERST \[T*R?] !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
&apos;&apos;)*+,-&apos;, ./01 !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ ./35 0.�v
./02 ./34 ./35 ./35 ./36 ./0. ./07 ./33 ./76 ./24 ./67 ./37 ./35 0.�v
./0 /34
./34
./3=
./37
./35
./01
./33
./76
./24
./62
./31
+&amp;quot; &apos;89:;&lt; ./02 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./62 ./31 ./35 0.�v
+-&amp;*&gt;?*+,-&apos;, ./00 ./34 ./30 ./33 ./36 ./3 /30 ./35 ./31 ./7 /2= ./73 ./6= ./3= ./64
+,#&amp; &apos;9&amp;quot; ./63
+&amp;*@&lt;-&amp;quot;) ./1= ./33 ./34 ./31 ./31 ./64 ./36 ./30 ./3. ./1= ./23 ./13 ./62 ./33 ./63
:A9 ./63 ./34 ./33 ./36 ./37 ./65 ./30 ./34 ./32 ./14 ./24 ./73 ./60 ./30 ./6=
B&amp;9+,); ./3. ./3= ./30 ./30 ./33 ./32 ./34 ./02 ./37 ./72 ./2= ./73 ./6= ./3= ./64
./0 /34 ./34 ./34 ./37 ./35 ./01 ./36 ./77 ./24 ./6 /32 ./34 ./3.
CB&amp;*+, &apos;, ./33 ./0. ./3= ./3= ./3= ./37 ./35 ./01 ./36 ./77 ./24 ./75 ./3. ./34 ./65
CB&amp; ./02
,)#9)&amp;quot;D ./36 ./0 /3= ./3= ./3= ./37 ./34 ./02 ./36 ./71 ./2= ./75 ./64 ./33 ./60
,)#9)&lt;E 0.70 0.63 0.�9 0.62 0.6v 0.�6 0.62 0.6� 0.�7 0.36 0.20 0.�7 0.�� ./35 ./3.
&amp;&lt;:9) ./05 ./02 ./3= ./34 ./34 ./36 ./0 /07 ./33 ./77 ./24 ./61 ./31 ./35 0.�v
&amp;F+ ./32 ./35 ./30 ./33 ./33 ./32 ./34 ./0. ./37 ./72 ./2= ./7= ./64 ./34 ./65
&amp;FG*+, &apos;, ./00 ./02 ./34 ./35 ./34 ./36 ./0 /07 ./33 ./76 ./24 ./61 ./37 0.60 0.�v
B&amp;9+,); ./3=
./35 3/=7 ./65 0.03
,)#9)&lt;E 0.6v
0.63 6.03 0.�6 0.03
NSOF \O \8:,+ TR[B ESQN*S EZ&amp;quot;8%
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
./.6 ./74 ./3. ./76 ./01
</table>
<figure confidence="0.989024827586207">
./7= ./3 /77 ./02
./.=
./7= ./3. ./76 ./01
./.6
./.6
./76 ./60 ./7. ./3=
./14 ./62 ./1= ./30
./76 ./6= ./72 ./34
./.7
./.4
./76 ./64 ./72 ./35
./7= ./3 /77 ./02
./.3
./.2
./70 ./65 ./77 ./02
./.4
./22
./70 ./3 /71 ./02
0.�0 0.�3 0.36 0.6�
./74 ./32 ./76 ./01
0.2�
./21
./73 ./64 ./71 ./0.
./.7
./.6 ./74 ./3 /76 ./01
L #&lt;J&lt;8 ERST L #&lt;J&lt;8 8&lt;+&amp;quot;## MaH O&amp;quot;)Z NSMRD H NSMRD MI&amp;quot;)U,8:
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
+,#&amp;-&apos;9&amp;quot; ./33 ./3= 3/37 ./6= ./.1
+&amp;*@&lt;-&amp;quot;) ./36 ./30 3/60 ./61 ./.1
:A9 ./36 ./34 3/07 ./3. ./.1
CB&amp;*+,-&apos;, ./34 ./0. 3/47 ./32 0.03
CB&amp; ./34 ./0. 3/47 ./32 0.03
,)#9)&amp;quot;D ./3= ./35 3/=. ./31 0.03
&amp;&lt;:9) ./35 ./01 3/5= ./36 0.03
&amp;F+ ./30 ./0. 3/=3 ./3. 0.03
ERST 27&amp;quot; KLMN
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
./71 =/56
./7. =/42
./15 =/=1
./72 =/==
./72 =/4=
./1= =/25
./11 0/03
./17 0/0.
./10 =/7=
./14 =/==
0.33 �.39
./7. =/04
./10 =/13
&amp;FG*+,-&apos;, ./35 ./01 3/5= ./37 0.03
./72 =/41
&apos;&apos;)*+,-&apos;, ./35
+&amp;quot;-&apos;89:;&lt; ./34
+-&amp;*&gt;?*+,-&apos;, ./35
./02 3/51 ./31 0.03
./0. 3/43 ./32 0.03
./02 3/5. ./31 0.03
</figure>
<page confidence="0.523765">
48
</page>
<figure confidence="0.991452882352941">
)*&amp;+,-+).*&apos;.
)&amp;+&apos;.9&amp;quot;:
)&amp;+;&lt;);.
)&amp;+&gt;&lt;*&amp;quot;?
@)&amp;+).*&apos;.
@)&amp;
&lt;&amp;:.;:&amp;quot;?%
A.)+).*&apos;.
A.)
.?#B?&lt;C
.?#B?&lt;D
E)+;:&amp;quot;?%
E.;%@&amp;quot;*
:F;G+).*&apos;.
%H&amp;
&amp;&lt;@B?
&amp;EI+).*&apos;.
</figure>
<table confidence="0.965507090909091">
:&amp;quot;?A -./0123,45673 ++ R&lt;;:B)% %&amp;&apos;*B;&lt;@ ;. WOLM R&lt;;:B)%RCMK =/8/X E#&amp;% ;F. &apos;&amp;quot;%&lt;#B?&lt; *&lt;;:B)% !DSNY &amp;quot;?@ WOLM$0 L).:&lt;% H.: Z&amp;quot;##Z !&lt;?[:&lt; \RM8/ ;&lt;%;%&lt;;$ &amp;quot;?@ Z%&amp;&apos;Z !%&amp;&apos;%&lt;; .H ;G&lt; G&amp;*&amp;quot;?#] &amp;quot;%%&lt;%%&lt;@ @&amp;quot;;&amp;quot;$ &amp;quot;:&lt; %G.F?0 SK^D5 SK,D5
RM+W_J RM+*W_J D&amp;quot;@`&lt;: H&amp;## D&amp;quot;@`&lt;: #B;&lt; CMN_ =08 RNMNTK &amp;quot;@U RNMNTK G;&lt;: RNMNTK :&amp;quot;?A LNVOC L&lt;*VTL L&lt;*VTL DSNY J_Y+S-a
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
/012 /034 /035 /03/ /056 /076 !&amp;quot;#$ /076 /052 /038 /055
/011 /036 /035 /052 /051 /071 /057 /071 /056 /03/ /057
/01/ /036 /037 /051 /057 /075 /05/ /077 /05= /054 /058
/03/ /033 /03= /051 /057 /078 /072 /075 /05= /051 /072
!&amp;quot;%$ !&amp;quot;$&amp; !&amp;quot;$$ !&amp;quot;$&apos; !&amp;quot;#( !&amp;quot;)&amp; !&amp;quot;#$ !&amp;quot;)( !&amp;quot;$&apos; !&amp;quot;$* !&amp;quot;#+
/074 /035 /038 /051 /057 /07= /072 /077 /058 /052 /057
/035 /035 /038 /057 /058 /0=2 /076 /078 /074 /054 /05=
/011 /036 /035 /052 /051 /076 /057 /076 /056 /038 /053
/077 /03= /052 /055 /058 /0=2 /071 /077 /072 /052 /05=
/055 /033 /038 /053 /058 /078 /074 /077 /058 /054 /05=
/06/ /034 /035 /052 /051 /076 /055 /076 /054 /038 /053
/01= /033 /038 /055 /058 /078 /076 /0=2 /071 /052 /05=
/055 /033 /038 /053 /05= /078 /074 /075 /05= /052 /05=
/06/ /036 /037 /03/ /056 /074 /055 /076 /052 /038 /053
/053 /033 /03= /055 /058 /078 /074 /077 /058 /052 /057
/01= /036 /037 /054 /053 /071 /05= /076 /056 /03/ /055
/014 /034 /033 /03/ /056 /074 !&amp;quot;#$ /076 /052 /038 /053
O #&lt;P&lt;: DSNY O #&lt;P&lt;: :&lt;)&amp;quot;## LQR K&amp;quot;?A MNLSC R MNLSC L;&amp;quot;?H.:@ MNKE JK J:@.) YS_G DNFM+N DA&amp;quot;:%
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
)*&amp;+,-+).*&apos;. !&amp;quot;#+ /056 504/ !&amp;quot;$$ +/0/= /0/3 /037
)&amp;+&apos;.9&amp;quot;: /053 /051 506/ /037 ,!&amp;quot;!&apos; /0/1 /038
)&amp;+;&lt;);. /057 /053 501= /037 ,!&amp;quot;!&apos; /0/2 /03/
)&amp;+&gt;&lt;*&amp;quot;? /058 /05= 5071 /03/ ,!&amp;quot;!&apos; /0/3 /056
@)&amp;+).*&apos;. /056 !&amp;quot;#( #&amp;quot;&amp;! !&amp;quot;$$ ,!&amp;quot;!&apos; !&amp;quot;&apos;# !&amp;quot;$#
@)&amp; /05/
/058 50=3 /056 ,!&amp;quot;!&apos; /0/3 /051
&lt;&amp;:.;:&amp;quot;?% /05/ /05= 505= /038 ,!&amp;quot;!&apos; /0/5 /051
A.)+).*&apos;. /053 /051 506/ /037 ,!&amp;quot;!&apos; /0/= /038
A.) /074 /05/ 50=5 /057 ,!&amp;quot;!&apos; /0/7 /055
.?#B?&lt;C /058 /057 5054 /038 ,!&amp;quot;!&apos; /0/1 /056
.?#B?&lt;D !&amp;quot;#+ !&amp;quot;#( 5046 /035 ,!&amp;quot;!&apos; /08/ /037
E)+;:&amp;quot;?% /058 /057 503= /03/ ,!&amp;quot;!&apos; /0/3 /056
E.;%@&amp;quot;* /05/ /05= 5074 /052 ,!&amp;quot;!&apos; /0/7 /051
:F;G+).*&apos;. !&amp;quot;#+ /056 5048 !&amp;quot;$$ ,!&amp;quot;!&apos; /0/5 /038
%H&amp; /058 /057 5055 /03/ ,!&amp;quot;!&apos; /0/5 /056
&amp;&lt;@B? /053 /051 5067 /035 ,!&amp;quot;!&apos; /088 /03=
&amp;EI+).*&apos;. !&amp;quot;#+ !&amp;quot;#( 5045 /035 ,!&amp;quot;!&apos; /0/1 /037
-./0123*456.73 99 K/I+,)% %&amp;&apos;*,L/- I&lt; MNOP K/I+,)%KFPQ ?0@0R J#&amp;% IH&lt; &apos;&amp;quot;%/#,&gt;/ */I+,)% !GSTU &amp;quot;&gt;- MNOP$1 O)&lt;+/% V&lt;+ W&amp;quot;##W !/&gt;X+/ YKP@0 I/%I%/I$ &amp;quot;&gt;- W%&amp;&apos;W !%&amp;&apos;%/I &lt;V ID/ D&amp;*&amp;quot;&gt;#Z &amp;quot;%%/%%/- -&amp;quot;I&amp;quot;$
+&amp;quot;&gt;E KP9M[\ KP9*M[\ G&amp;quot;-./+ V&amp;## G&amp;quot;-./+ #,I/ FPT[ ?1@ KTPT`Q &amp;quot;-b KTPT`Q DI/+ KTPT`Q +&amp;quot;&gt;E OT_NF O/*_`O O/*_`O GSTU
DSNY I87&amp;quot; WOLM
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
</table>
<figure confidence="0.997229346153846">
/086 301=
/081 3056
/087 30=3
/08= 5021
!&amp;quot;&apos;&amp; $&amp;quot;&amp;+
/087 5023
/08/ 5055
/086 3034
/08= 5037
/08= 5013
/086 3031
/08/ 5031
/08= 5065
/084 304=
/088 5014
/081 3057
/084 3044
&amp;quot;+/ %D&lt;H&gt;1
\[U9S;] SQ^G6 SQ:G6
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
)&amp;quot;*&apos;+,-./ 0123 0145 0146 !&amp;quot;#$ 0144 0164 0167
)*&amp;9:;9)&lt;*&apos;&lt; 0156 0148 !&amp;quot;## !&amp;quot;#$ !&amp;quot;#% !&amp;quot;&amp;$ !&amp;quot;#&apos;
)&amp;9=/*&amp;quot;&gt; 01?2 014@ 0143 0165 0162 0138 0163
A, 0160 0146 014@ 0140 0167 0137 0166
/&amp; 013? 0143 014@ 0168 0165 0138 0166
./&gt;/B&amp;quot; 0134 0144 014@ 0167 0168 0135 016?
CD&amp; 0143 0145 0143 0146 0143 0166 0167
E&lt;)9)&lt;*&apos;&lt; 0128 !&amp;quot;#) !&amp;quot;## !&amp;quot;#$ !&amp;quot;#% !&amp;quot;&amp;$ !&amp;quot;#&apos;
E&lt;) 01?2 0143 014@ 0167 0168 0135 0163
#,*%, 0122 0148 0146 0144 0146 0164 0167
#,&amp;* 0123 0148 0146 0142 0144 0162 !&amp;quot;#&apos;
&gt;+) 012? 0145 0146 0146 0143 0164 0167
&lt;&gt;#,&gt;/F 0144 0145 0143 0146 0143 0166 0168
&lt;&gt;#,&gt;/G 0128 0148 0146 !&amp;quot;#$ 0144 0162 !&amp;quot;#&apos;
+&amp;quot;#, 0122 0148 0146 0142 0144 0162 0140
+HID9)&lt;*&apos;&lt; !&amp;quot;$# !&amp;quot;#) !&amp;quot;## !&amp;quot;#$ !&amp;quot;#% !&amp;quot;&amp;$ !&amp;quot;#&apos;
+HID 0123 0148 0146 0144 0146 0164 0140
&amp;/ ,&gt; 0150 0148 0146 0142 0144 0164 0140
&amp;JB9)&lt;*&apos;&lt; 0122 !&amp;quot;#) !&amp;quot;## !&amp;quot;#$ !&amp;quot;#% !&amp;quot;&amp;$ !&amp;quot;#&apos;
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
0148 0140
!&amp;quot;%! !&amp;quot;#(
0146 0163
0144 0162
0142 0162
0146 0166
0148 0168
!&amp;quot;%! !&amp;quot;#(
0144 0162
0147 0140
!&amp;quot;%! 014@
0148 0167
0148 0167
!&amp;quot;%! 014@
!&amp;quot;%! 014@
!&amp;quot;%! !&amp;quot;#(
0147 014@
!&amp;quot;%! 014@
!&amp;quot;%! !&amp;quot;#(
GSTU @3&amp;quot; MNOP
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
01?8 5165
!&amp;quot;+! 514?
01@2 4163
01?0 21@2
01@7 4176
01@8 2106
01?6 2124
01?7 5143
01?0 21@6
01?5 51?0
01?7 5138
01?5 51??
01?4 2177
01?8 513?
01?8 513?
!&amp;quot;+! 512?
01?8 51?8
01?8 51?6
!&amp;quot;+! $&amp;quot;%#
N #/L/+ GSTU N #/L/+ +/)&amp;quot;## OaK Q&amp;quot;&gt;E PTOSF K PTOSF OI&amp;quot;&gt;V&lt;+- PTQJ \Q \+-&lt;) US[D GTHP9T GE&amp;quot;+%
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
)&amp;quot;*&apos;+,-./ 0144 0145 4143 014? 9010? 0102 0148
)&amp;9=/*&amp;quot;&gt; 0140 014? 4103 0160 90103 0103 014?
0146 41?7 0162 9010? 0102 0146
/&amp; 0140 0146 41?@ 0162 9010? 0103 0143
./&gt;/B&amp;quot; 0140 014? 41@0 0168 90103 0104 014?
CD&amp; 0144 0145 414@ 0140 9010? 0104 0145
E&lt;)9)&lt;*&apos;&lt; !&amp;quot;#$ 0147 4152 0146 *!&amp;quot;!&apos; 0100 !&amp;quot;%!
E&lt;) 0167
014@ 4102 0164 9010? 010? 014?
#,*%, 0142 0148 4125 0146 9010? 0102 0148
#,&amp;* !&amp;quot;#$ 0147 4152 0146 *!&amp;quot;!&apos; 0105 0147
0148 412@ 014? *!&amp;quot;!&apos; 0102 0148
&lt;&gt;#,&gt;/F 0144 0145 4148 014? 9010? 0105 0148
&lt;&gt;#,&gt;/G !&amp;quot;#$ !&amp;quot;%! #&amp;quot;,&apos; 0146 9010? !&amp;quot;&apos;&amp; !&amp;quot;%!
+&amp;quot;#, !&amp;quot;#$ 0147 4153 0146 *!&amp;quot;!&apos; 0102 0147
+HID9)&lt;*&apos;&lt; !&amp;quot;#$ 0147 4157 !&amp;quot;## *!&amp;quot;!&apos; 0106 !&amp;quot;%!
+HID 0142 0148 4127 0143 *!&amp;quot;!&apos; 0102 0147
&amp;/-,&gt; 0142 0148 4150 0146 *!&amp;quot;!&apos; 0107 0147
&amp;JB9)&lt;*&apos;&lt; !&amp;quot;#$ 0147 4158 !&amp;quot;## *!&amp;quot;!&apos; 0104 !&amp;quot;%!
-A, 014@
&gt;+) 0142
)*&amp;9:;9)&lt;*&apos;&lt; !&amp;quot;#$ 0147 4156 0146 *!&amp;quot;!&apos; 0103 !&amp;quot;%!
</figure>
<page confidence="0.536817">
49
</page>
<figure confidence="0.960579345945946">
,-./012345678- ++ M9DG?)% %&amp;&apos;*?N9= D. OPQR M9DG?)%MERS 7/&lt;/T J#&amp;% DH. &apos;&amp;quot;%9#?:9 *9DG?)% !FUVW &amp;quot;:= OPQR$0 Q).G9% I.G X&amp;quot;##X !9:YG9 ZMR&lt;/ D9%D%9D$ &amp;quot;:= X%&amp;&apos;X !%&amp;&apos;%9D .I DC9 C&amp;*&amp;quot;:#[ &amp;quot;%%9%%9= =&amp;quot;D&amp;quot;$ &amp;quot;G9 %C.H:0
=&gt;? /031
/034 3042 /035 !&amp;quot;!#
ERV\ 70&lt; MVRVbS &amp;quot;=d MVRVbS CD9G MVRVbS G&amp;quot;:A QVaPE Q9*abQ Q9*abQ FUVW ]\W+U-_ US`F3 US,F3
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
/064 /074 !&amp;quot;#$ !&amp;quot;%&amp;
/06/ /077 /036 /06&lt;
/063 /073 /031 /066
/065 /075 /031 /063
/063 /071 /036 /06&lt;
/065 /075 /031 /066
/064 /075 /035 !&amp;quot;%&amp;
/06&lt; /077 /033 /067
/061 /071 /031 /066
/061 /071 /033 /066
/061 /071 /033 /067
!&amp;quot;%* !&amp;quot;&apos;) !&amp;quot;#$ !&amp;quot;%&amp;
/064 /074 /035 !&amp;quot;%&amp;
/061 /075 /031 /063
/072 /07&lt; /062 /07;
/065 /075 /035 /063
/061 /071 /031 /063
/064 /074 /031 /063
QD&amp;quot;:I.G= RVSJ ]S ]G=.) WU\C FVHR+V FA&amp;quot;G% FUVW &lt;6&amp;quot; OPQR
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
/0/1 /016 /0&lt;; 102;
/0/6 /034
/0&lt;7 10/6
!&amp;quot;+&apos; /01/
/0&lt;3 107;
/0&lt;5 10;1
/0/6 /017
/0/4 /01/
/0/5 /016
/0&lt;1 1042
/0&lt;4 102;
/0// /016
/0&lt;4 102;
/0&lt;3 10&lt;2
/0/7 /035
/0/1 /01&lt;
/0/3 /017
/0&lt;5 104/
/0&lt;5 10;1
/0/2 /017
/07/ !&amp;quot;&amp;&amp;
/0&lt;1 1051
!&amp;quot;+* (&amp;quot;+)
/0/6 /016
/0&lt;4 50/5
/0&lt;5 10;5
/0/1 /017
/0/6 /031
/0/; /016
/0&lt;&lt; 30;7
/0&lt;4 1023
/0/6 /01&lt;
/0&lt;5 1043
/0/6 /016 /0&lt;4 50&lt;/
G&amp;quot;:A MR+O\] MR+*O\] F&amp;quot;=^9G I&amp;## F&amp;quot;=^9G #?D9
)*&amp;+,-+).*&apos;. !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
)&amp;+89*&amp;quot;: /012 /011 /034 /034 /035
=&gt;? /07; /035 !&amp;quot;&amp;&apos; /037 /03&lt;
@A /057 /016 /035 /033 /033
/015 /011 /034 /031 /031
BC&amp; /035 /016 /034 /035 /031
A?D
A.)+).*&apos;. /05/ /011 /034 /034 /035
A.) /012 /011 /034 /034 /035
/06/ /01/ /031 /037 /037
#?*%? /01&lt; /013 /035 /031 /033
#?&amp;
.:#?:9E /011 /013 /034 /035 /031
.:#?:9F /012 /013 /035 /035 /031
!&amp;quot;$! !&amp;quot;&amp;( /03; !&amp;quot;#) !&amp;quot;#$
GHDC+).*&apos;. /051 /011 /034 /034 /035
GHDC /01&lt; /013 /034 /035 /031
%I&amp; /063 /01&lt; /031 /033 /036
&amp;9=?:
&amp;JJ%&amp;quot;#&amp;quot; /057 /011 /034 /035 /031
&amp;JK+).*&apos;. /034 /013 /034 /031 /033
/011 /011 /034 /034 !&amp;quot;#$
P #9N9G FUVW P #9N9G G9)&amp;quot;## QcM S&amp;quot;:A RVQUE M RVQUE
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
)*&amp;+C9&amp;quot;L9#=+).*&apos;. /034 /03; 30;5 /031 /0/6
@A /035 /034 3045 /031 /0/6
BC&amp; /033 /031 3017 /037 /0/6
A?D /034 /03; 30;6 /031 /0/6
A.)+).*&apos;. /035 /034 3044 /031 /0/6
A.) /03&lt;
/037 303/ /062 /0/6
#?*%? /031 /035 3055 /036 /0/6
#?&amp; /035 /03; 3045 /031 /0/6
.:#?:9E /035 /03; 30;7 /035 !&amp;quot;!#
.:#?:9F !&amp;quot;#* !&amp;quot;&amp;! &amp;&amp;quot;!&amp; !&amp;quot;#) !&amp;quot;!#
GHDC+).*&apos;. /035 /03; 3044 /031 /0/6
GHDC /031 /034 304&lt; /033 /0/6
%I&amp; /06; /062 6023 /06; /0/6
&amp;9=?: /034
/03; 30;1 /035 /0/6
&amp;JJ%&amp;quot;#&amp;quot; /031 /035 3051 /036 /0/6
&amp;JK+).*&apos;. /035 /034 3041 /033 /0/6
)&amp;+89*&amp;quot;: /03&lt; /037 3067 /066 /0/6
+,-./012345,/01 88 L/H+,)% %&amp;&apos;*,M/- H; NOPQ L/H+,)%LEQR 70@0S J#&amp;% HG; &apos;&amp;quot;%/#,&gt;/ */H+,)% !FTUV &amp;quot;&gt;- NOPQ$1 P);+/% I;+ W&amp;quot;##W !/&gt;X+/ YLQ@0 H/%H%/H$ &amp;quot;&gt;- W%&amp;&apos;W !%&amp;&apos;%/H ;I HC/ C&amp;*&amp;quot;&gt;#Z &amp;quot;%%/%%/- -&amp;quot;H&amp;quot;$ &amp;quot;+/ %C;G&gt;1
+&amp;quot;&gt;D LQ8N[\ LQ8*N[\ F&amp;quot;-./+ I&amp;## F&amp;quot;-./+ #,H/ EQU[ 71@ LUQU`R &amp;quot;-b LUQU`R CH/+ LUQU`R +&amp;quot;&gt;D PU_OE P/*_`P P/*_`P FTUV \[V8T:] TR^F3 TR9F3
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
)&amp;quot;*&apos;+,-./ 0123 0124 0125 0126 0122 0132 0172
)*&amp;89:8);*&apos;; 015&lt; 0150 !&amp;quot;#$ 0124 012&lt; 013&lt; !&amp;quot;%&amp;
)&amp;8=/*&amp;quot;&gt; 017? 0122 !&amp;quot;#$ 012@ 0120 013@ 0177
)&amp; 015@ 0150 !&amp;quot;#$ 012&lt; 0126 0136 0172
-A, 0122 0126 0123 0123 0127 0137 017?
BC&amp; 0122 0124 0125 0126 0122 0132 0172
D;)8);*&apos;; 0157 0150 !&amp;quot;#$ 012&lt; 0126 0136 !&amp;quot;%&amp;
D;) 01@4 0123 012? 012@ 0134 01?&lt; 017@
;&gt;#,&gt;/E 0154 0150 !&amp;quot;#$ 012&lt; 0126 013&lt; 0172
;&gt;#,&gt;/F !&amp;quot;$&apos; !&amp;quot;&amp;&apos; !&amp;quot;#$ !&amp;quot;&amp;! !&amp;quot;#( !&amp;quot;)( !&amp;quot;%&amp;
+GHC8);*&apos;; 0124 0124 0125 0124 012&lt; 013&lt; !&amp;quot;%&amp;
%I&amp; 0130 012&lt; 0122 0122 0123 0137 0173
&amp;/-,&gt; 015@ 0150 0125 012&lt; 0126 0136 0172
&amp;J&apos;8);*&apos;; 0150 0150 !&amp;quot;#$ 0124 012&lt; 013&lt; !&amp;quot;%&amp;
&amp;JK8&gt;&gt;#* 0123 0124 0125 0125 0122 0132 0173
&amp;JK 0122 0124 0125 0126 0125 0135 0172
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
0126 0120
012&lt; 0127
0127 013?
012&lt; 012@
0123 0136
0125 013&lt;
012&lt; 012@
012? 0135
012&lt; 012@
!&amp;quot;&amp;! !&amp;quot;#*
012&lt; 012@
012? 0132
012&lt; 012@
012&lt; 012@
0126 0134
0126 0120
O #/M/+ FTUV O #/M/+ +/)&amp;quot;## PaL R&amp;quot;&gt;D QUPTE L QUPTE PH&amp;quot;&gt;I;+- QURJ \R \+-;) VT[C FUGQ8U FD&amp;quot;+%
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
)&amp;quot;*&apos;+,-./ 0126 0124 2162 0132 !&amp;quot;!&apos; 0100 0150
)*&amp;89:8);*&apos;; 0124 015@ 2140 013&lt; 0100
!&amp;quot;!&apos; 0157
)&amp;8=/*&amp;quot;&gt; 0123 0125 213@ 0130 0100 0100 0125
-)&amp; 0124 015@ 2140 0136 0100
!&amp;quot;!&apos; 015@
-A, 0123 0126 2122 0137 0100 0100 0126
BC&amp; 0126 0124 2167 0132 0100
!&amp;quot;!&apos; 0150
D;)8);*&apos;; 012&lt; 0150 21&lt;3 0135 0100
!&amp;quot;!&apos; 015@
D;) 012@ 012? 21@4 01?5 0100 0100 012?
;&gt;#,&gt;/E 0124 0157 2146 013&lt; 0100
!&amp;quot;!&apos; 0157
;&gt;#,&gt;/F !&amp;quot;&amp;! 0100
!&amp;quot;&amp;* &amp;&amp;quot;&apos;% !&amp;quot;)( !&amp;quot;!&apos; 0153
+GHC8);*&apos;; 0124 015@ 21&lt;4 0136 0100
!&amp;quot;!&apos; 0150
%I&amp; 0123 0122 21?2 0137 0100 0100 0125
&amp;/-,&gt; 012&lt; 015@ 21&lt;5 0136 !&amp;quot;!&apos; !&amp;quot;!&apos; 015@
&amp;J&apos;8);*&apos;; 012&lt; 0150 21&lt;&lt; 0136 0100
!&amp;quot;!&apos; 015@
&amp;JK8&gt;&gt;#* 0126 0124 216@ 0133 0100
!&amp;quot;!&apos; 0124
&amp;JK 012&lt; 0150 21&lt;0 0132 0100
!&amp;quot;!&apos; 0150
FTUV @?&amp;quot; NOPQ
!&amp;quot;##$ !%&amp;&apos;$ !&amp;quot;##$ !%&amp;&apos;$
0174 6120
!&amp;quot;*% 6145
017@ 5123
01?0 6167
0173 51&lt;?
017&lt; 612&lt;
01?0 6164
017@ 51?6
01?0 6152
!&amp;quot;*% 61&lt;6
01?@ $&amp;quot;($
0173 61@&lt;
01?0 6155
01?@ 614?
017&lt; 61?4
0174 613&lt;
</figure>
<page confidence="0.957241">
50
</page>
<table confidence="0.9995068">
REF – .03‡ .02‡ .03‡ .01‡ .03‡ .02‡ .05‡ .02‡ .06‡ .03‡ .05‡ .03‡
AALTO .93‡ – .54‡ .54‡ .23‡ .36 .58‡ .56‡ .65‡ .69‡ .64‡ .67‡ .62‡
CMU .94‡ .30‡ – .47 .14‡ .22‡ .52‡ .41 .50‡ .57‡ .45† .44 .38
CU-BOJAR .94‡ .26‡ .38 – .10‡ .22‡ .61‡ .47† .46 .55‡ .42 .49‡ .44
CU-ZEMAN .98‡ .58‡ .73‡ .77‡ – .55‡ .79‡ .71‡ .84‡ .80‡ .77‡ .79‡ .75‡
ONLINEA .94‡ .41 .61‡ .57‡ .23‡ – .68‡ .63‡ .71‡ .71‡ .63‡ .54‡ .61‡
ONLINEB .93‡ .30‡ .31‡ .26‡ .10‡ .17‡ – .32† .35 .31 .22‡ .29* .38
UEDIN .91‡ .27‡ .35 .34† .11‡ .18‡ .47† – .54‡ .50‡ .35 .29 .35
BBN-C .95‡ .21‡ .22‡ .36 .06‡ .17‡ .38 .26‡ – .32 .24‡ .31* .26‡
CMU-HEA-C .90‡ .17‡ .19‡ .23‡ .09‡ .18‡ .32 .27‡ .34 – .31† .31* .30‡
JHU-C .93‡ .19‡ .30† .35 .09‡ .24‡ .50‡ .34 .47‡ .45† – .41‡ .36
RWTH-C .91‡ .16‡ .35 .29‡ .12‡ .27‡ .41* .37 .42* .42* .23‡ – .24†
UPy-C .94‡ .24‡ .40 .36 .09‡ .28‡ .39 .32 .46‡ .47‡ .33 .36† ?
&gt; others .93 .26 .37 .38 .11 .24 .47 .40 .49 .49 .38 .41 .40
&gt;=others .97 .42 .56 .55 .25 .39 .67 .62 .70 .70 .61 .65 .62
</table>
<tableCaption confidence="0.997553">
Table 21: Sentence-level ranking for the WMT10 Czech-English News Task (Combining expert and
</tableCaption>
<figure confidence="0.981622857142857">
non-expert Mechanical Turk judgments)
REF
AALTO
CMU
CU-BOJAR
CU-ZEMAN
ONLINEA
ONLINEB
UEDIN
BBN-C
CMU-HEA-C
JHU-C
RWTH-C
UPV-C
</figure>
<page confidence="0.989992">
51
</page>
<table confidence="0.999845892857143">
REF – .00‡ .02‡ .00‡ .07‡ .04‡ .03‡ .00‡ .06‡ .04‡ .00‡ .02‡ .07‡ .07‡ .07‡ .02‡ .09‡ .03‡ .03‡ .10‡ .04‡ .04‡ .03‡ .02‡ .07‡ .06‡
AALTO 1.00‡ – .43 .39 .48 .60‡ .38 .41 .74‡ .18‡ .42 .57‡ .50† .63‡ .55‡ .68‡ .79‡ .42 .33 .71‡ .61‡ .66‡ .54 .51‡ .66‡ .56‡
CMU .95‡ .34 – .19‡ .45 .52† .38 .50 .63‡ .17‡ .51‡ .55‡ .56† .66‡ .55‡ .60‡ .56‡ .30 .40 .62‡ .64‡ .49‡ .58‡ .46 .64‡ .46†
CU-ZEMAN 1.00‡ .44 .64‡ – .43 .72‡ .31 .45† .69‡ .36 .55 .62‡ .75‡ .75‡ .78‡ .75‡ .75‡ .48* .56† .79‡ .82‡ .72‡ .68‡ .63‡ .67‡ .84‡
DFKI .92‡ .29 .33 .35 – .37 .40 .34 .59 .08‡ .42 .50 .49 .64‡ .35 .44 .44 .50 .41 .70‡ .61† .57 .46 .47 .62‡ .44
FBK .93‡ .26‡ .23† .17‡ .49 – .12‡ .30 .52† .08‡ .20‡ .45* .41 .62‡ .44 .44 .48* .18‡ .25† .53‡ .47 .38 .38 .22† .41 .51*
HUICONG .92‡ .34 .39 .37 .38 .71‡ – .53† .67‡ .18‡ .51† .47 .60‡ .65‡ .49* .55‡ .78‡ .35 .41 .56‡ .77‡ .74‡ .58‡ .41 .65‡ .57‡
JHU .92‡ .35 .30 .17† .52 .45 .25† – .58‡ .16‡ .43 .38 .57† .60‡ .54‡ .60‡ .70‡ .29 .25 .65‡ .75‡ .56‡ .62‡ .49* .66‡ .48†
KIT .90‡ .14‡ .16‡ .14‡ .35 .28† .19‡ .16‡ – .03‡ .29* .20‡ .35 .53* .21‡ .24† .30 .20‡ .22‡ .44 .29 .38 .35 .24 .40 .24†
KOC .95‡ .66‡ .71‡ .51 .75‡ .80‡ .58‡ .68‡ .93‡ – .75‡ .87‡ .72‡ .74‡ .74‡ .81‡ .81‡ .78‡ .66‡ .89‡ .85‡ .80‡ .80‡ .72‡ .91‡ .73‡
LIMSI .99‡ .26 .24‡ .32 .45 .61‡ .25† .38 .50* .10‡ – .50* .55* .69‡ .52* .57‡ .57‡ .29† .22‡ .60‡ .52† .42 .47† .37 .60‡ .56‡
LIU .87‡ .17‡ .20‡ .14‡ .34 .22* .31 .38 .66‡ .04‡ .27* – .51* .53† .52* .53* .51 .20‡ .33 .64‡ .59‡ .48† .48 .51 .37 .53*
ONLINEA .90‡ .25† .29† .18‡ .34 .43 .23‡ .28† .49 .08‡ .32* .30* – .44 .38 .40 .42 .32† .35* .39 .47 .51 .27‡ .35 .43 .40
ONLINEB .76‡ .22‡ .24‡ .14‡ .27‡ .27‡ .25‡ .25‡ .32* .22‡ .21‡ .28† .32 – .27† .21‡ .30† .23‡ .15‡ .41 .31 .40 .23‡ .16‡ .42 .29
RWTH .89‡ .22‡ .23‡ .13‡ .49 .35 .29* .21‡ .62‡ .15‡ .32* .29* .46 .57† – .39 .49 .25 .38 .41 .27 .34 .36 .27 .48* .22‡
UEDIN .91‡ .15‡ .20‡ .12‡ .49 .35 .24‡ .22‡ .49† .04‡ .22‡ .30* .46 .62‡ .43 – .39 .11‡ .15‡ .45 .33 .40 .45 .33 .34 .33
UMD .91‡ .12‡ .23‡ .06‡ .35 .29* .11‡ .16‡ .47 .14‡ .23‡ .35 .40 .55† .36 .47 – .16‡ .17‡ .44 .29† .27 .37 .26 .27 .24†
UPPSALA .94‡ .30 .41 .23* .35 .53‡ .26 .37 .66‡ .03‡ .54† .71‡ .57† .65‡ .45 .72‡ .67‡ – .25 .59‡ .69‡ .49‡ .63‡ .33 .60‡ .64‡
UU-MS .83‡ .28 .42 .24† .41 .49† .28 .42 .68‡ .10‡ .55‡ .48 .55* .63‡ .49 .56‡ .60‡ .32 – .52† .58‡ .61‡ .64‡ .46‡ .64‡ .50*
BBN-C .90‡ .15‡ .16‡ .10‡ .22‡ .17‡ .22‡ .18‡ .41 .06‡ .16‡ .21‡ .35 .45 .30 .26 .34 .13‡ .20† – .42† .14† .27 .11‡ .25 .21†
CMU-HEA-C .83‡ .20‡ .18‡ .07‡ .29† .32 .06‡ .10‡ .49 .05‡ .26† .21‡ .41 .33 .37 .43 .58† .10‡ .14‡ .18† – .33 .32 .11‡ .34 .24*
CMU-HYPO-C .96‡ .24‡ .20‡ .07‡ .37 .33 .12‡ .21‡ .40 .10‡ .41 .26† .40 .54 .25 .37 .44 .13‡ .17‡ .49† .31 – .34 .23* .51† .45
JHU-C .97‡ .33 .22‡ .18‡ .31 .30 .27‡ .18‡ .33 .12‡ .19† .33 .59‡ .60‡ .39 .32 .30 .19‡ .20‡ .44 .29 .34 – .21* .36 .23
KOC-C .93‡ .11‡ .31 .17‡ .41 .50† .25 .27* .44 .11‡ .42 .36 .47 .68‡ .43 .41 .40 .33 .18‡ .59‡ .57‡ .46* .47* – .52† .43
RWTH-C .87‡ .20‡ .10‡ .21‡ .25‡ .27 .15‡ .23‡ .24 .02‡ .20‡ .30 .34 .47 .27* .34 .36 .14‡ .20‡ .33 .26 .21† .24 .20† – .17‡
UPV-C .93‡ .14‡ .20† .10‡ .42 .29* .25‡ .25† .57† .20‡ .22‡ .33* .39 .45 .47‡ .40 .50† .24‡ .28* .44† .42* .27 .34 .28 .56‡ ?
&gt; others .92 .25 .28 .18 .39 .41 .25 .30 .52 .12 .34 .39 .47 .57 .42 .46 .51 .27 .28 .52 .49 .45 .44 .34 .50 .42
&gt;= others .96 .46 .49 .35 .53 .62 .45 .51 .71 .24 .54 .58 .63 .72 .63 .66 .70 .50 .51 .75 .73 .68 .67 .59 .74 .64
</table>
<tableCaption confidence="0.908735">
Table 22: Sentence-level ranking for the WMT10 German-English News Task (Combining expert and
</tableCaption>
<table confidence="0.991349666666667">
non-expert Mechanical Turk judgments)
REF – .05‡ .01‡ .02‡ .03‡ .03‡ .01‡ .02‡ .04‡ .03‡ .04‡ .03‡ .07‡ .05‡ .04‡
CAMBRIDGE .90‡ – .24‡ .11‡ .35† .26‡ .43 .35 .50† .45† .33* .40 .46 .28* .41
COLUMBIA .97‡ .61‡ – .25‡ .47 .44 .61‡ .53‡ .62‡ .59‡ .48† .59‡ .57‡ .45† .57‡
CU-ZEMAN .92‡ .73‡ .59‡ – .62‡ .66‡ .71‡ .65‡ .75‡ .79‡ .58‡ .75‡ .78‡ .71‡ .72‡
DFKI .95‡ .50† .41 .21‡ – .46 .56‡ .52‡ .65‡ .62‡ .47 .52‡ .56‡ .52† .60‡
HUICONG .93‡ .57‡ .34 .21‡ .36 – .47† .43 .67‡ .58‡ .40 .51‡ .62‡ .46† .52‡
JHU .94‡ .39 .22‡ .16‡ .30‡ .32† – .41 .52‡ .47‡ .37 .41 .33† .28 .35
ONLINEA .92‡ .45 .35‡ .24‡ .34‡ .41 .41 – .60‡ .58‡ .38 .55‡ .46 .36 .57‡
ONLINEB .87‡ .34† .24‡ .15‡ .21‡ .19‡ .33‡ .25‡ – .34† .26‡ .34† .37* .24‡ .40
UEDIN .94‡ .33† .26‡ .12‡ .24‡ .22‡ .25‡ .25‡ .50† – .25‡ .28† .32* .25‡ .26
UPC .89‡ .45* .36† .23‡ .39 .37 .42 .48 .62‡ .57‡ – .54‡ .51‡ .50‡ .53‡
BBN-C .91‡ .33 .25‡ .11‡ .32‡ .30‡ .34 .31‡ .51† .41† .30‡ – .36 .26‡ .31
CMU-HEA-C .89‡ .37 .20‡ .10‡ .29‡ .23‡ .23† .35 .50* .44* .31‡ .34 – .23‡ .31
JHU-C .89‡ .39* .31† .17‡ .37† .33† .38 .42 .63‡ .47‡ .31‡ .42‡ .42‡ – .37†
UPV-C .91‡ .35 .30‡ .16‡ .29‡ .26‡ .32 .28‡ .44 .35 .27‡ .27 .30 .24† ?
&gt; others .92 .42 .29 .16 .33 .32 .39 .37 .54 .48 .34 .42 .44 .35 .43
&gt;=others .97 .62 .45 .29 .46 .50 .61 .52 .68 .68 .51 .64 .65 .58 .66
</table>
<tableCaption confidence="0.741973">
Table 23: Sentence-level ranking for the WMT10 Spanish-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
</tableCaption>
<figure confidence="0.99974756097561">
REF
AALTO
CMU
CU-ZEMAN
DFKI
FBK
HUICONG
JHU
KIT
KOC
LIMSI
LIU
ONLINEA
ONLINEB
RWTH
UEDIN
UMD
UPPSALA
UU-MS
BBN-C
CMU-HEA-C
CMU-HYPO-C
JHU-C
KOC-C
RWTH-C
UPV-C
REF
CAMBRIDGE
COLUMBIA
CU-ZEMAN
DFKI
HUICONG
JHU
ONLINEA
ONLINEB
UEDIN
UPC
BBN-C
CMU-HEA-C
JHU-C
UPV-C
</figure>
<page confidence="0.981654">
52
</page>
<table confidence="0.999534777777778">
REF – .02‡ .00‡ .00‡ .00‡ .00‡ .05‡ .02‡ .00‡ .00‡ .00‡ .02‡ .06‡ .02‡ .04‡ .02‡ .04‡ .03‡ .02‡ .05‡ .05‡ .04‡ .05‡ .06‡ .02‡
CAMBRIDGE .82‡ – .42 .16‡ .12‡ .35 .31 .45 .21‡ .47 .29 .38 .28† .54 .43 .33 .38 .28 .39 .45† .24 .25 .34 .54† .37
CMU-STATXFER .91‡ .50 – .17‡ .41 .17‡ .28 .44 .36 .48* .56‡ .57‡ .47 .56* .70‡ .49 .50 .47 .61‡ .68‡ .55† .50 .42 .52† .51†
CU-ZEMAN 1.00‡ .74‡ .71‡ – .74‡ .46 .67‡ .73‡ .73‡ .74‡ .75‡ .76‡ .75‡ .89‡ .78‡ .66‡ .83‡ .74‡ .87‡ .73‡ .80‡ .83‡ .77‡ .95‡ .82‡
DFKI 1.00‡ .77‡ .48 .17‡ – .27† .49 .52 .48 .64‡ .69‡ .67† .47 .62* .53 .47 .64‡ .60† .73‡ .72‡ .79‡ .58* .66‡ .73‡ .74‡
GENEVA .98‡ .58 .70‡ .44 .59† – .55* .67‡ .70‡ .70‡ .77‡ .73‡ .63‡ .81‡ .81‡ .69† .77‡ .73‡ .62† .66‡ .75‡ .60‡ .73‡ .88‡ .67†
HUICONG .89‡ .53 .34 .13‡ .34 .30* – .41 .36 .43 .70‡ .56‡ .57 .59† .56‡ .43 .55† .45 .51* .64‡ .48 .49 .49 .53† .57†
JHU .88‡ .36 .38 .11‡ .34 .25‡ .35 – .33* .46 .49* .48 .40 .50 .40 .34 .36 .39 .33 .59‡ .54* .41 .42 .40 .41
LIG .98‡ .65‡ .34 .18‡ .44 .26‡ .39 .56* – .60‡ .55‡ .51‡ .45 .54† .53 .39 .38 .52* .54† .53‡ .51* .53† .55 .51 .58†
LIMSI .98‡ .40 .24* .23‡ .23‡ .15‡ .29 .38 .25‡ – .28 .38 .27† .64‡ .35 .30 .41 .27 .33 .49 .45 .37 .28 .45 .39
LIUM .90‡ .40 .19‡ .12‡ .30‡ .11‡ .11‡ .26* .15‡ .36 – .36 .25† .37 .39 .26 .29 .24 .34 .49† .34 .33 .34 .31 .38
NRC .93‡ .31 .06‡ .15‡ .29† .23‡ .20‡ .32 .16‡ .38 .36 – .23† .53 .36 .24* .31 .44 .37 .47* .45* .29 .39 .38 .42
ONLINEA .92‡ .60† .47 .15‡ .44 .22‡ .32 .46 .34 .57† .52† .60† – .52* .34 .44 .57† .56 .51 .51 .64† .46 .51 .41 .60
ONLINEB .85‡ .35 .32* .09‡ .33* .10‡ .29† .31 .25† .17‡ .40 .34 .24* – .38 .32* .28 .39 .30 .42 .37 .41 .35 .32 .22‡
RALI .90‡ .31 .19‡ .10‡ .38 .10‡ .17‡ .47 .35 .38 .33 .38 .48 .48 – .29* .31 .29 .38 .40 .38 .34 .31 .57† .21†
RWTH .93‡ .43 .33 .12‡ .47 .26† .39 .40 .47 .35 .45 .49* .44 .53* .54* – .44* .42 .48 .51* .54* .48† .49 .50‡ .26
UEDIN .92‡ .42 .32 .10‡ .22‡ .10‡ .28† .30 .42 .30 .55 .36 .23† .43 .33 .20* – .41 .24 .52† .46 .25 .22 .27 .37
BBN-C .92‡ .49 .33 .24‡ .28† .18‡ .40 .39 .28* .45 .27 .27 .36 .39 .35 .35 .31 – .26 .45‡ .43 .26 .58‡ .36 .28
CMU-HEA-C .90‡ .41 .21‡ .06‡ .23‡ .29† .28* .27 .22† .39 .40 .22 .39 .43 .29 .30 .40 .28 – .43 .28 .15* .25 .26 .16
CMU-HYPO-C .84‡ .18† .20‡ .14‡ .20‡ .22‡ .21‡ .19‡ .16‡ .31 .22† .21* .36 .38 .34 .27* .22† .16‡ .24 – .36 .23 .10‡ .33 .24
DCU-C .92‡ .27 .24† .12‡ .17‡ .23‡ .30 .29* .24* .32 .43 .22* .28† .41 .23 .27* .28 .22 .23 .25 – .23 .23 .24 .17
JHU-C .88‡ .47 .26 .10‡ .33* .24‡ .36 .34 .24† .41 .39 .40 .42 .39 .34 .25† .42 .28 .37* .38 .39 – .37 .32 .38*
LIUM-C .90‡ .48 .42 .13‡ .25‡ .20‡ .33 .50 .30 .44 .37 .34 .37 .52 .43 .34 .33 .22‡ .34 .56‡ .33 .43 – .49‡ .44
RWTH-C .89‡ .22† .19† .03‡ .23‡ .12‡ .19† .23 .27 .30 .36 .19 .47 .54 .26† .16‡ .27 .19 .26 .28 .16 .22 .16‡ – .22
UPV-C .89‡ .27 .15† .10‡ .16‡ .29† .30† .31 .25† .36 .42 .24 .32 .64‡ .46† .34 .27 .44 .33 .44 .23 .17* .31 .24 ?
&gt; others .91 .43 .32 .14 .31 .21 .31 .39 .31 .42 .44 .40 .38 .52 .43 .33 .40 .37 .40 .49 .43 .38 .4 .44 .39
&gt;= others .97 .64 .51 .24 .40 .31 .50 .59 .50 .63 .68 .65 .51 .68 .65 .55 .66 .63 .69 .75 .71 .64 .62 .74 .67
</table>
<tableCaption confidence="0.996816">
Table 24: Sentence-level ranking for the WMT10 French-English News Task (Combining expert and
</tableCaption>
<figure confidence="0.986580076923077">
non-expert Mechanical Turk judgments)
REF
CAMBRIDGE
CMU-STATXFER
CU-ZEMAN
DFKI
GENEVA
HUICONG
JHU
LIG
LIMSI
LIUM
NRC
ONLINEA
ONLINEB
RALI
RWTH
UEDIN
BBN-C
CMU-HEA-C
CMU-HYPO-C
DCU-C
JHU-C
LIUM-C
RWTH-C
UPV-C
</figure>
<page confidence="0.97668">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.407903">
<title confidence="0.985053">Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation</title>
<author confidence="0.998394">Chris Callison-Burch Philipp Koehn Christof Monz</author>
<affiliation confidence="0.948939">Johns Hopkins University University of Edinburgh University of Amsterdam</affiliation>
<email confidence="0.923649">ccb@cs.jhu.edupkoehn@inf.ed.ac.ukc.monz@uva.nl</email>
<author confidence="0.991888">Peterson Przybocki Omar F Zaidan</author>
<affiliation confidence="0.992347">National Institute of Standards and Technology Johns Hopkins</affiliation>
<email confidence="0.998541">kay.peterson,mark.przybocki@nist.govozaidan@cs.jhu.edu</email>
<abstract confidence="0.985655333333333">This paper presents the results of the WMT10 and MetricsMATR10 shared which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon’s</abstract>
<intro confidence="0.504405">Mechanical Turk.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2006</date>
<booktitle>Workshop on Statistical Machine Translation,</booktitle>
<location>New York, New York.</location>
<marker>2006</marker>
<rawString>2006 Workshop on Statistical Machine Translation, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-2007 Demo and Poster Sessions,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="5858" citStr="Koehn et al., 2007" startWordPosition="897" endWordPosition="900">translation models, monolingual corpora to 2http://statmt.org/wmt10/results.html 3For more details see the XML test files. The docid tag gives the source and the date for each document in the test set, and the origlang tag indicates the original source language. 4http://www.ceet.eu/ train language models, and development sets to tune parameters. Some statistics about the training materials are given in Figure 1. 2.3 Baseline systems To lower the barrier of entry for newcomers to the field, we provided two open source toolkits for phrase-based and parsing-based statistical machine translation (Koehn et al., 2007; Li et al., 2009). 2.4 Submitted systems We received submissions from 33 groups from 29 institutions, as listed in Table 1, a 50% increase over last year’s shared task. We also evaluated 2 commercial off the shelf MT systems, and two online statistical machine translation systems. We note that these companies did not submit entries themselves. The entries for the online systems were done by translating the test data via their web interfaces. The data used to train the online systems is unconstrained. It is possible that part of the reference translations that were taken from online news sites</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL-2007 Demo and Poster Sessions, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Penkale</author>
<author>Rejwanul Haque</author>
<author>Sandipan Dandapat</author>
<author>Pratyush Banerjee</author>
<author>Ankit K Srivastava</author>
<author>Jinhua Du</author>
<author>Pavel Pecina</author>
<author>Sudip Kumar Naskar</author>
<author>Mikel L Forcada</author>
<author>Andy Way</author>
</authors>
<title>Matrex: The dcu mt system for wmt</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>124--129</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9795" citStr="Penkale et al., 2010" startWordPosition="1455" endWordPosition="1458">ber of words and the number of distinct words is based on the provided tokenizer. 19 ID Participant AALTO Aalto University, Finland (Virpioja et al., 2010) CAMBRIDGE Cambridge University (Pino et al., 2010) CMU Carnegie Mellon University’s Cunei system (Phillips, 2010) CMU-STATXFER Carnegie Mellon University’s statistical transfer system (Hanneman et al., 2010) COLUMBIA Columbia University CU-BOJAR Charles University Bojar (Bojar and Kos, 2010) CU-TECTO Charles University Tectogramatical MT (ˇZabokrtsk´y et al., 2010) CU-ZEMAN Charles University Zeman (Zeman, 2010) DCU Dublin City University (Penkale et al., 2010) DFKI Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz (Federmann et al., 2010) EU European Parliament, Luxembourg (Jellinghaus et al., 2010) EUROTRANS commercial MT provider from the Czech Republic FBK Fondazione Bruno Kessler (Hardmeier et al., 2010) GENEVA University of Geneva HUICONG Shanghai Jiao Tong University (Cong et al., 2010) JHU Johns Hopkins University (Schwartz, 2010) KIT Karlsruhe Institute for Technology (Niehues et al., 2010) KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010) LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al., 2010)</context>
</contexts>
<marker>Penkale, Haque, Dandapat, Banerjee, Srivastava, Du, Pecina, Naskar, Forcada, Way, 2010</marker>
<rawString>Sergio Penkale, Rejwanul Haque, Sandipan Dandapat, Pratyush Banerjee, Ankit K. Srivastava, Jinhua Du, Pavel Pecina, Sudip Kumar Naskar, Mikel L. Forcada, and Andy Way. 2010. Matrex: The dcu mt system for wmt 2010. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 124–129, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Phillips</author>
</authors>
<title>The cunei machine translation platform for wmt ’10.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>130--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9443" citStr="Phillips, 2010" startWordPosition="1412" endWordPosition="1413">,219 382,563,246 321,165,206 205,614,201 Distinct words 1,451,719 548,169 998,595 1,855,993 1,715,376 News Test Set English Spanish French German Czech Sentences 2489 Words 62,988 65,654 68,107 62,390 53,171 Distinct words 9,457 11,409 10,775 12,718 15,825 Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words is based on the provided tokenizer. 19 ID Participant AALTO Aalto University, Finland (Virpioja et al., 2010) CAMBRIDGE Cambridge University (Pino et al., 2010) CMU Carnegie Mellon University’s Cunei system (Phillips, 2010) CMU-STATXFER Carnegie Mellon University’s statistical transfer system (Hanneman et al., 2010) COLUMBIA Columbia University CU-BOJAR Charles University Bojar (Bojar and Kos, 2010) CU-TECTO Charles University Tectogramatical MT (ˇZabokrtsk´y et al., 2010) CU-ZEMAN Charles University Zeman (Zeman, 2010) DCU Dublin City University (Penkale et al., 2010) DFKI Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz (Federmann et al., 2010) EU European Parliament, Luxembourg (Jellinghaus et al., 2010) EUROTRANS commercial MT provider from the Czech Republic FBK Fondazione Bruno Kessler (Hardmeier e</context>
</contexts>
<marker>Phillips, 2010</marker>
<rawString>Aaron Phillips. 2010. The cunei machine translation platform for wmt ’10. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 130–135, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Pino</author>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Graeme Blackwood</author>
<author>Jamie Brunning</author>
<author>William Byrne</author>
</authors>
<title>The cued hifst system for the wmt10 translation shared task.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>136--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Pino, Iglesias, de Gispert, Blackwood, Brunning, Byrne, 2010</marker>
<rawString>Juan Pino, Gonzalo Iglesias, Adri`a de Gispert, Graeme Blackwood, Jamie Brunning, and William Byrne. 2010. The cued hifst system for the wmt10 translation shared task. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 136–141, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Potet</author>
<author>Laurent Besacier</author>
<author>Herv´e Blanchon</author>
</authors>
<title>The lig machine translation system for wmt 2010.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>142--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="10395" citStr="Potet et al., 2010" startWordPosition="1540" endWordPosition="1543">nkale et al., 2010) DFKI Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz (Federmann et al., 2010) EU European Parliament, Luxembourg (Jellinghaus et al., 2010) EUROTRANS commercial MT provider from the Czech Republic FBK Fondazione Bruno Kessler (Hardmeier et al., 2010) GENEVA University of Geneva HUICONG Shanghai Jiao Tong University (Cong et al., 2010) JHU Johns Hopkins University (Schwartz, 2010) KIT Karlsruhe Institute for Technology (Niehues et al., 2010) KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010) LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al., 2010) LIMSI LIMSI (Allauzen et al., 2010) LIU Link¨oping University (Stymne et al., 2010) LIUM University of Le Mans (Lambert et al., 2010) NRC National Research Council Canada (Larkin et al., 2010) ONLINEA an online machine translation system ONLINEB an online machine translation system PC-TRANS commercial MT provider from the Czech Republic POTSDAM Potsdam University RALI RALI - Universit´e de Montr´eal (Huet et al., 2010) RWTH RWTH Aachen (Heger et al., 2010) SFU Simon Fraser University (Sankaran et al., 2010) UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2</context>
</contexts>
<marker>Potet, Besacier, Blanchon, 2010</marker>
<rawString>Marion Potet, Laurent Besacier, and Herv´e Blanchon. 2010. The lig machine translation system for wmt 2010. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 142–147, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Przybocki</author>
<author>Kay Peterson</author>
<author>Sebastian Bronsart</author>
</authors>
<date>2008</date>
<booktitle>Official results of the NIST 2008 “Metrics for MAchine TRanslation” challenge (MetricsMATR08). In AMTA-2008 workshop on Metrics for Machine Translation,</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="1452" citStr="Przybocki et al., 2008" startWordPosition="205" endWordPosition="208"> with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon’s Mechanical Turk. 1 Introduction This paper presents the results of the shared tasks of the joint Workshop on statistical Machine Translation (WMT) and Metrics for MAchine TRanslation (MetricsMATR), which was held at ACL 2010. This builds on four previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009), and one previous MetricsMATR meeting (Przybocki et al., 2008). There were three shared tasks this year: a translation task between English and four other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The 1The MetricsMATR analysis was not complete in time for the publication deadline. An updated version of paper will be made available on http://statmt.org/wmt10/ prior to July 15, 2010. performance on each of these shared task was determined after a comprehensive human evaluation. There were a number of differences b</context>
<context position="26281" citStr="Przybocki et al., 2008" startWordPosition="4102" endWordPosition="4105">ceptability measure does not strongly correlate with the more established method of ranking translations relative to each other for all the language pairs. 5 Shared evaluation task overview In addition to allowing the analysis of subjective translation quality measures for different systems, the judgments gathered during the manual evaluation may be used to evaluate how well the automatic evaluation metrics serve as a surrogate to the manual evaluation processes. NIST began running a “Metrics for MAchine TRanslation” challenge (MetricsMATR), and presented their findings at a workshop at AMTA (Przybocki et al., 2008). This year we conducted a joint MetricsMATR and WMT workshop, with NIST running the shared evaluation task and analyzing the results. In this year’s shared evaluation task 14 different research groups submitted a total of 26 different automatic metrics for evaluation: Aalto University of Science and Technology (Dobrinkat et al., 2010) • MT-NCD – A machine translation metric based on normalized compression distance (NCD), a general information-theoretic measure of string similarity. MT-NCD measures the surface level similarity between two strings with a general compression algorithm. More simi</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, 2008</marker>
<rawString>Mark Przybocki, Kay Peterson, and Sebastian Bronsart. 2008. Official results of the NIST 2008 “Metrics for MAchine TRanslation” challenge (MetricsMATR08). In AMTA-2008 workshop on Metrics for Machine Translation, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Bbn system description for wmt10 system combination task.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>296--301</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="11519" citStr="Rosti et al., 2010" startWordPosition="1705" endWordPosition="1708">., 2010) UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010) UEDIN University of Edinburgh (Koehn et al., 2010) UMD University of Maryland (Eidelman et al., 2010) UPC Universitat Polit`ecnica de Catalunya (Henriquez Q. et al., 2010) UPPSALA Uppsala University (Tiedemann, 2010) UPV Universidad Polit´ecnica de Valencia (Sanchis-Trilles et al., 2010) UU-MS Uppsala University - Saers (Saers et al., 2010) Table 1: Participants in the shared translation task. Not all groups participated in all language pairs. 20 ID Participant BBN-COMBO BBN system combination (Rosti et al., 2010) CMU-COMBO-HEAFIELD CMU system combination (Heafield and Lavie, 2010) CMU-COMBO-HYPOSEL CMU system combo with hyp. selection (Hildebrand and Vogel, 2010) DCU-COMBO Dublin City University system combination (Du et al., 2010) JHU-COMBO Johns Hopkins University system combination (Narsale, 2010) KOC-COMBO Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010) LIUM-COMBO University of Le Mans system combination (Barrault, 2010) RWTH-COMBO RWTH Aachen system combination (Leusch and Ney, 2010) UPV-COMBO Universidad Polit´ecnica de Valencia (Gonz´alez-Rubio et al., 2010) Table 2: Par</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2010</marker>
<rawString>Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2010. Bbn system description for wmt10 system combination task. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 296– 301, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Linear inversion transduction grammar alignments as a second translation path.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>148--152</pages>
<location>Uppsala,</location>
<contexts>
<context position="11342" citStr="Saers et al., 2010" startWordPosition="1678" endWordPosition="1681">ch Republic POTSDAM Potsdam University RALI RALI - Universit´e de Montr´eal (Huet et al., 2010) RWTH RWTH Aachen (Heger et al., 2010) SFU Simon Fraser University (Sankaran et al., 2010) UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010) UEDIN University of Edinburgh (Koehn et al., 2010) UMD University of Maryland (Eidelman et al., 2010) UPC Universitat Polit`ecnica de Catalunya (Henriquez Q. et al., 2010) UPPSALA Uppsala University (Tiedemann, 2010) UPV Universidad Polit´ecnica de Valencia (Sanchis-Trilles et al., 2010) UU-MS Uppsala University - Saers (Saers et al., 2010) Table 1: Participants in the shared translation task. Not all groups participated in all language pairs. 20 ID Participant BBN-COMBO BBN system combination (Rosti et al., 2010) CMU-COMBO-HEAFIELD CMU system combination (Heafield and Lavie, 2010) CMU-COMBO-HYPOSEL CMU system combo with hyp. selection (Hildebrand and Vogel, 2010) DCU-COMBO Dublin City University system combination (Du et al., 2010) JHU-COMBO Johns Hopkins University system combination (Narsale, 2010) KOC-COMBO Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010) LIUM-COMBO University of Le Mans system combina</context>
</contexts>
<marker>Saers, Nivre, Wu, 2010</marker>
<rawString>Markus Saers, Joakim Nivre, and Dekai Wu. 2010. Linear inversion transduction grammar alignments as a second translation path. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 148–152, Uppsala,</rawString>
</citation>
<citation valid="false">
<authors>
<author>July Sweden</author>
</authors>
<title>Association for Computational Linguistics.</title>
<marker>Sweden, </marker>
<rawString>Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Germ´an Sanchis-Trilles</author>
<author>Jes´us Andr´es-Ferrer</author>
<author>Guillem Gasc´o</author>
<author>Jes´us Gonz´alez-Rubio</author>
</authors>
<location>Pascual MartinezG´omez, Martha-Alicia Rocha, Joan-Andreu</location>
<marker>Sanchis-Trilles, Andr´es-Ferrer, Gasc´o, Gonz´alez-Rubio, </marker>
<rawString>Germ´an Sanchis-Trilles, Jes´us Andr´es-Ferrer, Guillem Gasc´o, Jes´us Gonz´alez-Rubio, Pascual MartinezG´omez, Martha-Alicia Rocha, Joan-Andreu</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´anchez</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Upv-prhlt english–spanish system for wmt10.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>153--157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>S´anchez, Casacuberta, 2010</marker>
<rawString>S´anchez, and Francisco Casacuberta. 2010. Upv-prhlt english–spanish system for wmt10. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 153–157, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baskaran Sankaran</author>
<author>Ajeet Grewal</author>
<author>Anoop Sarkar</author>
</authors>
<title>Incremental decoding for phrase-based statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>197--204</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="10908" citStr="Sankaran et al., 2010" startWordPosition="1619" endWordPosition="1622">nd Kozat, 2010; Bicici and Yuret, 2010) LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al., 2010) LIMSI LIMSI (Allauzen et al., 2010) LIU Link¨oping University (Stymne et al., 2010) LIUM University of Le Mans (Lambert et al., 2010) NRC National Research Council Canada (Larkin et al., 2010) ONLINEA an online machine translation system ONLINEB an online machine translation system PC-TRANS commercial MT provider from the Czech Republic POTSDAM Potsdam University RALI RALI - Universit´e de Montr´eal (Huet et al., 2010) RWTH RWTH Aachen (Heger et al., 2010) SFU Simon Fraser University (Sankaran et al., 2010) UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010) UEDIN University of Edinburgh (Koehn et al., 2010) UMD University of Maryland (Eidelman et al., 2010) UPC Universitat Polit`ecnica de Catalunya (Henriquez Q. et al., 2010) UPPSALA Uppsala University (Tiedemann, 2010) UPV Universidad Polit´ecnica de Valencia (Sanchis-Trilles et al., 2010) UU-MS Uppsala University - Saers (Saers et al., 2010) Table 1: Participants in the shared translation task. Not all groups participated in all language pairs. 20 ID Participant BBN-COMBO BBN system combination (Rosti et</context>
</contexts>
<marker>Sankaran, Grewal, Sarkar, 2010</marker>
<rawString>Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar. 2010. Incremental decoding for phrase-based statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 197–204, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lane Schwartz</author>
</authors>
<title>Reproducible results in parsingbased machine translation: The jhu shared task submission.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>158--163</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="10187" citStr="Schwartz, 2010" startWordPosition="1510" endWordPosition="1511">JAR Charles University Bojar (Bojar and Kos, 2010) CU-TECTO Charles University Tectogramatical MT (ˇZabokrtsk´y et al., 2010) CU-ZEMAN Charles University Zeman (Zeman, 2010) DCU Dublin City University (Penkale et al., 2010) DFKI Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz (Federmann et al., 2010) EU European Parliament, Luxembourg (Jellinghaus et al., 2010) EUROTRANS commercial MT provider from the Czech Republic FBK Fondazione Bruno Kessler (Hardmeier et al., 2010) GENEVA University of Geneva HUICONG Shanghai Jiao Tong University (Cong et al., 2010) JHU Johns Hopkins University (Schwartz, 2010) KIT Karlsruhe Institute for Technology (Niehues et al., 2010) KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010) LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al., 2010) LIMSI LIMSI (Allauzen et al., 2010) LIU Link¨oping University (Stymne et al., 2010) LIUM University of Le Mans (Lambert et al., 2010) NRC National Research Council Canada (Larkin et al., 2010) ONLINEA an online machine translation system ONLINEB an online machine translation system PC-TRANS commercial MT provider from the Czech Republic POTSDAM Potsdam University RALI RALI - Universit´e d</context>
</contexts>
<marker>Schwartz, 2010</marker>
<rawString>Lane Schwartz. 2010. Reproducible results in parsingbased machine translation: The jhu shared task submission. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 158–163, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Maria Holmqvist</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Vs and oovs: Two problems for translation between german and english.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>164--169</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="10479" citStr="Stymne et al., 2010" startWordPosition="1553" endWordPosition="1556">Federmann et al., 2010) EU European Parliament, Luxembourg (Jellinghaus et al., 2010) EUROTRANS commercial MT provider from the Czech Republic FBK Fondazione Bruno Kessler (Hardmeier et al., 2010) GENEVA University of Geneva HUICONG Shanghai Jiao Tong University (Cong et al., 2010) JHU Johns Hopkins University (Schwartz, 2010) KIT Karlsruhe Institute for Technology (Niehues et al., 2010) KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010) LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al., 2010) LIMSI LIMSI (Allauzen et al., 2010) LIU Link¨oping University (Stymne et al., 2010) LIUM University of Le Mans (Lambert et al., 2010) NRC National Research Council Canada (Larkin et al., 2010) ONLINEA an online machine translation system ONLINEB an online machine translation system PC-TRANS commercial MT provider from the Czech Republic POTSDAM Potsdam University RALI RALI - Universit´e de Montr´eal (Huet et al., 2010) RWTH RWTH Aachen (Heger et al., 2010) SFU Simon Fraser University (Sankaran et al., 2010) UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010) UEDIN University of Edinburgh (Koehn et al., 2010) UMD University of Maryland (</context>
</contexts>
<marker>Stymne, Holmqvist, Ahrenberg, 2010</marker>
<rawString>Sara Stymne, Maria Holmqvist, and Lars Ahrenberg. 2010. Vs and oovs: Two problems for translation between german and english. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 164–169, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>To cache or not to cache? experiments with adaptive models in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>170--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="11216" citStr="Tiedemann, 2010" startWordPosition="1662" endWordPosition="1663">online machine translation system ONLINEB an online machine translation system PC-TRANS commercial MT provider from the Czech Republic POTSDAM Potsdam University RALI RALI - Universit´e de Montr´eal (Huet et al., 2010) RWTH RWTH Aachen (Heger et al., 2010) SFU Simon Fraser University (Sankaran et al., 2010) UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010) UEDIN University of Edinburgh (Koehn et al., 2010) UMD University of Maryland (Eidelman et al., 2010) UPC Universitat Polit`ecnica de Catalunya (Henriquez Q. et al., 2010) UPPSALA Uppsala University (Tiedemann, 2010) UPV Universidad Polit´ecnica de Valencia (Sanchis-Trilles et al., 2010) UU-MS Uppsala University - Saers (Saers et al., 2010) Table 1: Participants in the shared translation task. Not all groups participated in all language pairs. 20 ID Participant BBN-COMBO BBN system combination (Rosti et al., 2010) CMU-COMBO-HEAFIELD CMU system combination (Heafield and Lavie, 2010) CMU-COMBO-HYPOSEL CMU system combo with hyp. selection (Hildebrand and Vogel, 2010) DCU-COMBO Dublin City University system combination (Du et al., 2010) JHU-COMBO Johns Hopkins University system combination (Narsale, 2010) KOC</context>
</contexts>
<marker>Tiedemann, 2010</marker>
<rawString>J¨org Tiedemann. 2010. To cache or not to cache? experiments with adaptive models in statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 170–175, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Jaakko V¨ayrynen</author>
<author>Andre Mansikkaniemi</author>
<author>Mikko Kurimo</author>
</authors>
<title>Applying morphological decompositions to statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>176--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Virpioja, V¨ayrynen, Mansikkaniemi, Kurimo, 2010</marker>
<rawString>Sami Virpioja, Jaakko V¨ayrynen, Andre Mansikkaniemi, and Mikko Kurimo. 2010. Applying morphological decompositions to statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 176–181, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenˇek ˇZabokrtsk´y</author>
<author>Martin Popel</author>
<author>David Mareˇcek</author>
</authors>
<title>Maximum entropy translation model in dependency-based mt framework.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>182--187</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>ˇZabokrtsk´y, Popel, Mareˇcek, 2010</marker>
<rawString>Zdenˇek ˇZabokrtsk´y, Martin Popel, and David Mareˇcek. 2010. Maximum entropy translation model in dependency-based mt framework. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 182–187, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>The parameteroptimized atec metric for mt evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>335--339</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="34908" citStr="Wong and Kit, 2010" startWordPosition="5617" endWordPosition="5620">ment of each document. Also, a separate issue was found for the MT-mNCD metric, and according to the developer the scores reported here would like change with a correction of the issue. BabbleQuest International8 • Badger 2.0 full – Uses the Smith-Waterman alignment algorithm with Gotoh improvements to measure segment similarity. The full version uses a multilingual knowledge base to assign a substitution cost which supports normalization of word infection and similarity. • Badger 2.0 lite – The lite version uses default gap, gap extension and substitution costs. City University of Hong Kong (Wong and Kit, 2010) • ATEC 2.1 – This version of ATEC extends the measurement of word choice and word order by various means. The former is assessed by matching word forms at linguistic levels, including surface form, stem, sense and semantic similarity, and further by weighting the informativeness of both matched and unmatched words. The latter is quantified in term of the discordance of word position and word sequence between an MT output and its reference. Due to a version discrepancy of the metric, final scores for ATECD-2.1 differ from those reported here, but only minimally. 8http://www.babblequest.com/bad</context>
</contexts>
<marker>Wong, Kit, 2010</marker>
<rawString>Billy Wong and Chunyu Kit. 2010. The parameteroptimized atec metric for mt evaluation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 335– 339, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Zamora-Martinez</author>
<author>Germ´an SanchisTrilles</author>
</authors>
<title>Uch-upv english–spanish system for wmt10.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>188--192</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Zamora-Martinez, SanchisTrilles, 2010</marker>
<rawString>Francisco Zamora-Martinez and Germ´an SanchisTrilles. 2010. Uch-upv english–spanish system for wmt10. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 188–192, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Hierarchical phrase-based mt at the charles university for the wmt 2010 shared task.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>193--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9745" citStr="Zeman, 2010" startWordPosition="1449" endWordPosition="1450">ets used in the translation task. The number of words and the number of distinct words is based on the provided tokenizer. 19 ID Participant AALTO Aalto University, Finland (Virpioja et al., 2010) CAMBRIDGE Cambridge University (Pino et al., 2010) CMU Carnegie Mellon University’s Cunei system (Phillips, 2010) CMU-STATXFER Carnegie Mellon University’s statistical transfer system (Hanneman et al., 2010) COLUMBIA Columbia University CU-BOJAR Charles University Bojar (Bojar and Kos, 2010) CU-TECTO Charles University Tectogramatical MT (ˇZabokrtsk´y et al., 2010) CU-ZEMAN Charles University Zeman (Zeman, 2010) DCU Dublin City University (Penkale et al., 2010) DFKI Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz (Federmann et al., 2010) EU European Parliament, Luxembourg (Jellinghaus et al., 2010) EUROTRANS commercial MT provider from the Czech Republic FBK Fondazione Bruno Kessler (Hardmeier et al., 2010) GENEVA University of Geneva HUICONG Shanghai Jiao Tong University (Cong et al., 2010) JHU Johns Hopkins University (Schwartz, 2010) KIT Karlsruhe Institute for Technology (Niehues et al., 2010) KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010) LIG LIG Lab, Univer</context>
</contexts>
<marker>Zeman, 2010</marker>
<rawString>Daniel Zeman. 2010. Hierarchical phrase-based mt at the charles university for the wmt 2010 shared task. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 193–196, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>