<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009430">
<title confidence="0.999447">
Adaptation Data Selection using Neural Language Models:
Experiments in Machine Translation
</title>
<author confidence="0.999643">
Kevin Duh, Graham Neubig Katsuhito Sudoh, Hajime Tsukada
</author>
<affiliation confidence="0.998521">
Graduate School of Information Science NTT Communication Science Labs.
Nara Institute of Science and Technology NTT Corporation
</affiliation>
<address confidence="0.579505">
8916-5 Takayama, Ikoma, Japan 2-4 Hikaridai, Seika, Kyoto, Japan
</address>
<email confidence="0.994077">
kevinduh@is.naist.jp sudoh.katsuhito@lab.ntt.co.jp
neubig@is.naist.jp tsukada.hajime@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.993803" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999723923076923">
Data selection is an effective approach
to domain adaptation in statistical ma-
chine translation. The idea is to use lan-
guage models trained on small in-domain
text to select similar sentences from large
general-domain corpora, which are then
incorporated into the training data. Sub-
stantial gains have been demonstrated in
previous works, which employ standard n-
gram language models. Here, we explore
the use of neural language models for data
selection. We hypothesize that the con-
tinuous vector representation of words in
neural language models makes them more
effective than n-grams for modeling un-
known word contexts, which are prevalent
in general-domain text. In a comprehen-
sive evaluation of 4 language pairs (En-
glish to German, French, Russian, Span-
ish), we found that neural language mod-
els are indeed viable tools for data se-
lection: while the improvements are var-
ied (i.e. 0.1 to 1.7 gains in BLEU), they
are fast to train on small in-domain data
and can sometimes substantially outper-
form conventional n-grams.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966216216216">
A perennial challenge in building Statistical Ma-
chine Translation (SMT) systems is the dearth
of high-quality bitext in the domain of interest.
An effective and practical solution is adaptation
data selection: the idea is to use language models
(LMs) trained on in-domain text to select similar
sentences from large general-domain corpora. The
selected sentences are then incorporated into the
SMT training data. Analyses have shown that this
augmented data can lead to better statistical esti-
mation or word coverage (Duh et al., 2010; Had-
dow and Koehn, 2012).
Although previous works in data selection (Ax-
elrod et al., 2011; Koehn and Haddow, 2012; Ya-
suda et al., 2008) have shown substantial gains, we
suspect that the commonly-used n-gram LMs may
be sub-optimal. The small size of the in-domain
text implies that a large percentage of general-
domain sentences will contain words not observed
in the LM training data. In fact, as many as 60% of
general-domain sentences contain at least one un-
known word in our experiments. Although the LM
probabilities of these sentences could still be com-
puted by resorting to back-off and other smoothing
techniques, a natural question remains: will alter-
native, more robust LMs do better?
We hypothesize that the neural language model
(Bengio et al., 2003) is a viable alternative, since
its continuous vector representation of words is
well-suited for modeling sentences with frequent
unknown words, providing smooth probability es-
timates of unseen but similar contexts. Neu-
ral LMs have achieved positive results in speech
recognition and SMT reranking (Schwenk et al.,
2012; Mikolov et al., 2011a). To the best of our
knowledge, this paper is the first work that exam-
ines neural LMs for adaptation data selection.
</bodyText>
<sectionHeader confidence="0.95884" genericHeader="method">
2 Data Selection Method
</sectionHeader>
<bodyText confidence="0.999629375">
We employ the data selection method of (Ax-
elrod et al., 2011), which builds upon (Moore
and Lewis, 2010). The intuition is to select
general-domain sentences that are similar to in-
domain text, while being dis-similar to the average
general-domain text.
To do so, one defines the score of an general-
domain sentence pair (e, f) as:
</bodyText>
<equation confidence="0.9710105">
[INE(e) − GENE(e)] + [INF(f) − GENF(f)]
(1)
</equation>
<bodyText confidence="0.828749">
where INE(e) is the length-normalized cross-
entropy of e on the English in-domain LM.
GENE(e) is the length-normalized cross-entropy
</bodyText>
<page confidence="0.947146">
678
</page>
<note confidence="0.62909">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678–683,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999462">
Figure 1: Recurrent neural LM.
</figureCaption>
<bodyText confidence="0.999431375">
of e on the English general-domain LM, which
is built from a sub-sample of the general-domain
text. Similarly, INF (f) and GENF (f) are the
cross-entropies of f on Foreign-side LM. Finally,
sentence pairs are ranked according to Eq. 1 and
those with scores lower than some empirically-
chosen threshold are added to the bitext for trans-
lation model training.
</bodyText>
<subsectionHeader confidence="0.974586">
2.1 Neural Language Models
</subsectionHeader>
<bodyText confidence="0.956374027027027">
The four LMs used to compute Eq. 1 have con-
ventionally been n-grams. N-grams of the form
p(w(t)|w(t − 1), w(t − 2), ...) predict words by
using multinomial distributions conditioned on the
context (w(t−1), w(t−2), ...). But when the con-
text is rare or contains unknown words, n-grams
are forced to back-off to lower-order models, e.g.
p(w(t)|w(t − 1)). These backoffs are unfortu-
nately very frequent in adaptation data selection.
Neural LMs, in contrast, model word probabili-
ties using continuous vector representations. Fig-
ure 1 shows a type of neural LMs called recurrent
neural networks (Mikolov et al., 2011b).1 Rather
than representing context as an identity (n-gram
hit-or-miss) function on [w(t − 1), w(t − 2), ...],
neural LMs summarize the context by a hidden
state vector s(t). This is a continuous vector of
dimension |S |whose elements are predicted by
the previous word w(t − 1) and previous state
s(t − 1). This is robust to rare contexts because
continuous representations enable sharing of sta-
tistical strength between similar contexts. Bengio
(2009) shows that such representations are better
than multinomials in alleviating sparsity issues.
1Another major type of neural LMs are the so-called
feed-forward networks (Bengio et al., 2003; Schwenk, 2007;
Nakamura et al., 1990). Both types of neural LMs have seen
many improvements recently, in terms of computational scal-
ability (Le et al., 2011) and modeling power (Arisoy et al.,
2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006).
We focus on recurrent networks here since there are fewer
hyper-parameters and its ability to model infinite context us-
ing recursion is theoretically attractive. But we note that feed-
forward networks are just as viable.
Now, given state vector s(t), we can predict the
probability of the current word. Figure 1 is ex-
pressed formally in the following equations:
</bodyText>
<equation confidence="0.875731272727273">
w(t) = [w0(t), ... , wk(t),... w|W|(t)] (2)
⎛ ⎞
X |S|
wk(t) = g ⎝ sj(t)Vkj ⎠ (3)
j=0
|W ||S|
sj(t) = fX wi(t − 1)Uji + X si/(t − 1)Aji
(i=0 i/=0
(4)
Here, w(t) is viewed as a vector of dimension
|W  |(vocabulary size) where each element wk(t)
</equation>
<bodyText confidence="0.99898215">
represents the probability of the k-th vocabulary
item at sentence position t. The function g(zk) =
ezk/ Pk ezk is a softmax function that ensures the
neural LM outputs are proper probabilities, and
f(z) = 1/(1 + e−z) is a sigmoid activation that
induces the non-linearity critical to the neural net-
work’s expressive power. The matrices V , U, and
A are trained by maximizing likelihood on train-
ing data using a ”backpropagation-through-time”
method.2 Intuitively, U and A compress the con-
text (|S |&lt; |W |) such that contexts predictive of
the same word w(t) are close together.
Since proper modeling of unknown contexts is
important in our problem, training text for both n-
gram and neural LM is pre-processed by convert-
ing all low-frequency words in the training data
(frequency=1 in our case) to a special ”unknown”
token. This is used only in Eq. 1 for selecting
general-domain sentences; these words retain their
surface forms in the SMT train pipeline.
</bodyText>
<sectionHeader confidence="0.991988" genericHeader="method">
3 Experiment Setup
</sectionHeader>
<bodyText confidence="0.999773333333333">
We experimented with four language pairs in the
WITS corpus (Cettolo et al., 2012), with English
(en) as source and German (de), Spanish (es),
French (fr), Russian (ru) as target. This is the
in-domain corpus, and consists of TED Talk tran-
scripts covering topics in technology, entertain-
ment, and design. As general-domain corpora,
we collected bitext from the WMT2013 campaign,
including CommonCrawl and NewsCommentary
for all 4 languages, Europarl for de/es/fr, UN for
es/fr, Gigaword for fr, and Yandex for ru. The in-
domain data is divided into a training set (for SMT
</bodyText>
<footnote confidence="0.619686">
2The recurrent states are unrolled for several time-steps,
then stochastic gradient descent is applied.
</footnote>
<page confidence="0.993299">
679
</page>
<table confidence="0.743123">
en-de en-es en-fr en-ru 4 Results
</table>
<tableCaption confidence="0.841195">
Table 1: Data statistics. ”%unknown”=fraction of
general-domain sentences with unknown words.
</tableCaption>
<bodyText confidence="0.998278555555555">
pipeline and neural LM training), a tuning set (for
MERT), a validation set (for choosing the optimal
threshold in data selection), and finally a testset of
1616 sentences.3 Table 1 lists data statistics.
For each language pair, we built a baseline in-
data SMT system trained only on in-domain data,
and an alldata system using combined in-domain
and general-domain data.4 We then built 3 systems
from augmented data selected by different LMs:
</bodyText>
<listItem confidence="0.999360833333333">
• ngram: Data selection by 4-gram LMs with
Kneser-Ney smoothing (Axelrod et al., 2011)
• neuralnet: Data selection by Recurrent neu-
ral LM, with the RNNLM Toolkit.5
• combine: Data selection by interpolated LM
using n-gram &amp; neuralnet (equal weight).
</listItem>
<bodyText confidence="0.999348545454545">
All systems are built using standard settings in
the Moses toolkit (GIZA++ alignment, grow-diag-
final-and, lexical reordering models, and SRILM).
Note that standard n-grams are used as LMs for
SMT; neural LMs are only used for data selection.
Multiple SMT systems are trained by thresholding
on {10k,50k,100k,500k,1M} general-domain sen-
tence subsets, and we empirically determine the
single system for testing based on results on a sep-
arate validation set (in practice, 500k was chosen
for fr and 1M for es, de, ru.).
</bodyText>
<footnote confidence="0.999515166666667">
3The original data are provided by http://wit3.fbk.eu and
http://www.statmt.org/wmt13/. Our domain adaptation sce-
nario is similar to the IWSLT2012 campaign but we used our
own random train/test splits, since we wanted to ensure the
testset for all languages had identical source sentences for
comparison purposes. For replicability, our software is avail-
able at http://cl.naist.jp/∼kevinduh/a/acl2013.
4More advanced phrase table adaptation methods are pos-
sible. but our interest is in comparing data selection methods.
The conclusions should transfer to advanced methods such as
(Foster et al., 2010; Niehues and Waibel, 2012).
5http://www.fit.vutbr.cz/∼imikolov/rnnlm/
</footnote>
<subsectionHeader confidence="0.975912">
4.1 LM Perplexity and Training Time
</subsectionHeader>
<bodyText confidence="0.999750545454545">
First, we measured perplexity to check the gen-
eralization ability of our neural LMs as language
models. Recall that we train four LMs to com-
pute each of the components of Eq. 1. In Table 2,
we compared each of the four versions of ngram,
neuralnet, and combine LMs on in-domain test
sets or general-domain held-out sets. It re-affirms
previous positive results (Mikolov et al., 2011a),
with neuralnet outperforming ngram by 20-30%
perplexity across all tasks. Also, combine slightly
improves the perplexity of neuralnet.
</bodyText>
<table confidence="0.999901578947368">
Task ngram neuralnet combine
In-Domain Test Set
en-de de 157 110 (29%) 110 (29%)
en-de en 102 81 (20%) 78 (24%)
en-es es 129 102 (20%) 98 (24%)
en-es en 101 80 (21%) 77 (24%)
en-fr fr 90 67 (25%) 65 (27%)
en-fr en 102 80 (21%) 77 (24%)
en-ru ru 208 167 (19%) 155 (26%)
en-ru en 103 83 (19%) 79 (23%)
General-Domain Held-out Set
en-de de 234 174 (25%) 161 (31%)
en-de en 218 168 (23%) 155 (29%)
en-es es 62 43 (31%) 43 (31%)
en-es en 84 61 (27%) 59 (30%)
en-fr fr 64 43 (33%) 43 (33%)
en-fr en 95 67 (30%) 65 (32%)
en-ru ru 242 199 (18%) 176 (27%)
en-ru en 191 153 (20%) 142 (26%)
</table>
<tableCaption confidence="0.885475">
Table 2: Perplexity of various LMs. Number in
parenthesis is percentage improvement vs. ngram.
</tableCaption>
<bodyText confidence="0.999868571428571">
Second, we show that the usual concern of neu-
ral LM training time is not so critical for the in-
domain data sizes used domain adaptation. The
complexity of training Figure 1 is dominated by
computing Eq. 3 and scales as O(|W  |x |S|) in
the number of tokens. Since |W  |can be large, one
practical trick is to cluster the vocabulary so that
the output dimension is reduced. Table 3 shows
the training times on a 3.3GHz XeonE5 CPU by
varying these two main hyper-parameters (|S |and
cluster size). Note that the setting |S |= 200 and
cluster size of 100 already gives good perplexity
in reasonable training time. All neural LMs in this
paper use this setting, without additional tuning.
</bodyText>
<figure confidence="0.998762785714286">
129k
2.5M
26k
42k
#sentence
#token (en)
#vocab (en)
#vocab (f)
In-domain Training Set
139k
2.7M
27k
34k
117k
2.3M
25k
58k
140k
2.7M
27k
39k
General-domain Bitext
#sentence 4.4M 14.7M 38.9M
#token (en) 113M 385M 1012M
%unknown 60% 58% 64%
2.0M
51M
65%
</figure>
<page confidence="0.984368">
680
</page>
<table confidence="0.9992755">
|S |Cluster Time Perplexity
200 100 198m 110
100 |W |12915m 110
200 400 208m 113
100 100 52m 118
100 400 71m 120
</table>
<tableCaption confidence="0.991376">
Table 3: Training time (in minutes) for various
neural LM architectures (Task: en-de de).
</tableCaption>
<subsectionHeader confidence="0.981029">
4.2 End-to-end SMT Evaluation
</subsectionHeader>
<bodyText confidence="0.99914155">
Table 4 shows translation results in terms of BLEU
(Papineni et al., 2002), RIBES (Isozaki et al.,
2010), and TER (Snover et al., 2006). We observe
that all three data selection methods essentially
outperform alldata and indata for all language
pairs, and neuralnet tend to be the best in all met-
rics. E.g., BLEU improvements over ngram are
in the range of 0.4 for en-de, 0.5 for en-es, 0.1
for en-fr, and 1.7 for en-ru. Although not all im-
provements are large in absolute terms, many are
statistically significant (95% confidence).
We therefore believe that neural LMs are gen-
erally worthwhile to try for data selection, as it
rarely underperform n-grams. The open question
is: what can explain the significant improvements
in, for example Russian, Spanish, German, but the
lack thereof in French? One conjecture is that
neural LMs succeeded in lowering testset out-of-
vocabulary (OOV) rate, but we found that OOV
reduction is similar across all selection methods.
The improvements appear to be due to better
probability estimates of the translation/reordering
models. We performed a diagnostic by decoding
the testset using LMs trained on the same test-
set, while varying the translation/reordering ta-
bles with those of ngram and neuralnet; this is a
kind of pseudo forced-decoding that can inform us
about which table has better coverage. We found
that across all language pairs, BLEU differences of
translations under this diagnostic become insignif-
icant, implying that the raw probability value is
the differentiating factor between ngram and neu-
ralnet. Manual inspection of en-de revealed that
many improvements come from lexical choice in
morphological variants (”meinen Sohn” vs. ”mein
Sohn”), segmentation changes (”baking soda” -+
”Backpulver” vs. ”baken Soda”), and handling of
unaligned words at phrase boundaries.
Finally, we measured the intersection between
the sentence set selected by ngram vs neural-
</bodyText>
<table confidence="0.999869857142857">
Task System BLEU RIBES TER
en-de indata 20.8 80.1 59.0
alldata 21.5 80.1 59.1
ngram 21.5 80.3 58.9
neuralnet 21.9+ 80.5+ 58.4
combine 21.5 80.2 58.8
en-es indata 30.4 83.5 48.7
alldata 31.2 83.2 49.9
ngram 32.0 83.7 48.4
neuralnet 32.5+ 83.7 48.3+
combine 32.5+ 83.8 48.3+
en-fr indata 31.4 83.9 51.2
alldata 31.5 83.5 51.4
ngram 32.7 83.7 50.4
neuralnet 32.8 84.2+ 50.3
combine 32.5 84.0 50.5
en-ru indata 14.8 72.5 69.5
alldata 23.4 75.0 62.3
ngram 24.0 75.7 61.4
neuralnet 25.7+ 76.1 60.0+
combine 23.7 75.9 61.9−
</table>
<tableCaption confidence="0.985552">
Table 4: End-to-end Translation Results. The best
</tableCaption>
<bodyText confidence="0.928926111111111">
results are bold-faced. We also compare neural
LMs to ngram using pairwise bootstrap (Koehn,
2004): ”+” means statistically significant im-
provement and ”−” means significant degradation.
net. They share 60-75% of the augmented train-
ing data. This high overlap means that ngram
and neuralnet are actually not drastically different
systems, and neuralnet with its slightly better se-
lections represent an incremental improvement.6
</bodyText>
<sectionHeader confidence="0.996115" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999961">
We perform an evaluation of neural LMs for
adaptation data selection, based on the hypothe-
sis that their continuous vector representations are
effective at comparing general-domain sentences,
which contain frequent unknown words. Com-
pared to conventional n-grams, we observed end-
to-end translation improvements from 0.1 to 1.7
BLEU. Since neural LMs are fast to train in the
small in-domain data setting and achieve equal or
incrementally better results, we conclude that they
are an worthwhile option to include in the arsenal
of adaptation data selection techniques.
</bodyText>
<footnote confidence="0.990587666666667">
6This is corroborated by another analysis: taking the
union of sentences found by ngram and neuralnet gives sim-
ilar BLEU scores as neuralnet.
</footnote>
<page confidence="0.99686">
681
</page>
<sectionHeader confidence="0.998324" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999968142857143">
We thank Amittai Axelrod for discussions about
data selection implementation details, and an
anonymous reviewer for suggesting the union idea
for results analysis. K. D. would like to credit Spy-
ros Matsoukas (personal communication, 2010)
for the trick of using LM-based pseudo forced-
decoding for error analysis.
</bodyText>
<sectionHeader confidence="0.997383" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997101381443299">
Andrei Alexandrescu and Katrin Kirchhoff. 2006.
Factored neural language models. In Proceed-
ings of the Human Language Technology Confer-
ence of the NAACL, Companion Volume: Short Pa-
pers, NAACL-Short ’06, pages 1–4, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In Proceedings of the NAACL-
HLT 2012 Workshop: Will We Ever Really Replace
the N-gram Model? On the Future of Language
Modeling for HLT, pages 20–28, Montr´eal, Canada,
June. Association for Computational Linguistics.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355–362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage models. JMLR.
Yoshua Bengio. 2009. Learning Deep Architectures
for AI, volume Foundations and Trends in Machine
Learning. NOW Publishers.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 261–268, Trento, Italy,
May.
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation for
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT) - Technical Papers Track.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on smt systems. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 422–432, Montr´eal,
Canada, June. Association for Computational Lin-
guistics.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 944–952, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In WMT.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.
Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and
F. Yvon. 2011. Structured output layer neural net-
work language model. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5524–5527.
Tom´a˘s Mikolov, Anoop Deoras, Daniel Povey, Luk´a˘s
Burget, and Jan ˘Cernock´y. 2011a. Strategies for
training large scale neural network language model.
In ASRU.
Tom´a˘s Mikolov, Stefan Kombrink, Luk´a˘s Burget, Jan
ˇCernock´y, and Sanjeev Khudanpur. 2011b. Exten-
sions of recurrent neural network language model.
In Proceedings of the 2011 IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220–224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Masami Nakamura, Katsuteru Maruyama, Takeshi
Kawabata, and Kiyohiro Shikano. 1990. Neural
network approach to word category prediction for
english texts. In Proceedings of the 13th conference
on Computational linguistics - Volume 3, COLING
’90, pages 213–218, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jan Niehues and Alex Waibel. 2012. Detailed analysis
of different strategies for phrase table adaptation in
SMT. In AMTA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
</reference>
<page confidence="0.976758">
682
</page>
<reference confidence="0.999460888888889">
HLT, pages 11–19, Montr´eal, Canada, June. Associ-
ation for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492–518,
July.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Youzheng Wu, Xugang Lu, Hitoshi Yamamoto,
Shigeki Matsuda, Chiori Hori, and Hideki Kashioka.
2012. Factored language model based on recurrent
neural network. In Proceedings of COLING 2012,
pages 2835–2850, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In ICJNLP.
</reference>
<page confidence="0.999156">
683
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.861671">
<title confidence="0.9984335">Adaptation Data Selection using Neural Language Experiments in Machine Translation</title>
<author confidence="0.993998">Kevin Duh</author>
<author confidence="0.993998">Graham Neubig Katsuhito Sudoh</author>
<author confidence="0.993998">Hajime Tsukada</author>
<affiliation confidence="0.9998615">Graduate School of Information Science NTT Communication Science Labs. Nara Institute of Science and Technology NTT Corporation</affiliation>
<address confidence="0.997136">8916-5 Takayama, Ikoma, Japan 2-4 Hikaridai, Seika, Kyoto, Japan</address>
<abstract confidence="0.994540862068965">kevinduh@is.naist.jp sudoh.katsuhito@lab.ntt.co.jp neubig@is.naist.jp tsukada.hajime@lab.ntt.co.jp Abstract Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard ngram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrei Alexandrescu</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored neural language models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06,</booktitle>
<pages>1--4</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5883" citStr="Alexandrescu and Kirchhoff, 2006" startWordPosition="919" endWordPosition="922">vious word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 1Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractive. But we note that feedforward networks are just as viable. Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0(t), ... , wk(t),... w|W|(t)] (2) ⎛ ⎞ X |S| wk(t) = g ⎝ sj(t)Vkj ⎠ (3) j=0 |W ||S| sj(t) = fX wi(t − 1)Uji + X si/(t − 1)Aji (i=0 i/=0 (4) Here, w(t) is viewed as a vector of dimension |W |(vocabulary size) where each element w</context>
</contexts>
<marker>Alexandrescu, Kirchhoff, 2006</marker>
<rawString>Andrei Alexandrescu and Katrin Kirchhoff. 2006. Factored neural language models. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06, pages 1–4, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ebru Arisoy</author>
<author>Tara N Sainath</author>
<author>Brian Kingsbury</author>
<author>Bhuvana Ramabhadran</author>
</authors>
<title>Deep neural network language models.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACLHLT</booktitle>
<pages>20--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5831" citStr="Arisoy et al., 2012" startWordPosition="911" endWordPosition="914">hose elements are predicted by the previous word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 1Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractive. But we note that feedforward networks are just as viable. Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0(t), ... , wk(t),... w|W|(t)] (2) ⎛ ⎞ X |S| wk(t) = g ⎝ sj(t)Vkj ⎠ (3) j=0 |W ||S| sj(t) = fX wi(t − 1)Uji + X si/(t − 1)Aji (i=0 i/=0 (4) Here, w(t) is viewed as a vector of </context>
</contexts>
<marker>Arisoy, Sainath, Kingsbury, Ramabhadran, 2012</marker>
<rawString>Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhuvana Ramabhadran. 2012. Deep neural network language models. In Proceedings of the NAACLHLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 20–28, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2111" citStr="Axelrod et al., 2011" startWordPosition="311" endWordPosition="315">rennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). Although previous works in data selection (Axelrod et al., 2011; Koehn and Haddow, 2012; Yasuda et al., 2008) have shown substantial gains, we suspect that the commonly-used n-gram LMs may be sub-optimal. The small size of the in-domain text implies that a large percentage of generaldomain sentences will contain words not observed in the LM training data. In fact, as many as 60% of general-domain sentences contain at least one unknown word in our experiments. Although the LM probabilities of these sentences could still be computed by resorting to back-off and other smoothing techniques, a natural question remains: will alternative, more robust LMs do bett</context>
<context position="8802" citStr="Axelrod et al., 2011" startWordPosition="1405" endWordPosition="1408">ble 1: Data statistics. ”%unknown”=fraction of general-domain sentences with unknown words. pipeline and neural LM training), a tuning set (for MERT), a validation set (for choosing the optimal threshold in data selection), and finally a testset of 1616 sentences.3 Table 1 lists data statistics. For each language pair, we built a baseline indata SMT system trained only on in-domain data, and an alldata system using combined in-domain and general-domain data.4 We then built 3 systems from augmented data selected by different LMs: • ngram: Data selection by 4-gram LMs with Kneser-Ney smoothing (Axelrod et al., 2011) • neuralnet: Data selection by Recurrent neural LM, with the RNNLM Toolkit.5 • combine: Data selection by interpolated LM using n-gram &amp; neuralnet (equal weight). All systems are built using standard settings in the Moses toolkit (GIZA++ alignment, grow-diagfinal-and, lexical reordering models, and SRILM). Note that standard n-grams are used as LMs for SMT; neural LMs are only used for data selection. Multiple SMT systems are trained by thresholding on {10k,50k,100k,500k,1M} general-domain sentence subsets, and we empirically determine the single system for testing based on results on a separ</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355–362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language models.</title>
<date>2003</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="2782" citStr="Bengio et al., 2003" startWordPosition="423" endWordPosition="426"> shown substantial gains, we suspect that the commonly-used n-gram LMs may be sub-optimal. The small size of the in-domain text implies that a large percentage of generaldomain sentences will contain words not observed in the LM training data. In fact, as many as 60% of general-domain sentences contain at least one unknown word in our experiments. Although the LM probabilities of these sentences could still be computed by resorting to back-off and other smoothing techniques, a natural question remains: will alternative, more robust LMs do better? We hypothesize that the neural language model (Bengio et al., 2003) is a viable alternative, since its continuous vector representation of words is well-suited for modeling sentences with frequent unknown words, providing smooth probability estimates of unseen but similar contexts. Neural LMs have achieved positive results in speech recognition and SMT reranking (Schwenk et al., 2012; Mikolov et al., 2011a). To the best of our knowledge, this paper is the first work that examines neural LMs for adaptation data selection. 2 Data Selection Method We employ the data selection method of (Axelrod et al., 2011), which builds upon (Moore and Lewis, 2010). The intuit</context>
<context position="5632" citStr="Bengio et al., 2003" startWordPosition="878" endWordPosition="881">nting context as an identity (n-gram hit-or-miss) function on [w(t − 1), w(t − 2), ...], neural LMs summarize the context by a hidden state vector s(t). This is a continuous vector of dimension |S |whose elements are predicted by the previous word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 1Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractive. But we note that feedforward networks are just as viable. Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the followi</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language models. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning Deep Architectures for AI, volume Foundations and Trends in Machine Learning.</title>
<date>2009</date>
<publisher>NOW Publishers.</publisher>
<contexts>
<context position="5444" citStr="Bengio (2009)" startWordPosition="853" endWordPosition="854">, model word probabilities using continuous vector representations. Figure 1 shows a type of neural LMs called recurrent neural networks (Mikolov et al., 2011b).1 Rather than representing context as an identity (n-gram hit-or-miss) function on [w(t − 1), w(t − 2), ...], neural LMs summarize the context by a hidden state vector s(t). This is a continuous vector of dimension |S |whose elements are predicted by the previous word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 1Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractiv</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Yoshua Bengio. 2009. Learning Deep Architectures for AI, volume Foundations and Trends in Machine Learning. NOW Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>Wit3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>261--268</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="7547" citStr="Cettolo et al., 2012" startWordPosition="1208" endWordPosition="1211">ime” method.2 Intuitively, U and A compress the context (|S |&lt; |W |) such that contexts predictive of the same word w(t) are close together. Since proper modeling of unknown contexts is important in our problem, training text for both ngram and neural LM is pre-processed by converting all low-frequency words in the training data (frequency=1 in our case) to a special ”unknown” token. This is used only in Eq. 1 for selecting general-domain sentences; these words retain their surface forms in the SMT train pipeline. 3 Experiment Setup We experimented with four language pairs in the WITS corpus (Cettolo et al., 2012), with English (en) as source and German (de), Spanish (es), French (fr), Russian (ru) as target. This is the in-domain corpus, and consists of TED Talk transcripts covering topics in technology, entertainment, and design. As general-domain corpora, we collected bitext from the WMT2013 campaign, including CommonCrawl and NewsCommentary for all 4 languages, Europarl for de/es/fr, UN for es/fr, Gigaword for fr, and Yandex for ru. The indomain data is divided into a training set (for SMT 2The recurrent states are unrolled for several time-steps, then stochastic gradient descent is applied. 679 en</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit3: Web inventory of transcribed and translated talks. In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT), pages 261–268, Trento, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Analysis of translation model adaptation for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT)</booktitle>
<tech>Technical Papers Track.</tech>
<contexts>
<context position="2021" citStr="Duh et al., 2010" startWordPosition="296" endWordPosition="299">a and can sometimes substantially outperform conventional n-grams. 1 Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). Although previous works in data selection (Axelrod et al., 2011; Koehn and Haddow, 2012; Yasuda et al., 2008) have shown substantial gains, we suspect that the commonly-used n-gram LMs may be sub-optimal. The small size of the in-domain text implies that a large percentage of generaldomain sentences will contain words not observed in the LM training data. In fact, as many as 60% of general-domain sentences contain at least one unknown word in our experiments. Although the LM probabilities of these sentences could still be computed by resorting to back-off and other s</context>
</contexts>
<marker>Duh, Sudoh, Tsukada, 2010</marker>
<rawString>Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Analysis of translation model adaptation for statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT) - Technical Papers Track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="10083" citStr="Foster et al., 2010" startWordPosition="1598" endWordPosition="1601"> for es, de, ru.). 3The original data are provided by http://wit3.fbk.eu and http://www.statmt.org/wmt13/. Our domain adaptation scenario is similar to the IWSLT2012 campaign but we used our own random train/test splits, since we wanted to ensure the testset for all languages had identical source sentences for comparison purposes. For replicability, our software is available at http://cl.naist.jp/∼kevinduh/a/acl2013. 4More advanced phrase table adaptation methods are possible. but our interest is in comparing data selection methods. The conclusions should transfer to advanced methods such as (Foster et al., 2010; Niehues and Waibel, 2012). 5http://www.fit.vutbr.cz/∼imikolov/rnnlm/ 4.1 LM Perplexity and Training Time First, we measured perplexity to check the generalization ability of our neural LMs as language models. Recall that we train four LMs to compute each of the components of Eq. 1. In Table 2, we compared each of the four versions of ngram, neuralnet, and combine LMs on in-domain test sets or general-domain held-out sets. It re-affirms previous positive results (Mikolov et al., 2011a), with neuralnet outperforming ngram by 20-30% perplexity across all tasks. Also, combine slightly improves t</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Analysing the effect of out-of-domain data on smt systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>422--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="2046" citStr="Haddow and Koehn, 2012" startWordPosition="300" endWordPosition="304">s substantially outperform conventional n-grams. 1 Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). Although previous works in data selection (Axelrod et al., 2011; Koehn and Haddow, 2012; Yasuda et al., 2008) have shown substantial gains, we suspect that the commonly-used n-gram LMs may be sub-optimal. The small size of the in-domain text implies that a large percentage of generaldomain sentences will contain words not observed in the LM training data. In fact, as many as 60% of general-domain sentences contain at least one unknown word in our experiments. Although the LM probabilities of these sentences could still be computed by resorting to back-off and other smoothing techniques, a na</context>
</contexts>
<marker>Haddow, Koehn, 2012</marker>
<rawString>Barry Haddow and Philipp Koehn. 2012. Analysing the effect of out-of-domain data on smt systems. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 422–432, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Automatic evaluation of translation quality for distant language pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>944--952</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="12661" citStr="Isozaki et al., 2010" startWordPosition="2056" endWordPosition="2059">, without additional tuning. 129k 2.5M 26k 42k #sentence #token (en) #vocab (en) #vocab (f) In-domain Training Set 139k 2.7M 27k 34k 117k 2.3M 25k 58k 140k 2.7M 27k 39k General-domain Bitext #sentence 4.4M 14.7M 38.9M #token (en) 113M 385M 1012M %unknown 60% 58% 64% 2.0M 51M 65% 680 |S |Cluster Time Perplexity 200 100 198m 110 100 |W |12915m 110 200 400 208m 113 100 100 52m 118 100 400 71m 120 Table 3: Training time (in minutes) for various neural LM architectures (Task: en-de de). 4.2 End-to-end SMT Evaluation Table 4 shows translation results in terms of BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), and TER (Snover et al., 2006). We observe that all three data selection methods essentially outperform alldata and indata for all language pairs, and neuralnet tend to be the best in all metrics. E.g., BLEU improvements over ngram are in the range of 0.4 for en-de, 0.5 for en-es, 0.1 for en-fr, and 1.7 for en-ru. Although not all improvements are large in absolute terms, many are statistically significant (95% confidence). We therefore believe that neural LMs are generally worthwhile to try for data selection, as it rarely underperform n-grams. The open question is: what can explain the sign</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, 2010</marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic evaluation of translation quality for distant language pairs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944–952, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Towards effective use of training data in statistical machine translation.</title>
<date>2012</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="2135" citStr="Koehn and Haddow, 2012" startWordPosition="316" endWordPosition="319">uilding Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). Although previous works in data selection (Axelrod et al., 2011; Koehn and Haddow, 2012; Yasuda et al., 2008) have shown substantial gains, we suspect that the commonly-used n-gram LMs may be sub-optimal. The small size of the in-domain text implies that a large percentage of generaldomain sentences will contain words not observed in the LM training data. In fact, as many as 60% of general-domain sentences contain at least one unknown word in our experiments. Although the LM probabilities of these sentences could still be computed by resorting to back-off and other smoothing techniques, a natural question remains: will alternative, more robust LMs do better? We hypothesize that </context>
</contexts>
<marker>Koehn, Haddow, 2012</marker>
<rawString>Philipp Koehn and Barry Haddow. 2012. Towards effective use of training data in statistical machine translation. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="15137" citStr="Koehn, 2004" startWordPosition="2452" endWordPosition="2453">0 alldata 21.5 80.1 59.1 ngram 21.5 80.3 58.9 neuralnet 21.9+ 80.5+ 58.4 combine 21.5 80.2 58.8 en-es indata 30.4 83.5 48.7 alldata 31.2 83.2 49.9 ngram 32.0 83.7 48.4 neuralnet 32.5+ 83.7 48.3+ combine 32.5+ 83.8 48.3+ en-fr indata 31.4 83.9 51.2 alldata 31.5 83.5 51.4 ngram 32.7 83.7 50.4 neuralnet 32.8 84.2+ 50.3 combine 32.5 84.0 50.5 en-ru indata 14.8 72.5 69.5 alldata 23.4 75.0 62.3 ngram 24.0 75.7 61.4 neuralnet 25.7+ 76.1 60.0+ combine 23.7 75.9 61.9− Table 4: End-to-end Translation Results. The best results are bold-faced. We also compare neural LMs to ngram using pairwise bootstrap (Koehn, 2004): ”+” means statistically significant improvement and ”−” means significant degradation. net. They share 60-75% of the augmented training data. This high overlap means that ngram and neuralnet are actually not drastically different systems, and neuralnet with its slightly better selections represent an incremental improvement.6 5 Conclusions We perform an evaluation of neural LMs for adaptation data selection, based on the hypothesis that their continuous vector representations are effective at comparing general-domain sentences, which contain frequent unknown words. Compared to conventional n</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>I Oparin</author>
<author>A Allauzen</author>
<author>J Gauvain</author>
<author>F Yvon</author>
</authors>
<title>Structured output layer neural network language model.</title>
<date>2011</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,</booktitle>
<pages>5524--5527</pages>
<contexts>
<context position="5791" citStr="Le et al., 2011" startWordPosition="904" endWordPosition="907"> continuous vector of dimension |S |whose elements are predicted by the previous word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 1Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractive. But we note that feedforward networks are just as viable. Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0(t), ... , wk(t),... w|W|(t)] (2) ⎛ ⎞ X |S| wk(t) = g ⎝ sj(t)Vkj ⎠ (3) j=0 |W ||S| sj(t) = fX wi(t − 1)Uji + X si/(t − 1)Aji (i=0 i/=0 </context>
</contexts>
<marker>Le, Oparin, Allauzen, Gauvain, Yvon, 2011</marker>
<rawString>Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and F. Yvon. 2011. Structured output layer neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5524–5527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´a˘s Mikolov</author>
<author>Anoop Deoras</author>
<author>Daniel Povey</author>
<author>Luk´a˘s Burget</author>
<author>Jan ˘Cernock´y</author>
</authors>
<title>Strategies for training large scale neural network language model.</title>
<date>2011</date>
<booktitle>In ASRU.</booktitle>
<marker>Mikolov, Deoras, Povey, Burget, ˘Cernock´y, 2011</marker>
<rawString>Tom´a˘s Mikolov, Anoop Deoras, Daniel Povey, Luk´a˘s Burget, and Jan ˘Cernock´y. 2011a. Strategies for training large scale neural network language model. In ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´a˘s Mikolov</author>
<author>Stefan Kombrink</author>
<author>Luk´a˘s Burget</author>
<author>Jan ˇCernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<marker>Mikolov, Kombrink, Burget, ˇCernock´y, Khudanpur, 2011</marker>
<rawString>Tom´a˘s Mikolov, Stefan Kombrink, Luk´a˘s Burget, Jan ˇCernock´y, and Sanjeev Khudanpur. 2011b. Extensions of recurrent neural network language model. In Proceedings of the 2011 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>220--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3370" citStr="Moore and Lewis, 2010" startWordPosition="518" endWordPosition="521">nguage model (Bengio et al., 2003) is a viable alternative, since its continuous vector representation of words is well-suited for modeling sentences with frequent unknown words, providing smooth probability estimates of unseen but similar contexts. Neural LMs have achieved positive results in speech recognition and SMT reranking (Schwenk et al., 2012; Mikolov et al., 2011a). To the best of our knowledge, this paper is the first work that examines neural LMs for adaptation data selection. 2 Data Selection Method We employ the data selection method of (Axelrod et al., 2011), which builds upon (Moore and Lewis, 2010). The intuition is to select general-domain sentences that are similar to indomain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an generaldomain sentence pair (e, f) as: [INE(e) − GENE(e)] + [INF(f) − GENF(f)] (1) where INE(e) is the length-normalized crossentropy of e on the English in-domain LM. GENE(e) is the length-normalized cross-entropy 678 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678–683, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure </context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, pages 220–224, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masami Nakamura</author>
<author>Katsuteru Maruyama</author>
<author>Takeshi Kawabata</author>
<author>Kiyohiro Shikano</author>
</authors>
<title>Neural network approach to word category prediction for english texts.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th conference on Computational linguistics - Volume 3, COLING ’90,</booktitle>
<pages>213--218</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5671" citStr="Nakamura et al., 1990" startWordPosition="884" endWordPosition="887"> hit-or-miss) function on [w(t − 1), w(t − 2), ...], neural LMs summarize the context by a hidden state vector s(t). This is a continuous vector of dimension |S |whose elements are predicted by the previous word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 1Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractive. But we note that feedforward networks are just as viable. Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0(t), ... , wk(t</context>
</contexts>
<marker>Nakamura, Maruyama, Kawabata, Shikano, 1990</marker>
<rawString>Masami Nakamura, Katsuteru Maruyama, Takeshi Kawabata, and Kiyohiro Shikano. 1990. Neural network approach to word category prediction for english texts. In Proceedings of the 13th conference on Computational linguistics - Volume 3, COLING ’90, pages 213–218, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Detailed analysis of different strategies for phrase table adaptation in SMT.</title>
<date>2012</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="10110" citStr="Niehues and Waibel, 2012" startWordPosition="1602" endWordPosition="1605">he original data are provided by http://wit3.fbk.eu and http://www.statmt.org/wmt13/. Our domain adaptation scenario is similar to the IWSLT2012 campaign but we used our own random train/test splits, since we wanted to ensure the testset for all languages had identical source sentences for comparison purposes. For replicability, our software is available at http://cl.naist.jp/∼kevinduh/a/acl2013. 4More advanced phrase table adaptation methods are possible. but our interest is in comparing data selection methods. The conclusions should transfer to advanced methods such as (Foster et al., 2010; Niehues and Waibel, 2012). 5http://www.fit.vutbr.cz/∼imikolov/rnnlm/ 4.1 LM Perplexity and Training Time First, we measured perplexity to check the generalization ability of our neural LMs as language models. Recall that we train four LMs to compute each of the components of Eq. 1. In Table 2, we compared each of the four versions of ngram, neuralnet, and combine LMs on in-domain test sets or general-domain held-out sets. It re-affirms previous positive results (Mikolov et al., 2011a), with neuralnet outperforming ngram by 20-30% perplexity across all tasks. Also, combine slightly improves the perplexity of neuralnet.</context>
</contexts>
<marker>Niehues, Waibel, 2012</marker>
<rawString>Jan Niehues and Alex Waibel. 2012. Detailed analysis of different strategies for phrase table adaptation in SMT. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="12631" citStr="Papineni et al., 2002" startWordPosition="2051" endWordPosition="2054"> in this paper use this setting, without additional tuning. 129k 2.5M 26k 42k #sentence #token (en) #vocab (en) #vocab (f) In-domain Training Set 139k 2.7M 27k 34k 117k 2.3M 25k 58k 140k 2.7M 27k 39k General-domain Bitext #sentence 4.4M 14.7M 38.9M #token (en) 113M 385M 1012M %unknown 60% 58% 64% 2.0M 51M 65% 680 |S |Cluster Time Perplexity 200 100 198m 110 100 |W |12915m 110 200 400 208m 113 100 100 52m 118 100 400 71m 120 Table 3: Training time (in minutes) for various neural LM architectures (Task: en-de de). 4.2 End-to-end SMT Evaluation Table 4 shows translation results in terms of BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), and TER (Snover et al., 2006). We observe that all three data selection methods essentially outperform alldata and indata for all language pairs, and neuralnet tend to be the best in all metrics. E.g., BLEU improvements over ngram are in the range of 0.4 for en-de, 0.5 for en-es, 0.1 for en-fr, and 1.7 for en-ru. Although not all improvements are large in absolute terms, many are statistically significant (95% confidence). We therefore believe that neural LMs are generally worthwhile to try for data selection, as it rarely underperform n-grams. The open question</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Anthony Rousseau</author>
<author>Mohammed Attik</author>
</authors>
<title>Large, pruned or continuous space language models on a gpu for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT</booktitle>
<pages>11--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="3101" citStr="Schwenk et al., 2012" startWordPosition="470" endWordPosition="473">known word in our experiments. Although the LM probabilities of these sentences could still be computed by resorting to back-off and other smoothing techniques, a natural question remains: will alternative, more robust LMs do better? We hypothesize that the neural language model (Bengio et al., 2003) is a viable alternative, since its continuous vector representation of words is well-suited for modeling sentences with frequent unknown words, providing smooth probability estimates of unseen but similar contexts. Neural LMs have achieved positive results in speech recognition and SMT reranking (Schwenk et al., 2012; Mikolov et al., 2011a). To the best of our knowledge, this paper is the first work that examines neural LMs for adaptation data selection. 2 Data Selection Method We employ the data selection method of (Axelrod et al., 2011), which builds upon (Moore and Lewis, 2010). The intuition is to select general-domain sentences that are similar to indomain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an generaldomain sentence pair (e, f) as: [INE(e) − GENE(e)] + [INF(f) − GENF(f)] (1) where INE(e) is the length-normalized crossentropy of e on th</context>
</contexts>
<marker>Schwenk, Rousseau, Attik, 2012</marker>
<rawString>Holger Schwenk, Anthony Rousseau, and Mohammed Attik. 2012. Large, pruned or continuous space language models on a gpu for statistical machine translation. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 11–19, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Comput. Speech Lang.,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="5647" citStr="Schwenk, 2007" startWordPosition="882" endWordPosition="883">dentity (n-gram hit-or-miss) function on [w(t − 1), w(t − 2), ...], neural LMs summarize the context by a hidden state vector s(t). This is a continuous vector of dimension |S |whose elements are predicted by the previous word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 1Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractive. But we note that feedforward networks are just as viable. Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Comput. Speech Lang., 21(3):492–518, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="12692" citStr="Snover et al., 2006" startWordPosition="2062" endWordPosition="2065">k 2.5M 26k 42k #sentence #token (en) #vocab (en) #vocab (f) In-domain Training Set 139k 2.7M 27k 34k 117k 2.3M 25k 58k 140k 2.7M 27k 39k General-domain Bitext #sentence 4.4M 14.7M 38.9M #token (en) 113M 385M 1012M %unknown 60% 58% 64% 2.0M 51M 65% 680 |S |Cluster Time Perplexity 200 100 198m 110 100 |W |12915m 110 200 400 208m 113 100 100 52m 118 100 400 71m 120 Table 3: Training time (in minutes) for various neural LM architectures (Task: en-de de). 4.2 End-to-end SMT Evaluation Table 4 shows translation results in terms of BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), and TER (Snover et al., 2006). We observe that all three data selection methods essentially outperform alldata and indata for all language pairs, and neuralnet tend to be the best in all metrics. E.g., BLEU improvements over ngram are in the range of 0.4 for en-de, 0.5 for en-es, 0.1 for en-fr, and 1.7 for en-ru. Although not all improvements are large in absolute terms, many are statistically significant (95% confidence). We therefore believe that neural LMs are generally worthwhile to try for data selection, as it rarely underperform n-grams. The open question is: what can explain the significant improvements in, for ex</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youzheng Wu</author>
<author>Xugang Lu</author>
<author>Hitoshi Yamamoto</author>
<author>Shigeki Matsuda</author>
<author>Chiori Hori</author>
<author>Hideki Kashioka</author>
</authors>
<title>Factored language model based on recurrent neural network.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2835--2850</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="5848" citStr="Wu et al., 2012" startWordPosition="915" endWordPosition="918">dicted by the previous word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 1Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractive. But we note that feedforward networks are just as viable. Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0(t), ... , wk(t),... w|W|(t)] (2) ⎛ ⎞ X |S| wk(t) = g ⎝ sj(t)Vkj ⎠ (3) j=0 |W ||S| sj(t) = fX wi(t − 1)Uji + X si/(t − 1)Aji (i=0 i/=0 (4) Here, w(t) is viewed as a vector of dimension |W |(vo</context>
</contexts>
<marker>Wu, Lu, Yamamoto, Matsuda, Hori, Kashioka, 2012</marker>
<rawString>Youzheng Wu, Xugang Lu, Hitoshi Yamamoto, Shigeki Matsuda, Chiori Hori, and Hideki Kashioka. 2012. Factored language model based on recurrent neural network. In Proceedings of COLING 2012, pages 2835–2850, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Yasuda</author>
<author>Ruiqiang Zhang</author>
<author>Hirofumi Yamamoto</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Method of selecting training data to build a compact and efficient translation model.</title>
<date>2008</date>
<booktitle>In ICJNLP.</booktitle>
<contexts>
<context position="2157" citStr="Yasuda et al., 2008" startWordPosition="320" endWordPosition="324">ine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). Although previous works in data selection (Axelrod et al., 2011; Koehn and Haddow, 2012; Yasuda et al., 2008) have shown substantial gains, we suspect that the commonly-used n-gram LMs may be sub-optimal. The small size of the in-domain text implies that a large percentage of generaldomain sentences will contain words not observed in the LM training data. In fact, as many as 60% of general-domain sentences contain at least one unknown word in our experiments. Although the LM probabilities of these sentences could still be computed by resorting to back-off and other smoothing techniques, a natural question remains: will alternative, more robust LMs do better? We hypothesize that the neural language mo</context>
</contexts>
<marker>Yasuda, Zhang, Yamamoto, Sumita, 2008</marker>
<rawString>Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, and Eiichiro Sumita. 2008. Method of selecting training data to build a compact and efficient translation model. In ICJNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>