<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021157">
<title confidence="0.863182">
SATTY : Word Sense Induction Application in Web Search Clustering*
</title>
<author confidence="0.948095">
Satyabrata Behera
</author>
<affiliation confidence="0.847157">
IIT Bombay
Mumbai, India
</affiliation>
<email confidence="0.974064">
satty@cse.iitb.ac.in
</email>
<author confidence="0.977858">
Ramakrishna Bairi
</author>
<affiliation confidence="0.854538">
IIT Bombay
Mumbai, India
</affiliation>
<email confidence="0.990475">
bairi@cse.iitb.ac.in
</email>
<author confidence="0.937659">
Upasana Gaikwad
</author>
<affiliation confidence="0.83812">
IIT Bombay
Mumbai, India
</affiliation>
<email confidence="0.971143">
upasana@cse.iitb.ac.in
</email>
<author confidence="0.981215">
Ganesh Ramakrishnan
</author>
<affiliation confidence="0.8538505">
IIT Bombay
Mumbai, India
</affiliation>
<email confidence="0.994913">
ganesh@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.995583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998055125">
The aim of this paper is to perform Word
Sense induction (WSI); which clusters web
search results and produces a diversified list of
search results. It describes the WSI system de-
veloped for Task 11 of SemEval - 2013. This
paper implements the idea of monotone sub-
modular function optimization using greedy
algorithm.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953882352941">
Two different types of systems were submitted un-
der Task 11 of SemEval - 2013 (Roberto Navigli and
Daniele Vannella, 2013). The two system types are
WSI (Word Sense Induction) and WSD (Word Sense
Disambiguation). WSD is the task of automatically
associating meaning with words. In WSD the pos-
sible meanings for a given word are drawn from an
existing sense inventory. In contrast, WSI aims at
automatically identifying the meanings of a given
word from raw text. A WSI system will be asked to
identify the meaning of the input query and cluster
the search results into semantically-related groups
according to their meanings. Instead, a WSD sys-
tem will be requested to sense-tag the above search
results with the appropriate senses of the input query
and this, again, will implicitly determine a clustering
of snippets (i.e., one cluster per sense).
</bodyText>
<footnote confidence="0.961939">
*This system was designed and submitted in the com-
petition SemEval-2013 under task 11 : Evaluating Word
Sense Induction &amp; Disambiguation within An End-User
Application (Roberto Navigli and Daniele Vannella2013).
http://www.cs.york.ac.uk/semeval-2013/.
</footnote>
<bodyText confidence="0.999662965517241">
Our system implements the idea given in (Jin-
grui He and Hanghang Tong and Qiaozhu Mei and
Boleslaw Szymanski, 2012). This developed sys-
tem uses the concept of submodularity. The task
is treated as a submodular function maximization
which has its benefits. On the one hand, there exists
a simple greedy algorithm for monotone submod-
ular function maximization where the solution ob-
tained is guaranteed to be almost as good as the best
possible solution according to an objective. More
precisely, the greedy algorithm is a constant factor
approximation to the cardinality constrained version
of the problem, so that the approximate soultion is
in the bound of (1 − 1/e) of optimal solution. It is
also important to note that this is a worst case bound,
and in most cases the quality of the solution ob-
tained will be much better than this bound suggests.
In our system, monotone submodular objective of
(Jingrui He and Hanghang Tong and Qiaozhu Mei
and Boleslaw Szymanski, 2012) was implemented
to find the top k simultaneously relevant and diver-
sified list of search results. Once these top k results
are obtained, they are used as centroids to form clus-
ters by classifying each of remaining search results
to one of the centroid with maximum similarity, pro-
ducing k clusters. Those results which are not simi-
lar to any of the centroids are either put in a different
cluster or are assigned to cluster with highest simi-
larity.
</bodyText>
<sectionHeader confidence="0.810989" genericHeader="method">
2 Background on Submodularity
</sectionHeader>
<bodyText confidence="0.922551666666667">
Our system uses the concept of submodularity.
Given a set of objects V = v1, ..., v,,, and a function
F : 2V —* R that returns a real value for any subset
</bodyText>
<page confidence="0.952027">
207
</page>
<bodyText confidence="0.992681142857143">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 207–211, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
S C_ V . The function F is said to be submodular if
it satisfies the property of diminishing returns, i.e.,
A C_ B C_ V \ v, a submodular function F must sat-
isfy F(A + v) — F(A) &gt; F(B + v) — F(B). A
set function F is monotone nondecreasing if VA C_
B, F(A) &gt; F(B). A monotone nondecreasing sub-
modular function is referred to as monotone sub-
modular.
We need to find the subset of bounded size
|S |&lt; k that maximizes the function F, e.g.
argmaxScV F(S). In general, this operation is in-
tractable. As shown in (G.L. Nemhauser and L.A.
Wolsey, 1978), if function F is monotone submod-
ular, then a simple greedy algorithm finds an ap-
proximate solution which is guaranteed to be within
(1-1/e) — 0.63 of optimal solution. Many proper-
ties of submodular functions are common with con-
vex and concave functions (L. Lovasz, 1983). One
of those is that they are closed under a number of
common combination operations such as summa-
tion, certain compositions, restrictions etc.
Previous work on submodularity is in (Hui Lin
and Jeff Bilmes, 2011) where a monotone submodu-
lar objective is maximized using a greedy algorithm
for document summarization. The objective func-
tion is:
</bodyText>
<equation confidence="0.982169">
F(S) = L(S) + AR(S)
</equation>
<bodyText confidence="0.999978076923077">
where L(S) measures the coverage of summary
set S to the document V and R(S) measures di-
versity in S, which are properties of a good sum-
mary. A &gt; 0 is trade-off coefficient. V represents
all the sentences (or other linguistic units) in a docu-
ment (or document collection). Also L(S) and R(S)
are monotone submodular functions. This work was
again extended in (Hui Lin and Jeff A. Bilmes, 2012)
where the submodular objective is itself a weighted
combination of several submodular functions, where
the weights are learnt in a max-margin setting. This
work also demonstrates the use of this idea for doc-
ument summarization.
</bodyText>
<sectionHeader confidence="0.992049" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.971725">
The system works in 2 stages:
1. The first stage produces top k diversified and
relevant set of search results.
2. The second stage forms k clusters of search re-
sults treating top k results as centroids.
The problem of finding top k diversified and rele-
vant search results is posed as an optimization prob-
lem. This optimization function has the property
of diminishing returns and monotonicity, which is
a monotone submodular function. This enables
to design a scalable, greedy algorithm to find the
(1 — 1/e) near-optimal solution. The optimization
function is taken from (Jingrui He and Hanghang
Tong and Qiaozhu Mei and Boleslaw Szymanski,
2012) and presented below.
Objective Function : The aim is to find a subset
T of k search results which optimizes the objective
function.
</bodyText>
<equation confidence="0.9286065">
�argmax|T |=k w �qiri �
iET iJET
</equation>
<bodyText confidence="0.999684615384615">
where, T is the subset of search results. q = S.r is
a nx1 vector. Intuitively, its ith element qi measures
the importance of ith search result. To be specific,
if xi is similar to many search results that are highly
relevant to the query, it is more important than the
search results whose neighbours are less relevant. S
is a nxn similarity matrix between search results. r
is a nx1 relevance vector of search results to query.
w is a regularization parameter which defines trade-
off between two terms.
The first term of the objective function measures
the total weighted relevance of T with respect to
query. It favours relevant search results from big
clusters. In other words, if two search results are
equally relevant to the query, one from a big cluster
and the other isolated, by using weighted relevance,
it prefers the former.
The second term measures the similarity among
the search results within T such that it penalizes the
selection of multiple relevant search results that are
very similar to each other. By including this term in
the objective function, we try to find a set of search
results which are highly relevant to the query and
also dissimilar to each other.
As the objective function is monotone submodu-
lar, the greedy algorithm finds the top k search re-
</bodyText>
<equation confidence="0.449645">
riSi9r9
</equation>
<page confidence="0.99408">
208
</page>
<bodyText confidence="0.998196571428571">
sults (i.e. near optimal solution) with approximation
guarantee of (1 − 1/e).
The second stage performs clustering using re-
sults of previous stage. The top k search results out-
put by the previous stage are treated as centroids and
the remaining search results are assigned to the cen-
troid with the maximum similarity.
</bodyText>
<sectionHeader confidence="0.997143" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999946933333333">
The implemented system was tested on data given
by SemEval - 20131(Roberto Navigli and Daniele
Vannella, 2013). Data contains 100 queries, each
with 64 search results. Each search result contains
title, url and snippet.
Only title and snippet information was used. The
relevance between query and a search result is cal-
culated using weighted Jaccard. Cosine similarity is
used to calculate the similarity between search re-
sults using only title and snippet. It was just bag of
words (i.e. unigram) approach and no other prepro-
cessing of data was done. In the first stage, system
produces top 10 diversified search results which are
then used as centroids to form 10 clusters. Those
results which are not similar to any of the centroids
are put in a different cluster, sometimes resulting in
11 clusters.
The evaluation method required : (i) to rank the
search results within each cluster according to the
confidence with which they belong to that cluster,
(ii) to rank the clusters according to their diversity.
The cluster ranking is kept same as the rank of
their centroids in top 10 results returned in first stage
of the system.
Also search results within each cluster are then
ranked by their average similarity to rest of the
search results in the same cluster, in descending or-
der with respect to the ranking score. The ranking
score of search result xi in cluster C is calculated as
below, which is used in our system:
</bodyText>
<footnote confidence="0.9267225">
1http://www.cs.york.ac.uk/semeval-
2013/task11/index.php?id=data
</footnote>
<bodyText confidence="0.997909">
The other way of ranking search results within a
cluster can be ranking by their relevance to the
query. In that case, it depends on how good the
relevance scores are. This ranking affects the abil-
ity of the system to diversify search results, i.e.,
Subtopic Recall@K and Subtopic Precision@r mea-
sures. The clustering quality is measured by mea-
sures of Rand Index (RI), Adjusted Rand Index
(ARI), F1-measure (F1) and Jaccard Index (JI). All
these evaluation metrics used are described in (An-
tonio Di Marco and Roberto Navigli, 2013). All
the given evaluation metric values are obtained for
the described data using the java evaluator provided
by SemEval - 2013 (Roberto Navigli and Daniele
Vannella, 2013). Our system’s evaluation measures
along with other systems, submitted in SemEval -
2013 are shown in tables 1, 2 and 3. Our system’s
name is task11-satty-approach1.
The clustering quality was found to be good as
indicated by F1 and RI while scoring low for ARI,
JI. In terms of diversification of search results, it did
not perform that well indicating that either ranking
of search results within each cluster or cluster rank-
ing or both were not that good.
</bodyText>
<sectionHeader confidence="0.993911" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999334631578947">
In this paper Word Sense Induction was imple-
mented on web search clustering. The developed
system evaluated with respect to different evaluation
metrics. The system’s clustering quality was found
to be good while its ability to diversify search re-
sults was not that good. Better ranking of clusters as
well as ranking of search results within each cluster
can improve the system’s ability to diversify search
results.
The similarity score between search results were
calculated using only title and snippet, but it can also
be evaluated by fetching whole document. Since the
relevance score of each search result to the query
was not available, it was calculated by considering
occurrence frequency of query words in search re-
sults (i.e. title and snippet). If a better relevance
score were available by the search engine, the sys-
tem might have performed better. These two aspects
can be tested in further work.
</bodyText>
<figure confidence="0.960004142857143">
1
score(xi) =
|
|
Sij
C
− 1 jJC:C,i��j
</figure>
<page confidence="0.988879">
209
</page>
<table confidence="0.999743">
System Type F1 ARI RI Jaccard Avg. No. Avg.
of Cluster
Clusters Size
hdp-clusters-lemma WSI 0.683 0.2131 0.6522 0.3302 6.63 11.0756
hdp-clusters-nolemma WSI 0.6803 0.2149 0.6486 0.3375 6.54 11.6803
task11-satty-approach1 WSI 0.6709 0.0719 0.5955 0.1505 9.9 6.4631
task11-ukp-wsi-wp-pmi WSI 0.6048 0.0364 0.505 0.2932 5.86 30.3098
task11.duluth.sys7.pk2 WSI 0.5878 0.0678 0.5204 0.3103 3.01 25.1596
task11-ukp-wsi-wp-llr2 WSI 0.5864 0.0377 0.5109 0.3177 4.17 21.8702
task11-ukp-wsi-wacky- WSI 0.5826 0.0253 0.5002 0.3394 3.64 32.3434
llr
task11.duluth.sys9.pk2 WSI 0.5702 0.0259 0.5463 0.2224 3.32 19.84
task11.duluth.sys1.pk2 WSI 0.5683 0.0574 0.5218 0.3179 2.53 26.4533
rakesh WSD 0.3949 0.0811 0.5876 0.3052 9.07 2.9441
singleton 1.0000 0.0000 0.6009 0.0000 64.0000 1.0000
allinone 0.5442 0.0000 0.3990 0.3990 1.0000 64.0000
gold 1.0000 0.9900 1.0000 1.0000 7.6900 11.5630
</table>
<tableCaption confidence="0.9906625">
Table 1: The best result for each column is presented in boldface. singleton and allinone are baseline systems and
gold is the theoretical upper-bound for the task. WSI : Word Sense Induction, WSD : Word Sense Disambiguation
</tableCaption>
<table confidence="0.999965833333333">
System Type K=5 K=10 K=20 K=40 K=60
hdp-clusters-nolemma WSI 0.508 0.6321 0.7926 0.9248 0.9821
hdp-clusters-lemma WSI 0.4813 0.6551 0.7886 0.9168 0.9856
task11-ukp-wsi-wacky- WSI 0.4119 0.5541 0.6861 0.839 0.9691
llr
task11-ukp-wsi-wp-llr2 WSI 0.4107 0.5376 0.6887 0.8587 0.983
task11-ukp-wsi-wp-pmi WSI 0.4045 0.5625 0.687 0.8492 0.978
task11-satty-approach1 WSI 0.3897 0.489 0.6272 0.8214 0.9745
task11.duluth.sys7.pk2 WSI 0.3888 0.5379 0.7038 0.8623 0.9844
task11.duluth.sys9.pk2 WSI 0.3715 0.499 0.6891 0.8365 0.9734
task11.duluth.sys1.pk2 WSI 0.3711 0.5329 0.7124 0.8848 0.9849
rakesh WSD 0.4648 0.6236 0.7866 0.9072 0.9903
</table>
<tableCaption confidence="0.908094">
Table 2: S-recall@K for different values of K averaged over 100 queries.
</tableCaption>
<page confidence="0.858959">
210
</page>
<table confidence="0.99997575">
System Type r=0.5 r=0.6 r=0.7 r=0.8 r=0.9
hdp-clusters-lemma WSI 0.4885 0.4293 0.3519 0.2762 0.2376
hdp-clusters-nolemma WSI 0.4818 0.4388 0.3485 0.293 0.2485
task11-ukp-wsi-wp-pmi WSI 0.4283 0.334 0.2663 0.2292 0.2039
task11-ukp-wsi-wacky- WSI 0.4247 0.3173 0.2539 0.2271 0.1849
llr
task11-ukp-wsi-wp-llr2 WSI 0.4206 0.3204 0.2657 0.2241 0.1858
task11.duluth.sys1.pk2 WSI 0.4008 0.3131 0.2673 0.2451 0.2177
task11.duluth.sys7.pk2 WSI 0.3911 0.3042 0.2654 0.2343 0.1995
task11.duluth.sys9.pk2 WSI 0.359 0.2972 0.2526 0.2126 0.1951
task11-satty-approach1 WSI 0.3494 0.2688 0.2355 0.204 0.1736
rakesh WSD 0.48 0.3904 0.3272 0.2792 0.2394
</table>
<tableCaption confidence="0.995738">
Table 3: S-precision@r for different values of r averaged over 100 queries.
</tableCaption>
<sectionHeader confidence="0.997343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998753433333333">
Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. The 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), Portland, OR, June.
Hui Lin and Jeff A Bilmes. 2012. Learning mixtures of
submodular shells with application to document sum-
marization. arXiv preprint arXiv:1210.4871.
Jingrui He and Hanghang Tong and Qiaozhu Mei and
Boleslaw Szymanski. 2012. GenDeR: A Generic Di-
versified Ranking Algorithm. Advances in Neural In-
formation Processing Systems 25.
Antonio Di Marco and Roberto Navigli. 2013. Cluster-
ing and Diversifying Web Search Results with Graph-
Based Word Sense Induction. Computational Linguis-
tics, 39(4), MIT Press.
Roberto Navigli and Daniele Vannella. 2013. SemEval-
2013 Task 11: Evaluating Word Sense Induction Dis-
ambiguation within An End-User Application. Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013), in conjunction with
the Second Joint Conference on Lexical and Compu-
tational Semantcis (*SEM 2013), Atlanta, USA, 2013.
L. Lovasz. 1983. Submodular functions and convexity.
Mathematical programming-The state of the art,(eds.
A. Bachem, M. Grotschel and B. Korte) Springer,
pages 235257.
G.L. Nemhauser and L.A. Wolsey. 1978 An analysis of
approximations for maximizing submodular set func-
tions I. Mathematical Programming, 14(1):265294.
</reference>
<page confidence="0.998779">
211
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.015545">
<title confidence="0.988857">Word Sense Induction Application in Web Search</title>
<author confidence="0.874946">Satyabrata</author>
<affiliation confidence="0.985666">IIT</affiliation>
<address confidence="0.485684">Mumbai,</address>
<email confidence="0.550292">Ramakrishna</email>
<affiliation confidence="0.971742">IIT</affiliation>
<address confidence="0.701506">Mumbai,</address>
<email confidence="0.978079">bairi@cse.iitb.ac.in</email>
<author confidence="0.375521">Upasana</author>
<affiliation confidence="0.8602875">IIT Mumbai,</affiliation>
<author confidence="0.509066">Ganesh</author>
<affiliation confidence="0.981205">IIT</affiliation>
<address confidence="0.709234">Mumbai,</address>
<email confidence="0.993824">ganesh@cse.iitb.ac.in</email>
<abstract confidence="0.986812888888889">The aim of this paper is to perform Word Sense induction (WSI); which clusters web search results and produces a diversified list of search results. It describes the WSI system developed for Task 11 of SemEval - 2013. This implements the idea of suboptimization using greedy algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT),</booktitle>
<location>Portland, OR,</location>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT), Portland, OR, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff A Bilmes</author>
</authors>
<title>Learning mixtures of submodular shells with application to document summarization. arXiv preprint arXiv:1210.4871.</title>
<date>2012</date>
<marker>Lin, Bilmes, 2012</marker>
<rawString>Hui Lin and Jeff A Bilmes. 2012. Learning mixtures of submodular shells with application to document summarization. arXiv preprint arXiv:1210.4871.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingrui He</author>
<author>Hanghang Tong</author>
<author>Qiaozhu Mei</author>
<author>Boleslaw Szymanski</author>
</authors>
<title>GenDeR: A Generic Diversified Ranking Algorithm.</title>
<date>2012</date>
<booktitle>Advances in Neural Information Processing Systems 25.</booktitle>
<marker>He, Tong, Mei, Szymanski, 2012</marker>
<rawString>Jingrui He and Hanghang Tong and Qiaozhu Mei and Boleslaw Szymanski. 2012. GenDeR: A Generic Diversified Ranking Algorithm. Advances in Neural Information Processing Systems 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Di Marco</author>
<author>Roberto Navigli</author>
</authors>
<title>Clustering and Diversifying Web Search Results with GraphBased Word Sense Induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<publisher>MIT Press.</publisher>
<marker>Di Marco, Navigli, 2013</marker>
<rawString>Antonio Di Marco and Roberto Navigli. 2013. Clustering and Diversifying Web Search Results with GraphBased Word Sense Induction. Computational Linguistics, 39(4), MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Daniele Vannella</author>
</authors>
<title>SemEval2013 Task 11: Evaluating Word Sense Induction Disambiguation within An End-User Application.</title>
<date>2013</date>
<booktitle>Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantcis (*SEM 2013),</booktitle>
<location>Atlanta, USA,</location>
<marker>Navigli, Vannella, 2013</marker>
<rawString>Roberto Navigli and Daniele Vannella. 2013. SemEval2013 Task 11: Evaluating Word Sense Induction Disambiguation within An End-User Application. Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantcis (*SEM 2013), Atlanta, USA, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lovasz</author>
</authors>
<title>Submodular functions and convexity. Mathematical programming-The state of the art,(eds.</title>
<date>1983</date>
<pages>235257</pages>
<editor>A. Bachem, M. Grotschel and B. Korte</editor>
<publisher>Springer,</publisher>
<contexts>
<context position="4451" citStr="Lovasz, 1983" startWordPosition="731" endWordPosition="732">unction F is monotone nondecreasing if VA C_ B, F(A) &gt; F(B). A monotone nondecreasing submodular function is referred to as monotone submodular. We need to find the subset of bounded size |S |&lt; k that maximizes the function F, e.g. argmaxScV F(S). In general, this operation is intractable. As shown in (G.L. Nemhauser and L.A. Wolsey, 1978), if function F is monotone submodular, then a simple greedy algorithm finds an approximate solution which is guaranteed to be within (1-1/e) — 0.63 of optimal solution. Many properties of submodular functions are common with convex and concave functions (L. Lovasz, 1983). One of those is that they are closed under a number of common combination operations such as summation, certain compositions, restrictions etc. Previous work on submodularity is in (Hui Lin and Jeff Bilmes, 2011) where a monotone submodular objective is maximized using a greedy algorithm for document summarization. The objective function is: F(S) = L(S) + AR(S) where L(S) measures the coverage of summary set S to the document V and R(S) measures diversity in S, which are properties of a good summary. A &gt; 0 is trade-off coefficient. V represents all the sentences (or other linguistic units) i</context>
</contexts>
<marker>Lovasz, 1983</marker>
<rawString>L. Lovasz. 1983. Submodular functions and convexity. Mathematical programming-The state of the art,(eds. A. Bachem, M. Grotschel and B. Korte) Springer, pages 235257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G L Nemhauser</author>
<author>L A Wolsey</author>
</authors>
<title>An analysis of approximations for maximizing submodular set functions I.</title>
<date>1978</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>14</volume>
<issue>1</issue>
<marker>Nemhauser, Wolsey, 1978</marker>
<rawString>G.L. Nemhauser and L.A. Wolsey. 1978 An analysis of approximations for maximizing submodular set functions I. Mathematical Programming, 14(1):265294.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>