<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000801">
<title confidence="0.9989825">
Optimizing Endpointing Thresholds using Dialogue
Features in a Spoken Dialogue System
</title>
<author confidence="0.6901">
Antoine Raux and Maxine Eskenazi
</author>
<email confidence="0.856774">
{antoine,max}@cs.cmu.edu
</email>
<affiliation confidence="0.89427">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<sectionHeader confidence="0.98665" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949823529412">
This paper describes a novel algorithm to dy-
namically set endpointing thresholds based on
a rich set of dialogue features to detect the end
of user utterances in a dialogue system. By
analyzing the relationship between silences in
user’s speech to a spoken dialogue system and
a wide range of automatically extracted fea-
tures from discourse, semantics, prosody, tim-
ing and speaker characteristics, we found that
all features correlate with pause duration and
with whether a silence indicates the end of the
turn, with semantics and timing being the most
informative. Based on these features, the pro-
posed method reduces latency by up to 24%
over a fixed threshold baseline. Offline evalu-
ation results were confirmed by implementing
the proposed algorithm in the Let’s Go system.
</bodyText>
<sectionHeader confidence="0.999638" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.99808">
1.1 Responsiveness in Dialogue
</subsectionHeader>
<bodyText confidence="0.999967285714286">
Although the quality of speech technologies has im-
proved drastically and spoken interaction with ma-
chines is becoming a part of the everyday life of
many people, dialogues with artificial agents still
fall far short of their human counterpart in terms of
both comfort and efficiency. Besides lingering prob-
lems in speech recognition and understanding, Ward
et al (Ward et al., 2005) identified turn-taking is-
sues, specifically responsiveness, as important short-
comings. Dialogues with artificial agents are typi-
cally rigid, following a strict one-speaker-at-a-time
structure with significant latencies between turns.
In a previous paper, we concurred with these find-
ings when analyzing issues with the Let’s Go system
</bodyText>
<page confidence="0.844922">
1
</page>
<bodyText confidence="0.999478823529412">
(Raux et al., 2006). In contrast, empirical studies
of conversation have shown that human-human dia-
logues commonly feature swift exchanges with lit-
tle or no gap between turns, or even non-disruptive
overlap (Jaffe and Feldstein, 1970; Sacks et al.,
1974). According to Conversation Analysis and
psycholinguistic studies, responsiveness in human
conversations is possible because participants in the
conversation exchange cues indicating when a turn
might end, and are able to anticipate points at which
they can take over the floor smoothly. Much re-
search has been devoted to finding these cues, lead-
ing to the identification of many aspects of language
and dialogue that relate to turn-taking behavior, in-
cluding syntax (Sacks et al., 1974; Ford and Thomp-
son, 1996; Furo, 2001), prosody (Duncan, 1972;
Orestr¨om, 1983; Chafe, 1992; Ford and Thompson,
1996; Koiso et al., 1998; Furo, 2001), and seman-
tics (Orestr¨om, 1983; Furo, 2001). However, re-
garding this last aspect, Orestrom notes about his
corpus that ”there is no simple way to formaliz-
ing a semantic analysis of this conversational mate-
rial”. This difficulty in formalizing higher levels of
conversation might explain the relatively low inter-
est that conversational analysts have had in seman-
tics and discourse. Yet, as conversational analysts
focused on micro-levels of dialogue such as turn-
taking, computational linguists uncovered and for-
malized macro-level dialogue structure and devised
well-defined representations of semantics for at least
some forms of dialogues (Allen and Perrault, 1980;
Grosz and Sidner, 1986; Clark, 1996), which have in
turn been implemented in spoken dialogue systems
(Rich and Sidner, 1998; Allen et al., 2005).
</bodyText>
<note confidence="0.874395">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 1–10,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.6665105">
1.2 Current Approaches to Turn-Taking in
Spoken Dialogue Systems
</subsectionHeader>
<bodyText confidence="0.999959333333333">
Unfortunately, while socio- and psycho-linguists re-
vealed the complexity of conversational turn-taking
behavior, designers of practical spoken dialogue sys-
tems have stuck to a simplistic approach to end-of-
turn detection (hereafter endpointing). Typically, si-
lences in user speech are detected using a low-level
Voice Activity Detector (VAD) and a turn is consid-
ered finished once a silence lasts longer than a fixed
threshold. This approach has the advantage of being
simple, only relying on easily computable low-level
features. However, it leads to suboptimal behavior
in many instances. First, False Alarms (FA) hap-
pen when a pause lasts longer than the threshold and
gets wrongly classified as a gap1. Second, latency
occurs at every gap, because the system must wait
for the duration of the threshold before classifying a
silence as gap. When setting the threshold, system
designers must consider the trade-off between these
two issues: setting a low threshold reduces latency
but increases FA rate, while setting a high threshold
reduces FA rate but increases latency.
To help overcome the shortcomings of the single-
threshold approach, several researchers have pro-
posed to exploit various features. Sato et al (Sato
et al., 2002) used decision trees to classify pauses
longer than 750 ms as gap or pause. By using fea-
tures from semantics, syntax, dialogue state, and
prosody, they were able to improve the classification
accuracy from a baseline of 76.2% to 83.9%. While
this important study shows encouraging results on
the value of using various sources of information in
a dialogue system, the proposed approach (classify-
ing long silences) is not completely realistic (what
happens when a gap is misclassified as a pause?) and
does not attempt to optimize latency. An extension
to this approach was proposed in (Takeuchi et al.,
2004), in which a turn-taking decision is made every
100 ms during pauses. However, in this latter work
the features are limited to timing, prosody, and syn-
tax (part-of-speech). Also the reported classification
results, with F-measures around 50% or below do
not seem to be sufficient for practical use.
</bodyText>
<footnote confidence="0.960204">
1We use the terminology from (Sacks et al., 1974) where a
pause is a silence within a turn while a gap is a silence between
turns. We use the term silence to encompass both types.
</footnote>
<bodyText confidence="0.999821029411765">
Similarly, Ferrer and her colleagues (Ferrer et al.,
2003) proposed the use of multiple decision trees,
each triggered at a specific time in the pause, to de-
cide to either endpoint or defer the decision to the
next tree, unless the user resumes speaking. Using
features like vowel duration or pitch for the region
immediately preceding the silence, combined with a
language model that predicts gaps based on the pre-
ceding words, Ferrer et al are able shorten latency
while keeping the FA rate constant. On a corpus
of recorded spoken dialogue-like utterances (ATIS),
they report reductions of up to 81% for some FA
rates. While very promising, this approach has sev-
eral disadvantages. First it relies on a small set of
possible decision points for each pause, preventing
fine optimization between them. Second, the trees
are trained on increasingly smaller datasets requir-
ing smoothing of the tree scores to compensate for
poor training of the later trees (which are trained
on increasingly small subsets of pauses from the
training set). Finally, and perhaps most importantly,
these authors have investigated prosodic and lexical
features, but not other aspects of dialogue, such as
discourse structure, timing, and semantics.
In this paper, we propose a new approach to end-
pointing that directly optimizes thresholds using au-
tomatically extracted dialogue features ranging from
discourse to timing and prosody. Section 2 out-
lines the proposed algorithm. Section 3 describes
the analysis of the relationship between silences and
a wide range of features available to a standard spo-
ken dialogue system (hereafter dialogue features).
Evaluation results, both offline and in the deployed
Let’s Go system are given in Section 4.
</bodyText>
<sectionHeader confidence="0.9963265" genericHeader="method">
2 Dynamic Endpointing Threshold
Decision Trees
</sectionHeader>
<subsectionHeader confidence="0.993947">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.999946444444444">
One issue with current approaches to endpointing
is that they rely on binary gap/pause classifiers and
the relationship between optimizing for classifica-
tion accuracy vs optmizing to minimize latency is
unclear. Also, the performance we obtained when
applying classification-based approaches to the Let’s
Go data was disappointing. The accuracy of the clas-
sifiers was not sufficient for practical purposes, even
with the improvements proposed by (Ferrer et al.,
</bodyText>
<page confidence="0.988275">
2
</page>
<bodyText confidence="0.999957705882353">
2003). We hypothesize that the discrepancy between
these results and the good performances reported by
others is due to the noisiness of the Let’s Go data
(see Section 3.1.1). To overcome these issues, we
propose a method that directly optimizes endpoint-
ing thresholds using a two-stage process. First, si-
lences are clustered based on dialogue features so
as to create groups of silences with similar proper-
ties. Second, a single threshold is set for each clus-
ter, so as to minimize the overall latency at a given
false alarm rate. The result of the training process
is thus a decision tree on dialogue features that con-
tains thresholds at its leaves. At runtime, every time
a silence is detected, the dialogue system runs the
decision tree and sets its endpointing threshold ac-
cordingly. The following sections describe the two
training stages.
</bodyText>
<subsectionHeader confidence="0.996362">
2.2 Feature-based Silence Clustering
</subsectionHeader>
<bodyText confidence="0.999988230769231">
The goal of the first stage of training is to clus-
ter silences with a similar FA rate/latency trade-
off. The intuition is that we would like to generate
low-threshold clusters, which contain mostly gaps
and short pauses, and clusters where long pauses
would be concentrated with no or very few gaps,
allowing to set high thresholds that reduce cut-in
rate without hurting overall latency. We used a
standard top-down clustering algorithm that exhaus-
tively searches binary splits of the data based on fea-
ture values. The split that yields the minimal overall
cost is kept, where the cost Cn of cluster Kn is de-
fined by the following function:
</bodyText>
<equation confidence="0.9997815">
�Cn = Gn x |1 1:Duration(p)2 (1)
K |pEK
</equation>
<bodyText confidence="0.999978733333333">
where Gn the number of gaps in Kn and
Duration(p) the duration of a pause p, set to zero
for gaps. While other cost functions are possible, the
intuition behind this formula is that it captures both
the cluster’s gap ratio (first factor) and its pause du-
ration distribution (second factor: root mean square
of pause duration). The splitting process is repeated
recursively until the reduction in cost between the
original cost and the sum of the costs of the two split
clusters falls below a certain threshold. By minimiz-
ing C(K), the clustering algorithm will find ques-
tions that yield clusters with either a small Gn, i.e.
mostly pauses, or a small root mean square pause
duration. Ultimately, at the leaves of the tree are sets
of silences that will share the same threshold.
</bodyText>
<subsectionHeader confidence="0.998134">
2.3 Cluster Threshold Optimization
</subsectionHeader>
<bodyText confidence="0.999824090909091">
Given the clusters generated by the first phase, the
goal of the second phase is to find a threshold for
each cluster so that the overall latency is minimized
at a given FA rate. Under the assumption that pause
durations follow an exponential distribution, which
is supported by previous work and our own data (see
Section 3.2), we show in Figure 3 in appendix that
there is a unique set of thresholds that minimizes la-
tency and that the threshold for any cluster n is given
by:
where An and on can be estimated from the data.
</bodyText>
<sectionHeader confidence="0.981927" genericHeader="method">
3 Silences and Dialogue Features
</sectionHeader>
<subsectionHeader confidence="0.999015">
3.1 Overview of the Data
3.1.1 The Let’s Go Corpus
</subsectionHeader>
<bodyText confidence="0.999976166666667">
Let’s Go is a telephone-based spoken dialogue
system that provides bus schedule information for
the Pittsburgh metropolitan area. It is built on the
Olympus architecture (Bohus et al., 2007), using the
RavenClaw dialogue management framework, and
the Apollo interaction manager (Raux et al., 2007)
as core components. Outside of business hours
callers to the bus company’s customer service are
offered the option to use Let’s Go. All calls are
recorded and extensively logged for further analy-
sis. The corpus used for this study was collected
between December 26, 2007 and January 25, 2008,
with a total of 1326 dialogues, and 18013 user turns.
Of the calls that had at least 4 user turns, 73% were
complete, meaning that the system provided some
schedule information to the user.
While working on real user data has its advan-
tages (large amounts of data, increased validity of
the results), it also has its challenges. In the case of
Let’s Go, users call from phones of varying quality
(cell phones and landlines), often with background
noises such as cars, infant cries, loud television sets,
etc. The wide variability of the acoustic conditions
makes any sound processing more prone to error
</bodyText>
<figure confidence="0.552274">
On =
An x lo9&apos;(Qn x F_ An )
µn
(2)
Gn
</figure>
<page confidence="0.976447">
3
</page>
<bodyText confidence="0.999899909090909">
than on carefully recorded corpora. For example, as
reported in (Raux et al., 2005), the original speech
recognizer had been found to yield a 17% word error
rate on a corpus of dialogues collected by recruit-
ing subjects to call the system from an office. On
the live Let’s Go data, that same recognizer had a
68% WER. After acoustic and language model re-
training/adaptation, that number was brought down
to about 30% but it is still a testimony to the diffi-
culty of obtaining robust features, particularly from
acoustics.
</bodyText>
<subsectionHeader confidence="0.948195">
3.1.2 Correcting Runtime Endpointing Errors
</subsectionHeader>
<bodyText confidence="0.999955285714286">
Let’s Go uses a GMM-based VAD trained on pre-
viously transcribed dialogues. Endpointing deci-
sions are based on a fixed 700 ms threshold on the
duration of the detected silences. One issue when
analyzing pause distributions from the corpus is that
observed user behavior was affected by system’s be-
havior at runtime. Most notably, because of the fixed
threshold, no recorded pause lasts more than 700 ms.
To compensate for that, we used a simple heuristic
to rule some online endpointing decisions as erro-
neous. If a user turn is followed within 1200 ms by
another user turn, we consider these two turns to be
in fact a single turn, unless the first turn was a user
barge-in. This heuristic was established by hand-
labeling 200 dialogues from a previous corpus with
endpointing errors (i.e. each turn was annotated as
correctly or incorrectly endpointed). On this dataset,
the heuristic has a precision of 70.6% and a recall of
75.5% for endpointing errors. Unless specified, all
subsequent results are based on this modified cor-
pus.
</bodyText>
<subsectionHeader confidence="0.997919">
3.2 Turn-Internal Pause Duration Distribution
</subsectionHeader>
<bodyText confidence="0.998411083333333">
Overall there were 9563 pauses in the corpus, which
amounts to 0.53 pauses per turn. The latency / FA
rate trade-off for the corpus is plotted in Figure 1.
This curve follows an exponential function (the R2
on the linear regression of latency on Log(FA) is
0.99). This stems from the fact that pause duration
approximately follows an exponential distribution,
which has been observed by others in the past (Jaffe
and Feldstein, 1970; Lennes and Anttila, 2002).
One consequence of the exponential-like distribu-
tion is that short pauses strongly dominate the distri-
bution. We decided to exclude silences shorter than
</bodyText>
<figureCaption confidence="0.939798333333333">
Figure 1: Overall False Alarm / Latency trade-off in the
Let’s Go corpus. The dashed line represents a fitted curve
of the form FA = e0+a·Latency.
</figureCaption>
<bodyText confidence="0.995633454545455">
200 ms from most of the following analysis for two
reasons: 1) they are more prone to voice activity
detection errors or short non-pause silences within
speech (e.g. unvoiced stop closure), and 2) in order
to apply the results found here to online endpointing
by the system, some amount of time is required to
detect the silence and compute necessary features,
making endpointing decisions on such very short si-
lences impractical. Once short silences have been
excluded, there are 3083 pauses in the corpus, 0.17
per turn.
</bodyText>
<subsectionHeader confidence="0.944544">
3.3 Relationship Between Dialogue Features
and Silence Distributions
3.3.1 Statistical Analysis
</subsectionHeader>
<bodyText confidence="0.9999653125">
In order to get some insight into the interaction
of the various aspects of dialogue and silence char-
acteristics, we investigated a number of features au-
tomatically extracted from the dialogue recordings
and system logs. Each feature is used to split the
set of silences into two subsets. For nominal fea-
tures, all possible splits of one value vs all the others
are tested, while for continuous and ordinal features,
we tried a number of thresholds and report the one
that yielded the strongest results. In order to avoid
extreme cases that split the data into one very large
and one very small set, we excluded all splits where
either of the two sets had fewer than 1000 silences.
All the investigated splits are reported in Appendix,
in Table 1 and 2. We compare the two subsets gen-
erated by each possible split in terms of two metrics:
</bodyText>
<listItem confidence="0.986159">
• Gap Ratio (GR), defined as the proportion of
</listItem>
<page confidence="0.985288">
4
</page>
<bodyText confidence="0.998647714285714">
gaps among all silences of a given set. We re-
port the absolute difference in GR between the
two sets, and use chi-square in a 2x2 design
(pause vs gap and one subset vs the other) to
test for statistical significance at the 0.01 level,
using Bonferroni correction to compensate for
multiple testings.
</bodyText>
<listItem confidence="0.642687">
• Mean pause duration. The strength of the in-
teraction is shown by the difference in mean
pause duration, and we use Mann Whitney’s
Rank Sum test for statistical significance, again
at the 0.01 level, using Bonferroni correction.
</listItem>
<bodyText confidence="0.998881">
We group features into five categories: discourse,
semantics, prosody, turn-taking, and speaker charac-
teristics, described in the following sections.
</bodyText>
<subsectionHeader confidence="0.88724">
3.3.2 Discourse Structure
</subsectionHeader>
<bodyText confidence="0.99996484">
Discourse structure is captured by the system’s di-
alogue act immediately preceding the current user
turn. In the Let’s Go dialogues, 97.9% of sys-
tem dialogue acts directly preceding user turns are
questions2. Of these, 13% are open questions (e.g.
”What can I do for you?”), 39% are closed ques-
tions (e.g. ”Where are you leaving from?”) and 46%
are confirmation requests (e.g. ”Leaving from the
airport. Is this correct?”)3. There are many more
pauses in user responses to open questions than to
the other types (cf Table 1). One explanation is that
user answers to open questions tend to be longer
(2046 ms on average, to be contrasted with 1268 ms
for turns following closed questions and 819 ms for
responses to confirmation questions). Conversely,
confirmation questions lead to responses with sig-
nificantly fewer pauses. 78% of such turns con-
tained only one word, single YES and NO answers
accounting for 81% of these one-word responses,
which obviously do not lend themselves to pauses.
Discourse context also has an effect on pause dura-
tions, albeit a weak one, with open questions leading
to turns with shorter pauses. One possible explana-
tion for this is that pauses after closed and confirma-
tion questions tend to reflect more hesitations and/or
</bodyText>
<footnote confidence="0.9780138">
2The remaining 2.1% belong to other cases such as the user
barging in right after the system utters a statement.
3The high number of confirmations comes from the fact that
Let’s Go is designed to ask the user to explicitly confirm every
concept.
</footnote>
<bodyText confidence="0.982131666666667">
confusion on the user’s side, whereas responses to
open questions also have pauses in the normal flow
of speech.
</bodyText>
<subsectionHeader confidence="0.799502">
3.3.3 Semantics
</subsectionHeader>
<bodyText confidence="0.999980815789474">
Semantic features are based on partial speech
recognition results and on their interpretation in the
current dialogue context. We use the most recent
recognition hypothesis available at the time when
the silence starts, parse it using the system’s standard
parser and grammar, and match the parse against the
”expectation agenda” that RavenClaw (Bohus and
Rudnicky, 2003) maintains. The expectation level
of a partial utterance indicates how well it fits in the
current dialogue context. A level of 0 means that
the utterance can be interpreted as a direct answer
to the last system prompt (e.g. a ”PLACE” con-
cept as an answer to ”Where are you leaving from?”,
a ”YES” or a ”NO” after a confirmation question).
Higher levels correspond to utterances that fit in a
broader dialogue context (e.g. a place name after
the system asks ”Leaving from the airport. Is this
correct?”, or ”HELP” in any context). Finally, non-
understandings, which do not match any expecta-
tion, are given a matching level of +oo.
Expectation level is strongly related to both fi-
nality and pause duration. Pauses following par-
tial utterances of expectation level 0 are signifi-
cantly more likely to be gaps than those matching
any higher level. Also, very unexpected partial ut-
terances (and non-understandings) contain shorter
pauses than more expected ones. Another indica-
tive feature for finality is the presence of a posi-
tive marker (i.e. a word like ”YES” or ”SURE”) in
the partial utterance. Utterances that contain such a
marker are more likely to be finished than others. In
contrast, the effect of negative markers is not signif-
icant. This can be explained by the fact that nega-
tive responses to confirmation often lead to longer
corrective utterances more prone to pauses. Indeed,
91% of complete utterances that contain a positive
marker are single-word, against 67% for negative
markers.
</bodyText>
<subsectionHeader confidence="0.61486">
3.3.4 Prosody
</subsectionHeader>
<bodyText confidence="0.994507333333333">
We extracted three types of prosodic features:
acoustic energy of the last vowel, pitch of the last
voiced region, and duration of the last vowel. Vowel
</bodyText>
<page confidence="0.985064">
5
</page>
<bodyText confidence="0.999982136363636">
location and duration were estimated by performing
phoneme alignment with the speech recognizer. Du-
ration was normalized to account for both vowel and
speaker identity. Energy was computed as the log-
transformed signal intensity on 10ms frames. Pitch
was extracted using the Snack toolkit (Sjolander,
2004), also at 10ms intervals. For both energy and
pitch, the slope of the contour was computed by lin-
ear regression, and the mean value was normalized
by Z-transformation using statistics of the dialogue-
so-far. As a consequence, all threshold values for
means are expressed in terms of standard deviations
from the current speaker’s mean value.
Vowel energy, both slope and mean, yielded the
highest correlation with silence finality, although it
did not rank as high as features from other cate-
gories. As expected, vowels immediately preced-
ing gaps tend to have lower and falling intensity,
whereas rising intensity makes it more likely that the
turn is not finished. On the other hand, extremely
high pitch is a strong cue to longer pauses, but only
happen in 5.6% of the pauses.
</bodyText>
<subsectionHeader confidence="0.859503">
3.3.5 Timing
</subsectionHeader>
<bodyText confidence="0.999878333333333">
Timing features, available from the Interaction
Manager, provide the strongest cue to finality. The
longer the on-going turn has been, the less likely it is
that the current silence is a gap. This is true both in
terms of time elapsed since the beginning of the ut-
terance and number of pauses observed so far. This
latter feature also correlates well with mean pause
duration, earlier pauses of a turn tending to be longer
than later ones.
</bodyText>
<subsectionHeader confidence="0.863522">
3.3.6 Speaker Characteristics
</subsectionHeader>
<bodyText confidence="0.999955923076923">
These features correspond to the observed pausal
behavior so far in the dialogue. The idea is that dif-
ferent speakers follow different patterns in the way
they speak (and pause), and that the system should
be able to learn these patterns to anticipate future
behavior. Specifically, we look at the mean num-
ber of pauses per utterance observed so far, and the
mean pause duration observed so far for the current
dialogue. Both features correlate reasonably well
with silence finality: a higher mean duration indi-
cates that upcoming silences are also less likely to
be final, so does a higher mean number of pauses
per turn.
</bodyText>
<subsectionHeader confidence="0.765489">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999985461538461">
What emerges from the analysis above is that fea-
tures from all aspects of dialogue provide informa-
tion on silence characteristics. While most previous
research has focused on prosody as a cue to detect
the end of utterances, timing, discourse, semantic
and previously observed silences appear to corre-
late more strongly with silence finality in our corpus.
This can be partly explained by the fact that prosodic
features are harder to reliably estimate on noisy data
and that prosodic features are in fact correlated to
higher levels of dialogue such as discourse and se-
mantics. However, we believe our results make a
strong case in favor of a broader approach to turn-
taking for conversational agents, making the most
of all the features that are readily available to such
systems. Indeed, particularly in constrained systems
like Let’s Go, higher level features like discourse
and semantics might be more robust to poor acoustic
conditions than prosodic features. Still, our findings
on mean pause durations suggest that prosodic fea-
tures might be best put to use when trying to pre-
dict pause duration, or whether a pause will occur
or not. The key to more natural and responsive di-
alogue systems lies in their ability to combine all
these features in order to make prompt and robust
turn-taking decisions.
</bodyText>
<sectionHeader confidence="0.827134" genericHeader="evaluation">
4 Evaluation of Threshold Decision Trees
</sectionHeader>
<subsectionHeader confidence="0.985224">
4.1 Offline Evaluation Set-Up
</subsectionHeader>
<bodyText confidence="0.9998478">
We evaluated the approach introduced in Section 2
on the Let’s Go corpus. The set of features was ex-
tended to contain a total of 4 discourse features, 6
semantic features, 5 timing/turn-taking features, 43
prosodic features, and 6 speaker characteristic fea-
tures. All evaluations were performed by 10-fold
cross-validation on the corpus. Based on the pro-
posed algorithm, we built a decision tree and com-
puted optimal cluster thresholds for different overall
FA rates. We report average latency as a function
of the proportion of turns for which any pause was
erroneously endpointed, which is closer to real per-
formance than silence FA rate since, once a turn has
been endpointed, all subsequent silences are irrele-
vant.
</bodyText>
<page confidence="0.997709">
6
</page>
<figureCaption confidence="0.9977445">
Figure 2: Performance of the proposed approach using
different feature sets.
</figureCaption>
<subsectionHeader confidence="0.996744">
4.2 Performance of Different Feature Sets
</subsectionHeader>
<bodyText confidence="0.999982870967742">
First we evaluated each feature set individually. The
results are shown in Figure 2. We concentrate on the
2-6% range of turn cut-in rate where any reasonable
operational value is likely to lie (the 700 ms thresh-
old of the baseline Let’s Go system yields about 4%
cut-in rate). All feature sets improve over the base-
line. Statistical significance of the result was tested
by performing a paired sign test on latencies for the
whole dataset, comparing, for each FA rate the pro-
portion of gaps for which the proposed approach
gives a shorter threshold than the single-threshold
baseline. Latencies produced by the decision tree
for all feature sets were all found to be significantly
shorter (p &lt; 0.0001) than the corresponding base-
line threshold.
The best performing feature set is semantics, fol-
lowed by timing, prosody, speaker, and discourse.
The maximum relative latency reductions for each
feature set range from 12% to 22%. When using all
features, the performance improves by a small but
significant amount compared to any single set, up to
a maximum latency reduction of 24%. This confirms
that the algorithm is able to combine features effec-
tively, and that the features themselves are not com-
pletely redundant. However, while removing seman-
tic or timing features from the complete set degrades
the performance, this is not the case for discourse,
speaker, nor prosodic features. This result, similar
to what (Sato et al., 2002) reported in their own ex-
periment, indicates that prosodic features might be
redundant with semantic and timing features.
</bodyText>
<subsectionHeader confidence="0.999315">
4.3 Live Evaluation
</subsectionHeader>
<bodyText confidence="0.999996696969697">
We confirmed the offline evaluation’s findings by
implementing the proposed approach in Let’s Go’s
Interaction Manager. Since prosodic features were
not found to be helpful and since their online ex-
traction is costly and error-prone, we did not include
them. At the beginning of each dialogue, the sys-
tem was randomly set as a baseline version, using a
700 ms fixed threshold, or as an experimental ver-
sion using the tree learned from the offline corpus.
Results show that median latency (which includes
both the endpointing threshold and the time to pro-
duce the system’s response) is significantly shorter
in the experimental version (561 ms) than in the
baseline (957 ms). Overall, the proposed approach
reduced latency by 50% or more in about 48% of the
turns. However, global results like these might not
reflect the actual improvement in user experience.
Indeed, we know from human-human dialogues that
relatively long latencies are normal in some circum-
stances while very short or no latency is expected
in others. The proposed algorithm reproduces some
of these aspects. For example, after open questions,
where more uncertainty and variability is expected,
the experimental version is in fact slightly slower
(1047 ms vs 993 ms). On the other hand, it is faster
after closed question (800 ms vs 965 ms) and par-
ticularly after confirmation requests (324 ms vs 965
ms), which are more predictable parts of the dia-
logue where high responsiveness is both achievable
and natural. This latter result indicates that our ap-
proach has the potential to improve explicit confir-
mations, which are often thought to be tedious and
irritating to the user.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999962636363636">
In this paper, we described an algorithm to dynami-
cally set endpointing threshold for each silence. We
analyzed the relationship between silence distribu-
tion and a wide range of automatically extracted fea-
tures from discourse, semantics, prosody, timing and
speaker characteristics. When all features are used,
the proposed method reduced latency by up to 24%
for reasonable false alarm rates. Prosodic features
did not help threshold optimization once other fea-
ture were included. The practicality of the approach
and the offline evaluation results were confirmed by
</bodyText>
<page confidence="0.99771">
7
</page>
<bodyText confidence="0.989155">
implementing the proposed algorithm in the Let’s
Go system.
</bodyText>
<sectionHeader confidence="0.995488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999884857142857">
This work is supported by the US National Science
Foundation under grant number 0208835. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the NSF.
We would like to thank Alan Black for his many
comments and advice.
</bodyText>
<sectionHeader confidence="0.99839" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993996280898877">
J. F. Allen and C. R. Perrault. 1980. Analyzing intention
in utterances. Artificial Intelligence, 15:143–178.
J. F. Allen, G. Ferguson, A. Stent, S. Stoness, M. Swift,
L. Galescu, N. Chambers, E. Campana, and G. S. Aist.
2005. Two diverse systems built using generic compo-
nents for spoken dialogue (recent progress on trips). In
Interactive Demonstration Track, Association of Com-
putational Linguistics Annual Meeting, Ann Arbor,
MI.
D. Bohus and A. Rudnicky. 2003. RavenClaw: Dia-
log management using hierarchical task decomposi-
tion and an expectation agenda. In Eurospeech03,
Geneva, Switzerland.
D. Bohus, A. Raux, T. Harris, M. Eskenazi, and A. Rud-
nicky. 2007. Olympus: an open-source framework
for conversational spoken language interface research.
In HLT-NAACL 2007 workshop on Bridging the Gap:
Academic and Industrial Research in Dialog Technol-
ogy, Rochester, NY, USA.
W. L. Chafe, 1992. Talking Data: Transcription
and Coding Methods for Language Research, chapter
Prosodic and Functional Units of Language, pages 33–
43. Lawrence Erlbaum.
H.H. Clark. 1996. Using language. Cambridge Univer-
sity Press.
S. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23(2):283–292.
L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A prosody-
based approach to end-of-utterance detection that does
not require speech recognition. In ICASSP, Hong
Kong.
C. E. Ford and S. A. Thompson, 1996. Interaction and
Grammar, chapter Interactional Units in Conversation:
Syntactic, Intonational, and Pragmatic Resources for
the Management of Turns, pages 134–184. Cambridge
University Press.
H. Furo. 2001. Turn-Taking in English and Japanese.
Projectability in Grammar, Intonation, and Semantics.
Routeledge.
B. J. Grosz and C. Sidner. 1986. Attention, intentions,
and the structure of discourse. Computational Lin-
guistics, 12(3):175–204.
J. Jaffe and S. Feldstein. 1970. Rhythms of Dialogue.
Academic Press.
H. Koiso, Y. Horiuchi, S. Tutiya, A. Ichikawa, and
Y. Den. 1998. An analysis of turn-taking and
backchannels based on prosodic and syntactic features
in japanese map task dialogs. Language and Speech,
41(3-4):295–321.
Mietta Lennes and Hanna Anttila. 2002. Prosodic fea-
tures associated with the distribution of turns in finnish
informal dialogues. In Petri Korhonen, editor, The
Phonetics Symposium 2002, volume Report 67, pages
149–158. Laboratory of Acoustics and Audio Signal
Processing, Helsinki University of Technology.
B. Orestr¨om. 1983. Turn-Taking in English Conversa-
tion. CWK Gleerup, Lund.
A. Raux, B. Langner, D. Bohus, A. W. Black, and M. Es-
kenazi. 2005. Let’s Go Public! taking a spoken dialog
system to the real world. In Proc. Interspeech 2005,
Lisbon, Portugal.
A. Raux, D. Bohus, B. Langner, A. W. Black, and M. Es-
kenazi. 2006. Doing research on a deployed spoken
dialogue system: One year of Let’s Go! experience.
In Proc. Interspeech 2006, Pittsburgh, PA, USA.
A. Raux, , and M. Eskenazi. 2007. A multi-layer ar-
chitecture for semi-synchronous event-driven dialogue
management. In Proc. ASRU 2007, Kyoto, Japan.
C. Rich and C.L. Sidner. 1998. Collagen: A collabora-
tion manager for software interface agents. An Inter-
national Journal: User Modeling and User-Adapted
Interaction, 8(3-4):315–350.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696–735.
R. Sato, R. Higashinaka, M. Tamoto, M. Nakano, and
K. Aikawa. 2002. Learning decision trees to deter-
mine turn-taking by spoken dialogue systems. In IC-
SLP 2002, Denver, CO.
Kare Sjolander. 2004. The snack sound toolkit.
http://www.speech.kth.se/snack/.
M. Takeuchi, N. Kitaoka, and S. Nakagawa. 2004.
Timing detection for realtime dialog systems using
prosodic and linguistic information. In Proc. Speech
Prosody 04, Nara, Japan.
N. Ward, A. Rivera, K. Ward, and D. Novick. 2005. Root
causes of lost time and user stress in a simple dialog
system. In Interspeech 2005, Lisbon, Portugal.
</reference>
<page confidence="0.997251">
8
</page>
<table confidence="0.998947055555556">
Category Feature test Number of Gap Ratio Difference
Silences
Timing Pause start time &gt; 3000 ms 1836 / 19260 65% / 87% -23%
Timing Pause number &gt; 2 3379 / 17717 69% / 88% -19%
Discourse Previous question is open 3376 / 17720 70% / 88% -18%
Semantics Utterance expectation level &gt; 1 10025 / 11071 78% / 92% -14%
Individual Mean pause duration &gt; 500 ms 1336 / 19760 72% / 86% -14%
Semantics Utterance contains a positive marker 4690 / 16406 96% / 82% 13%
Prosody Mean energy of last vowel &gt; 5 1528 / 19568 74% / 86% -12%
Prosody Slope of energy on last vowel &gt; 0 6922 / 14174 78% / 89% -10%
Individual Mean number of pauses per utterance &gt; 3 1929 / 19267 76% / 86% -10%
Semantic Utterance is a non-understanding 6023/15073 79% / 88% -9%
Discourse Previous question is a confirmation 8893 / 12203 90% / 82% 8%
Prosody Duration of last vowel &gt; 1 1319 / 19777 78% / 86% -8%
Prosody Mean pitch on last voiced region &gt; 5 1136 / 19960 92% / 85% 7%
Prosody Slope of pitch on last voiced region &gt; 0 6617 / 14479 82% / 87% -4%
Semantics Utterance contains a negative marker 2667 / 18429 87% / 85% 2%*
Discourse Previous question is closed 8451 / 12645 86% / 85% 1%*
</table>
<tableCaption confidence="0.983589333333333">
Table 1: Effect of Dialogue Features on Pause Finality. In columns 3 and 4, the first number is for silences for which
the condition in column 2 is true, while the second number is for those silences where the condition is false. * indicates
that the results are not statistically significant at the 0.01 level.
</tableCaption>
<table confidence="0.997454055555556">
Category Feature test Number of Mean pause Difference
Pauses Duration (ms) (ms)
Prosody Mean pitch on last voiced region &gt; 4 172 / 2911 608 / 482 126
Semantics Utterance Expectation Level &gt; 4 2202 / 881 475 / 526 -51
Prosody Slope of energy on last vowel &gt; 1 382 / 2701 446 / 495 -39
Timing Pause number &gt; 2 1031 / 2052 459 / 504 -45
Discourse Previous question is open 1015 / 2068 460 / 504 -43
Individual Mean pause duration &gt; 500 ms 370 / 2713 455 / 494 -39*
Prosody Mean energy of last vowel &gt; 4.5 404 / 2679 456 / 494 -38*
Semantics Utterance contains a positive marker 211 / 2872 522 / 487 35*
Discourse Previous question is closed 1178 / 1905 510 / 477 33*
Timing Pause start time &gt; 3000 ms 650 / 2433 465 / 496 -31*
Semantic Utterance is a non-understanding 1247 / 1836 472 / 502 -30*
Prosody Duration of last vowel &gt; 0.4 1194 / 1889 507 / 478 29*
Individual Mean number of pauses per utterance &gt; 2 461 / 2622 474 / 492 -19*
Semantics Utterance contains a negative marker 344 / 2739 504 / 488 16*
Prosody Slope of pitch on last voiced segment &gt; 0 1158 / 1925 482 / 494 -12*
Discourse Previous question is a confirmation 867 / 2216 496 / 487 9*
</table>
<tableCaption confidence="0.987009333333333">
Table 2: Effect of Dialogue Features on Pause Duration. In columns 3 and 4, the first number is for silences for which
the condition in column 2 is true, while the second number is for those silences where the condition is false. * indicates
that the results are not statistically significant at the 0.01 level.
</tableCaption>
<page confidence="0.996198">
9
</page>
<bodyText confidence="0.99993375">
Let (Kn) be a set of n silence clusters, the goal is to set the thresholds (θn) that minimize overall mean
latency, while yielding a fixed, given number of false alarms E. let us define Gn the number of gaps among
the silences of Kn. For each cluster, let us define En(θn) the number of false alarms yielded by threshold
θn in cluster n, and the total latency Ln by:
</bodyText>
<equation confidence="0.995184">
Ln(θn) = Gn X θn (3)
</equation>
<bodyText confidence="0.989211">
Assuming pause durations follow an exponential distribution, as shown in Section 3, the following relation
holds between Ln and En:
</bodyText>
<equation confidence="0.993512666666667">
Ln(θn)
e
µn = βn X En(θn)
</equation>
<bodyText confidence="0.999896">
where µK and βK are cluster-specific coefficients estimated by linear regression in the log domain. If we
take the log of both sides, we obtain:
</bodyText>
<equation confidence="0.963972">
Ln(θn) = µn X log(βn X En(θn)) (5)
Theorem 1. If (θn) is a set of thresholds that minimizes En Ln such that En En(θn) = E, then
]As.t.bn, dLn (θn) = A
dEn
</equation>
<bodyText confidence="0.954571833333333">
Informal proof. The proof can be done by contradiction. Let us assume (θn) is a set of thresholds that
minimizes En Ln, and ](p, q)s.t.dLp
dEp (θp) &gt; dLq
dEq (θq). Then, there exists small neighborhoods of θp and θq
where Lp(Ep) and Lq(Eq) can be approximated by their tangents. Since their slopes differ, it is possible to
find a small E such that the decrease in FA yielded by θp + E is exactly compensated by the increase yielded
by θq − E, but the reduction in latency in Kq is bigger than the increase in Kp, which contradicts the fact
that (θn) minimizes L.
From Theorem 1, we get ]As.t.bndLn dEn=A. Thus, by deriving Equation 5, Enµn= A which gives En = µnA .
Given that E En = E, F- µn F- µn
A = E. Hence, A = E . From 5, we can infer the values of Ln(θn) and,
using 3, the optimal threshold θn for each cluster:
</bodyText>
<equation confidence="0.9031585">
EXµn
µn X log(βn X F- µn )
θn = (6)
Gn
</equation>
<bodyText confidence="0.997255">
where the values of µn and βn can be estimated by linear regression from the data based on 5.
</bodyText>
<figure confidence="0.948635">
(4)
</figure>
<figureCaption confidence="0.999761">
Figure 3: Derivation of the formula for optimal thresholds
</figureCaption>
<page confidence="0.9876">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.392609">
<title confidence="0.73028">Optimizing Endpointing Thresholds using Features in a Spoken Dialogue System Raux Language Technologies</title>
<affiliation confidence="0.979044">Carnegie Mellon</affiliation>
<address confidence="0.991265">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.999128">This paper describes a novel algorithm to dynamically set endpointing thresholds based on a rich set of dialogue features to detect the end of user utterances in a dialogue system. By analyzing the relationship between silences in user’s speech to a spoken dialogue system and a wide range of automatically extracted features from discourse, semantics, prosody, timing and speaker characteristics, we found that all features correlate with pause duration and with whether a silence indicates the end of the turn, with semantics and timing being the most informative. Based on these features, the proposed method reduces latency by up to 24% over a fixed threshold baseline. Offline evaluation results were confirmed by implementing the proposed algorithm in the Let’s Go system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>C R Perrault</author>
</authors>
<title>Analyzing intention in utterances.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>15--143</pages>
<contexts>
<context position="3339" citStr="Allen and Perrault, 1980" startWordPosition="503" endWordPosition="506">1). However, regarding this last aspect, Orestrom notes about his corpus that ”there is no simple way to formalizing a semantic analysis of this conversational material”. This difficulty in formalizing higher levels of conversation might explain the relatively low interest that conversational analysts have had in semantics and discourse. Yet, as conversational analysts focused on micro-levels of dialogue such as turntaking, computational linguists uncovered and formalized macro-level dialogue structure and devised well-defined representations of semantics for at least some forms of dialogues (Allen and Perrault, 1980; Grosz and Sidner, 1986; Clark, 1996), which have in turn been implemented in spoken dialogue systems (Rich and Sidner, 1998; Allen et al., 2005). Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 1–10, Columbus, June 2008. c�2008 Association for Computational Linguistics 1.2 Current Approaches to Turn-Taking in Spoken Dialogue Systems Unfortunately, while socio- and psycho-linguists revealed the complexity of conversational turn-taking behavior, designers of practical spoken dialogue systems have stuck to a simplistic approach to end-ofturn detection (hereafter endpoin</context>
</contexts>
<marker>Allen, Perrault, 1980</marker>
<rawString>J. F. Allen and C. R. Perrault. 1980. Analyzing intention in utterances. Artificial Intelligence, 15:143–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>G Ferguson</author>
<author>A Stent</author>
<author>S Stoness</author>
<author>M Swift</author>
<author>L Galescu</author>
<author>N Chambers</author>
<author>E Campana</author>
<author>G S Aist</author>
</authors>
<title>Two diverse systems built using generic components for spoken dialogue (recent progress on trips).</title>
<date>2005</date>
<booktitle>In Interactive Demonstration Track, Association of Computational Linguistics Annual Meeting,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="3485" citStr="Allen et al., 2005" startWordPosition="527" endWordPosition="530">sational material”. This difficulty in formalizing higher levels of conversation might explain the relatively low interest that conversational analysts have had in semantics and discourse. Yet, as conversational analysts focused on micro-levels of dialogue such as turntaking, computational linguists uncovered and formalized macro-level dialogue structure and devised well-defined representations of semantics for at least some forms of dialogues (Allen and Perrault, 1980; Grosz and Sidner, 1986; Clark, 1996), which have in turn been implemented in spoken dialogue systems (Rich and Sidner, 1998; Allen et al., 2005). Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 1–10, Columbus, June 2008. c�2008 Association for Computational Linguistics 1.2 Current Approaches to Turn-Taking in Spoken Dialogue Systems Unfortunately, while socio- and psycho-linguists revealed the complexity of conversational turn-taking behavior, designers of practical spoken dialogue systems have stuck to a simplistic approach to end-ofturn detection (hereafter endpointing). Typically, silences in user speech are detected using a low-level Voice Activity Detector (VAD) and a turn is considered finished once a si</context>
</contexts>
<marker>Allen, Ferguson, Stent, Stoness, Swift, Galescu, Chambers, Campana, Aist, 2005</marker>
<rawString>J. F. Allen, G. Ferguson, A. Stent, S. Stoness, M. Swift, L. Galescu, N. Chambers, E. Campana, and G. S. Aist. 2005. Two diverse systems built using generic components for spoken dialogue (recent progress on trips). In Interactive Demonstration Track, Association of Computational Linguistics Annual Meeting, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>RavenClaw: Dialog management using hierarchical task decomposition and an expectation agenda.</title>
<date>2003</date>
<booktitle>In Eurospeech03,</booktitle>
<contexts>
<context position="19118" citStr="Bohus and Rudnicky, 2003" startWordPosition="3105" endWordPosition="3108">h number of confirmations comes from the fact that Let’s Go is designed to ask the user to explicitly confirm every concept. confusion on the user’s side, whereas responses to open questions also have pauses in the normal flow of speech. 3.3.3 Semantics Semantic features are based on partial speech recognition results and on their interpretation in the current dialogue context. We use the most recent recognition hypothesis available at the time when the silence starts, parse it using the system’s standard parser and grammar, and match the parse against the ”expectation agenda” that RavenClaw (Bohus and Rudnicky, 2003) maintains. The expectation level of a partial utterance indicates how well it fits in the current dialogue context. A level of 0 means that the utterance can be interpreted as a direct answer to the last system prompt (e.g. a ”PLACE” concept as an answer to ”Where are you leaving from?”, a ”YES” or a ”NO” after a confirmation question). Higher levels correspond to utterances that fit in a broader dialogue context (e.g. a place name after the system asks ”Leaving from the airport. Is this correct?”, or ”HELP” in any context). Finally, nonunderstandings, which do not match any expectation, are </context>
</contexts>
<marker>Bohus, Rudnicky, 2003</marker>
<rawString>D. Bohus and A. Rudnicky. 2003. RavenClaw: Dialog management using hierarchical task decomposition and an expectation agenda. In Eurospeech03,</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Raux</author>
<author>T Harris</author>
<author>M Eskenazi</author>
<author>A Rudnicky</author>
</authors>
<title>Olympus: an open-source framework for conversational spoken language interface research.</title>
<date>2007</date>
<booktitle>In HLT-NAACL 2007 workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technology,</booktitle>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="11452" citStr="Bohus et al., 2007" startWordPosition="1827" endWordPosition="1830">er the assumption that pause durations follow an exponential distribution, which is supported by previous work and our own data (see Section 3.2), we show in Figure 3 in appendix that there is a unique set of thresholds that minimizes latency and that the threshold for any cluster n is given by: where An and on can be estimated from the data. 3 Silences and Dialogue Features 3.1 Overview of the Data 3.1.1 The Let’s Go Corpus Let’s Go is a telephone-based spoken dialogue system that provides bus schedule information for the Pittsburgh metropolitan area. It is built on the Olympus architecture (Bohus et al., 2007), using the RavenClaw dialogue management framework, and the Apollo interaction manager (Raux et al., 2007) as core components. Outside of business hours callers to the bus company’s customer service are offered the option to use Let’s Go. All calls are recorded and extensively logged for further analysis. The corpus used for this study was collected between December 26, 2007 and January 25, 2008, with a total of 1326 dialogues, and 18013 user turns. Of the calls that had at least 4 user turns, 73% were complete, meaning that the system provided some schedule information to the user. While wor</context>
</contexts>
<marker>Bohus, Raux, Harris, Eskenazi, Rudnicky, 2007</marker>
<rawString>Geneva, Switzerland. D. Bohus, A. Raux, T. Harris, M. Eskenazi, and A. Rudnicky. 2007. Olympus: an open-source framework for conversational spoken language interface research. In HLT-NAACL 2007 workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technology, Rochester, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Chafe</author>
</authors>
<title>Talking Data: Transcription and Coding Methods for Language Research, chapter Prosodic and Functional Units of Language, pages 33– 43. Lawrence Erlbaum.</title>
<date>1992</date>
<contexts>
<context position="2614" citStr="Chafe, 1992" startWordPosition="396" endWordPosition="397">ldstein, 1970; Sacks et al., 1974). According to Conversation Analysis and psycholinguistic studies, responsiveness in human conversations is possible because participants in the conversation exchange cues indicating when a turn might end, and are able to anticipate points at which they can take over the floor smoothly. Much research has been devoted to finding these cues, leading to the identification of many aspects of language and dialogue that relate to turn-taking behavior, including syntax (Sacks et al., 1974; Ford and Thompson, 1996; Furo, 2001), prosody (Duncan, 1972; Orestr¨om, 1983; Chafe, 1992; Ford and Thompson, 1996; Koiso et al., 1998; Furo, 2001), and semantics (Orestr¨om, 1983; Furo, 2001). However, regarding this last aspect, Orestrom notes about his corpus that ”there is no simple way to formalizing a semantic analysis of this conversational material”. This difficulty in formalizing higher levels of conversation might explain the relatively low interest that conversational analysts have had in semantics and discourse. Yet, as conversational analysts focused on micro-levels of dialogue such as turntaking, computational linguists uncovered and formalized macro-level dialogue s</context>
</contexts>
<marker>Chafe, 1992</marker>
<rawString>W. L. Chafe, 1992. Talking Data: Transcription and Coding Methods for Language Research, chapter Prosodic and Functional Units of Language, pages 33– 43. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
</authors>
<title>Using language.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3377" citStr="Clark, 1996" startWordPosition="511" endWordPosition="512">notes about his corpus that ”there is no simple way to formalizing a semantic analysis of this conversational material”. This difficulty in formalizing higher levels of conversation might explain the relatively low interest that conversational analysts have had in semantics and discourse. Yet, as conversational analysts focused on micro-levels of dialogue such as turntaking, computational linguists uncovered and formalized macro-level dialogue structure and devised well-defined representations of semantics for at least some forms of dialogues (Allen and Perrault, 1980; Grosz and Sidner, 1986; Clark, 1996), which have in turn been implemented in spoken dialogue systems (Rich and Sidner, 1998; Allen et al., 2005). Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 1–10, Columbus, June 2008. c�2008 Association for Computational Linguistics 1.2 Current Approaches to Turn-Taking in Spoken Dialogue Systems Unfortunately, while socio- and psycho-linguists revealed the complexity of conversational turn-taking behavior, designers of practical spoken dialogue systems have stuck to a simplistic approach to end-ofturn detection (hereafter endpointing). Typically, silences in user spe</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>H.H. Clark. 1996. Using language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Duncan</author>
</authors>
<title>Some signals and rules for taking speaking turns in conversations.</title>
<date>1972</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="2584" citStr="Duncan, 1972" startWordPosition="392" endWordPosition="393">isruptive overlap (Jaffe and Feldstein, 1970; Sacks et al., 1974). According to Conversation Analysis and psycholinguistic studies, responsiveness in human conversations is possible because participants in the conversation exchange cues indicating when a turn might end, and are able to anticipate points at which they can take over the floor smoothly. Much research has been devoted to finding these cues, leading to the identification of many aspects of language and dialogue that relate to turn-taking behavior, including syntax (Sacks et al., 1974; Ford and Thompson, 1996; Furo, 2001), prosody (Duncan, 1972; Orestr¨om, 1983; Chafe, 1992; Ford and Thompson, 1996; Koiso et al., 1998; Furo, 2001), and semantics (Orestr¨om, 1983; Furo, 2001). However, regarding this last aspect, Orestrom notes about his corpus that ”there is no simple way to formalizing a semantic analysis of this conversational material”. This difficulty in formalizing higher levels of conversation might explain the relatively low interest that conversational analysts have had in semantics and discourse. Yet, as conversational analysts focused on micro-levels of dialogue such as turntaking, computational linguists uncovered and for</context>
</contexts>
<marker>Duncan, 1972</marker>
<rawString>S. Duncan. 1972. Some signals and rules for taking speaking turns in conversations. Journal of Personality and Social Psychology, 23(2):283–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ferrer</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>A prosodybased approach to end-of-utterance detection that does not require speech recognition.</title>
<date>2003</date>
<booktitle>In ICASSP, Hong Kong.</booktitle>
<contexts>
<context position="6081" citStr="Ferrer et al., 2003" startWordPosition="940" endWordPosition="943">mize latency. An extension to this approach was proposed in (Takeuchi et al., 2004), in which a turn-taking decision is made every 100 ms during pauses. However, in this latter work the features are limited to timing, prosody, and syntax (part-of-speech). Also the reported classification results, with F-measures around 50% or below do not seem to be sufficient for practical use. 1We use the terminology from (Sacks et al., 1974) where a pause is a silence within a turn while a gap is a silence between turns. We use the term silence to encompass both types. Similarly, Ferrer and her colleagues (Ferrer et al., 2003) proposed the use of multiple decision trees, each triggered at a specific time in the pause, to decide to either endpoint or defer the decision to the next tree, unless the user resumes speaking. Using features like vowel duration or pitch for the region immediately preceding the silence, combined with a language model that predicts gaps based on the preceding words, Ferrer et al are able shorten latency while keeping the FA rate constant. On a corpus of recorded spoken dialogue-like utterances (ATIS), they report reductions of up to 81% for some FA rates. While very promising, this approach </context>
</contexts>
<marker>Ferrer, Shriberg, Stolcke, 2003</marker>
<rawString>L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A prosodybased approach to end-of-utterance detection that does not require speech recognition. In ICASSP, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Ford</author>
<author>S A Thompson</author>
</authors>
<title>Interaction and Grammar, chapter Interactional Units in Conversation: Syntactic, Intonational, and Pragmatic Resources for the Management of Turns,</title>
<date>1996</date>
<pages>134--184</pages>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2548" citStr="Ford and Thompson, 1996" startWordPosition="384" endWordPosition="388">h little or no gap between turns, or even non-disruptive overlap (Jaffe and Feldstein, 1970; Sacks et al., 1974). According to Conversation Analysis and psycholinguistic studies, responsiveness in human conversations is possible because participants in the conversation exchange cues indicating when a turn might end, and are able to anticipate points at which they can take over the floor smoothly. Much research has been devoted to finding these cues, leading to the identification of many aspects of language and dialogue that relate to turn-taking behavior, including syntax (Sacks et al., 1974; Ford and Thompson, 1996; Furo, 2001), prosody (Duncan, 1972; Orestr¨om, 1983; Chafe, 1992; Ford and Thompson, 1996; Koiso et al., 1998; Furo, 2001), and semantics (Orestr¨om, 1983; Furo, 2001). However, regarding this last aspect, Orestrom notes about his corpus that ”there is no simple way to formalizing a semantic analysis of this conversational material”. This difficulty in formalizing higher levels of conversation might explain the relatively low interest that conversational analysts have had in semantics and discourse. Yet, as conversational analysts focused on micro-levels of dialogue such as turntaking, compu</context>
</contexts>
<marker>Ford, Thompson, 1996</marker>
<rawString>C. E. Ford and S. A. Thompson, 1996. Interaction and Grammar, chapter Interactional Units in Conversation: Syntactic, Intonational, and Pragmatic Resources for the Management of Turns, pages 134–184. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Furo</author>
</authors>
<date>2001</date>
<booktitle>Turn-Taking in English and Japanese. Projectability in Grammar, Intonation, and Semantics. Routeledge.</booktitle>
<contexts>
<context position="2561" citStr="Furo, 2001" startWordPosition="389" endWordPosition="390">n turns, or even non-disruptive overlap (Jaffe and Feldstein, 1970; Sacks et al., 1974). According to Conversation Analysis and psycholinguistic studies, responsiveness in human conversations is possible because participants in the conversation exchange cues indicating when a turn might end, and are able to anticipate points at which they can take over the floor smoothly. Much research has been devoted to finding these cues, leading to the identification of many aspects of language and dialogue that relate to turn-taking behavior, including syntax (Sacks et al., 1974; Ford and Thompson, 1996; Furo, 2001), prosody (Duncan, 1972; Orestr¨om, 1983; Chafe, 1992; Ford and Thompson, 1996; Koiso et al., 1998; Furo, 2001), and semantics (Orestr¨om, 1983; Furo, 2001). However, regarding this last aspect, Orestrom notes about his corpus that ”there is no simple way to formalizing a semantic analysis of this conversational material”. This difficulty in formalizing higher levels of conversation might explain the relatively low interest that conversational analysts have had in semantics and discourse. Yet, as conversational analysts focused on micro-levels of dialogue such as turntaking, computational ling</context>
</contexts>
<marker>Furo, 2001</marker>
<rawString>H. Furo. 2001. Turn-Taking in English and Japanese. Projectability in Grammar, Intonation, and Semantics. Routeledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="3363" citStr="Grosz and Sidner, 1986" startWordPosition="507" endWordPosition="510">s last aspect, Orestrom notes about his corpus that ”there is no simple way to formalizing a semantic analysis of this conversational material”. This difficulty in formalizing higher levels of conversation might explain the relatively low interest that conversational analysts have had in semantics and discourse. Yet, as conversational analysts focused on micro-levels of dialogue such as turntaking, computational linguists uncovered and formalized macro-level dialogue structure and devised well-defined representations of semantics for at least some forms of dialogues (Allen and Perrault, 1980; Grosz and Sidner, 1986; Clark, 1996), which have in turn been implemented in spoken dialogue systems (Rich and Sidner, 1998; Allen et al., 2005). Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 1–10, Columbus, June 2008. c�2008 Association for Computational Linguistics 1.2 Current Approaches to Turn-Taking in Spoken Dialogue Systems Unfortunately, while socio- and psycho-linguists revealed the complexity of conversational turn-taking behavior, designers of practical spoken dialogue systems have stuck to a simplistic approach to end-ofturn detection (hereafter endpointing). Typically, silenc</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>B. J. Grosz and C. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jaffe</author>
<author>S Feldstein</author>
</authors>
<title>Rhythms of Dialogue.</title>
<date>1970</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="2016" citStr="Jaffe and Feldstein, 1970" startWordPosition="301" endWordPosition="304">tion and understanding, Ward et al (Ward et al., 2005) identified turn-taking issues, specifically responsiveness, as important shortcomings. Dialogues with artificial agents are typically rigid, following a strict one-speaker-at-a-time structure with significant latencies between turns. In a previous paper, we concurred with these findings when analyzing issues with the Let’s Go system 1 (Raux et al., 2006). In contrast, empirical studies of conversation have shown that human-human dialogues commonly feature swift exchanges with little or no gap between turns, or even non-disruptive overlap (Jaffe and Feldstein, 1970; Sacks et al., 1974). According to Conversation Analysis and psycholinguistic studies, responsiveness in human conversations is possible because participants in the conversation exchange cues indicating when a turn might end, and are able to anticipate points at which they can take over the floor smoothly. Much research has been devoted to finding these cues, leading to the identification of many aspects of language and dialogue that relate to turn-taking behavior, including syntax (Sacks et al., 1974; Ford and Thompson, 1996; Furo, 2001), prosody (Duncan, 1972; Orestr¨om, 1983; Chafe, 1992; </context>
<context position="14572" citStr="Jaffe and Feldstein, 1970" startWordPosition="2354" endWordPosition="2357">has a precision of 70.6% and a recall of 75.5% for endpointing errors. Unless specified, all subsequent results are based on this modified corpus. 3.2 Turn-Internal Pause Duration Distribution Overall there were 9563 pauses in the corpus, which amounts to 0.53 pauses per turn. The latency / FA rate trade-off for the corpus is plotted in Figure 1. This curve follows an exponential function (the R2 on the linear regression of latency on Log(FA) is 0.99). This stems from the fact that pause duration approximately follows an exponential distribution, which has been observed by others in the past (Jaffe and Feldstein, 1970; Lennes and Anttila, 2002). One consequence of the exponential-like distribution is that short pauses strongly dominate the distribution. We decided to exclude silences shorter than Figure 1: Overall False Alarm / Latency trade-off in the Let’s Go corpus. The dashed line represents a fitted curve of the form FA = e0+a·Latency. 200 ms from most of the following analysis for two reasons: 1) they are more prone to voice activity detection errors or short non-pause silences within speech (e.g. unvoiced stop closure), and 2) in order to apply the results found here to online endpointing by the sys</context>
</contexts>
<marker>Jaffe, Feldstein, 1970</marker>
<rawString>J. Jaffe and S. Feldstein. 1970. Rhythms of Dialogue. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Koiso</author>
<author>Y Horiuchi</author>
<author>S Tutiya</author>
<author>A Ichikawa</author>
<author>Y Den</author>
</authors>
<title>An analysis of turn-taking and backchannels based on prosodic and syntactic features in japanese map task dialogs. Language and Speech,</title>
<date>1998</date>
<pages>41--3</pages>
<contexts>
<context position="2659" citStr="Koiso et al., 1998" startWordPosition="402" endWordPosition="405">cording to Conversation Analysis and psycholinguistic studies, responsiveness in human conversations is possible because participants in the conversation exchange cues indicating when a turn might end, and are able to anticipate points at which they can take over the floor smoothly. Much research has been devoted to finding these cues, leading to the identification of many aspects of language and dialogue that relate to turn-taking behavior, including syntax (Sacks et al., 1974; Ford and Thompson, 1996; Furo, 2001), prosody (Duncan, 1972; Orestr¨om, 1983; Chafe, 1992; Ford and Thompson, 1996; Koiso et al., 1998; Furo, 2001), and semantics (Orestr¨om, 1983; Furo, 2001). However, regarding this last aspect, Orestrom notes about his corpus that ”there is no simple way to formalizing a semantic analysis of this conversational material”. This difficulty in formalizing higher levels of conversation might explain the relatively low interest that conversational analysts have had in semantics and discourse. Yet, as conversational analysts focused on micro-levels of dialogue such as turntaking, computational linguists uncovered and formalized macro-level dialogue structure and devised well-defined representat</context>
</contexts>
<marker>Koiso, Horiuchi, Tutiya, Ichikawa, Den, 1998</marker>
<rawString>H. Koiso, Y. Horiuchi, S. Tutiya, A. Ichikawa, and Y. Den. 1998. An analysis of turn-taking and backchannels based on prosodic and syntactic features in japanese map task dialogs. Language and Speech, 41(3-4):295–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mietta Lennes</author>
<author>Hanna Anttila</author>
</authors>
<title>Prosodic features associated with the distribution of turns in finnish informal dialogues.</title>
<date>2002</date>
<booktitle>The Phonetics Symposium 2002, volume Report 67,</booktitle>
<pages>149--158</pages>
<editor>In Petri Korhonen, editor,</editor>
<institution>Helsinki University of Technology.</institution>
<contexts>
<context position="14599" citStr="Lennes and Anttila, 2002" startWordPosition="2358" endWordPosition="2361">d a recall of 75.5% for endpointing errors. Unless specified, all subsequent results are based on this modified corpus. 3.2 Turn-Internal Pause Duration Distribution Overall there were 9563 pauses in the corpus, which amounts to 0.53 pauses per turn. The latency / FA rate trade-off for the corpus is plotted in Figure 1. This curve follows an exponential function (the R2 on the linear regression of latency on Log(FA) is 0.99). This stems from the fact that pause duration approximately follows an exponential distribution, which has been observed by others in the past (Jaffe and Feldstein, 1970; Lennes and Anttila, 2002). One consequence of the exponential-like distribution is that short pauses strongly dominate the distribution. We decided to exclude silences shorter than Figure 1: Overall False Alarm / Latency trade-off in the Let’s Go corpus. The dashed line represents a fitted curve of the form FA = e0+a·Latency. 200 ms from most of the following analysis for two reasons: 1) they are more prone to voice activity detection errors or short non-pause silences within speech (e.g. unvoiced stop closure), and 2) in order to apply the results found here to online endpointing by the system, some amount of time is</context>
</contexts>
<marker>Lennes, Anttila, 2002</marker>
<rawString>Mietta Lennes and Hanna Anttila. 2002. Prosodic features associated with the distribution of turns in finnish informal dialogues. In Petri Korhonen, editor, The Phonetics Symposium 2002, volume Report 67, pages 149–158. Laboratory of Acoustics and Audio Signal Processing, Helsinki University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Orestr¨om</author>
</authors>
<date>1983</date>
<booktitle>Turn-Taking in English Conversation. CWK Gleerup,</booktitle>
<location>Lund.</location>
<marker>Orestr¨om, 1983</marker>
<rawString>B. Orestr¨om. 1983. Turn-Taking in English Conversation. CWK Gleerup, Lund.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>B Langner</author>
<author>D Bohus</author>
<author>A W Black</author>
<author>M Eskenazi</author>
</authors>
<title>Let’s Go Public! taking a spoken dialog system to the real world.</title>
<date>2005</date>
<booktitle>In Proc. Interspeech</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="12580" citStr="Raux et al., 2005" startWordPosition="2021" endWordPosition="2024">omplete, meaning that the system provided some schedule information to the user. While working on real user data has its advantages (large amounts of data, increased validity of the results), it also has its challenges. In the case of Let’s Go, users call from phones of varying quality (cell phones and landlines), often with background noises such as cars, infant cries, loud television sets, etc. The wide variability of the acoustic conditions makes any sound processing more prone to error On = An x lo9&apos;(Qn x F_ An ) µn (2) Gn 3 than on carefully recorded corpora. For example, as reported in (Raux et al., 2005), the original speech recognizer had been found to yield a 17% word error rate on a corpus of dialogues collected by recruiting subjects to call the system from an office. On the live Let’s Go data, that same recognizer had a 68% WER. After acoustic and language model retraining/adaptation, that number was brought down to about 30% but it is still a testimony to the difficulty of obtaining robust features, particularly from acoustics. 3.1.2 Correcting Runtime Endpointing Errors Let’s Go uses a GMM-based VAD trained on previously transcribed dialogues. Endpointing decisions are based on a fixed</context>
</contexts>
<marker>Raux, Langner, Bohus, Black, Eskenazi, 2005</marker>
<rawString>A. Raux, B. Langner, D. Bohus, A. W. Black, and M. Eskenazi. 2005. Let’s Go Public! taking a spoken dialog system to the real world. In Proc. Interspeech 2005, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>D Bohus</author>
<author>B Langner</author>
<author>A W Black</author>
<author>M Eskenazi</author>
</authors>
<title>Doing research on a deployed spoken dialogue system: One year of Let’s Go! experience.</title>
<date>2006</date>
<booktitle>In Proc. Interspeech</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="1802" citStr="Raux et al., 2006" startWordPosition="269" endWordPosition="272">art of the everyday life of many people, dialogues with artificial agents still fall far short of their human counterpart in terms of both comfort and efficiency. Besides lingering problems in speech recognition and understanding, Ward et al (Ward et al., 2005) identified turn-taking issues, specifically responsiveness, as important shortcomings. Dialogues with artificial agents are typically rigid, following a strict one-speaker-at-a-time structure with significant latencies between turns. In a previous paper, we concurred with these findings when analyzing issues with the Let’s Go system 1 (Raux et al., 2006). In contrast, empirical studies of conversation have shown that human-human dialogues commonly feature swift exchanges with little or no gap between turns, or even non-disruptive overlap (Jaffe and Feldstein, 1970; Sacks et al., 1974). According to Conversation Analysis and psycholinguistic studies, responsiveness in human conversations is possible because participants in the conversation exchange cues indicating when a turn might end, and are able to anticipate points at which they can take over the floor smoothly. Much research has been devoted to finding these cues, leading to the identifi</context>
</contexts>
<marker>Raux, Bohus, Langner, Black, Eskenazi, 2006</marker>
<rawString>A. Raux, D. Bohus, B. Langner, A. W. Black, and M. Eskenazi. 2006. Doing research on a deployed spoken dialogue system: One year of Let’s Go! experience. In Proc. Interspeech 2006, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Eskenazi</author>
</authors>
<title>A multi-layer architecture for semi-synchronous event-driven dialogue management.</title>
<date>2007</date>
<booktitle>In Proc. ASRU</booktitle>
<location>Kyoto, Japan.</location>
<marker>Eskenazi, 2007</marker>
<rawString>A. Raux, , and M. Eskenazi. 2007. A multi-layer architecture for semi-synchronous event-driven dialogue management. In Proc. ASRU 2007, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rich</author>
<author>C L Sidner</author>
</authors>
<title>Collagen: A collaboration manager for software interface agents. An International Journal: User Modeling and User-Adapted Interaction,</title>
<date>1998</date>
<pages>8--3</pages>
<contexts>
<context position="3464" citStr="Rich and Sidner, 1998" startWordPosition="523" endWordPosition="526">analysis of this conversational material”. This difficulty in formalizing higher levels of conversation might explain the relatively low interest that conversational analysts have had in semantics and discourse. Yet, as conversational analysts focused on micro-levels of dialogue such as turntaking, computational linguists uncovered and formalized macro-level dialogue structure and devised well-defined representations of semantics for at least some forms of dialogues (Allen and Perrault, 1980; Grosz and Sidner, 1986; Clark, 1996), which have in turn been implemented in spoken dialogue systems (Rich and Sidner, 1998; Allen et al., 2005). Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 1–10, Columbus, June 2008. c�2008 Association for Computational Linguistics 1.2 Current Approaches to Turn-Taking in Spoken Dialogue Systems Unfortunately, while socio- and psycho-linguists revealed the complexity of conversational turn-taking behavior, designers of practical spoken dialogue systems have stuck to a simplistic approach to end-ofturn detection (hereafter endpointing). Typically, silences in user speech are detected using a low-level Voice Activity Detector (VAD) and a turn is consider</context>
</contexts>
<marker>Rich, Sidner, 1998</marker>
<rawString>C. Rich and C.L. Sidner. 1998. Collagen: A collaboration manager for software interface agents. An International Journal: User Modeling and User-Adapted Interaction, 8(3-4):315–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sacks</author>
<author>E A Schegloff</author>
<author>G Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turntaking for conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>4</issue>
<contexts>
<context position="2037" citStr="Sacks et al., 1974" startWordPosition="305" endWordPosition="308">d et al (Ward et al., 2005) identified turn-taking issues, specifically responsiveness, as important shortcomings. Dialogues with artificial agents are typically rigid, following a strict one-speaker-at-a-time structure with significant latencies between turns. In a previous paper, we concurred with these findings when analyzing issues with the Let’s Go system 1 (Raux et al., 2006). In contrast, empirical studies of conversation have shown that human-human dialogues commonly feature swift exchanges with little or no gap between turns, or even non-disruptive overlap (Jaffe and Feldstein, 1970; Sacks et al., 1974). According to Conversation Analysis and psycholinguistic studies, responsiveness in human conversations is possible because participants in the conversation exchange cues indicating when a turn might end, and are able to anticipate points at which they can take over the floor smoothly. Much research has been devoted to finding these cues, leading to the identification of many aspects of language and dialogue that relate to turn-taking behavior, including syntax (Sacks et al., 1974; Ford and Thompson, 1996; Furo, 2001), prosody (Duncan, 1972; Orestr¨om, 1983; Chafe, 1992; Ford and Thompson, 19</context>
<context position="5892" citStr="Sacks et al., 1974" startWordPosition="905" endWordPosition="908">tion in a dialogue system, the proposed approach (classifying long silences) is not completely realistic (what happens when a gap is misclassified as a pause?) and does not attempt to optimize latency. An extension to this approach was proposed in (Takeuchi et al., 2004), in which a turn-taking decision is made every 100 ms during pauses. However, in this latter work the features are limited to timing, prosody, and syntax (part-of-speech). Also the reported classification results, with F-measures around 50% or below do not seem to be sufficient for practical use. 1We use the terminology from (Sacks et al., 1974) where a pause is a silence within a turn while a gap is a silence between turns. We use the term silence to encompass both types. Similarly, Ferrer and her colleagues (Ferrer et al., 2003) proposed the use of multiple decision trees, each triggered at a specific time in the pause, to decide to either endpoint or defer the decision to the next tree, unless the user resumes speaking. Using features like vowel duration or pitch for the region immediately preceding the silence, combined with a language model that predicts gaps based on the preceding words, Ferrer et al are able shorten latency wh</context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>H. Sacks, E. A. Schegloff, and G. Jefferson. 1974. A simplest systematics for the organization of turntaking for conversation. Language, 50(4):696–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sato</author>
<author>R Higashinaka</author>
<author>M Tamoto</author>
<author>M Nakano</author>
<author>K Aikawa</author>
</authors>
<title>Learning decision trees to determine turn-taking by spoken dialogue systems.</title>
<date>2002</date>
<booktitle>In ICSLP 2002,</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="4936" citStr="Sato et al., 2002" startWordPosition="748" endWordPosition="751">ppen when a pause lasts longer than the threshold and gets wrongly classified as a gap1. Second, latency occurs at every gap, because the system must wait for the duration of the threshold before classifying a silence as gap. When setting the threshold, system designers must consider the trade-off between these two issues: setting a low threshold reduces latency but increases FA rate, while setting a high threshold reduces FA rate but increases latency. To help overcome the shortcomings of the singlethreshold approach, several researchers have proposed to exploit various features. Sato et al (Sato et al., 2002) used decision trees to classify pauses longer than 750 ms as gap or pause. By using features from semantics, syntax, dialogue state, and prosody, they were able to improve the classification accuracy from a baseline of 76.2% to 83.9%. While this important study shows encouraging results on the value of using various sources of information in a dialogue system, the proposed approach (classifying long silences) is not completely realistic (what happens when a gap is misclassified as a pause?) and does not attempt to optimize latency. An extension to this approach was proposed in (Takeuchi et al</context>
<context position="26645" citStr="Sato et al., 2002" startWordPosition="4350" endWordPosition="4353">y, speaker, and discourse. The maximum relative latency reductions for each feature set range from 12% to 22%. When using all features, the performance improves by a small but significant amount compared to any single set, up to a maximum latency reduction of 24%. This confirms that the algorithm is able to combine features effectively, and that the features themselves are not completely redundant. However, while removing semantic or timing features from the complete set degrades the performance, this is not the case for discourse, speaker, nor prosodic features. This result, similar to what (Sato et al., 2002) reported in their own experiment, indicates that prosodic features might be redundant with semantic and timing features. 4.3 Live Evaluation We confirmed the offline evaluation’s findings by implementing the proposed approach in Let’s Go’s Interaction Manager. Since prosodic features were not found to be helpful and since their online extraction is costly and error-prone, we did not include them. At the beginning of each dialogue, the system was randomly set as a baseline version, using a 700 ms fixed threshold, or as an experimental version using the tree learned from the offline corpus. Res</context>
</contexts>
<marker>Sato, Higashinaka, Tamoto, Nakano, Aikawa, 2002</marker>
<rawString>R. Sato, R. Higashinaka, M. Tamoto, M. Nakano, and K. Aikawa. 2002. Learning decision trees to determine turn-taking by spoken dialogue systems. In ICSLP 2002, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kare Sjolander</author>
</authors>
<title>The snack sound toolkit.</title>
<date>2004</date>
<note>http://www.speech.kth.se/snack/.</note>
<contexts>
<context position="21090" citStr="Sjolander, 2004" startWordPosition="3432" endWordPosition="3433">es more prone to pauses. Indeed, 91% of complete utterances that contain a positive marker are single-word, against 67% for negative markers. 3.3.4 Prosody We extracted three types of prosodic features: acoustic energy of the last vowel, pitch of the last voiced region, and duration of the last vowel. Vowel 5 location and duration were estimated by performing phoneme alignment with the speech recognizer. Duration was normalized to account for both vowel and speaker identity. Energy was computed as the logtransformed signal intensity on 10ms frames. Pitch was extracted using the Snack toolkit (Sjolander, 2004), also at 10ms intervals. For both energy and pitch, the slope of the contour was computed by linear regression, and the mean value was normalized by Z-transformation using statistics of the dialogueso-far. As a consequence, all threshold values for means are expressed in terms of standard deviations from the current speaker’s mean value. Vowel energy, both slope and mean, yielded the highest correlation with silence finality, although it did not rank as high as features from other categories. As expected, vowels immediately preceding gaps tend to have lower and falling intensity, whereas risi</context>
</contexts>
<marker>Sjolander, 2004</marker>
<rawString>Kare Sjolander. 2004. The snack sound toolkit. http://www.speech.kth.se/snack/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Takeuchi</author>
<author>N Kitaoka</author>
<author>S Nakagawa</author>
</authors>
<title>Timing detection for realtime dialog systems using prosodic and linguistic information.</title>
<date>2004</date>
<booktitle>In Proc. Speech Prosody 04,</booktitle>
<location>Nara, Japan.</location>
<contexts>
<context position="5544" citStr="Takeuchi et al., 2004" startWordPosition="848" endWordPosition="851"> et al., 2002) used decision trees to classify pauses longer than 750 ms as gap or pause. By using features from semantics, syntax, dialogue state, and prosody, they were able to improve the classification accuracy from a baseline of 76.2% to 83.9%. While this important study shows encouraging results on the value of using various sources of information in a dialogue system, the proposed approach (classifying long silences) is not completely realistic (what happens when a gap is misclassified as a pause?) and does not attempt to optimize latency. An extension to this approach was proposed in (Takeuchi et al., 2004), in which a turn-taking decision is made every 100 ms during pauses. However, in this latter work the features are limited to timing, prosody, and syntax (part-of-speech). Also the reported classification results, with F-measures around 50% or below do not seem to be sufficient for practical use. 1We use the terminology from (Sacks et al., 1974) where a pause is a silence within a turn while a gap is a silence between turns. We use the term silence to encompass both types. Similarly, Ferrer and her colleagues (Ferrer et al., 2003) proposed the use of multiple decision trees, each triggered at</context>
</contexts>
<marker>Takeuchi, Kitaoka, Nakagawa, 2004</marker>
<rawString>M. Takeuchi, N. Kitaoka, and S. Nakagawa. 2004. Timing detection for realtime dialog systems using prosodic and linguistic information. In Proc. Speech Prosody 04, Nara, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ward</author>
<author>A Rivera</author>
<author>K Ward</author>
<author>D Novick</author>
</authors>
<title>Root causes of lost time and user stress in a simple dialog system.</title>
<date>2005</date>
<booktitle>In Interspeech</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1445" citStr="Ward et al., 2005" startWordPosition="218" endWordPosition="221">s, the proposed method reduces latency by up to 24% over a fixed threshold baseline. Offline evaluation results were confirmed by implementing the proposed algorithm in the Let’s Go system. 1 Introduction 1.1 Responsiveness in Dialogue Although the quality of speech technologies has improved drastically and spoken interaction with machines is becoming a part of the everyday life of many people, dialogues with artificial agents still fall far short of their human counterpart in terms of both comfort and efficiency. Besides lingering problems in speech recognition and understanding, Ward et al (Ward et al., 2005) identified turn-taking issues, specifically responsiveness, as important shortcomings. Dialogues with artificial agents are typically rigid, following a strict one-speaker-at-a-time structure with significant latencies between turns. In a previous paper, we concurred with these findings when analyzing issues with the Let’s Go system 1 (Raux et al., 2006). In contrast, empirical studies of conversation have shown that human-human dialogues commonly feature swift exchanges with little or no gap between turns, or even non-disruptive overlap (Jaffe and Feldstein, 1970; Sacks et al., 1974). Accord</context>
</contexts>
<marker>Ward, Rivera, Ward, Novick, 2005</marker>
<rawString>N. Ward, A. Rivera, K. Ward, and D. Novick. 2005. Root causes of lost time and user stress in a simple dialog system. In Interspeech 2005, Lisbon, Portugal.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>