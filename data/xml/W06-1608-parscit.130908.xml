<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000140">
<title confidence="0.990862">
The impact of parse quality on syntactically-informed statistical machine
translation
</title>
<author confidence="0.989621">
Chris Quirk and Simon Corston-Oliver
</author>
<affiliation confidence="0.95781">
Microsoft Research
</affiliation>
<address confidence="0.945386">
One Microsoft Way
Redmond, WA 98052 USA
</address>
<email confidence="0.999375">
{chrisq,simonco}@microsoft.com
</email>
<sectionHeader confidence="0.993909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997935">
We investigate the impact of parse quality
on a syntactically-informed statistical ma-
chine translation system applied to techni-
cal text. We vary parse quality by vary-
ing the amount of data used to train the
parser. As the amount of data increases,
parse quality improves, leading to im-
provements in machine translation output
and results that significantly outperform a
state-of-the-art phrasal baseline.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999819">
The current study is a response to a question
that proponents of syntactically-informed machine
translation frequently encounter: How sensitive is
a syntactically-informed machine translation sys-
tem to the quality of the input syntactic analysis?
It has been shown that phrasal machine translation
systems are not affected by the quality of the in-
put word alignments (Koehn et al., 2003). This
finding has generally been cast in favorable terms:
such systems are robust to poor quality word align-
ment. A less favorable interpretation of these re-
sults might be to conclude that phrasal statistical
machine translation (SMT) systems do not stand
to benefit from improvements in word alignment.
In a similar vein, one might ask whether con-
temporary syntactically-informed machine trans-
lation systems would benefit from improvements
in parse accuracy. One possibility is that cur-
rent syntactically-informed SMT systems are de-
riving only limited value from the syntactic anal-
yses, and would therefore not benefit from im-
proved analyses. Another possibility is that syn-
tactic analysis does indeed contain valuable infor-
mation that could be exploited by machine learn-
ing techniques, but that current parsers are not of
sufficient quality to be of use in SMT.
With these questions and concerns, let us be-
gin. Following some background discussion we
describe a set of experiments intended to elucidate
the impact of parse quality on SMT.
</bodyText>
<sectionHeader confidence="0.987651" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999965666666667">
We trained statistical machine translation systems
on technical text. In the following sections we
provide background on the data used for training,
the dependency parsing framework used to pro-
duce treelets, the treelet translation framework and
salient characteristics of the target languages.
</bodyText>
<subsectionHeader confidence="0.97776">
2.1 Dependency parsing
</subsectionHeader>
<bodyText confidence="0.988815944444445">
Dependency analysis is an alternative to con-
stituency analysis (Tesni`ere, 1959; Melˇcuk, 1988).
In a dependency analysis of syntax, words di-
rectly modify other words, with no intervening
non-lexical nodes. We use the terms child node
and parent node to denote the tokens in a depen-
dency relation. Each child has a single parent, with
the lexical root of the sentence dependent on a syn-
thetic ROOT node.
We use the parsing approach described in
(Corston-Oliver et al., 2006). The parser is trained
on dependencies extracted from the English Penn
Treebank version 3.0 (Marcus et al., 1993) by
using the head-percolation rules of (Yamada and
Matsumoto, 2003).
Given a sentence x, the goal of the parser is to
find the highest-scoring parse yˆ among all possible
parses y E Y:
</bodyText>
<equation confidence="0.7543155">
yˆ = argmax s(x,y) (1)
yEY
</equation>
<bodyText confidence="0.754253">
The score of a given parse y is the sum of the
</bodyText>
<page confidence="0.990204">
62
</page>
<note confidence="0.718503333333333">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 62–69,
Sydney, July 2006. c�2006 Association for Computational Linguistics
scores of all its dependency links (i, j) E y:
</note>
<equation confidence="0.978386">
s(x,y) = Z d(i, j) = Z w · f(i, j) (2)
(i,j)Ey (i,j)Ey
</equation>
<bodyText confidence="0.999952931034483">
where the link (i, j) indicates a parent-child de-
pendency between the token at position i and the
token at position j. The score d(i, j) of each de-
pendency link (i, j) is further decomposed as the
weighted sum of its features f(i, j).
The feature vector f(i, j) computed for each
possible parent-child dependency includes the
part-of-speech (POS), lexeme and stem of the par-
ent and child tokens, the POS of tokens adjacent
to the child and parent, and the POS of each to-
ken that intervenes between the parent and child.
Various combinations of these features are used,
for example a new feature is created that combines
the POS of the parent, lexeme of the parent, POS
of the child and lexeme of the child. Each feature
is also conjoined with the direction and distance
of the parent, e.g. does the child precede or follow
the parent, and how many tokens intervene?
To set the weight vector w, we train twenty
averaged perceptrons (Collins, 2002) on different
shuffles of data drawn from sections 02–21 of the
Penn Treebank. The averaged perceptrons are then
combined to form a Bayes Point Machine (Her-
brich et al., 2001; Harrington et al., 2003), result-
ing in a linear classifier that is competitive with
wide margin techniques.
To find the optimal parse given the weight vec-
tor w and feature vector f(i, j) we use the decoder
described in (Eisner, 1996).
</bodyText>
<subsectionHeader confidence="0.998088">
2.2 Treelet translation
</subsectionHeader>
<bodyText confidence="0.999456571428571">
For syntactically-informed translation, we fol-
low the treelet translation approach described
in (Quirk et al., 2005). In this approach, trans-
lation is guided by treelet translation pairs. Here,
a treelet is a connected subgraph of a dependency
tree. A treelet translation pair consists of a source
treelet S, a target treelet T, and a word alignment
A C S x T such that for all s E S, there exists a
unique t E T such that (s,t) E A, and if t is the root
of T, there is a unique s E S such that (s,t) E A.
Translation of a sentence begins by parsing
that sentence into a dependency representation.
This dependency graph is partitioned into treelets;
like (Koehn et al., 2003), we assume a uniform
probability distribution over all partitions. Each
source treelet is matched to a treelet translation
pair; together, the target language treelets in those
treelet translation pairs will form the target trans-
lation. Next the target language treelets are joined
to form a single tree: the parent of the root of each
treelet is dictated by the source. Let tr be the root
of the target language treelet, and sr be the source
node aligned to it. If sr is the root of the source
sentence, then tr is made the root of the target lan-
guage tree. Otherwise let sp be the parent of sr,
and tp be the target node aligned to sp: tr is at-
tached to tp. Finally the ordering of all the nodes
is determined, and the target tree is specified, and
the target sentence is produced by reading off the
labels of the nodes in order.
Translations are scored according to a log-linear
combination of feature functions, each scoring dif-
ferent aspects of the translation process. We use a
beam search decoder to find the best translation T*
according to the log-linear combination of models:
</bodyText>
<equation confidence="0.998764">
� T* = argmax Z λff(S,T,A) (3)
T
</equation>
<bodyText confidence="0.998800423076923">
of mappings to be
O(n3) in the worst case, where
n is the number of nodes in the dependency tree.
The models include inverted and direct channel
models estimated by relative frequency, lexical
weighting channel models following (Vogel et al.,
2003), a trigram target language model using mod-
ified Kneser-Ney smoothing (Goodman, 2001),
an order model following (Quirk et al., 2005),
and word count and phrase count functions. The
weights for these models are determined using the
method described in (Och, 2003).
To estimate the models and extract the treelets,
we begin from a parallel corpus. First the cor-
pus is word-aligned using GIZA++ (Och and Ney,
2000), then the source sentence are parsed, and
finally dependencies are projected onto the target
side following the heuristics described in (Quirk et
al., 2005). This word aligned parallel dependency
tree corpus provides training material for an order
model and a target language tree-based language
model. We also extract treelet translation pairs
from this parallel corpus. To limit the combina-
torial explosion of treelets, we only gather treelets
that contain at most four words and at most two
gaps in the surface string. This limits the number
</bodyText>
<subsectionHeader confidence="0.988026">
2.3 Language pairs
</subsectionHeader>
<bodyText confidence="0.999847">
In the present paper we focus on English-to-
German and English-to-Japanese machine tran
</bodyText>
<equation confidence="0.4271145">
sla-
fEF
</equation>
<page confidence="0.956025">
63
</page>
<bodyText confidence="0.674504">
you can set this property using Visual Basic
Sie können diese Eigenschaft auch mit Visual Basic festlegen
</bodyText>
<figureCaption confidence="0.999343">
Figure 1: Example German-English and Japanese-English sentence pairs, with word alignments.
</figureCaption>
<bodyText confidence="0.994959166666667">
tion. Both German and Japanese differ markedly
from English in ways that we believe illumi-
nate well the strengths of a syntactically-informed
SMT system. We provide a brief sketch of the lin-
guistic characteristics of German and Japanese rel-
evant to the present study.
</bodyText>
<subsectionHeader confidence="0.556361">
2.3.1 German
</subsectionHeader>
<bodyText confidence="0.99989815">
Although English and German are closely re-
lated – they both belong to the western branch of
the Germanic family of Indo-European languages
– the languages differ typologically in ways that
are especially problematic for current approaches
to statistical machine translation as we shall now
illustrate. We believe that these typological differ-
ences make English-to-German machine transla-
tion a fertile test bed for syntax-based SMT.
German has richer inflectional morphology than
English, with obligatory marking of case, num-
ber and lexical gender on nominal elements and
person, number, tense and mood on verbal ele-
ments. This morphological complexity, combined
with pervasive, productive noun compounding is
problematic for current approaches to word align-
ment (Corston-Oliver and Gamon, 2004).
Equally problematic for machine translation is
the issue of word order. The position of verbs is
strongly determined by clause type. For exam-
ple, in main clauses in declarative sentences, finite
verbs occur as the second constituent of the sen-
tence, but certain non-finite verb forms occur in fi-
nal position. In Figure 1, for example, the English
“can” aligns with German “k¨onnen” in second po-
sition and “set” aligns with German “festlegen” in
final position.
Aside from verbs, German is usually charac-
terized as a “free word-order” language: major
constituents of the sentence may occur in various
orders, so-called “separable prefixes” may occur
bound to the verb or may detach and occur at a
considerable distance from the verb on which they
depend, and extraposition of various kinds of sub-
ordinate clause is common. In the case of extrapo-
sition, for example, more than one third of relative
clauses in human-translated German technical text
are extraposed. For comparable English text the
figure is considerably less than one percent (Ga-
mon et al., 2002).
</bodyText>
<subsectionHeader confidence="0.707649">
2.3.2 Japanese
</subsectionHeader>
<bodyText confidence="0.999967043478261">
Word order in Japanese is rather different from
English. English has the canonical constituent or-
der subject-verb-object, whereas Japanese prefers
subject-object-verb order. Prepositional phrases
in English generally correspond to postpositional
phrases in Japanese. Japanese noun phrases are
strictly head-final whereas English noun phrases
allow postmodifiers such as prepositional phrases,
relative clauses and adjectives. Japanese has lit-
tle nominal morphology and does not obligatorily
mark number, gender or definiteness. Verbal mor-
phology in Japanese is complex with morphologi-
cal marking of tense, mood, and politeness. Top-
icalization and subjectless clauses are pervasive,
and problematic for current SMT approaches.
The Japanese sentence in Figure 1 illustrates
several of these typological differences. The
sentence-initial imperative verb “move” in the En-
glish corresponds to a sentence-final verb in the
Japanese. The Japanese translation of the object
noun phrase “the camera slider switch” precedes
the verb in Japanese. The English preposition “to”
aligns to a postposition in Japanese.
</bodyText>
<sectionHeader confidence="0.9995" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9996702">
Our goal in the current paper is to measure the
impact of parse quality on syntactically-informed
statistical machine translation. One method for
producing parsers of varying quality might be to
train a parser and then to transform its output, e.g.
</bodyText>
<page confidence="0.997186">
64
</page>
<bodyText confidence="0.999988837209303">
by replacing the parser’s selection of the parent for
certain tokens with different nodes.
Rather than randomly adding noise to the
parses, we decided to vary the quality in ways that
more closely mimic the situation that confronts us
as we develop machine translation systems. An-
notating data for POS requires considerably less
human time and expertise than annotating syntac-
tic relations. We therefore used an automatic POS
tagger (Toutanova et al., 2003) trained on the com-
plete training section of the Penn Treebank (sec-
tions 02–21). Annotating syntactic dependencies
is time consuming and requires considerable lin-
guistic expertise.1 We can well imagine annotat-
ing syntactic dependencies in order to develop a
machine translation system by annotating first a
small quantity of data, training a parser, training a
system that uses the parses produced by that parser
and assessing the quality of the machine transla-
tion output. Having assessed the quality of the out-
put, one might annotate additional data and train
systems until it appears that the quality of the ma-
chine translation output is no longer improving.
We therefore produced parsers of varying quality
by training on the first n sentences of sections 02–
21 of the Penn Treebank, where n ranged from 250
to 39,892 (the complete training section). At train-
ing time, the gold-standard POS tags were used.
For parser evaluation and for the machine transla-
tion experiments reported here, we used an auto-
matic POS tagger (Toutanova et al., 2003) trained
on sections 02–21 of the Penn Treebank.
We trained English-to-German and English-to-
Japanese treelet translation systems on approxi-
mately 500,000 manually aligned sentence pairs
drawn from technical computer documentation.
The sentence pairs consisted of the English source
sentence and a human-translation of that sentence.
Table 1 summarizes the characteristics of this data.
Note that German vocabulary and singleton counts
are slightly more than double the corresponding
English counts due to complex morphology and
pervasive compounding (see section 2.3.1).
</bodyText>
<subsectionHeader confidence="0.998256">
3.1 Parser accuracy
</subsectionHeader>
<bodyText confidence="0.994708">
To evaluate the accuracy of the parsers trained on
different samples of sentences we used the tradi-
</bodyText>
<footnote confidence="0.8688784">
1Various people have suggested to us that the linguistic
expertise required to annotate syntactic dependencies is less
than the expertise required to apply a formal theory of con-
stituency like the one that informs the Penn Treebank. We
tend to agree, but have not put this claim to the test.
</footnote>
<figure confidence="0.837345">
0 10,000 20,000 30,000 40,000
Sample size
</figure>
<figureCaption confidence="0.7861695">
Figure 2: Unlabeled dependency accuracy of
parsers trained on different numbers of sentences.
</figureCaption>
<bodyText confidence="0.990144970588235">
The graph compares accuracy on the blind test sec-
tion of the Penn Treebank to accuracy on a set of
250 sentences drawn from technical text. Punctu-
ation tokens are excluded from the measurement
of dependency accuracy.
tional blind test section of the Penn Treebank (sec-
tion 23). As is well-known in the parsing commu-
nity, parse quality degrades when a parser trained
on the Wall Street Journal text in the Penn Tree-
bank is applied to a different genre or semantic do-
main. Since the technical materials that we were
training the translation system on differ from the
Wall Street Journal in lexicon and syntax, we an-
notated a set of 250 sentences of technical material
to use in evaluating the parser. Each of the authors
independently annotated the same set of 250 sen-
tences. The annotation took less than six hours for
each author to complete. Inter-annotator agree-
ment excluding punctuation was 91.8%. Differ-
ences in annotation were resolved by discussion,
and the resulting set of annotations was used to
evaluate the parsers.
Figure 2 shows the accuracy of parsers trained
on samples of various sizes, excluding punctua-
tion tokens from the evaluation, as is customary
in evaluating dependency parsers. When mea-
sured against section 23 of the Penn Treebank,
the section traditionally used for blind evaluation,
the parsers range in accuracy from 77.8% when
trained on 250 sentences to 90.8% when trained
on all of sections 02–21. As expected, parse accu-
racy degrades when measured on text that differs
greatly from the training text. A parser trained on
250 Penn Treebank sentences has a dependency
</bodyText>
<figure confidence="0.99903125">
Dependency accuracy
95%
90%
75%
85%
80%
PTB Section 23
Technical text
</figure>
<page confidence="0.997025">
65
</page>
<table confidence="0.999561285714286">
English German English Japanese
Training Sentences 515,318 500,000
Words 7,292,903 8,112,831 7,909,198 9,379,240
Vocabulary 59,473 134,829 66,731 68,048
Singletons 30,452 66,724 50,381 52,911
Test Sentences 2,000 2,000
Words 28,845 31,996 30,616 45,744
</table>
<tableCaption confidence="0.999798">
Table 1: Parallel data characteristics
</tableCaption>
<bodyText confidence="0.999951041666667">
accuracy of 76.6% on the technical text. A parser
trained on the complete Penn Treebank training
section has a dependency accuracy of 84.3% on
the technical text.
Since the parsers make extensive use of lexi-
cal features, it is not surprising that the perfor-
mance on the two corpora should be so similar
with only 250 training sentences; there were not
sufficient instances of each lexical item to train re-
liable weights or lexical features. As the amount
of training data increases, the parsers are able to
learn interesting facts about specific lexical items,
leading to improved accuracy on the Penn Tree-
bank. Many of the lexical items that occur in the
Penn Treebank, however, occur infrequently or not
at all in the technical materials so the lexical infor-
mation is of little benefit. This reflects the mis-
match of content. The Wall Street Journal articles
in the Penn Treebank concern such topics as world
affairs and the policies of the Reagan administra-
tion; these topics are absent in the technical mate-
rials. Conversely, the Wall Street Journal articles
contain no discussion of such topics as the intrica-
cies of SQL database queries.
</bodyText>
<subsectionHeader confidence="0.999632">
3.2 Translation quality
</subsectionHeader>
<bodyText confidence="0.999938416666667">
Table 2 presents the impact of parse quality on a
treelet translation system, measured using BLEU
(Papineni et al., 2002). Since our main goal is to
investigate the impact of parser accuracy on trans-
lation quality, we have varied the parser training
data, but have held the MT training data, part-of-
speech-tagger, and all other factors constant. We
observe an upward trend in BLEU score as more
training data is made available to the parser; the
trend is even clearer in Japanese.2 As a baseline,
we include right-branching dependency trees, i.e.,
trees in which the parent of each word is its left
</bodyText>
<footnote confidence="0.99162925">
2This is particularly encouraging since various people
have remarked to us that syntax-based SMT systems may
be disadvantaged under n-gram scoring techniques such as
BLEU.
</footnote>
<table confidence="0.999720875">
EG EJ
Phrasal decoder 31.7±1.2 32.9±0.9
Treelet decoder 31.4±1.3 28.0±0.7
Right-branching
250 sentences 32.8±1.4 34.1±0.9
2,500 sentences 33.0±1.4 34.6±1.0
25,000 sentences 33.7±1.5 35.7±0.9
39,892 sentences 33.6±1.5 36.0±1.0
</table>
<tableCaption confidence="0.993827">
Table 2: BLEU score vs. decoder and parser vari-
</tableCaption>
<bodyText confidence="0.981011433333333">
ants. Here sentences refer to the amount of parser
training data, not MT training data.
neighbor and the root of a sentence is the first
word. With this analysis, treelets are simply sub-
sequences of the sentence, and therefore are very
similar to the phrases of Phrasal SMT. In English-
to-German, this result produces results very com-
parable to a phrasal SMT system (Koehn et al.,
2003) trained on the same data. For English-to-
Japanese, however, this baseline performs much
worse than a phrasal SMT system. Although
phrases and treelets should be nearly identical
under this scenario, the decoding constraints are
somewhat different: the treelet decoder assumes
phrasal cohesion during translation. This con-
straint may account for the drop in quality.
Since the confidence intervals for many pairs
overlap, we ran pairwise tests for each system to
determine which differences were significant at
the p &lt; 0.05 level using the bootstrap method de-
scribed in (Zhang and Vogel, 2004); Table 3 sum-
marizes this comparison. Neither language pair
achieves a statistically significant improvement
from increasing the training data from 25,000
pairs to the full training set; this is not surprising
since the increase in parse accuracy is quite small
(90.2% to 90.8% on Wall Street Journal text).
To further understand what differences in de-
pendency analysis were affecting translation qual-
ity, we compared a treelet translation system that
</bodyText>
<page confidence="0.981816">
66
</page>
<table confidence="0.964767714285714">
Pharaoh Right-branching 250 2,500 25,000 39,892
Pharaoh ∼ &gt; &gt; &gt; &gt;
Right-branching &gt; &gt; &gt; &gt;
250 ∼ &gt; &gt;
2,500 &gt; &gt;
25,000 ∼
(a) English-German
Pharaoh Right-branching 250 2,500 25,000 39,892
Pharaoh &lt; ∼ &gt; &gt; &gt;
Right-branching &gt; &gt; &gt; &gt;
250 &gt; &gt; &gt;
2,500 &gt; &gt;
25,000 ∼
(b) English-Japanese
</table>
<tableCaption confidence="0.879009666666667">
Table 3: Pairwise statistical significance tests. &gt; indicates that the system on the top is significantly better
than the system on the left; &lt; indicates that the system on top is significantly worse than the system on
the left; ∼ indicates that difference between the two systems is not statistically significant.
</tableCaption>
<figure confidence="0.8504675">
100 1000 10000 100000
Parser training sentences
</figure>
<figureCaption confidence="0.9880745">
Figure 3: BLEU score vs. number of sentences
used to train the dependency parser
</figureCaption>
<bodyText confidence="0.998438585365853">
used a parser trained on 250 Penn Treebank sen-
tences to a treelet translation system that used
a parser trained on 39,892 Treebank sentences.
From the test data, we selected 250 sentences
where these two parsers produced different anal-
yses. A native speaker of German categorized the
differences in machine translation output as either
improvements or regressions. We then examined
and categorized the differences in the dependency
analyses. Table 4 summarizes the results of this
comparison. Note that this table simply identifies
correlations between parse changes and translation
changes; it does not attempt to identify a causal
link. In the analysis, we borrow the term “NP
[Noun Phrase] identification” from constituency
analysis to describe the identification of depen-
dency treelets spanning complete noun phrases.
There were 141 sentences for which the ma-
chine translated output improved, 71 sentences for
which the output regressed and 38 sentences for
which the output was identical. Improvements in
the attachment of prepositions, adverbs, gerunds
and dependent verbs were common amongst im-
proved translations, but rare amongst regressed
translations. Correct identification of the depen-
dent of a preposition3 was also much more com-
mon amongst improvements.
Certain changes, such as improved root identifi-
cation and final punctuation attachment, were very
common across the corpus. Therefore their com-
mon occurrence amongst regressions is not very
surprising. It was often the case that improve-
ments in root identification or final punctuation at-
tachment were offset by regressions elsewhere in
the same sentence.
Improvements in the parsers are cases where
the syntactic analysis more closely resembles the
analysis of dependency structure that results from
applying Yamada and Matsumoto’s head-finding
rules to the Penn Treebank. Figure 4 shows dif-
ferent parses produced by parsers trained on dif-
</bodyText>
<footnote confidence="0.894892666666667">
3In terms of constituency analysis, a prepositional phrase
should consist of a preposition governing a single noun
phrase
</footnote>
<figure confidence="0.956175285714286">
BLEU score
37
36
35
34
33
32
Japanese
German
67
ROOT You can manipulate Microsoft Access objects from another application that also supports automation .
(a) Dependency analysis produced by parser trained on 2 50 Wall Street Journal sentences.
ROOT You can manipulate Microsoft Access objects from another application that also supports automation .
(b) Dependency analysis produced by parser trained on 39&apos;892 Wall Street Journal sentences.
</figure>
<figureCaption confidence="0.999929">
Figure 4: Parses produced by parsers trained on different numbers of sentences.
</figureCaption>
<bodyText confidence="0.999836962962963">
ferent numbers of sentences. The parser trained
on 250 sentences incorrectly attaches the prepo-
sition “from” as a dependent of the noun “ob-
jects” whereas the parser trained on the complete
Penn Treebank training section correctly attaches
the preposition as a dependent of the verb “ma-
nipulate”. These two parsers also yield different
analyses of the phrase “Microsoft Access objects”.
In parse (a), “objects” governs “Office” and “Of-
fice” in turn governs “Microsoft”. This analy-
sis is linguistically well-motivated, and makes a
treelet spanning “Microsoft Office” available to
the treelet translation system. In parse (b), the
parser has analyzed this phrase so that “objects”
directly governs “Microsoft” and “Office”. The
analysis more closely reflects the flat branching
structure of the Penn Treebank but obscures the
affinity of “Microsoft” and “Office”.
An additional measure of parse utility for MT
is the amount of translation material that can be
extracted from a parallel corpus. We increased the
parser training data from 250 sentences to 39,986
sentences, but held the number of aligned sentence
pairs used train other modules constant. The count
of treelet translation pairs occurring at least twice
in the English-German parallel corpus grew from
1,895,007 to 2,010,451.
</bodyText>
<sectionHeader confidence="0.999723" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9974665">
We return now to the questions and concerns
raised in the introduction. First, is a treelet SMT
system sensitive to parse quality? We have shown
that such a system is sensitive to the quality of
</bodyText>
<table confidence="0.9993977">
Error category Regress Improve
Attachment of prep 1% 22%
Root identification 13% 28%
Final punctuation 18% 30%
Coordination 6% 16%
Dependent verbs 14% 32%
Arguments of verb 6% 15%
NP identification 24% 33%
Dependent of prep 0% 7%
Other attachment 3% 22%
</table>
<tableCaption confidence="0.978296">
Table 4: Error analysis, showing percentage of
</tableCaption>
<bodyText confidence="0.95157195">
regressed and improved translations exhibiting a
parse improvement in each specified category
the input syntactic analyses. With the less accu-
rate parsers that result from training on extremely
small numbers of sentences, performance is com-
parable to state-of-the-art phrasal SMT systems.
As the amount of data used to train the parser in-
creases, both English-to-German and English-to-
Japanese treelet SMT improve, and produce re-
sults that are statistically significantly better than
the phrasal baseline.
In the introduction we mentioned the concern
that others have raised when we have presented
our research: syntax might contain valuable infor-
mation but current parsers might not be of suffi-
cient quality. It is certainly true that the accuracy
of the best parser used here falls well short of what
we might hope for. A parser that achieves 90.8%
dependency accuracy when trained on the Penn
Treebank Wall Street Journal corpus and evalu-
</bodyText>
<page confidence="0.997947">
68
</page>
<bodyText confidence="0.999931322580645">
ated on comparable text degrades to 84.3% accu-
racy when evaluated on technical text. Despite the
degradation in parse accuracy caused by the dra-
matic differences between the Wall Street Journal
text and the technical articles, the treelet SMT sys-
tem was able to extract useful patterns. Research
on syntactically-informed SMT is not impeded by
the accuracy of contemporary parsers.
One significant finding is that as few as 250
sentences suffice to train a dependency parser for
use in the treelet SMT framework. To date our
research has focused on translation from English
to other languages. One concern in applying the
treelet SMT framework to translation from lan-
guages other than English has been the expense
of data annotation: would we require 40,000 sen-
tences annotated for syntactic dependencies, i.e.,
an amount comparable to the Penn Treebank, in
order to train a parser that was sufficiently accu-
rate to achieve the machine translation quality that
we have seen when translating from English? The
current study gives hope that source languages can
be added with relatively modest investments in
data annotation. As more data is annotated with
syntactic dependencies and more accurate parsers
are trained, we would hope to see similar improve-
ments in machine translation output.
We challenge others who are conducting re-
search on syntactically-informed SMT to verify
whether or to what extent their systems are sen-
sitive to parse quality.
</bodyText>
<sectionHeader confidence="0.999078" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998971471428572">
M. Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Simon Corston-Oliver and Michael Gamon. 2004.
Normalizing German and English inflectional mor-
phology to improve statistical word alignment. In
R. E. Frederking and K. B. Taylor, editors, Machine
translation: From real users to research. Springer
Verlag.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency pars-
ing using Bayes Point Machines. In Proceedings of
HLT/NAACL.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of COLING, pages 340–345.
Michael Gamon, Eric Ringger, Zhu Zhang, Robert
Moore, and Simon Corston-Oliver. 2002. Extrapo-
sition: A case study in German sentence realization.
In Proceedings of COLING, pages 301–307.
Joshua Goodman. 2001. A bit of progress in lan-
guage modeling, extended version. Technical Re-
port MSR-TR-2001-72, Microsoft Research.
Edward Harrington, Ralf Herbrich, Jyrki Kivinen,
John C. Platt, and Robert C. Williamson. 2003. On-
line bayes point machines. In Proc. 7th Pacific-Asia
Conference on Knowledge Discovery and Data Min-
ing, pages 241–252.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes Point Machines. Journal of Machine
Learning Research, pages 245–278.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings ofHLT/NAACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
Igor A. Melˇcuk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
ACL, pages 440–447, Hongkong, China, October.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311–318, Philadelpha, Pennsylvania.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of the ACL.
Lucien Tesni`ere. 1959. ´El´ements de syntaxe struc-
turale. Librairie C. Klincksieck.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings ofHLT/EMNLP, pages 252–259.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,
Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In Proceedings of the MT Summit.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings ofIWPT, pages 195–206.
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for mt evaluation metrics. In Pro-
ceedings of TMI.
</reference>
<page confidence="0.999317">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.675903">
<title confidence="0.991119">The impact of parse quality on syntactically-informed statistical machine translation</title>
<author confidence="0.909211">Quirk</author>
<affiliation confidence="0.911141">Microsoft</affiliation>
<address confidence="0.8778505">One Microsoft Redmond, WA 98052</address>
<abstract confidence="0.998860909090909">We investigate the impact of parse quality on a syntactically-informed statistical machine translation system applied to technical text. We vary parse quality by varying the amount of data used to train the parser. As the amount of data increases, parse quality improves, leading to improvements in machine translation output and results that significantly outperform a state-of-the-art phrasal baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4483" citStr="Collins, 2002" startWordPosition="724" endWordPosition="725">f-speech (POS), lexeme and stem of the parent and child tokens, the POS of tokens adjacent to the child and parent, and the POS of each token that intervenes between the parent and child. Various combinations of these features are used, for example a new feature is created that combines the POS of the parent, lexeme of the parent, POS of the child and lexeme of the child. Each feature is also conjoined with the direction and distance of the parent, e.g. does the child precede or follow the parent, and how many tokens intervene? To set the weight vector w, we train twenty averaged perceptrons (Collins, 2002) on different shuffles of data drawn from sections 02–21 of the Penn Treebank. The averaged perceptrons are then combined to form a Bayes Point Machine (Herbrich et al., 2001; Harrington et al., 2003), resulting in a linear classifier that is competitive with wide margin techniques. To find the optimal parse given the weight vector w and feature vector f(i, j) we use the decoder described in (Eisner, 1996). 2.2 Treelet translation For syntactically-informed translation, we follow the treelet translation approach described in (Quirk et al., 2005). In this approach, translation is guided by tree</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
</authors>
<title>Normalizing German and English inflectional morphology to improve statistical word alignment. In</title>
<date>2004</date>
<editor>R. E. Frederking and K. B. Taylor, editors,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="9308" citStr="Corston-Oliver and Gamon, 2004" startWordPosition="1526" endWordPosition="1529">ypologically in ways that are especially problematic for current approaches to statistical machine translation as we shall now illustrate. We believe that these typological differences make English-to-German machine translation a fertile test bed for syntax-based SMT. German has richer inflectional morphology than English, with obligatory marking of case, number and lexical gender on nominal elements and person, number, tense and mood on verbal elements. This morphological complexity, combined with pervasive, productive noun compounding is problematic for current approaches to word alignment (Corston-Oliver and Gamon, 2004). Equally problematic for machine translation is the issue of word order. The position of verbs is strongly determined by clause type. For example, in main clauses in declarative sentences, finite verbs occur as the second constituent of the sentence, but certain non-finite verb forms occur in final position. In Figure 1, for example, the English “can” aligns with German “k¨onnen” in second position and “set” aligns with German “festlegen” in final position. Aside from verbs, German is usually characterized as a “free word-order” language: major constituents of the sentence may occur in variou</context>
</contexts>
<marker>Corston-Oliver, Gamon, 2004</marker>
<rawString>Simon Corston-Oliver and Michael Gamon. 2004. Normalizing German and English inflectional morphology to improve statistical word alignment. In R. E. Frederking and K. B. Taylor, editors, Machine translation: From real users to research. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Anthony Aue</author>
<author>Kevin Duh</author>
<author>Eric Ringger</author>
</authors>
<title>Multilingual dependency parsing using Bayes Point Machines.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="2876" citStr="Corston-Oliver et al., 2006" startWordPosition="436" endWordPosition="439">parsing framework used to produce treelets, the treelet translation framework and salient characteristics of the target languages. 2.1 Dependency parsing Dependency analysis is an alternative to constituency analysis (Tesni`ere, 1959; Melˇcuk, 1988). In a dependency analysis of syntax, words directly modify other words, with no intervening non-lexical nodes. We use the terms child node and parent node to denote the tokens in a dependency relation. Each child has a single parent, with the lexical root of the sentence dependent on a synthetic ROOT node. We use the parsing approach described in (Corston-Oliver et al., 2006). The parser is trained on dependencies extracted from the English Penn Treebank version 3.0 (Marcus et al., 1993) by using the head-percolation rules of (Yamada and Matsumoto, 2003). Given a sentence x, the goal of the parser is to find the highest-scoring parse yˆ among all possible parses y E Y: yˆ = argmax s(x,y) (1) yEY The score of a given parse y is the sum of the 62 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 62–69, Sydney, July 2006. c�2006 Association for Computational Linguistics scores of all its dependency links (i, j)</context>
</contexts>
<marker>Corston-Oliver, Aue, Duh, Ringger, 2006</marker>
<rawString>Simon Corston-Oliver, Anthony Aue, Kevin Duh, and Eric Ringger. 2006. Multilingual dependency parsing using Bayes Point Machines. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="4892" citStr="Eisner, 1996" startWordPosition="795" endWordPosition="796"> the direction and distance of the parent, e.g. does the child precede or follow the parent, and how many tokens intervene? To set the weight vector w, we train twenty averaged perceptrons (Collins, 2002) on different shuffles of data drawn from sections 02–21 of the Penn Treebank. The averaged perceptrons are then combined to form a Bayes Point Machine (Herbrich et al., 2001; Harrington et al., 2003), resulting in a linear classifier that is competitive with wide margin techniques. To find the optimal parse given the weight vector w and feature vector f(i, j) we use the decoder described in (Eisner, 1996). 2.2 Treelet translation For syntactically-informed translation, we follow the treelet translation approach described in (Quirk et al., 2005). In this approach, translation is guided by treelet translation pairs. Here, a treelet is a connected subgraph of a dependency tree. A treelet translation pair consists of a source treelet S, a target treelet T, and a word alignment A C S x T such that for all s E S, there exists a unique t E T such that (s,t) E A, and if t is the root of T, there is a unique s E S such that (s,t) E A. Translation of a sentence begins by parsing that sentence into a dep</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Eric Ringger</author>
<author>Zhu Zhang</author>
<author>Robert Moore</author>
<author>Simon Corston-Oliver</author>
</authors>
<title>Extraposition: A case study in German sentence realization.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>301--307</pages>
<contexts>
<context position="10370" citStr="Gamon et al., 2002" startWordPosition="1699" endWordPosition="1703"> in final position. Aside from verbs, German is usually characterized as a “free word-order” language: major constituents of the sentence may occur in various orders, so-called “separable prefixes” may occur bound to the verb or may detach and occur at a considerable distance from the verb on which they depend, and extraposition of various kinds of subordinate clause is common. In the case of extraposition, for example, more than one third of relative clauses in human-translated German technical text are extraposed. For comparable English text the figure is considerably less than one percent (Gamon et al., 2002). 2.3.2 Japanese Word order in Japanese is rather different from English. English has the canonical constituent order subject-verb-object, whereas Japanese prefers subject-object-verb order. Prepositional phrases in English generally correspond to postpositional phrases in Japanese. Japanese noun phrases are strictly head-final whereas English noun phrases allow postmodifiers such as prepositional phrases, relative clauses and adjectives. Japanese has little nominal morphology and does not obligatorily mark number, gender or definiteness. Verbal morphology in Japanese is complex with morpholog</context>
</contexts>
<marker>Gamon, Ringger, Zhang, Moore, Corston-Oliver, 2002</marker>
<rawString>Michael Gamon, Eric Ringger, Zhu Zhang, Robert Moore, and Simon Corston-Oliver. 2002. Extraposition: A case study in German sentence realization. In Proceedings of COLING, pages 301–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A bit of progress in language modeling, extended version.</title>
<date>2001</date>
<tech>Technical Report MSR-TR-2001-72, Microsoft Research.</tech>
<contexts>
<context position="7047" citStr="Goodman, 2001" startWordPosition="1177" endWordPosition="1178">tions are scored according to a log-linear combination of feature functions, each scoring different aspects of the translation process. We use a beam search decoder to find the best translation T* according to the log-linear combination of models: � T* = argmax Z λff(S,T,A) (3) T of mappings to be O(n3) in the worst case, where n is the number of nodes in the dependency tree. The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2003), a trigram target language model using modified Kneser-Ney smoothing (Goodman, 2001), an order model following (Quirk et al., 2005), and word count and phrase count functions. The weights for these models are determined using the method described in (Och, 2003). To estimate the models and extract the treelets, we begin from a parallel corpus. First the corpus is word-aligned using GIZA++ (Och and Ney, 2000), then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (Quirk et al., 2005). This word aligned parallel dependency tree corpus provides training material for an order model and a target langua</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. A bit of progress in language modeling, extended version. Technical Report MSR-TR-2001-72, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Harrington</author>
<author>Ralf Herbrich</author>
<author>Jyrki Kivinen</author>
<author>John C Platt</author>
<author>Robert C Williamson</author>
</authors>
<title>Online bayes point machines.</title>
<date>2003</date>
<booktitle>In Proc. 7th Pacific-Asia Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>241--252</pages>
<contexts>
<context position="4683" citStr="Harrington et al., 2003" startWordPosition="756" endWordPosition="759">rious combinations of these features are used, for example a new feature is created that combines the POS of the parent, lexeme of the parent, POS of the child and lexeme of the child. Each feature is also conjoined with the direction and distance of the parent, e.g. does the child precede or follow the parent, and how many tokens intervene? To set the weight vector w, we train twenty averaged perceptrons (Collins, 2002) on different shuffles of data drawn from sections 02–21 of the Penn Treebank. The averaged perceptrons are then combined to form a Bayes Point Machine (Herbrich et al., 2001; Harrington et al., 2003), resulting in a linear classifier that is competitive with wide margin techniques. To find the optimal parse given the weight vector w and feature vector f(i, j) we use the decoder described in (Eisner, 1996). 2.2 Treelet translation For syntactically-informed translation, we follow the treelet translation approach described in (Quirk et al., 2005). In this approach, translation is guided by treelet translation pairs. Here, a treelet is a connected subgraph of a dependency tree. A treelet translation pair consists of a source treelet S, a target treelet T, and a word alignment A C S x T such </context>
</contexts>
<marker>Harrington, Herbrich, Kivinen, Platt, Williamson, 2003</marker>
<rawString>Edward Harrington, Ralf Herbrich, Jyrki Kivinen, John C. Platt, and Robert C. Williamson. 2003. Online bayes point machines. In Proc. 7th Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 241–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Thore Graepel</author>
<author>Colin Campbell</author>
</authors>
<title>Bayes Point Machines.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>245--278</pages>
<contexts>
<context position="4657" citStr="Herbrich et al., 2001" startWordPosition="751" endWordPosition="755">he parent and child. Various combinations of these features are used, for example a new feature is created that combines the POS of the parent, lexeme of the parent, POS of the child and lexeme of the child. Each feature is also conjoined with the direction and distance of the parent, e.g. does the child precede or follow the parent, and how many tokens intervene? To set the weight vector w, we train twenty averaged perceptrons (Collins, 2002) on different shuffles of data drawn from sections 02–21 of the Penn Treebank. The averaged perceptrons are then combined to form a Bayes Point Machine (Herbrich et al., 2001; Harrington et al., 2003), resulting in a linear classifier that is competitive with wide margin techniques. To find the optimal parse given the weight vector w and feature vector f(i, j) we use the decoder described in (Eisner, 1996). 2.2 Treelet translation For syntactically-informed translation, we follow the treelet translation approach described in (Quirk et al., 2005). In this approach, translation is guided by treelet translation pairs. Here, a treelet is a connected subgraph of a dependency tree. A treelet translation pair consists of a source treelet S, a target treelet T, and a word</context>
</contexts>
<marker>Herbrich, Graepel, Campbell, 2001</marker>
<rawString>Ralf Herbrich, Thore Graepel, and Colin Campbell. 2001. Bayes Point Machines. Journal of Machine Learning Research, pages 245–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="1029" citStr="Koehn et al., 2003" startWordPosition="145" endWordPosition="148"> used to train the parser. As the amount of data increases, parse quality improves, leading to improvements in machine translation output and results that significantly outperform a state-of-the-art phrasal baseline. 1 Introduction The current study is a response to a question that proponents of syntactically-informed machine translation frequently encounter: How sensitive is a syntactically-informed machine translation system to the quality of the input syntactic analysis? It has been shown that phrasal machine translation systems are not affected by the quality of the input word alignments (Koehn et al., 2003). This finding has generally been cast in favorable terms: such systems are robust to poor quality word alignment. A less favorable interpretation of these results might be to conclude that phrasal statistical machine translation (SMT) systems do not stand to benefit from improvements in word alignment. In a similar vein, one might ask whether contemporary syntactically-informed machine translation systems would benefit from improvements in parse accuracy. One possibility is that current syntactically-informed SMT systems are deriving only limited value from the syntactic analyses, and would t</context>
<context position="5593" citStr="Koehn et al., 2003" startWordPosition="921" endWordPosition="924">eelet translation approach described in (Quirk et al., 2005). In this approach, translation is guided by treelet translation pairs. Here, a treelet is a connected subgraph of a dependency tree. A treelet translation pair consists of a source treelet S, a target treelet T, and a word alignment A C S x T such that for all s E S, there exists a unique t E T such that (s,t) E A, and if t is the root of T, there is a unique s E S such that (s,t) E A. Translation of a sentence begins by parsing that sentence into a dependency representation. This dependency graph is partitioned into treelets; like (Koehn et al., 2003), we assume a uniform probability distribution over all partitions. Each source treelet is matched to a treelet translation pair; together, the target language treelets in those treelet translation pairs will form the target translation. Next the target language treelets are joined to form a single tree: the parent of the root of each treelet is dictated by the source. Let tr be the root of the target language treelet, and sr be the source node aligned to it. If sr is the root of the source sentence, then tr is made the root of the target language tree. Otherwise let sp be the parent of sr, an</context>
<context position="18942" citStr="Koehn et al., 2003" startWordPosition="3056" endWordPosition="3059"> Treelet decoder 31.4±1.3 28.0±0.7 Right-branching 250 sentences 32.8±1.4 34.1±0.9 2,500 sentences 33.0±1.4 34.6±1.0 25,000 sentences 33.7±1.5 35.7±0.9 39,892 sentences 33.6±1.5 36.0±1.0 Table 2: BLEU score vs. decoder and parser variants. Here sentences refer to the amount of parser training data, not MT training data. neighbor and the root of a sentence is the first word. With this analysis, treelets are simply subsequences of the sentence, and therefore are very similar to the phrases of Phrasal SMT. In Englishto-German, this result produces results very comparable to a phrasal SMT system (Koehn et al., 2003) trained on the same data. For English-toJapanese, however, this baseline performs much worse than a phrasal SMT system. Although phrases and treelets should be nearly identical under this scenario, the decoding constraints are somewhat different: the treelet decoder assumes phrasal cohesion during translation. This constraint may account for the drop in quality. Since the confidence intervals for many pairs overlap, we ran pairwise tests for each system to determine which differences were significant at the p &lt; 0.05 level using the bootstrap method described in (Zhang and Vogel, 2004); Table </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2990" citStr="Marcus et al., 1993" startWordPosition="454" endWordPosition="457">nguages. 2.1 Dependency parsing Dependency analysis is an alternative to constituency analysis (Tesni`ere, 1959; Melˇcuk, 1988). In a dependency analysis of syntax, words directly modify other words, with no intervening non-lexical nodes. We use the terms child node and parent node to denote the tokens in a dependency relation. Each child has a single parent, with the lexical root of the sentence dependent on a synthetic ROOT node. We use the parsing approach described in (Corston-Oliver et al., 2006). The parser is trained on dependencies extracted from the English Penn Treebank version 3.0 (Marcus et al., 1993) by using the head-percolation rules of (Yamada and Matsumoto, 2003). Given a sentence x, the goal of the parser is to find the highest-scoring parse yˆ among all possible parses y E Y: yˆ = argmax s(x,y) (1) yEY The score of a given parse y is the sum of the 62 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 62–69, Sydney, July 2006. c�2006 Association for Computational Linguistics scores of all its dependency links (i, j) E y: s(x,y) = Z d(i, j) = Z w · f(i, j) (2) (i,j)Ey (i,j)Ey where the link (i, j) indicates a parent-child depend</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Melˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press.</publisher>
<marker>Melˇcuk, 1988</marker>
<rawString>Igor A. Melˇcuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China,</location>
<contexts>
<context position="7373" citStr="Och and Ney, 2000" startWordPosition="1230" endWordPosition="1233">, where n is the number of nodes in the dependency tree. The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2003), a trigram target language model using modified Kneser-Ney smoothing (Goodman, 2001), an order model following (Quirk et al., 2005), and word count and phrase count functions. The weights for these models are determined using the method described in (Och, 2003). To estimate the models and extract the treelets, we begin from a parallel corpus. First the corpus is word-aligned using GIZA++ (Och and Ney, 2000), then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (Quirk et al., 2005). This word aligned parallel dependency tree corpus provides training material for an order model and a target language tree-based language model. We also extract treelet translation pairs from this parallel corpus. To limit the combinatorial explosion of treelets, we only gather treelets that contain at most four words and at most two gaps in the surface string. This limits the number 2.3 Language pairs In the present paper we focus on En</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the ACL, pages 440–447, Hongkong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="7224" citStr="Och, 2003" startWordPosition="1206" endWordPosition="1207"> translation T* according to the log-linear combination of models: � T* = argmax Z λff(S,T,A) (3) T of mappings to be O(n3) in the worst case, where n is the number of nodes in the dependency tree. The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2003), a trigram target language model using modified Kneser-Ney smoothing (Goodman, 2001), an order model following (Quirk et al., 2005), and word count and phrase count functions. The weights for these models are determined using the method described in (Och, 2003). To estimate the models and extract the treelets, we begin from a parallel corpus. First the corpus is word-aligned using GIZA++ (Och and Ney, 2000), then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (Quirk et al., 2005). This word aligned parallel dependency tree corpus provides training material for an order model and a target language tree-based language model. We also extract treelet translation pairs from this parallel corpus. To limit the combinatorial explosion of treelets, we only gather treelets that</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelpha, Pennsylvania.</location>
<contexts>
<context position="17634" citStr="Papineni et al., 2002" startWordPosition="2846" endWordPosition="2849">nk, however, occur infrequently or not at all in the technical materials so the lexical information is of little benefit. This reflects the mismatch of content. The Wall Street Journal articles in the Penn Treebank concern such topics as world affairs and the policies of the Reagan administration; these topics are absent in the technical materials. Conversely, the Wall Street Journal articles contain no discussion of such topics as the intricacies of SQL database queries. 3.2 Translation quality Table 2 presents the impact of parse quality on a treelet translation system, measured using BLEU (Papineni et al., 2002). Since our main goal is to investigate the impact of parser accuracy on translation quality, we have varied the parser training data, but have held the MT training data, part-ofspeech-tagger, and all other factors constant. We observe an upward trend in BLEU score as more training data is made available to the parser; the trend is even clearer in Japanese.2 As a baseline, we include right-branching dependency trees, i.e., trees in which the parent of each word is its left 2This is particularly encouraging since various people have remarked to us that syntax-based SMT systems may be disadvanta</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the ACL, pages 311–318, Philadelpha, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="5034" citStr="Quirk et al., 2005" startWordPosition="812" endWordPosition="815">eight vector w, we train twenty averaged perceptrons (Collins, 2002) on different shuffles of data drawn from sections 02–21 of the Penn Treebank. The averaged perceptrons are then combined to form a Bayes Point Machine (Herbrich et al., 2001; Harrington et al., 2003), resulting in a linear classifier that is competitive with wide margin techniques. To find the optimal parse given the weight vector w and feature vector f(i, j) we use the decoder described in (Eisner, 1996). 2.2 Treelet translation For syntactically-informed translation, we follow the treelet translation approach described in (Quirk et al., 2005). In this approach, translation is guided by treelet translation pairs. Here, a treelet is a connected subgraph of a dependency tree. A treelet translation pair consists of a source treelet S, a target treelet T, and a word alignment A C S x T such that for all s E S, there exists a unique t E T such that (s,t) E A, and if t is the root of T, there is a unique s E S such that (s,t) E A. Translation of a sentence begins by parsing that sentence into a dependency representation. This dependency graph is partitioned into treelets; like (Koehn et al., 2003), we assume a uniform probability distrib</context>
<context position="7094" citStr="Quirk et al., 2005" startWordPosition="1183" endWordPosition="1186"> combination of feature functions, each scoring different aspects of the translation process. We use a beam search decoder to find the best translation T* according to the log-linear combination of models: � T* = argmax Z λff(S,T,A) (3) T of mappings to be O(n3) in the worst case, where n is the number of nodes in the dependency tree. The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2003), a trigram target language model using modified Kneser-Ney smoothing (Goodman, 2001), an order model following (Quirk et al., 2005), and word count and phrase count functions. The weights for these models are determined using the method described in (Och, 2003). To estimate the models and extract the treelets, we begin from a parallel corpus. First the corpus is word-aligned using GIZA++ (Och and Ney, 2000), then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (Quirk et al., 2005). This word aligned parallel dependency tree corpus provides training material for an order model and a target language tree-based language model. We also extract t</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
</authors>
<title>El´ements de syntaxe structurale.</title>
<date>1959</date>
<journal>Librairie C. Klincksieck.</journal>
<marker>Tesni`ere, 1959</marker>
<rawString>Lucien Tesni`ere. 1959. ´El´ements de syntaxe structurale. Librairie C. Klincksieck.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/EMNLP,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="12213" citStr="Toutanova et al., 2003" startWordPosition="1971" endWordPosition="1974">ly-informed statistical machine translation. One method for producing parsers of varying quality might be to train a parser and then to transform its output, e.g. 64 by replacing the parser’s selection of the parent for certain tokens with different nodes. Rather than randomly adding noise to the parses, we decided to vary the quality in ways that more closely mimic the situation that confronts us as we develop machine translation systems. Annotating data for POS requires considerably less human time and expertise than annotating syntactic relations. We therefore used an automatic POS tagger (Toutanova et al., 2003) trained on the complete training section of the Penn Treebank (sections 02–21). Annotating syntactic dependencies is time consuming and requires considerable linguistic expertise.1 We can well imagine annotating syntactic dependencies in order to develop a machine translation system by annotating first a small quantity of data, training a parser, training a system that uses the parses produced by that parser and assessing the quality of the machine translation output. Having assessed the quality of the output, one might annotate additional data and train systems until it appears that the qual</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings ofHLT/EMNLP, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Ying Zhang</author>
<author>Fei Huang</author>
<author>Alicia Tribble</author>
<author>Ashish Venugopal</author>
<author>Bing Zhao</author>
<author>Alex Waibel</author>
</authors>
<title>The CMU statistical machine translation system.</title>
<date>2003</date>
<booktitle>In Proceedings of the MT Summit.</booktitle>
<contexts>
<context position="6962" citStr="Vogel et al., 2003" startWordPosition="1163" endWordPosition="1166">d the target sentence is produced by reading off the labels of the nodes in order. Translations are scored according to a log-linear combination of feature functions, each scoring different aspects of the translation process. We use a beam search decoder to find the best translation T* according to the log-linear combination of models: � T* = argmax Z λff(S,T,A) (3) T of mappings to be O(n3) in the worst case, where n is the number of nodes in the dependency tree. The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2003), a trigram target language model using modified Kneser-Ney smoothing (Goodman, 2001), an order model following (Quirk et al., 2005), and word count and phrase count functions. The weights for these models are determined using the method described in (Och, 2003). To estimate the models and extract the treelets, we begin from a parallel corpus. First the corpus is word-aligned using GIZA++ (Och and Ney, 2000), then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (Quirk et al., 2005). This word aligned parallel dep</context>
</contexts>
<marker>Vogel, Zhang, Huang, Tribble, Venugopal, Zhao, Waibel, 2003</marker>
<rawString>Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble, Ashish Venugopal, Bing Zhao, and Alex Waibel. 2003. The CMU statistical machine translation system. In Proceedings of the MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings ofIWPT,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="3058" citStr="Yamada and Matsumoto, 2003" startWordPosition="464" endWordPosition="467">ernative to constituency analysis (Tesni`ere, 1959; Melˇcuk, 1988). In a dependency analysis of syntax, words directly modify other words, with no intervening non-lexical nodes. We use the terms child node and parent node to denote the tokens in a dependency relation. Each child has a single parent, with the lexical root of the sentence dependent on a synthetic ROOT node. We use the parsing approach described in (Corston-Oliver et al., 2006). The parser is trained on dependencies extracted from the English Penn Treebank version 3.0 (Marcus et al., 1993) by using the head-percolation rules of (Yamada and Matsumoto, 2003). Given a sentence x, the goal of the parser is to find the highest-scoring parse yˆ among all possible parses y E Y: yˆ = argmax s(x,y) (1) yEY The score of a given parse y is the sum of the 62 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 62–69, Sydney, July 2006. c�2006 Association for Computational Linguistics scores of all its dependency links (i, j) E y: s(x,y) = Z d(i, j) = Z w · f(i, j) (2) (i,j)Ey (i,j)Ey where the link (i, j) indicates a parent-child dependency between the token at position i and the token at position j. Th</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings ofIWPT, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>Measuring confidence intervals for mt evaluation metrics.</title>
<date>2004</date>
<booktitle>In Proceedings of TMI.</booktitle>
<contexts>
<context position="19534" citStr="Zhang and Vogel, 2004" startWordPosition="3148" endWordPosition="3151">MT system (Koehn et al., 2003) trained on the same data. For English-toJapanese, however, this baseline performs much worse than a phrasal SMT system. Although phrases and treelets should be nearly identical under this scenario, the decoding constraints are somewhat different: the treelet decoder assumes phrasal cohesion during translation. This constraint may account for the drop in quality. Since the confidence intervals for many pairs overlap, we ran pairwise tests for each system to determine which differences were significant at the p &lt; 0.05 level using the bootstrap method described in (Zhang and Vogel, 2004); Table 3 summarizes this comparison. Neither language pair achieves a statistically significant improvement from increasing the training data from 25,000 pairs to the full training set; this is not surprising since the increase in parse accuracy is quite small (90.2% to 90.8% on Wall Street Journal text). To further understand what differences in dependency analysis were affecting translation quality, we compared a treelet translation system that 66 Pharaoh Right-branching 250 2,500 25,000 39,892 Pharaoh ∼ &gt; &gt; &gt; &gt; Right-branching &gt; &gt; &gt; &gt; 250 ∼ &gt; &gt; 2,500 &gt; &gt; 25,000 ∼ (a) English-German Pharaoh</context>
</contexts>
<marker>Zhang, Vogel, 2004</marker>
<rawString>Ying Zhang and Stephan Vogel. 2004. Measuring confidence intervals for mt evaluation metrics. In Proceedings of TMI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>