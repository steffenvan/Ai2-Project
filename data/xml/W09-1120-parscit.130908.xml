<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000353">
<title confidence="0.997536">
Automatic Selection of High Quality Parses Created By a Fully
Unsupervised Parser
</title>
<author confidence="0.997117">
Roi Reichart Ari Rappoport
</author>
<affiliation confidence="0.9883095">
ICNC Institute of computer science
The Hebrew University The Hebrew University
</affiliation>
<email confidence="0.990556">
roiri@cs.huji.ac.il arir@cs.huji.ac.il
</email>
<sectionHeader confidence="0.995501" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9765464">
The average results obtained by unsupervised
statistical parsers have greatly improved in the
last few years, but on many specific sentences
they are of rather low quality. The output of
such parsers is becoming valuable for vari-
ous applications, and it is radically less expen-
sive to create than manually annotated training
data. Hence, automatic selection of high qual-
ity parses created by unsupervised parsers is
an important problem.
In this paper we present PUPA, a POS-based
Unsupervised Parse Assessment algorithm.
The algorithm assesses the quality of a parse
tree using POS sequence statistics collected
from a batch of parsed sentences. We eval-
uate the algorithm by using an unsupervised
POS tagger and an unsupervised parser, se-
lecting high quality parsed sentences from En-
glish (WSJ) and German (NEGRA) corpora.
We show that PUPA outperforms the leading
previous parse assessment algorithm for su-
pervised parsers, as well as a strong unsuper-
vised baseline. Consequently, PUPA allows
obtaining high quality parses without any hu-
man involvement.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99974880952381">
In unsupervised parsing an algorithm should un-
cover the syntactic structure of an input sentence
without using any manually created structural train-
ing data. The last decade has seen significant
progress in this field of research (Klein and Man-
ning, 2002; Klein and Manning, 2004; Bod, 2006a;
Bod, 2006b; Smith and Eisner, 2006; Seginer,
2007).
Many NLP systems use the output of supervised
parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan
et al., 2003) for IE, (Punyakanok et al., 2008) for
SRL, (Srikumar et al., 2008) for Textual Inference
and (Avramidis and Koehn, 2008) for MT). To
achieve good performance, these parsers should be
trained on large amounts of manually created train-
ing data from a domain similar to that of the sen-
tences they parse (Lease and Charniak, 2005; Mc-
Closky and Charniak, 2008). In the highly variable
Web, where many of these systems are used, it is
very difficult to create a representative corpus for
manual annotation. The high cost of manual annota-
tion of training data for supervised parsers imposes
a significant burden on their usage.
A possible answer to this problem can be pro-
vided by high quality parses produced by unsuper-
vised parsers that require little to no manual efforts
for their training. These parses can be used either
as input for applications, or as training material for
modern supervised parsers whose output will in turn
be used by applications.
Although unsupervised parser results improve,
the quality of many of the parses they produce is still
too low for such goals. For example, the Seginer
(2007) parser achieves an F-score of 75.9% on the
WSJ10 corpus and 59% on the NEGRA10 corpus,
but the percentage of individual sentences with an
F-score of 100% is 21.5% for WSJ10 and 11% for
NEGRA10. When requirements are relaxed, only
asking for an F-score higher than 85%, percentage
is still low, 42% for WSJ10 and 15% for NEGRA10.
In this paper we address the task of a fully un-
supervised assessment of high quality parses cre-
</bodyText>
<page confidence="0.980793">
156
</page>
<note confidence="0.9900785">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.998127097560976">
ated by an unsupervised parser. The assessment
should be unsupervised in order to avoid the prob-
lems mentioned above with manually trained super-
vised parsers. Assessing the quality of a learning al-
gorithm’s output and selecting high quality instances
has been addressed for supervised algorithms (Caru-
ana and Niculescu-Mizil, 2006) and specifically for
supervised parsers (Yates et al., 2006; Reichart and
Rappoport, 2007; Kawahara and Uchimoto, 2008;
Ravi et al., 2008). Moreover, it has been shown
to be valuable for supervised parser adaptation be-
tween domains (Sagae and Tsujii, 2007; Kawahara
and Uchimoto, 2008; Chen et al., 2008). However,
as far as we know the present paper is the first to
address the task of unsupervised assessment of the
quality of parses created by unsupervised parsers.
Our POS-based Unsupervised Parse Assessment
(PUPA) algorithm uses statistics about POS tag se-
quences in a batch of parsed sentences1. The con-
stituents in the batch are represented using the POS
sequences of their yield and of the yields of neigh-
boring constituents. Constituents whose representa-
tion is frequent in the output of the parser are con-
sidered to be of a high quality. A score for each
range of constituent length is calculated, reflecting
the robustness of statistics used for the creation of
the constituents of that length. The final sentence
score is a weighted average of the scores calculated
for each constituent length. The score thus integrates
the quality of short and long constituents into one
score reflecting the quality of the whole parse tree.
PUPA provides a quality score for every sentence
in a parsed sentences set. An NLP application can
then decide if to use a parse or not, according to
its own definition of a high quality parse. For ex-
ample, it can select every sentence whose score is
above some threshold, or the k top scored sentences.
The selection strategy is application dependent and
is beyond the scope of this paper.
The unsupervised parser we use is the Seginer
(2007) incremental parser2, which achieves state-of-
</bodyText>
<footnote confidence="0.960207285714286">
1The algorithm can be used with supervised POS taggers
and parsers, but we focus here on the fully unsupervised sce-
nario, which is novel and more useful. For completeness of
analysis, we experimented with PUPA using a supervised POS
tagger (see Section 5). Using PUPA with supervised parsers is
left for future work.
2www.seggu.net/ccl.
</footnote>
<bodyText confidence="0.99968196875">
the-art results without using manually created POS
tags. The POS tags we use are induced by the un-
supervised tagger of (Clark, 2003)3. Since both tag-
ger and parser do not require any manual annotation,
PUPA identifies high quality parses without any hu-
man involvement.
The incremental parser of (Seginer, 2007) does
not give any prediction of its output quality, and
extracting such a prediction from its internal data
structures is not straightforward. Such a predic-
tion can be given by supervised parsers in terms
of the parse likelihood, but this was shown to be
of medium quality (Reichart and Rappoport, 2007).
While the algorithms of Yates et al. (2006), Kawa-
hara and Uchimoto (2008) and Ravi et al. (2008) are
supervised (Section 3), the ensemble based SEPA al-
gorithm (Reichart and Rappoport, 2007) can be ap-
plied to unsupervised parsers in a way that preserves
the unsupervised nature of the selection task.
To compare between two algorithms, we use each
of them to assess the quality of the sentences in En-
glish and German corpora (WSJ and NEGRA)4. We
show that for every sentence length (up to 20) the
quality of the top scored k sentences according to
PUPA is higher than the quality of SEPA’s list (for
every k). As in (Reichart and Rappoport, 2007), the
quality of a set selected from the parser’s output is
evaluated using two measures: constituent F-score5
and average sentence F-score.
Section 2 describes the PUPA algorithm, Sec-
tion 3 discusses previous work, and Sections 4 and
5 present the evaluation setup and results.
</bodyText>
<sectionHeader confidence="0.9424365" genericHeader="method">
2 The POS-based Unsupervised Parse
Assessment (PUPA) Algorithm
</sectionHeader>
<bodyText confidence="0.995619833333333">
In this section we detail our parse assessment algo-
rithm. Its input consists of a set I of parsed sen-
tences, which in our evaluation scenario are pro-
duced by an unsupervised parser. The algorithm
assigns each parsed sentence a score reflecting its
quality.
</bodyText>
<footnote confidence="0.996867285714286">
3www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html,
the neyessenmorph model.
4This is in contrast to algorithms for selection from the re-
sults of supervised constituency parsers, which were evaluated
only for English (Yates et al., 2006; Reichart and Rappoport,
2007; Ravi et al., 2008).
5This is the traditional parsing F-score.
</footnote>
<page confidence="0.997468">
157
</page>
<bodyText confidence="0.999657787234042">
The algorithm has three steps. First, the words in
I are POS tagged (in our case, using the fully unsu-
pervised POS induction algorithm of Clark (2003)).
Second, POS statistics about the constituents in I
are collected. Finally, a quality score is calculated
for each parsed sentence in I using the POS statis-
tics. In the following we detail the last two steps.
Collecting POS statistics. In its second step, the
algorithm collects statistics about the constituents in
the input set I. Recall that the yield of a constituent
is the set of words covered by it. The PUPA con-
stituent representation (PCR) consists of three fea-
tures: (1) the ordered POS tag sequence of the con-
stituent’s yield, (2) the constituents’ right context,
and (3) the constituents’ left context.
We define context to be the leftmost and rightmost
POS tags in the yield of the neighbor of the con-
stituent (if there is only one POS tag in the neigh-
bor’s yield, this POS tag is the context). For the
right and left contexts we consider the right and left
neighbors respectively. A constituent C1 is the right
neighbor of a constituent C2 if C1 is the highest level
constituent such that the first word in the yield of C1
comes immediately after the last word in the yield of
C2. A constituent C1 is the left neighbor of a con-
stituent C2 if C1 is the highest level constituent such
that the first word in the yield of C2 comes immedi-
ately after the last word in the yield of C1.
Figure 1 shows an example, an unlabeled tree for
the sentence ‘I will give you the ball’. The tree has
6 constituents (C0-C5). C3 and C4 have both right
and left neighbors. For C3, the POS sequence of its
yield is POS2, POS3 , the left neighbor is C1 and thus
the left context is POS1, and the right neighbor is C4
and thus the right context is POS4. Note that the
left and right neighbors of C3 have only one POS
tag in their yield and therefore this POS tag is the
context. For C4 the yield is POS4, the left neighbor
is C3 (and thus the left context is POS2,POS3), and
the right neighbor is C5 (and thus the right context
is POS5,POS6). C1, whose yield is POS1, has only
a right neighbor, C2, and thus its right context is
POS2,POS6 and its left context is NULL. C2 and C5
(whose yields are POS2, POS3, POS4, POS5, POS6 for
C2 and POS5, POS6 for C5) have only a left neigh-
bor. For C2, this is C1 (and the context is POS1)
while for C5 this is C4 (with the context POS4).
</bodyText>
<figureCaption confidence="0.997599">
Figure 1: An example parse tree for contexts and neigh-
bors (see text).
</figureCaption>
<bodyText confidence="0.999943766666667">
The right context of both constituents is NULL. As
all sentence level constituents, C0 has no neighbors,
and thus both its left and right contexts are NULL.
We have also explored other representations of
left and right contexts based on the POS tags of their
yields. In these, we represented the left/right neigh-
bor using only the leftmost/rightmost POS tags of
its yield or other subsets of the yield’s POS tags.
These variations produced lower quality results than
the main variant above in our experiments, which
were for English and German. Exploring the suit-
ability of our representation for other languages is
left for future research.
Score computation. The third and last step of the
algorithm is a second pass over I for computing a
quality score for each parse tree.
Short constituents tend to be more frequent than
long ones. In order not to distort our score due to
parsing errors in short constituents, PUPA computes
the grade using a division into lengths, in three steps.
First, constituents are assigned to bins according to
their length, each bin containing the constituents of
a certain range of lengths. Denote this range by
W (for width), and the number of bins by N(W).
For example, in our experiments the longest possible
constituent is of length 20, so we can take W = 5,
resulting in N(W) = 4: bin 1 for constituents of
length 1-5, bin 2 for constituents of length 6-10, and
so on for bins 3, 4.
The score of binz is given by
</bodyText>
<equation confidence="0.99609">
(1) BinScore(binz) = Et=2 (X − t + 2)·|ci|
t|
</equation>
<bodyText confidence="0.91732">
Where X is the maximal number of occurrences
of constituents in the bin that we consider as impor-
tant for the score (see below for its selection), |Cz�|
is the number of constituents in bin i occurring at
</bodyText>
<figure confidence="0.998973722222222">
0
the
ball
POS5
4
POS4
you
3
POS3
5
POS6
2
1
POS1
I
POS2
will
give
</figure>
<page confidence="0.978987">
158
</page>
<bodyText confidence="0.999591444444444">
least t times in the batch of parsed sentences, and
|Ci |is the number of constituents in bin i. In words,
the score is a weighted average: the fraction of the
constituents in the bin occuring at least 2 times (with
weight X), plus the fraction of the constituents in the
bin occuring at least 3 times (with weight X − 1),
etc, until the fraction of the constituents in the bin
occuring at least X times (with weight 2).
A score for the division into N bins is given by
</bodyText>
<equation confidence="0.59987125">
(2) 5core(N(W)) = EN(W BinScore(bini)
Z·M
Where Z is the maximum bin score (according to
(1)) and M is the number of bins containing at least
</equation>
<bodyText confidence="0.999894085714286">
one constituent. If, for example, N(W) = 4 and
there is no constituent whose length is between 11
and 15 then bin number 3 is empty. If every other
bin contains at least one constituent, M = 3.
To get a final score for the parse tree of sentence
5 that is independent of a specific bin division, we
sum the scores of the various bin division:
where Y is the length of 5 (which is also its max-
imum bin width). Pupa5core thus takes values in
the [0, 1] range.
In equation (1), if, for example, X = 20 then
the weight of the fraction of the bin’s constituents
occurring at least 2 times is 20 while the weight of
the fraction of the constituents occurring at least 10
times is 12 and of the fraction of constituents occur-
ring at least 20 times is 2. We consider the number
of times a constituent appears in a batch to be an in-
dication of its correctness. The difference between 3
and 2 occurrences is therefore more indicative than
the difference between 20 and 19 occurrences. More
generally, the more times a constituent occurs, the
less indicative any additional appearance is.
In equation (2) we give all bins the same weight.
Short constituents are more frequent and are gener-
ally more likely to be correct. However, the cor-
rectness of long constituents is an indication that the
parser has a correct interpretation of the tree struc-
ture and that it is likely to create a high quality tree.
The usage of equal bin weights was done to balance
the tendency of parse trees to have more short con-
stituents.
Parameters. PUPA has two parameters: X, the
maximal number of occurrences considered in equa-
tion (1), and P, the number of POS tags induced by
the unsupervised POS tagger. In the following we
present the unsupervised technique we used to tune
these parameters.
Figure 2 shows nc(t), the number of constituents
appearing at least t times in WSJ20 (left) and NE-
GRA20 (right). For both corpora, the pattern is
shown when using 5 POS tags (P = 5, solid line)
and 50 POS tags (P = 50, dashed line). The distri-
bution obeys Zipf’s law: many constituents appear a
small number of times while a few constituents ap-
pear a large number of times. We denote the t value
where the slope changes from steep to moderate by
telbow. Practically, we approximate the ‘real’ elbow
value and define telbow to be the smallest t for which
nc(t + 1) − nc(t) = 1. When P = 5, telbow is 32
for WSJ and 19 for NEGRA. When P = 50, telbow is
15 for WSJ and 9 for NEGRA.
The number of constituents appearing more than
telbow times is considerably smaller than the number
of constituents appearing telbow times or less. There-
fore, the fact that a constituent appears telbow + 5
times (for a positive integer 5) is not a better indica-
tion of its quality than the fact that it appears telbow
times. We thus select X to be telbow.
The graphs also demonstrate that for both cor-
pora, telbow for P = 50 is smaller than telbow for
P = 5. Generally, telbow is a monotonically decreas-
ing function of P. Lower telbow values imply that
PUPA would be less distinctive between constituents
quality (see equation (1); recall that X = telbow).
We thus want to select the P value that maximizes
telbow. We therefore minimize P. telbow values for
P ∈ {3, ... ,10} are very similar. Indeed, PUPA
achieves its best performance for P ∈ {3,... ,10}
and it is insensitive to the selection of P in this
range. In Section 5 we report results with P = 5.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="method">
3 Related Work
</sectionHeader>
<bodyText confidence="0.998643">
Unsupervised parsing has been explored for several
decades (see (Klein, 2005) for a recent review). Re-
cently, unsupervised parsing algorithms have for the
first time outperformed the right branching heuristic
baseline for English. These include CCM (Klein and
Manning, 2002), the DMV and DMV+CCM models
(Klein and Manning, 2004), (U)DOP based mod-
</bodyText>
<figure confidence="0.9393112">
(3) Pupa5core(5) =
1 Score(N(W))
Y
�W=Y
W
</figure>
<page confidence="0.648424">
159
</page>
<figureCaption confidence="0.9975554">
Figure 2: Number of constituents appearing at least t
times (nc(t)) as a function of t. Shown are WSJ (left)
and NEGRA (right), where constituents are represented
according to PUPA’s PCR with 5 POS tags (P = 5, solid
line) or 50 POS tags (P = 50, dashed line).
</figureCaption>
<bodyText confidence="0.999605086419753">
els (Bod, 2006a; Bod, 2006b), an exemplar based
approach (Dennis, 2005), guiding EM using con-
trastive estimation (Smith and Eisner, 2006), and the
incremental parser of Seginer (2007) that we use in
this work. To obtain good results, manually created
POS tags are used as input in all of these algorithms
except Seginer’s, which uses plain text.
Quality assessment of a learning algorithm’s out-
put and selection of high quality instances have been
addressed for supervised algorithms (see (Caruana
and Niculescu-Mizil, 2006) for a survey) and specif-
ically for supervised constituency parsers (Yates et
al., 2006; Reichart and Rappoport, 2007; Ravi et al.,
2008). For dependency parsing in a corpus adapta-
tion scenario, (Kawahara and Uchimoto, 2008) built
a binary classifier that classifies each parse in the
parser’s output as reliable or not. To do that, they
selected 2500 sentences from the parser’s output,
compared them to their manually created gold stan-
dard, and used accurate (inaccurate) parses as posi-
tive (negative) examples for the classifier. Their ap-
proach is supervised and the features used by the
classifier are dependency motivated.
As far as we know, the present paper is the first to
address the task of selecting high quality parses from
the output of unsupervised parsers. The algorithms
of Yates et al. (2006), Kawahara and Uchimoto
(2008) and Ravi et al. (2008) are supervised, per-
forming semantic analysis of the parse tree and gold
standard-based calssification, respectively. How-
ever, the SEPA algorithm of Reichart and Rappoport
(2007), an algorithm for supervised constituency
parsers, can be applied to unsupervised parsers in
a way that preserves the unsupervised nature of the
selection task. In Section 5 we provide a detailed
comparison between PUPA and SEPA showing the
first to be superior. Below is a brief description of
the SEPA algorithm.
The input of the SEPA algorithm consists of a
parsing algorithm A, a training set, and a test set
(which in the unsupervised case might be the same
set). The algorithm provides, for each of the test
set’s parses generated by A when trained on the full
training set, a grade assessing the parse quality, on
a continuous scale between 0 to 100. The qual-
ity grade is calculated in the following way: N ran-
dom samples of size S are sampled from the train-
ing data and used for training the parsing algorithm
A. In that way N committee members are created.
Then, each of the test sentences is parsed by each of
the N committee members and an agreement score
ranging from 0 to 100 between the committee mem-
bers is calculated. All unsupervised parsers men-
tioned above (including the Seginer parser), have a
training phase where parameter values are estimated
from unlabeled data. SEPA can thus be applied to the
unsupervised case.
Automatic selection of high quality parses has
been shown to improve parser adaptation. Sagae and
Tsujii (2007) and Kawahara and Uchimoto (2008)
applied a self-training protocol to a parser adaptation
scenario but used only high quality parses to retrain
the parser. In the first work, high quality parses were
selected using an ensemble method, while in the sec-
ond a binary classifier was used (see above). The
first system achieved the highest score in the CoNLL
2007 shared task on domain adaptation of depen-
dency parsers, and the second system improved over
the basic self-training protocol. Chen et al. (2008)
parsed target domain sentences and used short de-
pendencies information, which is often accurate, to
adapt a dependency parser to the Chinese language.
Automatic quality assessment has been exten-
sively explored for machine translation (Ueffing and
Ney, 2007) and speech recognition (Koo et al.,
2001). Other NLP tasks where it has been explored
include semi-supervised relation extraction (Rosen-
feld and Feldman, 2007), IE (Culotta and McCal-
lum, 2004), QA (Chu-Carroll et al., 2003), and dia-
log systems (Lin and Weng, 2008).
The idea of representing a constituent by its yield
</bodyText>
<figure confidence="0.979640333333333">
t t
# of constituents appearing at least t times
15000
10000
5000
0
0 50 100
P = 5
P = 50
4000
8000
7000
6000
5000
3000
2000
1000
0
0 50 100
P = 5
P = 50
</figure>
<page confidence="0.969195">
160
</page>
<bodyText confidence="0.9998334">
and (a different definition of) context is used by the
CCM unsupervised parsing model (Klein and Man-
ning, 2002). As far as we know the current work is
the first to use unsupervised POS tags for the selec-
tion of high quality parses.
</bodyText>
<sectionHeader confidence="0.995151" genericHeader="method">
4 Evaluation Setup
</sectionHeader>
<bodyText confidence="0.972820083333333">
We experiment with sentences of up to 20 words
from the English WSJ Penn Treebank (WSJ20,
25236 sentences, 225126 constituents) and the Ger-
man NEGRA corpus (Brants, 1997) (NEGRA20,
15610 sentences, 108540 constiteunts), both con-
taining newspaper texts.
The unsupervised parsers of the kind addressed
in this paper output unlabeled parse trees. To eval-
uate the quality of a single parse tree with respect
to another, we use the unlabeled F-score (UF =
2·UR·UP),
where UR and UP are unlabeled recall
</bodyText>
<equation confidence="0.353237">
UR+UP
</equation>
<bodyText confidence="0.999819594594595">
and unlabeled precision respectively.
Following the unsupervised parsing literature,
multiple brackets and brackets covering a single
word are not counted, but the sentence level bracket
is. We exclude punctuation and null elements ac-
cording to the scheme of (Klein, 2005).
The performance of unsupervised parsers
markedly degrades as sentence length increases.
For example, the Average sentence F–score for WSJ
sentences of length 10 is 71.4% compared to 58.5
for sentences of length 20 (the numbers for NEGRA
are 48.2% and 36.9%). We therefore evaluate PUPA
(and the baseline) for sentences of a given length.
We do this for every sentence of length 2-20 in
WSJ20 and NEGRA20.
For every sentence length L, we use PUPA and the
baseline algorithm (SEPA) to give a quality score to
each of the sentences of that length in the experi-
mental corpus. We then compare the quality of the
top k parsed sentences according to each algorithm.
We do this for every k from 1 to the number of sen-
tences of length L.
Following Reichart and Rappoport (2007), we use
two measures to evaluate the quality of a set of
parses: the constituent F-score (the traditional F-
score used in the parsing literature), and the average
F-score of the parses in the set. In the first mea-
sure we treat the whole set as a bag of constituents.
Each constituent is marked as correct (if it appears
in the gold standard parses of the set) or erroneous
(if it does not). Then, recall, precision and F-score
are calculated over these constituents. In the sec-
ond measure, the constituent F-score of each of the
parses in the set is computed, and then results are
averaged.
There are applications that use individual con-
stituents from the output of a parser while others
need the whole parse tree. For example, if the se-
lected set is used for training supervised parsers such
as the Collins parser (Collins, 1999), which collects
constituent statistics, the constituent F-score of the
selected set is the important measure. In applica-
tions such as the syntax based machine translation
model of (Yamada and Knight, 2001), a low qual-
ity tree might lead to errorenous translation of the
sentence. For such applications the average F-score
is more indicative. These measures thus represent
complementary aspects of a set quality and we con-
sider both of them.
The parser we use is the incremental parser of
(Seginer, 2007), POS tags are induced using the un-
supervised POS tagger of ((Clark, 2003), neyessen-
morph model). In each experiment, the tagger was
trained with the raw sentences of the experiment cor-
pus, and then the corpus words were POS tagged.
The output of the unsupervised POS tagger de-
pends on a random initialization. We ran the tagger
5 times, each time with a different random initializa-
tion, and then ran PUPA with its output. The results
we report for PUPA are the average over these 5 runs.
Random selection results (given for reference) were
also averages over 5 samples.
PUPA ’s parameter estimation is completely unsu-
pervised (see Section 2). No development data was
used to tune its parameters.
A 200 sentences development set from each cor-
pus was used for calibrating the parameters of the
SEPA algorithm. Based on the analysis of SEPA per-
formance with different assignments of its param-
eters given by Reichart and Rappoport (2007) (see
Section 3), we ran the SEPA algorithm with sam-
ple size (SEPA parameter 5) of 30% and 80%, and
with 2 – 10 committee members (N)6. The optimal
parameters were N = 10,5 = 80 for WSJ20, and
</bodyText>
<footnote confidence="0.991957">
6We tried higher N values but observed no improvements in
SEPA’s performance.
</footnote>
<page confidence="0.994317">
161
</page>
<figure confidence="0.9951263125">
90
64
80
70
0 200 400 600
48
0 500 1000 1500 2000
500
600
Number of Sentences
(a) WSJ, length 5
Number of Sentences
(d) WSJ, length 20
85
75
62
Average F Score
Average F Score
Average F Score
Average F Score
60
80
70
58
75
65
56
54
70
60
52
65
55
50
100
95
90
85
80
75
500 1000 1500
Number of Sentences
(c) WSJ, length 15
500 1000
Number of Sentences
(b) WSJ, length 10
65
80
60
Average F Score
60
55
50
Average F Score
55
50
45
40
Average F Score
75
70
65
60
80
75
70
65
60
55
45
35
55
300
50
0
50
0 200 400 600 800
400 200 400 600 800 1000
200 400 600 800
200 400 600 800
</figure>
<figureCaption confidence="0.996400857142857">
Figure 3: In all graphs: PUPA: solid line. SEPA: line with triangles. MC: line with circles. Random selection is
presented for reference as a dotted line. Top two rows: Average F-score for PUPA, SEPA and MC for sentences from
WSJ (top row) and NEGRA (bottom row). Bottom two rows: Constituents F-score for PUPA, SEPA and MC for
sentences from WSJ (top row) and NEGRA (bottom row). Results are presented for sentence lengths of 5,10,15 and
20 (patterns for other sentence lengths between 2 and 20 are very similar). PUPA is superior in all cases. The graphs
for PUPA and SEPA show a downward trend because parsed sentences were sorted according to score, which correlates
positively with F-score (unlike MC). The graphs converge because on the extreme right all test sentences were selected.
</figureCaption>
<figure confidence="0.981954510869565">
Number of Sentneces
(e) NEGRA, length5
95
90
85
80
75
500 1000 1500 2000
Number of Constituents
(i) WSJ, length 5
69
68
67
66
65
64
63
62
0 500 1000 1500 2000 2500 3000
Number of Constituents
(m) NEGRA, length5
Number of Sentences
(f) NEGRA, length 10
85
80
75
70
65
60
0 2000 4000 6000 8000 10000
Number of Constituents
(j) WSJ, length 10
58
56
54
52
50
48
46
440 1000 2000 3000 4000 5000
Number of Constituents
(n) NEGRA, length 10
Number of Sentences
(g) NEGRA, length 15
70
65
60
55
0 0.5 1 1.5 2
Number of Constituents x 104
(k) WSJ, length 15
50
48
46
44
42
40
0 2000 4000 6000 8000
Number of Constituents
(o) NEGRA, length 15
Number of Sentneces
(h) NEGRA, length 20
64
62
60
58
56
540 0.5 1 1.5 2 2.5
Number of Constituents x 104
(l) WSJ, length 20
69
68
67
66
65
64
63
62
0 500 1000 1500 2000 2500 3000
Number of Constituents
(p) NEGRA, length 20
Constituents F Score
Constituents F Score
700
Constituents F Score
Constituent F Score
Constituents F Score
Constituents F Score
Average F Score
Constituents F Score
Constituents F Score
N = 10, S = 30 for NEGRA20.
</figure>
<bodyText confidence="0.9752535">
We also compare PUPA to a baseline selecting the
sentences with the lowest number of constituents.
Since the number of constituents is an indication of
the complexity of the syntactic structure of a sen-
tence, it is reasonable to assume that selecting the
sentences with the lowest number of constituents is
a good selection strategy. We denote this baseline by
MC (for minimum constituents).
The incremental parser does not give any predic-
tion of its output quality as supervised generative
parsers do. We are thus not able to compare to such
a score.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9919105">
Figure 3 shows Average F-score and Constituents F-
score results for PUPA SEPA and MC, for sentences
</bodyText>
<page confidence="0.993097">
162
</page>
<bodyText confidence="0.999849188679246">
of lengths 5,10,15 and 20 in WSJ20 and NEGRA20.
The top two rows are for Average F-score (top row:
WSJ, bottom row: NEGRA), while the bottom two
rows are for Constituents F-score (top row: WSJ,
bottom row: NEGRA).
PUPA and SEPA are both better than random selec-
tion for both corpora for every sentence length. The
MC baseline is better than random selection only for
NEGRA (in which case it outperforms SEPA). For
WSJ, however, random selection is a better strategy
than MC.
It is clear from the graphs that PUPA outperforms
SEPA and MC in all experimental conditions. We
observed very similar patterns in all other sentence
lengths in WSJ20 and NEGRA20 for both Average
F-score and Constituent F-score. In other words, for
every sentence length in both corpora, PUPA outper-
forms SEPA and MC in terms of both measures. we
present our results per sentence length to deprive the
possibility that PUPA is useful only for short sen-
tences or that it prefers sentences whose syntactic
structure is not complex (i.e. with a small number of
constituents, like MC).
Table 1 shows that the same pattern of results
holds when evaluating on the whole corpus (WSJ20
or NEGRA20) without any sentence length restric-
tion.
Note that while PUPA is a fully unsupervised al-
gorithm, SEPA requires a few hundreds of sentences
for its parameters tuning.
The main result of this paper is for sentences
whose length is up to 20 words (note that most un-
supervised parser literature reports numbers for sen-
tences up to length 10). We have also ran the exper-
iments for the remaining length range, 20-40. For
NEGRA, PUPA is superior over MC up to length 36,
and both are much better than SEPA. For WSJ, PUPA
and SEPA both outperform MC, but SEPA is a bit bet-
ter than PUPA. When evaluating on the whole corpus
(i.e. without sentence length restriction, like in Ta-
ble 1) PUPA is superior over both SEPA and MC for
WSJ40 and NEGRA40.
For completeness of analysis we also experi-
mented in the condition where PUPA uses gold stan-
dard POS tags as input. The number of these tags is
35 for WSJ and 57 for NEGRA. Interestingly, PUPA
achieves in this condition the same performance as
when using the same number of POS tags induced
by an unsupervised POS tagger. Since PUPA’s per-
formance for a smaller number of POS tags is better
(see our parameter tuning discussion above), the bot-
tom line is that PUPA pefers using induced POS tags
over gold POS tags.
</bodyText>
<table confidence="0.999389">
5% 10% 20% 30% 40% 50%
WSJ20
PUPA 82.75 79.34 75.77 73.46 71.68 70.3
SEPA 78.68 75.7 72.64 70.72 69.54 68.58
MC 76.75 74.6 72.1 70.35 68.97 67.77
NEGRA20
PUPA 70.66 67.06 61.89 58.75 56.6 54.73
SEPA 66.19 62.75 59.41 57.16 55.23 53.7
MC 69.41 65.79 60.87 58.08 55.9 54.36
</table>
<tableCaption confidence="0.8180365">
Table 1: Average F–score for the top k% of constituents
selected from WSJ20 (up) and NEGRA20 (down). No sen-
</tableCaption>
<bodyText confidence="0.487462">
tence length restriction is imposed. Results presented for
PUPA , SEPA and MC. Average F–score of random se-
lection is 66.55 (WSJ20) and 47.05 (NEGRA20). PUPA is
superior over all methods.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999980892857143">
We introduced PUPA, an algorithm for unsupervised
parse assessment that utilizes POS sequence statis-
tics. PUPA is a fully unsupervised algorithm whose
parameters can be tuned in an unsupervised man-
ner. Experimenting with the Seginer unsupervised
parser and Clark’s unsupervised POS tagger on En-
glish and German corpora, PUPA was shown to out-
perform both the leading parse assessment algorithm
for supervised parsers (SEPA, even when its param-
eters are tuned on manually annotated development
data) and a strong baseline (MC).
Using PUPA, we extracted high quality parses
from the output of a parser which requires raw text
as input, using POS tags induced by an unsupervised
tagger. PUPA thus provides a way of obtaining high
quality parses without any human involvement.
For future work, we intend to use parses selected
by PUPA from the output of unsupervised parsers
as training data for supervised parsers, and in NLP
applications that use parse trees. A challenge for
the first direction is the fact that state of the art su-
pervised parsers require labeled parse trees, while
modern unsupervised parsers create unlabeled trees.
Combining PUPA with algorithms for labeled parse
trees induction (Haghighi and Klein, 2006; Reichart
and Rappoport, 2008) is a one direction to overcome
this challenge. We also intend to use PUPA to assess
the quality of parses created by supervised parsers.
</bodyText>
<page confidence="0.998669">
163
</page>
<sectionHeader confidence="0.995866" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999920340206186">
Eleftherios Avramidis and Philipp Koehn, 2008. En-
riching Morphologically Poor Languages for Statisti-
cal Machine Translation. ACL ’08.
Rens Bod, 2006a. An All-Subtrees Approach to Unsu-
pervised Parsing. ACL ’06.
Rens Bod, 2006b. Unsupervised Parsing with U-DOP.
CoNLL X.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Rich Caruana and Alexandru Niculescu-Mizil, 2006. An
Empirical Comparison of Supervised Learning Algo-
rithms. ICML ’06.
Jennifer Chu-Carroll, Krzysztof Czuba, John Prager and
Abraham Ittycheriah, 2003. In Question Answering,
Two Heads Are Better Than One. HLT-NAACL ’03.
Wenliang Chen, Youzheng Wu and Hitoshi Isahara,
2008. Learning Reliable Information for Dependency
Parsing Adaptation. Coling ’08.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech Induc-
tion. EACL ’03.
Michael Collins, 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Aron Culotta and Andrew McCallum, 2004. Confidence
Estimation for Information Extraction. HLT-NAACL
’04.
Simon Dennis, 2005. An Exemplar-based Approach to
Unsupervised Parsing. Proceedings of the 27th Con-
ference of the Cognitive Science Society.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
Grammar Induction. ACL ’06.
Daisuke Kawahara and Kiyotaka Uchimoto 2008.
Learning Reliability of Parses for Domain Adaptation
of Dependency Parsing. IJCNLP ’08.
Dan Klein and Christopher Manning, 2002. A Gener-
ative Constituent-Context Model for Improved Gram-
mar Induction. ACL ’02.
Dan Klein and Christopher Manning, 2004. Corpus-
based Induction of Syntactic Structure: Models of De-
pendency and Constituency. ACL ’04.
Dan Klein, 2005. The Unsupervised Learning ofNatural
Language Structure. Ph.D. thesis, Stanford University.
Myoung–Wan Koo, Chin-Hui Lee and Biing–Hwang
Juang 2001. Speech Recognition and Utterance Ver-
ification Based on a Generalized Confidence Score.
IEEE Transactions on Speech and Audio Processing,
9(8):821–832.
Cody Kwok, Oren Etzioni and Daniel S. Weld, 2001.
Scaling Question Answering to the Web. WWW ’01.
Matthew Lease and Eugene Charniak, 2005. Towards a
Syntactic Account of Punctuation. IJCNLP ’05.
Feng Lin and Fuliang Weng, 2008. Computing Confi-
dence Scores for All Sub Parse Trees. ACL ’08, short
paper.
David McClosky and Eugene Charniak, 2008. Self-
Training for Biomedical Parsing. ACL ’08, short pa-
per.
Dan Moldovan, Christine Clark, Sanda Harabagiu and
Steve Maiorano, 2003. Cogex: A Logic Prover for
Question Answering. HLT-NAACL ’03.
Vasin Punyakanok and Dan Roth and Wen-tau Yih, 2008.
The Importance of Syntactic Parsing and Inference in
Semantic Role Labeling. Computational Linguistics,
34(2):257-287.
Sujith Ravi, Kevin Knight and Radu Soricut, 2008. Au-
tomatic Prediction of Parser Accuracy. EMNLP ’08.
Roi Reichart and Ari Rappoport, 2007. An Ensemble
Method for Selection of High Quality Parses. ACL
’07.
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. COLING ’08.
Benjamin Rosenfeld and Ronen Feldman, 2007. Us-
ing Corpus Statistics on Entities to Improve Semi–
Supervised Relation Extraction From The WEB. ACL
’07.
Kenji Sagae and Junichi Tsujii, 2007. Dependency
Parsing and Domain Adaptation with LR Models and
Parser Ensemble. EMNLP-CoNLL ’07.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ’07.
Vivek Srikumar, Roi Reichart, Mark Sammons, Ari Rap-
poport and Dan Roth, 2008. Extraction of Entailed
Semantic Relations Through Syntax-based Comma
Resolution. ACL ’08.
Noah A. Smith and Jason Eisner, 2006. Annealing Struc-
tural Bias in Multilingual Weighted Grammar Induc-
tion. ACL ’06.
Nicola Ueffing and Hermann Ney, 2007. Word-
Level Confidence Estimation for Machine Translation.
Computational Linguistics, 33(1):9–40.
Kenji Yamada and Kevin Knight, 2001. A Syntax-Based
Statistical Translation Model. ACL ’01.
Alexander Yates, Stefan Schoenmackers and Oren Et-
zioni, 2006. Detecting Parser Errors Using Web-
based Semantic Filters. EMNLP ’06.
</reference>
<page confidence="0.99852">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.869108">
<title confidence="0.9989075">Automatic Selection of High Quality Parses Created By a Unsupervised Parser</title>
<author confidence="0.999621">Roi Reichart Ari Rappoport</author>
<affiliation confidence="0.9990355">ICNC Institute of computer science The Hebrew University The Hebrew University</affiliation>
<email confidence="0.943447">roiri@cs.huji.ac.ilarir@cs.huji.ac.il</email>
<abstract confidence="0.996980730769231">The average results obtained by unsupervised statistical parsers have greatly improved in the last few years, but on many specific sentences they are of rather low quality. The output of such parsers is becoming valuable for various applications, and it is radically less expensive to create than manually annotated training data. Hence, automatic selection of high quality parses created by unsupervised parsers is an important problem. this paper we present a Parse Assessment The algorithm assesses the quality of a parse tree using POS sequence statistics collected from a batch of parsed sentences. We evaluate the algorithm by using an unsupervised POS tagger and an unsupervised parser, selecting high quality parsed sentences from English (WSJ) and German (NEGRA) corpora. show that the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsuperbaseline. Consequently, obtaining high quality parses without any human involvement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eleftherios Avramidis</author>
<author>Philipp Koehn</author>
</authors>
<title>Enriching Morphologically Poor Languages for Statistical Machine Translation.</title>
<date>2008</date>
<journal>ACL</journal>
<volume>08</volume>
<contexts>
<context position="1885" citStr="Avramidis and Koehn, 2008" startWordPosition="288" endWordPosition="291">s without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for manual annotation. The high cost of manual annotation of training data for supervised parsers imposes a significant burden on their usage. A possible answer to this problem can be provided by high quality parses produced by unsupervised p</context>
</contexts>
<marker>Avramidis, Koehn, 2008</marker>
<rawString>Eleftherios Avramidis and Philipp Koehn, 2008. Enriching Morphologically Poor Languages for Statistical Machine Translation. ACL ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An All-Subtrees Approach to Unsupervised Parsing.</title>
<date>2006</date>
<journal>ACL</journal>
<volume>06</volume>
<contexts>
<context position="1597" citStr="Bod, 2006" startWordPosition="242" endWordPosition="243">quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is ver</context>
<context position="17040" citStr="Bod, 2006" startWordPosition="2962" endWordPosition="2963"> (see (Klein, 2005) for a recent review). Recently, unsupervised parsing algorithms have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based mod(3) Pupa5core(5) = 1 Score(N(W)) Y �W=Y W 159 Figure 2: Number of constituents appearing at least t times (nc(t)) as a function of t. Shown are WSJ (left) and NEGRA (right), where constituents are represented according to PUPA’s PCR with 5 POS tags (P = 5, solid line) or 50 POS tags (P = 50, dashed line). els (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as input in all of these algorithms except Seginer’s, which uses plain text. Quality assessment of a learning algorithm’s output and selection of high quality instances have been addressed for supervised algorithms (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised constituency parsers (Yates et al., 2006; R</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod, 2006a. An All-Subtrees Approach to Unsupervised Parsing. ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Unsupervised Parsing with U-DOP.</title>
<date>2006</date>
<journal>CoNLL X.</journal>
<contexts>
<context position="1597" citStr="Bod, 2006" startWordPosition="242" endWordPosition="243">quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is ver</context>
<context position="17040" citStr="Bod, 2006" startWordPosition="2962" endWordPosition="2963"> (see (Klein, 2005) for a recent review). Recently, unsupervised parsing algorithms have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based mod(3) Pupa5core(5) = 1 Score(N(W)) Y �W=Y W 159 Figure 2: Number of constituents appearing at least t times (nc(t)) as a function of t. Shown are WSJ (left) and NEGRA (right), where constituents are represented according to PUPA’s PCR with 5 POS tags (P = 5, solid line) or 50 POS tags (P = 50, dashed line). els (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as input in all of these algorithms except Seginer’s, which uses plain text. Quality assessment of a learning algorithm’s output and selection of high quality instances have been addressed for supervised algorithms (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised constituency parsers (Yates et al., 2006; R</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod, 2006b. Unsupervised Parsing with U-DOP. CoNLL X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>The NEGRA Export Format.</title>
<date>1997</date>
<tech>CLAUS Report,</tech>
<institution>Saarland University.</institution>
<contexts>
<context position="21599" citStr="Brants, 1997" startWordPosition="3729" endWordPosition="3730">enting a constituent by its yield t t # of constituents appearing at least t times 15000 10000 5000 0 0 50 100 P = 5 P = 50 4000 8000 7000 6000 5000 3000 2000 1000 0 0 50 100 P = 5 P = 50 160 and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ20, 25236 sentences, 225126 constituents) and the German NEGRA corpus (Brants, 1997) (NEGRA20, 15610 sentences, 108540 constiteunts), both containing newspaper texts. The unsupervised parsers of the kind addressed in this paper output unlabeled parse trees. To evaluate the quality of a single parse tree with respect to another, we use the unlabeled F-score (UF = 2·UR·UP), where UR and UP are unlabeled recall UR+UP and unlabeled precision respectively. Following the unsupervised parsing literature, multiple brackets and brackets covering a single word are not counted, but the sentence level bracket is. We exclude punctuation and null elements according to the scheme of (Klein,</context>
</contexts>
<marker>Brants, 1997</marker>
<rawString>Thorsten Brants, 1997. The NEGRA Export Format. CLAUS Report, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
<author>Alexandru Niculescu-Mizil</author>
</authors>
<title>An Empirical Comparison of Supervised Learning Algorithms.</title>
<date>2006</date>
<journal>ICML</journal>
<volume>06</volume>
<contexts>
<context position="3821" citStr="Caruana and Niculescu-Mizil, 2006" startWordPosition="603" endWordPosition="607">15% for NEGRA10. In this paper we address the task of a fully unsupervised assessment of high quality parses cre156 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentence</context>
<context position="17550" citStr="Caruana and Niculescu-Mizil, 2006" startWordPosition="3039" endWordPosition="3042">represented according to PUPA’s PCR with 5 POS tags (P = 5, solid line) or 50 POS tags (P = 50, dashed line). els (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as input in all of these algorithms except Seginer’s, which uses plain text. Quality assessment of a learning algorithm’s output and selection of high quality instances have been addressed for supervised algorithms (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised constituency parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). For dependency parsing in a corpus adaptation scenario, (Kawahara and Uchimoto, 2008) built a binary classifier that classifies each parse in the parser’s output as reliable or not. To do that, they selected 2500 sentences from the parser’s output, compared them to their manually created gold standard, and used accurate (inaccurate) parses as positive (negative) examples for the classifier. Their approach is supervised and the features used by the classifie</context>
</contexts>
<marker>Caruana, Niculescu-Mizil, 2006</marker>
<rawString>Rich Caruana and Alexandru Niculescu-Mizil, 2006. An Empirical Comparison of Supervised Learning Algorithms. ICML ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Krzysztof Czuba</author>
<author>John Prager</author>
<author>Abraham Ittycheriah</author>
</authors>
<date>2003</date>
<booktitle>In Question Answering, Two Heads Are Better Than One. HLT-NAACL ’03.</booktitle>
<contexts>
<context position="20925" citStr="Chu-Carroll et al., 2003" startWordPosition="3594" endWordPosition="3597">hared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield t t # of constituents appearing at least t times 15000 10000 5000 0 0 50 100 P = 5 P = 50 4000 8000 7000 6000 5000 3000 2000 1000 0 0 50 100 P = 5 P = 50 160 and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ20, 25236 </context>
</contexts>
<marker>Chu-Carroll, Czuba, Prager, Ittycheriah, 2003</marker>
<rawString>Jennifer Chu-Carroll, Krzysztof Czuba, John Prager and Abraham Ittycheriah, 2003. In Question Answering, Two Heads Are Better Than One. HLT-NAACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
</authors>
<title>Youzheng Wu and Hitoshi Isahara,</title>
<date>2008</date>
<journal>Coling</journal>
<volume>08</volume>
<marker>Chen, 2008</marker>
<rawString>Wenliang Chen, Youzheng Wu and Hitoshi Isahara, 2008. Learning Reliable Information for Dependency Parsing Adaptation. Coling ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining Distributional and Morphological Information for Part of Speech Induction.</title>
<date>2003</date>
<journal>EACL</journal>
<volume>03</volume>
<contexts>
<context position="6019" citStr="Clark, 2003" startWordPosition="970" endWordPosition="971">endent and is beyond the scope of this paper. The unsupervised parser we use is the Seginer (2007) incremental parser2, which achieves state-of1The algorithm can be used with supervised POS taggers and parsers, but we focus here on the fully unsupervised scenario, which is novel and more useful. For completeness of analysis, we experimented with PUPA using a supervised POS tagger (see Section 5). Using PUPA with supervised parsers is left for future work. 2www.seggu.net/ccl. the-art results without using manually created POS tags. The POS tags we use are induced by the unsupervised tagger of (Clark, 2003)3. Since both tagger and parser do not require any manual annotation, PUPA identifies high quality parses without any human involvement. The incremental parser of (Seginer, 2007) does not give any prediction of its output quality, and extracting such a prediction from its internal data structures is not straightforward. Such a prediction can be given by supervised parsers in terms of the parse likelihood, but this was shown to be of medium quality (Reichart and Rappoport, 2007). While the algorithms of Yates et al. (2006), Kawahara and Uchimoto (2008) and Ravi et al. (2008) are supervised (Sec</context>
<context position="8230" citStr="Clark (2003)" startWordPosition="1332" endWordPosition="1333">tion scenario are produced by an unsupervised parser. The algorithm assigns each parsed sentence a score reflecting its quality. 3www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html, the neyessenmorph model. 4This is in contrast to algorithms for selection from the results of supervised constituency parsers, which were evaluated only for English (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). 5This is the traditional parsing F-score. 157 The algorithm has three steps. First, the words in I are POS tagged (in our case, using the fully unsupervised POS induction algorithm of Clark (2003)). Second, POS statistics about the constituents in I are collected. Finally, a quality score is calculated for each parsed sentence in I using the POS statistics. In the following we detail the last two steps. Collecting POS statistics. In its second step, the algorithm collects statistics about the constituents in the input set I. Recall that the yield of a constituent is the set of words covered by it. The PUPA constituent representation (PCR) consists of three features: (1) the ordered POS tag sequence of the constituent’s yield, (2) the constituents’ right context, and (3) the constituent</context>
<context position="24383" citStr="Clark, 2003" startWordPosition="4199" endWordPosition="4200">the Collins parser (Collins, 1999), which collects constituent statistics, the constituent F-score of the selected set is the important measure. In applications such as the syntax based machine translation model of (Yamada and Knight, 2001), a low quality tree might lead to errorenous translation of the sentence. For such applications the average F-score is more indicative. These measures thus represent complementary aspects of a set quality and we consider both of them. The parser we use is the incremental parser of (Seginer, 2007), POS tags are induced using the unsupervised POS tagger of ((Clark, 2003), neyessenmorph model). In each experiment, the tagger was trained with the raw sentences of the experiment corpus, and then the corpus words were POS tagged. The output of the unsupervised POS tagger depends on a random initialization. We ran the tagger 5 times, each time with a different random initialization, and then ran PUPA with its output. The results we report for PUPA are the average over these 5 runs. Random selection results (given for reference) were also averages over 5 samples. PUPA ’s parameter estimation is completely unsupervised (see Section 2). No development data was used t</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark, 2003. Combining Distributional and Morphological Information for Part of Speech Induction. EACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="23805" citStr="Collins, 1999" startWordPosition="4105" endWordPosition="4106">e we treat the whole set as a bag of constituents. Each constituent is marked as correct (if it appears in the gold standard parses of the set) or erroneous (if it does not). Then, recall, precision and F-score are calculated over these constituents. In the second measure, the constituent F-score of each of the parses in the set is computed, and then results are averaged. There are applications that use individual constituents from the output of a parser while others need the whole parse tree. For example, if the selected set is used for training supervised parsers such as the Collins parser (Collins, 1999), which collects constituent statistics, the constituent F-score of the selected set is the important measure. In applications such as the syntax based machine translation model of (Yamada and Knight, 2001), a low quality tree might lead to errorenous translation of the sentence. For such applications the average F-score is more indicative. These measures thus represent complementary aspects of a set quality and we consider both of them. The parser we use is the incremental parser of (Seginer, 2007), POS tags are induced using the unsupervised POS tagger of ((Clark, 2003), neyessenmorph model)</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins, 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>Confidence Estimation for Information Extraction.</title>
<date>2004</date>
<journal>HLT-NAACL</journal>
<volume>04</volume>
<contexts>
<context position="20894" citStr="Culotta and McCallum, 2004" startWordPosition="3588" endWordPosition="3592">highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield t t # of constituents appearing at least t times 15000 10000 5000 0 0 50 100 P = 5 P = 50 4000 8000 7000 6000 5000 3000 2000 1000 0 0 50 100 P = 5 P = 50 160 and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English W</context>
</contexts>
<marker>Culotta, McCallum, 2004</marker>
<rawString>Aron Culotta and Andrew McCallum, 2004. Confidence Estimation for Information Extraction. HLT-NAACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Dennis</author>
</authors>
<title>An Exemplar-based Approach to Unsupervised Parsing.</title>
<date>2005</date>
<booktitle>Proceedings of the 27th Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="17097" citStr="Dennis, 2005" startWordPosition="2970" endWordPosition="2971">nsupervised parsing algorithms have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based mod(3) Pupa5core(5) = 1 Score(N(W)) Y �W=Y W 159 Figure 2: Number of constituents appearing at least t times (nc(t)) as a function of t. Shown are WSJ (left) and NEGRA (right), where constituents are represented according to PUPA’s PCR with 5 POS tags (P = 5, solid line) or 50 POS tags (P = 50, dashed line). els (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as input in all of these algorithms except Seginer’s, which uses plain text. Quality assessment of a learning algorithm’s output and selection of high quality instances have been addressed for supervised algorithms (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised constituency parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). For depe</context>
</contexts>
<marker>Dennis, 2005</marker>
<rawString>Simon Dennis, 2005. An Exemplar-based Approach to Unsupervised Parsing. Proceedings of the 27th Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven Grammar Induction.</title>
<date>2006</date>
<journal>ACL</journal>
<volume>06</volume>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein, 2006. Prototype-driven Grammar Induction. ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>Learning Reliability of Parses for Domain Adaptation of Dependency Parsing.</title>
<date>2008</date>
<journal>IJCNLP</journal>
<volume>08</volume>
<contexts>
<context position="3940" citStr="Kawahara and Uchimoto, 2008" startWordPosition="621" endWordPosition="624">s of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1. The constituents in the batch are represented using the POS sequences of their yield and of the yields of neighbori</context>
<context position="6576" citStr="Kawahara and Uchimoto (2008)" startWordPosition="1059" endWordPosition="1063">POS tags we use are induced by the unsupervised tagger of (Clark, 2003)3. Since both tagger and parser do not require any manual annotation, PUPA identifies high quality parses without any human involvement. The incremental parser of (Seginer, 2007) does not give any prediction of its output quality, and extracting such a prediction from its internal data structures is not straightforward. Such a prediction can be given by supervised parsers in terms of the parse likelihood, but this was shown to be of medium quality (Reichart and Rappoport, 2007). While the algorithms of Yates et al. (2006), Kawahara and Uchimoto (2008) and Ravi et al. (2008) are supervised (Section 3), the ensemble based SEPA algorithm (Reichart and Rappoport, 2007) can be applied to unsupervised parsers in a way that preserves the unsupervised nature of the selection task. To compare between two algorithms, we use each of them to assess the quality of the sentences in English and German corpora (WSJ and NEGRA)4. We show that for every sentence length (up to 20) the quality of the top scored k sentences according to PUPA is higher than the quality of SEPA’s list (for every k). As in (Reichart and Rappoport, 2007), the quality of a set selec</context>
<context position="17774" citStr="Kawahara and Uchimoto, 2008" startWordPosition="3074" endWordPosition="3077">and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as input in all of these algorithms except Seginer’s, which uses plain text. Quality assessment of a learning algorithm’s output and selection of high quality instances have been addressed for supervised algorithms (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised constituency parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). For dependency parsing in a corpus adaptation scenario, (Kawahara and Uchimoto, 2008) built a binary classifier that classifies each parse in the parser’s output as reliable or not. To do that, they selected 2500 sentences from the parser’s output, compared them to their manually created gold standard, and used accurate (inaccurate) parses as positive (negative) examples for the classifier. Their approach is supervised and the features used by the classifier are dependency motivated. As far as we know, the present paper is the first to address the task of selecting high quality parses from the output of unsupervised parsers. The algorithms of Yates et al. (2006), Kawahara and </context>
<context position="19973" citStr="Kawahara and Uchimoto (2008)" startWordPosition="3443" endWordPosition="3446">m the training data and used for training the parsing algorithm A. In that way N committee members are created. Then, each of the test sentences is parsed by each of the N committee members and an agreement score ranging from 0 to 100 between the committee members is calculated. All unsupervised parsers mentioned above (including the Seginer parser), have a training phase where parameter values are estimated from unlabeled data. SEPA can thus be applied to the unsupervised case. Automatic selection of high quality parses has been shown to improve parser adaptation. Sagae and Tsujii (2007) and Kawahara and Uchimoto (2008) applied a self-training protocol to a parser adaptation scenario but used only high quality parses to retrain the parser. In the first work, high quality parses were selected using an ensemble method, while in the second a binary classifier was used (see above). The first system achieved the highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to </context>
</contexts>
<marker>Kawahara, Uchimoto, 2008</marker>
<rawString>Daisuke Kawahara and Kiyotaka Uchimoto 2008. Learning Reliability of Parses for Domain Adaptation of Dependency Parsing. IJCNLP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>A Generative Constituent-Context Model for Improved Grammar Induction.</title>
<date>2002</date>
<journal>ACL</journal>
<volume>02</volume>
<contexts>
<context position="1561" citStr="Klein and Manning, 2002" startWordPosition="233" endWordPosition="237">tagger and an unsupervised parser, selecting high quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many </context>
<context position="16647" citStr="Klein and Manning, 2002" startWordPosition="2887" endWordPosition="2890">ion (1); recall that X = telbow). We thus want to select the P value that maximizes telbow. We therefore minimize P. telbow values for P ∈ {3, ... ,10} are very similar. Indeed, PUPA achieves its best performance for P ∈ {3,... ,10} and it is insensitive to the selection of P in this range. In Section 5 we report results with P = 5. 3 Related Work Unsupervised parsing has been explored for several decades (see (Klein, 2005) for a recent review). Recently, unsupervised parsing algorithms have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based mod(3) Pupa5core(5) = 1 Score(N(W)) Y �W=Y W 159 Figure 2: Number of constituents appearing at least t times (nc(t)) as a function of t. Shown are WSJ (left) and NEGRA (right), where constituents are represented according to PUPA’s PCR with 5 POS tags (P = 5, solid line) or 50 POS tags (P = 50, dashed line). els (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain g</context>
<context position="21289" citStr="Klein and Manning, 2002" startWordPosition="3671" endWordPosition="3675"> translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield t t # of constituents appearing at least t times 15000 10000 5000 0 0 50 100 P = 5 P = 50 4000 8000 7000 6000 5000 3000 2000 1000 0 0 50 100 P = 5 P = 50 160 and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ20, 25236 sentences, 225126 constituents) and the German NEGRA corpus (Brants, 1997) (NEGRA20, 15610 sentences, 108540 constiteunts), both containing newspaper texts. The unsupervised parsers of the kind addressed in this paper output unlabeled parse trees. To evaluate the quality of a single parse tree with respect to another, we use the unlabeled F-score (UF = 2·UR·UP),</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher Manning, 2002. A Generative Constituent-Context Model for Improved Grammar Induction. ACL ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Corpusbased Induction of Syntactic Structure: Models of Dependency and Constituency.</title>
<date>2004</date>
<journal>ACL</journal>
<volume>04</volume>
<contexts>
<context position="1586" citStr="Klein and Manning, 2004" startWordPosition="238" endWordPosition="241">d parser, selecting high quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used</context>
<context position="16701" citStr="Klein and Manning, 2004" startWordPosition="2896" endWordPosition="2899">ct the P value that maximizes telbow. We therefore minimize P. telbow values for P ∈ {3, ... ,10} are very similar. Indeed, PUPA achieves its best performance for P ∈ {3,... ,10} and it is insensitive to the selection of P in this range. In Section 5 we report results with P = 5. 3 Related Work Unsupervised parsing has been explored for several decades (see (Klein, 2005) for a recent review). Recently, unsupervised parsing algorithms have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based mod(3) Pupa5core(5) = 1 Score(N(W)) Y �W=Y W 159 Figure 2: Number of constituents appearing at least t times (nc(t)) as a function of t. Shown are WSJ (left) and NEGRA (right), where constituents are represented according to PUPA’s PCR with 5 POS tags (P = 5, solid line) or 50 POS tags (P = 50, dashed line). els (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as inp</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher Manning, 2004. Corpusbased Induction of Syntactic Structure: Models of Dependency and Constituency. ACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>The Unsupervised Learning ofNatural Language Structure.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="16450" citStr="Klein, 2005" startWordPosition="2860" endWordPosition="2861">an telbow for P = 5. Generally, telbow is a monotonically decreasing function of P. Lower telbow values imply that PUPA would be less distinctive between constituents quality (see equation (1); recall that X = telbow). We thus want to select the P value that maximizes telbow. We therefore minimize P. telbow values for P ∈ {3, ... ,10} are very similar. Indeed, PUPA achieves its best performance for P ∈ {3,... ,10} and it is insensitive to the selection of P in this range. In Section 5 we report results with P = 5. 3 Related Work Unsupervised parsing has been explored for several decades (see (Klein, 2005) for a recent review). Recently, unsupervised parsing algorithms have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based mod(3) Pupa5core(5) = 1 Score(N(W)) Y �W=Y W 159 Figure 2: Number of constituents appearing at least t times (nc(t)) as a function of t. Shown are WSJ (left) and NEGRA (right), where constituents are represented according to PUPA’s PCR with 5 POS tags (P = 5, solid line) or 50 POS tags (P = 50, dashed line). els (Bod, 2006a; Bod, 20</context>
<context position="22205" citStr="Klein, 2005" startWordPosition="3823" endWordPosition="3824"> 1997) (NEGRA20, 15610 sentences, 108540 constiteunts), both containing newspaper texts. The unsupervised parsers of the kind addressed in this paper output unlabeled parse trees. To evaluate the quality of a single parse tree with respect to another, we use the unlabeled F-score (UF = 2·UR·UP), where UR and UP are unlabeled recall UR+UP and unlabeled precision respectively. Following the unsupervised parsing literature, multiple brackets and brackets covering a single word are not counted, but the sentence level bracket is. We exclude punctuation and null elements according to the scheme of (Klein, 2005). The performance of unsupervised parsers markedly degrades as sentence length increases. For example, the Average sentence F–score for WSJ sentences of length 10 is 71.4% compared to 58.5 for sentences of length 20 (the numbers for NEGRA are 48.2% and 36.9%). We therefore evaluate PUPA (and the baseline) for sentences of a given length. We do this for every sentence of length 2-20 in WSJ20 and NEGRA20. For every sentence length L, we use PUPA and the baseline algorithm (SEPA) to give a quality score to each of the sentences of that length in the experimental corpus. We then compare the qualit</context>
</contexts>
<marker>Klein, 2005</marker>
<rawString>Dan Klein, 2005. The Unsupervised Learning ofNatural Language Structure. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myoung–Wan Koo</author>
</authors>
<title>Chin-Hui Lee and Biing–Hwang Juang</title>
<date>2001</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<pages>9--8</pages>
<marker>Koo, 2001</marker>
<rawString>Myoung–Wan Koo, Chin-Hui Lee and Biing–Hwang Juang 2001. Speech Recognition and Utterance Verification Based on a Generalized Confidence Score. IEEE Transactions on Speech and Audio Processing, 9(8):821–832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cody Kwok</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling Question Answering to the Web.</title>
<date>2001</date>
<journal>WWW</journal>
<volume>01</volume>
<contexts>
<context position="1732" citStr="Kwok et al., 2001" startWordPosition="262" endWordPosition="265">se assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for manual annotation. The high cost of manual annotation of training data for supervised</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>Cody Kwok, Oren Etzioni and Daniel S. Weld, 2001. Scaling Question Answering to the Web. WWW ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
</authors>
<title>Towards a Syntactic Account of Punctuation.</title>
<date>2005</date>
<journal>IJCNLP</journal>
<volume>05</volume>
<contexts>
<context position="2090" citStr="Lease and Charniak, 2005" startWordPosition="324" endWordPosition="327">he last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for manual annotation. The high cost of manual annotation of training data for supervised parsers imposes a significant burden on their usage. A possible answer to this problem can be provided by high quality parses produced by unsupervised parsers that require little to no manual efforts for their training. These parses can be used either as input for applications, or as training material for modern supervised parsers whose output will in tur</context>
</contexts>
<marker>Lease, Charniak, 2005</marker>
<rawString>Matthew Lease and Eugene Charniak, 2005. Towards a Syntactic Account of Punctuation. IJCNLP ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Lin</author>
<author>Fuliang Weng</author>
</authors>
<title>Computing Confidence Scores for All Sub Parse Trees. ACL ’08, short paper.</title>
<date>2008</date>
<contexts>
<context position="20966" citStr="Lin and Weng, 2008" startWordPosition="3602" endWordPosition="3605">arsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield t t # of constituents appearing at least t times 15000 10000 5000 0 0 50 100 P = 5 P = 50 4000 8000 7000 6000 5000 3000 2000 1000 0 0 50 100 P = 5 P = 50 160 and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ20, 25236 sentences, 225126 constituents) and the G</context>
</contexts>
<marker>Lin, Weng, 2008</marker>
<rawString>Feng Lin and Fuliang Weng, 2008. Computing Confidence Scores for All Sub Parse Trees. ACL ’08, short paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>SelfTraining for Biomedical Parsing. ACL ’08, short paper.</title>
<date>2008</date>
<contexts>
<context position="2120" citStr="McClosky and Charniak, 2008" startWordPosition="328" endWordPosition="332">gnificant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for manual annotation. The high cost of manual annotation of training data for supervised parsers imposes a significant burden on their usage. A possible answer to this problem can be provided by high quality parses produced by unsupervised parsers that require little to no manual efforts for their training. These parses can be used either as input for applications, or as training material for modern supervised parsers whose output will in turn be used by applications. Alt</context>
</contexts>
<marker>McClosky, Charniak, 2008</marker>
<rawString>David McClosky and Eugene Charniak, 2008. SelfTraining for Biomedical Parsing. ACL ’08, short paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Christine Clark</author>
<author>Sanda Harabagiu</author>
<author>Steve Maiorano</author>
</authors>
<title>Cogex: A Logic Prover for Question Answering.</title>
<date>2003</date>
<journal>HLT-NAACL</journal>
<volume>03</volume>
<contexts>
<context position="1764" citStr="Moldovan et al., 2003" startWordPosition="268" endWordPosition="271">supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for manual annotation. The high cost of manual annotation of training data for supervised parsers imposes a significant b</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>Dan Moldovan, Christine Clark, Sanda Harabagiu and Steve Maiorano, 2003. Cogex: A Logic Prover for Question Answering. HLT-NAACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<date>2008</date>
<booktitle>The Importance of Syntactic Parsing and Inference in Semantic Role Labeling. Computational Linguistics,</booktitle>
<pages>34--2</pages>
<contexts>
<context position="1798" citStr="Punyakanok et al., 2008" startWordPosition="274" endWordPosition="277"> strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for manual annotation. The high cost of manual annotation of training data for supervised parsers imposes a significant burden on their usage. A possible a</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok and Dan Roth and Wen-tau Yih, 2008. The Importance of Syntactic Parsing and Inference in Semantic Role Labeling. Computational Linguistics, 34(2):257-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
<author>Radu Soricut</author>
</authors>
<title>Automatic Prediction of Parser Accuracy.</title>
<date>2008</date>
<journal>EMNLP</journal>
<volume>08</volume>
<contexts>
<context position="3960" citStr="Ravi et al., 2008" startWordPosition="625" endWordPosition="628">e on Computational Natural Language Learning (CoNLL), pages 156–164, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1. The constituents in the batch are represented using the POS sequences of their yield and of the yields of neighboring constituents. Con</context>
<context position="6599" citStr="Ravi et al. (2008)" startWordPosition="1065" endWordPosition="1068">e unsupervised tagger of (Clark, 2003)3. Since both tagger and parser do not require any manual annotation, PUPA identifies high quality parses without any human involvement. The incremental parser of (Seginer, 2007) does not give any prediction of its output quality, and extracting such a prediction from its internal data structures is not straightforward. Such a prediction can be given by supervised parsers in terms of the parse likelihood, but this was shown to be of medium quality (Reichart and Rappoport, 2007). While the algorithms of Yates et al. (2006), Kawahara and Uchimoto (2008) and Ravi et al. (2008) are supervised (Section 3), the ensemble based SEPA algorithm (Reichart and Rappoport, 2007) can be applied to unsupervised parsers in a way that preserves the unsupervised nature of the selection task. To compare between two algorithms, we use each of them to assess the quality of the sentences in English and German corpora (WSJ and NEGRA)4. We show that for every sentence length (up to 20) the quality of the top scored k sentences according to PUPA is higher than the quality of SEPA’s list (for every k). As in (Reichart and Rappoport, 2007), the quality of a set selected from the parser’s o</context>
<context position="8032" citStr="Ravi et al., 2008" startWordPosition="1296" endWordPosition="1299">and results. 2 The POS-based Unsupervised Parse Assessment (PUPA) Algorithm In this section we detail our parse assessment algorithm. Its input consists of a set I of parsed sentences, which in our evaluation scenario are produced by an unsupervised parser. The algorithm assigns each parsed sentence a score reflecting its quality. 3www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html, the neyessenmorph model. 4This is in contrast to algorithms for selection from the results of supervised constituency parsers, which were evaluated only for English (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). 5This is the traditional parsing F-score. 157 The algorithm has three steps. First, the words in I are POS tagged (in our case, using the fully unsupervised POS induction algorithm of Clark (2003)). Second, POS statistics about the constituents in I are collected. Finally, a quality score is calculated for each parsed sentence in I using the POS statistics. In the following we detail the last two steps. Collecting POS statistics. In its second step, the algorithm collects statistics about the constituents in the input set I. Recall that the yield of a constituent is the set of words covered </context>
<context position="17687" citStr="Ravi et al., 2008" startWordPosition="3061" endWordPosition="3064">ased approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as input in all of these algorithms except Seginer’s, which uses plain text. Quality assessment of a learning algorithm’s output and selection of high quality instances have been addressed for supervised algorithms (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised constituency parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). For dependency parsing in a corpus adaptation scenario, (Kawahara and Uchimoto, 2008) built a binary classifier that classifies each parse in the parser’s output as reliable or not. To do that, they selected 2500 sentences from the parser’s output, compared them to their manually created gold standard, and used accurate (inaccurate) parses as positive (negative) examples for the classifier. Their approach is supervised and the features used by the classifier are dependency motivated. As far as we know, the present paper is the first to address the task of selecting high quality parses from t</context>
</contexts>
<marker>Ravi, Knight, Soricut, 2008</marker>
<rawString>Sujith Ravi, Kevin Knight and Radu Soricut, 2008. Automatic Prediction of Parser Accuracy. EMNLP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>An Ensemble Method for Selection of High Quality Parses.</title>
<date>2007</date>
<journal>ACL</journal>
<volume>07</volume>
<contexts>
<context position="3911" citStr="Reichart and Rappoport, 2007" startWordPosition="617" endWordPosition="620">ality parses cre156 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1. The constituents in the batch are represented using the POS sequences of their yield a</context>
<context position="6501" citStr="Reichart and Rappoport, 2007" startWordPosition="1047" endWordPosition="1050">seggu.net/ccl. the-art results without using manually created POS tags. The POS tags we use are induced by the unsupervised tagger of (Clark, 2003)3. Since both tagger and parser do not require any manual annotation, PUPA identifies high quality parses without any human involvement. The incremental parser of (Seginer, 2007) does not give any prediction of its output quality, and extracting such a prediction from its internal data structures is not straightforward. Such a prediction can be given by supervised parsers in terms of the parse likelihood, but this was shown to be of medium quality (Reichart and Rappoport, 2007). While the algorithms of Yates et al. (2006), Kawahara and Uchimoto (2008) and Ravi et al. (2008) are supervised (Section 3), the ensemble based SEPA algorithm (Reichart and Rappoport, 2007) can be applied to unsupervised parsers in a way that preserves the unsupervised nature of the selection task. To compare between two algorithms, we use each of them to assess the quality of the sentences in English and German corpora (WSJ and NEGRA)4. We show that for every sentence length (up to 20) the quality of the top scored k sentences according to PUPA is higher than the quality of SEPA’s list (for</context>
<context position="8012" citStr="Reichart and Rappoport, 2007" startWordPosition="1292" endWordPosition="1295"> present the evaluation setup and results. 2 The POS-based Unsupervised Parse Assessment (PUPA) Algorithm In this section we detail our parse assessment algorithm. Its input consists of a set I of parsed sentences, which in our evaluation scenario are produced by an unsupervised parser. The algorithm assigns each parsed sentence a score reflecting its quality. 3www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html, the neyessenmorph model. 4This is in contrast to algorithms for selection from the results of supervised constituency parsers, which were evaluated only for English (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). 5This is the traditional parsing F-score. 157 The algorithm has three steps. First, the words in I are POS tagged (in our case, using the fully unsupervised POS induction algorithm of Clark (2003)). Second, POS statistics about the constituents in I are collected. Finally, a quality score is calculated for each parsed sentence in I using the POS statistics. In the following we detail the last two steps. Collecting POS statistics. In its second step, the algorithm collects statistics about the constituents in the input set I. Recall that the yield of a constituent is the s</context>
<context position="17667" citStr="Reichart and Rappoport, 2007" startWordPosition="3057" endWordPosition="3060">6a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as input in all of these algorithms except Seginer’s, which uses plain text. Quality assessment of a learning algorithm’s output and selection of high quality instances have been addressed for supervised algorithms (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised constituency parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). For dependency parsing in a corpus adaptation scenario, (Kawahara and Uchimoto, 2008) built a binary classifier that classifies each parse in the parser’s output as reliable or not. To do that, they selected 2500 sentences from the parser’s output, compared them to their manually created gold standard, and used accurate (inaccurate) parses as positive (negative) examples for the classifier. Their approach is supervised and the features used by the classifier are dependency motivated. As far as we know, the present paper is the first to address the task of selecting high q</context>
<context position="22975" citStr="Reichart and Rappoport (2007)" startWordPosition="3956" endWordPosition="3959">entences of length 10 is 71.4% compared to 58.5 for sentences of length 20 (the numbers for NEGRA are 48.2% and 36.9%). We therefore evaluate PUPA (and the baseline) for sentences of a given length. We do this for every sentence of length 2-20 in WSJ20 and NEGRA20. For every sentence length L, we use PUPA and the baseline algorithm (SEPA) to give a quality score to each of the sentences of that length in the experimental corpus. We then compare the quality of the top k parsed sentences according to each algorithm. We do this for every k from 1 to the number of sentences of length L. Following Reichart and Rappoport (2007), we use two measures to evaluate the quality of a set of parses: the constituent F-score (the traditional Fscore used in the parsing literature), and the average F-score of the parses in the set. In the first measure we treat the whole set as a bag of constituents. Each constituent is marked as correct (if it appears in the gold standard parses of the set) or erroneous (if it does not). Then, recall, precision and F-score are calculated over these constituents. In the second measure, the constituent F-score of each of the parses in the set is computed, and then results are averaged. There are</context>
<context position="25243" citStr="Reichart and Rappoport (2007)" startWordPosition="4343" endWordPosition="4346">tion. We ran the tagger 5 times, each time with a different random initialization, and then ran PUPA with its output. The results we report for PUPA are the average over these 5 runs. Random selection results (given for reference) were also averages over 5 samples. PUPA ’s parameter estimation is completely unsupervised (see Section 2). No development data was used to tune its parameters. A 200 sentences development set from each corpus was used for calibrating the parameters of the SEPA algorithm. Based on the analysis of SEPA performance with different assignments of its parameters given by Reichart and Rappoport (2007) (see Section 3), we ran the SEPA algorithm with sample size (SEPA parameter 5) of 30% and 80%, and with 2 – 10 committee members (N)6. The optimal parameters were N = 10,5 = 80 for WSJ20, and 6We tried higher N values but observed no improvements in SEPA’s performance. 161 90 64 80 70 0 200 400 600 48 0 500 1000 1500 2000 500 600 Number of Sentences (a) WSJ, length 5 Number of Sentences (d) WSJ, length 20 85 75 62 Average F Score Average F Score Average F Score Average F Score 60 80 70 58 75 65 56 54 70 60 52 65 55 50 100 95 90 85 80 75 500 1000 1500 Number of Sentences (c) WSJ, length 15 500</context>
</contexts>
<marker>Reichart, Rappoport, 2007</marker>
<rawString>Roi Reichart and Ari Rappoport, 2007. An Ensemble Method for Selection of High Quality Parses. ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised Induction of Labeled Parse Trees by Clustering with Syntactic Features.</title>
<date>2008</date>
<journal>COLING</journal>
<volume>08</volume>
<marker>Reichart, Rappoport, 2008</marker>
<rawString>Roi Reichart and Ari Rappoport, 2008. Unsupervised Induction of Labeled Parse Trees by Clustering with Syntactic Features. COLING ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Rosenfeld</author>
<author>Ronen Feldman</author>
</authors>
<title>Using Corpus Statistics on Entities to Improve Semi– Supervised Relation Extraction From The WEB.</title>
<date>2007</date>
<journal>ACL</journal>
<volume>07</volume>
<contexts>
<context position="20861" citStr="Rosenfeld and Feldman, 2007" startWordPosition="3582" endWordPosition="3586">e). The first system achieved the highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield t t # of constituents appearing at least t times 15000 10000 5000 0 0 50 100 P = 5 P = 50 4000 8000 7000 6000 5000 3000 2000 1000 0 0 50 100 P = 5 P = 50 160 and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of </context>
</contexts>
<marker>Rosenfeld, Feldman, 2007</marker>
<rawString>Benjamin Rosenfeld and Ronen Feldman, 2007. Using Corpus Statistics on Entities to Improve Semi– Supervised Relation Extraction From The WEB. ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Junichi Tsujii</author>
</authors>
<date>2007</date>
<booktitle>Dependency Parsing and Domain Adaptation with LR Models and Parser Ensemble. EMNLP-CoNLL ’07.</booktitle>
<contexts>
<context position="4077" citStr="Sagae and Tsujii, 2007" startWordPosition="644" endWordPosition="647">tion for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1. The constituents in the batch are represented using the POS sequences of their yield and of the yields of neighboring constituents. Constituents whose representation is frequent in the output of the parser are considered to be of a high quality. A scor</context>
<context position="19940" citStr="Sagae and Tsujii (2007)" startWordPosition="3438" endWordPosition="3441">es of size S are sampled from the training data and used for training the parsing algorithm A. In that way N committee members are created. Then, each of the test sentences is parsed by each of the N committee members and an agreement score ranging from 0 to 100 between the committee members is calculated. All unsupervised parsers mentioned above (including the Seginer parser), have a training phase where parameter values are estimated from unlabeled data. SEPA can thus be applied to the unsupervised case. Automatic selection of high quality parses has been shown to improve parser adaptation. Sagae and Tsujii (2007) and Kawahara and Uchimoto (2008) applied a self-training protocol to a parser adaptation scenario but used only high quality parses to retrain the parser. In the first work, high quality parses were selected using an ensemble method, while in the second a binary classifier was used (see above). The first system achieved the highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate,</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Junichi Tsujii, 2007. Dependency Parsing and Domain Adaptation with LR Models and Parser Ensemble. EMNLP-CoNLL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Fast Unsupervised Incremental Parsing.</title>
<date>2007</date>
<journal>ACL</journal>
<volume>07</volume>
<contexts>
<context position="1650" citStr="Seginer, 2007" startWordPosition="250" endWordPosition="251">German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for man</context>
<context position="2874" citStr="Seginer (2007)" startWordPosition="457" endWordPosition="458">tion. The high cost of manual annotation of training data for supervised parsers imposes a significant burden on their usage. A possible answer to this problem can be provided by high quality parses produced by unsupervised parsers that require little to no manual efforts for their training. These parses can be used either as input for applications, or as training material for modern supervised parsers whose output will in turn be used by applications. Although unsupervised parser results improve, the quality of many of the parses they produce is still too low for such goals. For example, the Seginer (2007) parser achieves an F-score of 75.9% on the WSJ10 corpus and 59% on the NEGRA10 corpus, but the percentage of individual sentences with an F-score of 100% is 21.5% for WSJ10 and 11% for NEGRA10. When requirements are relaxed, only asking for an F-score higher than 85%, percentage is still low, 42% for WSJ10 and 15% for NEGRA10. In this paper we address the task of a fully unsupervised assessment of high quality parses cre156 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, Boulder, Colorado, June 2009. c�2009 Association for Computatio</context>
<context position="5505" citStr="Seginer (2007)" startWordPosition="888" endWordPosition="889">alculated for each constituent length. The score thus integrates the quality of short and long constituents into one score reflecting the quality of the whole parse tree. PUPA provides a quality score for every sentence in a parsed sentences set. An NLP application can then decide if to use a parse or not, according to its own definition of a high quality parse. For example, it can select every sentence whose score is above some threshold, or the k top scored sentences. The selection strategy is application dependent and is beyond the scope of this paper. The unsupervised parser we use is the Seginer (2007) incremental parser2, which achieves state-of1The algorithm can be used with supervised POS taggers and parsers, but we focus here on the fully unsupervised scenario, which is novel and more useful. For completeness of analysis, we experimented with PUPA using a supervised POS tagger (see Section 5). Using PUPA with supervised parsers is left for future work. 2www.seggu.net/ccl. the-art results without using manually created POS tags. The POS tags we use are induced by the unsupervised tagger of (Clark, 2003)3. Since both tagger and parser do not require any manual annotation, PUPA identifies </context>
<context position="17209" citStr="Seginer (2007)" startWordPosition="2987" endWordPosition="2988">English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based mod(3) Pupa5core(5) = 1 Score(N(W)) Y �W=Y W 159 Figure 2: Number of constituents appearing at least t times (nc(t)) as a function of t. Shown are WSJ (left) and NEGRA (right), where constituents are represented according to PUPA’s PCR with 5 POS tags (P = 5, solid line) or 50 POS tags (P = 50, dashed line). els (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as input in all of these algorithms except Seginer’s, which uses plain text. Quality assessment of a learning algorithm’s output and selection of high quality instances have been addressed for supervised algorithms (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised constituency parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). For dependency parsing in a corpus adaptation scenario, (Kawahara and Uchimoto, 2008) built a binary classifier that cla</context>
<context position="24309" citStr="Seginer, 2007" startWordPosition="4186" endWordPosition="4187">xample, if the selected set is used for training supervised parsers such as the Collins parser (Collins, 1999), which collects constituent statistics, the constituent F-score of the selected set is the important measure. In applications such as the syntax based machine translation model of (Yamada and Knight, 2001), a low quality tree might lead to errorenous translation of the sentence. For such applications the average F-score is more indicative. These measures thus represent complementary aspects of a set quality and we consider both of them. The parser we use is the incremental parser of (Seginer, 2007), POS tags are induced using the unsupervised POS tagger of ((Clark, 2003), neyessenmorph model). In each experiment, the tagger was trained with the raw sentences of the experiment corpus, and then the corpus words were POS tagged. The output of the unsupervised POS tagger depends on a random initialization. We ran the tagger 5 times, each time with a different random initialization, and then ran PUPA with its output. The results we report for PUPA are the average over these 5 runs. Random selection results (given for reference) were also averages over 5 samples. PUPA ’s parameter estimation </context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer, 2007. Fast Unsupervised Incremental Parsing. ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Srikumar</author>
<author>Roi Reichart</author>
<author>Mark Sammons</author>
<author>Ari Rappoport</author>
<author>Dan Roth</author>
</authors>
<title>Extraction of Entailed Semantic Relations Through Syntax-based Comma Resolution.</title>
<date>2008</date>
<journal>ACL</journal>
<volume>08</volume>
<contexts>
<context position="1831" citStr="Srikumar et al., 2008" startWordPosition="280" endWordPosition="283">equently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for manual annotation. The high cost of manual annotation of training data for supervised parsers imposes a significant burden on their usage. A possible answer to this problem can be prov</context>
</contexts>
<marker>Srikumar, Reichart, Sammons, Rappoport, Roth, 2008</marker>
<rawString>Vivek Srikumar, Roi Reichart, Mark Sammons, Ari Rappoport and Dan Roth, 2008. Extraction of Entailed Semantic Relations Through Syntax-based Comma Resolution. ACL ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Annealing Structural Bias in Multilingual Weighted Grammar Induction.</title>
<date>2006</date>
<journal>ACL</journal>
<volume>06</volume>
<contexts>
<context position="1634" citStr="Smith and Eisner, 2006" startWordPosition="246" endWordPosition="249"> from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representativ</context>
<context position="17163" citStr="Smith and Eisner, 2006" startWordPosition="2978" endWordPosition="2981">utperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based mod(3) Pupa5core(5) = 1 Score(N(W)) Y �W=Y W 159 Figure 2: Number of constituents appearing at least t times (nc(t)) as a function of t. Shown are WSJ (left) and NEGRA (right), where constituents are represented according to PUPA’s PCR with 5 POS tags (P = 5, solid line) or 50 POS tags (P = 50, dashed line). els (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as input in all of these algorithms except Seginer’s, which uses plain text. Quality assessment of a learning algorithm’s output and selection of high quality instances have been addressed for supervised algorithms (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised constituency parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). For dependency parsing in a corpus adaptation scenario, (Kawahara and Uchi</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>Noah A. Smith and Jason Eisner, 2006. Annealing Structural Bias in Multilingual Weighted Grammar Induction. ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>WordLevel Confidence Estimation for Machine Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="20701" citStr="Ueffing and Ney, 2007" startWordPosition="3559" endWordPosition="3562">rain the parser. In the first work, high quality parses were selected using an ensemble method, while in the second a binary classifier was used (see above). The first system achieved the highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield t t # of constituents appearing at least t times 15000 10000 5000 0 0 50 100 P = 5 P = 50 4000 8000 7000 6000 5000 3000 2000 1000 0 0 50 100 P = 5 P = 50 160 and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as </context>
</contexts>
<marker>Ueffing, Ney, 2007</marker>
<rawString>Nicola Ueffing and Hermann Ney, 2007. WordLevel Confidence Estimation for Machine Translation. Computational Linguistics, 33(1):9–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A Syntax-Based Statistical Translation Model.</title>
<date>2001</date>
<journal>ACL</journal>
<volume>01</volume>
<contexts>
<context position="24011" citStr="Yamada and Knight, 2001" startWordPosition="4134" endWordPosition="4137">on and F-score are calculated over these constituents. In the second measure, the constituent F-score of each of the parses in the set is computed, and then results are averaged. There are applications that use individual constituents from the output of a parser while others need the whole parse tree. For example, if the selected set is used for training supervised parsers such as the Collins parser (Collins, 1999), which collects constituent statistics, the constituent F-score of the selected set is the important measure. In applications such as the syntax based machine translation model of (Yamada and Knight, 2001), a low quality tree might lead to errorenous translation of the sentence. For such applications the average F-score is more indicative. These measures thus represent complementary aspects of a set quality and we consider both of them. The parser we use is the incremental parser of (Seginer, 2007), POS tags are induced using the unsupervised POS tagger of ((Clark, 2003), neyessenmorph model). In each experiment, the tagger was trained with the raw sentences of the experiment corpus, and then the corpus words were POS tagged. The output of the unsupervised POS tagger depends on a random initial</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight, 2001. A Syntax-Based Statistical Translation Model. ACL ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
</authors>
<title>Detecting Parser Errors Using Webbased Semantic Filters.</title>
<date>2006</date>
<journal>EMNLP</journal>
<volume>06</volume>
<contexts>
<context position="3881" citStr="Yates et al., 2006" startWordPosition="613" endWordPosition="616">ssessment of high quality parses cre156 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1. The constituents in the batch are represented using the </context>
<context position="6546" citStr="Yates et al. (2006)" startWordPosition="1055" endWordPosition="1058">reated POS tags. The POS tags we use are induced by the unsupervised tagger of (Clark, 2003)3. Since both tagger and parser do not require any manual annotation, PUPA identifies high quality parses without any human involvement. The incremental parser of (Seginer, 2007) does not give any prediction of its output quality, and extracting such a prediction from its internal data structures is not straightforward. Such a prediction can be given by supervised parsers in terms of the parse likelihood, but this was shown to be of medium quality (Reichart and Rappoport, 2007). While the algorithms of Yates et al. (2006), Kawahara and Uchimoto (2008) and Ravi et al. (2008) are supervised (Section 3), the ensemble based SEPA algorithm (Reichart and Rappoport, 2007) can be applied to unsupervised parsers in a way that preserves the unsupervised nature of the selection task. To compare between two algorithms, we use each of them to assess the quality of the sentences in English and German corpora (WSJ and NEGRA)4. We show that for every sentence length (up to 20) the quality of the top scored k sentences according to PUPA is higher than the quality of SEPA’s list (for every k). As in (Reichart and Rappoport, 200</context>
<context position="7982" citStr="Yates et al., 2006" startWordPosition="1288" endWordPosition="1291">and Sections 4 and 5 present the evaluation setup and results. 2 The POS-based Unsupervised Parse Assessment (PUPA) Algorithm In this section we detail our parse assessment algorithm. Its input consists of a set I of parsed sentences, which in our evaluation scenario are produced by an unsupervised parser. The algorithm assigns each parsed sentence a score reflecting its quality. 3www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html, the neyessenmorph model. 4This is in contrast to algorithms for selection from the results of supervised constituency parsers, which were evaluated only for English (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). 5This is the traditional parsing F-score. 157 The algorithm has three steps. First, the words in I are POS tagged (in our case, using the fully unsupervised POS induction algorithm of Clark (2003)). Second, POS statistics about the constituents in I are collected. Finally, a quality score is calculated for each parsed sentence in I using the POS statistics. In the following we detail the last two steps. Collecting POS statistics. In its second step, the algorithm collects statistics about the constituents in the input set I. Recall that the y</context>
<context position="17637" citStr="Yates et al., 2006" startWordPosition="3053" endWordPosition="3056">line). els (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. To obtain good results, manually created POS tags are used as input in all of these algorithms except Seginer’s, which uses plain text. Quality assessment of a learning algorithm’s output and selection of high quality instances have been addressed for supervised algorithms (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised constituency parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008). For dependency parsing in a corpus adaptation scenario, (Kawahara and Uchimoto, 2008) built a binary classifier that classifies each parse in the parser’s output as reliable or not. To do that, they selected 2500 sentences from the parser’s output, compared them to their manually created gold standard, and used accurate (inaccurate) parses as positive (negative) examples for the classifier. Their approach is supervised and the features used by the classifier are dependency motivated. As far as we know, the present paper is the first to addres</context>
</contexts>
<marker>Yates, Schoenmackers, Etzioni, 2006</marker>
<rawString>Alexander Yates, Stefan Schoenmackers and Oren Etzioni, 2006. Detecting Parser Errors Using Webbased Semantic Filters. EMNLP ’06.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>