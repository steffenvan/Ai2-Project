<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005849">
<title confidence="0.9960965">
Easy as ABC? Facilitating Pictorial Communication
via Semantically Enhanced Layout
</title>
<author confidence="0.997901">
Andrew B. Goldberg, Xiaojin Zhu, Charles R. Dyer, Mohamed Eldawy, Lijie Heng
</author>
<affiliation confidence="0.999248">
Department of Computer Sciences
University of Wisconsin, Madison, WI 53706, USA
</affiliation>
<email confidence="0.983163">
{goldberg, jerryzhu, dyer, eldawy, ljheng}@cs.wisc.edu
</email>
<sectionHeader confidence="0.995443" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99954">
Pictorial communication systems convert
natural language text into pictures to as-
sist people with limited literacy. We define
a novel and challenging problem: picture
layout optimization. Given an input sen-
tence, we seek the optimal way to lay out
word icons such that the resulting picture
best conveys the meaning of the input sen-
tence. To this end, we propose a family
of intuitive “ABC” layouts, which organize
icons in three groups. We formalize layout
optimization as a sequence labeling prob-
lem, employing conditional random fields
as our machine learning method. Enabled
by novel applications of semantic role la-
beling and syntactic parsing, our trained
model makes layout predictions that agree
well with human annotators. In addition,
we conduct a user study to compare our
ABC layout versus the standard linear lay-
out. The study shows that our semantically
enhanced layout is preferred by non-native
speakers, suggesting it has the potential to
be useful for people with other forms of
limited literacy, too.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999575833333333">
A picture is worth a thousand words—especially
when you are someone with communicative dis-
orders, a foreign language speaker, or a young
child. Pictorial communication systems aim to au-
tomatically convert general natural language text
into meaningful pictures. A perfect pictorial
</bodyText>
<note confidence="0.723227">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967309">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999746285714286">
communication system can turn signs and opera-
tion instructions into easy-to-understand graphical
forms; combined with optical character recogni-
tion input, a personal assistant device could create
such visual translations on-the-fly without the help
of a caretaker. Pictorial communication may also
facilitate literacy development and rapid browsing
of documents through pictorial summaries.
Pictorial communication research is in its in-
fancy with a spectrum of experimental systems,
which we review in Section 2. At one end of
the spectrum, some systems render highly realis-
tic 3D scenes but require specific scene-descriptive
language. At the other end, some systems per-
form dictionary-based iconic transliteration (turn-
ing words into icons1 one by one) on arbitrary text
but the pictures can be hard to understand. We are
interested in using pictorial communication as an
assistive communication tool. Thus, our system
needs to be able to handle general text yet produce
easy-to-understand pictures, which is in the middle
of the spectrum. To this end, our system adopts
a “collage” approach (Zhu et al., 2007). Given a
piece of text (e.g., a sentence), it first identifies im-
portant and easy-to-depict words (or phrases) with
natural language processing (NLP) techniques. It
then finds one good icon per word, either from a
manually created picture-dictionary, or via image
analysis on image search results. Finally, it lays
out the icons to create the picture. Each step in-
volves several interesting research problems.
This paper focuses exclusively on the picture
layout component and addresses the following
question: Can we use machine learning and NLP
techniques to learn a good picture layout that im-
</bodyText>
<footnote confidence="0.998852666666667">
1In this paper, an icon refers to a small thumbnail image
corresponding to a word or phrase. A picture refers to the
overall large image corresponding to the whole text.
</footnote>
<page confidence="0.952427">
119
</page>
<note confidence="0.8814645">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 119–126
Manchester, August 2008
</note>
<bodyText confidence="0.999335416666667">
proves picture comprehension for our target audi-
ences of limited literacy? We first propose a sim-
ple yet novel picture layout scheme called “ABC.”
Next, we design a Conditional Random Field-
based semantic tagger for predicting the ABC lay-
out. Finally, we conduct a user study contrasting
our ABC layout to the linear layout used in iconic
transliteration. The main contribution of this paper
is to introduce the novel task of layout prediction,
learned using linguistic features including Prop-
Bank role labels, part-of-speech tags, and lexical
features.
</bodyText>
<sectionHeader confidence="0.945169" genericHeader="method">
2 Prior Pictorial Communication Work
</sectionHeader>
<bodyText confidence="0.999970897959184">
At one extreme, there has been significant prior
work on “text-to-scene” type systems, which were
often intended to aid graphic designers in placing
objects in a 3D environment. Example systems in-
clude NALIG (Adorni et al., 1983), SPRINT (Ya-
mada et al., 1992), Put (Clay and Wilhelms,
1996), and others (Brown and Chandrasekaran,
1981). Perhaps the best known system of this type,
WordsEye (Coyne and Sproat, 2001), uses a large
manually tagged collection of 3D polyhedral mod-
els to create photo-realistic scenes. Similarly, Car-
Sim (Johansson et al., 2005) can create animated
scenes, but operates exclusively in the limited do-
main of reconstructing road accidents from traffic
reports. These systems cater to detailed descriptive
text with visual and spatial elements. They are not
intended as assistive tools to communicate general
text, which is our goal.
Several systems (Zhu et al., 2007; Mihalcea and
Leong, 2006; Joshi et al., 2006) attempt to bal-
ance language coverage versus picture sophistica-
tion. They perform some form of keyword selec-
tion, and select corresponding icons automatically
from a 2D image database. The result is a pictorial
summary representing the main idea of the origi-
nal text, but precisely determining the original text
by looking at the picture can be difficult.
At the other extreme, augmentative and alterna-
tive communication software allows users to in-
put arbitrary text. The words, and sometimes
common phrases, are semi-automatically translit-
erated into icons, and displayed in sequential or-
der. Users must learn special icons, which corre-
spond to function words, before the resulting pic-
tures can be fully understood. Examples include
SymWriter (Widgit Software, 2007) and Blissym-
bols (Hehner, 1980).
Other than explicit scene-descriptive languages,
pictorial communication systems have not suffi-
ciently addressed the issue of picture layout for
general text. We believe a good layout can better
communicate the text a picture is trying to convey.
The present work studies the use of a semantically
inspired layout to enhance pictorial communica-
tion. For simplicity, we restrict our attention to the
layout of a single sentence. We anticipate the use
of text simplification (Chandrasekar et al., 1996;
Vickrey and Koller, 2008) to convert complex text
into a set of appropriate inputs for our system.
</bodyText>
<sectionHeader confidence="0.996239" genericHeader="method">
3 The ABC Layout
</sectionHeader>
<bodyText confidence="0.998059428571428">
A good picture layout scheme must be intuitive to
humans and easy to generate by computers. To
design such a layout, we conducted a pilot study.
Five human annotators produced free-hand pic-
tures of many sentences. Analyzing these pictures,
we found a large amount of agreement in the use
of arrows to mark actions and to provide structure
to what would otherwise be a jumble of icons.
Motivated by the pilot study, we propose a sim-
ple layout scheme called ABC. It features three
positions, referred to as A, B, and C. In addition,
an arrow points from A through B to C (Figure 1).
These positions are meant to denote certain seman-
tic roles: roughly speaking, A denotes “who,” B
denotes “what action,” and C denotes “to whom,
for what.” Each position can contain any number
of icons, each representing a word or phrase in the
text. Words that do not play a significant role in
the text will be omitted from the ABC layout.
There are two main advantages of the ABC lay-
out:
</bodyText>
<listItem confidence="0.971899714285714">
1. The ABC positioning of icons allows users to
infer the semantic role of the corresponding con-
cepts. In particular, we found that verbs can be dif-
ficult to depict and understand without such hints.
The B position serves as an action indicator to dis-
ambiguate between multiple senses of the same
icon. For example, in Figure 1, the school bus icon
clearly represents the verb phrase “rides the bus,”
rather than just the noun “bus.”
2. Such a layout is particularly amenable to ma-
chine learning. Specifically, we can turn the prob-
lem of finding the optimal layout for an input sen-
tence into a sequence tagging problem, which is
well-studied in NLP.
</listItem>
<page confidence="0.988074">
120
</page>
<bodyText confidence="0.6883115">
The girl rides the bus to school in the morning
O A B B B O C O O B
</bodyText>
<figureCaption confidence="0.9799075">
Figure 1: Example ABC picture layout, original
text, and tag sequence.
</figureCaption>
<subsectionHeader confidence="0.997744">
3.1 ABC Layout as Sequence Tagging
</subsectionHeader>
<bodyText confidence="0.9999565">
Given an input sentence, one can assign each word
a tag from the set {A, B, C, O}. The bottom row in
Figure 1 shows an example tag sequence. The tag
specifies the ABC layout position of the icon cor-
responding to that word. Tag O means “other” and
marks words not included in the picture. Within
each position, icons appear in the word order in the
input sentence. Therefore, a tag sequence uniquely
determines an ABC layout of the picture.
Finding the optimal ABC layout of the input
sentence is thus equivalent to computing the most
likely tag sequence given the input sentence. We
adopt a machine learning approach by training a
sequence tagger for this task. To do so, we need
to collect labeled training data in the form of sen-
tences with manually annotated tag sequences. We
discuss our annotation effort next, and present our
machine learning models in Section 4.
</bodyText>
<subsectionHeader confidence="0.999641">
3.2 Human Annotated Training Data
</subsectionHeader>
<bodyText confidence="0.998608522727273">
We asked the five annotators to manually label 571
sentences compiled from several online sources,
including grade school texts about history and sci-
ence, children’s books, and recent news headlines.
Some sentences were written by the annotators and
describe daily activities. The annotators tagged
each sentence using a Web-based tool to drag-and-
drop icons into the desired positions in the layout2.
To gauge the quality of the manually labeled
data, and to understand the difficulty of the ABC
2The manual tagging actually employs a more detailed tag
set to denote phrase structure: Each A, B, or C tag is com-
bined with a modifier of b (begin phrase) or i (inside phrase).
For example, the phrase “rides the bus” in Figure 1 is tagged
with Bb Bi Bi, and shares one icon. The icons were also
manually selected by the annotator from a list of Web image
search results.
layout, we computed inter annotator agreement
among three of the five annotators on a common
set of 48 sentences. Considering all pair-wise com-
parisons of the three annotators, the overall aver-
age tag agreement was 77%. This measures the to-
tal number of matching tags (across all sentences)
divided by the total number of tags. Matching
strictly requires both the correct tag and the correct
modifier. We also computed Fleiss’ kappa, which
measures the degree of inter-annotator agreement
beyond the amount expected by chance (Fleiss,
1971). The values range from 0 to 1, with 1 indi-
cating perfect agreement. The kappa statistic was
0.71, which is often considered moderate to high
agreement.
Further inspection revealed that most disagree-
ment was due to annotators reversing A and C
tags. This could arise from interpreting passive
sentences in different ways or trying to represent
physical movement. For example, some annotators
found it more natural to depict eating by placing a
food item in A and the eater in C, treating the ar-
row as the transfer of food. It was also common for
annotators to disagree on whether certain adverbs
and time modifiers belong in B or in C. These dif-
ferences all suggest the highly subjective nature of
conceptualizing pictures from text.
</bodyText>
<sectionHeader confidence="0.997297" genericHeader="method">
4 A Conditional Random Field Model for
ABC Layout Prediction
</sectionHeader>
<bodyText confidence="0.999480333333333">
We now introduce our approach to automatically
predicting the ABC layout of an input sentence.
While it was most natural for human annotators to
annotate text at the word level, early experiments
quickly revealed that predicting tags at this level is
quite challenging. Most of this stems from the fact
that human annotators tend to fragment the text
into many small segments based on the availability
of good icons. For example, the phrase “the white
pygmy elephant” may be tagged as “O A O A” be-
cause it is difficult for the annotator to find an icon
of this exact phrase or the word “pygmy,” but easy
to find icons of “white” and “elephant” separately.
Essentially, human annotation combines two tasks
in one: deciding where each phrase goes in the lay-
out, and deciding which words within a phrase can
be depicted with icons.
To rectify this situation, we make layout predic-
tions at the level of chunks (phrases); that is, we
automatically break the text into chunks, then pre-
dict one A, B, C, or O tag for each chunk. Since the
</bodyText>
<page confidence="0.990351">
121
</page>
<bodyText confidence="0.999988076923077">
tag choices made for different chunks may depend
on each other, we employ Conditional Random
Fields (CRF) (Lafferty et al., 2001), which are fre-
quently used in sequential labeling tasks like infor-
mation extraction. Our choice of chunking is de-
scribed in Section 4.1, and the CRF models and in-
put features are described in Section 4.2. The task
of deciding which words within a chunk should ap-
pear in the picture is addressed by a “word pictura-
bility” model, and is discussed in a separate paper.
For training, we automatically map the word-
level tags in our annotated data to chunk-level tags
based on the majority ABC tag within a chunk.
</bodyText>
<subsectionHeader confidence="0.997617">
4.1 Chunking by Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.999990385714286">
Ideally, we would like semantically coherent text
chunks to be represented pictorially in the same
layout position. To obtain such chunks, we lever-
age existing semantic role labeling (SRL) tech-
nology (Palmer et al., 2005; Gildea and Jurafsky,
2002). SRL is an active NLP task in which words
or phrases in a sentence are assigned a label indi-
cating the role they play with respect to a particu-
lar verb (also known as the target predicate). SRL
systems like FrameNet (Baker et al., 1998) and
PropBank (Palmer et al., 2005) aim to provide a
rich representation for applications requiring some
degree of natural language understanding, and are
thus perfectly suited for our needs. We shall fo-
cus on PropBank labels because they are easier to
use for our task. To obtain semantic role labels,
we use the automatic statistical semantic role la-
beler ASSERT (Pradhan et al., 2004), trained to
identify PropBank arguments through the use of
support vector machines and full syntactic parses.
To understand how SRL can be useful for deriv-
ing pictorial layouts, consider the sentence “The
boy gave the ball to the girl.” PropBank marks
the semantic role labels of the “arguments” of
verbs. The target verb “give” is part of the frameset
“transfer,” with core arguments “Arg0: giver” (the
boy), “Arg1: thing given” (the ball), and “Arg2:
entity given to” (the girl). Verbs can also in-
volve non-core modifier arguments, such as ArgM-
TMP (time), ArgM-LOC (location), ArgM-CAU
(cause), etc. The entities playing semantic roles
are likely to be entities we want to portray in a
picture. For PropBank, Arg0 often represents an
Agent, and Arg1 the Patient or Theme. If we could
map the different semantic role labels to ABC tags
with simple rules, then we would be done.
Unfortunately, it is not this simple, as Prop-
Bank roles are verb-specific. As Palmer et al.
pointed out, “No consistent generalizations can be
made across verbs for the higher-numbered argu-
ments” (Palmer et al., 2005). In the above exam-
ple, we might expect a layout rule of [Arg0]→A,
[Target, Arg1]→B, [Arg2]→C. However, this rule
does not generalize to other verbs, such as “drive,”
as in the sentence “The boy drives his parents
crazy,” which also has three core arguments “Arg0:
driver,” “Arg1: thing in motion,” and “Arg2: sec-
ondary predication on Arg1.” However, here the
action is figurative, and we would expect a lay-
out rule that puts Arg1 in position C: [Arg0]→A,
[Target]→B, [Arg1,Arg2]→C.
In addition, while modifier arguments have the
same meaning across verbs, their pictorial repre-
sentation may differ based on context. Consider
the sentences “Polar bears live in the Arctic.” and
“Yesterday at the zoo, the students saw a polar
bear.” In the former, a human annotator is likely
to place an icon for the ArgM-LOC “in the Arc-
tic” in position C (e.g., following a polar bear icon
in A and a house icon in B). However, the ArgM-
LOC in the second sentence, “at the zoo,” seems
more appropriately placed in position B since it de-
scribes where this particular action occurred.
Finally, the situation is further complicated
when a sentence contains multiple verbs. SRL
treats each verb in isolation, producing multiple
sets of role labels, yet our goal is to produce a sin-
gle picture. Clearly, the mapping from semantic
roles to layout positions is non-trivial. We describe
our statistical machine learning approach next.
</bodyText>
<subsectionHeader confidence="0.95439">
4.2 Our CRF Models and Features
</subsectionHeader>
<bodyText confidence="0.998580714285714">
We use a linear-chain CRF as our sequence tag-
ging model. A CRF is a discriminative model of
the conditional probability p(y|x), where y is the
sequence of layout tags in Y ={A,B,C,O}, and x
is the sequence of SRL chunks produced by the
process described in Section 4.1. Our CRF has the
general form
</bodyText>
<equation confidence="0.99868325">
1 |X |K
p(y|x) = Z(x) exp(t=1
1:1:λkfk(yt, yt−1, x, t)
k=1
</equation>
<bodyText confidence="0.7821995">
where the model parameters are {λk}. We
use binary features fk(yt, yt−1, x, t) detailed be-
low. Finally, we use an isotropic Gaussian prior
N(0, σ2I) on parameters as regularization.
</bodyText>
<page confidence="0.990838">
122
</page>
<bodyText confidence="0.9990085">
We explored three versions of the above model
by specializing the weighted feature function
λkfk(). Model 1 ignores the pairwise label poten-
tials and treats each labeling prediction indepen-
dently: λjk1{yt=j}fk(x, t), where 1{z} is an indi-
cator function on z. This is equivalent to a multi-
class logistic regression classifier. Model 2 resem-
bles a Hidden Markov Model (HMM) by factoring
pairwise label potentials and emission potentials:
λij1{yt−1=i}1{yt=j}+λjk1{yt=j}fk(x,t). Finally,
Model 3 has the most general linear-chain poten-
tial: λijk1{yt−1=i}1{yt=j}fk(x, t). Model 3 is the
most flexible, but has the most weights to learn.
We use the following binary predicate features
fk(x, t) in all our models, evaluated on each chunk
produced by the semantic role labeler:
</bodyText>
<listItem confidence="0.9493845625">
1. PropBank role label(s) of the chunk (e.g., Tar-
get, Arg0, Arg1, ArgM-LOC). A chunk can have
multiple role labels if the sentence contains multi-
ple verbs; in this case, we merge the multiple SRL
results by taking their union.
2. Part-of-speech tags of all the words in the
chunk. All syntactic parsing results are obtained
from the Stanford Parser (Klein and Manning,
2003), using the default PCFG model.
3. Phrase type (e.g., NP, VP, PP) of the deepest
syntactic parse tree node covering the entire chunk.
We also include a feature indicating whether the
phrase is nested within an ancestor VP.
4. Lexical features: individual word identities in
the top 5000 most frequent words in the Google 1T
5gram corpus (Brants and Franz, 2006). For other
</listItem>
<bodyText confidence="0.995140214285714">
words, we use their automatically predicted Word-
Net supersenses (Ciaramita and Altun, 2006). Su-
persenses are 41 broad semantic categories (e.g.,
noun.location, verb.communication). By dividing
lexical features in this way, we hope to learn spe-
cific qualities of common words, but generalize
across rarer words.
We also experimented with features derived
from typed dependency relations, but these did not
improve our models. We suspect the PropBank
role labels capture much of the same information.
In addition, the Google 5000-word list was the best
among several word lists that we explored for split-
ting up the lexical features.
</bodyText>
<subsectionHeader confidence="0.996095">
4.3 CRF Experimental Results
</subsectionHeader>
<bodyText confidence="0.999855">
We trained our CRF models using the MAL-
LET toolkit (McCallum, 2002). Our complete
dataset consists of the 571 manually annotated sen-
</bodyText>
<figure confidence="0.733178">
Variance
</figure>
<figureCaption confidence="0.806707888888889">
Figure 2: 5-fold cross validation results for dif-
ferent values of the regularization parameter (vari-
ance σ2) and three CRF models predicting A, B,
C, or O layout tags.
tences (tags mapped to chunk-level). The only
tuning parameter is the Gaussian prior variance,
σ2. We performed 5-fold cross validation, vary-
ing σ2 and comparing performance across models.
Figure 2 demonstrates that peak per-chunk accu-
</figureCaption>
<bodyText confidence="0.97893732">
racy (77.6%) and macro-averaged F1 scores are
achieved using the most general sequence labeling
model. As a result, the user study in the next sec-
tion is based on layouts predicted by Model 3 with
σ2 = 1.0, trained on all the data.
To understand which features contribute most
to performance, we experimented with removing
each of the four types (individually). Peak accu-
racy drops the most when lexical features are re-
moved (76.4%), followed by PropBank features
(76.5%), phrase features (76.9%), and POS fea-
tures (77.1%).
The features in the final learned model make in-
tuitive sense. It prefers tag transitions A→B and
B→C, but not A→C or C→A. The model likes the
word “I” and noun phrases (not nested in a verb
phrase) to have tag A. Verbs and ArgM-NEGs are
frequently tagged B, while noun.object’s, Arg4s,
and ArgM-CAUs are typically C. The model dis-
courages Arg0s and conjunctions in B, and dislikes
adverbial phrases and noun.time’s in C.
While 77.6% cross validation accuracy may
seem low, it is in fact close to the 81% inter an-
notator agreement3, and thus close to optimal. The
confusion matrix (not shown) reveals that most er-
</bodyText>
<footnote confidence="0.751499666666667">
3The 81% agreement is on mapped chunk-level tags with-
out modifiers (Fleiss’ kappa 0.74), while the 77% agreement
in Section 3.2 is on word-level tags with modifiers.
</footnote>
<figure confidence="0.996317">
0.71
10−1 100 101
0.78
0.77
0.76
0.75
0.74
Accuracy
F1
Model 1
Model 2
Model 3
0.73
0.72
Accuracy and F1
</figure>
<page confidence="0.997616">
123
</page>
<bodyText confidence="0.999986928571429">
rors probably arise from disagreements in the in-
dividual annotators. The most common errors are
predicting B for chunks labeled O and confusing
tags B and C. Manually inspecting the pictures in
our training set shows that annotators often omit-
ted the verb (such as “is” or “has”) and left the B
position empty, since it could be inferred by the
presence of the arrow and the images in A and C.
Also, annotators tended to disagree on the location
of adverbial expressions, dividing them between
positions B and C. Finally, only 3.3% of chunks
were incorrectly omitted from the pictures. There-
fore, we conclude that our CRF models are capable
of predicting the ABC layouts.
</bodyText>
<sectionHeader confidence="0.966375" genericHeader="method">
5 User Study
</sectionHeader>
<bodyText confidence="0.999979323529412">
We have proposed the ABC layout, and showed
that we can learn to predict it reasonably well. But
an important question remains: Can the proposed
ABC layout help a target audience of limited lit-
eracy understand pictures better, compared to the
linear layout used in state-of-the-art augmentative
and alternative communication software? We de-
scribe a user study as our first attempt to answer
this question. This line of work has two main chal-
lenges: one is the practical difficulty of working
with human subjects of limited literacy; the other is
the lack of a quantitative measure of picture com-
prehension.
[Subjects]: To partially overcome the first chal-
lenge, we recruited two groups of subjects with
medium and high literacy respectively, in hopes
of extrapolating our findings towards the low lit-
eracy group. Specifically, the medium group con-
sisted of seven non-native English speakers who
speak some degree of English—“medium literacy”
refers to their English fluency; twelve native En-
glish speakers comprised the high literacy group.
All subjects were adults and did not include the
authors of this paper or the five annotators. The
subjects had no prior exposure to pictorial com-
munication systems.
[Material]: We randomly chose 90 test sen-
tences from three sources4 representing our
target application domains: short narratives
written by and for individuals with commu-
nicative disorders (symbolworld.org);
one-sentence news synopses written in simple
English targeting foreign language learners
(simpleenglishnews.com); and the child
</bodyText>
<footnote confidence="0.541688">
4Distinct from the sources of the 571 training sentences.
</footnote>
<bodyText confidence="0.999908176470588">
writing sections of the LUCY corpus (Sampson,
2003). We created two pictures for each test
sentence: one using a linear layout and one
using an ABC layout. For the linear layout,
we used SymWriter. Typing text in SymWriter
automatically produces a left-to-right sequence
of icons, chosen from an icon database. In cases
where SymWriter suggests several possible icons
for a word, we manually selected the best one. For
words not in the database, we found appropriate
thumbnail images using Web image search. This
is how a typical user would use SymWriter. To
produce the ABC layout, we applied the trained
CRF tagger Model 3 to the test sentence. After
obtaining A, B, C, and O tags for text chunks, we
placed the corresponding icons (from SymWriter’s
linear layout) in the correct layout positions. Icons
for words tagged O did not appear in the ABC
version of the picture. Aside from this difference,
both pictures of each test sentence contained
exactly the same icons—the only difference was
the layout.
[Protocol]: All 19 subjects observed each of
the 90 test sentences exactly once: 45 with the
linear layout and 45 with the ABC layout. The
layouts and the order of sentences were both ran-
domized throughout the sequence, and the subjects
were counter-balanced so each sentence’s linear
and ABC layouts were viewed by roughly equal
numbers of subjects. At the start of the study,
each subject read a brief introduction describing
the task and saw an example of each layout style.
Then for each test sentence, we displayed a pic-
ture, and the subject typed a guess of the underly-
ing sentence. Finally, the subject provided a confi-
dence rating (2=“almost sure,” 1=“maybe correct,”
or 0=“no idea”). We measured response time as
the time from image display until sentence/rating
submission. Figure 3 shows a test sentence in both
layouts, together with several subjects’ guesses.
[Evaluation metrics]: As noted above, the
second main challenge is measuring picture
comprehension—we need a way to compare the
original sentences with the subjects’ guesses. In
many ways, this is like machine translation (via
pictures), so we turned to two automatic eval-
uation metrics: BLEU-1 (Papineni et al., 2002)
and METEOR (Lavie and Agarwal, 2007). BLEU-1
computes unigram precision (i.e., fraction of re-
sponse words that exactly match words in the orig-
inal), multiplied by a brevity penalty for omit-
</bodyText>
<page confidence="0.995005">
124
</page>
<bodyText confidence="0.9956605">
“we sing a song about a farm.”
“i sing about the farm and animals”
“we sang for the farmer and he gave us animals.”
“Someone went to his grandfather’s farm
and played with the animals”
“i can’t sing in the choir because i have to tend
to the animals.”
“twins sing old macdonald has a farm”
“they sang about a farm”
“they sing old mcdonald had a farm.”
“we have a farm with a sheep, a pig and a cow.”
“two people sing old mcdonald had a farm”
“we sang old mcdonald on the farm.”
“they both sing ‘old macdonald had a farm’.”
</bodyText>
<figureCaption confidence="0.8993735">
Figure 3: The linear and ABC layout pictures for the test sentence “We sang Old MacDonald had a
farm.” and some subjects’ guesses. Note the predicted ABC layout omits the ambiguous “had” icon.
</figureCaption>
<bodyText confidence="0.988526735294118">
ting words. In contrast, METEOR finds a one-to-
one word alignment between the texts that allows
partial matches (after stemming and by consider-
ing WordNet-based synonyms) and optionally ig-
nores stop words. Based on this alignment, uni-
gram precision, recall, and weighted F measure are
computed, and the final METEOR score is obtained
by scaling F to account for word-order preserva-
tion. We computed METEOR using its default pa-
rameters and the stop word list from the Snowball
project (Porter, 2001).
[Results]: We report average METEOR and BLEU
scores, confidence ratings, and response time for
the 4 conditions (native vs. non-native, ABC vs.
linear) in Table 1. The most striking observation
is that native speakers perform better (in terms of
METEOR and BLEU) with the linear layout, while
non-native speakers do better with ABC. 5
To explain this finding, it is worth noting that
SymWriter pictures include function words, whose
icons are abstract but distinct. We speculate that
even though none of our subjects were trained to
recognize these function-word icons, the native
speakers are more accustomed to the English syn-
tactic structure, so they may be able to transliter-
ate those icons back to words. In an ABC lay-
5Using a Mann-Whitney rank sum test, the difference in
native speakers’ METEOR scores is statistically significant
(p = 0.003), though the other differences are not (native
BLEU, p = 0.085; non-native METEOR, p = 0.172; non-
native BLEU, p = 0.170). Nevertheless, we observe some
evidence to support our hypothesis that non-native speak-
ers benefit from the ABC layout, and we intend to conduct
follow-up experiments to test the claim further.
</bodyText>
<table confidence="0.998979666666667">
Non-native Native
ABC Linear ABC Linear
METEOR 0.1975 0.1800 0.2955 0.3335
BLEU 0.1497 0.1456 0.2710 0.3011
Conf. 0.50 0.47 0.90 0.89
Time 47.4s 47.8s 38.1s 38.6s
</table>
<tableCaption confidence="0.99985">
Table 1: User study results.
</tableCaption>
<bodyText confidence="0.9999282">
out, the sentence order is mostly removed, and
some phrases might be omitted due to the O tag.
Thus native speakers do not get as many syntactic
hints. On the other hand, non-native speakers do
not have the same degree of built-in English syn-
tactic knowledge. As such, they do not gain much
from seeing the whole sentence sequence includ-
ing function-word icons. Instead, they may have
benefited from the ABC layout’s added organiza-
tion and potential exclusion of irrelevant icons.
If this reasoning holds, it has interesting impli-
cations for viewers who have lower English liter-
acy: they might take away more meaning from a
semantically structured layout like ABC. Verifying
this is a direction for future work.
Finally, it is interesting that all subjects feel
more confident in their responses to ABC layouts
than linear layouts, and, despite their added com-
plexity, ABC layouts do not require more response
time than linear layouts.
</bodyText>
<page confidence="0.998164">
125
</page>
<sectionHeader confidence="0.999645" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999994909090909">
We proposed a semantically enhanced picture lay-
out for pictorial communication. We formulated
our ABC layout prediction problem as sequence
tagging, and trained CRF models with linguistic
features including semantic role labels. A user
study indicated that our ABC layout has the poten-
tial to facilitate picture comprehension for people
with limited literacy. Future work includes incor-
porating ABC layouts into our pictorial communi-
cation system, improving other components, and
verifying our findings with additional user studies.
</bodyText>
<sectionHeader confidence="0.998282" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9961335">
This work is supported by NSF IIS-0711887, and
by the Wisconsin Alumni Research Foundation.
</bodyText>
<sectionHeader confidence="0.999304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836175675676">
Adorni, G., M. Di Manzo, and G. Ferrari. 1983. Natu-
ral language input for scene generation. In ACL.
Baker, C. F., C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. In COLING.
Brants, T. and A. Franz. 2006. Web 1T 5-gram version
1.1. Linguistic Data Consortium, Philadelphia.
Brown, D. C. and B. Chandrasekaran. 1981. Design
considerations for picture production in a natural lan-
guage graphics system. SIGGRAPH, 15(2).
Chandrasekar, R., C. Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
COLING.
Ciaramita, M. and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In EMNLP.
Clay, S. R. and J. Wilhelms. 1996. Put: Language-
based interactive manipulation of objects. IEEE
Computer Graphics and Applications, 16(2).
Coyne, B. and R. Sproat. 2001. WordsEye: An au-
tomatic text-to-scene conversion system. In SIG-
GRAPH.
Fleiss, J. L. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 76(5).
Gildea, D. and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics, 28(3).
Hehner, B. 1980. Blissymbols for use. Blissymbolics
Communication Institute.
Johansson, R., A. Berglund, M. Danielsson, and
P. Nugues. 2005. Automatic text-to-scene conver-
sion in the traffic accident domain. In IJCAI.
Joshi, D., J. Z. Wang, and J. Li. 2006. The story pictur-
ing engine—a system for automatic text illustration.
ACM Transactions on Multimedia Computing, Com-
munications, and Applications, 2(1).
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML.
Lavie, A. and A. Agarwal. 2007. METEOR: An au-
tomatic metric for MT evaluation with high levels of
correlation with human judgments. In Second Work-
shop on Statistical Machine Translation, June.
McCallum, A. K. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Mihalcea, R. and B. Leong. 2006. Toward commu-
nicating simple sentences using pictorial representa-
tions. In Association of Machine Translation in the
Americas.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
Porter, M. F. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/.
Pradhan, S., W. Ward, K. Hacioglu, J. Martin, and
D. Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In HLT/NAACL.
Sampson, G. 2003. The structure of children’s writ-
ing: Moving from spoken to adult written norms. In
Granger, S. and S. Petch-Tyson, editors, Extending
the Scope of Corpus-Based Research. Rodopi.
Vickrey, D. and D. Koller. 2008. Sentence simplifica-
tion for semantic role labeling. In ACL. To appear.
Widgit Software. 2007. SymWriter.
http://www.mayer-johnson.com.
Yamada, A., T. Yamamoto, H. Ikeda, T. Nishida, and
S. Doshita. 1992. Reconstructing spatial image
from natural language texts. In COLING.
Zhu, X., A. B. Goldberg, M. Eldawy, C. Dyer, and
B. Strock. 2007. A Text-to-Picture synthesis system
for augmenting communication. In AAAI.
</reference>
<page confidence="0.998526">
126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.441910">
<title confidence="0.9553715">Easy as ABC? Facilitating Pictorial via Semantically Enhanced Layout</title>
<author confidence="0.998458">Andrew B Goldberg</author>
<author confidence="0.998458">Xiaojin Zhu</author>
<author confidence="0.998458">Charles R Dyer</author>
<author confidence="0.998458">Mohamed Eldawy</author>
<author confidence="0.998458">Lijie</author>
<affiliation confidence="0.829225">Department of Computer University of Wisconsin, Madison, WI 53706,</affiliation>
<email confidence="0.840509">jerryzhu,dyer,eldawy,</email>
<abstract confidence="0.996875192307692">Pictorial communication systems convert natural language text into pictures to assist people with limited literacy. We define a novel and challenging problem: picture layout optimization. Given an input sentence, we seek the optimal way to lay out word icons such that the resulting picture best conveys the meaning of the input sentence. To this end, we propose a family of intuitive “ABC” layouts, which organize icons in three groups. We formalize layout optimization as a sequence labeling problem, employing conditional random fields as our machine learning method. Enabled by novel applications of semantic role labeling and syntactic parsing, our trained model makes layout predictions that agree well with human annotators. In addition, we conduct a user study to compare our ABC layout versus the standard linear layout. The study shows that our semantically enhanced layout is preferred by non-native speakers, suggesting it has the potential to be useful for people with other forms of limited literacy, too.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Adorni</author>
<author>M Di Manzo</author>
<author>G Ferrari</author>
</authors>
<title>Natural language input for scene generation.</title>
<date>1983</date>
<booktitle>In ACL.</booktitle>
<marker>Adorni, Di Manzo, Ferrari, 1983</marker>
<rawString>Adorni, G., M. Di Manzo, and G. Ferrari. 1983. Natural language input for scene generation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="13861" citStr="Baker et al., 1998" startWordPosition="2273" endWordPosition="2276">l tags in our annotated data to chunk-level tags based on the majority ABC tag within a chunk. 4.1 Chunking by Semantic Role Labeling Ideally, we would like semantically coherent text chunks to be represented pictorially in the same layout position. To obtain such chunks, we leverage existing semantic role labeling (SRL) technology (Palmer et al., 2005; Gildea and Jurafsky, 2002). SRL is an active NLP task in which words or phrases in a sentence are assigned a label indicating the role they play with respect to a particular verb (also known as the target predicate). SRL systems like FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) aim to provide a rich representation for applications requiring some degree of natural language understanding, and are thus perfectly suited for our needs. We shall focus on PropBank labels because they are easier to use for our task. To obtain semantic role labels, we use the automatic statistical semantic role labeler ASSERT (Pradhan et al., 2004), trained to identify PropBank arguments through the use of support vector machines and full syntactic parses. To understand how SRL can be useful for deriving pictorial layouts, consider the sentence “The boy gav</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Baker, C. F., C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley FrameNet Project. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<title>Web 1T 5-gram version 1.1. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia.</location>
<contexts>
<context position="18837" citStr="Brants and Franz, 2006" startWordPosition="3101" endWordPosition="3104">ls if the sentence contains multiple verbs; in this case, we merge the multiple SRL results by taking their union. 2. Part-of-speech tags of all the words in the chunk. All syntactic parsing results are obtained from the Stanford Parser (Klein and Manning, 2003), using the default PCFG model. 3. Phrase type (e.g., NP, VP, PP) of the deepest syntactic parse tree node covering the entire chunk. We also include a feature indicating whether the phrase is nested within an ancestor VP. 4. Lexical features: individual word identities in the top 5000 most frequent words in the Google 1T 5gram corpus (Brants and Franz, 2006). For other words, we use their automatically predicted WordNet supersenses (Ciaramita and Altun, 2006). Supersenses are 41 broad semantic categories (e.g., noun.location, verb.communication). By dividing lexical features in this way, we hope to learn specific qualities of common words, but generalize across rarer words. We also experimented with features derived from typed dependency relations, but these did not improve our models. We suspect the PropBank role labels capture much of the same information. In addition, the Google 5000-word list was the best among several word lists that we expl</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Brants, T. and A. Franz. 2006. Web 1T 5-gram version 1.1. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Brown</author>
<author>B Chandrasekaran</author>
</authors>
<title>Design considerations for picture production in a natural language graphics system.</title>
<date>1981</date>
<journal>SIGGRAPH,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="4735" citStr="Brown and Chandrasekaran, 1981" startWordPosition="714" endWordPosition="717"> layout to the linear layout used in iconic transliteration. The main contribution of this paper is to introduce the novel task of layout prediction, learned using linguistic features including PropBank role labels, part-of-speech tags, and lexical features. 2 Prior Pictorial Communication Work At one extreme, there has been significant prior work on “text-to-scene” type systems, which were often intended to aid graphic designers in placing objects in a 3D environment. Example systems include NALIG (Adorni et al., 1983), SPRINT (Yamada et al., 1992), Put (Clay and Wilhelms, 1996), and others (Brown and Chandrasekaran, 1981). Perhaps the best known system of this type, WordsEye (Coyne and Sproat, 2001), uses a large manually tagged collection of 3D polyhedral models to create photo-realistic scenes. Similarly, CarSim (Johansson et al., 2005) can create animated scenes, but operates exclusively in the limited domain of reconstructing road accidents from traffic reports. These systems cater to detailed descriptive text with visual and spatial elements. They are not intended as assistive tools to communicate general text, which is our goal. Several systems (Zhu et al., 2007; Mihalcea and Leong, 2006; Joshi et al., 2</context>
</contexts>
<marker>Brown, Chandrasekaran, 1981</marker>
<rawString>Brown, D. C. and B. Chandrasekaran. 1981. Design considerations for picture production in a natural language graphics system. SIGGRAPH, 15(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>C Doran</author>
<author>B Srinivas</author>
</authors>
<title>Motivations and methods for text simplification.</title>
<date>1996</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="6638" citStr="Chandrasekar et al., 1996" startWordPosition="1009" endWordPosition="1012">esulting pictures can be fully understood. Examples include SymWriter (Widgit Software, 2007) and Blissymbols (Hehner, 1980). Other than explicit scene-descriptive languages, pictorial communication systems have not sufficiently addressed the issue of picture layout for general text. We believe a good layout can better communicate the text a picture is trying to convey. The present work studies the use of a semantically inspired layout to enhance pictorial communication. For simplicity, we restrict our attention to the layout of a single sentence. We anticipate the use of text simplification (Chandrasekar et al., 1996; Vickrey and Koller, 2008) to convert complex text into a set of appropriate inputs for our system. 3 The ABC Layout A good picture layout scheme must be intuitive to humans and easy to generate by computers. To design such a layout, we conducted a pilot study. Five human annotators produced free-hand pictures of many sentences. Analyzing these pictures, we found a large amount of agreement in the use of arrows to mark actions and to provide structure to what would otherwise be a jumble of icons. Motivated by the pilot study, we propose a simple layout scheme called ABC. It features three pos</context>
</contexts>
<marker>Chandrasekar, Doran, Srinivas, 1996</marker>
<rawString>Chandrasekar, R., C. Doran, and B. Srinivas. 1996. Motivations and methods for text simplification. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="18940" citStr="Ciaramita and Altun, 2006" startWordPosition="3116" endWordPosition="3119">g their union. 2. Part-of-speech tags of all the words in the chunk. All syntactic parsing results are obtained from the Stanford Parser (Klein and Manning, 2003), using the default PCFG model. 3. Phrase type (e.g., NP, VP, PP) of the deepest syntactic parse tree node covering the entire chunk. We also include a feature indicating whether the phrase is nested within an ancestor VP. 4. Lexical features: individual word identities in the top 5000 most frequent words in the Google 1T 5gram corpus (Brants and Franz, 2006). For other words, we use their automatically predicted WordNet supersenses (Ciaramita and Altun, 2006). Supersenses are 41 broad semantic categories (e.g., noun.location, verb.communication). By dividing lexical features in this way, we hope to learn specific qualities of common words, but generalize across rarer words. We also experimented with features derived from typed dependency relations, but these did not improve our models. We suspect the PropBank role labels capture much of the same information. In addition, the Google 5000-word list was the best among several word lists that we explored for splitting up the lexical features. 4.3 CRF Experimental Results We trained our CRF models usin</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Ciaramita, M. and Y. Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Clay</author>
<author>J Wilhelms</author>
</authors>
<title>Put: Languagebased interactive manipulation of objects.</title>
<date>1996</date>
<journal>IEEE Computer Graphics and Applications,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="4690" citStr="Clay and Wilhelms, 1996" startWordPosition="708" endWordPosition="711">nduct a user study contrasting our ABC layout to the linear layout used in iconic transliteration. The main contribution of this paper is to introduce the novel task of layout prediction, learned using linguistic features including PropBank role labels, part-of-speech tags, and lexical features. 2 Prior Pictorial Communication Work At one extreme, there has been significant prior work on “text-to-scene” type systems, which were often intended to aid graphic designers in placing objects in a 3D environment. Example systems include NALIG (Adorni et al., 1983), SPRINT (Yamada et al., 1992), Put (Clay and Wilhelms, 1996), and others (Brown and Chandrasekaran, 1981). Perhaps the best known system of this type, WordsEye (Coyne and Sproat, 2001), uses a large manually tagged collection of 3D polyhedral models to create photo-realistic scenes. Similarly, CarSim (Johansson et al., 2005) can create animated scenes, but operates exclusively in the limited domain of reconstructing road accidents from traffic reports. These systems cater to detailed descriptive text with visual and spatial elements. They are not intended as assistive tools to communicate general text, which is our goal. Several systems (Zhu et al., 20</context>
</contexts>
<marker>Clay, Wilhelms, 1996</marker>
<rawString>Clay, S. R. and J. Wilhelms. 1996. Put: Languagebased interactive manipulation of objects. IEEE Computer Graphics and Applications, 16(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coyne</author>
<author>R Sproat</author>
</authors>
<title>WordsEye: An automatic text-to-scene conversion system.</title>
<date>2001</date>
<booktitle>In SIGGRAPH.</booktitle>
<contexts>
<context position="4814" citStr="Coyne and Sproat, 2001" startWordPosition="727" endWordPosition="730">his paper is to introduce the novel task of layout prediction, learned using linguistic features including PropBank role labels, part-of-speech tags, and lexical features. 2 Prior Pictorial Communication Work At one extreme, there has been significant prior work on “text-to-scene” type systems, which were often intended to aid graphic designers in placing objects in a 3D environment. Example systems include NALIG (Adorni et al., 1983), SPRINT (Yamada et al., 1992), Put (Clay and Wilhelms, 1996), and others (Brown and Chandrasekaran, 1981). Perhaps the best known system of this type, WordsEye (Coyne and Sproat, 2001), uses a large manually tagged collection of 3D polyhedral models to create photo-realistic scenes. Similarly, CarSim (Johansson et al., 2005) can create animated scenes, but operates exclusively in the limited domain of reconstructing road accidents from traffic reports. These systems cater to detailed descriptive text with visual and spatial elements. They are not intended as assistive tools to communicate general text, which is our goal. Several systems (Zhu et al., 2007; Mihalcea and Leong, 2006; Joshi et al., 2006) attempt to balance language coverage versus picture sophistication. They p</context>
</contexts>
<marker>Coyne, Sproat, 2001</marker>
<rawString>Coyne, B. and R. Sproat. 2001. WordsEye: An automatic text-to-scene conversion system. In SIGGRAPH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="10868" citStr="Fleiss, 1971" startWordPosition="1754" endWordPosition="1755">ually selected by the annotator from a list of Web image search results. layout, we computed inter annotator agreement among three of the five annotators on a common set of 48 sentences. Considering all pair-wise comparisons of the three annotators, the overall average tag agreement was 77%. This measures the total number of matching tags (across all sentences) divided by the total number of tags. Matching strictly requires both the correct tag and the correct modifier. We also computed Fleiss’ kappa, which measures the degree of inter-annotator agreement beyond the amount expected by chance (Fleiss, 1971). The values range from 0 to 1, with 1 indicating perfect agreement. The kappa statistic was 0.71, which is often considered moderate to high agreement. Further inspection revealed that most disagreement was due to annotators reversing A and C tags. This could arise from interpreting passive sentences in different ways or trying to represent physical movement. For example, some annotators found it more natural to depict eating by placing a food item in A and the eater in C, treating the arrow as the transfer of food. It was also common for annotators to disagree on whether certain adverbs and </context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Fleiss, J. L. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="13624" citStr="Gildea and Jurafsky, 2002" startWordPosition="2228" endWordPosition="2231">tures are described in Section 4.2. The task of deciding which words within a chunk should appear in the picture is addressed by a “word picturability” model, and is discussed in a separate paper. For training, we automatically map the wordlevel tags in our annotated data to chunk-level tags based on the majority ABC tag within a chunk. 4.1 Chunking by Semantic Role Labeling Ideally, we would like semantically coherent text chunks to be represented pictorially in the same layout position. To obtain such chunks, we leverage existing semantic role labeling (SRL) technology (Palmer et al., 2005; Gildea and Jurafsky, 2002). SRL is an active NLP task in which words or phrases in a sentence are assigned a label indicating the role they play with respect to a particular verb (also known as the target predicate). SRL systems like FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) aim to provide a rich representation for applications requiring some degree of natural language understanding, and are thus perfectly suited for our needs. We shall focus on PropBank labels because they are easier to use for our task. To obtain semantic role labels, we use the automatic statistical semantic role labeler ASSER</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D. and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hehner</author>
</authors>
<title>Blissymbols for use.</title>
<date>1980</date>
<journal>Blissymbolics Communication Institute.</journal>
<contexts>
<context position="6137" citStr="Hehner, 1980" startWordPosition="934" endWordPosition="935">The result is a pictorial summary representing the main idea of the original text, but precisely determining the original text by looking at the picture can be difficult. At the other extreme, augmentative and alternative communication software allows users to input arbitrary text. The words, and sometimes common phrases, are semi-automatically transliterated into icons, and displayed in sequential order. Users must learn special icons, which correspond to function words, before the resulting pictures can be fully understood. Examples include SymWriter (Widgit Software, 2007) and Blissymbols (Hehner, 1980). Other than explicit scene-descriptive languages, pictorial communication systems have not sufficiently addressed the issue of picture layout for general text. We believe a good layout can better communicate the text a picture is trying to convey. The present work studies the use of a semantically inspired layout to enhance pictorial communication. For simplicity, we restrict our attention to the layout of a single sentence. We anticipate the use of text simplification (Chandrasekar et al., 1996; Vickrey and Koller, 2008) to convert complex text into a set of appropriate inputs for our system</context>
</contexts>
<marker>Hehner, 1980</marker>
<rawString>Hehner, B. 1980. Blissymbols for use. Blissymbolics Communication Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>A Berglund</author>
<author>M Danielsson</author>
<author>P Nugues</author>
</authors>
<title>Automatic text-to-scene conversion in the traffic accident domain.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="4956" citStr="Johansson et al., 2005" startWordPosition="749" endWordPosition="752">h tags, and lexical features. 2 Prior Pictorial Communication Work At one extreme, there has been significant prior work on “text-to-scene” type systems, which were often intended to aid graphic designers in placing objects in a 3D environment. Example systems include NALIG (Adorni et al., 1983), SPRINT (Yamada et al., 1992), Put (Clay and Wilhelms, 1996), and others (Brown and Chandrasekaran, 1981). Perhaps the best known system of this type, WordsEye (Coyne and Sproat, 2001), uses a large manually tagged collection of 3D polyhedral models to create photo-realistic scenes. Similarly, CarSim (Johansson et al., 2005) can create animated scenes, but operates exclusively in the limited domain of reconstructing road accidents from traffic reports. These systems cater to detailed descriptive text with visual and spatial elements. They are not intended as assistive tools to communicate general text, which is our goal. Several systems (Zhu et al., 2007; Mihalcea and Leong, 2006; Joshi et al., 2006) attempt to balance language coverage versus picture sophistication. They perform some form of keyword selection, and select corresponding icons automatically from a 2D image database. The result is a pictorial summar</context>
</contexts>
<marker>Johansson, Berglund, Danielsson, Nugues, 2005</marker>
<rawString>Johansson, R., A. Berglund, M. Danielsson, and P. Nugues. 2005. Automatic text-to-scene conversion in the traffic accident domain. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Joshi</author>
<author>J Z Wang</author>
<author>J Li</author>
</authors>
<title>The story picturing engine—a system for automatic text illustration.</title>
<date>2006</date>
<journal>ACM Transactions on Multimedia Computing, Communications, and Applications,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="5339" citStr="Joshi et al., 2006" startWordPosition="809" endWordPosition="812">asekaran, 1981). Perhaps the best known system of this type, WordsEye (Coyne and Sproat, 2001), uses a large manually tagged collection of 3D polyhedral models to create photo-realistic scenes. Similarly, CarSim (Johansson et al., 2005) can create animated scenes, but operates exclusively in the limited domain of reconstructing road accidents from traffic reports. These systems cater to detailed descriptive text with visual and spatial elements. They are not intended as assistive tools to communicate general text, which is our goal. Several systems (Zhu et al., 2007; Mihalcea and Leong, 2006; Joshi et al., 2006) attempt to balance language coverage versus picture sophistication. They perform some form of keyword selection, and select corresponding icons automatically from a 2D image database. The result is a pictorial summary representing the main idea of the original text, but precisely determining the original text by looking at the picture can be difficult. At the other extreme, augmentative and alternative communication software allows users to input arbitrary text. The words, and sometimes common phrases, are semi-automatically transliterated into icons, and displayed in sequential order. Users </context>
</contexts>
<marker>Joshi, Wang, Li, 2006</marker>
<rawString>Joshi, D., J. Z. Wang, and J. Li. 2006. The story picturing engine—a system for automatic text illustration. ACM Transactions on Multimedia Computing, Communications, and Applications, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="18476" citStr="Klein and Manning, 2003" startWordPosition="3040" endWordPosition="3043">chain potential: λijk1{yt−1=i}1{yt=j}fk(x, t). Model 3 is the most flexible, but has the most weights to learn. We use the following binary predicate features fk(x, t) in all our models, evaluated on each chunk produced by the semantic role labeler: 1. PropBank role label(s) of the chunk (e.g., Target, Arg0, Arg1, ArgM-LOC). A chunk can have multiple role labels if the sentence contains multiple verbs; in this case, we merge the multiple SRL results by taking their union. 2. Part-of-speech tags of all the words in the chunk. All syntactic parsing results are obtained from the Stanford Parser (Klein and Manning, 2003), using the default PCFG model. 3. Phrase type (e.g., NP, VP, PP) of the deepest syntactic parse tree node covering the entire chunk. We also include a feature indicating whether the phrase is nested within an ancestor VP. 4. Lexical features: individual word identities in the top 5000 most frequent words in the Google 1T 5gram corpus (Brants and Franz, 2006). For other words, we use their automatically predicted WordNet supersenses (Ciaramita and Altun, 2006). Supersenses are 41 broad semantic categories (e.g., noun.location, verb.communication). By dividing lexical features in this way, we h</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. and C. D. Manning. 2003. Accurate unlexicalized parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="12828" citStr="Lafferty et al., 2001" startWordPosition="2092" endWordPosition="2095">d an icon of this exact phrase or the word “pygmy,” but easy to find icons of “white” and “elephant” separately. Essentially, human annotation combines two tasks in one: deciding where each phrase goes in the layout, and deciding which words within a phrase can be depicted with icons. To rectify this situation, we make layout predictions at the level of chunks (phrases); that is, we automatically break the text into chunks, then predict one A, B, C, or O tag for each chunk. Since the 121 tag choices made for different chunks may depend on each other, we employ Conditional Random Fields (CRF) (Lafferty et al., 2001), which are frequently used in sequential labeling tasks like information extraction. Our choice of chunking is described in Section 4.1, and the CRF models and input features are described in Section 4.2. The task of deciding which words within a chunk should appear in the picture is addressed by a “word picturability” model, and is discussed in a separate paper. For training, we automatically map the wordlevel tags in our annotated data to chunk-level tags based on the majority ABC tag within a chunk. 4.1 Chunking by Semantic Role Labeling Ideally, we would like semantically coherent text ch</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>A Agarwal</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Second Workshop on Statistical Machine Translation,</booktitle>
<contexts>
<context position="25988" citStr="Lavie and Agarwal, 2007" startWordPosition="4267" endWordPosition="4270">bject provided a confidence rating (2=“almost sure,” 1=“maybe correct,” or 0=“no idea”). We measured response time as the time from image display until sentence/rating submission. Figure 3 shows a test sentence in both layouts, together with several subjects’ guesses. [Evaluation metrics]: As noted above, the second main challenge is measuring picture comprehension—we need a way to compare the original sentences with the subjects’ guesses. In many ways, this is like machine translation (via pictures), so we turned to two automatic evaluation metrics: BLEU-1 (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). BLEU-1 computes unigram precision (i.e., fraction of response words that exactly match words in the original), multiplied by a brevity penalty for omit124 “we sing a song about a farm.” “i sing about the farm and animals” “we sang for the farmer and he gave us animals.” “Someone went to his grandfather’s farm and played with the animals” “i can’t sing in the choir because i have to tend to the animals.” “twins sing old macdonald has a farm” “they sang about a farm” “they sing old mcdonald had a farm.” “we have a farm with a sheep, a pig and a cow.” “two people sing old mcdonald had a farm” “</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Lavie, A. and A. Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Second Workshop on Statistical Machine Translation, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="19577" citStr="McCallum, 2002" startWordPosition="3218" endWordPosition="3219">road semantic categories (e.g., noun.location, verb.communication). By dividing lexical features in this way, we hope to learn specific qualities of common words, but generalize across rarer words. We also experimented with features derived from typed dependency relations, but these did not improve our models. We suspect the PropBank role labels capture much of the same information. In addition, the Google 5000-word list was the best among several word lists that we explored for splitting up the lexical features. 4.3 CRF Experimental Results We trained our CRF models using the MALLET toolkit (McCallum, 2002). Our complete dataset consists of the 571 manually annotated senVariance Figure 2: 5-fold cross validation results for different values of the regularization parameter (variance σ2) and three CRF models predicting A, B, C, or O layout tags. tences (tags mapped to chunk-level). The only tuning parameter is the Gaussian prior variance, σ2. We performed 5-fold cross validation, varying σ2 and comparing performance across models. Figure 2 demonstrates that peak per-chunk accuracy (77.6%) and macro-averaged F1 scores are achieved using the most general sequence labeling model. As a result, the use</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>McCallum, A. K. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>B Leong</author>
</authors>
<title>Toward communicating simple sentences using pictorial representations.</title>
<date>2006</date>
<booktitle>In Association of Machine Translation in the Americas.</booktitle>
<contexts>
<context position="5318" citStr="Mihalcea and Leong, 2006" startWordPosition="805" endWordPosition="808">d others (Brown and Chandrasekaran, 1981). Perhaps the best known system of this type, WordsEye (Coyne and Sproat, 2001), uses a large manually tagged collection of 3D polyhedral models to create photo-realistic scenes. Similarly, CarSim (Johansson et al., 2005) can create animated scenes, but operates exclusively in the limited domain of reconstructing road accidents from traffic reports. These systems cater to detailed descriptive text with visual and spatial elements. They are not intended as assistive tools to communicate general text, which is our goal. Several systems (Zhu et al., 2007; Mihalcea and Leong, 2006; Joshi et al., 2006) attempt to balance language coverage versus picture sophistication. They perform some form of keyword selection, and select corresponding icons automatically from a 2D image database. The result is a pictorial summary representing the main idea of the original text, but precisely determining the original text by looking at the picture can be difficult. At the other extreme, augmentative and alternative communication software allows users to input arbitrary text. The words, and sometimes common phrases, are semi-automatically transliterated into icons, and displayed in seq</context>
</contexts>
<marker>Mihalcea, Leong, 2006</marker>
<rawString>Mihalcea, R. and B. Leong. 2006. Toward communicating simple sentences using pictorial representations. In Association of Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="13596" citStr="Palmer et al., 2005" startWordPosition="2224" endWordPosition="2227"> models and input features are described in Section 4.2. The task of deciding which words within a chunk should appear in the picture is addressed by a “word picturability” model, and is discussed in a separate paper. For training, we automatically map the wordlevel tags in our annotated data to chunk-level tags based on the majority ABC tag within a chunk. 4.1 Chunking by Semantic Role Labeling Ideally, we would like semantically coherent text chunks to be represented pictorially in the same layout position. To obtain such chunks, we leverage existing semantic role labeling (SRL) technology (Palmer et al., 2005; Gildea and Jurafsky, 2002). SRL is an active NLP task in which words or phrases in a sentence are assigned a label indicating the role they play with respect to a particular verb (also known as the target predicate). SRL systems like FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) aim to provide a rich representation for applications requiring some degree of natural language understanding, and are thus perfectly suited for our needs. We shall focus on PropBank labels because they are easier to use for our task. To obtain semantic role labels, we use the automatic statistical</context>
<context position="15345" citStr="Palmer et al., 2005" startWordPosition="2519" endWordPosition="2522">irl). Verbs can also involve non-core modifier arguments, such as ArgMTMP (time), ArgM-LOC (location), ArgM-CAU (cause), etc. The entities playing semantic roles are likely to be entities we want to portray in a picture. For PropBank, Arg0 often represents an Agent, and Arg1 the Patient or Theme. If we could map the different semantic role labels to ABC tags with simple rules, then we would be done. Unfortunately, it is not this simple, as PropBank roles are verb-specific. As Palmer et al. pointed out, “No consistent generalizations can be made across verbs for the higher-numbered arguments” (Palmer et al., 2005). In the above example, we might expect a layout rule of [Arg0]→A, [Target, Arg1]→B, [Arg2]→C. However, this rule does not generalize to other verbs, such as “drive,” as in the sentence “The boy drives his parents crazy,” which also has three core arguments “Arg0: driver,” “Arg1: thing in motion,” and “Arg2: secondary predication on Arg1.” However, here the action is figurative, and we would expect a layout rule that puts Arg1 in position C: [Arg0]→A, [Target]→B, [Arg1,Arg2]→C. In addition, while modifier arguments have the same meaning across verbs, their pictorial representation may differ b</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="25951" citStr="Papineni et al., 2002" startWordPosition="4261" endWordPosition="4264">nderlying sentence. Finally, the subject provided a confidence rating (2=“almost sure,” 1=“maybe correct,” or 0=“no idea”). We measured response time as the time from image display until sentence/rating submission. Figure 3 shows a test sentence in both layouts, together with several subjects’ guesses. [Evaluation metrics]: As noted above, the second main challenge is measuring picture comprehension—we need a way to compare the original sentences with the subjects’ guesses. In many ways, this is like machine translation (via pictures), so we turned to two automatic evaluation metrics: BLEU-1 (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). BLEU-1 computes unigram precision (i.e., fraction of response words that exactly match words in the original), multiplied by a brevity penalty for omit124 “we sing a song about a farm.” “i sing about the farm and animals” “we sang for the farmer and he gave us animals.” “Someone went to his grandfather’s farm and played with the animals” “i can’t sing in the choir because i have to tend to the animals.” “twins sing old macdonald has a farm” “they sang about a farm” “they sing old mcdonald had a farm.” “we have a farm with a sheep, a pig and a cow.” “two p</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<note>http://snowball.tartarus.org/.</note>
<contexts>
<context position="27358" citStr="Porter, 2001" startWordPosition="4512" endWordPosition="4513">ld MacDonald had a farm.” and some subjects’ guesses. Note the predicted ABC layout omits the ambiguous “had” icon. ting words. In contrast, METEOR finds a one-toone word alignment between the texts that allows partial matches (after stemming and by considering WordNet-based synonyms) and optionally ignores stop words. Based on this alignment, unigram precision, recall, and weighted F measure are computed, and the final METEOR score is obtained by scaling F to account for word-order preservation. We computed METEOR using its default parameters and the stop word list from the Snowball project (Porter, 2001). [Results]: We report average METEOR and BLEU scores, confidence ratings, and response time for the 4 conditions (native vs. non-native, ABC vs. linear) in Table 1. The most striking observation is that native speakers perform better (in terms of METEOR and BLEU) with the linear layout, while non-native speakers do better with ABC. 5 To explain this finding, it is worth noting that SymWriter pictures include function words, whose icons are abstract but distinct. We speculate that even though none of our subjects were trained to recognize these function-word icons, the native speakers are more</context>
</contexts>
<marker>Porter, 2001</marker>
<rawString>Porter, M. F. 2001. Snowball: A language for stemming algorithms. http://snowball.tartarus.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>W Ward</author>
<author>K Hacioglu</author>
<author>J Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In HLT/NAACL.</booktitle>
<contexts>
<context position="14248" citStr="Pradhan et al., 2004" startWordPosition="2337" endWordPosition="2340">RL is an active NLP task in which words or phrases in a sentence are assigned a label indicating the role they play with respect to a particular verb (also known as the target predicate). SRL systems like FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) aim to provide a rich representation for applications requiring some degree of natural language understanding, and are thus perfectly suited for our needs. We shall focus on PropBank labels because they are easier to use for our task. To obtain semantic role labels, we use the automatic statistical semantic role labeler ASSERT (Pradhan et al., 2004), trained to identify PropBank arguments through the use of support vector machines and full syntactic parses. To understand how SRL can be useful for deriving pictorial layouts, consider the sentence “The boy gave the ball to the girl.” PropBank marks the semantic role labels of the “arguments” of verbs. The target verb “give” is part of the frameset “transfer,” with core arguments “Arg0: giver” (the boy), “Arg1: thing given” (the ball), and “Arg2: entity given to” (the girl). Verbs can also involve non-core modifier arguments, such as ArgMTMP (time), ArgM-LOC (location), ArgM-CAU (cause), et</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Pradhan, S., W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky. 2004. Shallow semantic parsing using support vector machines. In HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>The structure of children’s writing: Moving from spoken to adult written norms.</title>
<date>2003</date>
<booktitle>Extending the Scope of Corpus-Based Research. Rodopi.</booktitle>
<editor>In Granger, S. and S. Petch-Tyson, editors,</editor>
<contexts>
<context position="23803" citStr="Sampson, 2003" startWordPosition="3910" endWordPosition="3911">. All subjects were adults and did not include the authors of this paper or the five annotators. The subjects had no prior exposure to pictorial communication systems. [Material]: We randomly chose 90 test sentences from three sources4 representing our target application domains: short narratives written by and for individuals with communicative disorders (symbolworld.org); one-sentence news synopses written in simple English targeting foreign language learners (simpleenglishnews.com); and the child 4Distinct from the sources of the 571 training sentences. writing sections of the LUCY corpus (Sampson, 2003). We created two pictures for each test sentence: one using a linear layout and one using an ABC layout. For the linear layout, we used SymWriter. Typing text in SymWriter automatically produces a left-to-right sequence of icons, chosen from an icon database. In cases where SymWriter suggests several possible icons for a word, we manually selected the best one. For words not in the database, we found appropriate thumbnail images using Web image search. This is how a typical user would use SymWriter. To produce the ABC layout, we applied the trained CRF tagger Model 3 to the test sentence. Afte</context>
</contexts>
<marker>Sampson, 2003</marker>
<rawString>Sampson, G. 2003. The structure of children’s writing: Moving from spoken to adult written norms. In Granger, S. and S. Petch-Tyson, editors, Extending the Scope of Corpus-Based Research. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vickrey</author>
<author>D Koller</author>
</authors>
<title>Sentence simplification for semantic role labeling.</title>
<date>2008</date>
<booktitle>In ACL. To appear. Widgit Software.</booktitle>
<note>SymWriter. http://www.mayer-johnson.com.</note>
<contexts>
<context position="6665" citStr="Vickrey and Koller, 2008" startWordPosition="1013" endWordPosition="1016">lly understood. Examples include SymWriter (Widgit Software, 2007) and Blissymbols (Hehner, 1980). Other than explicit scene-descriptive languages, pictorial communication systems have not sufficiently addressed the issue of picture layout for general text. We believe a good layout can better communicate the text a picture is trying to convey. The present work studies the use of a semantically inspired layout to enhance pictorial communication. For simplicity, we restrict our attention to the layout of a single sentence. We anticipate the use of text simplification (Chandrasekar et al., 1996; Vickrey and Koller, 2008) to convert complex text into a set of appropriate inputs for our system. 3 The ABC Layout A good picture layout scheme must be intuitive to humans and easy to generate by computers. To design such a layout, we conducted a pilot study. Five human annotators produced free-hand pictures of many sentences. Analyzing these pictures, we found a large amount of agreement in the use of arrows to mark actions and to provide structure to what would otherwise be a jumble of icons. Motivated by the pilot study, we propose a simple layout scheme called ABC. It features three positions, referred to as A, B</context>
</contexts>
<marker>Vickrey, Koller, 2008</marker>
<rawString>Vickrey, D. and D. Koller. 2008. Sentence simplification for semantic role labeling. In ACL. To appear. Widgit Software. 2007. SymWriter. http://www.mayer-johnson.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yamada</author>
<author>T Yamamoto</author>
<author>H Ikeda</author>
<author>T Nishida</author>
<author>S Doshita</author>
</authors>
<title>Reconstructing spatial image from natural language texts.</title>
<date>1992</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="4659" citStr="Yamada et al., 1992" startWordPosition="702" endWordPosition="706"> ABC layout. Finally, we conduct a user study contrasting our ABC layout to the linear layout used in iconic transliteration. The main contribution of this paper is to introduce the novel task of layout prediction, learned using linguistic features including PropBank role labels, part-of-speech tags, and lexical features. 2 Prior Pictorial Communication Work At one extreme, there has been significant prior work on “text-to-scene” type systems, which were often intended to aid graphic designers in placing objects in a 3D environment. Example systems include NALIG (Adorni et al., 1983), SPRINT (Yamada et al., 1992), Put (Clay and Wilhelms, 1996), and others (Brown and Chandrasekaran, 1981). Perhaps the best known system of this type, WordsEye (Coyne and Sproat, 2001), uses a large manually tagged collection of 3D polyhedral models to create photo-realistic scenes. Similarly, CarSim (Johansson et al., 2005) can create animated scenes, but operates exclusively in the limited domain of reconstructing road accidents from traffic reports. These systems cater to detailed descriptive text with visual and spatial elements. They are not intended as assistive tools to communicate general text, which is our goal. </context>
</contexts>
<marker>Yamada, Yamamoto, Ikeda, Nishida, Doshita, 1992</marker>
<rawString>Yamada, A., T. Yamamoto, H. Ikeda, T. Nishida, and S. Doshita. 1992. Reconstructing spatial image from natural language texts. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>A B Goldberg</author>
<author>M Eldawy</author>
<author>C Dyer</author>
<author>B Strock</author>
</authors>
<title>A Text-to-Picture synthesis system for augmenting communication.</title>
<date>2007</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="2910" citStr="Zhu et al., 2007" startWordPosition="426" endWordPosition="429">iew in Section 2. At one end of the spectrum, some systems render highly realistic 3D scenes but require specific scene-descriptive language. At the other end, some systems perform dictionary-based iconic transliteration (turning words into icons1 one by one) on arbitrary text but the pictures can be hard to understand. We are interested in using pictorial communication as an assistive communication tool. Thus, our system needs to be able to handle general text yet produce easy-to-understand pictures, which is in the middle of the spectrum. To this end, our system adopts a “collage” approach (Zhu et al., 2007). Given a piece of text (e.g., a sentence), it first identifies important and easy-to-depict words (or phrases) with natural language processing (NLP) techniques. It then finds one good icon per word, either from a manually created picture-dictionary, or via image analysis on image search results. Finally, it lays out the icons to create the picture. Each step involves several interesting research problems. This paper focuses exclusively on the picture layout component and addresses the following question: Can we use machine learning and NLP techniques to learn a good picture layout that im1In</context>
<context position="5292" citStr="Zhu et al., 2007" startWordPosition="801" endWordPosition="804">ilhelms, 1996), and others (Brown and Chandrasekaran, 1981). Perhaps the best known system of this type, WordsEye (Coyne and Sproat, 2001), uses a large manually tagged collection of 3D polyhedral models to create photo-realistic scenes. Similarly, CarSim (Johansson et al., 2005) can create animated scenes, but operates exclusively in the limited domain of reconstructing road accidents from traffic reports. These systems cater to detailed descriptive text with visual and spatial elements. They are not intended as assistive tools to communicate general text, which is our goal. Several systems (Zhu et al., 2007; Mihalcea and Leong, 2006; Joshi et al., 2006) attempt to balance language coverage versus picture sophistication. They perform some form of keyword selection, and select corresponding icons automatically from a 2D image database. The result is a pictorial summary representing the main idea of the original text, but precisely determining the original text by looking at the picture can be difficult. At the other extreme, augmentative and alternative communication software allows users to input arbitrary text. The words, and sometimes common phrases, are semi-automatically transliterated into i</context>
</contexts>
<marker>Zhu, Goldberg, Eldawy, Dyer, Strock, 2007</marker>
<rawString>Zhu, X., A. B. Goldberg, M. Eldawy, C. Dyer, and B. Strock. 2007. A Text-to-Picture synthesis system for augmenting communication. In AAAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>