<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9978755">
Ranking Text Units According to Textual Saliency, Connectivity
and Topic Aptness
</title>
<author confidence="0.890265">
Antonio Sanfilippo*
</author>
<affiliation confidence="0.519685">
LINGLINK
</affiliation>
<address confidence="0.683491">
Anite Systems
13 rue Robert Stumper
L-2557 Luxembourg
</address>
<sectionHeader confidence="0.925894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970583333333">
An efficient use of lexical cohesion is described
for ranking text units according to their contri-
bution in defining the meaning of a text (textual
saliency), their ability to form a cohesive sub-
text (textual connectivity) and the extent and
effectiveness to which they address the different
topics which characterize the subject matter of
the text (topic aptness). A specific application
is also discussed where the method described is
employed to build the indexing component of a
summarization system to provide both generic
and query-based indicative summaries.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958176470588">
As information systems become a more inte-
gral part of personal computing, it appears
clear that summarization technology must be
able to address users&apos; needs effectively if it is
to meet the demands of a growing market in
the area of document management. Minimally,
the abridgement of a text according to a user&apos;s
needs involves selecting the most salient por-
tions of the text which are topically best suited
to represent the user&apos;s interests. This selec-
tion must also take into consideration the de-
gree of connectivity among the chosen text por-
tions so as to minimize the danger of produc-
ing summaries which contain poorly linked sen-
tences. In addition, the assessment of textual
saliency, connectivity and topic aptness must
be computed efficiently enough so that summa-
</bodyText>
<listItem confidence="0.721904625">
• This work was carried out within the Information
Technology Group at SHARP Laboratories of Europe,
Oxford, UK. I am indebted to Julian Asquith, Jan 13-
dens, Ian Johnson and Victor Poznariski for helpful com-
ments on previous versions of this document. Many
thanks also to Stephen Burns for intemet programming
support, Ralf Steinberger for assistance in dictionary
conversion, and Charlotte Boynton for editorial help.
</listItem>
<bodyText confidence="0.9933732">
rization can be conveniently performed on-line.
The goal of this paper is to show how these ob-
jectives can be achieved through a conceptual
indexing technique based on an efficient use of
lexical cohesion.
</bodyText>
<sectionHeader confidence="0.987744" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999942333333333">
Lexical cohesion has been widely used in text
analysis for the comparative assessment of
saliency and connectivity of text fragments.
Following Hoey (1991), a simple way of com-
puting lexical cohesion in a text is to segment
the text into units (e.g sentences) and to count
non-stop words1 which co-occur in each pair of
distinct text units, as shown in Table 2 for the
text in Table 1. Text units which contain a
greater number of shared non-stop words are
more likely to provide a better abridgement of
the original text for two reasons:
</bodyText>
<listItem confidence="0.653431285714286">
• the more often a word with high informa-
tional content occurs in a text, the more
topical and germane to the text the word
is likely to be, and
• the greater the number of times two text
units share a word, the more connected
they are likely to be.
</listItem>
<bodyText confidence="0.992640076923077">
Text saliency and connectivity for each text unit
is therefore established by summing the num-
ber of shared words associated with the text
unit. According to Hoey, the number of links
(e.g. shared words) across two text units must
be above a certain threshold for the two text
units to achieve a lexical cohesion rank. For ex-
ample, if only individual scores greater than 2
&apos;Non-stop words can be intuitively thought of as
words which have high informational content. They usu-
ally exclude words with a very high fequency of occur-
rence, especially closed class words such as determiners,
prepositions and conjunctions (Fox, 1992).
</bodyText>
<page confidence="0.992698">
1157
</page>
<tableCaption confidence="0.698031304347826">
#1# Apple Looking for a Partner
#2# NEW YORK (Reuter) - Apple is actively
looking for a friendly merger partner,
according to several executives close
to the company, the New York Times
said on Thursday.
#3# One executive who does business with
Apple said Apple employees told him
the company was again in talks with
Sun Microsystems, the paper said.
#4# On Wednesday, Saudi Arabia&apos;s Prince
Alwaleed Bin Talal Bin Abdulaziz Al
Saud said he owned more than five
percent of the computer maker&apos;s stock,
recently buying shares on the open
market for a total of $116 million.
#6# Oracle Corp Chairman Larry Ellison
confirmed on March 27 he had formed an
independent investor group to gauge
interest in taking over Apple.
#6# The company was not immediately
available to comment.
Table 1: Sample text with numbered text units
</tableCaption>
<table confidence="0.998839352941177">
Text units Words shared Score
#11t *2* Apple, look, partner 3
#1# #3# Apple, Apple 2
#1# #4# 0
#1# #5# Apple 1
*1* *6* 0
#2# #3# Apple, Apple,
executive, company 4
#2# #4# 0
#2# #5# Apple 1
*2* *6* company 1
*3* *4* 0
#3# #5# Apple, Apple 2
#3# #6# company 1
*4* *5* 0
*4* *6* 0
*5* *6* 0
</table>
<tableCaption confidence="0.9144445">
Table 2: Measuring lexical cohesion in text unit
pairs.
</tableCaption>
<bodyText confidence="0.619743">
are taken into account, the final scores and con-
sequent ranking order computable from Table 2
are:
</bodyText>
<listItem confidence="0.999978">
• first: text unit #2# (final score: 7);
• second: text unit #3# (final score: 4), and
• third: text unit #1# (final score: 3).
</listItem>
<bodyText confidence="0.992207981481482">
A text abridgement can be obtained by select-
ing text units in ranking order according to the
text percentage specified by the user. For ex-
ample, a 35% abridgement of the text in Ta-
ble 2 would result in the selection of text units
#2# and #3#.
As Hoey points out, additional techniques
can be used to refine the assessment of lexi-
cal cohesion. A typical example is the use of
thesaurus functions such as synonymy and hy-
ponymy to extend the notion of word sharing
across text units, as exemplified in Hirst and St-
C)nge (1997) and Barzilay and Elhadad (1997)
with reference to WordNet (Miller et al 1990).
Such an extension may improve on the assess-
naent of textual saliency and connectivity thus
providing better generic summaries, as argued
in Barzilay and Elhadad (1997).
There are basically two problems with the
uses of lexical cohesion for summarization re-
viewed above. First, the basic algorithm re-
quires that (0 all unique pairwise permutations
of distinct text units be processed, and (ii) all
cross-sentence word combinations be evaluated
for each such text unit pair. The complexity of
this algorithm will therefore be 0(n2 * m2) for
n text units in a text and m words in a text
unit of average length in the text at hand. This
estimate may get worse as conditions such as
synonymy and hyponymy are checked for each
word pair to extend the notion of lexical cohe-
sion, e.g. using WordNet as in Barzilay and El-
hadad (1997). Consequently, the approach may
not be suitable for on-line use with longer input
texts. Secondly, the use of thesauri envisaged
in both Hirst and St-Onge (1997) and Barzi-
lay and Elhadad (1997) does not address the
question of topical aptness. Thesaural relations
such as synonymy and hyponymy are meant to
capture word similarity in order to assess lexical
cohesion among text units, and not to provide a
thematic characterization of text units.2 Con-
sequently, it will not be possible to index and
retrieve text units in term of topic aptness ac-
cording to users&apos; needs. In the remaining part
of the paper, we will show how these concerns
of efficiency and thematic characterization can
be addressed with specific reference to a system
performing generic and query-based indicative
2Notice incidentally that such thematic characteriza-
tion could not be achieved using thesauri such as Word-
Net since since WordNet does not provide an arrange-
ment of synonym sets into classes of discourse topics (e.g.
finance, sport, health).
</bodyText>
<page confidence="0.964836">
1158
</page>
<bodyText confidence="0.925236">
summaries.
</bodyText>
<sectionHeader confidence="0.880528" genericHeader="method">
3 An Efficient Method for
Computing Lexical Cohesion
</sectionHeader>
<bodyText confidence="0.955082">
The method we are about to describe comprises
three phases:
</bodyText>
<listItem confidence="0.9759599">
• a preparatory phase where the input
text undergoes a number of normalizations
so as to facilitate the process of assessing
lexical cohesion;
• an indexing phase where the sharing of
elements indicative of lexical cohesion is as-
sessed for each text unit, and
• a ranking phase where the assessment of
lexical cohesion carried out in the indexing
phase is used to rank text units.
</listItem>
<subsectionHeader confidence="0.998439">
3.1 Preparatory Phase
</subsectionHeader>
<bodyText confidence="0.998994">
During the preparatory phase, the text under-
goes a number of normalizations which have the
purpose of facilitating the process of computing
lexical cohesion, including:
</bodyText>
<listItem confidence="0.99994075">
• removal of formatting commands
• text segmentation, i.e. breaking the input
text into text units
• part-of-speech tagging
• recognition of proper names
• recognition of multi-word expressions
• removal of stop words
• word tokenization, e.g. lemmatization.
</listItem>
<subsectionHeader confidence="0.999807">
3.2 Indexing Phase
</subsectionHeader>
<bodyText confidence="0.999973066666667">
In providing a solution for the efficiency prob-
lem, our aim is to compute lexical cohesion for
all text units in a text without having to pro-
cess all cross-sentence word combinations for all
unique and distinct pair-wise text unit permu-
tations. To achieve this objective, we index
each text unit with reference to each word oc-
curring in it and reverse-index each such word
with reference to all other text units in which
the word occurs, as shown in Table 3 for text
unit #2#. The sharing of words can then be
measured by counting all occurrences of iden-
tical text units linked to the words associated
with the &amp;quot;head&amp;quot; text unit (#2# in Table 3), as
shown in Table 4. By repeating the two opera-
</bodyText>
<table confidence="0.799407">
&lt; Apple {#1#,#3#,#3#,#5#} &gt;
&lt; company {#3#,#6#} &gt;
&lt; executive {#3#} &gt;
&lt; look {#1#} &gt;
&lt; partner {#1#} &gt;
</table>
<tableCaption confidence="0.909513">
Table 3: Text unit #2# and its words with point-
ers to the other text units in which they occur.
</tableCaption>
<equation confidence="0.596572">
#1# #3# #5# #6#
#2# 3 4 1 1
</equation>
<bodyText confidence="0.972862793103448">
Table 4: Total number of lexical cohesion links
which text unit #2# has with all other text units
tions described above for each text unit in the
text shown in Table 1, we will obtain a table of
lexical cohesion links equivalent to that shown
on Table 2.
According to this method, we are still pro-
cessing pair-wise permutations of text units to
collect lexical cohesion links as shown in Ta-
ble 4. However, there are two important differ-
ences with the original algorithm. First, non-
cohesive text units are not taken into account
(e.g. the pair #2#-#4# in the example un-
der analysis); therefore, on average the number
of text unit permutations will be significantly
smaller than that processed in the original al-
gorithm. With reference to the text in Table 1,
for example, we would be processing 7 text unit
permutations less which is over 41% of the num-
ber of text unit permutations which need com-
puting according to the original algorithm, as
shown in Table 2. Secondly, although pair-wise
text unit combinations are still processed, we
avoid doing so for all cross-sentence word per-
mutations. Consequently, the complexity of the
algorithm is 0(n2*m) for n text units in a text
and m words in a text unit of average length
in the text as compared to 0(n2 * m2) for the
original algorithm.&apos;
</bodyText>
<tableCaption confidence="0.6709255">
3A further improvement yet would be to avoid count-
ing lexical cohesion links per text unit as in Table 4,
and just sum all text unit occurrences associated with
reversed-indexed words in structures such as those in
Table 3, e.g. the lexical cohesion score for text unit
#2# would simply be 9. This would remove the need
of processing pair-wise text unit permutations for the
assessment of lexical cohesion links, thus bringing the
complexity down to 0(n * m). Such further step, how-
ever, would preempt the possibility of excluding lexical
cohesion scores for text unit pairs which are below a
given threshold.
</tableCaption>
<page confidence="0.983538">
1159
</page>
<listItem confidence="0.950507035714286">
• Let
— T RS H be the lexical cohesion threshold
— TU be the current text unit
— LCTu be the current lexical cohesion score
of TU (i.e. LCTu is the count of tokenized
words TU shares with some other text unit)
— C Level be the level of the current lexical co-
hesion score calculated as the difference be-
tween LCTu and T RS H
— Score be the lexical cohesion score previously
assigned TU (if any)
— Level be the level for the lexical cohesion
score previously assigned to TU (if any)
• — if LC&amp;quot; u = 0, then do nothing
— else, if the scoring structure
(Level, TU, Score) exists, then
* if Level &gt; C Level, then do nothing
* else, if Level = C Level, then the new
scoring structure is
(Level, TU, Score + LCTu)
* else, if C Level &gt; 0, then
• if Level &gt; 0, then the new scoring
structure is (1, TU, Score + LCTu)
• else, if Level &lt;0, then the new scor-
ing structure is (1, TU, LCTu)
* else the new scoring structure is
(C Level,TU, LCTu)
— else
</listItem>
<table confidence="0.83135625">
* if C Level &gt; 0, then create the scoring
structure (1, TU, LCTu)
* else create the scoring structure
(C Level,TU, LCTu)
</table>
<tableCaption confidence="0.908502">
Table 5: Method for ranking text units accord-
ing to lexical cohesion scores.
</tableCaption>
<subsectionHeader confidence="0.999645">
3.3 Ranking Phase
</subsectionHeader>
<bodyText confidence="0.9999614">
Each text unit is ranked with reference to the
total number of lexical cohesion scores collected,
such as those shown in Table 4. The objective
of such a ranking process is to assess the im-
port of each score and combine all scores into
a rank for each text unit. In performing this
assessment, provisions are made for a thresh-
old which specifies the minimal number of links
required for text units to be lexically cohesive,
following Hoey&apos;s approach (see §1). The proce-
dure outlined in Table 5 describes the scoring
methodology adopted. Ranking a text unit ac-
cording to this procedure involves adding the
lexical cohesion scores associated with the text
unit which are either
</bodyText>
<listItem confidence="0.9890614">
• Costant values
— T RS H = 2
— TU = #2#
• Scoring text unit #2#
— Lexical cohesion with text unit #6#
</listItem>
<table confidence="0.425723263157895">
* LCTu = 1
* C Level = —1 (i.e. LCTu — T RS H)
* no previous scoring structure
* current scoring structure: (-1, #2#, 1)
— Lexical cohesion with text unit #5#
* LCTu = 1
* C Level = —1
* previous scoring structure: (-1, #2#, 1)
* current scoring structure: (-1, #2#, 2)
— Lexical cohesion with text unit #3#
* LCTu = 4
* C Level = 2
* previous scoring structure: (-1, #2#, 2)
* current scoring structure: (0, #2#, 4)
— Lexical cohesion with text unit #1#
* LCTu = 3
* C Level = 1
* previous scoring structure: (1, #2#, 4)
* final scoring structure: (1, #2#, 7)
</table>
<tableCaption confidence="0.7277025">
Table 6: Ranking text unit #2# for lexical cohe-
sion.
</tableCaption>
<listItem confidence="0.996840333333333">
• above the threshold, or
• below the threshold and of the same mag-
nitude.
</listItem>
<bodyText confidence="0.999864333333333">
If the threshold is 0, then there is a single level
and the final score is the sum of all scores. Sup-
pose for example, we are ranking text units #2#
with reference to the scores in Table 4 with a
lexical cohesion threshold of 2. In this case we
apply the ranking procedure in Table 5 to each
score in Table 4, as shown in Table 6. Following
this procedure for all text units in Table 1, we
will obtain the ranking in Table 7.
</bodyText>
<sectionHeader confidence="0.953515" genericHeader="method">
4 Assessing Topic Aptness
</sectionHeader>
<bodyText confidence="0.999868666666667">
When used with a dictionary database provid-
ing information about the thematic domain of
words (e.g. business, politics, sport), the same
method can be slightly modified to compute lex-
ical cohesion with reference to discourse topics
rather than words. Such an application makes
</bodyText>
<page confidence="0.97487">
1160
</page>
<table confidence="0.960516428571428">
( #1*-partner
Rank Text unit Level Score
1st *2* 1 7
2nd *3* 1 4
3rd #1* 1 3
4th *5* 0 2
5th *6* —1 2
6th *4* — 0
&lt; DA {#2#-partner} &gt;
&lt; F {#2#-partner,
#3#-company,
#6#-company} &gt;
&lt; MGE {#2#-partner} &gt;
&lt; TG {#2#-partner} &gt;
</table>
<tableCaption confidence="0.8535405">
Table 7: Ranking for all text units in the text
shown on Table 1.
</tableCaption>
<table confidence="0.9933268">
WORD_POS CODE EXPLANATION
company_n F Finance &amp; Business
MI Military (the armed forces)
SCG Scouting &amp; Girl Guides
TH Theatre
partner_n DA Dance &amp; Choreography
Finance &amp; Business
MGE Marriage, Divorce,
Relationships &amp; Infidelity
TG Team Games
</table>
<tableCaption confidence="0.9730855">
Table 8: Fragment of dictionary database pro-
viding subject domain information.
</tableCaption>
<bodyText confidence="0.999283">
it possible to detect the major topics of a docu-
ment automatically and to assess how well each
text unit represents these topics.
In our implementation, we used the &amp;quot;subject
domain codes&amp;quot; provided in the machine read-
able version of CIDE (Cambridge International
Dictionary of English (Procter, 1995)). Table 8
provides an illustrative example of the infor-
mation used. Both the indexing and ranking
phases are carried out with reference to subject
domain codes rather than words.
As shown in Table 9 for text unit #1#, the in-
dexing procedure provides a record of the sub-
ject domain codes occurring in each text unit;
each such subject code is reverse-indexed with
reference to all other text units in which the
subject code occurs. In addition, a record of
which word originates which cohesion link is
kept for each text unit index. The main func-
tion of keeping track of this information is to
avoid counting lexical cohesion links generated
by overlapping domain codes which relate to the
same word — for words associated with more
than one code. Such provision is required in or-
der to avoid, or at least reduce the chances of,
counting codes which are out of context, that is
codes which relate to senses of the word other
than the intended sense. For example, the word
partner occurring in the first two text units of
the text in Table 1 is associated with four dif-
</bodyText>
<tableCaption confidence="0.768613666666667">
Table 9: Text unit #1# and its subject domain
codes with pointers to the other text units in
which they occur.
</tableCaption>
<table confidence="0.99303875">
*3* *6*
*1*-partner 1 1
F F
company company
</table>
<tableCaption confidence="0.926330666666667">
Table 10: Total number of lexical cohesion links
induced by subject domain codes for text unit
#1#.
</tableCaption>
<bodyText confidence="0.999271393939394">
ferent subject codes pertaining to the domains
of Dance (DA), Finance (F), Marriage (M) and
Team Games (TG), as shown in Table 8. How-
ever, only the Finance reading is appropriate in
the given context. If we count the cohesion links
generated by partner we would therefore count
three incorrect cohesion links. By excluding all
four cohesion links, the inclusion of contextually
inappropriate cohesion links is avoided. Need-
less to say, we will also throw away the correct
cohesion link (F in this case). However, this loss
can be redressed if we also compute lexical co-
hesion links generated from shared words across
text units as discussed in §2, and combine the
results with the lexical cohesion ranks obtained
with subject domain codes.
The lexical cohesion links for text unit #1#
will therefore be scored as shown in Table 10,
where associations between link scores and rele-
vant codes as well as the words generating them
are maintained. As can be observed, only the
appropriate code expansion F (Finance) for the
words partner and company is taken into ac-
count. This is simply because F is the only code
shared by the two words (see Table 8).
As mentioned earlier, lexical cohesion links
induced by subject domain scores can be used
to rank text units using the procedure shown in
Table 5. Other uses include providing a topic
profile of the text and an indication of how well
each text unit represents a given topic. For ex-
ample, the code BZ (Business &amp; Commerce) is
associated with the words:
</bodyText>
<page confidence="0.969312">
1161
</page>
<table confidence="0.9998855">
#2 *3* *4* #5*
#2*-executive 1 1 1
BZ BZ BZ
business market interest
#3#-executive 1 1
BZ BZ
market interest
#3*- business 1 1 1
BZ BZ BZ
execut. market interest
#4*-market 1 2 1
BZ BZ BZ
execut. execut. interest
business
#5*-interest 1 2 1
BZ BZ BZ
execut. execut. market
business
</table>
<tableCaption confidence="0.841033">
Table 11: Lexical cohesion links relating to code
BZ.
</tableCaption>
<table confidence="0.999582428571429">
CODES TEXT UNIT PAIRS
BZ 2-3 2-4 2-5 3-4 3-5 3-2 3-4 3-5
4-2 4-3 4-3 4-5 5-2 5-3 5-3 5-4
1-2 1-3 1-6 2-1 2-3 2-6 3-1 3-2 6-1 6-2
FA 2-5 5-2
IV 4-5 5-4
CN 3-4 4-3
</table>
<tableCaption confidence="0.962461">
Table 12: Subject domain codes and the text
units pairs they relate.
</tableCaption>
<listItem confidence="0.999381">
• executive occurring once in text units #2#
and #3#;
• business occurring once in text unit #3#;
• market occurring once in text unit #4#, and
• interest occurring once in text unit #5#.
</listItem>
<bodyText confidence="0.998645888888889">
After calculating the lexical cohesion links for
all text units following the method illustrated
in Tables 9-10 for text unit #lit, the links scored
for the code BZ will be as shown in Table 11. By
repeating this operation for all codes for which
there are lexical cohesion scores — F, FA, IV
and CN for the text under analysis — we could
then count all text unit pairs which each code
relates, as shown in Table 12. The relations be-
tween subject domain codes and text unit pairs
in Table 12 can subsequently be turned into per-
centage ratios to provide a topic/theme profile
of the text as shown in Table 13.
By keeping track of the links among text
units, relevant codes and their originating
words, it is also possible to retrieve text units
on the basis of specific subject domain codes
or specific words. When retrieving on specific
</bodyText>
<table confidence="0.996699166666667">
50% BZ Business &amp; Commerce
31.25% Finance &amp; Business
6.25% IV Investment Sz Stock Markets
6.25% FA Overseas Politics &amp;
International Relations
6.25% CN Communications
</table>
<tableCaption confidence="0.629652666666667">
Table 13: Topic profile of document in Table 1,
according to the distribution of subject domain
codes across text units shown in Table 12.
</tableCaption>
<bodyText confidence="0.9989658">
words, there is also the option of expanding the
word into subject domain codes and using these
to retrieve text units. The retrieved text units
can then be ordered according to the ranking
order previously computed.
</bodyText>
<sectionHeader confidence="0.997938" genericHeader="method">
5 Applications, Extensions and
Evaluation
</sectionHeader>
<bodyText confidence="0.954331272727273">
An implementation of this approach to lexical
cohesion has been used as the driving engine of
a summarization system developed at SHARP
Laboratories of Europe. The system is designed
to handle requests for both generic and query-
based indicative summaries. The level-based
differentiation of text units obtained through
the ranking procedure discussed in §3.3, is used
to select the most salient and better connected
portion of text units in a text corresponding to
the summary ratio requested by the user. In
addition, the user can display a topic profile of
the input text, as shown in Table 13 and choose
whichever code(s) s/he is interested in, specify a
summary ratio and retrieve the wanted portion
of the text which best represents the topic(s)
selected. Query-based summaries can also be
issued by entering keywords; in this case there
is the option of expanding key-words into codes
and use these to issue a summary query.
The method described can also be used to de-
velop a conceptal indexing component for infor-
mation retrieval, following Dobrov et al. (1997).
Because an attempt is made to prune contex-
tually inappropriate sense expansions of words,
the present method may help reducing the am-
biguity problem.
Possible improvements of this approach can
be implemented taking into account additional
ways of assessing lexical cohesion such as:
• the presence of synonyms or hyponyms
across text units (Hoey, 1991; Hirst and St-
Onge, 1997; Barzilay and Elhadad 1997);
</bodyText>
<page confidence="0.966505">
1162
</page>
<listItem confidence="0.993297125">
• the presence of lexical cohesion established
with reference to lexical databases offer-
ing a semantic classification of words other
than synonyms, hyponyms and subject do-
main codes;
• the presence of near-synonymous words
across text units established by using a
method for estimating the degree of seman-
tic similarity between word pairs such as
the one proposed by Resnik (1995);
• the presence of anaphoric links across text
units (Hoey, 1991; Boguraev &amp; Kennedy,
1997), and
• the presence of formatting commands as in-
dicators of the relevance of particular types
of text fragments.
</listItem>
<bodyText confidence="0.98850396">
To evaluate the utility of the approach to
lexical cohesion developed for summarization,
a testsuite was created using 41 Reuter&apos;s news
stories and related summaries (available at
http: //www.yahoo .com/headlines/news/),
by annotating each story with best summary
lines. In one evaluation experiment, summary
ratio was set at 20% and generic summaries
were obtained for the 41 texts. On average,
60% of each summary contained best summary
lines. The ranking method used in this evalu-
ation was based on combined lexical cohesion
scores based on lemmas and their associated
subject domain codes given in CIDE. Summary
results obtained with the Autosummarize
facility in Microsoft Word 97 were used as
baseline for comparison. On average, only
30% of each summary in Word 97 contained
best summary lines. In future work, we hope
to corroborate these results and to extend
their validity with reference to query-based
indicative summaries using the evaluation
framework set within the context of SUMMAC
(Automatic Text Summarization Conference,
see http: //www . tipster .org/).
</bodyText>
<sectionHeader confidence="0.998479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999519024390244">
Barzilay, R. and M. Elhadad (1997) Using
Lexical Chains for Text Summarization.
In I. Mani and M. Maybury (eds) Intel-
ligent Scalable Text Summarization, Pro-
ceedings of a Workshop Sponsored by the
Association for Computational Linguistics,
Madrid, Spain.
Boguraev, B. &amp; C. Kennedy (1997) Salience-
based Content Characterization of Text
Documents. In I. Mani and M. Maybury
(eds) Intelligent Scalable Text Summariza-
tion, Prooceedings of a Workshop Spon-
sored by the Association for Computational
Linguistics, Madrid, Spain.
Dobrov, B., N. Loukachevitch and T. Yud-
ina (1997) Conceptual Indexing Using The-
matic Representation of Texts. In The 6th
Text Retrieval Conference (TREC-6).
Fox, C. (1992) Lexical Analysis and Stoplists.
In Frakes W and Baeza-Yates R (eds) Infor-
mation Retrieval: Data Structures &amp; Algo-
rithms. Prentice Hall, Upper Saddle River,
NJ, USA, pp. 102-130.
Hirst, G. and D. St-Onge (1997) Lexical
Chains as Representation of context for the
detection and correction of malapropism.
In C. Fellbaum (ed) WordNet: An elec-
tronic lexical database and some of its ap-
plications. MIT Press, Cambridge, Mass.
Hoey, M. (1991) Patterns of Lexis in Text.
OUP, Oford, UK.
Miller, G., Beckwith, R., C. Fellbaum, D.
Gross and K. Miller (1990) Introduc-
tion to WordNet: An on-line lexical
database. International Journal of Lexi-
cography, 3(4):235-312.
Procter, P. (1995) Cambridge International
Dictionary of English, CUP, London.
Philip Resnik (1995) Using information con-
tent to evaluate semantic similarity in a
taxonomy. In IJCAI-95.
</reference>
<page confidence="0.937638">
1163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.832661">
<title confidence="0.9972685">Ranking Text Units According to Textual Saliency, Connectivity and Topic Aptness</title>
<author confidence="0.999926">Antonio Sanfilippo</author>
<affiliation confidence="0.960682">LINGLINK Anite Systems</affiliation>
<address confidence="0.931579">13 rue Robert Stumper L-2557 Luxembourg</address>
<abstract confidence="0.999644461538461">efficient of lexical cohesion is described for ranking text units according to their contribution in defining the meaning of a text (textual saliency), their ability to form a cohesive subtext (textual connectivity) and the extent and effectiveness to which they address the different topics which characterize the subject matter of the text (topic aptness). A specific application is also discussed where the method described is employed to build the indexing component of a summarization system to provide both generic and query-based indicative summaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using Lexical Chains for Text Summarization. In</title>
<date>1997</date>
<booktitle>Proceedings of a Workshop Sponsored by the Association for Computational Linguistics,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="5534" citStr="Barzilay and Elhadad (1997)" startWordPosition="944" endWordPosition="947">unit #3# (final score: 4), and • third: text unit #1# (final score: 3). A text abridgement can be obtained by selecting text units in ranking order according to the text percentage specified by the user. For example, a 35% abridgement of the text in Table 2 would result in the selection of text units #2# and #3#. As Hoey points out, additional techniques can be used to refine the assessment of lexical cohesion. A typical example is the use of thesaurus functions such as synonymy and hyponymy to extend the notion of word sharing across text units, as exemplified in Hirst and StC)nge (1997) and Barzilay and Elhadad (1997) with reference to WordNet (Miller et al 1990). Such an extension may improve on the assessnaent of textual saliency and connectivity thus providing better generic summaries, as argued in Barzilay and Elhadad (1997). There are basically two problems with the uses of lexical cohesion for summarization reviewed above. First, the basic algorithm requires that (0 all unique pairwise permutations of distinct text units be processed, and (ii) all cross-sentence word combinations be evaluated for each such text unit pair. The complexity of this algorithm will therefore be 0(n2 * m2) for n text units </context>
<context position="21962" citStr="Barzilay and Elhadad 1997" startWordPosition="3861" endWordPosition="3864">he option of expanding key-words into codes and use these to issue a summary query. The method described can also be used to develop a conceptal indexing component for information retrieval, following Dobrov et al. (1997). Because an attempt is made to prune contextually inappropriate sense expansions of words, the present method may help reducing the ambiguity problem. Possible improvements of this approach can be implemented taking into account additional ways of assessing lexical cohesion such as: • the presence of synonyms or hyponyms across text units (Hoey, 1991; Hirst and StOnge, 1997; Barzilay and Elhadad 1997); 1162 • the presence of lexical cohesion established with reference to lexical databases offering a semantic classification of words other than synonyms, hyponyms and subject domain codes; • the presence of near-synonymous words across text units established by using a method for estimating the degree of semantic similarity between word pairs such as the one proposed by Resnik (1995); • the presence of anaphoric links across text units (Hoey, 1991; Boguraev &amp; Kennedy, 1997), and • the presence of formatting commands as indicators of the relevance of particular types of text fragments. To eval</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Barzilay, R. and M. Elhadad (1997) Using Lexical Chains for Text Summarization. In I. Mani and M. Maybury (eds) Intelligent Scalable Text Summarization, Proceedings of a Workshop Sponsored by the Association for Computational Linguistics, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>C Kennedy</author>
</authors>
<title>Saliencebased Content Characterization of Text Documents. In</title>
<date>1997</date>
<location>Madrid,</location>
<contexts>
<context position="22441" citStr="Boguraev &amp; Kennedy, 1997" startWordPosition="3938" endWordPosition="3941">lexical cohesion such as: • the presence of synonyms or hyponyms across text units (Hoey, 1991; Hirst and StOnge, 1997; Barzilay and Elhadad 1997); 1162 • the presence of lexical cohesion established with reference to lexical databases offering a semantic classification of words other than synonyms, hyponyms and subject domain codes; • the presence of near-synonymous words across text units established by using a method for estimating the degree of semantic similarity between word pairs such as the one proposed by Resnik (1995); • the presence of anaphoric links across text units (Hoey, 1991; Boguraev &amp; Kennedy, 1997), and • the presence of formatting commands as indicators of the relevance of particular types of text fragments. To evaluate the utility of the approach to lexical cohesion developed for summarization, a testsuite was created using 41 Reuter&apos;s news stories and related summaries (available at http: //www.yahoo .com/headlines/news/), by annotating each story with best summary lines. In one evaluation experiment, summary ratio was set at 20% and generic summaries were obtained for the 41 texts. On average, 60% of each summary contained best summary lines. The ranking method used in this evaluati</context>
</contexts>
<marker>Boguraev, Kennedy, 1997</marker>
<rawString>Boguraev, B. &amp; C. Kennedy (1997) Saliencebased Content Characterization of Text Documents. In I. Mani and M. Maybury (eds) Intelligent Scalable Text Summarization, Prooceedings of a Workshop Sponsored by the Association for Computational Linguistics, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dobrov</author>
<author>N Loukachevitch</author>
<author>T Yudina</author>
</authors>
<title>Conceptual Indexing Using Thematic Representation of Texts.</title>
<date>1997</date>
<booktitle>In The 6th Text Retrieval Conference (TREC-6).</booktitle>
<contexts>
<context position="21557" citStr="Dobrov et al. (1997)" startWordPosition="3797" endWordPosition="3800">corresponding to the summary ratio requested by the user. In addition, the user can display a topic profile of the input text, as shown in Table 13 and choose whichever code(s) s/he is interested in, specify a summary ratio and retrieve the wanted portion of the text which best represents the topic(s) selected. Query-based summaries can also be issued by entering keywords; in this case there is the option of expanding key-words into codes and use these to issue a summary query. The method described can also be used to develop a conceptal indexing component for information retrieval, following Dobrov et al. (1997). Because an attempt is made to prune contextually inappropriate sense expansions of words, the present method may help reducing the ambiguity problem. Possible improvements of this approach can be implemented taking into account additional ways of assessing lexical cohesion such as: • the presence of synonyms or hyponyms across text units (Hoey, 1991; Hirst and StOnge, 1997; Barzilay and Elhadad 1997); 1162 • the presence of lexical cohesion established with reference to lexical databases offering a semantic classification of words other than synonyms, hyponyms and subject domain codes; • the</context>
</contexts>
<marker>Dobrov, Loukachevitch, Yudina, 1997</marker>
<rawString>Dobrov, B., N. Loukachevitch and T. Yudina (1997) Conceptual Indexing Using Thematic Representation of Texts. In The 6th Text Retrieval Conference (TREC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fox</author>
</authors>
<title>Lexical Analysis and Stoplists. In Frakes W and Baeza-Yates R (eds) Information Retrieval: Data Structures &amp; Algorithms. Prentice Hall, Upper Saddle River,</title>
<date>1992</date>
<pages>102--130</pages>
<location>NJ, USA,</location>
<contexts>
<context position="3580" citStr="Fox, 1992" startWordPosition="588" endWordPosition="589">d connectivity for each text unit is therefore established by summing the number of shared words associated with the text unit. According to Hoey, the number of links (e.g. shared words) across two text units must be above a certain threshold for the two text units to achieve a lexical cohesion rank. For example, if only individual scores greater than 2 &apos;Non-stop words can be intuitively thought of as words which have high informational content. They usually exclude words with a very high fequency of occurrence, especially closed class words such as determiners, prepositions and conjunctions (Fox, 1992). 1157 #1# Apple Looking for a Partner #2# NEW YORK (Reuter) - Apple is actively looking for a friendly merger partner, according to several executives close to the company, the New York Times said on Thursday. #3# One executive who does business with Apple said Apple employees told him the company was again in talks with Sun Microsystems, the paper said. #4# On Wednesday, Saudi Arabia&apos;s Prince Alwaleed Bin Talal Bin Abdulaziz Al Saud said he owned more than five percent of the computer maker&apos;s stock, recently buying shares on the open market for a total of $116 million. #6# Oracle Corp Chairm</context>
</contexts>
<marker>Fox, 1992</marker>
<rawString>Fox, C. (1992) Lexical Analysis and Stoplists. In Frakes W and Baeza-Yates R (eds) Information Retrieval: Data Structures &amp; Algorithms. Prentice Hall, Upper Saddle River, NJ, USA, pp. 102-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical Chains as Representation of context for the detection and correction of malapropism. In C. Fellbaum (ed) WordNet: An electronic lexical database and some of its applications.</title>
<date>1997</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="6569" citStr="Hirst and St-Onge (1997)" startWordPosition="1121" endWordPosition="1124"> units be processed, and (ii) all cross-sentence word combinations be evaluated for each such text unit pair. The complexity of this algorithm will therefore be 0(n2 * m2) for n text units in a text and m words in a text unit of average length in the text at hand. This estimate may get worse as conditions such as synonymy and hyponymy are checked for each word pair to extend the notion of lexical cohesion, e.g. using WordNet as in Barzilay and Elhadad (1997). Consequently, the approach may not be suitable for on-line use with longer input texts. Secondly, the use of thesauri envisaged in both Hirst and St-Onge (1997) and Barzilay and Elhadad (1997) does not address the question of topical aptness. Thesaural relations such as synonymy and hyponymy are meant to capture word similarity in order to assess lexical cohesion among text units, and not to provide a thematic characterization of text units.2 Consequently, it will not be possible to index and retrieve text units in term of topic aptness according to users&apos; needs. In the remaining part of the paper, we will show how these concerns of efficiency and thematic characterization can be addressed with specific reference to a system performing generic and qu</context>
</contexts>
<marker>Hirst, St-Onge, 1997</marker>
<rawString>Hirst, G. and D. St-Onge (1997) Lexical Chains as Representation of context for the detection and correction of malapropism. In C. Fellbaum (ed) WordNet: An electronic lexical database and some of its applications. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hoey</author>
</authors>
<title>Patterns of Lexis in Text.</title>
<date>1991</date>
<publisher>OUP, Oford, UK.</publisher>
<contexts>
<context position="2320" citStr="Hoey (1991)" startWordPosition="362" endWordPosition="363">oznariski for helpful comments on previous versions of this document. Many thanks also to Stephen Burns for intemet programming support, Ralf Steinberger for assistance in dictionary conversion, and Charlotte Boynton for editorial help. rization can be conveniently performed on-line. The goal of this paper is to show how these objectives can be achieved through a conceptual indexing technique based on an efficient use of lexical cohesion. 2 Background Lexical cohesion has been widely used in text analysis for the comparative assessment of saliency and connectivity of text fragments. Following Hoey (1991), a simple way of computing lexical cohesion in a text is to segment the text into units (e.g sentences) and to count non-stop words1 which co-occur in each pair of distinct text units, as shown in Table 2 for the text in Table 1. Text units which contain a greater number of shared non-stop words are more likely to provide a better abridgement of the original text for two reasons: • the more often a word with high informational content occurs in a text, the more topical and germane to the text the word is likely to be, and • the greater the number of times two text units share a word, the more</context>
<context position="21910" citStr="Hoey, 1991" startWordPosition="3854" endWordPosition="3855">ng keywords; in this case there is the option of expanding key-words into codes and use these to issue a summary query. The method described can also be used to develop a conceptal indexing component for information retrieval, following Dobrov et al. (1997). Because an attempt is made to prune contextually inappropriate sense expansions of words, the present method may help reducing the ambiguity problem. Possible improvements of this approach can be implemented taking into account additional ways of assessing lexical cohesion such as: • the presence of synonyms or hyponyms across text units (Hoey, 1991; Hirst and StOnge, 1997; Barzilay and Elhadad 1997); 1162 • the presence of lexical cohesion established with reference to lexical databases offering a semantic classification of words other than synonyms, hyponyms and subject domain codes; • the presence of near-synonymous words across text units established by using a method for estimating the degree of semantic similarity between word pairs such as the one proposed by Resnik (1995); • the presence of anaphoric links across text units (Hoey, 1991; Boguraev &amp; Kennedy, 1997), and • the presence of formatting commands as indicators of the rele</context>
</contexts>
<marker>Hoey, 1991</marker>
<rawString>Hoey, M. (1991) Patterns of Lexis in Text. OUP, Oford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="5580" citStr="Miller et al 1990" startWordPosition="952" endWordPosition="955">final score: 3). A text abridgement can be obtained by selecting text units in ranking order according to the text percentage specified by the user. For example, a 35% abridgement of the text in Table 2 would result in the selection of text units #2# and #3#. As Hoey points out, additional techniques can be used to refine the assessment of lexical cohesion. A typical example is the use of thesaurus functions such as synonymy and hyponymy to extend the notion of word sharing across text units, as exemplified in Hirst and StC)nge (1997) and Barzilay and Elhadad (1997) with reference to WordNet (Miller et al 1990). Such an extension may improve on the assessnaent of textual saliency and connectivity thus providing better generic summaries, as argued in Barzilay and Elhadad (1997). There are basically two problems with the uses of lexical cohesion for summarization reviewed above. First, the basic algorithm requires that (0 all unique pairwise permutations of distinct text units be processed, and (ii) all cross-sentence word combinations be evaluated for each such text unit pair. The complexity of this algorithm will therefore be 0(n2 * m2) for n text units in a text and m words in a text unit of averag</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, G., Beckwith, R., C. Fellbaum, D. Gross and K. Miller (1990) Introduction to WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Procter</author>
</authors>
<title>Cambridge International Dictionary of English,</title>
<date>1995</date>
<location>CUP, London.</location>
<contexts>
<context position="15503" citStr="Procter, 1995" startWordPosition="2734" endWordPosition="2735">S CODE EXPLANATION company_n F Finance &amp; Business MI Military (the armed forces) SCG Scouting &amp; Girl Guides TH Theatre partner_n DA Dance &amp; Choreography Finance &amp; Business MGE Marriage, Divorce, Relationships &amp; Infidelity TG Team Games Table 8: Fragment of dictionary database providing subject domain information. it possible to detect the major topics of a document automatically and to assess how well each text unit represents these topics. In our implementation, we used the &amp;quot;subject domain codes&amp;quot; provided in the machine readable version of CIDE (Cambridge International Dictionary of English (Procter, 1995)). Table 8 provides an illustrative example of the information used. Both the indexing and ranking phases are carried out with reference to subject domain codes rather than words. As shown in Table 9 for text unit #1#, the indexing procedure provides a record of the subject domain codes occurring in each text unit; each such subject code is reverse-indexed with reference to all other text units in which the subject code occurs. In addition, a record of which word originates which cohesion link is kept for each text unit index. The main function of keeping track of this information is to avoid </context>
</contexts>
<marker>Procter, 1995</marker>
<rawString>Procter, P. (1995) Cambridge International Dictionary of English, CUP, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In IJCAI-95.</booktitle>
<contexts>
<context position="22349" citStr="Resnik (1995)" startWordPosition="3925" endWordPosition="3926">is approach can be implemented taking into account additional ways of assessing lexical cohesion such as: • the presence of synonyms or hyponyms across text units (Hoey, 1991; Hirst and StOnge, 1997; Barzilay and Elhadad 1997); 1162 • the presence of lexical cohesion established with reference to lexical databases offering a semantic classification of words other than synonyms, hyponyms and subject domain codes; • the presence of near-synonymous words across text units established by using a method for estimating the degree of semantic similarity between word pairs such as the one proposed by Resnik (1995); • the presence of anaphoric links across text units (Hoey, 1991; Boguraev &amp; Kennedy, 1997), and • the presence of formatting commands as indicators of the relevance of particular types of text fragments. To evaluate the utility of the approach to lexical cohesion developed for summarization, a testsuite was created using 41 Reuter&apos;s news stories and related summaries (available at http: //www.yahoo .com/headlines/news/), by annotating each story with best summary lines. In one evaluation experiment, summary ratio was set at 20% and generic summaries were obtained for the 41 texts. On average</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik (1995) Using information content to evaluate semantic similarity in a taxonomy. In IJCAI-95.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>