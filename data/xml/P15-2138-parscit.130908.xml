<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007571">
<title confidence="0.997368">
Unsupervised extractive summarization via coverage maximization
with syntactic and semantic concepts
</title>
<author confidence="0.995082">
Natalie Schluter and Anders Søgaard
</author>
<affiliation confidence="0.997857">
Center for Language Technology
University of Copenhagen
</affiliation>
<email confidence="0.998611">
{natschluter,soegaard}@hum.ku.dk
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999699357142857">
Coverage maximization with bigram con-
cepts is a state-of-the-art approach to un-
supervised extractive summarization. It
has been argued that such concepts are ad-
equate and, in contrast to more linguistic
concepts such as named entities or syn-
tactic dependencies, more robust, since
they do not rely on automatic process-
ing. In this paper, we show that while this
seems to be the case for a commonly used
newswire dataset, use of syntactic and se-
mantic concepts leads to significant im-
provements in performance in other do-
mains.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816272727273">
State-of-the-art approaches to extractive summa-
rization are based on the notion of coverage max-
imization (Berg-Kirkpatrick et al., 2011). The
assumption is that a good summary is a selec-
tion of sentences from the document that contains
as many of the important concepts as possible.
The importance of concepts is implemented by as-
signing weights wi to each concept i with binary
variable ci, yielding the following coverage maxi-
mization objective, subject to the appropriate con-
straints:
</bodyText>
<equation confidence="0.983666666666667">
N
wici (1)
i
</equation>
<bodyText confidence="0.999975466666667">
In proposing bigrams as concepts for their system,
Gillick and Favre (2009) explain that:
[c]oncepts could be words, named enti-
ties, syntactic subtrees or semantic re-
lations, for example. While deeper se-
mantics make more appealing concepts,
their extraction and weighting are much
more error-prone. Any error in concept
extraction can result in a biased objec-
tive function, leading to poor sentence
selection. (Gillick and Favre, 2009)
Several authors, e.g., Woodsend and Lapata
(2012), and Li et al. (2013), have followed Gillick
and Favre (2009) in assuming that bigrams would
lead to better practical performance than more
syntactic or semantic concepts, even though bi-
grams serve as only an approximation of these.
In this paper, we revisit this assumption and
evaluate the maximum coverage objective for ex-
tractive text summarization with syntactic and se-
mantic concepts. Specifically, we replace bigram
concepts with new ones based on syntactic depen-
dencies, semantic frames, as well as named enti-
ties. We show that using such concepts can lead
to significant improvements in text summariza-
tion performance outside of the newswire domain.
We evaluate coverage maximization incorporating
syntactic and semantic concepts across three dif-
ferent domains: newswire, legal judgments, and
Wikipedia articles.
</bodyText>
<sectionHeader confidence="0.885778" genericHeader="method">
2 Concept coverage maximization for
extractive summarization
</sectionHeader>
<bodyText confidence="0.99952425">
In extractive summarization, the unsupervised ver-
sion of the task is sometimes set up as that of find-
ing a subset of sentences in a document, within
some relatively small budget, that covers as many
of the important concepts in the document as pos-
sible. In the maximum coverage objective, con-
cepts are considered as independent of each other.
Concepts are weighted by the number of times
they appear in a document. Moreover, due the
NP-hardness of coverage maximization, for an ex-
act solution to the concept coverage optimization
problem, we resort to fast solvers for integer linear
programming, under some appropriate constraints.
Bigrams. Gillick and Favre (2009) proposed to
use bigrams as concepts, and to weight their con-
tribution to the objective function in Equation (1)
</bodyText>
<page confidence="0.849475">
840
</page>
<bodyText confidence="0.965124975609756">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 840–844,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
by the frequency with which they occur in the doc-
ument. Some pre-processing is first carried out to
these bigrams: all bigrams consisting uniquely of
stop-words are removed from consideration, and
each word is stemmed. They also require bigrams
to occur with a minimal frequency (cf. Section
3.2).
Named entities. We consider three new types of
concepts, all suggested, but subsequently rejected
by Gillick and Favre (2009). The first is simply
to use named entities, e.g., Court of Justice of the
European Union, as concepts. This reflects the in-
tuition that persons, organizations, and locations
are particularly important for extractive summa-
rization. We use an NER maximum entropy tag-
ger1 to augment documents with named entities.
Syntactic dependencies. The second type of
concept is dependency subtrees. In particular,
we extract labeled and unlabeled syntactic depen-
dencies, e.g., DEPENDENCY(walks,John) or SUB-
JECT(walks,John), from sentences and represent
them by such syntactic concepts. We use the Stan-
ford parser2 to augment documents with syntac-
tic dependencies. As was done for bigrams, each
word in the dependency is stemmed. Syntactic
dependency-based concepts are intuitively a closer
approximation than bigrams to concepts in gen-
eral.
Semantic frames. The intuition behind our use
of frame semantics is that a summary should rep-
resent the most central semantic frames (Fillmore,
1982; Fillmore et al., 2003) present in the cor-
responding document—indeed, we consider these
frames to be actual types of concepts. We ex-
tract frame names from sentences for a further
type of concepts under consideration. We use SE-
MAFOR3 to augment documents with semantic
frames.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996776">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.995542666666667">
In order to investigate the importance of concept
types across different domains, we evaluate our
systems across three distinct domains, which we
refer to as ECHR, TAC08, and WIKIPEDIA.
ECHR consists of judgment-summary pairs
scraped from the European Court of Hu-
</bodyText>
<footnote confidence="0.9986035">
1http://www.nltk.org/
2http://nlp.stanford.edu/software/
lex-parser.shtml
3http://www.ark.cs.cmu.edu/SEMAFOR/
</footnote>
<bodyText confidence="0.999930606060606">
man Rights case-law website, HUDOC4. The
document-summary pairs were split into training,
development and test sets, consisting of 1018, 117,
and 138 pairs, respectively. In the training set
(pruning sentences of length less than 5), the aver-
age document length is 13,184 words or 455 sen-
tences. The average summary length is 806 words
or 28 sentences. For both documents and sum-
maries, the average sentence length is 29 words.
TAC08 consists of 48 queries and 2 newswire
document sets for each query, each set contain-
ing 10 documents. Document sets contain 235 in-
put sentences on average, and the mean sentence
length is 25 words. Summaries consist of 4 sen-
tences or 100 words on average.
WIKIPEDIA consists of 992 Wikipedia articles
(all labeled “good article”5) from a comprehen-
sive dump of English language Wikipedia arti-
cles6. We use the Wikipedia abstracts (the leading
paragraphs before the table of contents) as sum-
maries. The (document,summary) pairs were split
into training, development and test sets, consist-
ing of 784, 97, and 111 pairs, respectively. In the
training set (pruning sentences of length less than
5), the average document length is around 8918
words or 339 sentences. The average summary
length is 335 words or 13 sentences. For both
documents and summaries, the average sentence
length is around 26 words.
In our main experiments, we use unsupervised
summarization techniques, and we only use the
training summaries (and not the documents) to de-
termine output summary lengths.
</bodyText>
<subsectionHeader confidence="0.99944">
3.2 Baseline and systems
</subsectionHeader>
<bodyText confidence="0.999919818181818">
Our baseline is the bigram-based extraction sum-
marization system of Gillick and Favre (2009),
icsisumm7. Their system was originally in-
tended for multi-document update summarization,
and summaries are extracted from document sen-
tences that share more than k content words with
some query. We follow this approach for the
TAC08 data. For ECHR and WIKIPEDIA, the
task is single document summarization, and the
now irrelevant topic-document intersection pre-
processing step is eliminated.
</bodyText>
<footnote confidence="0.999023857142857">
4http://hudoc.echr.coe.int/
5http://en.wikipedia.org/wiki/
Wikipedia:Good_articles
6https://dumps.wikimedia.org/
enwiki/latest/enwiki-latest-pages
-articles-multistream.xml.bz2
7https://code.google.com/p/icsisumm/
</footnote>
<page confidence="0.99562">
841
</page>
<bodyText confidence="0.999630235294118">
The original system uses the GNU linear pro-
gramming kit8 with a time limit of 100 sec-
onds. For all experiments presented in this pa-
per, we double this time limit; we experimented
with longer time limits on the development set
for the ECHR data, without any performance im-
provements. Once the summarizer reaches the
time limit, a summary is output based on the cur-
rent feasible solution, whether the solution is op-
timal or not. Moreover, the current icsisumm
(v1) distribution prunes sentences shorter than 10
words. We note that we also tried replacing glpk
by gurobi9, for which no time limit was neces-
sary, but found poorer results on the development
set of the ECHR data.
The original system takes several important in-
put parameters.
</bodyText>
<listItem confidence="0.997203806451613">
1. Summary length, for TAC08, is specified
by the TAC 2008 conference guidelines as
100 words. For WIKIPEDIA and ECHR, we
have access to training sets which gave an
average summary length of around 335 and
805 words respectively, which we take as the
standard output summary length.
2. Concept count cut-off is the minimum fre-
quency of concepts from the document (set)
that qualifies them for consideration in cov-
erage maximization. For bigrams of the orig-
inal system on TAC08, there are two types
of document sets: ‘A’ and ‘B’. For ‘A’ type
documents, Gillick and Favre (2009) set this
threshold to 3 and for ‘B’ type documents,
they set this to 4. For WIKIPEDIA and ECHR,
we take the bigram threshold to be 4. In our
extension of the system to other concepts, we
do not use any threshold.
3. First concept weighting: in multi-document
summarization, there is the possibility for
repeated sentences. Concepts from first-
encountered sentences may be weighted
higher: these concept counts from first-
encountered sentences are doubled for ‘B’
documents and remain unchanged for ‘A’
documents in the original system on TAC08.
For other concepts, we do not alter frequen-
cies in this manner, which is justified by the
task change to single-document summariza-
tion.
</listItem>
<footnote confidence="0.999765">
8http://www.gnu.org/software/glpk/
9http://www.gurobi.com/
</footnote>
<sectionHeader confidence="0.574529" genericHeader="method">
4. Query-sentence intersection threshold, is
</sectionHeader>
<bodyText confidence="0.994424192307692">
set to 1 for ‘A’ documents and 0 to ‘B’
documents in the original system on TAC08.
This threshold is only for the update summa-
rization task and therefore does not concern
ECHR and WIKIPEDIA.
In addition to our baseline, we consider five
single-concept systems using (a) named entities,
(b) labeled dependencies, (c) unlabeled dependen-
cies, (d) semantic frame names, and (e) seman-
tic frame dependencies, as well as the five sys-
tems combining each of these new concept types
with bigrams. For the combination of these new
concepts with bigrams, we extend the objective
function to maximise in, Equation (1), into two
sums—one for bigram concepts and the other for
the new concept type—with their relative impor-
tance controlled by a parameter α. N1 and N2 are
the number of bigram and other concept types oc-
curring with the permitted threshold frequency in
the document, relatively. Given that we are carry-
ing out unsupervised summarization, rather than
tune α, we set α = 0.5, so the concepts are con-
sidered in their totality (i.e., N1 + N2 concepts to-
gether) with no explicit favouring of one over the
other that does not naturally fall out of concept fre-
quency.
</bodyText>
<equation confidence="0.905397">
N1 N2
(1−α) wibigrami+α wjnew conceptj
i j
</equation>
<subsectionHeader confidence="0.767874">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.9999793">
We evaluate output summaries using ROUGE-1,
ROUGE-2, and ROUGE-SU4 (Lin, 2004), with
no stemming and retaining all stopwords. These
measures have been shown to correlate best with
human judgments in general, but among the au-
tomatic measures, ROUGE-1 and ROUGE-2 also
correlate best with the Pyramid (Nenkova and Pas-
sonneau, 2004; Nenkova et al., 2007) and Re-
sponsiveness manual metrics (Louis and Nenkova,
2009). Moreover, ROUGE-1 has been shown to
best reflect human-automatic summary compar-
isons (Owczarzak et al., 2012).
For single concept systems, the results are
shown in Table 1, and concept combination sys-
tem results are given in Table 2.
We first note that our runs of the current dis-
tribution of icsisumm yield significantly worse
ROUGE-2 results than reported in (Gillick and
Favre, 2009) (see Table 1, BIGRAMS): 0.081 com-
pared to 0.110 respectively.
</bodyText>
<page confidence="0.993581">
842
</page>
<bodyText confidence="0.9999815">
On the TAC08 data, we observe no improve-
ments over the baseline BIGRAM system for any
ROUGE metric here. Hence, Gillick and Favre
(2009) were right in their assumption that syntac-
tic and semantic concepts would not lead to perfor-
mance improvements, when restricting ourselves
to this dataset. However, when we change domain
to the legal judgments or Wikipedia articles, using
syntactic and semantic concepts leads to signifi-
cant gains across all the ROUGE metrics.
For ECHR, replacing bigrams by frame names
(FRAME) results in an increase of +0.1 in
ROUGE-1, +0.031 in ROUGE-2 and +0.046 in
ROUGE-SU4. We note that FrameNet 1.5 covers
the legal domain quite well, which may explain
why these concepts are particularly useful for the
ECHR dataset. However, labeled (LDEP) and unla-
beled (UDEP) dependencies also significantly out-
perform the baseline.
For WIKIPEDIA, replacing bigrams by labeled
or unlabeled syntactic dependencies results in sig-
nificant improvements: an increase of +0.088
for ROUGE-1, +0.015 for ROUGE-2, and +0.03
for ROUGE-SU4. Interestingly, the NER sys-
tem also yields significantly better performance
over the baseline, which may reflect the nature
of Wikipedia articles, often being about historical
figures, famous places, organizations, etc.
We observe in Table 2, that for concept combi-
nation systems as well, ROUGE scores on TAC08
do not indicate any improvement in performance.
However, best ROUGE-1 scores are produced
for both ECHR and WIKIPEDIA data with sys-
tems that incorporate semantic frame names. For
WIKIPEDIA, best ROUGE-2 and ROUGE-SU4
scores incorporate named-entity information.
</bodyText>
<sectionHeader confidence="0.999904" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.998985785714286">
Most researchers have used bigrams as concepts in
coverage maximization-based approaches to unsu-
pervised extractive summarization. Filatova and
Hatzivassiloglou (2004), however,use relations be-
tween named entities as concepts in extractive
summarization. They use slightly different extrac-
tion algorithms, but their work is similar in spirit
to ours. Nishikawa et al. (2010), also, use opin-
ions – tuples of targets, aspects, and polarity –
as concepts in opinion summarization. In early
work on summarization, Silber and McCoy (2000)
used WordNet synsets as concepts. Kitajima and
Kobayashi (2011) replace words by syntactic de-
pendencies in the Maximal Marginal Relevance
</bodyText>
<table confidence="0.987697641025641">
ECHR
concept R-1 R-2 R-SU4
(95% conf.) (95% conf.) (95% conf.)
BIGRAMS 0.544 0.204 0.266
(0.528-0.562) (0.195-0.215) (0.257-0.277)
NER 0.549 0.184 0.254
(0.534-0.564) (0.174-0.193) (0.244-0.264)
LDEP 0.609 0.225 0.293
(0.597-0.621) (0.217-0.235) (0.285-0.302)
UDEP 0.612 0.227 0.295
(0.6-0.626) (0.218-0.238) (0.287-0.305)
FRAMES 0.643 0.235 0.312
(0.63-0.657) (0.224-0.248) (0.302-0.323)
TAC08
R-1 R-2 R-SU4
concept (95% conf.) (95% conf.) (95% conf.)
BIGRAMS 0.35 0.081 0.119
(0.34-0.36) (0.073-0.089) (0.113-0.126)
NER 0.307 0.054 0.093
(0.297-0.317) (0.049-0.06) (0.089-0.099)
LDEP 0.335 0.072 0.109
(0.325-0.346) (0.065-0.08) (0.103-0.116)
UDEP 0.342 0.075 0.113
(0.331-0.353) (0.067-0.083) (0.106-0.12)
FRAMES 0.301 0.048 0.089
(0.292-0.31) (0.042-0.053) (0.085-0.094)
WIKIPEDIA
R-1 R-2 R-SU4
concept (95% conf.) (95% conf.) (95% conf.)
BIGRAMS 0.391 0.103 0.152
(0.364-0.415) (0.094-0.113) (0.134-0.163)
NER 0.473 0.114 0.178
(0.46-0.487) (0.105-0.123) (0.169-0.186)
LDEP 0.478 0.116 0.179
(0.461-0.495) (0.107-0.125) (0.169-0.188)
UDEP 0.479 0.118 0.18
(0.462-0.497) (0.109-0.128) (0.17-0.189)
FRAMES 0.476 0.102 0.172
(0.461-0.494) (0.094-0.112) (0.164-0.182)
</table>
<tableCaption confidence="0.866773">
Table 1: Single concept results on ECHR, TAC08,
and WIKIPEDIA.
</tableCaption>
<bodyText confidence="0.9994624">
Multidocument measure first proposed by Gold-
stein et al. (2000) for evaluating the importance
of sentences in query-based extractive summa-
rization, yielding improvements for their Japanese
newswire dataset.
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999957625">
This paper challenges the assumption that bigrams
make better concepts for unsupervised extractive
summarization than syntactic and semantic con-
cepts relying on automatic processing. We show
that using concepts relying on syntactic dependen-
cies or semantic frames instead of bigrams leads
to significant performance improvements of cover-
age maximization summarization across domains.
</bodyText>
<sectionHeader confidence="0.992464" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.82333">
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proc of ACL, Portland, OR, USA.
</reference>
<page confidence="0.997894">
843
</page>
<table confidence="0.989755909090909">
ECHR
concept R-1 R-2 R-SU4
(95% conf.) (95% conf.) (95% conf.)
NER 0.605 0.228 0.093
(0.595-0.616) (0.22- 0.237) (0.288-0.303)
LDEP 0.614 0.235 0.301
(0.597-0.632) (0.225-0.246) (0.29-0.312)
UDEP 0.62 0.237 0.304
(0.605-0.634) (0.227-0.247) (0.294-0.313)
FRAMES 0.65 0.251 0.322
(0.638-0.662) (0.24-0.262) (0.313-0.333)
TAC0S
R-1 R-2 R-SU4
concept (95% conf.) (95% conf.) (95% conf.)
NER 0.35 0.082 0.119
(0.339-0.361) (0.074-0.09) (0.112-0.126)
LDEP 0.345 0.08 0.117
(0.334-0.355) (0.072-0.088) (0.11-0.124)
UDEP 0.347 0.08 0.12
(0.336-0.358) (0.072-0.088) (0.11-0.125)
FRAMES 0.344 0.078 0.115
(0.334-0.354) (0.071-0.086) (0.11-0.122)
WIKIPEDIA
R-1 R-2 R-SU4
concept (95% conf.) (95% conf.) (95% conf.)
NER 0.496 0.136 0.195
(0.483-0.51) (0.127-0.146) (0.187-0.204)
LDEP 0.495 0.132 0.192
(0.479-0.511) (0.122-0.141) (0.183-0.202)
UDEP 0.493 0.13 0.18
(0.478-0.511) (0.121-0.14) (0.181-0.199)
FRAMES 0.497 0.124 0.187
(0.482-0.513) (0.114-0.133) (0.179-0.197)
</table>
<tableCaption confidence="0.949584">
Table 2: Results for systems combining bi-
</tableCaption>
<reference confidence="0.982162147540984">
grams with new concepts, on ECHR, TAC08 and
WIKIPEDIA.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In ACL
Workshop on Text Summarization Branches Out.
Charles J. Fillmore, Christopher. R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
framenet. International Journal of Lexicography,
16.
Charles J. Fillmore, 1982. Linguistics in the Morn-
ing Calm, chapter Frame Semantics, pages 111–137.
Hanshin Publishing Co., Seoul, South Korea.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proc of ILP, pages 10–
18.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Proc of the
ANLP/NAACL Workshop on Automatic Summariza-
tion, pages 40–48.
Risa Kitajima and Ichiro Kobayashi. 2011. A latent
topic extracting method based on events in a docu-
ment and its application. In Proc of ACL-HLT Stu-
dent Session, pages 30–35, Portland, OR, USA.
Chen Li, Xian Qian, and Yang Liu. 2013. Using super-
vised bigram-based ilp for extractive summarization.
In Proc of ACL, Sofia, Bulgaria.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Proc of WAS,
Barcelona, Spain.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluation content selection in summarization with-
out human models. In Proc of EMNLP, pages 306–
314, Singapore.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proc. of HLT-NAACL.
Ani Nenkova, Rebecca Passonneau, and Kath-
leen McKeown. 2007. The pyramid
method:incorporating human content selection
variation in summarization evaluation. ACM
Transactions on Speech and Language Processing,
4.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Genichiro Kikui. 2010. Opinion summa-
rization with integer linear programming formula-
tion for sentence extraction and ordering. In COL-
ING.
Karolina Owczarzak, John M. Conroy, Hoa
Trang Dang, and Ani Nenkova. 2012. An as-
sessment of the accuracy of automatic evaluation
in summarization. In Proc of the Workshop on
Evaluation Metrics and System Comparison for
Automatic Summarization, pages 1–9, Montr´eal,
Canada.
H. Gregory Silber and Kathleen McCoy. 2000. An effi-
cient text summarizer using lexical chains. In INLG.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proc of EMNLP/CoNLL, pages 233–
243, Jeju Island, Korea.
</reference>
<page confidence="0.998701">
844
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839720">
<title confidence="0.968287">Unsupervised extractive summarization via coverage with syntactic and semantic concepts</title>
<author confidence="0.928134">Schluter</author>
<affiliation confidence="0.997987">Center for Language University of</affiliation>
<abstract confidence="0.995547266666667">Coverage maximization with bigram concepts is a state-of-the-art approach to unsupervised extractive summarization. It has been argued that such concepts are adequate and, in contrast to more linguistic concepts such as named entities or syntactic dependencies, more robust, since they do not rely on automatic processing. In this paper, we show that while this seems to be the case for a commonly used newswire dataset, use of syntactic and semantic concepts leads to significant improvements in performance in other domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proc of ACL,</booktitle>
<location>Portland, OR, USA.</location>
<contexts>
<context position="913" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="130" endWordPosition="133">state-of-the-art approach to unsupervised extractive summarization. It has been argued that such concepts are adequate and, in contrast to more linguistic concepts such as named entities or syntactic dependencies, more robust, since they do not rely on automatic processing. In this paper, we show that while this seems to be the case for a commonly used newswire dataset, use of syntactic and semantic concepts leads to significant improvements in performance in other domains. 1 Introduction State-of-the-art approaches to extractive summarization are based on the notion of coverage maximization (Berg-Kirkpatrick et al., 2011). The assumption is that a good summary is a selection of sentences from the document that contains as many of the important concepts as possible. The importance of concepts is implemented by assigning weights wi to each concept i with binary variable ci, yielding the following coverage maximization objective, subject to the appropriate constraints: N wici (1) i In proposing bigrams as concepts for their system, Gillick and Favre (2009) explain that: [c]oncepts could be words, named entities, syntactic subtrees or semantic relations, for example. While deeper semantics make more appealing conc</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proc of ACL, Portland, OR, USA. grams with new concepts, on ECHR, TAC08 and WIKIPEDIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Event-based extractive summarization.</title>
<date>2004</date>
<booktitle>In ACL Workshop on Text Summarization Branches Out.</booktitle>
<contexts>
<context position="13993" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="2162" endWordPosition="2165">ture of Wikipedia articles, often being about historical figures, famous places, organizations, etc. We observe in Table 2, that for concept combination systems as well, ROUGE scores on TAC08 do not indicate any improvement in performance. However, best ROUGE-1 scores are produced for both ECHR and WIKIPEDIA data with systems that incorporate semantic frame names. For WIKIPEDIA, best ROUGE-2 and ROUGE-SU4 scores incorporate named-entity information. 4 Related work Most researchers have used bigrams as concepts in coverage maximization-based approaches to unsupervised extractive summarization. Filatova and Hatzivassiloglou (2004), however,use relations between named entities as concepts in extractive summarization. They use slightly different extraction algorithms, but their work is similar in spirit to ours. Nishikawa et al. (2010), also, use opinions – tuples of targets, aspects, and polarity – as concepts in opinion summarization. In early work on summarization, Silber and McCoy (2000) used WordNet synsets as concepts. Kitajima and Kobayashi (2011) replace words by syntactic dependencies in the Maximal Marginal Relevance ECHR concept R-1 R-2 R-SU4 (95% conf.) (95% conf.) (95% conf.) BIGRAMS 0.544 0.204 0.266 (0.528</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. Event-based extractive summarization. In ACL Workshop on Text Summarization Branches Out.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johnson</author>
<author>Miriam R L Petruck</author>
</authors>
<title>Background to framenet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<volume>16</volume>
<marker>Johnson, Petruck, 2003</marker>
<rawString>Charles J. Fillmore, Christopher. R. Johnson, and Miriam R. L. Petruck. 2003. Background to framenet. International Journal of Lexicography, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Linguistics in the Morning Calm, chapter Frame Semantics,</title>
<date>1982</date>
<pages>111--137</pages>
<publisher>Hanshin Publishing Co.,</publisher>
<location>Seoul, South</location>
<contexts>
<context position="5106" citStr="Fillmore, 1982" startWordPosition="777" endWordPosition="778">is dependency subtrees. In particular, we extract labeled and unlabeled syntactic dependencies, e.g., DEPENDENCY(walks,John) or SUBJECT(walks,John), from sentences and represent them by such syntactic concepts. We use the Stanford parser2 to augment documents with syntactic dependencies. As was done for bigrams, each word in the dependency is stemmed. Syntactic dependency-based concepts are intuitively a closer approximation than bigrams to concepts in general. Semantic frames. The intuition behind our use of frame semantics is that a summary should represent the most central semantic frames (Fillmore, 1982; Fillmore et al., 2003) present in the corresponding document—indeed, we consider these frames to be actual types of concepts. We extract frame names from sentences for a further type of concepts under consideration. We use SEMAFOR3 to augment documents with semantic frames. 3 Experiments 3.1 Data In order to investigate the importance of concept types across different domains, we evaluate our systems across three distinct domains, which we refer to as ECHR, TAC08, and WIKIPEDIA. ECHR consists of judgment-summary pairs scraped from the European Court of Hu1http://www.nltk.org/ 2http://nlp.sta</context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>Charles J. Fillmore, 1982. Linguistics in the Morning Calm, chapter Frame Semantics, pages 111–137. Hanshin Publishing Co., Seoul, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In Proc of ILP,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="1353" citStr="Gillick and Favre (2009)" startWordPosition="204" endWordPosition="207">in performance in other domains. 1 Introduction State-of-the-art approaches to extractive summarization are based on the notion of coverage maximization (Berg-Kirkpatrick et al., 2011). The assumption is that a good summary is a selection of sentences from the document that contains as many of the important concepts as possible. The importance of concepts is implemented by assigning weights wi to each concept i with binary variable ci, yielding the following coverage maximization objective, subject to the appropriate constraints: N wici (1) i In proposing bigrams as concepts for their system, Gillick and Favre (2009) explain that: [c]oncepts could be words, named entities, syntactic subtrees or semantic relations, for example. While deeper semantics make more appealing concepts, their extraction and weighting are much more error-prone. Any error in concept extraction can result in a biased objective function, leading to poor sentence selection. (Gillick and Favre, 2009) Several authors, e.g., Woodsend and Lapata (2012), and Li et al. (2013), have followed Gillick and Favre (2009) in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigra</context>
<context position="3313" citStr="Gillick and Favre (2009)" startWordPosition="505" endWordPosition="508">sed version of the task is sometimes set up as that of finding a subset of sentences in a document, within some relatively small budget, that covers as many of the important concepts in the document as possible. In the maximum coverage objective, concepts are considered as independent of each other. Concepts are weighted by the number of times they appear in a document. Moreover, due the NP-hardness of coverage maximization, for an exact solution to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints. Bigrams. Gillick and Favre (2009) proposed to use bigrams as concepts, and to weight their contribution to the objective function in Equation (1) 840 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 840–844, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics by the frequency with which they occur in the document. Some pre-processing is first carried out to these bigrams: all bigrams consisting uniquely of stop-words are removed from consideration, and each word </context>
<context position="7401" citStr="Gillick and Favre (2009)" startWordPosition="1132" endWordPosition="1135">and test sets, consisting of 784, 97, and 111 pairs, respectively. In the training set (pruning sentences of length less than 5), the average document length is around 8918 words or 339 sentences. The average summary length is 335 words or 13 sentences. For both documents and summaries, the average sentence length is around 26 words. In our main experiments, we use unsupervised summarization techniques, and we only use the training summaries (and not the documents) to determine output summary lengths. 3.2 Baseline and systems Our baseline is the bigram-based extraction summarization system of Gillick and Favre (2009), icsisumm7. Their system was originally intended for multi-document update summarization, and summaries are extracted from document sentences that share more than k content words with some query. We follow this approach for the TAC08 data. For ECHR and WIKIPEDIA, the task is single document summarization, and the now irrelevant topic-document intersection preprocessing step is eliminated. 4http://hudoc.echr.coe.int/ 5http://en.wikipedia.org/wiki/ Wikipedia:Good_articles 6https://dumps.wikimedia.org/ enwiki/latest/enwiki-latest-pages -articles-multistream.xml.bz2 7https://code.google.com/p/ics</context>
<context position="9326" citStr="Gillick and Favre (2009)" startWordPosition="1424" endWordPosition="1427">stem takes several important input parameters. 1. Summary length, for TAC08, is specified by the TAC 2008 conference guidelines as 100 words. For WIKIPEDIA and ECHR, we have access to training sets which gave an average summary length of around 335 and 805 words respectively, which we take as the standard output summary length. 2. Concept count cut-off is the minimum frequency of concepts from the document (set) that qualifies them for consideration in coverage maximization. For bigrams of the original system on TAC08, there are two types of document sets: ‘A’ and ‘B’. For ‘A’ type documents, Gillick and Favre (2009) set this threshold to 3 and for ‘B’ type documents, they set this to 4. For WIKIPEDIA and ECHR, we take the bigram threshold to be 4. In our extension of the system to other concepts, we do not use any threshold. 3. First concept weighting: in multi-document summarization, there is the possibility for repeated sentences. Concepts from firstencountered sentences may be weighted higher: these concept counts from firstencountered sentences are doubled for ‘B’ documents and remain unchanged for ‘A’ documents in the original system on TAC08. For other concepts, we do not alter frequencies in this </context>
<context position="12126" citStr="Gillick and Favre, 2009" startWordPosition="1880" endWordPosition="1883">with human judgments in general, but among the automatic measures, ROUGE-1 and ROUGE-2 also correlate best with the Pyramid (Nenkova and Passonneau, 2004; Nenkova et al., 2007) and Responsiveness manual metrics (Louis and Nenkova, 2009). Moreover, ROUGE-1 has been shown to best reflect human-automatic summary comparisons (Owczarzak et al., 2012). For single concept systems, the results are shown in Table 1, and concept combination system results are given in Table 2. We first note that our runs of the current distribution of icsisumm yield significantly worse ROUGE-2 results than reported in (Gillick and Favre, 2009) (see Table 1, BIGRAMS): 0.081 compared to 0.110 respectively. 842 On the TAC08 data, we observe no improvements over the baseline BIGRAM system for any ROUGE metric here. Hence, Gillick and Favre (2009) were right in their assumption that syntactic and semantic concepts would not lead to performance improvements, when restricting ourselves to this dataset. However, when we change domain to the legal judgments or Wikipedia articles, using syntactic and semantic concepts leads to significant gains across all the ROUGE metrics. For ECHR, replacing bigrams by frame names (FRAME) results in an inc</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proc of ILP, pages 10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
<author>Mark Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Proc of the ANLP/NAACL Workshop on Automatic Summarization,</booktitle>
<pages>40--48</pages>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. In Proc of the ANLP/NAACL Workshop on Automatic Summarization, pages 40–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risa Kitajima</author>
<author>Ichiro Kobayashi</author>
</authors>
<title>A latent topic extracting method based on events in a document and its application.</title>
<date>2011</date>
<booktitle>In Proc of ACL-HLT Student Session,</booktitle>
<pages>30--35</pages>
<location>Portland, OR, USA.</location>
<contexts>
<context position="14423" citStr="Kitajima and Kobayashi (2011)" startWordPosition="2228" endWordPosition="2231">ity information. 4 Related work Most researchers have used bigrams as concepts in coverage maximization-based approaches to unsupervised extractive summarization. Filatova and Hatzivassiloglou (2004), however,use relations between named entities as concepts in extractive summarization. They use slightly different extraction algorithms, but their work is similar in spirit to ours. Nishikawa et al. (2010), also, use opinions – tuples of targets, aspects, and polarity – as concepts in opinion summarization. In early work on summarization, Silber and McCoy (2000) used WordNet synsets as concepts. Kitajima and Kobayashi (2011) replace words by syntactic dependencies in the Maximal Marginal Relevance ECHR concept R-1 R-2 R-SU4 (95% conf.) (95% conf.) (95% conf.) BIGRAMS 0.544 0.204 0.266 (0.528-0.562) (0.195-0.215) (0.257-0.277) NER 0.549 0.184 0.254 (0.534-0.564) (0.174-0.193) (0.244-0.264) LDEP 0.609 0.225 0.293 (0.597-0.621) (0.217-0.235) (0.285-0.302) UDEP 0.612 0.227 0.295 (0.6-0.626) (0.218-0.238) (0.287-0.305) FRAMES 0.643 0.235 0.312 (0.63-0.657) (0.224-0.248) (0.302-0.323) TAC08 R-1 R-2 R-SU4 concept (95% conf.) (95% conf.) (95% conf.) BIGRAMS 0.35 0.081 0.119 (0.34-0.36) (0.073-0.089) (0.113-0.126) NER 0.3</context>
</contexts>
<marker>Kitajima, Kobayashi, 2011</marker>
<rawString>Risa Kitajima and Ichiro Kobayashi. 2011. A latent topic extracting method based on events in a document and its application. In Proc of ACL-HLT Student Session, pages 30–35, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Using supervised bigram-based ilp for extractive summarization.</title>
<date>2013</date>
<booktitle>In Proc of ACL,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1785" citStr="Li et al. (2013)" startWordPosition="271" endWordPosition="274">yielding the following coverage maximization objective, subject to the appropriate constraints: N wici (1) i In proposing bigrams as concepts for their system, Gillick and Favre (2009) explain that: [c]oncepts could be words, named entities, syntactic subtrees or semantic relations, for example. While deeper semantics make more appealing concepts, their extraction and weighting are much more error-prone. Any error in concept extraction can result in a biased objective function, leading to poor sentence selection. (Gillick and Favre, 2009) Several authors, e.g., Woodsend and Lapata (2012), and Li et al. (2013), have followed Gillick and Favre (2009) in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these. In this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts. Specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities. We show that using such concepts can lead to significant improvements in text summarization perfor</context>
</contexts>
<marker>Li, Qian, Liu, 2013</marker>
<rawString>Chen Li, Xian Qian, and Yang Liu. 2013. Using supervised bigram-based ilp for extractive summarization. In Proc of ACL, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proc of WAS,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="11405" citStr="Lin, 2004" startWordPosition="1766" endWordPosition="1767">th their relative importance controlled by a parameter α. N1 and N2 are the number of bigram and other concept types occurring with the permitted threshold frequency in the document, relatively. Given that we are carrying out unsupervised summarization, rather than tune α, we set α = 0.5, so the concepts are considered in their totality (i.e., N1 + N2 concepts together) with no explicit favouring of one over the other that does not naturally fall out of concept frequency. N1 N2 (1−α) wibigrami+α wjnew conceptj i j 3.3 Results We evaluate output summaries using ROUGE-1, ROUGE-2, and ROUGE-SU4 (Lin, 2004), with no stemming and retaining all stopwords. These measures have been shown to correlate best with human judgments in general, but among the automatic measures, ROUGE-1 and ROUGE-2 also correlate best with the Pyramid (Nenkova and Passonneau, 2004; Nenkova et al., 2007) and Responsiveness manual metrics (Louis and Nenkova, 2009). Moreover, ROUGE-1 has been shown to best reflect human-automatic summary comparisons (Owczarzak et al., 2012). For single concept systems, the results are shown in Table 1, and concept combination system results are given in Table 2. We first note that our runs of </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proc of WAS, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatically evaluation content selection in summarization without human models.</title>
<date>2009</date>
<booktitle>In Proc of EMNLP,</booktitle>
<pages>306--314</pages>
<contexts>
<context position="11738" citStr="Louis and Nenkova, 2009" startWordPosition="1817" endWordPosition="1820">d in their totality (i.e., N1 + N2 concepts together) with no explicit favouring of one over the other that does not naturally fall out of concept frequency. N1 N2 (1−α) wibigrami+α wjnew conceptj i j 3.3 Results We evaluate output summaries using ROUGE-1, ROUGE-2, and ROUGE-SU4 (Lin, 2004), with no stemming and retaining all stopwords. These measures have been shown to correlate best with human judgments in general, but among the automatic measures, ROUGE-1 and ROUGE-2 also correlate best with the Pyramid (Nenkova and Passonneau, 2004; Nenkova et al., 2007) and Responsiveness manual metrics (Louis and Nenkova, 2009). Moreover, ROUGE-1 has been shown to best reflect human-automatic summary comparisons (Owczarzak et al., 2012). For single concept systems, the results are shown in Table 1, and concept combination system results are given in Table 2. We first note that our runs of the current distribution of icsisumm yield significantly worse ROUGE-2 results than reported in (Gillick and Favre, 2009) (see Table 1, BIGRAMS): 0.081 compared to 0.110 respectively. 842 On the TAC08 data, we observe no improvements over the baseline BIGRAM system for any ROUGE metric here. Hence, Gillick and Favre (2009) were rig</context>
</contexts>
<marker>Louis, Nenkova, 2009</marker>
<rawString>Annie Louis and Ani Nenkova. 2009. Automatically evaluation content selection in summarization without human models. In Proc of EMNLP, pages 306– 314, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="11655" citStr="Nenkova and Passonneau, 2004" startWordPosition="1803" endWordPosition="1807">rvised summarization, rather than tune α, we set α = 0.5, so the concepts are considered in their totality (i.e., N1 + N2 concepts together) with no explicit favouring of one over the other that does not naturally fall out of concept frequency. N1 N2 (1−α) wibigrami+α wjnew conceptj i j 3.3 Results We evaluate output summaries using ROUGE-1, ROUGE-2, and ROUGE-SU4 (Lin, 2004), with no stemming and retaining all stopwords. These measures have been shown to correlate best with human judgments in general, but among the automatic measures, ROUGE-1 and ROUGE-2 also correlate best with the Pyramid (Nenkova and Passonneau, 2004; Nenkova et al., 2007) and Responsiveness manual metrics (Louis and Nenkova, 2009). Moreover, ROUGE-1 has been shown to best reflect human-automatic summary comparisons (Owczarzak et al., 2012). For single concept systems, the results are shown in Table 1, and concept combination system results are given in Table 2. We first note that our runs of the current distribution of icsisumm yield significantly worse ROUGE-2 results than reported in (Gillick and Favre, 2009) (see Table 1, BIGRAMS): 0.081 compared to 0.110 respectively. 842 On the TAC08 data, we observe no improvements over the baselin</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ani Nenkova</author>
</authors>
<title>Rebecca Passonneau,</title>
<location>and Kath-</location>
<marker>Nenkova, </marker>
<rawString>Ani Nenkova, Rebecca Passonneau, and Kath-</rawString>
</citation>
<citation valid="true">
<authors>
<author>leen McKeown</author>
</authors>
<title>The pyramid method:incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<marker>McKeown, 2007</marker>
<rawString>leen McKeown. 2007. The pyramid method:incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hitoshi Nishikawa</author>
<author>Takaaki Hasegawa</author>
<author>Yoshihiro Matsuo</author>
<author>Genichiro Kikui</author>
</authors>
<title>Opinion summarization with integer linear programming formulation for sentence extraction and ordering.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="14200" citStr="Nishikawa et al. (2010)" startWordPosition="2193" endWordPosition="2196">ent in performance. However, best ROUGE-1 scores are produced for both ECHR and WIKIPEDIA data with systems that incorporate semantic frame names. For WIKIPEDIA, best ROUGE-2 and ROUGE-SU4 scores incorporate named-entity information. 4 Related work Most researchers have used bigrams as concepts in coverage maximization-based approaches to unsupervised extractive summarization. Filatova and Hatzivassiloglou (2004), however,use relations between named entities as concepts in extractive summarization. They use slightly different extraction algorithms, but their work is similar in spirit to ours. Nishikawa et al. (2010), also, use opinions – tuples of targets, aspects, and polarity – as concepts in opinion summarization. In early work on summarization, Silber and McCoy (2000) used WordNet synsets as concepts. Kitajima and Kobayashi (2011) replace words by syntactic dependencies in the Maximal Marginal Relevance ECHR concept R-1 R-2 R-SU4 (95% conf.) (95% conf.) (95% conf.) BIGRAMS 0.544 0.204 0.266 (0.528-0.562) (0.195-0.215) (0.257-0.277) NER 0.549 0.184 0.254 (0.534-0.564) (0.174-0.193) (0.244-0.264) LDEP 0.609 0.225 0.293 (0.597-0.621) (0.217-0.235) (0.285-0.302) UDEP 0.612 0.227 0.295 (0.6-0.626) (0.218-</context>
</contexts>
<marker>Nishikawa, Hasegawa, Matsuo, Kikui, 2010</marker>
<rawString>Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo, and Genichiro Kikui. 2010. Opinion summarization with integer linear programming formulation for sentence extraction and ordering. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>John M Conroy</author>
<author>Hoa Trang Dang</author>
<author>Ani Nenkova</author>
</authors>
<title>An assessment of the accuracy of automatic evaluation in summarization.</title>
<date>2012</date>
<booktitle>In Proc of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization,</booktitle>
<pages>1--9</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="11849" citStr="Owczarzak et al., 2012" startWordPosition="1833" endWordPosition="1836"> not naturally fall out of concept frequency. N1 N2 (1−α) wibigrami+α wjnew conceptj i j 3.3 Results We evaluate output summaries using ROUGE-1, ROUGE-2, and ROUGE-SU4 (Lin, 2004), with no stemming and retaining all stopwords. These measures have been shown to correlate best with human judgments in general, but among the automatic measures, ROUGE-1 and ROUGE-2 also correlate best with the Pyramid (Nenkova and Passonneau, 2004; Nenkova et al., 2007) and Responsiveness manual metrics (Louis and Nenkova, 2009). Moreover, ROUGE-1 has been shown to best reflect human-automatic summary comparisons (Owczarzak et al., 2012). For single concept systems, the results are shown in Table 1, and concept combination system results are given in Table 2. We first note that our runs of the current distribution of icsisumm yield significantly worse ROUGE-2 results than reported in (Gillick and Favre, 2009) (see Table 1, BIGRAMS): 0.081 compared to 0.110 respectively. 842 On the TAC08 data, we observe no improvements over the baseline BIGRAM system for any ROUGE metric here. Hence, Gillick and Favre (2009) were right in their assumption that syntactic and semantic concepts would not lead to performance improvements, when re</context>
</contexts>
<marker>Owczarzak, Conroy, Dang, Nenkova, 2012</marker>
<rawString>Karolina Owczarzak, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2012. An assessment of the accuracy of automatic evaluation in summarization. In Proc of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1–9, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gregory Silber</author>
<author>Kathleen McCoy</author>
</authors>
<title>An efficient text summarizer using lexical chains.</title>
<date>2000</date>
<booktitle>In INLG.</booktitle>
<contexts>
<context position="14359" citStr="Silber and McCoy (2000)" startWordPosition="2219" endWordPosition="2222">A, best ROUGE-2 and ROUGE-SU4 scores incorporate named-entity information. 4 Related work Most researchers have used bigrams as concepts in coverage maximization-based approaches to unsupervised extractive summarization. Filatova and Hatzivassiloglou (2004), however,use relations between named entities as concepts in extractive summarization. They use slightly different extraction algorithms, but their work is similar in spirit to ours. Nishikawa et al. (2010), also, use opinions – tuples of targets, aspects, and polarity – as concepts in opinion summarization. In early work on summarization, Silber and McCoy (2000) used WordNet synsets as concepts. Kitajima and Kobayashi (2011) replace words by syntactic dependencies in the Maximal Marginal Relevance ECHR concept R-1 R-2 R-SU4 (95% conf.) (95% conf.) (95% conf.) BIGRAMS 0.544 0.204 0.266 (0.528-0.562) (0.195-0.215) (0.257-0.277) NER 0.549 0.184 0.254 (0.534-0.564) (0.174-0.193) (0.244-0.264) LDEP 0.609 0.225 0.293 (0.597-0.621) (0.217-0.235) (0.285-0.302) UDEP 0.612 0.227 0.295 (0.6-0.626) (0.218-0.238) (0.287-0.305) FRAMES 0.643 0.235 0.312 (0.63-0.657) (0.224-0.248) (0.302-0.323) TAC08 R-1 R-2 R-SU4 concept (95% conf.) (95% conf.) (95% conf.) BIGRAMS </context>
</contexts>
<marker>Silber, McCoy, 2000</marker>
<rawString>H. Gregory Silber and Kathleen McCoy. 2000. An efficient text summarizer using lexical chains. In INLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In Proc of EMNLP/CoNLL,</booktitle>
<pages>233--243</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1763" citStr="Woodsend and Lapata (2012)" startWordPosition="266" endWordPosition="269">cept i with binary variable ci, yielding the following coverage maximization objective, subject to the appropriate constraints: N wici (1) i In proposing bigrams as concepts for their system, Gillick and Favre (2009) explain that: [c]oncepts could be words, named entities, syntactic subtrees or semantic relations, for example. While deeper semantics make more appealing concepts, their extraction and weighting are much more error-prone. Any error in concept extraction can result in a biased objective function, leading to poor sentence selection. (Gillick and Favre, 2009) Several authors, e.g., Woodsend and Lapata (2012), and Li et al. (2013), have followed Gillick and Favre (2009) in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these. In this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts. Specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities. We show that using such concepts can lead to significant improvements in tex</context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proc of EMNLP/CoNLL, pages 233– 243, Jeju Island, Korea.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>