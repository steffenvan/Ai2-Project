<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000067">
<title confidence="0.936759">
Using Semantic and Syntactic Graphs for Call Classification
</title>
<author confidence="0.536209">
Dilek Hakkani-T¨ur Gokhan Tur
</author>
<affiliation confidence="0.320011">
AT&amp;T Labs – Research
</affiliation>
<address confidence="0.570322">
Florham Park, NJ, 07932
</address>
<email confidence="0.98494">
dtur,gtur @research.att.com
</email>
<author confidence="0.535136">
Ananlada Chotimongkol
</author>
<affiliation confidence="0.618995">
Carnegie Mellon University
</affiliation>
<address confidence="0.560707">
Pittsburgh, PA 15213
</address>
<email confidence="0.998637">
ananlada@cs.cmu.edu
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988975">
In this paper, we introduce a new data
representation format for language pro-
cessing, the syntactic and semantic graphs
(SSGs), and show its use for call classifi-
cation in spoken dialog systems. For each
sentence or utterance, these graphs in-
clude lexical information (words), syntac-
tic information (such as the part of speech
tags of the words and the syntactic parse of
the utterance), and semantic information
(such as the named entities and seman-
tic role labels). In our experiments, we
used written language as the training data
while computing SSGs and tested on spo-
ken language. In spite of this mismatch,
we have shown that this is a very promis-
ing approach for classifying complex ex-
amples, and by using SSGs it is possible
to reduce the call classification error rate
by 4.74% relative.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998427093023256">
Goal-oriented spoken dialog systems aim to iden-
tify intents of humans, expressed in natural lan-
guage, and take actions accordingly to satisfy their
requests. The intent of each speaker is identified
using a natural language understanding component.
This step can be seen as a multi-label, multi-class
call classification problem for customer care appli-
cations (Gorin et al., 1997; Chu-Carroll and Carpen-
ter, 1999; Gupta et al., To appear, among others).
As an example, consider the utterance I would like
to know my account balance, from a financial do-
main customer care application. Assuming that the
utterance is recognized correctly by the automatic
speech recognizer (ASR), the corresponding intent
(call-type) would be Request(Balance) and the ac-
tion would be telling the balance to the user after
prompting for the account number or routing this
call to the billing department.
Typically these application specific call-types are
pre-designed and large amounts of utterances man-
ually labeled with call-types are used for training
call classification systems. For classification, gen-
erally word -grams are used as features: In the
How May I Help You? (HMIHY) call routing sys-
tem, selected word -grams, namely salient phrases,
which are salient to certain call-types play an im-
portant role (Gorin et al., 1997). For instance, for
the above example, the salient phrase “account bal-
ance” is strongly associated with the call-type Re-
quest(Balance). Instead of using salient phrases, one
can leave the decision of determining useful features
(word -grams) to a classification algorithm used as
described in (Di Fabbrizio et al., 2002) and (Gupta
et al., To appear). An alternative would be using
a vector space model for classification where call-
types and utterances are represented as vectors in-
cluding word -grams (Chu-Carroll and Carpenter,
1999).
Call classification is similar to text categorization,
except the following:
The utterances are much shorter than typical
documents used for text categorization (such as
broadcast news or newspaper articles).
</bodyText>
<page confidence="0.987136">
24
</page>
<note confidence="0.99471">
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 24–31,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<figure confidence="0.988321">
&lt;bos&gt; WORD:I WORD:paid WORD:six WORD:dollacs &lt;eos&gt;
0 1 2 3 4 5 6
</figure>
<figureCaption confidence="0.999818">
Figure 1: An example utterance represented as a single path FSM.
</figureCaption>
<bodyText confidence="0.999803111111111">
Since it deals with spontaneous speech, the ut-
terances frequently include disfluencies or are
ungrammatical, and
ASR output is very noisy, typically one out of
every four words is misrecognized (Riccardi
and Hakkani-T¨ur, 2003).
Even though the shortness of the utterances may
imply the easiness of the call classification task, un-
fortunately this is not the case. The call classifi-
cation error rates typically range between 15% to
30% depending on the application (Gupta et al., To
appear). This is mainly due to the data sparseness
problem because of the nature of the input. Even for
simple call-types like Request(Balance), there are
many ways of uttering the same intent. For instance,
in one of the applications we used in our experi-
ments, as a response to the greeting prompt, there
are 2,697 unique utterances out of 3,547 utterances
for that call-type. Some examples include:
I would like to know my account balance
How much do I owe you
How much is my bill
What is my current bill
account balance
You can help me by telling me what my phone
bill is
...
Given this data sparseness, current classification ap-
proaches require an extensive amount of labeled data
in order to train a call classification system with a
reasonable performance. In this paper, we present
methods for extending the classifier’s feature set by
generalizing word sequences using syntactic and se-
mantic information represented in compact graphs,
called syntactic and semantic graphs (SSGs). For
each sentence or utterance, these graphs include
lexical information (words), syntactic information
(such as the part of speech tags of the words and the
syntactic parse of the utterance), and semantic in-
formation (such as the named entities and semantic
role labels). The generalization is expected to help
reduce the data sparseness problem by applying var-
ious groupings on word sequences. Furthermore, the
classifier is provided with additional syntactic and
semantic information which might be useful for the
call classification task.
In the following section, we describe the syntac-
tic and semantic graphs. In Section 3, we describe
our approach for call classification using SSGs. In
Section 4, we present the computation of syntactic
and semantic information for SSGs. In the last Sec-
tion, we present our experiments and results using
a spoken dialog system AT&amp;T VoiceTone ✞R Spoken
Dialog System (Gupta et al., To appear).
</bodyText>
<sectionHeader confidence="0.937361" genericHeader="introduction">
2 Semantic and Syntactic Graphs
</sectionHeader>
<bodyText confidence="0.999959">
Consider the typical case, where only lexical infor-
mation, i.e. word -grams are used for call classifi-
cation. This is equivalent to representing the words
in an utterance as a directed acyclic graph where
the words are the labels of the transitions and then
extracting the transition -grams from it. Figure 1
shows the graph for the example sentence I paid six
dollars, where bos and eos denote the begin-
ning and end of the sentence, respectively.
Syntactic and semantic graphs are also directed
acyclic graphs, formed by adding transitions encod-
ing syntactic and semantic categories of words or
word sequences to the word graph. The first addi-
tional information is the part of speech tags of the
words. In the graph, as a parallel transition for each
word of the utterance, the part of speech category
of the word is added, as shown in Figure 2 for the
example sentence. Note that, the word is prefixed
by the token WORD: and the part-of-speech tag is
prefixed by the token POS:, in order to distinguish
between different types of transitions in the graph.
The other type of information that is encoded in
these graphs is the syntactic parse of each utterance,
namely the syntactic phrases with their head words.
For example in the sentence I paid six dollars, six
dollars is a noun phrase with the head word dollars.
In Figure 2, the labels of the transitions for syntactic
phrases are prefixed by the token PHRASE:. There-
</bodyText>
<page confidence="0.998826">
25
</page>
<figureCaption confidence="0.9696175">
Figure 2: The SSG for the utterance I paid six dollars, where words (WORD:), part-of-speech tags (POS:),
syntactic parse (PHRASE:), named entities (NE:) and semantic roles (SRL:) are included.
</figureCaption>
<figure confidence="0.9405234">
E:E E:E
E:�
E:E E:E E:E
0 1 2 3
E:E
</figure>
<figureCaption confidence="0.9963225">
Figure 3: The FST used to extract unigram, bigram and trigrams. represents the alphabet, represents the
epsilon transition.
</figureCaption>
<figure confidence="0.997807391304348">
PHRASE:VP_paid
NE:m
POS:PRP
0 1
&lt;bos&gt;
WORD:I
SRL:pay.A0
POS:VBD
2 WORD:paid
SRL:pay.V
3
POS:CD
SRL:pay.A1
PHRASE:NP_dollars
POS:NNS
5
&lt;eos&gt;
6
PHRASE:NP_I
WORD:six
4
WORD:dollars
PHRASE:S_paid
</figure>
<bodyText confidence="0.999964764705882">
fore, six dollars is also represented by the transition
labeled PHRASE:NP dollars. As an alternative, one
may drop the head word of the phrase from the rep-
resentation, or insert an epsilon transition parallel to
the transitions of the modifiers of the head word to
eliminate them from some -grams.
Generic named entity tags, such as person, lo-
cation and organization names and task-dependent
named entity tags, such as drug names in a medical
domain, are also incorporated into the graph, where
applicable. For instance, for the example sentence,
six dollars is a monetary amount, so the arc NE:m is
inserted parallel to that sequence.
As another source of semantic information, se-
mantic role labels of the utterance components are
incorporated to the SSGs. The semantic role labels
represent the predicate/argument structure of each
sentence: Given a predicate, the goal is to identify
all of its arguments and their semantic roles. For
example, in the example sentence the predicate is
pay, the agent of this predicate is I and the amount
is six dollars. In the graph, the labels of the tran-
sitions for semantic roles are prefixed by the token
SRL: and the corresponding predicate. For exam-
ple, the sequence six dollars is the amount of the
predicate pay, and this is shown by the transition
with label SRL:pay.A1 following the PropBank no-
tation (Kingsbury et al., 2002)1.
In this work, we were only able to incorporate
part-of-speech tags, syntactic parses, named entity
tags and semantic role labels in the syntactic and se-
mantic graphs. Insertion of further information such
as supertags (Bangalore and Joshi, 1999) or word
stems can also be beneficial for further processing.
</bodyText>
<sectionHeader confidence="0.715958" genericHeader="method">
3 Using SSGs for Call Classification
</sectionHeader>
<bodyText confidence="0.978242333333333">
In this paper we propose extracting all -grams from
the SSGs to use them for call classification. The -
grams in an utterance SSG can be extracted by con-
verting it to a finite state transducer (FST), . Each
transition of has the labels of the arcs on the SSG
as input and output symbols2. Composing this FST
with another FST, , representing all the possible
-grams, forms the FST, , which includes all -
grams in the SSG:
</bodyText>
<footnote confidence="0.8879038">
1A1 or Arg1 indicates the object of the predicate, in this case
the amount.
2Instead of the standard notation where “:” is used to sepa-
rate the input and output symbols in finite state transducers, we
use “:” to separate the type of the token and its value.
</footnote>
<page confidence="0.994342">
26
</page>
<bodyText confidence="0.999755375">
Then, extracting the -grams in the SSG is equiva-
lent to enumerating all paths of . For ,
is shown in Figure 3. The alphabet contains all
the symbols in .
We expect the SSGs to help call classification be-
cause of the following reasons:
First of all, the additional information is ex-
pected to provide some generalization, by al-
lowing new -grams to be encoded in the utter-
ance graph since SSGs provide syntactic and
semantic groupings. For example, the words
a and the both have the part-of-speech tag
category DT (determiner), or all the numbers
are mapped to a cardinal number (CD), like
the six in the example sentence. So the bi-
grams WORD:six WORD:dollars and POS:CD
WORD:dollars will both be in the SSG. Simi-
larly the sentences I paid six dollars and I paid
seventy five dollars and sixty five cents will both
have the trigram WORD:I WORD:paid NE:m in
their SSGs.
The head words of the syntactic phrases and
predicate of the arguments are included in the
SSGs. This enables the classifier to handle long
distance dependencies better than using other
simpler methods, such as extracting all gappy
-grams. For example, consider the following
two utterances: I need a copy of my bill and
I need a copy of a past due bill. As shown
in Figures 4 and 5, the -gram WORD:copy
WORD:of PHRASE:NP bill appears for both
utterances, since both subsequences my bill and
a past due bill are nothing but noun phrases
with the head word bill.
Another motivation is that, when using simply
the word -grams in an utterance, the classi-
fier is only given lexical information. Now the
classifier is provided with more and different
information using these extra syntactic and se-
mantic features. For example, a named entity
of type monetary amount may be strongly as-
sociated with some call-type.
Furthermore, there is a close relationship be-
tween the call-types and semantic roles. For
example, if the predicate is order this is most
probably the call-type Order(Item) in a retail
domain application. The simple -gram ap-
proach would consider all the appearances of
the unigram order as equal. However consider
the utterance I’d like to check an order of a dif-
ferent call-type, where the order is not a pred-
icate but an object. Word -gram features will
fail to capture this distinction.
Once the SSG of an utterance is formed, all the
-grams are extracted as features, and the decision
of which one to select/use is left to the classifier.
</bodyText>
<sectionHeader confidence="0.879425" genericHeader="method">
4 Computation of the SSGs
</sectionHeader>
<bodyText confidence="0.999701833333333">
In this section, the tools used to compute the in-
formation in SSGs are described and their perfor-
mances on manually transcribed spoken dialog ut-
terances are presented. All of these components may
be improved independently, for the specific applica-
tion domain.
</bodyText>
<subsectionHeader confidence="0.994766">
4.1 Part-of-Speech Tagger
</subsectionHeader>
<bodyText confidence="0.99995572">
Part-of-speech tagging has been very well studied
in the literature for many languages, and the ap-
proaches vary from rule-based to HMM-based and
classifier-based (Church, 1988; Brill, 1995, among
others) tagging. In our framework, we employ a
simple HMM-based tagger, where the most prob-
able tag sequence, , given the words, , is out-
put (Weischedel et al., 1993):
Since we do not have enough data which is manually
tagged with part-of-speech tags for our applications,
we used Penn Treebank (Marcus et al., 1994) as our
training set. Penn Treebank includes data from Wall
Street Journal, Brown, ATIS, and Switchboard cor-
pora. The final two sets are the most useful for our
domain, since they are also from spoken language
and include disfluencies. As a test set, we manu-
ally labeled 2,000 words of user utterances from an
AT&amp;T VoiceTone spoken dialog system application,
and we achieved an accuracy of 94.95% on manu-
ally transcribed utterances. When we examined the
errors, we have seen that the frequent word please
is mis-labeled or frequently occurs as a verb in the
training data, even when it is not. Given that the lat-
est literature on POS tagging using Penn Treebank
reports an accuracy of around 97% with in-domain
</bodyText>
<page confidence="0.982381">
27
</page>
<figure confidence="0.802394">
PHRASE:VP_need
</figure>
<figureCaption confidence="0.999926">
Figure 4: An example SSG for the utterance I need a copy of my bill.
Figure 5: An example SSG for the utterance I need a copy of a past due bill.
</figureCaption>
<figure confidence="0.999329859649123">
POS:PRP
POS:VBP
PHRASE:NP-A_copy
SRL:need.A1
WORD:I
WORD:need
2 3
POS:DT
PHRASE:PP_of
POS:NN &lt;eos&gt;
SRL:need.V WORD:a 4 PHRASE:NP_bill 8 9
&lt;bos&gt;
0 1
SRL:need.A0
PHRASE:NP_I
PHRASE:NP_copy
WORD:copy
5
WORD:of
POS:IN
POS:PRP$
6
WORD:my
7
WORD:bill
POS:NN
PHRASE:S_need
SRL:need.A1
PHRASE:NP-A_copy
POS:PRP
POS:VBP
WORD:need
POS:DT
WORD:I
2
3
&lt;bos&gt;
0 1
SRL:need.A0
PHRASE:NP_I
PHRASE:NP_copy
POS:NN
WORD:of
6
POS:DT
POS:NN PHRASE:PP_of &lt;eos&gt;
SRL:need.V 4 10 11
WORD:a
PHRASE:NP_bill
WORD:copy 5 POS:IN
WORD:bill
POS:JJ POS:JJ
7 8 9
WORD:a WORD:past WORD:due
PHRASE:S_need
PHRASE:VP_need
a very reasonable performan
</figure>
<bodyText confidence="0.949071111111111">
ce, considering these er-
rors.
For syntactic parsing, we use the
parser (Collins, 1999), which is reported to
give over 88% labeled recall and precision on
Wall Street Journal portion of the Penn Treebank.
chunklink script to extract
information from the parse trees3. Since we do not
have any data from our domain, we do not have a
performan
Collins’
Buchholz’s
ce figure for this task for our domain.
name finder (Bikel et al., 1999), and a
classifier-based tagger using Boostexter (Schapire
and Singer, 2000). In the simple HMM-based ap-
proach, which is the same as the part-of-speech tag-
ging, the goal is to find the tag sequence, ,which
maximizes for the word sequence, .The
tags in this case are named entity categories (such
as P and p for Person names, O and o for Orga-
nization names, etc. where upper-case indicates
the first word in the named entity) or NA if the
word is not a part
BBN’s
of a named entity. In the sim-
plified version of BBN’s name finder, the states of
</bodyText>
<footnote confidence="0.7809625">
3http://ilk.kub.nl/ sabine/chunklink/chunklink 2-2-
2000 for conll.pl
</footnote>
<bodyText confidence="0.977137769230769">
28 the model were word/tag combinations, where the
tag for word is the named entity category of
each word. Transition probabilities consisted of tri
-
gram probabilities
over these combined tokens. In the final version,
we extended this model with an unknown words
model (Hakkani-T¨ur et al., 1999). In the classifier-
based approach, we used simple features such as the
current word and surrounding 4 words, binary tags
indicating if the word considered contains any dig-
its or is formed from digits, and features checking
capitalization (Carreras et al., 2003).
To test these approaches, we have used data from
an AT&amp;T VoiceTone spoken dialog system applica-
tion for a pharmaceutical domain, where some of
the named entity categories were person, organiza-
tion, drug name, prescription number, and date. The
training and test sets contained around 11,000 and
5,000 utterances, respectively. Table 1 summarizes
the overall F-measure results as well as F-measure
for the most frequent named entity categories. Over-
all, the classifier based approach resulted in the best
performance, so it is also used for the call classifica-
tion experiments.
training data (van Halteren et al., 2001), we achieve
</bodyText>
<subsectionHeader confidence="0.984118">
4.2 Syntactic Parser
</subsectionHeader>
<bodyText confidence="0.877751">
We use
</bodyText>
<subsectionHeader confidence="0.727757">
4.3 Named Entity Extractor
</subsectionHeader>
<bodyText confidence="0.998706">
For named entity extraction, we tried using a sim-
ple HMM-based approach, a simplified version of
</bodyText>
<subsectionHeader confidence="0.995838">
4.4 Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.834993666666667">
The goal of semantic role labeling is to extract all
the constituents which fill a semantic role of a tar-
get verb. Typical semantic arguments include Agent,
Patient, Instrument, etc. an
d also adjuncts such as
Locative, Temporal, Manner, Cause, etc. In this
</bodyText>
<table confidence="0.998846">
Category Count EMM IF Boostexter
Org. 132 62.0 73.8 70.9
Person 150 45.0 62.4 54.4
Date 178 51.4 61.9 72.0
Drug 220 65.7 62.3 63.1
Overall 836 54.5 56.8 64.0
</table>
<tableCaption confidence="0.994459">
Table 1: F-Measure results for named entity extrac-
</tableCaption>
<bodyText confidence="0.9943345">
tion with various approaches. HMM is the sim-
ple HMM-based approach, IF is the simplified ver-
sion of BBN’s name finder with an unknown words
model.
work, we use the semantic roles and annotations
from the PropBank corpus (Kingsbury et al., 2002),
where the arguments are given mnemonic names,
such as Arg0, Arg1, Arg-LOC, etc. For example,
for the sentence I have bought myself a blue jacket
from your summer catalog for twenty five dollars last
week, the agent (buyer, or Arg0) is I, the predicate
is buy, the thing bought (Arg1) is a blue jacket, the
seller or source (Arg2) is from your summer catalog,
the price paid (Arg3) is twenty five dollars, the bene-
factive (Arg4) is myself, and the date (ArgM-TMP)
is last week4.
Semantic role labeling can be viewed as a multi-
class classification problem. Given a word (or
phrase) and its features, the goal is to output the
most probable semantic label. For semantic role la-
beling, we have used the exact same feature set that
Hacioglu et al. (2004) have used, since their sys-
tem performed the best among others in the CoNLL-
2004 shared task (Carreras and M`arquez, 2004).
We have used Boostexter (Schapire and Singer,
2000) as the classifier. The features include token-
level features (such as the current (head) word, its
part-of-speech tag, base phrase type and position,
etc.), predicate-level features (such as the predicate’s
lemma, frequency, part-of-speech tag, etc.) and
argument-level features which capture the relation-
ship between the token (head word/phrase) and the
predicate (such as the syntactic path between the to-
ken and the predicate, their distance, token position
relative to the predicate, etc.).
In order to evaluate the performance of semantic
role labeling, we have manually annotated 285 utter-
ances from an AT&amp;T VoiceTone spoken dialog sys-
</bodyText>
<footnote confidence="0.898228">
4See http://www.cis.upenn.edu/ dgildea/Verbs for more
details
</footnote>
<bodyText confidence="0.99990105">
tem application for a retail domain. The utterances
include 645 predicates (2.3 predicates/utterance).
First we have computed recall and precision rates for
evaluating the predicate identification performance.
The precision is found to be 93.04% and recall is
91.16%. More than 90% of false alarms for pred-
icate extraction are due to the word please, which
is very frequent in customer care domain and erro-
neously tagged as explained above. Most of the false
rejections are due to disfluencies and ungrammatical
utterances. For example in the utterance I’d like to
order place an order, the predicate place is tagged
as a noun erroneously, probably because of the pre-
ceding verb order. Then we have evaluated the argu-
ment labeling performance. We have used a stricter
measure than the CoNLL-2004 shared task. The la-
beling is correct if both the boundary and the role of
all the arguments of a predicate are correct. In our
test set, we have found out that our SRL tool cor-
rectly tags all arguments of 57.4% of the predicates.
</bodyText>
<sectionHeader confidence="0.995021" genericHeader="method">
5 Call Classification Experiments and
Results
</sectionHeader>
<bodyText confidence="0.999836461538462">
In order to evaluate our approach, we carried out call
classification experiments using human-machine di-
alogs collected by the spoken dialog system used
for customer care. We have only considered utter-
ances which are responses to the greeting prompt
How may I help you? in order not to deal with confir-
mation and clarification utterances. We first describe
this data, and then give the results obtained by the se-
mantic classifier. We have performed our tests using
the Boostexter tool, an implementation of the Boost-
ing algorithm, which iteratively selects the most dis-
criminative features for a given task (Schapire and
Singer, 2000).
</bodyText>
<subsectionHeader confidence="0.990356">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999898166666667">
Table 2 summarizes the characteristics of our appli-
cation including the amount of training and test data,
total number of call-types, average utterance length,
and call-type perplexity. Perplexity is computed us-
ing the prior distribution over all the call-types in the
training data.
</bodyText>
<subsectionHeader confidence="0.80075">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.966796">
For call classification, we have generated SSGs for
the training and test set utterances using the tools
</bodyText>
<page confidence="0.995698">
29
</page>
<table confidence="0.9999274">
Training Data Size 3,725 utterances
Test Data Size 1,954 utterances
Number of Call-Types 79
Call-Type Perplexity 28.86
Average Utterance Length 12.08 words
</table>
<tableCaption confidence="0.9707955">
Table 2: Characteristics of the data used in the ex-
periments.
</tableCaption>
<table confidence="0.9999196">
Baseline Using SSG Increase
Unigram 2,303 6,875 2.99 times
Bigram 15,621 112,653 7.21 times
Trigram 34,185 705,673 20.64 times
Total 52,109 825,201 15.84 times
</table>
<tableCaption confidence="0.9998">
Table 3: A comparison of number of features.
</tableCaption>
<bodyText confidence="0.999831333333333">
described above. When -grams are extracted from
these SSGs, instead of the word graphs (Baseline),
there is a huge increase in the number of features
given to the classifier, as seen in Table 3. The clas-
sifier has now 15 times more features to work with.
Although one can apply a feature selection approach
before classification as frequently done in the ma-
chine learning community, we left the burden of an-
alyzing 825,201 features to the classifier.
Table 4 presents the percentage of the features se-
lected by Boostexter using SSGs for each informa-
tion category. As expected the lexical information is
the most frequently used, and 54.06% of the selected
features have at least one word in its -gram. The to-
tal is more than 100%, since some features contain
more than one category, as in the bigram feature ex-
ample: POS:DT WORD:bill. This shows the use of
other information sources as well as words.
Table 5 presents our results for call classification.
As the evaluation metric, we use the top class error
rate (TCER), which is the ratio of utterances, where
the top scoring call-type is not one of the true call-
types assigned to each utterance by the human la-
belers. The baseline TCER on the test set using only
word -grams is 23.80%. When we extract features
from the SSGs, we see a 2.14% relative decrease in
the error rate down to 23.29%. When we analyze
these results, we have seen that:
For “easy to classify” utterances, the classifier
already assigns a high score to the true call-type
</bodyText>
<table confidence="0.998443166666667">
Category Frequency
Lexical Words 54.06%
Syntactic Part-of-Speech 49.98%
Syntactic Parse 27.10%
Semantic Named Entity 1.70%
Semantic Role Label 11.74%
</table>
<tableCaption confidence="0.9937175">
Table 4: The percentage of the features selected by
the classifier for each information category
</tableCaption>
<table confidence="0.999862333333333">
Baseline SSGs Decrease
All utterances 23.80% 23.29% 2.14%
Low confidence 68.77% 62.16% 9.61%
utterances
All utterances 23.80% 22.67% 4.74%
(Cascaded)
</table>
<tableCaption confidence="0.9849495">
Table 5: Call classification error rates using words
and SSGs.
</tableCaption>
<bodyText confidence="0.993015884615385">
using just word -grams.
The syntactic and semantic features extracted
from the SSGs are not 100% accurate, as pre-
sented earlier. So, although many of these fea-
tures have been useful, there is certain amount
of noise introduced in the call classification
training data.
The particular classifier we use, namely Boost-
ing, is known to handle large feature spaces
poorer than some others, such as SVMs. This
is especially important with 15 times more fea-
tures.
Due to this analysis, we have focused on a sub-
set of utterances, namely utterances with low confi-
dence scores, i.e. cases where the score given to the
top scoring call-type by the baseline model is be-
low a certain threshold. In this subset we had 333
utterances, which is about 17% of the test set. As
expected the error rates are much higher than the
overall and we get much larger improvement in per-
formance when we use SSGs. The baseline for this
set is 68.77%, and using extra features, this reduces
to 62.16% which is a 9.61% relative reduction in the
error rate.
This final experiment suggests a cascaded ap-
proach for exploiting SSGs for call classification.
</bodyText>
<page confidence="0.993769">
30
</page>
<bodyText confidence="0.999935">
That is, first the baseline word -gram based clas-
sifier is used to classify all the utterances, then if
this model fails to commit on a call-type, we per-
form extra feature extraction using SSGs, and use
the classification model trained with SSGs. This cas-
caded approach reduced the overall error rate of all
utterances from 23.80% to 22.67%, which is 4.74%
relative reduction in error rate.
</bodyText>
<sectionHeader confidence="0.999463" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999772428571429">
In this paper, we have introduced syntactic and se-
mantic graphs (SSGs) for speech and language pro-
cessing. We have described their use for the task of
call classification. We have presented results show-
ing 4.74% improvement, using utterances collected
from AT&amp;T VoiceTone spoken dialog system. SSGs
can also be useful for text classification and other
similar language processing applications. Our fu-
ture work includes feature selection prior to classifi-
cation and developing methods that are more robust
to ASR errors while computing the SSGs. We also
plan to improve the syntactic and semantic process-
ing components by adapting the models with some
amount of labeled in-domain spoken dialog data.
</bodyText>
<sectionHeader confidence="0.999224" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999599658536585">
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Compu-
tational Linguistics, 25(2), June.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what’s
in a name. Machine Learning Journal Special Issue
on Natural Language Learning, 34(1-3):211–231.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4):543–565, December.
Xavier Carreras and Llu´ıs M`arquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role label-
ing. In Proceedings of the Conference on Computa-
tional Natural Language Learning (CoNLL), Boston,
MA, May.
Xavier Carreras, Llu´ıs M`arquez, and Llu´ıs Padr´o. 2003.
A simple named entity extractor using AdaBoost.
In Proceedings of the Conference on Computational
Natural Language Learning (CoNLL), Edmonton,
Canada.
Jennifer Chu-Carroll and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational
Linguistics, 25(3):361–388.
Kenneth W. Church. 1988. A stochastic parts program
and noun phrase parser for unrestricted text. In Second
Conference on Applied Natural Language Processing
(ANLP), pages 136–143, Austin, Texas.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Computer and Information Sci-
ence, Philadelphia, PA.
Giuseppe Di Fabbrizio , Dawn Dutton, Narendra Gupta,
Barbara Hollister, Mazin Rahim, Giuseppe Riccardi,
Robert Schapire, and Juergen Schroeter. 2002. AT&amp;T
help desk. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), Den-
ver, CO, September.
Allen L. Gorin, Giuseppe Riccardi, and Jerry H. Wright.
1997. How May I Help You? . Speech Communica-
tion, 23:113–127.
Narendra Gupta, Gokhan Tur, Dilek Hakkani-T¨ur, Srini-
vas Bangalore, Giuseppe Riccardi, and Mazin Rahim.
To appear. The AT&amp;T spoken language understand-
ing system. IEEE Transactions on Speech and Audio
Processing.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Dan Jurafsky. 2004. Semantic role label-
ing by tagging syntactic chunks. In Proceedings of
the Conference on Computational Natural Language
Learning (CoNLL), Boston, MA, May.
Dilek Hakkani-T¨ur, Gokhan Tur, Andreas Stolcke, and
Elizabeth Shriberg. 1999. Combining words and
prosody for information extraction from speech. In
Proceedings of the EUROSPEECH’99, European
Conference on Speech Communication and Technol-
ogy, Budapest, Hungary, September.
Paul Kingsbury, Mitch Marcus, and Martha Palmer.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference (HLT), San Diego, CA, March.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Giuseppe Riccardi and Dilek Hakkani-T¨ur. 2003. Ac-
tive and unsupervised learning for automatic speech
recognition. In Proceedings of the European Confer-
ence on Speech Communication and Technology (EU-
ROSPEECH), Geneva, Switzerland, September.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2-3):135–168.
Hans van Halteren, Jakub Zavrel, and Walter Daele-
mans. 2001. Improving accuracy in word class tag-
ging through combination of machine learning sys-
tems. Computational Linguistics, 27(2):199–230.
Ralph Weischedel, Richard Schwartz, Jeff Palmucci,
Marie Meteer, and Lance Ramshaw. 1993. Coping
with ambiguity and unknown words through proba-
bilistic models. Computational Linguistics, SpecialIs-
sue on Using Large Corpora, 19(2):361–382, June.
</reference>
<page confidence="0.999913">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.632459">
<title confidence="0.999989">Using Semantic and Syntactic Graphs for Call Classification</title>
<author confidence="0.998511">Dilek Hakkani-T¨ur Gokhan Tur</author>
<affiliation confidence="0.999671">AT&amp;T Labs – Research</affiliation>
<address confidence="0.999714">Florham Park, NJ, 07932</address>
<email confidence="0.988899">dtur,gtur@research.att.com</email>
<author confidence="0.8291">Ananlada</author>
<affiliation confidence="0.997511">Carnegie Mellon</affiliation>
<address confidence="0.791685">Pittsburgh, PA</address>
<email confidence="0.998861">ananlada@cs.cmu.edu</email>
<abstract confidence="0.998925904761905">In this paper, we introduce a new data representation format for language processing, the syntactic and semantic graphs (SSGs), and show its use for call classification in spoken dialog systems. For each sentence or utterance, these graphs include lexical information (words), syntactic information (such as the part of speech tags of the words and the syntactic parse of the utterance), and semantic information (such as the named entities and semantic role labels). In our experiments, we used written language as the training data while computing SSGs and tested on spoken language. In spite of this mismatch, we have shown that this is a very promising approach for classifying complex examples, and by using SSGs it is possible to reduce the call classification error rate by 4.74% relative.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="9466" citStr="Bangalore and Joshi, 1999" startWordPosition="1523" endWordPosition="1526">he agent of this predicate is I and the amount is six dollars. In the graph, the labels of the transitions for semantic roles are prefixed by the token SRL: and the corresponding predicate. For example, the sequence six dollars is the amount of the predicate pay, and this is shown by the transition with label SRL:pay.A1 following the PropBank notation (Kingsbury et al., 2002)1. In this work, we were only able to incorporate part-of-speech tags, syntactic parses, named entity tags and semantic role labels in the syntactic and semantic graphs. Insertion of further information such as supertags (Bangalore and Joshi, 1999) or word stems can also be beneficial for further processing. 3 Using SSGs for Call Classification In this paper we propose extracting all -grams from the SSGs to use them for call classification. The - grams in an utterance SSG can be extracted by converting it to a finite state transducer (FST), . Each transition of has the labels of the arcs on the SSG as input and output symbols2. Composing this FST with another FST, , representing all the possible -grams, forms the FST, , which includes all - grams in the SSG: 1A1 or Arg1 indicates the object of the predicate, in this case the amount. 2In</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<journal>Machine Learning Journal Special Issue on Natural Language Learning,</journal>
<pages>34--1</pages>
<contexts>
<context position="15410" citStr="Bikel et al., 1999" startWordPosition="2534" endWordPosition="2537">PP_of &lt;eos&gt; SRL:need.V 4 10 11 WORD:a PHRASE:NP_bill WORD:copy 5 POS:IN WORD:bill POS:JJ POS:JJ 7 8 9 WORD:a WORD:past WORD:due PHRASE:S_need PHRASE:VP_need a very reasonable performan ce, considering these errors. For syntactic parsing, we use the parser (Collins, 1999), which is reported to give over 88% labeled recall and precision on Wall Street Journal portion of the Penn Treebank. chunklink script to extract information from the parse trees3. Since we do not have any data from our domain, we do not have a performan Collins’ Buchholz’s ce figure for this task for our domain. name finder (Bikel et al., 1999), and a classifier-based tagger using Boostexter (Schapire and Singer, 2000). In the simple HMM-based approach, which is the same as the part-of-speech tagging, the goal is to find the tag sequence, ,which maximizes for the word sequence, .The tags in this case are named entity categories (such as P and p for Person names, O and o for Organization names, etc. where upper-case indicates the first word in the named entity) or NA if the word is not a part BBN’s of a named entity. In the simplified version of BBN’s name finder, the states of 3http://ilk.kub.nl/ sabine/chunklink/chunklink 2-2- 2000</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning Journal Special Issue on Natural Language Learning, 34(1-3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="13166" citStr="Brill, 1995" startWordPosition="2168" endWordPosition="2169">l the -grams are extracted as features, and the decision of which one to select/use is left to the classifier. 4 Computation of the SSGs In this section, the tools used to compute the information in SSGs are described and their performances on manually transcribed spoken dialog utterances are presented. All of these components may be improved independently, for the specific application domain. 4.1 Part-of-Speech Tagger Part-of-speech tagging has been very well studied in the literature for many languages, and the approaches vary from rule-based to HMM-based and classifier-based (Church, 1988; Brill, 1995, among others) tagging. In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, , given the words, , is output (Weischedel et al., 1993): Since we do not have enough data which is manually tagged with part-of-speech tags for our applications, we used Penn Treebank (Marcus et al., 1994) as our training set. Penn Treebank includes data from Wall Street Journal, Brown, ATIS, and Switchboard corpora. The final two sets are the most useful for our domain, since they are also from spoken language and include disfluencies. As a test set, we manually labeled 2,000</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543–565, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<location>Boston, MA,</location>
<marker>Carreras, M`arquez, 2004</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL), Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
<author>Llu´ıs Padr´o</author>
</authors>
<title>A simple named entity extractor using AdaBoost.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<location>Edmonton, Canada.</location>
<marker>Carreras, M`arquez, Padr´o, 2003</marker>
<rawString>Xavier Carreras, Llu´ıs M`arquez, and Llu´ıs Padr´o. 2003. A simple named entity extractor using AdaBoost. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Bob Carpenter</author>
</authors>
<title>Vectorbased natural language call routing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="1486" citStr="Chu-Carroll and Carpenter, 1999" startWordPosition="227" endWordPosition="231">e of this mismatch, we have shown that this is a very promising approach for classifying complex examples, and by using SSGs it is possible to reduce the call classification error rate by 4.74% relative. 1 Introduction Goal-oriented spoken dialog systems aim to identify intents of humans, expressed in natural language, and take actions accordingly to satisfy their requests. The intent of each speaker is identified using a natural language understanding component. This step can be seen as a multi-label, multi-class call classification problem for customer care applications (Gorin et al., 1997; Chu-Carroll and Carpenter, 1999; Gupta et al., To appear, among others). As an example, consider the utterance I would like to know my account balance, from a financial domain customer care application. Assuming that the utterance is recognized correctly by the automatic speech recognizer (ASR), the corresponding intent (call-type) would be Request(Balance) and the action would be telling the balance to the user after prompting for the account number or routing this call to the billing department. Typically these application specific call-types are pre-designed and large amounts of utterances manually labeled with call-type</context>
<context position="2923" citStr="Chu-Carroll and Carpenter, 1999" startWordPosition="453" endWordPosition="456">ient phrases, which are salient to certain call-types play an important role (Gorin et al., 1997). For instance, for the above example, the salient phrase “account balance” is strongly associated with the call-type Request(Balance). Instead of using salient phrases, one can leave the decision of determining useful features (word -grams) to a classification algorithm used as described in (Di Fabbrizio et al., 2002) and (Gupta et al., To appear). An alternative would be using a vector space model for classification where calltypes and utterances are represented as vectors including word -grams (Chu-Carroll and Carpenter, 1999). Call classification is similar to text categorization, except the following: The utterances are much shorter than typical documents used for text categorization (such as broadcast news or newspaper articles). 24 Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 24–31, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics &lt;bos&gt; WORD:I WORD:paid WORD:six WORD:dollacs &lt;eos&gt; 0 1 2 3 4 5 6 Figure 1: An example utterance represented as a single path FSM. Since it deals with spontaneous speech, the utterances frequently include disfluencies o</context>
</contexts>
<marker>Chu-Carroll, Carpenter, 1999</marker>
<rawString>Jennifer Chu-Carroll and Bob Carpenter. 1999. Vectorbased natural language call routing. Computational Linguistics, 25(3):361–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Second Conference on Applied Natural Language Processing (ANLP),</booktitle>
<pages>136--143</pages>
<location>Austin, Texas.</location>
<contexts>
<context position="13153" citStr="Church, 1988" startWordPosition="2166" endWordPosition="2167"> is formed, all the -grams are extracted as features, and the decision of which one to select/use is left to the classifier. 4 Computation of the SSGs In this section, the tools used to compute the information in SSGs are described and their performances on manually transcribed spoken dialog utterances are presented. All of these components may be improved independently, for the specific application domain. 4.1 Part-of-Speech Tagger Part-of-speech tagging has been very well studied in the literature for many languages, and the approaches vary from rule-based to HMM-based and classifier-based (Church, 1988; Brill, 1995, among others) tagging. In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, , given the words, , is output (Weischedel et al., 1993): Since we do not have enough data which is manually tagged with part-of-speech tags for our applications, we used Penn Treebank (Marcus et al., 1994) as our training set. Penn Treebank includes data from Wall Street Journal, Brown, ATIS, and Switchboard corpora. The final two sets are the most useful for our domain, since they are also from spoken language and include disfluencies. As a test set, we manually </context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Second Conference on Applied Natural Language Processing (ANLP), pages 136–143, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania, Computer and Information Science,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="15062" citStr="Collins, 1999" startWordPosition="2475" endWordPosition="2476">of POS:NN &lt;eos&gt; SRL:need.V WORD:a 4 PHRASE:NP_bill 8 9 &lt;bos&gt; 0 1 SRL:need.A0 PHRASE:NP_I PHRASE:NP_copy WORD:copy 5 WORD:of POS:IN POS:PRP$ 6 WORD:my 7 WORD:bill POS:NN PHRASE:S_need SRL:need.A1 PHRASE:NP-A_copy POS:PRP POS:VBP WORD:need POS:DT WORD:I 2 3 &lt;bos&gt; 0 1 SRL:need.A0 PHRASE:NP_I PHRASE:NP_copy POS:NN WORD:of 6 POS:DT POS:NN PHRASE:PP_of &lt;eos&gt; SRL:need.V 4 10 11 WORD:a PHRASE:NP_bill WORD:copy 5 POS:IN WORD:bill POS:JJ POS:JJ 7 8 9 WORD:a WORD:past WORD:due PHRASE:S_need PHRASE:VP_need a very reasonable performan ce, considering these errors. For syntactic parsing, we use the parser (Collins, 1999), which is reported to give over 88% labeled recall and precision on Wall Street Journal portion of the Penn Treebank. chunklink script to extract information from the parse trees3. Since we do not have any data from our domain, we do not have a performan Collins’ Buchholz’s ce figure for this task for our domain. name finder (Bikel et al., 1999), and a classifier-based tagger using Boostexter (Schapire and Singer, 2000). In the simple HMM-based approach, which is the same as the part-of-speech tagging, the goal is to find the tag sequence, ,which maximizes for the word sequence, .The tags in </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Computer and Information Science, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dawn Dutton</author>
<author>Narendra Gupta</author>
<author>Barbara Hollister</author>
<author>Mazin Rahim</author>
<author>Giuseppe Riccardi</author>
<author>Robert Schapire</author>
<author>Juergen Schroeter</author>
</authors>
<title>AT&amp;T help desk.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<location>Denver, CO,</location>
<marker>Dutton, Gupta, Hollister, Rahim, Riccardi, Schapire, Schroeter, 2002</marker>
<rawString>Giuseppe Di Fabbrizio , Dawn Dutton, Narendra Gupta, Barbara Hollister, Mazin Rahim, Giuseppe Riccardi, Robert Schapire, and Juergen Schroeter. 2002. AT&amp;T help desk. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allen L Gorin</author>
<author>Giuseppe Riccardi</author>
<author>Jerry H Wright</author>
</authors>
<title>How May I Help You?</title>
<date>1997</date>
<journal>Speech Communication,</journal>
<pages>23--113</pages>
<contexts>
<context position="1453" citStr="Gorin et al., 1997" startWordPosition="223" endWordPosition="226">en language. In spite of this mismatch, we have shown that this is a very promising approach for classifying complex examples, and by using SSGs it is possible to reduce the call classification error rate by 4.74% relative. 1 Introduction Goal-oriented spoken dialog systems aim to identify intents of humans, expressed in natural language, and take actions accordingly to satisfy their requests. The intent of each speaker is identified using a natural language understanding component. This step can be seen as a multi-label, multi-class call classification problem for customer care applications (Gorin et al., 1997; Chu-Carroll and Carpenter, 1999; Gupta et al., To appear, among others). As an example, consider the utterance I would like to know my account balance, from a financial domain customer care application. Assuming that the utterance is recognized correctly by the automatic speech recognizer (ASR), the corresponding intent (call-type) would be Request(Balance) and the action would be telling the balance to the user after prompting for the account number or routing this call to the billing department. Typically these application specific call-types are pre-designed and large amounts of utterance</context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>Allen L. Gorin, Giuseppe Riccardi, and Jerry H. Wright. 1997. How May I Help You? . Speech Communication, 23:113–127.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Narendra Gupta</author>
<author>Gokhan Tur</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Srinivas Bangalore</author>
<author>Giuseppe Riccardi</author>
<author>Mazin Rahim</author>
</authors>
<title>To appear. The AT&amp;T spoken language understanding system.</title>
<booktitle>IEEE Transactions on Speech and Audio Processing.</booktitle>
<marker>Gupta, Tur, Hakkani-T¨ur, Bangalore, Riccardi, Rahim, </marker>
<rawString>Narendra Gupta, Gokhan Tur, Dilek Hakkani-T¨ur, Srinivas Bangalore, Giuseppe Riccardi, and Mazin Rahim. To appear. The AT&amp;T spoken language understanding system. IEEE Transactions on Speech and Audio Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Semantic role labeling by tagging syntactic chunks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<location>Boston, MA,</location>
<contexts>
<context position="18856" citStr="Hacioglu et al. (2004)" startWordPosition="3114" endWordPosition="3117">elf a blue jacket from your summer catalog for twenty five dollars last week, the agent (buyer, or Arg0) is I, the predicate is buy, the thing bought (Arg1) is a blue jacket, the seller or source (Arg2) is from your summer catalog, the price paid (Arg3) is twenty five dollars, the benefactive (Arg4) is myself, and the date (ArgM-TMP) is last week4. Semantic role labeling can be viewed as a multiclass classification problem. Given a word (or phrase) and its features, the goal is to output the most probable semantic label. For semantic role labeling, we have used the exact same feature set that Hacioglu et al. (2004) have used, since their system performed the best among others in the CoNLL2004 shared task (Carreras and M`arquez, 2004). We have used Boostexter (Schapire and Singer, 2000) as the classifier. The features include tokenlevel features (such as the current (head) word, its part-of-speech tag, base phrase type and position, etc.), predicate-level features (such as the predicate’s lemma, frequency, part-of-speech tag, etc.) and argument-level features which capture the relationship between the token (head word/phrase) and the predicate (such as the syntactic path between the token and the predica</context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2004</marker>
<rawString>Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H. Martin, and Dan Jurafsky. 2004. Semantic role labeling by tagging syntactic chunks. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL), Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Hakkani-T¨ur</author>
<author>Gokhan Tur</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Combining words and prosody for information extraction from speech.</title>
<date>1999</date>
<booktitle>In Proceedings of the EUROSPEECH’99, European Conference on Speech Communication and Technology,</booktitle>
<location>Budapest, Hungary,</location>
<marker>Hakkani-T¨ur, Tur, Stolcke, Shriberg, 1999</marker>
<rawString>Dilek Hakkani-T¨ur, Gokhan Tur, Andreas Stolcke, and Elizabeth Shriberg. 1999. Combining words and prosody for information extraction from speech. In Proceedings of the EUROSPEECH’99, European Conference on Speech Communication and Technology, Budapest, Hungary, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Mitch Marcus</author>
<author>Martha Palmer</author>
</authors>
<title>Adding semantic annotation to the Penn TreeBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT),</booktitle>
<location>San Diego, CA,</location>
<contexts>
<context position="9218" citStr="Kingsbury et al., 2002" startWordPosition="1485" endWordPosition="1488">he SSGs. The semantic role labels represent the predicate/argument structure of each sentence: Given a predicate, the goal is to identify all of its arguments and their semantic roles. For example, in the example sentence the predicate is pay, the agent of this predicate is I and the amount is six dollars. In the graph, the labels of the transitions for semantic roles are prefixed by the token SRL: and the corresponding predicate. For example, the sequence six dollars is the amount of the predicate pay, and this is shown by the transition with label SRL:pay.A1 following the PropBank notation (Kingsbury et al., 2002)1. In this work, we were only able to incorporate part-of-speech tags, syntactic parses, named entity tags and semantic role labels in the syntactic and semantic graphs. Insertion of further information such as supertags (Bangalore and Joshi, 1999) or word stems can also be beneficial for further processing. 3 Using SSGs for Call Classification In this paper we propose extracting all -grams from the SSGs to use them for call classification. The - grams in an utterance SSG can be extracted by converting it to a finite state transducer (FST), . Each transition of has the labels of the arcs on th</context>
<context position="18105" citStr="Kingsbury et al., 2002" startWordPosition="2981" endWordPosition="2984">ntic role of a target verb. Typical semantic arguments include Agent, Patient, Instrument, etc. an d also adjuncts such as Locative, Temporal, Manner, Cause, etc. In this Category Count EMM IF Boostexter Org. 132 62.0 73.8 70.9 Person 150 45.0 62.4 54.4 Date 178 51.4 61.9 72.0 Drug 220 65.7 62.3 63.1 Overall 836 54.5 56.8 64.0 Table 1: F-Measure results for named entity extraction with various approaches. HMM is the simple HMM-based approach, IF is the simplified version of BBN’s name finder with an unknown words model. work, we use the semantic roles and annotations from the PropBank corpus (Kingsbury et al., 2002), where the arguments are given mnemonic names, such as Arg0, Arg1, Arg-LOC, etc. For example, for the sentence I have bought myself a blue jacket from your summer catalog for twenty five dollars last week, the agent (buyer, or Arg0) is I, the predicate is buy, the thing bought (Arg1) is a blue jacket, the seller or source (Arg2) is from your summer catalog, the price paid (Arg3) is twenty five dollars, the benefactive (Arg4) is myself, and the date (ArgM-TMP) is last week4. Semantic role labeling can be viewed as a multiclass classification problem. Given a word (or phrase) and its features, </context>
</contexts>
<marker>Kingsbury, Marcus, Palmer, 2002</marker>
<rawString>Paul Kingsbury, Mitch Marcus, and Martha Palmer. 2002. Adding semantic annotation to the Penn TreeBank. In Proceedings of the Human Language Technology Conference (HLT), San Diego, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="13490" citStr="Marcus et al., 1994" startWordPosition="2222" endWordPosition="2225">se components may be improved independently, for the specific application domain. 4.1 Part-of-Speech Tagger Part-of-speech tagging has been very well studied in the literature for many languages, and the approaches vary from rule-based to HMM-based and classifier-based (Church, 1988; Brill, 1995, among others) tagging. In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, , given the words, , is output (Weischedel et al., 1993): Since we do not have enough data which is manually tagged with part-of-speech tags for our applications, we used Penn Treebank (Marcus et al., 1994) as our training set. Penn Treebank includes data from Wall Street Journal, Brown, ATIS, and Switchboard corpora. The final two sets are the most useful for our domain, since they are also from spoken language and include disfluencies. As a test set, we manually labeled 2,000 words of user utterances from an AT&amp;T VoiceTone spoken dialog system application, and we achieved an accuracy of 94.95% on manually transcribed utterances. When we examined the errors, we have seen that the frequent word please is mis-labeled or frequently occurs as a verb in the training data, even when it is not. Given </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Riccardi</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Active and unsupervised learning for automatic speech recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH),</booktitle>
<location>Geneva, Switzerland,</location>
<marker>Riccardi, Hakkani-T¨ur, 2003</marker>
<rawString>Giuseppe Riccardi and Dilek Hakkani-T¨ur. 2003. Active and unsupervised learning for automatic speech recognition. In Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), Geneva, Switzerland, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Boostexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="15486" citStr="Schapire and Singer, 2000" startWordPosition="2544" endWordPosition="2547">IN WORD:bill POS:JJ POS:JJ 7 8 9 WORD:a WORD:past WORD:due PHRASE:S_need PHRASE:VP_need a very reasonable performan ce, considering these errors. For syntactic parsing, we use the parser (Collins, 1999), which is reported to give over 88% labeled recall and precision on Wall Street Journal portion of the Penn Treebank. chunklink script to extract information from the parse trees3. Since we do not have any data from our domain, we do not have a performan Collins’ Buchholz’s ce figure for this task for our domain. name finder (Bikel et al., 1999), and a classifier-based tagger using Boostexter (Schapire and Singer, 2000). In the simple HMM-based approach, which is the same as the part-of-speech tagging, the goal is to find the tag sequence, ,which maximizes for the word sequence, .The tags in this case are named entity categories (such as P and p for Person names, O and o for Organization names, etc. where upper-case indicates the first word in the named entity) or NA if the word is not a part BBN’s of a named entity. In the simplified version of BBN’s name finder, the states of 3http://ilk.kub.nl/ sabine/chunklink/chunklink 2-2- 2000 for conll.pl 28 the model were word/tag combinations, where the tag for wor</context>
<context position="19030" citStr="Schapire and Singer, 2000" startWordPosition="3143" endWordPosition="3146">ket, the seller or source (Arg2) is from your summer catalog, the price paid (Arg3) is twenty five dollars, the benefactive (Arg4) is myself, and the date (ArgM-TMP) is last week4. Semantic role labeling can be viewed as a multiclass classification problem. Given a word (or phrase) and its features, the goal is to output the most probable semantic label. For semantic role labeling, we have used the exact same feature set that Hacioglu et al. (2004) have used, since their system performed the best among others in the CoNLL2004 shared task (Carreras and M`arquez, 2004). We have used Boostexter (Schapire and Singer, 2000) as the classifier. The features include tokenlevel features (such as the current (head) word, its part-of-speech tag, base phrase type and position, etc.), predicate-level features (such as the predicate’s lemma, frequency, part-of-speech tag, etc.) and argument-level features which capture the relationship between the token (head word/phrase) and the predicate (such as the syntactic path between the token and the predicate, their distance, token position relative to the predicate, etc.). In order to evaluate the performance of semantic role labeling, we have manually annotated 285 utterances</context>
<context position="21439" citStr="Schapire and Singer, 2000" startWordPosition="3527" endWordPosition="3530"> to evaluate our approach, we carried out call classification experiments using human-machine dialogs collected by the spoken dialog system used for customer care. We have only considered utterances which are responses to the greeting prompt How may I help you? in order not to deal with confirmation and clarification utterances. We first describe this data, and then give the results obtained by the semantic classifier. We have performed our tests using the Boostexter tool, an implementation of the Boosting algorithm, which iteratively selects the most discriminative features for a given task (Schapire and Singer, 2000). 5.1 Data Table 2 summarizes the characteristics of our application including the amount of training and test data, total number of call-types, average utterance length, and call-type perplexity. Perplexity is computed using the prior distribution over all the call-types in the training data. 5.2 Results For call classification, we have generated SSGs for the training and test set utterances using the tools 29 Training Data Size 3,725 utterances Test Data Size 1,954 utterances Number of Call-Types 79 Call-Type Perplexity 28.86 Average Utterance Length 12.08 words Table 2: Characteristics of t</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. Boostexter: A boosting-based system for text categorization. Machine Learning, 39(2-3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Improving accuracy in word class tagging through combination of machine learning systems.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<marker>van Halteren, Zavrel, Daelemans, 2001</marker>
<rawString>Hans van Halteren, Jakub Zavrel, and Walter Daelemans. 2001. Improving accuracy in word class tagging through combination of machine learning systems. Computational Linguistics, 27(2):199–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Richard Schwartz</author>
<author>Jeff Palmucci</author>
<author>Marie Meteer</author>
<author>Lance Ramshaw</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.</title>
<date>1993</date>
<journal>Computational Linguistics, SpecialIssue on Using Large Corpora,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="13340" citStr="Weischedel et al., 1993" startWordPosition="2197" endWordPosition="2200">ls used to compute the information in SSGs are described and their performances on manually transcribed spoken dialog utterances are presented. All of these components may be improved independently, for the specific application domain. 4.1 Part-of-Speech Tagger Part-of-speech tagging has been very well studied in the literature for many languages, and the approaches vary from rule-based to HMM-based and classifier-based (Church, 1988; Brill, 1995, among others) tagging. In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, , given the words, , is output (Weischedel et al., 1993): Since we do not have enough data which is manually tagged with part-of-speech tags for our applications, we used Penn Treebank (Marcus et al., 1994) as our training set. Penn Treebank includes data from Wall Street Journal, Brown, ATIS, and Switchboard corpora. The final two sets are the most useful for our domain, since they are also from spoken language and include disfluencies. As a test set, we manually labeled 2,000 words of user utterances from an AT&amp;T VoiceTone spoken dialog system application, and we achieved an accuracy of 94.95% on manually transcribed utterances. When we examined </context>
</contexts>
<marker>Weischedel, Schwartz, Palmucci, Meteer, Ramshaw, 1993</marker>
<rawString>Ralph Weischedel, Richard Schwartz, Jeff Palmucci, Marie Meteer, and Lance Ramshaw. 1993. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, SpecialIssue on Using Large Corpora, 19(2):361–382, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>