<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014274">
<title confidence="0.979566">
Deep Questions without Deep Understanding
</title>
<author confidence="0.991103">
Igor Labutov Sumit Basu Lucy Vanderwende
</author>
<affiliation confidence="0.996966">
Cornell University Microsoft Research Microsoft Research
</affiliation>
<address confidence="0.9760335">
124 Hoy Road One Microsoft Way One Microsoft Way
Ithaca, NY Redmond, WA Redmond, WA
</address>
<email confidence="0.998449">
iil4@cornell.edu sumitb@microsoft.com lucyv@microsoft.com
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997915">
We develop an approach for generating
deep (i.e, high-level) comprehension
questions from novel text that bypasses
the myriad challenges of creating a full se-
mantic representation. We do this by de-
composing the task into an ontology-
crowd-relevance workflow, consisting of
first representing the original text in a
low-dimensional ontology, then crowd-
sourcing candidate question templates
aligned with that space, and finally rank-
ing potentially relevant templates for a
novel region of text. If ontological labels
are not available, we infer them from the
text. We demonstrate the effectiveness of
this method on a corpus of articles from
Wikipedia alongside human judgments,
and find that we can generate relevant
deep questions with a precision of over
85% while maintaining a recall of 70%.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999730857142857">
Questions are a fundamental tool for teachers in
assessing the understanding of their students.
Writing good questions, though, is hard work, and
harder still when the questions need to be deep
(i.e., high-level) rather than factoid-oriented.
These deep questions are the sort of open-ended
queries that require deep thinking and recall rather
than a rote response, that span significant amounts
of content rather than a single sentence. Unsur-
prisingly, it is these deep questions that have the
greatest educational value (Anderson, 1975; An-
dre, 1979; McMillan, 2001). They are thus a key
assessment mechanism for a spectrum of online
educational options, from MOOCs to interactive
tutoring systems. As such, the problem of auto-
matic question generation has long been of inter-
est to the online education community (Mitkov
and Ha, 2003; Schwartz, 2004), both as a means
of providing self-assessments directly to students
and as a tool to help teachers with question author-
ing. Much work to date has focused on questions
based on a single sentence of the text (Becker et
al., 2012; Lindberg et al., 2013; Mazidi and Niel-
sen, 2014), and the ideal of creating deep, concep-
tual questions has remained elusive. In this work,
we hope to take a significant step towards this
challenge by approaching the problem in a some-
what unconventional way.
</bodyText>
<figureCaption confidence="0.9942945">
Figure 1: Overview of our ontology-crowd-rele-
vance approach.
</figureCaption>
<bodyText confidence="0.999930214285714">
While one might expect the natural path to gener-
ating deep questions to involve first extracting a
semantic representation of the entire text, the
state-of-the-art in this area is at too early a stage
to achieve such a representation effectively. Ra-
ther we take a step back from full understanding,
and instead propose an ontology-crowd-relevance
workflow for generating high-level questions,
shown in Figure 1. This involves 1) decomposing
a text into a meaningful, intermediate, low-dimen-
sional ontology, 2) soliciting high-level templates
from the crowd, aligned with this intermediate
representation, and 3) for a target text segment, re-
trieving a subset of the collected templates based
</bodyText>
<page confidence="0.98171">
889
</page>
<note confidence="0.978086">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 889–898,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99995980952381">
on its ontological categories and then ranking
these questions by estimating the relevance of
each to the text at hand.
In this work, we apply the proposed workflow
to the Wikipedia corpus. For our ontology, we use
a Cartesian product of article categories (derived
from Freebase) and article section names (directly
from Wikipedia) as the intermediate representa-
tion (e.g. category: Person, section: Early life),
henceforth referred to as category-section pairs.
We use these pairs to prompt our crowd workers
to create relevant templates; for instance, (Person,
Early Life) might lead a worker to generate the
question “Who were the key influences on &lt;Per-
son&gt; in their childhood?”, a good example of the
sort of deep question that can’t be answered from
a single sentence in the article. We also develop
classifiers for inferring these categories when ex-
plicit or matching labels are not available. Given
a database of such category-section-specific ques-
tion templates, we then train a binary classifier
that can estimate the relevance of each to a new
document. We hypothesize that the resulting
ranked questions will be both high-level and rele-
vant, without requiring full machine understand-
ing of the text – in other words, deep questions
without deep understanding.
In the sections that follow, we detail the various
components of this method and describe the ex-
periments showing their efficacy at generating
high-quality questions. We begin by motivating
our choice of ontology and demonstrating its cov-
erage properties (Section 3). We then describe our
crowdsourcing methodology for soliciting ques-
tions and question-article relevance judgments
(Section 4), and outline our model for determining
the relevance of these questions to new text (Sec-
tion 5). After this we describe the two datasets that
we construct for the evaluation of our approach
and present quantitative results (Section 6) as well
as examples of our output and an error analysis
(Section 7) before concluding (Section 8).
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955983050848">
We consider three aspects of past research in au-
tomatic question generation: work that focuses on
the grammaticality of natural language question
generation, work that focuses on the semantic
quality of generated questions, i.e. the “what to
ask about” rather than “how to ask it,” and finally
work that builds a semantic representation of text
in order to generate higher-level questions.
Approaches focusing on the grammaticality of
question generation date back to the AU-
TOQUEST system (Wolfe, 1976), which exam-
ined the generation of Wh-questions from single
sentences. Later systems addressing the same goal
include methods that use transformation rules
(Mitkov and Ha, 2003), template-based genera-
tion (Chen et al., 2009; Curto et al., 2011) and
overgenerate-and-rank methods (Heilman and
Smith, 2010a). Another approach has been to cre-
ate fill-in-the-blank questions from single sen-
tences to ensure grammaticality (Agarwal et al.
2011, Becker et al. 2012).
More relevant to our direction is work on the
semantic aspect of question generation, which has
become a more active research area in the past
several years. Several authors (Mazidi and Niel-
sen 2014; Linberg et al. 2013) generate questions
according to the semantic role patterns extracted
from the source sentence. Becker et al. (2012) also
leverage semantic role labeling within a sentence
in a supervised setting. We hope to continue in
this direction of semantic focus, but extend the ca-
pabilities of question generation to include open-
ended questions that go far beyond the scope of a
single sentence.
Other work has taken on the challenge of
deeper questions by attempting to build a seman-
tic representation of arbitrary text. This has in-
cluded work using concept maps over keywords
(Olney et al. 2012) and minimal recursion seman-
tics (Yao 2010) to reason over concepts in the text.
While the work of (Olney et al. 2012) is impres-
sive in its possibilities, the range of the types of
questions that can be generated is restricted by a
relatively specific set of relations (e.g. Is-A, Part-
Of) captured in the ontology of the domain (biol-
ogy textbook). Mannem et al. (2010) observe as
we have that &amp;quot;capturing the exact true meaning of
a paragraph is beyond the reach of current NLP
systems;&amp;quot; thus, in their system for Shared Task A
(for paragraph-level questions (Rus et al. 2010))
they make use of predicate argument structures
along with semantic role labeling. However, the
generation of these questions is restricted to the
first sentence of the paragraph. Though motivated
by the same noble impulses of these authors to
achieve higher-level questions, our hope is that we
can bypass the challenges and constraints of se-
mantic parsing and generate deep questions via a
more holistic approach.
</bodyText>
<page confidence="0.99767">
890
</page>
<figureCaption confidence="0.64237775">
Figure 2: Coverage properties of our category-section representation: (a) fraction of Wikipedia
articles covered by the top j most common Freebase types, grouped by our eight higher-level
categories. (b) Average fraction of sections covered per document if only the top k most frequent
sections are used; each line represents one of our eight categories.
</figureCaption>
<sectionHeader confidence="0.991351" genericHeader="method">
3 An Ontology of Categories and Sec-
tions
</sectionHeader>
<bodyText confidence="0.99918774509804">
The key insight of our approach is that we can lev-
erage an easily interpretable (for crowd workers),
low-dimensional ontology for text segments in or-
der to crowdsource a set of high-level, reusable
templates that generalize well to many docu-
ments. The choice of this representation must
strike a balance between domain coverage and the
crowdsourcing effort required to obtain that cov-
erage. Inasmuch as Wikipedia is deemed to have
broad coverage of human knowledge, we can es-
timate domain coverage by measuring what frac-
tion of that corpus is covered by the proposed rep-
resentation. In our work, we have developed a cat-
egory-section ontology using annotations from
Freebase and Wikipedia (English), and now de-
scribe its structure and coverage in detail.
For the high-level categories, we make use of
the Freebase “notable type” for each Wikipedia
article. In contrast to the noisy default Wikipedia
categories, the Freebase “notable types” provide a
clean high-level encapsulation of the topic or en-
tity discussed in a Wikipedia article. As we wish
to maximize coverage, we compute the histogram
by type and take the 300 most common ones
across Wikipedia. We further merge these into
eight broad categories to reduce crowdsourcing
effort: Person, Location, Event, Organization,
Art, Science, Health, and Religion. These eight
categories cover 78% of Wikipedia articles (see
Figure 2a); the mapping between Freebase types
and our categories will be made available as part
of our corpus (see Section 8).
To achieve greater specificity of questions
within the articles, we make use of Wikipedia sec-
tions, which offer a high-level segmentation of the
content. The Cartesian product of our categories
from above and the most common Wikipedia sec-
tion titles (per category) then yield an interpreta-
ble, low-dimensional representation of the article.
For instance, the set of category-section pairs for
an article about Albert Einstein contains (Person,
Early_life), (Person, Awards), and (Person, Polit-
ical_views) as well as several others.
For each category, the section titles that occur
most frequently represent central themes in arti-
cles belonging to that category. We therefore hy-
pothesize that question templates authored for
such high-coverage titles are likely to generalize
to a large number of articles in that category. Ta-
ble 1 below shows the four most frequent sections
for each of our eight categories.
</bodyText>
<subsectionHeader confidence="0.879674">
Person Location Organiza- Art
</subsectionHeader>
<bodyText confidence="0.528143">
tion
</bodyText>
<table confidence="0.9640064">
Early life History History Plot
Career Geography Geography Reception
Pers. life Economy Academics History
Biography Demo- Demo- Production
graphics graphics
Science Event Health Religion
Descript. Background Treatment Etymology
Taxonomy Aftermath Diagnosis Icongraphy
History Battle Causes Worship
Distributn. Prelude History Mythology
</table>
<tableCaption confidence="0.999372">
Table 1: Most frequent section titles by category.
</tableCaption>
<bodyText confidence="0.99970675">
As the crowdsourcing effort is directly propor-
tional to the size of the ontology, our goal is to
select the smallest set of pairs that will provide
sufficient coverage. As with categories, the cut-
</bodyText>
<page confidence="0.99099">
891
</page>
<bodyText confidence="0.999981473684211">
off for the number of sections used for each cate-
gory is guided by the trade-off between coverage
and crowdsourcing costs. Figure 2b plots the av-
erage fraction of an article covered by the top k
sections from each category. We found that the
top 50 sections cover 30% to 55% of the sections
of an individual article (on average) across our
categories. This implies that by only crowdsourc-
ing question templates for those 50 sections per
category, we would be able to ask questions about
a third to a half of the sections of any article.
Of course, if we were to limit ourselves to only
segments with these labels at runtime, we would
completely miss many articles as well as texts out-
side of Wikipedia. To extend our reach, we also
develop the means for category and section infer-
ence from raw text in Section 5 below, for cases
in which ontological labels are either not available
or are not contained within our limited set.
</bodyText>
<sectionHeader confidence="0.982058" genericHeader="method">
4 Crowdsourcing Methodology
</sectionHeader>
<bodyText confidence="0.999928461538462">
We designed a two-stage crowdsourcing pipeline
to 1) collect templates targeted to a set of cate-
gory-section pairs and 2) obtain binary relevance
judgments for the generated templates in relation
to a set of article segments (for Wikipedia, these
are simply sections) that match in category-sec-
tion labels. We recruit Mechanical Turk workers
for both stages of the pipeline, filtering for work-
ers from the United States due to native English
proficiency. A total of 307 unique workers partic-
ipated in the two tasks combined (78 and 229
workers for the generation and ratings tasks re-
spectively).
</bodyText>
<figureCaption confidence="0.9402215">
Figure 3: Prompt for the generation task for the
category-section pair (Person, Legacy).
</figureCaption>
<subsectionHeader confidence="0.995631">
4.1 Question generation task
</subsectionHeader>
<bodyText confidence="0.999701975609756">
Following the coverage analysis above, we select
the 50 most frequent sections for the top two cat-
egories, Person and Location, yielding 100 cate-
gory-section pairs. As these two categories cover
nearly 50% of all articles on Wikipedia, we be-
lieve that they suffice in demonstrating the effec-
tiveness of the proposed methodology. For each
category-section pair, we instructed 10 (median)
workers to generate a question regarding a hypo-
thetical entity belonging to the target with the
prompt in Figure 3. Additional instructions and an
interactive tutorial were pre-administered, guid-
ing the workers to formulate appropriately deep
questions, i.e. questions that are likely to general-
ize to many articles, while avoiding factoid ques-
tions like “When was X born?”
In total, 995 question templates were added to
our question database using this methodology
(only 0.5% of all generated questions were exact
repeats of existing questions). We confirm in sec-
tion 4.2 that workers were able to formulate deep,
interesting and relevant questions whose answers
spanned more than a single sentence and that gen-
eralized to many articles using this prompt.
In earlier pilots, we tried an alternative prompt
which also presented the text of a specific article
segment. In Figure 4, we show the average scope
and relevance of questions generated by workers
under both prompt conditions. As the figure
demonstrates, the alternative prompt showing
specific article text resulted in questions that gen-
eralized less well (workers’ questions were found
to be relevant to fewer articles), likely because the
details in the text distracted the workers from
thinking broadly about the domain. These ques-
tions also had a smaller scope on average, i.e., an-
swers to these questions were contained in shorter
spans in the text. The differences in scope and rel-
evance between the two prompt designs were both
significant (p-values: 0.006 and 4.5e-11 respec-
tively, via two-sided Welch’s t-tests).
</bodyText>
<figureCaption confidence="0.645610666666667">
Figure 4: Average relevance and scope of
worker-generated questions versus how the
workers were prompted.
</figureCaption>
<page confidence="0.985154">
892
</page>
<subsectionHeader confidence="0.994161">
4.2 Question relevance rating task
</subsectionHeader>
<bodyText confidence="0.9999822">
For our 100 category-section pairs, 4 (median) ar-
ticle segments within reasonable length for a Me-
chanical Turk task (200-1000 tokens) were drawn
at random from the Wikipedia corpus; this re-
sulted in a set of 513 article segments. Each
worker was then presented with one of these seg-
ments alongside at most 10 questions from the
question template database matching in category-
section; templates were converted into questions
by filling in the article-specific entity extracted
from the title. Workers were requested to rate each
question along three dimensions: relevance, qual-
ity, and scope, as detailed below. Quality and
scope ratings were only requested when the
worker determined the question to be relevant.
</bodyText>
<listItem confidence="0.990552571428571">
• Relevance: 1 (not relevant) – 4 (relevant)
Does the article answer the question?
• Quality: 1 (poor) – 4 (excellent)
Is this question well-written?
• Scope: 1 (single-sentence) – 4 (multi-sen-
tence/paragraph)
How long is the answer to this question?
</listItem>
<bodyText confidence="0.999368">
A median of 3 raters provided an independent
judgment for each question-article pair. The mean
relevance, quality and scope ratings across the 995
questions were 2.3 (sd=0.83), 3.5 (sd=.65) and 2.6
(sd=1.0) respectively. Note that the sample sizes
for scope and quality were smaller, 774 and 778
respectively, as quality/scope judgments were not
gathered for questions deemed irrelevant. We note
that 80% of the relevant crowd-sourced questions
had a median scope rating larger than 1 sentence,
and 23% had a median scope rating of 4, defined
as “the answer to this question can be found in
many sentences and paragraphs,” corresponding
to the maximum attainable scope rating. Note that
while in this work, we have only used the scope
judgments to report summary statistics about the
generated questions, in future work these ratings
could be used to build a scope classifier to filter
out questions targeting short spans of text.
As described in Section 5.2, the relevance judg-
ments are converted to binary relevance ratings
for training the relevance classifier (we consider
relevance ratings {1, 2} as “not relevant” and {3,
4} as “relevant”). In terms of agreement between
raters for these binary relevance labels, we ob-
tained a Fleiss’ Kappa of 0.33, indicating fair
agreement.
</bodyText>
<sectionHeader confidence="0.996149" genericHeader="method">
5 Model
</sectionHeader>
<bodyText confidence="0.999997375">
There are two key models to our system: the first
is for category and section inference of a novel ar-
ticle segment, which allows us to infer the keys to
our question database when explicit labels are not
available. The second is for question relevance
prediction, which lets us decide which question
templates from the database’s store for that cate-
gory-section actually apply to the text at hand.
</bodyText>
<subsectionHeader confidence="0.959406">
5.1 Category/section inference
</subsectionHeader>
<bodyText confidence="0.999989891891892">
Both category and section inference were cast as
standard text-classification problems. Category
inference is performed on the whole article, while
section inference is performed on the individual
article segments (i.e., sections). We trained indi-
vidual logistic regression classifiers for the eight
categories and the 50 top section types for each
one (a total of 400) using the default L2 regulari-
zation parameter in LIBLINEAR (Fan, 2008). For
section inference, a total of 736,947 article seg-
ments were sampled from Wikipedia (June 2014
snapshot), each belonging to one of the 400 sec-
tion types and within the same length bounds from
Section 4.2 (200-1000 tokens). For category infer-
ence, we sampled a total of 86,348 articles with at
least 10 sentences and belonging to one of our
eight categories.
In both cases, a binary dataset was constructed
for a one-against-all evaluation, where the nega-
tive instances were sampled randomly from the
negative categories or sections (there was an av-
erage 17% and 32% positive skew in the section
and category datasets, respectively). Basic tf-idf
features (using a vocabulary of 200,000 after
eliminating stopwords) were used in both text
classification tasks. Applying the category/section
inference to held-out portions of the dataset (30%
for each category/section) resulted in balanced ac-
curacies of 83%/95% respectively, which gave us
confidence in the inference. Keep in mind that this
is not a strict bound on our question generation
performance, since the inferred category/section,
while not matching the label perfectly, could still
be sufficiently close to produce relevant questions
(for instance, we could misrecognize “Childhood”
as “Early Life”). We explore the ramifications of
this in our end-to-end experiments in Section 6.
</bodyText>
<subsectionHeader confidence="0.985991">
5.2 Relevance Classification
</subsectionHeader>
<bodyText confidence="0.99929325">
We also cast the problem of question/article rele-
vance prediction as one of binary classification,
where we map a question-article pair to a rele-
vance score; as such our features had to combine
</bodyText>
<page confidence="0.995664">
893
</page>
<bodyText confidence="0.999970092592593">
aspects of both the question and the article. Our
core approach was to use a vector of the compo-
nent-wise Euclidean distances between individual
features of the question and article segment, i.e.,
the ith feature vector component fi is given by
fi = (qi − ai)2, where qi and ai are the compo-
nents of the question and article feature vectors.
For the feature representation, we utilized a con-
catenation of continuous embedding features: 300
features from a Word2Vec embedding (Mikolov,
2013) and 200,000 tfidf features (as with cate-
gory/section classification above).
As question templates are typically short,
though, we found that this representation alone
performed poorly. As a result, we augmented the
vector by concatenating additional distance fea-
tures between the target article segment and one
specific instance of an entire article for which the
question applied. This augmenting article was se-
lected at random from all those for which the tem-
plate was judged to be relevant. The resulting fea-
ture vector was thus doubled in length, where the
first k distances were between the question tem-
plate and the target segment, and the next k were
between the augmenting article and the target seg-
ment. Note that the augmenting article segments
were removed from the training/test sets.
To train this classifier, we assumed that we
would be able to acquire at least n positive rele-
vance labels for each question template, i.e., n ar-
ticle segments judged to be relevant to each tem-
plate for inclusion in the training set. We explore
the effect of increasing values of n, from 0 (where
no relevance labels are available) to 3 (referred to
as conditions T0..T3 in Figure 5). We then trained
and evaluated the relevance classifier, a single lo-
gistic regression model using LIBLINEAR with
default L2 regularization, using 10-fold cross-val-
idation on DATASET I (see Section 6).
Figure 5 depicts a series of ROC curves sum-
marizing the performance of our template rele-
vance classifier on unseen article segments. As
expected, we see increasing performance with in-
creasing n. However, the benefit drops off after 3
instances (i.e., T4 is only marginally better than
T3). While the character of the curves is modest,
keep in mind we are already filtering questions by
retrieving them from the database for the inferred
category-section (which by itself gives us a preci-
sion of .74 – see green bars in Figure 6); this ROC
represents the “lift” achieved by further filtering
the questions with our relevance classifier, result-
ing in far higher precision (.85 to .95 – see blue
bars in Figure 6).
</bodyText>
<figureCaption confidence="0.93501125">
Figure 5: ROC curves for the task of question-to-
article relevance prediction. Tn means that n pos-
itively labeled article segments were available
for each question template during training.
</figureCaption>
<sectionHeader confidence="0.997968" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999983888888889">
In this section, we describe the datasets used for
training the relevance classifier in Section 5.2
(DATASET I) as well as for end-to-end perfor-
mance on unlabeled text segments (DATASET II).
We then evaluate the performance on this second
dataset under three settings: first, when the cate-
gory and section are known, second, when those
labels are unavailable, and third, when neither the
labels nor the relevance classifier are available.
</bodyText>
<subsectionHeader confidence="0.974859">
6.1 DATASET I: for the Relevance Classifier
</subsectionHeader>
<bodyText confidence="0.999905714285715">
The first dataset (DATASET I) was intended for
training and evaluating the relevance classifier,
and for this we assumed the category and section
labels were known. As such, judgments were col-
lected only for questions templates authored for a
given article’s actual category and section labels.
After filtering out annotations from unreliable
workers (based on their pre-test results) as well as
those with inter-annotator agreement below 60%,
we were left with a set of 995 rated questions,
spanning across two categories (Person and Loca-
tion) and 50 sections per category (100 category-
section pairs total). This corresponded to a total of
4439 relevance tuples (label, question, article)
where label is a binary relevance rating aggre-
gated via majority vote across multiple raters. The
relevance labels were skewed towards the positive
(relevant) class with 63% relevant instances.
This is of course a mostly unrealistic data set-
ting for applications of question generation
(known category and section labels), but greatly
</bodyText>
<page confidence="0.99716">
894
</page>
<bodyText confidence="0.999893">
useful in developing and evaluating the relevance
classifier; we thus used this dataset only for that
purpose (see Section 5.2 and Figure 5).
</bodyText>
<subsectionHeader confidence="0.969252">
6.2 DATASET II: for End-to-End Evaluation
</subsectionHeader>
<bodyText confidence="0.999997580645161">
For an end-to-end evaluation we need to examine
situations where the category and section labels
are not available and we must rely on inference
instead. As this is the more typical use case for our
method, it is critical to understand how the perfor-
mance will be affected. For DATASET II, then, we
first sampled articles from the Wikipedia corpus
at random (satisfying the constraints described in
Section 3) and then performed category and sec-
tion inference on the article segments. The cate-
gory c with the highest posterior probability was
chosen as the inferred category, while all section
types 𝑠𝑖 with a posterior probability greater than
0.6 were considered as sources for templates.
Only articles whose inferred category was Person
or Location were considered, but given the noise
in inference there was no guarantee that the true
labels were of these categories. We continued this
process until we retrieved a total of 12 articles. For
each article segment in these 12, we drew a ran-
dom subset of at most 20 question templates from
our database matching the inferred category and
section(s), then ordered them by their estimated
relevance for presentation to judges.
We then solicited an additional 62 Mechanical
Turk workers to a rating task set up according to
the same protocol as for DATASET I. After aggre-
gation and filtering in the same way, the second
dataset contained a total 256 (label, question, ar-
ticle) relevance tuples, skewed towards the posi-
tive class with 72% relevant instances.
</bodyText>
<subsectionHeader confidence="0.998918">
6.3 Information Retrieval–based Evaluation
</subsectionHeader>
<bodyText confidence="0.999944095238095">
As our end-to-end task is framed as the retrieval
of a set of relevant questions for a given article
segment, we can measure performance in terms of
an information retrieval-based metric. Consider a
user who supplies an article segment (the “query”
in IR terms) for which she wants to generate a
quiz: the system then presents a ranked list of re-
trieved questions, ordered according to their esti-
mated relevance to the article. As she makes her
way down this ranked list of questions, adding a
question at a time to the quiz (set Q), the behavior
of the precision and recall (with respect to rele-
vance to the article segment) of the questions in
Q, summarizes the performance of the retrieval
system (i.e. the Precision-Recall (PR) curve
(Manning, 2008)). We summarize the perfor-
mance of our system by averaging the individual
article segments’ PR curves (linearly interpolated)
from DATASET II, and present the average preci-
sion over bins of recall values in Figure 6. We
consider the following experimental conditions:
</bodyText>
<listItem confidence="0.992161818181818">
• Known category/section, using relevance
classifier (red): This is the case in which the
actual category and section labels of the query
article are known, and only the questions that
match exactly in category and section are con-
sidered for relevance classification (i.e. added
to Q if found relevant by the classifier). Recall
is computed with respect to the total number
of relevant questions in DATASET II, including
those corresponding to sections different from
the section label of the article.
• Inferred category/section, using relevance
classifier (blue): This is the expected use
case, where the category/section labels are not
known. Questions matching in category and
section(s) to the inferred category and section
of each article are considered and ranked in Q
by their score from the relevance classifier.
Recall is computed with respect to the total
number of relevant questions in DATASET II.
• Inferred category/section, ignoring rele-
vance classifier (green): This is a baseline
</listItem>
<bodyText confidence="0.981658535714286">
where we only use category/section inference
and then retrieve questions from the database
without filtering: all questions that match in
inferred category and section(s) of the article
are added to Q in a random ranking order,
without performing relevance classification.
As we examine Figure 6, it is important to point
out a subtlety in our choice to calculate recall of
the known category/section condition (red bars)
with respect to the set of all relevant questions,
including those that are matched to sections dif-
ferent from the original (labeled) sections. While
this condition by construction does not have ac-
cess to questions of any other section, the result-
ing limitation in recall underlines the importance
of performing section inference: without infer-
ence, we achieve a recall of no greater than 0.4.
As we had hypothesized, while the labels of the
sections play an instrumental role in instructing
the crowd to generate relevant questions, the re-
sulting questions often tend to be relevant to con-
tent found under different but semantically related
sections as well. Leveraging the available ques-
tions of these related sections (by performing in-
ference) boosts recall at the expense of only a
small degree of precision (blue bars). If we forgo
relevance classification entirely, we get a constant
precision of 0.74 (green bars) as mentioned in
</bodyText>
<page confidence="0.997384">
895
</page>
<bodyText confidence="0.9982635625">
Section 5.2; it is clear that the relevance classifier
results in a significant advantage.
While there is a slight drop in precision when
using inference, this is at least partly due to the
constraints that were imposed during data-collec-
tion and relevance classifier training, i.e., all pairs
of articles and questions belonged to the same cat-
egory and section. While this constraint made the
crowdsourcing methodology proposed in this
work tractable, it also prevented the inclusion of
training examples for sections that could poten-
tially be inferred at test time. One possible ap-
proach to remedy this would be sample from arti-
cle segments that are similar in text (in terms of
our distance metric) as opposed to only segments
exactly matching in category and section.
</bodyText>
<figureCaption confidence="0.976041">
Figure 6: Precision-recall results for the end-to-
end experiment, grouped in bins of recall ranges.
</figureCaption>
<sectionHeader confidence="0.941155" genericHeader="evaluation">
7 Examples and Error Analysis
</sectionHeader>
<bodyText confidence="0.999944428571429">
In Table 2 we show a set of sample retrieved ques-
tions and the corresponding correctness of the rel-
evance classifier’s decision with respect to the
judgment labels; examining the errors yields some
interesting insights. Consider the false positive
example shown in row 8, where the category cor-
rectly inferred as Location, but section title was
inferred as Transportation instead of Services.
This mismatch resulted in the following template
authored for (Location, Transportation) being re-
trieved: &amp;quot;What geographic factors influence the
preferred transport methods in &lt;entity&gt;?&amp;quot; To the
relevance classifier, this particular template (con-
taining the word “transport”) appears to be rele-
vant on the surface level to the text of an article
segment about schedules (Services) at a railway
station. However, as this template never appeared
to judges in the context of a Services segment – a
section that differs considerably in theme from the
inferred section (Transportation) – the relevance
classifier unsurprisingly makes the wrong call.
</bodyText>
<table confidence="0.92171485">
True Inferred Re- Generated
section section sult Question
What accomplishments
characterized the later ca-
reer of Colin Cowdrey?
How did Corbin Bern-
stein’s television career
evolve over time?
What are some unique ge-
ographic features of
Puerto Rico Highway 10?
How much significance do
people of DeMartha Cath-
olic High School place on
athletics?
How does the geography
of Puerto Rico Highway
10 impact its resources?
Recep- FN What type of reaction did
Work tion Thornton Dial receive?
</table>
<tableCaption confidence="0.547172181818182">
What were the most im-
portant events in the later
career of Corbin Berstein?
What geographic factors
influence the preferred
transport methods in Wey-
mouth Railway Station?
How has Freddy Mitch-
ell’s legacy shaped current
events?
Table 2: Examples of retrieved questions. TP, TN,
</tableCaption>
<bodyText confidence="0.98781524">
FP, FN stand for true/false positive/negative with
respect to the relevance classification.
In considering additional sources of relevance
classification errors, recall that we employ a sin-
gle relevant article segment for the purpose of
augmenting a template’s feature representation. In
the case of the false negative example (row 6 in
Table 2), the sensitivity of the classifier to the par-
ticular augmenting article used is apparent. Upon
inspecting the target article segment (article:
Thornton Dial, section: Work), and the augment-
ing article segment (article: Syed Masood, section:
Reception), it’s clear that the inferred section Re-
ception is a reasonable title for the Work section
of the article on Thornton Dial, making the ques-
tion “What type of reaction did Thornton Dial re-
ceive?” a relevant question to the target article (as
reflected in the human judgment). However, alt-
hough both segments generally talk about “recep-
tion,” the language across the two segments is dis-
tinct: the critical reception of Thornton Dial the
visual artist is described in a different way from
the reception of Syed Masood the actor, resulting
in little overlap in surface text, and as a result the
relevance classifier falsely rejects the question.
</bodyText>
<figure confidence="0.999568764705882">
Later TP
Life
Hon-
ours
Televi- TP
sion
Acting
Career
Geogra- TP
phy
Route
De-
scrip-
tion
Athlet- Athletics TN
ics
Geogra- TN
phy
Route
De-
scrip-
tion
Later
FP
Career
Acting
Career
Transpor- FP
tation
Ser-
vices
Later
Legacy FP
Career
</figure>
<page confidence="0.996339">
896
</page>
<bodyText confidence="0.998767416666667">
Reasonable substitutions for inferred sections
can also lead to false positives, as in row 9, for the
article Freddy Mitchell. In this case, while Legacy
(the inferred section) is a believable substitute for
the true label of Later Career, in this case the ar-
ticle segment did not discuss his legacy. However,
there was a good match between the augmenting
article for this template and the section. We hy-
pothesize that in both this and the previous exam-
ples a broader sample of augmenting article seg-
ments for each category/section is likely to be ef-
fective at mitigating these types of errors.
</bodyText>
<sectionHeader confidence="0.998507" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999986464285714">
We have presented an approach for generating rel-
evant, deep questions that are broad in scope and
apply to a wide range of documents, all without
constructing a detailed semantic representation of
the text. Our three primary contributions are 1)
our insight that a low-dimensional ontological
document representation can be used as an inter-
mediary for retrieving and generalizing high-level
question templates to new documents, 2) an effi-
cient crowdsourcing scheme for soliciting such
templates and relevance judgments (of templates
to article) from the crowd in order to train a rele-
vance classification model, and 3) using cate-
gory/section inference and relevance prediction to
retrieve and rank relevant deep questions for new
text segments. Note that the approach and work-
flow presented here constitute a general frame-
work that could potentially be useful in other lan-
guage generation applications. For example, a
similar setup could be used for high-level summa-
rization, where question templates would be re-
placed with “summary snippets.”
Finally, to encourage the community to further
explore this approach as well as to compare it with
others, we are releasing all of our data (category
mappings, generated templates, and relevance
judgments) at http://research.microsoft.com/~su-
mitb/questiongeneration .
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999822655737705">
Manish Agarwal, Rakshit Shah, and Prashanth Man-
nem. 2011. Automatic Question Generation Using
Discourse Cues. In Proceedings of the 6th Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications.
Richard C. Anderson and W. Barry Biddle. 1975. On
Asking People Questions About What they are
Reading. Psychology of Learning and Motivation.
9:90-132.
Thomas Andre. 1979. Does Answering Higher-level
Questions while Reading Facilitate Productive
Learning? Review of Educational Research 49(2):
280-318.
Lee Becker, Sumit Basu, and Lucy Vanderwende.
2012. Mind the Gap: Learning to Choose Gaps for
Question Generation. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.
Wei Chen, Gregory Aist, and Jack Mostow. 2009. Gen-
erating Questions Automatically from Informational
Text. In S. Craig &amp; S. Dicheva (Ed.), Proceedings
of the 2nd Workshop on Question Generation.
Sérgio Curto, Ana Cristina Mendes, and Luisa Coheur.
2011. Exploring Linguistically-rich Patterns for
Question Generation. In Proceedings of the
UCNLG+Eval: Language Generation and Evalua-
tion Workshop.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research 9: 1871-1874.
Michael Heilman and Noah Smith. 2010. Good Ques-
tion! Statistical Ranking for Question Generation. In
Proceedings of NAACL/HLT.
David Lindberg, Fred Popowich, John Nesbit, and Phil
Winne. 2013. Generating Natural Language Ques-
tions to Support Learning On-line. In Proceedings
of the 14th European Workshop on Natural Lan-
guage Generation.
Prashanth Mannem, Rashmi Prasad, and Aravind
Joshi. 2010. Question generation from paragraphs at
UPenn: QGSTEC system description. In Proceed-
ings of the Third Workshop on Question Generation.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schutze. 2008. Introduction to Information
Retrieval. Cambridge: Cambridge university press
Karen Mazidi and Rodney D. Nielsen. 2014. Linguistic
Considerations in Automatic Question Generation.
In Proceedings of ACL.
James H. McMillan. 2001. Secondary Teachers&apos; Class-
room Assessment and Grading Practices.&amp;quot; Educa-
tional Measurement: Issues and Practice 20(1): 20-
32.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S.
Corrado, and Jeff Dean. 2013. Distributed Repre-
sentations of Words and Phrases and their Compo-
sitionality. In Proceedings of Advances in Neural
Information Processing Systems.
Ruslan Mitkov and Le An Ha. 2003. Computer-Aided
Generation of Multiple-Choice Tests. In Proceed-
</reference>
<page confidence="0.980763">
897
</page>
<reference confidence="0.99888775862069">
ings of the HLT-NAACL 2003 Workshop on Build-
ing Educational Applications Using Natural Lan-
guage Processing.
Andrew M. Olney, Arthur C. Graesser, and Natalie K.
Person. 2012. Question Generation from Concept
Maps. Dialogue &amp; Discourse 3(2): 75-99.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
Supervised Topic Model for Credit Attribution in
Multi-labeled Corpora. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing.
Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean,
Svetlana Stoyanchev, and Cristian Moldovan. 2010.
Overview of The First Question Generation Shared
Task Evaluation Challenge. In Proceedings of the
Third Workshop on Question Generation.
Lee Schwartz, Takako Aikawa, and Michel Pahud.
2004. Dynamic Language Learning Tools. In Pro-
ceedings of STIL/ICALL Symposium on Computer
Assisted Learning.
John H. Wolfe. 1976. Automatic Question Generation
from Text - an Aid to Independent Study. In Pro-
ceedings of ACM SIGCSE-SIGCUE Joint Sympo-
sium on Computer Science Education.
Xuchen Yao and Yi Zhang. 2010. Question generation
with minimal recursion semantics. In Proceedings
of QG2010: The Third Workshop on Question Gen-
eration.
</reference>
<page confidence="0.997871">
898
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.727441">
<title confidence="0.999611">Deep Questions without Deep Understanding</title>
<author confidence="0.995894">Igor Labutov Sumit Basu Lucy Vanderwende</author>
<affiliation confidence="0.999688">Cornell University Microsoft Research Microsoft Research</affiliation>
<address confidence="0.884614">124 Hoy Road One Microsoft Way One Microsoft Way Ithaca, NY Redmond, WA Redmond,</address>
<email confidence="0.99562">iil4@cornell.edusumitb@microsoft.comlucyv@microsoft.com</email>
<abstract confidence="0.99748919047619">We develop an approach for generating deep (i.e, high-level) comprehension questions from novel text that bypasses the myriad challenges of creating a full semantic representation. We do this by dethe task into an ontologyconsisting of first representing the original text in a low-dimensional ontology, then crowdsourcing candidate question templates aligned with that space, and finally ranking potentially relevant templates for a novel region of text. If ontological labels are not available, we infer them from the text. We demonstrate the effectiveness of this method on a corpus of articles from Wikipedia alongside human judgments, and find that we can generate relevant deep questions with a precision of over 85% while maintaining a recall of 70%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Manish Agarwal</author>
<author>Rakshit Shah</author>
<author>Prashanth Mannem</author>
</authors>
<title>Automatic Question Generation Using Discourse Cues.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<contexts>
<context position="6410" citStr="Agarwal et al. 2011" startWordPosition="983" endWordPosition="986"> representation of text in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic role labeling within a sentence in a supervised setting. We hope to continue in this direction of semantic focus, but extend the capabilities of question generation to include openended questions that go far beyond th</context>
</contexts>
<marker>Agarwal, Shah, Mannem, 2011</marker>
<rawString>Manish Agarwal, Rakshit Shah, and Prashanth Mannem. 2011. Automatic Question Generation Using Discourse Cues. In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Anderson</author>
<author>W Barry Biddle</author>
</authors>
<title>On Asking People Questions About What they are Reading. Psychology of Learning and Motivation.</title>
<date>1975</date>
<pages>9--90</pages>
<marker>Anderson, Biddle, 1975</marker>
<rawString>Richard C. Anderson and W. Barry Biddle. 1975. On Asking People Questions About What they are Reading. Psychology of Learning and Motivation. 9:90-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Andre</author>
</authors>
<title>Does Answering Higher-level Questions while Reading Facilitate Productive Learning?</title>
<date>1979</date>
<journal>Review of Educational Research</journal>
<volume>49</volume>
<issue>2</issue>
<pages>280--318</pages>
<contexts>
<context position="1650" citStr="Andre, 1979" startWordPosition="243" endWordPosition="245">f over 85% while maintaining a recall of 70%. 1 Introduction Questions are a fundamental tool for teachers in assessing the understanding of their students. Writing good questions, though, is hard work, and harder still when the questions need to be deep (i.e., high-level) rather than factoid-oriented. These deep questions are the sort of open-ended queries that require deep thinking and recall rather than a rote response, that span significant amounts of content rather than a single sentence. Unsurprisingly, it is these deep questions that have the greatest educational value (Anderson, 1975; Andre, 1979; McMillan, 2001). They are thus a key assessment mechanism for a spectrum of online educational options, from MOOCs to interactive tutoring systems. As such, the problem of automatic question generation has long been of interest to the online education community (Mitkov and Ha, 2003; Schwartz, 2004), both as a means of providing self-assessments directly to students and as a tool to help teachers with question authoring. Much work to date has focused on questions based on a single sentence of the text (Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014), and the ideal of cre</context>
</contexts>
<marker>Andre, 1979</marker>
<rawString>Thomas Andre. 1979. Does Answering Higher-level Questions while Reading Facilitate Productive Learning? Review of Educational Research 49(2): 280-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Becker</author>
<author>Sumit Basu</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Mind the Gap: Learning to Choose Gaps for Question Generation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of</booktitle>
<contexts>
<context position="2178" citStr="Becker et al., 2012" startWordPosition="331" endWordPosition="334"> these deep questions that have the greatest educational value (Anderson, 1975; Andre, 1979; McMillan, 2001). They are thus a key assessment mechanism for a spectrum of online educational options, from MOOCs to interactive tutoring systems. As such, the problem of automatic question generation has long been of interest to the online education community (Mitkov and Ha, 2003; Schwartz, 2004), both as a means of providing self-assessments directly to students and as a tool to help teachers with question authoring. Much work to date has focused on questions based on a single sentence of the text (Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014), and the ideal of creating deep, conceptual questions has remained elusive. In this work, we hope to take a significant step towards this challenge by approaching the problem in a somewhat unconventional way. Figure 1: Overview of our ontology-crowd-relevance approach. While one might expect the natural path to generating deep questions to involve first extracting a semantic representation of the entire text, the state-of-the-art in this area is at too early a stage to achieve such a representation effectively. Rather we take a step back from </context>
<context position="6431" citStr="Becker et al. 2012" startWordPosition="987" endWordPosition="990">xt in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic role labeling within a sentence in a supervised setting. We hope to continue in this direction of semantic focus, but extend the capabilities of question generation to include openended questions that go far beyond the scope of a single s</context>
</contexts>
<marker>Becker, Basu, Vanderwende, 2012</marker>
<rawString>Lee Becker, Sumit Basu, and Lucy Vanderwende. 2012. Mind the Gap: Learning to Choose Gaps for Question Generation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Chen</author>
<author>Gregory Aist</author>
<author>Jack Mostow</author>
</authors>
<title>Generating Questions Automatically from Informational Text.</title>
<date>2009</date>
<booktitle>In S. Craig &amp; S. Dicheva (Ed.), Proceedings of the 2nd Workshop on Question Generation.</booktitle>
<contexts>
<context position="6195" citStr="Chen et al., 2009" startWordPosition="952" endWordPosition="955">ticality of natural language question generation, work that focuses on the semantic quality of generated questions, i.e. the “what to ask about” rather than “how to ask it,” and finally work that builds a semantic representation of text in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic ro</context>
</contexts>
<marker>Chen, Aist, Mostow, 2009</marker>
<rawString>Wei Chen, Gregory Aist, and Jack Mostow. 2009. Generating Questions Automatically from Informational Text. In S. Craig &amp; S. Dicheva (Ed.), Proceedings of the 2nd Workshop on Question Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sérgio Curto</author>
<author>Ana Cristina Mendes</author>
<author>Luisa Coheur</author>
</authors>
<title>Exploring Linguistically-rich Patterns for Question Generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop.</booktitle>
<contexts>
<context position="6216" citStr="Curto et al., 2011" startWordPosition="956" endWordPosition="959"> language question generation, work that focuses on the semantic quality of generated questions, i.e. the “what to ask about” rather than “how to ask it,” and finally work that builds a semantic representation of text in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic role labeling within a </context>
</contexts>
<marker>Curto, Mendes, Coheur, 2011</marker>
<rawString>Sérgio Curto, Ana Cristina Mendes, and Luisa Coheur. 2011. Exploring Linguistically-rich Patterns for Question Generation. In Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research</journal>
<volume>9</volume>
<pages>1871--1874</pages>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research 9: 1871-1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah Smith</author>
</authors>
<title>Good Question! Statistical Ranking for Question Generation.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL/HLT.</booktitle>
<contexts>
<context position="6275" citStr="Heilman and Smith, 2010" startWordPosition="963" endWordPosition="966"> semantic quality of generated questions, i.e. the “what to ask about” rather than “how to ask it,” and finally work that builds a semantic representation of text in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic role labeling within a sentence in a supervised setting. We hope to continue in th</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah Smith. 2010. Good Question! Statistical Ranking for Question Generation. In Proceedings of NAACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lindberg</author>
<author>Fred Popowich</author>
<author>John Nesbit</author>
<author>Phil Winne</author>
</authors>
<title>Generating Natural Language Questions to Support Learning On-line.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="2201" citStr="Lindberg et al., 2013" startWordPosition="335" endWordPosition="338"> that have the greatest educational value (Anderson, 1975; Andre, 1979; McMillan, 2001). They are thus a key assessment mechanism for a spectrum of online educational options, from MOOCs to interactive tutoring systems. As such, the problem of automatic question generation has long been of interest to the online education community (Mitkov and Ha, 2003; Schwartz, 2004), both as a means of providing self-assessments directly to students and as a tool to help teachers with question authoring. Much work to date has focused on questions based on a single sentence of the text (Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014), and the ideal of creating deep, conceptual questions has remained elusive. In this work, we hope to take a significant step towards this challenge by approaching the problem in a somewhat unconventional way. Figure 1: Overview of our ontology-crowd-relevance approach. While one might expect the natural path to generating deep questions to involve first extracting a semantic representation of the entire text, the state-of-the-art in this area is at too early a stage to achieve such a representation effectively. Rather we take a step back from full understanding, and</context>
</contexts>
<marker>Lindberg, Popowich, Nesbit, Winne, 2013</marker>
<rawString>David Lindberg, Fred Popowich, John Nesbit, and Phil Winne. 2013. Generating Natural Language Questions to Support Learning On-line. In Proceedings of the 14th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashanth Mannem</author>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
</authors>
<title>Question generation from paragraphs at UPenn: QGSTEC system description.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third Workshop on Question Generation.</booktitle>
<contexts>
<context position="7609" citStr="Mannem et al. (2010)" startWordPosition="1188" endWordPosition="1191">that go far beyond the scope of a single sentence. Other work has taken on the challenge of deeper questions by attempting to build a semantic representation of arbitrary text. This has included work using concept maps over keywords (Olney et al. 2012) and minimal recursion semantics (Yao 2010) to reason over concepts in the text. While the work of (Olney et al. 2012) is impressive in its possibilities, the range of the types of questions that can be generated is restricted by a relatively specific set of relations (e.g. Is-A, PartOf) captured in the ontology of the domain (biology textbook). Mannem et al. (2010) observe as we have that &amp;quot;capturing the exact true meaning of a paragraph is beyond the reach of current NLP systems;&amp;quot; thus, in their system for Shared Task A (for paragraph-level questions (Rus et al. 2010)) they make use of predicate argument structures along with semantic role labeling. However, the generation of these questions is restricted to the first sentence of the paragraph. Though motivated by the same noble impulses of these authors to achieve higher-level questions, our hope is that we can bypass the challenges and constraints of semantic parsing and generate deep questions via a </context>
</contexts>
<marker>Mannem, Prasad, Joshi, 2010</marker>
<rawString>Prashanth Mannem, Rashmi Prasad, and Aravind Joshi. 2010. Question generation from paragraphs at UPenn: QGSTEC system description. In Proceedings of the Third Workshop on Question Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schutze</author>
</authors>
<title>Introduction to Information Retrieval. Cambridge:</title>
<date>2008</date>
<publisher>Cambridge university press</publisher>
<marker>Manning, Raghavan, Schutze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schutze. 2008. Introduction to Information Retrieval. Cambridge: Cambridge university press</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Mazidi</author>
<author>Rodney D Nielsen</author>
</authors>
<title>Linguistic Considerations in Automatic Question Generation.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2228" citStr="Mazidi and Nielsen, 2014" startWordPosition="339" endWordPosition="343"> educational value (Anderson, 1975; Andre, 1979; McMillan, 2001). They are thus a key assessment mechanism for a spectrum of online educational options, from MOOCs to interactive tutoring systems. As such, the problem of automatic question generation has long been of interest to the online education community (Mitkov and Ha, 2003; Schwartz, 2004), both as a means of providing self-assessments directly to students and as a tool to help teachers with question authoring. Much work to date has focused on questions based on a single sentence of the text (Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014), and the ideal of creating deep, conceptual questions has remained elusive. In this work, we hope to take a significant step towards this challenge by approaching the problem in a somewhat unconventional way. Figure 1: Overview of our ontology-crowd-relevance approach. While one might expect the natural path to generating deep questions to involve first extracting a semantic representation of the entire text, the state-of-the-art in this area is at too early a stage to achieve such a representation effectively. Rather we take a step back from full understanding, and instead propose an ontolog</context>
<context position="6631" citStr="Mazidi and Nielsen 2014" startWordPosition="1020" endWordPosition="1024">h-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic role labeling within a sentence in a supervised setting. We hope to continue in this direction of semantic focus, but extend the capabilities of question generation to include openended questions that go far beyond the scope of a single sentence. Other work has taken on the challenge of deeper questions by attempting to build a semantic representation of arbitrary text. This has included work using concept maps over keywords (Olney et</context>
</contexts>
<marker>Mazidi, Nielsen, 2014</marker>
<rawString>Karen Mazidi and Rodney D. Nielsen. 2014. Linguistic Considerations in Automatic Question Generation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James H McMillan</author>
</authors>
<title>Secondary Teachers&apos; Classroom Assessment and Grading Practices.&amp;quot;</title>
<date>2001</date>
<journal>Educational Measurement: Issues and Practice</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1667" citStr="McMillan, 2001" startWordPosition="246" endWordPosition="247">ile maintaining a recall of 70%. 1 Introduction Questions are a fundamental tool for teachers in assessing the understanding of their students. Writing good questions, though, is hard work, and harder still when the questions need to be deep (i.e., high-level) rather than factoid-oriented. These deep questions are the sort of open-ended queries that require deep thinking and recall rather than a rote response, that span significant amounts of content rather than a single sentence. Unsurprisingly, it is these deep questions that have the greatest educational value (Anderson, 1975; Andre, 1979; McMillan, 2001). They are thus a key assessment mechanism for a spectrum of online educational options, from MOOCs to interactive tutoring systems. As such, the problem of automatic question generation has long been of interest to the online education community (Mitkov and Ha, 2003; Schwartz, 2004), both as a means of providing self-assessments directly to students and as a tool to help teachers with question authoring. Much work to date has focused on questions based on a single sentence of the text (Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014), and the ideal of creating deep, conce</context>
</contexts>
<marker>McMillan, 2001</marker>
<rawString>James H. McMillan. 2001. Secondary Teachers&apos; Classroom Assessment and Grading Practices.&amp;quot; Educational Measurement: Issues and Practice 20(1): 20-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<date>2013</date>
<booktitle>Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Le An Ha</author>
</authors>
<title>Computer-Aided Generation of Multiple-Choice Tests.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 Workshop on Building Educational Applications Using Natural Language Processing.</booktitle>
<contexts>
<context position="1934" citStr="Mitkov and Ha, 2003" startWordPosition="288" endWordPosition="291">ther than factoid-oriented. These deep questions are the sort of open-ended queries that require deep thinking and recall rather than a rote response, that span significant amounts of content rather than a single sentence. Unsurprisingly, it is these deep questions that have the greatest educational value (Anderson, 1975; Andre, 1979; McMillan, 2001). They are thus a key assessment mechanism for a spectrum of online educational options, from MOOCs to interactive tutoring systems. As such, the problem of automatic question generation has long been of interest to the online education community (Mitkov and Ha, 2003; Schwartz, 2004), both as a means of providing self-assessments directly to students and as a tool to help teachers with question authoring. Much work to date has focused on questions based on a single sentence of the text (Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014), and the ideal of creating deep, conceptual questions has remained elusive. In this work, we hope to take a significant step towards this challenge by approaching the problem in a somewhat unconventional way. Figure 1: Overview of our ontology-crowd-relevance approach. While one might expect the natural </context>
<context position="6149" citStr="Mitkov and Ha, 2003" startWordPosition="945" endWordPosition="948">stion generation: work that focuses on the grammaticality of natural language question generation, work that focuses on the semantic quality of generated questions, i.e. the “what to ask about” rather than “how to ask it,” and finally work that builds a semantic representation of text in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. </context>
</contexts>
<marker>Mitkov, Ha, 2003</marker>
<rawString>Ruslan Mitkov and Le An Ha. 2003. Computer-Aided Generation of Multiple-Choice Tests. In Proceedings of the HLT-NAACL 2003 Workshop on Building Educational Applications Using Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew M Olney</author>
<author>Arthur C Graesser</author>
<author>Natalie K Person</author>
</authors>
<date>2012</date>
<booktitle>Question Generation from Concept Maps. Dialogue &amp; Discourse</booktitle>
<volume>3</volume>
<issue>2</issue>
<pages>75--99</pages>
<contexts>
<context position="7241" citStr="Olney et al. 2012" startWordPosition="1122" endWordPosition="1125">sen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic role labeling within a sentence in a supervised setting. We hope to continue in this direction of semantic focus, but extend the capabilities of question generation to include openended questions that go far beyond the scope of a single sentence. Other work has taken on the challenge of deeper questions by attempting to build a semantic representation of arbitrary text. This has included work using concept maps over keywords (Olney et al. 2012) and minimal recursion semantics (Yao 2010) to reason over concepts in the text. While the work of (Olney et al. 2012) is impressive in its possibilities, the range of the types of questions that can be generated is restricted by a relatively specific set of relations (e.g. Is-A, PartOf) captured in the ontology of the domain (biology textbook). Mannem et al. (2010) observe as we have that &amp;quot;capturing the exact true meaning of a paragraph is beyond the reach of current NLP systems;&amp;quot; thus, in their system for Shared Task A (for paragraph-level questions (Rus et al. 2010)) they make use of predic</context>
</contexts>
<marker>Olney, Graesser, Person, 2012</marker>
<rawString>Andrew M. Olney, Arthur C. Graesser, and Natalie K. Person. 2012. Question Generation from Concept Maps. Dialogue &amp; Discourse 3(2): 75-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled LDA: A Supervised Topic Model for Credit Attribution in Multi-labeled Corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. 2009. Labeled LDA: A Supervised Topic Model for Credit Attribution in Multi-labeled Corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Brendan Wyse</author>
<author>Paul Piwek</author>
<author>Mihai Lintean</author>
<author>Svetlana Stoyanchev</author>
<author>Cristian Moldovan</author>
</authors>
<title>Overview of The First Question Generation Shared Task Evaluation Challenge.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third Workshop on Question Generation.</booktitle>
<contexts>
<context position="7816" citStr="Rus et al. 2010" startWordPosition="1224" endWordPosition="1227">pt maps over keywords (Olney et al. 2012) and minimal recursion semantics (Yao 2010) to reason over concepts in the text. While the work of (Olney et al. 2012) is impressive in its possibilities, the range of the types of questions that can be generated is restricted by a relatively specific set of relations (e.g. Is-A, PartOf) captured in the ontology of the domain (biology textbook). Mannem et al. (2010) observe as we have that &amp;quot;capturing the exact true meaning of a paragraph is beyond the reach of current NLP systems;&amp;quot; thus, in their system for Shared Task A (for paragraph-level questions (Rus et al. 2010)) they make use of predicate argument structures along with semantic role labeling. However, the generation of these questions is restricted to the first sentence of the paragraph. Though motivated by the same noble impulses of these authors to achieve higher-level questions, our hope is that we can bypass the challenges and constraints of semantic parsing and generate deep questions via a more holistic approach. 890 Figure 2: Coverage properties of our category-section representation: (a) fraction of Wikipedia articles covered by the top j most common Freebase types, grouped by our eight high</context>
</contexts>
<marker>Rus, Wyse, Piwek, Lintean, Stoyanchev, Moldovan, 2010</marker>
<rawString>Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean, Svetlana Stoyanchev, and Cristian Moldovan. 2010. Overview of The First Question Generation Shared Task Evaluation Challenge. In Proceedings of the Third Workshop on Question Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Schwartz</author>
<author>Takako Aikawa</author>
<author>Michel Pahud</author>
</authors>
<title>Dynamic Language Learning Tools.</title>
<date>2004</date>
<booktitle>In Proceedings of STIL/ICALL Symposium on Computer Assisted Learning.</booktitle>
<marker>Schwartz, Aikawa, Pahud, 2004</marker>
<rawString>Lee Schwartz, Takako Aikawa, and Michel Pahud. 2004. Dynamic Language Learning Tools. In Proceedings of STIL/ICALL Symposium on Computer Assisted Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John H Wolfe</author>
</authors>
<title>Automatic Question Generation from Text - an Aid to Independent Study.</title>
<date>1976</date>
<booktitle>In Proceedings of ACM SIGCSE-SIGCUE Joint Symposium on Computer Science Education.</booktitle>
<contexts>
<context position="5972" citStr="Wolfe, 1976" startWordPosition="921" endWordPosition="922">ell as examples of our output and an error analysis (Section 7) before concluding (Section 8). 2 Related Work We consider three aspects of past research in automatic question generation: work that focuses on the grammaticality of natural language question generation, work that focuses on the semantic quality of generated questions, i.e. the “what to ask about” rather than “how to ask it,” and finally work that builds a semantic representation of text in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the p</context>
</contexts>
<marker>Wolfe, 1976</marker>
<rawString>John H. Wolfe. 1976. Automatic Question Generation from Text - an Aid to Independent Study. In Proceedings of ACM SIGCSE-SIGCUE Joint Symposium on Computer Science Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Yi Zhang</author>
</authors>
<title>Question generation with minimal recursion semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of QG2010: The Third Workshop on Question Generation.</booktitle>
<marker>Yao, Zhang, 2010</marker>
<rawString>Xuchen Yao and Yi Zhang. 2010. Question generation with minimal recursion semantics. In Proceedings of QG2010: The Third Workshop on Question Generation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>