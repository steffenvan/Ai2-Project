<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<note confidence="0.821182">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 7-12, Lisbon, Portugal, 2000.
</note>
<title confidence="0.865609">
Corpus-Based Grammar Specialization
</title>
<author confidence="0.703303">
Nicola Cancedda and Christer Samuelsson
</author>
<affiliation confidence="0.639216">
Xerox Research Centre Europe
</affiliation>
<address confidence="0.8228505">
6, chemin de Maupertuis
38240, Meylan, France
</address>
<email confidence="0.842326">
{Nicola.Cancedda, Christer.Samuelsson}ftrce.xerox.com
</email>
<sectionHeader confidence="0.981807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958083333333">
Broad-coverage grammars tend to be highly am-
biguous. When such grammars are used in a
restricted domain, it may be desirable to spe-
cialize them, in effect trading some coverage for
a reduction in ambiguity. Grammar specializa-
tion is here given a novel formulation as an opti-
mization problem, in which the search is guided
by a global measure combining coverage, ambi-
guity and grammar size. The method, applica-
ble to any unification grammar with a phrase-
structure backbone, is shown to be effective in
specializing a broad-coverage LFG for French.
</bodyText>
<sectionHeader confidence="0.995596" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999987571428571">
Expressive grammar formalisms allow grammar
developers to capture complex linguistic gener-
alizations concisely and elegantly, thus greatly
facilitating grammar development and main-
tenance. Broad-coverage grammars, however,
tend to overgenerate considerably, thus allowing
large amounts of spurious ambiguity. If the ben-
efits resulting from more concise grammatical
descriptions are to outweigh the costs of spuri-
ous ambiguity, the latter must be brought down.
We here investigate a corpus-based compilation
technique that reduces overgeneration and spu-
rious ambiguity without jeopardizing coverage
or burdening the grammar developer.
The current work extends previous work
on corpus-based grammar specialization, which
applies variants of explanation-based learning
(EBL) to grammars of natural languages. The
earliest work (Rayner, 1988; Samuelsson and
Rayner, 1991) builds a specialized grammar by
chunking together grammar rule combinations
while parsing training examples. What rules to
combine is specified by hand-coded criteria.
Subsequent work (Rayner and Carter, 1996;
Samuelsson, 1994) views the problem as that
of cutting up each tree in a treebank of cor-
rect parse trees into subtrees, after which the
rule combinations corresponding to the subtrees
determine the rules of the specialized gram-
mar. This approach reports experimental re-
sults, using the SRI Core Language Engine,
(Alshawi, 1992), in the ATIS domain, of more
than a 3-fold speedup at a cost of 5% in gram-
matical coverage, the latter which is compen-
sated by an increase in parsing accuracy. Later
work (Samuelsson, 1994; Sima&apos;an, 1999) at-
tempts to automatically determine appropriate
tree-cutting criteria, the former using local mea-
sures, the latter using global ones.
The current work reverts to the view of EBL
as chunking grammar rules. It extends the
latter work by formulating grammar special-
ization as a global optimization problem over
the space of all possible specialized grammars
with an objective function based on the cover-
age, ambiguity and size of the resulting gram-
mar. The method was evaluated on the LFG
grammar for French developed within the PAR-
GRAM project (Butt et al., 1999), but it is
applicable to any unification grammar with a
phrase-structure backbone where the reference
treebank contains all possible analyses for each
training example, along with an indication of
which one is the correct one.
To explore the space of possible grammars, a
special treebank representation was developed,
called a folded treebank, which allows the ob-
jective function to be computed very efficiently
for each candidate grammar. This representa-
tion relies on the fact that all possible parses
returned by the original grammar for each train-
ing sentence are available and the fact that the
grammar specialization never introduces new
</bodyText>
<page confidence="0.997691">
7
</page>
<bodyText confidence="0.993501555555556">
parses; it only removes existing ones.
The rest of this paper is organized as follows:
Section 2 describes the initial candidate gram-
mar and the operators used to generate new
candidate grammars from any given one. The
function to be maximized is introduced and mo-
tivated in Section 3. The folded treebank repre-
sentation is described in Section 4, while Sec-
tion 5 presents the experimental results.
</bodyText>
<sectionHeader confidence="0.73578" genericHeader="introduction">
2 Unfolding and Specialization
</sectionHeader>
<bodyText confidence="0.999995913043478">
The initial grammar is the grammar underly-
ing the subset of correct parses in the training
set. This is in itself a specialization of the gram-
mar which was used to parse the treebank, since
some rules may not show up in any correct parse
in the training set; experimental results for this
first-order specialization are reported in (Can-
cedda and Samuelsson, 2000). This grammar
is further specialized by inhibiting rule combi-
nations that show up in incorrect parses much
more often than in correct parses.
In more detail, we considered downward un-
folding of grammar rules (see Fig.1).1 A gram-
mar rule is unfolded downwards on one of the
symbols in its right-hand side if it is replaced
by a set of rules, each corresponding to the ex-
pansion of the chosen symbol by means of an-
other grammar rule. More formally, let G =
(E, EN, S, R) be a context-free grammar, and
let r, r&apos; E R, k E ./V-± such that rhs(r) = aA13,
jal= k — 1, lhs(r&apos;) = A, rhs(r&apos;) = 7. The rule
adjunction of r&apos; in the kth position of r is defined
as a new rule RA(r, k, r&apos;) = r&amp;quot;, such that:
</bodyText>
<equation confidence="0.996729333333333">
lhs(r&amp;quot;) = lhs(r)
rhs(r&amp;quot;) = cry/3
For unification grammars, we instead require
lhs(r&apos;) U rhs(r)(k)
lhs(r&amp;quot;) = 0(1hs(r))
rhs(r&amp;quot;) = 0(cry(3)
</equation>
<bodyText confidence="0.99972">
where rhs(r)(k) is the kth symbol of rhs(r),
where X U Y indicates that X and Y unify, and
where 0 is the most general unifier of lhs(r&apos;) and
rhs(r)(k).
The downward rule unfolding of rule r on its
kth position is then defined as:
</bodyText>
<equation confidence="0.638167">
DRU(r, k) =
</equation>
<bodyText confidence="0.94939252173913">
&apos;The converse operation, upward unfolding, was not
used in the current experiments.
fr&apos;Pr&amp;quot;V = RA(r, k,r&amp;quot;)11 if
{r} otherwise
It is easy to see that if all r&apos; e DRU(r, k) are
retained then the new grammar has exactly the
same coverage as the old one. Once the rule
has been unfolded, however, the grammar can
be specialized. This involves inhibiting some
rule combinations by simply removing the cor-
responding newly created rules. Any subset
X C DRU(r, k) is called a downward special-
ization of rule r on the kth element of its rhs.
Given a grammar, all possible (downward)
unfoldings of its rules are considered and, for
each unfolding, the specialization leading to the
best increase in the objective function is deter-
mined. The set of all such best specializations
defines the set of candidate successor grammars.
In the experiments, a simple hill-climbing algo-
rithm was adopted. Other iterative-refinement
schemes, such as simulated annealing, could eas-
ily be implemented.
</bodyText>
<sectionHeader confidence="0.984695" genericHeader="method">
3 The Objective Function
</sectionHeader>
<bodyText confidence="0.999955296296296">
Previous research approached the task of de-
termining which rule combinations to allow ei-
ther by a process of manual trial and error or
by statistical measures based on a collection of
positive examples only: if the original grammar
produces more than a single parse of a sentence,
only the &amp;quot;correct&amp;quot; parse was stored in the tree-
bank. However, we here also have access to all
incorrect parses assigned by the original gram-
mar. This in turn means that we do not need
to estimate ambiguity through some correlated
statistical indicator, since we can measure it di-
rectly simply by checking which parse trees are
licensed by every new candidate grammar G.
There are many possible ways of combining the
counts of correct and incorrect parses in a suit-
able objective function. For the sake of sim-
plicity we opted for a linear combination. How-
ever, simply maximizing correct parses and min-
imizing incorrect ones would most likely lead to
overfitting. In fact, a grammar with one large
flat rule for each correct parse in the treebank
would achieve a very high score during training,
but most likely perform poorly on unseen data.
A way to avoid overfitting consists in penalizing
large grammars by introducing an appropriate
term in the linear combination. The objective
</bodyText>
<page confidence="0.977235">
8
</page>
<figure confidence="0.999927052631579">
B EF
B
D C
A&amp;quot;--&amp;quot;&amp;quot; E F C
Downward unfolding Specialization
of A -&gt;B C on &amp;quot;B&amp;quot;
BC
B-0.- A D
B-■ A
E A
B B C
C&apos;EBC
Specialization
Upward unfolding
of A-&gt; B C
D C
A—o-EFC
B C
C&apos;EBC
</figure>
<figureCaption confidence="0.999704">
Figure 1: Schematic examples of upward and downward unfolding of rules.
</figureCaption>
<bodyText confidence="0.958089652173913">
function Score was thus formulated as follows:
ScoreG = Aeorr CorrG — Ai„IncG — Asize Sizec
where Corr and Inc are the number of correct
and incorrect parses allowed by the grammar,
and Size is the size of the grammar measured
as the total number of symbol occurrences in the
right-hand sides of its rules. A corr and Ainc are
weights controlling the pruning aggressiveness:
their ratioA
—corr Amc intuitively corresponds to
the number of incorrect trees a specialization
must disallow for each disallowed correct tree,
if the specialization is to lead to an improve-
ment over the current grammar. The lower this
ratio is, the more aggressive the pruning is. The
relative value of Asize with respect to the other
As also controls the depth to which the search is
conducted: most specializations result in an in-
crease in grammar size, which tends to be more
and more significant as the number and the size
of rules grows; a larger A size thus has the effect
of stopping the search earlier. Note that only
two of the three weights are independent.
</bodyText>
<sectionHeader confidence="0.99428" genericHeader="method">
4 Treebank Representation
</sectionHeader>
<bodyText confidence="0.99980275">
A folded tree bank is a representation of a set
of parse trees which allows an immediate as-
sessment of the effects of inhibiting specific rule
combinations. It is based on the idea of &amp;quot;fold-
ing&amp;quot; each tree onto a representation of the gram-
mar itself. Any phrase-structure grammar can
be represented as a concatenation/or graph —
a directed bipartite multigraph with an or-node
for each symbol and a concatenation-node for
each rule in the grammar. The present de-
scription covers context-free grammars, but the
scheme can easily be extended to any unifica-
tion grammar with a context-free backbone by
replacing symbol eqality with unification.
Given a grammar G (E, EN, S, R), we can
define a relation 97E and a (partial) function 77R:
</bodyText>
<listItem confidence="0.993997333333333">
• nE c EN X R s.t. (A, r) E nE iff A = lhs(r)
• 71R : R x Af+ E s.t. nR(r, i) = X if
rhs(r) = OX-y , 101 = i — 1
</listItem>
<bodyText confidence="0.948017833333333">
Figure 2 shows the correspondence between a
simple grammar fragment and its concatena-
tion/or graph.
Each tree can be represented by folding it
onto the concatenation/or graph representing
the grammar it was derived with, or, in other
words, by appropriately annotating the graph
itself. If N is the set of nodes of a parse tree
obtained using grammar G, the corresponding
folded tree is a partial function f
f:NxN—&gt;Rx AT+
such that f (n, n&apos;) = (r,k) implies that node n
was expanded using rule r, and that node n&apos; is
its kth daughter in the tree (Fig.3). In the fol-
lowing, we will use the inverse image of (r, k) un-
der f, which we denote 0(r , k) = f-1((r,k)) =
{(n, n&apos;) I f (n, 711) = (r, k)} c N x N. This can in
turn be seen as a partial function
: R x Jf _+ 2Nx N
Disallowing the expansion of the kth element
in the right-hand side of rule r by means of rule
r&apos; (assuming symbols match, i.e., (nR(r,k), r&apos;) E
gm) results in suppressing a tree where:
3n, n&apos;, n&amp;quot; E N, k&apos; e
</bodyText>
<equation confidence="0.774706">
[(n, n&apos;) E cb(r,k) A (re , n&amp;quot;) E 0(r1 , ki)]
</equation>
<bodyText confidence="0.999864571428572">
This check can be performed very efficiently
once the tree is represented by the function,
i.e., once it is folded, as all this requires is to
compare the entries for (r, k) and (r&apos;, k&apos;)2 with a
procedure linear in the size of the entries them-
selves. If we used a more traditional represen-
tation, the same check would require traversing
</bodyText>
<page confidence="0.767554">
2111 fact, it suffices to check the entries for (r&apos;,1).
9
</page>
<figure confidence="0.9285555">
A
nlor\n°
n20 B
AVN na
•n3 Az•N• A
• n60 B
c n5
• n7•n8
</figure>
<equation confidence="0.934634181818182">
O(ri, 1) = {(Tho,rti),(n4,n5)}
(WO) = {(no,n2), (n4,726)}
O(r2, 1) =0
0(r2,2) = 0
0(7&apos;3,1) = {(n2,n3)}
0(7&apos;3,2) = {(n2,n4)}
Or4,1) = {(n6,n7)}
0(7&apos;4,2) = {(n6,n8)}
r 3 (3, a;&apos;
r4
• &apos;
</equation>
<figureCaption confidence="0.996099">
Figure 3: A tree and its folded representation.
</figureCaption>
<bodyText confidence="0.970243142857143">
the whole tree. The worst-case complexity is
still linear in the size of the tree, but in prac-
tice, the number of nodes expanded using any
given rule is much smaller than the total num-
ber of nodes.
Whenever a specialization is performed, all
folded trees that are no longer licensed are
removed; the concatenation/or graph for the
grammar is updated to reflect the introduction
and the elimination of rules; and the annota-
tions on the affected edges are appropriately re-
combined and distributed. If the performed spe-
cialization is X C DRU(r, k) {r} , 3 then the
concatenation/or graph is updated as follows
</bodyText>
<equation confidence="0.999849">
R = RUX\{r}
77E = U {(1hs(r),f)If E X} \ {(1hs(r),r)}
</equation>
<bodyText confidence="0.999535666666667">
where m = arity(r1) = Irhs(r1)1 is the number
of right-hand-side symbols of rule r&apos;. For each
tree that is not eliminated as a consequence of
</bodyText>
<equation confidence="0.741173">
31f X = DRU(r, k) = {r}, then no update is needed.
the specialization we have
0(r&amp;quot;, i),
if r&amp;quot; V X ,r&amp;quot; r&apos;
0(r&amp;quot; , i) \ {(n&apos; ,n&amp;quot;)13n[(n, n&apos;) E 0(r , k)]} ,
if r&amp;quot; = r&apos;
{(n,n&amp;quot;)Pn&apos;,n&amp;quot;, k&apos; [(n , n&apos;) E cb(r, k)
A(n&apos;, n&amp;quot;) E CT&apos;, k&apos;) A (n, n&apos;&amp;quot;) E ck(r ,
if r&amp;quot; = RA(r, k, r&apos;) E X, i &lt; k
{(n, n&amp;quot;1)13721, n&amp;quot; , k&apos; [(n , n&apos;) E 0(r, k) A
(n&apos; , n&amp;quot;) E
</equation>
<bodyText confidence="0.9967038">
(n , n&apos;&amp;quot;) E 0(r, i — m + 1)]}
if r&amp;quot; = RA(r, k,r&apos;) E X, i &gt; k + m — 1,
where again m = arity(r&apos;). These updates can
be implemented efficiently, requiring neither a
traversal of the tree nor of the grammar.
</bodyText>
<sectionHeader confidence="0.996258" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999856166666667">
We specialized a broad-coverage LFG grammar
for French on a corpus of technical documen-
tation using the method described above. The
treebank consisted of 960 sentences which were
all known to be covered by the original gram-
mar. For each sentence, all the trees returned by
</bodyText>
<table confidence="0.716153727272727">
=
TI&apos; V X
r&amp;quot; E X, i &lt;k
r&amp;quot; = RA(r, k, r&apos;) E X,
k&lt;i&lt;k+m-1,
TI&apos; = RA(r, k, r&apos;) E X,
i&gt;k+m-1,
11R(r,i m +1),
{(n, n&amp;quot;)13n1[(n,n&apos;) e 0(r, k) A
(n&apos; ,n&amp;quot;) E q5(r&apos; , i — k+ 1)]}
if r&amp;quot; = RA(r, k, r&apos;) EX,k &lt;i &lt; k +m — 1,
</table>
<page confidence="0.924879">
10
</page>
<equation confidence="0.795873928571428">
R = r2, r3, r4} s.t.
r1:
r2:A—&gt;eA
r3: B f A
r4:B—&gt;fE
r/E= {(A, ri), (A, r2), (B, r3), (1 r 4)1
71R: (ri, 1) -÷ c
(ri, 2) B
(r2, 1) e
(r2,2) -4 A
(r3, 1) f
(r3,2) A
(r4, 1) f
(r4, e
</equation>
<figure confidence="0.719251666666667">
0 0
r4
•
</figure>
<figureCaption confidence="0.963846">
Figure 2: A tiny grammar and the correspond-
ing concatenation/or graph.
</figureCaption>
<bodyText confidence="0.999930873015873">
the original grammar were available, together
with a manually assigned indication of which
was the correct one. The environment used,
the Xerox Linguistic Environment (Kaplan and
Maxwell, 1996) implements a version of opti-
mality theory where parses are assigned &amp;quot;opti-
mality marks&amp;quot; based on a number of criteria,
and are ranked according to these marks. The
set of parses with the best marks are called the
optimal parses for a sentence. The correct parse
was also an optimal parse for 913 out of 960 sen-
tences. Given this, the specialization was aimed
at reducing the number of optimal parses per
sentence.
We ran a series of ten-fold cross-validation ex-
periments; the results are summarized in the ta-
ble in Fig.4. The first line contains values for
the original grammar. The second line contains
measures for the first-order pruning grammar,
i.e., the grammar with all and only those rules
actually used in correct parses in the training
set, with no combination inhibited. Lines 3 and
4 list results for fully specialized grammars. Re-
sults in the third line were obtained with a value
for A„, equal to 15 times the value of Ain, in
the objective function: in other words, during
training we were willing to lose a correct parse
only if at least 15 incorrect parses were canceled
as well. Results in the fourth line were obtained
when this ratio was reduced to 10. The average
number of parses per sentence is reported in the
first column, whereas the second lists the av-
erage number of optimal parses. Coverage was
measured as the fraction of sentences which still
receive the correct parse with the specialized
grammar. To assess the trade off between cover-
age and ambiguity reduction, we computed the
F-score4 considering only optimal parses when
computing precision. This measure should not
be confused with the F-score on labelled brack-
eting reported for many stochastic parsers; here
precision and recall concern perfect matching of
whole trees. Recall is the same as coverage: the
ratio between the number of correct parses pro-
duced by the specialized grammar and the to-
tal number of correct parses (equalling the total
number of sentences in the test set). Precision is
the ratio between the number of correct parses
produced by the specialized grammar and the
total number of parses produced by the same
grammar. The fourth column lists values for the
F-score when equal weight is given to precision
and recall. Intuitively, however, in many cases
missing the correct parse is more of a problem
than returning spurious parses, so we also com-
puted the F-score with a much larger emphasis
on recall, i.e., with a = 0.1. The corresponding
values are listed in the last column.
The average number of parses per sentence,
both optimal and non-optimal, decreases signif-
icantly as more and more aggressive specializa-
tion. is carried out, and consequently, more cov-
erage is lost. The most aggressive form of spe-
</bodyText>
<footnote confidence="0.976405">
4The F-score is the harmonic mean of recall and pre-
cision, where precision is weighted a and recall 1 — a.
</footnote>
<page confidence="0.995997">
11
</page>
<table confidence="0.9942884">
Avg.p/s Avg. o.p./s. Coverage (%) F(a = 0.5) F(a = 0.1)
orig. 1941 4.69 100 35.15 73.05
fo.pruning 184 3.38 89 40.64 71.89
Ac,=15Ain, 82 2.23 86 53.25 76.58
A corr —10A,&amp;quot; 63 2.03 82.5 54.46 74.80
</table>
<figureCaption confidence="0.999149">
Figure 4: Results of the specialization experiments.
</figureCaption>
<bodyText confidence="0.999922285714286">
cialization gives the highest F-score for a = 0.5,
whereas somewhat more conservative parame-
ter settings lead to a better F-score when re-
call is valued more. A speedup of a factor 4
is achieved already by first-order pruning and
remains approximately the same after further
specialization.
</bodyText>
<sectionHeader confidence="0.999307" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999968911764706">
Broad-coverage grammars tend to be highly am-
biguous, which may constitute a serious prob-
lem when using them for natural-language pro-
cessing. Corpus-independent compilation tech-
niques, although useful for increasing efficiency,
do little in terms of reducing ambiguity.
In this paper we proposed a corpus-based
technique for specializing a grammar on a do-
main for which a treebank exists containing all
trees returned for each sentence. This tech-
nique, which builds extensively on previous
work on explanation-based learning for NLP,
consists in casting the problem as an optimiza-
tion problem in the space of all possible spe-
cializations of the original grammar. As initial
candidate grammar, the first-order pruning of
the original grammar is considered. Candidate
successor grammars are obtained through the
downward rule unfolding and specialization op-
erator, that has the desirable property of never
causing previously unseen parses to become
available for sentences in the training set. Can-
didate grammars are then assessed according to
an objecting function combining grammar am-
biguity and coverage, adapted to avoid overfit-
ting. In order to ensure efficient computability
of the objective function, the treebank is pre-
viously folded onto the grammar itself. Exper-
imental results using a broad-coverage lexical-
functional grammar of French show that the
technique allows effectively trading coverage for
ambiguity reduction. Moreover, the parameters
of the objective function can be used to control
the trade off.
</bodyText>
<sectionHeader confidence="0.993862" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999898571428571">
We would like to thank the members of the
MLTT group at the Xerox Research Centre Eu-
rope in Grenoble, France, and the three anony-
mous reviewers for valuable discussions and
comments. This research was funded by the Eu-
ropean TMR network Learning Computational
Grammars.
</bodyText>
<sectionHeader confidence="0.997921" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856647058823">
Hiyan Alshawi, editor. 1992. The Core Language
Engine. MIT Press.
M. Butt, T.H. King, M.E. Nino, and F. Segond.
1999. A Grammar Writer&apos;s Cookbook. CSLI Pub-
lications, Stanford, CA.
Nicola Cancedda and Christer Samuelsson. 2000.
Experiments with corpus-based lfg specialization.
In Proceedings of the NAACL-ANLP 2000 Con-
ference, Seattle, WA.
Ronald Kaplan and John T. Maxwell. 1996.
LFG grammar writer&apos;s workbench. Technical
report, Xerox PARC. Available on-line as
ftp://ftp.parc.xerox.com/pub/lfg/lfgmanual.ps.
Manny Rayner and David Carter. 1996. Fast pars-
ing using pruning and grammar specialization. In
Proceedings of the ACL-96, Santa Cruz, CA.
Manny Rayner. 1988. Applying explanation-based
generalization to natural-language processing.
In Proceedings of the International Conference
on Fifth Generation Computer Systems, Tokyo,
Japan.
Christer Samuelsson and Manny Rayner. 1991.
Quantitative evaluation of explanation-based
learning as an optimization tool for a large-scale
natural language system. In Proceedings of the
IJCAI-91, Sydney, Australia.
Christer Samuelsson. 1994. Grammar specialization
through entropy thresholds. In Proceedings of the
ACL-94, Las Cruces, New Mexico. Available as
cmp-lg/9405022.
Khalil Sima&apos;an. 1999. Learning Efficient Dis-
ambiguation. Ph.D. thesis, Institute for Logic,
Language and Computation, Amsterdam, The
Netherlands.
</reference>
<page confidence="0.998458">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.694601">
<note confidence="0.935471">of CoNLL-2000 and LLL-2000, 7-12, Lisbon, Portugal, 2000.</note>
<title confidence="0.946489">Corpus-Based Grammar Specialization</title>
<author confidence="0.978557">Nicola Cancedda</author>
<author confidence="0.978557">Christer</author>
<affiliation confidence="0.990361">Xerox Research Centre</affiliation>
<address confidence="0.9705815">6, chemin de 38240, Meylan,</address>
<email confidence="0.86749">Nicola.Canceddaftrce.xerox.com</email>
<email confidence="0.86749">Christer.Samuelssonftrce.xerox.com</email>
<abstract confidence="0.997176538461539">Broad-coverage grammars tend to be highly ambiguous. When such grammars are used in a restricted domain, it may be desirable to specialize them, in effect trading some coverage for a reduction in ambiguity. Grammar specialization is here given a novel formulation as an optimization problem, in which the search is guided by a global measure combining coverage, ambiguity and grammar size. The method, applicable to any unification grammar with a phrasestructure backbone, is shown to be effective in specializing a broad-coverage LFG for French.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The Core Language Engine.</title>
<date>1992</date>
<editor>Hiyan Alshawi, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1992</marker>
<rawString>Hiyan Alshawi, editor. 1992. The Core Language Engine. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Butt</author>
<author>T H King</author>
<author>M E Nino</author>
<author>F Segond</author>
</authors>
<title>A Grammar Writer&apos;s Cookbook.</title>
<date>1999</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="3028" citStr="Butt et al., 1999" startWordPosition="448" endWordPosition="451">g accuracy. Later work (Samuelsson, 1994; Sima&apos;an, 1999) attempts to automatically determine appropriate tree-cutting criteria, the former using local measures, the latter using global ones. The current work reverts to the view of EBL as chunking grammar rules. It extends the latter work by formulating grammar specialization as a global optimization problem over the space of all possible specialized grammars with an objective function based on the coverage, ambiguity and size of the resulting grammar. The method was evaluated on the LFG grammar for French developed within the PARGRAM project (Butt et al., 1999), but it is applicable to any unification grammar with a phrase-structure backbone where the reference treebank contains all possible analyses for each training example, along with an indication of which one is the correct one. To explore the space of possible grammars, a special treebank representation was developed, called a folded treebank, which allows the objective function to be computed very efficiently for each candidate grammar. This representation relies on the fact that all possible parses returned by the original grammar for each training sentence are available and the fact that th</context>
</contexts>
<marker>Butt, King, Nino, Segond, 1999</marker>
<rawString>M. Butt, T.H. King, M.E. Nino, and F. Segond. 1999. A Grammar Writer&apos;s Cookbook. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
<author>Christer Samuelsson</author>
</authors>
<title>Experiments with corpus-based lfg specialization.</title>
<date>2000</date>
<booktitle>In Proceedings of the NAACL-ANLP 2000 Conference,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="4469" citStr="Cancedda and Samuelsson, 2000" startWordPosition="682" endWordPosition="686">generate new candidate grammars from any given one. The function to be maximized is introduced and motivated in Section 3. The folded treebank representation is described in Section 4, while Section 5 presents the experimental results. 2 Unfolding and Specialization The initial grammar is the grammar underlying the subset of correct parses in the training set. This is in itself a specialization of the grammar which was used to parse the treebank, since some rules may not show up in any correct parse in the training set; experimental results for this first-order specialization are reported in (Cancedda and Samuelsson, 2000). This grammar is further specialized by inhibiting rule combinations that show up in incorrect parses much more often than in correct parses. In more detail, we considered downward unfolding of grammar rules (see Fig.1).1 A grammar rule is unfolded downwards on one of the symbols in its right-hand side if it is replaced by a set of rules, each corresponding to the expansion of the chosen symbol by means of another grammar rule. More formally, let G = (E, EN, S, R) be a context-free grammar, and let r, r&apos; E R, k E ./V-± such that rhs(r) = aA13, jal= k — 1, lhs(r&apos;) = A, rhs(r&apos;) = 7. The rule ad</context>
</contexts>
<marker>Cancedda, Samuelsson, 2000</marker>
<rawString>Nicola Cancedda and Christer Samuelsson. 2000. Experiments with corpus-based lfg specialization. In Proceedings of the NAACL-ANLP 2000 Conference, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>John T Maxwell</author>
</authors>
<title>LFG grammar writer&apos;s workbench. Technical report, Xerox PARC. Available on-line as ftp://ftp.parc.xerox.com/pub/lfg/lfgmanual.ps.</title>
<date>1996</date>
<contexts>
<context position="13980" citStr="Kaplan and Maxwell, 1996" startWordPosition="2475" endWordPosition="2478">E X, k&lt;i&lt;k+m-1, TI&apos; = RA(r, k, r&apos;) E X, i&gt;k+m-1, 11R(r,i m +1), {(n, n&amp;quot;)13n1[(n,n&apos;) e 0(r, k) A (n&apos; ,n&amp;quot;) E q5(r&apos; , i — k+ 1)]} if r&amp;quot; = RA(r, k, r&apos;) EX,k &lt;i &lt; k +m — 1, 10 R = r2, r3, r4} s.t. r1: r2:A—&gt;eA r3: B f A r4:B—&gt;fE r/E= {(A, ri), (A, r2), (B, r3), (1 r 4)1 71R: (ri, 1) -÷ c (ri, 2) B (r2, 1) e (r2,2) -4 A (r3, 1) f (r3,2) A (r4, 1) f (r4, e 0 0 r4 • Figure 2: A tiny grammar and the corresponding concatenation/or graph. the original grammar were available, together with a manually assigned indication of which was the correct one. The environment used, the Xerox Linguistic Environment (Kaplan and Maxwell, 1996) implements a version of optimality theory where parses are assigned &amp;quot;optimality marks&amp;quot; based on a number of criteria, and are ranked according to these marks. The set of parses with the best marks are called the optimal parses for a sentence. The correct parse was also an optimal parse for 913 out of 960 sentences. Given this, the specialization was aimed at reducing the number of optimal parses per sentence. We ran a series of ten-fold cross-validation experiments; the results are summarized in the table in Fig.4. The first line contains values for the original grammar. The second line conta</context>
</contexts>
<marker>Kaplan, Maxwell, 1996</marker>
<rawString>Ronald Kaplan and John T. Maxwell. 1996. LFG grammar writer&apos;s workbench. Technical report, Xerox PARC. Available on-line as ftp://ftp.parc.xerox.com/pub/lfg/lfgmanual.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
<author>David Carter</author>
</authors>
<title>Fast parsing using pruning and grammar specialization.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL-96,</booktitle>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="1932" citStr="Rayner and Carter, 1996" startWordPosition="266" endWordPosition="269"> down. We here investigate a corpus-based compilation technique that reduces overgeneration and spurious ambiguity without jeopardizing coverage or burdening the grammar developer. The current work extends previous work on corpus-based grammar specialization, which applies variants of explanation-based learning (EBL) to grammars of natural languages. The earliest work (Rayner, 1988; Samuelsson and Rayner, 1991) builds a specialized grammar by chunking together grammar rule combinations while parsing training examples. What rules to combine is specified by hand-coded criteria. Subsequent work (Rayner and Carter, 1996; Samuelsson, 1994) views the problem as that of cutting up each tree in a treebank of correct parse trees into subtrees, after which the rule combinations corresponding to the subtrees determine the rules of the specialized grammar. This approach reports experimental results, using the SRI Core Language Engine, (Alshawi, 1992), in the ATIS domain, of more than a 3-fold speedup at a cost of 5% in grammatical coverage, the latter which is compensated by an increase in parsing accuracy. Later work (Samuelsson, 1994; Sima&apos;an, 1999) attempts to automatically determine appropriate tree-cutting crit</context>
</contexts>
<marker>Rayner, Carter, 1996</marker>
<rawString>Manny Rayner and David Carter. 1996. Fast parsing using pruning and grammar specialization. In Proceedings of the ACL-96, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
</authors>
<title>Applying explanation-based generalization to natural-language processing.</title>
<date>1988</date>
<booktitle>In Proceedings of the International Conference on Fifth Generation Computer Systems,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="1693" citStr="Rayner, 1988" startWordPosition="235" endWordPosition="236">nd to overgenerate considerably, thus allowing large amounts of spurious ambiguity. If the benefits resulting from more concise grammatical descriptions are to outweigh the costs of spurious ambiguity, the latter must be brought down. We here investigate a corpus-based compilation technique that reduces overgeneration and spurious ambiguity without jeopardizing coverage or burdening the grammar developer. The current work extends previous work on corpus-based grammar specialization, which applies variants of explanation-based learning (EBL) to grammars of natural languages. The earliest work (Rayner, 1988; Samuelsson and Rayner, 1991) builds a specialized grammar by chunking together grammar rule combinations while parsing training examples. What rules to combine is specified by hand-coded criteria. Subsequent work (Rayner and Carter, 1996; Samuelsson, 1994) views the problem as that of cutting up each tree in a treebank of correct parse trees into subtrees, after which the rule combinations corresponding to the subtrees determine the rules of the specialized grammar. This approach reports experimental results, using the SRI Core Language Engine, (Alshawi, 1992), in the ATIS domain, of more th</context>
</contexts>
<marker>Rayner, 1988</marker>
<rawString>Manny Rayner. 1988. Applying explanation-based generalization to natural-language processing. In Proceedings of the International Conference on Fifth Generation Computer Systems, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
<author>Manny Rayner</author>
</authors>
<title>Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale natural language system.</title>
<date>1991</date>
<booktitle>In Proceedings of the IJCAI-91,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="1723" citStr="Samuelsson and Rayner, 1991" startWordPosition="237" endWordPosition="240">rate considerably, thus allowing large amounts of spurious ambiguity. If the benefits resulting from more concise grammatical descriptions are to outweigh the costs of spurious ambiguity, the latter must be brought down. We here investigate a corpus-based compilation technique that reduces overgeneration and spurious ambiguity without jeopardizing coverage or burdening the grammar developer. The current work extends previous work on corpus-based grammar specialization, which applies variants of explanation-based learning (EBL) to grammars of natural languages. The earliest work (Rayner, 1988; Samuelsson and Rayner, 1991) builds a specialized grammar by chunking together grammar rule combinations while parsing training examples. What rules to combine is specified by hand-coded criteria. Subsequent work (Rayner and Carter, 1996; Samuelsson, 1994) views the problem as that of cutting up each tree in a treebank of correct parse trees into subtrees, after which the rule combinations corresponding to the subtrees determine the rules of the specialized grammar. This approach reports experimental results, using the SRI Core Language Engine, (Alshawi, 1992), in the ATIS domain, of more than a 3-fold speedup at a cost </context>
</contexts>
<marker>Samuelsson, Rayner, 1991</marker>
<rawString>Christer Samuelsson and Manny Rayner. 1991. Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale natural language system. In Proceedings of the IJCAI-91, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
</authors>
<title>Grammar specialization through entropy thresholds.</title>
<date>1994</date>
<booktitle>In Proceedings of the ACL-94,</booktitle>
<location>Las Cruces, New</location>
<note>Available as cmp-lg/9405022.</note>
<contexts>
<context position="1951" citStr="Samuelsson, 1994" startWordPosition="270" endWordPosition="271">e a corpus-based compilation technique that reduces overgeneration and spurious ambiguity without jeopardizing coverage or burdening the grammar developer. The current work extends previous work on corpus-based grammar specialization, which applies variants of explanation-based learning (EBL) to grammars of natural languages. The earliest work (Rayner, 1988; Samuelsson and Rayner, 1991) builds a specialized grammar by chunking together grammar rule combinations while parsing training examples. What rules to combine is specified by hand-coded criteria. Subsequent work (Rayner and Carter, 1996; Samuelsson, 1994) views the problem as that of cutting up each tree in a treebank of correct parse trees into subtrees, after which the rule combinations corresponding to the subtrees determine the rules of the specialized grammar. This approach reports experimental results, using the SRI Core Language Engine, (Alshawi, 1992), in the ATIS domain, of more than a 3-fold speedup at a cost of 5% in grammatical coverage, the latter which is compensated by an increase in parsing accuracy. Later work (Samuelsson, 1994; Sima&apos;an, 1999) attempts to automatically determine appropriate tree-cutting criteria, the former us</context>
</contexts>
<marker>Samuelsson, 1994</marker>
<rawString>Christer Samuelsson. 1994. Grammar specialization through entropy thresholds. In Proceedings of the ACL-94, Las Cruces, New Mexico. Available as cmp-lg/9405022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima&apos;an</author>
</authors>
<title>Learning Efficient Disambiguation.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Institute for Logic, Language and Computation,</institution>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="2466" citStr="Sima&apos;an, 1999" startWordPosition="358" endWordPosition="359">ne is specified by hand-coded criteria. Subsequent work (Rayner and Carter, 1996; Samuelsson, 1994) views the problem as that of cutting up each tree in a treebank of correct parse trees into subtrees, after which the rule combinations corresponding to the subtrees determine the rules of the specialized grammar. This approach reports experimental results, using the SRI Core Language Engine, (Alshawi, 1992), in the ATIS domain, of more than a 3-fold speedup at a cost of 5% in grammatical coverage, the latter which is compensated by an increase in parsing accuracy. Later work (Samuelsson, 1994; Sima&apos;an, 1999) attempts to automatically determine appropriate tree-cutting criteria, the former using local measures, the latter using global ones. The current work reverts to the view of EBL as chunking grammar rules. It extends the latter work by formulating grammar specialization as a global optimization problem over the space of all possible specialized grammars with an objective function based on the coverage, ambiguity and size of the resulting grammar. The method was evaluated on the LFG grammar for French developed within the PARGRAM project (Butt et al., 1999), but it is applicable to any unificat</context>
</contexts>
<marker>Sima&apos;an, 1999</marker>
<rawString>Khalil Sima&apos;an. 1999. Learning Efficient Disambiguation. Ph.D. thesis, Institute for Logic, Language and Computation, Amsterdam, The Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>