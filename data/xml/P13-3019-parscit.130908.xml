<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001417">
<title confidence="0.998289">
A New Syntactic Metric for Evaluation of Machine Translation
</title>
<author confidence="0.997007">
Melania Duma
</author>
<affiliation confidence="0.975764">
Department of Computer
Science
University of Hamburg
</affiliation>
<address confidence="0.9433445">
Vogt-Kölln-Straße 30
22527 Hamburg
</address>
<email confidence="0.843154">
duma@informatik.uni
-hamburg.de
</email>
<author confidence="0.977328">
Cristina Vertan
</author>
<affiliation confidence="0.981259">
Faculty for Language,
Literature and Media
University of Hamburg
</affiliation>
<address confidence="0.6741985">
Von Melle Park 6
20146 Hamburg
</address>
<email confidence="0.6054345">
cristina.vertan@uni
-hamburg.de
</email>
<author confidence="0.997826">
Wolfgang Menzel
</author>
<affiliation confidence="0.975742">
Department of Computer
Science
University of Hamburg
</affiliation>
<address confidence="0.9422825">
Vogt-Kölln-Straße 30
22527 Hamburg
</address>
<email confidence="0.8003665">
menzel@informatik.uni
-hamburg.de
</email>
<sectionHeader confidence="0.926895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.4761690625">
Machine translation (MT) evaluation aims at
measuring the quality of a candidate
translation by comparing it with a reference
translation. This comparison can be
performed on multiple levels: lexical,
syntactic or semantic. In this paper, we
propose a new syntactic metric for MT
evaluation based on the comparison of the
dependency structures of the reference and
the candidate translations. The dependency
structures are obtained by means of a
Weighted Constraints Dependency Grammar
parser. Based on experiments performed on
English to German translations, we show that
the new metric correlates well with human
judgments at the system level.
</bodyText>
<sectionHeader confidence="0.997867" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944522727273">
Research in automatic machine translation (MT)
evaluation has the goal of developing a set of
computer-based methods that measure accurately
the correctness of the output generated by a MT
system. However, this task is a difficult one
mainly because there is no unique reference
output that can be used in the comparison with
the candidate translation. One sentence can have
several correct translations. Thus, it is difficult to
decide if the deviation from an existing reference
translation is a matter of style (the use of
synonymous words, different syntax etc.) or a
real translation error.
Most of the automatic evaluation metrics
developed so far are focused on the idea of
lexical matching between the tokens of one or
more reference translations and the tokens of a
candidate translation. However, structural
similarity between a reference translation and a
candidate one cannot be captured by lexical
features. Therefore, research in MT evaluation
experiences a gradual shift of focus from lexical
metrics to structural ones, whether they are
syntactic or semantic or a combination of both.
This paper introduces a new syntactic
automatic MT evaluation method. At this stage
of research the new metric is evaluating
translations from any source language into
German. Given that a set of constraint-based
grammar rules are available for that language,
extensions to other target languages are anytime
possible. The chosen tool for providing syntactic
information for German is the Weighted
Constraints Dependency Grammar (WCDG)
parser (Menzel and Schröder, 1998), which is
preferred over other parsers because of its
robustness to ungrammatical input, as it is typical
for MT output. The rest of this paper is organized
as follows. In Section 2 the state of the art in MT
evaluation is presented, while in Section 3 the
new syntactic metric is described. The
experimental setup and results are presented in
Section 4. The last section deals with the
conclusions and future work.
</bodyText>
<sectionHeader confidence="0.338458" genericHeader="method">
2 State of the art
</sectionHeader>
<bodyText confidence="0.9934736">
Automatic evaluation of MT systems relies on
the existence of at least one reference1 created by
a human annotator. Using an automatic method
of evaluation a score is computed, based on the
similarity between the output of the MT system
and the reference. This similarity can be
computed at different levels: lexical, syntactic or
semantic. At the lexical level, the metrics
developed so far can be divided into two major
categories: n-gram based and edit distance based.
</bodyText>
<footnote confidence="0.991799333333333">
1 We will use the term reference for the reference
translation and the term translation for the candidate
translation.
</footnote>
<page confidence="0.895227">
130
</page>
<note confidence="0.531028">
Proceedings of the ACL Student Research Workshop, pages 130–135,
</note>
<page confidence="0.370339">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</page>
<bodyText confidence="0.999977616666667">
Among the n-gram based metrics, one of the
most popular methods of evaluation is BLEU
(Papineni et al., 2001). It provides a score that is
computed as the summed number of n-grams
shared by the references and the output, divided
by the total number of n-grams. Lexical metrics
that use the edit distance are constructed using
the Levenshtein distance applied at the word
level. Among these metrics, WER (Niessen et
al., 2000) is the one which is used more
frequently; it calculates the minimal number of
insertion, substitutions and deletions needed to
transform the candidate translation into a
reference.
Metrics based on lexical matching suffer from
not being able to consider the variation
encountered in natural language. Thus, they
reward a low score to an otherwise fluent and
syntactically correct candidate translation, if it
does not share a certain number of words with
the set of references. Because of this, major
disagreements between the scores assigned by
BLEU and human judgments have been reported
in Koehn and Monz (2006) and Callison-Burch
et al. (2006). Another disadvantage is that many
of them cannot be applied at the segment level,
which is often needed in order to better assess
the quality of MT output and to determine which
improvements should be made to the MT system.
Because of these disadvantages there is an
increasing need for other approaches to MT
evaluation that go beyond the lexical level of the
phrases compared.
In Liu and Gildea (2005), three syntactic
evaluation metrics are presented. The first of
these metrics, the Subtree Metric (SMT), is
based on determining the number of subtrees that
can be found in both the candidate translation
and the reference phrase structure trees. The
second metric, which is a kernel-based subtree
metric, is defined as the maximum of the cosine
measure between the MT output and the set of
references. The third metric proposed computes
the number of matching n-grams between the
headword chains of the reference and the
candidate translation dependency trees obtained
using the parser described in (Collins, 1999).
The idea of syntactic similarity is further
exploited in Owczarzak et al. (2007) which uses
a Lexical Functional Grammar (LFG) parser.
The similarity between the translation and the
reference is computed using the precision and the
recall of the dependencies that illustrate the pair
of sentences. Furthermore, paraphrases are used
in order to improve the correlation with human
judgments. Another set of syntactic metrics has
been introduced in Gimenez (2008); some of
them are based on analyzing different types of
linguistic information (i.e. part-of-speech or
lemma).
</bodyText>
<sectionHeader confidence="0.94962" genericHeader="method">
3 A new syntactic automatic metric
</sectionHeader>
<bodyText confidence="0.999979571428571">
In this section we introduce the new syntactic
metric which is based on constraint dependency
parsing. In the first subsection, the WCDG parser
is presented, together with the advantages of
using this parser over the other ones available,
while the second subsection provides a detailed
description of the new metric.
</bodyText>
<subsectionHeader confidence="0.9544785">
3.1 Weighted Constraint Dependency
Grammar Parser
</subsectionHeader>
<bodyText confidence="0.999962756756757">
Our research was performed using a dependency
parser. We decided on this type of parser
because, as opposed to constituent parsers, it
offers the possibility of better representing non-
projective structures. Moreover, it has been
shown in Kuebler and Prokic (2006) that, at least
in the case of German, the results achieved by a
dependency parser are more accurate than the
ones obtained when parsing using constituent
parsers, and this is because dependency parsers
can handle better long distance relations and
coordination.
The goal of constraint dependency
grammars (CDG) is to create dependency
structures that represent a given phrase (Schröder
et al., 2000) on parallel levels of analysis. A
relation between two words in a sentence is
represented using an edge, which connects the
regent and the dependent. Edges are annotated
using labels in order to distinguish between
different types of relations. A constraint is made
up of a logical formula that describes properties
of the tree. One property, for example, that is
always enforced is that no word can have more
than one regent on any level at a time. During the
analysis, each of the constraints is applied to
every edge or every pair of edges belonging to
the constructed dependency parse tree. The main
advantage of using constraint dependency
grammars over dependency grammars based on
generative rules is that they can deal better with
free word order languages (Foth, 2004).
Weighted Constraint Dependency Grammar
(WCDG) (Menzel and Schröder, 1998) assigns
different weights to the constraints of the
grammar. Every constraint in WCDG is assigned
a score which is a number between 0.0 and 1.0,
</bodyText>
<page confidence="0.992126">
131
</page>
<bodyText confidence="0.999980892857143">
while the general score of a parse is calculated as
the product of all the scores of all the instances
of constraints that have not been satisfied. Rules
that have a score of 0 are called hard rules,
meaning that they cannot be ignored, which is
the case of the one regent only rule mentioned
earlier. The advantage of using graded
constraints, as opposed to crisp ones, stems from
the fact that weights allow the parser to tolerate
constraint violations, which, in turn, makes the
parser robust against ungrammaticality. The
parser was evaluated using different types of
texts, and the results show that it has an accuracy
between 80% and 90% in computing correct
dependency attachments depending on the type
of text (Foth et al., 2004a).
The benefit of using WCDG over other parsers
is that it provides further information on a parse,
like the general score of the parse and the
constraints that are violated by the final result.
This information can be further explored in order
to perform an error analysis. Moreover, because
of the fact that the candidate translations are
sometimes not well-formed, parsing them
represents a challenge. However, WCDG will
always provide a final result, in the form of a
dependency structure, even though it might have
a low score due to the violated constraints.
</bodyText>
<subsectionHeader confidence="0.99968">
3.2 Description of the metric
</subsectionHeader>
<bodyText confidence="0.99439858974359">
In order to define a new syntactic metric for MT
evaluation, we have incorporated the WCDG
parser in the process of evaluation. Because the
output of the WCDG parser is a dependency tree,
we have looked into techniques of measuring
how similar two trees are. Our aim was to
determine whether a tree similarity metric
applied on the two dependency parse trees would
prove to be an efficient way of capturing the
similarity between the reference and the
translation. Let us consider this example, in
which the reference sentence is `Die schwarze
Katze springt schnell auf den roten Stuhl.”(engl.
The black cat jumps quickly on the red chair)
and the candidate translation is`Auf den roten
Stuhl schnell springt die schwarze Katze”(engl.
On the red chair quickly jumps the red cat). Even
though the word order of the two segments is
quite different, and the translation has an
incorrect syntax, they roughly have the same
meaning. We present in Figure 1 the dependency
parse trees obtained using WCDG for the
sentences considered. We can observe that the
general structure of the translation is similar to
that of the reference, the only difference being
the reverse order between the left subtree and the
right subtree. The tree similarity measure that we
chose to use was the All Common Embedded
Subtrees (ACET) (Lin et al., 2008) similarity.
Given a tree T, an embedded subtree is obtained
by removing one or more nodes, except for the
root, from the tree T. The idea behind ACET is
that, the more substructures two trees share, the
more similar they are. Therefore, ACET is
defined as the number of common embedded
subtrees shared between two trees. The results
reported in Lin et al. (2008) show that ACET
outperforms tree edit distance (Zhang and
Shasha, 1989) in terms of efficiency.
</bodyText>
<figureCaption confidence="0.876899">
Figure 1. Example of dependency parse trees for
reference and candidate translations
</figureCaption>
<bodyText confidence="0.999974941176471">
In our experiments, we have applied the ACET
algorithm, and computed the number of common
embedded subtrees between the dependency
parse trees of the hypothesis and the reference.
Because of the additional information provided
by the parsing, pre-processing of the output of
the WCDG parser was necessary in order to
transform the dependency tree into a general tree.
We first removed the labels assigned to every
edge, but maintained the nodes and the left to
right order between them.
In the following, we will refer to the new
proposed metric using CESM (Common
Embedded Subtree Metric). CESM was
computed using the precision, the recall and the
F-measure of the common embedded subtrees of
the reference and the translation:
</bodyText>
<page confidence="0.9911">
132
</page>
<bodyText confidence="0.999977">
where treeref and treehyp represent the
preprocessed dependency trees of the reference
and the hypothesis translations.
</bodyText>
<sectionHeader confidence="0.979593" genericHeader="method">
4 Experimental setup and evaluation
</sectionHeader>
<bodyText confidence="0.999804603773585">
In order to determine how accurate CESM is in
capturing the similarity between references and
translations, we evaluated it at the system level
and at the segment level. The evaluation was
conducted using data provided by the NAACL
2012 WMT workshop (Callison-Burch et al.,
2012). The test data for the workshop consisted
of 99 translated news articles in English,
German, French, Spanish and Czech.
At the system level, the initial German test set
provided at the workshop was filtered according
to the length of segments. This was done in order
to limit the time requirements of WCDG. As a
result, 500 segments with a length between 50
and 80 characters were extracted from the
German reference file. In the next step, we
arbitrarily selected the outputs of 7 of the 15
systems that were submitted for evaluation in the
English to German translation task: DFKI
(Vilar, 2012), JHU (Ganitkevitch et al., 2012),
KIT (Niehues et al., 2012), UK (Zeman, 2012)
and three anonymized system outputs referred to
as OnlineA, OnlineB, OnlineC.
After this initial step of filtering the data, the 7
systems were evaluated by calculating the CESM
score for every pair of reference and translation
segments corresponding to a system. The
average scores obtained are depicted in Table 1.
Evaluation of the metric at the system level was
performed by measuring the correlation of the
CESM metric with human judgments using
Spearman&apos;s rank correlation coefficient p:
where n represents the number of MT systems
considered during evaluation, and di2 represents
the difference between the ranks, assigned to a
system, by the metric and the human judgments.
The minimum value of p is -1, when there is no
correlation between the two rankings, while the
maximum value is 1, when the two rankings
correlate perfectly (Callison-Burch et al., 2012).
In order to compute the p score, the scores
attributed to every system by CESM, were
converted into ranks. From the different ranking
strategies that were presented by the WMT12
workshop, the standard ranking order was
chosen. The p rank correlation coefficient was
calculated as being p = 0.92, which shows there
is a strong correlation between the results of
CESM and the human judgments. In order to
better assess the quality of CESM, the test set
was also evaluated using NIST (Doddington,
2002), which managed to obtain the same rank
correlation coefficient of p = 0.92.
</bodyText>
<table confidence="0.987334888888889">
No. System CESM NIST
name score score
1 DFKI 0.069 4.7709
2 JHU 0.073 4.9904
3 KIT 0.090 5.1358
4 OnlineA 0.093 5.3039
5 OnlineB 0.091 5.3039
6 OnlineC 0.085 4.8022
7 UK 0.075 4.6579
</table>
<tableCaption confidence="0.999396">
Table 1. System level evaluation results
</tableCaption>
<bodyText confidence="0.999948555555555">
The first step in evaluating at the segment level
was filtering the initial test set provided by the
WMT12 workshop. For this purpose, 2500
reference and translation segments were selected
with a length between 50 and 80 characters. The
Kendall tau rank correlation coefficient was
calculated in order to measure the correlation
with human judgments, where Kendall tau
(Callison-Burch et al., 2012) is defined as:
In order to compute the value of Kendall tau, we
determined the number of concordant pairs and
the number of discordant pairs of judgments.
Similarly to the guideline followed during the
WMT12 workshop (Callison-Burch et al., 2012),
we penalized ties given by CESM and ignored
ties assigned by the human judgments. The
obtained result was a correlation of 0.058. As a
term of comparison, the highest correlation for
segment level reported in Callinson-Burch et al.
(2012) was 0.19 obtained by TerrorCat (Fishel et
al., 2012) and the lowest was BlockErrCats
(Popovic, 2012) with 0.040. However, these
results were obtained by evaluating on the entire
test set. The rather low correlation result we
obtained can be partially explained by the fact
that only one judgment of a pair of reference and
translation was taken into account. It will be
</bodyText>
<page confidence="0.99802">
133
</page>
<bodyText confidence="0.999340666666667">
interesting to see how the averaging of the ranks
of a translation influences the correlation
coefficient.
produce new references has increased the BLEU
score, therefore this is an approach that will be
further investigated.
</bodyText>
<sectionHeader confidence="0.943418" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999992897959184">
In this paper, a new evaluation metric for MT
was introduced, which is based on the
comparison of dependency parse trees. The
dependency trees were obtained using the
WCDG German parser. The reason why we
chose this parser was that, due to its architecture,
it is able to handle ungrammatical and
ambiguous input data. The experiments
conducted so far show that using the data made
available at the NAACL 2012 WMT workshop,
CESM correlates well with the human judgments
at the system level. One of the future
experiments that we intend to perform is to
assess metric quality on the entire evaluation set.
Moreover, we plan to compare CESM with other
tree-based MT metrics. Furthermore, the
WMT12 workshop offers different ranking
possibilities, like the ones presented in Bojar et
al (2011) and in Lopez (2012). It will be
determined how much are the segment level
evaluation results influenced by these ranking
orders.
One limitation of the proposed metric is that,
at the moment it is restricted to translations from
any source language to German as a target
language. Because of this reason, we plan to
extend the metric to other languages and see how
well it performs in different settings. In further
experiments we also intend to test CESM using
statistical based dependency parsers, like the
Malt Parser (Nivre et al., 2007) and the MST
parser (McDonald et al., 2006), in order to
decide whether the choice of parser influences
the performance of the metric.
Another approach that we will explore for
improving CESM is to compare dependency
parse trees using the base form and the part-of-
speech of the tokens, instead of the exact lexical
match. We will try this approach in order to
avoid penalizing lexical variation.
The accuracy of CESM can be further
increased by the use of paraphrases, which can
be obtained by using a German thesaurus or a
lexical resource like GermaNet (Hamp and
Feldweg, 1997). Furthermore, a technique like
the one described in Owczarzak (2008) can be
implemented for generating domain specific
paraphrases. The results reported show that the
use of this kind of paraphrases in order to
</bodyText>
<sectionHeader confidence="0.994554" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996626142857143">
This work was funded by the University of
Hamburg Doctoral Fellowships in accordance
with the Hamburg Act for the Promotion of
Young Researchers and Artists (HmbNFG), and
the EAMT Project “Using Syntactic and
Semantic Information in the Evaluation of
Corpus-based Machine Translation”.
</bodyText>
<sectionHeader confidence="0.98854" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.998727216216217">
O. Bojar, M. Ercegovčević, M Popel and O. Zaidan.
2011. A Grain of Salt for the WMT Manual
Evaluation. Proceedings of the Sixth Workshop
on Statistical Machine Translation.
C. Callison-Burch, M. Osborne and P. Koehn. 2006.
Re-evaluating the Role of Bleu in Machine
Translation Research. Proceedings of EACL-
2006.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R.
Soricut and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine
Translation. Proceedings of WMT12.
M. J. Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
G. Doddington. 2002. Automatic Evaluation of
Machine Translation Quality Using N-gram
Co-Occurrence Statistics. Proceedings of the
2nd International Conference on Human Language
Technology.
K. Foth. 2004. Writing weighted constraints for
large de-pendency grammars. Recent Advances
in De-pendency Grammar, Workshop COLING
2004.
K. Foth, M. Daum and W. Menzel. 2004a. A broad-
coverage parser for German based on
defeasible constraints. KONVENS 2004,
Beiträge zur 7, Konferenz zur Verarbeitung
natürlicher Sprache, Wien.
K. Foth, M. Daum and W. Menzel. 2004b.
Interactive grammar development with
WCDG. Proc. of the 42nd Annual Meeting of the
Association for Com-putational Linguistics.
K. Foth, T. By and W. Menzel. 2006. Guiding a
con-straint dependency parser with supertags.
Proceedings of the 21st Int. Conf. on
Computational Linguistics.
</reference>
<page confidence="0.994053">
134
</page>
<reference confidence="0.999220910891089">
M. Fishel, R. Sennrich, M. Popovic and O. Bojar.
2012. TerrorCat: a translation error
categorization-based MT quality metric.
Proceedings of the Seventh Workshop on
Statistical Machine Translation.
J. Ganitkevitch, Y. Cao, J. Weese, M. Post and C.
Callison-Burch. 2012. Joshua 4.0: Packing,
PRO, and paraphrases. Proceedings of the
Seventh Workshop on Statistical Machine
Translation.
J. Gimenez. 2008. Empirical Machine Translation
and its Evaluation. Ph. D. thesis.
B. Hamp and H. Feldweg. 1997. GermaNet - a
Lexical-Semantic Net for German. Proc. of
ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for
NLP Applications.
P. Koehn and C. Monz. 2006. Manual and
Automatic Evaluation of Machine Translation
between European Languages. NAACL 2006
Workshop on Statistical Machine Translation.
P. Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
S. Kübler and J. Prokic. 2006. Why is German
Dependency Parsing more Reliable than
Constituent Parsing?. Proceedings of the Fifth
International Work-shop on Treebanks and
Linguistic Theories.
Z. Lin, H. Wang, S. McClean and C. Liu. 2008. All
Common Embedded Subtrees for Measuring
Tree Similarity. International Symposium on
Computational Intelligence and Design.
D. Liu and D. Gildea. 2005. Syntactic Features for
Evaluation of Machine Translation. ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or
Summarization.
A. Lopez. 2012. Putting human assessments of
machine translation systems in order.
Proceedings of the Seventh Workshop on
Statistical Machine Translation.
R. McDonald, K. Lerman and F. Pereira. 2006.
Multilingual Dependency Parsing with a Two-
Stage Discriminative Parser. Tenth Conference
on Computational Natural Language Learning.
W. Menzel and I. Schröder. 1998. Decision
Procedures for Dependency Parsing Using
Graded Constraints. Workshop On Processing
Of Dependency-Based Grammars.
J. Niehues, Y. Zhang, M. Mediani, T. Herrmann, E.
Cho and A. Waibel. 2012. The karlsruhe institute
of technology translation systems for the WMT
2012. Proceedings of the Seventh Workshop on
Statistical Machine Translation.
S. Niessen, F. J. Och, G. Leusch and H. Ney. 2000.
An Evaluation Tool for Machine Translation:
Fast Evaluation for MT Research. Proceedings
of the 2nd International Conference on Language
Resources and Evaluation (LREC).
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S.
Kübler, S. Marinov and E. Marsi. 2007.
MaltParser: A language-independent system
for data-driven dependency parsing. Natural
Language Engineering.
K. Owczarzak, J. van Genabith and A. Way. 2007.
Dependency-based automatic evaluation for
machine translation. Proceedings of SSST,
NAACL-HLT 2007 / AMTA Workshop on Syntax
and Structure in Statistical Translation.
K. Owczarzak. 2008. A Novel Dependency-Based
Evaluation Metric for Machine Translation,
Ph.D. thesis.
K. Papineni, S. Roukos, T. Ward and W.-J. Zhu.
2001. Bleu: a method for automatic evaluation
of machine translation. RC22176 (Technical
Report), IBM T.J. Watson Research Center.
M. Popovic. 2012. Class error rates for evaluation
of machine translation output. Proceedings of
the Seventh Workshop on Statistical Machine
Translation.
I. Schröder, W. Menzel, K. Foth and M. Schulz. 2000.
Modeling dependency grammar with
restricted constraints. Traitement Automatique
des Langues.
I. Schröder, H. Pop, W. Menzel and K. Foth. 2001.
Learning grammar weights using genetic
algorithms. Proceedings Euroconference Recent
Advances in Natural Language Processing.
I. Schröder. 2002. Natural Language Parsing with
Graded Constraints. Ph.D. thesis, Dept. of
Computer Science, University of Hamburg.
D. Vilar. 2012. DFKI’s SMT system for WMT
2012. Proceedings of the Seventh Workshop on
Statistical Machine Translation.
D. Zeman. 2012. Data issues of the multilingual
translation matrix. Proceedings of the Seventh
Workshop on Statistical Machine Translation.
K. Zhang and D. Shasha. 1989. Simple fast
algorithms for the editing distance between
trees and related problems. SIAM Journal on
Computing.
</reference>
<page confidence="0.998776">
135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.099677">
<title confidence="0.9999">A New Syntactic Metric for Evaluation of Machine Translation</title>
<author confidence="0.987161">Melania</author>
<affiliation confidence="0.972012">Department of University of Vogt-Kölln-Straße</affiliation>
<address confidence="0.998971">22527 Hamburg</address>
<email confidence="0.889744">duma@informatik.uni-hamburg.de</email>
<author confidence="0.641964">Cristina</author>
<affiliation confidence="0.659833">Faculty for Literature and University of Von Melle Park</affiliation>
<address confidence="0.999542">20146 Hamburg</address>
<email confidence="0.957824">cristina.vertan@uni-hamburg.de</email>
<author confidence="0.955244">Wolfgang</author>
<affiliation confidence="0.970899333333333">Department of University of Vogt-Kölln-Straße</affiliation>
<address confidence="0.99447">22527 Hamburg</address>
<email confidence="0.952017">menzel@informatik.uni-hamburg.de</email>
<abstract confidence="0.998020117647059">Machine translation (MT) evaluation aims at measuring the quality of a candidate translation by comparing it with a reference translation. This comparison can be performed on multiple levels: lexical, syntactic or semantic. In this paper, we propose a new syntactic metric for MT evaluation based on the comparison of the dependency structures of the reference and the candidate translations. The dependency structures are obtained by means of a Weighted Constraints Dependency Grammar parser. Based on experiments performed on English to German translations, we show that the new metric correlates well with human judgments at the system level.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>M Ercegovčević</author>
<author>M Popel</author>
<author>O Zaidan</author>
</authors>
<title>A Grain of Salt for the WMT Manual Evaluation.</title>
<date>2011</date>
<booktitle>Proceedings of the Sixth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="17657" citStr="Bojar et al (2011)" startWordPosition="2834" endWordPosition="2837">German parser. The reason why we chose this parser was that, due to its architecture, it is able to handle ungrammatical and ambiguous input data. The experiments conducted so far show that using the data made available at the NAACL 2012 WMT workshop, CESM correlates well with the human judgments at the system level. One of the future experiments that we intend to perform is to assess metric quality on the entire evaluation set. Moreover, we plan to compare CESM with other tree-based MT metrics. Furthermore, the WMT12 workshop offers different ranking possibilities, like the ones presented in Bojar et al (2011) and in Lopez (2012). It will be determined how much are the segment level evaluation results influenced by these ranking orders. One limitation of the proposed metric is that, at the moment it is restricted to translations from any source language to German as a target language. Because of this reason, we plan to extend the metric to other languages and see how well it performs in different settings. In further experiments we also intend to test CESM using statistical based dependency parsers, like the Malt Parser (Nivre et al., 2007) and the MST parser (McDonald et al., 2006), in order to de</context>
</contexts>
<marker>Bojar, Ercegovčević, Popel, Zaidan, 2011</marker>
<rawString>O. Bojar, M. Ercegovčević, M Popel and O. Zaidan. 2011. A Grain of Salt for the WMT Manual Evaluation. Proceedings of the Sixth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<date>2006</date>
<booktitle>Re-evaluating the Role of Bleu in Machine Translation Research. Proceedings of EACL2006.</booktitle>
<contexts>
<context position="4969" citStr="Callison-Burch et al. (2006)" startWordPosition="763" endWordPosition="766"> used more frequently; it calculates the minimal number of insertion, substitutions and deletions needed to transform the candidate translation into a reference. Metrics based on lexical matching suffer from not being able to consider the variation encountered in natural language. Thus, they reward a low score to an otherwise fluent and syntactically correct candidate translation, if it does not share a certain number of words with the set of references. Because of this, major disagreements between the scores assigned by BLEU and human judgments have been reported in Koehn and Monz (2006) and Callison-Burch et al. (2006). Another disadvantage is that many of them cannot be applied at the segment level, which is often needed in order to better assess the quality of MT output and to determine which improvements should be made to the MT system. Because of these disadvantages there is an increasing need for other approaches to MT evaluation that go beyond the lexical level of the phrases compared. In Liu and Gildea (2005), three syntactic evaluation metrics are presented. The first of these metrics, the Subtree Metric (SMT), is based on determining the number of subtrees that can be found in both the candidate tr</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>C. Callison-Burch, M. Osborne and P. Koehn. 2006. Re-evaluating the Role of Bleu in Machine Translation Research. Proceedings of EACL2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. Proceedings of WMT12.</booktitle>
<contexts>
<context position="13005" citStr="Callison-Burch et al., 2012" startWordPosition="2068" endWordPosition="2071">etric using CESM (Common Embedded Subtree Metric). CESM was computed using the precision, the recall and the F-measure of the common embedded subtrees of the reference and the translation: 132 where treeref and treehyp represent the preprocessed dependency trees of the reference and the hypothesis translations. 4 Experimental setup and evaluation In order to determine how accurate CESM is in capturing the similarity between references and translations, we evaluated it at the system level and at the segment level. The evaluation was conducted using data provided by the NAACL 2012 WMT workshop (Callison-Burch et al., 2012). The test data for the workshop consisted of 99 translated news articles in English, German, French, Spanish and Czech. At the system level, the initial German test set provided at the workshop was filtered according to the length of segments. This was done in order to limit the time requirements of WCDG. As a result, 500 segments with a length between 50 and 80 characters were extracted from the German reference file. In the next step, we arbitrarily selected the outputs of 7 of the 15 systems that were submitted for evaluation in the English to German translation task: DFKI (Vilar, 2012), J</context>
<context position="14550" citStr="Callison-Burch et al., 2012" startWordPosition="2322" endWordPosition="2325">responding to a system. The average scores obtained are depicted in Table 1. Evaluation of the metric at the system level was performed by measuring the correlation of the CESM metric with human judgments using Spearman&apos;s rank correlation coefficient p: where n represents the number of MT systems considered during evaluation, and di2 represents the difference between the ranks, assigned to a system, by the metric and the human judgments. The minimum value of p is -1, when there is no correlation between the two rankings, while the maximum value is 1, when the two rankings correlate perfectly (Callison-Burch et al., 2012). In order to compute the p score, the scores attributed to every system by CESM, were converted into ranks. From the different ranking strategies that were presented by the WMT12 workshop, the standard ranking order was chosen. The p rank correlation coefficient was calculated as being p = 0.92, which shows there is a strong correlation between the results of CESM and the human judgments. In order to better assess the quality of CESM, the test set was also evaluated using NIST (Doddington, 2002), which managed to obtain the same rank correlation coefficient of p = 0.92. No. System CESM NIST n</context>
<context position="15997" citStr="Callison-Burch et al., 2012" startWordPosition="2562" endWordPosition="2565"> evaluating at the segment level was filtering the initial test set provided by the WMT12 workshop. For this purpose, 2500 reference and translation segments were selected with a length between 50 and 80 characters. The Kendall tau rank correlation coefficient was calculated in order to measure the correlation with human judgments, where Kendall tau (Callison-Burch et al., 2012) is defined as: In order to compute the value of Kendall tau, we determined the number of concordant pairs and the number of discordant pairs of judgments. Similarly to the guideline followed during the WMT12 workshop (Callison-Burch et al., 2012), we penalized ties given by CESM and ignored ties assigned by the human judgments. The obtained result was a correlation of 0.058. As a term of comparison, the highest correlation for segment level reported in Callinson-Burch et al. (2012) was 0.19 obtained by TerrorCat (Fishel et al., 2012) and the lowest was BlockErrCats (Popovic, 2012) with 0.040. However, these results were obtained by evaluating on the entire test set. The rather low correlation result we obtained can be partially explained by the fact that only one judgment of a pair of reference and translation was taken into account. </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut and L. Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. Proceedings of WMT12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5989" citStr="Collins, 1999" startWordPosition="931" endWordPosition="932"> three syntactic evaluation metrics are presented. The first of these metrics, the Subtree Metric (SMT), is based on determining the number of subtrees that can be found in both the candidate translation and the reference phrase structure trees. The second metric, which is a kernel-based subtree metric, is defined as the maximum of the cosine measure between the MT output and the set of references. The third metric proposed computes the number of matching n-grams between the headword chains of the reference and the candidate translation dependency trees obtained using the parser described in (Collins, 1999). The idea of syntactic similarity is further exploited in Owczarzak et al. (2007) which uses a Lexical Functional Grammar (LFG) parser. The similarity between the translation and the reference is computed using the precision and the recall of the dependencies that illustrate the pair of sentences. Furthermore, paraphrases are used in order to improve the correlation with human judgments. Another set of syntactic metrics has been introduced in Gimenez (2008); some of them are based on analyzing different types of linguistic information (i.e. part-of-speech or lemma). 3 A new syntactic automati</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. J. Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics.</title>
<date>2002</date>
<booktitle>Proceedings of the 2nd International Conference on Human Language Technology.</booktitle>
<contexts>
<context position="15051" citStr="Doddington, 2002" startWordPosition="2408" endWordPosition="2409"> two rankings, while the maximum value is 1, when the two rankings correlate perfectly (Callison-Burch et al., 2012). In order to compute the p score, the scores attributed to every system by CESM, were converted into ranks. From the different ranking strategies that were presented by the WMT12 workshop, the standard ranking order was chosen. The p rank correlation coefficient was calculated as being p = 0.92, which shows there is a strong correlation between the results of CESM and the human judgments. In order to better assess the quality of CESM, the test set was also evaluated using NIST (Doddington, 2002), which managed to obtain the same rank correlation coefficient of p = 0.92. No. System CESM NIST name score score 1 DFKI 0.069 4.7709 2 JHU 0.073 4.9904 3 KIT 0.090 5.1358 4 OnlineA 0.093 5.3039 5 OnlineB 0.091 5.3039 6 OnlineC 0.085 4.8022 7 UK 0.075 4.6579 Table 1. System level evaluation results The first step in evaluating at the segment level was filtering the initial test set provided by the WMT12 workshop. For this purpose, 2500 reference and translation segments were selected with a length between 50 and 80 characters. The Kendall tau rank correlation coefficient was calculated in ord</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics. Proceedings of the 2nd International Conference on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Foth</author>
</authors>
<title>Writing weighted constraints for large de-pendency grammars.</title>
<date>2004</date>
<booktitle>Recent Advances in De-pendency Grammar, Workshop COLING</booktitle>
<contexts>
<context position="8407" citStr="Foth, 2004" startWordPosition="1312" endWordPosition="1313">ted using labels in order to distinguish between different types of relations. A constraint is made up of a logical formula that describes properties of the tree. One property, for example, that is always enforced is that no word can have more than one regent on any level at a time. During the analysis, each of the constraints is applied to every edge or every pair of edges belonging to the constructed dependency parse tree. The main advantage of using constraint dependency grammars over dependency grammars based on generative rules is that they can deal better with free word order languages (Foth, 2004). Weighted Constraint Dependency Grammar (WCDG) (Menzel and Schröder, 1998) assigns different weights to the constraints of the grammar. Every constraint in WCDG is assigned a score which is a number between 0.0 and 1.0, 131 while the general score of a parse is calculated as the product of all the scores of all the instances of constraints that have not been satisfied. Rules that have a score of 0 are called hard rules, meaning that they cannot be ignored, which is the case of the one regent only rule mentioned earlier. The advantage of using graded constraints, as opposed to crisp ones, stem</context>
</contexts>
<marker>Foth, 2004</marker>
<rawString>K. Foth. 2004. Writing weighted constraints for large de-pendency grammars. Recent Advances in De-pendency Grammar, Workshop COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Foth</author>
<author>M Daum</author>
<author>W Menzel</author>
</authors>
<title>A broadcoverage parser for German based on defeasible constraints.</title>
<date>2004</date>
<booktitle>KONVENS 2004, Beiträge zur 7, Konferenz zur Verarbeitung natürlicher Sprache,</booktitle>
<location>Wien.</location>
<contexts>
<context position="9368" citStr="Foth et al., 2004" startWordPosition="1471" endWordPosition="1474">hat have not been satisfied. Rules that have a score of 0 are called hard rules, meaning that they cannot be ignored, which is the case of the one regent only rule mentioned earlier. The advantage of using graded constraints, as opposed to crisp ones, stems from the fact that weights allow the parser to tolerate constraint violations, which, in turn, makes the parser robust against ungrammaticality. The parser was evaluated using different types of texts, and the results show that it has an accuracy between 80% and 90% in computing correct dependency attachments depending on the type of text (Foth et al., 2004a). The benefit of using WCDG over other parsers is that it provides further information on a parse, like the general score of the parse and the constraints that are violated by the final result. This information can be further explored in order to perform an error analysis. Moreover, because of the fact that the candidate translations are sometimes not well-formed, parsing them represents a challenge. However, WCDG will always provide a final result, in the form of a dependency structure, even though it might have a low score due to the violated constraints. 3.2 Description of the metric In o</context>
</contexts>
<marker>Foth, Daum, Menzel, 2004</marker>
<rawString>K. Foth, M. Daum and W. Menzel. 2004a. A broadcoverage parser for German based on defeasible constraints. KONVENS 2004, Beiträge zur 7, Konferenz zur Verarbeitung natürlicher Sprache, Wien.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Foth</author>
<author>M Daum</author>
<author>W Menzel</author>
</authors>
<title>Interactive grammar development with WCDG.</title>
<date>2004</date>
<booktitle>Proc. of the 42nd Annual Meeting of the Association for Com-putational Linguistics.</booktitle>
<contexts>
<context position="9368" citStr="Foth et al., 2004" startWordPosition="1471" endWordPosition="1474">hat have not been satisfied. Rules that have a score of 0 are called hard rules, meaning that they cannot be ignored, which is the case of the one regent only rule mentioned earlier. The advantage of using graded constraints, as opposed to crisp ones, stems from the fact that weights allow the parser to tolerate constraint violations, which, in turn, makes the parser robust against ungrammaticality. The parser was evaluated using different types of texts, and the results show that it has an accuracy between 80% and 90% in computing correct dependency attachments depending on the type of text (Foth et al., 2004a). The benefit of using WCDG over other parsers is that it provides further information on a parse, like the general score of the parse and the constraints that are violated by the final result. This information can be further explored in order to perform an error analysis. Moreover, because of the fact that the candidate translations are sometimes not well-formed, parsing them represents a challenge. However, WCDG will always provide a final result, in the form of a dependency structure, even though it might have a low score due to the violated constraints. 3.2 Description of the metric In o</context>
</contexts>
<marker>Foth, Daum, Menzel, 2004</marker>
<rawString>K. Foth, M. Daum and W. Menzel. 2004b. Interactive grammar development with WCDG. Proc. of the 42nd Annual Meeting of the Association for Com-putational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Foth</author>
<author>T By</author>
<author>W Menzel</author>
</authors>
<title>Guiding a con-straint dependency parser with supertags.</title>
<date>2006</date>
<booktitle>Proceedings of the 21st Int. Conf. on Computational Linguistics.</booktitle>
<marker>Foth, By, Menzel, 2006</marker>
<rawString>K. Foth, T. By and W. Menzel. 2006. Guiding a con-straint dependency parser with supertags. Proceedings of the 21st Int. Conf. on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fishel</author>
<author>R Sennrich</author>
<author>M Popovic</author>
<author>O Bojar</author>
</authors>
<title>TerrorCat: a translation error categorization-based MT quality metric.</title>
<date>2012</date>
<booktitle>Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="16290" citStr="Fishel et al., 2012" startWordPosition="2610" endWordPosition="2613">orrelation with human judgments, where Kendall tau (Callison-Burch et al., 2012) is defined as: In order to compute the value of Kendall tau, we determined the number of concordant pairs and the number of discordant pairs of judgments. Similarly to the guideline followed during the WMT12 workshop (Callison-Burch et al., 2012), we penalized ties given by CESM and ignored ties assigned by the human judgments. The obtained result was a correlation of 0.058. As a term of comparison, the highest correlation for segment level reported in Callinson-Burch et al. (2012) was 0.19 obtained by TerrorCat (Fishel et al., 2012) and the lowest was BlockErrCats (Popovic, 2012) with 0.040. However, these results were obtained by evaluating on the entire test set. The rather low correlation result we obtained can be partially explained by the fact that only one judgment of a pair of reference and translation was taken into account. It will be 133 interesting to see how the averaging of the ranks of a translation influences the correlation coefficient. produce new references has increased the BLEU score, therefore this is an approach that will be further investigated. 5 Conclusions and future work In this paper, a new ev</context>
</contexts>
<marker>Fishel, Sennrich, Popovic, Bojar, 2012</marker>
<rawString>M. Fishel, R. Sennrich, M. Popovic and O. Bojar. 2012. TerrorCat: a translation error categorization-based MT quality metric. Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ganitkevitch</author>
<author>Y Cao</author>
<author>J Weese</author>
<author>M Post</author>
<author>C Callison-Burch</author>
</authors>
<title>Joshua 4.0: Packing, PRO, and paraphrases.</title>
<date>2012</date>
<booktitle>Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="13635" citStr="Ganitkevitch et al., 2012" startWordPosition="2175" endWordPosition="2178">e test data for the workshop consisted of 99 translated news articles in English, German, French, Spanish and Czech. At the system level, the initial German test set provided at the workshop was filtered according to the length of segments. This was done in order to limit the time requirements of WCDG. As a result, 500 segments with a length between 50 and 80 characters were extracted from the German reference file. In the next step, we arbitrarily selected the outputs of 7 of the 15 systems that were submitted for evaluation in the English to German translation task: DFKI (Vilar, 2012), JHU (Ganitkevitch et al., 2012), KIT (Niehues et al., 2012), UK (Zeman, 2012) and three anonymized system outputs referred to as OnlineA, OnlineB, OnlineC. After this initial step of filtering the data, the 7 systems were evaluated by calculating the CESM score for every pair of reference and translation segments corresponding to a system. The average scores obtained are depicted in Table 1. Evaluation of the metric at the system level was performed by measuring the correlation of the CESM metric with human judgments using Spearman&apos;s rank correlation coefficient p: where n represents the number of MT systems considered duri</context>
</contexts>
<marker>Ganitkevitch, Cao, Weese, Post, Callison-Burch, 2012</marker>
<rawString>J. Ganitkevitch, Y. Cao, J. Weese, M. Post and C. Callison-Burch. 2012. Joshua 4.0: Packing, PRO, and paraphrases. Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gimenez</author>
</authors>
<title>Empirical Machine Translation and its Evaluation.</title>
<date>2008</date>
<tech>Ph. D. thesis.</tech>
<contexts>
<context position="6451" citStr="Gimenez (2008)" startWordPosition="1001" endWordPosition="1002">grams between the headword chains of the reference and the candidate translation dependency trees obtained using the parser described in (Collins, 1999). The idea of syntactic similarity is further exploited in Owczarzak et al. (2007) which uses a Lexical Functional Grammar (LFG) parser. The similarity between the translation and the reference is computed using the precision and the recall of the dependencies that illustrate the pair of sentences. Furthermore, paraphrases are used in order to improve the correlation with human judgments. Another set of syntactic metrics has been introduced in Gimenez (2008); some of them are based on analyzing different types of linguistic information (i.e. part-of-speech or lemma). 3 A new syntactic automatic metric In this section we introduce the new syntactic metric which is based on constraint dependency parsing. In the first subsection, the WCDG parser is presented, together with the advantages of using this parser over the other ones available, while the second subsection provides a detailed description of the new metric. 3.1 Weighted Constraint Dependency Grammar Parser Our research was performed using a dependency parser. We decided on this type of pars</context>
</contexts>
<marker>Gimenez, 2008</marker>
<rawString>J. Gimenez. 2008. Empirical Machine Translation and its Evaluation. Ph. D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hamp</author>
<author>H Feldweg</author>
</authors>
<title>GermaNet - a Lexical-Semantic Net for German.</title>
<date>1997</date>
<booktitle>Proc. of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications.</booktitle>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>B. Hamp and H. Feldweg. 1997. GermaNet - a Lexical-Semantic Net for German. Proc. of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<date>2006</date>
<booktitle>Manual and Automatic Evaluation of Machine Translation between European Languages. NAACL 2006 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="4936" citStr="Koehn and Monz (2006)" startWordPosition="758" endWordPosition="761"> 2000) is the one which is used more frequently; it calculates the minimal number of insertion, substitutions and deletions needed to transform the candidate translation into a reference. Metrics based on lexical matching suffer from not being able to consider the variation encountered in natural language. Thus, they reward a low score to an otherwise fluent and syntactically correct candidate translation, if it does not share a certain number of words with the set of references. Because of this, major disagreements between the scores assigned by BLEU and human judgments have been reported in Koehn and Monz (2006) and Callison-Burch et al. (2006). Another disadvantage is that many of them cannot be applied at the segment level, which is often needed in order to better assess the quality of MT output and to determine which improvements should be made to the MT system. Because of these disadvantages there is an increasing need for other approaches to MT evaluation that go beyond the lexical level of the phrases compared. In Liu and Gildea (2005), three syntactic evaluation metrics are presented. The first of these metrics, the Subtree Metric (SMT), is based on determining the number of subtrees that can </context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>P. Koehn and C. Monz. 2006. Manual and Automatic Evaluation of Machine Translation between European Languages. NAACL 2006 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<marker>Koehn, 2010</marker>
<rawString>P. Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kübler</author>
<author>J Prokic</author>
</authors>
<title>Why is German Dependency Parsing more Reliable than Constituent Parsing?.</title>
<date>2006</date>
<booktitle>Proceedings of the Fifth International Work-shop on Treebanks and Linguistic Theories.</booktitle>
<marker>Kübler, Prokic, 2006</marker>
<rawString>S. Kübler and J. Prokic. 2006. Why is German Dependency Parsing more Reliable than Constituent Parsing?. Proceedings of the Fifth International Work-shop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Lin</author>
<author>H Wang</author>
<author>S McClean</author>
<author>C Liu</author>
</authors>
<title>All Common Embedded Subtrees for Measuring Tree Similarity.</title>
<date>2008</date>
<booktitle>International Symposium on Computational Intelligence and Design.</booktitle>
<contexts>
<context position="11293" citStr="Lin et al., 2008" startWordPosition="1792" endWordPosition="1795"> schwarze Katze”(engl. On the red chair quickly jumps the red cat). Even though the word order of the two segments is quite different, and the translation has an incorrect syntax, they roughly have the same meaning. We present in Figure 1 the dependency parse trees obtained using WCDG for the sentences considered. We can observe that the general structure of the translation is similar to that of the reference, the only difference being the reverse order between the left subtree and the right subtree. The tree similarity measure that we chose to use was the All Common Embedded Subtrees (ACET) (Lin et al., 2008) similarity. Given a tree T, an embedded subtree is obtained by removing one or more nodes, except for the root, from the tree T. The idea behind ACET is that, the more substructures two trees share, the more similar they are. Therefore, ACET is defined as the number of common embedded subtrees shared between two trees. The results reported in Lin et al. (2008) show that ACET outperforms tree edit distance (Zhang and Shasha, 1989) in terms of efficiency. Figure 1. Example of dependency parse trees for reference and candidate translations In our experiments, we have applied the ACET algorithm, </context>
</contexts>
<marker>Lin, Wang, McClean, Liu, 2008</marker>
<rawString>Z. Lin, H. Wang, S. McClean and C. Liu. 2008. All Common Embedded Subtrees for Measuring Tree Similarity. International Symposium on Computational Intelligence and Design.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Liu</author>
<author>D Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="5374" citStr="Liu and Gildea (2005)" startWordPosition="833" endWordPosition="836">in number of words with the set of references. Because of this, major disagreements between the scores assigned by BLEU and human judgments have been reported in Koehn and Monz (2006) and Callison-Burch et al. (2006). Another disadvantage is that many of them cannot be applied at the segment level, which is often needed in order to better assess the quality of MT output and to determine which improvements should be made to the MT system. Because of these disadvantages there is an increasing need for other approaches to MT evaluation that go beyond the lexical level of the phrases compared. In Liu and Gildea (2005), three syntactic evaluation metrics are presented. The first of these metrics, the Subtree Metric (SMT), is based on determining the number of subtrees that can be found in both the candidate translation and the reference phrase structure trees. The second metric, which is a kernel-based subtree metric, is defined as the maximum of the cosine measure between the MT output and the set of references. The third metric proposed computes the number of matching n-grams between the headword chains of the reference and the candidate translation dependency trees obtained using the parser described in </context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>D. Liu and D. Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Putting human assessments of machine translation systems in order.</title>
<date>2012</date>
<booktitle>Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="17677" citStr="Lopez (2012)" startWordPosition="2840" endWordPosition="2841">why we chose this parser was that, due to its architecture, it is able to handle ungrammatical and ambiguous input data. The experiments conducted so far show that using the data made available at the NAACL 2012 WMT workshop, CESM correlates well with the human judgments at the system level. One of the future experiments that we intend to perform is to assess metric quality on the entire evaluation set. Moreover, we plan to compare CESM with other tree-based MT metrics. Furthermore, the WMT12 workshop offers different ranking possibilities, like the ones presented in Bojar et al (2011) and in Lopez (2012). It will be determined how much are the segment level evaluation results influenced by these ranking orders. One limitation of the proposed metric is that, at the moment it is restricted to translations from any source language to German as a target language. Because of this reason, we plan to extend the metric to other languages and see how well it performs in different settings. In further experiments we also intend to test CESM using statistical based dependency parsers, like the Malt Parser (Nivre et al., 2007) and the MST parser (McDonald et al., 2006), in order to decide whether the cho</context>
</contexts>
<marker>Lopez, 2012</marker>
<rawString>A. Lopez. 2012. Putting human assessments of machine translation systems in order. Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual Dependency Parsing with a TwoStage Discriminative Parser.</title>
<date>2006</date>
<booktitle>Tenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="18241" citStr="McDonald et al., 2006" startWordPosition="2933" endWordPosition="2936">e ones presented in Bojar et al (2011) and in Lopez (2012). It will be determined how much are the segment level evaluation results influenced by these ranking orders. One limitation of the proposed metric is that, at the moment it is restricted to translations from any source language to German as a target language. Because of this reason, we plan to extend the metric to other languages and see how well it performs in different settings. In further experiments we also intend to test CESM using statistical based dependency parsers, like the Malt Parser (Nivre et al., 2007) and the MST parser (McDonald et al., 2006), in order to decide whether the choice of parser influences the performance of the metric. Another approach that we will explore for improving CESM is to compare dependency parse trees using the base form and the part-ofspeech of the tokens, instead of the exact lexical match. We will try this approach in order to avoid penalizing lexical variation. The accuracy of CESM can be further increased by the use of paraphrases, which can be obtained by using a German thesaurus or a lexical resource like GermaNet (Hamp and Feldweg, 1997). Furthermore, a technique like the one described in Owczarzak (</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R. McDonald, K. Lerman and F. Pereira. 2006. Multilingual Dependency Parsing with a TwoStage Discriminative Parser. Tenth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Menzel</author>
<author>I Schröder</author>
</authors>
<title>Decision Procedures for Dependency Parsing Using Graded Constraints. Workshop On Processing Of Dependency-Based Grammars.</title>
<date>1998</date>
<contexts>
<context position="2717" citStr="Menzel and Schröder, 1998" startWordPosition="396" endWordPosition="399">T evaluation experiences a gradual shift of focus from lexical metrics to structural ones, whether they are syntactic or semantic or a combination of both. This paper introduces a new syntactic automatic MT evaluation method. At this stage of research the new metric is evaluating translations from any source language into German. Given that a set of constraint-based grammar rules are available for that language, extensions to other target languages are anytime possible. The chosen tool for providing syntactic information for German is the Weighted Constraints Dependency Grammar (WCDG) parser (Menzel and Schröder, 1998), which is preferred over other parsers because of its robustness to ungrammatical input, as it is typical for MT output. The rest of this paper is organized as follows. In Section 2 the state of the art in MT evaluation is presented, while in Section 3 the new syntactic metric is described. The experimental setup and results are presented in Section 4. The last section deals with the conclusions and future work. 2 State of the art Automatic evaluation of MT systems relies on the existence of at least one reference1 created by a human annotator. Using an automatic method of evaluation a score </context>
<context position="8482" citStr="Menzel and Schröder, 1998" startWordPosition="1319" endWordPosition="1322">ypes of relations. A constraint is made up of a logical formula that describes properties of the tree. One property, for example, that is always enforced is that no word can have more than one regent on any level at a time. During the analysis, each of the constraints is applied to every edge or every pair of edges belonging to the constructed dependency parse tree. The main advantage of using constraint dependency grammars over dependency grammars based on generative rules is that they can deal better with free word order languages (Foth, 2004). Weighted Constraint Dependency Grammar (WCDG) (Menzel and Schröder, 1998) assigns different weights to the constraints of the grammar. Every constraint in WCDG is assigned a score which is a number between 0.0 and 1.0, 131 while the general score of a parse is calculated as the product of all the scores of all the instances of constraints that have not been satisfied. Rules that have a score of 0 are called hard rules, meaning that they cannot be ignored, which is the case of the one regent only rule mentioned earlier. The advantage of using graded constraints, as opposed to crisp ones, stems from the fact that weights allow the parser to tolerate constraint violat</context>
</contexts>
<marker>Menzel, Schröder, 1998</marker>
<rawString>W. Menzel and I. Schröder. 1998. Decision Procedures for Dependency Parsing Using Graded Constraints. Workshop On Processing Of Dependency-Based Grammars.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Niehues</author>
<author>Y Zhang</author>
<author>M Mediani</author>
<author>T Herrmann</author>
<author>E Cho</author>
<author>A Waibel</author>
</authors>
<title>The karlsruhe institute of technology translation systems for the WMT</title>
<date>2012</date>
<booktitle>Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="13663" citStr="Niehues et al., 2012" startWordPosition="2180" endWordPosition="2183">isted of 99 translated news articles in English, German, French, Spanish and Czech. At the system level, the initial German test set provided at the workshop was filtered according to the length of segments. This was done in order to limit the time requirements of WCDG. As a result, 500 segments with a length between 50 and 80 characters were extracted from the German reference file. In the next step, we arbitrarily selected the outputs of 7 of the 15 systems that were submitted for evaluation in the English to German translation task: DFKI (Vilar, 2012), JHU (Ganitkevitch et al., 2012), KIT (Niehues et al., 2012), UK (Zeman, 2012) and three anonymized system outputs referred to as OnlineA, OnlineB, OnlineC. After this initial step of filtering the data, the 7 systems were evaluated by calculating the CESM score for every pair of reference and translation segments corresponding to a system. The average scores obtained are depicted in Table 1. Evaluation of the metric at the system level was performed by measuring the correlation of the CESM metric with human judgments using Spearman&apos;s rank correlation coefficient p: where n represents the number of MT systems considered during evaluation, and di2 repre</context>
</contexts>
<marker>Niehues, Zhang, Mediani, Herrmann, Cho, Waibel, 2012</marker>
<rawString>J. Niehues, Y. Zhang, M. Mediani, T. Herrmann, E. Cho and A. Waibel. 2012. The karlsruhe institute of technology translation systems for the WMT 2012. Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niessen</author>
<author>F J Och</author>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research.</title>
<date>2000</date>
<booktitle>Proceedings of the 2nd International Conference on Language Resources and Evaluation</booktitle>
<contexts>
<context position="4321" citStr="Niessen et al., 2000" startWordPosition="661" endWordPosition="664"> translation for the candidate translation. 130 Proceedings of the ACL Student Research Workshop, pages 130–135, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Among the n-gram based metrics, one of the most popular methods of evaluation is BLEU (Papineni et al., 2001). It provides a score that is computed as the summed number of n-grams shared by the references and the output, divided by the total number of n-grams. Lexical metrics that use the edit distance are constructed using the Levenshtein distance applied at the word level. Among these metrics, WER (Niessen et al., 2000) is the one which is used more frequently; it calculates the minimal number of insertion, substitutions and deletions needed to transform the candidate translation into a reference. Metrics based on lexical matching suffer from not being able to consider the variation encountered in natural language. Thus, they reward a low score to an otherwise fluent and syntactically correct candidate translation, if it does not share a certain number of words with the set of references. Because of this, major disagreements between the scores assigned by BLEU and human judgments have been reported in Koehn </context>
</contexts>
<marker>Niessen, Och, Leusch, Ney, 2000</marker>
<rawString>S. Niessen, F. J. Och, G. Leusch and H. Ney. 2000. An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research. Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC). J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Marinov Kübler</author>
<author>E Marsi</author>
</authors>
<title>MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering.</title>
<date>2007</date>
<marker>Kübler, Marsi, 2007</marker>
<rawString>Kübler, S. Marinov and E. Marsi. 2007. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Owczarzak</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Dependency-based automatic evaluation for machine translation.</title>
<date>2007</date>
<booktitle>Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>K. Owczarzak, J. van Genabith and A. Way. 2007. Dependency-based automatic evaluation for machine translation. Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Owczarzak</author>
</authors>
<title>A Novel Dependency-Based Evaluation Metric for Machine Translation,</title>
<date>2008</date>
<tech>Ph.D. thesis.</tech>
<marker>Owczarzak, 2008</marker>
<rawString>K. Owczarzak. 2008. A Novel Dependency-Based Evaluation Metric for Machine Translation, Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>RC22176 (Technical Report), IBM</tech>
<institution>T.J. Watson Research Center.</institution>
<contexts>
<context position="4005" citStr="Papineni et al., 2001" startWordPosition="608" endWordPosition="611">system and the reference. This similarity can be computed at different levels: lexical, syntactic or semantic. At the lexical level, the metrics developed so far can be divided into two major categories: n-gram based and edit distance based. 1 We will use the term reference for the reference translation and the term translation for the candidate translation. 130 Proceedings of the ACL Student Research Workshop, pages 130–135, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Among the n-gram based metrics, one of the most popular methods of evaluation is BLEU (Papineni et al., 2001). It provides a score that is computed as the summed number of n-grams shared by the references and the output, divided by the total number of n-grams. Lexical metrics that use the edit distance are constructed using the Levenshtein distance applied at the word level. Among these metrics, WER (Niessen et al., 2000) is the one which is used more frequently; it calculates the minimal number of insertion, substitutions and deletions needed to transform the candidate translation into a reference. Metrics based on lexical matching suffer from not being able to consider the variation encountered in </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward and W.-J. Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. RC22176 (Technical Report), IBM T.J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Popovic</author>
</authors>
<title>Class error rates for evaluation of machine translation output.</title>
<date>2012</date>
<booktitle>Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="16338" citStr="Popovic, 2012" startWordPosition="2619" endWordPosition="2620">llison-Burch et al., 2012) is defined as: In order to compute the value of Kendall tau, we determined the number of concordant pairs and the number of discordant pairs of judgments. Similarly to the guideline followed during the WMT12 workshop (Callison-Burch et al., 2012), we penalized ties given by CESM and ignored ties assigned by the human judgments. The obtained result was a correlation of 0.058. As a term of comparison, the highest correlation for segment level reported in Callinson-Burch et al. (2012) was 0.19 obtained by TerrorCat (Fishel et al., 2012) and the lowest was BlockErrCats (Popovic, 2012) with 0.040. However, these results were obtained by evaluating on the entire test set. The rather low correlation result we obtained can be partially explained by the fact that only one judgment of a pair of reference and translation was taken into account. It will be 133 interesting to see how the averaging of the ranks of a translation influences the correlation coefficient. produce new references has increased the BLEU score, therefore this is an approach that will be further investigated. 5 Conclusions and future work In this paper, a new evaluation metric for MT was introduced, which is </context>
</contexts>
<marker>Popovic, 2012</marker>
<rawString>M. Popovic. 2012. Class error rates for evaluation of machine translation output. Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Schröder</author>
<author>W Menzel</author>
<author>K Foth</author>
<author>M Schulz</author>
</authors>
<title>Modeling dependency grammar with restricted constraints. Traitement Automatique des Langues.</title>
<date>2000</date>
<contexts>
<context position="7629" citStr="Schröder et al., 2000" startWordPosition="1180" endWordPosition="1183">ency parser. We decided on this type of parser because, as opposed to constituent parsers, it offers the possibility of better representing nonprojective structures. Moreover, it has been shown in Kuebler and Prokic (2006) that, at least in the case of German, the results achieved by a dependency parser are more accurate than the ones obtained when parsing using constituent parsers, and this is because dependency parsers can handle better long distance relations and coordination. The goal of constraint dependency grammars (CDG) is to create dependency structures that represent a given phrase (Schröder et al., 2000) on parallel levels of analysis. A relation between two words in a sentence is represented using an edge, which connects the regent and the dependent. Edges are annotated using labels in order to distinguish between different types of relations. A constraint is made up of a logical formula that describes properties of the tree. One property, for example, that is always enforced is that no word can have more than one regent on any level at a time. During the analysis, each of the constraints is applied to every edge or every pair of edges belonging to the constructed dependency parse tree. The </context>
</contexts>
<marker>Schröder, Menzel, Foth, Schulz, 2000</marker>
<rawString>I. Schröder, W. Menzel, K. Foth and M. Schulz. 2000. Modeling dependency grammar with restricted constraints. Traitement Automatique des Langues.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Schröder</author>
<author>H Pop</author>
<author>W Menzel</author>
<author>K Foth</author>
</authors>
<title>Learning grammar weights using genetic algorithms.</title>
<date>2001</date>
<booktitle>Proceedings Euroconference Recent Advances in Natural Language Processing.</booktitle>
<marker>Schröder, Pop, Menzel, Foth, 2001</marker>
<rawString>I. Schröder, H. Pop, W. Menzel and K. Foth. 2001. Learning grammar weights using genetic algorithms. Proceedings Euroconference Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Schröder</author>
</authors>
<title>Natural Language Parsing with Graded Constraints.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Dept. of Computer Science, University of Hamburg.</institution>
<marker>Schröder, 2002</marker>
<rawString>I. Schröder. 2002. Natural Language Parsing with Graded Constraints. Ph.D. thesis, Dept. of Computer Science, University of Hamburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vilar</author>
</authors>
<title>DFKI’s SMT system for WMT</title>
<date>2012</date>
<booktitle>Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="13602" citStr="Vilar, 2012" startWordPosition="2172" endWordPosition="2173">h et al., 2012). The test data for the workshop consisted of 99 translated news articles in English, German, French, Spanish and Czech. At the system level, the initial German test set provided at the workshop was filtered according to the length of segments. This was done in order to limit the time requirements of WCDG. As a result, 500 segments with a length between 50 and 80 characters were extracted from the German reference file. In the next step, we arbitrarily selected the outputs of 7 of the 15 systems that were submitted for evaluation in the English to German translation task: DFKI (Vilar, 2012), JHU (Ganitkevitch et al., 2012), KIT (Niehues et al., 2012), UK (Zeman, 2012) and three anonymized system outputs referred to as OnlineA, OnlineB, OnlineC. After this initial step of filtering the data, the 7 systems were evaluated by calculating the CESM score for every pair of reference and translation segments corresponding to a system. The average scores obtained are depicted in Table 1. Evaluation of the metric at the system level was performed by measuring the correlation of the CESM metric with human judgments using Spearman&apos;s rank correlation coefficient p: where n represents the num</context>
</contexts>
<marker>Vilar, 2012</marker>
<rawString>D. Vilar. 2012. DFKI’s SMT system for WMT 2012. Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zeman</author>
</authors>
<title>Data issues of the multilingual translation matrix.</title>
<date>2012</date>
<booktitle>Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="13681" citStr="Zeman, 2012" startWordPosition="2185" endWordPosition="2186"> articles in English, German, French, Spanish and Czech. At the system level, the initial German test set provided at the workshop was filtered according to the length of segments. This was done in order to limit the time requirements of WCDG. As a result, 500 segments with a length between 50 and 80 characters were extracted from the German reference file. In the next step, we arbitrarily selected the outputs of 7 of the 15 systems that were submitted for evaluation in the English to German translation task: DFKI (Vilar, 2012), JHU (Ganitkevitch et al., 2012), KIT (Niehues et al., 2012), UK (Zeman, 2012) and three anonymized system outputs referred to as OnlineA, OnlineB, OnlineC. After this initial step of filtering the data, the 7 systems were evaluated by calculating the CESM score for every pair of reference and translation segments corresponding to a system. The average scores obtained are depicted in Table 1. Evaluation of the metric at the system level was performed by measuring the correlation of the CESM metric with human judgments using Spearman&apos;s rank correlation coefficient p: where n represents the number of MT systems considered during evaluation, and di2 represents the differen</context>
</contexts>
<marker>Zeman, 2012</marker>
<rawString>D. Zeman. 2012. Data issues of the multilingual translation matrix. Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zhang</author>
<author>D Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>SIAM Journal on Computing.</journal>
<contexts>
<context position="11727" citStr="Zhang and Shasha, 1989" startWordPosition="1867" endWordPosition="1870">rence being the reverse order between the left subtree and the right subtree. The tree similarity measure that we chose to use was the All Common Embedded Subtrees (ACET) (Lin et al., 2008) similarity. Given a tree T, an embedded subtree is obtained by removing one or more nodes, except for the root, from the tree T. The idea behind ACET is that, the more substructures two trees share, the more similar they are. Therefore, ACET is defined as the number of common embedded subtrees shared between two trees. The results reported in Lin et al. (2008) show that ACET outperforms tree edit distance (Zhang and Shasha, 1989) in terms of efficiency. Figure 1. Example of dependency parse trees for reference and candidate translations In our experiments, we have applied the ACET algorithm, and computed the number of common embedded subtrees between the dependency parse trees of the hypothesis and the reference. Because of the additional information provided by the parsing, pre-processing of the output of the WCDG parser was necessary in order to transform the dependency tree into a general tree. We first removed the labels assigned to every edge, but maintained the nodes and the left to right order between them. In </context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>K. Zhang and D. Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAM Journal on Computing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>