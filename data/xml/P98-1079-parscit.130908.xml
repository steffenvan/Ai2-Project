<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.997808">
A Text Understander that Learns
</title>
<author confidence="0.998525">
Udo Hahn &amp; Klemens Schnattinger
</author>
<affiliation confidence="0.976892">
Computational Linguistics Lab, Freiburg University
</affiliation>
<address confidence="0.784681">
Werthmannplatz 1, D-79085 Freiburg, Germany
</address>
<figure confidence="0.9375385">
{hahn,schnattinger}Ocoling.uni-freiburg.de
Fly,,,,d–i,,
7 spatv-I
J
Hypothesis
sluttrl
‘ Hypothesis
pe-fl
Quadifkr
Qmday lachine
</figure>
<sectionHeader confidence="0.704304" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999002">
We introduce an approach to the automatic ac-
quisition of new concepts from natural language
texts which is tightly integrated with the under-
lying text understanding process. The learning
model is centered around the &apos;quality&apos; of differ-
ent forms of linguistic and conceptual evidence
which underlies the incremental generation and
refinement of alternative concept hypotheses,
each one capturing a different conceptual read-
ing for an unknown lexical item.
</bodyText>
<sectionHeader confidence="0.995507" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940259259259">
The approach to learning new concepts as a
result of understanding natural language texts
we present here builds on two different sources
of evidence — the prior knowledge of the do-
main the texts are about, and grammatical con-
structions in which unknown lexical items oc-
cur. While there may be many reasonable inter-
pretations when an unknown item occurs for the
very first time in a text, their number rapidly
decreases when more and more evidence is gath-
ered. Our model tries to make explicit the rea-
soning processes behind this learning pattern.
Unlike the current mainstream in automatic
linguistic knowledge acquisition, which can be
characterized as quantitative, surface-oriented
bulk processing of large corpora of texts (Hin-
dle, 1989; Zernik and Jacobs, 1990; Hearst,
1992; Manning, 1993), we propose here a
knowledge-intensive model of concept learning
from few, positive-only examples that is tightly
integrated with the non-learning mode of text
understanding. Both learning and understand-
ing build on a given core ontology in the format
of terminological assertions and, hence, make
abundant use of terminological reasoning. The
&apos;plain&apos; text understanding mode can be consid-
ered as the instantiation and continuous filling
</bodyText>
<figure confidence="0.836167">
Hypothesis
space-p
Hypothesis
spave-ti
</figure>
<figureCaption confidence="0.998097">
Figure 1 Architecture of the Text Learner
</figureCaption>
<bodyText confidence="0.999760733333334">
of roles with respect to single concepts already
available in the knowledge base. Under learning
conditions, however, a set of alternative concept
hypotheses has to be maintained for each un-
known item, with each hypothesis denoting a
newly created conceptual interpretation tenta-
tively associated with the unknown item.
The underlying methodology is summarized
in Fig. 1. The test parser (for an overview, cf.
Broker et al. (1994)) yields information from
the grammatical constructions in which an un-
known lexical item (symbolized by the black
square) occurs in terms of the corresponding de-
pendency parse tree. The kinds of syntactic con-
structions (e.g., genitive, apposition, compara-
tive), in which unknown lexical items appear,
are recorded and later assessed relative to the
credit they lend to a particular hypothesis. The
conceptual interpretation of parse trees involv-
ing unknown lexical items in the domain knowl-
edge base leads to the derivation of concept hy-
potheses, which are further enriched by concep-
tual annotations. These reflect structural pat-
terns of consistency, mutual justification, anal-
ogy, etc. relative to already available concept
descriptions in the domain knowledge base or
other hypothesis spaces. This kind of initial ev-
idence, in particular its predictive &amp;quot;goodness&amp;quot;
for the learning task, is represented by corre-
sponding sets of linguistic and conceptual (pal-
</bodyText>
<page confidence="0.998702">
476
</page>
<table confidence="0.999216857142857">
Syntax Semantics Axiom Semantics
C 11 D ci n Di A --= C AI = CI
CuD CluDI a : C al E CI
V R.0 {d E AI I RI (d) C CI
Rns RI nsl Q = R QI = RI
cIR {(d,ce) E RI I d E cl) a Rb (a&apos;, b&apos;) ER&apos;
Ric {(d,d&apos;) E RI I d&apos; E CI)
</table>
<tableCaption confidence="0.7438365">
Table 1: Some Concept and Table 2: Axioms for
Role Terms Concepts and Roles
</tableCaption>
<bodyText confidence="0.9998753">
ity labels. Multiple concept hypotheses for each
unknown lexical item are organized in terms of
corresponding hypothesis spaces, each of which
holds different or further specialized conceptual
readings.
The quality machine estimates the overall
credibility of single concept hypotheses by tak-
ing the available set of quality labels for each
hypothesis into account. The final computa-
tion of a preference order for the entire set of
competing hypotheses takes place in the qual-
ifier, a terminological classifier extended by an
evaluation metric for quality-based selection cri-
teria. The output of the quality machine is a
ranked list of concept hypotheses. The ranking
yields, in decreasing order of significance, either
the most plausible concept classes which classify
the considered instance or more general concept
classes subsuming the considered concept class
(cf. Schnattinger and Hahn (1998) for details).
</bodyText>
<sectionHeader confidence="0.95376" genericHeader="introduction">
2 Methodological Framework
</sectionHeader>
<bodyText confidence="0.999628">
In this section, we present the major method-
ological decisions underlying our approach.
</bodyText>
<subsectionHeader confidence="0.995139">
2.1 Terminological Logics
</subsectionHeader>
<bodyText confidence="0.998410538461538">
We use a standard terminological, KL-ONE-
style concept description language, here referred
to as CD ,C (for a survey of this paradigm, cf.
Woods and Schmolze (1992)). It has several
constructors combining atomic concepts, roles
and individuals to define the terminological the-
ory of a domain. Concepts are unary predicates,
roles are binary predicates over a domain A,
with individuals being the elements of A. We
assume a common set-theoretical semantics for
CDL — an interpretation I is a function that
assigns to each concept symbol (the set A) a
subset of the domain A, I : A —+ 2°, to each
role symbol (the set P) a binary relation of A,
I : P 24&apos;x°, and to each individual symbol
(the set I) an element of A, I : I -4 A.
Concept terms and role terms are defined in-
ductively. Table 1 contains some constructors
and their semantics, where C and D denote con-
cept terms, while R and S denote roles. RI (d)
represents the set of role fillers of the individual
d, i.e., the set of individuals e with (d, e) E .
By means of terminological axioms (for a sub-
set, see Table 2) a symbolic name can be intro-
duced for each concept to which are assigned
necessary and sufficient constraints using the
definitional operator A finite set of such
axioms is called the terminology or TBox. Con-
cepts and roles are associated with concrete in-
dividuals by assertional axioms (see Table 2; a, b
denote individuals). A finite set of such axioms
is called the world description or A Box. An in-
terpretation I is a model of an ABox with re-
gard to a TBox, if I satisfies the assertional
and terminological axioms.
Considering, e.g., a phrase such as &apos;The
switch of the Itoh-Ci-8 a straightforward
translation into corresponding terminological
concept descriptions is illustrated by:
</bodyText>
<equation confidence="0.9918962">
switch.1 : SWITCH
Itoh-Ci-8 HAS-SWITCH switch.1
HAS-SWITCH -=
(OuTPuTDEv u INpuTIDEv u IHAS-PART1SWITCH
STORAGED EV U COMPUTER)
</equation>
<bodyText confidence="0.995419608695652">
Assertion P1 indicates that the instance
switch.1 belongs to the concept class SWITCH.
P2 relates Itoh-Ci-8 and switch.1 via the re-
lation HAS-SWITCH. The relation HAS-SWITCH
is defined, finally, as the set of all HAS-PART
relations which have their domain restricted to
the disjunction of the concepts OUTPUTDEV,
INPUTDEV, STORAGEDEV or COMPUTER and
their range restricted to SWITCH.
In order to represent and reason about con-
cept hypotheses we have to properly extend the
formalism of CDC. Terminological hypotheses,
in our framework, are characterized by the fol-
lowing properties: for all stipulated hypotheses
(1) the same domain A holds, (2) the same con-
cept definitions are used, and (3) only different
assertional axioms can be established. These
conditions are sufficient, because each hypoth-
esis is based on a unique discourse entity (cf.
(1)), which can be directly mapped to associ-
ated instances (so concept definitions are stable
(2)). Only relations (including the ISA-relation)
among the instances may be different (3).
</bodyText>
<page confidence="0.98583">
477
</page>
<figure confidence="0.952167">
Axiom Semantics
(a : C)h al E Cih
(a R b)h (al , 61) E Rlh
</figure>
<tableCaption confidence="0.994804">
Table 3: Axioms in CDL4P°
</tableCaption>
<bodyText confidence="0.99989135">
Given these constraints, we may annotate
each assertional axiom of the form &apos;a : C&apos; and
&apos;a R b&apos; by a corresponding hypothesis label h so
that (a : C)h and (a R b)h are valid terminolog-
ical expressions. The extended terminological
language (cf. Table 3) will be called CD,ChYP°.
Its semantics is given by a special interpreta-
tion function II, for each hypothesis h, which is
applied to each concept and role symbol in the
canonical way: /h : A -4. 26&apos;; /h : P
Notice that the instances a, b are interpreted by
the interpretation function I, because there ex-
ists only one domain A. Only the interpretation
of the concept symbol C and the role symbol R
may be different in each hypothesis h.
Assume that we want to represent two of the
four concept hypotheses that can be derived
from (P3), viz. Itoh-Ci-8 considered as a storage
device or an output device. The corresponding
ABox expressions are then given by:
</bodyText>
<equation confidence="0.99367575">
(Roh-Ci-8 HAS-SWITCH switch. 1)h,
(Itoh-Ci-8 : SToRAGEDEv)h,
(Itoh-Ci-8 HAS-SWITCH switch.1)h2
(Itoh-Ci-8 : OuTPuTDEv)h,
</equation>
<bodyText confidence="0.999835">
The semantics associated with this ABox
fragment has the following form:
</bodyText>
<equation confidence="0.999731">
/h, (HAS-SWITCH) = {(Itoh-Ci-8,switch.1)},
Ihi(SToRAGEDEv) = {Roh-Ci-8},
/hi (OuTPuTDEv) = 0
/h,, (HAS-SWITCH) = {(Roh-Ci-8, switch .1)} ,
Ih2(SToRAGEDEv) = 0,
Ih2(OuTPuTDEv) = {Roh-Ci-8}
</equation>
<subsectionHeader confidence="0.999771">
2.2 Hypothesis Generation Rules
</subsectionHeader>
<bodyText confidence="0.999920016666667">
As mentioned above, text parsing and con-
cept acquisition from texts are tightly coupled.
Whenever, e.g., two nominals or a nominal and
a verb are supposed to be syntactically related
in the regular parsing mode, the semantic in-
terpreter simultaneously evaluates the concep-
tual compatibility of the items involved. Since
these reasoning processes are fully embedded in
a terminological representation system, checks
are made as to whether a concept denoted by
one of these objects is allowed to fill a role of
the other one. If one of the items involved is
unknown, i.e., a lexical and conceptual gap is
encountered, this interpretation mode generates
initial concept hypotheses about the class mem-
bership of the unknown object, and, as a conse-
quence of inheritance mechanisms holding for
concept taxonomies, provides conceptual role
information for the unknown item.
Given the structural foundations of termi-
nological theories, two dimensions of concep-
tual learning can be distinguished — the tax-
onomic one by which new concepts are located
in conceptual hierarchies, and the aggregational
one by which concepts are supplied with clus-
ters of conceptual relations (these will be used
subsequently by the terminological classifier to
determine the current position of the item to
be learned in the taxonomy). In the follow-
ing, let target.con be an unknown concept de-
noted by the corresponding lexical item tar-
get.lex, base.con be a given knowledge base con-
cept denoted by the corresponding lexical item
base.lex, and let target.lex and base.lex be re-
lated by some dependency relation. Further-
more, in the hypothesis generation rules below
variables are indicated by names with leading
`?&apos;; the operator TELL is used to initiate the
creation of assertional axioms in CD.ChYP°.
Typical linguistic indicators that can be ex-
ploited for taxonomic integration are apposi-
tions C.. the printer @A@ exemplification
phrases (&apos;.. printers like the @A@ ..&apos; or nomi-
nal compounds ( `.. the @A@ printer .. These
constructions almost unequivocally determine
`0A0&apos; (target.lex) when considered as a proper
namel to denote an instance of a PRINTER (tar-
get.con), given its characteristic dependency re-
lation to &apos;printer&apos; (base.lex), the conceptual cor-
relate of which is the concept class PRINTER
(base.con). This conclusion is justified indepen-
dent of conceptual conditions, simply due to the
nature of these linguistic constructions.
The generation of corresponding concept hy-
potheses is achieved by the rule sub-hypo (Ta-
ble 4). Basically, the type of target. con is carried
over from base.con (function type-of). In addi-
tion, the syntactic label is asserted which char-
acterizes the grammatical construction figuring
as the structural source for that particular hy-
</bodyText>
<footnote confidence="0.668017">
&apos;Such a part-of-speech hypothesis can be derived
from the inventory of valence and word order specifi-
cations underlying the dependency grammar model we
use (Broker et al., 1994).
</footnote>
<page confidence="0.98554">
478
</page>
<table confidence="0.511558">
sub-hypo (target .con, base.con, h, label)
?type := type-of (base.con)
TELL (target .con :?type)h
add-label ((target.con : ?type)h ,label)
</table>
<tableCaption confidence="0.995478">
Table 4: Taxonomic Hypothesis Generation Rule
</tableCaption>
<bodyText confidence="0.989974872340425">
pothesis (h denotes the identifier for the selected
hypothesis space), e.g., APPOSITION, EXEMPLI-
FICATION, Or NCOMPOUND.
The aggregational dimension of terminologi-
cal theories is addressed, e.g., by grammatical
constructions causing case frame assignments.
In the example `.. @B@ is equipped with 32 MB
of RAM role filler constraints of the verb
form &apos;equipped&apos; that relate to its PATIENT role
carry over to `0B0&apos;. After subsequent seman-
tic interpretation of the entire verbal complex,
&apos;OBO&apos; may be anything that can be equipped
with memory. Constructions like prepositional
phrases ( `.. @C) from IBM or genitives ( `..
IBM&apos;s LaCg) in which either target .lex or
base.lex occur as head or modifier have a simi-
lar effect. Attachments of prepositional phrases
or relations among nouns in genitives, however,
open a wider interpretation space for &apos;OCO&apos;
than for `©BA&apos;, since verbal case frames provide
a higher role selectivity than PP attachments
or, even more so, genitive NPs. So, any concept
that can reasonably be related to the concept
IBM will be considered a potential hypothesis
for &apos;©C©&apos;, e.g., its departments, products, For-
tune 500 ranking.
Generalizing from these considerations, we
state a second hypothesis generation rule which
accounts for aggregational patterns of concept
learning. The basic assumption behind this
rule, perm-hypo (cf. Table 5), is that target .con
fills (exactly) one of the n roles of base.con it
is currently permitted to fill (this set is deter-
mined by the function perm-filler). Depend-
ing on the actual linguistic construction one en-
counters, it may occur, in particular for PP
and NP constructions, that one cannot decide
on the correct role yet. Consequently, several
alternative hypothesis spaces are opened and
target.con is assigned as a potential filler of
the i-th role (taken from ?roleSet, the set of
admitted roles) in its corresponding hypothesis
space. As a result, the classifier is able to de-
rive a suitable concept hypothesis by specializ-
ing target .con according to the value restriction
of base.con&apos;s i-th role. The function member-of
perm-hypo (tar g et .con, base.con, h, label)
</bodyText>
<equation confidence="0.9315706">
?roleSet :=perm-filler(target.con, base.con,h)
?r := l?roleSeti
FORALL ?i :=?r DOWNTO 1 DO
?role; := member-of (?roleSet)
?roleSet :=?roleSet\ {?role}
IF ?i = 1
THEN ?hypo := h
ELSE ?hypo := gen-hypo(h)
TELL (base.con ?role i target.con)?hypo
add-label((base.con ?rolei target.con)Thypo ,label)
</equation>
<tableCaption confidence="0.97">
Table 5: Aggregational Hypothesis Generation Rule
</tableCaption>
<bodyText confidence="0.999843294117647">
selects a role from the set ?roleSet; gen-hypo
creates a new hypothesis space by asserting
the given axioms of h and outputs its identi-
fier. Thereupon, the hypothesis space identified
by ?hypo is augmented through a TELL op-
eration by the hypothesized assertion. As for
sub-hypo, perm-hypo assigns a syntactic qual-
ity label (function add-label) to each i-th hy-
pothesis indicating the type of syntactic con-
struction in which target.lex and base.lex are
related in the text, e.g., CASEFRAME, PPAT-
TACH or GENITivENP.
Getting back to our example, let us assume
that the target Itoh-Ci-8 is predicted already as
a PRODUCT as a result of preceding interpreta-
tion processes, i.e., Itoh-Ci-8 : PRODUCT holds.
Let PRODUCT be defined as:
</bodyText>
<equation confidence="0.977627333333333">
PRODUCT =
VHAS-PART.PHYSICALOBJECT fl VHAS-sizE.SIZE fl
VHAS-PRICE.PRICE fl VHAS-WEIGHT.WEIGHT
</equation>
<bodyText confidence="0.999842842105263">
At this level of conceptual restriction, four
roles have to be considered for relating the tar-
get Itoh-Ci-8 - as a tentative PRODUCT - to
the base concept SWITCH when interpreting the
phrase &apos;The switch of the Itoh-Ci-8 Three of
them, HAS-SIZE, HAS-PRICE, and HAS-WEIGHT,
are ruled out due to the violation of a simple
integrity constraint (&apos;switch&apos; does not denote a
measure unit). Therefore, only the role HAS-
PART must be considered in terms of the expres-
sion Itoh-Ci-8 HAS-PART switch.1 (or, equiva-
lently, switch. 1 PART-OF Itoh-Ci-8). Due to the
definition of HAS-SWITCH (cf. P3, Subsection
2.1), the instantiation of HAS-PART is special-
ized to HAS-SWITCH by the classifier, since the
range of the HAS-PART relation is already re-
stricted to SWITCH (P1). Since the classifier ag-
gressively pushes hypothesizing to be maximally
specific, the disjunctive concept referred to in
</bodyText>
<page confidence="0.998441">
479
</page>
<bodyText confidence="0.999620857142857">
the domain restriction of the role HAS-SWITCH
is split into four distinct hypotheses, two of
which are sketched below. Hence, we assume
Itoh-Ci-8 to denote either a STORAGEDEVice
or an OUTPUTDEVice or an INPuTDEvice or a
COMPUTER (note that we also include parts of
the IS-A hierarchy in the example below).
</bodyText>
<equation confidence="0.9327525">
(Itoh-Ci-8 : SToRAGEDEv)h„
(Itoh-Ci-8 :DEvrcE)hi,••,
(Itoh-Ci-8 HAS-SWITCH switch. 1)h,
(Itoh-Ci-8 : OuTPuTDEv)h2,
(Itoh-Ci-8 :DEvIcE)h„..,
(Itoh-Ci-8 HAS-SWITCH switch.1)h2, • • •
</equation>
<subsectionHeader confidence="0.985525">
2.3 Hypothesis Annotation Rules
</subsectionHeader>
<bodyText confidence="0.9999955">
In this section, we will focus on the quality as-
sessment of concept hypotheses which occurs at
the knowledge base level only; it is due to the
operation of hypothesis annotation rules which
continuously evaluate the hypotheses that have
been derived from linguistic evidence.
The M-Deduction rule (see Table 6) is trig-
gered for any repetitive assignment of the same
role filler to one specific conceptual relation that
occurs in different hypothesis spaces. This rule
captures the assumption that a role filler which
has been multiply derived at different occasions
must be granted more strength than one which
has been derived at a single occasion only.
</bodyText>
<equation confidence="0.957207333333333">
EXISTS oi, 02, R, hi, h2
(01 R 02)h1 A (01 R 02)1,2 A h1 h2
TELL (01 R 02)h1 : M-DEDUCTION
</equation>
<tableCaption confidence="0.912666">
Table 6: The Rule M-Deduction
</tableCaption>
<bodyText confidence="0.999973">
Considering our example at the end of subsec-
tion 2.2, for Itoh-Ci-8&apos; the concept hypotheses
STORAGEDEV and OUTPUTDEV were derived
independently of each other in different hypoth-
esis spaces. Hence, DEVICE as their common
superconcept has been multiply derived by the
classifier in each of these spaces as a result of
transitive closure computations, too. Accord-
ingly, this hypothesis is assigned a high degree
of confidence by the classifier which derives the
conceptual quality label M-DEDUCTION:
</bodyText>
<equation confidence="0.9907735">
(Itoh-Ci-8 : DEvICE)h, A (Itoh-Ci-8 :DEvicE)h2
(Itoh-Ci-8 :DEvicE)h, : M-DEDUCTION
</equation>
<bodyText confidence="0.999734636363636">
The C-Support rule (see Table 7) is triggered
whenever, within the same hypothesis space,
a hypothetical relation, R1, between two in-
stances can be justified by another relation, R2,
involving the same two instances, but where the
role fillers occur in &apos;inverted&apos; order (R1 and R2
need not necessarily be semantically inverse re-
lations, as with &apos;buy&apos; and &apos;sell). This causes
the generation of the quality label C-SUPPORT
which captures the inherent symmetry between
concepts related via quasi-inverse relations.
</bodyText>
<equation confidence="0.607179">
EXISTS oi , 02, R2, h :
(01 R1 02)h A (02 R2 01)h A RI R2
TELL (oi R1 02)h : C-SUPPORT
</equation>
<tableCaption confidence="0.972185">
Table 7: The Rule C-Support
</tableCaption>
<table confidence="0.67134425">
Example:
(Itoh SELLS Itoh-Ci-8)h A
(Itoh-Ci-8 DEVELOPED-By Roh)h
(Itoh SELLS Itoh-Ci-8)h : C-SUPPORT
</table>
<bodyText confidence="0.998185105263158">
Whenever an already filled conceptual rela-
tion receives an additional, yet different role
filler in the same hypothesis space, the Add-
Filler rule is triggered (see Table 8). This
application-specific rule is particularly suited to
our natural language understanding task and
has its roots in the distinction between manda-
tory and optional case roles for (ACTION) verbs.
Roughly, it yields a negative assessment in
terms of the quality label ADDFILLER for any
attempt to fill the same mandatory case role
more than once (unless coordinations are in-
volved). In contradistinction, when the same
role of a non-ACTION concept (typically de-
noted by nouns) is multiply filled we assign the
positive quality label SUPPORT, since it reflects
the conceptual proximity a relation induces on
its component fillers, provided that they share
a common, non-ACTION concept class.
</bodyText>
<equation confidence="0.898073666666667">
EXISTS oi , o2, o3, R, h :
(01 R 02)h A (01 R 03)h A (01 : ACTION),,
TELL (01 R 02)h : ADDFILLER
</equation>
<tableCaption confidence="0.975529">
Table 8: The Rule AddFiller
</tableCaption>
<bodyText confidence="0.999381">
We give examples both for the assignment of
an ADDFILLER as well as for a SUPPORT label:
</bodyText>
<equation confidence="0.734647333333333">
Examples:
(produces.1 : ACTION),, A
(produces.1 AGENT Itoh),, A
</equation>
<footnote confidence="0.6546955">
(produces.1 AGENT IBM),,
(produces.1 AGENT Itoh),, : ADDFILLER
(Itoh-Ci-8: PRINTER),, A (Itoh-Ct : PRINTER),, A
(Itoh SELLS Itoh-Ci-8),, A (Doh SELLS Itoh-Ct)h A
(Itoh : -&apos;ACTION),,
(Itoh-Ci-8: PRINTER)h : SUPPORT
</footnote>
<page confidence="0.99332">
480
</page>
<subsectionHeader confidence="0.992725">
2.4 Quality Dimensions
</subsectionHeader>
<bodyText confidence="0.999448787234043">
The criteria from which concept hypotheses
are derived differ in the dimension from which
they are drawn (grammatical vs. conceptual ev-
idence), as well as the strength by which they
lend support to the corresponding hypotheses
(e.g., apposition vs. genitive, multiple deduc-
tion vs. additional role filling, etc.). In order
to make these distinctions explicit we have de-
veloped a &amp;quot;quality calculus&amp;quot; at the core of which
lie the definition of and inference rules for qual-
ity labels (cf. Schnattinger and Hahn (1998) for
more details). A design methodology for specific
quality calculi may proceed along the follow-
ing lines: (1) Define the dimensions from which
quality labels can be drawn. In our application,
we chose the set LQ := {11,...,40 of linguistic
quality labels and CQ :=..,c} of con-
ceptual quality labels. (2) Determine a partial
ordering p among the quality labels from one di-
znension reflecting different degrees of strength
among the quality labels. (3) Determine a total
ordering among the dimensions.
In our application, we have empirical evi-
dence to grant linguistic criteria priority over
conceptual ones. Hence, we state the following
constraint: V/ E LQ,Vc E CQ:1 &gt;p C
The dimension LQ. Linguistic quality labels
reflect structural properties of phrasal patterns
or discourse contexts in which unknown lexi-
cal items occur&apos; — we here assume that the
type of grammatical construction exercises a
particular interpretative force on the unknown
item and, at the same time, yields a particu-
lar level of credibility for the hypotheses being
derived. Taking the considerations from Sub-
section 2.2 into account, concrete examples of
high-quality labels are given by APPOSITION or
N COMPOUND labels. Still of good quality but
already less constraining are occurrences of the
unknown item in a CASEFRAME construction.
Finally, in a PPATTACH or GENITIvENP con-
struction the unknown lexical item is still less
constrained. Hence, at the quality level, these
latter two labels (just as the first two labels we
considered) form an equivalence class whose el-
ements cannot be further discriminated. So we
end up with the following quality orderings:
</bodyText>
<footnote confidence="0.513321333333333">
2In the future, we intend to integrate additional types
of constraints, e.g., quality criteria reflecting the degree
of completeness vs. partiality of the parse.
</footnote>
<table confidence="0.895947333333333">
NCOMPOUND APPOSITION
NCOMPOUND &gt;p CASEFRAME
APPOSITION &gt;p CASEFRAME
CASEFRAME &gt;p GENITIVENP
CASEFRAME &gt;p PPATTACH
GENITIVENP =p PPATTACH
</table>
<bodyText confidence="0.989051285714286">
The dimension CQ. Conceptual quality labels
result from comparing the conceptual represen-
tation structures of a concept hypothesis with
already existing representation structures in the
underlying domain knowledge base or other con-
cept hypotheses from the viewpoint of struc-
tural similarity, compatibility, etc. The closer
the match, the more credit is lent to a hypoth-
esis. A very positive conceptual quality label,
e.g., is M-DEDUCTION, whereas ADDFILLER is
a negative one. Still positive strength is ex-
pressed by SUPPORT or C-SUPPORT, both being
indistinguishable, however, from a quality point
of view. Accordingly, we may state:
</bodyText>
<table confidence="0.6850484">
M-DEDUCTION &gt;p SUPPORT
M-DEDUCTION &gt;9 C-SUPPORT
SUPPORT --P C-SUPPORT
SUPPORT &gt;9 ADD FILLER
C-SUPPORT &gt;9 ADD FILLER
</table>
<subsectionHeader confidence="0.970656">
2.5 Hypothesis Ranking
</subsectionHeader>
<bodyText confidence="0.999827083333333">
Each new clue available for a target concept to
be learned results in the generation of additional
linguistic or conceptual quality labels. So hy-
pothesis spaces get incrementally augmented by
quality statements. In order to select the most
credible one(s) among them we apply a two-step
procedure (the details of which are explained
in Schnattinger and Hahn (1998)). First, those
concept hypotheses are chosen which have ac-
cumulated the greatest amount of high-quality
labels according to the linguistic dimension LQ.
Second, further hypotheses are selected from
this linguistically plausible candidate set based
on the quality ordering underlying CQ.
We have also made considerable efforts to
evaluate the performance of the text learner
based on the quality calculus. In order to ac-
count for the incrementality of the learning pro-
cess, a new evaluation measure capturing the
system&apos;s on-line learning accuracy was defined,
which is sensitive to taxonomic hierarchies. The
results we got were consistently favorable, as
our system outperformed those closest in spirit,
CAMILLE (Hastings, 1996) and SCISOR (Rau et
</bodyText>
<page confidence="0.996358">
481
</page>
<bodyText confidence="0.999728166666667">
al., 1989), by a gain in accuracy on the or-
der of 8%. Also, the system requires relatively
few hypothesis spaces (2 to 6 on average) and
prunes the concept search space radically, re-
quiring only a few examples (for evaluation de-
tails, cf. Hahn and Schnattinger (1998)).
</bodyText>
<sectionHeader confidence="0.999904" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999978071428571">
We are not concerned with lexical acquisition
from very large corpora using surface-level collo-
cational data as proposed by Zernik and Jacobs
(1990) and Velardi et al. (1991), or with hy-
ponym extraction based on entirely syntactic
criteria as in Hearst (1992) or lexico-semantic
associations (e.g., Resnik (1992) or Sekine et al.
(1994)). This is mainly due to the fact that
these studies aim at a shallower level of learn-
ing (e.g., selectional restrictions or thematic re-
lations of verbs), while our focus is on much
more fine-grained conceptual knowledge (roles,
role filler constraints, integrity conditions).
Our approach bears a close relationship, how-
ever, to the work of Mooney (1987), Berwick
(1989), Rau et al. (1989), Gomez and Segami
(1990), and Hastings (1996), who all aim at the
automated learning of word meanings from con-
text using a knowledge-intensive approach. But
our work differs from theirs in that the need to
cope with several competing concept hypotheses
and to aim at a reason-based selection in terms
of the quality of arguments is not an issue in
these studies. Learning from real-world texts
usually provides the learner with only sparse
and fragmentary evidence, such that multiple
hypotheses are likely to be derived and a need
for a hypothesis evaluation arises.
</bodyText>
<sectionHeader confidence="0.998961" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999543166666667">
We have introduced a solution for the semantic
acquisition problem on the basis of the auto-
matic processing of expository texts. The learn-
ing methodology we propose is based on the
incremental assignment and evaluation of the
quality of linguistic and conceptual evidence for
emerging concept hypotheses. No specialized
learning algorithm is needed, since learning is
a reasoning task carried out by the classifier
of a terminological reasoning system. However,
strong heuristic guidance for selecting between
plausible hypotheses comes from linguistic and
conceptual quality criteria.
Acknowledgements. We would like to thank
our colleagues in the CLIF group for fruitful discus-
sions, in particular Joe Bush who polished the text
as a native speaker. K. Schnattinger is supported by
a grant from DFG (Ha 2097/3-1).
</bodyText>
<sectionHeader confidence="0.99165" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989908">
R. Berwick. 1989. Learning word meanings from
examples. In D. Waltz, editor, Semantic Struc-
tures., pages 89-124. Lawrence Erlbaum.
N. Broker, U. Hahn, and S. Schacht. 1994.
Concurrent lexicalized dependency parsing: the
PARSETALK model. In Proc. of the COLING&apos;94.
Vol.1, pages 379-385.
F. Gomez and C. Segami. 1990. Knowledge acqui-
sition from natural language for expert systems
based on classification problem-solving methods.
Knowledge Acquisition, 2(2):107-128.
U. Hahn and K. Schnattinger. 1998. Towards text
knowledge engineering. In Proc. of the AAAI&apos;98.
P. Hastings. 1996. Implications of an automatic lex-
ical acquisition system. In S. Wermter, E. Riloff,
and G. Scheler, editors, Connectionist, Statistical
and Symbolic Approaches to Learning for Natural
Language Processing, pages 261-274. Springer.
M. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of the
COLING&apos;92. Vol.2, pages 539-545.
D. Hindle. 1989. Acquiring disambiguation rules
from text. In Proc. of the ACL&apos;89, pages 26-29.
C. Manning. 1993. Automatic acquisition of large
subcategorization dictionary from corpora. In
Proc. of the ACL&apos;93, pages 235-242.
R. Mooney. 1987. Integrated learning of words
and their underlying concepts. In Proc. of the
CogSci&apos;87, pages 974-978.
L. Rau, P. Jacobs, and U. Zernik. 1989. Information
extraction and text summarization using linguis-
tic knowledge acquisition. Information Processing
6 Management, 25(4):419-428.
P. Resnik. 1992. A class-based approach to lexical
discovery. In Proc. of the ACL &apos;92, pages 327-329.
K. Schnattinger and U. Hahn. 1998. Quality-based
learning. In Proc. of the ECAI&apos;98, pages 160-164.
S. Sekine, J. Carroll, S. Ananiadou, and J. Tsujii.
1994. Automatic learning for semantic colloca-
tion. In Proc. of the ANLPN, pages 104-110.
P. Velardi, M. Pazienza, and M. Fasolo. 1991.
How to encode semantic knowledge: a method for
meaning representation and computer-aided ac-
quisition. Computational Linguistics, 17:153-170.
W. Woods and J. Schmolze. 1992. The KL-ONE
family. Computers &amp; Mathematics with Applica-
tions, 23(2/5):133-177.
U. Zernik and P. Jacobs. 1990. Tagging for learn-
ing: collecting thematic relations from corpus. In
Proc. of the COLING&apos;90. Vol.1, pages 34-39.
</reference>
<page confidence="0.998464">
482
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.215147">
<title confidence="0.999281">A Text Understander that Learns</title>
<author confidence="0.999311">Udo Hahn</author>
<author confidence="0.999311">Klemens Schnattinger</author>
<affiliation confidence="0.999939">Computational Linguistics Lab, Freiburg University</affiliation>
<address confidence="0.994872">Werthmannplatz 1, D-79085 Freiburg, Germany</address>
<email confidence="0.992726">hahnOcoling.uni-freiburg.de</email>
<email confidence="0.992726">schnattingerOcoling.uni-freiburg.de</email>
<abstract confidence="0.807634533333333">Fly,,,,d–i,, J Hypothesis sluttrl Quadifkr Abstract We introduce an approach to the automatic acquisition of new concepts from natural language texts which is tightly integrated with the underlying text understanding process. The learning model is centered around the &apos;quality&apos; of different forms of linguistic and conceptual evidence which underlies the incremental generation and refinement of alternative concept hypotheses, each one capturing a different conceptual reading for an unknown lexical item.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Berwick</author>
</authors>
<title>Learning word meanings from examples. In</title>
<date>1989</date>
<booktitle>Semantic Structures.,</booktitle>
<pages>89--124</pages>
<editor>D. Waltz, editor,</editor>
<publisher>Lawrence Erlbaum.</publisher>
<contexts>
<context position="25773" citStr="Berwick (1989)" startWordPosition="4094" endWordPosition="4095">locational data as proposed by Zernik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations (e.g., Resnik (1992) or Sekine et al. (1994)). This is mainly due to the fact that these studies aim at a shallower level of learning (e.g., selectional restrictions or thematic relations of verbs), while our focus is on much more fine-grained conceptual knowledge (roles, role filler constraints, integrity conditions). Our approach bears a close relationship, however, to the work of Mooney (1987), Berwick (1989), Rau et al. (1989), Gomez and Segami (1990), and Hastings (1996), who all aim at the automated learning of word meanings from context using a knowledge-intensive approach. But our work differs from theirs in that the need to cope with several competing concept hypotheses and to aim at a reason-based selection in terms of the quality of arguments is not an issue in these studies. Learning from real-world texts usually provides the learner with only sparse and fragmentary evidence, such that multiple hypotheses are likely to be derived and a need for a hypothesis evaluation arises. 4 Conclusion</context>
</contexts>
<marker>Berwick, 1989</marker>
<rawString>R. Berwick. 1989. Learning word meanings from examples. In D. Waltz, editor, Semantic Structures., pages 89-124. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Broker</author>
<author>U Hahn</author>
<author>S Schacht</author>
</authors>
<title>Concurrent lexicalized dependency parsing: the PARSETALK model.</title>
<date>1994</date>
<booktitle>In Proc. of the COLING&apos;94. Vol.1,</booktitle>
<pages>379--385</pages>
<contexts>
<context position="2513" citStr="Broker et al. (1994)" startWordPosition="370" endWordPosition="373">reasoning. The &apos;plain&apos; text understanding mode can be considered as the instantiation and continuous filling Hypothesis space-p Hypothesis spave-ti Figure 1 Architecture of the Text Learner of roles with respect to single concepts already available in the knowledge base. Under learning conditions, however, a set of alternative concept hypotheses has to be maintained for each unknown item, with each hypothesis denoting a newly created conceptual interpretation tentatively associated with the unknown item. The underlying methodology is summarized in Fig. 1. The test parser (for an overview, cf. Broker et al. (1994)) yields information from the grammatical constructions in which an unknown lexical item (symbolized by the black square) occurs in terms of the corresponding dependency parse tree. The kinds of syntactic constructions (e.g., genitive, apposition, comparative), in which unknown lexical items appear, are recorded and later assessed relative to the credit they lend to a particular hypothesis. The conceptual interpretation of parse trees involving unknown lexical items in the domain knowledge base leads to the derivation of concept hypotheses, which are further enriched by conceptual annotations.</context>
<context position="12075" citStr="Broker et al., 1994" startWordPosition="1937" endWordPosition="1940">n is justified independent of conceptual conditions, simply due to the nature of these linguistic constructions. The generation of corresponding concept hypotheses is achieved by the rule sub-hypo (Table 4). Basically, the type of target. con is carried over from base.con (function type-of). In addition, the syntactic label is asserted which characterizes the grammatical construction figuring as the structural source for that particular hy&apos;Such a part-of-speech hypothesis can be derived from the inventory of valence and word order specifications underlying the dependency grammar model we use (Broker et al., 1994). 478 sub-hypo (target .con, base.con, h, label) ?type := type-of (base.con) TELL (target .con :?type)h add-label ((target.con : ?type)h ,label) Table 4: Taxonomic Hypothesis Generation Rule pothesis (h denotes the identifier for the selected hypothesis space), e.g., APPOSITION, EXEMPLIFICATION, Or NCOMPOUND. The aggregational dimension of terminological theories is addressed, e.g., by grammatical constructions causing case frame assignments. In the example `.. @B@ is equipped with 32 MB of RAM role filler constraints of the verb form &apos;equipped&apos; that relate to its PATIENT role carry over to `0</context>
</contexts>
<marker>Broker, Hahn, Schacht, 1994</marker>
<rawString>N. Broker, U. Hahn, and S. Schacht. 1994. Concurrent lexicalized dependency parsing: the PARSETALK model. In Proc. of the COLING&apos;94. Vol.1, pages 379-385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Gomez</author>
<author>C Segami</author>
</authors>
<title>Knowledge acquisition from natural language for expert systems based on classification problem-solving methods.</title>
<date>1990</date>
<journal>Knowledge Acquisition,</journal>
<pages>2--2</pages>
<contexts>
<context position="25817" citStr="Gomez and Segami (1990)" startWordPosition="4100" endWordPosition="4103">ik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations (e.g., Resnik (1992) or Sekine et al. (1994)). This is mainly due to the fact that these studies aim at a shallower level of learning (e.g., selectional restrictions or thematic relations of verbs), while our focus is on much more fine-grained conceptual knowledge (roles, role filler constraints, integrity conditions). Our approach bears a close relationship, however, to the work of Mooney (1987), Berwick (1989), Rau et al. (1989), Gomez and Segami (1990), and Hastings (1996), who all aim at the automated learning of word meanings from context using a knowledge-intensive approach. But our work differs from theirs in that the need to cope with several competing concept hypotheses and to aim at a reason-based selection in terms of the quality of arguments is not an issue in these studies. Learning from real-world texts usually provides the learner with only sparse and fragmentary evidence, such that multiple hypotheses are likely to be derived and a need for a hypothesis evaluation arises. 4 Conclusion We have introduced a solution for the seman</context>
</contexts>
<marker>Gomez, Segami, 1990</marker>
<rawString>F. Gomez and C. Segami. 1990. Knowledge acquisition from natural language for expert systems based on classification problem-solving methods. Knowledge Acquisition, 2(2):107-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>K Schnattinger</author>
</authors>
<title>Towards text knowledge engineering.</title>
<date>1998</date>
<booktitle>In Proc. of the AAAI&apos;98.</booktitle>
<contexts>
<context position="25048" citStr="Hahn and Schnattinger (1998)" startWordPosition="3977" endWordPosition="3980">ty calculus. In order to account for the incrementality of the learning process, a new evaluation measure capturing the system&apos;s on-line learning accuracy was defined, which is sensitive to taxonomic hierarchies. The results we got were consistently favorable, as our system outperformed those closest in spirit, CAMILLE (Hastings, 1996) and SCISOR (Rau et 481 al., 1989), by a gain in accuracy on the order of 8%. Also, the system requires relatively few hypothesis spaces (2 to 6 on average) and prunes the concept search space radically, requiring only a few examples (for evaluation details, cf. Hahn and Schnattinger (1998)). 3 Related Work We are not concerned with lexical acquisition from very large corpora using surface-level collocational data as proposed by Zernik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations (e.g., Resnik (1992) or Sekine et al. (1994)). This is mainly due to the fact that these studies aim at a shallower level of learning (e.g., selectional restrictions or thematic relations of verbs), while our focus is on much more fine-grained conceptual knowledge (roles, role filler const</context>
</contexts>
<marker>Hahn, Schnattinger, 1998</marker>
<rawString>U. Hahn and K. Schnattinger. 1998. Towards text knowledge engineering. In Proc. of the AAAI&apos;98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hastings</author>
</authors>
<title>Implications of an automatic lexical acquisition system.</title>
<date>1996</date>
<booktitle>Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing,</booktitle>
<pages>261--274</pages>
<editor>In S. Wermter, E. Riloff, and G. Scheler, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="24757" citStr="Hastings, 1996" startWordPosition="3926" endWordPosition="3927">ording to the linguistic dimension LQ. Second, further hypotheses are selected from this linguistically plausible candidate set based on the quality ordering underlying CQ. We have also made considerable efforts to evaluate the performance of the text learner based on the quality calculus. In order to account for the incrementality of the learning process, a new evaluation measure capturing the system&apos;s on-line learning accuracy was defined, which is sensitive to taxonomic hierarchies. The results we got were consistently favorable, as our system outperformed those closest in spirit, CAMILLE (Hastings, 1996) and SCISOR (Rau et 481 al., 1989), by a gain in accuracy on the order of 8%. Also, the system requires relatively few hypothesis spaces (2 to 6 on average) and prunes the concept search space radically, requiring only a few examples (for evaluation details, cf. Hahn and Schnattinger (1998)). 3 Related Work We are not concerned with lexical acquisition from very large corpora using surface-level collocational data as proposed by Zernik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations</context>
</contexts>
<marker>Hastings, 1996</marker>
<rawString>P. Hastings. 1996. Implications of an automatic lexical acquisition system. In S. Wermter, E. Riloff, and G. Scheler, editors, Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing, pages 261-274. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of the COLING&apos;92. Vol.2,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="1547" citStr="Hearst, 1992" startWordPosition="228" endWordPosition="229">dge of the domain the texts are about, and grammatical constructions in which unknown lexical items occur. While there may be many reasonable interpretations when an unknown item occurs for the very first time in a text, their number rapidly decreases when more and more evidence is gathered. Our model tries to make explicit the reasoning processes behind this learning pattern. Unlike the current mainstream in automatic linguistic knowledge acquisition, which can be characterized as quantitative, surface-oriented bulk processing of large corpora of texts (Hindle, 1989; Zernik and Jacobs, 1990; Hearst, 1992; Manning, 1993), we propose here a knowledge-intensive model of concept learning from few, positive-only examples that is tightly integrated with the non-learning mode of text understanding. Both learning and understanding build on a given core ontology in the format of terminological assertions and, hence, make abundant use of terminological reasoning. The &apos;plain&apos; text understanding mode can be considered as the instantiation and continuous filling Hypothesis space-p Hypothesis spave-ti Figure 1 Architecture of the Text Learner of roles with respect to single concepts already available in th</context>
<context position="25325" citStr="Hearst (1992)" startWordPosition="4024" endWordPosition="4025">e closest in spirit, CAMILLE (Hastings, 1996) and SCISOR (Rau et 481 al., 1989), by a gain in accuracy on the order of 8%. Also, the system requires relatively few hypothesis spaces (2 to 6 on average) and prunes the concept search space radically, requiring only a few examples (for evaluation details, cf. Hahn and Schnattinger (1998)). 3 Related Work We are not concerned with lexical acquisition from very large corpora using surface-level collocational data as proposed by Zernik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations (e.g., Resnik (1992) or Sekine et al. (1994)). This is mainly due to the fact that these studies aim at a shallower level of learning (e.g., selectional restrictions or thematic relations of verbs), while our focus is on much more fine-grained conceptual knowledge (roles, role filler constraints, integrity conditions). Our approach bears a close relationship, however, to the work of Mooney (1987), Berwick (1989), Rau et al. (1989), Gomez and Segami (1990), and Hastings (1996), who all aim at the automated learning of word meanings from context using a knowledge</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of the COLING&apos;92. Vol.2, pages 539-545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Acquiring disambiguation rules from text.</title>
<date>1989</date>
<booktitle>In Proc. of the ACL&apos;89,</booktitle>
<pages>26--29</pages>
<contexts>
<context position="1508" citStr="Hindle, 1989" startWordPosition="221" endWordPosition="223"> sources of evidence — the prior knowledge of the domain the texts are about, and grammatical constructions in which unknown lexical items occur. While there may be many reasonable interpretations when an unknown item occurs for the very first time in a text, their number rapidly decreases when more and more evidence is gathered. Our model tries to make explicit the reasoning processes behind this learning pattern. Unlike the current mainstream in automatic linguistic knowledge acquisition, which can be characterized as quantitative, surface-oriented bulk processing of large corpora of texts (Hindle, 1989; Zernik and Jacobs, 1990; Hearst, 1992; Manning, 1993), we propose here a knowledge-intensive model of concept learning from few, positive-only examples that is tightly integrated with the non-learning mode of text understanding. Both learning and understanding build on a given core ontology in the format of terminological assertions and, hence, make abundant use of terminological reasoning. The &apos;plain&apos; text understanding mode can be considered as the instantiation and continuous filling Hypothesis space-p Hypothesis spave-ti Figure 1 Architecture of the Text Learner of roles with respect to </context>
</contexts>
<marker>Hindle, 1989</marker>
<rawString>D. Hindle. 1989. Acquiring disambiguation rules from text. In Proc. of the ACL&apos;89, pages 26-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
</authors>
<title>Automatic acquisition of large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>In Proc. of the ACL&apos;93,</booktitle>
<pages>235--242</pages>
<contexts>
<context position="1563" citStr="Manning, 1993" startWordPosition="230" endWordPosition="231">ain the texts are about, and grammatical constructions in which unknown lexical items occur. While there may be many reasonable interpretations when an unknown item occurs for the very first time in a text, their number rapidly decreases when more and more evidence is gathered. Our model tries to make explicit the reasoning processes behind this learning pattern. Unlike the current mainstream in automatic linguistic knowledge acquisition, which can be characterized as quantitative, surface-oriented bulk processing of large corpora of texts (Hindle, 1989; Zernik and Jacobs, 1990; Hearst, 1992; Manning, 1993), we propose here a knowledge-intensive model of concept learning from few, positive-only examples that is tightly integrated with the non-learning mode of text understanding. Both learning and understanding build on a given core ontology in the format of terminological assertions and, hence, make abundant use of terminological reasoning. The &apos;plain&apos; text understanding mode can be considered as the instantiation and continuous filling Hypothesis space-p Hypothesis spave-ti Figure 1 Architecture of the Text Learner of roles with respect to single concepts already available in the knowledge base</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>C. Manning. 1993. Automatic acquisition of large subcategorization dictionary from corpora. In Proc. of the ACL&apos;93, pages 235-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mooney</author>
</authors>
<title>Integrated learning of words and their underlying concepts.</title>
<date>1987</date>
<booktitle>In Proc. of the CogSci&apos;87,</booktitle>
<pages>974--978</pages>
<contexts>
<context position="25757" citStr="Mooney (1987)" startWordPosition="4092" endWordPosition="4093">rface-level collocational data as proposed by Zernik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations (e.g., Resnik (1992) or Sekine et al. (1994)). This is mainly due to the fact that these studies aim at a shallower level of learning (e.g., selectional restrictions or thematic relations of verbs), while our focus is on much more fine-grained conceptual knowledge (roles, role filler constraints, integrity conditions). Our approach bears a close relationship, however, to the work of Mooney (1987), Berwick (1989), Rau et al. (1989), Gomez and Segami (1990), and Hastings (1996), who all aim at the automated learning of word meanings from context using a knowledge-intensive approach. But our work differs from theirs in that the need to cope with several competing concept hypotheses and to aim at a reason-based selection in terms of the quality of arguments is not an issue in these studies. Learning from real-world texts usually provides the learner with only sparse and fragmentary evidence, such that multiple hypotheses are likely to be derived and a need for a hypothesis evaluation aris</context>
</contexts>
<marker>Mooney, 1987</marker>
<rawString>R. Mooney. 1987. Integrated learning of words and their underlying concepts. In Proc. of the CogSci&apos;87, pages 974-978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rau</author>
<author>P Jacobs</author>
<author>U Zernik</author>
</authors>
<title>Information extraction and text summarization using linguistic knowledge acquisition.</title>
<date>1989</date>
<booktitle>Information Processing 6 Management,</booktitle>
<pages>25--4</pages>
<contexts>
<context position="25792" citStr="Rau et al. (1989)" startWordPosition="4096" endWordPosition="4099">as proposed by Zernik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations (e.g., Resnik (1992) or Sekine et al. (1994)). This is mainly due to the fact that these studies aim at a shallower level of learning (e.g., selectional restrictions or thematic relations of verbs), while our focus is on much more fine-grained conceptual knowledge (roles, role filler constraints, integrity conditions). Our approach bears a close relationship, however, to the work of Mooney (1987), Berwick (1989), Rau et al. (1989), Gomez and Segami (1990), and Hastings (1996), who all aim at the automated learning of word meanings from context using a knowledge-intensive approach. But our work differs from theirs in that the need to cope with several competing concept hypotheses and to aim at a reason-based selection in terms of the quality of arguments is not an issue in these studies. Learning from real-world texts usually provides the learner with only sparse and fragmentary evidence, such that multiple hypotheses are likely to be derived and a need for a hypothesis evaluation arises. 4 Conclusion We have introduced</context>
</contexts>
<marker>Rau, Jacobs, Zernik, 1989</marker>
<rawString>L. Rau, P. Jacobs, and U. Zernik. 1989. Information extraction and text summarization using linguistic knowledge acquisition. Information Processing 6 Management, 25(4):419-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>A class-based approach to lexical discovery.</title>
<date>1992</date>
<booktitle>In Proc. of the ACL &apos;92,</booktitle>
<pages>327--329</pages>
<contexts>
<context position="25378" citStr="Resnik (1992)" startWordPosition="4030" endWordPosition="4031">SOR (Rau et 481 al., 1989), by a gain in accuracy on the order of 8%. Also, the system requires relatively few hypothesis spaces (2 to 6 on average) and prunes the concept search space radically, requiring only a few examples (for evaluation details, cf. Hahn and Schnattinger (1998)). 3 Related Work We are not concerned with lexical acquisition from very large corpora using surface-level collocational data as proposed by Zernik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations (e.g., Resnik (1992) or Sekine et al. (1994)). This is mainly due to the fact that these studies aim at a shallower level of learning (e.g., selectional restrictions or thematic relations of verbs), while our focus is on much more fine-grained conceptual knowledge (roles, role filler constraints, integrity conditions). Our approach bears a close relationship, however, to the work of Mooney (1987), Berwick (1989), Rau et al. (1989), Gomez and Segami (1990), and Hastings (1996), who all aim at the automated learning of word meanings from context using a knowledge-intensive approach. But our work differs from theirs</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>P. Resnik. 1992. A class-based approach to lexical discovery. In Proc. of the ACL &apos;92, pages 327-329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Schnattinger</author>
<author>U Hahn</author>
</authors>
<title>Quality-based learning.</title>
<date>1998</date>
<booktitle>In Proc. of the ECAI&apos;98,</booktitle>
<pages>160--164</pages>
<contexts>
<context position="4664" citStr="Schnattinger and Hahn (1998)" startWordPosition="729" endWordPosition="732">cept hypotheses by taking the available set of quality labels for each hypothesis into account. The final computation of a preference order for the entire set of competing hypotheses takes place in the qualifier, a terminological classifier extended by an evaluation metric for quality-based selection criteria. The output of the quality machine is a ranked list of concept hypotheses. The ranking yields, in decreasing order of significance, either the most plausible concept classes which classify the considered instance or more general concept classes subsuming the considered concept class (cf. Schnattinger and Hahn (1998) for details). 2 Methodological Framework In this section, we present the major methodological decisions underlying our approach. 2.1 Terminological Logics We use a standard terminological, KL-ONEstyle concept description language, here referred to as CD ,C (for a survey of this paradigm, cf. Woods and Schmolze (1992)). It has several constructors combining atomic concepts, roles and individuals to define the terminological theory of a domain. Concepts are unary predicates, roles are binary predicates over a domain A, with individuals being the elements of A. We assume a common set-theoretical</context>
<context position="20952" citStr="Schnattinger and Hahn (1998)" startWordPosition="3341" endWordPosition="3344">oh-Ci-8),, A (Doh SELLS Itoh-Ct)h A (Itoh : -&apos;ACTION),, (Itoh-Ci-8: PRINTER)h : SUPPORT 480 2.4 Quality Dimensions The criteria from which concept hypotheses are derived differ in the dimension from which they are drawn (grammatical vs. conceptual evidence), as well as the strength by which they lend support to the corresponding hypotheses (e.g., apposition vs. genitive, multiple deduction vs. additional role filling, etc.). In order to make these distinctions explicit we have developed a &amp;quot;quality calculus&amp;quot; at the core of which lie the definition of and inference rules for quality labels (cf. Schnattinger and Hahn (1998) for more details). A design methodology for specific quality calculi may proceed along the following lines: (1) Define the dimensions from which quality labels can be drawn. In our application, we chose the set LQ := {11,...,40 of linguistic quality labels and CQ :=..,c} of conceptual quality labels. (2) Determine a partial ordering p among the quality labels from one diznension reflecting different degrees of strength among the quality labels. (3) Determine a total ordering among the dimensions. In our application, we have empirical evidence to grant linguistic criteria priority over concept</context>
<context position="24027" citStr="Schnattinger and Hahn (1998)" startWordPosition="3816" endWordPosition="3819">sed by SUPPORT or C-SUPPORT, both being indistinguishable, however, from a quality point of view. Accordingly, we may state: M-DEDUCTION &gt;p SUPPORT M-DEDUCTION &gt;9 C-SUPPORT SUPPORT --P C-SUPPORT SUPPORT &gt;9 ADD FILLER C-SUPPORT &gt;9 ADD FILLER 2.5 Hypothesis Ranking Each new clue available for a target concept to be learned results in the generation of additional linguistic or conceptual quality labels. So hypothesis spaces get incrementally augmented by quality statements. In order to select the most credible one(s) among them we apply a two-step procedure (the details of which are explained in Schnattinger and Hahn (1998)). First, those concept hypotheses are chosen which have accumulated the greatest amount of high-quality labels according to the linguistic dimension LQ. Second, further hypotheses are selected from this linguistically plausible candidate set based on the quality ordering underlying CQ. We have also made considerable efforts to evaluate the performance of the text learner based on the quality calculus. In order to account for the incrementality of the learning process, a new evaluation measure capturing the system&apos;s on-line learning accuracy was defined, which is sensitive to taxonomic hierarc</context>
</contexts>
<marker>Schnattinger, Hahn, 1998</marker>
<rawString>K. Schnattinger and U. Hahn. 1998. Quality-based learning. In Proc. of the ECAI&apos;98, pages 160-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>J Carroll</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>Automatic learning for semantic collocation.</title>
<date>1994</date>
<booktitle>In Proc. of the ANLPN,</booktitle>
<pages>104--110</pages>
<contexts>
<context position="25402" citStr="Sekine et al. (1994)" startWordPosition="4033" endWordPosition="4036">l., 1989), by a gain in accuracy on the order of 8%. Also, the system requires relatively few hypothesis spaces (2 to 6 on average) and prunes the concept search space radically, requiring only a few examples (for evaluation details, cf. Hahn and Schnattinger (1998)). 3 Related Work We are not concerned with lexical acquisition from very large corpora using surface-level collocational data as proposed by Zernik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations (e.g., Resnik (1992) or Sekine et al. (1994)). This is mainly due to the fact that these studies aim at a shallower level of learning (e.g., selectional restrictions or thematic relations of verbs), while our focus is on much more fine-grained conceptual knowledge (roles, role filler constraints, integrity conditions). Our approach bears a close relationship, however, to the work of Mooney (1987), Berwick (1989), Rau et al. (1989), Gomez and Segami (1990), and Hastings (1996), who all aim at the automated learning of word meanings from context using a knowledge-intensive approach. But our work differs from theirs in that the need to cop</context>
</contexts>
<marker>Sekine, Carroll, Ananiadou, Tsujii, 1994</marker>
<rawString>S. Sekine, J. Carroll, S. Ananiadou, and J. Tsujii. 1994. Automatic learning for semantic collocation. In Proc. of the ANLPN, pages 104-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Velardi</author>
<author>M Pazienza</author>
<author>M Fasolo</author>
</authors>
<title>How to encode semantic knowledge: a method for meaning representation and computer-aided acquisition.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--153</pages>
<contexts>
<context position="25240" citStr="Velardi et al. (1991)" startWordPosition="4008" endWordPosition="4011"> hierarchies. The results we got were consistently favorable, as our system outperformed those closest in spirit, CAMILLE (Hastings, 1996) and SCISOR (Rau et 481 al., 1989), by a gain in accuracy on the order of 8%. Also, the system requires relatively few hypothesis spaces (2 to 6 on average) and prunes the concept search space radically, requiring only a few examples (for evaluation details, cf. Hahn and Schnattinger (1998)). 3 Related Work We are not concerned with lexical acquisition from very large corpora using surface-level collocational data as proposed by Zernik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations (e.g., Resnik (1992) or Sekine et al. (1994)). This is mainly due to the fact that these studies aim at a shallower level of learning (e.g., selectional restrictions or thematic relations of verbs), while our focus is on much more fine-grained conceptual knowledge (roles, role filler constraints, integrity conditions). Our approach bears a close relationship, however, to the work of Mooney (1987), Berwick (1989), Rau et al. (1989), Gomez and Segami (1990), and Hastings (1996), </context>
</contexts>
<marker>Velardi, Pazienza, Fasolo, 1991</marker>
<rawString>P. Velardi, M. Pazienza, and M. Fasolo. 1991. How to encode semantic knowledge: a method for meaning representation and computer-aided acquisition. Computational Linguistics, 17:153-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
<author>J Schmolze</author>
</authors>
<date>1992</date>
<booktitle>The KL-ONE family. Computers &amp; Mathematics with Applications,</booktitle>
<pages>23--2</pages>
<contexts>
<context position="4983" citStr="Woods and Schmolze (1992)" startWordPosition="777" endWordPosition="780"> of the quality machine is a ranked list of concept hypotheses. The ranking yields, in decreasing order of significance, either the most plausible concept classes which classify the considered instance or more general concept classes subsuming the considered concept class (cf. Schnattinger and Hahn (1998) for details). 2 Methodological Framework In this section, we present the major methodological decisions underlying our approach. 2.1 Terminological Logics We use a standard terminological, KL-ONEstyle concept description language, here referred to as CD ,C (for a survey of this paradigm, cf. Woods and Schmolze (1992)). It has several constructors combining atomic concepts, roles and individuals to define the terminological theory of a domain. Concepts are unary predicates, roles are binary predicates over a domain A, with individuals being the elements of A. We assume a common set-theoretical semantics for CDL — an interpretation I is a function that assigns to each concept symbol (the set A) a subset of the domain A, I : A —+ 2°, to each role symbol (the set P) a binary relation of A, I : P 24&apos;x°, and to each individual symbol (the set I) an element of A, I : I -4 A. Concept terms and role terms are defi</context>
</contexts>
<marker>Woods, Schmolze, 1992</marker>
<rawString>W. Woods and J. Schmolze. 1992. The KL-ONE family. Computers &amp; Mathematics with Applications, 23(2/5):133-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Zernik</author>
<author>P Jacobs</author>
</authors>
<title>Tagging for learning: collecting thematic relations from corpus.</title>
<date>1990</date>
<booktitle>In Proc. of the COLING&apos;90. Vol.1,</booktitle>
<pages>34--39</pages>
<contexts>
<context position="1533" citStr="Zernik and Jacobs, 1990" startWordPosition="224" endWordPosition="227">idence — the prior knowledge of the domain the texts are about, and grammatical constructions in which unknown lexical items occur. While there may be many reasonable interpretations when an unknown item occurs for the very first time in a text, their number rapidly decreases when more and more evidence is gathered. Our model tries to make explicit the reasoning processes behind this learning pattern. Unlike the current mainstream in automatic linguistic knowledge acquisition, which can be characterized as quantitative, surface-oriented bulk processing of large corpora of texts (Hindle, 1989; Zernik and Jacobs, 1990; Hearst, 1992; Manning, 1993), we propose here a knowledge-intensive model of concept learning from few, positive-only examples that is tightly integrated with the non-learning mode of text understanding. Both learning and understanding build on a given core ontology in the format of terminological assertions and, hence, make abundant use of terminological reasoning. The &apos;plain&apos; text understanding mode can be considered as the instantiation and continuous filling Hypothesis space-p Hypothesis spave-ti Figure 1 Architecture of the Text Learner of roles with respect to single concepts already a</context>
<context position="25214" citStr="Zernik and Jacobs (1990)" startWordPosition="4003" endWordPosition="4006">ich is sensitive to taxonomic hierarchies. The results we got were consistently favorable, as our system outperformed those closest in spirit, CAMILLE (Hastings, 1996) and SCISOR (Rau et 481 al., 1989), by a gain in accuracy on the order of 8%. Also, the system requires relatively few hypothesis spaces (2 to 6 on average) and prunes the concept search space radically, requiring only a few examples (for evaluation details, cf. Hahn and Schnattinger (1998)). 3 Related Work We are not concerned with lexical acquisition from very large corpora using surface-level collocational data as proposed by Zernik and Jacobs (1990) and Velardi et al. (1991), or with hyponym extraction based on entirely syntactic criteria as in Hearst (1992) or lexico-semantic associations (e.g., Resnik (1992) or Sekine et al. (1994)). This is mainly due to the fact that these studies aim at a shallower level of learning (e.g., selectional restrictions or thematic relations of verbs), while our focus is on much more fine-grained conceptual knowledge (roles, role filler constraints, integrity conditions). Our approach bears a close relationship, however, to the work of Mooney (1987), Berwick (1989), Rau et al. (1989), Gomez and Segami (19</context>
</contexts>
<marker>Zernik, Jacobs, 1990</marker>
<rawString>U. Zernik and P. Jacobs. 1990. Tagging for learning: collecting thematic relations from corpus. In Proc. of the COLING&apos;90. Vol.1, pages 34-39.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>