<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007052">
<title confidence="0.976519">
UTH: Semantic Relation Classification using Physical Sizes
</title>
<author confidence="0.974225">
Eiji ARAMAKI Takeshi IMAI Kengo MIYO Kazuhiko OHE
</author>
<affiliation confidence="0.989961">
The University of Tokyo Hospital department
</affiliation>
<address confidence="0.941924">
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
</address>
<email confidence="0.999696">
aramaki@hcc.h.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997144">
Although researchers have shown increas-
ing interest in extracting/classifying seman-
tic relations, most previous studies have ba-
sically relied on lexical patterns between
terms. This paper proposes a novel way to
accomplish the task: a system that captures
a physical size of an entity. Experimental
results revealed that our proposed method is
feasible and prevents the problems inherent
in other methods.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99752148">
Classification of semantic relations is important to
NLP as it would benefit many NLP applications,
such as machine translation and information re-
trieval.
Researchers have already proposed various
schemes. For example, Hearst (1992) manually de-
signed lexico-syntactic patterns for extracting is-a
relations. Berland and Charniak (1999) proposed a
similar method for part-whole relations. Brin (1998)
employed a bootstrapping algorithm for more spe-
cific relations (author-book relations). Kim and
Baldwin (2006) and Moldovan et al.(2004) focused
on nominal relations in compound nouns. Turney
(2005) measured relation similarity between two
words. While these methods differ, they all utilize
lexical patterns between two entities.
Within this context, our goal was to utilize infor-
mation specific to an entity. Although entities con-
tain many types of information, we focused on the
physical size of an entity. Here, physical size refers
to the typical width/height of an entity. For example,
we consider book to have a physical size of 20×25
cm, and book to have a size of 10×10 m, etc.
We chose to use physical size for the following
reasons:
</bodyText>
<listItem confidence="0.995539666666667">
1. Most entities (except abstract entities) have a
physical size.
2. Several semantic relations are sensitive to phys-
</listItem>
<bodyText confidence="0.828478928571429">
ical size. For example, a content-container rela-
tion (e1 content-container e2) naturally means
that e1 has a smaller size than e2. A book is
also smaller than its container, library. A part-
whole relation has a similar constraint.
Our next problem was how to determine physi-
cal sizes. First, we used Google to conduct Web
searches using queries such as “book (*cm x*cm)”
and “library (*m x*m)”. Next, we extracted numeric
expressions from the search results and used the av-
erage value as the physical size.
Experimental results revealed that our proposed
approach is feasible and prevents the problems in-
herent in other methods.
</bodyText>
<sectionHeader confidence="0.990434" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.99452925">
We used a corpus provided by SemEval2007 Task
#4 training set. This corpus consisted of 980 anno-
tated sentences (140 sentencesx7 relations). Table
1 presents an example.
Although the corpus contained a large quantity of
information such as WordNet sense keys, comments,
etc., we used only the most pertinent information:
entity1 (e1), entity2 (e2), and its relation (true/false)
</bodyText>
<page confidence="0.993347">
464
</page>
<bodyText confidence="0.6898715">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 464–467,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<equation confidence="0.844218833333333">
The &lt;e1&gt;library&lt;/e1&gt; contained &lt;e2&gt;books
&lt;/e2&gt; of guidance on the processes.
WordNet(e1) = &amp;quot;library\%1:14:00::&amp;quot;,
WordNet(e2) = &amp;quot;book\%1:10:00::&amp;quot;,
Content-Container(e2, e1) = &amp;quot;true&amp;quot;,
Query = &amp;quot;the * contained books&amp;quot;
</equation>
<bodyText confidence="0.99996825">
We gathered basic patterns for each relation, and
identified if each pattern had been obtained as a
SVM feature or not (1 or 0). We refer to these fea-
tures as basic pattern features.
</bodyText>
<subsectionHeader confidence="0.592454">
3.2 Selected Pattern Features
</subsectionHeader>
<tableCaption confidence="0.965336">
Table 1: An Example of Task#4 Corpus.
</tableCaption>
<listItem confidence="0.8392265">
1. For example, we extracted a triple example (li-
brary, book, true from Table 1.
</listItem>
<sectionHeader confidence="0.974618" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999987833333333">
We applied support vector machine (SVM)-based
learning (Vapnik, 1999) using three types of fea-
tures: (1) basic pattern features (Section 3.1), (2) se-
lected pattern features (Section 3.2), and (3) physical
size features (Section 3.3). Figure 1 presents some
examples of these features.
</bodyText>
<subsectionHeader confidence="0.999281">
3.1 Basic Pattern Features
</subsectionHeader>
<bodyText confidence="0.999981083333333">
First, the system finds lexical patterns that co-occur
with semantic relations between two entities (e1 and
e2). It does so by conducting searches using two
queries “e1 * e2” and “e2 * e1”. For example, two
queries, “library * book” and “book * library”, are
generated from Table 1.
Then, the system extracts the word (or word se-
quences) between two entities from the snippets in
the top 1,000 search results. We considered the ex-
tracted word sequences to be basic patterns. For ex-
ample, given “...library contains the book...”, the ba-
sic pattern is “(e1) contains the (e2)” 2.
</bodyText>
<footnote confidence="0.991676">
1Our system is classified as an A4 system, and therefore
does not use WordNet or Query.
2This operation does not handle any stop-words. Therefore,
</footnote>
<bodyText confidence="0.99298605">
Because basic pattern features are generated only
from snippets, precise co-occurrence statistics are
not available. Therefore, the system searches again
with more specific queries, such as “library contains
the book”. However, this second search is a heavy
burden for a search engine, requiring huge numbers
of queries (# of samples x # of basic patterns).
We thus selected the most informative n patterns
(STEP1) and conducted specific searches (# of sam-
ples x n basic patterns)(STEP2) as follows:
STEP1: To select the most informative patterns,
we applied a decision tree (C4.5)(Quinlan,
1987) and selected the basic patterns located in
the top n branches 3.
STEP2: Then, the system searched again us-
ing the selected patterns. We considered log
weighted hits (loglo |hits|) to be selected pat-
tern features. For example, if “library contains
the book” produced 120,000 hits in Google, it
yields the value log10(12, 000) = 5.
</bodyText>
<subsectionHeader confidence="0.997909">
3.3 Physical Size Features
</subsectionHeader>
<bodyText confidence="0.9940864">
As noted in Section 1, we theorized that an entity’s
size could be a strong clue for some semantic rela-
tions.
We estimated entity size using the following
queries:
</bodyText>
<listItem confidence="0.99029975">
1. “&lt; entity &gt; (* cm x * cm)”,
2. “&lt; entity &gt; (* x * cm)”,
3. “&lt; entity &gt; (* m x * m)”,
4. “&lt; entity &gt; (* x * m)”.
</listItem>
<bodyText confidence="0.9964204">
In these queries, &lt; entity &gt; indicates a slot for
each entity, such as “book”, “library”, etc. Then, the
system examines the search results for the numerous
expressions located in “*” and considers the average
value to be the size.
</bodyText>
<footnote confidence="0.943994666666667">
“(e1) contains THE (e2)” and “(e1) contains (e2)” are different
patterns.
3In the experiments in Section 4, we set n = 10.
</footnote>
<figureCaption confidence="0.998347">
Figure 1: Three types of Features.
</figureCaption>
<page confidence="0.976292">
465
</page>
<table confidence="0.9999642">
Precision Recall Fβ=1
PROPOSED 0.57 (=284/497) 0.60 (=284/471) 0.58
+SEL 0.56 (=281/496) 0.59 (=281/471) 0.57
+SIZE 0.53 (=269/507) 0.57 (=269/471) 0.54
BASELINE 0.53 (=259/487) 0.54 (=259/471) 0.53
</table>
<tableCaption confidence="0.998067">
Table 2: Results.
</tableCaption>
<bodyText confidence="0.8370472">
When results of size expressions were insufficient
(numbers &lt; 10), we considered the entity to be non-
physical, i.e., to have no size.
By applying the obtained sizes, the system gener-
ates a size feature, consisting of six flags:
</bodyText>
<listItem confidence="0.999721">
1. LARGE-e1: (e1’s X &gt; e2’s X) and (e1’s Y &gt; e2’s Y)
2. LARGE-e2: (e1’s X &lt; e2’s X) and (e1’s Y &lt; e2’s Y)
3. NOSIZE-e1: only e1 has no size.
4. NOSIZE-e2: only e2 has no size.
5. NOSIZE-BOTH: Both e1 and e2 have no size.
6. OTHER: Other.
</listItem>
<sectionHeader confidence="0.999489" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.988902">
4.1 Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.998813">
To evaluate the performance of our system, we
used a SemEval-Task No#4 training set. We com-
pared the following methods using a ten-fold cross-
validation test:
</bodyText>
<listItem confidence="0.9974586">
1. BASELINE: with only basic pattern features.
2. +SIZE: BASELINE with size features.
3. +SEL: BASELINE with selected pattern features.
4. PROPOSED: BASELINE with both size and selected
pattern features.
</listItem>
<bodyText confidence="0.798409">
For SVM learning, we used TinySVM with a lin-
ear kernel4.
</bodyText>
<sectionHeader confidence="0.600452" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.901024285714286">
Table 2 presents the results. PROPOSED was the
most accurate, demonstrating the basic feasibility of
our approach.
Table 3 presents more detailed results. +SIZE
made a contribution to some relations (REL2 and
REL4). Particularly for REL4, +SIZE significantly
boosted accuracy (using McNemar tests (Gillick and
</bodyText>
<footnote confidence="0.996066">
4http://chasen.org/ taku/software/TinySVM/
</footnote>
<figureCaption confidence="0.999902">
Figure 2: The Size of a “Car”.
</figureCaption>
<bodyText confidence="0.9965871">
Cox, 1989); p = 0.05). However, contrary to our ex-
pectations, size features were disappointing for part-
whole relations (REL6) and content-container rela-
tions (REL7).
The reason for this was mainly the difficulty in es-
timating size. Table 4 lists the sizes of several enti-
ties, revealing some strange results, such as a library
sized 12.1 x 8.4 cm, a house sized 53 x 38 cm, and
a car sized 39 x 25 cm. These sizes are unusually
small for the following reasons:
</bodyText>
<listItem confidence="0.991491">
1. Some entities (e.g.“car”) rarely appear with
their size,
2. In contrast, entities such as “toy car” or “mini
car” frequently appear with a size.
</listItem>
<bodyText confidence="0.946331888888889">
Figure 2 presents the size distribution of “car.”
Few instances appeared of real cars sized approxi-
mately 500 x 400 cm, while very small cars smaller
than 100 x 100 cm appeared frequently. Our current
method of calculating average size is ineffective un-
der this type of situation.
In the future, using physical size as a clue for de-
termining a semantic relation will require resolving
this problem.
</bodyText>
<sectionHeader confidence="0.996908" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999069666666667">
We briefly presented a method for obtaining the size
of an entity and proposed a method for classifying
semantic relations using entity size. Experimental
results revealed that the proposed approach yielded
slightly higher performance than a baseline, demon-
strating its feasibility. If we are able to estimate en-
</bodyText>
<page confidence="0.998723">
466
</page>
<table confidence="0.999980681818182">
Relation PROPOSED +SEL +SIZE BASELINE
Precision 0.60 (=50/83) 0.56 (=53/93) 0.54 (=53/98) 0.50 (=53/106)
REL1 Recall 0.68 (=50/73) 0.72 (=53/73) 0.72 (=53/73) 0.72 (=53/73)
(Cause-Effect) Fp=1 0.64 0.63 0.59 0.61
Precision 0.59 (=43/72) 0.60 (=44/73) 0.56 (=45/79) 0.55 (=44/79)
REL2 Recall 0.60 (=43/71) 0.61 (=44/71) 0.63 (=45/71) 0.61 (=44/71)
(Instrument-Agency) Fp=1 0.60 0.61 0.59 0.58
Precision 0.70 (=56/80) 0.73 (=55/75) 0.65 (=54/82) 0.68 (=51/74)
REL3 Recall 0.65 (=56/85) 0.64 (=55/85) 0.63 (=54/85) 0.60 (=51/85)
(Product-Producer) Fp=1 0.67 0.68 0.64 0.64
Precision 0.41 (=23/56) 0.35 (=18/51) 0.48 (=24/49) 0.52 (=13/25)
REL4 Recall 0.42 (=23/54) 0.33 (=18/54) 0.44 (=24/54) 0.24 (=13/54)
(Origin-Entity) Fp=1 0.41 0.34 0.46 0.32
Precision 0.62 (=40/64) 0.61 (=40/65) 0.56 (=28/50) 0.56 (=29/51)
REL5 Recall 0.68 (=40/58) 0.68 (=40/58) 0.48 (=28/58) 0.50 (=29/58)
(Theme-Tool) Fp=1 0.65 0.65 0.51 0.53
Precision 0.45 (=46/101) 0.46 (=46/100) 0.41 (=49/118) 0.43 (=53/123)
REL6 Recall 0.70 (=46/65) 0.70 (=46/65) 0.75 (=49/65) 0.81 (=53/65)
(Part-Whole) Fp=1 0.55 0.55 0.53 0.56
Precision 0.63 (26/41) 0.64 (=25/39) 0.51 (=16/31) 0.55 (=16/29)
REL7 Recall 0.40 (26/65) 0.38 (=25/65) 0.24 (=16/65) 0.24 (=16/65)
(Content-Container) Fp=1 0.49 0.48 0.33 0.34
</table>
<tableCaption confidence="0.986417">
Table 3: Detailed Results.
</tableCaption>
<table confidence="0.9818054">
entity # size
library 51 12.1x8.4 m
room 204 5.4x3.5 m
man 75 1.5x0.5 m
benches 33 93x42 cm
granite 68 76x48 cm
sink 34 57x25 cm
house 86 53x38 cm
books 50 46x24 cm
car 91 39x25 cm
turtles 15 38x23 cm
food 38 35x26 cm
oats 16 24x13 cm
tumor shrinkage 6 -
habitat degradation 5 -
</table>
<tableCaption confidence="0.999181">
Table 4: Some Examples of Entity Sizes.
</tableCaption>
<bodyText confidence="0.9931555">
“#” indicates the number of obtained size expressions.
“-” indicates a “NO-SIZE” entity.
tity sizes more precisely in the future, the system
will become much more accurate.
</bodyText>
<sectionHeader confidence="0.999642" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987390625">
Matthew Berland and Eugene Charniak. 1999. Finding parts
in very large corpora. In Proceedings of the Annual Con-
ference of the Association for Computational Linguistics
(ACL1999), pages 57–64.
Sergey Brin. 1998. Extracting patterns and relations from the
world wide web. In WebDB Workshop at 6th International
Conference on Extending Database Technology, EDBT’98,
pages 172–183.
L. Gillick and SJ Cox. 1989. Some statistical issues in the com-
parison of speech recognition algorithms. In Proceedings of
IEEE International Conference on Acoustics, Speech, and
Signal Processing, pages 532–535.
M. Hearst. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proceedings of International Confer-
ence on Computational Linguistics (COLING1992), pages
539–545.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting seman-
tic relations in noun compounds via verb semantics. In Pro-
ceedings of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 491–498.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.
2004. Models for the semantic classification of noun
phrases. Proceedings of HLT/NAACL-2004 Workshop on
Computational Lexical Semantics.
J.R. Quinlan. 1987. Simplifying decision trees. International
Journal ofMan-Machine Studies, 27(1):221–234.
Peter D. Turney. 2005. Measuring semantic similarity by latent
relational analysis. In Proceedings of the Nineteenth Inter-
national Joint Conference on Artificial Intelligence (IJCAI-
05), pages 1136–1141.
Vladimir Vapnik. 1999. The Nature of Statistical Learning
Theory. Springer-Verlag.
</reference>
<page confidence="0.999298">
467
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925373">
<title confidence="0.999772">UTH: Semantic Relation Classification using Physical Sizes</title>
<author confidence="0.992356">Eiji ARAMAKI Takeshi IMAI Kengo MIYO Kazuhiko OHE</author>
<affiliation confidence="0.997538">The University of Tokyo Hospital department</affiliation>
<address confidence="0.973848">7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan</address>
<email confidence="0.973233">aramaki@hcc.h.u-tokyo.ac.jp</email>
<abstract confidence="0.997934909090909">Although researchers have shown increasing interest in extracting/classifying semantic relations, most previous studies have basically relied on lexical patterns between terms. This paper proposes a novel way to accomplish the task: a system that captures a physical size of an entity. Experimental results revealed that our proposed method is feasible and prevents the problems inherent in other methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL1999),</booktitle>
<pages>57--64</pages>
<contexts>
<context position="992" citStr="Berland and Charniak (1999)" startWordPosition="134" endWordPosition="137">cally relied on lexical patterns between terms. This paper proposes a novel way to accomplish the task: a system that captures a physical size of an entity. Experimental results revealed that our proposed method is feasible and prevents the problems inherent in other methods. 1 Introduction Classification of semantic relations is important to NLP as it would benefit many NLP applications, such as machine translation and information retrieval. Researchers have already proposed various schemes. For example, Hearst (1992) manually designed lexico-syntactic patterns for extracting is-a relations. Berland and Charniak (1999) proposed a similar method for part-whole relations. Brin (1998) employed a bootstrapping algorithm for more specific relations (author-book relations). Kim and Baldwin (2006) and Moldovan et al.(2004) focused on nominal relations in compound nouns. Turney (2005) measured relation similarity between two words. While these methods differ, they all utilize lexical patterns between two entities. Within this context, our goal was to utilize information specific to an entity. Although entities contain many types of information, we focused on the physical size of an entity. Here, physical size refer</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL1999), pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1998</date>
<booktitle>In WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT’98,</booktitle>
<pages>172--183</pages>
<contexts>
<context position="1056" citStr="Brin (1998)" startWordPosition="145" endWordPosition="146">to accomplish the task: a system that captures a physical size of an entity. Experimental results revealed that our proposed method is feasible and prevents the problems inherent in other methods. 1 Introduction Classification of semantic relations is important to NLP as it would benefit many NLP applications, such as machine translation and information retrieval. Researchers have already proposed various schemes. For example, Hearst (1992) manually designed lexico-syntactic patterns for extracting is-a relations. Berland and Charniak (1999) proposed a similar method for part-whole relations. Brin (1998) employed a bootstrapping algorithm for more specific relations (author-book relations). Kim and Baldwin (2006) and Moldovan et al.(2004) focused on nominal relations in compound nouns. Turney (2005) measured relation similarity between two words. While these methods differ, they all utilize lexical patterns between two entities. Within this context, our goal was to utilize information specific to an entity. Although entities contain many types of information, we focused on the physical size of an entity. Here, physical size refers to the typical width/height of an entity. For example, we cons</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergey Brin. 1998. Extracting patterns and relations from the world wide web. In WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT’98, pages 172–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gillick</author>
<author>SJ Cox</author>
</authors>
<title>Some statistical issues in the comparison of speech recognition algorithms.</title>
<date>1989</date>
<booktitle>In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>532--535</pages>
<marker>Gillick, Cox, 1989</marker>
<rawString>L. Gillick and SJ Cox. 1989. Some statistical issues in the comparison of speech recognition algorithms. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 532–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics (COLING1992),</booktitle>
<pages>539--545</pages>
<contexts>
<context position="889" citStr="Hearst (1992)" startWordPosition="123" endWordPosition="124">ng interest in extracting/classifying semantic relations, most previous studies have basically relied on lexical patterns between terms. This paper proposes a novel way to accomplish the task: a system that captures a physical size of an entity. Experimental results revealed that our proposed method is feasible and prevents the problems inherent in other methods. 1 Introduction Classification of semantic relations is important to NLP as it would benefit many NLP applications, such as machine translation and information retrieval. Researchers have already proposed various schemes. For example, Hearst (1992) manually designed lexico-syntactic patterns for extracting is-a relations. Berland and Charniak (1999) proposed a similar method for part-whole relations. Brin (1998) employed a bootstrapping algorithm for more specific relations (author-book relations). Kim and Baldwin (2006) and Moldovan et al.(2004) focused on nominal relations in compound nouns. Turney (2005) measured relation similarity between two words. While these methods differ, they all utilize lexical patterns between two entities. Within this context, our goal was to utilize information specific to an entity. Although entities con</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of International Conference on Computational Linguistics (COLING1992), pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>Interpreting semantic relations in noun compounds via verb semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>491--498</pages>
<contexts>
<context position="1167" citStr="Kim and Baldwin (2006)" startWordPosition="158" endWordPosition="161">vealed that our proposed method is feasible and prevents the problems inherent in other methods. 1 Introduction Classification of semantic relations is important to NLP as it would benefit many NLP applications, such as machine translation and information retrieval. Researchers have already proposed various schemes. For example, Hearst (1992) manually designed lexico-syntactic patterns for extracting is-a relations. Berland and Charniak (1999) proposed a similar method for part-whole relations. Brin (1998) employed a bootstrapping algorithm for more specific relations (author-book relations). Kim and Baldwin (2006) and Moldovan et al.(2004) focused on nominal relations in compound nouns. Turney (2005) measured relation similarity between two words. While these methods differ, they all utilize lexical patterns between two entities. Within this context, our goal was to utilize information specific to an entity. Although entities contain many types of information, we focused on the physical size of an entity. Here, physical size refers to the typical width/height of an entity. For example, we consider book to have a physical size of 20×25 cm, and book to have a size of 10×10 m, etc. We chose to use physica</context>
</contexts>
<marker>Kim, Baldwin, 2006</marker>
<rawString>Su Nam Kim and Timothy Baldwin. 2006. Interpreting semantic relations in noun compounds via verb semantics. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 491–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>A Badulescu</author>
<author>M Tatu</author>
<author>D Antohe</author>
<author>R Girju</author>
</authors>
<title>Models for the semantic classification of noun phrases.</title>
<date>2004</date>
<booktitle>Proceedings of HLT/NAACL-2004 Workshop on Computational Lexical Semantics.</booktitle>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju. 2004. Models for the semantic classification of noun phrases. Proceedings of HLT/NAACL-2004 Workshop on Computational Lexical Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Simplifying decision trees.</title>
<date>1987</date>
<journal>International Journal ofMan-Machine Studies,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="5295" citStr="Quinlan, 1987" startWordPosition="820" endWordPosition="821">words. Therefore, Because basic pattern features are generated only from snippets, precise co-occurrence statistics are not available. Therefore, the system searches again with more specific queries, such as “library contains the book”. However, this second search is a heavy burden for a search engine, requiring huge numbers of queries (# of samples x # of basic patterns). We thus selected the most informative n patterns (STEP1) and conducted specific searches (# of samples x n basic patterns)(STEP2) as follows: STEP1: To select the most informative patterns, we applied a decision tree (C4.5)(Quinlan, 1987) and selected the basic patterns located in the top n branches 3. STEP2: Then, the system searched again using the selected patterns. We considered log weighted hits (loglo |hits|) to be selected pattern features. For example, if “library contains the book” produced 120,000 hits in Google, it yields the value log10(12, 000) = 5. 3.3 Physical Size Features As noted in Section 1, we theorized that an entity’s size could be a strong clue for some semantic relations. We estimated entity size using the following queries: 1. “&lt; entity &gt; (* cm x * cm)”, 2. “&lt; entity &gt; (* x * cm)”, 3. “&lt; entity &gt; (* m</context>
</contexts>
<marker>Quinlan, 1987</marker>
<rawString>J.R. Quinlan. 1987. Simplifying decision trees. International Journal ofMan-Machine Studies, 27(1):221–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Measuring semantic similarity by latent relational analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI05),</booktitle>
<pages>1136--1141</pages>
<contexts>
<context position="1255" citStr="Turney (2005)" startWordPosition="173" endWordPosition="174">1 Introduction Classification of semantic relations is important to NLP as it would benefit many NLP applications, such as machine translation and information retrieval. Researchers have already proposed various schemes. For example, Hearst (1992) manually designed lexico-syntactic patterns for extracting is-a relations. Berland and Charniak (1999) proposed a similar method for part-whole relations. Brin (1998) employed a bootstrapping algorithm for more specific relations (author-book relations). Kim and Baldwin (2006) and Moldovan et al.(2004) focused on nominal relations in compound nouns. Turney (2005) measured relation similarity between two words. While these methods differ, they all utilize lexical patterns between two entities. Within this context, our goal was to utilize information specific to an entity. Although entities contain many types of information, we focused on the physical size of an entity. Here, physical size refers to the typical width/height of an entity. For example, we consider book to have a physical size of 20×25 cm, and book to have a size of 10×10 m, etc. We chose to use physical size for the following reasons: 1. Most entities (except abstract entities) have a phy</context>
</contexts>
<marker>Turney, 2005</marker>
<rawString>Peter D. Turney. 2005. Measuring semantic similarity by latent relational analysis. In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI05), pages 1136–1141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1999</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="3731" citStr="Vapnik, 1999" startWordPosition="566" endWordPosition="567">rary&lt;/e1&gt; contained &lt;e2&gt;books &lt;/e2&gt; of guidance on the processes. WordNet(e1) = &amp;quot;library\%1:14:00::&amp;quot;, WordNet(e2) = &amp;quot;book\%1:10:00::&amp;quot;, Content-Container(e2, e1) = &amp;quot;true&amp;quot;, Query = &amp;quot;the * contained books&amp;quot; We gathered basic patterns for each relation, and identified if each pattern had been obtained as a SVM feature or not (1 or 0). We refer to these features as basic pattern features. 3.2 Selected Pattern Features Table 1: An Example of Task#4 Corpus. 1. For example, we extracted a triple example (library, book, true from Table 1. 3 Method We applied support vector machine (SVM)-based learning (Vapnik, 1999) using three types of features: (1) basic pattern features (Section 3.1), (2) selected pattern features (Section 3.2), and (3) physical size features (Section 3.3). Figure 1 presents some examples of these features. 3.1 Basic Pattern Features First, the system finds lexical patterns that co-occur with semantic relations between two entities (e1 and e2). It does so by conducting searches using two queries “e1 * e2” and “e2 * e1”. For example, two queries, “library * book” and “book * library”, are generated from Table 1. Then, the system extracts the word (or word sequences) between two entitie</context>
</contexts>
<marker>Vapnik, 1999</marker>
<rawString>Vladimir Vapnik. 1999. The Nature of Statistical Learning Theory. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>