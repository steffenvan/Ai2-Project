<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.965777">
Context Inducing Nouns
</title>
<author confidence="0.677726">
Charlotte Price Valeria de Paiva Tracy Holloway King
</author>
<affiliation confidence="0.727755">
Palo Alto Research Center Palo Alto Research Center Palo Alto Research Center
</affiliation>
<address confidence="0.6674155">
3333 Coyote Hill Rd. 3333 Coyote Hill Rd. 3333 Coyote Hill Rd.
Palo Alto, CA 94304 USA Palo Alto, CA 94304 USA Palo Alto, CA 94304 USA
</address>
<email confidence="0.998563">
lprice@parc.com valeria.paiva@gmail.com thking@parc.com
</email>
<sectionHeader confidence="0.995635" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999830285714286">
It is important to identify complement-
taking nouns in order to properly analyze
the grammatical and implicative structure
of the sentence. This paper examines the
ways in which these nouns were identified
and classified for addition to the BRIDGE
natural language understanding system.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971428571429">
One of the goals of computational linguistics is to
draw inferences from a text: that is, for the sys-
tem to be able to process a text, and then to con-
clude, based on the text, whether some other state-
ment is true.1 Clausal complements confound the
process because, despite their surface similarity to
adjuncts, they generate very different inferences.
In this paper we examine complement-taking
nouns: how to identify them and how to incorpo-
rate them into an inferencing system. We first dis-
cuss what we mean by complement-taking nouns
(section 2) and how to identify a list of such
nouns (section 3). We then describe the question-
answering system that uses the complement-taking
nouns as part of its inferencing (section 4), how the
nouns are added to the system (section 5), and how
the coverage is tested (section 6). Finally, we dis-
cuss several avenues for future work (section 7),
including automating the search process, identify-
ing other context-inducing forms, and taking ad-
vantage of cross-linguistic data.
</bodyText>
<footnote confidence="0.969993">
c 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1We would like to thank the Natural Language Theory and
Technology group at PARC, Dick Crouch, and the three re-
viewers for their input.
</footnote>
<sectionHeader confidence="0.548156" genericHeader="introduction">
2 What is a complement-taking noun?
</sectionHeader>
<bodyText confidence="0.999731714285714">
Identifying complement-taking nouns is somewhat
involved. It is important to identify the clause, to
ensure that the clause is indeed a complement and
not an adjunct (e.g. a relative clause or a purpose
infinitive), and to figure out what is licensing the
complement, as it is not only nouns that license
complements.
</bodyText>
<subsectionHeader confidence="0.932488">
2.1 Verbal vs. nominal complements
</subsectionHeader>
<bodyText confidence="0.999732714285714">
A clause is a portion of a sentence that includes a
predicate and its arguments. Clauses come in a va-
riety of forms, a subset of which is shown in (1)
for verbs taking complements. The italicized part
is the complement, and the part in bold is what li-
censes it. The surface form of the clause can vary
significantly depending on the licensing verb.
</bodyText>
<listItem confidence="0.99398675">
(1) a. Mary knows that Bob is happy.
b. John wants (Mary) to leave right now.
c. John likes fixing his bike.
d. John let Mary fix his bike.
</listItem>
<bodyText confidence="0.9841379">
For this paper, we touch briefly on nouns taking
to clauses, as in (2b), but the main focus is on that
clauses, as in (2a).
(2) a. the fact that Mary hopped
b. the courage to hop
Both types of complements pose problems in
mining corpora for lexicon development. The that
clauses can superficially resemble relative clauses,
as in (3), and the to clauses can resemble purpose
infinitives, as in (4).
</bodyText>
<listItem confidence="0.601258333333333">
(3) a. COMPLEMENT-TAKING NOUN: John
liked the idea that Mary sang last
evening.
</listItem>
<page confidence="0.882964">
9
</page>
<note confidence="0.7424675">
Coling 2008: Proceedings of the workshop on Knowledge and Reasoning forAnswering Questions, pages 9–16
Manchester, August 2008
</note>
<listItem confidence="0.9031842">
b. RELATIVE CLAUSE: John liked the song
that Mary sang last evening.
(4) a. COMPLEMENT-TAKING NOUN: John had
a chance to sing that song.
b. PURPOSE INFINITIVE: John had a song
</listItem>
<bodyText confidence="0.989607">
book (in order) to sing that song.
As discussed in section 3, this superficial re-
semblance makes the automatic identification of
complement-taking nouns very difficult: simple
string-based searches would return large numbers
of incorrect candidates which would have to be vet-
ted before incorporating the new nouns into the
system.
</bodyText>
<subsectionHeader confidence="0.964986">
2.2 Contexts introduced by nominals
</subsectionHeader>
<bodyText confidence="0.9859695">
Complements and relative clause adjuncts allow
very different inferences. Whereas the speaker’s
beliefs about adjuncts take on the truth value of
the clause they are embedded in, the truth value of
clausal complements is also affected by the licens-
ing noun. Compare the sentences below. The itali-
cized clause in (5) is a complement, while in (6) it
is an adjunct.
</bodyText>
<listItem confidence="0.704865666666667">
(5) The lie that Mary was ill paralyzed Bob.
Mary was not ill.
(6) The situation that she had gotten herself into
</listItem>
<bodyText confidence="0.96940352631579">
paralyzed Bob. She had gotten herself
into a situation.
To explain how this is possible, we introduce the
notion of implicative contexts (Nairn et al., 2006),
and claim that complement-taking nouns introduce
a context for the complement, whereas no such
context is created for the adjuncts. Perhaps the eas-
iest way to think of a context is to imagine em-
bedding the complement in an extra layer, with the
layer adding information about how to adjust the
truth-value of its contents.2 This allows us to con-
clude in (5) that the speaker believes that Mary and
Bob exist, as does the event of Bob’s paralysis, but
the event Mary was ill does not. These are ref-
ered to as the (un)instantiability of the components
in the sentence. Contexts can be embedded within
each other recursively, as in (7). Note that these se-
mantic contexts often, but not always, correspond
to syntactic embedding.
</bodyText>
<footnote confidence="0.952785666666667">
2In the semantic representations, the contexts are flattened,
or projected, onto the leaf nodes of the parse tree, so that every
leaf has access to information locally.
</footnote>
<listItem confidence="0.825163">
(7) Paul believes [that John’s lie [that Mary wor-
ries [that fish can fly]] surprised us].
</listItem>
<bodyText confidence="0.997488642857143">
Contexts may have an implication signature
(Nairn et al., 2006) attached to them, specifying,
for example, that the clause is something that the
speaker presupposes to be true or that the speaker
believes the truth value of the clause should be re-
versed. The default for a context is to allow no
implications to be drawn, as in (1b), where the
speaker has not committed to whether or not Mary
is leaving.
Below is a more detailed example showing how
the context introduced by a noun changes the im-
plications of the sentence, and how it would behave
differently from a relative clause adjunct to a noun.
Consider the pair of sentences in (8).
</bodyText>
<listItem confidence="0.841705">
(8) a. The lie that Mary had won surprised John.
Mary did not win.
</listItem>
<bodyText confidence="0.970268173913043">
b. The bonus that Mary had won surprised
John. Mary won a bonus.
In (8), that John was surprised is in the speaker’s
top context, which is what the author commits to as
truth. In (8a), lie is within the context of surprised.
Surprised does not change the implications of ele-
ments within its context.3 Therefore, lie gets a true
value: that a lie was told is considered true. That
Mary won, however, is within the context of lie,
which reverses the polarity of implications within
its scope or context. If that Mary won were only
within the context of surprised instead of within
lie, which would be the case if lie did not create
a context, then that Mary won would fall within
the context of surprised. The implication signa-
ture of surprised would determine the veridicality
of the embedded clause instead of the signature of
lie: this would incorrectly allow the conclusion that
Mary won.
The content of the relative clause in (8b) is in the
same context as surprise since no additional con-
text is introduced by bonus. As such, we can con-
clude that Mary did win a bonus.
</bodyText>
<subsectionHeader confidence="0.994844">
2.3 Complements introduced by to
</subsectionHeader>
<bodyText confidence="0.9999095">
The previous subsection focused on finite comple-
ments introduced by that. From the perspective
</bodyText>
<footnote confidence="0.9671882">
3We say surprise has the implication signature ++/--: el-
ements within its context have a positive implication in a pos-
itive context and negative in a negative context. See (Nairn et
al., 2006) for detailed discussion of possible implication sig-
natures and how to propagate them through contexts.
</footnote>
<page confidence="0.998877">
10
</page>
<bodyText confidence="0.9806112">
of aiding inferencing in the BRIDGE system, the
nouns that take to complements that are not dever-
bal nouns (see section 2.4 for discussion of dever-
bals) seem to fall into three main classes:4 ability,
bravery, and chance. Examples are shown in (9).
</bodyText>
<listItem confidence="0.990625333333333">
(9) a. John has the ability to sing.
b. John has the guts to sing out loud.
c. John’s chance to sing came quickly.
</listItem>
<bodyText confidence="0.90830075">
These all have an implication signature that
gives a (negative) implication only in a negative
context, as in (10); in a positive context as in (9),
no implication can be drawn.
</bodyText>
<listItem confidence="0.6923446">
(10) John didn’t have the opportunity to sing.
John didn’t sing.
Note also that the implication only applies when
the verb is have. Other light verbs, such as take in
(11) change the implications.
</listItem>
<bodyText confidence="0.9999625">
For this reason, these nouns are treated differ-
ently than those with that complements. They are
marked in the grammar as taking a complement in
the same way that that complements are (section 5),
but the mechanism which attaches an implication
signature takes the governing verb into account.
</bodyText>
<subsectionHeader confidence="0.994678">
2.4 Deverbal nouns
</subsectionHeader>
<bodyText confidence="0.999992166666667">
A large number of complement-taking nouns are
related to verbs that take complements. These
nouns are analyzed differently than non-deverbal
nouns. They are linked to their related verb and
classified according to how the arguments of the
noun and the sentence relate to the arguments for
the verb (e.g. -ee, -er).5 The BRIDGE system uses
this linking to map these nouns to their verbal coun-
terparts and to draw conclusions of implicativity as
if they were verbs, as explained in (Gurevich et al.,
2006). Consider (12) where the paraphrases using
fear as a verb or a noun are clearly related.
</bodyText>
<listItem confidence="0.953367333333333">
(12) a. The fear that Mary was ill paralyzed Bob.
b. Bob feared that Mary was ill; this fear par-
alyzed Bob.
</listItem>
<footnote confidence="0.98654625">
4The work described in this section was done by Lauri
Karttunen and Karl Pichotta (Pichotta, 2008).
5NOMLEX (Macleod et al., 1998) is an excellent source of
these deverbal nouns.
</footnote>
<bodyText confidence="0.9988542">
Deverbal nouns can take that complements or, as
in (13), to complements. Most often, the context
introduced by a deverbal noun does not add an im-
plication signature, as in (11), which results in the
answer UNKNOWN to the question Was Mary ill?.
</bodyText>
<listItem confidence="0.70206275">
(13) a. John’s promise to go swimming surprised
us.
b. John’s persuasion of Mary to sing at the
party surprised us.
</listItem>
<bodyText confidence="0.993284333333333">
Gerunds, being even more verb-like, are treated as
verbs in our system and hence inherit the implica-
tive properties from the corresponding verb.
</bodyText>
<listItem confidence="0.983091">
(14) Knowing that Mary had sung upset John.
Mary sang.
</listItem>
<bodyText confidence="0.967229666666667">
Gerunds and deverbal nouns are discussed in de-
tail in (Gurevich et al., 2006) and are outside of the
scope of this paper.
</bodyText>
<sectionHeader confidence="0.830773" genericHeader="method">
3 Finding complement-taking nouns
</sectionHeader>
<bodyText confidence="0.999909272727273">
In order for the system to draw the inferences dis-
cussed above, the complement-taking nouns must
first be identified and then classified and incorpo-
rated into the BRIDGE system (section 4). First,
the gerunds are removed since these are mapped by
the syntax into their verbal counterparts. Then the
non-gerund deverbal nouns (section 2.4) are linked
to their verbal counterpart so that they can be ana-
lyzed by the system as events. These two classes
represent a significant number of the nouns that
take that complements.
</bodyText>
<subsectionHeader confidence="0.999211">
3.1 Syntactic classification
</subsectionHeader>
<bodyText confidence="0.999898833333333">
However, there are many complement-taking
nouns that are not deverbal. To expand our
lexicon of these nouns, we started with a seed
set garnered from the Penn Treebank (Marcus et
al., 1994), which uses distinctive tree structures
for complement-taking nouns, and a small list
of linguistically prominent nouns. For each of
these lexical items, we extracted words in the
same semantic class from WordNet. Classes
include words like fact, which direct attention
to the clausal complement, as in (15), and nouns
expressing emotion, as in (16).
</bodyText>
<listItem confidence="0.9700658">
(15) It’s a fact that Mary came.
(16) Bob’s joy that Mary had returned reduced him
to tears.
(11) John took the opportunity to sing.
John sang.
</listItem>
<page confidence="0.996498">
11
</page>
<bodyText confidence="0.966853653846154">
These semantic classes provided a starting point
for discovering more of these nouns: the class of
emotion nouns, for example, has more than a hun-
dred hyponyms.
Identifying the class is not enough, as not all
members take clausal complements. Compare joy
in (16) and warmheartedness in (17) from the emo-
tion class. The sentence containing joy is much
more natural than that in (17).
(17) #Bob’s warmheartedness that Mary had re-
turned reduced him to tears.
From the candidate list, the deverbal nouns are
added to the lexicon of deverbal noun mappings.
The remaining list is checked word-by-word. To
ease the process, test sentences that take a range of
meanings are created for each class of nouns, as in
(18).
(18) Bob’s that Mary visited her mother re-
duced him to tears.
If the noun does not fit the test sentences, a
web search is done on “X that” to extract po-
tential complement-bearing sentences. These are
checked to eliminate sentences with adjuncts, or
where some other feature licenses the clause, such
as in (19) where the bold faced structure is licens-
ing the italicized clause.
</bodyText>
<listItem confidence="0.757415">
(19) a. John is so warmhearted that he took her in
without question.
b. They had such a good friendship that she
could tell him anything.
</listItem>
<bodyText confidence="0.971387777777778">
Using these methods, from a seed set of 13
nouns, 170 non-deverbal complement-taking
nouns were identified, most in the emotion and
feeling classes. The same techniques were then
applied to the state and information classes. Once
the Penn Treebank seeds were incorporated, the
same process was applied to the complement-
taking nouns from NOMLEX (Macleod et al.,
1998).
</bodyText>
<subsectionHeader confidence="0.998611">
3.2 Determining implications
</subsectionHeader>
<bodyText confidence="0.99990875">
As examples (8a) and (8b) showed, whether a word
takes a complement is lexically determined; so is
the type of implication signature introduced by the
word. Compare the implications in (20).
</bodyText>
<listItem confidence="0.752815333333333">
(20) a. The fact that Mary had returned surprised
John. Mary had returned.
b. The falsehood that Mary had returned sur-
prised John. Mary had not returned.
c. The possibility that Mary had returned
surprised John. ? Mary had returned.
</listItem>
<bodyText confidence="0.999955083333333">
These nouns have different implication signa-
tures: facts imply truth; lies imply falsehood; and
possibilities do not allow truth or falsehood to be
established. The default for complements is that no
implications can be drawn, as in (20c), which in the
BRIDGE system is expressed as the noun having no
implication signature.6
Once identified and its implication signature de-
termined, adding the complement-taking noun to
the BRIDGE system and deriving the correct infer-
ences is straightforward. This process is described
in section 5.
</bodyText>
<sectionHeader confidence="0.995578" genericHeader="method">
4 The BRIDGE system
</sectionHeader>
<bodyText confidence="0.999924620689655">
The BRIDGE system (Bobrow et al., 2007) includes
a syntactic grammar, a semantics rule set (Crouch
and King, 2006), an abstract knowledge represen-
tation (AKR) rule set, and an entailment and con-
tradiction detection (ECD) system. The syntax, se-
mantics, and AKR all depend on lexicons.
The BRIDGE grammar defines syntactic proper-
ties of words, such as predicate-argument structure,
tense, number, and nominal specifiers. The gram-
mar produces a packed representation of the sen-
tence which allows ambiguity to be dealt with effi-
ciently (Maxwell and Kaplan, 1991).
The parses are passed to the semantic rules
which also work on packed structures (Crouch,
2005). The semantic layer looks up words in a
Unified Lexicon (UL), connects surface arguments
of verbs to their roles, and determines the context
within which a word occurs in the sentence. Nega-
tion introduces a context, as do the complement-
taking nouns discussed here (Bobrow et al., 2005).
The UL combines several sources of information
(Crouch and King, 2005). Much of the information
comes from the syntactic lexicon, VerbNet (Kipper
et al., 2000), and WordNet (Fellbaum, 1998), but
there are also handcoded entries that add semanti-
cally relevant information such as its implication
signature. A sample UL entry is given in Figure 1.
The current number of complement-taking
nouns in the system is shown in (21). Only a
</bodyText>
<footnote confidence="0.991752">
6A context is still generated for these. Adjuncts, having no
context of their own, inherit the implication signature of the
clause containing them (section 2.2).
</footnote>
<page confidence="0.994065">
12
</page>
<bodyText confidence="0.761176833333333">
(cat(N), word(fact), subcat(NOUN-EXTRA),
concept(%1),
source(hand annotated data), source(xle),
xfr:concept for(%1,fact),
xfr:lex class(%1,impl pp nn),
xfr:wordnet classes(%1,[])).
</bodyText>
<figureCaption confidence="0.794071">
Figure 1: One entry for the word fact in the Uni-
fied Lexicon. NOUN-EXTRA states that this use of
</figureCaption>
<bodyText confidence="0.9786763">
fact fits in structures such as it is a fact that The
WordNet meaning is found by looking up the con-
cept forfact in the WordNet database. The implica-
tion signature of the word is impl pp nn or ++/--
as seen in (22). Lastly, the sources for this informa-
tion are noted.
fifth of the nouns have implication signatures.
However, all of the nouns introduce contexts; the
default implication for contexts is to allow neither
true nor false to be concluded, as in (20c).
</bodyText>
<figure confidence="0.8858218">
(21)
Complement-taking Nouns
that complements 411
to complements 173
with implication signatures 107
</figure>
<bodyText confidence="0.988209333333333">
The output of the semantics level is fed into
the AKR. At this level, contexts are used to deter-
mine (un)instantiability based on the relationship
between contexts.7 An entity’s (un)instantiability
encodes whether it exists in some context. In (8a),
for example, we can conclude that the speaker be-
lieves that Mary exists, but that the event Mary won
is uninstantiated: the speaker believes it did not
happen.
The final layer is the ECD, which uses the struc-
tures built by the AKR to reason about a given
passage-query pair to determine whether or not the
query is inferred by the passage, answering with
YES, NO, UNKNOWN, or AMBIGUOUS. For more
details, see (Bobrow et al., 2005).
</bodyText>
<sectionHeader confidence="0.5825855" genericHeader="method">
5 Adding complement-taking nouns to
the system
</sectionHeader>
<bodyText confidence="0.999523333333333">
Adding complement-taking nouns to the BRIDGE
system is straightforward. A syntactic entry is
added indicating that the noun takes a complement.
The syntactic classes are defined by templates, and
the relevant template is called in the lexical en-
try for that word. For example, the template call
</bodyText>
<footnote confidence="0.857978">
7See (Bobrow et al., 2007; Bobrow et al., 2005) for other
information contained in the AKR.
</footnote>
<bodyText confidence="0.995099533333333">
@(NOUN-EXTRA %stem) is added to the entry for
fact.
If there is an implication signature for the com-
plement, this is added to the noun’s entry in the
file for hand-annotated data used to build the UL.
The fifth line in Figure 1 is an example. The AKR
and ECD rules that calculate the context and im-
plications on verbs and deverbal nouns general-
ize to handle implications on complement-taking
nouns and so do not need to be altered as new
complement-taking nouns are found.
As described in section 3, deciding which nouns
take complements is currently hand curated, as it is
quite difficult to distinguish them entirely automat-
ically.
</bodyText>
<sectionHeader confidence="0.995478" genericHeader="method">
6 Testing
</sectionHeader>
<bodyText confidence="0.995572666666667">
To ensure that complement-taking nouns are work-
ing properly in the system, for each noun, a
passage-query-correct answer triplet such as:
</bodyText>
<listItem confidence="0.757163">
(22) PASSAGE: The fact that Mary had returned
surprised John.
QUERY: Had Mary returned?
ANSWER: YES
</listItem>
<bodyText confidence="0.999885038461538">
is added to a testsuite. The testsuites are run and
the results reported as part of the daily regres-
sion testing (Chatzichrisafis et al., 2007). Both
naturally occurring and hand-crafted examples are
used to ensure that the correct implications are
being drawn. Natural examples test interactions
between phenomena such as noun complementa-
tion and copular constructions, while hand-crafted
examples allow isolation of the phenomenon and
show that all cases are being tested (Cohen et al.,
2008), e.g., that the correct entailments emerge un-
der negation as well as in the positive case.
Our current testsuites contain about 180 hand-
crafted examples. The number of natural exam-
ples is harder to count as they occur somewhat
rarely in the mixed-phenomena testsuites. One
of our natural example files, which is based on
newswire extracts from the PASCAL Recognizing
Textual Entailment Challenge (Dagan et al., 2005),
shows an approximate breakdown of the uses of the
word that is as shown in (23). This sample, which
is somewhat biased towards verbal complements
since it contains many examples that can be para-
phrased as said that, nonetheless shows the relative
scarcity of noun complements in the wild and un-
derscores the importance of hand-crafted examples
</bodyText>
<page confidence="0.997953">
13
</page>
<bodyText confidence="0.9984215">
for testing purposes. It it is clear that these noun
complements were being analyzed incorrectly be-
fore; what is unclear is how much of an impact
the misanalysis would have caused. Perhaps some
other domain would demonstrate a significantly
higher presence of non-deverbal nouns that take
complements and would be more significantly im-
pacted by their misanalysis.
</bodyText>
<table confidence="0.935689714285714">
(23)
Uses of the word that in RTE 2007
verbal complements 68
adjuncts 50
deverbal complements 14
noun complements 3
other 8 19
</table>
<sectionHeader confidence="0.993846" genericHeader="method">
7 Future work
</sectionHeader>
<bodyText confidence="0.999979833333333">
The detection and incorporation of noun comple-
ments for use in the BRIDGE system can be ex-
panded in several directions, such as automat-
ing the search process, identifying and classifying
other parts of speech that take complements, and
exploring transferability to other languages.
</bodyText>
<subsectionHeader confidence="0.999729">
7.1 Automating the search
</subsectionHeader>
<bodyText confidence="0.992069102564103">
Testing whether a clause is an adjunct or a noun
complement or is licensed by something else is cur-
rently done by hand. Automating the testing would
allow many more nouns to be tested. However, this
is non-trivial. As (8a) and (8b) demonstrated, the
surface structure can appear very similar; it is only
when we try to figure out the implications of the ex-
amples that the differences emerge.
The Penn Treebank (Marcus et al., 1994) was
initially used to extract complement-taking nouns.
As more tree and dependency banks, as well as lex-
ical resources (Macleod et al., 1998), are available,
further lexical items can be extracted in this way.
However, such resources are costly to build and
so are only slowly added to the available NLP re-
sources.
Rather than trying to identify all potential noun
complement clauses, a simpler approach would be
to reduce the search space for the human judge. For
example, some adjuncts (perhaps three quarters of
them) could be eliminated from natural examples
by using a part-of-speech tagger to identify occur-
rences where a conjugated verb immediately fol-
8This includes demonstrative uses, uses licensed by other
parts of speech such as so, and clauses which are the subject
of a sentence or the object of a prepositional phrase.
lows the word that, as in (24). These commonly
identify adjuncts.
(24) The shark that bit the swimmer appears to
have left.
By eliminating these adjuncts and by removing
those sentences where it is known that the clause
is a complement of the verb based on the syntac-
tic classification of that verb (the syntactic lexicon
contains 2500 verbs with various clausal comple-
ments), as in (25), the search space could be signif-
icantly reduced.
(25) The judge announced that the defendant was
guilty.
</bodyText>
<subsectionHeader confidence="0.8401665">
7.2 Other parts of speech that introduce
contexts
</subsectionHeader>
<bodyText confidence="0.999613">
Verbs, adjectives, and adverbs can also license
complements and hence contexts with implication
signatures. Examples in (26) show different parts
of speech that introduce contexts.9
</bodyText>
<listItem confidence="0.996412428571429">
(26) a. Verb: John said that Paul had arrived.
b. Adjective: It is possible that someone ate
the last piece of cake.
c. Adjective: John was available to see
Mary.
d. Adverb: John falsely reported that Mary
saw Bill.
</listItem>
<bodyText confidence="0.999486928571429">
Many classes of verbs have already been iden-
tified and are incorporated into the system (Nairn
et al., 2006): verbs relating to speech (e.g., say,
report, etc.), implicative verbs such as manage
and fail (Karttunen, 2007), and factive verbs (e.g.
agree, realize, consider) (Vendler, 1967; Kiparsky
and Kiparsky, 1971), to name a few. Many adjec-
tives have also been added to the system, includ-
ing ones taking to and that complements.10 As with
the complement-taking nouns, a significant part of
the effort in incorporating the complement-taking
adjectives into the system was identifying which
adjectives license complements. The adverbs have
not been explored in as much depth.
</bodyText>
<footnote confidence="0.997839833333333">
9From a syntactic perspective, the adverb falsely does not
take a complement. However, it does introduce a context in
the semantics and hence requires a lexical entry similar to
those discussed for the complement-taking nouns.
10This work was largely done by Hannah Copperman dur-
ing her internship at PARC.
</footnote>
<page confidence="0.998032">
14
</page>
<subsectionHeader confidence="0.976431">
7.3 Other languages
</subsectionHeader>
<bodyText confidence="0.999735384615384">
The fact that it has been productive to search
for complement-taking nouns through synonyms
and WordNet classes suggests that other languages
could benefit from the work done in English. It
would be interesting to see to what extent the im-
plicative signatures from one language carry over
into another, and to what extent they differ. Strong
similarities could, for example, suggest some com-
mon mechanism at work in these nouns that we
have been unable to identify by studying only one
language. Searching in other languages could also
potentially turn up classes or candidates that were
missed in English.11
</bodyText>
<sectionHeader confidence="0.999417" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999978125">
It is important to identify complement-taking
nouns in order to properly analyze the grammati-
cal and implicative structure of the sentence. Here
we described a bootstrapping approach whereby
annotated corpora and existing lexical resources
were used to identify complement-taking nouns.
WordNet was used to find semantically similar
nouns. These were then tested in closed examples
and in Web searches in order to determine whether
they licensed complements and what the implica-
tive signature of the complement was. Although
identifying the complete set of these nouns is
non-trivial, the context mechanism for dealing
with implicatives makes adding them to the
BRIDGE system to derive the correct implications
straightforward.
</bodyText>
<sectionHeader confidence="0.946433" genericHeader="acknowledgments">
9 Appendix: Complement-taking nouns
</sectionHeader>
<bodyText confidence="0.999073333333333">
This appendix contains sample complement-taking
nouns and their classification in the BRIDGE sys-
tem.
</bodyText>
<subsectionHeader confidence="0.970583">
9.1 Noun that take to clauses
</subsectionHeader>
<bodyText confidence="0.978084162790698">
Ability nouns (impl nn with verb have): ability,
choice, energy, flexibility, freedom, heart, means,
way, wherewithal
Asset nouns (impl nn with verb have): money, op-
tion, time
Bravery nouns (impl nn with verb have): au-
dacity, ball, cajones, cheek, chutzpah, cojones,
11Thanks to Martin Forst (p.c.) for suggesting this direc-
tion.
courage, decency, foresight, gall, gumption, gut,
impudence, nerve, strength, temerity
Chance nouns (impl nn with verb have): chance,
occasion, opportunity
Effort nouns (impl nn with verb have): initiative,
liberty, trouble
Other nouns (no implicativity or not yet classi-
fied): accord, action, agreement, aim, ambition,
appetite, application, appointment, approval, at-
tempt, attitude, audition, authority, authorization,
battle, bid, blessing, campaign, capacity, clear-
ance, commission, commitment, concession, con-
fidence, consent, consideration, conspiracy, con-
tract, cost, decision, demand, desire, determina-
tion, directive, drive, duty, eagerness, effort, ev-
idence, expectation, failure, fear, fight, figure,
franchise, help, honor, hunger, hurry, idea, im-
pertinence, inability, incentive, inclination, indi-
cation, information, intent, intention, invitation,
itch, job, journey, justification, keenness, legisla-
tion, license, luck, mandate, moment, motion, mo-
tive, move, movement, need, note, notice, notifi-
cation, notion, obligation, offer, order, pact, pat-
tern, permission, plan, pledge, ploy, police, posi-
tion, potential, power, pressure, principle, process,
program, promise, propensity, proposal, proposi-
tion, provision, push, readiness, reason, recom-
mendation, refusal, reluctance, reminder, removal,
request, requirement, responsibility, right, rush,
scheme, scramble, sense, sentiment, shame, sign,
signal, stake, stampede, strategy, study, support,
task, temptation, tendency, threat, understanding,
undertaking, unwillingness, urge, venture, vote,
willingness, wish, word, work
</bodyText>
<subsectionHeader confidence="0.984526">
9.2 Nouns that take that clauses
</subsectionHeader>
<bodyText confidence="0.999953846153846">
Nouns with impl pp nn: abomination, angriness,
angst, animosity, anxiousness, apprehensiveness,
ardor, awe, bereavement, bitterness, case, choler,
consequence, consternation, covetousness, discon-
certion, disconcertment, disquiet, disquietude, ec-
stasy, edginess, enmity, enviousness, event, fact,
fearfulness, felicity, fright, frustration, fury, gall,
gloom, gloominess, grudge, happiness, hesitancy,
hostility, huffiness, huffishness, inquietude, in-
security, ire, jealousy, jitteriness, joy, joyous-
ness, jubilance, jumpiness, lovingness, poignance,
poignancy, premonition, presentiment, problem,
qualm, rancor, rapture, sadness, shyness, situa-
</bodyText>
<page confidence="0.992444">
15
</page>
<bodyText confidence="0.996481764705882">
tion, somberness, sorrow, sorrowfulness, suspense,
terror, trepidation, truth, uneasiness, unhappiness,
wrath
Nouns with fact p: absurdity, accident, hypocrisy,
idiocy, irony, miracle
Nouns with impl pn np: falsehood, lie
Other nouns (no implicativity or not yet classi-
fied): avowal, axiom, conjecture, conviction, cri-
tique, effort, fear, feeling, hunch, hysteria, idea,
impudence, inability, incentive, likelihood, news,
notion, opinion, optimism, option, outrage, pact,
ploy, point, police, possibility, potential, power,
precedent, premise, principle, problem, prospect,
proviso, reluctance, responsibility, right, rumor,
scramble, sentiment, showing, sign, skepticism,
stake, stand, story, strategy, tendency, unwilling-
ness, viewpoint, vision, willingness, word
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905243589744">
Bobrow, Daniel G., Cleo Condoravdi, Richard Crouch,
Ron Kaplan, Lauri Karttunen, Tracy Holloway King,
Valeria de Paiva, and Annie Zaenen. 2005. A ba-
sic logic for textual inference. In Proceedings of
the AAAI Workshop on Inferencefor Textual Question
Answering.
Bobrow, Daniel G., Bob Cheslow, Cleo Condoravdi,
Lauri Karttunen, Tracy Holloway King, Rowan
Nairn, Valeria de Paiva, Charlotte Price, and Annie
Zaenen. 2007. PARC’s Bridge and question answer-
ing system. In Grammar Engineering Across Frame-
works, pages 46–66. CSLI Publications.
Chatzichrisafis, Nikos, Dick Crouch, Tracy Holloway
King, Rowan Nairn, Manny Rayner, and Marianne
Santaholma. 2007. Regression testing for grammar-
based systems. In Grammar Engineering Across
Frameworks, pages 28–143. CSLI Publications.
Cohen, K. Bretonnel, William A. Baumgartner Jr., and
Lawrence Hunter. 2008. Software testing and the
naturally occurring data assumption in natural lan-
guage processing. In Software Engineering, Testing,
and Quality Assurance for Natural Language Pro-
cessing, pages 23–30. Association for Computational
Linguistics.
Crouch, Dick and Tracy Holloway King. 2005. Unify-
ing lexical resources. In Proceedings of the Interdis-
ciplinary Workshop on the Identification and Repre-
sentation of Verb Features and Verb Classes.
Crouch, Dick and Tracy Holloway King. 2006. Seman-
tics via f-structure rewriting. In LFG06 Proceedings.
CSLI Publications.
Crouch, Dick. 2005. Packed rewriting for mapping se-
mantics to KR. In Proceedings of the International
Workshop on Computational Semantics.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognizing textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognizing Textual Entailment,
Southampton, U.K.
Fellbaum, Christiane, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Gurevich, Olga, Richard Crouch, Tracy Holloway
King, and Valeria de Paiva. 2006. Deverbal nouns
in knowledge representation. In Proceedings of the
19th International Florida AI Research Society Con-
ference (FLAIRS ’06), pages 670–675.
Karttunen, Lauri. 2007. Word play. Computational
Linguistics, 33:443–467.
Kiparsky, Paul and Carol Kiparsky. 1971. Fact. In
Steinberg, D. and L. Jakobovits, editors, Semantics.
An Inderdisciplinary Reader, pages 345–369. Cam-
bridge University Press.
Kipper, Karin, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In AAAI-200017th National Conference on Artificial
Intelligence.
Macleod, Catherine, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In EURALEX’98.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies adn
Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn treebank: Annotative predicate
argument structure. In ARPA Human Language
Technology Workshop.
Maxwell, John and Ron Kaplan. 1991. A method for
disjunctive constraint satisfaction. Current Issues in
Parsing Technologies.
Nairn, Rowan, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual in-
ference. In Inference in Computational Semantics
(ICoS-5).
Pichotta, Karl. 2008. Processing paraphrases
and phrasal implicatives in the Bridge question-
answering system. Stanford University, Symbolic
Systems undergraduate honors thesis.
Vendler, Zeno. 1967. Linguistics and Philosophy. Cor-
nell University Press.
</reference>
<page confidence="0.998702">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.900394">
<title confidence="0.991023">Context Inducing Nouns</title>
<author confidence="0.942526">Charlotte Price Valeria de_Paiva Tracy Holloway King</author>
<affiliation confidence="0.99618">Palo Alto Research Center Palo Alto Research Center Palo Alto Research Center</affiliation>
<address confidence="0.9835735">3333 Coyote Hill Rd. 3333 Coyote Hill Rd. 3333 Coyote Hill Palo Alto, CA 94304 USA Palo Alto, CA 94304 USA Palo Alto, CA 94304 USA</address>
<email confidence="0.99905">lprice@parc.comvaleria.paiva@gmail.comthking@parc.com</email>
<abstract confidence="0.99893625">It is important to identify complementtaking nouns in order to properly analyze the grammatical and implicative structure of the sentence. This paper examines the ways in which these nouns were identified classified for addition to the natural language understanding system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel G Bobrow</author>
<author>Cleo Condoravdi</author>
<author>Richard Crouch</author>
<author>Ron Kaplan</author>
<author>Lauri Karttunen</author>
<author>Tracy Holloway King</author>
<author>Valeria de Paiva</author>
<author>Annie Zaenen</author>
</authors>
<title>A basic logic for textual inference.</title>
<date>2005</date>
<booktitle>In Proceedings of the AAAI Workshop on Inferencefor Textual Question Answering.</booktitle>
<marker>Bobrow, Condoravdi, Crouch, Kaplan, Karttunen, King, de Paiva, Zaenen, 2005</marker>
<rawString>Bobrow, Daniel G., Cleo Condoravdi, Richard Crouch, Ron Kaplan, Lauri Karttunen, Tracy Holloway King, Valeria de Paiva, and Annie Zaenen. 2005. A basic logic for textual inference. In Proceedings of the AAAI Workshop on Inferencefor Textual Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel G Bobrow</author>
<author>Bob Cheslow</author>
<author>Cleo Condoravdi</author>
<author>Lauri Karttunen</author>
<author>Tracy Holloway King</author>
<author>Rowan Nairn</author>
<author>Valeria de Paiva</author>
<author>Charlotte Price</author>
<author>Annie Zaenen</author>
</authors>
<title>PARC’s Bridge and question answering system.</title>
<date>2007</date>
<booktitle>In Grammar Engineering Across Frameworks,</booktitle>
<pages>46--66</pages>
<publisher>CSLI Publications.</publisher>
<marker>Bobrow, Cheslow, Condoravdi, Karttunen, King, Nairn, de Paiva, Price, Zaenen, 2007</marker>
<rawString>Bobrow, Daniel G., Bob Cheslow, Cleo Condoravdi, Lauri Karttunen, Tracy Holloway King, Rowan Nairn, Valeria de Paiva, Charlotte Price, and Annie Zaenen. 2007. PARC’s Bridge and question answering system. In Grammar Engineering Across Frameworks, pages 46–66. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Chatzichrisafis</author>
<author>Dick Crouch</author>
<author>Tracy Holloway King</author>
<author>Rowan Nairn</author>
<author>Manny Rayner</author>
<author>Marianne Santaholma</author>
</authors>
<title>Regression testing for grammarbased systems.</title>
<date>2007</date>
<booktitle>In Grammar Engineering Across Frameworks,</booktitle>
<pages>28--143</pages>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="18859" citStr="Chatzichrisafis et al., 2007" startWordPosition="3151" endWordPosition="3154">s and so do not need to be altered as new complement-taking nouns are found. As described in section 3, deciding which nouns take complements is currently hand curated, as it is quite difficult to distinguish them entirely automatically. 6 Testing To ensure that complement-taking nouns are working properly in the system, for each noun, a passage-query-correct answer triplet such as: (22) PASSAGE: The fact that Mary had returned surprised John. QUERY: Had Mary returned? ANSWER: YES is added to a testsuite. The testsuites are run and the results reported as part of the daily regression testing (Chatzichrisafis et al., 2007). Both naturally occurring and hand-crafted examples are used to ensure that the correct implications are being drawn. Natural examples test interactions between phenomena such as noun complementation and copular constructions, while hand-crafted examples allow isolation of the phenomenon and show that all cases are being tested (Cohen et al., 2008), e.g., that the correct entailments emerge under negation as well as in the positive case. Our current testsuites contain about 180 handcrafted examples. The number of natural examples is harder to count as they occur somewhat rarely in the mixed-p</context>
</contexts>
<marker>Chatzichrisafis, Crouch, King, Nairn, Rayner, Santaholma, 2007</marker>
<rawString>Chatzichrisafis, Nikos, Dick Crouch, Tracy Holloway King, Rowan Nairn, Manny Rayner, and Marianne Santaholma. 2007. Regression testing for grammarbased systems. In Grammar Engineering Across Frameworks, pages 28–143. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bretonnel Cohen</author>
<author>William A Baumgartner Jr</author>
<author>Lawrence Hunter</author>
</authors>
<title>Software testing and the naturally occurring data assumption in natural language processing.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</booktitle>
<pages>23--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19210" citStr="Cohen et al., 2008" startWordPosition="3202" endWordPosition="3205"> answer triplet such as: (22) PASSAGE: The fact that Mary had returned surprised John. QUERY: Had Mary returned? ANSWER: YES is added to a testsuite. The testsuites are run and the results reported as part of the daily regression testing (Chatzichrisafis et al., 2007). Both naturally occurring and hand-crafted examples are used to ensure that the correct implications are being drawn. Natural examples test interactions between phenomena such as noun complementation and copular constructions, while hand-crafted examples allow isolation of the phenomenon and show that all cases are being tested (Cohen et al., 2008), e.g., that the correct entailments emerge under negation as well as in the positive case. Our current testsuites contain about 180 handcrafted examples. The number of natural examples is harder to count as they occur somewhat rarely in the mixed-phenomena testsuites. One of our natural example files, which is based on newswire extracts from the PASCAL Recognizing Textual Entailment Challenge (Dagan et al., 2005), shows an approximate breakdown of the uses of the word that is as shown in (23). This sample, which is somewhat biased towards verbal complements since it contains many examples tha</context>
</contexts>
<marker>Cohen, Jr, Hunter, 2008</marker>
<rawString>Cohen, K. Bretonnel, William A. Baumgartner Jr., and Lawrence Hunter. 2008. Software testing and the naturally occurring data assumption in natural language processing. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 23–30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Tracy Holloway King</author>
</authors>
<title>Unifying lexical resources.</title>
<date>2005</date>
<booktitle>In Proceedings of the Interdisciplinary Workshop on the Identification and Representation of Verb Features and Verb Classes.</booktitle>
<contexts>
<context position="15349" citStr="Crouch and King, 2005" startWordPosition="2573" endWordPosition="2576">er, and nominal specifiers. The grammar produces a packed representation of the sentence which allows ambiguity to be dealt with efficiently (Maxwell and Kaplan, 1991). The parses are passed to the semantic rules which also work on packed structures (Crouch, 2005). The semantic layer looks up words in a Unified Lexicon (UL), connects surface arguments of verbs to their roles, and determines the context within which a word occurs in the sentence. Negation introduces a context, as do the complementtaking nouns discussed here (Bobrow et al., 2005). The UL combines several sources of information (Crouch and King, 2005). Much of the information comes from the syntactic lexicon, VerbNet (Kipper et al., 2000), and WordNet (Fellbaum, 1998), but there are also handcoded entries that add semantically relevant information such as its implication signature. A sample UL entry is given in Figure 1. The current number of complement-taking nouns in the system is shown in (21). Only a 6A context is still generated for these. Adjuncts, having no context of their own, inherit the implication signature of the clause containing them (section 2.2). 12 (cat(N), word(fact), subcat(NOUN-EXTRA), concept(%1), source(hand annotate</context>
</contexts>
<marker>Crouch, King, 2005</marker>
<rawString>Crouch, Dick and Tracy Holloway King. 2005. Unifying lexical resources. In Proceedings of the Interdisciplinary Workshop on the Identification and Representation of Verb Features and Verb Classes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Tracy Holloway King</author>
</authors>
<title>Semantics via f-structure rewriting.</title>
<date>2006</date>
<booktitle>In LFG06 Proceedings.</booktitle>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="14450" citStr="Crouch and King, 2006" startWordPosition="2429" endWordPosition="2432">es: facts imply truth; lies imply falsehood; and possibilities do not allow truth or falsehood to be established. The default for complements is that no implications can be drawn, as in (20c), which in the BRIDGE system is expressed as the noun having no implication signature.6 Once identified and its implication signature determined, adding the complement-taking noun to the BRIDGE system and deriving the correct inferences is straightforward. This process is described in section 5. 4 The BRIDGE system The BRIDGE system (Bobrow et al., 2007) includes a syntactic grammar, a semantics rule set (Crouch and King, 2006), an abstract knowledge representation (AKR) rule set, and an entailment and contradiction detection (ECD) system. The syntax, semantics, and AKR all depend on lexicons. The BRIDGE grammar defines syntactic properties of words, such as predicate-argument structure, tense, number, and nominal specifiers. The grammar produces a packed representation of the sentence which allows ambiguity to be dealt with efficiently (Maxwell and Kaplan, 1991). The parses are passed to the semantic rules which also work on packed structures (Crouch, 2005). The semantic layer looks up words in a Unified Lexicon (U</context>
</contexts>
<marker>Crouch, King, 2006</marker>
<rawString>Crouch, Dick and Tracy Holloway King. 2006. Semantics via f-structure rewriting. In LFG06 Proceedings. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
</authors>
<title>Packed rewriting for mapping semantics to KR.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Computational Semantics.</booktitle>
<contexts>
<context position="14991" citStr="Crouch, 2005" startWordPosition="2516" endWordPosition="2517">cludes a syntactic grammar, a semantics rule set (Crouch and King, 2006), an abstract knowledge representation (AKR) rule set, and an entailment and contradiction detection (ECD) system. The syntax, semantics, and AKR all depend on lexicons. The BRIDGE grammar defines syntactic properties of words, such as predicate-argument structure, tense, number, and nominal specifiers. The grammar produces a packed representation of the sentence which allows ambiguity to be dealt with efficiently (Maxwell and Kaplan, 1991). The parses are passed to the semantic rules which also work on packed structures (Crouch, 2005). The semantic layer looks up words in a Unified Lexicon (UL), connects surface arguments of verbs to their roles, and determines the context within which a word occurs in the sentence. Negation introduces a context, as do the complementtaking nouns discussed here (Bobrow et al., 2005). The UL combines several sources of information (Crouch and King, 2005). Much of the information comes from the syntactic lexicon, VerbNet (Kipper et al., 2000), and WordNet (Fellbaum, 1998), but there are also handcoded entries that add semantically relevant information such as its implication signature. A samp</context>
</contexts>
<marker>Crouch, 2005</marker>
<rawString>Crouch, Dick. 2005. Packed rewriting for mapping semantics to KR. In Proceedings of the International Workshop on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognizing textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognizing Textual Entailment,</booktitle>
<location>Southampton, U.K.</location>
<contexts>
<context position="19627" citStr="Dagan et al., 2005" startWordPosition="3269" endWordPosition="3272">ions between phenomena such as noun complementation and copular constructions, while hand-crafted examples allow isolation of the phenomenon and show that all cases are being tested (Cohen et al., 2008), e.g., that the correct entailments emerge under negation as well as in the positive case. Our current testsuites contain about 180 handcrafted examples. The number of natural examples is harder to count as they occur somewhat rarely in the mixed-phenomena testsuites. One of our natural example files, which is based on newswire extracts from the PASCAL Recognizing Textual Entailment Challenge (Dagan et al., 2005), shows an approximate breakdown of the uses of the word that is as shown in (23). This sample, which is somewhat biased towards verbal complements since it contains many examples that can be paraphrased as said that, nonetheless shows the relative scarcity of noun complements in the wild and underscores the importance of hand-crafted examples 13 for testing purposes. It it is clear that these noun complements were being analyzed incorrectly before; what is unclear is how much of an impact the misanalysis would have caused. Perhaps some other domain would demonstrate a significantly higher pre</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Dagan, Ido, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognizing textual entailment challenge. In Proceedings of the PASCAL Challenges Workshop on Recognizing Textual Entailment, Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Fellbaum, Christiane, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Fellbaum, Christiane, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Gurevich</author>
<author>Richard Crouch</author>
<author>Tracy Holloway King</author>
<author>Valeria de Paiva</author>
</authors>
<title>Deverbal nouns in knowledge representation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 19th International Florida AI Research Society Conference (FLAIRS ’06),</booktitle>
<pages>670--675</pages>
<marker>Gurevich, Crouch, King, de Paiva, 2006</marker>
<rawString>Gurevich, Olga, Richard Crouch, Tracy Holloway King, and Valeria de Paiva. 2006. Deverbal nouns in knowledge representation. In Proceedings of the 19th International Florida AI Research Society Conference (FLAIRS ’06), pages 670–675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<date>2007</date>
<booktitle>Word play. Computational Linguistics,</booktitle>
<pages>33--443</pages>
<contexts>
<context position="23218" citStr="Karttunen, 2007" startWordPosition="3865" endWordPosition="3866">bs, adjectives, and adverbs can also license complements and hence contexts with implication signatures. Examples in (26) show different parts of speech that introduce contexts.9 (26) a. Verb: John said that Paul had arrived. b. Adjective: It is possible that someone ate the last piece of cake. c. Adjective: John was available to see Mary. d. Adverb: John falsely reported that Mary saw Bill. Many classes of verbs have already been identified and are incorporated into the system (Nairn et al., 2006): verbs relating to speech (e.g., say, report, etc.), implicative verbs such as manage and fail (Karttunen, 2007), and factive verbs (e.g. agree, realize, consider) (Vendler, 1967; Kiparsky and Kiparsky, 1971), to name a few. Many adjectives have also been added to the system, including ones taking to and that complements.10 As with the complement-taking nouns, a significant part of the effort in incorporating the complement-taking adjectives into the system was identifying which adjectives license complements. The adverbs have not been explored in as much depth. 9From a syntactic perspective, the adverb falsely does not take a complement. However, it does introduce a context in the semantics and hence r</context>
</contexts>
<marker>Karttunen, 2007</marker>
<rawString>Karttunen, Lauri. 2007. Word play. Computational Linguistics, 33:443–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kiparsky</author>
<author>Carol Kiparsky</author>
</authors>
<date>1971</date>
<booktitle>An Inderdisciplinary Reader,</booktitle>
<pages>345--369</pages>
<editor>Fact. In Steinberg, D. and L. Jakobovits, editors, Semantics.</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="23314" citStr="Kiparsky and Kiparsky, 1971" startWordPosition="3876" endWordPosition="3879">lication signatures. Examples in (26) show different parts of speech that introduce contexts.9 (26) a. Verb: John said that Paul had arrived. b. Adjective: It is possible that someone ate the last piece of cake. c. Adjective: John was available to see Mary. d. Adverb: John falsely reported that Mary saw Bill. Many classes of verbs have already been identified and are incorporated into the system (Nairn et al., 2006): verbs relating to speech (e.g., say, report, etc.), implicative verbs such as manage and fail (Karttunen, 2007), and factive verbs (e.g. agree, realize, consider) (Vendler, 1967; Kiparsky and Kiparsky, 1971), to name a few. Many adjectives have also been added to the system, including ones taking to and that complements.10 As with the complement-taking nouns, a significant part of the effort in incorporating the complement-taking adjectives into the system was identifying which adjectives license complements. The adverbs have not been explored in as much depth. 9From a syntactic perspective, the adverb falsely does not take a complement. However, it does introduce a context in the semantics and hence requires a lexical entry similar to those discussed for the complement-taking nouns. 10This work </context>
</contexts>
<marker>Kiparsky, Kiparsky, 1971</marker>
<rawString>Kiparsky, Paul and Carol Kiparsky. 1971. Fact. In Steinberg, D. and L. Jakobovits, editors, Semantics. An Inderdisciplinary Reader, pages 345–369. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In AAAI-200017th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="15438" citStr="Kipper et al., 2000" startWordPosition="2587" endWordPosition="2590">ich allows ambiguity to be dealt with efficiently (Maxwell and Kaplan, 1991). The parses are passed to the semantic rules which also work on packed structures (Crouch, 2005). The semantic layer looks up words in a Unified Lexicon (UL), connects surface arguments of verbs to their roles, and determines the context within which a word occurs in the sentence. Negation introduces a context, as do the complementtaking nouns discussed here (Bobrow et al., 2005). The UL combines several sources of information (Crouch and King, 2005). Much of the information comes from the syntactic lexicon, VerbNet (Kipper et al., 2000), and WordNet (Fellbaum, 1998), but there are also handcoded entries that add semantically relevant information such as its implication signature. A sample UL entry is given in Figure 1. The current number of complement-taking nouns in the system is shown in (21). Only a 6A context is still generated for these. Adjuncts, having no context of their own, inherit the implication signature of the clause containing them (section 2.2). 12 (cat(N), word(fact), subcat(NOUN-EXTRA), concept(%1), source(hand annotated data), source(xle), xfr:concept for(%1,fact), xfr:lex class(%1,impl pp nn), xfr:wordnet</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>Kipper, Karin, Hoa Trang Dang, and Martha Palmer. 2000. Class-based construction of a verb lexicon. In AAAI-200017th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>NOMLEX: A lexicon of nominalizations.</title>
<date>1998</date>
<booktitle>In EURALEX’98.</booktitle>
<contexts>
<context position="9727" citStr="Macleod et al., 1998" startWordPosition="1643" endWordPosition="1646"> to how the arguments of the noun and the sentence relate to the arguments for the verb (e.g. -ee, -er).5 The BRIDGE system uses this linking to map these nouns to their verbal counterparts and to draw conclusions of implicativity as if they were verbs, as explained in (Gurevich et al., 2006). Consider (12) where the paraphrases using fear as a verb or a noun are clearly related. (12) a. The fear that Mary was ill paralyzed Bob. b. Bob feared that Mary was ill; this fear paralyzed Bob. 4The work described in this section was done by Lauri Karttunen and Karl Pichotta (Pichotta, 2008). 5NOMLEX (Macleod et al., 1998) is an excellent source of these deverbal nouns. Deverbal nouns can take that complements or, as in (13), to complements. Most often, the context introduced by a deverbal noun does not add an implication signature, as in (11), which results in the answer UNKNOWN to the question Was Mary ill?. (13) a. John’s promise to go swimming surprised us. b. John’s persuasion of Mary to sing at the party surprised us. Gerunds, being even more verb-like, are treated as verbs in our system and hence inherit the implicative properties from the corresponding verb. (14) Knowing that Mary had sung upset John. M</context>
<context position="13326" citStr="Macleod et al., 1998" startWordPosition="2249" endWordPosition="2252">ther feature licenses the clause, such as in (19) where the bold faced structure is licensing the italicized clause. (19) a. John is so warmhearted that he took her in without question. b. They had such a good friendship that she could tell him anything. Using these methods, from a seed set of 13 nouns, 170 non-deverbal complement-taking nouns were identified, most in the emotion and feeling classes. The same techniques were then applied to the state and information classes. Once the Penn Treebank seeds were incorporated, the same process was applied to the complementtaking nouns from NOMLEX (Macleod et al., 1998). 3.2 Determining implications As examples (8a) and (8b) showed, whether a word takes a complement is lexically determined; so is the type of implication signature introduced by the word. Compare the implications in (20). (20) a. The fact that Mary had returned surprised John. Mary had returned. b. The falsehood that Mary had returned surprised John. Mary had not returned. c. The possibility that Mary had returned surprised John. ? Mary had returned. These nouns have different implication signatures: facts imply truth; lies imply falsehood; and possibilities do not allow truth or falsehood to </context>
<context position="21362" citStr="Macleod et al., 1998" startWordPosition="3557" endWordPosition="3560">y to other languages. 7.1 Automating the search Testing whether a clause is an adjunct or a noun complement or is licensed by something else is currently done by hand. Automating the testing would allow many more nouns to be tested. However, this is non-trivial. As (8a) and (8b) demonstrated, the surface structure can appear very similar; it is only when we try to figure out the implications of the examples that the differences emerge. The Penn Treebank (Marcus et al., 1994) was initially used to extract complement-taking nouns. As more tree and dependency banks, as well as lexical resources (Macleod et al., 1998), are available, further lexical items can be extracted in this way. However, such resources are costly to build and so are only slowly added to the available NLP resources. Rather than trying to identify all potential noun complement clauses, a simpler approach would be to reduce the search space for the human judge. For example, some adjuncts (perhaps three quarters of them) could be eliminated from natural examples by using a part-of-speech tagger to identify occurrences where a conjugated verb immediately fol8This includes demonstrative uses, uses licensed by other parts of speech such as </context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Macleod, Catherine, Ralph Grishman, Adam Meyers, Leslie Barrett, and Ruth Reeves. 1998. NOMLEX: A lexicon of nominalizations. In EURALEX’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies adn Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn treebank: Annotative predicate argument structure.</title>
<date>1994</date>
<booktitle>In ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="11233" citStr="Marcus et al., 1994" startWordPosition="1896" endWordPosition="1899"> then classified and incorporated into the BRIDGE system (section 4). First, the gerunds are removed since these are mapped by the syntax into their verbal counterparts. Then the non-gerund deverbal nouns (section 2.4) are linked to their verbal counterpart so that they can be analyzed by the system as events. These two classes represent a significant number of the nouns that take that complements. 3.1 Syntactic classification However, there are many complement-taking nouns that are not deverbal. To expand our lexicon of these nouns, we started with a seed set garnered from the Penn Treebank (Marcus et al., 1994), which uses distinctive tree structures for complement-taking nouns, and a small list of linguistically prominent nouns. For each of these lexical items, we extracted words in the same semantic class from WordNet. Classes include words like fact, which direct attention to the clausal complement, as in (15), and nouns expressing emotion, as in (16). (15) It’s a fact that Mary came. (16) Bob’s joy that Mary had returned reduced him to tears. (11) John took the opportunity to sing. John sang. 11 These semantic classes provided a starting point for discovering more of these nouns: the class of em</context>
<context position="21220" citStr="Marcus et al., 1994" startWordPosition="3534" endWordPosition="3537"> such as automating the search process, identifying and classifying other parts of speech that take complements, and exploring transferability to other languages. 7.1 Automating the search Testing whether a clause is an adjunct or a noun complement or is licensed by something else is currently done by hand. Automating the testing would allow many more nouns to be tested. However, this is non-trivial. As (8a) and (8b) demonstrated, the surface structure can appear very similar; it is only when we try to figure out the implications of the examples that the differences emerge. The Penn Treebank (Marcus et al., 1994) was initially used to extract complement-taking nouns. As more tree and dependency banks, as well as lexical resources (Macleod et al., 1998), are available, further lexical items can be extracted in this way. However, such resources are costly to build and so are only slowly added to the available NLP resources. Rather than trying to identify all potential noun complement clauses, a simpler approach would be to reduce the search space for the human judge. For example, some adjuncts (perhaps three quarters of them) could be eliminated from natural examples by using a part-of-speech tagger to </context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies adn Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn treebank: Annotative predicate argument structure. In ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Maxwell</author>
<author>Ron Kaplan</author>
</authors>
<title>A method for disjunctive constraint satisfaction. Current Issues in Parsing Technologies.</title>
<date>1991</date>
<contexts>
<context position="14894" citStr="Maxwell and Kaplan, 1991" startWordPosition="2498" endWordPosition="2501">rward. This process is described in section 5. 4 The BRIDGE system The BRIDGE system (Bobrow et al., 2007) includes a syntactic grammar, a semantics rule set (Crouch and King, 2006), an abstract knowledge representation (AKR) rule set, and an entailment and contradiction detection (ECD) system. The syntax, semantics, and AKR all depend on lexicons. The BRIDGE grammar defines syntactic properties of words, such as predicate-argument structure, tense, number, and nominal specifiers. The grammar produces a packed representation of the sentence which allows ambiguity to be dealt with efficiently (Maxwell and Kaplan, 1991). The parses are passed to the semantic rules which also work on packed structures (Crouch, 2005). The semantic layer looks up words in a Unified Lexicon (UL), connects surface arguments of verbs to their roles, and determines the context within which a word occurs in the sentence. Negation introduces a context, as do the complementtaking nouns discussed here (Bobrow et al., 2005). The UL combines several sources of information (Crouch and King, 2005). Much of the information comes from the syntactic lexicon, VerbNet (Kipper et al., 2000), and WordNet (Fellbaum, 1998), but there are also handc</context>
</contexts>
<marker>Maxwell, Kaplan, 1991</marker>
<rawString>Maxwell, John and Ron Kaplan. 1991. A method for disjunctive constraint satisfaction. Current Issues in Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rowan Nairn</author>
<author>Cleo Condoravdi</author>
<author>Lauri Karttunen</author>
</authors>
<title>Computing relative polarity for textual inference.</title>
<date>2006</date>
<booktitle>In Inference in Computational Semantics (ICoS-5).</booktitle>
<contexts>
<context position="4649" citStr="Nairn et al., 2006" startWordPosition="759" endWordPosition="762">relative clause adjuncts allow very different inferences. Whereas the speaker’s beliefs about adjuncts take on the truth value of the clause they are embedded in, the truth value of clausal complements is also affected by the licensing noun. Compare the sentences below. The italicized clause in (5) is a complement, while in (6) it is an adjunct. (5) The lie that Mary was ill paralyzed Bob. Mary was not ill. (6) The situation that she had gotten herself into paralyzed Bob. She had gotten herself into a situation. To explain how this is possible, we introduce the notion of implicative contexts (Nairn et al., 2006), and claim that complement-taking nouns introduce a context for the complement, whereas no such context is created for the adjuncts. Perhaps the easiest way to think of a context is to imagine embedding the complement in an extra layer, with the layer adding information about how to adjust the truth-value of its contents.2 This allows us to conclude in (5) that the speaker believes that Mary and Bob exist, as does the event of Bob’s paralysis, but the event Mary was ill does not. These are refered to as the (un)instantiability of the components in the sentence. Contexts can be embedded within</context>
<context position="7738" citStr="Nairn et al., 2006" startWordPosition="1301" endWordPosition="1304">y of the embedded clause instead of the signature of lie: this would incorrectly allow the conclusion that Mary won. The content of the relative clause in (8b) is in the same context as surprise since no additional context is introduced by bonus. As such, we can conclude that Mary did win a bonus. 2.3 Complements introduced by to The previous subsection focused on finite complements introduced by that. From the perspective 3We say surprise has the implication signature ++/--: elements within its context have a positive implication in a positive context and negative in a negative context. See (Nairn et al., 2006) for detailed discussion of possible implication signatures and how to propagate them through contexts. 10 of aiding inferencing in the BRIDGE system, the nouns that take to complements that are not deverbal nouns (see section 2.4 for discussion of deverbals) seem to fall into three main classes:4 ability, bravery, and chance. Examples are shown in (9). (9) a. John has the ability to sing. b. John has the guts to sing out loud. c. John’s chance to sing came quickly. These all have an implication signature that gives a (negative) implication only in a negative context, as in (10); in a positive</context>
<context position="23105" citStr="Nairn et al., 2006" startWordPosition="3846" endWordPosition="3849">duced. (25) The judge announced that the defendant was guilty. 7.2 Other parts of speech that introduce contexts Verbs, adjectives, and adverbs can also license complements and hence contexts with implication signatures. Examples in (26) show different parts of speech that introduce contexts.9 (26) a. Verb: John said that Paul had arrived. b. Adjective: It is possible that someone ate the last piece of cake. c. Adjective: John was available to see Mary. d. Adverb: John falsely reported that Mary saw Bill. Many classes of verbs have already been identified and are incorporated into the system (Nairn et al., 2006): verbs relating to speech (e.g., say, report, etc.), implicative verbs such as manage and fail (Karttunen, 2007), and factive verbs (e.g. agree, realize, consider) (Vendler, 1967; Kiparsky and Kiparsky, 1971), to name a few. Many adjectives have also been added to the system, including ones taking to and that complements.10 As with the complement-taking nouns, a significant part of the effort in incorporating the complement-taking adjectives into the system was identifying which adjectives license complements. The adverbs have not been explored in as much depth. 9From a syntactic perspective,</context>
</contexts>
<marker>Nairn, Condoravdi, Karttunen, 2006</marker>
<rawString>Nairn, Rowan, Cleo Condoravdi, and Lauri Karttunen. 2006. Computing relative polarity for textual inference. In Inference in Computational Semantics (ICoS-5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Pichotta</author>
</authors>
<title>Processing paraphrases and phrasal implicatives in the Bridge questionanswering system. Stanford University, Symbolic Systems undergraduate honors thesis.</title>
<date>2008</date>
<contexts>
<context position="9695" citStr="Pichotta, 2008" startWordPosition="1640" endWordPosition="1641">b and classified according to how the arguments of the noun and the sentence relate to the arguments for the verb (e.g. -ee, -er).5 The BRIDGE system uses this linking to map these nouns to their verbal counterparts and to draw conclusions of implicativity as if they were verbs, as explained in (Gurevich et al., 2006). Consider (12) where the paraphrases using fear as a verb or a noun are clearly related. (12) a. The fear that Mary was ill paralyzed Bob. b. Bob feared that Mary was ill; this fear paralyzed Bob. 4The work described in this section was done by Lauri Karttunen and Karl Pichotta (Pichotta, 2008). 5NOMLEX (Macleod et al., 1998) is an excellent source of these deverbal nouns. Deverbal nouns can take that complements or, as in (13), to complements. Most often, the context introduced by a deverbal noun does not add an implication signature, as in (11), which results in the answer UNKNOWN to the question Was Mary ill?. (13) a. John’s promise to go swimming surprised us. b. John’s persuasion of Mary to sing at the party surprised us. Gerunds, being even more verb-like, are treated as verbs in our system and hence inherit the implicative properties from the corresponding verb. (14) Knowing </context>
</contexts>
<marker>Pichotta, 2008</marker>
<rawString>Pichotta, Karl. 2008. Processing paraphrases and phrasal implicatives in the Bridge questionanswering system. Stanford University, Symbolic Systems undergraduate honors thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Vendler</author>
</authors>
<title>Linguistics and Philosophy.</title>
<date>1967</date>
<publisher>Cornell University Press.</publisher>
<contexts>
<context position="23284" citStr="Vendler, 1967" startWordPosition="3874" endWordPosition="3875">ntexts with implication signatures. Examples in (26) show different parts of speech that introduce contexts.9 (26) a. Verb: John said that Paul had arrived. b. Adjective: It is possible that someone ate the last piece of cake. c. Adjective: John was available to see Mary. d. Adverb: John falsely reported that Mary saw Bill. Many classes of verbs have already been identified and are incorporated into the system (Nairn et al., 2006): verbs relating to speech (e.g., say, report, etc.), implicative verbs such as manage and fail (Karttunen, 2007), and factive verbs (e.g. agree, realize, consider) (Vendler, 1967; Kiparsky and Kiparsky, 1971), to name a few. Many adjectives have also been added to the system, including ones taking to and that complements.10 As with the complement-taking nouns, a significant part of the effort in incorporating the complement-taking adjectives into the system was identifying which adjectives license complements. The adverbs have not been explored in as much depth. 9From a syntactic perspective, the adverb falsely does not take a complement. However, it does introduce a context in the semantics and hence requires a lexical entry similar to those discussed for the complem</context>
</contexts>
<marker>Vendler, 1967</marker>
<rawString>Vendler, Zeno. 1967. Linguistics and Philosophy. Cornell University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>