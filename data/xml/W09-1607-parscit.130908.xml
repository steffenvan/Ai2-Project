<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026330">
<title confidence="0.9731595">
Sentence Position revisited:
A robust light-weight Update Summarization ‘baseline’ Algorithm
</title>
<author confidence="0.664552">
Rahul Katragadda Prasad Pingali Vasudeva Varma
</author>
<email confidence="0.544828">
rahul k@research.iiit.ac.in pvvpr@iiit.ac.in vv@iiit.ac.in
</email>
<affiliation confidence="0.6813365">
Language Technologies Research Center
IIIT Hyderabad
</affiliation>
<sectionHeader confidence="0.974214" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999536588235294">
In this paper, we describe a sentence po-
sition based summarizer that is built based
on a sentence position policy, created from
the evaluation testbed of recent summariza-
tion tasks at Document Understanding Con-
ferences (DUC). We show that the summa-
rizer thus built is able to outperform most sys-
tems participating in task focused summariza-
tion evaluations at Text Analysis Conferences
(TAC) 2008. Our experiments also show that
such a method would perform better at pro-
ducing short summaries (upto 100 words) than
longer summaries. Further, we discuss the
baselines traditionally used for summarization
evaluation and suggest the revival of an old
baseline to suit the current summarization task
at TAC: the Update Summarization task.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875133333333">
Document summarization received a lot of atten-
tion since an early work by Luhn (1958). Statis-
tical information derived from word frequency and
distribution was used by the machine to compute
a relative measure of significance, first for individ-
ual words and then for sentences. Later, Edmund-
son (1969) introduced four clues for identifying sig-
nificant words (topics) in a text. Among them title
and location are related to position methods, while
the other two are presence of cue words and high
frequency content words. Edmundson assigned pos-
itive weights to sentences according to their ordinal
position in the text, giving more weight to the first
sentence in the first paragraph and last sentence in
the last paragraph.
</bodyText>
<page confidence="0.989772">
46
</page>
<bodyText confidence="0.99984753125">
Position of a sentence in a document or the po-
sition of a word in a sentence give good clues to-
wards importance of the sentence or word respec-
tively. Such features are called locational features,
and a sentence position feature deals with presence
of key sentences at specific locations in the text.
Sentence Position has been well studied in summa-
rization research since its inception, early in Ed-
mundson’s work (1969). Earlier, Baxendale (1958)
investigated a sample of 200 paragraphs to deter-
mine where the important words are most likely to
be found. He concluded that in 85% of the para-
graphs, the first sentence was a topic sentence and in
7% of the paragraphs, the final one.
Recent advances in machine learning have been
adapted to summarization problem through the years
and locational features have been consistently used
to identify salience of a sentence. Some represen-
tative work in ‘learning’ sentence extraction would
include training a binary classifier (Kupiec et al.,
1995), training a Markov model (Conroy et al.,
2004), training a CRF (Shen et al., 2007), and learn-
ing pairwise-ranking of sentences (Toutanova et al.,
2007).
In recent years, at the Document Understand-
ing Conferences (DUC1), Text Summarization re-
search evolved through task focused evaluations
ranging from ‘generic single-document summariza-
tion’ to ‘query-focused multi-document summariza-
tion (QFMDS)’. The QFMDS task models the real-
world complex question answering task wherein,
given a topic and a set of 25 relevant documents, the
</bodyText>
<footnote confidence="0.994051">
1http://duc.nist.gov/
</footnote>
<note confidence="0.7286735">
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 46–52,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999707090909091">
task is to synthesize a fluent, well-organized 250-
word summary of the documents that answers the
question(s) in the topic statement. Recent focus
in the community has been towards query-focused
update-summarization task at DUC and the Text
Analysis Conference (TAC2). The update task was to
produce short (~100 words) multi-document update
summaries of newswire articles under the assump-
tion that the user has already read a set of earlier
articles. The purpose of each update summary will
be to inform the reader of new information about a
particular topic.
The rest of the paper is organized as follows. In
Section 2, we describe a Sub-optimal Position Pol-
icy (SPP) based on Pyramid Annotated Data, then
we derive a simple algorithm for summarization
based on the SPP in Section 3, and show evaluation
results. Next, in Section 4, we explain the current
baselines and evaluation for Multi-Document Sum-
marization and finally in Section 5, we discuss the
need for an older baseline in the current context of
the short summary task of update summarization.
</bodyText>
<sectionHeader confidence="0.902962" genericHeader="method">
2 Sub-Optimal Sentence Position Policy
</sectionHeader>
<bodyText confidence="0.999952375">
Given a large text collection and a way to approxi-
mate the relevance for a reasonably large subset of
sentences, we could identify significant positional
attributes for the genre of the collection. Our ex-
periments are based on the work described in (Lin
and Hovy, 1997), whose experiments using the Ziff-
Davis corpus gave great insights on the selective
power of the position method.
</bodyText>
<subsectionHeader confidence="0.9941765">
2.1 Sentence Position Yield and Optimal
Position Policy (OPP)
</subsectionHeader>
<bodyText confidence="0.9994862">
Lin and Hovy (1997) provide an empirical validation
for the position hypothesis. They describe a method
of deriving an Optimal Position Policy for a collec-
tion of texts within a genre, as long as a small set
of topic keywords is defined for each text. They de-
fined sentence yield (strength of relevance) of a sen-
tence based on the mention of topic keywords in the
sentence.
The positional yield is defined as the average sen-
tence yield for that position in the document. They
</bodyText>
<footnote confidence="0.840098">
2http://www.nist.gov/tac/
</footnote>
<bodyText confidence="0.999689894736842">
computed the yield of each sentence position in each
document by counting the number of different key-
words contained in the respective sentence in each
document, and averaging over all documents. An
Optimal Position Policy (OPP) is derived based on
the decreasing values of positional yield.
Their experiments grounded on the assumption
that abstract is an ideal representation of central
topic(s) of a text. For their evaluations, they used
the abstract to compare whether the sentences found
based on their Optimal Position Policy are indeed a
good selection. They used precision-recall measures
to establish those findings.
At our disposal we had data from pyramid eval-
uations that provided sentences and their mapping
to any content units in the gold standard summaries.
The annotations in the data provide a unique prop-
erty that each sentence can derive for itself a score
for relevance.
</bodyText>
<subsectionHeader confidence="0.992059">
2.2 Documents
</subsectionHeader>
<bodyText confidence="0.99994975">
There are a wide variety of document types across
genre. In our case of newswire collection we have
identified two primary types of documents: small
document and large document. This distinction is
made based on the total sentences in the document.
All documents that have the number of sentences
above a threshold should be considered large. We
experimented on thresholds varying from 10 to 35
sentences and figured out that documents’ distribu-
tion into the two categories was acceptable when
threshold-ed at 20 sentences. This decision is also
well supported by the fact that the last sentences of
a document were more important than the others in
the middle (Baxendale, 1958).
Sentence Position Yield (SPY) is obtained sep-
arately for both types of documents. For a small
document, sentence positions have values from 1
through 20. Meanwhile, for a large document we
compute SPY for position 1 through 20, then the last
15 sentences labeled 136 through 150 and ‘any other
sentence’ is labeled 100. It can be seen in figure 3
that sentences that do not come from leading or trail-
ing part of large documents do not contribute much
content to the summaries.
</bodyText>
<page confidence="0.999006">
47
</page>
<figureCaption confidence="0.998651">
Figure 1: A sample mapping of SCU annotation to source document sentences. An excerpt from mapping of topic
D0701A of DUC 2007 QF-MDS task.
Figure 2: Sentence Position Yield for small documents.
</figureCaption>
<subsectionHeader confidence="0.998561">
2.3 Pyramid Data
</subsectionHeader>
<bodyText confidence="0.999973607843137">
Summary content units, referred as SCUs hereafter,
are semantically motivated, sub-sentential units that
are variable in length but not bigger than a sentential
clause. SCUs emerge from annotation of a collec-
tion of human summaries for the same input. They
are identified by noting information that is repeated
across summaries, whether the repetition is as small
as a modifier of a noun phrase or as large as a clause.
The weight an SCU obtains is directly proportional
to the number of reference summaries that support
that piece of information. The evaluation method
that is based on overlapping SCUs in human and
automatic summaries is described in the Pyramid
method (Nenkova et al., 2007).
The University of Ottawa has organized the pyra-
mid annotation data such that for some of the sen-
tences in the original document collection (those
that were picked by systems participating in pyra-
mid evaluation), a list of corresponding content units
is known (Copeck et al., 2006). We used this data to
identify locations in a document from where most
sentences were being picked, and which of those lo-
cations were being most content responsive to the
query.
A sample of SCU mapping is shown in figure 1.
Three sentences are seen in the figure among which
two have been annotated with system IDs and SCU
weights wherever applicable. The first sentence has
not been picked by any of the summarizers partici-
pating in Pyramid Evaluations, hence it is unknown
if the sentence would have contributed to any SCU.
The second sentence was picked by 8 summarizers
and that sentence contributed to an SCU of weight
3. The third sentence in the example was picked
by one summarizer, however, it did not contribute
to any SCU. This example shows all the three types
of sentences available in the corpus: unknown sam-
ples, positive samples and negative samples.
For each SCU, a weight is associated in pyramid
annotations. Thus a sentential score could be de-
fined as sum of weights of all the contributing SCUs
of the sentence. For an unknown sample and a neg-
ative sample, sentential score is 0. For example, in
the second sentence in figure 1 the score is 3, con-
tributed by a single SCU. While the same for the first
and third sentences is 0.
For each sentence position the sentential score is
averaged over all documents, which we call Sen-
tence Position Yield. SPY for small and large doc-
uments is shown in figures 2 and 3. Based on these
values for various positions, a simple Position Pol-
</bodyText>
<page confidence="0.998356">
48
</page>
<figureCaption confidence="0.999539">
Figure 3: Sentence Position Yield for large documents
</figureCaption>
<bodyText confidence="0.99990925">
icy was framed as shown below. A position policy is
an ordered set consisting of elements in the order of
most importance. Within a subset, each sub-element
is equally important and treated likewise.
</bodyText>
<equation confidence="0.781587">
{s1, 51, {s2, 52, s3}, {53, s4, s5, s6, s7, s8, s20},
{54, s9} ... }
</equation>
<bodyText confidence="0.996307">
In the above position policy, sentences from small
documents and large documents are represented by
si and 5j respectively.
The position policy described above provides an
ordering of ranked sentence positions based on a
very accurate ‘relevance’ annotations on sentences.
However, there is a large subset of sentences that are
not annotated with either positive or negative rele-
vance judgment. Hence, the policy derived is based
on a high-precision low-recall corpus3 for sentence
relevance. If all the sentences were annotated with
such judgements, the policy could have been differ-
ent. For this reason we call the above derived policy,
a Sub-optimal Position Policy (SPP).
</bodyText>
<sectionHeader confidence="0.955835" genericHeader="method">
3 SPP as an algorithm
</sectionHeader>
<bodyText confidence="0.997404">
The goal of creating a position policy was to identify
its effectiveness as a summarization algorithm. The
</bodyText>
<footnote confidence="0.98185">
3DUC 2005 and 2006 data has been used for learning the
SPP. In further experiments in section 3, DUC 2007 and TAC
2008 data have been used as test data.
</footnote>
<bodyText confidence="0.999795263157895">
above simple heuristic was easily incorporated as an
algorithm based on simple scoring for each distinct
set in the policy. For instance, based on the policy
above, all s1 get the highest weight followed by next
best weight to all 51 and so on.
As it can be observed, only the first sentence of
each document could end up comprising the sum-
mary. This is okay, till we don’t get redundant infor-
mation in the summary. Hence we also used a sim-
ple unigram match based redundancy measure that
doesn’t allow a sentence if it matches any of the al-
ready selected sentences in at least 40% of content
words in it. We also dis-allow sentences greater than
25 content words.
We applied the above algorithm to generate multi-
document summaries for various tasks. We have ap-
plied it to Query-Focused Multi-Document Summa-
rization (QF-MDS) task of DUC 2007 and Query-
Focused Update Summarization task of TAC 2008.
</bodyText>
<subsectionHeader confidence="0.937878">
3.1 Query-Focused Multi-Document
Summarization
</subsectionHeader>
<bodyText confidence="0.9992955">
The query-focused multi-document summarization
task at DUC models the real world complex ques-
tion answering task. Given a topic and a set of 25
relevant documents, this task is to synthesize a flu-
ent, well-organized 250 word summary of the docu-
ments that answers the question(s) in the topic state-
</bodyText>
<page confidence="0.998711">
49
</page>
<bodyText confidence="0.999304545454546">
ment/narration.
The summaries from the above algorithm for the
QF-MDS were evaluated based on ROUGE met-
rics (Lin, 2004). The average4 recall scores are re-
ported for ROUGE-2 and ROUGE-SU4 in Table 1.
Also reported are the performance of the top per-
forming system and the official baseline(s). This al-
gorithm performed worse than most systems partic-
ipating in the task that year and performed better5
than only the ‘first x words’ baseline and 3 other sys-
tems.
</bodyText>
<table confidence="0.998903">
system ROUGE-2 ROUGE-SU4
‘first x words’ baseline 0.06039 0.10507
‘generic’ baseline 0.09382 0.14641
SPP algorithm 0.06913 0.12492
system 15 (top system) 0.12448 0.17711
</table>
<tableCaption confidence="0.958933">
Table 1: ROUGE 2, SU4 Recall scores for two base-
lines, the SPP algorithm and a top performing system
at Query-Focused Multi-Document Summarization task,
DUC 2007.
</tableCaption>
<subsectionHeader confidence="0.99764">
3.2 Update Summarization Task
</subsectionHeader>
<bodyText confidence="0.999870454545454">
The update summarization task is to produce short
(~100 words) multi-document update summaries of
newswire articles under the assumption that the user
has already read a set of earlier articles. The initial
document set is called cluster A and the next set of
articles are called cluster B. For cluster A, a query-
focused multi-document summary is expected. The
purpose of each ‘update summary’ (summary of
cluster B) will be to inform the reader of new in-
formation about a particular topic. Summaries from
the above algorithm for the Query Focused Up-
date Summarization task were evaluated based on
ROUGE metrics. This algorithm performed surpris-
ingly better at this task when compared to QF-MDS.
The rouge scores suggest that this algorithm is well
above the median for cluster A and among the top 5
systems for cluster B.
It must be noted that consistent performance
across clusters (both A and B) shows the robustness
of the ‘SPP algorithm’ at the update summarization
task. Also, it is evident that such an algorithm is
computationally simple and light-weight.
</bodyText>
<footnote confidence="0.958433">
4Averaged over all the 45 topics of DUC 2007 dataset.
5Better in a statistical sense, based on 95% confidence inter-
vals of the two systems’ evaluation based on ROUGE-2.
</footnote>
<bodyText confidence="0.999225941176471">
These surprisingly high scores on ROUGE met-
rics prompted us to evaluate the summaries based on
Pyramid Evaluation (Nenkova et al., 2007). Pyramid
evaluation provides a more semantic approach to
evaluation of content based on SCUs as discussed in
Section 2.3. The average6 modified pyramid scores
of cluster A and cluster B summaries is shown in
Table 2, along with the average recall scores for
ROUGE-2, ROUGE-SU4 scores. The pyramid eval-
uation7 suggests that this algorithm performs better
than all other automated systems at TAC 2008. Ta-
ble 3 shows the average performance (across clus-
ters) of ‘first x words’ baseline, SPP algorithm and
two top performing systems (System ID=43 and
ID=11). System 43 was adjudged best system based
on ROUGE metrics, and system 11 was top per-
former based on pyramid evaluations at TAC 2008.
</bodyText>
<table confidence="0.977566">
ROUGE-2 ROUGE-SU4 pyramid
cluster A 0.08987 0.1213 0.3432
cluster B 0.09319 0.1283 0.3576
</table>
<tableCaption confidence="0.919270333333333">
Table 2: Cluster wise ROUGE 2, SU4 Recall scores and
modified Pyramid Scores for SPP algorithm at the Update
Summarization task.
</tableCaption>
<subsectionHeader confidence="0.988418">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999950117647059">
It is interesting to observe that the algorithm that
performs very poorly at QF-MDS, does very well
in the Update Summarization task. A possible ex-
planation for such behavior could be based on sum-
mary length. For a 250 word summary in the QF-
MDS task, human summaries might provide a de-
scriptive answer to the query that includes informa-
tion nuggets accompanied by background informa-
tion. Indeed, it has been earlier reported that humans
appreciate receiving more information than just the
answer to the query, whenever possible (Lin et al.,
2003; Bosma, 2005).
Whereas, in the case of Update Summarization
task the summary length is only 100 words. In such
a short length humans need to trade-off between an-
swer sentences and supporting sentences, and usu-
ally answers are preferred. And since our method
</bodyText>
<footnote confidence="0.998709333333333">
6Averaged over all the 48 topics of TAC 2008 dataset.
7Pyramid Annotation were done by a volunteer who also
volunteered for annotations during DUC 2007.
</footnote>
<page confidence="0.922694">
50
</page>
<table confidence="0.9996276">
system ROUGE-2 ROUGE-SU4 pyramid
‘first x words’ baseline 0.05896 0.09327 0.166
SPP algorithm 0.09153 0.1245 0.3504
System 43 (top in ROUGE) 0.10395 0.13646 0.289
System 11 (top in pyramid) 0.08858 0.12484 0.336
</table>
<tableCaption confidence="0.9872705">
Table 3: Average ROUGE 2, SU4 Recall scores and modified Pyramid Scores for baseline, SPP algorithm and two top
performing systems at TAC 2008.
</tableCaption>
<bodyText confidence="0.997159909090909">
identifies sentences that are known to be contribut-
ing towards the needed answers, it performs better
at the shorter version of the task.
Another possible explanation is that as a shorter
summary length is required, the task of choosing the
most important information becomes more difficult
and no approach works well consistently. Also, it
has often been noted that this baseline is indeed quite
strong for this genre, due to the journalistic conven-
tion for putting the most important part of an article
in the initial paragraphs.
</bodyText>
<sectionHeader confidence="0.923076" genericHeader="method">
4 Baselines in Summarization Tasks
</sectionHeader>
<bodyText confidence="0.999807666666667">
Over the years, as summarization research followed
trends from generic single-document summariza-
tion, to generic multi-document summarization, to
focused multi-document summarization there were
two major baselines that stayed throughout the eval-
uations. Those two baselines are:
</bodyText>
<listItem confidence="0.99310575">
1. First N words of the document (or of the most re-
cent document).
2. First sentence from each document in chronological
order until the length requirement is reached.
</listItem>
<bodyText confidence="0.999973846153846">
The first baseline was in place ever since the first
evaluation of generic single document summariza-
tion took place in DUC 2001. For multi-document
summarization, first N words of the most recent
document (chronologically) was chosen as the base-
line 1. In the recent summarization evaluations at
Text Analysis Conference (TAC 2008), where up-
date summarization was evaluated; baseline 1 still
persists. This baseline performs pretty poorly at con-
tent evaluations based on all manual and automatic
metrics. However, since it doesn’t disturb the orig-
inal flow and ordering of a document, linguistically
these summaries are the best. Indeed it outperforms
all the automated systems based on linguistic quality
evaluations.
The second baseline had been used occasionally
with multi-document summarization from 2001 to
2004 with both generic multi-document summariza-
tion and focused multi-document summarization. In
2001 only one system significantly outperformed the
baseline 2 (Nenkova, 2005). In 2003 QF-MDS how-
ever, only one system outperformed the baseline 2
above, while in 2004 at the same task, no system
significantly outperforms the baseline. This baseline
as can be seen, over the years has been pretty much
untouched by systems based on content evaluation.
However, the linguistic aspects of summary quality
would be compromised in such a summary.
Currently, for the Update Summarization task at
TAC 2008, NIST’s baseline is the baseline 1 (‘first x
words’ baseline). And all systems (except one) per-
form better than the baseline in all forms of content
evaluation. Since the task is to generate 100 word
summaries (short summaries), based on past experi-
ences, there is no doubt that baseline 2 would per-
form well.
It is interesting to observe that baseline 2 is a close
approximation to the ‘SPP algorithm’ described in
this paper. There are two main differences that we
draw between ‘baseline 2’ and SPP algorithm. First,
‘baseline 2’ picks only the first sentence in each
document, while ‘SPP algorithm’ could pick other
sentences in an order described by the position pol-
icy. Second, ‘baseline 2’ puts no restriction on re-
dundancy, thus due to journalistic conventions entire
summary might be comprised of the same ‘informa-
tion nuggets’, wasting the minimal real-estate avail-
able (~100 words). On the other hand, in our ‘SPP
algorithm’ we consider a simple unigram-overlap
measure to identify redundant information in sen-
tence pairs that avoids redundant nuggets in the final
summary.
</bodyText>
<page confidence="0.998">
51
</page>
<sectionHeader confidence="0.995355" genericHeader="conclusions">
5 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999556674418605">
Baselines 1 and 2 mentioned above, could together
act as a balancing mechanism to compare for lin-
guistic quality and responsive content in the sum-
mary. The availability of a stronger content respon-
sive summary as a baseline would enable steady
progress in the field. While all the linguistically
motivated systems would compare themselves with
baseline 1, the summary content motivated systems
would compare with the stronger baseline 2 and get
better than it.
Over the years to come, the usage of ‘baseline 1’
doesn’t help in understanding whether there has
been significant improvement in the field. This is be-
cause almost every simple algorithm beats the base-
line performance. Having a better baseline, like the
one based on the position hypothesis, would raise
the bar for systems participating in coming years,
and tracking progress of the field over the years is
easier.
In this paper, we derived a method to identify a
‘sub-optimal position policy’ based on pyramid an-
notation data, that were previously unavailable. We
also distinguish small and large documents to obtain
the position policy. We described the Sub-optimal
Sentence Position Policy (SPP) based on pyramid
annotation data and implemented the SPP as an al-
gorithm to show that a position policy thus formed
is a good representative of the genre and thus per-
forms way above median performance. We further
describe the baselines used in summarization evalu-
ation and discuss the need to bring back baseline 2
(or the ‘SPP algorithm’) as an official baseline for
update summarization task.
Ultimately, as Lin and Hovy (1997) suggest, the
position method can only take us certain distance. It
has a limited power of resolution (the sentence) and
its limited method of identification (the position in a
text). Which is why we intend to use it as a baseline.
Currently, as we can see the algorithm generates a
generic summary, it doesn’t consider the topic or
query to generate a query-focused summary. In fu-
ture we plan to extend the SPP algorithm with some
basic method for bringing in relevance.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998629509433962">
P. B. Baxendale. 1958. Machine-made index for tech-
nical literature – an experiment. IBM Journal of Re-
search and Development, 2(Non-topical Issue).
Wauter Bosma. 2005. Extending answers using dis-
course structures. In Horacio Saggion and J. L. Minel,
editors, RANLP workshop on Crossing Barriers in Text
summarization Research, pages 2–9. Incoma Ltd.
John M. Conroy, Judith D. Schlesinger, Jade Goldstein,
and Dianne P. O’leary. 2004. Left-brain/right-brain
multi-document summarization. In the proceedings of
Document Understanding Conference (DUC) 2004.
Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy,
D Kipp, Vivi Nastase, and Stan Szpakowicz. 2006.
Leveraging duc. In proceedings of DUC 2006.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. In Journal of the ACM, volume 16, pages
264–285. ACM.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In the proceedings
of ACM SIGIR’95, pages 68–73. ACM.
Chin-Yew Lin and Eduard Hovy. 1997. Identifying top-
ics by position. In Proceedings of the fifth conference
on Applied natural language processing, pages 283–
290. ACL.
Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,
David Huynh, Boris Katz, and David R. Karger. 2003.
The role of context in question answering systems. In
the proceedings of CHI’04. ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In the proceedings of ACL
Workshop on Text Summarization Branches Out. ACL.
H.P. Luhn. 1958. The automatic creation of literature ab-
stracts. In IBM Journal of Research and Development,
Vol. 2, No. 2, pp. 159-165, April 1958.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. In ACM Trans. Speech Lang. Process., vol-
ume 4, New York, NY, USA. ACM.
Ani Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document under-
standing conference. In Manuela M. Veloso and Sub-
barao Kambhampati, editors, AAAI, pages 1436–1441.
AAAI Press / The MIT Press.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng
Chen. 2007. Document summarization using condi-
tional random fields. In the proceedings of IJCAI ’07.,
pages 2862–2867. IJCAI.
Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-
gadeesh Jagarlamundi, Hisami Suzuki, and Lucy Van-
derwende. 2007. The pythy summarization system:
Microsoft research at duc 2007. In the proceedings of
Document Understanding Conference 2007.
</reference>
<page confidence="0.998801">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.254277">
<title confidence="0.999343">Sentence Position revisited: A robust light-weight Update Summarization ‘baseline’ Algorithm</title>
<author confidence="0.6983465">Rahul Katragadda Prasad Pingali Vasudeva Varma rahul kresearch iiit ac in pvvpriiit ac in vviiit ac in</author>
<affiliation confidence="0.991687">Language Technologies Research Center</affiliation>
<address confidence="0.71742">IIIT Hyderabad</address>
<abstract confidence="0.993966666666667">In this paper, we describe a sentence position based summarizer that is built based on a sentence position policy, created from the evaluation testbed of recent summarization tasks at Document Understanding Conferences (DUC). We show that the summarizer thus built is able to outperform most systems participating in task focused summarization evaluations at Text Analysis Conferences (TAC) 2008. Our experiments also show that such a method would perform better at producing short summaries (upto 100 words) than longer summaries. Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P B Baxendale</author>
</authors>
<title>Machine-made index for technical literature – an experiment.</title>
<date>1958</date>
<journal>IBM Journal of Research and Development, 2(Non-topical Issue).</journal>
<contexts>
<context position="2184" citStr="Baxendale (1958)" startWordPosition="337" endWordPosition="338">to sentences according to their ordinal position in the text, giving more weight to the first sentence in the first paragraph and last sentence in the last paragraph. 46 Position of a sentence in a document or the position of a word in a sentence give good clues towards importance of the sentence or word respectively. Such features are called locational features, and a sentence position feature deals with presence of key sentences at specific locations in the text. Sentence Position has been well studied in summarization research since its inception, early in Edmundson’s work (1969). Earlier, Baxendale (1958) investigated a sample of 200 paragraphs to determine where the important words are most likely to be found. He concluded that in 85% of the paragraphs, the first sentence was a topic sentence and in 7% of the paragraphs, the final one. Recent advances in machine learning have been adapted to summarization problem through the years and locational features have been consistently used to identify salience of a sentence. Some representative work in ‘learning’ sentence extraction would include training a binary classifier (Kupiec et al., 1995), training a Markov model (Conroy et al., 2004), traini</context>
<context position="7090" citStr="Baxendale, 1958" startWordPosition="1119" endWordPosition="1120">r case of newswire collection we have identified two primary types of documents: small document and large document. This distinction is made based on the total sentences in the document. All documents that have the number of sentences above a threshold should be considered large. We experimented on thresholds varying from 10 to 35 sentences and figured out that documents’ distribution into the two categories was acceptable when threshold-ed at 20 sentences. This decision is also well supported by the fact that the last sentences of a document were more important than the others in the middle (Baxendale, 1958). Sentence Position Yield (SPY) is obtained separately for both types of documents. For a small document, sentence positions have values from 1 through 20. Meanwhile, for a large document we compute SPY for position 1 through 20, then the last 15 sentences labeled 136 through 150 and ‘any other sentence’ is labeled 100. It can be seen in figure 3 that sentences that do not come from leading or trailing part of large documents do not contribute much content to the summaries. 47 Figure 1: A sample mapping of SCU annotation to source document sentences. An excerpt from mapping of topic D0701A of </context>
</contexts>
<marker>Baxendale, 1958</marker>
<rawString>P. B. Baxendale. 1958. Machine-made index for technical literature – an experiment. IBM Journal of Research and Development, 2(Non-topical Issue).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wauter Bosma</author>
</authors>
<title>Extending answers using discourse structures.</title>
<date>2005</date>
<booktitle>In Horacio Saggion</booktitle>
<pages>2--9</pages>
<editor>and J. L. Minel, editors,</editor>
<publisher>Incoma Ltd.</publisher>
<contexts>
<context position="16460" citStr="Bosma, 2005" startWordPosition="2695" endWordPosition="2696">thm at the Update Summarization task. 3.3 Discussion It is interesting to observe that the algorithm that performs very poorly at QF-MDS, does very well in the Update Summarization task. A possible explanation for such behavior could be based on summary length. For a 250 word summary in the QFMDS task, human summaries might provide a descriptive answer to the query that includes information nuggets accompanied by background information. Indeed, it has been earlier reported that humans appreciate receiving more information than just the answer to the query, whenever possible (Lin et al., 2003; Bosma, 2005). Whereas, in the case of Update Summarization task the summary length is only 100 words. In such a short length humans need to trade-off between answer sentences and supporting sentences, and usually answers are preferred. And since our method 6Averaged over all the 48 topics of TAC 2008 dataset. 7Pyramid Annotation were done by a volunteer who also volunteered for annotations during DUC 2007. 50 system ROUGE-2 ROUGE-SU4 pyramid ‘first x words’ baseline 0.05896 0.09327 0.166 SPP algorithm 0.09153 0.1245 0.3504 System 43 (top in ROUGE) 0.10395 0.13646 0.289 System 11 (top in pyramid) 0.08858 0</context>
</contexts>
<marker>Bosma, 2005</marker>
<rawString>Wauter Bosma. 2005. Extending answers using discourse structures. In Horacio Saggion and J. L. Minel, editors, RANLP workshop on Crossing Barriers in Text summarization Research, pages 2–9. Incoma Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Jade Goldstein</author>
<author>Dianne P O’leary</author>
</authors>
<title>Left-brain/right-brain multi-document summarization.</title>
<date>2004</date>
<booktitle>In the proceedings of Document Understanding Conference (DUC)</booktitle>
<marker>Conroy, Schlesinger, Goldstein, O’leary, 2004</marker>
<rawString>John M. Conroy, Judith D. Schlesinger, Jade Goldstein, and Dianne P. O’leary. 2004. Left-brain/right-brain multi-document summarization. In the proceedings of Document Understanding Conference (DUC) 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Copeck</author>
<author>D Inkpen</author>
<author>Anna Kazantseva</author>
<author>A Kennedy</author>
<author>D Kipp</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Leveraging duc.</title>
<date>2006</date>
<booktitle>In proceedings of DUC</booktitle>
<contexts>
<context position="8760" citStr="Copeck et al., 2006" startWordPosition="1397" endWordPosition="1400">s small as a modifier of a noun phrase or as large as a clause. The weight an SCU obtains is directly proportional to the number of reference summaries that support that piece of information. The evaluation method that is based on overlapping SCUs in human and automatic summaries is described in the Pyramid method (Nenkova et al., 2007). The University of Ottawa has organized the pyramid annotation data such that for some of the sentences in the original document collection (those that were picked by systems participating in pyramid evaluation), a list of corresponding content units is known (Copeck et al., 2006). We used this data to identify locations in a document from where most sentences were being picked, and which of those locations were being most content responsive to the query. A sample of SCU mapping is shown in figure 1. Three sentences are seen in the figure among which two have been annotated with system IDs and SCU weights wherever applicable. The first sentence has not been picked by any of the summarizers participating in Pyramid Evaluations, hence it is unknown if the sentence would have contributed to any SCU. The second sentence was picked by 8 summarizers and that sentence contrib</context>
</contexts>
<marker>Copeck, Inkpen, Kazantseva, Kennedy, Kipp, Nastase, Szpakowicz, 2006</marker>
<rawString>Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy, D Kipp, Vivi Nastase, and Stan Szpakowicz. 2006. Leveraging duc. In proceedings of DUC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>In Journal of the ACM,</journal>
<volume>16</volume>
<pages>264--285</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1312" citStr="Edmundson (1969)" startWordPosition="191" endWordPosition="193">method would perform better at producing short summaries (upto 100 words) than longer summaries. Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task. 1 Introduction Document summarization received a lot of attention since an early work by Luhn (1958). Statistical information derived from word frequency and distribution was used by the machine to compute a relative measure of significance, first for individual words and then for sentences. Later, Edmundson (1969) introduced four clues for identifying significant words (topics) in a text. Among them title and location are related to position methods, while the other two are presence of cue words and high frequency content words. Edmundson assigned positive weights to sentences according to their ordinal position in the text, giving more weight to the first sentence in the first paragraph and last sentence in the last paragraph. 46 Position of a sentence in a document or the position of a word in a sentence give good clues towards importance of the sentence or word respectively. Such features are called</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H. P. Edmundson. 1969. New methods in automatic extracting. In Journal of the ACM, volume 16, pages 264–285. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In the proceedings of ACM SIGIR’95,</booktitle>
<pages>68--73</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2729" citStr="Kupiec et al., 1995" startWordPosition="424" endWordPosition="427"> its inception, early in Edmundson’s work (1969). Earlier, Baxendale (1958) investigated a sample of 200 paragraphs to determine where the important words are most likely to be found. He concluded that in 85% of the paragraphs, the first sentence was a topic sentence and in 7% of the paragraphs, the final one. Recent advances in machine learning have been adapted to summarization problem through the years and locational features have been consistently used to identify salience of a sentence. Some representative work in ‘learning’ sentence extraction would include training a binary classifier (Kupiec et al., 1995), training a Markov model (Conroy et al., 2004), training a CRF (Shen et al., 2007), and learning pairwise-ranking of sentences (Toutanova et al., 2007). In recent years, at the Document Understanding Conferences (DUC1), Text Summarization research evolved through task focused evaluations ranging from ‘generic single-document summarization’ to ‘query-focused multi-document summarization (QFMDS)’. The QFMDS task models the realworld complex question answering task wherein, given a topic and a set of 25 relevant documents, the 1http://duc.nist.gov/ Proceedings of CLIAWS3, Third International Cro</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In the proceedings of ACM SIGIR’95, pages 68–73. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying topics by position.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied natural language processing,</booktitle>
<pages>283--290</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4827" citStr="Lin and Hovy, 1997" startWordPosition="749" endWordPosition="752">ization based on the SPP in Section 3, and show evaluation results. Next, in Section 4, we explain the current baselines and evaluation for Multi-Document Summarization and finally in Section 5, we discuss the need for an older baseline in the current context of the short summary task of update summarization. 2 Sub-Optimal Sentence Position Policy Given a large text collection and a way to approximate the relevance for a reasonably large subset of sentences, we could identify significant positional attributes for the genre of the collection. Our experiments are based on the work described in (Lin and Hovy, 1997), whose experiments using the ZiffDavis corpus gave great insights on the selective power of the position method. 2.1 Sentence Position Yield and Optimal Position Policy (OPP) Lin and Hovy (1997) provide an empirical validation for the position hypothesis. They describe a method of deriving an Optimal Position Policy for a collection of texts within a genre, as long as a small set of topic keywords is defined for each text. They defined sentence yield (strength of relevance) of a sentence based on the mention of topic keywords in the sentence. The positional yield is defined as the average sen</context>
</contexts>
<marker>Lin, Hovy, 1997</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 1997. Identifying topics by position. In Proceedings of the fifth conference on Applied natural language processing, pages 283– 290. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Dennis Quan</author>
<author>Vineet Sinha</author>
<author>Karun Bakshi</author>
<author>David Huynh</author>
<author>Boris Katz</author>
<author>David R Karger</author>
</authors>
<title>The role of context in question answering systems.</title>
<date>2003</date>
<booktitle>In the proceedings of CHI’04.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="16446" citStr="Lin et al., 2003" startWordPosition="2691" endWordPosition="2694">res for SPP algorithm at the Update Summarization task. 3.3 Discussion It is interesting to observe that the algorithm that performs very poorly at QF-MDS, does very well in the Update Summarization task. A possible explanation for such behavior could be based on summary length. For a 250 word summary in the QFMDS task, human summaries might provide a descriptive answer to the query that includes information nuggets accompanied by background information. Indeed, it has been earlier reported that humans appreciate receiving more information than just the answer to the query, whenever possible (Lin et al., 2003; Bosma, 2005). Whereas, in the case of Update Summarization task the summary length is only 100 words. In such a short length humans need to trade-off between answer sentences and supporting sentences, and usually answers are preferred. And since our method 6Averaged over all the 48 topics of TAC 2008 dataset. 7Pyramid Annotation were done by a volunteer who also volunteered for annotations during DUC 2007. 50 system ROUGE-2 ROUGE-SU4 pyramid ‘first x words’ baseline 0.05896 0.09327 0.166 SPP algorithm 0.09153 0.1245 0.3504 System 43 (top in ROUGE) 0.10395 0.13646 0.289 System 11 (top in pyra</context>
</contexts>
<marker>Lin, Quan, Sinha, Bakshi, Huynh, Katz, Karger, 2003</marker>
<rawString>Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi, David Huynh, Boris Katz, and David R. Karger. 2003. The role of context in question answering systems. In the proceedings of CHI’04. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In the proceedings of ACL Workshop on Text Summarization Branches Out. ACL.</booktitle>
<contexts>
<context position="12905" citStr="Lin, 2004" startWordPosition="2112" endWordPosition="2113">have applied it to Query-Focused Multi-Document Summarization (QF-MDS) task of DUC 2007 and QueryFocused Update Summarization task of TAC 2008. 3.1 Query-Focused Multi-Document Summarization The query-focused multi-document summarization task at DUC models the real world complex question answering task. Given a topic and a set of 25 relevant documents, this task is to synthesize a fluent, well-organized 250 word summary of the documents that answers the question(s) in the topic state49 ment/narration. The summaries from the above algorithm for the QF-MDS were evaluated based on ROUGE metrics (Lin, 2004). The average4 recall scores are reported for ROUGE-2 and ROUGE-SU4 in Table 1. Also reported are the performance of the top performing system and the official baseline(s). This algorithm performed worse than most systems participating in the task that year and performed better5 than only the ‘first x words’ baseline and 3 other systems. system ROUGE-2 ROUGE-SU4 ‘first x words’ baseline 0.06039 0.10507 ‘generic’ baseline 0.09382 0.14641 SPP algorithm 0.06913 0.12492 system 15 (top system) 0.12448 0.17711 Table 1: ROUGE 2, SU4 Recall scores for two baselines, the SPP algorithm and a top perform</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In the proceedings of ACL Workshop on Text Summarization Branches Out. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>In IBM Journal of Research and Development,</journal>
<volume>2</volume>
<pages>159--165</pages>
<contexts>
<context position="1096" citStr="Luhn (1958)" startWordPosition="158" endWordPosition="159">. We show that the summarizer thus built is able to outperform most systems participating in task focused summarization evaluations at Text Analysis Conferences (TAC) 2008. Our experiments also show that such a method would perform better at producing short summaries (upto 100 words) than longer summaries. Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task. 1 Introduction Document summarization received a lot of attention since an early work by Luhn (1958). Statistical information derived from word frequency and distribution was used by the machine to compute a relative measure of significance, first for individual words and then for sentences. Later, Edmundson (1969) introduced four clues for identifying significant words (topics) in a text. Among them title and location are related to position methods, while the other two are presence of cue words and high frequency content words. Edmundson assigned positive weights to sentences according to their ordinal position in the text, giving more weight to the first sentence in the first paragraph an</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H.P. Luhn. 1958. The automatic creation of literature abstracts. In IBM Journal of Research and Development, Vol. 2, No. 2, pp. 159-165, April 1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
<author>Kathleen McKeown</author>
</authors>
<title>The pyramid method: Incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<journal>In ACM Trans. Speech Lang. Process.,</journal>
<volume>4</volume>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8478" citStr="Nenkova et al., 2007" startWordPosition="1350" endWordPosition="1353">ated, sub-sentential units that are variable in length but not bigger than a sentential clause. SCUs emerge from annotation of a collection of human summaries for the same input. They are identified by noting information that is repeated across summaries, whether the repetition is as small as a modifier of a noun phrase or as large as a clause. The weight an SCU obtains is directly proportional to the number of reference summaries that support that piece of information. The evaluation method that is based on overlapping SCUs in human and automatic summaries is described in the Pyramid method (Nenkova et al., 2007). The University of Ottawa has organized the pyramid annotation data such that for some of the sentences in the original document collection (those that were picked by systems participating in pyramid evaluation), a list of corresponding content units is known (Copeck et al., 2006). We used this data to identify locations in a document from where most sentences were being picked, and which of those locations were being most content responsive to the query. A sample of SCU mapping is shown in figure 1. Three sentences are seen in the figure among which two have been annotated with system IDs an</context>
<context position="14977" citStr="Nenkova et al., 2007" startWordPosition="2449" endWordPosition="2452"> above the median for cluster A and among the top 5 systems for cluster B. It must be noted that consistent performance across clusters (both A and B) shows the robustness of the ‘SPP algorithm’ at the update summarization task. Also, it is evident that such an algorithm is computationally simple and light-weight. 4Averaged over all the 45 topics of DUC 2007 dataset. 5Better in a statistical sense, based on 95% confidence intervals of the two systems’ evaluation based on ROUGE-2. These surprisingly high scores on ROUGE metrics prompted us to evaluate the summaries based on Pyramid Evaluation (Nenkova et al., 2007). Pyramid evaluation provides a more semantic approach to evaluation of content based on SCUs as discussed in Section 2.3. The average6 modified pyramid scores of cluster A and cluster B summaries is shown in Table 2, along with the average recall scores for ROUGE-2, ROUGE-SU4 scores. The pyramid evaluation7 suggests that this algorithm performs better than all other automated systems at TAC 2008. Table 3 shows the average performance (across clusters) of ‘first x words’ baseline, SPP algorithm and two top performing systems (System ID=43 and ID=11). System 43 was adjudged best system based on</context>
</contexts>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The pyramid method: Incorporating human content selection variation in summarization evaluation. In ACM Trans. Speech Lang. Process., volume 4, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
</authors>
<title>Automatic text summarization of newswire: Lessons learned from the document understanding conference.</title>
<date>2005</date>
<pages>1436--1441</pages>
<editor>In Manuela M. Veloso and Subbarao Kambhampati, editors, AAAI,</editor>
<publisher>AAAI Press / The MIT Press.</publisher>
<contexts>
<context position="19219" citStr="Nenkova, 2005" startWordPosition="3121" endWordPosition="3122"> baseline 1 still persists. This baseline performs pretty poorly at content evaluations based on all manual and automatic metrics. However, since it doesn’t disturb the original flow and ordering of a document, linguistically these summaries are the best. Indeed it outperforms all the automated systems based on linguistic quality evaluations. The second baseline had been used occasionally with multi-document summarization from 2001 to 2004 with both generic multi-document summarization and focused multi-document summarization. In 2001 only one system significantly outperformed the baseline 2 (Nenkova, 2005). In 2003 QF-MDS however, only one system outperformed the baseline 2 above, while in 2004 at the same task, no system significantly outperforms the baseline. This baseline as can be seen, over the years has been pretty much untouched by systems based on content evaluation. However, the linguistic aspects of summary quality would be compromised in such a summary. Currently, for the Update Summarization task at TAC 2008, NIST’s baseline is the baseline 1 (‘first x words’ baseline). And all systems (except one) perform better than the baseline in all forms of content evaluation. Since the task i</context>
</contexts>
<marker>Nenkova, 2005</marker>
<rawString>Ani Nenkova. 2005. Automatic text summarization of newswire: Lessons learned from the document understanding conference. In Manuela M. Veloso and Subbarao Kambhampati, editors, AAAI, pages 1436–1441. AAAI Press / The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Jian-Tao Sun</author>
<author>Hua Li</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Document summarization using conditional random fields.</title>
<date>2007</date>
<booktitle>In the proceedings of IJCAI ’07.,</booktitle>
<pages>2862--2867</pages>
<publisher>IJCAI.</publisher>
<contexts>
<context position="2812" citStr="Shen et al., 2007" startWordPosition="439" endWordPosition="442">ted a sample of 200 paragraphs to determine where the important words are most likely to be found. He concluded that in 85% of the paragraphs, the first sentence was a topic sentence and in 7% of the paragraphs, the final one. Recent advances in machine learning have been adapted to summarization problem through the years and locational features have been consistently used to identify salience of a sentence. Some representative work in ‘learning’ sentence extraction would include training a binary classifier (Kupiec et al., 1995), training a Markov model (Conroy et al., 2004), training a CRF (Shen et al., 2007), and learning pairwise-ranking of sentences (Toutanova et al., 2007). In recent years, at the Document Understanding Conferences (DUC1), Text Summarization research evolved through task focused evaluations ranging from ‘generic single-document summarization’ to ‘query-focused multi-document summarization (QFMDS)’. The QFMDS task models the realworld complex question answering task wherein, given a topic and a set of 25 relevant documents, the 1http://duc.nist.gov/ Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 46–52, Boulder, Colorado, June 2009. </context>
</contexts>
<marker>Shen, Sun, Li, Yang, Chen, 2007</marker>
<rawString>Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document summarization using conditional random fields. In the proceedings of IJCAI ’07., pages 2862–2867. IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Chris Brockett</author>
<author>Michael Gamon</author>
<author>Jagadeesh Jagarlamundi</author>
<author>Hisami Suzuki</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The pythy summarization system: Microsoft research at duc</title>
<date>2007</date>
<booktitle>In the proceedings of Document Understanding Conference</booktitle>
<contexts>
<context position="2881" citStr="Toutanova et al., 2007" startWordPosition="449" endWordPosition="452">words are most likely to be found. He concluded that in 85% of the paragraphs, the first sentence was a topic sentence and in 7% of the paragraphs, the final one. Recent advances in machine learning have been adapted to summarization problem through the years and locational features have been consistently used to identify salience of a sentence. Some representative work in ‘learning’ sentence extraction would include training a binary classifier (Kupiec et al., 1995), training a Markov model (Conroy et al., 2004), training a CRF (Shen et al., 2007), and learning pairwise-ranking of sentences (Toutanova et al., 2007). In recent years, at the Document Understanding Conferences (DUC1), Text Summarization research evolved through task focused evaluations ranging from ‘generic single-document summarization’ to ‘query-focused multi-document summarization (QFMDS)’. The QFMDS task models the realworld complex question answering task wherein, given a topic and a set of 25 relevant documents, the 1http://duc.nist.gov/ Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 46–52, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics task is to synthesiz</context>
</contexts>
<marker>Toutanova, Brockett, Gamon, Jagarlamundi, Suzuki, Vanderwende, 2007</marker>
<rawString>Kristina Toutanova, Chris Brockett, Michael Gamon, Jagadeesh Jagarlamundi, Hisami Suzuki, and Lucy Vanderwende. 2007. The pythy summarization system: Microsoft research at duc 2007. In the proceedings of Document Understanding Conference 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>