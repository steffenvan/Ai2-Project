<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997505">
Semantic Consistency: A Local Subspace Based Method for Distant
Supervised Relation Extraction
</title>
<author confidence="0.996328">
Xianpei Han and Le Sun
</author>
<affiliation confidence="0.881611333333333">
State Key Laboratory of Computer Science
Institute of Software, Chinese Academy of Sciences
HaiDian District, Beijing, China.
</affiliation>
<email confidence="0.990271">
{xianpei, sunle}@nfs.iscas.ac.cn
</email>
<sectionHeader confidence="0.994807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935066666667">
One fundamental problem of distant supervi-
sion is the noisy training corpus problem. In
this paper, we propose a new distant supervi-
sion method, called Semantic Consistency,
which can identify reliable instances from
noisy instances by inspecting whether an in-
stance is located in a semantically consistent
region. Specifically, we propose a semantic
consistency model, which first models the lo-
cal subspace around an instance as a sparse
linear combination of training instances, then
estimate the semantic consistency by exploit-
ing the characteristics of the local subspace.
Experimental results verified the effectiveness
of our method.
</bodyText>
<sectionHeader confidence="0.998128" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937166666667">
Relation extraction aims to identify and categorize
relations between pairs of entities in text. Due to
the time-consuming annotation process, one criti-
cal challenge of relation extraction is the lack of
training data. To address this limitation, a promis-
ing approach is distant supervision (DS), which
can automatically gather labeled data by heuristi-
cally aligning entities in text with those in a
knowledge base (Mintz et al., 2009). The under-
lying assumption of distant supervision is that
every sentence that mentions two entities is likely
to express their relation in a knowledge base.
</bodyText>
<note confidence="0.984070666666667">
Relation Instance Label
S1: Jobs was the founder of Apple Founder-of, CEO-of
S2: Jobs joins Apple Founder-of, CEO-of
</note>
<figureCaption confidence="0.957394">
Figure 1. Labeled instances by distant supervi-
</figureCaption>
<bodyText confidence="0.9182953">
sion, using relations CEO-of(Steve Jobs, Apple
Inc.) and Founder-of(Steve Jobs, Apple Inc.)
The distant supervision assumption, unfortu-
nately, can often fail and result in a noisy training
corpus. For example, in Figure 1 DS assumption
will wrongly label S1 as a CEO-of instance and S2
as instance of Founder-of and CEO-of. The noisy
training corpus in turn will lead to noisy extrac-
tions that hurt extraction accuracy (Riedel et al.,
2010).
</bodyText>
<figure confidence="0.9434754">
×
× S1 + + ×
+
: Manager-of
: CTO-of
</figure>
<figureCaption confidence="0.89870525">
Figure 2. The regions the two instances in Figure
1 located, where: 1) S1 locates in a semantically
consistent region; and 2) S2 locates in a semanti-
cally inconsistent region
</figureCaption>
<bodyText confidence="0.999983142857143">
To resolve the noisy training corpus problem,
this paper proposes a new distant supervision
method, called Semantic Consistency, which can
effectively identify reliable instances from noisy
instances by inspecting whether an instance is lo-
cated in a semantically consistent region. Figure 2
shows two intuitive examples. We can see that,
semantic consistency is an effective way to iden-
tify reliable instances. For example, in Figure 2 S1
is highly likely a reliable Founder-of instance be-
cause its neighbors are highly semantically con-
sistent, i.e., most of them express the same rela-
tion type – Founder-of. On contrast S2 is highly
likely a noisy instance because its neighbors are
semantically inconsistent, i.e., they have a diverse
relation types. The problem now is how to model
the semantic consistency around an instance.
To model the semantic consistency, this paper
proposes a local subspace based method. Specifi-
cally, given sufficient training instances, our
method first models each relation type as a linear
subspace spanned by its training instances. Then,
the local subspace around an instance is modeled
and characterized by seeking the sparsest linear
combination of training instances which can re-
construct the instance. Finally, we estimate the se-
mantic consistency of an instance by exploiting
the characteristics of its local subspace.
</bodyText>
<equation confidence="0.961156133333333">
+: CEO-of
× ×: Founder-of
×
×
× ×
×
×
+
+
+
+
×
+
S2
718
</equation>
<bodyText confidence="0.958610833333334">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 718–724,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
This paper is organized as follows. Section 2
reviews related work. Section 3 describes the pro-
posed method. Section 4 presents the experiments.
Finally Section 5 concludes this paper.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999970678571428">
This section briefly reviews the related work. Cra-
ven and Kumlien (1999), Wu et al. (2007) and
Mintz et al.(2009) were several pioneer work of
distant supervision. One main problem of DS as-
sumption is that it often will lead to false positives
in training data. To resolve this problem, Bunescu
and Mooney (2007), Riedel et al. (2010) and Yao
et al. (2010) relaxed the DS assumption to the at-
least-one assumption and employed multi-in-
stance learning techniques to identify wrongly la-
beled instances. Takamatsu et al. (2012) proposed
a generative model to eliminate noisy instances.
Another research issue of distant supervision is
that a pair of entities may participate in more than
one relation. To resolve this problem, Hoffmann
et al. (2010) proposed a method which can com-
bine a sentence-level model with a corpus-level
model to resolve the multi-label problem.
Surdeanu et al. (2012) proposed a multi-instance
multi-label learning approach which can jointly
model all instances of an entity pair and all their
labels. Several other research issues also have
been addressed. Xu et al. (2013), Min et al. (2013)
and Zhang et al. (2013) try to resolve the false
negative problem raised by the incomplete
knowledge base problem. Hoffmann et al. (2010)
and Zhang et al. (2010) try to improve the extrac-
tion precision by learning a dynamic lexicon.
</bodyText>
<sectionHeader confidence="0.938247" genericHeader="method">
3 The Semantic Consistency Model for
Relation Extraction
</sectionHeader>
<bodyText confidence="0.999972714285714">
In this section, we describe our semantic con-
sistency model for relation extraction. We first
model the subspaces of all relation types in the
original feature space, then model and character-
ize the local subspace around an instance, finally
estimate the semantic consistency of an instance
and exploit it for relation extraction.
</bodyText>
<subsectionHeader confidence="0.9974995">
3.1 Testing Instance as a Linear Combina-
tion of Training Instances
</subsectionHeader>
<bodyText confidence="0.999709264705882">
In this paper, we assume that there exist k distinct
relation types of interest and each relation type is
represented with an integer index from 1 to k. For
ith relation type, we assume that totally ni training
instances T i = {vi,l, vi,2, ..., vi,,,i} have been
collected using DS assumption. And each instance
is represented as a weighted feature vector, such
as the features used in (Mintz, 2009) or (Surdeanu
et al., 2012), with each feature is TFIDF weighted
by taking each instance as an individual document.
To model the subspace of ith relation type in
the original feature space, a variety of models
have been proposed to discover the underlying
patterns of Vi. In this paper, we make a simple and
effective assumption that the instances of a single
relation type can be represented as the linear
combination of other instances of the same rela-
tion type. This assumption is well motived in rela-
tion extraction, because although there is nearly
unlimited ways to express a specific relation, in
many cases basic principles of economy of ex-
pression and/or conventions of genre will ensure
that certain systematic ways will be used to ex-
press a specific relation (Wang et al., 2012). For
example, as shown in (Hearst, 1992), the IS-A re-
lation is usually expressed using several regular
patterns, such as “such NP as {NP ,}* {(or  |and)}
NP” and “NP {, NP}* {,} or other NP”.
Based on the above assumption, we hold many
instances for each relation type and directly use
these instances to model the subspace of a relation
type. Specifically, we represent an instance y of
ith type as the linear combination of training in-
stances associated with ith type:
</bodyText>
<equation confidence="0.999017">
Y = ai,ivi,l + ai,2Vi,2 + ... + +ai,niVi,ni (1)
</equation>
<bodyText confidence="0.99666925">
for some scalars , with j = 1, 2, ...,ni. For ex-
ample, we can represent the CEO-of instance
“Jobs was the CEO of Apple” as the following lin-
ear combination of CEO-of instances:
</bodyText>
<listItem confidence="0.9998835">
• 0.8: Steve Ballmer is the CEO of Microsoft
• 0.2: Rometty was served as the CEO of IBM
</listItem>
<bodyText confidence="0.608337">
For simplicity, we arrange the given ni training in-
stances of ith relation type as columns of a matrix
AZ = [Vi, 17 vi,27 ..., , then we can write the
matrix form of Formula 1 as:
</bodyText>
<equation confidence="0.883444">
y = Aixi (2)
</equation>
<bodyText confidence="0.9997066">
where is the coefficient vec-
tor. In this way, the subspace of a relation type is
the linear subspace spanned by its training in-
stances, and if we can find a valid xi, we can ex-
plain y as a valid instance of ith relation type.
</bodyText>
<subsectionHeader confidence="0.994914">
3.2 Local Subspace Modeling
via Sparse Representation
</subsectionHeader>
<bodyText confidence="0.999384">
Based on the above model, the local subspace of
an instance is modeled as the linear combination
of training instances which can reconstruct the in-
stance. Specifically, to model the local subspace,
we first concatenate the n training instances of all
k relation types:
</bodyText>
<equation confidence="0.403839">
A = [A1, A2, ..., Ak]
719
</equation>
<bodyText confidence="0.994965">
Then the local subspace around y is modeled by
seeking the solution of the following formula:
</bodyText>
<equation confidence="0.952515">
y = Ax (3)
</equation>
<bodyText confidence="0.999975">
However, because of the redundancy of train-
ing instances, Formula 3 usually has more than
one solution. In this paper, following the idea in
(Wright et al., 2009) for robust face recognition,
we use the sparsest solution (i.e., how to recon-
struct an instance using minimal training in-
stances), which have been shown is both discrimi-
nant and robust to noisiness. Concretely, we seek
the sparse linear combination of training instances
to reconstruct y by solving:
</bodyText>
<equation confidence="0.563784">
(4)
</equation>
<bodyText confidence="0.999128">
where X = [al,l ..., al � l ..., ai,l ai,2 ...7 a2,ni 7 ...I
is a coefficient vector which identifies the span-
ning instances of y’s local subspace, i.e., the in-
stances whose 𝛼𝑖j≠ 0 . In practice, the training
corpus may be too large to direct solve Formula 4.
Therefore, this paper uses the K-Nearest-Neigh-
bors (KNN) of y (1000 nearest neighbors in this
paper) to construct the training instance matrix A
for each y, and KNN can be searched very effi-
ciently using specialized algorithms such as the
LSH functions in (Andoni &amp; Indyk, 2006).
Through the above semantic decomposition,
we can see that, the entries of x can encode the
underlying semantic information of instance y.
For ith relation type, let be a new vector
whose only nonzero entries are the entries in x that
are associated with ith relation type, then we can
compute the semantic component corresponding
to ith relation type as . In this way a
testing instance y will be decomposed into k se-
mantic components, with each component corre-
sponds to one relation type (with an additional
noise component ):
</bodyText>
<figure confidence="0.950192857142857">
S1 = 0.8 £
2 3
6 co-founder 7
6 7
2 3
6 Apple 7
6 7
S2 = 0.1 ·joi.n¸ + 0.1 ·join¸ + 0.1 ·joi.n¸ + ...
(5)
was
4of 5
...
Founder-of noise
Founder-of CEO-of CTO-of
</figure>
<figureCaption confidence="0.999971">
Figure 3. The seman
</figureCaption>
<bodyText confidence="0.973629">
tic decomposition of the two
instances in Figure 1
tic
be scattered on many different semantic compo-
nents. On contrast if the instances around an in-
stance have consistent relation types
</bodyText>
<equation confidence="0.520473">
y’s
(S1 for ex-
</equation>
<bodyText confidence="0.9991355">
ample), most of its information will concentrate
on the corresponding relation type.
</bodyText>
<subsectionHeader confidence="0.987286">
3.3 Semantic Consistency based
Relation Extraction
</subsectionHeader>
<bodyText confidence="0.9942262">
consistency information of
local subspace: if
the instances around an instance have diverse re-
lation types (S2 for example), its information will
Specifically, given
</bodyText>
<equation confidence="0.7747115">
semantic decomposition:
Y =Y1 +... +yz +... +yk +e
</equation>
<bodyText confidence="0.965232888888889">
we observe that if instance y locates at a semantic
consistent region, then all its information will con-
centrate on a specific component
with all other
components equal to zero vector 0. However,
modeling errors, expression ambiguity and noisy
features will lead to small nonzero components.
Based on the above discussion, we define the se-
mantic consistency of an instance as the seman
</bodyText>
<table confidence="0.708327916666667">
y’s
yi,
tic
concentration degree of its decomposition:
Definition 1(Semantic Consistency). For an in-
stance y, its semantic consistency with ith relation
type is:
sim(Y, Yi) = Y &apos; Yi
Y &apos; Y
+ 0.2 £ Jobs
4the 5
...
</table>
<bodyText confidence="0.979137423076923">
Figure 3 shows an example of semantic decom-
position. We can see that, the semantic decompo-
sition can effectively summarize the seman
This section describes how to estimate and exploit
the semantic consistency for relation extraction.
where Consistency(y, i) and will be 1.0 if
all information of y is consistent with ith relation
type; on contrast it will be 0 if no information in y
is consistent with ith relation type.
Semantic Consistency based Relation Ex-
traction. To get accurate extractions, we deter-
mine the relation type ofy based on both: 1) How
much information
in
y is related to ith type; and 2)
its semantic consistency score with ith type, i.e.,
whether is a reliable instan
ce of ith type.
To measure how much information in y is re-
lated to ith relation type, we compute the propor-
tion of common information between y and yi:
(6)
Then the likelihood for a testing instance y ex-
pressing ith relation type is scored by summariz-
ing both its information and semantic consistency:
re
</bodyText>
<equation confidence="0.968541285714286">
y, i) = sim(y, y)
Consistency(y, i)
and y will be classified into ith relation type if its
likelihood is larger than a threshold:
rel(y
i)
i(7)
</equation>
<bodyText confidence="0.967134">
where is a relation type specific threshold
</bodyText>
<equation confidence="0.669427142857143">
learned fr
l(
£
;
¸¿
om training dataset.
720
</equation>
<bodyText confidence="0.975077125">
Multi-Instance Evidence Combination. It is
often that an entity pair will match more than one
sentence. To exploit such redundancy for more
confident extraction, this paper first combines the
evidence from different instances by combing
their underlying components. That is, given the
matched m instances Y={y1, y2, ..., ym} for an en-
tity pair (e1, e2), we first decompose each instance
as , then the entity-pair
level decomposition is ob-
tained by summarizing semantic components of
different instances: . Finally, the
likelihood of an entity pair expressing ith relation
type is scored as:
where is a score used to encourage
extractions with more matching instances.
</bodyText>
<subsectionHeader confidence="0.825688">
3.4 One further Issue for Distant Supervi-
sion: Training Instance Selection
</subsectionHeader>
<bodyText confidence="0.999869230769231">
The above model further provides new insights
into one issue for distant supervision: training in-
stance selection. In this paper, we select informa-
tive training instances by seeking a most compact
subset of instances which can span the whole sub-
space of a relation type. That is, all instances of
ith type can be represented as a linear combination
of these selected instances.
However, finding the optimal subset of training
instances is difficult, as there exist 2N possible so-
lutions for a relation type with N training instances.
Therefore, this paper proposes an approximate
training instance selection algorithm as follows:
</bodyText>
<listItem confidence="0.797223">
1) Computing the centroid of ith relation type as
Vi = E1&lt;j&lt;ni Vij
2) Finding the set of training instances which
can most compactly span the centroid by
solving:
(l&apos;) : xi = arg min jjxjjl s.t. jjAZx— ViII2 &lt; e
3) Ranking all training instances according to
</listItem>
<bodyText confidence="0.930167214285714">
their absolute coefficient weight value ;
4) Selecting top p percent ranked instances as
final training instances.
The above training instance selection has two
benefits. First, it will select informative instances
and remove redundant instances: an informative
instance will receive a high value because
many other instances can be represented using it;
and if two instances are redundant, the sparse so-
lution will only retain one of them. Second, most
of the wrongly labeled training instances will be
filtered, because these instances are usually not
regular expressions of ith type, so they appear
only a few times and will receive a small .
</bodyText>
<sectionHeader confidence="0.999494" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99993325">
In this section, we assess the performance of our
method and compare it with other methods.
Dataset. We assess our method using the KBP
dataset developed by Surdeanu et al. (2012). The
KBP is constructed by aligning the relations from
a subset of English Wikipedia infoboxes against a
document collection that merges two distinct
sources: (1) a 1.5 million documents collection
provided by the KBP shared task(Ji et al., 2010; Ji
et al., 2011); and (2) a complete snapshot of the
June 2010 version of Wikipedia. Totally 183,062
training relations and 3,334 testing relations are
collected. For tuning and testing, we used the
same partition as Surdeanu et al. (2012): 40 que-
ries for development and 160 queries for formal
evaluation. In this paper, each instance in KBP is
represented as a feature vector using the features
as the same as in (Surdeanu et al., 2012).
Baselines. We compare our method with four
baselines as follows:
</bodyText>
<listItem confidence="0.989877428571428">
• Mintz++. This is a traditional DS assump-
tion based model proposed by Mintz et al.(2009).
• Hoffmann. This is an at-least-one as-
sumption based multi-instance learning method
proposed by Hoffmann et al. (2011).
• MIML. This is a multi-instance multi-la-
bel model proposed by Surdeanu et al. (2012).
• K11. This is a classical K-N earest-
N eighbor�classifier baseline. Specifically, given
an entity pair, we first classify each matching in-
stance using the labels of its 5 (tuned on training
corpus) nearest neighbors with cosine similarity,
then all matching instances’ classification results
are added together.
</listItem>
<bodyText confidence="0.999919">
Evaluation. We use the same evaluation set-
tings as Surdeanu et al. (2012). That is, we use the
official KBP scorer with two changes: (a) relation
mentions are evaluated regardless of their support
document; and (b) we score only on the subset of
gold relations that have at least one mention in
matched sentences. For evaluation, we use
Mintz++, Hoffmann, and MIML implementation
from Stanford’s MIMLRE package (Surdeanu et
al., 2012) and implement KNN by ourselves.
</bodyText>
<subsectionHeader confidence="0.994967">
4.1 Experimental Results
</subsectionHeader>
<subsubsectionHeader confidence="0.843947">
4.1.1 Overall Results
</subsubsectionHeader>
<bodyText confidence="0.9971915">
We conduct experiments using all baselines and
our semantic consistency based method. For our
</bodyText>
<page confidence="0.710807">
721
</page>
<bodyText confidence="0.9999118">
method, we use top 10% weighted training in-
stances. All features occur less than 5 times are
filtered. All l1-minimization problems in this pa-
per are solved using the augmented Lagrange
multiplier algorithm (Yang et al., 2010), which
has been proven is accurate, efficient, and robust.
To select the classification threshold for ith re-
lation type, we use the value which can achieve
the best F-measure on training dataset (with an ad-
ditional restriction that precision should &gt; 10%).
</bodyText>
<figureCaption confidence="0.953066">
Figure 4. Precision/recall curves in KBP dataset
</figureCaption>
<table confidence="0.9998675">
System Precision Recall F1
Mintz++ 0.260 0.250 0.255
Hoffmann 0.306 0.198 0.241
MIML 0.249 0.314 0.278
KNN 0.261 0.295 0.277
Our method 0.286 0.342 0.311
</table>
<tableCaption confidence="0.999915">
Table 1. The best F1-measures in KBP dataset
</tableCaption>
<bodyText confidence="0.998578720930233">
Figure 4 shows the precision/recall curves of
different systems, and Table 1 shows their best
F1-measures. From these results, we can see that:
1) The semantic consistency based method
can achieve robust and competitive performance:
in KBP dataset, our method correspondingly
achieves 5.6%, 7%, 3.3% and 3.4% F1 improve-
ments over the Mintz++, Hoffmann, MIML and
KII baselines. We believe this verifies that the
semantic consistency around an instance is an ef-
fective way to identify reliable instances.
2) From Figure 4 we can see that our method
achieves a consistent improvement on the high-re-
call region of the KBP curves (when recall &gt; 0.1).
We believe this is because by modeling the se-
mantic consistency using the local subspace
around each testing instance, our method can bet-
ter solve the classification of long tail instances
which are not expressed using salient patterns.
3) The local subspace around an instance
can be effectively modeled as a linear subspace
spanned by training instances. From Table 1 we
can see that both our method and KII baseline
(where the local subspace is spanned using its k
nearest neighbors) achieve competitive perfor-
mance: even the simple KII baseline can achieve
a competitive performance (0.277 in F1). This re-
sult shows: a) the effectiveness of instance-based
subspace modeling; and b) by partitioning sub-
space into many local subspaces, the subspace
model is more adaptive and robust to model prior.
4) The sparse representation is an effective
way to model the local subspace using training in-
stances. Compared with KII baseline, our
method can achieve a 3.4% F1 improvement. We
believe this is because: (1) the discriminative na-
ture of sparse representation as shown in (Wright
et al., 2009); and (2) the sparse representation
globally seeks the combination of training in-
stances to characterize the local subspace, on con-
trast KII uses only its nearest neighbor in the
training data, which is more easily affected by
noisy training instances(e.g., false positives).
</bodyText>
<subsubsectionHeader confidence="0.993075">
4.1.2 Training Instance Selection Results
</subsubsectionHeader>
<bodyText confidence="0.998795">
To demonstrate the effect of training instance se-
lection, Table 2 reports our method’s performance
using different proportions of training instances.
</bodyText>
<table confidence="0.994545">
Proportion 5% 10% 20% 100%
Best F1 0.282 0.311 0.305 0.280
</table>
<tableCaption confidence="0.940489">
Table 2. The best F1-measures using different
proportions of top weighted training instances
</tableCaption>
<bodyText confidence="0.999793333333333">
From Table 2, we can see that: ① Our training in-
stance selection algorithm is effective: our method
can achieve performance improvement using only
top weighted instances. ② The training instances
are highly redundant: using only 10% weighted
instances can achieve a competitive performance.
</bodyText>
<sectionHeader confidence="0.993925" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999991166666667">
This paper proposes a semantic consistency
method, which can identify reliable instances
from noisy instances for distant supervised rela-
tion extraction. For future work, we want to de-
sign a more effective instance selection algorithm
and embed it into our extraction framework.
</bodyText>
<sectionHeader confidence="0.997698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9897182">
This work is supported by the National Natural
Science Foundation of China under Grants no.
61100152 and 61272324, and the National High
Technology Development 863 Program of China
under Grants no. 2013AA01A603.
</bodyText>
<page confidence="0.597408">
722
</page>
<sectionHeader confidence="0.905736" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999179038095239">
Andoni, Alexandr, and Piotr Indyk . 2006. Near-opti-
mal hashing algorithms for approximate nearest
neighbor in high dimensions. In: Foundations of
Computer Science, 2006, pp. 459-468.
Bunescu, Razvan, and Raymond Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In: ACL 2007, pp. 576.
Craven, Mark, and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting infor-
mation from text sources. In : Proceedings of AAAI
1999.
Downey, Doug, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction, In: Proceeding of IJCAI 2005.
Gupta, Rahul, and Sunita Sarawagi. 2011. Joint train-
ing for open-domain extraction on the web: exploit-
ing overlap when supervision is limited. In: Pro-
ceedings of WSDM 2011, pp. 217-226.
Hearst, Marti A. 1992. Automatic acquisition of hypo-
nyms from large text corpora. In: Proceedings of
COLING 1992, pp. 539-545.
Hoffmann, Raphael, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors. In:
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, 2010, pp.
286-295.
Hoffmann, Raphael, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In: Proceedings of ACL
2011, pp. 541-550.
Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the TAC 2010
knowledge base population track. In: Proceedings
of the Text Analytics Conference.
Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2011. Overview of the TAC 2011
knowledge base population track. In Proceedings of
the Text Analytics Conference.
Krause, Sebastian, Hong Li, Hans Uszkoreit, and Feiyu
Xu. 2012. Large-Scale learning of relation-extrac-
tion rules with distant supervision from the web. In:
ISWC 2012, pp. 263-278.
Mintz, Mike, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In: Proceedings ACL-
AFNLP 2009, pp. 1003-1011.
Min, Bonan, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant Supervision for
Relation Extraction with an Incomplete Knowledge
Base. In: Proceedings of NAACL-HLT 2013,pp.
777-782.
Min, Bonan, Xiang Li, Ralph Grishman, and Ang Sun.
2012. New york university 2012 system for kbp slot
filling. In: Proceedings of TAC 2012.
Nguyen, Truc-Vien T., and Alessandro Moschitti.
2011. Joint distant and direct supervision for rela-
tion extraction. In: Proceedings of IJCNLP 2011, pp.
732-740.
Riedel, Sebastian, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In: Machine Learning and
Knowledge Discovery in Databases, 2010, pp. 148-
163.
Riedel, Sebastian, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation Extraction
with Matrix Factorization and Universal Schemas.
In: Proceedings of NAACL-HLT 2013, pp. 74-84.
Roth, Benjamin, and Dietrich Klakow. 2013. Combin-
ing Generative and Discriminative Model Scores for
Distant Supervision. In: Proceedings of ACL 2013,
pp. 24-29.
Surdeanu, Mihai, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In: Pro-
ceedings of EMNLP-CoNLL 2012, pp. 455-465.
Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In: ACL 2012,pp. 721-729.
Wang, Chang, Aditya Kalyanpur, James Fan, Branimir
K. Boguraev, and D. C. Gondek. 2012. Relation ex-
traction and scoring in DeepQA. In: IBM Journal of
Research and Development, 56(3.4), pp. 9-1.
Wang, Chang, James Fan, Aditya Kalyanpur, and Da-
vid Gondek. 2011. Relation extraction with relation
topics. In: Proceedings of EMNLP 2011, pp. 1426-
1436.
Wright, John, Allen Y. Yang, Arvind Ganesh, Shankar
S. Sastry, and Yi Ma. 2009. Robust face recognition
via sparse representation. In: Pattern Analysis and
Machine Intelligence, IEEE Transactions on, 31(2),
210-227
Wu, Fei, and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In: Proceedings of CIKM
2007,pp. 41-50.
Xu, Wei, Raphael Hoffmann Le Zhao, and Ralph
Grishman. 2013. Filling Knowledge Base Gaps for
Distant Supervision of Relation Extraction. In: Pro-
ceedings of Proceedings of 2013, pp. 665-670.
Yang, Allen Y., Shankar S. Sastry, Arvind Ganesh, and
Yi Ma. 2010. Fast l1-Minimization Algorithms and
An Application in Robust Face Recognition: A Re-
view. In: Proceedings of ICIP 2010.
Yao, Limin, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
</reference>
<page confidence="0.613925">
723
</page>
<reference confidence="0.999526727272727">
without labelled data. In: Proceedings of EMNLP
2010, pp. 1013-1023.
Zhang, Congle, Raphael Hoffmann, and Daniel S.
Weld. 2012. Ontological smoothing for relation ex-
traction with minimal supervision. In: Proceedings
of AAAI 2012, pp. 157-163.
Zhang, Xingxing, Zhang, Jianwen, Zeng, Junyu, Yan,
Jun, Chen, Zheng and Sui, Zhifang. 2013. Towards
Accurate Distant Supervision for Relational Facts
Extraction. In: Proceedings of ACL 2013, pp. 810-
815.
</reference>
<page confidence="0.918002">
724
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.528102">
<title confidence="0.99968">Semantic Consistency: A Local Subspace Based Method for Supervised Relation Extraction</title>
<author confidence="0.972336">Han</author>
<affiliation confidence="0.850894333333333">State Key Laboratory of Computer Institute of Software, Chinese Academy of HaiDian District, Beijing, China.</affiliation>
<email confidence="0.972427">xianpei@nfs.iscas.ac.cn</email>
<email confidence="0.972427">sunle@nfs.iscas.ac.cn</email>
<abstract confidence="0.9985461875">One fundamental problem of distant supervision is the noisy training corpus problem. In this paper, we propose a new distant supervimethod, called which can identify reliable instances from noisy instances by inspecting whether an instance is located in a semantically consistent Specifically, we propose a which first models the local subspace around an instance as a sparse linear combination of training instances, then estimate the semantic consistency by exploiting the characteristics of the local subspace. Experimental results verified the effectiveness of our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandr Andoni</author>
<author>Piotr Indyk</author>
</authors>
<title>Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions.</title>
<date>2006</date>
<journal>In: Foundations of Computer Science,</journal>
<pages>459--468</pages>
<contexts>
<context position="9832" citStr="Andoni &amp; Indyk, 2006" startWordPosition="1630" endWordPosition="1633">e sparse linear combination of training instances to reconstruct y by solving: (4) where X = [al,l ..., al � l ..., ai,l ai,2 ...7 a2,ni 7 ...I is a coefficient vector which identifies the spanning instances of y’s local subspace, i.e., the instances whose 𝛼𝑖j≠ 0 . In practice, the training corpus may be too large to direct solve Formula 4. Therefore, this paper uses the K-Nearest-Neighbors (KNN) of y (1000 nearest neighbors in this paper) to construct the training instance matrix A for each y, and KNN can be searched very efficiently using specialized algorithms such as the LSH functions in (Andoni &amp; Indyk, 2006). Through the above semantic decomposition, we can see that, the entries of x can encode the underlying semantic information of instance y. For ith relation type, let be a new vector whose only nonzero entries are the entries in x that are associated with ith relation type, then we can compute the semantic component corresponding to ith relation type as . In this way a testing instance y will be decomposed into k semantic components, with each component corresponds to one relation type (with an additional noise component ): S1 = 0.8 £ 2 3 6 co-founder 7 6 7 2 3 6 Apple 7 6 7 S2 = 0.1 ·joi.n¸ +</context>
</contexts>
<marker>Andoni, Indyk, 2006</marker>
<rawString>Andoni, Alexandr, and Piotr Indyk . 2006. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In: Foundations of Computer Science, 2006, pp. 459-468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision. In: ACL</title>
<date>2007</date>
<pages>576</pages>
<contexts>
<context position="4489" citStr="Bunescu and Mooney (2007)" startWordPosition="701" endWordPosition="704">ers), pages 718–724, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics This paper is organized as follows. Section 2 reviews related work. Section 3 describes the proposed method. Section 4 presents the experiments. Finally Section 5 concludes this paper. 2 Related Work This section briefly reviews the related work. Craven and Kumlien (1999), Wu et al. (2007) and Mintz et al.(2009) were several pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-insta</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Bunescu, Razvan, and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In: ACL 2007, pp. 576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In : Proceedings of AAAI</booktitle>
<contexts>
<context position="4249" citStr="Craven and Kumlien (1999)" startWordPosition="658" endWordPosition="662"> consistency of an instance by exploiting the characteristics of its local subspace. +: CEO-of × ×: Founder-of × × × × × × + + + + × + S2 718 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 718–724, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics This paper is organized as follows. Section 2 reviews related work. Section 3 describes the proposed method. Section 4 presents the experiments. Finally Section 5 concludes this paper. 2 Related Work This section briefly reviews the related work. Craven and Kumlien (1999), Wu et al. (2007) and Mintz et al.(2009) were several pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate i</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Craven, Mark, and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In : Proceedings of AAAI 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Oren Etzioni</author>
<author>Stephen Soderland</author>
</authors>
<title>A probabilistic model of redundancy in information extraction, In: Proceeding of IJCAI</title>
<date>2005</date>
<marker>Downey, Etzioni, Soderland, 2005</marker>
<rawString>Downey, Doug, Oren Etzioni, and Stephen Soderland. 2005. A probabilistic model of redundancy in information extraction, In: Proceeding of IJCAI 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Gupta</author>
<author>Sunita Sarawagi</author>
</authors>
<title>Joint training for open-domain extraction on the web: exploiting overlap when supervision is limited. In:</title>
<date>2011</date>
<booktitle>Proceedings of WSDM 2011,</booktitle>
<pages>217--226</pages>
<marker>Gupta, Sarawagi, 2011</marker>
<rawString>Gupta, Rahul, and Sunita Sarawagi. 2011. Joint training for open-domain extraction on the web: exploiting overlap when supervision is limited. In: Proceedings of WSDM 2011, pp. 217-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora. In:</title>
<date>1992</date>
<booktitle>Proceedings of COLING</booktitle>
<pages>539--545</pages>
<contexts>
<context position="7210" citStr="Hearst, 1992" startWordPosition="1153" endWordPosition="1154">en proposed to discover the underlying patterns of Vi. In this paper, we make a simple and effective assumption that the instances of a single relation type can be represented as the linear combination of other instances of the same relation type. This assumption is well motived in relation extraction, because although there is nearly unlimited ways to express a specific relation, in many cases basic principles of economy of expression and/or conventions of genre will ensure that certain systematic ways will be used to express a specific relation (Wang et al., 2012). For example, as shown in (Hearst, 1992), the IS-A relation is usually expressed using several regular patterns, such as “such NP as {NP ,}* {(or |and)} NP” and “NP {, NP}* {,} or other NP”. Based on the above assumption, we hold many instances for each relation type and directly use these instances to model the subspace of a relation type. Specifically, we represent an instance y of ith type as the linear combination of training instances associated with ith type: Y = ai,ivi,l + ai,2Vi,2 + ... + +ai,niVi,ni (1) for some scalars , with j = 1, 2, ...,ni. For example, we can represent the CEO-of instance “Jobs was the CEO of Apple” as</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, Marti A. 1992. Automatic acquisition of hyponyms from large text corpora. In: Proceedings of COLING 1992, pp. 539-545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors. In:</title>
<date>2010</date>
<booktitle>Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>286--295</pages>
<contexts>
<context position="4922" citStr="Hoffmann et al. (2010)" startWordPosition="770" endWordPosition="773">ral pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013), Min et al. (2013) and Zhang et al. (2013) try to resolve the false negative problem raised by the incomplete knowledge base problem. Hoffmann et al. (2010) and Zhang et al. (2010) try to improve the extraction precision by learning a dynamic lexic</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Hoffmann, Raphael, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In: Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, 2010, pp. 286-295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations. In:</title>
<date>2011</date>
<booktitle>Proceedings of ACL 2011,</booktitle>
<pages>541--550</pages>
<contexts>
<context position="16357" citStr="Hoffmann et al. (2011)" startWordPosition="2730" endWordPosition="2733"> 183,062 training relations and 3,334 testing relations are collected. For tuning and testing, we used the same partition as Surdeanu et al. (2012): 40 queries for development and 160 queries for formal evaluation. In this paper, each instance in KBP is represented as a feature vector using the features as the same as in (Surdeanu et al., 2012). Baselines. We compare our method with four baselines as follows: • Mintz++. This is a traditional DS assumption based model proposed by Mintz et al.(2009). • Hoffmann. This is an at-least-one assumption based multi-instance learning method proposed by Hoffmann et al. (2011). • MIML. This is a multi-instance multi-label model proposed by Surdeanu et al. (2012). • K11. This is a classical K-N earestN eighbor�classifier baseline. Specifically, given an entity pair, we first classify each matching instance using the labels of its 5 (tuned on training corpus) nearest neighbors with cosine similarity, then all matching instances’ classification results are added together. Evaluation. We use the same evaluation settings as Surdeanu et al. (2012). That is, we use the official KBP scorer with two changes: (a) relation mentions are evaluated regardless of their support do</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Hoffmann, Raphael, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In: Proceedings of ACL 2011, pp. 541-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Hoa Trang Dang</author>
<author>Kira Griffitt</author>
<author>Joe Ellis</author>
</authors>
<title>knowledge base population track. In:</title>
<date>2010</date>
<journal>Overview of the TAC</journal>
<booktitle>Proceedings of the Text Analytics Conference.</booktitle>
<contexts>
<context position="15641" citStr="Ji et al., 2010" startWordPosition="2610" endWordPosition="2613">abeled training instances will be filtered, because these instances are usually not regular expressions of ith type, so they appear only a few times and will receive a small . 4 Experiments In this section, we assess the performance of our method and compare it with other methods. Dataset. We assess our method using the KBP dataset developed by Surdeanu et al. (2012). The KBP is constructed by aligning the relations from a subset of English Wikipedia infoboxes against a document collection that merges two distinct sources: (1) a 1.5 million documents collection provided by the KBP shared task(Ji et al., 2010; Ji et al., 2011); and (2) a complete snapshot of the June 2010 version of Wikipedia. Totally 183,062 training relations and 3,334 testing relations are collected. For tuning and testing, we used the same partition as Surdeanu et al. (2012): 40 queries for development and 160 queries for formal evaluation. In this paper, each instance in KBP is represented as a feature vector using the features as the same as in (Surdeanu et al., 2012). Baselines. We compare our method with four baselines as follows: • Mintz++. This is a traditional DS assumption based model proposed by Mintz et al.(2009). • </context>
</contexts>
<marker>Ji, Grishman, Dang, Griffitt, Ellis, 2010</marker>
<rawString>Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Griffitt, and Joe Ellis. 2010. Overview of the TAC 2010 knowledge base population track. In: Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Hoa Trang Dang</author>
<author>Kira Griffitt</author>
<author>Joe Ellis</author>
</authors>
<title>knowledge base population track.</title>
<date>2011</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<contexts>
<context position="15659" citStr="Ji et al., 2011" startWordPosition="2614" endWordPosition="2617">nstances will be filtered, because these instances are usually not regular expressions of ith type, so they appear only a few times and will receive a small . 4 Experiments In this section, we assess the performance of our method and compare it with other methods. Dataset. We assess our method using the KBP dataset developed by Surdeanu et al. (2012). The KBP is constructed by aligning the relations from a subset of English Wikipedia infoboxes against a document collection that merges two distinct sources: (1) a 1.5 million documents collection provided by the KBP shared task(Ji et al., 2010; Ji et al., 2011); and (2) a complete snapshot of the June 2010 version of Wikipedia. Totally 183,062 training relations and 3,334 testing relations are collected. For tuning and testing, we used the same partition as Surdeanu et al. (2012): 40 queries for development and 160 queries for formal evaluation. In this paper, each instance in KBP is represented as a feature vector using the features as the same as in (Surdeanu et al., 2012). Baselines. We compare our method with four baselines as follows: • Mintz++. This is a traditional DS assumption based model proposed by Mintz et al.(2009). • Hoffmann. This is </context>
</contexts>
<marker>Ji, Grishman, Dang, Griffitt, Ellis, 2011</marker>
<rawString>Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Griffitt, and Joe Ellis. 2011. Overview of the TAC 2011 knowledge base population track. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Krause</author>
<author>Hong Li</author>
<author>Hans Uszkoreit</author>
<author>Feiyu Xu</author>
</authors>
<title>Large-Scale learning of relation-extraction rules with distant supervision from the web. In: ISWC</title>
<date>2012</date>
<pages>263--278</pages>
<marker>Krause, Li, Uszkoreit, Xu, 2012</marker>
<rawString>Krause, Sebastian, Hong Li, Hans Uszkoreit, and Feiyu Xu. 2012. Large-Scale learning of relation-extraction rules with distant supervision from the web. In: ISWC 2012, pp. 263-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data. In:</title>
<date>2009</date>
<booktitle>Proceedings ACLAFNLP</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="1376" citStr="Mintz et al., 2009" startWordPosition="198" endWordPosition="201">nstances, then estimate the semantic consistency by exploiting the characteristics of the local subspace. Experimental results verified the effectiveness of our method. 1 Introduction Relation extraction aims to identify and categorize relations between pairs of entities in text. Due to the time-consuming annotation process, one critical challenge of relation extraction is the lack of training data. To address this limitation, a promising approach is distant supervision (DS), which can automatically gather labeled data by heuristically aligning entities in text with those in a knowledge base (Mintz et al., 2009). The underlying assumption of distant supervision is that every sentence that mentions two entities is likely to express their relation in a knowledge base. Relation Instance Label S1: Jobs was the founder of Apple Founder-of, CEO-of S2: Jobs joins Apple Founder-of, CEO-of Figure 1. Labeled instances by distant supervision, using relations CEO-of(Steve Jobs, Apple Inc.) and Founder-of(Steve Jobs, Apple Inc.) The distant supervision assumption, unfortunately, can often fail and result in a noisy training corpus. For example, in Figure 1 DS assumption will wrongly label S1 as a CEO-of instance </context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mintz, Mike, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In: Proceedings ACLAFNLP 2009, pp. 1003-1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Ralph Grishman</author>
<author>Li Wan</author>
<author>Chang Wang</author>
<author>David Gondek</author>
</authors>
<title>Distant Supervision for Relation Extraction with an Incomplete Knowledge Base. In:</title>
<date>2013</date>
<booktitle>Proceedings of NAACL-HLT 2013,pp.</booktitle>
<pages>777--782</pages>
<contexts>
<context position="5292" citStr="Min et al. (2013)" startWordPosition="829" endWordPosition="832">. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013), Min et al. (2013) and Zhang et al. (2013) try to resolve the false negative problem raised by the incomplete knowledge base problem. Hoffmann et al. (2010) and Zhang et al. (2010) try to improve the extraction precision by learning a dynamic lexicon. 3 The Semantic Consistency Model for Relation Extraction In this section, we describe our semantic consistency model for relation extraction. We first model the subspaces of all relation types in the original feature space, then model and characterize the local subspace around an instance, finally estimate the semantic consistency of an instance and exploit it for</context>
</contexts>
<marker>Min, Grishman, Wan, Wang, Gondek, 2013</marker>
<rawString>Min, Bonan, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant Supervision for Relation Extraction with an Incomplete Knowledge Base. In: Proceedings of NAACL-HLT 2013,pp. 777-782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Xiang Li</author>
<author>Ralph Grishman</author>
<author>Ang Sun</author>
</authors>
<title>system for kbp slot filling. In:</title>
<date>2012</date>
<booktitle>Proceedings of TAC</booktitle>
<location>New york university</location>
<marker>Min, Li, Grishman, Sun, 2012</marker>
<rawString>Min, Bonan, Xiang Li, Ralph Grishman, and Ang Sun. 2012. New york university 2012 system for kbp slot filling. In: Proceedings of TAC 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Joint distant and direct supervision for relation extraction. In:</title>
<date>2011</date>
<booktitle>Proceedings of IJCNLP 2011,</booktitle>
<pages>732--740</pages>
<marker>Nguyen, Moschitti, 2011</marker>
<rawString>Nguyen, Truc-Vien T., and Alessandro Moschitti. 2011. Joint distant and direct supervision for relation extraction. In: Proceedings of IJCNLP 2011, pp. 732-740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text. In:</title>
<date>2010</date>
<booktitle>Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>148--163</pages>
<contexts>
<context position="2137" citStr="Riedel et al., 2010" startWordPosition="319" endWordPosition="322"> knowledge base. Relation Instance Label S1: Jobs was the founder of Apple Founder-of, CEO-of S2: Jobs joins Apple Founder-of, CEO-of Figure 1. Labeled instances by distant supervision, using relations CEO-of(Steve Jobs, Apple Inc.) and Founder-of(Steve Jobs, Apple Inc.) The distant supervision assumption, unfortunately, can often fail and result in a noisy training corpus. For example, in Figure 1 DS assumption will wrongly label S1 as a CEO-of instance and S2 as instance of Founder-of and CEO-of. The noisy training corpus in turn will lead to noisy extractions that hurt extraction accuracy (Riedel et al., 2010). × × S1 + + × + : Manager-of : CTO-of Figure 2. The regions the two instances in Figure 1 located, where: 1) S1 locates in a semantically consistent region; and 2) S2 locates in a semantically inconsistent region To resolve the noisy training corpus problem, this paper proposes a new distant supervision method, called Semantic Consistency, which can effectively identify reliable instances from noisy instances by inspecting whether an instance is located in a semantically consistent region. Figure 2 shows two intuitive examples. We can see that, semantic consistency is an effective way to iden</context>
<context position="4511" citStr="Riedel et al. (2010)" startWordPosition="705" endWordPosition="708">ore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics This paper is organized as follows. Section 2 reviews related work. Section 3 describes the proposed method. Section 4 presents the experiments. Finally Section 5 concludes this paper. 2 Related Work This section briefly reviews the related work. Craven and Kumlien (1999), Wu et al. (2007) and Mintz et al.(2009) were several pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learni</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Riedel, Sebastian, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In: Machine Learning and Knowledge Discovery in Databases, 2010, pp. 148-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation Extraction with Matrix Factorization and Universal Schemas. In:</title>
<date>2013</date>
<booktitle>Proceedings of NAACL-HLT 2013,</booktitle>
<pages>74--84</pages>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Riedel, Sebastian, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation Extraction with Matrix Factorization and Universal Schemas. In: Proceedings of NAACL-HLT 2013, pp. 74-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Roth</author>
<author>Dietrich Klakow</author>
</authors>
<title>Combining Generative and Discriminative Model Scores for Distant Supervision. In:</title>
<date>2013</date>
<booktitle>Proceedings of ACL 2013,</booktitle>
<pages>24--29</pages>
<marker>Roth, Klakow, 2013</marker>
<rawString>Roth, Benjamin, and Dietrich Klakow. 2013. Combining Generative and Discriminative Model Scores for Distant Supervision. In: Proceedings of ACL 2013, pp. 24-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction. In:</title>
<date>2012</date>
<booktitle>Proceedings of EMNLP-CoNLL 2012,</booktitle>
<pages>455--465</pages>
<contexts>
<context position="5066" citStr="Surdeanu et al. (2012)" startWordPosition="793" endWordPosition="796">solve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013), Min et al. (2013) and Zhang et al. (2013) try to resolve the false negative problem raised by the incomplete knowledge base problem. Hoffmann et al. (2010) and Zhang et al. (2010) try to improve the extraction precision by learning a dynamic lexicon. 3 The Semantic Consistency Model for Relation Extraction In this section, we describe our semantic consistency model for relation extraction</context>
<context position="6407" citStr="Surdeanu et al., 2012" startWordPosition="1015" endWordPosition="1018">al subspace around an instance, finally estimate the semantic consistency of an instance and exploit it for relation extraction. 3.1 Testing Instance as a Linear Combination of Training Instances In this paper, we assume that there exist k distinct relation types of interest and each relation type is represented with an integer index from 1 to k. For ith relation type, we assume that totally ni training instances T i = {vi,l, vi,2, ..., vi,,,i} have been collected using DS assumption. And each instance is represented as a weighted feature vector, such as the features used in (Mintz, 2009) or (Surdeanu et al., 2012), with each feature is TFIDF weighted by taking each instance as an individual document. To model the subspace of ith relation type in the original feature space, a variety of models have been proposed to discover the underlying patterns of Vi. In this paper, we make a simple and effective assumption that the instances of a single relation type can be represented as the linear combination of other instances of the same relation type. This assumption is well motived in relation extraction, because although there is nearly unlimited ways to express a specific relation, in many cases basic princi</context>
<context position="15395" citStr="Surdeanu et al. (2012)" startWordPosition="2571" endWordPosition="2574">d remove redundant instances: an informative instance will receive a high value because many other instances can be represented using it; and if two instances are redundant, the sparse solution will only retain one of them. Second, most of the wrongly labeled training instances will be filtered, because these instances are usually not regular expressions of ith type, so they appear only a few times and will receive a small . 4 Experiments In this section, we assess the performance of our method and compare it with other methods. Dataset. We assess our method using the KBP dataset developed by Surdeanu et al. (2012). The KBP is constructed by aligning the relations from a subset of English Wikipedia infoboxes against a document collection that merges two distinct sources: (1) a 1.5 million documents collection provided by the KBP shared task(Ji et al., 2010; Ji et al., 2011); and (2) a complete snapshot of the June 2010 version of Wikipedia. Totally 183,062 training relations and 3,334 testing relations are collected. For tuning and testing, we used the same partition as Surdeanu et al. (2012): 40 queries for development and 160 queries for formal evaluation. In this paper, each instance in KBP is repres</context>
<context position="16831" citStr="Surdeanu et al. (2012)" startWordPosition="2805" endWordPosition="2808">oposed by Mintz et al.(2009). • Hoffmann. This is an at-least-one assumption based multi-instance learning method proposed by Hoffmann et al. (2011). • MIML. This is a multi-instance multi-label model proposed by Surdeanu et al. (2012). • K11. This is a classical K-N earestN eighbor�classifier baseline. Specifically, given an entity pair, we first classify each matching instance using the labels of its 5 (tuned on training corpus) nearest neighbors with cosine similarity, then all matching instances’ classification results are added together. Evaluation. We use the same evaluation settings as Surdeanu et al. (2012). That is, we use the official KBP scorer with two changes: (a) relation mentions are evaluated regardless of their support document; and (b) we score only on the subset of gold relations that have at least one mention in matched sentences. For evaluation, we use Mintz++, Hoffmann, and MIML implementation from Stanford’s MIMLRE package (Surdeanu et al., 2012) and implement KNN by ourselves. 4.1 Experimental Results 4.1.1 Overall Results We conduct experiments using all baselines and our semantic consistency based method. For our 721 method, we use top 10% weighted training instances. All featu</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Surdeanu, Mihai, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In: Proceedings of EMNLP-CoNLL 2012, pp. 455-465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shingo Takamatsu</author>
<author>Issei Sato</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Reducing wrong labels in distant supervision for relation extraction. In:</title>
<date>2012</date>
<booktitle>ACL 2012,pp.</booktitle>
<pages>721--729</pages>
<contexts>
<context position="4700" citStr="Takamatsu et al. (2012)" startWordPosition="735" endWordPosition="738"> method. Section 4 presents the experiments. Finally Section 5 concludes this paper. 2 Related Work This section briefly reviews the related work. Craven and Kumlien (1999), Wu et al. (2007) and Mintz et al.(2009) were several pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013), Min et al. (2013) and Zha</context>
</contexts>
<marker>Takamatsu, Sato, Nakagawa, 2012</marker>
<rawString>Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for relation extraction. In: ACL 2012,pp. 721-729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Wang</author>
<author>Aditya Kalyanpur</author>
<author>James Fan</author>
<author>Branimir K Boguraev</author>
<author>D C Gondek</author>
</authors>
<title>Relation extraction and scoring in DeepQA. In:</title>
<date>2012</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>56</volume>
<issue>3</issue>
<pages>9--1</pages>
<contexts>
<context position="7169" citStr="Wang et al., 2012" startWordPosition="1144" endWordPosition="1147">nal feature space, a variety of models have been proposed to discover the underlying patterns of Vi. In this paper, we make a simple and effective assumption that the instances of a single relation type can be represented as the linear combination of other instances of the same relation type. This assumption is well motived in relation extraction, because although there is nearly unlimited ways to express a specific relation, in many cases basic principles of economy of expression and/or conventions of genre will ensure that certain systematic ways will be used to express a specific relation (Wang et al., 2012). For example, as shown in (Hearst, 1992), the IS-A relation is usually expressed using several regular patterns, such as “such NP as {NP ,}* {(or |and)} NP” and “NP {, NP}* {,} or other NP”. Based on the above assumption, we hold many instances for each relation type and directly use these instances to model the subspace of a relation type. Specifically, we represent an instance y of ith type as the linear combination of training instances associated with ith type: Y = ai,ivi,l + ai,2Vi,2 + ... + +ai,niVi,ni (1) for some scalars , with j = 1, 2, ...,ni. For example, we can represent the CEO-o</context>
</contexts>
<marker>Wang, Kalyanpur, Fan, Boguraev, Gondek, 2012</marker>
<rawString>Wang, Chang, Aditya Kalyanpur, James Fan, Branimir K. Boguraev, and D. C. Gondek. 2012. Relation extraction and scoring in DeepQA. In: IBM Journal of Research and Development, 56(3.4), pp. 9-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Wang</author>
<author>James Fan</author>
<author>Aditya Kalyanpur</author>
<author>David Gondek</author>
</authors>
<title>Relation extraction with relation topics. In:</title>
<date>2011</date>
<booktitle>Proceedings of EMNLP 2011,</booktitle>
<pages>1426--1436</pages>
<marker>Wang, Fan, Kalyanpur, Gondek, 2011</marker>
<rawString>Wang, Chang, James Fan, Aditya Kalyanpur, and David Gondek. 2011. Relation extraction with relation topics. In: Proceedings of EMNLP 2011, pp. 1426-1436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Wright</author>
<author>Allen Y Yang</author>
<author>Arvind Ganesh</author>
<author>Shankar S Sastry</author>
<author>Yi Ma</author>
</authors>
<title>Robust face recognition via sparse representation. In: Pattern Analysis and Machine Intelligence,</title>
<date>2009</date>
<journal>IEEE Transactions on,</journal>
<volume>31</volume>
<issue>2</issue>
<pages>210--227</pages>
<contexts>
<context position="8989" citStr="Wright et al., 2009" startWordPosition="1483" endWordPosition="1486">pe. 3.2 Local Subspace Modeling via Sparse Representation Based on the above model, the local subspace of an instance is modeled as the linear combination of training instances which can reconstruct the instance. Specifically, to model the local subspace, we first concatenate the n training instances of all k relation types: A = [A1, A2, ..., Ak] 719 Then the local subspace around y is modeled by seeking the solution of the following formula: y = Ax (3) However, because of the redundancy of training instances, Formula 3 usually has more than one solution. In this paper, following the idea in (Wright et al., 2009) for robust face recognition, we use the sparsest solution (i.e., how to reconstruct an instance using minimal training instances), which have been shown is both discriminant and robust to noisiness. Concretely, we seek the sparse linear combination of training instances to reconstruct y by solving: (4) where X = [al,l ..., al � l ..., ai,l ai,2 ...7 a2,ni 7 ...I is a coefficient vector which identifies the spanning instances of y’s local subspace, i.e., the instances whose 𝛼𝑖j≠ 0 . In practice, the training corpus may be too large to direct solve Formula 4. Therefore, this paper uses the K-Ne</context>
<context position="19842" citStr="Wright et al., 2009" startWordPosition="3292" endWordPosition="3295">t neighbors) achieve competitive performance: even the simple KII baseline can achieve a competitive performance (0.277 in F1). This result shows: a) the effectiveness of instance-based subspace modeling; and b) by partitioning subspace into many local subspaces, the subspace model is more adaptive and robust to model prior. 4) The sparse representation is an effective way to model the local subspace using training instances. Compared with KII baseline, our method can achieve a 3.4% F1 improvement. We believe this is because: (1) the discriminative nature of sparse representation as shown in (Wright et al., 2009); and (2) the sparse representation globally seeks the combination of training instances to characterize the local subspace, on contrast KII uses only its nearest neighbor in the training data, which is more easily affected by noisy training instances(e.g., false positives). 4.1.2 Training Instance Selection Results To demonstrate the effect of training instance selection, Table 2 reports our method’s performance using different proportions of training instances. Proportion 5% 10% 20% 100% Best F1 0.282 0.311 0.305 0.280 Table 2. The best F1-measures using different proportions of top weighted</context>
</contexts>
<marker>Wright, Yang, Ganesh, Sastry, Ma, 2009</marker>
<rawString>Wright, John, Allen Y. Yang, Arvind Ganesh, Shankar S. Sastry, and Yi Ma. 2009. Robust face recognition via sparse representation. In: Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(2), 210-227</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying wikipedia. In:</title>
<date>2007</date>
<booktitle>Proceedings of CIKM 2007,pp.</booktitle>
<pages>41--50</pages>
<marker>Wu, Weld, 2007</marker>
<rawString>Wu, Fei, and Daniel S. Weld. 2007. Autonomously semantifying wikipedia. In: Proceedings of CIKM 2007,pp. 41-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Raphael Hoffmann Le Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction. In:</title>
<date>2013</date>
<booktitle>Proceedings of Proceedings of 2013,</booktitle>
<pages>665--670</pages>
<marker>Xu, Le Zhao, Grishman, 2013</marker>
<rawString>Xu, Wei, Raphael Hoffmann Le Zhao, and Ralph Grishman. 2013. Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction. In: Proceedings of Proceedings of 2013, pp. 665-670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allen Y Yang</author>
<author>Shankar S Sastry</author>
<author>Arvind Ganesh</author>
<author>Yi Ma</author>
</authors>
<title>Fast l1-Minimization Algorithms and An Application in Robust Face Recognition: A Review. In:</title>
<date>2010</date>
<booktitle>Proceedings of ICIP</booktitle>
<contexts>
<context position="17596" citStr="Yang et al., 2010" startWordPosition="2926" endWordPosition="2929">ore only on the subset of gold relations that have at least one mention in matched sentences. For evaluation, we use Mintz++, Hoffmann, and MIML implementation from Stanford’s MIMLRE package (Surdeanu et al., 2012) and implement KNN by ourselves. 4.1 Experimental Results 4.1.1 Overall Results We conduct experiments using all baselines and our semantic consistency based method. For our 721 method, we use top 10% weighted training instances. All features occur less than 5 times are filtered. All l1-minimization problems in this paper are solved using the augmented Lagrange multiplier algorithm (Yang et al., 2010), which has been proven is accurate, efficient, and robust. To select the classification threshold for ith relation type, we use the value which can achieve the best F-measure on training dataset (with an additional restriction that precision should &gt; 10%). Figure 4. Precision/recall curves in KBP dataset System Precision Recall F1 Mintz++ 0.260 0.250 0.255 Hoffmann 0.306 0.198 0.241 MIML 0.249 0.314 0.278 KNN 0.261 0.295 0.277 Our method 0.286 0.342 0.311 Table 1. The best F1-measures in KBP dataset Figure 4 shows the precision/recall curves of different systems, and Table 1 shows their best </context>
</contexts>
<marker>Yang, Sastry, Ganesh, Ma, 2010</marker>
<rawString>Yang, Allen Y., Shankar S. Sastry, Arvind Ganesh, and Yi Ma. 2010. Fast l1-Minimization Algorithms and An Application in Robust Face Recognition: A Review. In: Proceedings of ICIP 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective cross-document relation extraction without labelled data. In:</title>
<date>2010</date>
<booktitle>Proceedings of EMNLP</booktitle>
<pages>1013--1023</pages>
<contexts>
<context position="4533" citStr="Yao et al. (2010)" startWordPosition="710" endWordPosition="713">23-25 2014. c�2014 Association for Computational Linguistics This paper is organized as follows. Section 2 reviews related work. Section 3 describes the proposed method. Section 4 presents the experiments. Finally Section 5 concludes this paper. 2 Related Work This section briefly reviews the related work. Craven and Kumlien (1999), Wu et al. (2007) and Mintz et al.(2009) were several pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can </context>
</contexts>
<marker>Yao, Riedel, McCallum, 2010</marker>
<rawString>Yao, Limin, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extraction without labelled data. In: Proceedings of EMNLP 2010, pp. 1013-1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Congle Zhang</author>
<author>Raphael Hoffmann</author>
<author>Daniel S Weld</author>
</authors>
<title>Ontological smoothing for relation extraction with minimal supervision. In:</title>
<date>2012</date>
<booktitle>Proceedings of AAAI 2012,</booktitle>
<pages>157--163</pages>
<marker>Zhang, Hoffmann, Weld, 2012</marker>
<rawString>Zhang, Congle, Raphael Hoffmann, and Daniel S. Weld. 2012. Ontological smoothing for relation extraction with minimal supervision. In: Proceedings of AAAI 2012, pp. 157-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingxing Zhang</author>
<author>Jianwen Zhang</author>
<author>Junyu Zeng</author>
<author>Jun Yan</author>
<author>Zheng Chen</author>
<author>Zhifang Sui</author>
</authors>
<title>Towards Accurate Distant Supervision for Relational Facts Extraction. In:</title>
<date>2013</date>
<booktitle>Proceedings of ACL 2013,</booktitle>
<pages>810--815</pages>
<contexts>
<context position="5316" citStr="Zhang et al. (2013)" startWordPosition="834" endWordPosition="837">12) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013), Min et al. (2013) and Zhang et al. (2013) try to resolve the false negative problem raised by the incomplete knowledge base problem. Hoffmann et al. (2010) and Zhang et al. (2010) try to improve the extraction precision by learning a dynamic lexicon. 3 The Semantic Consistency Model for Relation Extraction In this section, we describe our semantic consistency model for relation extraction. We first model the subspaces of all relation types in the original feature space, then model and characterize the local subspace around an instance, finally estimate the semantic consistency of an instance and exploit it for relation extraction. 3.</context>
</contexts>
<marker>Zhang, Zhang, Zeng, Yan, Chen, Sui, 2013</marker>
<rawString>Zhang, Xingxing, Zhang, Jianwen, Zeng, Junyu, Yan, Jun, Chen, Zheng and Sui, Zhifang. 2013. Towards Accurate Distant Supervision for Relational Facts Extraction. In: Proceedings of ACL 2013, pp. 810-815.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>