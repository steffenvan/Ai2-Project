<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000078">
<title confidence="0.950096">
Human Evaluation of a German Surface Realisation Ranker
</title>
<author confidence="0.90766">
Aoife Cahill
</author>
<affiliation confidence="0.802039333333333">
Institut f¨ur Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
70174 Stuttgart, Germany
</affiliation>
<email confidence="0.990409">
aoife.cahill@ims.uni-stuttgart.de
</email>
<author confidence="0.991171">
Martin Forst
</author>
<affiliation confidence="0.994781">
Palo Alto Research Center
</affiliation>
<address confidence="0.887777">
3333 Coyote Hill Road
Palo Alto, CA 94304, USA
</address>
<email confidence="0.999629">
mforst@parc.com
</email>
<sectionHeader confidence="0.993968" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999147428571429">
In this paper we present a human-based
evaluation of surface realisation alterna-
tives. We examine the relative rankings of
naturally occurring corpus sentences and
automatically generated strings chosen by
statistical models (language model, log-
linear model), as well as the naturalness of
the strings chosen by the log-linear model.
We also investigate to what extent preced-
ing context has an effect on choice. We
show that native speakers do accept quite
some variation in word order, but there are
also clearly factors that make certain real-
isation alternatives more natural.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996225862069">
An important component of research on surface
realisation (the task of generating strings for a
given abstract representation) is evaluation, espe-
cially if we want to be able to compare across sys-
tems. There is consensus that exact match with
respect to an actually observed corpus sentence is
too strict a metric and that BLEU score measured
against corpus sentences can only give a rough im-
pression of the quality of the system output. It is
unclear, however, what kind of metric would be
most suitable for the evaluation of string realisa-
tions, so that, as a result, there have been a range of
automatic metrics applied including inter alia ex-
act match, string edit distance, NIST SSA, BLEU,
NIST, ROUGE, generation string accuracy, gener-
ation tree accuracy, word accuracy (Bangalore et
al., 2000; Callaway, 2003; Nakanishi et al., 2005;
Velldal and Oepen, 2006; Belz and Reiter, 2006).
It is not always clear how appropriate these met-
rics are, especially at the level of individual sen-
tences. Using automatic evaluation metrics cannot
be avoided, but ideally, a metric for the evaluation
of realisation rankers would rank alternative real-
isations in the same way as native speakers of the
language for which the surface realisation system
is developed, and not only globally, but also at the
level of individual sentences.
Another major consideration in evaluation is
what to take as the gold standard. The easiest op-
tion is to take the original corpus string that was
used to produce the abstract representation from
which we generate. However, there may well be
other realisations of the same input that are as
suitable in the given context. Reiter and Sripada
(2002) argue that while we should take advantage
of large corpora in NLG, we also need to take care
that we do not introduce errors by learning from
incorrect data present in corpora.
In order to better understand what makes good
evaluation data (and metrics), we designed and im-
plemented an experiment in which human judges
evaluated German string realisations. The main
aims of this experiment were: (i) to establish how
much variation in German word order is accept-
able for human judges, (ii) to find an automatic
evaluation metric that mirrors the findings of the
human evaluation, (iii) to provide detailed feed-
back for the designers of the surface realisation
ranking model and (iv) to establish what effect
preceding context has on the choice of realisation.
In this paper, we concentrate on points (i) and (iv).
The remainder of the paper is structured as fol-
lows: In Section 2 we outline the realisation rank-
ing system that provided the data for the experi-
ment. In Section 3 we outline the design of the
experiment and in Section 4 we present our find-
ings. In Section 5 we relate this to other work and
finally we conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.993134" genericHeader="introduction">
2 A Realisation Ranking System for
German
</sectionHeader>
<bodyText confidence="0.999771">
We take the realisation ranking system for German
described in Cahill et al. (2007) and present the
output to human judges. One goal of this series
of experiments is to examine whether the results
</bodyText>
<note confidence="0.9230085">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 112–120,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.996667">
112
</page>
<bodyText confidence="0.995218333333333">
based on automatic evaluation metrics published
in that paper are confirmed in an evaluation by hu-
mans. Another goal is to collect data that will al-
low us and other researchers1 to explore more fine-
grained and reliable automatic evaluation metrics
for realisation ranking.
The system presented by Cahill et al. (2007)
ranks the strings generated by a hand-crafted
broad-coverage Lexical Functional Grammar
(Bresnan, 2001) for German (Rohrer and Forst,
2006) on the basis of a given input f-structure.
In these experiments, we use f-structures from
their held-out and test sets, of which 96% can
be associated with surface realisations by the
grammar. F-structures are attribute-value ma-
trices representing grammatical functions and
morphosyntactic features; roughly speaking,
they are predicate-argument structures. In LFG,
f-structures are assumed to be a crosslinguistically
relatively parallel syntactic representation level,
alongside the more surface-oriented c-structures,
which are context-free trees. Figure 1 shows
the f-structure2 associated with TIGER Corpus
sentence 8609, glossed in (1), as well as the 4
string realisations that the German LFG generates
from this f-structure. The LFG is reversible,
i.e. the same grammar is used for parsing as for
generation. It is a hand-crafted grammar, and
has been carefully constructed to only parse (and
therefore generate) grammatical strings.3
¨außerst
extremely
umstritten.
controversial.
‘Williams was extremely controversial in British
politics.’
The ranker consists of a log-linear model that
is based on linguistically informed structural fea-
tures as well as a trigram language model, whose
</bodyText>
<footnote confidence="0.973889">
1The data is available for download from
http://www.ims.uni-stuttgart.de/projekte/pargram/geneval/data/
2Note that only grammatical functions are displayed;
morphosyntactic features are omitted due to space con-
straints. Also note that the discourse function TOPIC was
ignored in generation.
3A ranking mechanism based on so-called optimality
marks can lead to a certain “asymmetry” between parsing and
generation in the sense that not all sentences that can be as-
sociated with a certain f-structure are necessarily generated
from this same f-structure. E.g. the sentence Williams war
¨aulferst umstritten in der britischen Politik. can be parsed
into the f-structure in Figure 1, but it is not generated because
an optimality mark penalizes the extraposition of PPs to the
right of a clause. Only few optimality marks were used in the
process of generating the data for our experiments, so that the
bias they introduce should not be too noticeable.
</footnote>
<bodyText confidence="0.999589588235294">
score is integrated into the model simply as an ad-
ditional feature. The log-linear model is trained on
corpus data, in this case sentences from the TIGER
Corpus (Brants et al., 2002), for which f-structures
are available; the observed corpus sentences are
considered as references whose probability is to
be maximised during the training process.
The output of the realisation ranker is evalu-
ated in terms of exact match and BLEU score,
both measured against the actually observed cor-
pus sentences. In addition to the figures achieved
by the ranker, the corresponding figures achieved
by the employed trigram language model on its
own are given as a baseline, and the exact match
figure of the best possible string selection is given
as an upper bound.4 We summarise these figures
in Table 1.
</bodyText>
<table confidence="0.99747">
Exact Match BLEU score
Language model 27% 0.7306
Log-linear model 37% 0.7939
Upper bound 62% –
</table>
<tableCaption confidence="0.999526">
Table 1: Results achieved by trigram LM ranker
</tableCaption>
<bodyText confidence="0.966058818181818">
and log-linear model ranker in Cahill et al. (2007)
By means of these figures, Cahill et al. (2007)
show that a log-linear model based on structural
features and a language model score performs con-
siderably better realisation ranking than just a lan-
guage model. In our experiments, presented in de-
tail in the following section, we examine whether
human judges confirm this and how natural and/or
acceptable the selection performed by the realisa-
tion ranker under consideration is for German na-
tive speakers.
</bodyText>
<sectionHeader confidence="0.994566" genericHeader="method">
3 Experiment Design
</sectionHeader>
<bodyText confidence="0.999708888888889">
The experiment was divided into three parts. Each
part took between 30 and 45 minutes to complete,
and participants were asked to leave some time
(e.g. a week) between each part. In total, 24 par-
ticipants completed the experiment. All were na-
tive German speakers (mostly from South-Western
Germany) and almost all had a linguistic back-
ground. Table 2 gives a breakdown of the items
in each part of the experiment.5
</bodyText>
<footnote confidence="0.738838875">
4The observed corpus sentence can be (re)generated from
the corresponding f-structure for only 62% of the sentences
used, usually because of differences in punctuation. Hence
this exact match upper bound. An upper bound in terms
of BLEU score cannot be computed because BLEU score is
computed on entire corpora rather than individual sentences.
5Experiments 3a and 3b contained the same items as ex-
periments 1a and 1b.
</footnote>
<figure confidence="0.966759727272727">
britischen
British
Politik
politics
(1) Williams
war
Williams was
in
in
der
the
</figure>
<page confidence="0.984352">
113
</page>
<bodyText confidence="0.9308876">
&amp;quot;Williams war in der britischen Politik äußerst umstritten.&amp;quot;
Williams war in der britischen Politik ¨außerst umstritten.
In der britischen Politik war Williams ¨außerst umstritten.
¨Außerst umstritten war Williams in der britischen Politik.
¨Außerst umstritten war in der britischen Politik Williams.
</bodyText>
<figureCaption confidence="0.999577">
Figure 1: F-structure associated with (1) and strings generated from it.
</figureCaption>
<figure confidence="0.915054807692308">
PRED &apos;sein&lt;[378:umstritten]&gt;[1:Williams]&apos;
SUBJ 1PRED &apos;Williams&apos;
PRED &apos;umstritten&lt;[1:Williams]&gt;&apos;
SUBJ [1:Williams]
XCOMP-PRED
378
PRED &apos;äußerst&apos;
PRED &apos;in&lt;[115:Politik]&gt;&apos;
PRED &apos;Politik&apos;
ADJUNCT
SPEC
PRED &apos;britisch&lt;[115:Politik]&gt;&apos;
SUBJ [115:Politik]
PRED &apos;die&apos;
88
OBJ
DET
171
115
ADJUNCT
TOPIC [1:Williams]
65
ADJUNCT 274
Exp 1a Exp 1b Exp 2
Num. items 44 52 41
Avg. sent length 14.4 12.1 9.4
</figure>
<tableCaption confidence="0.918188">
Table 2: Statistics for each experiment part
</tableCaption>
<subsectionHeader confidence="0.995781">
3.1 Part 1
</subsectionHeader>
<bodyText confidence="0.99993852173913">
The aim of part 1 of the experiment was twofold.
First, to identify the relative rankings of the sys-
tems evaluated in Cahill et al. (2007) according to
the human judges, and second to evaluate the qual-
ity of the strings as chosen by the log-linear model
of Cahill et al. (2007). To these ends, part 1 was
further subdivided into two tasks: 1a and b.
Task 1a: During the first task, participants were
presented with alternative realisations for an input
f-structure (but not shown the original f-structure)
and asked to rank them in order of how natural
sounding they were, 1 being the best and 3 be-
ing the worst.6 Each item contained three alter-
natives, (i) the original string found in TIGER, (ii)
the string chosen as most likely by the trigram lan-
guage model, and (iii) the string chosen as most
likely by the log-linear model. Only items where
each system chose a different alternative were cho-
sen from the evaluation data of Cahill et al. (2007).
The three alternatives were presented in random
order for each item, and the items were presented
in random order for each participant. Some items
were presented randomly to participants more than
</bodyText>
<footnote confidence="0.757677666666667">
6Joint rankings were not allowed, i.e. the participants
were forced to make strict ranking decisions, and in hindsight
this may have introduced some noise into the data.
</footnote>
<bodyText confidence="0.9998405">
once as a sanity check, and in total for Part 1a, par-
ticipants made 52 ranking judgements on 44 items.
Figure 2 shows a screen shot of what the partici-
pant was presented with for this task.
Task 1b: In the second task of part 1, partic-
ipants were presented with the string chosen by
the log-linear model as being the most likely and
asked to evaluate it on a scale from 1 to 5 on how
natural sounding it was, 1 being very unnatural
or marked and 5 being completely natural. Fig-
ure 3 shows a screen shot of what the participant
saw during the experiment. Again some random
items were presented to the participant more than
once, and the items themselves were presented in
random order. In total, the participants made 58
judgements on 52 items.
</bodyText>
<subsectionHeader confidence="0.996434">
3.2 Part 2
</subsectionHeader>
<bodyText confidence="0.999960888888889">
In the second part of the experiment, participants
were presented between 4 and 8 alternative sur-
face realisations for an input f-structure, as well
as some preceding context. This preceding con-
text was automatically determined using informa-
tion from the export release of the TIGER treebank
and was not hand-checked for relevance.7 The par-
ticipants were then asked to choose the realisation
that they felt fit best given the preceding sentences.
</bodyText>
<footnote confidence="0.998443142857143">
7The export release of the TIGER treebank includes an
article ID for each sentence. Unfortunately, this is not com-
pletely reliable for determining relevant context, since an ar-
ticle can also contain several short news snippets which are
completely unrelated. Paragraph boundaries are not marked.
This leads to some noise, which unfortunately is difficult to
measure objectively
</footnote>
<page confidence="0.99181">
114
</page>
<figureCaption confidence="0.999738">
Figure 2: Screenshot of Part 1a of the Experiment
Figure 3: Screenshot of Part 1b of the Experiment
</figureCaption>
<table confidence="0.9992834">
Rank 1 Total Rank 3 Average
Rank 2 Rank
Original String 817 366 65 1.40
LL String 303 593 352 2.04
LM String 128 289 831 2.56
</table>
<tableCaption confidence="0.999682">
Table 3: Task 1a: Ranks for each system
</tableCaption>
<figureCaption confidence="0.97847">
Figure 5: Task 1b: Naturalness scores for strings
chosen by log-linear model, 1=worst
</figureCaption>
<bodyText confidence="0.999777">
The items were presented in random order, and the
list of alternatives were presented in random order
to each participant. Some items were randomly
presented more than once, resulting in 50 judge-
ments on 41 items. Figure 4 shows a screen shot
of what the participant saw.
</bodyText>
<subsectionHeader confidence="0.990391">
3.3 Part 3
</subsectionHeader>
<bodyText confidence="0.954446222222222">
Part 3 of the experiment was identical to Part 1,
except that now, rather than the participants being
presented with sentences in isolation, they were
given some preceding context. The context was
determined automatically, in the same way as in
Part 2. The items themselves were the same as in
Part 1. The aim of this part of the experiment was
to see what effect preceding context had on judge-
ments.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999903">
In this section we present the result and analysis
of the experiments outlined above.
</bodyText>
<subsectionHeader confidence="0.989648">
4.1 How good were the strings?
</subsectionHeader>
<bodyText confidence="0.999976818181818">
The data collected in Experiment 1a showed the
overall human relative ranking of the three sys-
tems. We calculate the total numbers of each
rank for each system. Table 3 summarises the re-
sults. The original string is the string found in the
TIGER Corpus, the LM String is the string cho-
sen as being most likely by the trigram language
model and the LL String is the string chosen as
being most likely by the log-linear model.
Table 3 confirms the overall relative rankings
of the three systems as determined using BLEU
scores. The original TIGER strings are ranked best
(average 1.4), the strings chosen by the log-linear
model are ranked better than the strings chosen by
the language model (average 2.65 vs 2.04).
In Experiment 1b, the aim was to find out how
acceptable the strings chosen by the log-linear
model were, although they were not the same as
the original string. Figure 5 summarises the data.
The graph shows that the majority of strings cho-
sen by the log-linear model ranked very highly on
the naturalness scale.
</bodyText>
<subsectionHeader confidence="0.7566035">
4.2 Did the human judges agree with the
original authors?
</subsectionHeader>
<bodyText confidence="0.999856625">
In Experiment 2, the aim was to find out how of-
ten the human judges chose the same string as the
original author (given alternatives generated by the
LFG grammar). Most items had between 4 and 6
alternative strings. In 70% of all items, the human
judges chose the same string as the original au-
thor. However, the remaining 30% of the time, the
human judges picked an alternative as being the
</bodyText>
<page confidence="0.997389">
115
</page>
<figureCaption confidence="0.999849">
Figure 4: Screenshot of Part 2 of the Experiment
</figureCaption>
<bodyText confidence="0.999723416666667">
most fitting in the given context.8 This suggests
that there is quite some variation in what native
German speakers will accept, but that this varia-
tion is by no means random, as indicated by 70%
of choices being the same string as the original au-
thor’s.
Figure 6 shows for each bin of possible alterna-
tives, the percentage of items with a given num-
ber of choices made. For example, for the items
with 4 possible alternatives, over 70% of the time,
the judges chose between only 2 of them. For the
items with 5 possible alternatives, in 10% of those
items the human judges chose only 1 of those al-
ternatives; in 30% of cases, the human judges all
chose the same 2 solutions, and for the remain-
ing 60% they chose between only 3 of the 5 pos-
sible alternatives. These figures indicate that al-
though judges could not always agree on one best
string, often they were only choosing between 2 or
3 of the possible alternatives. This suggests that,
on the one hand, native speakers do accept quite
some variation, but that, on the other hand, there
are clearly factors that make certain realisation al-
ternatives more preferable than others.
</bodyText>
<figureCaption confidence="0.987071">
Figure 6: Exp 2: Number of Alternatives Chosen
</figureCaption>
<bodyText confidence="0.827574705882353">
8Recall that almost all strings presented to the judges were
grammatical.
The graph in Figure 6 shows that only in two
cases did the human judges choose from among
all possible alternatives. In one case, there were 4
possible alternatives and in the other 6. The origi-
nal sentence that had 4 alternatives is given in (2).
The four alternatives that participants were asked
to choose from are given in Table 4, with the fre-
quency of each choice. The original sentence that
had 6 alternatives is given in (3). The six alterna-
tives generated by the grammar and the frequen-
cies with which they were chosen is given in Table
5.
(2) Die Brandursache blieb zun¨achst unbekannt.
The cause of fire remained initially unknown.
‘The cause of the fire remained unknown initially.’
</bodyText>
<table confidence="0.983982">
Alternative Freq.
Zun¨achst blieb die Brandursache unbekannt. 2
Die Brandursache blieb zun¨achst unbekannt. 24
Unbekannt blieb die Brandursache zun¨achst. 1
Unbekannt blieb zun¨achst die Brandursache. 1
</table>
<tableCaption confidence="0.977888">
Table 4: The 4 alternatives given by the grammar
</tableCaption>
<bodyText confidence="0.98010125">
for (2) and their frequencies
Tables 4 and 5 tell different stories. On the one
hand, although each of the 4 alternatives was cho-
sen at least once from Table 4, there is a clear pref-
erence for one string (and this is also the origi-
nal string from the TIGER Corpus). On the other
hand, there is no clear preference9 for any one of
the alternatives in Table 5, and, in fact, the alterna-
tive that was selected most frequently by the par-
ticipants is not the original string. Interestingly,
out of the 41 items presented to participants, the
original string was chosen by the majority of par-
ticipants in 36 cases. Again, this confirms the
hypothesis that there is a certain amount of ac-
ceptable variation for native speakers but there are
clear preferences for certain strings over others.
</bodyText>
<footnote confidence="0.987908">
9Although it is clear that alternative 2 is dispreferred.
</footnote>
<page confidence="0.996174">
116
</page>
<note confidence="0.5781055">
(3) Die Unternehmensgruppe Tengelmann f¨ordert mit einem sechsstelligen Betrag die Arbeit im brandenburgischen
The group of companies Tengelmann assists with a 6-figure sum the work in of-Brandenburg
</note>
<table confidence="0.994341470588235">
Biosph¨arenreservat Schorfheide.
biosphere reserve Schorfheide.
‘The Tengelmann group of companies is supporting the work at the biosphere reserve in Schorfheide, Brandenburg,
with a 6-figure sum.’
Alternative Freq.
Mit einem sechsstelligen Betrag f¨ordert die Unternehmensgruppe Tengelmann die Arbeit im brandenburgischen
Biosph¨arenreservat Schorfheide. 7
Mit einem sechsstelligen Betrag f¨ordert die Arbeit im brandenburgischen Biosph¨arenreservat Schorfheide 1
die Unternehmensgruppe Tengelmann.
Die Arbeit im brandenburgischen Biosph¨arenreservat Schorfheide f¨ordert die Unternehmensgruppe Tengelmann 4
mit einem sechsstelligen Betrag.
Die Arbeit im brandenburgischen Biosph¨arenreservat Schorfheide f¨ordert mit einem sechsstelligen Betrag 5
die Unternehmensgruppe Tengelmann.
Die Unternehmensgruppe Tengelmann f¨ordert die Arbeit im brandenburgischen Biosph¨arenreservat Schorfheide 5
mit einem sechsstelligen Betrag.
Die Unternehmensgruppe Tengelmann f¨ordert mit einem sechsstelligen Betrag die Arbeit im brandenburgischen
Biosph¨arenreservat Schorfheide. 5
</table>
<tableCaption confidence="0.999857">
Table 5: The 6 alternatives given by the grammar for (3) and their frequencies
</tableCaption>
<subsectionHeader confidence="0.99718">
4.3 Effects of context
</subsectionHeader>
<bodyText confidence="0.999792903225806">
As explained in Section 3.1, Part 3 of our exper-
iment was identical to Part 1, except that the par-
ticipants could see some preceding context. The
aim of this part was to investigate to what extent
discourse factors influence the way in which hu-
man judges evaluate the output of the realisation
ranker. In Task 3a, we expected the original strings
to be ranked (even) higher in context than out of
context; consequently, the ranks of the realisations
selected by the log-linear and the language model
would have to go down. With respect to Task 3b,
we had no particular expectation, but were just in-
terested in seeing whether some preceding context
would affect the evaluation results for the strings
selected as most probable by the log-linear model
ranker in any way.
Table 6 summarises the results of Task 3a. It
shows that, at least overall, our expectation that the
original corpus sentences would be ranked higher
within context than out of context was not borne
out. Actually, they were ranked a bit lower than
they were when presented in isolation, and the
only realisations that are ranked slightly higher
overall are the ones selected by the trigram LM.
The overall results of Task 3b are presented in
Figure 7. Interestingly, although we did not ex-
pect any particular effect of preceding context on
the way the participants would rate the realisa-
tions selected by the log-linear model, the natu-
ralness scores were higher in the condition with
context (Task 3b) than in the one without context
</bodyText>
<table confidence="0.999516">
Rank 1 Total Rank 3 Average
Rank 2 Rank
Original String 810 365 71 1.41
(-7) (-1) (+6) (+0.01)
LL String 274 615 357 2.07
(-29) (+22) (+5) (+0.03)
LM String 162 266 818 2.53
(+34) (-23) (-13) (-0.03)
</table>
<tableCaption confidence="0.9288835">
Table 6: Task 3a: Ranks for each system (com-
pared to ranks in Task 1a)
</tableCaption>
<bodyText confidence="0.999041590909091">
(Task 1b). One explanation might be that sen-
tences in some sort of default order are generally
rated higher in context than out of context, simply
because the context makes sentences less surpris-
ing.
Since, contrary to our expectations, we could
not detect a clear effect of context in the overall re-
sults of Task 3a, we investigated how the average
ranks of the three alternatives presented for indi-
vidual items differ between Task 1a and Task 3a.
An example of an original corpus sentence which
many participants ranked higher in context than in
isolation is given in (4a.). The realisations selected
by the the log-linear model and the trigram LM are
given in (4b.) and (4c.) respectively, and the con-
text shown to the participants is given above these
alternatives. We believe that the context has this
effect because it prepares the reader for the struc-
ture with the sentence-initial predicative partici-
ple entscheidend; usually, these elements appear
rather in clause-final position.
In contrast, (5a) is an example of a corpus
</bodyText>
<page confidence="0.978255">
117
</page>
<figure confidence="0.995808225352112">
Antibabypillen Femovan,
contraceptive pills Femovan,
Lovelle,
Lovelle,
Dimirel.
Dimirel.
(4) -2 Betroffen sind die
Concerned are the
und
and
[...]
[...],
-1 Das
The
Bundesinstitut
federal institute
schliel3t
excludes
aus, dal3
that
sich die
the
Thrombose-Warnung als
thrombosis warning as
grundlos
unfounded
erweisen
turn out
k¨onnte.
could.
nicht
not
a. Entscheidend sei die [...] abschliel3ende Bewertung, sagte J¨urgen Beckmann vom Institut dem ZDF.
Decisive is the [...] final evaluation, said J¨urgen Beckmann of the institute the ZDF.
b. Die [...] abschliel3ende Bewertung sei entscheidend, sagte J¨urgen Beckmann vom Institut dem ZDF.
c. Die [...] abschliel3ende Bewertung sei entscheidend, sagte dem ZDF J¨urgen Beckmann vom Institut.
Fall
In the concrete case
allerdings
however
trotz
despite
der Entscheidung
the decision
der Bundesrichter
of the federal judges
(5) -2 Im konkreten
der
the
Kurde
Kurd
darf
may
in
to
nicht
not
die
the
T¨urkei abgeschoben werden, weil ihm dort nach den Feststellungen der Vorinstanz
Turkey deported be because him there according to the conclusions of the court of lower instance
politische Verfolgung droht.
political persecution threatens.
-1 Es besteht Abschiebeschutz nach dem Ausl¨andergesetz.
It exists deportation protection according to the foreigner law.
a. Der 9. Senat [...] ¨aul3erte sich in seiner Entscheidung nicht zur Verfassungsgem¨al3heit der
The 9th senate [...] expressed itself in its decision not to the constitutionality of the
Drittstaatenregelung.
third-country rule.
b. In seiner Entscheidung ¨aul3erte sich der 9. Senat [...] nicht zur Verfassungsgem¨al3heit der Drittstaatenregelung.
c. Der 9. Senat [...] ¨aul3erte sich in seiner Entscheidung zur Verfassungsgem¨al3heit der Drittstaatenregelung nicht.
</figure>
<figureCaption confidence="0.995837">
Figure 7: Tasks 1b and 3b: Naturalness scores
</figureCaption>
<bodyText confidence="0.957119">
for strings chosen by log-linear model, presented
without and with context
sentence which our participants tended to rank
lower in context than in isolation. Actually, the
human judges preferred the realisation selected
by the trigram LM to the original sentence and
the realisation chosen by the log-linear model in
both conditions, but this preference was even re-
inforced when context was available. One expla-
nation might be that the two preceding sentences
are precisely about the decision to which the ini-
tial phrase of variant (5b) refers, which ensures a
smooth flow of the discourse.
</bodyText>
<subsectionHeader confidence="0.69774">
4.4 Inter-Annotator Agreement
</subsectionHeader>
<bodyText confidence="0.999969291666667">
We measure two types of annotator agreement.
First we measure how well each annotator agrees
with him/herself. This is done by evaluating what
percentage of the time an annotator made the same
choice when presented with the same item choices
(recall that as described in Section 3, a number of
items were presented randomly more than once to
each participant). The results are given in Table 7.
The results show that in between 70% and 74% of
cases, judges make the same decision when pre-
sented with the same data. We found this to be a
surprisingly low number and think that it is most
likely due to the acceptable variation in word or-
der for speakers. Another measure of agreement
is how well the individual participants agree with
each other. In order to establish this, we cal-
culate an average Spearman’s correlation coeffi-
cient (non-parametric Pearson’s correlation coef-
ficient) between each participant for each experi-
ment. The results are summarised in Table 8. Al-
though these figures indicate a high level of inter-
annotator agreement, more tests are required to es-
tablish exactly what these figures mean for each
experiment.
</bodyText>
<sectionHeader confidence="0.999957" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999122">
The work that is most closely related to what is
presented in this paper is that of Velldal (2008). In
</bodyText>
<page confidence="0.992915">
118
</page>
<table confidence="0.999244333333333">
Experiment Agreement (%)
Part 1a 77.43
Part 1b 71.05
Part 2 74.32
Part 3a 72.63
Part 3b 70.89
</table>
<tableCaption confidence="0.863537">
Table 7: How often did a participant make the
same choice?
</tableCaption>
<table confidence="0.999836">
Experiment Spearman coefficient
Part 1a 0.62
Part 1b 0.60
Part 2 0.58
Part 3a 0.61
Part 3b 0.51
</table>
<tableCaption confidence="0.824354">
Table 8: Inter-Annotator Agreement for each ex-
periment
</tableCaption>
<bodyText confidence="0.999894344827586">
his thesis several models of realisation ranking are
presented and evaluated against the original cor-
pus text. Chapter 8 describes a small human-based
experiment, where 7 native English speakers rank
the output of 4 systems. One system is the orig-
inal text, another is a randomly chosen baseline,
another is a string chosen by a log-linear model
and the fourth is one chosen by a language model.
Joint rankings were allowed. The results presented
in Velldal (2008) mirror our findings in Exper-
iments 1a and 3a, that native speakers rank the
original strings higher than the log-linear model
strings which are ranked higher than the language
model strings. In both cases, the log-linear mod-
els include the language model score as a feature
in the log-linear model. Nakanishi et al. (2005) re-
port that they achieve the best BLEU scores when
they do not include the language model score in
their log-linear model, but they also admit that
their language model was not trained on enough
data.
Belz and Reiter (2006) carry out a comparison
of automatic evaluation metrics against human do-
main experts and human non-experts in the do-
main of weather forecast statements. In their eval-
uations, the NIST score correlated more closely
than BLEU or ROUGE to the human judgements.
They conclude that more than 4 reference texts are
needed for automatic evaluation of NLG systems.
</bodyText>
<sectionHeader confidence="0.9913495" genericHeader="evaluation">
6 Conclusion and Outlook to Future
Work
</sectionHeader>
<bodyText confidence="0.999976152173913">
In this paper, we have presented a human-based
experiment to evaluate the output of a realisation
ranking system for German. We evaluated the
original corpus text, and strings chosen by a lan-
guage model and a log-linear model. We found
that, at a global level, the human judgements mir-
rored the relative rankings of the three system ac-
cording to the BLEU score. In terms of natural-
ness, the strings chosen by the log-linear model
were generally given 4 or 5, indicating that al-
though the log-linear model might not choose the
same string as the original author had written, the
strings it was choosing were mostly very natural
strings.
When presented with all alternatives generated
by the grammar for a given input f-structure, the
human judges chose the same string as the origi-
nal author 70% of the time. In 5 out of 41 cases,
the majority of judges chose a string other than
the original string. These figures show that native
speakers accept some variation in word order, and
so caution should be exercised when using corpus-
derived reference data. The observed acceptable
variation was often linked to information struc-
tural considerations, and further experiments will
be carried out to investigate this relationship be-
tween word order and information structure.
In examining the effect of preceding context, we
found that overall context had very little effect. At
the level of individual sentences, however, clear
tendencies were observed, but there were some
sentences which were judged better in context and
others which were ranked lower. This again indi-
cates that corpus-derived reference data should be
used with caution.
An obvious next step is to examine how well
automatic metrics correlate with the human judge-
ments collected, not only at an individual sen-
tence level, but also at a global level. This can be
done using statistical techniques to correlate the
human judgements with the scores from the auto-
matic metrics. We will also examine the sentences
that were consistently judged to be of poor quality,
so that we can provide feedback to the developers
of the log-linear model in terms of possible addi-
tional features for disambiguation.
</bodyText>
<sectionHeader confidence="0.998842" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99999125">
We are extremely grateful to all of our participants
for taking part in this experiment. This work was
partly funded by the Collaborative Research Cen-
tre (SFB 732) at the University of Stuttgart.
</bodyText>
<page confidence="0.998705">
119
</page>
<sectionHeader confidence="0.995874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999340933333334">
Srinivas Bangalore, Owen Rambow, and Steve Whit-
taker. 2000. Evaluation metrics for generation. In
Proceedings of the First International Natural Lan-
guage Generation Conference (INLG2000), pages
1–8, Mitzpe Ramon, Israel.
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 313–320, Trento, Italy.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol, Bulgaria.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic Realisation Ranking for a Free Word Or-
der Language. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 17–24, Saarbr¨ucken, Germany, June. DFKI
GmbH. Document D-07-01.
Charles Callaway. 2003. Evaluating Coverage for
Large Symbolic NLG Grammars. In Proceedings
of the 18th International Joint Conference on Artifi-
cial Intelligence (IJCAI2003), pages 811–817, Aca-
pulco, Mexico.
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2005. Probabilistic models for disambiguation
of an HPSG-based chart generator. In Proceedings
ofIWPT 2005.
Ehud Reiter and Somayajulu Sripada. 2002. Should
Corpora Texts Be Gold Standards for NLG? In Pro-
ceedings ofINLG-02, pages 97–104, Harriman, NY.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC-2006),
Genoa, Italy.
Erik Velldal and Stephan Oepen. 2006. Statistical
ranking in tactical generation. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, Sydney, Australia.
Erik Velldal. 2008. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo.
</reference>
<page confidence="0.996321">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894409">
<title confidence="0.997141">Human Evaluation of a German Surface Realisation Ranker</title>
<author confidence="0.943403">Aoife Cahill</author>
<affiliation confidence="0.999395">Institut f¨ur Maschinelle Sprachverarbeitung (IMS) University of Stuttgart</affiliation>
<address confidence="0.999959">70174 Stuttgart, Germany</address>
<email confidence="0.996542">aoife.cahill@ims.uni-stuttgart.de</email>
<author confidence="0.981398">Martin Forst</author>
<affiliation confidence="0.994141">Palo Alto Research Center</affiliation>
<address confidence="0.998788">3333 Coyote Hill Road Palo Alto, CA 94304, USA</address>
<email confidence="0.999871">mforst@parc.com</email>
<abstract confidence="0.998672866666667">In this paper we present a human-based evaluation of surface realisation alternatives. We examine the relative rankings of naturally occurring corpus sentences and automatically generated strings chosen by statistical models (language model, loglinear model), as well as the naturalness of the strings chosen by the log-linear model. We also investigate to what extent preceding context has an effect on choice. We show that native speakers do accept quite some variation in word order, but there are also clearly factors that make certain realisation alternatives more natural.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
<author>Steve Whittaker</author>
</authors>
<title>Evaluation metrics for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Natural Language Generation Conference (INLG2000),</booktitle>
<pages>1--8</pages>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="1707" citStr="Bangalore et al., 2000" startWordPosition="261" endWordPosition="264"> compare across systems. There is consensus that exact match with respect to an actually observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output. It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006). It is not always clear how appropriate these metrics are, especially at the level of individual sentences. Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences. Another major consideration in evaluation is what to take as the gold st</context>
</contexts>
<marker>Bangalore, Rambow, Whittaker, 2000</marker>
<rawString>Srinivas Bangalore, Owen Rambow, and Steve Whittaker. 2000. Evaluation metrics for generation. In Proceedings of the First International Natural Language Generation Conference (INLG2000), pages 1–8, Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Ehud Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>313--320</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="1796" citStr="Belz and Reiter, 2006" startWordPosition="275" endWordPosition="278">observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output. It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006). It is not always clear how appropriate these metrics are, especially at the level of individual sentences. Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences. Another major consideration in evaluation is what to take as the gold standard. The easiest option is to take the original corpus string that was used to produce</context>
<context position="27780" citStr="Belz and Reiter (2006)" startWordPosition="4519" endWordPosition="4522">model. Joint rankings were allowed. The results presented in Velldal (2008) mirror our findings in Experiments 1a and 3a, that native speakers rank the original strings higher than the log-linear model strings which are ranked higher than the language model strings. In both cases, the log-linear models include the language model score as a feature in the log-linear model. Nakanishi et al. (2005) report that they achieve the best BLEU scores when they do not include the language model score in their log-linear model, but they also admit that their language model was not trained on enough data. Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements. In their evaluations, the NIST score correlated more closely than BLEU or ROUGE to the human judgements. They conclude that more than 4 reference texts are needed for automatic evaluation of NLG systems. 6 Conclusion and Outlook to Future Work In this paper, we have presented a human-based experiment to evaluate the output of a realisation ranking system for German. We evaluated the original corpus text, and strings chosen by a language model </context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 313–320, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="6924" citStr="Brants et al., 2002" startWordPosition="1088" endWordPosition="1091">ily generated from this same f-structure. E.g. the sentence Williams war ¨aulferst umstritten in der britischen Politik. can be parsed into the f-structure in Figure 1, but it is not generated because an optimality mark penalizes the extraposition of PPs to the right of a clause. Only few optimality marks were used in the process of generating the data for our experiments, so that the bias they introduce should not be too noticeable. score is integrated into the model simply as an additional feature. The log-linear model is trained on corpus data, in this case sentences from the TIGER Corpus (Brants et al., 2002), for which f-structures are available; the observed corpus sentences are considered as references whose probability is to be maximised during the training process. The output of the realisation ranker is evaluated in terms of exact match and BLEU score, both measured against the actually observed corpus sentences. In addition to the figures achieved by the ranker, the corresponding figures achieved by the employed trigram language model on its own are given as a baseline, and the exact match figure of the best possible string selection is given as an upper bound.4 We summarise these figures i</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-Functional Syntax.</title>
<date>2001</date>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="4561" citStr="Bresnan, 2001" startWordPosition="740" endWordPosition="741"> the results Proceedings of the 12th Conference of the European Chapter of the ACL, pages 112–120, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 112 based on automatic evaluation metrics published in that paper are confirmed in an evaluation by humans. Another goal is to collect data that will allow us and other researchers1 to explore more finegrained and reliable automatic evaluation metrics for realisation ranking. The system presented by Cahill et al. (2007) ranks the strings generated by a hand-crafted broad-coverage Lexical Functional Grammar (Bresnan, 2001) for German (Rohrer and Forst, 2006) on the basis of a given input f-structure. In these experiments, we use f-structures from their held-out and test sets, of which 96% can be associated with surface realisations by the grammar. F-structures are attribute-value matrices representing grammatical functions and morphosyntactic features; roughly speaking, they are predicate-argument structures. In LFG, f-structures are assumed to be a crosslinguistically relatively parallel syntactic representation level, alongside the more surface-oriented c-structures, which are context-free trees. Figure 1 sho</context>
</contexts>
<marker>Bresnan, 2001</marker>
<rawString>Joan Bresnan. 2001. Lexical-Functional Syntax. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Forst</author>
<author>Christian Rohrer</author>
</authors>
<title>Stochastic Realisation Ranking for a Free Word Order Language.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation,</booktitle>
<pages>17--24</pages>
<publisher>DFKI GmbH. Document</publisher>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="3846" citStr="Cahill et al. (2007)" startWordPosition="625" endWordPosition="628">ce realisation ranking model and (iv) to establish what effect preceding context has on the choice of realisation. In this paper, we concentrate on points (i) and (iv). The remainder of the paper is structured as follows: In Section 2 we outline the realisation ranking system that provided the data for the experiment. In Section 3 we outline the design of the experiment and in Section 4 we present our findings. In Section 5 we relate this to other work and finally we conclude in Section 6. 2 A Realisation Ranking System for German We take the realisation ranking system for German described in Cahill et al. (2007) and present the output to human judges. One goal of this series of experiments is to examine whether the results Proceedings of the 12th Conference of the European Chapter of the ACL, pages 112–120, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 112 based on automatic evaluation metrics published in that paper are confirmed in an evaluation by humans. Another goal is to collect data that will allow us and other researchers1 to explore more finegrained and reliable automatic evaluation metrics for realisation ranking. The system presented by Cahill et</context>
<context position="7728" citStr="Cahill et al. (2007)" startWordPosition="1221" endWordPosition="1224">ealisation ranker is evaluated in terms of exact match and BLEU score, both measured against the actually observed corpus sentences. In addition to the figures achieved by the ranker, the corresponding figures achieved by the employed trigram language model on its own are given as a baseline, and the exact match figure of the best possible string selection is given as an upper bound.4 We summarise these figures in Table 1. Exact Match BLEU score Language model 27% 0.7306 Log-linear model 37% 0.7939 Upper bound 62% – Table 1: Results achieved by trigram LM ranker and log-linear model ranker in Cahill et al. (2007) By means of these figures, Cahill et al. (2007) show that a log-linear model based on structural features and a language model score performs considerably better realisation ranking than just a language model. In our experiments, presented in detail in the following section, we examine whether human judges confirm this and how natural and/or acceptable the selection performed by the realisation ranker under consideration is for German native speakers. 3 Experiment Design The experiment was divided into three parts. Each part took between 30 and 45 minutes to complete, and participants were as</context>
<context position="10081" citStr="Cahill et al. (2007)" startWordPosition="1590" endWordPosition="1593"> (1) and strings generated from it. PRED &apos;sein&lt;[378:umstritten]&gt;[1:Williams]&apos; SUBJ 1PRED &apos;Williams&apos; PRED &apos;umstritten&lt;[1:Williams]&gt;&apos; SUBJ [1:Williams] XCOMP-PRED 378 PRED &apos;äußerst&apos; PRED &apos;in&lt;[115:Politik]&gt;&apos; PRED &apos;Politik&apos; ADJUNCT SPEC PRED &apos;britisch&lt;[115:Politik]&gt;&apos; SUBJ [115:Politik] PRED &apos;die&apos; 88 OBJ DET 171 115 ADJUNCT TOPIC [1:Williams] 65 ADJUNCT 274 Exp 1a Exp 1b Exp 2 Num. items 44 52 41 Avg. sent length 14.4 12.1 9.4 Table 2: Statistics for each experiment part 3.1 Part 1 The aim of part 1 of the experiment was twofold. First, to identify the relative rankings of the systems evaluated in Cahill et al. (2007) according to the human judges, and second to evaluate the quality of the strings as chosen by the log-linear model of Cahill et al. (2007). To these ends, part 1 was further subdivided into two tasks: 1a and b. Task 1a: During the first task, participants were presented with alternative realisations for an input f-structure (but not shown the original f-structure) and asked to rank them in order of how natural sounding they were, 1 being the best and 3 being the worst.6 Each item contained three alternatives, (i) the original string found in TIGER, (ii) the string chosen as most likely by the</context>
</contexts>
<marker>Cahill, Forst, Rohrer, 2007</marker>
<rawString>Aoife Cahill, Martin Forst, and Christian Rohrer. 2007. Stochastic Realisation Ranking for a Free Word Order Language. In Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 17–24, Saarbr¨ucken, Germany, June. DFKI GmbH. Document D-07-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Callaway</author>
</authors>
<title>Evaluating Coverage for Large Symbolic NLG Grammars.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI2003),</booktitle>
<pages>811--817</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="1723" citStr="Callaway, 2003" startWordPosition="265" endWordPosition="266"> There is consensus that exact match with respect to an actually observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output. It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006). It is not always clear how appropriate these metrics are, especially at the level of individual sentences. Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences. Another major consideration in evaluation is what to take as the gold standard. The easi</context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>Charles Callaway. 2003. Evaluating Coverage for Large Symbolic NLG Grammars. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI2003), pages 811–817, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic models for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings ofIWPT</booktitle>
<contexts>
<context position="1747" citStr="Nakanishi et al., 2005" startWordPosition="267" endWordPosition="270">sus that exact match with respect to an actually observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output. It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006). It is not always clear how appropriate these metrics are, especially at the level of individual sentences. Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences. Another major consideration in evaluation is what to take as the gold standard. The easiest option is to take th</context>
<context position="27556" citStr="Nakanishi et al. (2005)" startWordPosition="4479" endWordPosition="4482">here 7 native English speakers rank the output of 4 systems. One system is the original text, another is a randomly chosen baseline, another is a string chosen by a log-linear model and the fourth is one chosen by a language model. Joint rankings were allowed. The results presented in Velldal (2008) mirror our findings in Experiments 1a and 3a, that native speakers rank the original strings higher than the log-linear model strings which are ranked higher than the language model strings. In both cases, the log-linear models include the language model score as a feature in the log-linear model. Nakanishi et al. (2005) report that they achieve the best BLEU scores when they do not include the language model score in their log-linear model, but they also admit that their language model was not trained on enough data. Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements. In their evaluations, the NIST score correlated more closely than BLEU or ROUGE to the human judgements. They conclude that more than 4 reference texts are needed for automatic evaluation of NLG systems. 6 Conclusion and Ou</context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic models for disambiguation of an HPSG-based chart generator. In Proceedings ofIWPT 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
</authors>
<title>Should Corpora Texts Be Gold Standards for NLG?</title>
<date>2002</date>
<booktitle>In Proceedings ofINLG-02,</booktitle>
<pages>97--104</pages>
<location>Harriman, NY.</location>
<contexts>
<context position="2581" citStr="Reiter and Sripada (2002)" startWordPosition="405" endWordPosition="408">d, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences. Another major consideration in evaluation is what to take as the gold standard. The easiest option is to take the original corpus string that was used to produce the abstract representation from which we generate. However, there may well be other realisations of the same input that are as suitable in the given context. Reiter and Sripada (2002) argue that while we should take advantage of large corpora in NLG, we also need to take care that we do not introduce errors by learning from incorrect data present in corpora. In order to better understand what makes good evaluation data (and metrics), we designed and implemented an experiment in which human judges evaluated German string realisations. The main aims of this experiment were: (i) to establish how much variation in German word order is acceptable for human judges, (ii) to find an automatic evaluation metric that mirrors the findings of the human evaluation, (iii) to provide det</context>
</contexts>
<marker>Reiter, Sripada, 2002</marker>
<rawString>Ehud Reiter and Somayajulu Sripada. 2002. Should Corpora Texts Be Gold Standards for NLG? In Proceedings ofINLG-02, pages 97–104, Harriman, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Rohrer</author>
<author>Martin Forst</author>
</authors>
<title>Improving coverage and parsing quality of a large-scale LFG for German.</title>
<date>2006</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC-2006),</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="4597" citStr="Rohrer and Forst, 2006" startWordPosition="744" endWordPosition="747"> the 12th Conference of the European Chapter of the ACL, pages 112–120, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 112 based on automatic evaluation metrics published in that paper are confirmed in an evaluation by humans. Another goal is to collect data that will allow us and other researchers1 to explore more finegrained and reliable automatic evaluation metrics for realisation ranking. The system presented by Cahill et al. (2007) ranks the strings generated by a hand-crafted broad-coverage Lexical Functional Grammar (Bresnan, 2001) for German (Rohrer and Forst, 2006) on the basis of a given input f-structure. In these experiments, we use f-structures from their held-out and test sets, of which 96% can be associated with surface realisations by the grammar. F-structures are attribute-value matrices representing grammatical functions and morphosyntactic features; roughly speaking, they are predicate-argument structures. In LFG, f-structures are assumed to be a crosslinguistically relatively parallel syntactic representation level, alongside the more surface-oriented c-structures, which are context-free trees. Figure 1 shows the f-structure2 associated with </context>
</contexts>
<marker>Rohrer, Forst, 2006</marker>
<rawString>Christian Rohrer and Martin Forst. 2006. Improving coverage and parsing quality of a large-scale LFG for German. In Proceedings of the Language Resources and Evaluation Conference (LREC-2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Statistical ranking in tactical generation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="1772" citStr="Velldal and Oepen, 2006" startWordPosition="271" endWordPosition="274">h respect to an actually observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output. It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006). It is not always clear how appropriate these metrics are, especially at the level of individual sentences. Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences. Another major consideration in evaluation is what to take as the gold standard. The easiest option is to take the original corpus string </context>
</contexts>
<marker>Velldal, Oepen, 2006</marker>
<rawString>Erik Velldal and Stephan Oepen. 2006. Statistical ranking in tactical generation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
</authors>
<title>Empirical Realization Ranking.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Oslo.</institution>
<contexts>
<context position="26456" citStr="Velldal (2008)" startWordPosition="4293" endWordPosition="4294">d order for speakers. Another measure of agreement is how well the individual participants agree with each other. In order to establish this, we calculate an average Spearman’s correlation coefficient (non-parametric Pearson’s correlation coefficient) between each participant for each experiment. The results are summarised in Table 8. Although these figures indicate a high level of interannotator agreement, more tests are required to establish exactly what these figures mean for each experiment. 5 Related Work The work that is most closely related to what is presented in this paper is that of Velldal (2008). In 118 Experiment Agreement (%) Part 1a 77.43 Part 1b 71.05 Part 2 74.32 Part 3a 72.63 Part 3b 70.89 Table 7: How often did a participant make the same choice? Experiment Spearman coefficient Part 1a 0.62 Part 1b 0.60 Part 2 0.58 Part 3a 0.61 Part 3b 0.51 Table 8: Inter-Annotator Agreement for each experiment his thesis several models of realisation ranking are presented and evaluated against the original corpus text. Chapter 8 describes a small human-based experiment, where 7 native English speakers rank the output of 4 systems. One system is the original text, another is a randomly chosen </context>
</contexts>
<marker>Velldal, 2008</marker>
<rawString>Erik Velldal. 2008. Empirical Realization Ranking. Ph.D. thesis, University of Oslo.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>