<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.997941">
Abductive Explanation-based Learning Improves Parsing Accuracy and
Efficiency
</title>
<author confidence="0.962346">
Oliver Streiter
</author>
<affiliation confidence="0.923438">
Language and Law, European Academy, Bolzano, Italy
</affiliation>
<email confidence="0.998215">
ostreiter@eurac.edu
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877333333333">
Natural language parsing has to be accu-
rate and quick. Explanation-based Learn-
ing (EBL) is a technique to speed-up pars-
ing. The accuracy however often declines
with EBL. The paper shows that this accu-
racy loss is not due to the EBL framework
as such, but to deductive parsing. Abduc-
tive EBL allows extending the deductive
closure of the parser. We present a Chi-
nese parser based on abduction. Exper-
iments show improvements in accuracy
and efficiency.1
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998586357142857">
The difficulties of natural language parsing, in gen-
eral, and of parsing Chinese, in particular, are due to
local ambiguities of words and phrases. Extensive
linguistic and non-linguistic knowledge is required
for their resolution (Chang, 1994; Chen, 1996). Dif-
ferent parsing approaches provide different types of
knowledge. Example-based parsing approaches of-
fer rich syntagmatic contexts for disambiguation,
richer than rule-based approaches do (Yuang et al.,
1992). Statistical approaches to parsing acquire
mainly paradigmatic knowledge and require larger
corpora, c.f. (Carl and Langlais, 2003). Statisti-
cal approaches handle unseen events via smoothing.
Rule-based approaches use abstract category labels.
</bodyText>
<footnote confidence="0.988258666666667">
1This research has been carried out within Logos Gaias
project, which integrates NLP technologies into a Internet-
based natural language learning platform (Streiter et al., 2003).
</footnote>
<bodyText confidence="0.99960225">
Example-based parsing generalizes examples dur-
ing compilation time, e.g. (Bod and Kaplan, 1998),
or performs a similarity-based fuzzy match during
runtime (Zavrel and Daelemans, 1997). Both tech-
niques may be computationally demanding, their ef-
fect on parsing however is quite different, c.f. (Stre-
iter, 2002a).
Explanation-based learning (EBL) is a method to
speed-up rule-based parsing via the caching of ex-
amples. EBL however trades speed for accuracy.
For many systems, a small loss in accuracy is accept-
able if an order of magnitude less computing time
is required. Apart from speed, one generally rec-
ognizes that EBL acquires some kind of knowledge
from texts. However, what is this knowledge like
if it does not help with parsing? Couldn’t a system
improve by learning its own output? Can a system
learn to parse Chinese by parsing Chinese? The pa-
per sets out to tackle these questions in theory and
practice.
</bodyText>
<subsectionHeader confidence="0.992772">
1.1 Explanation-based Learning (EBL)
</subsectionHeader>
<bodyText confidence="0.999953586206897">
Explanation-based learning techniques transform a
general problem solver (PS) into a specific and op-
erational PS (Mitchel et al., 1986). The caching of
the general PS’s output accounts for this transfor-
mation. The PS generates, besides the output, a doc-
umentation of the reasoning steps involved (the ex-
planation). This determines which output the system
will cache.
The utility problem questions the claim of
speeding-up applications (Minton, 1990): Retriev-
ing cached solutions in addition to regular process-
ing requires extra time. If retrieval is slow and
cached solutions are rarely re-used, the cost-benefit
ratio is negative.
The accuracy of the derived PS is generally be-
low that of the general PS. This may be due to the
EBL framework as such or the deductive base of
the PS. Research in abductive EBL (A-EBL) seems
to suggest the latter: A-EBL has the potential to
acquire new knowledge (Dimopoulos and Kakas,
1996). The relation between knowledge and accu-
racy however is not a direct and logical one. The
U-shaped language learning curves in children ex-
emplifies the indirect relation (Marcus et al., 1992).
Wrong regular word forms supplant correct irregu-
lar forms when rules are learned. We therefore can-
not simply equate automatic knowledge acquisition
and accuracy improvement, in particular for com-
plex language tasks.
</bodyText>
<subsectionHeader confidence="0.952988">
1.2 EBL and Natural Language Parsing
</subsectionHeader>
<bodyText confidence="0.99989071875">
Previous research has applied EBL for the speed-up
of large and slow grammars. Sentences are parsed.
Then the parse trees are filtered and cached. Sub-
sequent parsing uses the cached trees. A com-
plex HPSG-grammar transforms into tree-structures
with instantiated values (Neumann, 1994). One
hash table lookup of POS-sequences replaces typed-
feature unification. Experiments conducted in EBL-
augmented parsing consistently report a speed-up
of the parser and a drop in accuracy (Rayner and
Samuelsson, 1994; Srinivas and Joshi, 1995).
A loss of information may explain the drop of ac-
curacy. Contextual information, taken into account
by the original parser, may be unavailable in the
new operational format (Sima’an, 1997), especially
if partial, context-dependent solutions are retrieved.
In addition, the set of cached parse trees, judged to
be ”sure to cache”, is necessarily biased (Streiter,
2002b). Most cached tree structures are short noun
phrases. Parsing from biased examples will bias the
parsing.
A further reason for the loss in accuracy are incor-
rect parses which leak into the cache. A stricter filter
does not solve the problem. It increases the bias in
the cache, reduces the size of the cache, and evokes
the utility problem.
EBL actually can improve parsing accuracy (Stre-
iter, 2002b) if the grammar does not derive the
parses to be cached via deduction but via abduction.
The deductive closure2 which cannot increase with
EBL from deductive parsing may increase with ab-
ductive parsing.
</bodyText>
<sectionHeader confidence="0.828162" genericHeader="method">
2 A Formal View on Parsing and Learning
</sectionHeader>
<bodyText confidence="0.9196868">
We use the following notation throughout the paper:
(function applied to yields x),
(relation applied to yields x).
and represent tuples and sets respec-
tively. The prefix denotes the cardinality of a col-
lection, e.g. .
Uppercase variables stand for collections and
lowercase variables for elements. Collections may
contain the anonymous variable (the variable _ in
PROLOG). Over-braces or under-braces should fa-
cilitate reading: .
A theory is where is a set of
rules . and are two disjoint sets of attributes
and (e.g.
). A rule is written as
or . A rule specifies the rela-
tion between an observable fact and an attribute
assigned to it. is the set of observable data with
each being a tuple .3
is the set of data classified according to✹, with
. , and may have an internal struc-
ture in the form of ordered or unordered collections
of more elementary , and respectively.
Transferring this notation to the description of
parsing, is a syntactic formalism and a gram-
mar. is the union of syntax trees and morpho-
syntactic tags. is a corpus tagged with . cor-
responds to a list of words, phrases or sentences (the
surface strings). is a treebank, a cache of parse
trees, or a history of explanations.
</bodyText>
<subsectionHeader confidence="0.923012">
2.1 Parsing:
</subsectionHeader>
<bodyText confidence="0.948768666666667">
A parser defines a relation between and (c.f.
2). Parsing is a relation between and a subset of
(c.f. 3).
</bodyText>
<footnote confidence="0.9993">
2The deductive closure of the set of axioms is the set
which can be proved from it.
3The formalization follows (Dimopoulos and Kakas, 1996).
</footnote>
<equation confidence="0.780699666666667">
(9)
(10)
(3)
</equation>
<bodyText confidence="0.999171">
Simplifying, we can assume that is defined as
the set of rules, i.e. . A spe-
cific parser is derived by the application of to the
training material (e.g. ): . The set of
possible relations is . Elements of are caching
(no generalization), induction (hypothesis after data
inspection) and abduction (hypothesis during classi-
fication). Equation (5) describes the cycle of gram-
mar learning and grammar application.
</bodyText>
<subsubsectionHeader confidence="0.946674">
2.1.1 Memory-based Parsing
</subsubsectionHeader>
<bodyText confidence="0.951925125">
is based on memory if
. in (6) is the trivial formalization of
caching. Parsing proceeds via recalling defined
in (7). The cycle of grammar learning and parsing
is defined in (8): The training material
yields the parsing output .4
parsing
learning from
</bodyText>
<subsubsectionHeader confidence="0.931137">
2.1.2 Deduction-based Parsing
</subsubsectionHeader>
<bodyText confidence="0.969827416666667">
Let be a function which replaces one or
more elements of a collection by a named variable
or . is a deductive inference if is obtained
from an induction (a reduction of with the help of
). The following expressions define induction
(9), deduction (10) and the inductive-deductive
cycle (11):
4We use subscripts to indicate the identity of variables. The
same subscript of two variables implies the identity of both vari-
ables. Different subscripts imply nothing. The variables may
be identical or not identical. In memory-based parsing, learning
material and parsing output are identical.
</bodyText>
<figure confidence="0.592677">
parsing
(11)
2.1.3 Abduction-based Parsing
Abduction, defined as is a run-
time generalization which is triggered by a concrete
to be classified. We separate and for presen-
tation purpose only.5 The relation may express a
similarity, a temporal or causal relation. (12) and the
cycle of (13) define abduction.
parsing
learning from
(13)
</figure>
<bodyText confidence="0.99528175">
Abduction subsumes reasoning by analogy. Ab-
duction is an analogy, if describes a similarity.
Reasoning from rain to snow is a typical analogy.
Reasoning from wet street to rain is an abductive
reasoning. For a parsing approach based on analogy
c.f. (Lepage, 1999).
5Abduction is a process of hypothesis generation. Deduc-
tion and abduction may work conjointly whenever deductive in-
ferences encounter gaps. A deductive inference stops in front of
a gap between the premises and a possible conclusion. Abduc-
tion creates a new hypothesis, which allows to bridge the gap
and to continue the inference.
</bodyText>
<figure confidence="0.5152405">
(8)
(12)
</figure>
<subsectionHeader confidence="0.974426">
2.2 Learning:
</subsectionHeader>
<bodyText confidence="0.999864625">
In this section, we formalize EBL. We mechanically
substitute in the definition of EBL by to
show their learning potentials.
A learning system changes internal states, which
influence the performance. The internal states of
are determined by and . We assume that, for a
given , remains identical before and after learn-
ing. Therefore, the comparison of (before learn-
ing) with (after learning) reveals the ac-
quired knowledge.
We define EBL in (14). is the parser
before learning. This parser applies to and yields
, formalized as . The new
parser is the application of to the union of and
From two otherwise identical parsers, the parser
with not present in the other
has a greater deductive closure. The cardinality of
reflects an empirical knowl-
edge. The empirical knowledge does not allow to
conclude something new, but to resolve ambigui-
ties in accordance with observed data, e.g. for a
sub-language as shown in (Rayner and Samuelsson,
1994). Both learning techniques have the potential
of improving the accuracy.
</bodyText>
<subsectionHeader confidence="0.940023">
2.2.1 Learning through Parsing
</subsectionHeader>
<bodyText confidence="0.994299666666667">
A substitution of with reveals the trans-
formation of to . We start with caching and
recalling (Equation 15).
</bodyText>
<equation confidence="0.57372">
(15)
</equation>
<bodyText confidence="0.4869625">
Parsing with the cache of yields . The de-
ductive closure is not enlarged. Quantitative rela-
tions with respect to change in . If is not cached
twice, memory-based EBL is idempotent.6
</bodyText>
<footnote confidence="0.965988">
6Idempotence is the property of an operation that results in
the same state no matter how many times it is executed.
</footnote>
<bodyText confidence="0.989569166666667">
EBL with induction and deduction is shown in
(16). Here the subscripts merit special attention:
is parsed from
. This yields . In-
tegrating into C changes the empirical knowl-
edge with respect to and . If the empirical
knowledge does not influence ,D-EBL is idem-
potent. The deductive closure does not increase as
Abductive EBL (A-EBL) is shown in (17). A-
EBL acquires empirical knowledge similarly to D-
EBL. In addition, a new
learning
</bodyText>
<subsectionHeader confidence="0.738562">
2.2.2 Recursive Rule Application
</subsectionHeader>
<bodyText confidence="0.983796272727273">
Parsing is a classification task in which is
assigned to . Differently from typical classifi-
cation tasks in machine learning, natural language
parsing requires an open set . This is obtained
via the recursive application of , which unlike
non-recursive styles of analysis (Srinivas and Joshi,
1999) yields (syntax trees) of any complexity.
Then is applied to so that
can be matched by further rules (c.f. 18). With-
out this reduction, recursive parsing could not go be-
yond memory-based parsing.
</bodyText>
<figure confidence="0.807493166666667">
.
.
is ac-
(14) quired. This may differ from with respect
to and/or . In the experiments in A-EBL we
reported below, and holds.
</figure>
<figureCaption confidence="0.991905">
Figure 1: An explanation produced by OCTOPUS. At the top, the final parse obtained via deductive substi-
tutions. Abductive term identification bridges gaps in the deduction (X Y). The marker ’?’ is a graphical
</figureCaption>
<bodyText confidence="0.993403809523809">
shortcut for the set of lexemes in .
The function defines an induction and re-
cursive parsing is thus a deduction. Combinations of
memory-based and deduction-based parsing are de-
ductions, combinations of abduction-based parsing
with any another parsing are abductions.
Macro Learning is the common term for the com-
bination of EBL with recursive deduction (Tade-
palli, 1991). A macro is a rule which
yields the same result as a set of rules with
and does. In terms of
a grammar, such macros correspond to redundant
phrases, i.e. phrases that are obtained by composing
smaller phrases of . Macros represent shortcuts
for the parser and, possibly, improved likelihood es-
timate of the composed structure compared to the
estimates under independency assumption (Abney,
1996). When the usage of macros excludes certain
types of analysis, e.g. by trying to find longest/best
matches we can speak of pruning. This is the contri-
bution of D-EBL for parsing.
</bodyText>
<sectionHeader confidence="0.999381" genericHeader="method">
3 Experiments in EBL
</sectionHeader>
<subsectionHeader confidence="0.999933">
3.1 Experimental purpose and setup
</subsectionHeader>
<bodyText confidence="0.933635125">
The aim of the experiments is to verify whether new
knowledge is acquired in A-EBL and D-EBL. Sec-
ondly, we want to test the influence of new knowl-
edge on parsing accuracy and speed.
The general setup of the experiment is the follow-
ing. We use a section of a treebank as seed-corpus
( ). We train the seed-corpus to a corpus-based
parser. Using a test-corpus we establish the parsing
</bodyText>
<figureCaption confidence="0.544918666666667">
Figure 2: The main parsing algorithm of OCTO-
PUS. The parser interleaves memory-based, deduc-
tive, and abductive parsing strategies in five steps:
Recalling, non-recursive deduction, deduction via
chunk substitution, first with lexemes, then without
lexemes and finally abduction.
</figureCaption>
<figure confidence="0.8833535">
# 1 recalling from POS (a) and lexeme (i)
RETURN IF ( )
# 2 deduction on the basis of POS (a)
RETURN IF ( )
# 3 deductive, recursive parsing with POS and lexeme
# Substitutions are defined as in TAGs (Joshi, 2003) IF
(
)
# 4a deductive recursive parsing with lexeme,
# 4b compared to abductive parsing
IF (
)
RETURN (
, #abduction
#deduction
))
# 5 abduction as robust parsing solution
RETURN
RETURN # deduction
)
</figure>
<figureCaption confidence="0.986409">
Figure 3: Abductive parsing with k-nn retrieval and
adaptation of retrieved examples.
</figureCaption>
<bodyText confidence="0.99775625">
are obtained by parsing step 4a or 5 where only
one POS-labels may be different in the last char-
acters (e.g. ). The re-
sulting corpora are and
</bodyText>
<equation confidence="0.7669065">
.
RETURN
</equation>
<bodyText confidence="0.999287794117647">
accuracy and speed of the parser (
(recall,precision,f-score,time)). Then, we
parse a large corpus ( ). A
filter criterion that works on the explanation ap-
plies. We train those trees which pass the filter
to the parser ( ).
Then the parsing accuracy and speed is tested
against the same training corpus (
(recall,precision,f-score,time)).
Sections of the Chinese Sinica Treebank (Huang
et al., 2000) are used as seed-treebank and gold stan-
dard for parsing evaluation. Seed-corpora range be-
tween 1.000 and 20.000 trees. We train them to
the parser OCTOPUS (Streiter, 2002a). This parser
integrates memory- deduction- and abduction-based
parsing in a hierarchy of preferences, starting from
1 memory-based parsing, 2 non-recursive deductive
parsing, 3 recursive deductive parsing and 5 finally
abductive parsing (Fig. 2).
Learning the seed corpora ( )
results in
1992).
For every the parser produces one parse-
tree and an explanation. The expla-
nation has the form of a derivation tree in TAGs, c.f
(Joshi, 2003). The deduction and abduction steps are
visible in the explanation. Filters apply on the ex-
planation and create sub-corpora that belong to one
inference type.
The first filter requires the explanation to contain
only one non-recursive deduction, i.e. only pars-
ing step 2. As deductive parsing is attempted after
memory-based parsing (1), holds.
A second filter extracts those structures, which
</bodyText>
<subsectionHeader confidence="0.998873">
3.2 The Acquired Knowledge
</subsectionHeader>
<bodyText confidence="0.9951588">
We want to know whether or not new knowledge has
been acquired and what the nature of this acquired
knowledge is. As parsing was not recursive, we can
approach the closure by the types of POS-sequences
from all trees and their subtrees in a corpus. We con-
trast this with to the types of lexeme-sequences. The
data show that only A-EBL increases the closure.
But even when looking at lexemes, i.e. empirical
knowledge, the A-EBL acquires richer information
than D-EBL does.
</bodyText>
<figureCaption confidence="0.964081">
Figure 4: The number of types of POS-sequences as
</figureCaption>
<bodyText confidence="0.974636666666667">
approximation of the closure with , A-EBL and
D-EBL. Below the number of type of LEXEME-
sequences.
</bodyText>
<subsectionHeader confidence="0.694616">
C_seed
C_seed + C_A
C_seed + C_D
</subsectionHeader>
<bodyText confidence="0.989254142857143">
0✡ 5000 10000✡ 15000✡ 20000✡ 25000✡ 30000✡ 35000✡
size of seed� co rp us
The representatives of the cached parses is gauged
by the percentage of top NPs and VPs (including Ss)
as top-nodes. Fig 5 shows the bias of cached parses
which is more pronounced with D-EBL than with
A-EBL.
</bodyText>
<figure confidence="0.9721182">
40000
closure: number of POS−sequences
30000
20000
10000
</figure>
<figureCaption confidence="0.642936">
closure with C_seed
closure with C_seed + C_A
closure with C_seed + C_D
</figureCaption>
<figure confidence="0.8757705625">
0
0� 5000 10000✟ 15000 20000 25000 30000
size of seed corpus
. For each
, a POS tagged corpus with
is parsed, producing the corpora
. The corpus used is a subset of
the 5 Million word Sinica Corpus (Huang and Chen,
number of LEXEME−sequences 70000
60000
50000
40000
30000
20000
10000
0
</figure>
<figureCaption confidence="0.8708855">
Figure 5: The proportion of top-NPs and top-VP(S)
in abduced and deduced corpora.
</figureCaption>
<subsectionHeader confidence="0.999358">
3.3 Evaluating Parsing
</subsectionHeader>
<bodyText confidence="0.874467">
The experiments consist in evaluating
the parsing accuracy and speed for each
.
</bodyText>
<figureCaption confidence="0.95605">
Figure 6: The parsing accuracy with abductive EBL
</figureCaption>
<bodyText confidence="0.997046523809524">
( ) and deductive EBL ( ).
We test the parsing accuracy on 300 untrained and
randomly selected sentences using the f-score on un-
labeled dependency relations. Fig. 6 shows parsing
accuracy depending on the size of the seed-corpus.
The graphs show side branches where we introduce
the EBL-derived training material. This allows com-
paring the effect of A-EBL, D-EBL and hand-coded
trees (the baseline). Fig. 7 shows the parsing speed
in words per second (Processor:1000 MHz, Mem-
ory:128 MB) for the same experiments. Rising lines
indicate a speed-up in parsing. We have interpolated
and smoothed the curves.
The experimental results confirm the drop in pars-
ing accuracy with D-EBL. This fact is consistent
across all experiments. With A-EBL, the parsing ac-
curacy increases beyond the level of departure.
The data also show a speed-up in parsing. This
speed-up is more pronounced and less data-hungry
with A-EBL. Improving accuracy and efficiency are
thus not mutually exclusive, at least for A-EBL.
</bodyText>
<sectionHeader confidence="0.997906" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999779142857143">
Explanation-based Learning has been used to speed-
up natural language parsing. We show that the
loss in accuracy results from the deductive basis of
parsers, not the EBL framework. D-EBL does not
extend the deductive closure and acquires only em-
pirical (disambiguation) knowledge. The accuracy
declines due to cached errors, the statistical bias the
filters introduce and the usage of shortcuts with lim-
ited contextual information.
Alternatively, if the parser uses abduction, the de-
ductive closure of the parser enlarges. This makes
accuracy improvements possible - not a logical con-
sequence. In practice, the extended deductive clo-
sure compensates for negative factors such as wrong
parses or unbalanced distributions in the cache.
On a more abstract level, the paper treats the prob-
lem of automatic knowledge acquisition for Chi-
nese NLP. Theory and practice show that abduction-
based NLP applications acquire new knowledge and
increase accuracy and speed. Future research will
maximize the gains.
</bodyText>
<figure confidence="0.952172048780488">
0� 5000 10000 15000 20000
size of seed corpus
80.00
60.00
40.00
✗
20.00
0.00
% top−NP in C_D
% top−NP in C_A
% top−VP in C_D
% top−VP in C_A
% top−NP standard
% top−VP standard
0 5000✒ 10000✒ 15000✒ 20000✒ 25000✒
size of seed� corpus
coverage (f−score)
0.74
0.73
0.72
0.71
0.70
0.69
0.68
parsing accuracy with C_seed
parsing�accuracy with C_seed+C_A
parsing accuracy with C_seed + C_D
0 5000✕ 10000✕ 15000✕ 20000✕ 25000✕
size of seed� corpus
parsing speed with C_seed
parsing speed with C see_d + C_A
parsingspeed with C_seed + C_D
54.00
52.00
50.00
words per second
�
e
48.00
46.00
44.00
</figure>
<figureCaption confidence="0.999754">
Figure 7: The parsing time with A-EBL (
</figureCaption>
<bodyText confidence="0.554342">
) and D-EBL ( ).
</bodyText>
<sectionHeader confidence="0.979868" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833683673469">
Steven Abney. 1996. Partial Parsing via Finite-State Cas-
cades. In Proceedings of the ESSLLI ’96 Robust Pars-
ing Workshop.
Rens Bod and Ronald M. Kaplan. 1998. A probabilistic
corpus-driven model for lexical-functional analysis. In
COLING-ACL’98.
Michael Carl and Philippe Langlais. 2003. Tuning gen-
eral translation knowledge to a sublanguage. In Pro-
ceedings of CLAW 2003, Dublin, Ireland, May, 15-17.
Hsing-Wu Chang. 1994. Word segmentation and sen-
tence parsing in reading Chinese. In Advances in the
Study of Chinese Language Processing, National Tai-
wan University, Taipei.
Keh-Jiann Chen. 1996. A model for robust Chinese
parser. Computational Linguistics and Chinese Lan-
guage, 1(1):183–204.
Yanis Dimopoulos and Antonis Kakas. 1996. Abduc-
tion and inductive learning. In L. De Taedt, editor, Ad-
vances in Inductive Logic Programming, pages 144–
171. IOS Press.
Chu-Ren Huang and Keh-Jiann Chen. 1992. A Chi-
nese corpus for linguistics research. In COLING’92,
Nantes, France.
Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao-
ming Gao, and Kuang-Yu Chen. 2000. Sinica tree-
bank: Design criteria, annotation guidelines and on-
line interface. In M. Palmer, M. Marcus, A. K. Joshi,
and F. Xia, editors, Proceedings of the Second Chinese
Language Processing Workshop, Hong Kong, October.
ACL.
Aravind K. Joshi. 2003. Tree-adjoining grammars. In
R. Mitkov, editor, The Oxford Handbook of Computa-
tional Linguistics. Oxford University Press, Oxford.
Yves Lepage. 1999. Open set experiments with direct
analysis by analogy. In Proceedings NLPRS’99 (Nat-
ural Language Processing Pacific Rim Symposium),
pages 363–368, Beijing.
Gary F. Marcus, Steven Pinker, Michael Ullman,
Michelle Hollander, John T. Rosen, and Fei Xu. 1992.
Overregularization in Language Learning. Mono-
graphs of the Society for Research in Child Develop-
ment, 57 (No. 4, Serial No. 228).
Steven Minton. 1990. Quantitative results concerning
the utility problem of explanation-based learning. Ar-
tificial Intelligence, 42:363–393.
Tom S. Mitchel, R. Keller, and S. Kedar-Cabelli. 1986.
Explanation-based generalization: A unifying view.
Machine Learning, 1(1).
G¨unter Neumann. 1994. Application of explanation-
based learning for efficient processing of constraint-
based grammars. In The 10th Conference on Artificial
Intelligence for Applications, San Antonio, Texas.
Manny Rayner and Christer Samuelsson. 1994. Corpus-
based grammar specification for fast analysis. In
Spoken Language Translator: First Year Report, SRI
Technical Report CRC-043, pg. 41-54.
Khalil Sima’an. 1997. Explanation-based leaning
of partial-parsers. In W. Daelemans, A. van den
Bosch, and A. Weijters, editors, Workshop Notes of
the ECML/ML Workshop on Empirical Learning of
Natural Language Processing Tasks, pages 137–146,
Prague, Czech Republic, April.
Bangalore Srinivas and Aravind K. Joshi. 1995. Some
novel applications of explanation-based learning to
parsing lexicalized tree-adjoining grammars. In 33th
Annual Meeting of the ACL, Cambridge, MA.
Bangalore Srinivas and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Compu-
tational Linguistics, 25(2):237–265.
Oliver Streiter, Judith Knapp, and Leonhard Voltmer.
2003. Gymn@zilla: A browser-like repository for
open learning resources. In ED-Media, World Con-
ference on Educational Multimedia, Hypermedia &amp;
Telecommunications, Honolulu, Hawaii, June, 23-28.
Oliver Streiter. 2002a. Abduction, induction and memo-
rizing in corpus-based parsing. In ESSLLI-2002 Work-
shop on ”Machine Learning Approaches in Computa-
tional Linguistics”, pages 73–90, Trento, Italy, August
5-9.
Oliver Streiter. 2002b. Treebank development with de-
ductive and abductive explanation-based learning: Ex-
ploratory experiments. In Workshop on Treebanks and
Linguistic Theories 2002, Sozopol, Bulgaria, Septem-
ber 20-21.
Prasad Tadepalli. 1991. A formalization of explanation-
based macro-operator learning. In IJCAI, Proceedings
of the International Joint Conference of Artificial In-
telligence, pages 616–622, Sydney, Australia. Morgan
Kaufmann.
Chunfa Yuang, Changming Huang, and Shimei Pan.
1992. Knowledge acquisition and Chinese parsing
based on corpus. In COLING’92.
Jakub Zavrel and Walter Daelemans. 1997. Memory-
based learning: Using similarity for smoothing. In
W. Daelemans, A. van den Bosch, and A. Weijters, ed-
itors, Workshop Notes of the ECML/ML Workshop on
Empirical Learning of Natural Language Processing
Tasks, pages 71–84, Prague, Czech Republic, April.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.537822">
<title confidence="0.9816585">Abductive Explanation-based Learning Improves Parsing Accuracy and Efficiency</title>
<author confidence="0.992897">Oliver Streiter</author>
<affiliation confidence="0.885632">Language and Law, European Academy, Bolzano,</affiliation>
<email confidence="0.99987">ostreiter@eurac.edu</email>
<abstract confidence="0.998796909090909">Natural language parsing has to be accurate and quick. Explanation-based Learning (EBL) is a technique to speed-up parsing. The accuracy however often declines with EBL. The paper shows that this accuracy loss is not due to the EBL framework as such, but to deductive parsing. Abductive EBL allows extending the deductive closure of the parser. We present a Chinese parser based on abduction. Exper-</abstract>
<intro confidence="0.635288">iments show improvements in accuracy</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial Parsing via Finite-State Cascades.</title>
<date>1996</date>
<booktitle>In Proceedings of the ESSLLI ’96 Robust Parsing Workshop.</booktitle>
<contexts>
<context position="12650" citStr="Abney, 1996" startWordPosition="2042" endWordPosition="2043">n-based parsing are deductions, combinations of abduction-based parsing with any another parsing are abductions. Macro Learning is the common term for the combination of EBL with recursive deduction (Tadepalli, 1991). A macro is a rule which yields the same result as a set of rules with and does. In terms of a grammar, such macros correspond to redundant phrases, i.e. phrases that are obtained by composing smaller phrases of . Macros represent shortcuts for the parser and, possibly, improved likelihood estimate of the composed structure compared to the estimates under independency assumption (Abney, 1996). When the usage of macros excludes certain types of analysis, e.g. by trying to find longest/best matches we can speak of pruning. This is the contribution of D-EBL for parsing. 3 Experiments in EBL 3.1 Experimental purpose and setup The aim of the experiments is to verify whether new knowledge is acquired in A-EBL and D-EBL. Secondly, we want to test the influence of new knowledge on parsing accuracy and speed. The general setup of the experiment is the following. We use a section of a treebank as seed-corpus ( ). We train the seed-corpus to a corpus-based parser. Using a test-corpus we esta</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Partial Parsing via Finite-State Cascades. In Proceedings of the ESSLLI ’96 Robust Parsing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
<author>Ronald M Kaplan</author>
</authors>
<title>A probabilistic corpus-driven model for lexical-functional analysis.</title>
<date>1998</date>
<booktitle>In COLING-ACL’98.</booktitle>
<contexts>
<context position="1627" citStr="Bod and Kaplan, 1998" startWordPosition="230" endWordPosition="233">ich syntagmatic contexts for disambiguation, richer than rule-based approaches do (Yuang et al., 1992). Statistical approaches to parsing acquire mainly paradigmatic knowledge and require larger corpora, c.f. (Carl and Langlais, 2003). Statistical approaches handle unseen events via smoothing. Rule-based approaches use abstract category labels. 1This research has been carried out within Logos Gaias project, which integrates NLP technologies into a Internetbased natural language learning platform (Streiter et al., 2003). Example-based parsing generalizes examples during compilation time, e.g. (Bod and Kaplan, 1998), or performs a similarity-based fuzzy match during runtime (Zavrel and Daelemans, 1997). Both techniques may be computationally demanding, their effect on parsing however is quite different, c.f. (Streiter, 2002a). Explanation-based learning (EBL) is a method to speed-up rule-based parsing via the caching of examples. EBL however trades speed for accuracy. For many systems, a small loss in accuracy is acceptable if an order of magnitude less computing time is required. Apart from speed, one generally recognizes that EBL acquires some kind of knowledge from texts. However, what is this knowled</context>
</contexts>
<marker>Bod, Kaplan, 1998</marker>
<rawString>Rens Bod and Ronald M. Kaplan. 1998. A probabilistic corpus-driven model for lexical-functional analysis. In COLING-ACL’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Carl</author>
<author>Philippe Langlais</author>
</authors>
<title>Tuning general translation knowledge to a sublanguage.</title>
<date>2003</date>
<booktitle>In Proceedings of CLAW 2003,</booktitle>
<pages>15--17</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="1240" citStr="Carl and Langlais, 2003" startWordPosition="177" endWordPosition="180">iciency.1 1 Introduction The difficulties of natural language parsing, in general, and of parsing Chinese, in particular, are due to local ambiguities of words and phrases. Extensive linguistic and non-linguistic knowledge is required for their resolution (Chang, 1994; Chen, 1996). Different parsing approaches provide different types of knowledge. Example-based parsing approaches offer rich syntagmatic contexts for disambiguation, richer than rule-based approaches do (Yuang et al., 1992). Statistical approaches to parsing acquire mainly paradigmatic knowledge and require larger corpora, c.f. (Carl and Langlais, 2003). Statistical approaches handle unseen events via smoothing. Rule-based approaches use abstract category labels. 1This research has been carried out within Logos Gaias project, which integrates NLP technologies into a Internetbased natural language learning platform (Streiter et al., 2003). Example-based parsing generalizes examples during compilation time, e.g. (Bod and Kaplan, 1998), or performs a similarity-based fuzzy match during runtime (Zavrel and Daelemans, 1997). Both techniques may be computationally demanding, their effect on parsing however is quite different, c.f. (Streiter, 2002a</context>
</contexts>
<marker>Carl, Langlais, 2003</marker>
<rawString>Michael Carl and Philippe Langlais. 2003. Tuning general translation knowledge to a sublanguage. In Proceedings of CLAW 2003, Dublin, Ireland, May, 15-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsing-Wu Chang</author>
</authors>
<title>Word segmentation and sentence parsing in reading Chinese.</title>
<date>1994</date>
<booktitle>In Advances in the Study of Chinese Language Processing,</booktitle>
<institution>National Taiwan University,</institution>
<location>Taipei.</location>
<contexts>
<context position="884" citStr="Chang, 1994" startWordPosition="132" endWordPosition="133">que to speed-up parsing. The accuracy however often declines with EBL. The paper shows that this accuracy loss is not due to the EBL framework as such, but to deductive parsing. Abductive EBL allows extending the deductive closure of the parser. We present a Chinese parser based on abduction. Experiments show improvements in accuracy and efficiency.1 1 Introduction The difficulties of natural language parsing, in general, and of parsing Chinese, in particular, are due to local ambiguities of words and phrases. Extensive linguistic and non-linguistic knowledge is required for their resolution (Chang, 1994; Chen, 1996). Different parsing approaches provide different types of knowledge. Example-based parsing approaches offer rich syntagmatic contexts for disambiguation, richer than rule-based approaches do (Yuang et al., 1992). Statistical approaches to parsing acquire mainly paradigmatic knowledge and require larger corpora, c.f. (Carl and Langlais, 2003). Statistical approaches handle unseen events via smoothing. Rule-based approaches use abstract category labels. 1This research has been carried out within Logos Gaias project, which integrates NLP technologies into a Internetbased natural lang</context>
</contexts>
<marker>Chang, 1994</marker>
<rawString>Hsing-Wu Chang. 1994. Word segmentation and sentence parsing in reading Chinese. In Advances in the Study of Chinese Language Processing, National Taiwan University, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
</authors>
<title>A model for robust Chinese parser.</title>
<date>1996</date>
<journal>Computational Linguistics and Chinese Language,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="897" citStr="Chen, 1996" startWordPosition="134" endWordPosition="135">up parsing. The accuracy however often declines with EBL. The paper shows that this accuracy loss is not due to the EBL framework as such, but to deductive parsing. Abductive EBL allows extending the deductive closure of the parser. We present a Chinese parser based on abduction. Experiments show improvements in accuracy and efficiency.1 1 Introduction The difficulties of natural language parsing, in general, and of parsing Chinese, in particular, are due to local ambiguities of words and phrases. Extensive linguistic and non-linguistic knowledge is required for their resolution (Chang, 1994; Chen, 1996). Different parsing approaches provide different types of knowledge. Example-based parsing approaches offer rich syntagmatic contexts for disambiguation, richer than rule-based approaches do (Yuang et al., 1992). Statistical approaches to parsing acquire mainly paradigmatic knowledge and require larger corpora, c.f. (Carl and Langlais, 2003). Statistical approaches handle unseen events via smoothing. Rule-based approaches use abstract category labels. 1This research has been carried out within Logos Gaias project, which integrates NLP technologies into a Internetbased natural language learning</context>
</contexts>
<marker>Chen, 1996</marker>
<rawString>Keh-Jiann Chen. 1996. A model for robust Chinese parser. Computational Linguistics and Chinese Language, 1(1):183–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanis Dimopoulos</author>
<author>Antonis Kakas</author>
</authors>
<title>Abduction and inductive learning.</title>
<date>1996</date>
<booktitle>Advances in Inductive Logic Programming,</booktitle>
<pages>144--171</pages>
<editor>In L. De Taedt, editor,</editor>
<publisher>IOS Press.</publisher>
<contexts>
<context position="3409" citStr="Dimopoulos and Kakas, 1996" startWordPosition="520" endWordPosition="523">the explanation). This determines which output the system will cache. The utility problem questions the claim of speeding-up applications (Minton, 1990): Retrieving cached solutions in addition to regular processing requires extra time. If retrieval is slow and cached solutions are rarely re-used, the cost-benefit ratio is negative. The accuracy of the derived PS is generally below that of the general PS. This may be due to the EBL framework as such or the deductive base of the PS. Research in abductive EBL (A-EBL) seems to suggest the latter: A-EBL has the potential to acquire new knowledge (Dimopoulos and Kakas, 1996). The relation between knowledge and accuracy however is not a direct and logical one. The U-shaped language learning curves in children exemplifies the indirect relation (Marcus et al., 1992). Wrong regular word forms supplant correct irregular forms when rules are learned. We therefore cannot simply equate automatic knowledge acquisition and accuracy improvement, in particular for complex language tasks. 1.2 EBL and Natural Language Parsing Previous research has applied EBL for the speed-up of large and slow grammars. Sentences are parsed. Then the parse trees are filtered and cached. Subseq</context>
<context position="6851" citStr="Dimopoulos and Kakas, 1996" startWordPosition="1089" endWordPosition="1092">tions of more elementary , and respectively. Transferring this notation to the description of parsing, is a syntactic formalism and a grammar. is the union of syntax trees and morphosyntactic tags. is a corpus tagged with . corresponds to a list of words, phrases or sentences (the surface strings). is a treebank, a cache of parse trees, or a history of explanations. 2.1 Parsing: A parser defines a relation between and (c.f. 2). Parsing is a relation between and a subset of (c.f. 3). 2The deductive closure of the set of axioms is the set which can be proved from it. 3The formalization follows (Dimopoulos and Kakas, 1996). (9) (10) (3) Simplifying, we can assume that is defined as the set of rules, i.e. . A specific parser is derived by the application of to the training material (e.g. ): . The set of possible relations is . Elements of are caching (no generalization), induction (hypothesis after data inspection) and abduction (hypothesis during classification). Equation (5) describes the cycle of grammar learning and grammar application. 2.1.1 Memory-based Parsing is based on memory if . in (6) is the trivial formalization of caching. Parsing proceeds via recalling defined in (7). The cycle of grammar learnin</context>
</contexts>
<marker>Dimopoulos, Kakas, 1996</marker>
<rawString>Yanis Dimopoulos and Antonis Kakas. 1996. Abduction and inductive learning. In L. De Taedt, editor, Advances in Inductive Logic Programming, pages 144– 171. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
<author>Keh-Jiann Chen</author>
</authors>
<title>A Chinese corpus for linguistics research.</title>
<date>1992</date>
<booktitle>In COLING’92,</booktitle>
<location>Nantes, France.</location>
<marker>Huang, Chen, 1992</marker>
<rawString>Chu-Ren Huang and Keh-Jiann Chen. 1992. A Chinese corpus for linguistics research. In COLING’92, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
<author>Feng-Yi Chen</author>
<author>Keh-Jiann Chen</author>
<author>Zhaoming Gao</author>
<author>Kuang-Yu Chen</author>
</authors>
<title>Sinica treebank: Design criteria, annotation guidelines and online interface.</title>
<date>2000</date>
<booktitle>Proceedings of the Second Chinese Language Processing Workshop,</booktitle>
<editor>In M. Palmer, M. Marcus, A. K. Joshi, and F. Xia, editors,</editor>
<publisher>ACL.</publisher>
<location>Hong Kong,</location>
<contexts>
<context position="14597" citStr="Huang et al., 2000" startWordPosition="2375" endWordPosition="2378">ve parsing with k-nn retrieval and adaptation of retrieved examples. are obtained by parsing step 4a or 5 where only one POS-labels may be different in the last characters (e.g. ). The resulting corpora are and . RETURN accuracy and speed of the parser ( (recall,precision,f-score,time)). Then, we parse a large corpus ( ). A filter criterion that works on the explanation applies. We train those trees which pass the filter to the parser ( ). Then the parsing accuracy and speed is tested against the same training corpus ( (recall,precision,f-score,time)). Sections of the Chinese Sinica Treebank (Huang et al., 2000) are used as seed-treebank and gold standard for parsing evaluation. Seed-corpora range between 1.000 and 20.000 trees. We train them to the parser OCTOPUS (Streiter, 2002a). This parser integrates memory- deduction- and abduction-based parsing in a hierarchy of preferences, starting from 1 memory-based parsing, 2 non-recursive deductive parsing, 3 recursive deductive parsing and 5 finally abductive parsing (Fig. 2). Learning the seed corpora ( ) results in 1992). For every the parser produces one parsetree and an explanation. The explanation has the form of a derivation tree in TAGs, c.f (Jos</context>
</contexts>
<marker>Huang, Chen, Chen, Gao, Chen, 2000</marker>
<rawString>Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhaoming Gao, and Kuang-Yu Chen. 2000. Sinica treebank: Design criteria, annotation guidelines and online interface. In M. Palmer, M. Marcus, A. K. Joshi, and F. Xia, editors, Proceedings of the Second Chinese Language Processing Workshop, Hong Kong, October. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Tree-adjoining grammars.</title>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics.</booktitle>
<editor>In R. Mitkov, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="13760" citStr="Joshi, 2003" startWordPosition="2234" endWordPosition="2235">ank as seed-corpus ( ). We train the seed-corpus to a corpus-based parser. Using a test-corpus we establish the parsing Figure 2: The main parsing algorithm of OCTOPUS. The parser interleaves memory-based, deductive, and abductive parsing strategies in five steps: Recalling, non-recursive deduction, deduction via chunk substitution, first with lexemes, then without lexemes and finally abduction. # 1 recalling from POS (a) and lexeme (i) RETURN IF ( ) # 2 deduction on the basis of POS (a) RETURN IF ( ) # 3 deductive, recursive parsing with POS and lexeme # Substitutions are defined as in TAGs (Joshi, 2003) IF ( ) # 4a deductive recursive parsing with lexeme, # 4b compared to abductive parsing IF ( ) RETURN ( , #abduction #deduction )) # 5 abduction as robust parsing solution RETURN RETURN # deduction ) Figure 3: Abductive parsing with k-nn retrieval and adaptation of retrieved examples. are obtained by parsing step 4a or 5 where only one POS-labels may be different in the last characters (e.g. ). The resulting corpora are and . RETURN accuracy and speed of the parser ( (recall,precision,f-score,time)). Then, we parse a large corpus ( ). A filter criterion that works on the explanation applies. </context>
<context position="15206" citStr="Joshi, 2003" startWordPosition="2473" endWordPosition="2474">00) are used as seed-treebank and gold standard for parsing evaluation. Seed-corpora range between 1.000 and 20.000 trees. We train them to the parser OCTOPUS (Streiter, 2002a). This parser integrates memory- deduction- and abduction-based parsing in a hierarchy of preferences, starting from 1 memory-based parsing, 2 non-recursive deductive parsing, 3 recursive deductive parsing and 5 finally abductive parsing (Fig. 2). Learning the seed corpora ( ) results in 1992). For every the parser produces one parsetree and an explanation. The explanation has the form of a derivation tree in TAGs, c.f (Joshi, 2003). The deduction and abduction steps are visible in the explanation. Filters apply on the explanation and create sub-corpora that belong to one inference type. The first filter requires the explanation to contain only one non-recursive deduction, i.e. only parsing step 2. As deductive parsing is attempted after memory-based parsing (1), holds. A second filter extracts those structures, which 3.2 The Acquired Knowledge We want to know whether or not new knowledge has been acquired and what the nature of this acquired knowledge is. As parsing was not recursive, we can approach the closure by the </context>
</contexts>
<marker>Joshi, 2003</marker>
<rawString>Aravind K. Joshi. 2003. Tree-adjoining grammars. In R. Mitkov, editor, The Oxford Handbook of Computational Linguistics. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
</authors>
<title>Open set experiments with direct analysis by analogy.</title>
<date>1999</date>
<booktitle>In Proceedings NLPRS’99 (Natural Language Processing Pacific Rim Symposium),</booktitle>
<pages>363--368</pages>
<location>Beijing.</location>
<contexts>
<context position="8772" citStr="Lepage, 1999" startWordPosition="1400" endWordPosition="1401"> are identical. parsing (11) 2.1.3 Abduction-based Parsing Abduction, defined as is a runtime generalization which is triggered by a concrete to be classified. We separate and for presentation purpose only.5 The relation may express a similarity, a temporal or causal relation. (12) and the cycle of (13) define abduction. parsing learning from (13) Abduction subsumes reasoning by analogy. Abduction is an analogy, if describes a similarity. Reasoning from rain to snow is a typical analogy. Reasoning from wet street to rain is an abductive reasoning. For a parsing approach based on analogy c.f. (Lepage, 1999). 5Abduction is a process of hypothesis generation. Deduction and abduction may work conjointly whenever deductive inferences encounter gaps. A deductive inference stops in front of a gap between the premises and a possible conclusion. Abduction creates a new hypothesis, which allows to bridge the gap and to continue the inference. (8) (12) 2.2 Learning: In this section, we formalize EBL. We mechanically substitute in the definition of EBL by to show their learning potentials. A learning system changes internal states, which influence the performance. The internal states of are determined by a</context>
</contexts>
<marker>Lepage, 1999</marker>
<rawString>Yves Lepage. 1999. Open set experiments with direct analysis by analogy. In Proceedings NLPRS’99 (Natural Language Processing Pacific Rim Symposium), pages 363–368, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary F Marcus</author>
<author>Steven Pinker</author>
<author>Michael Ullman</author>
<author>Michelle Hollander</author>
<author>John T Rosen</author>
<author>Fei Xu</author>
</authors>
<date>1992</date>
<booktitle>Overregularization in Language Learning. Monographs of the Society for Research in Child Development,</booktitle>
<volume>57</volume>
<contexts>
<context position="3601" citStr="Marcus et al., 1992" startWordPosition="551" endWordPosition="554">gular processing requires extra time. If retrieval is slow and cached solutions are rarely re-used, the cost-benefit ratio is negative. The accuracy of the derived PS is generally below that of the general PS. This may be due to the EBL framework as such or the deductive base of the PS. Research in abductive EBL (A-EBL) seems to suggest the latter: A-EBL has the potential to acquire new knowledge (Dimopoulos and Kakas, 1996). The relation between knowledge and accuracy however is not a direct and logical one. The U-shaped language learning curves in children exemplifies the indirect relation (Marcus et al., 1992). Wrong regular word forms supplant correct irregular forms when rules are learned. We therefore cannot simply equate automatic knowledge acquisition and accuracy improvement, in particular for complex language tasks. 1.2 EBL and Natural Language Parsing Previous research has applied EBL for the speed-up of large and slow grammars. Sentences are parsed. Then the parse trees are filtered and cached. Subsequent parsing uses the cached trees. A complex HPSG-grammar transforms into tree-structures with instantiated values (Neumann, 1994). One hash table lookup of POS-sequences replaces typedfeatur</context>
</contexts>
<marker>Marcus, Pinker, Ullman, Hollander, Rosen, Xu, 1992</marker>
<rawString>Gary F. Marcus, Steven Pinker, Michael Ullman, Michelle Hollander, John T. Rosen, and Fei Xu. 1992. Overregularization in Language Learning. Monographs of the Society for Research in Child Development, 57 (No. 4, Serial No. 228).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Minton</author>
</authors>
<title>Quantitative results concerning the utility problem of explanation-based learning.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>42--363</pages>
<contexts>
<context position="2934" citStr="Minton, 1990" startWordPosition="440" endWordPosition="441">? Can a system learn to parse Chinese by parsing Chinese? The paper sets out to tackle these questions in theory and practice. 1.1 Explanation-based Learning (EBL) Explanation-based learning techniques transform a general problem solver (PS) into a specific and operational PS (Mitchel et al., 1986). The caching of the general PS’s output accounts for this transformation. The PS generates, besides the output, a documentation of the reasoning steps involved (the explanation). This determines which output the system will cache. The utility problem questions the claim of speeding-up applications (Minton, 1990): Retrieving cached solutions in addition to regular processing requires extra time. If retrieval is slow and cached solutions are rarely re-used, the cost-benefit ratio is negative. The accuracy of the derived PS is generally below that of the general PS. This may be due to the EBL framework as such or the deductive base of the PS. Research in abductive EBL (A-EBL) seems to suggest the latter: A-EBL has the potential to acquire new knowledge (Dimopoulos and Kakas, 1996). The relation between knowledge and accuracy however is not a direct and logical one. The U-shaped language learning curves </context>
</contexts>
<marker>Minton, 1990</marker>
<rawString>Steven Minton. 1990. Quantitative results concerning the utility problem of explanation-based learning. Artificial Intelligence, 42:363–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom S Mitchel</author>
<author>R Keller</author>
<author>S Kedar-Cabelli</author>
</authors>
<title>Explanation-based generalization: A unifying view.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2620" citStr="Mitchel et al., 1986" startWordPosition="390" endWordPosition="393">tems, a small loss in accuracy is acceptable if an order of magnitude less computing time is required. Apart from speed, one generally recognizes that EBL acquires some kind of knowledge from texts. However, what is this knowledge like if it does not help with parsing? Couldn’t a system improve by learning its own output? Can a system learn to parse Chinese by parsing Chinese? The paper sets out to tackle these questions in theory and practice. 1.1 Explanation-based Learning (EBL) Explanation-based learning techniques transform a general problem solver (PS) into a specific and operational PS (Mitchel et al., 1986). The caching of the general PS’s output accounts for this transformation. The PS generates, besides the output, a documentation of the reasoning steps involved (the explanation). This determines which output the system will cache. The utility problem questions the claim of speeding-up applications (Minton, 1990): Retrieving cached solutions in addition to regular processing requires extra time. If retrieval is slow and cached solutions are rarely re-used, the cost-benefit ratio is negative. The accuracy of the derived PS is generally below that of the general PS. This may be due to the EBL fr</context>
</contexts>
<marker>Mitchel, Keller, Kedar-Cabelli, 1986</marker>
<rawString>Tom S. Mitchel, R. Keller, and S. Kedar-Cabelli. 1986. Explanation-based generalization: A unifying view. Machine Learning, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unter Neumann</author>
</authors>
<title>Application of explanationbased learning for efficient processing of constraintbased grammars.</title>
<date>1994</date>
<booktitle>In The 10th Conference on Artificial Intelligence for Applications,</booktitle>
<location>San Antonio, Texas.</location>
<contexts>
<context position="4140" citStr="Neumann, 1994" startWordPosition="634" endWordPosition="635"> curves in children exemplifies the indirect relation (Marcus et al., 1992). Wrong regular word forms supplant correct irregular forms when rules are learned. We therefore cannot simply equate automatic knowledge acquisition and accuracy improvement, in particular for complex language tasks. 1.2 EBL and Natural Language Parsing Previous research has applied EBL for the speed-up of large and slow grammars. Sentences are parsed. Then the parse trees are filtered and cached. Subsequent parsing uses the cached trees. A complex HPSG-grammar transforms into tree-structures with instantiated values (Neumann, 1994). One hash table lookup of POS-sequences replaces typedfeature unification. Experiments conducted in EBLaugmented parsing consistently report a speed-up of the parser and a drop in accuracy (Rayner and Samuelsson, 1994; Srinivas and Joshi, 1995). A loss of information may explain the drop of accuracy. Contextual information, taken into account by the original parser, may be unavailable in the new operational format (Sima’an, 1997), especially if partial, context-dependent solutions are retrieved. In addition, the set of cached parse trees, judged to be ”sure to cache”, is necessarily biased (S</context>
</contexts>
<marker>Neumann, 1994</marker>
<rawString>G¨unter Neumann. 1994. Application of explanationbased learning for efficient processing of constraintbased grammars. In The 10th Conference on Artificial Intelligence for Applications, San Antonio, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
<author>Christer Samuelsson</author>
</authors>
<title>Corpusbased grammar specification for fast analysis. In Spoken Language Translator: First Year Report,</title>
<date>1994</date>
<tech>SRI Technical Report CRC-043,</tech>
<pages>41--54</pages>
<contexts>
<context position="4358" citStr="Rayner and Samuelsson, 1994" startWordPosition="664" endWordPosition="667">owledge acquisition and accuracy improvement, in particular for complex language tasks. 1.2 EBL and Natural Language Parsing Previous research has applied EBL for the speed-up of large and slow grammars. Sentences are parsed. Then the parse trees are filtered and cached. Subsequent parsing uses the cached trees. A complex HPSG-grammar transforms into tree-structures with instantiated values (Neumann, 1994). One hash table lookup of POS-sequences replaces typedfeature unification. Experiments conducted in EBLaugmented parsing consistently report a speed-up of the parser and a drop in accuracy (Rayner and Samuelsson, 1994; Srinivas and Joshi, 1995). A loss of information may explain the drop of accuracy. Contextual information, taken into account by the original parser, may be unavailable in the new operational format (Sima’an, 1997), especially if partial, context-dependent solutions are retrieved. In addition, the set of cached parse trees, judged to be ”sure to cache”, is necessarily biased (Streiter, 2002b). Most cached tree structures are short noun phrases. Parsing from biased examples will bias the parsing. A further reason for the loss in accuracy are incorrect parses which leak into the cache. A stric</context>
<context position="10072" citStr="Rayner and Samuelsson, 1994" startWordPosition="1612" endWordPosition="1615">arning. Therefore, the comparison of (before learning) with (after learning) reveals the acquired knowledge. We define EBL in (14). is the parser before learning. This parser applies to and yields , formalized as . The new parser is the application of to the union of and From two otherwise identical parsers, the parser with not present in the other has a greater deductive closure. The cardinality of reflects an empirical knowledge. The empirical knowledge does not allow to conclude something new, but to resolve ambiguities in accordance with observed data, e.g. for a sub-language as shown in (Rayner and Samuelsson, 1994). Both learning techniques have the potential of improving the accuracy. 2.2.1 Learning through Parsing A substitution of with reveals the transformation of to . We start with caching and recalling (Equation 15). (15) Parsing with the cache of yields . The deductive closure is not enlarged. Quantitative relations with respect to change in . If is not cached twice, memory-based EBL is idempotent.6 6Idempotence is the property of an operation that results in the same state no matter how many times it is executed. EBL with induction and deduction is shown in (16). Here the subscripts merit specia</context>
</contexts>
<marker>Rayner, Samuelsson, 1994</marker>
<rawString>Manny Rayner and Christer Samuelsson. 1994. Corpusbased grammar specification for fast analysis. In Spoken Language Translator: First Year Report, SRI Technical Report CRC-043, pg. 41-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
</authors>
<title>Explanation-based leaning of partial-parsers. In</title>
<date>1997</date>
<booktitle>Workshop Notes of the ECML/ML Workshop on Empirical Learning of Natural Language Processing Tasks,</booktitle>
<pages>137--146</pages>
<editor>W. Daelemans, A. van den Bosch, and A. Weijters, editors,</editor>
<location>Prague, Czech Republic,</location>
<marker>Sima’an, 1997</marker>
<rawString>Khalil Sima’an. 1997. Explanation-based leaning of partial-parsers. In W. Daelemans, A. van den Bosch, and A. Weijters, editors, Workshop Notes of the ECML/ML Workshop on Empirical Learning of Natural Language Processing Tasks, pages 137–146, Prague, Czech Republic, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bangalore Srinivas</author>
<author>Aravind K Joshi</author>
</authors>
<title>Some novel applications of explanation-based learning to parsing lexicalized tree-adjoining grammars.</title>
<date>1995</date>
<booktitle>In 33th Annual Meeting of the ACL,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="4385" citStr="Srinivas and Joshi, 1995" startWordPosition="668" endWordPosition="671">acy improvement, in particular for complex language tasks. 1.2 EBL and Natural Language Parsing Previous research has applied EBL for the speed-up of large and slow grammars. Sentences are parsed. Then the parse trees are filtered and cached. Subsequent parsing uses the cached trees. A complex HPSG-grammar transforms into tree-structures with instantiated values (Neumann, 1994). One hash table lookup of POS-sequences replaces typedfeature unification. Experiments conducted in EBLaugmented parsing consistently report a speed-up of the parser and a drop in accuracy (Rayner and Samuelsson, 1994; Srinivas and Joshi, 1995). A loss of information may explain the drop of accuracy. Contextual information, taken into account by the original parser, may be unavailable in the new operational format (Sima’an, 1997), especially if partial, context-dependent solutions are retrieved. In addition, the set of cached parse trees, judged to be ”sure to cache”, is necessarily biased (Streiter, 2002b). Most cached tree structures are short noun phrases. Parsing from biased examples will bias the parsing. A further reason for the loss in accuracy are incorrect parses which leak into the cache. A stricter filter does not solve t</context>
</contexts>
<marker>Srinivas, Joshi, 1995</marker>
<rawString>Bangalore Srinivas and Aravind K. Joshi. 1995. Some novel applications of explanation-based learning to parsing lexicalized tree-adjoining grammars. In 33th Annual Meeting of the ACL, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bangalore Srinivas</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="11352" citStr="Srinivas and Joshi, 1999" startWordPosition="1825" endWordPosition="1828"> into C changes the empirical knowledge with respect to and . If the empirical knowledge does not influence ,D-EBL is idempotent. The deductive closure does not increase as Abductive EBL (A-EBL) is shown in (17). AEBL acquires empirical knowledge similarly to DEBL. In addition, a new learning 2.2.2 Recursive Rule Application Parsing is a classification task in which is assigned to . Differently from typical classification tasks in machine learning, natural language parsing requires an open set . This is obtained via the recursive application of , which unlike non-recursive styles of analysis (Srinivas and Joshi, 1999) yields (syntax trees) of any complexity. Then is applied to so that can be matched by further rules (c.f. 18). Without this reduction, recursive parsing could not go beyond memory-based parsing. . . is ac(14) quired. This may differ from with respect to and/or . In the experiments in A-EBL we reported below, and holds. Figure 1: An explanation produced by OCTOPUS. At the top, the final parse obtained via deductive substitutions. Abductive term identification bridges gaps in the deduction (X Y). The marker ’?’ is a graphical shortcut for the set of lexemes in . The function defines an inductio</context>
</contexts>
<marker>Srinivas, Joshi, 1999</marker>
<rawString>Bangalore Srinivas and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Streiter</author>
<author>Judith Knapp</author>
<author>Leonhard Voltmer</author>
</authors>
<title>Gymn@zilla: A browser-like repository for open learning resources.</title>
<date>2003</date>
<booktitle>In ED-Media, World Conference on Educational Multimedia, Hypermedia &amp; Telecommunications,</booktitle>
<pages>23--28</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1530" citStr="Streiter et al., 2003" startWordPosition="217" endWordPosition="220"> parsing approaches provide different types of knowledge. Example-based parsing approaches offer rich syntagmatic contexts for disambiguation, richer than rule-based approaches do (Yuang et al., 1992). Statistical approaches to parsing acquire mainly paradigmatic knowledge and require larger corpora, c.f. (Carl and Langlais, 2003). Statistical approaches handle unseen events via smoothing. Rule-based approaches use abstract category labels. 1This research has been carried out within Logos Gaias project, which integrates NLP technologies into a Internetbased natural language learning platform (Streiter et al., 2003). Example-based parsing generalizes examples during compilation time, e.g. (Bod and Kaplan, 1998), or performs a similarity-based fuzzy match during runtime (Zavrel and Daelemans, 1997). Both techniques may be computationally demanding, their effect on parsing however is quite different, c.f. (Streiter, 2002a). Explanation-based learning (EBL) is a method to speed-up rule-based parsing via the caching of examples. EBL however trades speed for accuracy. For many systems, a small loss in accuracy is acceptable if an order of magnitude less computing time is required. Apart from speed, one genera</context>
</contexts>
<marker>Streiter, Knapp, Voltmer, 2003</marker>
<rawString>Oliver Streiter, Judith Knapp, and Leonhard Voltmer. 2003. Gymn@zilla: A browser-like repository for open learning resources. In ED-Media, World Conference on Educational Multimedia, Hypermedia &amp; Telecommunications, Honolulu, Hawaii, June, 23-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Streiter</author>
</authors>
<title>Abduction, induction and memorizing in corpus-based parsing.</title>
<date>2002</date>
<booktitle>In ESSLLI-2002 Workshop on ”Machine Learning Approaches in Computational Linguistics”,</booktitle>
<pages>73--90</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="1839" citStr="Streiter, 2002" startWordPosition="263" endWordPosition="265">Langlais, 2003). Statistical approaches handle unseen events via smoothing. Rule-based approaches use abstract category labels. 1This research has been carried out within Logos Gaias project, which integrates NLP technologies into a Internetbased natural language learning platform (Streiter et al., 2003). Example-based parsing generalizes examples during compilation time, e.g. (Bod and Kaplan, 1998), or performs a similarity-based fuzzy match during runtime (Zavrel and Daelemans, 1997). Both techniques may be computationally demanding, their effect on parsing however is quite different, c.f. (Streiter, 2002a). Explanation-based learning (EBL) is a method to speed-up rule-based parsing via the caching of examples. EBL however trades speed for accuracy. For many systems, a small loss in accuracy is acceptable if an order of magnitude less computing time is required. Apart from speed, one generally recognizes that EBL acquires some kind of knowledge from texts. However, what is this knowledge like if it does not help with parsing? Couldn’t a system improve by learning its own output? Can a system learn to parse Chinese by parsing Chinese? The paper sets out to tackle these questions in theory and p</context>
<context position="4753" citStr="Streiter, 2002" startWordPosition="726" endWordPosition="727">). One hash table lookup of POS-sequences replaces typedfeature unification. Experiments conducted in EBLaugmented parsing consistently report a speed-up of the parser and a drop in accuracy (Rayner and Samuelsson, 1994; Srinivas and Joshi, 1995). A loss of information may explain the drop of accuracy. Contextual information, taken into account by the original parser, may be unavailable in the new operational format (Sima’an, 1997), especially if partial, context-dependent solutions are retrieved. In addition, the set of cached parse trees, judged to be ”sure to cache”, is necessarily biased (Streiter, 2002b). Most cached tree structures are short noun phrases. Parsing from biased examples will bias the parsing. A further reason for the loss in accuracy are incorrect parses which leak into the cache. A stricter filter does not solve the problem. It increases the bias in the cache, reduces the size of the cache, and evokes the utility problem. EBL actually can improve parsing accuracy (Streiter, 2002b) if the grammar does not derive the parses to be cached via deduction but via abduction. The deductive closure2 which cannot increase with EBL from deductive parsing may increase with abductive pars</context>
<context position="14768" citStr="Streiter, 2002" startWordPosition="2405" endWordPosition="2406">. ). The resulting corpora are and . RETURN accuracy and speed of the parser ( (recall,precision,f-score,time)). Then, we parse a large corpus ( ). A filter criterion that works on the explanation applies. We train those trees which pass the filter to the parser ( ). Then the parsing accuracy and speed is tested against the same training corpus ( (recall,precision,f-score,time)). Sections of the Chinese Sinica Treebank (Huang et al., 2000) are used as seed-treebank and gold standard for parsing evaluation. Seed-corpora range between 1.000 and 20.000 trees. We train them to the parser OCTOPUS (Streiter, 2002a). This parser integrates memory- deduction- and abduction-based parsing in a hierarchy of preferences, starting from 1 memory-based parsing, 2 non-recursive deductive parsing, 3 recursive deductive parsing and 5 finally abductive parsing (Fig. 2). Learning the seed corpora ( ) results in 1992). For every the parser produces one parsetree and an explanation. The explanation has the form of a derivation tree in TAGs, c.f (Joshi, 2003). The deduction and abduction steps are visible in the explanation. Filters apply on the explanation and create sub-corpora that belong to one inference type. The</context>
</contexts>
<marker>Streiter, 2002</marker>
<rawString>Oliver Streiter. 2002a. Abduction, induction and memorizing in corpus-based parsing. In ESSLLI-2002 Workshop on ”Machine Learning Approaches in Computational Linguistics”, pages 73–90, Trento, Italy, August 5-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Streiter</author>
</authors>
<title>Treebank development with deductive and abductive explanation-based learning: Exploratory experiments.</title>
<date>2002</date>
<booktitle>In Workshop on Treebanks and Linguistic Theories</booktitle>
<location>Sozopol, Bulgaria,</location>
<contexts>
<context position="1839" citStr="Streiter, 2002" startWordPosition="263" endWordPosition="265">Langlais, 2003). Statistical approaches handle unseen events via smoothing. Rule-based approaches use abstract category labels. 1This research has been carried out within Logos Gaias project, which integrates NLP technologies into a Internetbased natural language learning platform (Streiter et al., 2003). Example-based parsing generalizes examples during compilation time, e.g. (Bod and Kaplan, 1998), or performs a similarity-based fuzzy match during runtime (Zavrel and Daelemans, 1997). Both techniques may be computationally demanding, their effect on parsing however is quite different, c.f. (Streiter, 2002a). Explanation-based learning (EBL) is a method to speed-up rule-based parsing via the caching of examples. EBL however trades speed for accuracy. For many systems, a small loss in accuracy is acceptable if an order of magnitude less computing time is required. Apart from speed, one generally recognizes that EBL acquires some kind of knowledge from texts. However, what is this knowledge like if it does not help with parsing? Couldn’t a system improve by learning its own output? Can a system learn to parse Chinese by parsing Chinese? The paper sets out to tackle these questions in theory and p</context>
<context position="4753" citStr="Streiter, 2002" startWordPosition="726" endWordPosition="727">). One hash table lookup of POS-sequences replaces typedfeature unification. Experiments conducted in EBLaugmented parsing consistently report a speed-up of the parser and a drop in accuracy (Rayner and Samuelsson, 1994; Srinivas and Joshi, 1995). A loss of information may explain the drop of accuracy. Contextual information, taken into account by the original parser, may be unavailable in the new operational format (Sima’an, 1997), especially if partial, context-dependent solutions are retrieved. In addition, the set of cached parse trees, judged to be ”sure to cache”, is necessarily biased (Streiter, 2002b). Most cached tree structures are short noun phrases. Parsing from biased examples will bias the parsing. A further reason for the loss in accuracy are incorrect parses which leak into the cache. A stricter filter does not solve the problem. It increases the bias in the cache, reduces the size of the cache, and evokes the utility problem. EBL actually can improve parsing accuracy (Streiter, 2002b) if the grammar does not derive the parses to be cached via deduction but via abduction. The deductive closure2 which cannot increase with EBL from deductive parsing may increase with abductive pars</context>
<context position="14768" citStr="Streiter, 2002" startWordPosition="2405" endWordPosition="2406">. ). The resulting corpora are and . RETURN accuracy and speed of the parser ( (recall,precision,f-score,time)). Then, we parse a large corpus ( ). A filter criterion that works on the explanation applies. We train those trees which pass the filter to the parser ( ). Then the parsing accuracy and speed is tested against the same training corpus ( (recall,precision,f-score,time)). Sections of the Chinese Sinica Treebank (Huang et al., 2000) are used as seed-treebank and gold standard for parsing evaluation. Seed-corpora range between 1.000 and 20.000 trees. We train them to the parser OCTOPUS (Streiter, 2002a). This parser integrates memory- deduction- and abduction-based parsing in a hierarchy of preferences, starting from 1 memory-based parsing, 2 non-recursive deductive parsing, 3 recursive deductive parsing and 5 finally abductive parsing (Fig. 2). Learning the seed corpora ( ) results in 1992). For every the parser produces one parsetree and an explanation. The explanation has the form of a derivation tree in TAGs, c.f (Joshi, 2003). The deduction and abduction steps are visible in the explanation. Filters apply on the explanation and create sub-corpora that belong to one inference type. The</context>
</contexts>
<marker>Streiter, 2002</marker>
<rawString>Oliver Streiter. 2002b. Treebank development with deductive and abductive explanation-based learning: Exploratory experiments. In Workshop on Treebanks and Linguistic Theories 2002, Sozopol, Bulgaria, September 20-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prasad Tadepalli</author>
</authors>
<title>A formalization of explanationbased macro-operator learning.</title>
<date>1991</date>
<booktitle>In IJCAI, Proceedings of the International Joint Conference of Artificial Intelligence,</booktitle>
<pages>616--622</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Sydney, Australia.</location>
<contexts>
<context position="12254" citStr="Tadepalli, 1991" startWordPosition="1976" endWordPosition="1978">A-EBL we reported below, and holds. Figure 1: An explanation produced by OCTOPUS. At the top, the final parse obtained via deductive substitutions. Abductive term identification bridges gaps in the deduction (X Y). The marker ’?’ is a graphical shortcut for the set of lexemes in . The function defines an induction and recursive parsing is thus a deduction. Combinations of memory-based and deduction-based parsing are deductions, combinations of abduction-based parsing with any another parsing are abductions. Macro Learning is the common term for the combination of EBL with recursive deduction (Tadepalli, 1991). A macro is a rule which yields the same result as a set of rules with and does. In terms of a grammar, such macros correspond to redundant phrases, i.e. phrases that are obtained by composing smaller phrases of . Macros represent shortcuts for the parser and, possibly, improved likelihood estimate of the composed structure compared to the estimates under independency assumption (Abney, 1996). When the usage of macros excludes certain types of analysis, e.g. by trying to find longest/best matches we can speak of pruning. This is the contribution of D-EBL for parsing. 3 Experiments in EBL 3.1 </context>
</contexts>
<marker>Tadepalli, 1991</marker>
<rawString>Prasad Tadepalli. 1991. A formalization of explanationbased macro-operator learning. In IJCAI, Proceedings of the International Joint Conference of Artificial Intelligence, pages 616–622, Sydney, Australia. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunfa Yuang</author>
<author>Changming Huang</author>
<author>Shimei Pan</author>
</authors>
<title>Knowledge acquisition and Chinese parsing based on corpus.</title>
<date>1992</date>
<booktitle>In COLING’92.</booktitle>
<contexts>
<context position="1108" citStr="Yuang et al., 1992" startWordPosition="160" endWordPosition="163">uctive closure of the parser. We present a Chinese parser based on abduction. Experiments show improvements in accuracy and efficiency.1 1 Introduction The difficulties of natural language parsing, in general, and of parsing Chinese, in particular, are due to local ambiguities of words and phrases. Extensive linguistic and non-linguistic knowledge is required for their resolution (Chang, 1994; Chen, 1996). Different parsing approaches provide different types of knowledge. Example-based parsing approaches offer rich syntagmatic contexts for disambiguation, richer than rule-based approaches do (Yuang et al., 1992). Statistical approaches to parsing acquire mainly paradigmatic knowledge and require larger corpora, c.f. (Carl and Langlais, 2003). Statistical approaches handle unseen events via smoothing. Rule-based approaches use abstract category labels. 1This research has been carried out within Logos Gaias project, which integrates NLP technologies into a Internetbased natural language learning platform (Streiter et al., 2003). Example-based parsing generalizes examples during compilation time, e.g. (Bod and Kaplan, 1998), or performs a similarity-based fuzzy match during runtime (Zavrel and Daelemans</context>
</contexts>
<marker>Yuang, Huang, Pan, 1992</marker>
<rawString>Chunfa Yuang, Changming Huang, and Shimei Pan. 1992. Knowledge acquisition and Chinese parsing based on corpus. In COLING’92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Memorybased learning: Using similarity for smoothing.</title>
<date>1997</date>
<booktitle>Workshop Notes of the ECML/ML Workshop on Empirical Learning of Natural Language Processing Tasks,</booktitle>
<pages>71--84</pages>
<editor>In W. Daelemans, A. van den Bosch, and A. Weijters, editors,</editor>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1715" citStr="Zavrel and Daelemans, 1997" startWordPosition="242" endWordPosition="245">(Yuang et al., 1992). Statistical approaches to parsing acquire mainly paradigmatic knowledge and require larger corpora, c.f. (Carl and Langlais, 2003). Statistical approaches handle unseen events via smoothing. Rule-based approaches use abstract category labels. 1This research has been carried out within Logos Gaias project, which integrates NLP technologies into a Internetbased natural language learning platform (Streiter et al., 2003). Example-based parsing generalizes examples during compilation time, e.g. (Bod and Kaplan, 1998), or performs a similarity-based fuzzy match during runtime (Zavrel and Daelemans, 1997). Both techniques may be computationally demanding, their effect on parsing however is quite different, c.f. (Streiter, 2002a). Explanation-based learning (EBL) is a method to speed-up rule-based parsing via the caching of examples. EBL however trades speed for accuracy. For many systems, a small loss in accuracy is acceptable if an order of magnitude less computing time is required. Apart from speed, one generally recognizes that EBL acquires some kind of knowledge from texts. However, what is this knowledge like if it does not help with parsing? Couldn’t a system improve by learning its own </context>
</contexts>
<marker>Zavrel, Daelemans, 1997</marker>
<rawString>Jakub Zavrel and Walter Daelemans. 1997. Memorybased learning: Using similarity for smoothing. In W. Daelemans, A. van den Bosch, and A. Weijters, editors, Workshop Notes of the ECML/ML Workshop on Empirical Learning of Natural Language Processing Tasks, pages 71–84, Prague, Czech Republic, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>