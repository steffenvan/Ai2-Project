<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.599307">
Using Deep Belief Nets for Chinese Named Entity Categorization
</title>
<author confidence="0.910994">
Yu Chen1, You Ouyang2, Wenjie Li2, Dequan Zheng1, Tiejun Zhao1
</author>
<affiliation confidence="0.962567">
1School of Computer Science and Technology, Harbin Institute of Technology, China
</affiliation>
<email confidence="0.854406">
{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn
</email>
<affiliation confidence="0.941597">
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong
</affiliation>
<email confidence="0.953054">
{csyouyang, cswjli}@comp.polyu.edu.hk
</email>
<sectionHeader confidence="0.997094" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999619266666667">
Identifying named entities is essential in
understanding plain texts. Moreover, the
categories of the named entities are indicative
of their roles in the texts. In this paper, we
propose a novel approach, Deep Belief Nets
(DBN), for the Chinese entity mention
categorization problem. DBN has very strong
representation power and it is able to
elaborately self-train for discovering
complicated feature combinations. The
experiments conducted on the Automatic
Context Extraction (ACE) 2004 data set
demonstrate the effectiveness of DBN. It
outperforms the state-of-the-art learning
models such as SVM or BP neural network.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966472222223">
Named entities (NE) are defined as the names of
existing objects, such as persons, organizations
and etc. Identifying NEs in plain texts provides
structured information for semantic analysis.
Hence the named entity recognition (NER) task
is a fundamental task for a wide variety of
natural language processing applications, such as
question answering, information retrieval and etc.
In a text, an entity may either be referred to by a
common noun, a noun phrase, or a pronoun.
Each reference of the entity is called a mention.
NER indeed requires the systems to identify
these entity mentions from plain texts. The task
can be decomposed into two sub-tasks, i.e., the
identification of the entities in the text and the
classification of the entities into a set of pre-
defined categories. In the study of this paper, we
focus on the second sub-task and assume that the
boundaries of all the entity mentions to be
categorized are already correctly identified.
In early times, NER systems are mainly based
on handcrafted rule-based approaches. Although
rule-based approaches achieved reasonably good
results, they have some obvious flaws. First, they
require exhausted handcraft work to construct a
proper and complete rule set, which partially
expressing the meaning of entity. Moreover,
once the interest of task is transferred to a
different domain or language, rules have to be
revised or even rewritten. The discovered rules
are indeed heavily dependent on the task
interests and the particular corpus. Finally, the
manually-formatted rules are usually incomplete
and their qualities are not guaranteed.
Recently, more attentions are switched to the
applications of machine learning models with
statistic information. In this camp, entity
categorization is typically cast as a multi-class
classification process, where the named entities
are represented by feature vectors. Usually, the
vectors are abstracted by some lexical and
syntactic features instead of semantic feature.
Many learning models, such as Support Vector
Machine (SVM) and Neural Network (NN), are
then used to classify the entities by their feature
vectors.
Entity categorization in Chinese attracted less
attention when compared to English or other
western languages. This is mainly because the
unique characteristics of Chinese. One of the
most common problems is the lack of boundary
information in Chinese texts. For this problem,
character-based methods are reported to be a
possible substitution of word-based methods. As
to character-based methods, it is important to
study the implicit combination of characters.
In our study, we explore the use of Deep
Belief Net (DBN) in character-based entity
categorization. DBN is a neural network model
which is developed under the deep learning
architecture. It is claimed to be able to
automatically learn a deep hierarchy of the input
features with increasing levels of abstraction for
the complex problem. In our problem, DBN is
used to automatically discover the complicated
composite effects of the characters to the NE
categories from the input data. With DBN, we
need not to manually construct the character
combination features for expressing the semantic
relationship among characters in entities.
Moreover, the deep structure of DBN enables the
possibility of discovering very sophisticated
</bodyText>
<page confidence="0.985075">
102
</page>
<note confidence="0.633533">
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 102–109,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999861625">
combinations of the characters, which may even
be hard to discover by human.
The rest of this paper is organized as follow.
Section 2 reviews the related work on name
entity categorization. Section 3 introduces the
methodology of the proposed approach. Section
4 provides the experimental results. Finally,
section 5 concludes the whole paper.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99995825862069">
Over the past decades, NER has evolved from
simple rule-based approaches to adapted self-
training machine learning approaches.
As early rule-based approaches, MacDonald
(1993) utilized local context, which implicate
internal and external evidence, to aid on
categorization. Wacholder (1997) employed an
aggregation of classification method to capture
internal rules. Both used hand-written rules and
knowledge bases. Later, Collins (1999) adopted
the AdaBoost algorithm to find a weighted
combination of simple classifiers. They reported
that the combination of simple classifiers can
yield some powerful systems with much better
performances. As a matter of fact, these methods
all need manual studies on the construction of the
rule set or the simple classifiers.
Machine learning models attract more
attentions recently. Usually, they train
classification models based on context features.
Various lexical and syntactic features are
considered, such as N-grams, Part-Of-Speech
(POS), and etc. Zhou and Su (2002) integrated
four different kinds of features, which convey
different semantic information, for a
classification model based on the Hidden
Markov Model (HMM). Koen (2006) built a
classifier with the Conditional Random Field
(CRF) model to classify noun phrases in a text
with the WordNet SynSet. Isozaki and Kazawa
(2002) studied the use of SVM instead.
There were fewer studies in Chinese entity
categorization. Guo and Jiang (2005) applied
Robust Risk Minimization to classify the named
entities. The features include seven traditional
lexical features and two external-NE-hints based
features. An important result they reported is that
character-based features can be as good as word-
based features since they avoid the Chinese word
segmentation errors. In (Jing et al., 2003), it was
further reported that pure character-based models
can even outperform word-based models with
character combination features.
Deep Belief Net is introduced in (Hinton et al.,
2006). According to their definition, DBN is a
deep neural network that consists of one or more
Restricted Boltzmann Machine (RBM) layers
and a Back Propagation (BP) layer. This multi-
layer structure leads to a strong representation
power of DBN. Moreover, DBN is quite efficient
by using RBM to implement the middle layers,
since RBM can be learned very quickly by the
Contrastive Divergence (CD) approach.
Therefore, we believe that DBN is very suitable
for the character-level Chinese entity mention
categorization approach. It can be used to solve
the multi-class categorization problem with just
simple binary features as the input.
</bodyText>
<sectionHeader confidence="0.973881" genericHeader="method">
3 Deep Belief Network for Chinese
Entity Categorization
</sectionHeader>
<subsectionHeader confidence="0.999975">
3.1 Problem Formalization
</subsectionHeader>
<bodyText confidence="0.999939875">
An Entity mention categorization is a process of
classifying the entity mentions into different
categories. In this paper, we assume that the
entity mentions are already correctly detected
from the texts. Moreover, an entity mention
should belong to one and only one predefined
category. Formally, the categorization function
of the name entities is
</bodyText>
<equation confidence="0.633319">
f *(V(e; )) —&gt; C (1)
</equation>
<bodyText confidence="0.999760454545454">
where is an entity mention from all the
mention set E, is the binary feature
vector of , C={C1, C2, ..., CM} is the pre-
defined categories. Now the question is to find a
classification function which maps
the feature vector V(ei) of an entity mention to its
category. Generally, this classification function
is learned from training data consisting of entity
mentions with labeled categories. The learned
function is then used to predict the category of
new entity mentions by their feature vectors.
</bodyText>
<subsectionHeader confidence="0.997866">
3.2 Character-based Features
</subsectionHeader>
<bodyText confidence="0.999607111111111">
As mentioned in the introduction, we intend to
use character-level features for the purpose of
avoiding the impact of the Chinese word
segmentation errors. Denote the character
dictionary as D={d1, d2, ..., dN}. To an e, it‟s
feature vector is V(e)={ v1, v2, ..., vN }. Each unit
vi can be valued as Equation 2.
vi � 1. 1 di e e (2)
it 0 di 0- e
</bodyText>
<page confidence="0.971041">
103
</page>
<bodyText confidence="0.999847666666667">
For example, there is an entity mention A4
顿 `Clinton&apos;. So its feature vector is a vector
with the same length as the character dictionary,
in which all the dimensions are 0 except the three
dimensions standing for A, 4, and 顿. The
representation is clearly illustrated in Figure 1
below. Since our objective is to test the
effectiveness of DBN for this task. Therefore, we
do not involve any other feature.
</bodyText>
<figureCaption confidence="0.988218">
Fig. 1. Generating the character-level features
</figureCaption>
<bodyText confidence="0.999978583333333">
Characters compose the named entity and
express its meaning. As a matter of fact, the
composite effect of the characters to the
mention category is quite complicated. For
example, �&apos;li `Mr. Li&apos; and �挝 `Laos&apos; both
have character �, but �&apos;li `Mr. Li&apos; indicates
a person but �挝 `Laos&apos; indicates a country.
These are totally different NEs. Another
example is ER*_-69 `Capital of Paraguay&apos;
and �* `Asuncion&apos;. They are two entity
mentions point to the same entity despite that
the two entities do not have any common
characters. In such case, independent character
features are not sufficient to determine the
categories of the entity mentions. So we should
also introduce some features which are able to
represent the combinational effects of the
characters. However, such kind of features is
very hard to discover. Meanwhile, a complete
set of combinations is nearly impossible to be
found manually due to the exponential number
of all the possible combinations. As in our
study, we adopt DBN to automatically find the
character combinations.
</bodyText>
<subsectionHeader confidence="0.999159">
3.3 Deep Belief Nets
</subsectionHeader>
<bodyText confidence="0.987129454545455">
Deep Belief Network (DBN) is a complicated
model which combines a set of simple models
that are sequentially connected (Ackley, 1985).
This deep architecture can be viewed as multiple
layers. In DBN, upper layers are supposed to
represent more “abstract” concepts that explain
the input data whereas lower layers extract “low-
level features” from the data. DBN often consists
of many layers, including multiple Restricted
Boltzmann Machine (RBM) layers and a Back
Propagation (BP) layer.
</bodyText>
<figureCaption confidence="0.998386">
Fig. 2. The structure of a DBN.
</figureCaption>
<bodyText confidence="0.99996565">
As illustrated in Figure 2, when DBN receives
a feature vector, the feature vector is processed
from the bottom to the top through several RBM
layers in order to get the weights in each RBM
layer, maintaining as many features as possible
when they are transferred to the next layer. RBM
deals with feature vectors only and omits the la-
bel information. It is unsupervised. In addition,
each RBM layer learns its parameters indepen-
dently. This makes the parameters optimal for
the relevant RBM layer but not optimal for the
whole model. To solve this problem, there is a
supervised BP layer on top of the model which
fine-tunes the whole model in the learning
process and generates the output in the inference
process. After the processing of all these layers,
the final feature vector consists of some sophisti-
cated features, which reflect the structured in-
formation among the original features. With this
new feature vector, the classification perfor-
mance is better than directly using the original
feature vector.
None of the RBM is capable of guaranteeing
that all the information conveyed to the output is
accurate or important enough. However the
learned information produced by preceding RBM
layer will be continuously refined through the
next RBM layer to weaken the wrong or insigni-
ficant information in the input. Each layer can
detect feature in the relevant spaces. Multiple
layers help to detect more features in different
spaces. Lower layers could support object detec-
tion by spotting low-level features indicative of
object parts. Conversely, information about ob-
jects in the higher layers could resolve lower-
level ambiguities. The units in the final layer
share more information from the data. This in-
creases the representation power of the whole
model. It is certain that more layers mean more
computation time.
</bodyText>
<page confidence="0.995818">
104
</page>
<bodyText confidence="0.9935715">
DBN has some attractive features which make
it very suitable for our problem.
</bodyText>
<listItem confidence="0.822182846153846">
1) The unsupervised process can detect the
structures in the input and automatically ob-
tain better feature vectors for classification.
2) The supervised BP layer can modify the
whole network by back-propagation to im-
prove both the feature vectors and the classi-
fication results.
3) The generative model makes it easy to in-
terpret the distributed representations in the
deep hidden layers.
4) This is a fast learning algorithm that can
find a fairly good set of parameters quickly
and can ensure the efficiency of DBN.
</listItem>
<subsubsectionHeader confidence="0.398676">
3.3.1 Restricted Boltzmann Machine (RBM)
</subsubsectionHeader>
<bodyText confidence="0.985194095238095">
In this section, we will introduce RBM, which is
the core component of DBN. RBM is Boltzmann
Machine with no connection within the same
layer. An RBM is constructed with one visible
layer and one hidden layer. Each visible unit in
the visible layer is an observed variablev;
while each hidden unit in the hidden layer is
a hidden variable . Its joint distribution is
p(v, h) oc exp(—E(v, h)) = en- &amp;quot;&apos; X+&amp;quot; n (3)
In RBM, the parameters that need to be esti-
mated are and .
To learn RBM, the optimum parameters are
obtained by maximizing the above probability on
the training data (Hinton, 1999). However, the
probability is indeed very difficult in practical
calculation. A traditional way is to find the gra-
dient between the initial parameters and the re-
spect parameters. By modifying the previous pa-
rameters with the gradient, the expected parame-
ters can gradually approximate the target para-
meters as
</bodyText>
<equation confidence="0.985183666666667">
W(T +1) = W(T) + 17 aP(v0)
(4)
a W WT
</equation>
<bodyText confidence="0.993055666666667">
where is a parameter controlling the leaning
rate. It determines the speed of W converging to
the target.
Traditionally, the Markov chain Monte Carlo
method (MCMC) is used to calculate this kind of
gradient.
</bodyText>
<equation confidence="0.9543295">
a logp(v, h) = h0v0 — h&apos;v&apos; (5)
aw
</equation>
<bodyText confidence="0.994277722222222">
where logp(v,h) is the log probability of the
data. h°v° denotes the multiplication of the av-
erage over the data states and its relevant sample
in hidden unit. denotes the multiplication
of the average over the model states in visible
unit and its relevant sample in hidden unit.
However, MCMC requires estimating an ex-
ponential number of terms. Therefore, it typically
takes a long time to converge to . Hinton
(2002) introduced an alternative algorithm, i.e.,
the contrastive divergence (CD) algorithm, as a
substitution. It is reported that CD can train the
model much more efficiently than MCMC. To
estimate the distribution , CD considers a
series of distributions { pn(x) } which indicate the
distributions in n steps. It approximates the gap
of two different Kullback-Leiler divergences
(Kullback, 1987) as
</bodyText>
<equation confidence="0.733437">
CD =KL(po II p.)—KL(p„ II p.) (6)
</equation>
<bodyText confidence="0.999859">
Maximizing the log probability of the data is
exactly the same as minimizing the Kullback–
Leibler divergence between the distribution of
the data p0 and the equilibrium distribution p.
defined by the model. In each step, the gap is
approximately minimized so that we can obtain
the final distribution which has the smallest
Kullback-Leiler divergence with the fantasy dis-
tribution.
After n steps, the gradient can be estimated
and used in Equation 4 to adjust the weights of
RBM. In our experiments, we set n to be 1. It
means that in each step of gradient calculation,
the estimate of the gradient is used to adjust the
weight of RBM. In this case, the estimate of the
gradient is just the gap between the products of
the visual layer and the hidden layer, i.e.,
</bodyText>
<equation confidence="0.892527">
 log p(v, h)  h0v0  h1v1
 W
</equation>
<figureCaption confidence="0.671047">
Figure 3 below illustrates the process of learning
RBM with CD-based gradient estimation.
</figureCaption>
<figure confidence="0.513276">
(7)
</figure>
<page confidence="0.946824">
105
</page>
<figureCaption confidence="0.866405">
Fig. 3. Learning RBM with CD-based gradient
estimation
</figureCaption>
<subsectionHeader confidence="0.728543">
3.3.2 Back-propagation (BP)
</subsectionHeader>
<bodyText confidence="0.999524551724138">
The RBM layers provide an unsupervised analy-
sis on the structures of data set. They automati-
cally detect sophisticated feature vectors. The
last layer in DBN is the BP layer. It takes the
output from the last RBM layer and applies it in
the final supervised learning process. In DBN,
not only is the supervised BP layer used to gen-
erate the final categories, but it is also used to
fine-tune the whole network. Specifically speak-
ing, when the parameters in BP layer are
changed during its iterating process, the changes
are passed to the other RBM layers in a top-to-
bottom sequence.
The BP algorithm has a feed-forward step and
a back-propagation step. In the feed-forward step,
the input values are propagated to obtain the out-
put values. In the back-propagation step, the out-
put values are compared to the real category la-
bels and used them to modify the parameters of
the model. We consider the weight w;j which
indicates the edge pointing from the i-th node in
one RBM layer to the j-th node in its upper layer.
The computation in feed-forward is ,
where is the stored output for the unit i. In
the back-propagation step, we compute the error
E in the upper layers and also the gradient with
respect to this error, i.e., . Then the
weight waj will be adjusted by the gradient des-
cent.
</bodyText>
<equation confidence="0.952752">
ao w
i ij
aE
Awij _ —yoi _ —yoi8j
</equation>
<bodyText confidence="0.9983265">
where is used to control the length of the
moving step.
</bodyText>
<sectionHeader confidence="0.5319195" genericHeader="method">
3.3.3 DBN-based Entity Mention Categori-
zation
</sectionHeader>
<bodyText confidence="0.99987175">
For each entity mention, it is represented by the
character feature vector as introduced in section
3.2 and then fed to DBN. The training procedure
can be divided into two phases. The first phase is
the parameter estimation process of the RBMs on
all the inputted feature vectors. When a feature
vector is fed to DBN, the first RBM layer is
adjusted automatically according to this vector.
After the first RBM layer is ready, its output
becomes the input of the second RBM layer. The
weights of the second RBM layer are also
adjusted. The similar procedure is carried out on
all the RBM layers. Then DBN will operates in
the second phase, the back-propagation
algorithm. The labeled categories of the entity
mention are used to tune the parameters of the
BP layer. Moreover, the changes of the BP layer
are also fed back to the RBM layers. The
procedure will iterate until the terminating
condition is met. It can be a fixed number of
iterations or a pre-given precision threshold.
Once the weights of all the layers in DBN are
obtained, the estimated model could be used to
prediction.
</bodyText>
<figureCaption confidence="0.8536665">
Fig. 4. The mention categorization process
of DBN
</figureCaption>
<bodyText confidence="0.992796470588235">
Figure 4 illustrates the classification process of
DBN. In prediction, for an entity mention e, we
first calculate its feature vector V(e) and used as
the input of DBN. V(e) is passed through all the
layers to get the outputs for all RBM layers and
last back-propagation layer. In the ith RBM layer,
the dimensions in the input vector Vinput_i(e) are
combined to yield the dimensions of the next
feature vector Voutput_i(e) as input of the next layer.
After the feature vector V(e) goes through all the
RBM layers, it is indeed transformed to another
feature vector V’(e) which consists of
complicated combinations of the original
character features and contains rich structured
information between the characters. This feature
vector is then fed into the BP layer to get the
final category c(e).
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999433">
4.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999918571428572">
In our experiment, we use the ACE 2004 corpus
to evaluate our approach. The objective of this
study is that the correctly detected Chinese entity
mentions categorization using DBN from the text
and figure out the suitability of DBN on this task.
Moreover, an entity mention should belong to
one and only one category.
</bodyText>
<figure confidence="0.376732">
(8)
</figure>
<page confidence="0.991496">
106
</page>
<bodyText confidence="0.999956838709677">
According to the guideline of the ACE04 task,
there are five categories for consideration in total,
i.e., Person, Organization, Geo-political entity,
Location, and Facility. Moreover, each entity
mention is expressed in two forms, i.e., the head
and the extent. For example, 美国总统克林顿
„President Clinton of USA‟ is the extent of an
entity mention and 克林 顿 „Clinton‟ is the
corresponding head. The two phrases both point
to a named entity whose name is Clinton and he
is the president of USA. Here we make the
“breakdown” strategy mentioned in Li et al.
(2007) that only the entity head is considered to
generate the feature vector, considering that the
information from the entity head refines the
name entity. Although the entity extent includes
more information, it also brings many noises
which may make the learning process much
more difficult.
In our experiments, we test the machine
learning models under a 4-flod cross-validation.
All entity mentions are divided into four parts
randomly where three parts are used for training
and one for test. In total, 7746 mentions are used
for training and 2482 mentions are used for
testing at each round. Precision is chosen as the
evaluation criterion, calculated by the proportion
of the number of correctly categorized instances
and the number of total instances. Since all the
instances should be classified, the recall value is
equal to the precision value.
</bodyText>
<subsectionHeader confidence="0.847979">
4.2 Evaluation on Named Entity categoriza-
tion
</subsectionHeader>
<bodyText confidence="0.998457">
First of all, we provide some statistics of the data
set. The distribution of entity mentions in each
category is given in table 1. The size of the
character dictionary in the corpus is 1185, so
does the dimension of each feature vector.
</bodyText>
<table confidence="0.937044166666667">
Type Quantity
Person 4197
Organization 1783
Geo-political entity 287
Location 3263
Facility 399
</table>
<tableCaption confidence="0.895757">
Table 1. Number of entity mentions in each
category
</tableCaption>
<bodyText confidence="0.999336090909091">
In the first experiment, we compare the
performance of DBN with some popular
classification algorithms, including Support
Vector Machine (labeled by SVM) and a
traditional BP neutral network (labeled by NN
(BP)). To implement the models, we use the
LibSVM toolkit1 for SVM and the neural neutral
network toolbox in Matlab2 for BP. The DBN in
this experiment includes two RBM layers and
one BP layer. Results of the first experiment are
given in Table 2.
</bodyText>
<table confidence="0.733562">
Learning Model Precision
DBN 91.45%
SVM 90.29%
NN(BP) 87.23%
</table>
<tableCaption confidence="0.8647995">
Table 2. Performances of the systems with
different classification models
</tableCaption>
<bodyText confidence="0.987484833333333">
In this experiment, the DBN has three RBM
layers and one BP layer. And the numbers of
units in each RBM layer are 900, 600 and 300
respectively. NN (BP) has the same structure as
DBN. As for SVM, we choose the linear kernel
with the penalty parameter C=1 and set the other
parameters as default after comparing different
kernels and parameters.
In the results, DBN achieved better
performance than both SVM and BP neural
network. This clearly proved the advantages of
DBN. The deep architecture of DBN yields
stronger representation power which makes it
able to detect more complicated and efficient
features, thus better performance is achieved.
In the second experiment, we intend to
examine the performance of DBN with different
number of RBM layers, from one RBM layer
plus one BP layer to three RBM layers plus one
BP layer. The amount of the units in the first
RBM layer is set 900 and the amount in the
second RBM layer is 600, if the second layer
exists. As for the third RBM layers, the amount
of units is set to 300.
Construction of Neural Network Precision
Three RBMs and One BP 91.45%
Two RBMs and One BP 91.42%
One RBM and one BP 91.05%
Table 3. Performance of DBNs with different
number s of RBM layers
Results in Table 3 show that the performance
tends to be better when more RBM layers are
incorporated. More RBM layers do enhance the
representation power of DBN. However, it is
also noted that the improvement is not significant
from two layers to three layers. The reason may
</bodyText>
<footnote confidence="0.98386475">
1 available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/
2 available at
http://www.mathworks.com/access/helpdesk/help/toolbox
/nnet/backprop.html
</footnote>
<page confidence="0.997694">
107
</page>
<bodyText confidence="0.981705909090909">
be that two-RBM DBN already has enough
representation power for modeling this data set
and thus one more RBM layer brings
insignificant improvement. It is also mentioned
in Hinton (2006) that more than three RBM
layers are indeed not necessary. Another
important result in Table 3 is that the DBN with
One RBM and one BP performs much better than
the neutral network with only BP in Table 1.
This clearly showed the effectiveness of feature
combination by the RBM layer again.
As to the amount of units in each RBM layer,
it is manually fixed in upper experiments. This
number certainly affects the representation
power of an RBM layer, consequently the
representation power of the whole DBN. In this
set of experiment, we intend to study the
effectiveness of the unit size to the performance
of DBN. A series of DBNs with only one RBM
layer and different unit numbers for this RBM
layer is evaluated. The results are provided in
Table 4 below.
</bodyText>
<table confidence="0.999140142857143">
Construction of Neural Network Precision
one RBM(300 units) + one BP 90.61%
one RBM(600 units) + one BP 90.69%
one RBM(900 units) + one BP 91.05%
one RBM(1200 units) + one BP 90.98%
one RBM(1500 units) + one BP 90.61%
one RBM(1800 units) + one BP 90.57%
</table>
<tableCaption confidence="0.970529">
Table 4. Performance of One-RBM DBNs
</tableCaption>
<bodyText confidence="0.978051923076923">
with different number of units
Based on the results, we can see that the
performance is quite stable with different unit
numbers. But the numbers that are closer to the
original feature size seem to be some better. This
could suggest that we should not decrease or
increase the dimension of the vector feature too
much when casting the vector transformation by
RBM layers.
Finally, we show the results of the individual
categories. For each category, the Precision-
Recall-F values are provided in table 5, in which
the F-measure is calculated by
</bodyText>
<equation confidence="0.482901">
2*Precision*Recall
F-measure= (9)
Precision+Recall
</equation>
<table confidence="0.986709428571429">
Type P R F
Person 91.26% 96.26% 93.70%
Organization 89.86% 89.04% 89.45%
Location 77.58% 59.21% 76.17%
Geo-political 93.60% 91.89% 92.74%
entity
Facility 77.43% 63.72% 69.91%
</table>
<tableCaption confidence="0.907731">
Table 5. Performances of the system on each
category
</tableCaption>
<sectionHeader confidence="0.99909" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999789142857143">
In this paper we presented our recent work on
applying a novel machine learning model, the
Deep Belief Nets, on Chinese entity mention
categorization. It is demonstrated that DBN is
very suitable for character-level mention
categorization approaches due to its strong
representation power and the ability on
discovering complicated feature combinations.
We conducted a series of experiments to prove
the benefits of DBN. Experimental results
clearly showed the advantages of DBN that it
obtained better performance than existing
approaches such as SVM and traditional BP
neutral network.
</bodyText>
<sectionHeader confidence="0.999676" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999951166666667">
David Ackley, Geoffrey Hinton, and Terrence
Sejnowski. 1985. A learning algorithm for
Boltzmann machines. Cognitive Science. 9.
David MacDonald. 1993. Internal and external
evidence in the identification and semantic
categorization of proper names. Corpus
Processing for Lexical Acquisition, MIT Press, 61-
76.
Geoffrey Hinton. 1999. Products of experts. In
Proceedings of the Ninth International.
Conference on Artificial Neural Networks
(ICANN). Vol. 1, 1–6.
Geoffrey Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural
Computation, 14, 1771–1800.
Geoffrey Hinton, Simon Osindero, and Yee-Whey
Teh. 2006. A fast learning algorithm for deep
belief nets. Neural Computation. 18, 1527–1554 .
GuoDong Zhou and Jian Su. 2002. Named entity
recognition using an hmm-based chunk tagger. In
proceedings of ACL. 473-480.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity
recognition. In proceedings of IJCNLP. 1-7.
Honglei Guo, Jianmin Jiang, Guang Hu and Tong
Zhang. 2005. Chinese named entity recognition
based on multilevel linguistics features. In
proceedings of IJCNLP. 90-99.
Jing, Hongyan, Radu Florian, Xiaoqiang Luo, Tong
Zhang and Abraham Ittycheriah. 2003. How to get
a Chinese name (entity): Segmentation and
combination issues. In proceedings of EMNLP.
200-207.
Koen Deschacht and Marie-Francine Moens. 2006,
Efficient Hierarchical Entity Classifier Using
Conditional Random Field. In Proceedings of the
</reference>
<page confidence="0.986632">
108
</page>
<reference confidence="0.9992536">
2nd Workshop on Ontology Learning and
Population. 33-40.
Michael Collins and Yoram Singer. 1999.
Unsupervised models for named entity
classification. In Proceedings of EMNLP&apos;99.
Nina Wacholder, Yael Ravin and Misook Choi. 1997.
Disambiguation of Proper Names in Text. In
Proceedings of the Fifth Conference on Applied
Natural Language Processing.
Solomon Kullback. 1987. Letter to the Editor: The
Kullback-Leibler distance. The American
Statistician 41 (4): 340–341.
Wenjie Li and Donglei Qian. 2007. Detecting,
Categorizing and Clustering Entity Mentions in
Chinese Text, in Proceedings of the 30th Annual
International ACM SIGIR Conference (SIGIR’07).
647-654.
Yoshua Bengio and Yann LeCun. 2007. Scaling
learning algorithms towards ai. Large-Scale Ker-
nel Machines. MIT Press.
</reference>
<page confidence="0.99896">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.637561">
<title confidence="0.99996">Using Deep Belief Nets for Chinese Named Entity Categorization</title>
<author confidence="0.998658">You Wenjie Dequan Tiejun</author>
<affiliation confidence="0.998299">of Computer Science and Technology, Harbin Institute of Technology,</affiliation>
<address confidence="0.7530285">{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn of Computing, The Hong Kong Polytechnic University, Hong Kong</address>
<email confidence="0.970991">csyouyang@comp.polyu.edu.hk</email>
<email confidence="0.970991">cswjli@comp.polyu.edu.hk</email>
<abstract confidence="0.99929225">Identifying named entities is essential in understanding plain texts. Moreover, the categories of the named entities are indicative of their roles in the texts. In this paper, we propose a novel approach, Deep Belief Nets (DBN), for the Chinese entity mention categorization problem. DBN has very strong representation power and it is able to elaborately self-train for discovering complicated feature combinations. The experiments conducted on the Automatic Context Extraction (ACE) 2004 data set demonstrate the effectiveness of DBN. It outperforms the state-of-the-art learning models such as SVM or BP neural network.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Ackley</author>
<author>Geoffrey Hinton</author>
<author>Terrence Sejnowski</author>
</authors>
<title>A learning algorithm for Boltzmann machines.</title>
<date>1985</date>
<journal>Cognitive Science.</journal>
<volume>9</volume>
<marker>Ackley, Hinton, Sejnowski, 1985</marker>
<rawString>David Ackley, Geoffrey Hinton, and Terrence Sejnowski. 1985. A learning algorithm for Boltzmann machines. Cognitive Science. 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David MacDonald</author>
</authors>
<title>Internal and external evidence in the identification and semantic categorization of proper names. Corpus Processing for Lexical Acquisition,</title>
<date>1993</date>
<pages>61--76</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="5020" citStr="MacDonald (1993)" startWordPosition="749" endWordPosition="750">9, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics combinations of the characters, which may even be hard to discover by human. The rest of this paper is organized as follow. Section 2 reviews the related work on name entity categorization. Section 3 introduces the methodology of the proposed approach. Section 4 provides the experimental results. Finally, section 5 concludes the whole paper. 2 Related work Over the past decades, NER has evolved from simple rule-based approaches to adapted selftraining machine learning approaches. As early rule-based approaches, MacDonald (1993) utilized local context, which implicate internal and external evidence, to aid on categorization. Wacholder (1997) employed an aggregation of classification method to capture internal rules. Both used hand-written rules and knowledge bases. Later, Collins (1999) adopted the AdaBoost algorithm to find a weighted combination of simple classifiers. They reported that the combination of simple classifiers can yield some powerful systems with much better performances. As a matter of fact, these methods all need manual studies on the construction of the rule set or the simple classifiers. Machine l</context>
</contexts>
<marker>MacDonald, 1993</marker>
<rawString>David MacDonald. 1993. Internal and external evidence in the identification and semantic categorization of proper names. Corpus Processing for Lexical Acquisition, MIT Press, 61-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
</authors>
<title>Products of experts.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth International. Conference on Artificial Neural Networks (ICANN).</booktitle>
<volume>1</volume>
<pages>1--6</pages>
<contexts>
<context position="13882" citStr="Hinton, 1999" startWordPosition="2183" endWordPosition="2184">ltzmann Machine (RBM) In this section, we will introduce RBM, which is the core component of DBN. RBM is Boltzmann Machine with no connection within the same layer. An RBM is constructed with one visible layer and one hidden layer. Each visible unit in the visible layer is an observed variablev; while each hidden unit in the hidden layer is a hidden variable . Its joint distribution is p(v, h) oc exp(—E(v, h)) = en- &amp;quot;&apos; X+&amp;quot; n (3) In RBM, the parameters that need to be estimated are and . To learn RBM, the optimum parameters are obtained by maximizing the above probability on the training data (Hinton, 1999). However, the probability is indeed very difficult in practical calculation. A traditional way is to find the gradient between the initial parameters and the respect parameters. By modifying the previous parameters with the gradient, the expected parameters can gradually approximate the target parameters as W(T +1) = W(T) + 17 aP(v0) (4) a W WT where is a parameter controlling the leaning rate. It determines the speed of W converging to the target. Traditionally, the Markov chain Monte Carlo method (MCMC) is used to calculate this kind of gradient. a logp(v, h) = h0v0 — h&apos;v&apos; (5) aw where logp</context>
</contexts>
<marker>Hinton, 1999</marker>
<rawString>Geoffrey Hinton. 1999. Products of experts. In Proceedings of the Ninth International. Conference on Artificial Neural Networks (ICANN). Vol. 1, 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<pages>1771--1800</pages>
<contexts>
<context position="14890" citStr="Hinton (2002)" startWordPosition="2357" endWordPosition="2358">te. It determines the speed of W converging to the target. Traditionally, the Markov chain Monte Carlo method (MCMC) is used to calculate this kind of gradient. a logp(v, h) = h0v0 — h&apos;v&apos; (5) aw where logp(v,h) is the log probability of the data. h°v° denotes the multiplication of the average over the data states and its relevant sample in hidden unit. denotes the multiplication of the average over the model states in visible unit and its relevant sample in hidden unit. However, MCMC requires estimating an exponential number of terms. Therefore, it typically takes a long time to converge to . Hinton (2002) introduced an alternative algorithm, i.e., the contrastive divergence (CD) algorithm, as a substitution. It is reported that CD can train the model much more efficiently than MCMC. To estimate the distribution , CD considers a series of distributions { pn(x) } which indicate the distributions in n steps. It approximates the gap of two different Kullback-Leiler divergences (Kullback, 1987) as CD =KL(po II p.)—KL(p„ II p.) (6) Maximizing the log probability of the data is exactly the same as minimizing the Kullback– Leibler divergence between the distribution of the data p0 and the equilibrium </context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14, 1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
<author>Simon Osindero</author>
<author>Yee-Whey Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<journal>Neural Computation.</journal>
<volume>18</volume>
<pages>1527--1554</pages>
<contexts>
<context position="6824" citStr="Hinton et al., 2006" startWordPosition="1012" endWordPosition="1015">nstead. There were fewer studies in Chinese entity categorization. Guo and Jiang (2005) applied Robust Risk Minimization to classify the named entities. The features include seven traditional lexical features and two external-NE-hints based features. An important result they reported is that character-based features can be as good as wordbased features since they avoid the Chinese word segmentation errors. In (Jing et al., 2003), it was further reported that pure character-based models can even outperform word-based models with character combination features. Deep Belief Net is introduced in (Hinton et al., 2006). According to their definition, DBN is a deep neural network that consists of one or more Restricted Boltzmann Machine (RBM) layers and a Back Propagation (BP) layer. This multilayer structure leads to a strong representation power of DBN. Moreover, DBN is quite efficient by using RBM to implement the middle layers, since RBM can be learned very quickly by the Contrastive Divergence (CD) approach. Therefore, we believe that DBN is very suitable for the character-level Chinese entity mention categorization approach. It can be used to solve the multi-class categorization problem with just simpl</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>Geoffrey Hinton, Simon Osindero, and Yee-Whey Teh. 2006. A fast learning algorithm for deep belief nets. Neural Computation. 18, 1527–1554 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Jian Su</author>
</authors>
<title>Named entity recognition using an hmm-based chunk tagger.</title>
<date>2002</date>
<booktitle>In proceedings of ACL.</booktitle>
<pages>473--480</pages>
<contexts>
<context position="5859" citStr="Zhou and Su (2002)" startWordPosition="867" endWordPosition="870">and knowledge bases. Later, Collins (1999) adopted the AdaBoost algorithm to find a weighted combination of simple classifiers. They reported that the combination of simple classifiers can yield some powerful systems with much better performances. As a matter of fact, these methods all need manual studies on the construction of the rule set or the simple classifiers. Machine learning models attract more attentions recently. Usually, they train classification models based on context features. Various lexical and syntactic features are considered, such as N-grams, Part-Of-Speech (POS), and etc. Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). Koen (2006) built a classifier with the Conditional Random Field (CRF) model to classify noun phrases in a text with the WordNet SynSet. Isozaki and Kazawa (2002) studied the use of SVM instead. There were fewer studies in Chinese entity categorization. Guo and Jiang (2005) applied Robust Risk Minimization to classify the named entities. The features include seven traditional lexical features and two external-NE-hints based features. An i</context>
</contexts>
<marker>Zhou, Su, 2002</marker>
<rawString>GuoDong Zhou and Jian Su. 2002. Named entity recognition using an hmm-based chunk tagger. In proceedings of ACL. 473-480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Efficient support vector classifiers for named entity recognition.</title>
<date>2002</date>
<booktitle>In proceedings of IJCNLP.</booktitle>
<pages>1--7</pages>
<contexts>
<context position="6179" citStr="Isozaki and Kazawa (2002)" startWordPosition="916" endWordPosition="919"> construction of the rule set or the simple classifiers. Machine learning models attract more attentions recently. Usually, they train classification models based on context features. Various lexical and syntactic features are considered, such as N-grams, Part-Of-Speech (POS), and etc. Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). Koen (2006) built a classifier with the Conditional Random Field (CRF) model to classify noun phrases in a text with the WordNet SynSet. Isozaki and Kazawa (2002) studied the use of SVM instead. There were fewer studies in Chinese entity categorization. Guo and Jiang (2005) applied Robust Risk Minimization to classify the named entities. The features include seven traditional lexical features and two external-NE-hints based features. An important result they reported is that character-based features can be as good as wordbased features since they avoid the Chinese word segmentation errors. In (Jing et al., 2003), it was further reported that pure character-based models can even outperform word-based models with character combination features. Deep Beli</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. In proceedings of IJCNLP. 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglei Guo</author>
<author>Jianmin Jiang</author>
<author>Guang Hu</author>
<author>Tong Zhang</author>
</authors>
<title>Chinese named entity recognition based on multilevel linguistics features.</title>
<date>2005</date>
<booktitle>In proceedings of IJCNLP.</booktitle>
<pages>90--99</pages>
<marker>Guo, Jiang, Hu, Zhang, 2005</marker>
<rawString>Honglei Guo, Jianmin Jiang, Guang Hu and Tong Zhang. 2005. Chinese named entity recognition based on multilevel linguistics features. In proceedings of IJCNLP. 90-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Radu Florian</author>
</authors>
<title>Xiaoqiang Luo, Tong Zhang and Abraham Ittycheriah.</title>
<date>2003</date>
<booktitle>In proceedings of EMNLP.</booktitle>
<pages>200--207</pages>
<marker>Jing, Florian, 2003</marker>
<rawString>Jing, Hongyan, Radu Florian, Xiaoqiang Luo, Tong Zhang and Abraham Ittycheriah. 2003. How to get a Chinese name (entity): Segmentation and combination issues. In proceedings of EMNLP. 200-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Efficient Hierarchical Entity Classifier Using Conditional Random Field.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2nd Workshop on Ontology Learning and Population.</booktitle>
<pages>33--40</pages>
<marker>Deschacht, Moens, 2006</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2006, Efficient Hierarchical Entity Classifier Using Conditional Random Field. In Proceedings of the 2nd Workshop on Ontology Learning and Population. 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP&apos;99.</booktitle>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of EMNLP&apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Wacholder</author>
<author>Yael Ravin</author>
<author>Misook Choi</author>
</authors>
<title>Disambiguation of Proper Names in Text.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing.</booktitle>
<marker>Wacholder, Ravin, Choi, 1997</marker>
<rawString>Nina Wacholder, Yael Ravin and Misook Choi. 1997. Disambiguation of Proper Names in Text. In Proceedings of the Fifth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Kullback</author>
</authors>
<title>Letter to the Editor: The Kullback-Leibler distance.</title>
<date>1987</date>
<journal>The American Statistician</journal>
<volume>41</volume>
<issue>4</issue>
<pages>340--341</pages>
<contexts>
<context position="15282" citStr="Kullback, 1987" startWordPosition="2416" endWordPosition="2417">verage over the model states in visible unit and its relevant sample in hidden unit. However, MCMC requires estimating an exponential number of terms. Therefore, it typically takes a long time to converge to . Hinton (2002) introduced an alternative algorithm, i.e., the contrastive divergence (CD) algorithm, as a substitution. It is reported that CD can train the model much more efficiently than MCMC. To estimate the distribution , CD considers a series of distributions { pn(x) } which indicate the distributions in n steps. It approximates the gap of two different Kullback-Leiler divergences (Kullback, 1987) as CD =KL(po II p.)—KL(p„ II p.) (6) Maximizing the log probability of the data is exactly the same as minimizing the Kullback– Leibler divergence between the distribution of the data p0 and the equilibrium distribution p. defined by the model. In each step, the gap is approximately minimized so that we can obtain the final distribution which has the smallest Kullback-Leiler divergence with the fantasy distribution. After n steps, the gradient can be estimated and used in Equation 4 to adjust the weights of RBM. In our experiments, we set n to be 1. It means that in each step of gradient calc</context>
</contexts>
<marker>Kullback, 1987</marker>
<rawString>Solomon Kullback. 1987. Letter to the Editor: The Kullback-Leibler distance. The American Statistician 41 (4): 340–341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenjie Li</author>
<author>Donglei Qian</author>
</authors>
<title>Detecting, Categorizing and Clustering Entity Mentions in Chinese Text,</title>
<date>2007</date>
<booktitle>in Proceedings of the 30th Annual International ACM SIGIR Conference (SIGIR’07).</booktitle>
<pages>647--654</pages>
<marker>Li, Qian, 2007</marker>
<rawString>Wenjie Li and Donglei Qian. 2007. Detecting, Categorizing and Clustering Entity Mentions in Chinese Text, in Proceedings of the 30th Annual International ACM SIGIR Conference (SIGIR’07). 647-654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Yann LeCun</author>
</authors>
<title>Scaling learning algorithms towards ai. Large-Scale Kernel Machines.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<marker>Bengio, LeCun, 2007</marker>
<rawString>Yoshua Bengio and Yann LeCun. 2007. Scaling learning algorithms towards ai. Large-Scale Kernel Machines. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>