<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.9981815">
QARAB: A Question Answering System to Support
the Arabic Language
</title>
<author confidence="0.978746">
Bassam Hammo Hani Abu-Salem Steven Lytinen
</author>
<affiliation confidence="0.995297">
DePaul University
School of Computer Science, Telecommunications and Information Systems
</affiliation>
<address confidence="0.881081">
243 S. Wabash Avenue, Chicago IL 60604
</address>
<email confidence="0.998724">
bhammo@condor.depaul.edu habusalem@cti.depaul.edu lytinen@cs.depaul.edu
</email>
<author confidence="0.929997">
Martha Evens
</author>
<affiliation confidence="0.99775">
Illinois Institute of Technology
Computer Science Department
</affiliation>
<address confidence="0.932841">
10 West 31st Street, Chicago, IL 60616
</address>
<email confidence="0.996058">
evens@iit.edu
</email>
<sectionHeader confidence="0.993037" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987735294118">
We describe the design and
implementation of a question answering
(QA) system called QARAB. It is a
system that takes natural language
questions expressed in the Arabic
language and attempts to provide short
answers. The system’s primary source
of knowledge is a collection of Arabic
newspaper text extracted from Al-Raya,
a newspaper published in Qatar. During
the last few years the information
retrieval community has attacked this
problem for English using standard IR
techniques with only mediocre success.
We are tackling this problem for Arabic
using traditional Information Retrieval
(IR) techniques coupled with a
sophisticated Natural Language
Processing (NLP) approach. To identify
the answer, we adopt a keyword
matching strategy along with matching
simple structures extracted from both
the question and the candidate
documents selected by the IR system.
To achieve this goal, we use an existing
tagger to identify proper names and
other crucial lexical items and build
lexical entries for them on the fly. We
also carry out an analysis of Arabic
question forms and attempt a better
understanding of what kinds of answers
users find satisfactory. The paucity of
studies of real users has limited results
in earlier research.
</bodyText>
<sectionHeader confidence="0.978396" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999618939393939">
In recent years, there has been a marked increase
in the amount of data available on the Internet.
Users often have specific questions in mind, for
which they hope to get answers. They would like
the answers to be short and precise, and they
always prefer to express the questions in their
native language without being restricted to a
specific query language, query formation rules, or
even a specific knowledge domain. The new
approach taken to matching the user needs is to
carry out actual analysis of the question from a
linguistic point of view and to attempt to
understand what the user really means.
QARAB is the result of coupling traditional
Information Retrieval (IR) techniques with a
sophisticated Natural Language Processing (NLP)
approach. The approach can be summarized as
follows: the IR system treats the question as a
query in an attempt to identify the candidate
documents that may contain the answer; then the
NLP techniques are used to parse the question and
analyze the top ranked documents returned by the
IR system.
Natural Language Processing (NLP) in the
Arabic language is still in its initial stage
compared to the work in the English language,
which has already benefited from the extensive
research in this field. There are some aspects that
slow down progress in Arabic Natural Language
Processing (NLP) compared to the
accomplishments in English and other European
languages [Al-Daimi &amp; Abdel-Amir, 1994].
These aspects include:
</bodyText>
<listItem confidence="0.998168571428572">
• Arabic is highly inflectional and derivational,
which makes morphological analysis a very
complex task.
• The absence of diacritics (which represent
most vowels) in the written text creates
ambiguity and therefore, complex
morphological rules are required to identify
the tokens and parse the text.
• The writing direction is from right-to-left and
some of the characters change their shapes
based on their location in the word.
• Capitalization is not used in Arabic, which
makes it hard to identify proper names,
acronyms, and abbreviations.
</listItem>
<bodyText confidence="0.90410275">
In addition to the above linguistic issues, there
is also a lack of Arabic corpora, lexicons, and
machine-readable dictionaries, which are essential to
advance research in different areas.
</bodyText>
<sectionHeader confidence="0.986317" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999980274074074">
Advances in natural language processing (NLP),
information retrieval techniques (IR), information
extraction (IE), as well as the computer industry,
have given QA a strong boost. Modern question-
answering systems have started incorporating
NLP techniques to parse natural language
documents, extract entities and relations between
entities, resolve anaphora, and other language
ambiguities [Harabagiu et al., 2000; Vicedo &amp;
Ferrández, 2000].
Research in Question-Answering (QA) is not
new. The QA problem has been addressed in the
literature since the beginning of computing
machines. The AI/NLP communities initiated
traditional work to address question-answering
using structural methods. Early experiments in
this direction implemented systems that operate in
very restricted domains (e.g. SHRDLU
[Winogard, 1972] and LUNAR [Woods, 1972]).
In the QUALM system, Lehnert [1978] took a
further step, based on the conceptual theories of
Schank &amp; Abelson [1977], to understand the
nature of the questions and classify them in a way
similar to how human beings understand and
answer questions. SCISOR [Jacobs &amp; Rau 1990]
aimed at question answering and text extraction
more than information retrieval. It combined
natural language processing, knowledge
representation, and information retrieval
techniques with lexical analysis and word-based
text searches. The MURAX system [Kupiec,
1993] used robust linguistic methods to answer
closed-class natural language questions. It
presented the user with relevant text in which
noun phrases are marked. A less automated
approach like Ask Jeeves [1996] approached the
QA problem by pointing the questioner to Web
links that might contain information relevant to
the answer to the question. Ask Jeeves benefited
from advanced natural language processing
techniques combined with data mining processing
and a huge expanding knowledge base. Another
system, with a different approach, is the
FAQFinder system [Burke et al., 1997], which
attempted to solve the question-answering
problem using a database of question-answer pairs
built from existing frequently asked question
(FAQ) files. Two other important systems are the
START system [Katz, 1997], which is based on
annotations from the Web and the Q&amp;A system
[Budzik &amp; Hammond, 1999], which is a
semiautomatic, natural language question-
answering and referral system. The system is
based on a huge knowledge base and human
experts who volunteered their time to respond to
the users’ questions.
Recently, attention has begun to be focused
on developing question-answering systems that do
not rely on a knowledge base and that can fetch
answers from huge unstructured text. New QA
systems enhanced with NLP and IR techniques
have been developed to extract textual answers for
open-domain questions and provide a framework
for modern information retrieval [TREC-8, 1999;
TREC-9, 2000].
The overall aim of this QA track was to
retrieve small pieces of text that contain the actual
answer to the question rather than the list of
documents traditionally returned by retrieval
engines [Voorhees &amp; Tice, 2000]. The TREC-8
QA track attracted researchers from both industry
and academia. Twenty organizations participated
in this track with different approaches and their
systems were evaluated. The participating
systems were tested on a huge set of unstructured
documents and a set of fact-based questions.
Generally speaking, most of the TREC-8
long-string answer (250-bytes) participants
attempted to solve the QA problem from the
information retrieval (IR) point of view by
locating the most relevant documents from the
collection and then extracting the sentences most
relevant to the query from the documents just
located. The systems relying on this “bag-of-
words” approach (e.g. [Allan et al., 1999];
[Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin
et al., 1999] and the passage-retrieval run of
AT&amp;T [Singhal et al., 1999]) deal with the
question without considering its grammatical or
semantic characteristics and they apply
conventional IR techniques to extract the answer.
Even though the “bag-of-words” approach was
commonly used in TREC-8, the systems based on
this approach were inadequate to handle the short-
string (50-byte) answers.
On the contrary, the short string (50-byte)
participants (e.g. [Breck et al., 1999]; [Ferret et
al., 1999]; [Hull, 1999]; [Humphreys et al., 1999];
[Litkowski, 1999]; [Moldovan et al., 2000]; [Oard
et al., 1999]; [Singhal et al., 1999]) agreed on the
importance of applying several natural language
processing techniques to solve the problem.
Among these techniques are: part-of-speech
tagging, shallow parsing, query type identification
and named entity recognition. Because the
number of test documents to be analyzed for each
query was huge, the majority of the systems in
this band used the “bag-of-words” approach as an
initial step to retrieve the relevant passages that
contain the possible answer. Another approach to
the QA problem combines IR techniques with
Information Extraction (IE) techniques for
extracting named entities, e.g., [Ogden et al.,
1999]; [Takaki, 1999]; and [Srihari &amp; Li, 1999].
A detailed description of the track and the results
are available at [Voorhees &amp; Tice, 1999].
It is obvious from the increasing number of
systems participating in TREC-9 and the
worldwide interest in this research area that
Question Answering is the most promising
framework for finding answers to natural
language questions from a huge amount of textual
data. Cardie et al. [2000] pointed out that
building “open-ended question answering systems
that allow users to pose questions of any type and
in any language, without domain restrictions, is
still beyond the scope of any QA system today” (p.
180). Harabagiu et al. [2000] indicated that
advanced tools (such as dialog understanding and
text mining) are essential for the success of future
QA systems. Until the advanced tools are
implemented, she suggested that we keep
approximating the complexity of Question
Answering with NLP enhancements of IR and IE
techniques [Harabagiu et al., 2000].
</bodyText>
<sectionHeader confidence="0.998962" genericHeader="method">
3 QARAB System
</sectionHeader>
<subsectionHeader confidence="0.999504">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999486466666667">
In the last decade, the volume of Arabic
textual data has started growing on the Web and
Arabic software for browsing the Web is
improving. Unfortunately, much of the earlier
Arabic text available on the Web was posted as
images, which makes it unsuitable for search or
processing. As of today, there is an increase in the
amount of Arabic textual material available on the
Web in the form of news articles and books.
The main goal of the QARAB system is to
identify text passages that answer a natural
language question. The task can be summarized as
follows: Given a set of questions expressed in
Arabic, find answers to the questions under the
following assumptions:
</bodyText>
<listItem confidence="0.999052142857143">
• The answer exists in a collection of Arabic
newspaper text extracted from the Al-Raya
newspaper published in Qatar.
• The answer does not span through documents
(i.e. all supporting information for the answer
lies in one document)
• The answer is a short passage.
</listItem>
<bodyText confidence="0.8902645">
The basic QA processing in QARAB is
composed of three major steps:
</bodyText>
<listItem confidence="0.972245875">
• Processing the input question
• Retrieving the candidate documents
(paragraphs) containing answers from the IR
system
• Processing each one of the candidate
documents (paragraphs) in the same way as
the question is processed and returning
sentences that may contain the answer.
</listItem>
<bodyText confidence="0.9997488">
The QARAB system will be evaluated over a
wide range of question types provided by Arabic
users during the testing and the final phases. The
same users will then assess whether the answers
produced by the system are satisfactory.
</bodyText>
<subsectionHeader confidence="0.999594">
3.2 QARAB Structure
</subsectionHeader>
<bodyText confidence="0.8565705">
The complete QARAB system is depicted in
Figure 1; it has the following overall structure:
</bodyText>
<subsectionHeader confidence="0.95578">
3.2.1 The IR System
</subsectionHeader>
<bodyText confidence="0.986271105263158">
The IR system, which we are implementing from
scratch, is based on Salton’s vector space model
[Salton, 1971]. First, it processes the text
collection from the Al-Raya newspaper and
constructs an inverted file system, from which the
answers to the natural language questions will be
extracted. The purpose of the IR system is to
search the document collection to select
documents containing information relevant to the
user’s query.
Implementing the Information Retrieval
System
Information Retrieval (IR) systems can be
constructed in many various ways. Lundquist et
al. [1999] proposed an Information Retrieval (IR)
system that can be constructed using a relational
database management system (RDBMS). Our IR
system is depicted in Figure 2 and it contains the
following database relations:
</bodyText>
<listItem confidence="0.945942392857143">
• ROOT_TABLE (Root_ID, Root) – to store the
available distinct roots of the terms extracted
from the Al-Raya document collection (one
row per root).
• STEM_TABLE (Stem_ID, Root_ID, Stem,
Document_Frequency, IDF) – to store all
distinct stems from the document collection.
The stem frequency in the entire document
collection and the inverse document
frequency of each stem are calculated and
stored (one row per stem).
• POSTING_TABLE (Posting_ID, Stem_ID,
Document_ID, Paragraph_ID, Position,
Length) – to store all the occurrences of the
stems extracted from the entire document
collection (one row per stem).
• DOCUMENT_TABLE (Document_ID,
Document_Title, Document_Date,
Document_Path) — to store document
information (one row per document)
• PARAGRAPH_TABLE (Paragraph_ID,
Document_ID, Paragraph) — to store all the
paragraphs extracted from the document
collection (one row per paragraph). This
speeds up the analysis and the processing of
the relevant passages that might answer to the
user’s question.
•
</listItem>
<bodyText confidence="0.9259471">
QUERY_TABLE (Word, Weight) – to store
query information. This includes the original
query words and the set of expanded words.
The set of expanded words is obtained by
extracting the available roots of the original
query words, finding their equivalent
Root_ID’s in the ROOT_TABLE, and then
finding their corresponding terms stored in the
STEM_TABLE. The weight of each word is
calculated and stored (one row per word).
</bodyText>
<figureCaption confidence="0.980255">
Figure 2. Relational Database Information
</figureCaption>
<figure confidence="0.882958318181818">
Retrieval System
Tokenizer
System
Morphology
Analyzer
System
Lexicon
Type Finder
System &amp;
Parsing PNP
System
Verb
Table
Pronoun
Table
Adjective
Table
Main
Table
Particle
Table
Keyword
Table
Noun
Table
Propernoun
Table
Category
Table
Personal
Name
Product
Table
Location
Political
Events
Table
Natural
Location
Time
Table
Organization
Table
NLP Tools
</figure>
<figureCaption confidence="0.999639">
Figure 1. System Components
</figureCaption>
<subsectionHeader confidence="0.887724">
3.2.2 The NLP System
</subsectionHeader>
<bodyText confidence="0.99955485">
The second component of the system (the NLP
system) shown in Figure 1 was implemented by
Abuleil [1999] to experiment in building a large
Arabic lexicon. The NLP system is composed of a
set of tools to tokenize and tag Arabic text,
identify some features of the tokens and, most
important, to identify proper names. The
following is a description of the overall structure
and functionality of the NLP system.
The tagger was designed to construct a
comprehensive Arabic lexicon. The system is
used to parse Arabic words and determine their
parts of speech (verbs, nouns, particles). Also it is
used to figure out the features of each word
(gender, number, person, tense), mark proper
nouns in the text and determine their types
(personal names, locations, organizations, times,
dates, etc.).
The NLP system comprises the following
modules:
</bodyText>
<listItem confidence="0.998047125">
• The tokenizer, which is used to extract the
tokens.
• The type finder, which is used to assign a
part-of-speech to each token.
• The feature finder, which is used to determine
the features of each word.
• The proper noun phrase parser, which is used
to mark proper nouns.
</listItem>
<bodyText confidence="0.999992666666667">
The type finder module starts a lexicon lookup
process for each token. When there is an unknown
word in the text, the system can apply the proper
noun phrase parser to tag the word as a proper
noun. The recognition process occurs in multiple
stages in which a list of patterns and heuristics
may be applied to mark the proper noun. When
the word is tagged as a proper noun, it is added
automatically to the lexicon with all its possible
features. Being able to identify the proper names,
among other actual entities, in the text is an
important step in understanding and using the text.
Unfortunately, this is not a straightforward task in
Arabic as it is in English and most European
languages since the uppercase/lowercase
distinction does not exist in Arabic text. Thus, we
have to learn more about the common patterns in
which these entities occur in Arabic contexts.
</bodyText>
<sectionHeader confidence="0.920551" genericHeader="method">
4 The Basic Outline of Processing
in the IR System
</sectionHeader>
<subsectionHeader confidence="0.995799">
4.1 Document Processing
</subsectionHeader>
<bodyText confidence="0.9998704">
This step is essential for our system. First, the
newspaper articles from the Al-Raya newspaper
are saved in text format using the Arabic Windows
1256 encoding scheme. This is performed to
extract all the html tags and to get the pure text
contents of the articles. Second, the IR system is
constructed using the relational database model as
explained above. This step involves tokenization,
stop-word removal, root extraction, and term
weighting.
</bodyText>
<subsectionHeader confidence="0.995304">
4.2 Extracting the Root
</subsectionHeader>
<bodyText confidence="0.999814">
In general, to extract Arabic roots from their
words, the stemmer has to process each word in
the following order [Khoja, 1999]:
</bodyText>
<listItem confidence="0.9998126">
• Removing the Definite Article لا “al”
• Removing the Conjunction Letter و “w”
• Removing Suffixes
• Removing Prefixes
• Pattern Matching
</listItem>
<bodyText confidence="0.920036285714286">
The following example demonstrates the
whole stemming process applied to the Arabic
word ﺎهﻮﺳرﺪﻴﻟو “wlydrsooha”, which is mapped to
the complete English sentence “and they are
going to study it”. The root of this word can be
extracted as follows:
(w)-(l)-(y)-drs-(oo)-(ha) (ﺎه)(و)سرد -(ي) -(ل)-(و )
</bodyText>
<listItem confidence="0.99913705">
1. Removing the conjunction letter (w) (و)
+ (ﺎه)(و)سرد -(ي)-(ل)
2. Removing the suffix (ha) (ﺎه), which indicates
a feminine, singular patient
+ (و)سرد -(ي) -(ل)
3. Removing the suffix: (oo) (و), which indicates
a masculine third person plural agent
+ سرد -(ي) -(ل)
4. Removing the preposition prefix (l) (ل)
+ سرد -(ي)
5. Removing the prefix: (y) (ي), which indicates
a 3rd person, present tense + سرد
6. The pattern ﻞﻌﻓ F9L has the same length as the
word سرد drs. Then the stemmer detects that
the word سرد matches the pattern ﻞѧﻌﻓ, since
all the letters of the word match those in the
pattern (i.e. ل ،ع ،ف)
7. Finally, the stemmer checks the trilateral roots
table and concludes that the root سرد drs (he
studied) is a valid root.
</listItem>
<sectionHeader confidence="0.90501" genericHeader="method">
5 Question Processing in QARAB
</sectionHeader>
<bodyText confidence="0.999967565217391">
Achieving question understanding requires deep
semantic processing, which is a non-trivial task of
natural language processing. In fact, Arabic NLP
does not have solid research at the semantic level.
Therefore, QARAB uses shallow language
understanding to process questions and it does not
attempt to understand the content of the question
at a deep, semantic level.
QARAB treats the incoming question as a
“bag of words” against which the index file is
searched to obtain a list of ranked documents that
possibly contain the answer. The question
processing begins by performing tokenization to
extract individual terms. Then, the stop-words are
removed. The remaining words are tagged for
part-of-speech in an attempt to highlight the main
words that should appear in the hypothesized
answer. The greatest effort should be spent on
identifying proper names, as they are our best
guidance to identify the possible answer. The
interrogative particles that precede the questions
will determine what types of answers are expected
as shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.993511">
5.1 Query Expansion
</subsectionHeader>
<bodyText confidence="0.99997975">
To achieve better search and retrieval results the
query is expanded to include all the terms (verbs
and nouns derived from verbs) that occur in the
index file and have the same roots, which were
extracted from the original query words. The
result of the query processing is then passed to the
IR system to retrieve a ranked list of documents
that match the terms of the query.
</bodyText>
<subsectionHeader confidence="0.999075">
5.2 Query Type
</subsectionHeader>
<bodyText confidence="0.991699666666667">
Questions are classified based on a set of known
“question types”. These question types help us to
determine the type of processing needed to
identify and extract the final answer. The QARAB
system recognizes the following set of question
types (Table1):
</bodyText>
<tableCaption confidence="0.901443">
Table 1. Question Types Processed by the
QARAB System
</tableCaption>
<table confidence="0.995435444444444">
Query Starting with Query Type
ﻦﻣ Who, Whose Person
ﻰﺘﻣ When Date, Time
ا ذﺎﻣ ،ﺎﻣ What, Which Organization, Product,
Event
ﻦﻳا Where Location (natural,
political)
ﻢآ How Much, Number, Quantity
How Many
</table>
<bodyText confidence="0.99982875">
There are two other types of question
particles, namely ﻒﻴآ and اذﺎﻤﻟ (How and Why).
Although they will form legitimate query
structures, they require long and procedural
answers and are beyond the scope of our research.
It is worth mentioning that the How and the Why
queries also caused problems for many TREC-8
participants.
</bodyText>
<subsectionHeader confidence="0.995072">
5.3 Query Keyword Identification
</subsectionHeader>
<bodyText confidence="0.999953266666667">
speech. This process requires using the Type-
Finder &amp; the Proper Name-Finder system
implemented by Abuleil [1999]. Verbs, which
almost always follow clear morphological
patterns, are the easiest to identify. Nouns,
especially proper nouns, are considered as our
best guide to find the expected answer from the
relevant documents returned by the IR system.
They have to occur within the selected answer
passage and must be in the same order as they
appeared in the original question. A list of
keywords to identify personal names,
organization names, locations, numbers, money
and dates, has been constructed for Arabic to help
in identifying proper names.
</bodyText>
<sectionHeader confidence="0.988679" genericHeader="method">
6 Answer Processing in QARAB
</sectionHeader>
<bodyText confidence="0.999944722222222">
The input to the QARAB Answer Generator
module is a natural language question and a small
set of ranked documents. The question is first
processed by tagging all the words. Then the set
of relevant documents that may contain the
answer are retrieved by the IR system. In the
answer generation process, the passages of the
relevant documents that match (are similar to) the
query’s “bag of words” closely are collected for
further processing. The answer zones usually
include most of the terms appearing in the original
query in addition to the proper nouns that should
appear in the final answer. The following
example illustrates the whole process taken by the
QARAB system to answer a question.
The following document extracted from the
newspaper Al-Raya published in Qatar was
processed by the IR system:
</bodyText>
<equation confidence="0.574744285714286">
ﺰﻳﺰﻌﻟا ﺪﺒﻋ ﻢﻟﺎﺳ ﺦﻴﺸﻟا ﻲﺘﻳﻮﻜﻟا يﺰآﺮﻤ ﻟا ﻚﻨﺒﻟا ﻆﻓﺎﺤﻣ لﺎﻗ
ﻲﺘﻳﻮﻜﻟا رﺎﻨﻳﺪﻟا ﺔﻤﻴﻗ ﺾﻔﺨﻟ ﺔﻴﻨﻟا ﺎﻬﻳﺪﻟ ﺲﻴﻟ ﻩدﻼﺑ نا ﺲﻣا حﺎﺒﺼﻟا
ﺔﻤﻴﻗ ﺾﻔﺧ نﺄﺑ لﺎﻗو . ﺔﻴﻧاﺰﻴﻤﻟا ﻲﻓ ﺪﻳاﺰﺘﻤﻟا ﺰﺠﻌﻟا ﻦﻣ ﺪﺤﻠﻟ
ﺔﻴﻟﺎﻤﻟا قاﻮﺳﻻا ﻲﻓ ﺎﻬﺘﻴﻗاﺪﺼﻣو ﺖﻳﻮﻜﻟا دﺎﺼﺘﻗﺎﺑ ﺮﻀﻴﺳ رﺎﻨﻳﺪﻟا
.ﺔﻴﻟوﺪﻟا
ﺔﻠﻴﺳﻮآ ﺔﻠﻤﻌﻟا ﺔﻤﻴﻗ ﺾﻔﺨﻳ ﻦﻟ يﺰآﺮﻤﻟا ﻚﻨﺒﻟا ن ا ﻢﻟﺎﺳ ﺦﻴﺸﻟا ﺪآأو
ﻲﻓ ﺰﺠﻌﻟا ﻎﻠﺒﻳ نا ﻊﻗﻮﺘﻤﻟا ﻦﻣو . ﺔﻴﻧاﺰﻴﻤﻟا ﻲﻓ ﺰﺠﻌﻟا ﺺﻴﻠﻘﺘﻟ
</equation>
<bodyText confidence="0.955925526315789">
The remaining words of the query (after removing تارﺎﻴﻠﻣ ﺔﺘﺳ ﻮﻴﻧﻮﻳ ﻲﻓ ﻲﻬﺘﻨﺗ ﻲﺘﻟا ١٩٩٩ / ١٩٩٨ مﺎﻋ ﺔﻴﻧاﺰﻴﻣ
punctuation and stop-words) are tagged for part of . رﻻود
Translated by ajeeb: www.ajeeb.com
Said the governor of the Kuwaiti central bank is
sheikh Salem Abd Al-Aziz Al-Sabah yesterday
that his countries not have her the intention to the
Kuwaiti dinar devaluation to the restriction from
the increasing inability in the budget. And
believed that the dinar devaluation will harm the
Kuwait economy and her credibility in the
international exchanges.
And confirmed the sheikh Salem is that the central
bank will not reduce the currency value as a
means to the inability reduction in the budget.
From it is expected that the inability in a budget
reaches a year 1998 / 1999 that ends in June is six
billions dollar.
Assume the user posed the following question to
QARAB:
</bodyText>
<equation confidence="0.720663333333333">
ﺲﻴﻟ ﻩدﻼﺑ نﺄﺑ لﺎﻗ يﺬﻟاو ﻲﺘﻳﻮﻜﻟا يﺰآﺮﻤﻟا ﻚﻨﺒﻟا ﻆﻓﺎﺤﻣ ﻮه ﻦﻣ
؟ﺔﻴﻧاﺰﻴﻤﻟ ا ﺰﺠﻋ ﻦﻣ ﺪﺤﻠﻟ رﺎﻨﻳﺪﻟا ﺔﻤﻴﻗ ﺾﻔﺨﻟ ﺔﻴﻨﻟا ﺎﻬﻳﺪﻟ
Translated by ajeeb: www.ajeeb.com
</equation>
<bodyText confidence="0.999194">
Who he is the governor of the Kuwaiti central
bank and that believed by that his country not
have her the intention to the dinar devaluation to
the restriction from the budget inability?
</bodyText>
<tableCaption confidence="0.998408333333333">
Step 1: The query is processed as shown in
Table 2
Table 2. Query Processing
</tableCaption>
<table confidence="0.9916106">
Token Stem Part of Stop
Speech Word
ﻮه he ﻮه Pronoun Yes
ﻆﻓﺎﺤﻣ governor ﻆﻓﺎﺤﻣ Noun
ﻚﻨﺒﻟا bank ﻚﻨﺑ Noun
يﺰآﺮﻤﻟا central ﺰآﺮﻣ Noun
ﻲﺘﻳﻮﻜﻟا Kuwaiti ﺖﻳﻮآ Noun
و and و Conjunction Yes
يﺬﻟا that يﺬﻟا Pronoun Yes
لﺎﻗ said لﺎﻗ Verb
نﺄﺑ that نﺄﺑ Particle Yes
ﻩدﻼﺑ his country دﻼﺑ Noun
ﺲﻴﻟ not ﺲﻴﻟ Verb Yes
ﺎﻬﻳﺪﻟ have ىﺪﻟ Particle Yes
ﺔﻴﻨﻟا intention ﺔﻴﻧ Noun
ﺾﻔﺨﻟ devaluation ﺾﻔﺧ Noun
ﺔﻤﻴﻗ value ﺔﻤﻴﻗ Noun
رﺎﻨﻳﺪﻟا dinar رﺎﻨﻳد Noun
ﺪﺤﻠﻟ restriction ﺪﺣ Noun
ﻦﻣ from ﻦﻣ Preposition Yes
ﺰﺠﻋ inability ﺰﺠﻋ Noun
ﺔﻴﻧاﺰﻴﻤﻟا budget ﺔﻴﻧاﺰﻴﻣ Noun
؟ ؟ ؟ Punctuation Yes
Step 2: QARAB constructs the query as a “bag
of words” and passes it to the IR system
</table>
<tableCaption confidence="0.992995">
Table 3. Bag of words
</tableCaption>
<equation confidence="0.869551333333333">
ﻆﻓﺎﺤﻣ
ﻚﻨﺑ
ﺰآﺮﻣ
ﺖﻳﻮآ
دﻼﺑ
ﺔﻴﻧ
ﺾﻔﺧ
ﺔﻤﻴﻗ
رﺎﻨﻳد
ﺪﺣ
ﺰﺠﻋ
ﺔﻴﻧاﺰﻴﻣ
</equation>
<bodyText confidence="0.997192">
Assume the system returned the following
document as the top ranked document that closely
matches the query.
</bodyText>
<table confidence="0.997746222222222">
ﺰﻳﺰﻌﻟا ﺪﺒﻋ ﻢﻟﺎﺳ ﺦﻴﺸﻟا ﻲﺘﻳﻮﻜﻟا يﺰآﺮﻤﻟا ﻚﻨﺒﻟا ﻆﻓﺎﺤﻣ لﺎﻗ
رﺎﻨﻳﺪﻟا ﺔﻤﻴﻗ ﺾﻔﺨﻟ ﺔﻴﻨﻟا ﺎﻬﻳﺪﻟ ﺲﻴﻟ ﻩدﻼﺑ نا ﺲﻣا حﺎﺒﺼﻟا
ﺾﻔﺧ نﺄﺑ لﺎﻗو .ﺔﻴﻧاﺰﻴﻤﻟا ﻲﻓ ﺪﻳاﺰﺘﻤﻟا ﺰﺠﻌﻟا ﻦﻣ ﺪﺤﻠﻟ ﻲﺘﻳﻮﻜﻟا
قاﻮﺳﻻا ﻲﻓ ﺎﻬﺘﻴﻗاﺪﺼﻣو ﺖﻳﻮﻜﻟا دﺎﺼﺘﻗﺎﺑ ﺮﻀﻴﺳ رﺎﻨﻳﺪﻟا ﺔﻤﻴﻗ
.ﺔﻴﻟوﺪﻟا ﺔﻴﻟﺎﻤﻟا
ﺔﻠﻴﺳﻮآ ﺔﻠﻤﻌﻟا ﺔﻤﻴﻗ ﺾﻔﺨﻳ ﻦﻟ يﺰآﺮﻤﻟا ﻚﻨﺒﻟا نا ﻢﻟﺎﺳ ﺦﻴﺸﻟا ﺪآأو
ﻲﻓ ﺰﺠﻌﻟا ﻎﻠﺒﻳ نا ﻊﻗﻮﺘﻤﻟا ﻦﻣو . ﺔﻴﻧاﺰﻴﻤﻟا ﻲﻓ ﺰﺠﻌﻟا ﺺﻴﻠﻘﺘﻟ
تارﺎﻴﻠﻣ ﺔﺘﺳ ﻮﻴﻧﻮﻳ ﻲﻓ ﻲﻬﺘﻨﺗ ﻲﺘﻟا ١٩٩٩ / ١٩٩٨ مﺎﻋ ﺔﻴﻧاﺰﻴﻣ
.رﻻود
</table>
<footnote confidence="0.679026333333333">
Step 3: Determine the expected type of the
answer
ﻦﻣ “Who” Î Person Name
</footnote>
<figureCaption confidence="0.236129">
Step 4: Generating the answer
</figureCaption>
<bodyText confidence="0.998353666666667">
The Answer Generator looks for keywords that
might identify a person name using the personal
names keywords. The input to the Answer
Generator is the “bag of words” and the
paragraphs extracted from the top ranked relevant
documents.
</bodyText>
<equation confidence="0.976788111111111">
ﺰﻳﺰﻌﻟا ﺪﺒﻋ ﻢﻟﺎﺳ ﺦﻴﺸﻟا ﻲﺘﻳﻮﻜﻟا يﺰآﺮﻤﻟا ﻚﻨﺒﻟا ﻆﻓﺎﺤﻣ لﺎﻗ
رﺎﻨﻳﺪﻟا ﺔﻤﻴﻗ ﺾﻔﺨﻟ ﺔﻴﻨﻟا ﺎﻬﻳﺪﻟ ﺲﻴﻟ ﻩدﻼﺑ نا ﺲﻣا حﺎﺒﺼﻟا
ﺾﻔﺧ نﺄﺑ لﺎﻗو . ﺔﻴﻧاﺰﻴﻤﻟا ﻲﻓ ﺪﻳاﺰﺘﻤﻟا ﺰﺠﻌﻟا ﻦﻣ ﺪﺤﻠﻟ ﻲﺘﻳﻮﻜﻟا
قاﻮﺳﻻا ﻲﻓ ﺎﻬﺘﻴﻗاﺪﺼﻣو ﺖﻳﻮﻜﻟا دﺎﺼﺘﻗﺎﺑ ﺮﻀﻴﺳ رﺎﻨﻳﺪﻟا ﺔﻤﻴﻗ
.ﺔﻴﻟوﺪﻟا ﺔﻴﻟﺎﻤﻟا
ﺔﻠﻴﺳﻮآ ﺔﻠﻤﻌﻟا ﺔﻤﻴﻗ ﺾﻔﺨﻳ ﻦﻟ يﺰآﺮﻤﻟا ﻚﻨﺒﻟا نا ﻢﻟﺎﺳ ﺦﻴﺸﻟا ﺪآأو
ﻲﻓ ﺰﺠﻌﻟا ﻎﻠﺒﻳ نا ﻊﻗﻮﺘﻤﻟا ﻦﻣو .ﺔﻴﻧاﺰﻴﻤﻟا ﻲﻓ ﺰﺠﻌﻟا ﺺﻴﻠﻘﺘﻟ
تارﺎﻴﻠﻣ ﺔﺘﺳ ﻮﻴﻧﻮﻳ ﻲﻓ ﻲﻬﺘﻨﺗ ﻲﺘﻟا
. رﻻود
</equation>
<sectionHeader confidence="0.968208" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999767066666667">
We have described an approach to question
answering system that provides short answers to
questions expressed in the Arabic language. The
system utilizes techniques from IR and NLP to
process a collection of Arabic text documents as
its primary source of knowledge. An actual
system named QARAB is implemented and an
initial ad-hoc analysis seems to be promising. The
overall success of the system is limited to the
amount of available tools developed for the
Arabic language. Work is undergoing to get
retrieval integrated into the system and to extend
the functionality of the NLP system by developing
more sophisticated algorithms to produce a
concise answer in a timely manner.
</bodyText>
<figure confidence="0.8767205">
/
١٩٩٨
١٩٩٩
مﺎﻋ
ﺔﻴﻧاﺰﻴﻣ
Keywords that might identify personal names:
</figure>
<bodyText confidence="0.9902485">
The keyword ﺦﻴﺸﻟا sheikh is used to mark an
Arabic personal name.
The keyword ﺪﺒﻋ ِAbd is used to mark the
beginning of a personal name.
ﺰﻳﺰﻌﻟا ﺪﺒﻋ ﻢﻟﺎﺳ ﺦﻴﺸﻟا ﻲﺘﻳﻮﻜﻟا يﺰآﺮﻤﻟا ﻚﻨﺒﻟا ﻆﻓﺎﺤﻣ لﺎﻗ
ﻲﺘﻳﻮﻜﻟا رﺎﻨﻳﺪﻟا ﺔﻤﻴﻗ ﺾﻔﺨﻟ ﺔﻴﻨﻟا ﺎﻬﻳﺪﻟ ﺲﻴﻟ ﻩدﻼﺑ نا ﺲﻣا حﺎﺒﺼﻟا
رﺎﻨﻳﺪﻟا ﺔﻤﻴﻗ ﺾﻔﺧ نﺄﺑ لﺎﻗو . ﺔﻴﻧاﺰﻴﻤﻟا ﻲﻓ ﺪﻳاﺰﺘﻤﻟا ﺰﺠﻌﻟا ﻦﻣ ﺪﺤﻠﻟ
. ﺔﻴﻟوﺪﻟا ﺔﻴﻟﺎﻤﻟا قاﻮﺳﻻا ﻲﻓ ﺎﻬﺘﻴﻗاﺪﺼﻣو ﺖﻳﻮﻜﻟا دﺎﺼﺘﻗﺎﺑ ﺮﻀﻴﺳ
</bodyText>
<equation confidence="0.9640925">
ﺔﻠﻤﻌﻟا ﺔﻤﻴﻗ ﺾﻔﺨﻳ ﻦﻟ يﺰآﺮﻤﻟا ﻚﻨﺒﻟا نا ﻢﻟﺎﺳ ﺦﻴﺸﻟا ﺪآأو
ﺰﺠﻌﻟا ﻎﻠﺒﻳ نا ﻊﻗﻮﺘﻤﻟا ﻦﻣو . ﺔﻴﻧاﺰﻴﻤﻟا ﻲﻓ ﺰﺠﻌﻟا ﺺﻴﻠﻘﺘﻟ ﺔﻠﻴﺳﻮآ
تارﺎﻴﻠﻣ ﺔﺘﺳ ﻮﻴﻧﻮﻳ ﻲﻓ ﻲﻬﺘﻨﺗ ﻲﺘﻟا ١٩٩٩ / ١٩٩٨ مﺎﻋ ﺔﻴﻧاﺰﻴﻣ ﻲﻓ
. رﻻود
</equation>
<bodyText confidence="0.9995005">
The first paragraph has most of the query words
and the keywords that might identify a personal
name. Therefore, the first paragraph is returned as
the potential answer.
ﺰﻳﺰﻌﻟا ﺪﺒﻋ ﻢﻟﺎﺳ ﺦﻴﺸﻟاﻲﺘﻳﻮﻜﻟا يﺰآﺮﻤﻟا ﻚﻨﺒﻟا ﻆﻓﺎﺤﻣ لﺎﻗ
ﻲﺘﻳﻮﻜﻟا رﺎﻨﻳﺪﻟا ﺔﻤﻴﻗ ﺾﻔﺨﻟ ﺔﻴﻨﻟا ﺎﻬﻳﺪﻟ ﺲﻴﻟ ﻩدﻼﺑ نا ﺲﻣا حﺎﺒﺼﻟا
رﺎﻨﻳﺪﻟا ﺔﻤﻴﻗ ﺾﻔﺧ نﺄﺑ لﺎﻗو . ﺔﻴﻧاﺰﻴﻤﻟا ﻲﻓ ﺪﻳاﺰﺘﻤﻟا ﺰﺠﻌﻟا ﻦﻣ ﺪﺤﻠﻟ
.ﺔﻴﻟوﺪﻟا ﺔﻴﻟﺎﻤﻟا قاﻮﺳﻻا ﻲﻓ ﺎﻬﺘﻴﻗاﺪﺼﻣو ﺖﻳﻮﻜﻟا دﺎﺼﺘﻗﺎﺑ ﺮﻀﻴﺳ
</bodyText>
<sectionHeader confidence="0.979878" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.7113525">
Abuleil, S., and Evens, M., 1998. “Discovering
Lexical Information by Tagging Arabic
Newspaper Text”, Workshop on Semantic
Language Processing. COLING-ACL ’98,
University of Montreal, Montreal, PQ,
Canada, Aug. 16 1998, pp. 1-7.
</bodyText>
<reference confidence="0.999353443243243">
Al-Daimi, K., and Abdel-Amir, M. 1994. “The
Syntactic Analysis of Arabic by Machine”.
Computers and Humanities, Vol. 28, No. 1,
pp. 29-37.
Allan, J., Callan, J., Feng, F-F., and Malin D.
1999. “INQUERY and TREC-8”. Proceedings
of the 8th Text REtrieval Conference (TREC-
8), NIST Special Publications 500-246, pp.
637-645.
Ask Jeeves. 1996. www.ask.com Site last visited
in March 2001.
Breck, E., Burger, J., Ferro, L., House, D., Light,
M., and Mani, I. 1999. “A Sys Called Qanda”.
Proceedings of the 8th Text REtrieval
Conference, NIST Special Publications, pp.
499-507.
Budzik, J. and Hammond, K. 1999. “Q&amp;A: A
System for the Capture, Organization and
Reuse of Expertise”. Proceedings of the Sixty-
second Annual Meeting of the American
Society for Information Science. Information
Today, Inc., Medford, NJ. Available on the
Web at
http://dent.infolab.nwu.edu/infolab/downloads
/papers/paper10061.pdf. Site last visited in
August 2001.
Burke, R., Hammond, K., Kulyukin, V., Lytinen,
S., Tomuro, N., and Schoenberg, S. 1997.
“Question Answering from Frequently-Asked
Question Files: Experiences with the FAQ
Finder System”. AI Magazine, Vol. 18, No.2,
pp. 57-66.
Cardie, C., Ng, V., Pierce, D., and Buckley, C.
2000. “Examining the Role of Statistical and
Linguistic Knowledge Sources in a General-
Knowledge Question-Answering System”.
Proceedings of the Sixth Applied Natural
Language Processing Conference, pp. 180-
187.
Cormack, G., Clarke, C., and Kisman, D. 1999.
“Fast Automatic Passage Ranking (MultiText
Experiments for TREC-8)”. Proceedings of
the 8th Text REtrieval Conference (TREC-8),
NIST Special Publications 500-246, pp. 735-
743.
Ferret, O., Grau, B., Illouz, G., Jacquemin, C., and
Masson, N. 1999. “QALC - the Question-
Answering Program of the Language and
Cognition Group at LIMSI-CNRS”.
Proceedings of the 8th Text REtrieval
Conference, NIST Special Publications, pp.
465-475.
Harabagiu, S., Pasca, M., and Maiorano, S. 2000.
“Experiments with Open-Domain Textual
Question Answering”. Proceedings of 18th
International Conference on Computational
Linguistics (COLING-2000), Saarbrucken,
Germany, pp. 292-298
Hull, D. 1999. “Xerox TREC-8 Question
Answering Track Report”. Proceedings of the
8thText REtrieval Conference (TREC-8),
NIST Special Publications 500-246, pp. 743-
751.
Humphreys, K., Gaizauskas, R., Hepple, M., and
Sanderson, M. 1999. “University of Sheffield
TREC-8 Q &amp; A System”. Proceedings of the
8th Text REtrieval Conference (TREC-8),
NIST Special Publications 500-246, pp. 707-
717.
Jacobs, P., and Rau, L. 1990. “SCISOR:
Extracting Information from On-line News”.
Communications of the ACM, Vol. 33, No.11,
pp. 88-97.
Katz, B. 1997. “From Sentence Processing to
Information Access on the World Wide Web”.
Proceedings of the American Association for
Artificial Intelligence Conference, Spring
Symposium, NLP for WWW, pp. 77-86.
Khoja, S. 1999. “Stemming Arabic Text”.
Available on the Web at:
http://www.comp.lancs.ac.uk/computing/users
/khoja/stemmer.ps. Site last visited in March
2001.
Kupiec, J. 1993. “MURAX: A Robust Linguistic
Approach for Question Answering Using an
On-line Encyclopedia”. Proceedings of the
16th Annual Int. ACM SIGIR Conference, pp.
181-190.
Lehnert, W. 1978. The Process of Question
Answering. Lawrence Erlbaum Associates,
Hillsdale, NJ.
Lin, C-J, and Chen, H-H. 1999. “Description of
Preliminary Results to TREC-8 QA Task”.
Proceedings of the 8th Text REtrieval
Conference(TERC-8), NIST Special
Publications 500-246, pp. 507-513.
Litkowski, K. 1999. “Question-Answering Using
Semantic Relation Triples”. Proceedings of
the 8th Text REtrieval Conference (TREC-8),
NIST Special Publications 500-248, pp. 349-
357
Lundquist, C., Grossman, D., and Frieder, O.
1999. &amp;quot;Improving Relevance Feedback in the
Vector Space Model&amp;quot;. Proceedings of 6th
ACM Annual Conference on Information and
Knowledge Management (CIKM), pp. 16-23.
Moldovan, D., Harabagiu, S., Pasca, M.,
Mihalcea, R., Girju, R., Goodrum, R., and
Rus, V. 2000. “The Structure and
Performance of an Open-Domain Question-
Answering System”. Proceedings of the 38th
Annual Meeting of the Association for
Computational Linguistics, pp. 563-570.
Oard, D., Wang, J., Lin, D., and Soboroff, I. 1999.
“TREC-8 Experiments at Maryland: CLIR,
QA and Routing”. Proceedings of the 8th Text
REtrieval Conference (TERC-8), NIST
Special Publications 500-246, pp. 623-637.
Ogden, B., Cowie, J., Ludovik, E. Molina-
Salgado, H., Nirenburg, S., Sharples, N., and
Sheremtyeva, S. 1999. “CRL&apos;s TREC-8
Systems Cross-Lingual IR, and Q&amp;A”.
Proceedings of the 8th Text REtrieval
Conference (TERC-8), NIST Special
Publications 500-246, pp. 513-523.
Salton, G. 1971. The SMART Retrieval System
Experiments in Automatic Document
Processing. Prentice Hall Inc., Englewood
Cliffs, NJ.
Schank, R., and Abelson, R. 1977. Scripts, Plans,
Goals, and Understanding. Lawrence Erlbaum
Associates, Hillsdale, NJ.
Shin, D-H, Kim, Y-H, Kim, S., Eom, J-H, Shin,
H-J, and Zhang B-T. 1999. “SCAI TREC-8
Experiments”. Proceedings of the 8th Text
REtrieval Conference (TREC-8), NIST
Special Publications 500-246, pp. 583-591.
Singhal, A., Abney, S., Bacchiani, M., Collins,
M., Hindle, D., and Pereira, F. 1999. “AT&amp;T
at TREC-8”. Proceedings of the 8th Text
REtrieval Conference, NIST Special
Publications, pp. 317-331.
Srihari, R., and Li, W. 1999. “Information
Extraction Supported Question Answering”.
Proceedings of the 8th Text REtrieval
Conference (TREC-8), NIST Special
Publications 500-246, pp. 185-197.
Takaki, T. 1999. “NTT DATA: Overview of
System Approach at TREC-8 ad-hoc and
Question Answering”. Proceedings of the 8th
Text REtrieval Conference (TREC-8), NIST
Special Publications 500-246, pp. 523-531.
TREC-8. 1999. NIST Special Publication 500-
246: The Eighth Text REtrieval Conference.
Available on the Web at:
http://trec.nist.gov/pubs/trec8/t8 proceedings.
html. Site last visited in August 2001.
TREC-9. 2000. NIST Special Publication: The
Ninth Text REtrieval Conference. Available
on the Web at:
http://trec.nist.gov/pubs/trec9/t9 proceedings.
html. Site last visited in August 2001.
Vicedo, J., and Ferrández, A. 2000. “Importance
of Pronominal Anaphora Resolution in
Question- Answering System”. Proceedings
of the 38th Annual Meeting of the Association
for Computational Linguistics, pp. 555-562.
Voorhees, E., and Tice, D. 1999. &amp;quot;The TREC-8
Question Answering Track Evaluation&amp;quot;.
Proceedings of the 8th Text REtrieval
Conference (TREC-8), NIST Special
Publication 500-246, pp. 83-106.
Voorhees, E., and Tice, D. 2000. “Building a
Question Answering Test Collection”.
Proceedings of the 23rd Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
Athens, Greece, pp. 200-207.
Winograd, T. 1972. Understanding Natural
Language. Academic Press, New York, NY.
Woods, W., Kaplan, R., and Webber, B. 1972.
“The Lunar Sciences Natural Language
Information System: Final Report”. Bolt
Beranek and Newman Inc. (BBN), Report No.
2378.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814283">
<title confidence="0.9975215">QARAB: A Question Answering System to the Arabic Language</title>
<author confidence="0.999647">Bassam Hammo Hani Abu-Salem Steven Lytinen</author>
<affiliation confidence="0.999331">DePaul University School of Computer Science, Telecommunications and Information</affiliation>
<address confidence="0.991811">243 S. Wabash Avenue, Chicago IL 60604</address>
<email confidence="0.994531">habusalem@cti.depaul.edulytinen@cs.depaul.edu</email>
<author confidence="0.964948">Martha</author>
<affiliation confidence="0.9990555">Illinois Institute of Computer Science</affiliation>
<address confidence="0.901424">West Street, Chicago, IL</address>
<email confidence="0.999627">evens@iit.edu</email>
<abstract confidence="0.998626285714285">We describe the design and implementation of a question answering (QA) system called QARAB. It is a system that takes natural language questions expressed in the Arabic language and attempts to provide short answers. The system’s primary source of knowledge is a collection of Arabic newspaper text extracted from Al-Raya, a newspaper published in Qatar. During the last few years the information retrieval community has attacked this problem for English using standard IR techniques with only mediocre success. We are tackling this problem for Arabic using traditional Information Retrieval (IR) techniques coupled with a sophisticated Natural Language Processing (NLP) approach. To identify the answer, we adopt a keyword matching strategy along with matching simple structures extracted from both the question and the candidate documents selected by the IR system. To achieve this goal, we use an existing tagger to identify proper names and other crucial lexical items and build lexical entries for them on the fly. We also carry out an analysis of Arabic question forms and attempt a better understanding of what kinds of answers users find satisfactory. The paucity of studies of real users has limited results in earlier research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Al-Daimi</author>
<author>M Abdel-Amir</author>
</authors>
<title>The Syntactic Analysis of Arabic by Machine”.</title>
<date>1994</date>
<journal>Computers and Humanities,</journal>
<volume>28</volume>
<pages>29--37</pages>
<contexts>
<context position="3134" citStr="Al-Daimi &amp; Abdel-Amir, 1994" startWordPosition="479" endWordPosition="482">em treats the question as a query in an attempt to identify the candidate documents that may contain the answer; then the NLP techniques are used to parse the question and analyze the top ranked documents returned by the IR system. Natural Language Processing (NLP) in the Arabic language is still in its initial stage compared to the work in the English language, which has already benefited from the extensive research in this field. There are some aspects that slow down progress in Arabic Natural Language Processing (NLP) compared to the accomplishments in English and other European languages [Al-Daimi &amp; Abdel-Amir, 1994]. These aspects include: • Arabic is highly inflectional and derivational, which makes morphological analysis a very complex task. • The absence of diacritics (which represent most vowels) in the written text creates ambiguity and therefore, complex morphological rules are required to identify the tokens and parse the text. • The writing direction is from right-to-left and some of the characters change their shapes based on their location in the word. • Capitalization is not used in Arabic, which makes it hard to identify proper names, acronyms, and abbreviations. In addition to the above lin</context>
</contexts>
<marker>Al-Daimi, Abdel-Amir, 1994</marker>
<rawString>Al-Daimi, K., and Abdel-Amir, M. 1994. “The Syntactic Analysis of Arabic by Machine”. Computers and Humanities, Vol. 28, No. 1, pp. 29-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>J Callan</author>
<author>F-F Feng</author>
<author>D Malin</author>
</authors>
<date>1999</date>
<booktitle>INQUERY and TREC-8”. Proceedings of the 8th Text REtrieval Conference (TREC8), NIST Special Publications 500-246,</booktitle>
<pages>637--645</pages>
<contexts>
<context position="7692" citStr="Allan et al., 1999" startWordPosition="1159" endWordPosition="1162">Twenty organizations participated in this track with different approaches and their systems were evaluated. The participating systems were tested on a huge set of unstructured documents and a set of fact-based questions. Generally speaking, most of the TREC-8 long-string answer (250-bytes) participants attempted to solve the QA problem from the information retrieval (IR) point of view by locating the most relevant documents from the collection and then extracting the sentences most relevant to the query from the documents just located. The systems relying on this “bag-ofwords” approach (e.g. [Allan et al., 1999]; [Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litko</context>
</contexts>
<marker>Allan, Callan, Feng, Malin, 1999</marker>
<rawString>Allan, J., Callan, J., Feng, F-F., and Malin D. 1999. “INQUERY and TREC-8”. Proceedings of the 8th Text REtrieval Conference (TREC8), NIST Special Publications 500-246, pp. 637-645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ask Jeeves</author>
</authors>
<title>www.ask.com Site last visited in</title>
<date>1996</date>
<marker>Jeeves, 1996</marker>
<rawString>Ask Jeeves. 1996. www.ask.com Site last visited in March 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Breck</author>
<author>J Burger</author>
<author>L Ferro</author>
<author>D House</author>
<author>M Light</author>
<author>I Mani</author>
</authors>
<title>A Sys Called Qanda”.</title>
<date>1999</date>
<booktitle>Proceedings of the 8th Text REtrieval Conference, NIST Special Publications,</booktitle>
<pages>499--507</pages>
<contexts>
<context position="8220" citStr="Breck et al., 1999" startWordPosition="1240" endWordPosition="1243">just located. The systems relying on this “bag-ofwords” approach (e.g. [Allan et al., 1999]; [Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan et al., 2000]; [Oard et al., 1999]; [Singhal et al., 1999]) agreed on the importance of applying several natural language processing techniques to solve the problem. Among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that conta</context>
</contexts>
<marker>Breck, Burger, Ferro, House, Light, Mani, 1999</marker>
<rawString>Breck, E., Burger, J., Ferro, L., House, D., Light, M., and Mani, I. 1999. “A Sys Called Qanda”. Proceedings of the 8th Text REtrieval Conference, NIST Special Publications, pp. 499-507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Budzik</author>
<author>K Hammond</author>
</authors>
<title>Q&amp;A: A System for the Capture, Organization and Reuse of Expertise”.</title>
<date>1999</date>
<booktitle>Proceedings of the Sixtysecond Annual Meeting of the American Society for Information Science. Information Today,</booktitle>
<publisher>Inc.,</publisher>
<location>Medford, NJ.</location>
<contexts>
<context position="6179" citStr="Budzik &amp; Hammond, 1999" startWordPosition="926" endWordPosition="929">eb links that might contain information relevant to the answer to the question. Ask Jeeves benefited from advanced natural language processing techniques combined with data mining processing and a huge expanding knowledge base. Another system, with a different approach, is the FAQFinder system [Burke et al., 1997], which attempted to solve the question-answering problem using a database of question-answer pairs built from existing frequently asked question (FAQ) files. Two other important systems are the START system [Katz, 1997], which is based on annotations from the Web and the Q&amp;A system [Budzik &amp; Hammond, 1999], which is a semiautomatic, natural language questionanswering and referral system. The system is based on a huge knowledge base and human experts who volunteered their time to respond to the users’ questions. Recently, attention has begun to be focused on developing question-answering systems that do not rely on a knowledge base and that can fetch answers from huge unstructured text. New QA systems enhanced with NLP and IR techniques have been developed to extract textual answers for open-domain questions and provide a framework for modern information retrieval [TREC-8, 1999; TREC-9, 2000]. </context>
</contexts>
<marker>Budzik, Hammond, 1999</marker>
<rawString>Budzik, J. and Hammond, K. 1999. “Q&amp;A: A System for the Capture, Organization and Reuse of Expertise”. Proceedings of the Sixtysecond Annual Meeting of the American Society for Information Science. Information Today, Inc., Medford, NJ. Available on the Web at http://dent.infolab.nwu.edu/infolab/downloads /papers/paper10061.pdf. Site last visited in August 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Burke</author>
<author>K Hammond</author>
<author>V Kulyukin</author>
<author>S Lytinen</author>
<author>N Tomuro</author>
<author>S Schoenberg</author>
</authors>
<title>Question Answering from Frequently-Asked Question Files: Experiences with the FAQ Finder System”.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<pages>57--66</pages>
<contexts>
<context position="5871" citStr="Burke et al., 1997" startWordPosition="879" endWordPosition="882">earches. The MURAX system [Kupiec, 1993] used robust linguistic methods to answer closed-class natural language questions. It presented the user with relevant text in which noun phrases are marked. A less automated approach like Ask Jeeves [1996] approached the QA problem by pointing the questioner to Web links that might contain information relevant to the answer to the question. Ask Jeeves benefited from advanced natural language processing techniques combined with data mining processing and a huge expanding knowledge base. Another system, with a different approach, is the FAQFinder system [Burke et al., 1997], which attempted to solve the question-answering problem using a database of question-answer pairs built from existing frequently asked question (FAQ) files. Two other important systems are the START system [Katz, 1997], which is based on annotations from the Web and the Q&amp;A system [Budzik &amp; Hammond, 1999], which is a semiautomatic, natural language questionanswering and referral system. The system is based on a huge knowledge base and human experts who volunteered their time to respond to the users’ questions. Recently, attention has begun to be focused on developing question-answering syst</context>
</contexts>
<marker>Burke, Hammond, Kulyukin, Lytinen, Tomuro, Schoenberg, 1997</marker>
<rawString>Burke, R., Hammond, K., Kulyukin, V., Lytinen, S., Tomuro, N., and Schoenberg, S. 1997. “Question Answering from Frequently-Asked Question Files: Experiences with the FAQ Finder System”. AI Magazine, Vol. 18, No.2, pp. 57-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>V Ng</author>
<author>D Pierce</author>
<author>C Buckley</author>
</authors>
<title>Examining the Role of Statistical and Linguistic Knowledge Sources in a GeneralKnowledge Question-Answering System”.</title>
<date>2000</date>
<booktitle>Proceedings of the Sixth Applied Natural Language Processing Conference,</booktitle>
<pages>180--187</pages>
<marker>Cardie, Ng, Pierce, Buckley, 2000</marker>
<rawString>Cardie, C., Ng, V., Pierce, D., and Buckley, C. 2000. “Examining the Role of Statistical and Linguistic Knowledge Sources in a GeneralKnowledge Question-Answering System”. Proceedings of the Sixth Applied Natural Language Processing Conference, pp. 180-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cormack</author>
<author>C Clarke</author>
<author>D Kisman</author>
</authors>
<title>Fast Automatic Passage Ranking (MultiText Experiments for TREC-8)”.</title>
<date>1999</date>
<booktitle>Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-246,</booktitle>
<pages>735--743</pages>
<contexts>
<context position="7716" citStr="Cormack et al., 1999" startWordPosition="1163" endWordPosition="1166">articipated in this track with different approaches and their systems were evaluated. The participating systems were tested on a huge set of unstructured documents and a set of fact-based questions. Generally speaking, most of the TREC-8 long-string answer (250-bytes) participants attempted to solve the QA problem from the information retrieval (IR) point of view by locating the most relevant documents from the collection and then extracting the sentences most relevant to the query from the documents just located. The systems relying on this “bag-ofwords” approach (e.g. [Allan et al., 1999]; [Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan e</context>
</contexts>
<marker>Cormack, Clarke, Kisman, 1999</marker>
<rawString>Cormack, G., Clarke, C., and Kisman, D. 1999. “Fast Automatic Passage Ranking (MultiText Experiments for TREC-8)”. Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-246, pp. 735-743.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Ferret</author>
<author>B Grau</author>
<author>G Illouz</author>
<author>C Jacquemin</author>
<author>N Masson</author>
</authors>
<date>1999</date>
<booktitle>QALC - the QuestionAnswering Program of the Language and Cognition Group at LIMSI-CNRS”. Proceedings of the 8th Text REtrieval Conference, NIST Special Publications,</booktitle>
<pages>465--475</pages>
<contexts>
<context position="8243" citStr="Ferret et al., 1999" startWordPosition="1244" endWordPosition="1247">ems relying on this “bag-ofwords” approach (e.g. [Allan et al., 1999]; [Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan et al., 2000]; [Oard et al., 1999]; [Singhal et al., 1999]) agreed on the importance of applying several natural language processing techniques to solve the problem. Among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that contain the possible answer.</context>
</contexts>
<marker>Ferret, Grau, Illouz, Jacquemin, Masson, 1999</marker>
<rawString>Ferret, O., Grau, B., Illouz, G., Jacquemin, C., and Masson, N. 1999. “QALC - the QuestionAnswering Program of the Language and Cognition Group at LIMSI-CNRS”. Proceedings of the 8th Text REtrieval Conference, NIST Special Publications, pp. 465-475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>S Maiorano</author>
</authors>
<title>Experiments with Open-Domain Textual Question Answering”.</title>
<date>2000</date>
<booktitle>Proceedings of 18th International Conference on Computational Linguistics (COLING-2000),</booktitle>
<pages>292--298</pages>
<location>Saarbrucken, Germany,</location>
<contexts>
<context position="4320" citStr="Harabagiu et al., 2000" startWordPosition="652" endWordPosition="655">ations. In addition to the above linguistic issues, there is also a lack of Arabic corpora, lexicons, and machine-readable dictionaries, which are essential to advance research in different areas. 2 Background Advances in natural language processing (NLP), information retrieval techniques (IR), information extraction (IE), as well as the computer industry, have given QA a strong boost. Modern questionanswering systems have started incorporating NLP techniques to parse natural language documents, extract entities and relations between entities, resolve anaphora, and other language ambiguities [Harabagiu et al., 2000; Vicedo &amp; Ferrández, 2000]. Research in Question-Answering (QA) is not new. The QA problem has been addressed in the literature since the beginning of computing machines. The AI/NLP communities initiated traditional work to address question-answering using structural methods. Early experiments in this direction implemented systems that operate in very restricted domains (e.g. SHRDLU [Winogard, 1972] and LUNAR [Woods, 1972]). In the QUALM system, Lehnert [1978] took a further step, based on the conceptual theories of Schank &amp; Abelson [1977], to understand the nature of the questions and classi</context>
<context position="9993" citStr="Harabagiu et al., 2000" startWordPosition="1515" endWordPosition="1518"> from a huge amount of textual data. Cardie et al. [2000] pointed out that building “open-ended question answering systems that allow users to pose questions of any type and in any language, without domain restrictions, is still beyond the scope of any QA system today” (p. 180). Harabagiu et al. [2000] indicated that advanced tools (such as dialog understanding and text mining) are essential for the success of future QA systems. Until the advanced tools are implemented, she suggested that we keep approximating the complexity of Question Answering with NLP enhancements of IR and IE techniques [Harabagiu et al., 2000]. 3 QARAB System 3.1 Overview In the last decade, the volume of Arabic textual data has started growing on the Web and Arabic software for browsing the Web is improving. Unfortunately, much of the earlier Arabic text available on the Web was posted as images, which makes it unsuitable for search or processing. As of today, there is an increase in the amount of Arabic textual material available on the Web in the form of news articles and books. The main goal of the QARAB system is to identify text passages that answer a natural language question. The task can be summarized as follows: Given a </context>
</contexts>
<marker>Harabagiu, Pasca, Maiorano, 2000</marker>
<rawString>Harabagiu, S., Pasca, M., and Maiorano, S. 2000. “Experiments with Open-Domain Textual Question Answering”. Proceedings of 18th International Conference on Computational Linguistics (COLING-2000), Saarbrucken, Germany, pp. 292-298</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hull</author>
</authors>
<title>Xerox TREC-8 Question Answering Track Report”.</title>
<date>1999</date>
<booktitle>Proceedings of the 8thText REtrieval Conference (TREC-8), NIST Special Publications 500-246,</booktitle>
<pages>743--751</pages>
<contexts>
<context position="8257" citStr="Hull, 1999" startWordPosition="1248" endWordPosition="1249">g-ofwords” approach (e.g. [Allan et al., 1999]; [Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan et al., 2000]; [Oard et al., 1999]; [Singhal et al., 1999]) agreed on the importance of applying several natural language processing techniques to solve the problem. Among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that contain the possible answer. Another appro</context>
</contexts>
<marker>Hull, 1999</marker>
<rawString>Hull, D. 1999. “Xerox TREC-8 Question Answering Track Report”. Proceedings of the 8thText REtrieval Conference (TREC-8), NIST Special Publications 500-246, pp. 743-751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Humphreys</author>
<author>R Gaizauskas</author>
<author>M Hepple</author>
<author>M Sanderson</author>
</authors>
<date>1999</date>
<booktitle>University of Sheffield TREC-8 Q &amp; A System”. Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-246,</booktitle>
<pages>707--717</pages>
<contexts>
<context position="8283" citStr="Humphreys et al., 1999" startWordPosition="1250" endWordPosition="1253">roach (e.g. [Allan et al., 1999]; [Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan et al., 2000]; [Oard et al., 1999]; [Singhal et al., 1999]) agreed on the importance of applying several natural language processing techniques to solve the problem. Among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that contain the possible answer. Another approach to the QA problem comb</context>
</contexts>
<marker>Humphreys, Gaizauskas, Hepple, Sanderson, 1999</marker>
<rawString>Humphreys, K., Gaizauskas, R., Hepple, M., and Sanderson, M. 1999. “University of Sheffield TREC-8 Q &amp; A System”. Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-246, pp. 707-717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jacobs</author>
<author>L Rau</author>
</authors>
<title>SCISOR: Extracting Information from On-line News”.</title>
<date>1990</date>
<journal>Communications of the ACM,</journal>
<volume>33</volume>
<pages>88--97</pages>
<contexts>
<context position="5023" citStr="Jacobs &amp; Rau 1990" startWordPosition="758" endWordPosition="761">oblem has been addressed in the literature since the beginning of computing machines. The AI/NLP communities initiated traditional work to address question-answering using structural methods. Early experiments in this direction implemented systems that operate in very restricted domains (e.g. SHRDLU [Winogard, 1972] and LUNAR [Woods, 1972]). In the QUALM system, Lehnert [1978] took a further step, based on the conceptual theories of Schank &amp; Abelson [1977], to understand the nature of the questions and classify them in a way similar to how human beings understand and answer questions. SCISOR [Jacobs &amp; Rau 1990] aimed at question answering and text extraction more than information retrieval. It combined natural language processing, knowledge representation, and information retrieval techniques with lexical analysis and word-based text searches. The MURAX system [Kupiec, 1993] used robust linguistic methods to answer closed-class natural language questions. It presented the user with relevant text in which noun phrases are marked. A less automated approach like Ask Jeeves [1996] approached the QA problem by pointing the questioner to Web links that might contain information relevant to the answer to </context>
</contexts>
<marker>Jacobs, Rau, 1990</marker>
<rawString>Jacobs, P., and Rau, L. 1990. “SCISOR: Extracting Information from On-line News”. Communications of the ACM, Vol. 33, No.11, pp. 88-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Katz</author>
</authors>
<title>From Sentence Processing to Information Access on the World Wide Web”.</title>
<date>1997</date>
<booktitle>Proceedings of the American Association for Artificial Intelligence Conference, Spring Symposium, NLP for WWW,</booktitle>
<pages>77--86</pages>
<contexts>
<context position="6091" citStr="Katz, 1997" startWordPosition="912" endWordPosition="913"> Ask Jeeves [1996] approached the QA problem by pointing the questioner to Web links that might contain information relevant to the answer to the question. Ask Jeeves benefited from advanced natural language processing techniques combined with data mining processing and a huge expanding knowledge base. Another system, with a different approach, is the FAQFinder system [Burke et al., 1997], which attempted to solve the question-answering problem using a database of question-answer pairs built from existing frequently asked question (FAQ) files. Two other important systems are the START system [Katz, 1997], which is based on annotations from the Web and the Q&amp;A system [Budzik &amp; Hammond, 1999], which is a semiautomatic, natural language questionanswering and referral system. The system is based on a huge knowledge base and human experts who volunteered their time to respond to the users’ questions. Recently, attention has begun to be focused on developing question-answering systems that do not rely on a knowledge base and that can fetch answers from huge unstructured text. New QA systems enhanced with NLP and IR techniques have been developed to extract textual answers for open-domain questions</context>
</contexts>
<marker>Katz, 1997</marker>
<rawString>Katz, B. 1997. “From Sentence Processing to Information Access on the World Wide Web”. Proceedings of the American Association for Artificial Intelligence Conference, Spring Symposium, NLP for WWW, pp. 77-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khoja</author>
</authors>
<title>Stemming Arabic Text”. Available on the Web at: http://www.comp.lancs.ac.uk/computing/users /khoja/stemmer.ps. Site last visited in</title>
<date>1999</date>
<contexts>
<context position="16990" citStr="Khoja, 1999" startWordPosition="2645" endWordPosition="2646">t Processing This step is essential for our system. First, the newspaper articles from the Al-Raya newspaper are saved in text format using the Arabic Windows 1256 encoding scheme. This is performed to extract all the html tags and to get the pure text contents of the articles. Second, the IR system is constructed using the relational database model as explained above. This step involves tokenization, stop-word removal, root extraction, and term weighting. 4.2 Extracting the Root In general, to extract Arabic roots from their words, the stemmer has to process each word in the following order [Khoja, 1999]: • Removing the Definite Article لا “al” • Removing the Conjunction Letter و “w” • Removing Suffixes • Removing Prefixes • Pattern Matching The following example demonstrates the whole stemming process applied to the Arabic word ﺎهﻮﺳرﺪﻴﻟو “wlydrsooha”, which is mapped to the complete English sentence “and they are going to study it”. The root of this word can be extracted as follows: (w)-(l)-(y)-drs-(oo)-(ha) (ﺎه)(و)سرد -(ي) -(ل)-(و ) 1. Removing the conjunction letter (w) (و) + (ﺎه)(و)سرد -(ي)-(ل) 2. Removing the suffix (ha) (ﺎه), which indicates a feminine, singular patient + (و)سرد -(ي) -</context>
</contexts>
<marker>Khoja, 1999</marker>
<rawString>Khoja, S. 1999. “Stemming Arabic Text”. Available on the Web at: http://www.comp.lancs.ac.uk/computing/users /khoja/stemmer.ps. Site last visited in March 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>MURAX: A Robust Linguistic Approach for Question Answering Using an On-line Encyclopedia”.</title>
<date>1993</date>
<booktitle>Proceedings of the 16th Annual Int. ACM SIGIR Conference,</booktitle>
<pages>181--190</pages>
<contexts>
<context position="5292" citStr="Kupiec, 1993" startWordPosition="794" endWordPosition="795">icted domains (e.g. SHRDLU [Winogard, 1972] and LUNAR [Woods, 1972]). In the QUALM system, Lehnert [1978] took a further step, based on the conceptual theories of Schank &amp; Abelson [1977], to understand the nature of the questions and classify them in a way similar to how human beings understand and answer questions. SCISOR [Jacobs &amp; Rau 1990] aimed at question answering and text extraction more than information retrieval. It combined natural language processing, knowledge representation, and information retrieval techniques with lexical analysis and word-based text searches. The MURAX system [Kupiec, 1993] used robust linguistic methods to answer closed-class natural language questions. It presented the user with relevant text in which noun phrases are marked. A less automated approach like Ask Jeeves [1996] approached the QA problem by pointing the questioner to Web links that might contain information relevant to the answer to the question. Ask Jeeves benefited from advanced natural language processing techniques combined with data mining processing and a huge expanding knowledge base. Another system, with a different approach, is the FAQFinder system [Burke et al., 1997], which attempted to</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Kupiec, J. 1993. “MURAX: A Robust Linguistic Approach for Question Answering Using an On-line Encyclopedia”. Proceedings of the 16th Annual Int. ACM SIGIR Conference, pp. 181-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
</authors>
<title>The Process of Question Answering. Lawrence Erlbaum Associates,</title>
<date>1978</date>
<location>Hillsdale, NJ.</location>
<marker>Lehnert, 1978</marker>
<rawString>Lehnert, W. 1978. The Process of Question Answering. Lawrence Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-J Lin</author>
<author>H-H Chen</author>
</authors>
<title>Description of Preliminary Results to TREC-8 QA Task”.</title>
<date>1999</date>
<booktitle>Proceedings of the 8th Text REtrieval Conference(TERC-8), NIST Special Publications 500-246,</booktitle>
<pages>507--513</pages>
<contexts>
<context position="7736" citStr="Lin &amp; Chen, 1999" startWordPosition="1167" endWordPosition="1170">k with different approaches and their systems were evaluated. The participating systems were tested on a huge set of unstructured documents and a set of fact-based questions. Generally speaking, most of the TREC-8 long-string answer (250-bytes) participants attempted to solve the QA problem from the information retrieval (IR) point of view by locating the most relevant documents from the collection and then extracting the sentences most relevant to the query from the documents just located. The systems relying on this “bag-ofwords” approach (e.g. [Allan et al., 1999]; [Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan et al., 2000]; [Oard </context>
</contexts>
<marker>Lin, Chen, 1999</marker>
<rawString>Lin, C-J, and Chen, H-H. 1999. “Description of Preliminary Results to TREC-8 QA Task”. Proceedings of the 8th Text REtrieval Conference(TERC-8), NIST Special Publications 500-246, pp. 507-513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Litkowski</author>
</authors>
<title>Question-Answering Using Semantic Relation Triples”.</title>
<date>1999</date>
<booktitle>Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-248,</booktitle>
<pages>349--357</pages>
<contexts>
<context position="8302" citStr="Litkowski, 1999" startWordPosition="1254" endWordPosition="1255"> 1999]; [Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan et al., 2000]; [Oard et al., 1999]; [Singhal et al., 1999]) agreed on the importance of applying several natural language processing techniques to solve the problem. Among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that contain the possible answer. Another approach to the QA problem combines IR techniques </context>
</contexts>
<marker>Litkowski, 1999</marker>
<rawString>Litkowski, K. 1999. “Question-Answering Using Semantic Relation Triples”. Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-248, pp. 349-357</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lundquist</author>
<author>D Grossman</author>
<author>O Frieder</author>
</authors>
<title>Improving Relevance Feedback in the Vector Space Model&amp;quot;.</title>
<date>1999</date>
<booktitle>Proceedings of 6th ACM Annual Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>16--23</pages>
<marker>Lundquist, Grossman, Frieder, 1999</marker>
<rawString>Lundquist, C., Grossman, D., and Frieder, O. 1999. &amp;quot;Improving Relevance Feedback in the Vector Space Model&amp;quot;. Proceedings of 6th ACM Annual Conference on Information and Knowledge Management (CIKM), pp. 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>R Girju</author>
<author>R Goodrum</author>
<author>V Rus</author>
</authors>
<title>The Structure and Performance of an Open-Domain QuestionAnswering System”.</title>
<date>2000</date>
<booktitle>Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>563--570</pages>
<contexts>
<context position="8327" citStr="Moldovan et al., 2000" startWordPosition="1256" endWordPosition="1259"> al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan et al., 2000]; [Oard et al., 1999]; [Singhal et al., 1999]) agreed on the importance of applying several natural language processing techniques to solve the problem. Among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that contain the possible answer. Another approach to the QA problem combines IR techniques with Information Extracti</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Girju, Goodrum, Rus, 2000</marker>
<rawString>Moldovan, D., Harabagiu, S., Pasca, M., Mihalcea, R., Girju, R., Goodrum, R., and Rus, V. 2000. “The Structure and Performance of an Open-Domain QuestionAnswering System”. Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pp. 563-570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Oard</author>
<author>J Wang</author>
<author>D Lin</author>
<author>I Soboroff</author>
</authors>
<date>1999</date>
<booktitle>TREC-8 Experiments at Maryland: CLIR, QA and Routing”. Proceedings of the 8th Text REtrieval Conference (TERC-8), NIST Special Publications 500-246,</booktitle>
<pages>623--637</pages>
<contexts>
<context position="8348" citStr="Oard et al., 1999" startWordPosition="1260" endWordPosition="1263"> 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan et al., 2000]; [Oard et al., 1999]; [Singhal et al., 1999]) agreed on the importance of applying several natural language processing techniques to solve the problem. Among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that contain the possible answer. Another approach to the QA problem combines IR techniques with Information Extraction (IE) techniques fo</context>
</contexts>
<marker>Oard, Wang, Lin, Soboroff, 1999</marker>
<rawString>Oard, D., Wang, J., Lin, D., and Soboroff, I. 1999. “TREC-8 Experiments at Maryland: CLIR, QA and Routing”. Proceedings of the 8th Text REtrieval Conference (TERC-8), NIST Special Publications 500-246, pp. 623-637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Ogden</author>
<author>J Cowie</author>
<author>E MolinaSalgado Ludovik</author>
<author>H Nirenburg</author>
<author>S Sharples</author>
<author>N</author>
<author>S Sheremtyeva</author>
</authors>
<date>1999</date>
<booktitle>CRL&apos;s TREC-8 Systems Cross-Lingual IR, and Q&amp;A”. Proceedings of the 8th Text REtrieval Conference (TERC-8), NIST Special Publications 500-246,</booktitle>
<pages>513--523</pages>
<contexts>
<context position="9002" citStr="Ogden et al., 1999" startWordPosition="1357" endWordPosition="1360"> the importance of applying several natural language processing techniques to solve the problem. Among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that contain the possible answer. Another approach to the QA problem combines IR techniques with Information Extraction (IE) techniques for extracting named entities, e.g., [Ogden et al., 1999]; [Takaki, 1999]; and [Srihari &amp; Li, 1999]. A detailed description of the track and the results are available at [Voorhees &amp; Tice, 1999]. It is obvious from the increasing number of systems participating in TREC-9 and the worldwide interest in this research area that Question Answering is the most promising framework for finding answers to natural language questions from a huge amount of textual data. Cardie et al. [2000] pointed out that building “open-ended question answering systems that allow users to pose questions of any type and in any language, without domain restrictions, is still be</context>
</contexts>
<marker>Ogden, Cowie, Ludovik, Nirenburg, Sharples, N, Sheremtyeva, 1999</marker>
<rawString>Ogden, B., Cowie, J., Ludovik, E. MolinaSalgado, H., Nirenburg, S., Sharples, N., and Sheremtyeva, S. 1999. “CRL&apos;s TREC-8 Systems Cross-Lingual IR, and Q&amp;A”. Proceedings of the 8th Text REtrieval Conference (TERC-8), NIST Special Publications 500-246, pp. 513-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<date>1971</date>
<booktitle>The SMART Retrieval System Experiments in Automatic Document Processing.</booktitle>
<publisher>Prentice Hall Inc.,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="11776" citStr="Salton, 1971" startWordPosition="1817" endWordPosition="1818"> one of the candidate documents (paragraphs) in the same way as the question is processed and returning sentences that may contain the answer. The QARAB system will be evaluated over a wide range of question types provided by Arabic users during the testing and the final phases. The same users will then assess whether the answers produced by the system are satisfactory. 3.2 QARAB Structure The complete QARAB system is depicted in Figure 1; it has the following overall structure: 3.2.1 The IR System The IR system, which we are implementing from scratch, is based on Salton’s vector space model [Salton, 1971]. First, it processes the text collection from the Al-Raya newspaper and constructs an inverted file system, from which the answers to the natural language questions will be extracted. The purpose of the IR system is to search the document collection to select documents containing information relevant to the user’s query. Implementing the Information Retrieval System Information Retrieval (IR) systems can be constructed in many various ways. Lundquist et al. [1999] proposed an Information Retrieval (IR) system that can be constructed using a relational database management system (RDBMS). Our </context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>Salton, G. 1971. The SMART Retrieval System Experiments in Automatic Document Processing. Prentice Hall Inc., Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schank</author>
<author>R Abelson</author>
</authors>
<title>Scripts, Plans, Goals, and Understanding. Lawrence Erlbaum Associates,</title>
<date>1977</date>
<location>Hillsdale, NJ.</location>
<marker>Schank, Abelson, 1977</marker>
<rawString>Schank, R., and Abelson, R. 1977. Scripts, Plans, Goals, and Understanding. Lawrence Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D-H Shin</author>
<author>Y-H Kim</author>
<author>S Kim</author>
<author>J-H Eom</author>
<author>H-J Shin</author>
<author>B-T Zhang</author>
</authors>
<date>1999</date>
<booktitle>SCAI TREC-8 Experiments”. Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-246,</booktitle>
<pages>583--591</pages>
<contexts>
<context position="7757" citStr="Shin et al., 1999" startWordPosition="1171" endWordPosition="1174">roaches and their systems were evaluated. The participating systems were tested on a huge set of unstructured documents and a set of fact-based questions. Generally speaking, most of the TREC-8 long-string answer (250-bytes) participants attempted to solve the QA problem from the information retrieval (IR) point of view by locating the most relevant documents from the collection and then extracting the sentences most relevant to the query from the documents just located. The systems relying on this “bag-ofwords” approach (e.g. [Allan et al., 1999]; [Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan et al., 2000]; [Oard et al., 1999]; [Singh</context>
</contexts>
<marker>Shin, Kim, Kim, Eom, Shin, Zhang, 1999</marker>
<rawString>Shin, D-H, Kim, Y-H, Kim, S., Eom, J-H, Shin, H-J, and Zhang B-T. 1999. “SCAI TREC-8 Experiments”. Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-246, pp. 583-591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Singhal</author>
<author>S Abney</author>
<author>M Bacchiani</author>
<author>M Collins</author>
<author>D Hindle</author>
<author>F Pereira</author>
</authors>
<date>1999</date>
<booktitle>AT&amp;T at TREC-8”. Proceedings of the 8th Text REtrieval Conference, NIST Special Publications,</booktitle>
<pages>317--331</pages>
<contexts>
<context position="7818" citStr="Singhal et al., 1999" startWordPosition="1181" endWordPosition="1184">g systems were tested on a huge set of unstructured documents and a set of fact-based questions. Generally speaking, most of the TREC-8 long-string answer (250-bytes) participants attempted to solve the QA problem from the information retrieval (IR) point of view by locating the most relevant documents from the collection and then extracting the sentences most relevant to the query from the documents just located. The systems relying on this “bag-ofwords” approach (e.g. [Allan et al., 1999]; [Cormack et al., 1999]; [Lin &amp; Chen, 1999]; [Shin et al., 1999] and the passage-retrieval run of AT&amp;T [Singhal et al., 1999]) deal with the question without considering its grammatical or semantic characteristics and they apply conventional IR techniques to extract the answer. Even though the “bag-of-words” approach was commonly used in TREC-8, the systems based on this approach were inadequate to handle the shortstring (50-byte) answers. On the contrary, the short string (50-byte) participants (e.g. [Breck et al., 1999]; [Ferret et al., 1999]; [Hull, 1999]; [Humphreys et al., 1999]; [Litkowski, 1999]; [Moldovan et al., 2000]; [Oard et al., 1999]; [Singhal et al., 1999]) agreed on the importance of applying severa</context>
</contexts>
<marker>Singhal, Abney, Bacchiani, Collins, Hindle, Pereira, 1999</marker>
<rawString>Singhal, A., Abney, S., Bacchiani, M., Collins, M., Hindle, D., and Pereira, F. 1999. “AT&amp;T at TREC-8”. Proceedings of the 8th Text REtrieval Conference, NIST Special Publications, pp. 317-331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Srihari</author>
<author>W Li</author>
</authors>
<title>Information Extraction Supported Question Answering”.</title>
<date>1999</date>
<booktitle>Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-246,</booktitle>
<pages>185--197</pages>
<contexts>
<context position="9044" citStr="Srihari &amp; Li, 1999" startWordPosition="1364" endWordPosition="1367">l language processing techniques to solve the problem. Among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that contain the possible answer. Another approach to the QA problem combines IR techniques with Information Extraction (IE) techniques for extracting named entities, e.g., [Ogden et al., 1999]; [Takaki, 1999]; and [Srihari &amp; Li, 1999]. A detailed description of the track and the results are available at [Voorhees &amp; Tice, 1999]. It is obvious from the increasing number of systems participating in TREC-9 and the worldwide interest in this research area that Question Answering is the most promising framework for finding answers to natural language questions from a huge amount of textual data. Cardie et al. [2000] pointed out that building “open-ended question answering systems that allow users to pose questions of any type and in any language, without domain restrictions, is still beyond the scope of any QA system today” (p.</context>
</contexts>
<marker>Srihari, Li, 1999</marker>
<rawString>Srihari, R., and Li, W. 1999. “Information Extraction Supported Question Answering”. Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-246, pp. 185-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takaki</author>
</authors>
<title>NTT DATA: Overview of System Approach at TREC-8 ad-hoc and Question Answering”.</title>
<date>1999</date>
<booktitle>Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-246,</booktitle>
<pages>523--531</pages>
<contexts>
<context position="9018" citStr="Takaki, 1999" startWordPosition="1361" endWordPosition="1362">lying several natural language processing techniques to solve the problem. Among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that contain the possible answer. Another approach to the QA problem combines IR techniques with Information Extraction (IE) techniques for extracting named entities, e.g., [Ogden et al., 1999]; [Takaki, 1999]; and [Srihari &amp; Li, 1999]. A detailed description of the track and the results are available at [Voorhees &amp; Tice, 1999]. It is obvious from the increasing number of systems participating in TREC-9 and the worldwide interest in this research area that Question Answering is the most promising framework for finding answers to natural language questions from a huge amount of textual data. Cardie et al. [2000] pointed out that building “open-ended question answering systems that allow users to pose questions of any type and in any language, without domain restrictions, is still beyond the scope o</context>
</contexts>
<marker>Takaki, 1999</marker>
<rawString>Takaki, T. 1999. “NTT DATA: Overview of System Approach at TREC-8 ad-hoc and Question Answering”. Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publications 500-246, pp. 523-531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TREC-8</author>
</authors>
<title>NIST Special Publication 500-246: The Eighth Text REtrieval Conference. Available on the Web at: http://trec.nist.gov/pubs/trec8/t8 proceedings. html. Site last visited in</title>
<date>1999</date>
<marker>TREC-8, 1999</marker>
<rawString>TREC-8. 1999. NIST Special Publication 500-246: The Eighth Text REtrieval Conference. Available on the Web at: http://trec.nist.gov/pubs/trec8/t8 proceedings. html. Site last visited in August 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TREC-9</author>
</authors>
<title>NIST Special Publication: The Ninth Text REtrieval Conference. Available on the Web at: http://trec.nist.gov/pubs/trec9/t9 proceedings. html. Site last visited in</title>
<date>2000</date>
<marker>TREC-9, 2000</marker>
<rawString>TREC-9. 2000. NIST Special Publication: The Ninth Text REtrieval Conference. Available on the Web at: http://trec.nist.gov/pubs/trec9/t9 proceedings. html. Site last visited in August 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Vicedo</author>
<author>A Ferrández</author>
</authors>
<title>Importance of Pronominal Anaphora Resolution in Question- Answering System”.</title>
<date>2000</date>
<booktitle>Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>555--562</pages>
<contexts>
<context position="4346" citStr="Vicedo &amp; Ferrández, 2000" startWordPosition="656" endWordPosition="659">he above linguistic issues, there is also a lack of Arabic corpora, lexicons, and machine-readable dictionaries, which are essential to advance research in different areas. 2 Background Advances in natural language processing (NLP), information retrieval techniques (IR), information extraction (IE), as well as the computer industry, have given QA a strong boost. Modern questionanswering systems have started incorporating NLP techniques to parse natural language documents, extract entities and relations between entities, resolve anaphora, and other language ambiguities [Harabagiu et al., 2000; Vicedo &amp; Ferrández, 2000]. Research in Question-Answering (QA) is not new. The QA problem has been addressed in the literature since the beginning of computing machines. The AI/NLP communities initiated traditional work to address question-answering using structural methods. Early experiments in this direction implemented systems that operate in very restricted domains (e.g. SHRDLU [Winogard, 1972] and LUNAR [Woods, 1972]). In the QUALM system, Lehnert [1978] took a further step, based on the conceptual theories of Schank &amp; Abelson [1977], to understand the nature of the questions and classify them in a way similar t</context>
</contexts>
<marker>Vicedo, Ferrández, 2000</marker>
<rawString>Vicedo, J., and Ferrández, A. 2000. “Importance of Pronominal Anaphora Resolution in Question- Answering System”. Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pp. 555-562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
<author>D Tice</author>
</authors>
<title>The TREC-8 Question Answering Track Evaluation&amp;quot;.</title>
<date>1999</date>
<booktitle>Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publication 500-246,</booktitle>
<pages>83--106</pages>
<contexts>
<context position="9138" citStr="Voorhees &amp; Tice, 1999" startWordPosition="1380" endWordPosition="1383">speech tagging, shallow parsing, query type identification and named entity recognition. Because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the “bag-of-words” approach as an initial step to retrieve the relevant passages that contain the possible answer. Another approach to the QA problem combines IR techniques with Information Extraction (IE) techniques for extracting named entities, e.g., [Ogden et al., 1999]; [Takaki, 1999]; and [Srihari &amp; Li, 1999]. A detailed description of the track and the results are available at [Voorhees &amp; Tice, 1999]. It is obvious from the increasing number of systems participating in TREC-9 and the worldwide interest in this research area that Question Answering is the most promising framework for finding answers to natural language questions from a huge amount of textual data. Cardie et al. [2000] pointed out that building “open-ended question answering systems that allow users to pose questions of any type and in any language, without domain restrictions, is still beyond the scope of any QA system today” (p. 180). Harabagiu et al. [2000] indicated that advanced tools (such as dialog understanding and</context>
</contexts>
<marker>Voorhees, Tice, 1999</marker>
<rawString>Voorhees, E., and Tice, D. 1999. &amp;quot;The TREC-8 Question Answering Track Evaluation&amp;quot;. Proceedings of the 8th Text REtrieval Conference (TREC-8), NIST Special Publication 500-246, pp. 83-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
<author>D Tice</author>
</authors>
<title>Building a Question Answering Test Collection”.</title>
<date>2000</date>
<booktitle>Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>200--207</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="6996" citStr="Voorhees &amp; Tice, 2000" startWordPosition="1055" endWordPosition="1058">rs’ questions. Recently, attention has begun to be focused on developing question-answering systems that do not rely on a knowledge base and that can fetch answers from huge unstructured text. New QA systems enhanced with NLP and IR techniques have been developed to extract textual answers for open-domain questions and provide a framework for modern information retrieval [TREC-8, 1999; TREC-9, 2000]. The overall aim of this QA track was to retrieve small pieces of text that contain the actual answer to the question rather than the list of documents traditionally returned by retrieval engines [Voorhees &amp; Tice, 2000]. The TREC-8 QA track attracted researchers from both industry and academia. Twenty organizations participated in this track with different approaches and their systems were evaluated. The participating systems were tested on a huge set of unstructured documents and a set of fact-based questions. Generally speaking, most of the TREC-8 long-string answer (250-bytes) participants attempted to solve the QA problem from the information retrieval (IR) point of view by locating the most relevant documents from the collection and then extracting the sentences most relevant to the query from the docu</context>
</contexts>
<marker>Voorhees, Tice, 2000</marker>
<rawString>Voorhees, E., and Tice, D. 2000. “Building a Question Answering Test Collection”. Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, pp. 200-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press,</publisher>
<location>New York, NY.</location>
<marker>Winograd, 1972</marker>
<rawString>Winograd, T. 1972. Understanding Natural Language. Academic Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
<author>R Kaplan</author>
<author>B Webber</author>
</authors>
<title>The Lunar Sciences Natural Language Information System: Final Report”. Bolt Beranek and Newman Inc.</title>
<date>1972</date>
<tech>(BBN), Report No. 2378.</tech>
<marker>Woods, Kaplan, Webber, 1972</marker>
<rawString>Woods, W., Kaplan, R., and Webber, B. 1972. “The Lunar Sciences Natural Language Information System: Final Report”. Bolt Beranek and Newman Inc. (BBN), Report No. 2378.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>