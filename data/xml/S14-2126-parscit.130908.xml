<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005745">
<title confidence="0.9971925">
UKPDIPF: A Lexical Semantic Approach to Sentiment Polarity
Prediction in Twitter Data
</title>
<author confidence="0.957026">
Lucie Flekovatt, Oliver Ferschkett and Iryna Gurevychtt
</author>
<affiliation confidence="0.95131625">
t Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Computer Science Department, Technische Universit¨at Darmstadt
t Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research
</affiliation>
<email confidence="0.914139">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.992076" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884090909091">
We present a sentiment classification sys-
tem that participated in the SemEval 2014
shared task on sentiment analysis in Twit-
ter. Our system expands tokens in a tweet
with semantically similar expressions us-
ing a large novel distributional thesaurus
and calculates the semantic relatedness of
the expanded tweets to word lists repre-
senting positive and negative sentiment.
This approach helps to assess the polarity
of tweets that do not directly contain po-
larity cues. Moreover, we incorporate syn-
tactic, lexical and surface sentiment fea-
tures. On the message level, our system
achieved the 8th place in terms of macro-
averaged F-score among 50 systems, with
particularly good performance on the Life-
Journal corpus (Fi=71.92) and the Twitter
sarcasm (Fi=54.59) dataset. On the ex-
pression level, our system ranked 14 out
of 27 systems, based on macro-averaged
F-score.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.959466036363636">
Microblogging sites, such as Twitter, have become
an important source of information about current
events. The fact that users write about their ex-
periences, often directly during or shortly after
an event, contributes to the high level of emo-
tions in many such messages. Being able to auto-
matically and reliably evaluate these emotions in
context of a specific event or a product would be
highly beneficial not only in marketing (Jansen et
al., 2009) or public relations, but also in political
sciences (O’Connor et al., 2010), disaster manage-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
ment, stock market analysis (Bollen et al., 2011)
or the health sector (Culotta, 2010).
Due to its large number of applications, senti-
ment analysis on Twitter is a very popular task.
Challenges arise both from the character of the
task and from the language specifics of Twit-
ter messages. Messages are normally very short
and informal, frequently using slang, alternative
spelling, neologism and links, and mostly ignor-
ing the punctuation.
Our experiments have been carried out as part
of the SemEval 2014 Task 9 - Sentiment Anal-
ysis on Twitter (Rosenthal et al., 2014), a rerun
of a SemEval-2013 Task 2 (Nakov et al., 2013).
The datasets are thus described in detail in the
overview papers. The rerun uses the same train-
ing and development data, but new test data from
Twitter and a “surprise domain”. The task con-
sists of two subtasks: an expression-level subtask
(Subtask A) and a message-level subtask (Subtask
B). In subtask A, each tweet in a corpus contained
a marked instance of a word or phrase. The goal
is to determine whether that instance is positive,
negative or neutral in that context. In subtask B,
the goal is to classify whether the entire message
is of positive, negative, or neutral sentiment. For
messages conveying both a positive and negative
sentiment, the stronger one should be chosen.
The key components of our system are the sen-
timent polarity lexicons. In contrast to previous
approaches, we do not only count exact lexicon
hits, but also calculate explicit semantic related-
ness (Gabrilovich and Markovitch, 2007) between
the tweet and the sentiment list, benefiting from
resources such as Wiktionary and WordNet. On
top of that, we expand content words (adjectives,
adverbs, nouns and verbs) in the tweet with sim-
ilar words, which we derive from a novel corpus
of more than 80 million English Tweets gathered
by the Language Technology group1 at TU Darm-
</bodyText>
<footnote confidence="0.995073">
1http://www.lt.informatik.tu-darmstadt.de
</footnote>
<page confidence="0.922142">
704
</page>
<note confidence="0.749486">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 704–710,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.713694">
stadt.
</bodyText>
<sectionHeader confidence="0.984407" genericHeader="method">
2 Experimental setup
</sectionHeader>
<bodyText confidence="0.999663375">
Our experimental setup is based on an open-source
text classification framework DKPro TC2 (Daxen-
berger et al., 2014), which allows to combine NLP
pipelines into a configurable and modular system
for preprocessing, feature extraction and classifi-
cation. We use the unit classi�cation mode of
DKPro TC for Subtask A and the document clas-
si�cation mode for Subtask B.
</bodyText>
<subsectionHeader confidence="0.993041">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999964307692307">
We customized the message reader for Subtask B
to ignore the first part of the tweet when the word
but is found. This approach helps to reduce the
misleading positive hits when a negative message
is introduced positively (It’d be good, but).
For preprocessing the data, we use components
from DKPro Core3. Preprocessing is the same
for subtasks A and B, with the only difference
that in the subtask A the target expression is addi-
tionally annotated as text classi�cation unit, while
the rest of the tweet is considered to be a doc-
ument context. We first segment the data with
the Stanford Segmenter4, apply the Stanford POS
Tagger with a Twitter-trained model (Derczynski
et al., 2013), and subsequently apply the Stan-
ford Lemmatizer4, TreeTagger Chunker (Schmid,
1994), Stanford Named Entity Recognizer (Finkel
et al., 2005) and Stanford Parser (Klein and Man-
ning, 2003) to each tweet. After this linguistic pre-
processing, the token segmentation of the Stanford
tools is removed and overwritten by the ArkTweet
Tagger (Gimpel et al., 2011), which is more suit-
able for recognizing hashtags and smileys as one
particular token. Finally, we expand the tweet and
proceed to feature extraction as described in detail
in Section 3.
</bodyText>
<subsectionHeader confidence="0.996786">
2.2 Classification
</subsectionHeader>
<bodyText confidence="0.999882">
We trained our system on the provided training
data only, excluding the dev data. We use clas-
sifiers from the WEKA (Hall et al., 2009) toolkit,
which are integrated in the DKPro TC framework.
Our final configuration consists of a SVM-SMO
classifier with a gaussian kernel. The optimal hy-
perparameters have been experimentally derived
</bodyText>
<footnote confidence="0.999898333333333">
2http://code.google.com/p/dkpro-tc
3http://code.google.com/p/dkpro-core-asl
4http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<bodyText confidence="0.9996265">
and finally set to C=1 and G=0.01. The resulting
model was wrapped in a cost sensitive meta classi-
fier from the WEKA toolkit with the error costs set
to reflect the class imbalance in the training set.
</bodyText>
<sectionHeader confidence="0.992096" genericHeader="method">
3 Features used
</sectionHeader>
<bodyText confidence="0.9999929">
We now describe the features used in our exper-
iments. For Subtask A (contextual polarity), we
extracted each feature twice - once on the tweet
level and once on the focus expression level. Only
n-gram features were extracted solely from the ex-
pressions. For Subtask B (tweet polarity), we ex-
tracted features on tweet level only. In both cases,
we use the Information Gain feature selection ap-
proach in WEKA to rank the features and prune
the feature space with a threshold of T=0.005.
</bodyText>
<subsectionHeader confidence="0.998499">
3.1 Lexical features
</subsectionHeader>
<bodyText confidence="0.999392225806452">
As a basis for our similarity and expansion ex-
periments (sections 3.4 and 3.5), we use the bi-
nary sentiment polarity lexicon by Liu (2012) aug-
mented with the smiley polarity lexicon by Becker
et al. (2013) and an additional swear word list5
[further as Liuaugmented]. We selected this aug-
mented lexicon for two reasons: firstly, it was the
highest ranked lexical feature on the development-
test and crossvalidation experiments, secondly it
consists of two plain word lists and therefore does
not introduce another complexity dimension for
advanced feature calculations.
We further measure lexicon hits normalized per
number of tweet tokens for the following lexicons:
Pennebaker’s Linguistic Inquiry and Word Count
(LIWC) (Pennebaker et al., 2001), the NRC Emo-
tion Lexicon (Mohammad and Turney, 2013), the
NRC Hashtag Emotion Lexicon (Mohammad et
al., 2013) and the Sentiment140 lexicon (Moham-
mad et al., 2013). We use an additional lexicon
of positive, negative, very positive and very nega-
tive words, diminishers, intensifiers and negations
composed by Steinberger et al. (2012), where we
calculate the polarity score as described in their
paper.
In a complementary set of features we combine
each of the lexicons above with a list of weighted
intensifying expressions as published by Brooke
(2009). The intensity of any polar word found in
any of the emotion lexicons used is intensified or
diminished by a given weight if an intensifier (a
</bodyText>
<footnote confidence="0.996703">
5based on http://www.youswear.com
</footnote>
<page confidence="0.997456">
705
</page>
<bodyText confidence="0.999454333333333">
bit, very, slightly...) is found within the preceding
three tokens.
Additionally, we record the overall counts of
lexicon hits for positive words, negative words and
the difference of the two. In one set of features
we consider only lexicons clearly meant for binary
polarity, while a second set of features also in-
cludes other emotions, such as fear or anger, from
the NRC and the LIWC corpora.
</bodyText>
<subsectionHeader confidence="0.998705">
3.2 Negation
</subsectionHeader>
<bodyText confidence="0.99994661904762">
We handle negation in two ways. On the expres-
sion level (Subtask A) we rely on the negation
dependency tag provided by the Stanford Depen-
dency Parser. This one captures verb negations
rather precisely and thus helps to handle emotional
verb expressions such as like vs don’t like. On the
tweet level (all features of Subtask B and entire-
tweet-level features of Subtask A) we adopt the
approach of Pang et al. (2002), considering as a
negation context any sequence of tokens between
a negation expression and the end of a sentence
segment as annotated by the Stanford Segmenter.
The negation expressions (don’t, can’t...) are rep-
resented by the list of invertors from Steinberger’s
lexicon (Steinberger et al., 2012). We first assign
polarity score to each word in the tweet based on
the lexicon hits and then revert it for the words ly-
ing in the negation context. This approach is more
robust than the one of the dependency governor
but is error-prone in the area of overlapping (cas-
caded) negation contexts.
</bodyText>
<subsectionHeader confidence="0.978373">
3.3 N-gram features
</subsectionHeader>
<bodyText confidence="0.9998838125">
We extract the 5,000 most frequent word uni-
grams, bigrams and trigrams cleaned with the
Snowball stopword list6 as well as the same
amount of skip-n-grams and character trigrams.
These are extracted separately on the target ex-
pression level for subtask A and on document
level for subtask B. On the syntactic level, we
monitor the most frequent 5,000 part-of-speech
ngrams with the size up to part-of-speech quadru-
ples. Additionally, as an approximation for ex-
ploiting the key message of the sentence, we ex-
tract from the tweets a verb chunk and its left and
right neighboring noun chunks, obtaining combi-
nations such as we-go-cinema. The 1,000 most
frequent chunk triples are then used as features
similarly to ngrams.
</bodyText>
<footnote confidence="0.955085">
6http://snowball.tartarus.org/algorithms/english/stop.txt
</footnote>
<table confidence="0.999982333333333">
Word Score Word (continued) Score
awesome 1,000 fun 60
amazing 194 sexy 59
great 148 cold 59
cool 104 crazy 57
good 96 fantastic 56
best 93 bored 55
beautiful 93 excited 54
nice 87 true 53
funny 84 stupid 53
cute 81 gr8 52
perfect 70 entertaining 52
wonderful 67 favorite 52
lovely 66 talented 49
tired 65 other 49
annoying 63 depressing 48
Great 63 flawless 48
new 62 inspiring 47
hilarious 62 incredible 46
bad 61 complicated 46
hot 61 gorgeous 45
</table>
<tableCaption confidence="0.999759">
Table 1: Unsupervised expansion of ‘awesome’
</tableCaption>
<subsectionHeader confidence="0.919516">
3.4 Tweet expansion
</subsectionHeader>
<bodyText confidence="0.999981523809524">
We expanded the content words in a tweet, i.e.
nouns, verbs, adjectives and adverbs, with sim-
ilar words from a word similarity thesaurus that
was computed on 80 million English tweets from
2012 using the JoBim contextual semantics frame-
work (Biemann and Riedl, 2013). Table 1 shows
an example for a lexical expansion of the word
awesome. The score was computed using left and
right neighbor bigram features for the holing oper-
ation. The value hence shows how often the word
appeared in the same left and right context as the
original word. The upper limit of the score is set
to 1,000.
We then match the expanded tweet against the
Liuauymented positive and negative lexicons. We
assign to the lexicon hits of the expanded words
their (contextual similarity) expansion score, us-
ing a score of 1,000 as an anchor-value for the
original tweet, setting an expansion cut at 100.
The overall tweet score is then normalized by the
sum of word expansion scores.
</bodyText>
<subsectionHeader confidence="0.979615">
3.5 Semantic similarity
</subsectionHeader>
<bodyText confidence="0.9999185">
Tweet messages are short and each emotional
word is very valuable for the task, even when it
may not be present in a specific lexicon. There-
fore, we calculate a semantic relatedness score
between the tweet and the positive or negative
word list. We use the ESA similarity measure
(Gabrilovich and Markovitch, 2007) as imple-
mented in the DKPro similarity software pack-
</bodyText>
<page confidence="0.969344">
706
</page>
<bodyText confidence="0.998256525">
age (B¨ar et al., 2013), calculated on English Wik- Features with the highest Information Gain
tionary and WordNet as two separate concept were the ones based on Liuaugmented. Adding the
spaces. The ESA vectors are freely available7. weighted intensifiers of Brooke to the sentiment
This way we obtain in total six features: sim(orig- lexicons did not outperform the simple lexicon
inal tweet word list, positive word list), sim(orig- lookup. They were followed by features derived
inal tweet word list, negative word list), differ- from the lexicons of Steinberger, which includes
ence between the two, sim(expanded tweet word invertors, intensifiers and four polarity levels of
list, positive word list), sim(expanded tweet word words. On the other hand, adding the weighted
list, negative word list) and difference between the intensifiers of Brooke to lexicons did not outper-
two. Our SemEval run was submitted using Word- form the simple lexicon lookup. Overall, lexicon-
Net vectors mainly for the shorter computation based features contributed to the highest perfor-
time and lower memory requirements. However, mance gain, as shown in Table 3. The negation
in our later experiments Wiktionary performed approach based on the Stanford dependency parser
better. We presume this can be due to a better was the most helpful, although it tripled the run-
coverage for the Twitter corpus, although detailed time. Using the simpler negation context as sug-
analysis of this aspect is yet to be performed. gested in Pang et al. (2002) performed still on av-
3.6 Other features erage better than using none.
Pak and Paroubek (2010) pointed out a relation When using WordNet, semantic similarity to
between the presence of different part-of-speech lexicons did not outperform direct lexicon hits.
types and sentiment polarity. We measure the Usage of Wiktionary instead lead to major im-
ratio of each part-of-speech type to each chunk. provement (Table 3), unfortunately after the Se-
We furthermore count the occurrences of the mEval challenge.
dependency tag for negation. We use the Stanford Tweet expansion appears to improve the clas-
Named Entity Recognizer to count occurrence sification performance, however the threshold of
of persons, organizations and locations in the 100 that we used in our setup was chosed too
tweet. Additionaly, beside basic surface metrics, conservatively, expanding mainly stopwords with
such as the number of tokens, characters and other stopwords or words with their spelling al-
sentences, we measure the number of elon- ternatives, resulting in a noisy, little valuable fea-
gated words (such as coool) in a tweet, ratio ture (expansion full in Table 3). Setting
of sentences ending with exclamation, ratio of up the threshold to 50 and cleaning up both the
questions and number of positive and negative tweet and the expansion with Snowball stopword
smileys and their proportion. We capture the list (expansion clean in Table 3), the perfor-
smileys with the following two regular expres- mance increased remarkably.
sions for positive, respectively negative ones: Amongst other prominent features were parts of
[&lt;&gt;]?[:;=8][-o*’]?[)]dDpPxXoO0*}], lexicons such as LIWC Positive emotions, LIWC
[&lt;&gt;]?[:;=8][-o*’]?[([/:{|]. We also Affect, LIWC Negative emotions, NRC Joy, NRC
separately measure the sentiment of smileys at Anger and NRC Disgust. Informative were also
the end of the tweet body, i.e. followed only by a the proportions of nouns, verbs and adverbs, the
hashtag, hyperlink or nothing. exclamation ratio or number of positive and nega-
tive smileys at the end of the tweet.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.9988915">
In Subtask A, our system achieved an averaged
F-score of 81.42 on the LiveJournal corpus and
79.67 on the Twitter 2014 corpus. The highest
scores achieved in related work were 85.61 and
86.63 respectively. For subtask B, we scored 71.92
on LifeJournal and 63.77 on Twitter 2014, while
the highest F-scores reported by related work were
74.84 and 70.96.
</bodyText>
<footnote confidence="0.927205">
7https://code.google.com/p/dkpro-similarity-asl/downloads/list
</footnote>
<table confidence="0.99617275">
Feature(s) ΔF, Ttvitter2014 ΔF, LifeJournal
Similarity Wikt. 0.56 3.65
Similarity WN 0.0 2.61
Expansion full 0.0 0.0
Expansion clean 0.59 3.82
Lexical negation 0.24 0.13
N-gram features 0.30 0.32
Lexicon-based f. 7.85 4.74
</table>
<tableCaption confidence="0.9745515">
Table 3: Performance increase where feature
added to the full setup
</tableCaption>
<page confidence="0.9361">
707
</page>
<figure confidence="0.967353090909091">
# Gold label Prediction
1 negative positive
2 neutral positive
3 neutral positive
4 positive negative
5 positive negative
6 positive negative
7 positive negative
8 positive negative
9 positive negative
Message
</figure>
<bodyText confidence="0.778878666666667">
Your plans of attending the Great Yorkshire Show may have been washed out because
of the weather, so how about...
sitting here with my belt in jean shorts watching Cena win his first title.
I think we tie for 1st my friend xD
saw your LJ post ... yay for Aussies ;)
haha , that sucks , because the drumline will be just fine
...woah, Deezer. Babel only came out on Monday, can you leave it up for longer than a day
to give slow people like me a chance?
Yeah so much has changed for the 6th. Lots of combat fighting. And inventory is different.
</bodyText>
<tableCaption confidence="0.750020833333333">
just finish doing it and tomorrow I’m going to the celtics game and don’t fucking say
”thanks for the invite” it’s annoying
Haha... Yup hopefully we will lose a few kg by mon. after hip hop can go orchard and weigh
U r just like my friends? I made them feel warm, happy, then make them angry and they cry?
Finally they left me? Will u leave 2? I hope not. Really hope so.
Table 2: Examples of misclassified messages
</tableCaption>
<sectionHeader confidence="0.988886" genericHeader="method">
5 Error analysis
</sectionHeader>
<bodyText confidence="0.999890518518519">
Table 2 lists a sample of misclassified messages.
The majority of errors resulted from misclassify-
ing neutral tweets as emotionally charged. This
was partly caused by the usage of emoticons and
expressions such as haha in a neutral context, such
as in examples 2 and 3. Other errors were cause by
lexicon hits of proper nouns (example 1), or by us-
ing negative words and swearwords in overall pos-
itive tweet (examples 4, 7, 9). Some tweets con-
tained domain specific vocabulary that would hit
the negative lexicon, e.g., discussing fighting and
violence in computer games would, in contrast to
other topic domains, usually have positive polar-
ity (example 6). Similar domain-specific polarity
distinction could be applied to certain verbs, e.g.,
lose weight vs. lose a game (example 8).
Another challenge for the system was the non-
standard language in twitter with a large number of
spelling variants, which was only partly captured
by the emotion lexicons tailored for this domain.
A twitter-specific lemmatizer, which would group
all variations of a misspelled word into one, could
help to improve the performance.
The length of the negation context window does
not suit all purposes. Also double negations such
as I don’t think he couldn’t... can easily misdirect
the polarity score.
</bodyText>
<sectionHeader confidence="0.99671" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999974333333333">
We presented a sentiment classification system
that can be used on both message level and ex-
pression level with only small changes in the
framework configuration. We employed a con-
textual similarity thesaurus for the lexical expan-
sion of the messages. The expansion was not
efficient without an extensive stopword cleaning,
overweighting more common words and introduc-
ing noise. Utilizing the semantic similarity of
tweets to lexicons instead of a direct match im-
proves the score only with certain lexicons, possi-
bly dependent on the coverage. Negation by de-
pendency parsing was more beneficial to the clas-
sifier than the negation by keyword span anno-
tation. Naive combination of sentiment lexicons
was not more helpful than using individual ones
separately. Among the common source of errors
were laughing signs used in neutral messages and
swearing used in positive messages. Even within
Twitter, same words can have different polarity in
different domains (lose weight, lose game, game
with nice violent fights...). Deeper semantic in-
sights are necessary to distinguish between polar
words in context.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="conclusions">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.999663571428571">
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
We warmly thank Chris Biemann, Martin Riedl
and Eugen Ruppert of the Language Technology
group at TU Darmstadt for providing us with the
Twitter-based distributional thesaurus.
</bodyText>
<sectionHeader confidence="0.999002" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991542571428571">
Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2013.
Dkpro similarity: An open source framework for
text similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 121–126,
Sofia, Bulgaria.
Lee Becker, George Erhart, David Skiba, and Valentine
</reference>
<page confidence="0.989757">
708
</page>
<reference confidence="0.998767190909091">
Matula. 2013. Avaya: Sentiment analysis on twit-
ter with self-training and polarity lexicon expansion.
Atlanta, Georgia, USA, page 333.
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2d! a framework for lexical expansion with con-
textual similarity. Journal of Language Modelling,
1(1):55–95.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 – 8.
Julian Brooke. 2009. A semantic approach to auto-
mated text sentiment analysis.
Aron Culotta. 2010. Towards detecting influenza epi-
demics by analyzing twitter messages. In Proceed-
ings of the First Workshop on Social Media Analyt-
ics, pages 115–122, New York, NY, USA.
Johannes Daxenberger, Oliver Ferschke, Iryna
Gurevych, and Torsten Zesch. 2014. Dkpro tc:
A java-based framework for supervised learning
experiments on textual data. In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics. System Demonstrations,
page (to appear), Baltimore, MD, USA.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing, Hissar,
Bulgaria.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), pages 363–370.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings
of the 20th International Joint Conference on Arti-
ficial Intelligence, volume 7, pages 1606–1611, Hy-
derabad, India.
Kevin Gimpel, Nathan Schneider, Brendan O’Con-
nor, Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A Smith. 2011. Part-of-speech tag-
ging for twitter: Annotation, features, and experi-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42–47.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an up-
date. ACM SIGKDD Explorations Newsletter,
11(1):10–18.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter power: Tweets as
electronic word of mouth. Journal of the Ameri-
can Society for Information Science and Technology,
60(11):2169–2188.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word–emotion association lexicon. Com-
putational Intelligence, 29(3):436–465.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Seman-
tic Evaluation Exercises (SemEval-2013), Atlanta,
Georgia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wil-
son. 2013. Semeval-2013 task 2: Sentiment anal-
ysis in twitter. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
312–320, Atlanta, Georgia, USA.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R Routledge, and Noah A Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Fourth International
AAAI Conference on Weblogs and Social Media,
pages 122–129.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opin-
ion mining. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC’10), Valletta, Malta.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Mahway: Lawrence Erlbaum Asso-
ciates, 71:2001.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
</reference>
<page confidence="0.983364">
709
</page>
<reference confidence="0.999598307692308">
Torsten Zesch, editors, Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation, Dublin,
Ireland.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of in-
ternational conference on new methods in language
processing, volume 12, pages 44–49.
Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova,
Ralf Steinberger, Hristo Tanev, Silvia V´azquez, and
Vanni Zavarella. 2012. Creating sentiment dictio-
naries via triangulation. Decision Support Systems,
53(4):689–694.
</reference>
<page confidence="0.997038">
710
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.465995">
<title confidence="0.9886575">UKPDIPF: A Lexical Semantic Approach to Sentiment Prediction in Twitter Data</title>
<author confidence="0.954609">Oliver Iryna</author>
<affiliation confidence="0.988098">Knowledge Processing Lab Computer Science Department, Technische Universit¨at Knowledge Processing Lab German Institute for Educational</affiliation>
<web confidence="0.979032">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.976285">We present a sentiment classification system that participated in the SemEval 2014 shared task on sentiment analysis in Twitter. Our system expands tokens in a tweet with semantically similar expressions using a large novel distributional thesaurus and calculates the semantic relatedness of the expanded tweets to word lists representing positive and negative sentiment. This approach helps to assess the polarity of tweets that do not directly contain polarity cues. Moreover, we incorporate syntactic, lexical and surface sentiment features. On the message level, our system achieved the 8th place in terms of macroaveraged F-score among 50 systems, with particularly good performance on the Lifecorpus and the Twitter dataset. On the expression level, our system ranked 14 out of 27 systems, based on macro-averaged F-score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Dkpro similarity: An open source framework for text similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>121--126</pages>
<location>Sofia, Bulgaria.</location>
<marker>B¨ar, Zesch, Gurevych, 2013</marker>
<rawString>Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2013. Dkpro similarity: An open source framework for text similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 121–126, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Becker</author>
<author>George Erhart</author>
<author>David Skiba</author>
<author>Valentine Matula</author>
</authors>
<title>Avaya: Sentiment analysis on twitter with self-training and polarity lexicon expansion.</title>
<date>2013</date>
<pages>333</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="7134" citStr="Becker et al. (2013)" startWordPosition="1119" endWordPosition="1122">we extracted each feature twice - once on the tweet level and once on the focus expression level. Only n-gram features were extracted solely from the expressions. For Subtask B (tweet polarity), we extracted features on tweet level only. In both cases, we use the Information Gain feature selection approach in WEKA to rank the features and prune the feature space with a threshold of T=0.005. 3.1 Lexical features As a basis for our similarity and expansion experiments (sections 3.4 and 3.5), we use the binary sentiment polarity lexicon by Liu (2012) augmented with the smiley polarity lexicon by Becker et al. (2013) and an additional swear word list5 [further as Liuaugmented]. We selected this augmented lexicon for two reasons: firstly, it was the highest ranked lexical feature on the developmenttest and crossvalidation experiments, secondly it consists of two plain word lists and therefore does not introduce another complexity dimension for advanced feature calculations. We further measure lexicon hits normalized per number of tweet tokens for the following lexicons: Pennebaker’s Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), the NRC Emotion Lexicon (Mohammad and Turney, 2013), the </context>
</contexts>
<marker>Becker, Erhart, Skiba, Matula, 2013</marker>
<rawString>Lee Becker, George Erhart, David Skiba, and Valentine Matula. 2013. Avaya: Sentiment analysis on twitter with self-training and polarity lexicon expansion. Atlanta, Georgia, USA, page 333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Martin Riedl</author>
</authors>
<title>Text: Now in 2d! a framework for lexical expansion with contextual similarity.</title>
<date>2013</date>
<journal>Journal of Language Modelling,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="11407" citStr="Biemann and Riedl, 2013" startWordPosition="1818" endWordPosition="1821">l 93 excited 54 nice 87 true 53 funny 84 stupid 53 cute 81 gr8 52 perfect 70 entertaining 52 wonderful 67 favorite 52 lovely 66 talented 49 tired 65 other 49 annoying 63 depressing 48 Great 63 flawless 48 new 62 inspiring 47 hilarious 62 incredible 46 bad 61 complicated 46 hot 61 gorgeous 45 Table 1: Unsupervised expansion of ‘awesome’ 3.4 Tweet expansion We expanded the content words in a tweet, i.e. nouns, verbs, adjectives and adverbs, with similar words from a word similarity thesaurus that was computed on 80 million English tweets from 2012 using the JoBim contextual semantics framework (Biemann and Riedl, 2013). Table 1 shows an example for a lexical expansion of the word awesome. The score was computed using left and right neighbor bigram features for the holing operation. The value hence shows how often the word appeared in the same left and right context as the original word. The upper limit of the score is set to 1,000. We then match the expanded tweet against the Liuauymented positive and negative lexicons. We assign to the lexicon hits of the expanded words their (contextual similarity) expansion score, using a score of 1,000 as an anchor-value for the original tweet, setting an expansion cut </context>
</contexts>
<marker>Biemann, Riedl, 2013</marker>
<rawString>Chris Biemann and Martin Riedl. 2013. Text: Now in 2d! a framework for lexical expansion with contextual similarity. Journal of Language Modelling, 1(1):55–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
<author>Xiaojun Zeng</author>
</authors>
<title>Twitter mood predicts the stock market.</title>
<date>2011</date>
<journal>Journal of Computational Science,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="2072" citStr="Bollen et al., 2011" startWordPosition="303" endWordPosition="306">ortly after an event, contributes to the high level of emotions in many such messages. Being able to automatically and reliably evaluate these emotions in context of a specific event or a product would be highly beneficial not only in marketing (Jansen et al., 2009) or public relations, but also in political sciences (O’Connor et al., 2010), disaster manageThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ ment, stock market analysis (Bollen et al., 2011) or the health sector (Culotta, 2010). Due to its large number of applications, sentiment analysis on Twitter is a very popular task. Challenges arise both from the character of the task and from the language specifics of Twitter messages. Messages are normally very short and informal, frequently using slang, alternative spelling, neologism and links, and mostly ignoring the punctuation. Our experiments have been carried out as part of the SemEval 2014 Task 9 - Sentiment Analysis on Twitter (Rosenthal et al., 2014), a rerun of a SemEval-2013 Task 2 (Nakov et al., 2013). The datasets are thus d</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of Computational Science, 2(1):1 – 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
</authors>
<title>A semantic approach to automated text sentiment analysis.</title>
<date>2009</date>
<contexts>
<context position="8228" citStr="Brooke (2009)" startWordPosition="1288" endWordPosition="1289">stic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), the NRC Emotion Lexicon (Mohammad and Turney, 2013), the NRC Hashtag Emotion Lexicon (Mohammad et al., 2013) and the Sentiment140 lexicon (Mohammad et al., 2013). We use an additional lexicon of positive, negative, very positive and very negative words, diminishers, intensifiers and negations composed by Steinberger et al. (2012), where we calculate the polarity score as described in their paper. In a complementary set of features we combine each of the lexicons above with a list of weighted intensifying expressions as published by Brooke (2009). The intensity of any polar word found in any of the emotion lexicons used is intensified or diminished by a given weight if an intensifier (a 5based on http://www.youswear.com 705 bit, very, slightly...) is found within the preceding three tokens. Additionally, we record the overall counts of lexicon hits for positive words, negative words and the difference of the two. In one set of features we consider only lexicons clearly meant for binary polarity, while a second set of features also includes other emotions, such as fear or anger, from the NRC and the LIWC corpora. 3.2 Negation We handle</context>
</contexts>
<marker>Brooke, 2009</marker>
<rawString>Julian Brooke. 2009. A semantic approach to automated text sentiment analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
</authors>
<title>Towards detecting influenza epidemics by analyzing twitter messages.</title>
<date>2010</date>
<booktitle>In Proceedings of the First Workshop on Social Media Analytics,</booktitle>
<pages>115--122</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="2109" citStr="Culotta, 2010" startWordPosition="311" endWordPosition="312">gh level of emotions in many such messages. Being able to automatically and reliably evaluate these emotions in context of a specific event or a product would be highly beneficial not only in marketing (Jansen et al., 2009) or public relations, but also in political sciences (O’Connor et al., 2010), disaster manageThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ ment, stock market analysis (Bollen et al., 2011) or the health sector (Culotta, 2010). Due to its large number of applications, sentiment analysis on Twitter is a very popular task. Challenges arise both from the character of the task and from the language specifics of Twitter messages. Messages are normally very short and informal, frequently using slang, alternative spelling, neologism and links, and mostly ignoring the punctuation. Our experiments have been carried out as part of the SemEval 2014 Task 9 - Sentiment Analysis on Twitter (Rosenthal et al., 2014), a rerun of a SemEval-2013 Task 2 (Nakov et al., 2013). The datasets are thus described in detail in the overview pa</context>
</contexts>
<marker>Culotta, 2010</marker>
<rawString>Aron Culotta. 2010. Towards detecting influenza epidemics by analyzing twitter messages. In Proceedings of the First Workshop on Social Media Analytics, pages 115–122, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Daxenberger</author>
<author>Oliver Ferschke</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Dkpro tc: A java-based framework for supervised learning experiments on textual data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. System Demonstrations,</booktitle>
<pages>page</pages>
<location>Baltimore, MD, USA.</location>
<contexts>
<context position="4234" citStr="Daxenberger et al., 2014" startWordPosition="651" endWordPosition="655">benefiting from resources such as Wiktionary and WordNet. On top of that, we expand content words (adjectives, adverbs, nouns and verbs) in the tweet with similar words, which we derive from a novel corpus of more than 80 million English Tweets gathered by the Language Technology group1 at TU Darm1http://www.lt.informatik.tu-darmstadt.de 704 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 704–710, Dublin, Ireland, August 23-24, 2014. stadt. 2 Experimental setup Our experimental setup is based on an open-source text classification framework DKPro TC2 (Daxenberger et al., 2014), which allows to combine NLP pipelines into a configurable and modular system for preprocessing, feature extraction and classification. We use the unit classi�cation mode of DKPro TC for Subtask A and the document classi�cation mode for Subtask B. 2.1 Preprocessing We customized the message reader for Subtask B to ignore the first part of the tweet when the word but is found. This approach helps to reduce the misleading positive hits when a negative message is introduced positively (It’d be good, but). For preprocessing the data, we use components from DKPro Core3. Preprocessing is the same f</context>
</contexts>
<marker>Daxenberger, Ferschke, Gurevych, Zesch, 2014</marker>
<rawString>Johannes Daxenberger, Oliver Ferschke, Iryna Gurevych, and Torsten Zesch. 2014. Dkpro tc: A java-based framework for supervised learning experiments on textual data. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. System Demonstrations, page (to appear), Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Twitter part-of-speech tagging for all: Overcoming sparse and noisy data.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing,</booktitle>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="5186" citStr="Derczynski et al., 2013" startWordPosition="811" endWordPosition="814">first part of the tweet when the word but is found. This approach helps to reduce the misleading positive hits when a negative message is introduced positively (It’d be good, but). For preprocessing the data, we use components from DKPro Core3. Preprocessing is the same for subtasks A and B, with the only difference that in the subtask A the target expression is additionally annotated as text classi�cation unit, while the rest of the tweet is considered to be a document context. We first segment the data with the Stanford Segmenter4, apply the Stanford POS Tagger with a Twitter-trained model (Derczynski et al., 2013), and subsequently apply the Stanford Lemmatizer4, TreeTagger Chunker (Schmid, 1994), Stanford Named Entity Recognizer (Finkel et al., 2005) and Stanford Parser (Klein and Manning, 2003) to each tweet. After this linguistic preprocessing, the token segmentation of the Stanford tools is removed and overwritten by the ArkTweet Tagger (Gimpel et al., 2011), which is more suitable for recognizing hashtags and smileys as one particular token. Finally, we expand the tweet and proceed to feature extraction as described in detail in Section 3. 2.2 Classification We trained our system on the provided t</context>
</contexts>
<marker>Derczynski, Ritter, Clark, Bontcheva, 2013</marker>
<rawString>Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all: Overcoming sparse and noisy data. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>363--370</pages>
<contexts>
<context position="5326" citStr="Finkel et al., 2005" startWordPosition="830" endWordPosition="833">uced positively (It’d be good, but). For preprocessing the data, we use components from DKPro Core3. Preprocessing is the same for subtasks A and B, with the only difference that in the subtask A the target expression is additionally annotated as text classi�cation unit, while the rest of the tweet is considered to be a document context. We first segment the data with the Stanford Segmenter4, apply the Stanford POS Tagger with a Twitter-trained model (Derczynski et al., 2013), and subsequently apply the Stanford Lemmatizer4, TreeTagger Chunker (Schmid, 1994), Stanford Named Entity Recognizer (Finkel et al., 2005) and Stanford Parser (Klein and Manning, 2003) to each tweet. After this linguistic preprocessing, the token segmentation of the Stanford tools is removed and overwritten by the ArkTweet Tagger (Gimpel et al., 2011), which is more suitable for recognizing hashtags and smileys as one particular token. Finally, we expand the tweet and proceed to feature extraction as described in detail in Section 3. 2.2 Classification We trained our system on the provided training data only, excluding the dev data. We use classifiers from the WEKA (Hall et al., 2009) toolkit, which are integrated in the DKPro T</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<volume>7</volume>
<pages>1606--1611</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="3566" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="552" endWordPosition="555">task B). In subtask A, each tweet in a corpus contained a marked instance of a word or phrase. The goal is to determine whether that instance is positive, negative or neutral in that context. In subtask B, the goal is to classify whether the entire message is of positive, negative, or neutral sentiment. For messages conveying both a positive and negative sentiment, the stronger one should be chosen. The key components of our system are the sentiment polarity lexicons. In contrast to previous approaches, we do not only count exact lexicon hits, but also calculate explicit semantic relatedness (Gabrilovich and Markovitch, 2007) between the tweet and the sentiment list, benefiting from resources such as Wiktionary and WordNet. On top of that, we expand content words (adjectives, adverbs, nouns and verbs) in the tweet with similar words, which we derive from a novel corpus of more than 80 million English Tweets gathered by the Language Technology group1 at TU Darm1http://www.lt.informatik.tu-darmstadt.de 704 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 704–710, Dublin, Ireland, August 23-24, 2014. stadt. 2 Experimental setup Our experimental setup is based on an open-sourc</context>
<context position="12433" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1994" endWordPosition="1997">gative lexicons. We assign to the lexicon hits of the expanded words their (contextual similarity) expansion score, using a score of 1,000 as an anchor-value for the original tweet, setting an expansion cut at 100. The overall tweet score is then normalized by the sum of word expansion scores. 3.5 Semantic similarity Tweet messages are short and each emotional word is very valuable for the task, even when it may not be present in a specific lexicon. Therefore, we calculate a semantic relatedness score between the tweet and the positive or negative word list. We use the ESA similarity measure (Gabrilovich and Markovitch, 2007) as implemented in the DKPro similarity software pack706 age (B¨ar et al., 2013), calculated on English Wik- Features with the highest Information Gain tionary and WordNet as two separate concept were the ones based on Liuaugmented. Adding the spaces. The ESA vectors are freely available7. weighted intensifiers of Brooke to the sentiment This way we obtain in total six features: sim(orig- lexicons did not outperform the simple lexicon inal tweet word list, positive word list), sim(orig- lookup. They were followed by features derived inal tweet word list, negative word list), differ- from the l</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipediabased explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, volume 7, pages 1606–1611, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papersVolume 2, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="5881" citStr="Hall et al., 2009" startWordPosition="923" endWordPosition="926">, 1994), Stanford Named Entity Recognizer (Finkel et al., 2005) and Stanford Parser (Klein and Manning, 2003) to each tweet. After this linguistic preprocessing, the token segmentation of the Stanford tools is removed and overwritten by the ArkTweet Tagger (Gimpel et al., 2011), which is more suitable for recognizing hashtags and smileys as one particular token. Finally, we expand the tweet and proceed to feature extraction as described in detail in Section 3. 2.2 Classification We trained our system on the provided training data only, excluding the dev data. We use classifiers from the WEKA (Hall et al., 2009) toolkit, which are integrated in the DKPro TC framework. Our final configuration consists of a SVM-SMO classifier with a gaussian kernel. The optimal hyperparameters have been experimentally derived 2http://code.google.com/p/dkpro-tc 3http://code.google.com/p/dkpro-core-asl 4http://nlp.stanford.edu/software/corenlp.shtml and finally set to C=1 and G=0.01. The resulting model was wrapped in a cost sensitive meta classifier from the WEKA toolkit with the error costs set to reflect the class imbalance in the training set. 3 Features used We now describe the features used in our experiments. For </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The weka data mining software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<title>Twitter power: Tweets as electronic word of mouth.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>11</issue>
<contexts>
<context position="1718" citStr="Jansen et al., 2009" startWordPosition="254" endWordPosition="257"> (Fi=71.92) and the Twitter sarcasm (Fi=54.59) dataset. On the expression level, our system ranked 14 out of 27 systems, based on macro-averaged F-score. 1 Introduction Microblogging sites, such as Twitter, have become an important source of information about current events. The fact that users write about their experiences, often directly during or shortly after an event, contributes to the high level of emotions in many such messages. Being able to automatically and reliably evaluate these emotions in context of a specific event or a product would be highly beneficial not only in marketing (Jansen et al., 2009) or public relations, but also in political sciences (O’Connor et al., 2010), disaster manageThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ ment, stock market analysis (Bollen et al., 2011) or the health sector (Culotta, 2010). Due to its large number of applications, sentiment analysis on Twitter is a very popular task. Challenges arise both from the character of the task and from the language specifics of Twitter messages. Messag</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter power: Tweets as electronic word of mouth. Journal of the American Society for Information Science and Technology, 60(11):2169–2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="5372" citStr="Klein and Manning, 2003" startWordPosition="837" endWordPosition="841">reprocessing the data, we use components from DKPro Core3. Preprocessing is the same for subtasks A and B, with the only difference that in the subtask A the target expression is additionally annotated as text classi�cation unit, while the rest of the tweet is considered to be a document context. We first segment the data with the Stanford Segmenter4, apply the Stanford POS Tagger with a Twitter-trained model (Derczynski et al., 2013), and subsequently apply the Stanford Lemmatizer4, TreeTagger Chunker (Schmid, 1994), Stanford Named Entity Recognizer (Finkel et al., 2005) and Stanford Parser (Klein and Manning, 2003) to each tweet. After this linguistic preprocessing, the token segmentation of the Stanford tools is removed and overwritten by the ArkTweet Tagger (Gimpel et al., 2011), which is more suitable for recognizing hashtags and smileys as one particular token. Finally, we expand the tweet and proceed to feature extraction as described in detail in Section 3. 2.2 Classification We trained our system on the provided training data only, excluding the dev data. We use classifiers from the WEKA (Hall et al., 2009) toolkit, which are integrated in the DKPro TC framework. Our final configuration consists </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="7067" citStr="Liu (2012)" startWordPosition="1109" endWordPosition="1110">in our experiments. For Subtask A (contextual polarity), we extracted each feature twice - once on the tweet level and once on the focus expression level. Only n-gram features were extracted solely from the expressions. For Subtask B (tweet polarity), we extracted features on tweet level only. In both cases, we use the Information Gain feature selection approach in WEKA to rank the features and prune the feature space with a threshold of T=0.005. 3.1 Lexical features As a basis for our similarity and expansion experiments (sections 3.4 and 3.5), we use the binary sentiment polarity lexicon by Liu (2012) augmented with the smiley polarity lexicon by Becker et al. (2013) and an additional swear word list5 [further as Liuaugmented]. We selected this augmented lexicon for two reasons: firstly, it was the highest ranked lexical feature on the developmenttest and crossvalidation experiments, secondly it consists of two plain word lists and therefore does not introduce another complexity dimension for advanced feature calculations. We further measure lexicon hits normalized per number of tweet tokens for the following lexicons: Pennebaker’s Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Crowdsourcing a word–emotion association lexicon.</title>
<date>2013</date>
<journal>Computational Intelligence,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="7728" citStr="Mohammad and Turney, 2013" startWordPosition="1207" endWordPosition="1210"> lexicon by Becker et al. (2013) and an additional swear word list5 [further as Liuaugmented]. We selected this augmented lexicon for two reasons: firstly, it was the highest ranked lexical feature on the developmenttest and crossvalidation experiments, secondly it consists of two plain word lists and therefore does not introduce another complexity dimension for advanced feature calculations. We further measure lexicon hits normalized per number of tweet tokens for the following lexicons: Pennebaker’s Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), the NRC Emotion Lexicon (Mohammad and Turney, 2013), the NRC Hashtag Emotion Lexicon (Mohammad et al., 2013) and the Sentiment140 lexicon (Mohammad et al., 2013). We use an additional lexicon of positive, negative, very positive and very negative words, diminishers, intensifiers and negations composed by Steinberger et al. (2012), where we calculate the polarity score as described in their paper. In a complementary set of features we combine each of the lexicons above with a list of weighted intensifying expressions as published by Brooke (2009). The intensity of any polar word found in any of the emotion lexicons used is intensified or dimini</context>
</contexts>
<marker>Mohammad, Turney, 2013</marker>
<rawString>Saif M Mohammad and Peter D Turney. 2013. Crowdsourcing a word–emotion association lexicon. Computational Intelligence, 29(3):436–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Nrc-canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013),</booktitle>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="7785" citStr="Mohammad et al., 2013" startWordPosition="1216" endWordPosition="1219"> list5 [further as Liuaugmented]. We selected this augmented lexicon for two reasons: firstly, it was the highest ranked lexical feature on the developmenttest and crossvalidation experiments, secondly it consists of two plain word lists and therefore does not introduce another complexity dimension for advanced feature calculations. We further measure lexicon hits normalized per number of tweet tokens for the following lexicons: Pennebaker’s Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), the NRC Emotion Lexicon (Mohammad and Turney, 2013), the NRC Hashtag Emotion Lexicon (Mohammad et al., 2013) and the Sentiment140 lexicon (Mohammad et al., 2013). We use an additional lexicon of positive, negative, very positive and very negative words, diminishers, intensifiers and negations composed by Steinberger et al. (2012), where we calculate the polarity score as described in their paper. In a complementary set of features we combine each of the lexicons above with a list of weighted intensifying expressions as published by Brooke (2009). The intensity of any polar word found in any of the emotion lexicons used is intensified or diminished by a given weight if an intensifier (a 5based on htt</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="2647" citStr="Nakov et al., 2013" startWordPosition="400" endWordPosition="403"> stock market analysis (Bollen et al., 2011) or the health sector (Culotta, 2010). Due to its large number of applications, sentiment analysis on Twitter is a very popular task. Challenges arise both from the character of the task and from the language specifics of Twitter messages. Messages are normally very short and informal, frequently using slang, alternative spelling, neologism and links, and mostly ignoring the punctuation. Our experiments have been carried out as part of the SemEval 2014 Task 9 - Sentiment Analysis on Twitter (Rosenthal et al., 2014), a rerun of a SemEval-2013 Task 2 (Nakov et al., 2013). The datasets are thus described in detail in the overview papers. The rerun uses the same training and development data, but new test data from Twitter and a “surprise domain”. The task consists of two subtasks: an expression-level subtask (Subtask A) and a message-level subtask (Subtask B). In subtask A, each tweet in a corpus contained a marked instance of a word or phrase. The goal is to determine whether that instance is positive, negative or neutral in that context. In subtask B, the goal is to classify whether the entire message is of positive, negative, or neutral sentiment. For messa</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312–320, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Fourth International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>122--129</pages>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R Routledge, and Noah A Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Fourth International AAAI Conference on Weblogs and Social Media, pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta.</location>
<contexts>
<context position="14109" citStr="Pak and Paroubek (2010)" startWordPosition="2261" endWordPosition="2264"> the shorter computation based features contributed to the highest perfortime and lower memory requirements. However, mance gain, as shown in Table 3. The negation in our later experiments Wiktionary performed approach based on the Stanford dependency parser better. We presume this can be due to a better was the most helpful, although it tripled the runcoverage for the Twitter corpus, although detailed time. Using the simpler negation context as suganalysis of this aspect is yet to be performed. gested in Pang et al. (2002) performed still on av3.6 Other features erage better than using none. Pak and Paroubek (2010) pointed out a relation When using WordNet, semantic similarity to between the presence of different part-of-speech lexicons did not outperform direct lexicon hits. types and sentiment polarity. We measure the Usage of Wiktionary instead lead to major imratio of each part-of-speech type to each chunk. provement (Table 3), unfortunately after the SeWe furthermore count the occurrences of the mEval challenge. dependency tag for negation. We use the Stanford Tweet expansion appears to improve the clasNamed Entity Recognizer to count occurrence sification performance, however the threshold of of p</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="9234" citStr="Pang et al. (2002)" startWordPosition="1458" endWordPosition="1461"> features we consider only lexicons clearly meant for binary polarity, while a second set of features also includes other emotions, such as fear or anger, from the NRC and the LIWC corpora. 3.2 Negation We handle negation in two ways. On the expression level (Subtask A) we rely on the negation dependency tag provided by the Stanford Dependency Parser. This one captures verb negations rather precisely and thus helps to handle emotional verb expressions such as like vs don’t like. On the tweet level (all features of Subtask B and entiretweet-level features of Subtask A) we adopt the approach of Pang et al. (2002), considering as a negation context any sequence of tokens between a negation expression and the end of a sentence segment as annotated by the Stanford Segmenter. The negation expressions (don’t, can’t...) are represented by the list of invertors from Steinberger’s lexicon (Steinberger et al., 2012). We first assign polarity score to each word in the tweet based on the lexicon hits and then revert it for the words lying in the negation context. This approach is more robust than the one of the dependency governor but is error-prone in the area of overlapping (cascaded) negation contexts. 3.3 N-</context>
<context position="14015" citStr="Pang et al. (2002)" startWordPosition="2245" endWordPosition="2248">mitted using Word- form the simple lexicon lookup. Overall, lexiconNet vectors mainly for the shorter computation based features contributed to the highest perfortime and lower memory requirements. However, mance gain, as shown in Table 3. The negation in our later experiments Wiktionary performed approach based on the Stanford dependency parser better. We presume this can be due to a better was the most helpful, although it tripled the runcoverage for the Twitter corpus, although detailed time. Using the simpler negation context as suganalysis of this aspect is yet to be performed. gested in Pang et al. (2002) performed still on av3.6 Other features erage better than using none. Pak and Paroubek (2010) pointed out a relation When using WordNet, semantic similarity to between the presence of different part-of-speech lexicons did not outperform direct lexicon hits. types and sentiment polarity. We measure the Usage of Wiktionary instead lead to major imratio of each part-of-speech type to each chunk. provement (Table 3), unfortunately after the SeWe furthermore count the occurrences of the mEval challenge. dependency tag for negation. We use the Stanford Tweet expansion appears to improve the clasNam</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Martha E Francis</author>
<author>Roger J Booth</author>
</authors>
<title>Linguistic inquiry and word count: Liwc</title>
<date>2001</date>
<pages>71--2001</pages>
<contexts>
<context position="7675" citStr="Pennebaker et al., 2001" startWordPosition="1198" endWordPosition="1201">on by Liu (2012) augmented with the smiley polarity lexicon by Becker et al. (2013) and an additional swear word list5 [further as Liuaugmented]. We selected this augmented lexicon for two reasons: firstly, it was the highest ranked lexical feature on the developmenttest and crossvalidation experiments, secondly it consists of two plain word lists and therefore does not introduce another complexity dimension for advanced feature calculations. We further measure lexicon hits normalized per number of tweet tokens for the following lexicons: Pennebaker’s Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), the NRC Emotion Lexicon (Mohammad and Turney, 2013), the NRC Hashtag Emotion Lexicon (Mohammad et al., 2013) and the Sentiment140 lexicon (Mohammad et al., 2013). We use an additional lexicon of positive, negative, very positive and very negative words, diminishers, intensifiers and negations composed by Steinberger et al. (2012), where we calculate the polarity score as described in their paper. In a complementary set of features we combine each of the lexicons above with a list of weighted intensifying expressions as published by Brooke (2009). The intensity of any polar word found in any </context>
</contexts>
<marker>Pennebaker, Francis, Booth, 2001</marker>
<rawString>James W Pennebaker, Martha E Francis, and Roger J Booth. 2001. Linguistic inquiry and word count: Liwc 2001. Mahway: Lawrence Erlbaum Associates, 71:2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2592" citStr="Rosenthal et al., 2014" startWordPosition="389" endWordPosition="392"> details: http://creativecommons.org/licenses/by/4.0/ ment, stock market analysis (Bollen et al., 2011) or the health sector (Culotta, 2010). Due to its large number of applications, sentiment analysis on Twitter is a very popular task. Challenges arise both from the character of the task and from the language specifics of Twitter messages. Messages are normally very short and informal, frequently using slang, alternative spelling, neologism and links, and mostly ignoring the punctuation. Our experiments have been carried out as part of the SemEval 2014 Task 9 - Sentiment Analysis on Twitter (Rosenthal et al., 2014), a rerun of a SemEval-2013 Task 2 (Nakov et al., 2013). The datasets are thus described in detail in the overview papers. The rerun uses the same training and development data, but new test data from Twitter and a “surprise domain”. The task consists of two subtasks: an expression-level subtask (Subtask A) and a message-level subtask (Subtask B). In subtask A, each tweet in a corpus contained a marked instance of a word or phrase. The goal is to determine whether that instance is positive, negative or neutral in that context. In subtask B, the goal is to classify whether the entire message is</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of international conference on new methods in language processing,</booktitle>
<volume>12</volume>
<pages>44--49</pages>
<contexts>
<context position="5270" citStr="Schmid, 1994" startWordPosition="824" endWordPosition="825">g positive hits when a negative message is introduced positively (It’d be good, but). For preprocessing the data, we use components from DKPro Core3. Preprocessing is the same for subtasks A and B, with the only difference that in the subtask A the target expression is additionally annotated as text classi�cation unit, while the rest of the tweet is considered to be a document context. We first segment the data with the Stanford Segmenter4, apply the Stanford POS Tagger with a Twitter-trained model (Derczynski et al., 2013), and subsequently apply the Stanford Lemmatizer4, TreeTagger Chunker (Schmid, 1994), Stanford Named Entity Recognizer (Finkel et al., 2005) and Stanford Parser (Klein and Manning, 2003) to each tweet. After this linguistic preprocessing, the token segmentation of the Stanford tools is removed and overwritten by the ArkTweet Tagger (Gimpel et al., 2011), which is more suitable for recognizing hashtags and smileys as one particular token. Finally, we expand the tweet and proceed to feature extraction as described in detail in Section 3. 2.2 Classification We trained our system on the provided training data only, excluding the dev data. We use classifiers from the WEKA (Hall et</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of international conference on new methods in language processing, volume 12, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Steinberger</author>
<author>Mohamed Ebrahim</author>
<author>Maud Ehrmann</author>
</authors>
<title>Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova, Ralf Steinberger, Hristo Tanev,</title>
<date>2012</date>
<pages>53--4</pages>
<location>Silvia V´azquez, and</location>
<contexts>
<context position="8008" citStr="Steinberger et al. (2012)" startWordPosition="1250" endWordPosition="1253"> plain word lists and therefore does not introduce another complexity dimension for advanced feature calculations. We further measure lexicon hits normalized per number of tweet tokens for the following lexicons: Pennebaker’s Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), the NRC Emotion Lexicon (Mohammad and Turney, 2013), the NRC Hashtag Emotion Lexicon (Mohammad et al., 2013) and the Sentiment140 lexicon (Mohammad et al., 2013). We use an additional lexicon of positive, negative, very positive and very negative words, diminishers, intensifiers and negations composed by Steinberger et al. (2012), where we calculate the polarity score as described in their paper. In a complementary set of features we combine each of the lexicons above with a list of weighted intensifying expressions as published by Brooke (2009). The intensity of any polar word found in any of the emotion lexicons used is intensified or diminished by a given weight if an intensifier (a 5based on http://www.youswear.com 705 bit, very, slightly...) is found within the preceding three tokens. Additionally, we record the overall counts of lexicon hits for positive words, negative words and the difference of the two. In on</context>
<context position="9534" citStr="Steinberger et al., 2012" startWordPosition="1504" endWordPosition="1507">n dependency tag provided by the Stanford Dependency Parser. This one captures verb negations rather precisely and thus helps to handle emotional verb expressions such as like vs don’t like. On the tweet level (all features of Subtask B and entiretweet-level features of Subtask A) we adopt the approach of Pang et al. (2002), considering as a negation context any sequence of tokens between a negation expression and the end of a sentence segment as annotated by the Stanford Segmenter. The negation expressions (don’t, can’t...) are represented by the list of invertors from Steinberger’s lexicon (Steinberger et al., 2012). We first assign polarity score to each word in the tweet based on the lexicon hits and then revert it for the words lying in the negation context. This approach is more robust than the one of the dependency governor but is error-prone in the area of overlapping (cascaded) negation contexts. 3.3 N-gram features We extract the 5,000 most frequent word unigrams, bigrams and trigrams cleaned with the Snowball stopword list6 as well as the same amount of skip-n-grams and character trigrams. These are extracted separately on the target expression level for subtask A and on document level for subta</context>
</contexts>
<marker>Steinberger, Ebrahim, Ehrmann, 2012</marker>
<rawString>Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann, Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova, Ralf Steinberger, Hristo Tanev, Silvia V´azquez, and Vanni Zavarella. 2012. Creating sentiment dictionaries via triangulation. Decision Support Systems, 53(4):689–694.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>