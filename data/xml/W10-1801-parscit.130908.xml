<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000106">
<title confidence="0.9907635">
EmotiBlog: a finer-grained and more precise learning of subjectivity
expression models
</title>
<author confidence="0.996719">
Ester Boldrini
</author>
<affiliation confidence="0.7965935">
University of Alicante, Department of
Software and Computing Systems
</affiliation>
<email confidence="0.992552">
eboldrini@dlsi.ua.es
</email>
<author confidence="0.998537">
Patricio Mart’nez-Barco
</author>
<affiliation confidence="0.799407">
University of Alicante, Department of
Software and Computing Systems
</affiliation>
<email confidence="0.995545">
patricio@dlsi.ua.es
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999703464285714">
The exponential growth of the subjective in-
formation in the framework of the Web 2.0
has led to the need to create Natural Language
Processing tools able to analyse and process
such data for multiple practical applications.
They require training on specifically annotated
corpora, whose level of detail must be fine
enough to capture the phenomena involved.
This paper presents EmotiBlog – a fine-
grained annotation scheme for subjectivity.
We show the manner in which it is built and
demonstrate the benefits it brings to the sys-
tems using it for training, through the experi-
ments we carried out on opinion mining and
emotion detection. We employ corpora of dif-
ferent textual genres –a set of annotated re-
ported speech extracted from news articles, the
set of news titles annotated with polarity and
emotion from the SemEval 2007 (Task 14)
and ISEAR, a corpus of real-life self-
expressed emotion. We also show how the
model built from the EmotiBlog annotations
can be enhanced with external resources. The
results demonstrate that EmotiBlog, through its
structure and annotation paradigm, offers high
quality training data for systems dealing both
with opinion mining, as well as emotion detec-
tion.
</bodyText>
<sectionHeader confidence="0.996228" genericHeader="keywords">
1 Credits
</sectionHeader>
<bodyText confidence="0.988757">
This paper has been supported by Ministe-
rio de Ciencia e Innovaci—n- Spanish Gov-
ernment (grant no. TIN2009-13391-C04-
01), and Conselleria d&apos;Educaci—n-
Generalitat Valenciana (grant no. PRO-
</bodyText>
<footnote confidence="0.3624075">
METEO/2009/119 and A-
COMP/2010/288).
</footnote>
<author confidence="0.752123">
Alexandra Balahur
</author>
<affiliation confidence="0.709537">
University of Alicante, Department of
Software and Computing Systems
</affiliation>
<email confidence="0.990912">
abalahur@dlsi.ua.es
</email>
<author confidence="0.960182">
Andres Montoyo
</author>
<affiliation confidence="0.867303">
University of Alicante, Department of
Software and Computing Systems
</affiliation>
<email confidence="0.991671">
montoyo@dlsi.ua.es
</email>
<sectionHeader confidence="0.999719" genericHeader="introduction">
2 Introduction
</sectionHeader>
<bodyText confidence="0.999965351351351">
The exponential growth of the subjective infor-
mation with Web 2.0 created the need to develop
new Natural Language Processing (NLP) tools to
automatically process and manage the content
available on the Internet. Apart from the tradi-
tional textual genres, at present we have new
ones such as blogs, forums and reviews. The
main difference between them is that the latter
are predominantly subjective, containing per-
sonal judgments. At the moment, NLP tools and
methods for analyzing objective information
have a better performance than the new ones the
research community is creating for managing the
subjective content. The survey called “The State
of the Blogosphere 2009”, published by Tech-
norati1 , demonstrates that users are blogging
more than ever. Furthermore, in contrast to the
general idea about bloggers, each day it is more
and more the number of professionals who de-
cide to use this means of communication, contra-
dicting the common belief about the predomi-
nance of an informal editing (Balahur et al.,
2009). Due to the growing interest in this text
type, the subjective data of the Web is increasing
on a daily basis, becoming a reflection of peo-
ple’s opinion about a wide range of topics. (Cui,
Mittal and Datar, 2006). Blogs represent an im-
portant source of real-time, unbiased informa-
tion, useful for the development of many applica-
tions for concrete purposes. Given the proved
importance of automatically processing this data,
a new task has appeared in NLP task, dealing
with the treatment of subjective data: Sentiment
Analysis (SA). The main objective of this paper
is to present EmotiBlog (Boldrini et al., 2009), a
fine-grained annotation scheme for labeling sub-
jectivity in the new textual genres. Subjectivity
</bodyText>
<footnote confidence="0.975432">
1 http://technorati.com/
</footnote>
<page confidence="0.504581">
1
</page>
<note confidence="0.964242">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 1–10,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99986284375">
can be reflected in text through expressions of
emotions beliefs, views (a way of considering
something) 2 and opinions, generally denomi-
nated “private states” (Uspensky, 1973), not
open to verification (Wiebe, 1994). We per-
formed a series of experiments focused on dem-
onstrating that EmotiBlog represents a step for-
ward to previous research in this field; its use
allows a finer-grained and more precise learning
of subjectivity expression models. Starting form
(Wiebe, Wilson and Cardie, 2005) we created an
annotation schema able to capture a wide range
and key elements, which give subjectivity, mov-
ing a step forward the mere polarity recognition.
In particular, the experiments concern expres-
sions of emotion, as a finer-grained analysis of
affect in text and a subsequent task to opinion
mining (OM) and classification. To that aim, we
employ corpora of different textual genres– a set
of annotated reported speech extracted from
news articles (denominated JRC quotes) (Bala-
hur et al., 2010) and the set of news titles anno-
tated with polarity and emotion from the SemE-
val 2007 Task No. 14 (Strapparava and Mihal-
cea, 2007), as well as a corpus of real-life self-
expressed emotion entitled ISEAR (Scherer and
Walbott, 1999). We subsequently show, through
the quality of the results obtained, that Emoti-
Blog, through its structure and annotation para-
digm, offers high quality training for systems
dealing both with opinion mining, as well as
emotion detection.
</bodyText>
<sectionHeader confidence="0.963534" genericHeader="background">
3 Motivation and Contribution
</sectionHeader>
<bodyText confidence="0.999907166666667">
The main motivation of this research is the dem-
onstrated necessity to work towards the harmoni-
zation and interoperability of the increasingly
large number of tools and frameworks that sup-
port the creation, instantiation, manipulation,
querying, and exploitation of annotated resource.
This necessity is stressed by the new tools and
resources, which have been recently created for
processing the subjectivity in the new-textual
genres born with the Web 2.0. Such predomi-
nantly subjective data is increasing at an expo-
nential rate (about 75000 new blogs are reported
to be created every day) and contains opinions on
the most diverse set of topics. Given its world-
wide availability, the subjective data on the Web
has become a primary source of information
(Balahur et al., 2009). As a consequence, new
mechanisms have to be implemented so that this
</bodyText>
<footnote confidence="0.715127">
2 http://dictionary.cambridge.org/
</footnote>
<bodyText confidence="0.999926072727273">
data is effectively analyzed and processed. The
main challenge of the opinionated content is that,
unlike the objective one, which presents facts,
the subjective information is most of the times
difficult and complex to extract and classify us-
ing in grammatically static and fixed rules. Ex-
pression of subjectivity is more spontaneous and
even if the majority is quite formal, new means
of expressivity can be encountered, such as the
use of colloquialisms, sayings, collocations or
anomalies in the use of punctuation; this is moti-
vated by the fact that subjectivity expression is
part of our daily life. For example, at the time of
taking a decision, people search for information
and opinions expressed on the Web on their mat-
ter of interest and base their final decision on the
information found. At the same time, when using
a product, people often write reviews on it, so
that others can have a better idea of the perform-
ance of that product before purchasing it. There-
fore, on the one hand, the growing volume of
opinion information available on the Web allows
for better and more informed decisions of the
users. On the other hand, the amount of data to
be analyzed requires the development of special-
ized NLP systems that automatically extract,
classify and summarize the data available on the
Web on different topics. (Esuli and Sebastiani,
2006) define OM as a recent discipline at the
crossroads of Information Retrieval and Compu-
tational Linguistics, which is concerned not with
the topic a document is about, but with the opin-
ion it expresses. Research in this field has proven
the task to be very difficult, due to the high se-
mantic variability of affective language. Differ-
ent authors have addressed the problem of ex-
tracting and classifying opinion from different
perspectives and at different levels, depending on
a series of factors which can be level of interest
(overall/specific), querying formula (“Nokia
E65”/”Why do people buy Nokia E65?”), type of
text (review on forum/blog/dialogue/press arti-
cle), and manner of expression of opinion - di-
rectly (using opinion statements, e.g. “I think this
product is wonderful!”/”This is a bright initia-
tive”), indirectly (using affect vocabulary, e.g. “I
love the pictures this camera
takes!”/”Personally, I am shocked one can pro-
pose such a law!”) or implicitly (using adjectives
and evaluative expressions, e.g. “It’s light as a
feather and fits right into my pocket!”). While
determining the overall opinion on a movie is
sufficient for taking the decision to watch it or
not, when buying a product, people are interested
in the individual opinions on the different prod-
</bodyText>
<page confidence="0.988274">
2
</page>
<bodyText confidence="0.999828068965518">
uct characteristics. When discussing a person,
one can judge and give opinion on the person’s
actions. Moreover, the approaches taken can vary
depending on the manner in which a user asks
for the data (general formula such as “opinions
on X” or a specific question “Why do people like
X?” and the text source that needs to be queried).
Retrieving opinion information in newspaper
articles or blogs posts is more complex, because
it involves the detection of different discussion
topics, the subjective phrases present and subse-
quently their classification according to polarity.
Especially in the blog area, determining points of
view expressed in dialogues together with the
mixture of quotes and pastes from newspapers on
a topic can, additionally, involve determining the
persons present and whether or not the opinion
expressed is on the required topic or on a point
previously made by another speaker. This diffi-
cult NLP problem requires the use of specialized
data for system training and tuning, gathered,
annotated and tested within the different text
spheres. At the present moment, these
specialized resources are scarce and when they
exist, they are rather simplistically annotated or
highly domain-dependent. Moreover, most of
these resources created are for the English. The
contribution we describe in this paper intends to
propose solutions to the above-mentioned
problems, and consists of the following points:
first of all, we overcome the problem of corpora
scarcity in other languages except English and
also improve the English ones; we present the
manner in which we compiled a multilingual
corpus of blog posts on different topics of
interest in three languages-Spanish, Italian and
English. The second issue we tried to solve was
the coarse-grained annotation schemas employed
in other annotation schema. Thus, we describe
the new annotation model, EmotiBlog built up in
order to capture the different
subjectivity/objectivity, emotion/opinion/attitude
aspects we are interested in at a finer-grained
level. We justify the need for a more detailed
annotation model, the sources and the reasons
taken into consideration when constructing the
corpus and its annotation. Thirdly, we address an
aspect strongly related to blogs annotation: due
the presence of “copy and pastes” from news
articles or other blogs, the frequent quotes, we
include the annotation of both the directly
indicated source, as well as the anaphoric
references at cross-document level. We discuss
on the problems encountered at different stages
and comment upon some of the conclusions we
have reached while performing this research.
this research. Finally, we conclude on our ap-
proach and propose the lines for future work.
</bodyText>
<sectionHeader confidence="0.99981" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999956255319149">
In recent years, different researchers have ad-
dressed the needs and possible methodologies
from the linguistic, theoretical and practical
points of view. Thus, the first step involved re-
sided in building lexical resources of affect, such
as WordNet Affect (Strapparava and Valitutti,
2004), SentiWordNet (Esuli and Sebastiani,
2006), Micro-WNOP (Cerini et. Al, 2007) or
“emotion triggers” (Balahur and Montoyo,
2009). All these lexicons contain single words,
whose polarity and emotions are not necessarily
the ones annotated within the resource in a larger
context. We also employed the ISEAR corpus,
consisting of phrases where people describe a
situation when they felt a certain emotion. Our
work, therefore, concentrates on annotating
larger text spans, in order to consider the undeni-
able influence of the context. The starting point
of research in emotion is represented by (Balahur
and Montoyo, 2008), who centered the idea of
subjectivity around that of private states, and set
the benchmark for subjectivity analysis as the
recognition of opinion-oriented language in order
to distinguish it from objective language and giv-
ing a method to annotate a corpus depending on
these two aspects – MPQA (Wiebe, Wilson and
Cardie, 2005). Furthermore, authors show that
this initial discrimination is crucial for the senti-
ment task, as part of Opinion Information Re-
trieval (last three editions of the TREC Blog
tracks 3 competitions, the TAC 2008 competi-
tion4), Information Extraction (Riloff and Wiebe,
2003) and Question Answering (Stoyanov et al.,
2004) systems. Once this discrimination is done,
or in the case of texts containing only or mostly
subjective language (such as e-reviews), opinion
mining becomes a polarity classification task.
Our work takes into consideration this initial dis-
crimination, but we also add a deeper level of
emotion annotation. Since expressions of emo-
tion are also highly related to opinions, related
work also includes customer review classifica-
tion at a document level, sentiment classification
using unsupervised methods (Turney, 2002),
Machine Learning techniques (Pang and Lee,
2002), scoring of features (Dave, Lawrence and
Pennock, 2003), using PMI, syntactic relations
</bodyText>
<footnote confidence="0.9988075">
3 http://trec.nist.gov/data/blog.html
4 http://www.nist.gov/tac/
</footnote>
<page confidence="0.99789">
3
</page>
<bodyText confidence="0.999945684210526">
and other attributes with SVM (Mullena and Col-
lier, 2004), sentiment classification considering
rating scales (Pang and Lee, 2002), supervised
and unsupervised methods (Chaovalit and Zhou,
2005) and semisupervised learning (Goldberg
and Zhou, 2006). Research in classification at a
document level included sentiment classification
of reviews (Ng, Dasgupta and Arifin, 2006), sen-
timent classification on customer feedback data
(Gamon, Aue, Corston-Oliver, Ringger, 2005),
comparative experiments (Cui, Mittal and Datar,
2006). Other research has been conducted in ana-
lysing sentiment at a sentence level using boot-
strapping techniques (Riloff, Wiebe, 2003), con-
sidering gradable adjectives (Hatzivassiloglou,
Wiebe, 2000), semisupervised learning with the
initial training some strong patterns and then ap-
plying NB or self-training (Wiebe and Riloff,
2005) finding strength of opinions (Wolson,
Wiebe, Hwa, 2004) sum up orientations of opin-
ion words in a sentence (or within some word
window) (Kim and Hovy, 2004), (Wilson and
Wiebe, 2004), determining the semantic orienta-
tion of words and phrases (Turney and Littman,
2003), identifying opinion holders (Stoyanov and
Cardie, 2006), comparative sentence and relation
extraction and feature-based opinion mining and
summarization (Turney, 2002). Finally, fine-
grained, feature-based opinion summarization is
defined in (Hu and Liu, 2004) and researched in
(Turney, 2002) or (Pang and Lee, 2002). All
these approaches concentrate on finding and
classifying the polarity of opinion words, which
are mostly adjectives, without taking into ac-
count modifiers or the context in general. Our
work, on the other hand, represents the first step
towards achieving a contextual comprehension
of the linguistic roots of emotion expression.
</bodyText>
<sectionHeader confidence="0.994981" genericHeader="method">
5 Corpora
</sectionHeader>
<bodyText confidence="0.999987405405406">
It is well known that nowadays blogs are the
second way of communication most used after
the e-mail. They are extremely useful and a poll
for discussing about any topic with the world.
For this reason, the first corpus object of our
study is a collection of blog posts extracted from
the Web. The texts we selected have distinctive
features, extremely different from traditional tex-
tual ones. In fact people writing a post can use an
informal language colloquialism, emoticons, etc.
to express their feelings and it is not rare to find
a mix of sources in the same post; people usually
mention some facts or discourses and then they
give their opinion about them. As we can deduce,
the source detection represents one of the most
complex tasks. As we mentioned above, we car-
ried out a multilingual research, collecting texts
in three languages: Spanish, Italian, and English
about three subjects of interest. The first one
contains blog posts commenting upon the signing
of the Kyoto Protocol against global warming,
the second collection consists of blog entries
about the Mugabe government in Zimbabwe, and
finally we selected a series of blog posts discuss-
ing the issues related to the 2008 USA presiden-
tial elections. For each of the abovementioned
topics, we have gathered 100 texts, summing up
a total of 30.000 words approximately for each
language. However in this research we start with
English but consider as future work labeling the
other languages we have. The second corpus we
employed for this research is a collection of 1592
quotes extracted from the news in April 2008. As
a consequence they are about many different top-
ics and in English (Balahur and Steinberg, 2009).
Both of these corpora have been annotated with
EmotiBlog that is presented in the next section.
</bodyText>
<sectionHeader confidence="0.99904" genericHeader="method">
6 EmotiBlog Annotation Model
</sectionHeader>
<bodyText confidence="0.999931625">
Our annotation schema can be defined as a fine-
grained model for labelling subjectivity of the
new-textual genres born with the Web 2.0. As
mentioned above, it represents a step forward to
previous research and it is focused on detecting
the linguistic elements, which give subjectivity
to the text. The EmotiBlog annotation is divided
into different levels (Figure 1).
</bodyText>
<figureCaption confidence="0.998719">
Figure 1: General structure of EmotiBlog.
</figureCaption>
<bodyText confidence="0.999826625">
As we can observe in Figure 1, the first distinc-
tion to be made is between objective and subjec-
tive speech. If we are labelling an objective sen-
tence, we insert the source element, while if we
are annotating a subjective discourse, a list of
elements with the corresponding attributes have
to be added. We select among the list of subjec-
tive elements and specify the element’s attrib-
</bodyText>
<page confidence="0.988112">
4
</page>
<bodyText confidence="0.7900875">
utes. Table 1 presents the annotation model in
detail.
</bodyText>
<table confidence="0.999397787878788">
Elem. Description
Obj. speech Confidence, comment, source, target.
Subj. speech Confidence, comment, level, emotion,
phenomenon, polarity, source and target.
Adjectives Confidence, comment, level, emotion,
phenomenon, modifier/not, polarity, source
and target.
Adverbs Confidence, comment, level, emotion,
phenomenon, modifier/not, polarity, source
and target.
Verbs Confidence, comment, level, emotion,
phenomenon, polarity, mode, source and
target.
Anaphora Confidence, comment, type, source and
target.
Capital letter Confidence, comment, level, emotion,
phenomenon, modifier/not, polarity, source
and target.
Punctuation Confidence, comment, level, emotion,
phenomenon, modifier/not, polarity, source
and target.
Names Confidence, comment, level, emotion,
phenomenon, modifier/not, polarity, and
source.
Phenomenon Confidence, comment, type: collocation,
saying, slang, title, and rhetoric.
Reader Inter- Confidence, comment, level, emotion,
pretation phenomenon, polarity, source and target.
Author Inter- Confidence, comment, level, emotion,
pretation phenomenon, polarity, source and target.
Emotions Confidence, comment, accept, anger,
anticipation, anxiety, appreciation, bad,
bewilderment, comfort, É
</table>
<tableCaption confidence="0.999863">
Table 1: EmotiBlog structure
</tableCaption>
<bodyText confidence="0.999615292682927">
Each element of the discourse has its own attrib-
utes with a series of features, which have to be
annotated. Due to space reasons it is impossible
to detail each one of them, however we would
like to underline the most innovative and rel-
evant. For each element we are labelling the an-
notator has to insert his level of confidence. In
this way we will assign each label a weight that
will be computed for future evaluations. More-
over, the annotator has to insert the polarity,
which can be positive or negative, the level
(high, medium, low) and also the sentiment this
element is expressing. Table 2 presents a com-
plete list of the emotions we selected to be part
of EmotiBlog. We grouped all sentiments into
subgroups in order to help the evaluation pro-
cess. In fact emotions of the same subgroup will
have less impact when calculating the inter-
annotation agreement. In order to make this sub-
division proper and effective division, we were
inspired by (Scherer, 2005) who created an alter-
native dimensional structure of the semantic
space for emotions. The graph below represents
the mapping of the term Russell (1983) uses for
his claim of an emotion circumflex in two-
dimensional valence by activity/arousal space
(upper-case terms). As we can appreciate, the
circle is divided by 4 axes. Moreover, Scherer
distinguishes between positive and negative sen-
timents and after that between active and passive.
Furthermore emotions are grouped between ob-
structive and conductive, and finally between
high power and low power control. We started
form this classification, grouping sentiments into
positive and negative, but we divided them as
high/low power control, obstructive/conductive
and active/passive. Further on, we distributed the
sentiments within our list into the Scherer slots
creating other smaller categories included in the
abovementioned general ones. The result of this
division is shown in Table 2:
</bodyText>
<table confidence="0.999763782608695">
Group Emotions
Criticism Sarcasm, irony, incorrect, criticism,
objection, opposition, scepticism.
Happiness Joy, joke.
Support Accept, correct, good, hope, support,
trust, rapture, respect, patience,
appreciation, excuse.
Importance Important, interesting, will, justice,
longing, anticipation, revenge.
Gratitude Thank.
Guilt Guilt, vexation.
Fear Fear, fright, troubledness, anxiety.
Surprise Surprise, bewilderment, disappoint-
ment, consternation.
Anger Rage, hatred, enmity, wrath, force,
anger, revendication.
Envy Envy, rivalry, jealousy.
Indifference Unimportant, yield, sluggishness.
Pity Compassion, shame, grief.
Pain Sadness, lament, remorse, mourning,
depression, despondency.
Shyness Timidity.
Bad Bad, malice, disgust, greed.
</table>
<tableCaption confidence="0.9974595">
Table 2: Alternative dimensional structures of the
semantic space for emotions
</tableCaption>
<bodyText confidence="0.999912631578947">
Following with the description of the model, we
said that the first distinction to be made is be-
tween objective and subjective speech. Analys-
ing the texts we collected, we realised that even
if the writer uses an objective speech, sometimes
it is just apparently objective and for this reason
we added two elements: reader and author inter-
pretation. The first one is the impres-
sion/feeling/reaction the reader has reading the
intervention and what s/he can deduce from the
piece of text and the author interpretation is what
we can understand from the author (politic orien-
tation, preferences). All this information can be
deduced form some linguistic elements that ap-
parently are not so objective as they may appear.
Another innovative element we inserted in the
model is the coreference but just at a cross-post
level. It is necessary because blogs are composed
by posts linked between them and thus cross-
</bodyText>
<page confidence="0.984947">
5
</page>
<bodyText confidence="0.987753307692308">
document coreference can help the reader to fol-
low the conversations. We also label the unusual
usage of capital letters and repeated punctuation.
In fact, it is very common in blogs to find words
written in capital letter or with no conventional
usage of punctuation; these features usually
mean shouts or a particular mood of the writer.
Using EmotiBlog, we annotate the single ele-
ments, but we also mark sayings or collocations,
representative of each language. A saying is a
well-known and wise statement, which often has
a meaning, different from the simple meanings of
the words it contains5; while a collocation is a
word or phrase, which is frequently used with
another word or phrase, in a way that sounds cor-
rect to native speakers, but might not be expected
from the individual words’ meanings6. Finally
we insert for each element the source and topic.
An example of annotation can be: &lt;phenomenon
target=&amp;quot;Kyoto Protocol&amp;quot; category=&amp;quot;phrase&amp;quot; degree=&amp;quot;medium&amp;quot;
source=&amp;quot;w&amp;quot; polarity=&amp;quot;positive&amp;quot; emotion=&amp;quot;good&amp;quot;&gt;The Onion has a
&lt;adjective target=&amp;quot;Kyoto Protocol&amp;quot; phenomenon=&amp;quot;phrase&amp;quot; de-
gree=&amp;quot;medium&amp;quot; polarity=&amp;quot;positive&amp;quot; emotion=&amp;quot;good&amp;quot; source=&amp;quot;w&amp;quot;
ismodifier=&amp;quot;yes&amp;quot;&gt;great&lt;/adjective&gt; story today titled “Bush Told
to Sign Birthday Treaty for Someone Named Kyoto.&amp;quot;
&lt;/phenomenon&gt;
</bodyText>
<sectionHeader confidence="0.992012" genericHeader="evaluation">
7 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999973086956522">
In order to evaluate the appropriateness of the
EmotiBlog annotation scheme and to prove that
the fine-grained level it aims at has a positive
impact on the performance of the systems em-
ploying it as training, we performed several ex-
periments. Given that a) EmotiBlog contains an-
notations for individual words, as well as for
multi-word expressions and at a sentence level,
and b) they are labeled with polarity, but also
emotion, our experiments show how the anno-
tated elements can be used as training for the
opinion mining and polarity classification task,
as well as for emotion detection. Moreover, tak-
ing into consideration the fact that EmotiBlog
labels the intensity level of the annotated ele-
ments, we performed a brief experiment on de-
termining the sentiment intensity, measured on a
three-level scale: low, medium and high. In order
to perform these three different evaluations, we
chose three different corpora. The first one is a
collection of quotes (reported speech) from
newspaper articles presented in (Balahur et al.,
2010), enriched with the manual fine-grained
</bodyText>
<footnote confidence="0.513765">
5 Definition according to the Cambridge Advanced Learner’s
Dictionary
6 Definition according to the Cambridge Advanced Learner’s
Dictionary
</footnote>
<bodyText confidence="0.9998505">
annotation of EmotiBlog7; the second one is the
collection of newspaper titles in the test set of the
SemEval 2007 task number 14 Ð Affective Text.
Finally, the third one is a corpus of self-reported
emotional response Ð ISEAR (Scherer and Wal-
bott, 1999). The intensity classification task is
evaluated only on the second corpus, given that it
is the only one in which scores between -100 and
0 and 0 and 100, respectively, are given for the
polarity of the titles.
</bodyText>
<subsectionHeader confidence="0.99979">
6.1 Creation of training models
</subsectionHeader>
<bodyText confidence="0.999905090909091">
For the OM and polarity classification task, we
first extracted the Named Entities contained in
the annotations using Lingpipe and united
through a “_” all the tokens pertaining to the NE.
All the annotations of punctuation signs that had
a specific meaning together were also united un-
der a single punctuation sign. Subsequently, we
processed the annotated data, using Minipar. We
compute, for each word in a sentence, a series of
features (some of these features are used in (Choi
et al., 2005):
</bodyText>
<listItem confidence="0.79791080952381">
• the part of speech (POS)
• capitalization (if all letters are in capitals, if
only the first letter is in capitals, and if it is a
NE or not)
• opinionatedness/intensity/emotion - if the
word is annotated as opinion word, its polar-
ity, i.e. 1 and -1 if the word is positive or
negative, respectively and 0 if it is not an
opinion word, its intensity (1.2 or 3) and 0 if
it is not a subjective word, its emotion (if it
has, none otherwise)
• syntactic relatedness with other opinion
word Ð if it is directly dependent of an opin-
ion word or modifier (0 or 1), plus the polar-
ity/intensity and emotion of this word (0 for
all the components otherwise)
• role in 2-word, 3-word and 4-word annota-
tions: opinionatedness, intensity and emo-
tion of the other words contained in the an-
notation, direct dependency relations with
them if they exist and 0 otherwise.
</listItem>
<bodyText confidence="0.999951">
We compute the length of the longest sentence in
EmotiBlog. The feature vector for each of the
sentences contains the feature vectors of each of
its words and 0s for the corresponding feature
vectors of the words, which the current sentence
has less than the longest annotated sentence. Fi-
nally, we add for each sentence as feature binary
features for subjectivity and polarity, the value
corresponding to the intensity of opinion and the
</bodyText>
<note confidence="0.434815">
7 Freely available on request to the authors.
</note>
<page confidence="0.999014">
6
</page>
<bodyText confidence="0.999918777777778">
general emotion. These feature vectors are fed
into the Weka8 SVM SMO ML algorithm and a
model is created (EmotiBlog I). A second model
(EmotiBlog II) is created by adding to the collec-
tion of single opinion and emotion words anno-
tated in EmotiBlog, the Opinion Finder lexicon
and the opinion words found in MicroWordNet,
the General Inquirer resource and WordNet Af-
fect.
</bodyText>
<subsectionHeader confidence="0.999952">
6.2 Evaluation of models on test sets
</subsectionHeader>
<bodyText confidence="0.99981024">
In order to evaluate the performance of the mod-
els extracted from the features of the annotations
in EmotiBlog, we performed different tests. The
first one regarded the evaluation of the polarity
and intensity classification task using the Emoit-
blog I and II constructed models on two test sets
– the JRC quotes collection and the SemEval
2007 Task Number 14 test set. Since the quotes
often contain more than a sentence, we consider
the polarity and intensity of the entire quote as
the most frequent result in each class, corre-
sponding to its constituent sentences. Also, given
the fact that the SemEval Affective Text head-
lines were given intensity values between -100
and 100, we mapped the values contained in the
Gold Standard of the task into three categories: [-
100, -67] is high (value 3 in intensity) and nega-
tive (value -1 in polarity), [-66, 34] medium
negative and [33, 1] is low negative. The values
between [1 and 100] are mapped in the same
manner to the positive category. 0 was consid-
ered objective, so containing the value 0 for in-
tensity. The results are presented in Table 3 (the
values I and II correspond to the models Emoti-
Blog I and EmotiBlog II):
</bodyText>
<table confidence="0.999609363636364">
Test Evaluation Precision Recall
Corpus type
JRC quotes I Polarity 32.13 54.09
Intensity 36.00 53.2
JRC quotes Polarity 36.4 51.00
II
Intensity 38.7 57.81
SemEval I Polarity 38.57 51.3
Intensity 37.39 50.9
SemEval II Polarity 35.8 58.68
Intensity 32.3 50.4
</table>
<tableCaption confidence="0.965326">
Table 3. Results for polarity and intensity classifi-
cation using the models built from the EmotiBlog
annotations
</tableCaption>
<bodyText confidence="0.99978625">
The results shown in Table 2 show a signifi-
cantly high improvement over the results ob-
tained in the SemEval task in 2007. This is ex-
plainable, on the one hand, by the fact that sys-
</bodyText>
<footnote confidence="0.658555">
8 http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<bodyText confidence="0.999913216216216">
tems performing the opinion task did not have at
their disposal the lexical resources for opinion
employed in the EmotiBlog II model, but also
because of the fact that they did not use machine
learning on a corpus comparable to EmotiBlog
(as seen from the results obtained when using
solely the EmotiBlog I corpus). Compared to the
NTCIR 8 Multilingual Analysis Task this year,
we obtained significant improvements in preci-
sion, with a recall that is comparable to most of
the participating systems. In the second experi-
ment, we tested the performance of emotion clas-
sification using the two models built using Emo-
tiBlog on the three corpora – JRC quotes, SemE-
val 2007 Task No.14 test set and the ISEAR cor-
pus. The JRC quotes are labeled using Emoti-
Blog; however, the other two are labeled with a
small set of emotions – 6 in the case of the Se-
mEval data (joy, surprise, anger, fear, sadness,
disgust) and 7 in ISEAR (joy, sadness, anger,
fear, guilt, shame, disgust). Moreover, the Se-
mEval data contains more than one emotion per
title in the Gold Standard, therefore we consider
as correct any of the classifications containing
one of them. In order to unify the results and ob-
tain comparable evaluations, we assessed the
performance of the system using the alternative
dimensional structures defined in Table 1. The
ones not overlapping with the category of any of
the 8 different emotions in SemEval and ISEAR
are considered as “Other” and are not included
either in the training, nor test set. The results of
the evaluation are presented in Table 4. Again,
the values I and II correspond to the models
EmotiBlog I and II. The “Emotions” category
contains the following emotions: joy, sadness,
anger, fear, guilt, shame, disgust, surprise.
</bodyText>
<table confidence="0.9992365">
Test Evaluation Precision Recall
corpus type
JRC Emotions 24.7 15.08
quotes I
JRC Emotions 33.65 18.98
quotes II
SemEval I Emotions 29.03 18.89
SemEval II Emotions 32.98 18.45
ISEAR I Emotions 22.31 15.01
ISEAR II Emotions 25.62 17.83
</table>
<tableCaption confidence="0.8413435">
Table 4. Results for emotion classification using the
models built from the EmotiBlog annotations.
</tableCaption>
<bodyText confidence="0.999867666666667">
The best results for emotion detection were ob-
tained for the “anger” category, where the preci-
sion was around 35 percent, for a recall of 19
percent. The worst results obtained were for the
ISEAR category of “shame”, where precision
was around 12 percent, with a recall of 15 per-
</bodyText>
<page confidence="0.998846">
7
</page>
<bodyText confidence="0.999990233333333">
cent. We believe this is due to the fact that the
latter emotion is a combination of more complex
affective states and it can be easily misclassified
to other categories of emotion. Moreover, from
the analysis performed on the errors, we realized
that many of the affective phenomena presented
were more explicit in the case of texts expressing
strong emotions such as “joy” and “anger”, and
were mostly related to common-sense interpreta-
tion of the facts presented in the weaker ones. As
it can be seen in Table 3, results for the texts per-
taining to the news category obtain better results,
most of all news titles. This is due to the fact that
such texts, although they contain a few words,
have a more direct and stronger emotional charge
than direct speech (which may be biased by the
need to be diplomatic, find the best suited words
etc.). Finally, the error analysis showed that emo-
tion that is directly reported by the persons expe-
riencing is more “hidden”, in the use of words
carrying special signification or related to gen-
eral human experience. This fact makes emotion
detection in such texts a harder task. Neverthe-
less, the results in all corpora are comparable,
showing that the approach is robust enough to
handle different text types. All in all, the results
obtained using the fine and coarse-grained anno-
tations in EmotiBlog increased the performance
of emotion detection as compared to the systems
in the SemEval competition.
</bodyText>
<subsectionHeader confidence="0.987617">
6.3 Discussion on the overall results
</subsectionHeader>
<bodyText confidence="0.999986533333333">
From the results obtained, we can see that this
approach combining the features extracted from
the EmotiBlog fine and coarse-grained annota-
tions helps to balance between the results ob-
tained for precision and recall. The impact of
using additional resources that contain opinion
words is that of increasing the recall of the sys-
tem, at the cost of a slight drop in precision,
which proves that the approach is robust enough
so that additional knowledge sources can be
added. Although the corpus is small, the results
obtained show that the phenomena it captures is
relevant in the OM task, not only for the blog
sphere, but also for other types of text (newspa-
per articles, self-reported affect).
</bodyText>
<sectionHeader confidence="0.991088" genericHeader="conclusions">
8 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999997305084746">
Due to the exponential increase of the subjective
information result of the high-level usage of the
Internet and the Web 2.0, NLP able to process
this data are required. In this paper we presented
the procedure by which we compiled a multilin-
gual corpus of blog posts on different topics of
interest in three languages: Spanish, Italian and
English. Further on, we explained the need to
create a finer-grained annotation schema that can
be used to improve the performance of subjectiv-
ity mining systems. Thus, we presented the new
annotation model, EmotiBlog and justified the
benefits of this detailed annotation schema, pre-
senting the sources and the reasons taken into
consideration when building up the corpus and
its labeling. Furthermore, we addressed the pres-
ence of “copy and pastes” from news articles or
other blogs, the frequent quotes. For solving this
possible ambiguity we included the annotation of
both the directly indicated source, as well as the
anaphoric references at cross-document level.
We performed several experiments on three dif-
ferent corpora, aimed at finding and classifying
both the opinion, as well as the expressions of
emotion they contained; we showed that the fine
and coarse-grained levels of annotation that
EmotiBlog contains offers important information
on the structure of affective texts, leading to an
improvement of the performance of systems
trained on it. Although the EmotiBlog corpus is
small, the results obtained are promising and
show that the phenomena it captures are relevant
in the OM task, not only for the blog sphere, but
also for other textual-genres. It is well known
that OM is an extremely challenging task and a
young discipline, thus there is room for im-
provement above all to solve linguistic phenom-
ena such as the correference resolution at a cross
document level, temporal expression recognition.
In addition to this, more experiments would need
to be done in order to verify the complete ro-
bustness of EmotiBlog. Last but not least, our
idea is to include the existing tools for a more
effective semi-supervised annotation. After the
training of the ML system we obtain automati-
cally some markables which have to be validated
or not by the annotator and the ideal option
would be to connect these terms the system de-
tects automatically with tools, such as the map-
ping with an opinion lexicon based on WordNet
(SentiWordNet, WordNet Affect, MicroWord-
Net), in order to automatically annotate all the
synonyms and antonyms with the same or the
opposite polarity respectively and assigning them
some other elements contemplated into the Emo-
tiBlog annotation schema. This would mean an
important step forward for saving time during the
annotation process and it will also assure a high
quality annotation due to the human supervision.
</bodyText>
<page confidence="0.997347">
8
</page>
<sectionHeader confidence="0.983263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999312059322034">
Balahur A., Steinberger R., Kabadjov M., Zavarella
V., van der Goot E., Halkia M., Pouliquen B., and
Belyaeva J. 2010. Sentiment Analysis in the News.
In Proceedings of LREC 2010.
Balahur A., Boldrini E., Montoyo A., Mart’nez-Barco
P. 2009. A Comparative Study of Open Domain
and Opinion Question Answering Systems for Fac-
tual and Opinionated Queries. In Proceedings of
the Recent Advances in Natural Language Proc-
essing.
Balahur A., Montoyo A. 2008. Applying a Culture
Dependent Emotion Triggers Database for Text
Valence and Emotion Classification. In Proceed-
ings of the AISB 2008 Symposium on Affective
Language in Human and Machine, Aberdeen, Scot-
land.
Balahur A., Steinberger R., Rethinking Sentiment
Analysis in the News: from Theory to Practice and
back. In Proceeding of WOMSA 2009. Seville.
Balahur A., Boldrini E., Montoyo A., Mart’nez-Barco
P. 2009. Summarizing Threads in Blogs Using
Opinion Polarity. In Proceedings of ETTS work-
shop. RANLP. 2009.
Boldrini E., Balahur A., Mart’nez-Barco P., Montoyo
A. 2009. EmotiBlog: a fine-grained model for
emotion detection in non-traditional textual gen-
res. In Proceedings of WOMSA. Seville, Spain.
Boldrini E., Fern‡ndez J., G—mez J.M., Mart’nez-
Barco P. 2009. Machine Learning Techniques for
Automatic Opinion Detection in Non-Traditional
Textual Genres. In Proceedings of WOMSA 2009.
Seville, Spain.
Chaovalit P, Zhou L. 2005. Movie Review Mining: a
Comparison between Supervised and Unsupervised
Classification Approaches. In Proceedings of
HICSS-05.
Carletta J. 1996. Assessing agreement on
classiÞcation task: the kappa statistic. Computa-
tional Linguistics, 22(2): 249–254.
Cui H., Mittal V., Datar M. 2006. Comparative Ex-
periments on Sentiment Classification for Online
Product Reviews. In Proceedings of the 21st Na-
tional Conference on Artificial Intelligence AAAI.
Cerini S., Compagnoni V., Demontis A., Formentelli
M., and Gandini G. 2007. Language resources and
linguistic theory: Typology, second language ac-
quisition. English linguistics (Forthcoming), chap-
ter Micro-WNOp: A gold standard for the evalua-
tion of automatically compiled lexical resources for
opinion mining. Franco Angeli Editore, Milano, IT.
Choi Y., Cardie C., Rilloff E., Padwardhan S. 2005.
Identifying Sources of Opinions with Conditional
Random Fields and Extraction Patterns. In Pro-
ceedings of the HLT/EMNLP.
Dave K., Lawrence S., Pennock, D. “Mining the Pea-
nut Gallery: Opinion Extraction and Semantic
Classification of Product Reviews”. In Proceedings
of WWW-03. 2003.
Esuli A., Sebastiani F. 2006. SentiWordNet: A Pub-
licly Available Resource for Opinion Mining. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation, LREC 2006,
Genoa, Italy.
Gamon M., Aue S., Corston-Oliver S., Ringger E.
2005. Mining Customer Opinions from Free Text.
Lecture Notes in Computer Science.
Goldberg A.B., Zhu J. 2006. Seeing stars when there
aren’t many stars: Graph-based semi-supervised
learning for sentiment categorization. In HLT-
NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Process-
ing.
Hu M., Liu B. 2004. Mining Opinion Features in Cus-
tomer Reviews. In Proceedings of Nineteenth Na-
tional Conference on Artificial Intelligence AAAI.
Hatzivassiloglou V., Wiebe J. 2000. Effects of adjec-
tive orientation and gradability on sentence subjec-
tivity. In Proceedings of COLING.
Kim S.M., Hovy E. 2004. Determining the Sentiment
of Opinions. In Proceedings of COLING.
Mullen T., Collier N. 2006. Sentiment Analysis Using
Support Vector Machines with Diverse Information
Sources. In Proceedings of EMNLP. 2004. Lin,
W.H., Wilson, T., Wiebe, J., Hauptman, A. “Which
Side are You On? Identifying Perspectives at the
Document and Sentence Levels”. In Proceedings of
the Tenth Conference on Natural Language Learn-
ing CoNLL.2006.
Ng V., Dasgupta S. and Arifin S. M. 2006. Examining
the Role of Linguistics Knowledge Sources in the
Automatic Identification and Classification of Re-
views. In the proceedings of the ACL, Sydney.
Pang B., Lee L., Vaithyanathan S. 2002. Thumbs up?
Sentiment classification using machine learning
techniques. In Proceedings of EMNLP-02, the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Riloff E., Wiebe J. 2003. Learning Extraction Pat-
terns for Subjective Expressions. In Proceedings of
the 2003 Conference on Empirical Methods in
Natural Language Processing.
Strapparava C. Valitutti A. 2004. WordNet-Affect: an
affective extension of WordNet. In Proceedings
ofthe 4th International Conference on Language
Resources and Evaluation, LREC.
Russell J.A. 1983. Pancultural aspects of the human
conceptual organization of emotions. Journal of
Personality and Social Psychology 45: 1281–8.
Scherer K. R. 2005. What are emotions? And how can
they be measured? Social Science Information,
44(4), 693–727.
Stoyanov V. and Cardie C. 2006. Toward Opinion
Summarization: Linking the Sources. COLING-
ACL. Workshop on Sentiment and Subjectivity in
Text.
Stoyanov V., Cardie C., Litman D., and Wiebe J.
2004. Evaluating an Opinion Annotation Scheme
Using a New Multi-Perspective Question and An-
</reference>
<page confidence="0.944729">
9
</page>
<reference confidence="0.999844516129032">
swer Corpus. AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text.
Strapparava and Mihalcea, 2007 - SemEval 2007
Task 14: Affective Text. In Proceedings of the
ACL.
Turney P. 2002. Thumbs Up or Thumbs Down? Se-
mantic Orientation Applied to Unsupervised Clas-
sification of Reviews. ACL 2002: 417-424.
Turney P., Littman M. 2003. Measuring praise and
criticism: Inference of semantic orientation from
association. ACM Transactions on Information
Systems 21.
Uspensky B. 1973. A Poetics of Composition. Univer-
sity of California Press, Berkeley, California.
Wiebe J. M. 1994. Tracking point of view in narra-
tive. Computational Linguistics, vol. 20, pp. 233–
287.
Wiebe J., Wilson T. and Cardie C. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation.
Wilson T., Wiebe J., Hwa R. 2004. Just how mad are
you? Finding strong and weak opinion clauses. In:
Proceedings of AAAI.
Wiebe J., Wilson T. and Cardie C. 2005. “Annotation
Expressions of Opinions and Emotions in Lan-
guage. Language Resources and Evaluation.
Wiebe J., Riloff E. 2005. Creating Subjective and
Objective Sentence Classifiers from Unannotated
Texts. In Proceedings of the 6th International Con-
ference on Computational Linguistics and Intelli-
gent Text Processing (CICLing).
</reference>
<page confidence="0.997786">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.012839">
<title confidence="0.9959175">EmotiBlog: a finer-grained and more precise learning of expression models</title>
<author confidence="0.987837">Ester</author>
<affiliation confidence="0.985864">University of Alicante, Department</affiliation>
<title confidence="0.8581565">Software and Computing eboldrini@dlsi.ua.es</title>
<author confidence="0.995137">Patricio</author>
<affiliation confidence="0.6946205">University of Alicante, Department Software and Computing</affiliation>
<email confidence="0.737118">patricio@dlsi.ua.es</email>
<abstract confidence="0.999578137931034">The exponential growth of the subjective information in the framework of the Web 2.0 has led to the need to create Natural Language Processing tools able to analyse and process such data for multiple practical applications. They require training on specifically annotated corpora, whose level of detail must be fine enough to capture the phenomena involved. paper presents a finegrained annotation scheme for subjectivity. We show the manner in which it is built and demonstrate the benefits it brings to the systems using it for training, through the experiments we carried out on opinion mining and emotion detection. We employ corpora of different textual genres –a set of annotated reported speech extracted from news articles, the set of news titles annotated with polarity and emotion from the SemEval 2007 (Task 14) and ISEAR, a corpus of real-life selfexpressed emotion. We also show how the model built from the EmotiBlog annotations can be enhanced with external resources. The demonstrate that through its structure and annotation paradigm, offers high quality training data for systems dealing both with opinion mining, as well as emotion detection.</abstract>
<note confidence="0.592626">1 Credits This paper has been supported by Ministerio de Ciencia e Innovaci—n- Spanish Government (grant no. TIN2009-13391-C04- 01), and Conselleria d&apos;Educaci—n- Valenciana (grant no. PROand COMP/2010/288). Alexandra University of Alicante, Department</note>
<title confidence="0.962659">Software and Computing Systems abalahur@dlsi.ua.es</title>
<author confidence="0.967748">Andres</author>
<affiliation confidence="0.7709735">University of Alicante, Department Software and Computing</affiliation>
<intro confidence="0.742419">montoyo@dlsi.ua.es</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>R Steinberger</author>
<author>M Kabadjov</author>
<author>V Zavarella</author>
<author>E van der Goot</author>
<author>M Halkia</author>
<author>B Pouliquen</author>
<author>J Belyaeva</author>
</authors>
<title>Sentiment Analysis in the News.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Balahur, Steinberger, Kabadjov, Zavarella, van der Goot, Halkia, Pouliquen, Belyaeva, 2010</marker>
<rawString>Balahur A., Steinberger R., Kabadjov M., Zavarella V., van der Goot E., Halkia M., Pouliquen B., and Belyaeva J. 2010. Sentiment Analysis in the News. In Proceedings of LREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>E Boldrini</author>
<author>A Montoyo</author>
<author>P Mart’nez-Barco</author>
</authors>
<title>A Comparative Study of Open Domain and Opinion Question Answering Systems for Factual and Opinionated Queries.</title>
<date>2009</date>
<booktitle>In Proceedings of the Recent Advances in Natural Language Processing.</booktitle>
<marker>Balahur, Boldrini, Montoyo, Mart’nez-Barco, 2009</marker>
<rawString>Balahur A., Boldrini E., Montoyo A., Mart’nez-Barco P. 2009. A Comparative Study of Open Domain and Opinion Question Answering Systems for Factual and Opinionated Queries. In Proceedings of the Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>A Montoyo</author>
</authors>
<title>Applying a Culture Dependent Emotion Triggers Database for Text Valence and Emotion Classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the AISB 2008 Symposium on Affective Language in Human and Machine,</booktitle>
<location>Aberdeen, Scotland.</location>
<contexts>
<context position="12554" citStr="Balahur and Montoyo, 2008" startWordPosition="1962" endWordPosition="1965">, 2004), SentiWordNet (Esuli and Sebastiani, 2006), Micro-WNOP (Cerini et. Al, 2007) or “emotion triggers” (Balahur and Montoyo, 2009). All these lexicons contain single words, whose polarity and emotions are not necessarily the ones annotated within the resource in a larger context. We also employed the ISEAR corpus, consisting of phrases where people describe a situation when they felt a certain emotion. Our work, therefore, concentrates on annotating larger text spans, in order to consider the undeniable influence of the context. The starting point of research in emotion is represented by (Balahur and Montoyo, 2008), who centered the idea of subjectivity around that of private states, and set the benchmark for subjectivity analysis as the recognition of opinion-oriented language in order to distinguish it from objective language and giving a method to annotate a corpus depending on these two aspects – MPQA (Wiebe, Wilson and Cardie, 2005). Furthermore, authors show that this initial discrimination is crucial for the sentiment task, as part of Opinion Information Retrieval (last three editions of the TREC Blog tracks 3 competitions, the TAC 2008 competition4), Information Extraction (Riloff and Wiebe, 200</context>
</contexts>
<marker>Balahur, Montoyo, 2008</marker>
<rawString>Balahur A., Montoyo A. 2008. Applying a Culture Dependent Emotion Triggers Database for Text Valence and Emotion Classification. In Proceedings of the AISB 2008 Symposium on Affective Language in Human and Machine, Aberdeen, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>R Steinberger</author>
</authors>
<title>Rethinking Sentiment Analysis in the News: from Theory to Practice and back.</title>
<date>2009</date>
<booktitle>In Proceeding of WOMSA</booktitle>
<publisher>Seville.</publisher>
<marker>Balahur, Steinberger, 2009</marker>
<rawString>Balahur A., Steinberger R., Rethinking Sentiment Analysis in the News: from Theory to Practice and back. In Proceeding of WOMSA 2009. Seville.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>E Boldrini</author>
<author>A Montoyo</author>
<author>P Mart’nez-Barco</author>
</authors>
<title>Summarizing Threads in Blogs Using Opinion Polarity.</title>
<date>2009</date>
<booktitle>In Proceedings of ETTS workshop. RANLP.</booktitle>
<marker>Balahur, Boldrini, Montoyo, Mart’nez-Barco, 2009</marker>
<rawString>Balahur A., Boldrini E., Montoyo A., Mart’nez-Barco P. 2009. Summarizing Threads in Blogs Using Opinion Polarity. In Proceedings of ETTS workshop. RANLP. 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Boldrini</author>
<author>A Balahur</author>
<author>P Mart’nez-Barco</author>
<author>A Montoyo</author>
</authors>
<title>EmotiBlog: a fine-grained model for emotion detection in non-traditional textual genres.</title>
<date>2009</date>
<booktitle>In Proceedings of WOMSA. Seville,</booktitle>
<marker>Boldrini, Balahur, Mart’nez-Barco, Montoyo, 2009</marker>
<rawString>Boldrini E., Balahur A., Mart’nez-Barco P., Montoyo A. 2009. EmotiBlog: a fine-grained model for emotion detection in non-traditional textual genres. In Proceedings of WOMSA. Seville, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Boldrini</author>
<author>J Fern‡ndez</author>
<author>J M G—mez</author>
<author>P Mart’nezBarco</author>
</authors>
<title>Machine Learning Techniques for Automatic Opinion Detection in Non-Traditional Textual Genres.</title>
<date>2009</date>
<booktitle>In Proceedings of WOMSA</booktitle>
<location>Seville,</location>
<marker>Boldrini, Fern‡ndez, G—mez, Mart’nezBarco, 2009</marker>
<rawString>Boldrini E., Fern‡ndez J., G—mez J.M., Mart’nezBarco P. 2009. Machine Learning Techniques for Automatic Opinion Detection in Non-Traditional Textual Genres. In Proceedings of WOMSA 2009. Seville, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Chaovalit</author>
<author>L Zhou</author>
</authors>
<title>Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches.</title>
<date>2005</date>
<booktitle>In Proceedings of HICSS-05.</booktitle>
<contexts>
<context position="14121" citStr="Chaovalit and Zhou, 2005" startWordPosition="2192" endWordPosition="2195">n annotation. Since expressions of emotion are also highly related to opinions, related work also includes customer review classification at a document level, sentiment classification using unsupervised methods (Turney, 2002), Machine Learning techniques (Pang and Lee, 2002), scoring of features (Dave, Lawrence and Pennock, 2003), using PMI, syntactic relations 3 http://trec.nist.gov/data/blog.html 4 http://www.nist.gov/tac/ 3 and other attributes with SVM (Mullena and Collier, 2004), sentiment classification considering rating scales (Pang and Lee, 2002), supervised and unsupervised methods (Chaovalit and Zhou, 2005) and semisupervised learning (Goldberg and Zhou, 2006). Research in classification at a document level included sentiment classification of reviews (Ng, Dasgupta and Arifin, 2006), sentiment classification on customer feedback data (Gamon, Aue, Corston-Oliver, Ringger, 2005), comparative experiments (Cui, Mittal and Datar, 2006). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff, Wiebe, 2003), considering gradable adjectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns a</context>
</contexts>
<marker>Chaovalit, Zhou, 2005</marker>
<rawString>Chaovalit P, Zhou L. 2005. Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches. In Proceedings of HICSS-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classiÞcation task: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>249--254</pages>
<marker>Carletta, 1996</marker>
<rawString>Carletta J. 1996. Assessing agreement on classiÞcation task: the kappa statistic. Computational Linguistics, 22(2): 249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>V Mittal</author>
<author>M Datar</author>
</authors>
<title>Comparative Experiments on Sentiment Classification for Online Product Reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence AAAI.</booktitle>
<marker>Cui, Mittal, Datar, 2006</marker>
<rawString>Cui H., Mittal V., Datar M. 2006. Comparative Experiments on Sentiment Classification for Online Product Reviews. In Proceedings of the 21st National Conference on Artificial Intelligence AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cerini</author>
<author>V Compagnoni</author>
<author>A Demontis</author>
<author>M Formentelli</author>
<author>G Gandini</author>
</authors>
<title>Language resources and linguistic theory: Typology, second language acquisition. English linguistics (Forthcoming), chapter Micro-WNOp: A gold standard for the evaluation of automatically compiled lexical resources for opinion mining. Franco Angeli Editore,</title>
<date>2007</date>
<location>Milano, IT.</location>
<marker>Cerini, Compagnoni, Demontis, Formentelli, Gandini, 2007</marker>
<rawString>Cerini S., Compagnoni V., Demontis A., Formentelli M., and Gandini G. 2007. Language resources and linguistic theory: Typology, second language acquisition. English linguistics (Forthcoming), chapter Micro-WNOp: A gold standard for the evaluation of automatically compiled lexical resources for opinion mining. Franco Angeli Editore, Milano, IT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choi</author>
<author>C Cardie</author>
<author>E Rilloff</author>
<author>S Padwardhan</author>
</authors>
<title>Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of the HLT/EMNLP.</booktitle>
<contexts>
<context position="26776" citStr="Choi et al., 2005" startWordPosition="4121" endWordPosition="4124">between -100 and 0 and 0 and 100, respectively, are given for the polarity of the titles. 6.1 Creation of training models For the OM and polarity classification task, we first extracted the Named Entities contained in the annotations using Lingpipe and united through a “_” all the tokens pertaining to the NE. All the annotations of punctuation signs that had a specific meaning together were also united under a single punctuation sign. Subsequently, we processed the annotated data, using Minipar. We compute, for each word in a sentence, a series of features (some of these features are used in (Choi et al., 2005): • the part of speech (POS) • capitalization (if all letters are in capitals, if only the first letter is in capitals, and if it is a NE or not) • opinionatedness/intensity/emotion - if the word is annotated as opinion word, its polarity, i.e. 1 and -1 if the word is positive or negative, respectively and 0 if it is not an opinion word, its intensity (1.2 or 3) and 0 if it is not a subjective word, its emotion (if it has, none otherwise) • syntactic relatedness with other opinion word Ð if it is directly dependent of an opinion word or modifier (0 or 1), plus the polarity/intensity and emotio</context>
</contexts>
<marker>Choi, Cardie, Rilloff, Padwardhan, 2005</marker>
<rawString>Choi Y., Cardie C., Rilloff E., Padwardhan S. 2005. Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns. In Proceedings of the HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dave</author>
<author>S Lawrence</author>
<author>D Pennock</author>
</authors>
<title>Mining the Peanut Gallery: Opinion Extraction and Semantic Classification of Product Reviews”.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW-03.</booktitle>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Dave K., Lawrence S., Pennock, D. “Mining the Peanut Gallery: Opinion Extraction and Semantic Classification of Product Reviews”. In Proceedings of WWW-03. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A Publicly Available Resource for Opinion Mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="7630" citStr="Esuli and Sebastiani, 2006" startWordPosition="1195" endWordPosition="1198">ter of interest and base their final decision on the information found. At the same time, when using a product, people often write reviews on it, so that others can have a better idea of the performance of that product before purchasing it. Therefore, on the one hand, the growing volume of opinion information available on the Web allows for better and more informed decisions of the users. On the other hand, the amount of data to be analyzed requires the development of specialized NLP systems that automatically extract, classify and summarize the data available on the Web on different topics. (Esuli and Sebastiani, 2006) define OM as a recent discipline at the crossroads of Information Retrieval and Computational Linguistics, which is concerned not with the topic a document is about, but with the opinion it expresses. Research in this field has proven the task to be very difficult, due to the high semantic variability of affective language. Different authors have addressed the problem of extracting and classifying opinion from different perspectives and at different levels, depending on a series of factors which can be level of interest (overall/specific), querying formula (“Nokia E65”/”Why do people buy Noki</context>
<context position="11978" citStr="Esuli and Sebastiani, 2006" startWordPosition="1873" endWordPosition="1876">oric references at cross-document level. We discuss on the problems encountered at different stages and comment upon some of the conclusions we have reached while performing this research. this research. Finally, we conclude on our approach and propose the lines for future work. 4 Related Work In recent years, different researchers have addressed the needs and possible methodologies from the linguistic, theoretical and practical points of view. Thus, the first step involved resided in building lexical resources of affect, such as WordNet Affect (Strapparava and Valitutti, 2004), SentiWordNet (Esuli and Sebastiani, 2006), Micro-WNOP (Cerini et. Al, 2007) or “emotion triggers” (Balahur and Montoyo, 2009). All these lexicons contain single words, whose polarity and emotions are not necessarily the ones annotated within the resource in a larger context. We also employed the ISEAR corpus, consisting of phrases where people describe a situation when they felt a certain emotion. Our work, therefore, concentrates on annotating larger text spans, in order to consider the undeniable influence of the context. The starting point of research in emotion is represented by (Balahur and Montoyo, 2008), who centered the idea </context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Esuli A., Sebastiani F. 2006. SentiWordNet: A Publicly Available Resource for Opinion Mining. In Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC 2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>S Aue</author>
<author>S Corston-Oliver</author>
<author>E Ringger</author>
</authors>
<title>Mining Customer Opinions from Free Text.</title>
<date>2005</date>
<journal>Lecture Notes in Computer Science.</journal>
<marker>Gamon, Aue, Corston-Oliver, Ringger, 2005</marker>
<rawString>Gamon M., Aue S., Corston-Oliver S., Ringger E. 2005. Mining Customer Opinions from Free Text. Lecture Notes in Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Goldberg</author>
<author>J Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In HLTNAACL 2006 Workshop on Textgraphs: Graphbased Algorithms for Natural Language Processing.</booktitle>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>Goldberg A.B., Zhu J. 2006. Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization. In HLTNAACL 2006 Workshop on Textgraphs: Graphbased Algorithms for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining Opinion Features in Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of Nineteenth National Conference on Artificial Intelligence AAAI.</booktitle>
<contexts>
<context position="15312" citStr="Hu and Liu, 2004" startWordPosition="2359" endWordPosition="2362">ing some strong patterns and then applying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opinion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orientation of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002). Finally, finegrained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002). All these approaches concentrate on finding and classifying the polarity of opinion words, which are mostly adjectives, without taking into account modifiers or the context in general. Our work, on the other hand, represents the first step towards achieving a contextual comprehension of the linguistic roots of emotion expression. 5 Corpora It is well known that nowadays blogs are the second way of communication most used after the e-mail. They are extremely useful and a poll for discussing about any topic with the world. For this reaso</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Hu M., Liu B. 2004. Mining Opinion Features in Customer Reviews. In Proceedings of Nineteenth National Conference on Artificial Intelligence AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>J Wiebe</author>
</authors>
<title>Effects of adjective orientation and gradability on sentence subjectivity.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING.</booktitle>
<marker>Hatzivassiloglou, Wiebe, 2000</marker>
<rawString>Hatzivassiloglou V., Wiebe J. 2000. Effects of adjective orientation and gradability on sentence subjectivity. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Kim</author>
<author>E Hovy</author>
</authors>
<title>Determining the Sentiment of Opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="14939" citStr="Kim and Hovy, 2004" startWordPosition="2309" endWordPosition="2312">cation on customer feedback data (Gamon, Aue, Corston-Oliver, Ringger, 2005), comparative experiments (Cui, Mittal and Datar, 2006). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff, Wiebe, 2003), considering gradable adjectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns and then applying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opinion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orientation of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002). Finally, finegrained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002). All these approaches concentrate on finding and classifying the polarity of opinion words, which are mostly adjectives, without taking into account modifiers or the con</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Kim S.M., Hovy E. 2004. Determining the Sentiment of Opinions. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mullen</author>
<author>N Collier</author>
</authors>
<title>Sentiment Analysis Using Support Vector Machines with Diverse Information Sources.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Mullen, Collier, 2006</marker>
<rawString>Mullen T., Collier N. 2006. Sentiment Analysis Using Support Vector Machines with Diverse Information Sources. In Proceedings of EMNLP. 2004. Lin, W.H., Wilson, T., Wiebe, J., Hauptman, A. “Which Side are You On? Identifying Perspectives at the Document and Sentence Levels”. In Proceedings of the Tenth Conference on Natural Language Learning CoNLL.2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>S Dasgupta</author>
<author>S M Arifin</author>
</authors>
<title>Examining the Role of Linguistics Knowledge Sources in the Automatic Identification and Classification of Reviews.</title>
<date>2006</date>
<booktitle>In the proceedings of the ACL,</booktitle>
<location>Sydney.</location>
<marker>Ng, Dasgupta, Arifin, 2006</marker>
<rawString>Ng V., Dasgupta S. and Arifin S. M. 2006. Examining the Role of Linguistics Knowledge Sources in the Automatic Identification and Classification of Reviews. In the proceedings of the ACL, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang B., Lee L., Vaithyanathan S. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning Extraction Patterns for Subjective Expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="13156" citStr="Riloff and Wiebe, 2003" startWordPosition="2057" endWordPosition="2060">ur and Montoyo, 2008), who centered the idea of subjectivity around that of private states, and set the benchmark for subjectivity analysis as the recognition of opinion-oriented language in order to distinguish it from objective language and giving a method to annotate a corpus depending on these two aspects – MPQA (Wiebe, Wilson and Cardie, 2005). Furthermore, authors show that this initial discrimination is crucial for the sentiment task, as part of Opinion Information Retrieval (last three editions of the TREC Blog tracks 3 competitions, the TAC 2008 competition4), Information Extraction (Riloff and Wiebe, 2003) and Question Answering (Stoyanov et al., 2004) systems. Once this discrimination is done, or in the case of texts containing only or mostly subjective language (such as e-reviews), opinion mining becomes a polarity classification task. Our work takes into consideration this initial discrimination, but we also add a deeper level of emotion annotation. Since expressions of emotion are also highly related to opinions, related work also includes customer review classification at a document level, sentiment classification using unsupervised methods (Turney, 2002), Machine Learning techniques (Pang</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Riloff E., Wiebe J. 2003. Learning Extraction Patterns for Subjective Expressions. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Strapparava C Valitutti A</author>
</authors>
<title>WordNet-Affect: an affective extension of WordNet.</title>
<date>2004</date>
<booktitle>In Proceedings ofthe 4th International Conference on Language Resources and Evaluation, LREC.</booktitle>
<marker>A, 2004</marker>
<rawString>Strapparava C. Valitutti A. 2004. WordNet-Affect: an affective extension of WordNet. In Proceedings ofthe 4th International Conference on Language Resources and Evaluation, LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Russell</author>
</authors>
<title>Pancultural aspects of the human conceptual organization of emotions.</title>
<date>1983</date>
<journal>Journal of Personality and Social Psychology</journal>
<volume>45</volume>
<pages>1281--8</pages>
<contexts>
<context position="20735" citStr="Russell (1983)" startWordPosition="3207" endWordPosition="3208">tive, the level (high, medium, low) and also the sentiment this element is expressing. Table 2 presents a complete list of the emotions we selected to be part of EmotiBlog. We grouped all sentiments into subgroups in order to help the evaluation process. In fact emotions of the same subgroup will have less impact when calculating the interannotation agreement. In order to make this subdivision proper and effective division, we were inspired by (Scherer, 2005) who created an alternative dimensional structure of the semantic space for emotions. The graph below represents the mapping of the term Russell (1983) uses for his claim of an emotion circumflex in twodimensional valence by activity/arousal space (upper-case terms). As we can appreciate, the circle is divided by 4 axes. Moreover, Scherer distinguishes between positive and negative sentiments and after that between active and passive. Furthermore emotions are grouped between obstructive and conductive, and finally between high power and low power control. We started form this classification, grouping sentiments into positive and negative, but we divided them as high/low power control, obstructive/conductive and active/passive. Further on, we</context>
</contexts>
<marker>Russell, 1983</marker>
<rawString>Russell J.A. 1983. Pancultural aspects of the human conceptual organization of emotions. Journal of Personality and Social Psychology 45: 1281–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R Scherer</author>
</authors>
<title>What are emotions? And how can they be measured?</title>
<date>2005</date>
<journal>Social Science Information,</journal>
<volume>44</volume>
<issue>4</issue>
<pages>693--727</pages>
<contexts>
<context position="20584" citStr="Scherer, 2005" startWordPosition="3183" endWordPosition="3184">ign each label a weight that will be computed for future evaluations. Moreover, the annotator has to insert the polarity, which can be positive or negative, the level (high, medium, low) and also the sentiment this element is expressing. Table 2 presents a complete list of the emotions we selected to be part of EmotiBlog. We grouped all sentiments into subgroups in order to help the evaluation process. In fact emotions of the same subgroup will have less impact when calculating the interannotation agreement. In order to make this subdivision proper and effective division, we were inspired by (Scherer, 2005) who created an alternative dimensional structure of the semantic space for emotions. The graph below represents the mapping of the term Russell (1983) uses for his claim of an emotion circumflex in twodimensional valence by activity/arousal space (upper-case terms). As we can appreciate, the circle is divided by 4 axes. Moreover, Scherer distinguishes between positive and negative sentiments and after that between active and passive. Furthermore emotions are grouped between obstructive and conductive, and finally between high power and low power control. We started form this classification, g</context>
</contexts>
<marker>Scherer, 2005</marker>
<rawString>Scherer K. R. 2005. What are emotions? And how can they be measured? Social Science Information, 44(4), 693–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
</authors>
<title>Toward Opinion Summarization: Linking the Sources.</title>
<date>2006</date>
<booktitle>COLINGACL. Workshop on Sentiment and Subjectivity in Text.</booktitle>
<contexts>
<context position="15108" citStr="Stoyanov and Cardie, 2006" startWordPosition="2333" endWordPosition="2336">cted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff, Wiebe, 2003), considering gradable adjectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns and then applying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opinion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orientation of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002). Finally, finegrained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002). All these approaches concentrate on finding and classifying the polarity of opinion words, which are mostly adjectives, without taking into account modifiers or the context in general. Our work, on the other hand, represents the first step towards achieving a contextual comprehension of the linguistic roots of emotion expression. 5 Cor</context>
</contexts>
<marker>Stoyanov, Cardie, 2006</marker>
<rawString>Stoyanov V. and Cardie C. 2006. Toward Opinion Summarization: Linking the Sources. COLINGACL. Workshop on Sentiment and Subjectivity in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
<author>D Litman</author>
<author>J Wiebe</author>
</authors>
<title>Evaluating an Opinion Annotation Scheme Using a New Multi-Perspective Question and Answer Corpus. AAAI Spring Symposium on Exploring Attitude and Affect in Text.</title>
<date>2004</date>
<contexts>
<context position="13203" citStr="Stoyanov et al., 2004" startWordPosition="2064" endWordPosition="2067">subjectivity around that of private states, and set the benchmark for subjectivity analysis as the recognition of opinion-oriented language in order to distinguish it from objective language and giving a method to annotate a corpus depending on these two aspects – MPQA (Wiebe, Wilson and Cardie, 2005). Furthermore, authors show that this initial discrimination is crucial for the sentiment task, as part of Opinion Information Retrieval (last three editions of the TREC Blog tracks 3 competitions, the TAC 2008 competition4), Information Extraction (Riloff and Wiebe, 2003) and Question Answering (Stoyanov et al., 2004) systems. Once this discrimination is done, or in the case of texts containing only or mostly subjective language (such as e-reviews), opinion mining becomes a polarity classification task. Our work takes into consideration this initial discrimination, but we also add a deeper level of emotion annotation. Since expressions of emotion are also highly related to opinions, related work also includes customer review classification at a document level, sentiment classification using unsupervised methods (Turney, 2002), Machine Learning techniques (Pang and Lee, 2002), scoring of features (Dave, Law</context>
</contexts>
<marker>Stoyanov, Cardie, Litman, Wiebe, 2004</marker>
<rawString>Stoyanov V., Cardie C., Litman D., and Wiebe J. 2004. Evaluating an Opinion Annotation Scheme Using a New Multi-Perspective Question and Answer Corpus. AAAI Spring Symposium on Exploring Attitude and Affect in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Strapparava</author>
<author>Mihalcea</author>
</authors>
<title>SemEval</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="5024" citStr="Strapparava and Mihalcea, 2007" startWordPosition="772" endWordPosition="776">e created an annotation schema able to capture a wide range and key elements, which give subjectivity, moving a step forward the mere polarity recognition. In particular, the experiments concern expressions of emotion, as a finer-grained analysis of affect in text and a subsequent task to opinion mining (OM) and classification. To that aim, we employ corpora of different textual genres– a set of annotated reported speech extracted from news articles (denominated JRC quotes) (Balahur et al., 2010) and the set of news titles annotated with polarity and emotion from the SemEval 2007 Task No. 14 (Strapparava and Mihalcea, 2007), as well as a corpus of real-life selfexpressed emotion entitled ISEAR (Scherer and Walbott, 1999). We subsequently show, through the quality of the results obtained, that EmotiBlog, through its structure and annotation paradigm, offers high quality training for systems dealing both with opinion mining, as well as emotion detection. 3 Motivation and Contribution The main motivation of this research is the demonstrated necessity to work towards the harmonization and interoperability of the increasingly large number of tools and frameworks that support the creation, instantiation, manipulation,</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>Strapparava and Mihalcea, 2007 - SemEval 2007 Task 14: Affective Text. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. ACL</title>
<date>2002</date>
<pages>417--424</pages>
<contexts>
<context position="13721" citStr="Turney, 2002" startWordPosition="2143" endWordPosition="2144"> Information Extraction (Riloff and Wiebe, 2003) and Question Answering (Stoyanov et al., 2004) systems. Once this discrimination is done, or in the case of texts containing only or mostly subjective language (such as e-reviews), opinion mining becomes a polarity classification task. Our work takes into consideration this initial discrimination, but we also add a deeper level of emotion annotation. Since expressions of emotion are also highly related to opinions, related work also includes customer review classification at a document level, sentiment classification using unsupervised methods (Turney, 2002), Machine Learning techniques (Pang and Lee, 2002), scoring of features (Dave, Lawrence and Pennock, 2003), using PMI, syntactic relations 3 http://trec.nist.gov/data/blog.html 4 http://www.nist.gov/tac/ 3 and other attributes with SVM (Mullena and Collier, 2004), sentiment classification considering rating scales (Pang and Lee, 2002), supervised and unsupervised methods (Chaovalit and Zhou, 2005) and semisupervised learning (Goldberg and Zhou, 2006). Research in classification at a document level included sentiment classification of reviews (Ng, Dasgupta and Arifin, 2006), sentiment classific</context>
<context position="15220" citStr="Turney, 2002" startWordPosition="2348" endWordPosition="2349">jectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns and then applying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opinion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orientation of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002). Finally, finegrained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002). All these approaches concentrate on finding and classifying the polarity of opinion words, which are mostly adjectives, without taking into account modifiers or the context in general. Our work, on the other hand, represents the first step towards achieving a contextual comprehension of the linguistic roots of emotion expression. 5 Corpora It is well known that nowadays blogs are the second way of communication most used after the e-mail. They a</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney P. 2002. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. ACL 2002: 417-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
<author>M Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems</journal>
<volume>21</volume>
<contexts>
<context position="15051" citStr="Turney and Littman, 2003" startWordPosition="2326" endWordPosition="2329">, Mittal and Datar, 2006). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff, Wiebe, 2003), considering gradable adjectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns and then applying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opinion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orientation of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002). Finally, finegrained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002). All these approaches concentrate on finding and classifying the polarity of opinion words, which are mostly adjectives, without taking into account modifiers or the context in general. Our work, on the other hand, represents the first step towards achieving a contextual comprehen</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Turney P., Littman M. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Uspensky</author>
</authors>
<title>A Poetics of Composition.</title>
<date>1973</date>
<publisher>University of California Press,</publisher>
<location>Berkeley, California.</location>
<contexts>
<context position="4072" citStr="Uspensky, 1973" startWordPosition="619" endWordPosition="620">ing with the treatment of subjective data: Sentiment Analysis (SA). The main objective of this paper is to present EmotiBlog (Boldrini et al., 2009), a fine-grained annotation scheme for labeling subjectivity in the new textual genres. Subjectivity 1 http://technorati.com/ 1 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 1–10, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics can be reflected in text through expressions of emotions beliefs, views (a way of considering something) 2 and opinions, generally denominated “private states” (Uspensky, 1973), not open to verification (Wiebe, 1994). We performed a series of experiments focused on demonstrating that EmotiBlog represents a step forward to previous research in this field; its use allows a finer-grained and more precise learning of subjectivity expression models. Starting form (Wiebe, Wilson and Cardie, 2005) we created an annotation schema able to capture a wide range and key elements, which give subjectivity, moving a step forward the mere polarity recognition. In particular, the experiments concern expressions of emotion, as a finer-grained analysis of affect in text and a subseque</context>
</contexts>
<marker>Uspensky, 1973</marker>
<rawString>Uspensky B. 1973. A Poetics of Composition. University of California Press, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Wiebe</author>
</authors>
<title>Tracking point of view in narrative.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<pages>233--287</pages>
<contexts>
<context position="4112" citStr="Wiebe, 1994" startWordPosition="625" endWordPosition="626">Sentiment Analysis (SA). The main objective of this paper is to present EmotiBlog (Boldrini et al., 2009), a fine-grained annotation scheme for labeling subjectivity in the new textual genres. Subjectivity 1 http://technorati.com/ 1 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 1–10, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics can be reflected in text through expressions of emotions beliefs, views (a way of considering something) 2 and opinions, generally denominated “private states” (Uspensky, 1973), not open to verification (Wiebe, 1994). We performed a series of experiments focused on demonstrating that EmotiBlog represents a step forward to previous research in this field; its use allows a finer-grained and more precise learning of subjectivity expression models. Starting form (Wiebe, Wilson and Cardie, 2005) we created an annotation schema able to capture a wide range and key elements, which give subjectivity, moving a step forward the mere polarity recognition. In particular, the experiments concern expressions of emotion, as a finer-grained analysis of affect in text and a subsequent task to opinion mining (OM) and class</context>
</contexts>
<marker>Wiebe, 1994</marker>
<rawString>Wiebe J. M. 1994. Tracking point of view in narrative. Computational Linguistics, vol. 20, pp. 233– 287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation.</title>
<date>2005</date>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Wiebe J., Wilson T. and Cardie C. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses. In:</title>
<date>2004</date>
<booktitle>Proceedings of AAAI.</booktitle>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>Wilson T., Wiebe J., Hwa R. 2004. Just how mad are you? Finding strong and weak opinion clauses. In: Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotation Expressions of Opinions and Emotions in Language. Language Resources and Evaluation.</title>
<date>2005</date>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Wiebe J., Wilson T. and Cardie C. 2005. “Annotation Expressions of Opinions and Emotions in Language. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>E Riloff</author>
</authors>
<title>Creating Subjective and Objective Sentence Classifiers from Unannotated Texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing).</booktitle>
<contexts>
<context position="14782" citStr="Wiebe and Riloff, 2005" startWordPosition="2282" endWordPosition="2285"> and Zhou, 2006). Research in classification at a document level included sentiment classification of reviews (Ng, Dasgupta and Arifin, 2006), sentiment classification on customer feedback data (Gamon, Aue, Corston-Oliver, Ringger, 2005), comparative experiments (Cui, Mittal and Datar, 2006). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff, Wiebe, 2003), considering gradable adjectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns and then applying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opinion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orientation of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002). Finally, finegrained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002). All these a</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Wiebe J., Riloff E. 2005. Creating Subjective and Objective Sentence Classifiers from Unannotated Texts. In Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>