<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9996175">
A Recursive Recurrent Neural Network
for Statistical Machine Translation
</title>
<author confidence="0.998774">
Shujie Liu&apos;, Nan Yang&apos;, Mu Li&apos; and Ming Zhou&apos;
</author>
<affiliation confidence="0.995053">
&apos;Microsoft Research Asia, Beijing, China
&apos;University of Science and Technology of China, Hefei, China
</affiliation>
<email confidence="0.974117">
shujliu, v-nayang, muli, mingzhou@microsoft.com
</email>
<sectionHeader confidence="0.994346" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999770260869565">
In this paper, we propose a novel recursive
recurrent neural network (R2NN) to mod-
el the end-to-end decoding process for s-
tatistical machine translation. R2NN is a
combination of recursive neural network
and recurrent neural network, and in turn
integrates their respective capabilities: (1)
new information can be used to generate
the next hidden state, like recurrent neu-
ral networks, so that language model and
translation model can be integrated natu-
rally; (2) a tree structure can be built, as
recursive neural networks, so as to gener-
ate the translation candidates in a bottom
up manner. A semi-supervised training ap-
proach is proposed to train the parameter-
s, and the phrase pair embedding is ex-
plored to model translation confidence di-
rectly. Experiments on a Chinese to En-
glish translation task show that our pro-
posed R2NN can outperform the state-
of-the-art baseline by about 1.5 points in
BLEU.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999884636363637">
Deep Neural Network (DNN), which essential-
ly is a multi-layer neural network, has re-gained
more and more attentions these years. With the
efficient training methods, such as (Hinton et al.,
2006), DNN is widely applied to speech and im-
age processing, and has achieved breakthrough re-
sults (Kavukcuoglu et al., 2010; Krizhevsky et al.,
2012; Dahl et al., 2012).
Applying DNN to natural language processing
(NLP), representation or embedding of words is
usually learnt first. Word embedding is a dense,
low dimensional, real-valued vector. Each dimen-
sion of the vector represents a latent aspect of
the word, and captures its syntactic and semantic
properties (Bengio et al., 2006). Word embedding
is usually learnt from large amount of monolin-
gual corpus at first, and then fine tuned for spe-
cial distinct tasks. Collobert et al. (2011) propose
a multi-task learning framework with DNN for
various NLP tasks, including part-of-speech tag-
ging, chunking, named entity recognition, and se-
mantic role labelling. Recurrent neural networks
are leveraged to learn language model, and they
keep the history information circularly inside the
network for arbitrarily long time (Mikolov et al.,
2010). Recursive neural networks, which have the
ability to generate a tree structured output, are ap-
plied to natural language parsing (Socher et al.,
2011), and they are extended to recursive neural
tensor networks to explore the compositional as-
pect of semantics (Socher et al., 2013).
DNN is also introduced to Statistical Machine
Translation (SMT) to learn several components
or features of conventional framework, includ-
ing word alignment, language modelling, transla-
tion modelling and distortion modelling. Yang et
al. (2013) adapt and extend the CD-DNN-HMM
(Dahl et al., 2012) method to HMM-based word
alignment model. In their work, bilingual word
embedding is trained to capture lexical translation
information, and surrounding words are utilized to
model context information. Auli et al. (2013) pro-
pose a joint language and translation model, based
on a recurrent neural network. Their model pre-
dicts a target word, with an unbounded history of
both source and target words. Liu et al. (2013) pro-
pose an additive neural network for SMT decod-
ing. Word embedding is used as the input to learn
translation confidence score, which is combined
with commonly used features in the convention-
al log-linear model. For distortion modeling, Li
et al. (2013) use recursive auto encoders to make
full use of the entire merging phrase pairs, going
beyond the boundary words with a maximum en-
tropy classifier (Xiong et al., 2006).
</bodyText>
<page confidence="0.932907">
1491
</page>
<note confidence="0.8297115">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99988036">
Different from the work mentioned above,
which applies DNN to components of conven-
tional SMT framework, in this paper, we propose
a novel R2NN to model the end-to-end decod-
ing process. R2NN is a combination of recursive
neural network and recurrent neural network. In
R2NN, new information can be used to generate
the next hidden state, like recurrent neural net-
works, and a tree structure can be built, as recur-
sive neural networks. To generate the translation
candidates in a commonly used bottom-up man-
ner, recursive neural networks are naturally adopt-
ed to build the tree structure. In recursive neural
networks, all the representations of nodes are gen-
erated based on their child nodes, and it is difficult
to integrate additional global information, such as
language model and distortion model. In order to
integrate these crucial information for better trans-
lation prediction, we combine recurrent neural net-
works into the recursive neural networks, so that
we can use global information to generate the next
hidden state, and select the better translation can-
didate.
We propose a three-step semi-supervised train-
ing approach to optimizing the parameters of
R2NN, which includes recursive auto-encoding
for unsupervised pre-training, supervised local
training based on the derivation trees of forced de-
coding, and supervised global training using ear-
ly update strategy. So as to model the transla-
tion confidence for a translation phrase pair, we
initialize the phrase pair embedding by leveraging
the sparse features and recurrent neural network.
The sparse features are phrase pairs in translation
table, and recurrent neural network is utilized to
learn a smoothed translation score with the source
and target side information. We conduct exper-
iments on a Chinese-to-English translation task
to test our proposed methods, and we get about
1.5 BLEU points improvement, compared with a
state-of-the-art baseline system.
The rest of this paper is organized as follows:
Section 2 introduces related work on applying
DNN to SMT. Our R2NN framework is introduced
in detail in Section 3, followed by our three-step
semi-supervised training approach in Section 4.
Phrase pair embedding method using translation
confidence is elaborated in Section 5. We intro-
duce our conducted experiments in Section 6, and
conclude our work in Section 7.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999501">
Yang et al. (2013) adapt and extend CD-DNN-
HMM (Dahl et al., 2012) to word alignment.
In their work, initial word embedding is firstly
trained with a huge mono-lingual corpus, then the
word embedding is adapted and fine tuned bilin-
gually in a context-depended DNN HMM frame-
work. Word embeddings capturing lexical trans-
lation information and surrounding words model-
ing context information are leveraged to improve
the word alignment performance. Unfortunately,
the better word alignment result generated by this
model, cannot bring significant performance im-
provement on a end-to-end SMT evaluation task.
To improve the SMT performance directly, Auli
et al. (2013) extend the recurrent neural network
language model, in order to use both the source
and target side information to scoring translation
candidates. In their work, not only the target word
embedding is used as the input of the network, but
also the embedding of the source word, which is
aligned to the current target word. To tackle the
large search space due to the weak independence
assumption, a lattice algorithm is proposed to re-
rank the n-best translation candidates, generated
by a given SMT decoder.
Liu et al. (2013) propose an additive neural net-
work for SMT decoding. RNNLM (Mikolov et al.,
2010) is firstly used to generate the source and tar-
get word embeddings, which are fed into a one-
hidden-layer neural network to get a translation
confidence score. Together with other common-
ly used features, the translation confidence score
is integrated into a conventional log-linear model.
The parameters are optimized with developmen-
t data set using mini-batch conjugate sub-gradient
method and a regularized ranking loss.
DNN is also brought into the distortion mod-
eling. Going beyond the previous work using
boundary words for distortion modeling in BTG-
based SMT decoder, Li et al. (2013) propose to ap-
ply recursive auto-encoder to make full use of the
entire merged blocks. The recursive auto-encoder
is trained with reordering examples extracted from
word-aligned bilingual sentences. Given the rep-
resentations of the smaller phrase pairs, recursive
auto-encoder can generate the representation of
the parent phrase pair with a re-ordering confi-
dence score. The combination of reconstruction
error and re-ordering error is used to be the objec-
tive function for the model training.
</bodyText>
<page confidence="0.99699">
1492
</page>
<sectionHeader confidence="0.998578" genericHeader="method">
3 Our Model
</sectionHeader>
<bodyText confidence="0.999703533333333">
In this section, we leverage DNN to model the
end-to-end SMT decoding process, using a novel
recursive recurrent neural network (R2NN), which
is different from the above mentioned work ap-
plying DNN to components of conventional SMT
framework. R2NN is a combination of recur-
sive neural network and recurrent neural network,
which not only integrates the conventional glob-
al features as input information for each combina-
tion, but also generates the representation of the
parent node for the future candidate generation.
In this section, we briefly recall the recurren-
t neural network and recursive neural network in
Section 3.1 and 3.2, and then we elaborate our
R2NN in detail in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.999199">
3.1 Recurrent Neural Network
</subsectionHeader>
<bodyText confidence="0.998892214285714">
Recurrent neural network is usually used for
sequence processing, such as language model
(Mikolov et al., 2010). Commonly used sequence
processing methods, such as Hidden Markov
Model (HMM) and n-gram language model, only
use a limited history for the prediction. In HMM,
the previous state is used as the history, and for n-
gram language model (for example n equals to
3), the history is the previous two words. Recur-
rent neural network is proposed to use unbounded
history information, and it has recurrent connec-
tions on hidden states, so that history information
can be used circularly inside the network for arbi-
trarily long time.
</bodyText>
<figureCaption confidence="0.9984">
Figure 1: Recurrent neural network
</figureCaption>
<bodyText confidence="0.999980071428571">
As shown in Figure 1, the network contains
three layers, an input layer, a hidden layer, and an
output layer. The input layer is a concatenation of
ht−1 and xt, where ht−1 is a real-valued vec-
tor, which is the history information from time 0
to t − 1. xt is the embedding of the input word at
time t . Word embedding xt is integrated with
previous history ht−1 to generate the current hid-
den layer, which is a new history vector ht . Based
on ht , we can predict the probability of the next
word, which forms the output layer yt . The new
history ht is used for the future prediction, and
updated with new information from word embed-
ding xt recurrently.
</bodyText>
<subsectionHeader confidence="0.99961">
3.2 Recursive Neural Network
</subsectionHeader>
<bodyText confidence="0.999983833333333">
In addition to the sequential structure above, tree
structure is also usually constructed in various
NLP tasks, such as parsing and SMT decoding.
To generate a tree structure, recursive neural net-
works are introduced for natural language parsing
(Socher et al., 2011). Similar with recurrent neural
networks, recursive neural networks can also use
unbounded history information from the sub-tree
rooted at the current node. The commonly used
binary recursive neural networks generate the rep-
resentation of the parent node, with the represen-
tations of two child nodes as the input.
</bodyText>
<figureCaption confidence="0.99727">
Figure 2: Recursive neural network
</figureCaption>
<bodyText confidence="0.999954363636364">
As shown in Figure 2, s[l,m] and s[m,n] are
the representations of the child nodes, and they are
concatenated into one vector to be the input of the
network. s[l,n] is the generated representation of
the parent node. y[l,n] is the confidence score of
how plausible the parent node should be created.
l, m, n are the indexes of the string. For example,
for nature language parsing, s[l,n] is the represen-
tation of the parent node, which could be a NP or
V P node, and it is also the representation of the
whole sub-tree covering from l to n .
</bodyText>
<subsectionHeader confidence="0.997342">
3.3 Recursive Recurrent Neural Network
</subsectionHeader>
<bodyText confidence="0.999964571428571">
Word embedding xt is integrated as new input
information in recurrent neural networks for each
prediction, but in recursive neural networks, no ad-
ditional input information is used except the two
representation vectors of the child nodes. How-
ever, some global information , which cannot be
generated by the child representations, is crucial
</bodyText>
<figure confidence="0.979958727272727">
Yt
ht
ht_1
U
V
W
xt
𝑦[I,n]
S [I,n]
𝑊
S [I,m] S [m,n]
</figure>
<page confidence="0.963987">
1493
</page>
<bodyText confidence="0.999645142857143">
for SMT performance, such as language model s-
core and distortion model score. So as to integrate
such global information, and also keep the ability
to generate tree structure, we combine the recur-
rent neural network and the recursive neural net-
work to be a recursive recurrent neural network
(R2NN).
</bodyText>
<figureCaption confidence="0.995901">
Figure 3: Recursive recurrent neural network
</figureCaption>
<bodyText confidence="0.985651575757576">
As shown in Figure 3, based on the recursive
network, we add three input vectors x[l,m] for
child node [l, m] , x[m,n] for child node [m, n] ,
and x[l,n] for parent node [l, n] . We call them
recurrent input vectors, since they are borrowed
from recurrent neural networks. The two recurrent
input vectors x[l,m] and x[m,n] are concatenat-
ed as the input of the network, with the original
child node representations s[l,m] and s[m,n] . The
recurrent input vector x[l,n] is concatenated with
parent node representation s[l,n] to compute the
confidence score y[l,n] .
The input, hidden and output layers are calcu-
lated as follows:
he eluosi de”, we first split it into phrases “laiz-
i”, “faguo he eluosi” and “de”. We then check
whether translation candidates can be found in the
translation table for each span, together with the
phrase pair embedding and recurrent input vec-
tor (global features). We call it the rule match-
ing phase. For a translation candidate of the s-
pan node [l, m] , the black dots stand for the node
representation s[l,m] , while the grey dots for re-
current input vector x[l,m] . Given s[l,m] and
x[l,m] for matched translation candidates, conven-
tional CKY decoding process is performed using
R2NN. R2NN can combine the translation pairs
of child nodes, and generate the translation can-
didates for parent nodes with their representations
and plausible scores. Only the n-best translation
candidates are kept for upper combination, accord-
ing to their plausible scores.
coming from France and Russia
</bodyText>
<figure confidence="0.998869529411765">
𝑥[𝑙,𝑛]
𝑉
𝑠[𝑙,𝑛]
𝑦[𝑙,𝑛]
𝑊
𝑥[𝑙,𝑚] 𝑠[𝑙,𝑚] 𝑠[𝑚,𝑛] 𝑥[𝑚,𝑛]
*P
laizi
n
de
a[A *fl WNW
faguo he eluosi
R2NN
coming from France and Russia
R2NN
coming from France and Russia NULL
Rule Match Rule Match Rule Match
</figure>
<figureCaption confidence="0.999865">
Figure 4: R2NN for SMT decoding
</figureCaption>
<equation confidence="0.985228571428571">
ˆx[l,n] = x[l,m] ./ s[l,m] ./ x[m,n] ./ s[m,n] (1)
X ˆx[l,n]
sj = f( i wji) (2)
[l,n]
i
y[l,n] = X (s[l,n] ./ x[l,n])jvj (3)
j
</equation>
<bodyText confidence="0.99964675">
where ./ is a concatenation operator in Equation
1 and Equation 3, and f is a non-linear function,
here we use HTanh function, which is defined
as:
</bodyText>
<equation confidence="0.994495">
−1, x &lt; −1
x, −1 &lt; x &gt; 1 (4)
1, x &gt; 1
</equation>
<bodyText confidence="0.9981462">
Figure 4 illustrates the R2NN architecture for
SMT decoding. For a source sentence “laizi faguo
We extract phrase pairs using the conventional
method (Och and Ney, 2004). The commonly used
features, such as translation score, language mod-
el score and distortion score, are used as the recur-
rent input vector x . During decoding, recurrent
input vectors x for internal nodes are calculat-
ed accordingly. The difference between our model
and the conventional log-linear model includes:
</bodyText>
<listItem confidence="0.993955333333333">
• R2NN is not linear, while the conventional
model is a linear combination.
• Representations of phrase pairs are automat-
ically learnt to optimize the translation per-
formance, while features used in convention-
al model are hand-crafted.
• History information of the derivation can be
recorded in the representation of internal n-
odes, while conventional model cannot.
</listItem>
<equation confidence="0.63788275">
HTanh(x) =
⎧
⎨⎪
⎪⎩
</equation>
<page confidence="0.952912">
1494
</page>
<bodyText confidence="0.997457">
Liu et al. (2013) apply DNN to SMT decoding,
but not in a recursive manner. A feature is learn-
t via a one-hidden-layer neural network, and the
embedding of words in the phrase pairs are used
as the input vector. Our model generates the rep-
resentation of a translation pair based on its child
nodes. Li et al. (2013) also generate the repre-
sentation of phrase pairs in a recursive way. In
their work, the representation is optimized to learn
a distortion model using recursive neural network,
only based on the representation of the child n-
odes. Our R2NN is used to model the end-to-end
translation process, with recurrent global informa-
tion added. We also explore phrase pair embed-
ding method to model translation confidence di-
rectly, which is introduced in Section 5.
In the next two sections, we will answer the fol-
lowing questions: (a) how to train the model, and
(b) how to generate the initial representations of
translation pairs.
</bodyText>
<sectionHeader confidence="0.969426" genericHeader="method">
4 Model Training
</sectionHeader>
<bodyText confidence="0.999964714285714">
In this section, we propose a three-step training
method to train the parameters of our proposed
R2NN, which includes unsupervised pre-training
using recursive auto-encoding, supervised local
training on the derivation tree of forced decoding,
and supervised global training using early update
training strategy.
</bodyText>
<subsectionHeader confidence="0.94084">
4.1 Unsupervised Pre-training
</subsectionHeader>
<bodyText confidence="0.999995">
We adopt the Recursive Auto Encoding (RAE)
(Socher et al., 2011) for our unsupervised pre-
training. The main idea of auto encoding is to
initialize the parameters of the neural network,
by minimizing the information lost, which means,
capturing as much information as possible in the
hidden states from the input vector.
As shown in Figure 5, RAE contains two part-
s, an encoder with parameter W , and a decoder
with parameter W&apos; . Given the representations of
child nodes s1 and s2 , the encoder generates the
representation of parent node s . With the parent
node representation s as the input vector, the de-
coder reconstructs the representation of two child
nodes s&apos;1 and s&apos;2 . The loss function is defined as
following so as to minimize the information lost:
</bodyText>
<equation confidence="0.9298705">
1 LRAE(s1,s2) = 2(I1s1 − s1I2 + IIs2 − s2I2)
(5)
</equation>
<bodyText confidence="0.864239">
where 11-11 is the Euclidean norm.
</bodyText>
<table confidence="0.78841">
coming from France and Russia
SiSy�
W′
coming from France and Russia
coming from France and Russia
S1 Sy
*0 919 *n AtoA
laizi faguo he eluosi
</table>
<figureCaption confidence="0.959542">
Figure 5: Recursive auto encoding for unsuper-
vised pre-training
</figureCaption>
<bodyText confidence="0.996049666666666">
The training samples for RAE are phrase pairs
{s1, s21 in translation table, where s1 and
s2 can form a continuous partial sentence pair in
the training data. When RAE training is done, on-
ly the encoding model W will be fine tuned in
the future training phases.
</bodyText>
<subsectionHeader confidence="0.983402">
4.2 Supervised Local Training
</subsectionHeader>
<bodyText confidence="0.9999275">
We use contrastive divergence method to fine tune
the parameters W and V . The loss function
is the commonly used ranking loss with a margin,
and it is defined as follows:
</bodyText>
<equation confidence="0.99382175">
LSLT(W, V, s[l,n]) = max(0,1 − y[l,n]
oracle + y[l,n]
t )
(6)
</equation>
<bodyText confidence="0.988784956521739">
where s[l,n] is the source span. y[l,n]
oracle is
the plausible score of a oracle translation result.
yt is the plausible score for the best transla-
[l,n]
tion candidate given the model parameters W and
V . The loss function aims to learn a model which
assigns the good translation candidate (the oracle
candidate) higher score than the bad ones, with a
margin 1.
Translation candidates generated by forced de-
coding (Wuebker et al., 2010) are used as ora-
cle translations, which are the positive samples.
Forced decoding performs sentence pair segmen-
tation using the same translation system as decod-
ing. For each sentence pair in the training data,
SMT decoder is applied to the source side, and
any candidate which is not the partial sub-string
of the target sentence is removed from the n-best
list during decoding. From the forced decoding
result, we can get the ideal derivation tree in the
decoder’s search space, and extract positive/oracle
translation candidates.
</bodyText>
<figure confidence="0.644765">
S
W
</figure>
<page confidence="0.953491">
1495
</page>
<subsectionHeader confidence="0.991276">
4.3 Supervised Global Training
</subsectionHeader>
<bodyText confidence="0.999974148148148">
The supervised local training uses the n-
odes/samples in the derivation tree of forced de-
coding to update the model, and the trained model
tends to over-fit to local decisions. In this subsec-
tion, a supervised global training is proposed to
tune the model according to the final translation
performance of the whole source sentence.
Actually, we can update the model from the root
of the decoding tree and perform back propaga-
tion along the tree structure. Due to the inexac-
t search nature of SMT decoding, search errors
may inevitably break theoretical properties, and
the final translation results may be not suitable
for model training. To handle this problem, we
use early update strategy for the supervised glob-
al training. Early update is testified to be useful
for SMT training with large scale features (Yu et
al., 2013). Instead of updating the model using
the final translation results, early update approach
optimizes the model, when the oracle translation
candidate is pruned from the n-best list, meaning
that, the model is updated once it performs a unre-
coverable mistake. Back propagation is performed
along the tree structure, and the phrase pair em-
beddings of the leaf nodess are updated.
The loss function for supervised global training
is defined as follows:
</bodyText>
<equation confidence="0.803161166666667">
� [l,n]
[l,n]
exp (yoracle) )
[l,n] )
Et∈nbest exp (W
(7)
</equation>
<bodyText confidence="0.999929615384615">
where y[lra�e is the model score of a oracle trans-
lation candidate for the span [l, n] . Oracle transla-
tion candidates are candidates get from forced de-
coding. If the span [l, n] is not the whole source
sentence, there may be several oracle translation
candidates, otherwise, there is only one, which is
exactly the target sentence. There are much few-
er training samples than those for supervised local
training, and it is not suitable to use ranking loss
for global training any longer. We use negative
log-likelihood to penalize all the other translation
candidates except the oracle ones, so as to leverage
all the translation candidates as training samples.
</bodyText>
<equation confidence="0.992599285714286">
S
,
=−
L
GT (W, V, s[l
n])
log( yoracle
</equation>
<bodyText confidence="0.854501666666667">
mono-lingual
ngual corpus is much more difficult
to acquire, compared with monolingual corpus.
</bodyText>
<table confidence="0.989041571428571">
Embedding #Data #Entry #Parameter
Word Pair 7M Word 1G 500K 20 x
Phrase Pair 7M (500K)2 20 x
(500K)4 20 x
500K
(500K)2
(500K)4
</table>
<bodyText confidence="0.965470916666667">
words, but bili
bits, and we may have
terms. For each ter-
m, we have a vector with length 20 as parameters,
so there are 20 x
parameters totally. But
for source-target word pair, we may only have 7M
bilingual corpus for training (taking IWSLT data
set as an example), and there are 20
parameters to be tuned. For phrase pairs, the sit-
uation becomes even worse, especially when the
limitation of word count in phrase pairs is relaxed.
It is very difficult to learn the phrase pair embed-
ding brute-forcedly as word embedding is learnt
(Mikolov et al., 2010; Collobert et al., 2011), s-
ince we may not have enough training data.
A simple approach to construct phrase pair em-
bedding is to use the average of the embeddings
of the words in the phrase pair. One problem is
that, word embedding may not be able to mod-
el the translation relationship between source and
target phrases at phrase level, since some phrases
cannot be decomposed. For example, the meaning
of
is not the composition of the mean-
ings of the words
and
In this section,
we split the phrase pair embedding into two parts
to model the translation confidence directly: trans-
lation confidence with sparse features and trans-
lation confidence with recurrent neural network.
We first get two translation confidence vectors sep-
arately using sparse features and recurrent neu-
ral network, and then concatenate them to be the
phrase pair embedding. We call it tran
</bodyText>
<equation confidence="0.934597">
500K
500K
x(500K)2
”hotdog”
”hot”
”dog”.
slation con-
</equation>
<bodyText confidence="0.978773">
fidence based phrase pair embedding (TCBPPE).
</bodyText>
<sectionHeader confidence="0.942682" genericHeader="method">
5 Phrase Pair Embedding
</sectionHeader>
<bodyText confidence="0.99620725">
The next question is how to initialize the phrase
pair embedding in the translation table, so as to
generate the leaf nodes of the derivation tree.
There are more phrase pairs than
</bodyText>
<tableCaption confidence="0.974173125">
Table 1: The relationship between the size of train-
ing data and the number of model parameters. The
numbers for word embedding is calculated on En-
glish Giga-Word corpus version 3. For word pair
and phrase pair embedding, the numbers are cal-
culated on IWSLT 2009 dialog tr
aining set. The
word count of each side of phrase pairs is limited
</tableCaption>
<bodyText confidence="0.45314175">
to be 2.
Table 1 shows the relationship between the size
of training data and the number of model parame-
ters. For word embedding, the training size is 1G
</bodyText>
<page confidence="0.966921">
1496
</page>
<subsectionHeader confidence="0.926168">
5.1 Translation Confidence with Sparse
Features
</subsectionHeader>
<bodyText confidence="0.999973473684211">
Large scale feature training has drawn more at-
tentions these years (Liang et al., 2006; Yu et al.,
2013). Instead of integrating the sparse features
directly into the log-linear model, we use them as
the input to learn a phrase pair embedding. For
the top 200,000 frequent translation pairs, each of
them is a feature in itself, and a special feature is
added for all the infrequent ones.
The one-hot representation vector is used as the
input, and a one-hidden-layer network generates
a confidence score. To train the neural network,
we add the confidence scores to the convention-
al log-linear model as features. Forced decoding
is utilized to get positive samples, and contrastive
divergence is used for model training. The neu-
ral network is used to reduce the space dimension
of sparse features, and the hidden layer of the net-
work is used as the phrase pair embedding. The
length of the hidden layer is empirically set to 20.
</bodyText>
<subsectionHeader confidence="0.8374775">
5.2 Translation Confidence with Recurrent
Neural Network
</subsectionHeader>
<figureCaption confidence="0.692426">
Figure 6: Recurrent neural network for translation
confidence
</figureCaption>
<bodyText confidence="0.999962">
We use recurrent neural network to generate two
smoothed translation confidence scores based on
source and target word embeddings. One is source
to target translation confidence score and the other
is target to source. These two confidence scores
are defined as:
</bodyText>
<equation confidence="0.932639">
TS2T (s, t) = � log p(ei|ei−1, fai, hi) (8)
i
TT2S(s, t) = � log p(fj|fj−1, eˆar, hj) (9)
j
</equation>
<bodyText confidence="0.997720333333333">
where, fai is the corresponding target word
aligned to ei , and it is similar for eˆar .
p(ei|ei−1, fai, hi) is produced by a recurrent net-
work as shown in Figure 6. The recurrent neural
network is trained with word aligned bilingual cor-
pus, similar as (Auli et al., 2013).
</bodyText>
<sectionHeader confidence="0.976777" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999940142857143">
In this section, we conduct experiments to test our
method on a Chinese-to-English translation task.
The evaluation method is the case insensitive IB-
M BLEU-4 (Papineni et al., 2002). Significant
testing is carried out using bootstrap re-sampling
method proposed by (Koehn, 2004) with a 95%
confidence level.
</bodyText>
<subsectionHeader confidence="0.999832">
6.1 Data Setting and Baseline
</subsectionHeader>
<bodyText confidence="0.999970193548387">
The data is from the IWSLT 2009 dialog task.
The training data includes the BTEC and SLDB
training data. The training data contains 81k sen-
tence pairs, 655K Chinese words and 806K En-
glish words. The language model is a 5-gram lan-
guage model trained with the target sentences in
the training data. The test set is development set
9, and the development set comprises both devel-
opment set 8 and the Chinese DIALOG set.
The training data for monolingual word embed-
ding is Giga-Word corpus version 3 for both Chi-
nese and English. Chinese training corpus con-
tains 32M sentences and 1.1G words. English
training data contains 8M sentences and 247M
terms. We only train the embedding for the top
100,000 frequent words following (Collobert et
al., 2011). With the trained monolingual word em-
bedding, we follow (Yang et al., 2013) to get the
bilingual word embedding using the IWSLT bilin-
gual training data.
Our baseline decoder is an in-house implemen-
tation of Bracketing Transduction Grammar (BT-
G) (Wu, 1997) in CKY-style decoding with a lex-
ical reordering model trained with maximum en-
tropy (Xiong et al., 2006). The features of the
baseline are commonly used features as standard
BTG decoder, such as translation probabilities,
lexical weights, language model, word penalty and
distortion probabilities. All these commonly used
features are used as recurrent input vector x in
our R2NN.
</bodyText>
<subsectionHeader confidence="0.99973">
6.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.976618">
As we mentioned in Section 5, constructing phrase
pair embeddings from word embeddings may be
not suitable. Here we conduct experiments to ver-
</bodyText>
<figure confidence="0.987138">
fai
hL_1
ei_1
p(ei)
Wf
hi
U
V
We
</figure>
<page confidence="0.990969">
1497
</page>
<bodyText confidence="0.999775125">
ify it. We first train the source and target word em-
beddings separately using large monolingual data,
following (Collobert et al., 2011). Using monolin-
gual word embedding as the initialization, we fine
tune them to get bilingual word embedding (Yang
et al., 2013).
The word embedding based phrase pair embed-
ding (WEPPE) is defined as:
</bodyText>
<equation confidence="0.967889">
Eppweb(s, t) = E EEwms(si) ./ Ewbs(sj)
i j
./
E Ewmt(tk) ./ E Ewbt(tl) (10)
l
k
</equation>
<bodyText confidence="0.999336866666667">
where ./ is a concatenation operator. s and
t are the source and target phrases. Ewms(si) and
Ewmt(tk) are the monolingual word embeddings,
and Ewbs(si) and Ewbt(tk) are the bilingual
word embeddings. Here the length of the word
embedding is also set to 20. Therefore, the length
of the phrase pair embedding is 20 x 4 = 80 .
We compare our phrase pair embedding meth-
ods and our proposed R2NN with baseline system,
in Table 2. We can see that, our R2NN models
with WEPPE and TCBPPE are both better than the
baseline system. WEPPE cannot get significan-
t improvement, while TCBPPE does, compared
with the baseline result. TCBPPE is much better
than WEPPE.
</bodyText>
<table confidence="0.2455905">
Setting Development Test
Baseline 46.81 39.29
WEPPE+R2NN 47.23 39.92
TCBPPE+R2NN 48.70 T 40.81 T
</table>
<tableCaption confidence="0.628354">
Table 2: Translation results of our proposed R2NN
</tableCaption>
<bodyText confidence="0.999602555555556">
Model with two phrase embedding methods, com-
pared with the baseline. Setting ”WEPPE+R2NN”
is the result with word embedding based phrase
pair embedding and our R2NN Model, and
”TCBPPE+R2NN” is the result of translation con-
fidence based phrase pair embedding and our
R2NN Model. The results with T are significantly
better than the baseline.
Word embedding can model translation rela-
tionship at word level, but it may not be power-
ful to model the phrase pair respondents at phrasal
level, since the meaning of some phrases cannot
be decomposed into the meaning of words. And
also, translation task is difference from other NLP
tasks, that, it is more important to model the trans-
lation confidence directly (the confidence of one
target phrase as a translation of the source phrase),
and our TCBPPE is designed for such purpose.
</bodyText>
<subsectionHeader confidence="0.998654">
6.3 Effects of Global Recurrent Input Vector
</subsectionHeader>
<bodyText confidence="0.999558333333333">
In order to compare R2NN with recursive network
for SMT decoding, we remove the recurrent input
vector in R2NN to test its effect, and the results
are shown in Table 3. Without the recurrent input
vectors, R2NN degenerates into recursive neural
network (RNN).
</bodyText>
<table confidence="0.4811804">
Setting Development Test
WEPPE+R2NN 47.23 40.81
WEPPE+RNN 37.62 33.29
TCBPPE+R2NN 48.70 40.81
TCBPPE+RNN 45.11 37.33
</table>
<tableCaption confidence="0.966232">
Table 3: Experimental results to test the effects of
</tableCaption>
<bodyText confidence="0.9929293125">
recurrent input vector. WEPPE /TCBPPE+RNN
are the results removing recurrent input vectors
with WEPPE /TCBPPE.
From Table 3 we can find that, the recurren-
t input vector is essential to SMT performance.
When we remove it from R2NN, WEPPE based
method drops about 10 BLEU points on devel-
opment data and more than 6 BLEU points on
test data. TCBPPE based method drops about 3
BLEU points on both development and test data
sets. When we remove the recurrent input vectors,
the representations of recursive network are gener-
ated with the child nodes, and it does not integrate
global information, such as language model and
distortion model, which are crucial to the perfor-
mance of SMT.
</bodyText>
<subsectionHeader confidence="0.993142">
6.4 Sparse Features and Recurrent Network
Features
</subsectionHeader>
<bodyText confidence="0.999347166666667">
To test the contributions of sparse features and re-
current network features, we first remove all the
recurrent network features to train and test our
R2NN model, and then remove all the sparse fea-
tures to test the contribution of recurrent network
features.
</bodyText>
<figure confidence="0.468434">
Setting Development Test
TCBPPE+R2NN 48.70 40.81
SF+R2NN 48.23 40.19
RNN+R2NN 47.89 40.01
</figure>
<tableCaption confidence="0.9567195">
Table 4: Experimental results to test the effects of
sparse features and recurrent network features.
</tableCaption>
<page confidence="0.990899">
1498
</page>
<bodyText confidence="0.9998965">
The results are shown in Table 6.4. From the
results, we can find that, sparse features are more
effective than the recurrent network features a lit-
tle bit. The sparse features can directly model the
translation correspondence, and they may be more
effective to rank the translation candidates, while
recurrent neural network features are smoothed
lexical translation confidence.
</bodyText>
<sectionHeader confidence="0.991042" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999970136363636">
In this paper, we propose a Recursive Recur-
rent Neural Network(R2NN) to combine the re-
current neural network and recursive neural net-
work. Our proposed R2NN cannot only inte-
grate global input information during each com-
bination, but also can generate the tree struc-
ture in a recursive way. We apply our model to
SMT decoding, and propose a three-step semi-
supervised training method. In addition, we ex-
plore phrase pair embedding method, which mod-
els translation confidence directly. We conduc-
t experiments on a Chinese-to-English translation
task, and our method outperforms a state-of-the-
art baseline about 1.5 points BLEU.
From the experiments, we find that, phrase pair
embedding is crucial to the performance of SMT.
In the future, we will explore better methods for
phrase pair embedding to model the translation e-
quivalent between source and target phrases. We
will apply our proposed R2NN to other tree struc-
ture learning tasks, such as natural language pars-
ing.
</bodyText>
<sectionHeader confidence="0.998974" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999800486111111">
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044–
1054, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. Inno-
vations in Machine Learning, pages 137–186.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
George E Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
Audio, Speech, and Language Processing, IEEE
Transactions on, 20(1):30–42.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep be-
lief nets. Neural computation, 18(7):1527–1554.
Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,
Karol Gregor, Micha¨el Mathieu, and Yann LeCun.
2010. Learning convolutional feature hierarchies for
visual recognition. Advances in Neural Information
Processing Systems, pages 1090–1098.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 388–395.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Infor-
mation Processing Systems 25, pages 1106–1114.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recur-
sive autoencoders for ITG-based translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 567–
577, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 761–768. Association for Computational Lin-
guistics.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and
Tiejun Zhao. 2013. Additive neural networks for s-
tatistical machine translation. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 791–801, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of the Annual Conference of Internation-
al Speech Communication Association, pages 1045–
1048.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational linguistics, 30(4):417–449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.
</reference>
<page confidence="0.931885">
1499
</page>
<reference confidence="0.999856378378378">
Richard Socher, Cliff C Lin, Andrew Y Ng, and
Christopher D Manning. 2011. Parsing natural
scenes and natural language with recursive neural
networks. In Proceedings of the 26th Internation-
al Conference on Machine Learning (ICML), vol-
ume 2, page 7.
Richard Socher, John Bauer, and Christopher D Man-
ning. 2013. Parsing with compositional vector
grammars. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistic-
s, volume 1, pages 455–465.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 475–484. Association for Computa-
tional Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for s-
tatistical machine translation. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, volume 44, page 521.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai
Yu. 2013. Word alignment modeling with contex-
t dependent deep neural network. In 51st Annual
Meeting of the Association for Computational Lin-
guistics.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1112–1123, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.98284">
1500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.350288">
<title confidence="0.999778">A Recursive Recurrent Neural for Statistical Machine Translation</title>
<author confidence="0.997294">Nan Mu Ming</author>
<affiliation confidence="0.922868">Research Asia, Beijing,</affiliation>
<address confidence="0.606003">of Science and Technology of China, Hefei,</address>
<email confidence="0.999457">shujliu,v-nayang,muli,mingzhou@microsoft.com</email>
<abstract confidence="0.983821958333333">In this paper, we propose a novel recursive neural network to model the end-to-end decoding process for smachine translation. is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to English translation task show that our procan outperform the stateof-the-art baseline by about 1.5 points in BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="3186" citStr="Auli et al. (2013)" startWordPosition="490" endWordPosition="493">ursive neural tensor networks to explore the compositional aspect of semantics (Socher et al., 2013). DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier </context>
<context position="7028" citStr="Auli et al. (2013)" startWordPosition="1093" endWordPosition="1096">M (Dahl et al., 2012) to word alignment. In their work, initial word embedding is firstly trained with a huge mono-lingual corpus, then the word embedding is adapted and fine tuned bilingually in a context-depended DNN HMM framework. Word embeddings capturing lexical translation information and surrounding words modeling context information are leveraged to improve the word alignment performance. Unfortunately, the better word alignment result generated by this model, cannot bring significant performance improvement on a end-to-end SMT evaluation task. To improve the SMT performance directly, Auli et al. (2013) extend the recurrent neural network language model, in order to use both the source and target side information to scoring translation candidates. In their work, not only the target word embedding is used as the input of the network, but also the embedding of the source word, which is aligned to the current target word. To tackle the large search space due to the weak independence assumption, a lattice algorithm is proposed to rerank the n-best translation candidates, generated by a given SMT decoder. Liu et al. (2013) propose an additive neural network for SMT decoding. RNNLM (Mikolov et al.</context>
<context position="25773" citStr="Auli et al., 2013" startWordPosition="4281" endWordPosition="4284">current neural network to generate two smoothed translation confidence scores based on source and target word embeddings. One is source to target translation confidence score and the other is target to source. These two confidence scores are defined as: TS2T (s, t) = � log p(ei|ei−1, fai, hi) (8) i TT2S(s, t) = � log p(fj|fj−1, eˆar, hj) (9) j where, fai is the corresponding target word aligned to ei , and it is similar for eˆar . p(ei|ei−1, fai, hi) is produced by a recurrent network as shown in Figure 6. The recurrent neural network is trained with word aligned bilingual corpus, similar as (Auli et al., 2013). 6 Experiments and Results In this section, we conduct experiments to test our method on a Chinese-to-English translation task. The evaluation method is the case insensitive IBM BLEU-4 (Papineni et al., 2002). Significant testing is carried out using bootstrap re-sampling method proposed by (Koehn, 2004) with a 95% confidence level. 6.1 Data Setting and Baseline The data is from the IWSLT 2009 dialog task. The training data includes the BTEC and SLDB training data. The training data contains 81k sentence pairs, 655K Chinese words and 806K English words. The language model is a 5-gram language</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044– 1054, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. Innovations in Machine Learning, pages 137–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2034" citStr="Collobert et al. (2011)" startWordPosition="319" endWordPosition="322">pplied to speech and image processing, and has achieved breakthrough results (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first. Word embedding is a dense, low dimensional, real-valued vector. Each dimension of the vector represents a latent aspect of the word, and captures its syntactic and semantic properties (Bengio et al., 2006). Word embedding is usually learnt from large amount of monolingual corpus at first, and then fine tuned for special distinct tasks. Collobert et al. (2011) propose a multi-task learning framework with DNN for various NLP tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic role labelling. Recurrent neural networks are leveraged to learn language model, and they keep the history information circularly inside the network for arbitrarily long time (Mikolov et al., 2010). Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al., 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect </context>
<context position="22409" citStr="Collobert et al., 2011" startWordPosition="3704" endWordPosition="3707">00K)2 20 x (500K)4 20 x 500K (500K)2 (500K)4 words, but bili bits, and we may have terms. For each term, we have a vector with length 20 as parameters, so there are 20 x parameters totally. But for source-target word pair, we may only have 7M bilingual corpus for training (taking IWSLT data set as an example), and there are 20 parameters to be tuned. For phrase pairs, the situation becomes even worse, especially when the limitation of word count in phrase pairs is relaxed. It is very difficult to learn the phrase pair embedding brute-forcedly as word embedding is learnt (Mikolov et al., 2010; Collobert et al., 2011), since we may not have enough training data. A simple approach to construct phrase pair embedding is to use the average of the embeddings of the words in the phrase pair. One problem is that, word embedding may not be able to model the translation relationship between source and target phrases at phrase level, since some phrases cannot be decomposed. For example, the meaning of is not the composition of the meanings of the words and In this section, we split the phrase pair embedding into two parts to model the translation confidence directly: translation confidence with sparse features and t</context>
<context position="26885" citStr="Collobert et al., 2011" startWordPosition="4466" endWordPosition="4469">contains 81k sentence pairs, 655K Chinese words and 806K English words. The language model is a 5-gram language model trained with the target sentences in the training data. The test set is development set 9, and the development set comprises both development set 8 and the Chinese DIALOG set. The training data for monolingual word embedding is Giga-Word corpus version 3 for both Chinese and English. Chinese training corpus contains 32M sentences and 1.1G words. English training data contains 8M sentences and 247M terms. We only train the embedding for the top 100,000 frequent words following (Collobert et al., 2011). With the trained monolingual word embedding, we follow (Yang et al., 2013) to get the bilingual word embedding using the IWSLT bilingual training data. Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The features of the baseline are commonly used features as standard BTG decoder, such as translation probabilities, lexical weights, language model, word penalty and distortion probabilities. All these commonly used features are used as r</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Dong Yu</author>
<author>Li Deng</author>
<author>Alex Acero</author>
</authors>
<title>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing,</title>
<date>2012</date>
<journal>IEEE Transactions on,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1558" citStr="Dahl et al., 2012" startWordPosition="243" endWordPosition="246">ers, and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the stateof-the-art baseline by about 1.5 points in BLEU. 1 Introduction Deep Neural Network (DNN), which essentially is a multi-layer neural network, has re-gained more and more attentions these years. With the efficient training methods, such as (Hinton et al., 2006), DNN is widely applied to speech and image processing, and has achieved breakthrough results (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first. Word embedding is a dense, low dimensional, real-valued vector. Each dimension of the vector represents a latent aspect of the word, and captures its syntactic and semantic properties (Bengio et al., 2006). Word embedding is usually learnt from large amount of monolingual corpus at first, and then fine tuned for special distinct tasks. Collobert et al. (2011) propose a multi-task learning framework with DNN for various NLP tasks, including part-of-speech tagging, chunking, named e</context>
<context position="2965" citStr="Dahl et al., 2012" startWordPosition="458" endWordPosition="461">trarily long time (Mikolov et al., 2010). Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al., 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al., 2013). DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the</context>
<context position="6431" citStr="Dahl et al., 2012" startWordPosition="1004" endWordPosition="1007">o test our proposed methods, and we get about 1.5 BLEU points improvement, compared with a state-of-the-art baseline system. The rest of this paper is organized as follows: Section 2 introduces related work on applying DNN to SMT. Our R2NN framework is introduced in detail in Section 3, followed by our three-step semi-supervised training approach in Section 4. Phrase pair embedding method using translation confidence is elaborated in Section 5. We introduce our conducted experiments in Section 6, and conclude our work in Section 7. 2 Related Work Yang et al. (2013) adapt and extend CD-DNNHMM (Dahl et al., 2012) to word alignment. In their work, initial word embedding is firstly trained with a huge mono-lingual corpus, then the word embedding is adapted and fine tuned bilingually in a context-depended DNN HMM framework. Word embeddings capturing lexical translation information and surrounding words modeling context information are leveraged to improve the word alignment performance. Unfortunately, the better word alignment result generated by this model, cannot bring significant performance improvement on a end-to-end SMT evaluation task. To improve the SMT performance directly, Auli et al. (2013) ex</context>
</contexts>
<marker>Dahl, Yu, Deng, Acero, 2012</marker>
<rawString>George E Dahl, Dong Yu, Li Deng, and Alex Acero. 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Simon Osindero</author>
<author>Yee-Whye Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<booktitle>Neural computation,</booktitle>
<pages>18--7</pages>
<contexts>
<context position="1394" citStr="Hinton et al., 2006" startWordPosition="215" endWordPosition="218">as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the stateof-the-art baseline by about 1.5 points in BLEU. 1 Introduction Deep Neural Network (DNN), which essentially is a multi-layer neural network, has re-gained more and more attentions these years. With the efficient training methods, such as (Hinton et al., 2006), DNN is widely applied to speech and image processing, and has achieved breakthrough results (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first. Word embedding is a dense, low dimensional, real-valued vector. Each dimension of the vector represents a latent aspect of the word, and captures its syntactic and semantic properties (Bengio et al., 2006). Word embedding is usually learnt from large amount of monolingual corpus at first, and then fine tuned for special</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koray Kavukcuoglu</author>
<author>Pierre Sermanet</author>
<author>Y-Lan Boureau</author>
<author>Karol Gregor</author>
<author>Micha¨el Mathieu</author>
<author>Yann LeCun</author>
</authors>
<title>Learning convolutional feature hierarchies for visual recognition.</title>
<date>2010</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>1090--1098</pages>
<marker>Kavukcuoglu, Sermanet, Boureau, Gregor, Micha¨el Mathieu, LeCun, 2010</marker>
<rawString>Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, Karol Gregor, Micha¨el Mathieu, and Yann LeCun. 2010. Learning convolutional feature hierarchies for visual recognition. Advances in Neural Information Processing Systems, pages 1090–1098.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="26079" citStr="Koehn, 2004" startWordPosition="4329" endWordPosition="4330"> � log p(fj|fj−1, eˆar, hj) (9) j where, fai is the corresponding target word aligned to ei , and it is similar for eˆar . p(ei|ei−1, fai, hi) is produced by a recurrent network as shown in Figure 6. The recurrent neural network is trained with word aligned bilingual corpus, similar as (Auli et al., 2013). 6 Experiments and Results In this section, we conduct experiments to test our method on a Chinese-to-English translation task. The evaluation method is the case insensitive IBM BLEU-4 (Papineni et al., 2002). Significant testing is carried out using bootstrap re-sampling method proposed by (Koehn, 2004) with a 95% confidence level. 6.1 Data Setting and Baseline The data is from the IWSLT 2009 dialog task. The training data includes the BTEC and SLDB training data. The training data contains 81k sentence pairs, 655K Chinese words and 806K English words. The language model is a 5-gram language model trained with the target sentences in the training data. The test set is development set 9, and the development set comprises both development set 8 and the Chinese DIALOG set. The training data for monolingual word embedding is Giga-Word corpus version 3 for both Chinese and English. Chinese traini</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoff Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems 25,</booktitle>
<pages>1106--1114</pages>
<contexts>
<context position="1538" citStr="Krizhevsky et al., 2012" startWordPosition="239" endWordPosition="242">osed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the stateof-the-art baseline by about 1.5 points in BLEU. 1 Introduction Deep Neural Network (DNN), which essentially is a multi-layer neural network, has re-gained more and more attentions these years. With the efficient training methods, such as (Hinton et al., 2006), DNN is widely applied to speech and image processing, and has achieved breakthrough results (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first. Word embedding is a dense, low dimensional, real-valued vector. Each dimension of the vector represents a latent aspect of the word, and captures its syntactic and semantic properties (Bengio et al., 2006). Word embedding is usually learnt from large amount of monolingual corpus at first, and then fine tuned for special distinct tasks. Collobert et al. (2011) propose a multi-task learning framework with DNN for various NLP tasks, including part-of-speech taggin</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
</authors>
<title>Recursive autoencoders for ITG-based translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>567--577</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="3638" citStr="Li et al. (2013)" startWordPosition="567" endWordPosition="570"> bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Different from the work mentioned above, which applies DNN to components of conventional SMT framework, in this paper, we propose a novel R2NN to model the end-to-end decoding process. R2NN is a combination of recursive neural </context>
<context position="8224" citStr="Li et al. (2013)" startWordPosition="1289" endWordPosition="1292">NNLM (Mikolov et al., 2010) is firstly used to generate the source and target word embeddings, which are fed into a onehidden-layer neural network to get a translation confidence score. Together with other commonly used features, the translation confidence score is integrated into a conventional log-linear model. The parameters are optimized with development data set using mini-batch conjugate sub-gradient method and a regularized ranking loss. DNN is also brought into the distortion modeling. Going beyond the previous work using boundary words for distortion modeling in BTGbased SMT decoder, Li et al. (2013) propose to apply recursive auto-encoder to make full use of the entire merged blocks. The recursive auto-encoder is trained with reordering examples extracted from word-aligned bilingual sentences. Given the representations of the smaller phrase pairs, recursive auto-encoder can generate the representation of the parent phrase pair with a re-ordering confidence score. The combination of reconstruction error and re-ordering error is used to be the objective function for the model training. 1492 3 Our Model In this section, we leverage DNN to model the end-to-end SMT decoding process, using a n</context>
<context position="16004" citStr="Li et al. (2013)" startWordPosition="2617" endWordPosition="2620">ions of phrase pairs are automatically learnt to optimize the translation performance, while features used in conventional model are hand-crafted. • History information of the derivation can be recorded in the representation of internal nodes, while conventional model cannot. HTanh(x) = ⎧ ⎨⎪ ⎪⎩ 1494 Liu et al. (2013) apply DNN to SMT decoding, but not in a recursive manner. A feature is learnt via a one-hidden-layer neural network, and the embedding of words in the phrase pairs are used as the input vector. Our model generates the representation of a translation pair based on its child nodes. Li et al. (2013) also generate the representation of phrase pairs in a recursive way. In their work, the representation is optimized to learn a distortion model using recursive neural network, only based on the representation of the child nodes. Our R2NN is used to model the end-to-end translation process, with recurrent global information added. We also explore phrase pair embedding method to model translation confidence directly, which is introduced in Section 5. In the next two sections, we will answer the following questions: (a) how to train the model, and (b) how to generate the initial representations </context>
</contexts>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for ITG-based translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567– 577, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>761--768</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 761–768. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lemao Liu</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Tiejun Zhao</author>
</authors>
<title>Additive neural networks for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>791--801</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3384" citStr="Liu et al. (2013)" startWordPosition="525" endWordPosition="528">atures of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for</context>
<context position="7553" citStr="Liu et al. (2013)" startWordPosition="1182" endWordPosition="1185"> end-to-end SMT evaluation task. To improve the SMT performance directly, Auli et al. (2013) extend the recurrent neural network language model, in order to use both the source and target side information to scoring translation candidates. In their work, not only the target word embedding is used as the input of the network, but also the embedding of the source word, which is aligned to the current target word. To tackle the large search space due to the weak independence assumption, a lattice algorithm is proposed to rerank the n-best translation candidates, generated by a given SMT decoder. Liu et al. (2013) propose an additive neural network for SMT decoding. RNNLM (Mikolov et al., 2010) is firstly used to generate the source and target word embeddings, which are fed into a onehidden-layer neural network to get a translation confidence score. Together with other commonly used features, the translation confidence score is integrated into a conventional log-linear model. The parameters are optimized with development data set using mini-batch conjugate sub-gradient method and a regularized ranking loss. DNN is also brought into the distortion modeling. Going beyond the previous work using boundary </context>
<context position="15706" citStr="Liu et al. (2013)" startWordPosition="2562" endWordPosition="2565">used as the recurrent input vector x . During decoding, recurrent input vectors x for internal nodes are calculated accordingly. The difference between our model and the conventional log-linear model includes: • R2NN is not linear, while the conventional model is a linear combination. • Representations of phrase pairs are automatically learnt to optimize the translation performance, while features used in conventional model are hand-crafted. • History information of the derivation can be recorded in the representation of internal nodes, while conventional model cannot. HTanh(x) = ⎧ ⎨⎪ ⎪⎩ 1494 Liu et al. (2013) apply DNN to SMT decoding, but not in a recursive manner. A feature is learnt via a one-hidden-layer neural network, and the embedding of words in the phrase pairs are used as the input vector. Our model generates the representation of a translation pair based on its child nodes. Li et al. (2013) also generate the representation of phrase pairs in a recursive way. In their work, the representation is optimized to learn a distortion model using recursive neural network, only based on the representation of the child nodes. Our R2NN is used to model the end-to-end translation process, with recur</context>
</contexts>
<marker>Liu, Watanabe, Sumita, Zhao, 2013</marker>
<rawString>Lemao Liu, Taro Watanabe, Eiichiro Sumita, and Tiejun Zhao. 2013. Additive neural networks for statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 791–801, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Conference of International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of the Annual Conference of International Speech Communication Association, pages 1045– 1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="14985" citStr="Och and Ney, 2004" startWordPosition="2446" endWordPosition="2449"> coming from France and Russia R2NN coming from France and Russia NULL Rule Match Rule Match Rule Match Figure 4: R2NN for SMT decoding ˆx[l,n] = x[l,m] ./ s[l,m] ./ x[m,n] ./ s[m,n] (1) X ˆx[l,n] sj = f( i wji) (2) [l,n] i y[l,n] = X (s[l,n] ./ x[l,n])jvj (3) j where ./ is a concatenation operator in Equation 1 and Equation 3, and f is a non-linear function, here we use HTanh function, which is defined as: −1, x &lt; −1 x, −1 &lt; x &gt; 1 (4) 1, x &gt; 1 Figure 4 illustrates the R2NN architecture for SMT decoding. For a source sentence “laizi faguo We extract phrase pairs using the conventional method (Och and Ney, 2004). The commonly used features, such as translation score, language model score and distortion score, are used as the recurrent input vector x . During decoding, recurrent input vectors x for internal nodes are calculated accordingly. The difference between our model and the conventional log-linear model includes: • R2NN is not linear, while the conventional model is a linear combination. • Representations of phrase pairs are automatically learnt to optimize the translation performance, while features used in conventional model are hand-crafted. • History information of the derivation can be rec</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25982" citStr="Papineni et al., 2002" startWordPosition="4314" endWordPosition="4317">rce. These two confidence scores are defined as: TS2T (s, t) = � log p(ei|ei−1, fai, hi) (8) i TT2S(s, t) = � log p(fj|fj−1, eˆar, hj) (9) j where, fai is the corresponding target word aligned to ei , and it is similar for eˆar . p(ei|ei−1, fai, hi) is produced by a recurrent network as shown in Figure 6. The recurrent neural network is trained with word aligned bilingual corpus, similar as (Auli et al., 2013). 6 Experiments and Results In this section, we conduct experiments to test our method on a Chinese-to-English translation task. The evaluation method is the case insensitive IBM BLEU-4 (Papineni et al., 2002). Significant testing is carried out using bootstrap re-sampling method proposed by (Koehn, 2004) with a 95% confidence level. 6.1 Data Setting and Baseline The data is from the IWSLT 2009 dialog task. The training data includes the BTEC and SLDB training data. The training data contains 81k sentence pairs, 655K Chinese words and 806K English words. The language model is a 5-gram language model trained with the target sentences in the training data. The test set is development set 9, and the development set comprises both development set 8 and the Chinese DIALOG set. The training data for mono</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning (ICML),</booktitle>
<volume>2</volume>
<pages>7</pages>
<contexts>
<context position="2538" citStr="Socher et al., 2011" startWordPosition="394" endWordPosition="397">large amount of monolingual corpus at first, and then fine tuned for special distinct tasks. Collobert et al. (2011) propose a multi-task learning framework with DNN for various NLP tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic role labelling. Recurrent neural networks are leveraged to learn language model, and they keep the history information circularly inside the network for arbitrarily long time (Mikolov et al., 2010). Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al., 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al., 2013). DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized </context>
<context position="11078" citStr="Socher et al., 2011" startWordPosition="1765" endWordPosition="1768">ted with previous history ht−1 to generate the current hidden layer, which is a new history vector ht . Based on ht , we can predict the probability of the next word, which forms the output layer yt . The new history ht is used for the future prediction, and updated with new information from word embedding xt recurrently. 3.2 Recursive Neural Network In addition to the sequential structure above, tree structure is also usually constructed in various NLP tasks, such as parsing and SMT decoding. To generate a tree structure, recursive neural networks are introduced for natural language parsing (Socher et al., 2011). Similar with recurrent neural networks, recursive neural networks can also use unbounded history information from the sub-tree rooted at the current node. The commonly used binary recursive neural networks generate the representation of the parent node, with the representations of two child nodes as the input. Figure 2: Recursive neural network As shown in Figure 2, s[l,m] and s[m,n] are the representations of the child nodes, and they are concatenated into one vector to be the input of the network. s[l,n] is the generated representation of the parent node. y[l,n] is the confidence score of </context>
<context position="17050" citStr="Socher et al., 2011" startWordPosition="2780" endWordPosition="2783">is introduced in Section 5. In the next two sections, we will answer the following questions: (a) how to train the model, and (b) how to generate the initial representations of translation pairs. 4 Model Training In this section, we propose a three-step training method to train the parameters of our proposed R2NN, which includes unsupervised pre-training using recursive auto-encoding, supervised local training on the derivation tree of forced decoding, and supervised global training using early update training strategy. 4.1 Unsupervised Pre-training We adopt the Recursive Auto Encoding (RAE) (Socher et al., 2011) for our unsupervised pretraining. The main idea of auto encoding is to initialize the parameters of the neural network, by minimizing the information lost, which means, capturing as much information as possible in the hidden states from the input vector. As shown in Figure 5, RAE contains two parts, an encoder with parameter W , and a decoder with parameter W&apos; . Given the representations of child nodes s1 and s2 , the encoder generates the representation of parent node s . With the parent node representation s as the input vector, the decoder reconstructs the representation of two child nodes</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Andrew Y Ng, and Christopher D Manning. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning (ICML), volume 2, page 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>455--465</pages>
<contexts>
<context position="2668" citStr="Socher et al., 2013" startWordPosition="415" endWordPosition="418">ti-task learning framework with DNN for various NLP tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic role labelling. Recurrent neural networks are leveraged to learn language model, and they keep the history information circularly inside the network for arbitrarily long time (Mikolov et al., 2010). Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al., 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al., 2013). DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural netwo</context>
</contexts>
<marker>Socher, Bauer, Manning, 2013</marker>
<rawString>Richard Socher, John Bauer, and Christopher D Manning. 2013. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, volume 1, pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="27141" citStr="Wu, 1997" startWordPosition="4510" endWordPosition="4511">the Chinese DIALOG set. The training data for monolingual word embedding is Giga-Word corpus version 3 for both Chinese and English. Chinese training corpus contains 32M sentences and 1.1G words. English training data contains 8M sentences and 247M terms. We only train the embedding for the top 100,000 frequent words following (Collobert et al., 2011). With the trained monolingual word embedding, we follow (Yang et al., 2013) to get the bilingual word embedding using the IWSLT bilingual training data. Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The features of the baseline are commonly used features as standard BTG decoder, such as translation probabilities, lexical weights, language model, word penalty and distortion probabilities. All these commonly used features are used as recurrent input vector x in our R2NN. 6.2 Translation Results As we mentioned in Section 5, constructing phrase pair embeddings from word embeddings may be not suitable. Here we conduct experiments to verfai hL_1 ei_1 p(ei) Wf hi U V We 1497 ify it. We firs</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Training phrase translation models with leaving-one-out.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>475--484</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19000" citStr="Wuebker et al., 2010" startWordPosition="3127" endWordPosition="3130">the parameters W and V . The loss function is the commonly used ranking loss with a margin, and it is defined as follows: LSLT(W, V, s[l,n]) = max(0,1 − y[l,n] oracle + y[l,n] t ) (6) where s[l,n] is the source span. y[l,n] oracle is the plausible score of a oracle translation result. yt is the plausible score for the best transla[l,n] tion candidate given the model parameters W and V . The loss function aims to learn a model which assigns the good translation candidate (the oracle candidate) higher score than the bad ones, with a margin 1. Translation candidates generated by forced decoding (Wuebker et al., 2010) are used as oracle translations, which are the positive samples. Forced decoding performs sentence pair segmentation using the same translation system as decoding. For each sentence pair in the training data, SMT decoder is applied to the source side, and any candidate which is not the partial sub-string of the target sentence is removed from the n-best list during decoding. From the forced decoding result, we can get the ideal derivation tree in the decoder’s search space, and extract positive/oracle translation candidates. S W 1495 4.3 Supervised Global Training The supervised local trainin</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Joern Wuebker, Arne Mauser, and Hermann Ney. 2010. Training phrase translation models with leaving-one-out. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 475–484. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>44</volume>
<pages>521</pages>
<contexts>
<context position="3806" citStr="Xiong et al., 2006" startWordPosition="596" endWordPosition="599">propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Different from the work mentioned above, which applies DNN to components of conventional SMT framework, in this paper, we propose a novel R2NN to model the end-to-end decoding process. R2NN is a combination of recursive neural network and recurrent neural network. In R2NN, new information can be used to generate the next hidden state, like recurrent neural networks, and a tree structure can b</context>
<context position="27245" citStr="Xiong et al., 2006" startWordPosition="4526" endWordPosition="4529">ersion 3 for both Chinese and English. Chinese training corpus contains 32M sentences and 1.1G words. English training data contains 8M sentences and 247M terms. We only train the embedding for the top 100,000 frequent words following (Collobert et al., 2011). With the trained monolingual word embedding, we follow (Yang et al., 2013) to get the bilingual word embedding using the IWSLT bilingual training data. Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The features of the baseline are commonly used features as standard BTG decoder, such as translation probabilities, lexical weights, language model, word penalty and distortion probabilities. All these commonly used features are used as recurrent input vector x in our R2NN. 6.2 Translation Results As we mentioned in Section 5, constructing phrase pair embeddings from word embeddings may be not suitable. Here we conduct experiments to verfai hL_1 ei_1 p(ei) Wf hi U V We 1497 ify it. We first train the source and target word embeddings separately using large monolingual data, following (Collob</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, volume 44, page 521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Nenghai Yu</author>
</authors>
<title>Word alignment modeling with context dependent deep neural network.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2913" citStr="Yang et al. (2013)" startWordPosition="449" endWordPosition="452">y information circularly inside the network for arbitrarily long time (Mikolov et al., 2010). Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al., 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al., 2013). DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, </context>
<context position="6384" citStr="Yang et al. (2013)" startWordPosition="995" endWordPosition="998">ents on a Chinese-to-English translation task to test our proposed methods, and we get about 1.5 BLEU points improvement, compared with a state-of-the-art baseline system. The rest of this paper is organized as follows: Section 2 introduces related work on applying DNN to SMT. Our R2NN framework is introduced in detail in Section 3, followed by our three-step semi-supervised training approach in Section 4. Phrase pair embedding method using translation confidence is elaborated in Section 5. We introduce our conducted experiments in Section 6, and conclude our work in Section 7. 2 Related Work Yang et al. (2013) adapt and extend CD-DNNHMM (Dahl et al., 2012) to word alignment. In their work, initial word embedding is firstly trained with a huge mono-lingual corpus, then the word embedding is adapted and fine tuned bilingually in a context-depended DNN HMM framework. Word embeddings capturing lexical translation information and surrounding words modeling context information are leveraged to improve the word alignment performance. Unfortunately, the better word alignment result generated by this model, cannot bring significant performance improvement on a end-to-end SMT evaluation task. To improve the </context>
<context position="26961" citStr="Yang et al., 2013" startWordPosition="4479" endWordPosition="4482">age model is a 5-gram language model trained with the target sentences in the training data. The test set is development set 9, and the development set comprises both development set 8 and the Chinese DIALOG set. The training data for monolingual word embedding is Giga-Word corpus version 3 for both Chinese and English. Chinese training corpus contains 32M sentences and 1.1G words. English training data contains 8M sentences and 247M terms. We only train the embedding for the top 100,000 frequent words following (Collobert et al., 2011). With the trained monolingual word embedding, we follow (Yang et al., 2013) to get the bilingual word embedding using the IWSLT bilingual training data. Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The features of the baseline are commonly used features as standard BTG decoder, such as translation probabilities, lexical weights, language model, word penalty and distortion probabilities. All these commonly used features are used as recurrent input vector x in our R2NN. 6.2 Translation Results As we mentioned</context>
</contexts>
<marker>Yang, Liu, Li, Zhou, Yu, 2013</marker>
<rawString>Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai Yu. 2013. Word alignment modeling with context dependent deep neural network. In 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Kai Zhao</author>
</authors>
<title>Max-violation perceptron and forced decoding for scalable MT training.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1112--1123</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="20399" citStr="Yu et al., 2013" startWordPosition="3358" endWordPosition="3361">obal training is proposed to tune the model according to the final translation performance of the whole source sentence. Actually, we can update the model from the root of the decoding tree and perform back propagation along the tree structure. Due to the inexact search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final translation results may be not suitable for model training. To handle this problem, we use early update strategy for the supervised global training. Early update is testified to be useful for SMT training with large scale features (Yu et al., 2013). Instead of updating the model using the final translation results, early update approach optimizes the model, when the oracle translation candidate is pruned from the n-best list, meaning that, the model is updated once it performs a unrecoverable mistake. Back propagation is performed along the tree structure, and the phrase pair embeddings of the leaf nodess are updated. The loss function for supervised global training is defined as follows: � [l,n] [l,n] exp (yoracle) ) [l,n] ) Et∈nbest exp (W (7) where y[lra�e is the model score of a oracle translation candidate for the span [l, n] . Ora</context>
<context position="24201" citStr="Yu et al., 2013" startWordPosition="4013" endWordPosition="4016">the size of training data and the number of model parameters. The numbers for word embedding is calculated on English Giga-Word corpus version 3. For word pair and phrase pair embedding, the numbers are calculated on IWSLT 2009 dialog tr aining set. The word count of each side of phrase pairs is limited to be 2. Table 1 shows the relationship between the size of training data and the number of model parameters. For word embedding, the training size is 1G 1496 5.1 Translation Confidence with Sparse Features Large scale feature training has drawn more attentions these years (Liang et al., 2006; Yu et al., 2013). Instead of integrating the sparse features directly into the log-linear model, we use them as the input to learn a phrase pair embedding. For the top 200,000 frequent translation pairs, each of them is a feature in itself, and a special feature is added for all the infrequent ones. The one-hot representation vector is used as the input, and a one-hidden-layer network generates a confidence score. To train the neural network, we add the confidence scores to the conventional log-linear model as features. Forced decoding is utilized to get positive samples, and contrastive divergence is used fo</context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao. 2013. Max-violation perceptron and forced decoding for scalable MT training. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1112–1123, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>