<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000028">
<title confidence="0.760706">
Active Learning Based Corpus Annotation
</title>
<author confidence="0.995282">
Hongyan Song1 and Tianfang Yao2
</author>
<affiliation confidence="0.876522666666667">
Shanghai Jiao Tong University
Department of Computer Science and Engineering
Shanghai, China 200240
</affiliation>
<email confidence="0.947361">
1songhongyan@sjtu.org
2yao-tf@cs.sjtu.edu.cn
</email>
<sectionHeader confidence="0.996988" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994276">
Opinion Mining aims to automatically acquire
useful opinioned information and knowledge
in subjective texts. Research of Chinese Opin-
ioned Mining requires the support of annotated
corpus for Chinese opinioned-subjective texts.
To facilitate the work of corpus annotators,
this paper implements an active learning based
annotation tool for Chinese opinioned ele-
ments which can identify topic, sentiment, and
opinion holder in a sentence automatically.
</bodyText>
<sectionHeader confidence="0.999389" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999831535211268">
Opinion Mining is a novel and important re-
search topic, aiming to automatically acquire
useful opinioned information and knowledge in
subjective texts (Liu et al, 2008). This technique
has wide and many real world applications, such
as e-commerce, business intelligence, informa-
tion monitoring, public opinion poll, e-learning,
newspaper and publication compilation, and
business management. For instance, a typical
opinion mining system produces statistical re-
sults from online product reviews, which can be
used by potential customers when deciding
which model to choose, by manufacturers to find
out the possible areas of improvement, and by
dealers for sales plan evaluation (Yao et al,
2008).
According to Kim and Hovy (2004), an opin-
ion is composed of four parts, namely, topic,
holder, sentiment, and claim, in which the holder
expresses the claim including positive or nega-
tive sentiment towards the topic. For example, in
the sentence I like this car, I is the holder, like is
the positive sentiment, car is the topic, and the
whole sentence is the claim.
Research on Chinese opinion mining technol-
ogy requires the support of annotated corpus for
Chinese opinioned-subjective text. Since the cor-
pus includes deep level information related to
word segmentation, part-of-speech, syntax, se-
mantics, opinioned elements, and some other
information, the finished annotation is very com-
plicated. Hence, it is necessary to develop an
automatic tool to facilitate the work of annotators
so that the efficiency and accuracy of annotation
can be improved.
When developing the automatic annotation tool,
we find it is most difficult for the tool to annotate
opinioned elements automatically. Because
unlike other elements such as part-of-speech, and
dependency relationship that needed to be anno-
tated in the corpus, there is no available tool that
can identify opinioned elements automatically.
Special classifiers should be constructed to solve
this problem.
In traditional supervised learning tasks, train-
ing process consumes all the available annotated
training instances, so a classifier with high classi-
fication accuracy might be constructed. When
training a classifier for opinioned elements, it is
very expensive and time-consuming to get anno-
tated instances. On the other hand, unannotated
instances are abundant in this case, because all
the texts in the corpus can be regarded as unan-
notated instances before being annotated. This
scenario is very appropriate for active learning
application. An active learning algorithm picks
up the instances which will improve the per-
formance of the classifier to the largest extent
into the training set, and often produce classifier
with higher accuracy using less training instances.
Active learning algorithm is featured with
smaller training set size, less influence from un-
balanced training data and better classification
performance comparing to classical learning al-
gorithm. This paper experimentally demonstrates
the validity of active learning algorithm when
used for opinioned elements identification and
proposes a computational method for overall sys-
tem performance evaluation which consists of F-
measure, training time, and number of training
instances.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999893025">
Common active learning algorithms can be di-
vided into two classes, membership query and
selective sampling (Dagan and Engelson, 1995).
For membership query, algorithm constructs
learning instances by itself according to the
knowledge learnt, and submits the instances for
human processing (Angluin, 1988) (Sammut and
Banerji, 1986) (Shapiro, 1982). Although this
method has proved high learning efficiency (Da-
gan and Engelson, 1995), it can be applied in
fewer scenarios. Since constructing meaningful
training instance without the knowledge of target
concept is rather difficult. As to selective sam-
pling, algorithm picks up training instances
which can improve the performance of the classi-
fier to the largest extent from a large variety of
available instances. Algorithm in this class can
be further divided into stream-based algorithm
and pool-based algorithm according to how in-
stances are saved (Long et al, 2008). For stream-
based algorithm (Engelson and Dagon, 1999)
(Freund et al, 1997), unannotated instances are
submitted to the system successively. All the
instances not selected by the algorithms will be
discarded. As to pool-based algorithm (Muslea et
al, 2006) (McCallum and Nigam, 1998) (Lewis
and Gail, 1994), the algorithm choose the most
appropriate training instances from all the avail-
able instances. Instance not selected might have
chance to be picked up in the next round. Though
its computational complexity is higher, selective
sampling is widely used as an active learning
method for no prior knowledge of the target con-
cept is required.
Although much research has been made in
the field, we found no case which deals with
multi-classification problem in active learning.
Besides, there is no available method to evaluate
the performance of active learning in information
extraction.
</bodyText>
<sectionHeader confidence="0.9992095" genericHeader="method">
3 Active Learning Based Corpus Anno-
tation
</sectionHeader>
<subsectionHeader confidence="0.999975">
3.1 System Structure
</subsectionHeader>
<bodyText confidence="0.999960214285714">
The pool-based active learning algorithm is
composed of two main parts: a learning engine
and a selecting engine (Figure 1). The learning
engine uses instances in the training set to im-
prove the performance of the classifier. The se-
lecting engine picks up unannotated instances
according to preset rules, submits these instances
for human annotation, and incorporates these
instances into the training set after the annotation
is completed. The learning engine and the select-
ing engine work in turns. The performance of the
classifier tends to improve with the increasing of
the training set size. When the preset condition is
met, the training process will finish.
</bodyText>
<figureCaption confidence="0.933334">
Figure 1 System Workflow
</figureCaption>
<bodyText confidence="0.994037">
For our active learning based annotation tool,
the workflow is as follows.
</bodyText>
<listItem confidence="0.999209333333333">
1. Convert raw texts into the format which
the algorithm can deal with.
2. Selecting engine picks up instances which
are expected to improve the performance of the
classifier to the largest extent.
3. Annotate these instances manually.
4. Learning engine incorporate these anno-
tated instances into the training set, and use the
new training set to train the classifier.
5. Find out whether the performance of the
classifier satisfies the preset standard. If not, go
to step 2.
6. Use the classifier to identify the opinioned
element in the unannotated dataset.
7. Convert the result into the required format.
</listItem>
<subsectionHeader confidence="0.999878">
3.2 Learning Engine
</subsectionHeader>
<bodyText confidence="0.999969416666667">
The learning engine maintains the classifier by
iteratively training classifiers with new training
sets. The classifier adopted determines the up
limit of the system performance. We use Support
Vector Machine (SVM) (Vapnik, 1995) (Boser et
al, 1992) (Chang and Lin, 1992) as the classifier
for our system for its high generalization per-
formance even with feature vectors of high di-
mension and its ability to manage kernel func-
tions that map input data to higher dimensional
space without increasing computational com-
plexity.
</bodyText>
<subsectionHeader confidence="0.999859">
3.3 Selecting Engine
</subsectionHeader>
<bodyText confidence="0.999939451612903">
In our system, selecting engine picks up in-
stances for human annotation, and puts the anno-
tated instance into the training set. The strategy
adopted when selecting training instance is criti-
cal to the overall performance of the active learn-
ing algorithm. A good strategy will more likely
to produce a classifier with high accuracy from
less training instances.
The strategy we adopted here is to choose the
instances which the classifier is most unsure
about which class they belong to. For a linear bi-
classification SVM, these instances are the ones
closest to the separating hyper plane. That means,
the selecting engine will choose training in-
stances according to their geometric distances to
the hyper plane. The instance with least distance
will be selected as the next instance to be added
into the training set while the other instances will
be saved for future reference.
The computational complexity of getting the
distance between an instance and the hyper plane
is low. However, this method can not be applied
to SVM with non-linear kernel for geometric
distances are meaningless in these cases. We use
radial basis function, which is non-linear, as the
kernel function in our system for it outperforms
linear kernel in the experiment. Hence, we must
find another method to pick up training instances.
Non-linear SVM decides the class an in-
stance belongs to according to its decision func-
tion value.
</bodyText>
<equation confidence="0.879909333333333">
� y(x) = 1: as ysK(z • zs) +b (1)
�
xs
</equation>
<bodyText confidence="0.940009">
The instance will be classified into one cer-
tain class if y(z) &gt; 0 , or the other class
�
if y( x ) &lt;0. However, it will be difficult to clas-
sify the instance according to SVM theory
ify(x�) = 0. Hence, we may deduce that SVM is
most unsure when classifying an instance with
least absolute decision function value.
We define the Predict Value (PV) as the
value based on which selecting engine picks up
training instances.
For bi-classification SVM, we have PV
equals to the absolute decision function value,
namely,
</bodyText>
<equation confidence="0.999377">
PV(x) ( )
y x (2)
</equation>
<bodyText confidence="0.939114090909091">
Instances with the minimum PV will be selected
into the training set before other instances.
For example, if we want to identify all the
topics in the sentence,
I like this car very much, but the price is a little
bit too high.
ᚒᓛ༑⃶䖭᱅䔺㧘ૉᤚચ䪅㜞ੌὐ㧍
The PV of each instance in the sentences are
listed in Table 1. They are calculated from the
decision function of the SVM gained from the
last round of iteration.
</bodyText>
<table confidence="0.915521214285714">
Instances PV
a I 0.260306643320642
ৰ very 0.553855024703612
໛㣴 like 0.427269428974918
㪤 this 0.031682276068012
ཱི type 0.366598504697780
剐 car 0.095961213527654
Δ 0.178633448748979
܀ਢ but 0.092571306234562
᪔匷 price 0.052164989563922
೏ high 0.539913276317129
Ա (auxiliary word) 0.458036102580422
㭠 a little bit 0.439936293288062
Μ 0.375263535139242
</table>
<tableCaption confidence="0.7737305">
Table 1 Example of 2-Classification SVM
Predict Value
</tableCaption>
<bodyText confidence="0.999609">
Suppose all the instances in this sentence
have not been added into the training set. This
(0.0316), price (0.0521), and but (0.0925) will be
selected into the training set successively for
they have the minimal PVs.
For multi-classification SVM, it will be more
complicated to find the training instances. Be-
cause common multi-classification SVM is im-
plemented by voting process (Hsu and Lin, 2002),
</bodyText>
<equation confidence="0.782194666666667">
S
E
there are 2 t • (t —1) decision function values in t-
</equation>
<bodyText confidence="0.935325666666667">
classification SVM.
In our system, we need to classify instances
into 4 classes, namely, topic, holder, sentiment
and other. So a 4-classification SVM is adopted.
Suppose for an instance, we get 6 Decision Func-
tion Values from 6 bi-classification SVMs as in
</bodyText>
<tableCaption confidence="0.990186">
Table 2.
</tableCaption>
<table confidence="0.999422428571429">
No. Classification Decision Function Value Result
1 Class 0 Vs Class 1 1.00032792289507 0
2 Class 0 Vs Class 2 0.999999993721249 0
3 Class 0 Vs Class 3 1.00032792289507 0
4 Class 1 Vs Class 2 0.106393804825973 1
5 Class 1 Vs Class 3 -5.20417042793042E-18 3
6 Class 2 Vs Class 3 -0.106393804825973 3
</table>
<tableCaption confidence="0.997918">
Table 2 Example of 4-Classification SVM Decision
</tableCaption>
<subsectionHeader confidence="0.34284">
Process
</subsectionHeader>
<bodyText confidence="0.999283625">
For each bi-classification SVM, the class in-
stance belongs to is determined by whether the
decision function value is greater than or less
than zero. The instance in Table 2 belongs to
Class 0 since there 3 votes out of 6 votes for
Class 0. When deciding which class an instance
belongs to, only the decision function values
from bi-classification SVMs with correct votes
will work on the certainty of the final result.
Hence, we define Predict Value for multi-
classification SVMs as the arithmetic mean value
of the absolute decision function value of every
bi-classification SVM with correct vote,
For the instance in Table2, the value is calculated
from the decision function values from bi-
classification SVMs numbered 1, 2, and 3.
</bodyText>
<sectionHeader confidence="0.50207" genericHeader="method">
3.4 Experiments
</sectionHeader>
<bodyText confidence="0.988063666666667">
To prove the validity of active learning algorithm
and find out the relations between the perform-
ance of the classifiers and the way the classifiers
are trained, we carried out batches of experi-
ments.
In most information extraction tasks, a word
and its context are considered a learning sample,
and encoded as feature vectors. In our experi-
ments, context data includes the part-of-speech
tag, dependency relation, word semantic mean-
ing, and word disambiguation information of the
word being classified, its neighboring words and
its parent word in dependency grammar. Part-of-
speech tag and dependency relation are common
features for Chinese Natural Language Process-
ing (NLP) tasks1. We get word semantic mean-
ing from HowNet, which is an online common-
sense knowledge base unveiling inter-conceptual
relations and inter-attribute relations of concepts
as connoting in lexicons of the Chinese and the
English equivalents (Zhendong Dong and Qiang
Dong, 1999). Given an occurrence of a word in
natural language text, word sense disambiguation
is the process of identifying which sense of the
word is intended if the word has a number of dis-
tinct senses. According to Song and Yao (2009),
this information may help in Chinese NLP tasks
such as topic identification.
Lack of explicit boundary between training
instances and testing instances is a great differ-
ence between common machine learning algo-
rithm and learning algorithm designed for corpus
annotation. For common machine learning algo-
rithm such as human face recognition, the quan-
tity of training instances is limited while the test-
ing instances could be infinite. It is unnecessary
and impossible to annotate all the testing in-
stances. However, when annotating a corpus, all
the texts need to be annotated are decided be-
forehand. Although tools automated part of the
annotation process, the results still need to be
reviewed for several times to ensure the quality
of annotation. That means in an annotation sce-
nario, all the data to be processed are available
during the training stage.
The raw texts used in our experiments are
taken from forums of chinacars.com. These texts
include explicit subjective opinion and informal
network language, which are necessary for opin-
ion mining research. Most of them are comments
composed of one or more sentences on certain
type of vehicle. The detailed opinion elements
distributions are showed in table 3.
We use all the texts as testing data set and a
subset of it as a training data set. First of all, we
pick up 10 instances for each class, and train a
simple classification model with them. Then, the
baseline system picks up k instances in sequence
and adds them into the training data set to train a
new classification model iteratively until the
training data set is as large as the testing data set,
1 We use Language Technology Platform (LTP), developed
by Center for Information Retrieval, Harbin Institute of
Technology, for part-of-speech tagging, dependency rela-
tionship analysis and word sense disambiguation in our
experiment.
</bodyText>
<equation confidence="0.907367166666667">
k
1PV ( x)= E y t( x)
k
t-1,te[bi-classification SVMs with correct votes}
(3)
ir
</equation>
<bodyText confidence="0.93809">
while the active learning system picks up in-
stances according to the strategy in Chapter 3.3.
</bodyText>
<figure confidence="0.701268666666667">
Type No. of Instances
Topic 638
Sentiment 769
Holder 46
Other 1500
Total 2953
</figure>
<tableCaption confidence="0.948249">
Table 3 Detailed Information of the Data Set
</tableCaption>
<bodyText confidence="0.999883846153846">
We use three bi-classification model to test
the performance of the active learning system on
topic, sentiment, and holder identification sepa-
rately and a four-classification model to identify
the three opinion elements simultaneously. The
results of the experiments are illustrated in Fig-
ure 2, 3, 4, and 5 respectively. Table 4, 5, and 6
provide the detailed F-measure trends while dif-
ferent numbers of instances are added into the
training data set in each rounds. For each ex-
periment, we try to compare the performances
when we add different number of instances into
the training data set in each round of iteration.
</bodyText>
<figureCaption confidence="0.9998355">
Figure 2 Topic Identification
Figure 3 Sentiment Identification
</figureCaption>
<bodyText confidence="0.999854838709677">
As are illustrated in the figures, the active
learning system can always achieve better or at
least no worse performance than baseline system.
For example, when adding 200 instances in each
round for topic identification task (Figure2 and
Table 4), the active learning system reaches its
peak value in F-measure (0.8644) with only 600
training instances. This F-measure value is even
higher than the value the baseline system get
(0.8604) after taking all the 2953 training in-
stances.
The active learning system outperforms the
baseline system greatly especially when dealing
with unbalanced data set (Figure 4 and Table 4).
In opinion holder identification task, the baseline
system can not find any holder until 1600 train-
ing instances are taken while the active learning
system reaches its peak F-measure value (0.8810)
with only 600 training instances. That means
when using active learning algorithm, it is possi-
ble for us to save some time for optimizing the
parameters when dealing with unbalanced data.
The number of instances added to the training
data set in each round (k) influences the perform-
ance of the active learning algorithm in a large
extent. When a smaller value is assigned to k, the
active learning system will tend to achieve better
F-measure (Table 4) with less training instances
comparing to the baseline system. Advantages of
the active learning system will be diminished by
the increase in k (Table 6).
</bodyText>
<figureCaption confidence="0.9995135">
Figure 4 Holder Identification
Figure 5 All Opinion Elements Identification
</figureCaption>
<bodyText confidence="0.954380384615385">
4 Evaluation of Active Learning Algo-
rithm
For active learning algorithm based on member-
ship query, its training process will probably take
longer time by the time the optimum classier is
found, since the training process consists of sev-
eral rounds of iteration. At the beginning of the
iteration, the classification speed of the model is
much faster due to less training instances are
used and the model is simple. With more and
more training instances are added into the train-
ing data set, the model will become more com-
plex and more time will be needed for classifica-
</bodyText>
<table confidence="0.999767789473684">
No. of Topic Sentiment Holder All Three Elements
Instances
Baseline Active Baseline Active Baseline Active Baseline Active
Learning Learning Learning Learning
200 0.7118 0.6221 0.6481 0.0103 0.0000 0.0000 0.6968 0.3874
400 0.8072 0.8287 0.7344 0.6239 0.0000 0.0000 0.7691 0.7336
600 0.8237 0.8644 0.7845 0.7860 0.0000 0.8810 0.7907 0.7979
800 0.8250 0.8625 0.7876 0.8133 0.0000 0.8810 0.8020 0.8240
1000 0.8386 0.8613 0.7878 0.8189 0.0000 0.8810 0.8101 0.8378
1200 0.8389 0.8588 0.7992 0.8153 0.0000 0.8810 0.8128 0.8377
1400 0.8489 0.8588 0.8011 0.8141 0.0000 0.8810 0.8178 0.8471
1600 0.8450 0.8581 0.8033 0.8150 0.0426 0.8810 0.8211 0.8468
1800 0.8521 0.8581 0.8059 0.8183 0.1224 0.8810 0.8271 0.8479
2000 0.8528 0.8585 0.8169 0.8197 0.6857 0.8810 0.8348 0.8481
2200 0.8560 0.8583 0.8109 0.8200 0.8101 0.8810 0.8372 0.8468
2400 0.8592 0.8592 0.8186 0.8195 0.8395 0.8810 0.8404 0.8474
2600 0.8620 0.8610 0.8165 0.8205 0.8675 0.8810 0.8440 0.8463
2800 0.8578 0.8610 0.8138 0.8177 0.8810 0.8810 0.8464 0.8443
2953 0.8604 0.8604 0.8183 0.8183 0.8810 0.8810 0.8446 0.8446
</table>
<tableCaption confidence="0.906571">
Table 4 F-measure Trends when 1=200
</tableCaption>
<table confidence="0.9999315">
No. of Topic Sentiment Holder All Three Elements
Instances
Baseline Active Baseline Active Baseline Active Baseline Active
Learning Learning Learning Learning
500 0.8198 0.7730 0.7616 0.1369 0.0000 0.0000 0.7831 0.5173
1000 0.8386 0.8508 0.7878 0.7566 0.0000 0.8837 0.8101 0.7776
1500 0.8468 0.8592 0.8039 0.8175 0.0833 0.8810 0.8194 0.8398
2000 0.8528 0.8610 0.8169 0.8183 0.6857 0.8810 0.8348 0.8484
2500 0.8626 0.8583 0.8168 0.8205 0.8395 0.8810 0.8427 0.8463
2953 0.8604 0.8604 0.8183 0.8183 0.8810 0.8810 0.8446 0.8446
</table>
<tableCaption confidence="0.836155">
Table 5 F-measure Trends when 1=500
</tableCaption>
<table confidence="0.999585571428572">
No. of Topic Sentiment Holder All Three Elements
Instances
Baseline Active Baseline Active Baseline Active Baseline Active
Learning Learning Learning Learning
1000 0.8386 0.8335 0.7878 0.3514 0.0000 0.0000 0.8101 0.7534
2000 0.8528 0.8581 0.8169 0.8170 0.6857 0.8810 0.8348 0.8376
2953 0.8604 0.8604 0.8183 0.8183 0.8810 0.8810 0.8446 0.8446
</table>
<tableCaption confidence="0.967188">
Table 6 F-measure Trends when 1=1000
</tableCaption>
<bodyText confidence="0.999888833333333">
tion. On account of the features of active learn-
ing algorithm, we believe it is necessary to find a
way to balance the performance of the classifier
and the time it take in training process for a thor-
ough evaluation of the algorithm.
We define the measurement for time as:
</bodyText>
<equation confidence="0.980659">
k
T = (4)
C
</equation>
<bodyText confidence="0.9990173">
where C is the number of all the possible training
instances available, k is the number of training
instances added into the training data set in each
round of iteration. T is the approximate value of
the inverse ratio of the time it takes for training
process. T will have a greater value if the training
process takes less time. Its range is (0, 1] just
similar to F-measure.
We define the measurement for the training
instances used as:
</bodyText>
<equation confidence="0.996109">
n (5)
K =(1—C)
</equation>
<bodyText confidence="0.996087363636364">
where n is the number of the training instances
actually used. K will have a greater value if less
training instances are used in the training process.
The range of K is [0, 1).
To judge the overall performance of an active
learning algorithm, we consider the F-measure
(F) of the classifier, the time it takes during the
training process, and the training instances used.
We define the Active Learning Performance
(ALP) as the harmonic mean of the three aspects:
pose a=,8=y=1 , we calculate the value of ALP
</bodyText>
<sectionHeader confidence="0.622724" genericHeader="method">
3
</sectionHeader>
<bodyText confidence="0.99993525">
for the two cases according to equation (6) and
get 0.1714 and 0.1507 as results respectively.
That means a people with no preference among
F-measure, the number of training instances
adopted and the time used during training proc-
ess will choose to get a classifier with less train-
ing instances, less training time and less F-
measure value.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999850333333333">
This paper experimentally demonstrates the va-
lidity of active learning algorithm when used for
opinioned elements identification and proposed a
computational method for overall system per-
formance evaluation which consists of F-
measure, training time, and number of training
instances. According to our tests, active learning
algorithm outperforms the base line system in
most of the cases especially when fewer in-
stances are added into the training data set in
each round of iteration. However, the method
could extent the training time in a large scale. To
balance the pros and cons of active learning algo-
rithm, it might be helpful to adjust the number of
training instances added in each round dynami-
cally in the training process. For instance, add
less training instances at the beginning of the
training process to ensure a high peak value of F-
measure could be achieved and add more train-
ing instances later so that time spent on training
process could be reduced.
</bodyText>
<equation confidence="0.8051884">
1
ALP
a ,8 y
+ +
K F T
</equation>
<bodyText confidence="0.999865083333333">
where a+,8+y=1 , and a,,8, y E [0,1] . They
are the weights for the three measurements. The
greater the value of a certain weight is, the more
important the measurement is in the overall per-
formance. The greater the value of the ALP is,
the better the performance of the active learning
algorithm. For instance, when training a classi-
fier for sentiment identification using active
learning algorithm, we get a classifier with F-
measure of 0.8189 using 1000 training instances
and a classifier with F-measure of 0.8200 using
2200 training instances (Table 4). Sup-
</bodyText>
<sectionHeader confidence="0.998133" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999609833333333">
The author of this paper would like to thank In-
formation Retrieval Lab, Harbin Institute of
Technology for providing the tool (LTP) used in
experiments. This research was supported by
National Natural Science Foundation of China
Grant No.60773087.
</bodyText>
<sectionHeader confidence="0.999641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999683857142857">
Andrew K. McCallum, Kamal Nigam. 1998. Employ-
ing EM in Pool-based Active Learning for Text
Classification. In Proceedings of the 15th Interna-
tional Conference on Machine Learning.
Bernhard E. Boser, Isabelle M. Guyon, and Vladimir
N. Vapnik. 1992. A Training Algorithm for Opti-
mal Margin Classifiers. In Proceedings of the Fifth
</reference>
<figure confidence="0.79974">
F •
a • F•C•k+,8•k
k • (C — n) (6)
(C—n)+y•F•C(C—n)
</figure>
<reference confidence="0.986144926470588">
Annual Workshop on Computational Learning
Theory.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM:
a library for support vector machines. Software
available at http://www.csie.ntu.edu.tw/~cjlin/
libsvm
Chih-Wei Hsu and Chih-Jen Lin. 2002. A Compari-
son of Methods for Multi-class Support Vector
Machines. IEEE Transactions on Neural Networks.
Claude Sammut and Ranan B. Banerji. 1986. Learn-
ing Concepts by Asking Questions. Machine
Learning: An Artificial Intelligence Approach,
1986, 2: 167-191
Dana Angluin. 1988. Queries and Concept Learning.
Machine Learning, 1988, 2(4): 319-342
David D. Lewis, William A. Gail. 1994. A Sequential
Algorithm for Training Text Classifiers. In Pro-
ceedings of the 17th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval.
Ehud Y. Shapiro. 1982. Algorithmic Program Debug-
ging. M.I.T. Press.
Ido Dagan, Sean P. Engelson. 1995. Committee-
Based Sampling for Training Probabilistic Classi-
fiers. In Proceedings of the International Confer-
ence on Machine Learning.
Ion Muslea, Steven Minton, Craig A. Knoblock. 2006.
Active Learning with Multiple Views. Journal of
Artificial Intelligence Research, 2006, 27(1): 203-
233.
Quansheng Liu, Tianfang Yao, Gaohui Huang, Jun
Liu, Hongyan Song. 2008. A Survey of Opinion
Mining for Texts. Journal of Chinese Information
Processing. 2008, 22(6):63-68.
Jun Long, Jianping Yin, En Zhu, and Wentao Zhao. A
Survey of Active Learning. 2008. Journal of Com-
puter Research and Development, 2008, 45(z1):
300-304.
Shlomo A. Engelson, Ido Dagon. 1999. Committee-
based Sample Selection for Probabilistic Classifi-
ers. Journal of Artificial Intelligence Research,
1999, 11: 335-360.
Hongyan Song, Jun Liu, Tianfang Yao, Quansheng
Liu, Gaohui Huang. 2009. Construction of an An-
notated Corpus for Chinese Opinioned-Subjective
Texts. Journal of Chinese Information Processing,
2009, 23(2): 123-128.
Hongyan Song and Tianfang Yao. 2009. Improving
Chinese Topic Extraction Using Word Sense Dis-
ambiguation Information. In Proceedings of the 4th
International Conference on Innovative Computing,
Information and Control.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the Sentiment of Opinions. In Proceedings of the
Conference on Computational Linguistics: 1367-
1373.
Tianfang Yao, Xiwen Cheng, Feiyu Xu, Hans
Uszkoreit, and Rui Wang. 2008. A Survey of Opin-
ion Mining for Texts. Journal of Chinese Informa-
tion Processing, 2008, 22(3): 71-80.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
Yoav Freund, H.Sebastian Seung, Eli Shamir, Naftali
Tishby. 1997. Selective Sampling Using the Query
by Committee Algorithm. Machine Learning,
28(2-3): 133-168
Zhendong Dong and Qiang Dong. 1999. HowNet.
http://www.keenage.com
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.844569">
<title confidence="0.9578345">Active Learning Based Corpus Annotation and Tianfang</title>
<author confidence="0.999778">Shanghai Jiao Tong</author>
<affiliation confidence="0.999974">Department of Computer Science and</affiliation>
<address confidence="0.94925">Shanghai, China</address>
<abstract confidence="0.997321818181818">Opinion Mining aims to automatically acquire useful opinioned information and knowledge in subjective texts. Research of Chinese Opinioned Mining requires the support of annotated corpus for Chinese opinioned-subjective texts. To facilitate the work of corpus annotators, this paper implements an active learning based annotation tool for Chinese opinioned elements which can identify topic, sentiment, and opinion holder in a sentence automatically.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>Employing EM in Pool-based Active Learning for Text Classification.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5121" citStr="McCallum and Nigam, 1998" startWordPosition="753" endWordPosition="756">lt. As to selective sampling, algorithm picks up training instances which can improve the performance of the classifier to the largest extent from a large variety of available instances. Algorithm in this class can be further divided into stream-based algorithm and pool-based algorithm according to how instances are saved (Long et al, 2008). For streambased algorithm (Engelson and Dagon, 1999) (Freund et al, 1997), unannotated instances are submitted to the system successively. All the instances not selected by the algorithms will be discarded. As to pool-based algorithm (Muslea et al, 2006) (McCallum and Nigam, 1998) (Lewis and Gail, 1994), the algorithm choose the most appropriate training instances from all the available instances. Instance not selected might have chance to be picked up in the next round. Though its computational complexity is higher, selective sampling is widely used as an active learning method for no prior knowledge of the target concept is required. Although much research has been made in the field, we found no case which deals with multi-classification problem in active learning. Besides, there is no available method to evaluate the performance of active learning in information ext</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew K. McCallum, Kamal Nigam. 1998. Employing EM in Pool-based Active Learning for Text Classification. In Proceedings of the 15th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard E Boser</author>
<author>Isabelle M Guyon</author>
<author>Vladimir N Vapnik</author>
</authors>
<title>A Training Algorithm for Optimal Margin Classifiers.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fifth Annual Workshop on Computational Learning Theory.</booktitle>
<contexts>
<context position="7437" citStr="Boser et al, 1992" startWordPosition="1121" endWordPosition="1124">porate these annotated instances into the training set, and use the new training set to train the classifier. 5. Find out whether the performance of the classifier satisfies the preset standard. If not, go to step 2. 6. Use the classifier to identify the opinioned element in the unannotated dataset. 7. Convert the result into the required format. 3.2 Learning Engine The learning engine maintains the classifier by iteratively training classifiers with new training sets. The classifier adopted determines the up limit of the system performance. We use Support Vector Machine (SVM) (Vapnik, 1995) (Boser et al, 1992) (Chang and Lin, 1992) as the classifier for our system for its high generalization performance even with feature vectors of high dimension and its ability to manage kernel functions that map input data to higher dimensional space without increasing computational complexity. 3.3 Selecting Engine In our system, selecting engine picks up instances for human annotation, and puts the annotated instance into the training set. The strategy adopted when selecting training instance is critical to the overall performance of the active learning algorithm. A good strategy will more likely to produce a cl</context>
</contexts>
<marker>Boser, Guyon, Vapnik, 1992</marker>
<rawString>Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. 1992. A Training Algorithm for Optimal Margin Classifiers. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/ libsvm</title>
<date>2001</date>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/ libsvm</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Wei Hsu</author>
<author>Chih-Jen Lin</author>
</authors>
<title>A Comparison of Methods for Multi-class Support Vector Machines.</title>
<date>2002</date>
<journal>IEEE Transactions on Neural Networks.</journal>
<contexts>
<context position="10926" citStr="Hsu and Lin, 2002" startWordPosition="1701" endWordPosition="1704"> but 0.092571306234562 匷 price 0.052164989563922  high 0.539913276317129 Ա (auxiliary word) 0.458036102580422 㭠 a little bit 0.439936293288062 Μ 0.375263535139242 Table 1 Example of 2-Classification SVM Predict Value Suppose all the instances in this sentence have not been added into the training set. This (0.0316), price (0.0521), and but (0.0925) will be selected into the training set successively for they have the minimal PVs. For multi-classification SVM, it will be more complicated to find the training instances. Because common multi-classification SVM is implemented by voting process (Hsu and Lin, 2002), S E there are 2 t • (t —1) decision function values in tclassification SVM. In our system, we need to classify instances into 4 classes, namely, topic, holder, sentiment and other. So a 4-classification SVM is adopted. Suppose for an instance, we get 6 Decision Function Values from 6 bi-classification SVMs as in Table 2. No. Classification Decision Function Value Result 1 Class 0 Vs Class 1 1.00032792289507 0 2 Class 0 Vs Class 2 0.999999993721249 0 3 Class 0 Vs Class 3 1.00032792289507 0 4 Class 1 Vs Class 2 0.106393804825973 1 5 Class 1 Vs Class 3 -5.20417042793042E-18 3 6 Class 2 Vs Class</context>
</contexts>
<marker>Hsu, Lin, 2002</marker>
<rawString>Chih-Wei Hsu and Chih-Jen Lin. 2002. A Comparison of Methods for Multi-class Support Vector Machines. IEEE Transactions on Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Sammut</author>
<author>Ranan B Banerji</author>
</authors>
<title>Learning Concepts by Asking Questions. Machine Learning: An Artificial Intelligence Approach,</title>
<date>1986</date>
<volume>2</volume>
<pages>167--191</pages>
<contexts>
<context position="4250" citStr="Sammut and Banerji, 1986" startWordPosition="619" endWordPosition="622">paper experimentally demonstrates the validity of active learning algorithm when used for opinioned elements identification and proposes a computational method for overall system performance evaluation which consists of Fmeasure, training time, and number of training instances. 2 Related Work Common active learning algorithms can be divided into two classes, membership query and selective sampling (Dagan and Engelson, 1995). For membership query, algorithm constructs learning instances by itself according to the knowledge learnt, and submits the instances for human processing (Angluin, 1988) (Sammut and Banerji, 1986) (Shapiro, 1982). Although this method has proved high learning efficiency (Dagan and Engelson, 1995), it can be applied in fewer scenarios. Since constructing meaningful training instance without the knowledge of target concept is rather difficult. As to selective sampling, algorithm picks up training instances which can improve the performance of the classifier to the largest extent from a large variety of available instances. Algorithm in this class can be further divided into stream-based algorithm and pool-based algorithm according to how instances are saved (Long et al, 2008). For stream</context>
</contexts>
<marker>Sammut, Banerji, 1986</marker>
<rawString>Claude Sammut and Ranan B. Banerji. 1986. Learning Concepts by Asking Questions. Machine Learning: An Artificial Intelligence Approach, 1986, 2: 167-191</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
</authors>
<title>Queries and Concept Learning.</title>
<date>1988</date>
<booktitle>Machine Learning,</booktitle>
<volume>2</volume>
<issue>4</issue>
<pages>319--342</pages>
<contexts>
<context position="4223" citStr="Angluin, 1988" startWordPosition="617" endWordPosition="618">algorithm. This paper experimentally demonstrates the validity of active learning algorithm when used for opinioned elements identification and proposes a computational method for overall system performance evaluation which consists of Fmeasure, training time, and number of training instances. 2 Related Work Common active learning algorithms can be divided into two classes, membership query and selective sampling (Dagan and Engelson, 1995). For membership query, algorithm constructs learning instances by itself according to the knowledge learnt, and submits the instances for human processing (Angluin, 1988) (Sammut and Banerji, 1986) (Shapiro, 1982). Although this method has proved high learning efficiency (Dagan and Engelson, 1995), it can be applied in fewer scenarios. Since constructing meaningful training instance without the knowledge of target concept is rather difficult. As to selective sampling, algorithm picks up training instances which can improve the performance of the classifier to the largest extent from a large variety of available instances. Algorithm in this class can be further divided into stream-based algorithm and pool-based algorithm according to how instances are saved (Lo</context>
</contexts>
<marker>Angluin, 1988</marker>
<rawString>Dana Angluin. 1988. Queries and Concept Learning. Machine Learning, 1988, 2(4): 319-342</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gail</author>
</authors>
<title>A Sequential Algorithm for Training Text Classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="5144" citStr="Lewis and Gail, 1994" startWordPosition="757" endWordPosition="760">g, algorithm picks up training instances which can improve the performance of the classifier to the largest extent from a large variety of available instances. Algorithm in this class can be further divided into stream-based algorithm and pool-based algorithm according to how instances are saved (Long et al, 2008). For streambased algorithm (Engelson and Dagon, 1999) (Freund et al, 1997), unannotated instances are submitted to the system successively. All the instances not selected by the algorithms will be discarded. As to pool-based algorithm (Muslea et al, 2006) (McCallum and Nigam, 1998) (Lewis and Gail, 1994), the algorithm choose the most appropriate training instances from all the available instances. Instance not selected might have chance to be picked up in the next round. Though its computational complexity is higher, selective sampling is widely used as an active learning method for no prior knowledge of the target concept is required. Although much research has been made in the field, we found no case which deals with multi-classification problem in active learning. Besides, there is no available method to evaluate the performance of active learning in information extraction. 3 Active Learn</context>
</contexts>
<marker>Lewis, Gail, 1994</marker>
<rawString>David D. Lewis, William A. Gail. 1994. A Sequential Algorithm for Training Text Classifiers. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Y Shapiro</author>
</authors>
<title>Algorithmic Program Debugging.</title>
<date>1982</date>
<publisher>M.I.T. Press.</publisher>
<contexts>
<context position="4266" citStr="Shapiro, 1982" startWordPosition="623" endWordPosition="624">trates the validity of active learning algorithm when used for opinioned elements identification and proposes a computational method for overall system performance evaluation which consists of Fmeasure, training time, and number of training instances. 2 Related Work Common active learning algorithms can be divided into two classes, membership query and selective sampling (Dagan and Engelson, 1995). For membership query, algorithm constructs learning instances by itself according to the knowledge learnt, and submits the instances for human processing (Angluin, 1988) (Sammut and Banerji, 1986) (Shapiro, 1982). Although this method has proved high learning efficiency (Dagan and Engelson, 1995), it can be applied in fewer scenarios. Since constructing meaningful training instance without the knowledge of target concept is rather difficult. As to selective sampling, algorithm picks up training instances which can improve the performance of the classifier to the largest extent from a large variety of available instances. Algorithm in this class can be further divided into stream-based algorithm and pool-based algorithm according to how instances are saved (Long et al, 2008). For streambased algorithm </context>
</contexts>
<marker>Shapiro, 1982</marker>
<rawString>Ehud Y. Shapiro. 1982. Algorithmic Program Debugging. M.I.T. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Sean P Engelson</author>
</authors>
<title>CommitteeBased Sampling for Training Probabilistic Classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="4052" citStr="Dagan and Engelson, 1995" startWordPosition="592" endWordPosition="595">ive learning algorithm is featured with smaller training set size, less influence from unbalanced training data and better classification performance comparing to classical learning algorithm. This paper experimentally demonstrates the validity of active learning algorithm when used for opinioned elements identification and proposes a computational method for overall system performance evaluation which consists of Fmeasure, training time, and number of training instances. 2 Related Work Common active learning algorithms can be divided into two classes, membership query and selective sampling (Dagan and Engelson, 1995). For membership query, algorithm constructs learning instances by itself according to the knowledge learnt, and submits the instances for human processing (Angluin, 1988) (Sammut and Banerji, 1986) (Shapiro, 1982). Although this method has proved high learning efficiency (Dagan and Engelson, 1995), it can be applied in fewer scenarios. Since constructing meaningful training instance without the knowledge of target concept is rather difficult. As to selective sampling, algorithm picks up training instances which can improve the performance of the classifier to the largest extent from a large v</context>
</contexts>
<marker>Dagan, Engelson, 1995</marker>
<rawString>Ido Dagan, Sean P. Engelson. 1995. CommitteeBased Sampling for Training Probabilistic Classifiers. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ion Muslea</author>
<author>Steven Minton</author>
<author>Craig A Knoblock</author>
</authors>
<title>Active Learning with Multiple Views.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>27</volume>
<issue>1</issue>
<pages>203--233</pages>
<contexts>
<context position="5094" citStr="Muslea et al, 2006" startWordPosition="749" endWordPosition="752">ept is rather difficult. As to selective sampling, algorithm picks up training instances which can improve the performance of the classifier to the largest extent from a large variety of available instances. Algorithm in this class can be further divided into stream-based algorithm and pool-based algorithm according to how instances are saved (Long et al, 2008). For streambased algorithm (Engelson and Dagon, 1999) (Freund et al, 1997), unannotated instances are submitted to the system successively. All the instances not selected by the algorithms will be discarded. As to pool-based algorithm (Muslea et al, 2006) (McCallum and Nigam, 1998) (Lewis and Gail, 1994), the algorithm choose the most appropriate training instances from all the available instances. Instance not selected might have chance to be picked up in the next round. Though its computational complexity is higher, selective sampling is widely used as an active learning method for no prior knowledge of the target concept is required. Although much research has been made in the field, we found no case which deals with multi-classification problem in active learning. Besides, there is no available method to evaluate the performance of active </context>
</contexts>
<marker>Muslea, Minton, Knoblock, 2006</marker>
<rawString>Ion Muslea, Steven Minton, Craig A. Knoblock. 2006. Active Learning with Multiple Views. Journal of Artificial Intelligence Research, 2006, 27(1): 203-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quansheng Liu</author>
</authors>
<title>Tianfang Yao, Gaohui Huang, Jun Liu, Hongyan Song.</title>
<date>2008</date>
<pages>22--6</pages>
<marker>Liu, 2008</marker>
<rawString>Quansheng Liu, Tianfang Yao, Gaohui Huang, Jun Liu, Hongyan Song. 2008. A Survey of Opinion Mining for Texts. Journal of Chinese Information Processing. 2008, 22(6):63-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Long</author>
<author>Jianping Yin</author>
<author>En Zhu</author>
<author>Wentao Zhao</author>
</authors>
<title>A Survey of Active Learning.</title>
<date>2008</date>
<journal>Journal of Computer Research</journal>
<pages>45--1</pages>
<contexts>
<context position="4838" citStr="Long et al, 2008" startWordPosition="710" endWordPosition="713">8) (Sammut and Banerji, 1986) (Shapiro, 1982). Although this method has proved high learning efficiency (Dagan and Engelson, 1995), it can be applied in fewer scenarios. Since constructing meaningful training instance without the knowledge of target concept is rather difficult. As to selective sampling, algorithm picks up training instances which can improve the performance of the classifier to the largest extent from a large variety of available instances. Algorithm in this class can be further divided into stream-based algorithm and pool-based algorithm according to how instances are saved (Long et al, 2008). For streambased algorithm (Engelson and Dagon, 1999) (Freund et al, 1997), unannotated instances are submitted to the system successively. All the instances not selected by the algorithms will be discarded. As to pool-based algorithm (Muslea et al, 2006) (McCallum and Nigam, 1998) (Lewis and Gail, 1994), the algorithm choose the most appropriate training instances from all the available instances. Instance not selected might have chance to be picked up in the next round. Though its computational complexity is higher, selective sampling is widely used as an active learning method for no prior</context>
</contexts>
<marker>Long, Yin, Zhu, Zhao, 2008</marker>
<rawString>Jun Long, Jianping Yin, En Zhu, and Wentao Zhao. A Survey of Active Learning. 2008. Journal of Computer Research and Development, 2008, 45(z1): 300-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo A Engelson</author>
<author>Ido Dagon</author>
</authors>
<title>Committeebased Sample Selection for Probabilistic Classifiers.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>11</volume>
<pages>335--360</pages>
<contexts>
<context position="4892" citStr="Engelson and Dagon, 1999" startWordPosition="718" endWordPosition="721"> Although this method has proved high learning efficiency (Dagan and Engelson, 1995), it can be applied in fewer scenarios. Since constructing meaningful training instance without the knowledge of target concept is rather difficult. As to selective sampling, algorithm picks up training instances which can improve the performance of the classifier to the largest extent from a large variety of available instances. Algorithm in this class can be further divided into stream-based algorithm and pool-based algorithm according to how instances are saved (Long et al, 2008). For streambased algorithm (Engelson and Dagon, 1999) (Freund et al, 1997), unannotated instances are submitted to the system successively. All the instances not selected by the algorithms will be discarded. As to pool-based algorithm (Muslea et al, 2006) (McCallum and Nigam, 1998) (Lewis and Gail, 1994), the algorithm choose the most appropriate training instances from all the available instances. Instance not selected might have chance to be picked up in the next round. Though its computational complexity is higher, selective sampling is widely used as an active learning method for no prior knowledge of the target concept is required. Although</context>
</contexts>
<marker>Engelson, Dagon, 1999</marker>
<rawString>Shlomo A. Engelson, Ido Dagon. 1999. Committeebased Sample Selection for Probabilistic Classifiers. Journal of Artificial Intelligence Research, 1999, 11: 335-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Song</author>
<author>Jun Liu</author>
</authors>
<title>Tianfang Yao, Quansheng Liu, Gaohui Huang.</title>
<date>2009</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>23</volume>
<issue>2</issue>
<pages>123--128</pages>
<marker>Song, Liu, 2009</marker>
<rawString>Hongyan Song, Jun Liu, Tianfang Yao, Quansheng Liu, Gaohui Huang. 2009. Construction of an Annotated Corpus for Chinese Opinioned-Subjective Texts. Journal of Chinese Information Processing, 2009, 23(2): 123-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Song</author>
<author>Tianfang Yao</author>
</authors>
<title>Improving Chinese Topic Extraction Using Word Sense Disambiguation Information.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th International Conference on Innovative Computing, Information and Control.</booktitle>
<contexts>
<context position="13540" citStr="Song and Yao (2009)" startWordPosition="2134" endWordPosition="2137">mar. Part-ofspeech tag and dependency relation are common features for Chinese Natural Language Processing (NLP) tasks1. We get word semantic meaning from HowNet, which is an online commonsense knowledge base unveiling inter-conceptual relations and inter-attribute relations of concepts as connoting in lexicons of the Chinese and the English equivalents (Zhendong Dong and Qiang Dong, 1999). Given an occurrence of a word in natural language text, word sense disambiguation is the process of identifying which sense of the word is intended if the word has a number of distinct senses. According to Song and Yao (2009), this information may help in Chinese NLP tasks such as topic identification. Lack of explicit boundary between training instances and testing instances is a great difference between common machine learning algorithm and learning algorithm designed for corpus annotation. For common machine learning algorithm such as human face recognition, the quantity of training instances is limited while the testing instances could be infinite. It is unnecessary and impossible to annotate all the testing instances. However, when annotating a corpus, all the texts need to be annotated are decided beforehand</context>
</contexts>
<marker>Song, Yao, 2009</marker>
<rawString>Hongyan Song and Tianfang Yao. 2009. Improving Chinese Topic Extraction Using Word Sense Disambiguation Information. In Proceedings of the 4th International Conference on Innovative Computing, Information and Control.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the Sentiment of Opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics:</booktitle>
<pages>1367--1373</pages>
<contexts>
<context position="1422" citStr="Kim and Hovy (2004)" startWordPosition="196" endWordPosition="199">ation and knowledge in subjective texts (Liu et al, 2008). This technique has wide and many real world applications, such as e-commerce, business intelligence, information monitoring, public opinion poll, e-learning, newspaper and publication compilation, and business management. For instance, a typical opinion mining system produces statistical results from online product reviews, which can be used by potential customers when deciding which model to choose, by manufacturers to find out the possible areas of improvement, and by dealers for sales plan evaluation (Yao et al, 2008). According to Kim and Hovy (2004), an opinion is composed of four parts, namely, topic, holder, sentiment, and claim, in which the holder expresses the claim including positive or negative sentiment towards the topic. For example, in the sentence I like this car, I is the holder, like is the positive sentiment, car is the topic, and the whole sentence is the claim. Research on Chinese opinion mining technology requires the support of annotated corpus for Chinese opinioned-subjective text. Since the corpus includes deep level information related to word segmentation, part-of-speech, syntax, semantics, opinioned elements, and s</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the Sentiment of Opinions. In Proceedings of the Conference on Computational Linguistics: 1367-1373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tianfang Yao</author>
<author>Xiwen Cheng</author>
<author>Feiyu Xu</author>
<author>Hans Uszkoreit</author>
<author>Rui Wang</author>
</authors>
<title>A Survey of Opinion Mining for Texts.</title>
<date>2008</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>22</volume>
<issue>3</issue>
<pages>71--80</pages>
<contexts>
<context position="1388" citStr="Yao et al, 2008" startWordPosition="190" endWordPosition="193">acquire useful opinioned information and knowledge in subjective texts (Liu et al, 2008). This technique has wide and many real world applications, such as e-commerce, business intelligence, information monitoring, public opinion poll, e-learning, newspaper and publication compilation, and business management. For instance, a typical opinion mining system produces statistical results from online product reviews, which can be used by potential customers when deciding which model to choose, by manufacturers to find out the possible areas of improvement, and by dealers for sales plan evaluation (Yao et al, 2008). According to Kim and Hovy (2004), an opinion is composed of four parts, namely, topic, holder, sentiment, and claim, in which the holder expresses the claim including positive or negative sentiment towards the topic. For example, in the sentence I like this car, I is the holder, like is the positive sentiment, car is the topic, and the whole sentence is the claim. Research on Chinese opinion mining technology requires the support of annotated corpus for Chinese opinioned-subjective text. Since the corpus includes deep level information related to word segmentation, part-of-speech, syntax, se</context>
</contexts>
<marker>Yao, Cheng, Xu, Uszkoreit, Wang, 2008</marker>
<rawString>Tianfang Yao, Xiwen Cheng, Feiyu Xu, Hans Uszkoreit, and Rui Wang. 2008. A Survey of Opinion Mining for Texts. Journal of Chinese Information Processing, 2008, 22(3): 71-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="7417" citStr="Vapnik, 1995" startWordPosition="1119" endWordPosition="1120">ng engine incorporate these annotated instances into the training set, and use the new training set to train the classifier. 5. Find out whether the performance of the classifier satisfies the preset standard. If not, go to step 2. 6. Use the classifier to identify the opinioned element in the unannotated dataset. 7. Convert the result into the required format. 3.2 Learning Engine The learning engine maintains the classifier by iteratively training classifiers with new training sets. The classifier adopted determines the up limit of the system performance. We use Support Vector Machine (SVM) (Vapnik, 1995) (Boser et al, 1992) (Chang and Lin, 1992) as the classifier for our system for its high generalization performance even with feature vectors of high dimension and its ability to manage kernel functions that map input data to higher dimensional space without increasing computational complexity. 3.3 Selecting Engine In our system, selecting engine picks up instances for human annotation, and puts the annotated instance into the training set. The strategy adopted when selecting training instance is critical to the overall performance of the active learning algorithm. A good strategy will more li</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>H Sebastian Seung</author>
<author>Eli Shamir</author>
<author>Naftali Tishby</author>
</authors>
<title>Selective Sampling Using the Query by Committee Algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--2</pages>
<contexts>
<context position="4913" citStr="Freund et al, 1997" startWordPosition="722" endWordPosition="725">roved high learning efficiency (Dagan and Engelson, 1995), it can be applied in fewer scenarios. Since constructing meaningful training instance without the knowledge of target concept is rather difficult. As to selective sampling, algorithm picks up training instances which can improve the performance of the classifier to the largest extent from a large variety of available instances. Algorithm in this class can be further divided into stream-based algorithm and pool-based algorithm according to how instances are saved (Long et al, 2008). For streambased algorithm (Engelson and Dagon, 1999) (Freund et al, 1997), unannotated instances are submitted to the system successively. All the instances not selected by the algorithms will be discarded. As to pool-based algorithm (Muslea et al, 2006) (McCallum and Nigam, 1998) (Lewis and Gail, 1994), the algorithm choose the most appropriate training instances from all the available instances. Instance not selected might have chance to be picked up in the next round. Though its computational complexity is higher, selective sampling is widely used as an active learning method for no prior knowledge of the target concept is required. Although much research has be</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Yoav Freund, H.Sebastian Seung, Eli Shamir, Naftali Tishby. 1997. Selective Sampling Using the Query by Committee Algorithm. Machine Learning, 28(2-3): 133-168</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
<author>Qiang Dong</author>
</authors>
<date>1999</date>
<note>HowNet. http://www.keenage.com</note>
<marker>Dong, Dong, 1999</marker>
<rawString>Zhendong Dong and Qiang Dong. 1999. HowNet. http://www.keenage.com</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>