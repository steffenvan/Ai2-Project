<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.977897">
A Probabilistic Corpus-Driven Model for Lexical-Functional Analysis
</title>
<author confidence="0.991884">
Rens Bod
</author>
<affiliation confidence="0.9962335">
Department of Computational Linguistics
University of Amsterdam
</affiliation>
<note confidence="0.5592745">
Spuistraat 134, NL-1012 VB Amsterdam
rens.bod @ let.uva.n1
</note>
<sectionHeader confidence="0.973094" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955">
We develop a Data-Oriented Parsing (DOP) model
based on the syntactic representations of Lexical-
Functional Grammar (LFG). We start by sum-
marizing the original DOP model for tree represen-
tations and then show how it can be extended with
corresponding functional structures. The resulting
LFG-DOP model triggers a new, corpus-based notion
of grammaticality, and its probability models exhibit
interesting behavior with respect to specificity and
the interpretation of ill-formed strings.
</bodyText>
<sectionHeader confidence="0.998142" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99747675">
Data-Oriented Parsing (DOP) models of natural
language embody the assumption that human
language perception and production works with
representations of past language experiences, rather
than with abstract grammar rules (cf. Bod 1992, 95;
Scha 1992; Sima&apos;an 1995; Rajman 1995). DOP
models therefore maintain large corpora of linguistic
representations of previously occurring utterances.
New utterances are analyzed by combining
(arbitrarily large) fragments from the corpus; the
occurrence-frequencies of the fragments are used to
determine which analysis is the most probable one. In
accordance with the general DOP architecture
outlined by Bod (1995), a particular DOP model is
described by specifying settings for the following four
parameters:
</bodyText>
<listItem confidence="0.968180777777778">
• a formal definition of a well-formed represen-
tation for utterance analyses,
• a set of decomposition operations that divide a
given utterance analysis into a set of fragments,
• a set of composition operations by which such
fragments may be recombined to derive an
analysis of a new utterance, and
• a definition of a probability model that indicates
how the probability of a new utterance analysis is
</listItem>
<bodyText confidence="0.967893941176471">
computed on the basis of the probabilities of the
fragments that combine to make it up.
Previous instantiations of the DOP architecture were
based on utterance-analyses represented as surface
phrase-structure trees (&amp;quot;Tree-DOP&amp;quot;, e.g. Bod 1993;
Rajman 1995; Sima&apos;an 1995; Goodman 1996;
Bonnema et al. 1997). Tree-DOP uses two
decomposition operations that produce connected
subtrees of utterance representations: (1) the Root
operation selects any node of a tree to be the root of
the new subtree and erases all nodes except the
selected node and the nodes it dominates; (2) the
Frontier operation then chooses a set (possibly
empty) of nodes in the new subtree different from its
root and erases all subtrees dominated by the chosen
nodes. The only composition operation used by Tree-
DOP is a node-substitution operation that replaces the
</bodyText>
<note confidence="0.93245725">
Ronald Kaplan
Xerox Palo Alto Research Center
3333 Coyote Hill Road
Palo Alto, California 94304
</note>
<email confidence="0.553983">
kaplan@parc.xerox.com
</email>
<bodyText confidence="0.9991861">
left-most nonterminal frontier node in a subtree with a
fragment whose root category matches the category of
the frontier node. Thus Tree-DOP provides tree-
representations for new utterances by combining
fragments from a corpus of phrase structure trees.
A Tree-DOP representation R can typically
be derived in many different ways. If each derivation
D has a probability P(D), then the probability of
deriving R is the sum of the individual derivation
probabilities:
</bodyText>
<equation confidence="0.972961">
P(R) = ED derives R P(D)
</equation>
<bodyText confidence="0.999004090909091">
A Tree-DOP derivation D = &lt;t1, t2 tk&gt; is produced
by a stochastic branching process. It starts by
randomly choosing a fragment t1 labeled with the
initial category (e.g. S). At each subsequent step, a
next fragment is chosen at random from among the
set of competitors for composition into the current
subtree. The process stops when a tree results with no
nonterminal leaves. Let CP(t I CS) denote the
probability of choosing a tree t from a competition set
CS containing t. Then the probability of a derivation
is
</bodyText>
<equation confidence="0.759586">
t2 tk&gt;) = CP(t, I CS,)
</equation>
<bodyText confidence="0.988351555555556">
where the competition probability CP(t I CS) is given
by
CP(t I CS) = P(t) I It&apos;E CS PO
Here, P(t) is the fragment probability for t in a given
corpus. Let Ti_i = t1 0 t2 0 ... 0 ti_i be the subanalysis
just before the ith step of the process, let LNC(Ti_j )
denote the category of the leftmost nonterminal of
Tii , and let r(t) denote the root category of a
fragment t. Then the competition set at the ith step is
</bodyText>
<equation confidence="0.544944">
CS, = ( t: r(t)=LNC(Ti_i ) 1
</equation>
<bodyText confidence="0.997449">
That is, the competition sets for Tree-DOP are
determined by the category of the leftmost
nonterminal of the current subanalysis. This is not the
only possible definition of competition set. As
Manning and Carpenter (1997) have shown, the
competition sets can be made dependent on the
composition operation. Their left-corner language
model would also apply to Tree-DOP, yielding a
different definition for the competition sets. But the
properties of such Tree-DOP models have not been
investigated.
Experiments with Tree-DOP on the Penn
Treebank and the OVIS corpus show a consistent
increase in parse accuracy when larger and more
complex subtrees are taken into account (cf. Bod
1993, 95, 98; Bonnema et al. 1997; Sekine &amp;
Grishman 1995; Sima&apos;an 1995). However, Tree-DOP
is limited in that it cannot account for underlying
syntactic (and semantic) dependencies that are not
</bodyText>
<page confidence="0.998288">
145
</page>
<bodyText confidence="0.999934428571429">
reflected directly in a surface tree. All modern
linguistic theories propose more articulated represen-
tations and mechanisms in order to characterize such
linguistic phenomena. DOP models for a number of
richer representations have been explored (van den
Berg et al. 1994; Tugwell 1995), but these approaches
have remained context-free in their generative power.
In contrast, Lexical-Functional Grammar (Kaplan &amp;
Bresnan 1982; Kaplan 1989), which assigns
representations consisting of a surface constituent tree
enriched with a corresponding functional structure, is
known to be beyond context-free. In the current work,
we develop a DOP model based on representations
defined by LFG theory (&amp;quot;LFG-DOP&amp;quot;). That is, we
provide a new instantiation for the four parameters of
the DOP architecture. We will see that this basic
LFG-DOP model triggers a new, corpus-based notion
of grammaticality, and that it leads to a different
class of its probability models which exhibit
interesting properties with respect to specificity and
the interpretation of ill-formed strings.
</bodyText>
<sectionHeader confidence="0.9966855" genericHeader="method">
2. A DOP model based on Lexical-Functional
representations
</sectionHeader>
<subsectionHeader confidence="0.836636">
Representations
</subsectionHeader>
<bodyText confidence="0.999070333333333">
The definition of a well-formed representation for
utterance-analyses follows from LFG theory, that is,
every utterance is annotated with a c-structure, an f-
structure and a mapping 4) between them. The c-
structure is a tree that describes the surface
constituent structure of an utterance; the f-structure is
an attribute-value matrix marking the grammatical
relations of subject, predicate and object, as well as
providing agreement features and semantic forms; and
4) is a correspondence function that maps nodes of the
c-structure into units of the f-structure (Kaplan &amp;
Bresnan 1982; Kaplan 1989). The following figure
shows a representation for the utterance Kim eats.
(We leave out some features to keep the example
simple.)
</bodyText>
<equation confidence="0.99122175">
(1)
SUBJ
TENSE PRES
PRED &apos;eat(SUBJ)&apos;
</equation>
<bodyText confidence="0.997476552631579">
Note that the 4) correspondence function gives an
explicit characterization of the relation between the
superficial and underlying syntactic properties of an
utterance, indicating how certain parts of the string
carry information about particular units of underlying
structure. As such, it will play a crucial role in our
definition for the decomposition and composition
operations of LFG-DOP. In (1) we see for instance
that the NP node maps to the subject f-structure, and
the S and VP nodes map to the outermost f-structure.
It is generally the case that the nodes in a
subtree carry information only about the f-structure
units that the subtree&apos;s root gives access to. The
notion of accessibility is made precise in the
following definition:
An f-structure unit f is 0-accessible from a node n
iff either n is 0-linked to f (that is, f= 0(n) ) orf
is contained within 4)(n) (that is, there is a chain
of attributes that leads from 4)(n) to f).
All the f-structure units in (1) are 0-accessible from
for instance the S node and the VP node, but the
TENSE and top-level PRED are not 0-accessible from
the NP node.
According to LFG theory, c-structures and f-
structures must satisfy certain formal well-formedness
conditions. A c-structure/f-structure pair is a valid
LFG representation only if it satisfies the Non-
branching Dominance, Uniqueness, Coherence and
Completeness conditions (Kaplan &amp; Bresnan 1982).
Nonbranching Dominance demands that no c-structure
category appears twice in a nonbranching dominance
chain; Uniqueness asserts that there can be at most
one value for any attribute in the f-structure;
Coherence prohibits the appearance of grammatical
functions that are not governed by the lexical
predicate; and Completeness requires that all the
functions that a predicate governs appear as attributes
in the local f-structure.
</bodyText>
<subsectionHeader confidence="0.940486">
Decomposition operations
</subsectionHeader>
<bodyText confidence="0.990626666666667">
Many different DOP models are compatible with the
system of LFG representations. In this paper we
outline a basic LFG-DOP model which extends the
operations of Tree-DOP to take correspondences and
f-structure features into account. The decomposition
operations for this model will produce fragments of
the composite LFG representations. These will consist
of connected subtrees whose nodes are in 0 -
correspondence with sub-units of f-structures. We
extend the Root and Frontier decomposition opera-
tions of Tree-DOP so that they also apply to the nodes
of the c-structure while respecting the fundamental
principles of c-structure/f-structure correspondence.
When a node is selected by the Root
operation, all nodes outside of that node&apos;s subtree are
erased, just as in Tree-DOP. Further, for LFG-DOP,
all 4) links leaving the erased nodes are removed and
all f-structure units that are not 0-accessible from the
remaining nodes are erased. Root thus maintains the
intuitive correlation between nodes and the
information in their corresponding f-structures. For
example, if Root selects the NP in (1), then the f-
structure corresponding to the S node is erased, giving
(2) as a possible fragment:
(2)
In addition the Root operation deletes from the
remaining f-structure all semantic forms that are local
to f-structures that correspond to erased c-structure
nodes, and it thereby also maintains the fundamental
two-way connection between words and meanings.
Thus, if Root selects the VP node so that the NP is
erased, the subject semantic form &amp;quot;Kim&amp;quot; is also
deleted:
</bodyText>
<figure confidence="0.929868222222222">
IPRED Kim
NUM SG
I PRED &apos;Kim&amp;quot;
NUM SG
146
(3)
SUM
VP TENSE
eats PRED
</figure>
<bodyText confidence="0.999660272727273">
As with Tree-DOP, the Frontier operation then
selects a set of frontier nodes and deletes all subtrees
they dominate. Like Root, it also removes the 0 links
of the deleted nodes and erases any semantic form
that corresponds to any of those nodes. Frontier does
not delete any other f-structure features. This reflects
the fact that all features are 0-accessible from the
fragment&apos;s root even when nodes below the frontier
are erased. For instance, if the VP in (1) is selected
as a frontier node, Frontier erases the predicate
&amp;quot;eat(SUBJ)&amp;quot; from the fragment:
</bodyText>
<figure confidence="0.918429">
(4) PRED
SUBJ I
NUM SG
NP VP - TENSE PRES
I
Kim
</figure>
<bodyText confidence="0.999937166666667">
Note that the Root and Frontier operations retain the
subject&apos;s NUM feature in the VP-rooted fragment (3),
even though the subject NP is not present. This
reflects the fact, usually encoded in particular
grammar rules or lexical entries, that verbs of English
carry agreement features for their subjects. On the
other hand, fragment (4) retains the predicate&apos;s
TENSE feature, reflecting the possibility that English
subjects might also carry information about their
predicate&apos;s tense. Subject-tense agreement as
encoded in (4) is a pattern seen in some languages
(e.g. the split-ergativity pattern of languages like
Hindi, Urdu and Georgian) and thus there is no
universal principle by which fragments such as (4)
can be ruled out. But in order to represent directly the
possibility that subject-tense agreement is not a
dependency of English, we also allow an S fragment
in which the TENSE feature is deleted, as in (5).
</bodyText>
<equation confidence="0.9584465">
(5)
I
PRED &apos;Kim&apos;
NUM SG
1P VP
Kim
</equation>
<bodyText confidence="0.9989725625">
Fragment (5) is produced by a third decomposition
operation, Discard, defined to construct generali-
zations of the fragments supplied by Root and
Frontier. Discard acts to delete combinations of
attribute-value pairs subject to the following
restriction: Discard does not delete pairs whose
values 0-correspond to remaining c-structure nodes.
This condition maintains the essential
correspondences of LFG representations: if a c-
structure and an f-structure are paired in one fragment
provided by Root and Frontier, then Discard also
pairs that c-structure with all generalizations of that
fragment&apos;s f-structure. Fragment (5) results from
applying Discard to the TENSE feature in (4).
Discard also produces fragments such as (6), where
the subject&apos;s number in (3) has been deleted:
</bodyText>
<equation confidence="0.901729666666667">
TENSE PRES
SUBSII
PRED &apos;eat(SUBJ)&apos; I
</equation>
<bodyText confidence="0.999916666666667">
Again, since we have no language-specific knowled-
ge apart from the corpus, we have no basis for ruling
out fragments like (6). Indeed, it is quite intuitive to
omit the subject&apos;s number in fragments derived from
sentences with past-tense verbs or modals. Thus the
specification of Discard reflects the fact that LFG
representations, unlike LFG grammars, do not
indicate unambiguously the c-structure source (or
sources) of their f-structure feature values.
</bodyText>
<subsectionHeader confidence="0.623007">
The composition operation
</subsectionHeader>
<bodyText confidence="0.9992863">
In LFG-DOP the operation for combining fragments,
again indicated by 0, is carried out in two steps. First
the c-structures are combined by left-most substitution
subject to the category-matching condition, just as in
Tree-DOP. This is followed by the recursive
unification of the f-structures corresponding to the
matching nodes. The result retains the 4&gt;
correspondences of the fragments being combined. A
derivation for an LFG-DOP representation R is a
sequence of fragments the first of which is labeled
with S and for which the iterative application of the
composition operation produces R.
We show in (7) the effect of the LFG
composition operation using two fragments from
representations of an imaginary corpus containing the
sentences Kim eats and People ate. The VP-rooted
fragment is substituted for the VP in the first
fragment, and the second f-structure unifies with the
first f-structure, resulting in a representation for the
new sentence Kim ate.
</bodyText>
<sectionHeader confidence="0.869103" genericHeader="method">
SUBS I I
</sectionHeader>
<table confidence="0.546468333333333">
TENSE PAST
PRED &apos;eat(suBi)&apos;
SUBS IPRED &apos;Kim&apos;l
NUM SG
TENSE PAST
PRED &apos;eat(SUBJ)&apos;
</table>
<bodyText confidence="0.998678">
This representation satisfies the well-formedness
conditions and is therefore valid. Note that in LFG-
DOP, as in Tree-DOP, the same representation may
be produced by several derivations involving different
fragments.
</bodyText>
<figure confidence="0.794869636363636">
[ NUM SG 1
PRES
&apos;eat(SUBJ).
1 SUBS
VP
eats
NP VP—&apos; SUBS I PRED
NUM
Kim
VP
ate
</figure>
<page confidence="0.9824">
147
</page>
<bodyText confidence="0.999726111111111">
Another valid representation for the sentence Kim ate
could be composed from a fragment for Kim that does
not preserve the number feature, leading to a
representation which is unmarked for number. The
probability models we discuss below have the
desirable property that they tend to assign higher
probabilities to more specific representations.
The following derivation produces a valid
representation for the intuitively ungrammatical
</bodyText>
<figure confidence="0.69203625">
sentence People eats:
(8)
INP VP
people
</figure>
<bodyText confidence="0.999667333333333">
This system of fragments and composition thus
provides a representational basis for a robust model of
language comprehension in that it assigns at least
some representations to many strings that would
generally be regarded as ill-formed. A correlate of this
advantage, however, is the fact that it does not offer a
direct formal account of metalinguistic judgments of
grammaticality. Nevertheless, we can reconstruct the
notion of grammaticality by means of the following
definition:
A sentence is grammatical with respect to a corpus
if and only if it has at least one valid
representation with at least one derivation whose
fragments are produced only by Root and Frontier
and not by Discard.
Thus the system is robust in that it assigns three
representations (singular, plural, and unmarked as the
subject&apos;s number) to the string People eats, based on
fragments for which the number feature of people,
eats, or both has been discarded. But unless the
corpus contains non-plural instances of people or non-
singular instances of eats, there will be no Discard-
free derivation and the string will be classified as
ungrammatical (with respect to the corpus).
</bodyText>
<subsectionHeader confidence="0.892873">
Probability models
</subsectionHeader>
<bodyText confidence="0.999479833333333">
As in Tree-DOP, an LFG-DOP representation R can
typically be derived in many different ways. If each
derivation D has a probability P(D ), then the
probability of deriving R is again the probability of
producing it by any of its derivations. This is the sum
of the individual derivation probabilities:
</bodyText>
<listItem confidence="0.798475">
(9) P(R) = ED derives R P(D)
</listItem>
<bodyText confidence="0.974519571428571">
An LFG-DOP derivation is also produced by a
stochastic branching process which at each step
makes a random selection from a competition set of
competing fragments. Let CP(f I CS) denote the
probability of choosing a fragment f from a
competition set CS containing f then the probability
of a derivation D = f2 ...fk&gt; is
</bodyText>
<equation confidence="0.823423">
(10) P(&lt;fj,f2 fk&gt;) = CP(fi I CSi)
</equation>
<bodyText confidence="0.99936">
where as in Tree-DOP, CP(f I CS) is expressed in
terms of fragment probabilities P(f) by the formula
</bodyText>
<listItem confidence="0.912317">
(11) CP(f I CS) = 13(f) I life CS P(f)
</listItem>
<bodyText confidence="0.996370769230769">
Tree-DOP is the special case where there are no
conditions of validity other than the ones that are
enforced at each step of the stochastic process by the
composition operation. This is not generally the case
and is certainly not the case for the Completeness
Condition of LFG representations: Completeness is a
property of a final representation that cannot be
evaluated at any intermediate steps of the process.
However, we can define probabilities for the valid
representations by sampling only from such
representations in the output of the stochastic process.
The probability of sampling a particular valid
representation R is given by
</bodyText>
<listItem confidence="0.735137">
(12) P(R I R is valid) = P(R) / ER&apos; is valid P(R&apos;)
</listItem>
<bodyText confidence="0.8673216">
This formula assigns probabilities to valid represent-
ations whether or not the stochastic process
guarantees validity. The valid representions for a
particular utterance u are obtained by a further
sampling step and their probabilities are given by:
</bodyText>
<equation confidence="0.900346">
(13) P(R I R is valid and yields u)=
P(R) / ER is valid and yields u P(R&apos;)
</equation>
<bodyText confidence="0.999952904761905">
The formulas (9) through (13) will be part of any
LFG-DOP probability model. The models will differ
only in how the competition sets are defined, and this
in turn depends on which well-formedness conditions
are enforced on-line during the stochastic branching
process and which are evaluated by the off-line
validity sampling process.
One model, which we call Ml, is a straight-
forward extension of Tree-DOP&apos;s probability model.
This computes the competition sets only on the basis
of the category-matching condition, leaving all other
well-formedness conditions for off-line sampling. Thus
for M1 the competition sets are defined simply in
terms of the categories of a fragment&apos;s c-structure root
node. Suppose that Fi..1 =fl of2 0 ... 0 fi_i is the
current subanalysis at the beginning of step i in the
process, that LNC(Fi_1) denotes the category of the
leftmost nonterminal node of the c-structure of Fi_i ,
and that r(f) is now interpreted as the root-node
category of f s c-structure component. Then the
competition set for the ith step is
</bodyText>
<listItem confidence="0.801695">
(14) CS, = { f : r(f)=LNC(Fi_i) }
</listItem>
<bodyText confidence="0.997614666666667">
Since these competition sets depend only on the
category of the leftmost nonterminal of the current c-
structure, the competition sets group together all
fragments with the same root category, independent
of any other properties they may have or that a
particular derivation may have. The competition
</bodyText>
<figure confidence="0.999065214285714">
I I
VP
TENSE PRES
eats FRED eat( SUB;)&apos;
SUBJ FRED &apos;people&amp;quot;
NUM PL
NP VP
people eats
I SUBJ
TENSE PRES
FRED &apos;eat(SUBJ)&apos;
I FRED &apos;people]]
NUM PL
SUB;
</figure>
<page confidence="0.983188">
148
</page>
<bodyText confidence="0.962917">
probability for a fragment can be expressed by the
formula
</bodyText>
<equation confidence="0.519255">
(15) CP(f) = P(f) IEf: op=r0 P(f)
</equation>
<bodyText confidence="0.989917833333333">
We see that the choice of a fragment at a particular
step in the stochastic process depends only on the
category of its root node; other well-formedness
properties of the representation are not used in
making fragment selections. Thus, with this model the
stochastic process may produce many invalid
representations; we rely on sampling of valid
representations and the conditional probabilities given
by (12) and (13) to take the Uniqueness, Coherence,
and Completeness Conditions into account.
Another possible model (M2) defines the
competition sets so that they take a second condition,
Uniqueness, into account in addition to the root node
category. For M2 the competing fragments at a
particular step in the stochastic derivation process are
those whose c-structures have the same root node
category as LNC(Fi_i ) and also whose f-structures are
consistently unifiable with the f-structure of Fi_i. Thus
the competition set for the ith step is
(16) CSi = f: r(f)=LNC(Fi.1) and f is unifiable
with the f-structure of Fj_i}
Although it is still the case that the category-
matching condition is independent of the derivation,
the unifiability requirement means that the
competition sets vary according to the representation
produced by the sequence of previous steps in the
stochastic process. Unifiability must be determined at
each step in the process to produce a new
competition set, and the competition probability
remains dependent on the particular step:
</bodyText>
<equation confidence="0.859127">
(17) CP( fi I CS i) =
P(fi)I r(f)=-1-(fi) and [is unifiable with Fi.1 P(i)
</equation>
<bodyText confidence="0.986622171428572">
On this model we again rely on sampling and the
conditional probabilities (12) and (13) to take just the
Coherence and Completeness Conditions into
account.
In model M3 we define the stochastic process
to enforce three conditions, Coherence, Uniqueness
and category-matching, so that it only produces
representations with well-formed c-structures that
correspond to coherent and consistent f-structures. The
competition probabilities for this model are given by
the obvious extension of (17). It is not possible,
however, to construct a model in which the
Completeness Condition is enforced during the
derivation process. This is because the satisfiability of
the Completeness Condition depends not only on the
results of previous steps of a derivation but also on
the following steps (see Kaplan &amp; Bresnan 1982).
This nonmonotonic property means that the
appropriate step-wise competition sets cannot be
defined and that this condition can only be enforced
at the final stage of validity sampling.
In each of these three models the category-
matching condition is evaluated on-line during the
derivation process while other conditions are either
evaluated on-line or off-line by the after-the-fact
sampling process. LFG-DOP is crucially different
from Tree-DOP in that at least one validity
requirement, the Completeness Condition, must
always be left to the post-derivation process. Note
that a number of other models are possible which
enforce other combinations of these three conditions.
3. Illustration and properties of LFG-DOP
We illustrate LFG-DOP using a very small corpus
consisting of the two simplified LFG representations
shown in (18):
</bodyText>
<figure confidence="0.710565833333333">
(18)
SUBJ
PRED
SUBJ
PRED
people wallted
</figure>
<bodyText confidence="0.999854">
The fragments from this corpus can be composed to
provide representations for the two observed
sentences plus two new utterances, John walked and
People fell. This is sufficient to demonstrate that the
probability models MI and M2 assign different
probabilities to particular representations. We have
omitted the TENSE feature and the lexical categories
N and V to reduce the number of the fragments we
have to deal with. Applying the Root and Frontier
operators systematically to the first corpus
representation produces the fragments in the first
column of (19), while the second column shows the
additional f-structure that is associated with each c-
structure by the Discard operation.
A total of 12 fragments are produced from
this representation, and by analogy 12 fragments with
either PL or unmarked NUM values will also result
from People walked. Note that the [S NP VP]
fragment with the unspecified NUM value is produced
for both sentences and thus its corpus frequency is 2.
There are 14 other S-rooted fragments, 4 NP-rooted
fragments, and 4 VP-rooted fragments; each of these
occurs only once.
These fragments can be used to derive three
different representations for John walked (singular,
plural, and unmarked as the subject&apos;s number). To
facilitate the presentation of our derivations and
probability calculations, we denote each fragment by
an abbreviated name that indicates its c-structure
root-node category, the sequence of its frontier-node
labels, and whether its subject&apos;s number is SG, PL, or
unmarked (indicated by U). Thus the first fragment in
(19) is referred to as S/John-fell/SG and the unmarked
fragment that Discard produces from it is referred to
as S/John-fell/U. Given this naming convention, we
can specify one of the derivations for John walked by
the expression S/NP-VP/U 0 NP/John/SG 0
VP/walked/U, corresponding to an analysis in which
the subject&apos;s number is marked as SG. The fragment
VP/walked/U of course comes from People walked,
</bodyText>
<figure confidence="0.745993090909091">
NP VP
PRED &apos;John]
NUM SG
&apos;fall(SUBJ)&apos;
IPRED people
NUM PL
&apos;walk(susi).
149
the second corpus sentence, and does not appear in
(19).
(19)
</figure>
<table confidence="0.970077416666667">
NIP VP IPRED John SUBJ [FRED &apos;John]]
John fell SUBJ PRED fall(SUBJ).
NUM SG
PRED lall(SUBJ).
1,&lt;*ii•■•••• [s
FRED &apos;John&amp;quot;&apos;
[ SUBJ I NUM SG I SUBJ I
PRED fall(SUBJf PRED &apos;fall(SUBJr
SUBJ 1 PRED &apos;fall(SUBJY I
FRED I NUM SG
fall(suBa
[ PEED &apos;John]
</table>
<bodyText confidence="0.9970493125">
Model MI evaluates only the Tree-DOP root-category
condition during the stochastic branching process, and
the competition sets are fixed independent of the
derivation. The probability of choosing the fragment
S/NP-VP/U, given that an S-rooted fragment is
required, is always 2/16, its frequency divided by the
sum of the frequencies of all the S fragments.
Similarly, the probability of then choosing
NP/John/SG to substitute at the NP frontier node is
1/4, since the NP competition set contains 4
fragments each with frequency 1. Thus, under model
M1 the probability of producing the complete
derivation S/NP-VP/U o NP/John/SG o VP/walked/U
is 2/16x1/4x1/4=2/256. This probability is small
because it indicates the likelihood of this derivation
compared to other derivations for John walked and for
the three other analyzable strings. The computation of
the other MI derivation probabilities for John walked
is left to the reader. There are 5 different derivations
for the representation with SG number and 5 for the
PL number, while there are only 3 ways of producing
the unmarked number U. The conditional probabilities
for the particular representations (SG, PL, U) can be
calculated by (9) and (13), and are given below.
P(NUM=SG I valid and yield = John walked) = .353
P(NUM=PL I valid and yield = John walked) = .353
P(NUM=U I valid and yield = John walked). .294
We see that the two specific representations are
equally likely and each of them is more probable than
the representation with unmarked NUM.
Model M2 produces a slightly different
distribution of probabilities. Under this model, the
consistency requirement is used in addition to the
root-category matching requirement to define the
competition sets at each step of the branching
process. This means that the first fragment that
instantiates the NUM feature to either SG or PL
constrains the competition sets for the following
choices in a derivation. Thus, having chosen the
NP/John/SG fragment in the derivation S/NP-VP/U
NP/John/SG 0 VP/walked/U, only 3 VP fragments
instead of 4 remain in the competition set at the next
step, since the VP/walked/PL fragment is no longer
available. The probability for this derivation under
model M2 is therefore 2/16x1/4x1/3=2/192, slightly
higher than the probability assigned to it by Ml.
Table 1 shows the complete set of derivations and
their M2 probabilities for John walked.
</bodyText>
<table confidence="0.978127368421053">
S/NP-VP/U &amp;quot;NP/John/SO 0 VP/walked/U SG 2/16 x 1/4 x 1/3
S/NP-VP/SG ° NP/John/SG n VP/walked/U SG 1/16 x 1/3 x 1/3
S/NP-VP/SG 0 NP/John/U ° VP/walked/U SG 1/16 x 1/3 x 1/3
S/NP-walked/U ° NP/John/SG SG 1/16 x 1/4
S/John-VP/SG ° VP/walked/U SG 1/16 x 1/3
P(NUM=SG and yield = John walked) = 35/576 = .061
P(NUM=SG I valid and yield = John walked)= 70/182 = .38
S/NP-VP/U o NP/John/U o VP/walked/PL PL 2/16 x 1/4 x 1/4
S/NP-VP/PL &amp;quot;NP/John/U oVP/walked/PL PL 1/16 x 1/3 x 1/3
S/NP-VP/PL 0 NP/John/U 0 VP/walked/U PL 1/16 x 1/3 x 1/3
S/NP-walked/PL 0 NP/John/U PL 1/16 x 1/3
S/John-VP/U ° VP/walked/PL PL 1/16 x 1/4
P(NUM=PL and yield = John walked) = 33.5/576 = .058
P(NUM=PL I valid and yield = John walked) = 67/182 = .37
S/NP-VP/U o NP/John/U &amp;quot;VP/walked/U U 2/16 x 1/4 x 1/4
S/NP-walked/U NP/John/U U 1/16 x 1/4
S/John-VP/U o VP/walked/U U 1/16 x 1/4
P(NUM=U and yield = John walked) = 22.5/576 = .039
P(NUM=U I valid and yield = John walked) = 45/182 = .25
</table>
<tableCaption confidence="0.9773445">
Table 1: Model M2 derivations, subject number features,
and probabilities for John walked
</tableCaption>
<bodyText confidence="0.9992235625">
The total probability for the derivations that produce
John walked is A58, and the conditional probabilities
for the three representations are:
P(NUM=SG I valid and yield = John walked) = .38
P(NUM=PL I valid and yield = John walked) = .37
P(NUM=U I valid and yield = John walked) = .25
For model M2 the unmarked representation is less
likely than under MI, and now there is a slight bias in
favor of the value SG over PL. The SG value is
favored because it is carried by substitutions for the
left-most word of the utterance and thus reduces
competition for subsequent choices. The value PL
would be more probable for the sentence People fell.
Thus both models give higher probability to the more
specific representations. Moreover, M1 assigns the
same probability to SG and PL, whereas M2 doesn&apos;t.
</bodyText>
<figure confidence="0.9993314">
I soil
PRED &apos;John]]
NUM SG
I SUBJ
I SUBJ
NP VP
fell
tel
IPRED John
NUM SG
</figure>
<page confidence="0.981701">
150
</page>
<bodyText confidence="0.99983240625">
M2 reflects a left-to-right bias (which might be
psycholinguistically interesting -- a so-called primacy
effect), whereas M1 is, like Tree-DOP, order indepen-
dent.
It turns out that all LFG-DOP probability
models (MI, M2 and M3) display a preference for the
most specific representation. This preference partly
depends on the number of derivations: specific
representations tend to have more derivations than
generalized (i.e., unmarked) representations, and
consequently tend to get higher probabilities -- other
things being equal. However, this preference also
depends on the number of feature values: the more
feature values, the longer the minimal derivation
length must be in order to get a preference for the
most specific representation (Cormons, forthcoming).
The bias in favor of more specific represen-
tations, and consequently fewer Discard-produced
feature generalizations, is especially interesting for
the interpretation of ill-formed input strings. Bod &amp;
Kaplan (1997) show that in analyzing an intuitively
ungrammatical string like These boys walks, there is a
probabilistic accumulation of evidence for the plural
interpretation over the singular and unmarked one (for
all models Ml, M2 and M3). This is because both
These and boys carry the PL feature while only walks
is a source for the SG feature, leading to more
derivations for the PL reading of These boys walks. In
case of &amp;quot;equal evidence&amp;quot; as in the ill-formed string
Boys walks, model MI assigns the same probability to
PL and SG, while models M2 and M3 prefer the PL
interpretation due to their left-to-right bias.
</bodyText>
<sectionHeader confidence="0.964534" genericHeader="conclusions">
4. Conclusion and computational issues
</sectionHeader>
<bodyText confidence="0.999941357142857">
Previous DOP models were based on context-free tree
representations that cannot adequately represent all
linguistic phenomena. In this paper, we gave a DOP
model based on the more articulated representations
provided by LFG theory. LFG-DOP combines the
advantages of two approaches: the linguistic
adequacy of LFG together with the robustness of
DOP. LFG-DOP triggers a new, corpus-based notion
of grammaticality, and its probability models exhibit
a preference for the most specific analysis containing
the fewest number of feature generalizations.
The main goal of this paper was to provide
the theoretical background of LFG-DOP. As to the
computational aspects of LFG-DOP, the problem of
finding the most probable representation of a sentence
is NP-hard even for Tree-DOP. This problem may be
tackled by Monte Carlo sampling techniques (as in
Tree-DOP, cf. Bod 1995) or by computing the Viterbi
n best derivations of a sentence. Other optimization
heuristics may consist of restricting the fragment
space, for example by putting an upper bound on the
fragment depth, or by constraining the decomposition
operations. To date, a couple of LFG-DOP implemen-
tations are either operational (Cormons, forthcoming)
or under development, and corpora with LFG
representations have recently been developed (at
XRCE France and Xerox PARC). Experiments with
these corpora will be presented in due time.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.985479">
We thank Joan Bresnan, Mary Dalrymple, Mark
Johnson, Martin Kay, John Maxwell, Remko Scha,
Khalil Sima&apos;an, Andy Way and three anonymous
reviewers for helpful comments. We are most grateful
to Boris Cormons whose comments were particularly
helpful. This research was supported by NWO, the
Dutch Organization for Scientific Research. The
initial stages of this work were carried out while the
second author was a Fellow of the Netherlands
Institute for Advanced Study (NIAS). Subsequent
stages were also carried out while the first author was
a Consultant at Xerox PARC.
</bodyText>
<sectionHeader confidence="0.998875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999308666666667">
M. van den Berg, R. Bod and R. Scha 1994. &amp;quot;A Corpus-
Based Approach to Semantic Interpretation&amp;quot;, Proceedings
Ninth Amsterdam Colloquium, Amsterdam, The Netherlands.
R. Bod 1992. &amp;quot;A Computational Model of Language
Performance: Data Oriented Parsing&amp;quot;, Proceedings
COLING-92, Nantes, France.
R. Bod 1993. &amp;quot;Using an Annotated Corpus as a Stochastic
Grammar&amp;quot;, Proceedings EACL&apos;93 , Utrecht, The Netherlands.
R. Bod 1995. Enriching Linguistics with Statistics:
Performance Models of Natural Language, ILLC
Dissertation Series 1995-14, University of Amsterdam
R. Bod 1998. &amp;quot;Spoken Dialogue Interpretation with the DOP
Model&amp;quot;, this proceedings.
R. Bod and R. Kaplan 1997. &amp;quot;On Performance models for
Lexical-Functional Analysis&amp;quot;, Paper presented at the
Computational Psycholinguistics Conference 1997, Berkeley
(Ca).
R. Bonnema, R. Bod and R. Scha 1997. &amp;quot;A DOP Model for
Semantic Interpretation&amp;quot;, Proceedings ACL/EACL-97,
Madrid, Spain.
B. Cormons, forthcoming. Analyse et desambiguisation: Une
approche purement a base de corpus (Data-Oriented Parsing)
pour le formalisme des Grammaires Lexicales
Fonctionnelles, PhD thesis, Universite de Rennes, France.
J. Goodman 1996. &amp;quot;Efficient Algorithms for Parsing the DOP
Model&amp;quot;, Proceedings Empirical Methods in Natural
Language Processing, Philadelphia, Pennsylvania.
R. Kaplan 1989. &amp;quot;The Formal Architecture of Lexical-
Functional Grammar&amp;quot;, Journal of Information Science and
Engineering, vol. 5, 305-322.
R. Kaplan and J. Bresnan 1982. &amp;quot;Lexical-Functional
Grammar: A Formal System for Grammatical
Representation&amp;quot;, in J. Bresnan (ed.), The Mental
Representation of Grammatical Relations, The MIT Press,
Cambridge, MA.
C. Manning and B. Carpenter 1997. &amp;quot;Probabilistic parsing
using left corner language models&amp;quot;, Proceedings IWPT&apos;97,
Boston (Mass.).
M. Rajman 1995. &amp;quot;Approche Probabiliste de l&apos;Analyse
Syntaxique&amp;quot;, Traitement Automatique des Langues, vol.
36(1-2).
R. Scha 1992. &amp;quot;Virtuele Grammatica&apos;s en Creatieve
Algoritmen&amp;quot;, Gramma/l11 1(1).
S. Sekine and R. Grishman 1995. &amp;quot;A Corpus-based
Probabilistic Grammar with Only Two Non-terminals&amp;quot;,
Proceedings Fourth International Workshop on Parsing
Technologies, Prague, Czech Republic.
K. Sima&apos;an 1995. &amp;quot;An optimized algorithm for Data Oriented
Parsing&amp;quot;, in R. Mitkov and N. Nicolov (eds.), Recent
Advances in Natural Language Processing 1995, John
Benjamins, Amsterdam.
D. Tugwell 1995. &amp;quot;A State-Transition Grammar for Data-
Oriented Parsing&amp;quot;, Proceedings European Chapter of the
ACL&apos;95 , Dublin, Ireland.
</reference>
<page confidence="0.998262">
151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.384532">
<title confidence="0.999756">A Probabilistic Corpus-Driven Model for Lexical-Functional Analysis</title>
<author confidence="0.98597">Rens Bod</author>
<affiliation confidence="0.9997">Department of Computational Linguistics University of Amsterdam</affiliation>
<address confidence="0.996129">Spuistraat 134, NL-1012 VB Amsterdam</address>
<email confidence="0.398888">rens.bod@let.uva.n1</email>
<abstract confidence="0.998120909090909">We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical- Functional Grammar (LFG). We start by summarizing the original DOP model for tree representations and then show how it can be extended with corresponding functional structures. The resulting LFG-DOP model triggers a new, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M van den Berg</author>
<author>R Bod</author>
<author>R Scha</author>
</authors>
<title>A CorpusBased Approach to Semantic Interpretation&amp;quot;,</title>
<date>1994</date>
<booktitle>Proceedings Ninth Amsterdam Colloquium,</booktitle>
<location>Amsterdam, The</location>
<marker>van den Berg, Bod, Scha, 1994</marker>
<rawString>M. van den Berg, R. Bod and R. Scha 1994. &amp;quot;A CorpusBased Approach to Semantic Interpretation&amp;quot;, Proceedings Ninth Amsterdam Colloquium, Amsterdam, The Netherlands. R. Bod 1992. &amp;quot;A Computational Model of Language Performance: Data Oriented Parsing&amp;quot;, Proceedings COLING-92, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Using an Annotated Corpus as a Stochastic Grammar&amp;quot;,</title>
<date>1993</date>
<booktitle>Proceedings EACL&apos;93 ,</booktitle>
<location>Utrecht, The</location>
<contexts>
<context position="2105" citStr="Bod 1993" startWordPosition="304" endWordPosition="305">d representation for utterance analyses, • a set of decomposition operations that divide a given utterance analysis into a set of fragments, • a set of composition operations by which such fragments may be recombined to derive an analysis of a new utterance, and • a definition of a probability model that indicates how the probability of a new utterance analysis is computed on the basis of the probabilities of the fragments that combine to make it up. Previous instantiations of the DOP architecture were based on utterance-analyses represented as surface phrase-structure trees (&amp;quot;Tree-DOP&amp;quot;, e.g. Bod 1993; Rajman 1995; Sima&apos;an 1995; Goodman 1996; Bonnema et al. 1997). Tree-DOP uses two decomposition operations that produce connected subtrees of utterance representations: (1) the Root operation selects any node of a tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates; (2) the Frontier operation then chooses a set (possibly empty) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes. The only composition operation used by TreeDOP is a node-substitution operation that replaces the Rona</context>
<context position="4979" citStr="Bod 1993" startWordPosition="790" endWordPosition="791">he leftmost nonterminal of the current subanalysis. This is not the only possible definition of competition set. As Manning and Carpenter (1997) have shown, the competition sets can be made dependent on the composition operation. Their left-corner language model would also apply to Tree-DOP, yielding a different definition for the competition sets. But the properties of such Tree-DOP models have not been investigated. Experiments with Tree-DOP on the Penn Treebank and the OVIS corpus show a consistent increase in parse accuracy when larger and more complex subtrees are taken into account (cf. Bod 1993, 95, 98; Bonnema et al. 1997; Sekine &amp; Grishman 1995; Sima&apos;an 1995). However, Tree-DOP is limited in that it cannot account for underlying syntactic (and semantic) dependencies that are not 145 reflected directly in a surface tree. All modern linguistic theories propose more articulated representations and mechanisms in order to characterize such linguistic phenomena. DOP models for a number of richer representations have been explored (van den Berg et al. 1994; Tugwell 1995), but these approaches have remained context-free in their generative power. In contrast, Lexical-Functional Grammar (K</context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>R. Bod 1993. &amp;quot;Using an Annotated Corpus as a Stochastic Grammar&amp;quot;, Proceedings EACL&apos;93 , Utrecht, The Netherlands. R. Bod 1995. Enriching Linguistics with Statistics: Performance Models of Natural Language, ILLC Dissertation Series 1995-14, University of Amsterdam R. Bod 1998. &amp;quot;Spoken Dialogue Interpretation with the DOP Model&amp;quot;, this proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
<author>R Kaplan</author>
</authors>
<title>On Performance models for Lexical-Functional Analysis&amp;quot;,</title>
<date>1997</date>
<booktitle>Paper presented at the Computational Psycholinguistics Conference</booktitle>
<location>Berkeley (Ca).</location>
<contexts>
<context position="30798" citStr="Bod &amp; Kaplan (1997)" startWordPosition="4933" endWordPosition="4936">entations tend to have more derivations than generalized (i.e., unmarked) representations, and consequently tend to get higher probabilities -- other things being equal. However, this preference also depends on the number of feature values: the more feature values, the longer the minimal derivation length must be in order to get a preference for the most specific representation (Cormons, forthcoming). The bias in favor of more specific representations, and consequently fewer Discard-produced feature generalizations, is especially interesting for the interpretation of ill-formed input strings. Bod &amp; Kaplan (1997) show that in analyzing an intuitively ungrammatical string like These boys walks, there is a probabilistic accumulation of evidence for the plural interpretation over the singular and unmarked one (for all models Ml, M2 and M3). This is because both These and boys carry the PL feature while only walks is a source for the SG feature, leading to more derivations for the PL reading of These boys walks. In case of &amp;quot;equal evidence&amp;quot; as in the ill-formed string Boys walks, model MI assigns the same probability to PL and SG, while models M2 and M3 prefer the PL interpretation due to their left-to-rig</context>
</contexts>
<marker>Bod, Kaplan, 1997</marker>
<rawString>R. Bod and R. Kaplan 1997. &amp;quot;On Performance models for Lexical-Functional Analysis&amp;quot;, Paper presented at the Computational Psycholinguistics Conference 1997, Berkeley (Ca).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bonnema</author>
<author>R Bod</author>
<author>R Scha</author>
</authors>
<title>A DOP Model for Semantic Interpretation&amp;quot;,</title>
<date>1997</date>
<booktitle>Proceedings ACL/EACL-97,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="2168" citStr="Bonnema et al. 1997" startWordPosition="312" endWordPosition="315"> decomposition operations that divide a given utterance analysis into a set of fragments, • a set of composition operations by which such fragments may be recombined to derive an analysis of a new utterance, and • a definition of a probability model that indicates how the probability of a new utterance analysis is computed on the basis of the probabilities of the fragments that combine to make it up. Previous instantiations of the DOP architecture were based on utterance-analyses represented as surface phrase-structure trees (&amp;quot;Tree-DOP&amp;quot;, e.g. Bod 1993; Rajman 1995; Sima&apos;an 1995; Goodman 1996; Bonnema et al. 1997). Tree-DOP uses two decomposition operations that produce connected subtrees of utterance representations: (1) the Root operation selects any node of a tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates; (2) the Frontier operation then chooses a set (possibly empty) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes. The only composition operation used by TreeDOP is a node-substitution operation that replaces the Ronald Kaplan Xerox Palo Alto Research Center 3333 Coyote Hill Road</context>
<context position="5008" citStr="Bonnema et al. 1997" startWordPosition="794" endWordPosition="797">minal of the current subanalysis. This is not the only possible definition of competition set. As Manning and Carpenter (1997) have shown, the competition sets can be made dependent on the composition operation. Their left-corner language model would also apply to Tree-DOP, yielding a different definition for the competition sets. But the properties of such Tree-DOP models have not been investigated. Experiments with Tree-DOP on the Penn Treebank and the OVIS corpus show a consistent increase in parse accuracy when larger and more complex subtrees are taken into account (cf. Bod 1993, 95, 98; Bonnema et al. 1997; Sekine &amp; Grishman 1995; Sima&apos;an 1995). However, Tree-DOP is limited in that it cannot account for underlying syntactic (and semantic) dependencies that are not 145 reflected directly in a surface tree. All modern linguistic theories propose more articulated representations and mechanisms in order to characterize such linguistic phenomena. DOP models for a number of richer representations have been explored (van den Berg et al. 1994; Tugwell 1995), but these approaches have remained context-free in their generative power. In contrast, Lexical-Functional Grammar (Kaplan &amp; Bresnan 1982; Kaplan </context>
</contexts>
<marker>Bonnema, Bod, Scha, 1997</marker>
<rawString>R. Bonnema, R. Bod and R. Scha 1997. &amp;quot;A DOP Model for Semantic Interpretation&amp;quot;, Proceedings ACL/EACL-97, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Cormons</author>
<author>forthcoming</author>
</authors>
<title>Analyse et desambiguisation: Une approche purement a base de corpus (Data-Oriented Parsing) pour le formalisme des Grammaires Lexicales Fonctionnelles,</title>
<date></date>
<tech>PhD thesis,</tech>
<institution>Universite de Rennes,</institution>
<marker>Cormons, forthcoming, </marker>
<rawString>B. Cormons, forthcoming. Analyse et desambiguisation: Une approche purement a base de corpus (Data-Oriented Parsing) pour le formalisme des Grammaires Lexicales Fonctionnelles, PhD thesis, Universite de Rennes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Efficient Algorithms for Parsing the DOP Model&amp;quot;,</title>
<date>1996</date>
<booktitle>Proceedings Empirical Methods in Natural Language Processing,</booktitle>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="2146" citStr="Goodman 1996" startWordPosition="310" endWordPosition="311">es, • a set of decomposition operations that divide a given utterance analysis into a set of fragments, • a set of composition operations by which such fragments may be recombined to derive an analysis of a new utterance, and • a definition of a probability model that indicates how the probability of a new utterance analysis is computed on the basis of the probabilities of the fragments that combine to make it up. Previous instantiations of the DOP architecture were based on utterance-analyses represented as surface phrase-structure trees (&amp;quot;Tree-DOP&amp;quot;, e.g. Bod 1993; Rajman 1995; Sima&apos;an 1995; Goodman 1996; Bonnema et al. 1997). Tree-DOP uses two decomposition operations that produce connected subtrees of utterance representations: (1) the Root operation selects any node of a tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates; (2) the Frontier operation then chooses a set (possibly empty) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes. The only composition operation used by TreeDOP is a node-substitution operation that replaces the Ronald Kaplan Xerox Palo Alto Research Center</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman 1996. &amp;quot;Efficient Algorithms for Parsing the DOP Model&amp;quot;, Proceedings Empirical Methods in Natural Language Processing, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
</authors>
<title>The Formal Architecture of LexicalFunctional Grammar&amp;quot;,</title>
<date>1989</date>
<journal>Journal of Information Science and Engineering,</journal>
<volume>5</volume>
<pages>305--322</pages>
<contexts>
<context position="5613" citStr="Kaplan 1989" startWordPosition="884" endWordPosition="885">l. 1997; Sekine &amp; Grishman 1995; Sima&apos;an 1995). However, Tree-DOP is limited in that it cannot account for underlying syntactic (and semantic) dependencies that are not 145 reflected directly in a surface tree. All modern linguistic theories propose more articulated representations and mechanisms in order to characterize such linguistic phenomena. DOP models for a number of richer representations have been explored (van den Berg et al. 1994; Tugwell 1995), but these approaches have remained context-free in their generative power. In contrast, Lexical-Functional Grammar (Kaplan &amp; Bresnan 1982; Kaplan 1989), which assigns representations consisting of a surface constituent tree enriched with a corresponding functional structure, is known to be beyond context-free. In the current work, we develop a DOP model based on representations defined by LFG theory (&amp;quot;LFG-DOP&amp;quot;). That is, we provide a new instantiation for the four parameters of the DOP architecture. We will see that this basic LFG-DOP model triggers a new, corpus-based notion of grammaticality, and that it leads to a different class of its probability models which exhibit interesting properties with respect to specificity and the interpretat</context>
<context position="6914" citStr="Kaplan 1989" startWordPosition="1079" endWordPosition="1080">resentations The definition of a well-formed representation for utterance-analyses follows from LFG theory, that is, every utterance is annotated with a c-structure, an fstructure and a mapping 4) between them. The cstructure is a tree that describes the surface constituent structure of an utterance; the f-structure is an attribute-value matrix marking the grammatical relations of subject, predicate and object, as well as providing agreement features and semantic forms; and 4) is a correspondence function that maps nodes of the c-structure into units of the f-structure (Kaplan &amp; Bresnan 1982; Kaplan 1989). The following figure shows a representation for the utterance Kim eats. (We leave out some features to keep the example simple.) (1) SUBJ TENSE PRES PRED &apos;eat(SUBJ)&apos; Note that the 4) correspondence function gives an explicit characterization of the relation between the superficial and underlying syntactic properties of an utterance, indicating how certain parts of the string carry information about particular units of underlying structure. As such, it will play a crucial role in our definition for the decomposition and composition operations of LFG-DOP. In (1) we see for instance that the NP</context>
</contexts>
<marker>Kaplan, 1989</marker>
<rawString>R. Kaplan 1989. &amp;quot;The Formal Architecture of LexicalFunctional Grammar&amp;quot;, Journal of Information Science and Engineering, vol. 5, 305-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>Lexical-Functional Grammar: A Formal System for Grammatical Representation&amp;quot;,</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations, The</booktitle>
<editor>in J. Bresnan (ed.),</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5599" citStr="Kaplan &amp; Bresnan 1982" startWordPosition="880" endWordPosition="883">3, 95, 98; Bonnema et al. 1997; Sekine &amp; Grishman 1995; Sima&apos;an 1995). However, Tree-DOP is limited in that it cannot account for underlying syntactic (and semantic) dependencies that are not 145 reflected directly in a surface tree. All modern linguistic theories propose more articulated representations and mechanisms in order to characterize such linguistic phenomena. DOP models for a number of richer representations have been explored (van den Berg et al. 1994; Tugwell 1995), but these approaches have remained context-free in their generative power. In contrast, Lexical-Functional Grammar (Kaplan &amp; Bresnan 1982; Kaplan 1989), which assigns representations consisting of a surface constituent tree enriched with a corresponding functional structure, is known to be beyond context-free. In the current work, we develop a DOP model based on representations defined by LFG theory (&amp;quot;LFG-DOP&amp;quot;). That is, we provide a new instantiation for the four parameters of the DOP architecture. We will see that this basic LFG-DOP model triggers a new, corpus-based notion of grammaticality, and that it leads to a different class of its probability models which exhibit interesting properties with respect to specificity and t</context>
<context position="6900" citStr="Kaplan &amp; Bresnan 1982" startWordPosition="1075" endWordPosition="1078">nal representations Representations The definition of a well-formed representation for utterance-analyses follows from LFG theory, that is, every utterance is annotated with a c-structure, an fstructure and a mapping 4) between them. The cstructure is a tree that describes the surface constituent structure of an utterance; the f-structure is an attribute-value matrix marking the grammatical relations of subject, predicate and object, as well as providing agreement features and semantic forms; and 4) is a correspondence function that maps nodes of the c-structure into units of the f-structure (Kaplan &amp; Bresnan 1982; Kaplan 1989). The following figure shows a representation for the utterance Kim eats. (We leave out some features to keep the example simple.) (1) SUBJ TENSE PRES PRED &apos;eat(SUBJ)&apos; Note that the 4) correspondence function gives an explicit characterization of the relation between the superficial and underlying syntactic properties of an utterance, indicating how certain parts of the string carry information about particular units of underlying structure. As such, it will play a crucial role in our definition for the decomposition and composition operations of LFG-DOP. In (1) we see for instan</context>
<context position="8491" citStr="Kaplan &amp; Bresnan 1982" startWordPosition="1334" endWordPosition="1337">m a node n iff either n is 0-linked to f (that is, f= 0(n) ) orf is contained within 4)(n) (that is, there is a chain of attributes that leads from 4)(n) to f). All the f-structure units in (1) are 0-accessible from for instance the S node and the VP node, but the TENSE and top-level PRED are not 0-accessible from the NP node. According to LFG theory, c-structures and fstructures must satisfy certain formal well-formedness conditions. A c-structure/f-structure pair is a valid LFG representation only if it satisfies the Nonbranching Dominance, Uniqueness, Coherence and Completeness conditions (Kaplan &amp; Bresnan 1982). Nonbranching Dominance demands that no c-structure category appears twice in a nonbranching dominance chain; Uniqueness asserts that there can be at most one value for any attribute in the f-structure; Coherence prohibits the appearance of grammatical functions that are not governed by the lexical predicate; and Completeness requires that all the functions that a predicate governs appear as attributes in the local f-structure. Decomposition operations Many different DOP models are compatible with the system of LFG representations. In this paper we outline a basic LFG-DOP model which extends </context>
<context position="22267" citStr="Kaplan &amp; Bresnan 1982" startWordPosition="3550" endWordPosition="3553">tic process to enforce three conditions, Coherence, Uniqueness and category-matching, so that it only produces representations with well-formed c-structures that correspond to coherent and consistent f-structures. The competition probabilities for this model are given by the obvious extension of (17). It is not possible, however, to construct a model in which the Completeness Condition is enforced during the derivation process. This is because the satisfiability of the Completeness Condition depends not only on the results of previous steps of a derivation but also on the following steps (see Kaplan &amp; Bresnan 1982). This nonmonotonic property means that the appropriate step-wise competition sets cannot be defined and that this condition can only be enforced at the final stage of validity sampling. In each of these three models the categorymatching condition is evaluated on-line during the derivation process while other conditions are either evaluated on-line or off-line by the after-the-fact sampling process. LFG-DOP is crucially different from Tree-DOP in that at least one validity requirement, the Completeness Condition, must always be left to the post-derivation process. Note that a number of other m</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>R. Kaplan and J. Bresnan 1982. &amp;quot;Lexical-Functional Grammar: A Formal System for Grammatical Representation&amp;quot;, in J. Bresnan (ed.), The Mental Representation of Grammatical Relations, The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>B Carpenter</author>
</authors>
<title>Probabilistic parsing using left corner language models&amp;quot;,</title>
<date>1997</date>
<booktitle>Proceedings IWPT&apos;97,</booktitle>
<location>Boston (Mass.).</location>
<contexts>
<context position="4515" citStr="Manning and Carpenter (1997)" startWordPosition="716" endWordPosition="719"> I CS) is given by CP(t I CS) = P(t) I It&apos;E CS PO Here, P(t) is the fragment probability for t in a given corpus. Let Ti_i = t1 0 t2 0 ... 0 ti_i be the subanalysis just before the ith step of the process, let LNC(Ti_j ) denote the category of the leftmost nonterminal of Tii , and let r(t) denote the root category of a fragment t. Then the competition set at the ith step is CS, = ( t: r(t)=LNC(Ti_i ) 1 That is, the competition sets for Tree-DOP are determined by the category of the leftmost nonterminal of the current subanalysis. This is not the only possible definition of competition set. As Manning and Carpenter (1997) have shown, the competition sets can be made dependent on the composition operation. Their left-corner language model would also apply to Tree-DOP, yielding a different definition for the competition sets. But the properties of such Tree-DOP models have not been investigated. Experiments with Tree-DOP on the Penn Treebank and the OVIS corpus show a consistent increase in parse accuracy when larger and more complex subtrees are taken into account (cf. Bod 1993, 95, 98; Bonnema et al. 1997; Sekine &amp; Grishman 1995; Sima&apos;an 1995). However, Tree-DOP is limited in that it cannot account for underly</context>
</contexts>
<marker>Manning, Carpenter, 1997</marker>
<rawString>C. Manning and B. Carpenter 1997. &amp;quot;Probabilistic parsing using left corner language models&amp;quot;, Proceedings IWPT&apos;97, Boston (Mass.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rajman</author>
</authors>
<title>Approche Probabiliste de l&apos;Analyse Syntaxique&amp;quot;, Traitement Automatique des Langues,</title>
<date>1995</date>
<volume>vol.</volume>
<pages>36--1</pages>
<contexts>
<context position="987" citStr="Rajman 1995" startWordPosition="134" endWordPosition="135">el for tree representations and then show how it can be extended with corresponding functional structures. The resulting LFG-DOP model triggers a new, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings. 1. Introduction Data-Oriented Parsing (DOP) models of natural language embody the assumption that human language perception and production works with representations of past language experiences, rather than with abstract grammar rules (cf. Bod 1992, 95; Scha 1992; Sima&apos;an 1995; Rajman 1995). DOP models therefore maintain large corpora of linguistic representations of previously occurring utterances. New utterances are analyzed by combining (arbitrarily large) fragments from the corpus; the occurrence-frequencies of the fragments are used to determine which analysis is the most probable one. In accordance with the general DOP architecture outlined by Bod (1995), a particular DOP model is described by specifying settings for the following four parameters: • a formal definition of a well-formed representation for utterance analyses, • a set of decomposition operations that divide a</context>
</contexts>
<marker>Rajman, 1995</marker>
<rawString>M. Rajman 1995. &amp;quot;Approche Probabiliste de l&apos;Analyse Syntaxique&amp;quot;, Traitement Automatique des Langues, vol. 36(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Scha</author>
</authors>
<title>Virtuele Grammatica&apos;s en Creatieve Algoritmen&amp;quot;,</title>
<date>1992</date>
<journal>Gramma/l11</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="959" citStr="Scha 1992" startWordPosition="130" endWordPosition="131">zing the original DOP model for tree representations and then show how it can be extended with corresponding functional structures. The resulting LFG-DOP model triggers a new, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings. 1. Introduction Data-Oriented Parsing (DOP) models of natural language embody the assumption that human language perception and production works with representations of past language experiences, rather than with abstract grammar rules (cf. Bod 1992, 95; Scha 1992; Sima&apos;an 1995; Rajman 1995). DOP models therefore maintain large corpora of linguistic representations of previously occurring utterances. New utterances are analyzed by combining (arbitrarily large) fragments from the corpus; the occurrence-frequencies of the fragments are used to determine which analysis is the most probable one. In accordance with the general DOP architecture outlined by Bod (1995), a particular DOP model is described by specifying settings for the following four parameters: • a formal definition of a well-formed representation for utterance analyses, • a set of decomposit</context>
</contexts>
<marker>Scha, 1992</marker>
<rawString>R. Scha 1992. &amp;quot;Virtuele Grammatica&apos;s en Creatieve Algoritmen&amp;quot;, Gramma/l11 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>A Corpus-based Probabilistic Grammar with Only Two Non-terminals&amp;quot;,</title>
<date>1995</date>
<booktitle>Proceedings Fourth International Workshop on Parsing Technologies,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5032" citStr="Sekine &amp; Grishman 1995" startWordPosition="798" endWordPosition="801">subanalysis. This is not the only possible definition of competition set. As Manning and Carpenter (1997) have shown, the competition sets can be made dependent on the composition operation. Their left-corner language model would also apply to Tree-DOP, yielding a different definition for the competition sets. But the properties of such Tree-DOP models have not been investigated. Experiments with Tree-DOP on the Penn Treebank and the OVIS corpus show a consistent increase in parse accuracy when larger and more complex subtrees are taken into account (cf. Bod 1993, 95, 98; Bonnema et al. 1997; Sekine &amp; Grishman 1995; Sima&apos;an 1995). However, Tree-DOP is limited in that it cannot account for underlying syntactic (and semantic) dependencies that are not 145 reflected directly in a surface tree. All modern linguistic theories propose more articulated representations and mechanisms in order to characterize such linguistic phenomena. DOP models for a number of richer representations have been explored (van den Berg et al. 1994; Tugwell 1995), but these approaches have remained context-free in their generative power. In contrast, Lexical-Functional Grammar (Kaplan &amp; Bresnan 1982; Kaplan 1989), which assigns rep</context>
</contexts>
<marker>Sekine, Grishman, 1995</marker>
<rawString>S. Sekine and R. Grishman 1995. &amp;quot;A Corpus-based Probabilistic Grammar with Only Two Non-terminals&amp;quot;, Proceedings Fourth International Workshop on Parsing Technologies, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima&apos;an</author>
</authors>
<title>An optimized algorithm for Data Oriented Parsing&amp;quot;,</title>
<date>1995</date>
<booktitle>Recent Advances in Natural Language Processing 1995, John Benjamins,</booktitle>
<editor>in R. Mitkov and N. Nicolov (eds.),</editor>
<location>Amsterdam.</location>
<contexts>
<context position="973" citStr="Sima&apos;an 1995" startWordPosition="132" endWordPosition="133">iginal DOP model for tree representations and then show how it can be extended with corresponding functional structures. The resulting LFG-DOP model triggers a new, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings. 1. Introduction Data-Oriented Parsing (DOP) models of natural language embody the assumption that human language perception and production works with representations of past language experiences, rather than with abstract grammar rules (cf. Bod 1992, 95; Scha 1992; Sima&apos;an 1995; Rajman 1995). DOP models therefore maintain large corpora of linguistic representations of previously occurring utterances. New utterances are analyzed by combining (arbitrarily large) fragments from the corpus; the occurrence-frequencies of the fragments are used to determine which analysis is the most probable one. In accordance with the general DOP architecture outlined by Bod (1995), a particular DOP model is described by specifying settings for the following four parameters: • a formal definition of a well-formed representation for utterance analyses, • a set of decomposition operations</context>
<context position="5047" citStr="Sima&apos;an 1995" startWordPosition="802" endWordPosition="803"> the only possible definition of competition set. As Manning and Carpenter (1997) have shown, the competition sets can be made dependent on the composition operation. Their left-corner language model would also apply to Tree-DOP, yielding a different definition for the competition sets. But the properties of such Tree-DOP models have not been investigated. Experiments with Tree-DOP on the Penn Treebank and the OVIS corpus show a consistent increase in parse accuracy when larger and more complex subtrees are taken into account (cf. Bod 1993, 95, 98; Bonnema et al. 1997; Sekine &amp; Grishman 1995; Sima&apos;an 1995). However, Tree-DOP is limited in that it cannot account for underlying syntactic (and semantic) dependencies that are not 145 reflected directly in a surface tree. All modern linguistic theories propose more articulated representations and mechanisms in order to characterize such linguistic phenomena. DOP models for a number of richer representations have been explored (van den Berg et al. 1994; Tugwell 1995), but these approaches have remained context-free in their generative power. In contrast, Lexical-Functional Grammar (Kaplan &amp; Bresnan 1982; Kaplan 1989), which assigns representations co</context>
</contexts>
<marker>Sima&apos;an, 1995</marker>
<rawString>K. Sima&apos;an 1995. &amp;quot;An optimized algorithm for Data Oriented Parsing&amp;quot;, in R. Mitkov and N. Nicolov (eds.), Recent Advances in Natural Language Processing 1995, John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tugwell</author>
</authors>
<title>A State-Transition Grammar for DataOriented Parsing&amp;quot;,</title>
<date>1995</date>
<booktitle>Proceedings European Chapter of the ACL&apos;95 ,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="5460" citStr="Tugwell 1995" startWordPosition="864" endWordPosition="865"> OVIS corpus show a consistent increase in parse accuracy when larger and more complex subtrees are taken into account (cf. Bod 1993, 95, 98; Bonnema et al. 1997; Sekine &amp; Grishman 1995; Sima&apos;an 1995). However, Tree-DOP is limited in that it cannot account for underlying syntactic (and semantic) dependencies that are not 145 reflected directly in a surface tree. All modern linguistic theories propose more articulated representations and mechanisms in order to characterize such linguistic phenomena. DOP models for a number of richer representations have been explored (van den Berg et al. 1994; Tugwell 1995), but these approaches have remained context-free in their generative power. In contrast, Lexical-Functional Grammar (Kaplan &amp; Bresnan 1982; Kaplan 1989), which assigns representations consisting of a surface constituent tree enriched with a corresponding functional structure, is known to be beyond context-free. In the current work, we develop a DOP model based on representations defined by LFG theory (&amp;quot;LFG-DOP&amp;quot;). That is, we provide a new instantiation for the four parameters of the DOP architecture. We will see that this basic LFG-DOP model triggers a new, corpus-based notion of grammaticali</context>
</contexts>
<marker>Tugwell, 1995</marker>
<rawString>D. Tugwell 1995. &amp;quot;A State-Transition Grammar for DataOriented Parsing&amp;quot;, Proceedings European Chapter of the ACL&apos;95 , Dublin, Ireland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>