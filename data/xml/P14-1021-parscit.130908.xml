<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000260">
<title confidence="0.992162">
Shift-Reduce CCG Parsing with a Dependency Model
</title>
<author confidence="0.997141">
Wenduan Xu Stephen Clark Yue Zhang
</author>
<affiliation confidence="0.998491">
University of Cambridge University of Cambridge Singapore University
Computer Laboratory Computer Laboratory of Technology and Design
</affiliation>
<email confidence="0.887892">
wx217@cam.ac.uk sc609@cam.ac.uk yue zhang@sutd.edu.sg
</email>
<sectionHeader confidence="0.996003" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988">
This paper presents the first dependency
model for a shift-reduce CCG parser. Mod-
elling dependencies is desirable for a num-
ber of reasons, including handling the
“spurious” ambiguity of CCG; fitting well
with the theory of CCG; and optimizing
for structures which are evaluated at test
time. We develop a novel training tech-
nique using a dependency oracle, in which
all derivations are hidden. A challenge
arises from the fact that the oracle needs
to keep track of exponentially many gold-
standard derivations, which is solved by
integrating a packed parse forest with the
beam-search decoder. Standard CCGBank
tests show the model achieves up to 1.05
labeled F-score improvements over three
existing, competitive CCG parsing models.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999651533333333">
Combinatory Categorial Grammar (CCG; Steed-
man (2000)) is able to derive typed dependency
structures (Hockenmaier, 2003; Clark and Curran,
2007), providing a useful approximation to the un-
derlying predicate-argument relations of “who did
what to whom”. To date, CCG remains the most
competitive formalism for recovering “deep” de-
pendencies arising from many linguistic phenom-
ena such as raising, control, extraction and coordi-
nation (Rimell et al., 2009; Nivre et al., 2010).
To achieve its expressiveness, CCG exhibits
so-called “spurious” ambiguity, permitting many
non-standard surface derivations which ease the
recovery of certain dependencies, especially those
arising from type-raising and composition. But
this raises the question of what is the most suit-
able model for CCG: should we model the deriva-
tions, the dependencies, or both? The choice for
some existing parsers (Hockenmaier, 2003; Clark
and Curran, 2007) is to model derivations directly,
restricting the gold-standard to be the normal-form
derivations (Eisner, 1996) from CCGBank (Hock-
enmaier and Steedman, 2007).
Modelling dependencies, as a proxy for the se-
mantic interpretation, fits well with the theory of
CCG, in which Steedman (2000) argues that the
derivation is merely a “trace” of the underlying
syntactic process, and that the structure which
is built, and predicated over when applying con-
straints on grammaticality, is the semantic inter-
pretation. The early dependency model of Clark
et al. (2002), in which model features were defined
over only dependency structures, was partly moti-
vated by these theoretical observations.
More generally, dependency models are desir-
able for a number of reasons. First, modelling
dependencies provides an elegant solution to the
spurious ambiguity problem (Clark and Curran,
2007). Second, obtaining training data for de-
pendencies is likely to be easier than for syn-
tactic derivations, especially for incomplete data
(Schneider et al., 2013). Clark and Curran (2006)
show how the dependency model from Clark and
Curran (2007) extends naturally to the partial-
training case, and also how to obtain dependency
data cheaply from gold-standard lexical category
sequences alone. And third, it has been argued that
dependencies are an ideal representation for parser
evaluation, especially for CCG (Briscoe and Car-
roll, 2006; Clark and Hockenmaier, 2002), and so
optimizing for dependency recovery makes sense
from an evaluation perspective.
In this paper, we fill a gap in the literature by
developing the first dependency model for a shift-
reduce CCG parser. Shift-reduce parsing applies
naturally to CCG (Zhang and Clark, 2011), and the
left-to-right, incremental nature of the decoding
fits with CCG’s cognitive claims. The discrimina-
tive model is global and trained with the structured
perceptron. The decoder is based on beam-search
</bodyText>
<page confidence="0.966448">
218
</page>
<note confidence="0.883705">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
(Zhang and Clark, 2008) with the advantage of
linear-time decoding (Goldberg et al., 2013).
</note>
<bodyText confidence="0.999133954545455">
A main contribution of the paper is a novel tech-
nique for training the parser using a dependency
oracle, in which all derivations are hidden. A
challenge arises from the potentially exponential
number of derivations leading to a gold-standard
dependency structure, which the oracle needs to
keep track of. Our solution is an integration of
a packed parse forest, which efficiently stores all
the derivations, with the beam-search decoder at
training time. The derivations are not explicitly
part of the data, since the forest is built from the
gold-standard dependencies. We also show how
perceptron learning with beam-search (Collins and
Roark, 2004) can be extended to handle the ad-
ditional ambiguity, by adapting the “violation-
fixing” perceptron of Huang et al. (2012).
Results on the standard CCGBank tests show
that our parser achieves absolute labeled F-score
gains of up to 0.5 over the shift-reduce parser of
Zhang and Clark (2011); and up to 1.05 and 0.64
over the normal-form and hybrid models of Clark
and Curran (2007), respectively.
</bodyText>
<sectionHeader confidence="0.938433" genericHeader="general terms">
2 Shift-Reduce with Beam-Search
</sectionHeader>
<bodyText confidence="0.988331115384615">
This section describes how shift-reduce tech-
niques can be applied to CCG, following Zhang
and Clark (2011). First we describe the determin-
istic process which a parser would follow when
tracing out a single, correct derivation; then we
describe how a model of normal-form derivations
— or, more accurately, a sequence of shift-reduce
actions leading to a normal-form derivation —
can be used with beam-search to develop a non-
deterministic parser which selects the highest scor-
ing sequence of actions. Note this section only de-
scribes a normal-form derivation model for shift-
reduce parsing. Section 3 explains how we extend
the approach to dependency models.
The shift-reduce algorithm adapted to CCG is
similar to that of shift-reduce dependency parsing
(Yamada and Matsumoto, 2003; Nivre and Mc-
Donald, 2008; Zhang and Clark, 2008; Huang and
Sagae, 2010). Following Zhang and Clark (2011),
we define each item in the parser as a pair (s, q),
where q is a queue of remaining input, consisting
of words and a set of possible lexical categories for
each word (with q0 being the front word), and s is
the stack that holds subtrees s0, s1,... (with s0 at
the top). Subtrees on the stack are partial deriva-
step stack (sn, ..., s1, s0) queue (q0, q1, ..., qm) action
</bodyText>
<table confidence="0.994841555555556">
0 N/N Mr. President visited Paris SHIFT
1 N/N N President visited Paris
N visited Paris
NP visited Paris
NP (S[dcl]\NP)/NP visited Paris
NP (S[dcl]\NP)/NP N Paris
NP (S[dcl]\NP)/NP NP
NP S[dcl]\NP
S[dcl]
</table>
<sectionHeader confidence="0.9823185" genericHeader="keywords">
2 SHIFT
3 REDUCE
4 UNARY
5 SHIFT
6 SHIFT
7 UNARY
8 REDUCE
9 REDUCE
</sectionHeader>
<figureCaption confidence="0.911841">
Figure 1: Deterministic example of shift-reduce
CCG parsing (lexical categories omitted on queue).
</figureCaption>
<bodyText confidence="0.998771725">
tions that have been built as part of the shift-reduce
process. SHIFT, REDUCE and UNARY are the three
types of actions that can be applied to an item. A
SHIFT action shifts one of the lexical categories
of q0 onto the stack. A REDUCE action combines
s0 and s1 according to a CCG combinatory rule,
producing a new category on the top of the stack.
A UNARY action applies either a type-raising or
type-changing rule to the stack-top category s0.1
Figure 1 shows a deterministic example for the
sentence Mr. President visited Paris, giving a sin-
gle sequence of shift-reduce actions which pro-
duces a correct derivation (i.e. one producing the
correct set of dependencies). Starting with the ini-
tial item (s, q)0 (row 0), which has an empty stack
and a full queue, a total of nine actions are applied
to produce the complete derivation.
Applying beam-search to a statistical shift-
reduce parser is a straightforward extension to the
deterministic example. At each step, a beam is
used to store the top-k highest-scoring items, re-
sulting from expanding all items in the previous
beam. An item becomes a candidate output once it
has an empty queue, and the parser keeps track of
the highest scored candidate output and returns the
best one as the final output. Compared with greedy
local-search (Nivre and Scholz, 2004), the use of
a beam allows the parser to explore a larger search
space and delay difficult ambiguity-resolving de-
cisions by considering multiple items in parallel.
We refer to the shift-reduce model of Zhang and
Clark (2011) as the normal-form model, where
the oracle for each sentence specifies a unique se-
quence of gold-standard actions which produces
the corresponding normal-form derivation. No de-
pendency structures are involved at training and
test time, except for evaluation. In the next sec-
tion, we describe a dependency oracle which con-
siders all sequences of actions producing a gold-
standard dependency structure to be correct.
</bodyText>
<footnote confidence="0.8971775">
1See Hockenmaier (2003) and Clark and Curran (2007)
for a description of CCG rules.
</footnote>
<page confidence="0.999025">
219
</page>
<figure confidence="0.995457095238095">
Mr. President visited Paris
N/N N (S[dcl]\NP)/NP NP
&gt; &gt;
N S[dcl]\NP
&gt;TC
&lt;
S[dcl]
(a)
Mr. President visited Paris
N/N N (S[dcl]\NP)/NP NP
&gt;
NP
&gt;T
S[dcl]/(S[dcl]\NP)
S[dcl]/NP
S[dcl] &gt;
(b)
NP
N
&gt;TC
&gt;s
</figure>
<figureCaption confidence="0.99754">
Figure 2: Two derivations leading to the same dependency structure. TC denotes type-changing.
</figureCaption>
<sectionHeader confidence="0.992079" genericHeader="method">
3 The Dependency Model
</sectionHeader>
<bodyText confidence="0.999879625">
Categories in CCG are either basic (such as NP
and PP) or complex (such as (S[dcl]\NP)/NP).
Each complex category in the lexicon defines one
or more predicate-argument relations, which can
be realized as a predicate-argument dependency
when the corresponding argument slot is con-
sumed. For example, the transitive verb category
above defines two relations: one for the subject
NP and one for the object NP. In this paper a
CCG predicate-argument dependency is a 4-tuple:
(hf, f, s, ha) where hf is the lexical item of the
lexical category expressing the relation; f is the
lexical category; s is the argument slot; and ha is
the head word of the argument. Since the lexical
items in a dependency are indexed by their sen-
tence positions, all dependencies for a sentence
form a set, which is referred to as a CCG depen-
dency structure. Clark and Curran (2007) contains
a detailed description of dependency structures.
Fig. 2 shows an example demonstrating spu-
rious ambiguity in relation to a CCG depen-
dency structure. In both derivations, the first
two lexical categories are combined using for-
ward application (&gt;) and the following depen-
dency is realized: (Mr., N/N1, 1, President). In
the normal-form derivation (a), the dependency
(visited, (S\NP1)/NP2, 2, Paris) is created by com-
bining the transitive verb category with the ob-
ject NP using forward application. One final de-
pendency, (visited, (S\NP1 )/NP2, 1, President), is re-
alized when the root node S[dcl] is produced
through backward application (&lt;).
Fig. 2(b) shows a non-normal-form derivation
which uses type-raising (T) and composition (B)
(which are not required to derive the correct de-
pendency structure). In this alternative derivation,
the dependency (visited, (S\NP1)/NP2, 1, President)
is realized using forward composition (B), and
(visited, (S\NP1)/NP2, 2, Paris) is realized when the
S[dcl] root is produced.
The chart-based dependency model of Clark
and Curran (2007) treats all derivations as hid-
den, and defines a probabilistic model for a de-
pendency structure by summing probabilities of
all derivations leading to a particular structure.
Features are defined over both derivations and
CCG predicate-argument dependencies. We fol-
low a similar approach, but rather than define
a probabilistic model (which requires summing),
we define a linear model over sequences of shift-
reduce actions, as for the normal-form shift-reduce
model. However, the difference compared to the
normal-form model is that we do not assume a sin-
gle gold-standard sequence of actions.
Similar to Goldberg and Nivre (2012), we de-
fine an oracle which determines, for a gold-
standard dependency structure, G, what the valid
transition sequences are (i.e. those sequences cor-
responding to derivations leading to G). More
specifically, the oracle can determine, given G and
an item (s, q), what the valid actions are for that
item (i.e. what actions can potentially lead to G,
starting with (s, q) and the dependencies already
built on s). However, there can be exponentially
many valid action sequences for G, which we rep-
resent efficiently using a packed parse forest. We
show how the forest can be used, during beam-
search decoding, to determine the valid actions
for a parse item (Section 3.2). We also show, in
Section 3.3, how perceptron training with early-
update (Collins and Roark, 2004) can be used in
this setting.
</bodyText>
<subsectionHeader confidence="0.998914">
3.1 The Oracle Forest
</subsectionHeader>
<bodyText confidence="0.999854166666667">
A CCG parse forest efficiently represents an
exponential number of derivations. Following
Clark and Curran (2007) (which builds on Miyao
and Tsujii (2002)), and using the same nota-
tion, we define a CCG parse forest Φ as a tuple
(C, D, R, γ, S), where C is a set of conjunctive
</bodyText>
<page confidence="0.974278">
220
</page>
<bodyText confidence="0.80859">
Algorithm 1(Clark and Curran, 2007)
Input: A packed forest (C, D, R, -y, 6), with dmax(c)
and dmax(d) already computed
</bodyText>
<listItem confidence="0.971393818181818">
1: function MAIN
2: for each d,. E R s.t. dmax. (d,.) = |G |do
3: MARK(d,.)
4: procedure MARK(d)
5: mark d as a correct node
6: for each c E -y(d) do
7: if dmax(c) == dmax(d) then
8: mark c as a correct node
9: for each d&apos; E 6(c) do
10: if d&apos; has not been visited then
11: MARK(d&apos;)
</listItem>
<bodyText confidence="0.998650723404255">
nodes and D is a set of disjunctive nodes.2 Con-
junctive nodes are individual CCG categories in Φ,
and are either obtained from the lexicon, or by
combining two disjunctive nodes using a CCG rule,
or by applying a unary rule to a disjunctive node.
Disjunctive nodes are equivalence classes of con-
junctive nodes. Two conjunctive nodes are equiv-
alent iff they have the same category, head and un-
filled dependencies (i.e. they will lead to the same
derivation, and produce the same dependencies, in
any future parsing). R ⊆ D is a set of root dis-
junctive nodes. γ : D → 2C is the conjunctive
child function and S : C → 2D is the disjunctive
child function. The former returns the set of all
conjunctive nodes of a disjunctive node, and the
latter returns the disjunctive child nodes of a con-
junctive node.
The dependency model requires all the conjunc-
tive and disjunctive nodes of Φ that are part of the
derivations leading to a gold-standard dependency
structure G. We refer to such derivations as cor-
rect derivations and the packed forest containing
all these derivations as the oracle forest, denoted
as ΦG, which is a subset of Φ. It is prohibitive to
enumerate all correct derivations, but it is possible
to identify, from Φ, all the conjunctive and dis-
junctive nodes that are part of ΦG. Clark and Cur-
ran (2007) gives an algorithm for doing so, which
we use here. The main intuition behind the algo-
rithm is that a gold-standard dependency structure
decomposes over derivations; thus gold-standard
dependencies realized at conjunctive nodes can be
counted when Φ is built, and all nodes that are part
of ΦG can then be marked out of Φ by traversing
it top-down. A key idea in understanding the algo-
2Under the hypergraph framework (Gallo et al., 1993;
Huang and Chiang, 2005), a conjunctive node corresponds to
a hyperedge and a disjunctive node corresponds to the head
of a hyperedge or hyperedge bundle.
rithm is that dependencies are created when dis-
junctive nodes are combined, and hence are asso-
ciated with, or “live on”, conjunctive nodes in the
forest.
Following Clark and Curran (2007), we also
define the following three values, where the first
decomposes only over local rule productions,
while the other two decompose over derivations:
</bodyText>
<equation confidence="0.991587555555556">
�
* if 3 τ E deps(c), τ E/ G
cdeps(c) =
|deps(c) |otherwise
* if cdeps(c) == *
* if dmax(d) == * for some d E 6(c)
�
�∈6(c) dmax(d) + cdeps(c) otherwise
dmax(d) = max{dmax(c)  |c E -y(d)}
</equation>
<bodyText confidence="0.999759153846154">
deps(c) is the set of all dependencies on con-
junctive node c, and cdeps(c) counts the number
of correct dependencies on c. dmax(c) is the max-
imum number of correct dependencies over any
sub-derivation headed by c and is calculated re-
cursively; dmax(d) returns the same value for a
disjunctive node. In all cases, a special value ∗
indicates the presence of incorrect dependencies.
To obtain the oracle forest, we first pre-compute
dmax(c) and dmax(d) for all d and c in Φ when Φ
is built using CKY, which are then used by Algo-
rithm 1 to identify all the conjunctive and disjunc-
tive nodes in ΦG.
</bodyText>
<subsectionHeader confidence="0.998552">
3.2 The Dependency Oracle Algorithm
</subsectionHeader>
<bodyText confidence="0.974189764705882">
We observe that the canonical shift-reduce algo-
rithm (as demonstrated in Fig. 1) applied to a sin-
gle parse tree exactly resembles bottom-up post-
order traversal of that tree. As an example, con-
sider the derivation in Fig. 2a, where the corre-
sponding sequence of actions is: sh N/N, sh N,
re N, un NP, sh (S[dcl]\NP)/NP, sh NP,
re S[dcl]\NP, re S[dcl].3 The order of traversal
is left-child, right-child and parent. For a single
parse, the corresponding shift-reduce action se-
quence is unique, and for a given item this canoni-
cal order restricts the possible derivations that can
be formed using further actions. We now extend
this observation to the more general case of an
oracle forest, where there may be more than one
gold-standard action for a given item.
Definition 1. Given a gold-standard dependency
</bodyText>
<footnote confidence="0.995732">
3The derivation is “upside down”, following the conven-
tion used for CCG, where the root is S[dcl]. We use sh, re
and un to denote the three types of shift-reduce action.
</footnote>
<equation confidence="0.890533">
dmax(c) = I
</equation>
<page confidence="0.931819">
221
</page>
<figure confidence="0.9922318">
Mr. President visited Paris
N/N N (S[dcl]\NP)/NP NP
&gt; &gt;
N S[dcl]\NP
(a)
Mr. President visited Paris
N/N N (S[dcl]\NP)/NP NP
&gt;
S[dcl]\NP
(b)
</figure>
<figureCaption confidence="0.980081">
Figure 3: Example subtrees on two stacks, with two subtrees in (a) and three in (b); roots of subtrees are
in bold.
</figureCaption>
<bodyText confidence="0.988137244186047">
structure G, an oracle forest 4bG, and an item
hs, qi, we say s is a realization of G, denoted
s &apos; G, if |s |= 1, q is empty and the single deriva-
tion on s is correct. If |s |&gt; 0 and the subtrees on
s can lead to a correct derivation in 4bG using fur-
ther actions, we say s is a partial-realization of
G, denoted as s ∼ G. And we define s ∼ G for
|s |= 0.
As an example, assume that 4bG contains only
the derivation in Fig. 2a; then a stack containing
the two subtrees in Fig. 3a is a partial-realization,
while a stack containing the three subtrees in
Fig. 3b is not. Note that each of the three sub-
trees in Fig. 3b is present in 4bG; however, these
subtrees cannot be combined into the single cor-
rect derivation, since the correct sequence of shift-
reduce actions must first combine the lexical cat-
egories for Mr. and President before shifting the
lexical category for visited.
We denote an action as a pair (x, c), where
x ∈ {SHIFT, REDUCE, UNARY} and c is the root
of the subtree resulting from that action. For all
three types of actions, c also corresponds to a
unique conjunctive node in the complete forest 4b;
and we use csz to denote the conjunctive node in
4b corresponding to subtree si on the stack. Let
hs&apos;, q&apos;i = hs, qi
applying the action (x, c) to hs, qi; and let the
set of all possible actions for hs, qi be X(s,Q) =
{(x, c)  |(x, c) is applicable to hs, qi}.
Definition 2. Given 4bG and an item hs, qi s.t. s ∼
G, we say an applicable action (x, c) for the item
is valid iff s&apos; ∼ G or s&apos; &apos; G, where hs&apos;, q&apos;i =
hs, qi ◦ (x, c).
Definition 3. Given 4bG, the dependency oracle
function fd is defined as:
�true if s&apos; ∼ G or s&apos; � G
fd(hs, qi, (x, c), 4bG) = false otherwise
where (x, c) ∈ X(s,Q) and hs&apos;, q&apos;i = hs, qi ◦ (x, c).
The pseudocode in Algorithm 2 implements fd.
It determines, for a given item, whether an appli-
cable action is valid in 4bG.
It is trivial to determine the validity of a SHIFT
action for the initial item, hs, qi0, since the SHIFT
action is valid iff its category matches the gold-
standard lexical category of the first word in
the sentence. For any subsequent SHIFT action
(SHIFT, c) to be valid, the necessary condition is
c ≡ clex0, where clex0 denotes the gold-standard
lexical category of the front word in the queue, qo
(line 3). However, this condition is not sufficient;
a counterexample is the case where all the gold-
standard lexical categories for the sentence in Fig-
ure 2 are shifted in succession. Hence, in general,
the conditions under which an action is valid are
more complex than the trivial case above.
First, suppose there is only one correct deriva-
tion in 4bG. A SHIFT action (SHIFT, clex0) is valid
whenever cs0 (the conjunctive node in 4bG cor-
responding to the subtree so on the stack) and
clex0 (the conjunctive node in 4bG corresponding
to the next gold-standard lexical category from
the queue) are both dominated by the conjunctive
node parent p of cs0 in 4bG.4 A REDUCE action
(REDUCE, c) is valid if c matches the category of
the conjunctive node parent of cs0 and cs1 in 4bG.
A UNARY action (UNARY, c) is valid if c matches
the conjunctive node parent of cs0 in 4bG. We now
generalize the case where 4bG contains a single
correct parse to the case of an oracle forest, where
each parent p is replaced by a set of conjunctive
nodes in 4bG.
Definition 4. The left parent set pL(c) of con-
junctive node c ∈ 4bG is the set of all parent con-
junctive nodes of c in 4bG, which have the disjunc-
tive node d containing c (i.e. c ∈ γ(d)) as a left
child.
Definition 5. The ancestor set A(c) of conjunc-
tive node c ∈ 4bG is the set of all reachable ances-
tor conjunctive nodes of c in 4bG.
Definition 6. Given an item hs, qi, if |s |= 1 we
say s is a frontier stack.
4Strictly speaking, the conjunctive node parent is a parent
of the disjunctive node containing the conjunctive node cs0.
We will continue to use this shorthand for parents of conjunc-
tive nodes throughout the paper.
</bodyText>
<equation confidence="0.631642">
◦ (x, c) be the resulting item from
</equation>
<page confidence="0.981246">
222
</page>
<figure confidence="0.9306412">
Algorithm 2 The Dependency Oracle Function fd
Input: ΦG, an item hs, qi s.t. s ∼ G, (x, c) ∈ X(3,q)
Let s&apos; be the stack of hs&apos;, q&apos;i = hs, qi ◦ (x, c)
1: function MAIN(hs, qi, (x, c), ΦG)
2: if x is SHIFT then
3: if c ≡6 clex0 then &gt; c not gold lexical category
4: return false
5: else if c ≡ clex0 and |s |= 0 then &gt; the initial item
6: return true
7: else if c ≡ clex0 and |s |=6 0 then
8: compute R(c30 , c3 )
1 0
9: return R(c30 , c3 ) =6 ∅
1 0
10: if x is REDUCE then &gt; s is non-frontier
11: if c ∈ R(c31, c30) then
12: compute R(c30 , c30)
13: return true 1 0
14: else return false
15: if x is UNARY then
16: if |s |= 1 then &gt; s is frontier
17: return c ∈ ΦG
18: if |s |=6 1 and c ∈ ΦG then &gt; s is non-frontier
19: compute R(c30 , c30)
20: return R(c301,c30 ) =6 ∅
</figure>
<bodyText confidence="0.979752722222222">
A key to defining the dependency oracle func-
tion is the notion of a shared ancestor set. In-
tuitively, shared ancestor sets are built up through
shift actions, and contain sets of nodes which can
potentially become the results of reduce or unary
actions. A further intuition is that shared ances-
tor sets define the space of possible correct deriva-
tions, and nodes in these sets are “ticked off” when
reduce and unary actions are applied, as a single
correct derivation is built through the shift-reduce
process (corresponding to a bottom-up post-order
traversal of the derivation). The following defi-
nition shows how the dependency oracle function
builds shared ancestor sets for each action type.
Definition 7. Let (s, q) be an item and let
(s&apos;, q&apos;) = (s, q) o (x, c). We define the shared an-
cestor set R(cs&apos; 1, cs&apos;0) of cs&apos;0, after applying action
(x, c), as:
</bodyText>
<listItem confidence="0.9798767">
• {c&apos;  |c&apos; ∈ pL(c30) ∩ A(c)}, if s is frontier and x =
SHIFT
• {c&apos;  |c&apos; ∈ pL(c30) ∩ A(c) and there is some c&apos;&apos; ∈
R(c31, c30) s.t. c&apos;&apos; ∈ A(c&apos;)}, if s is non-frontier and
x = SHIFT
• {c&apos;  |c&apos; ∈ R(c32, c31) ∩ A(c)}, if x = REDUCE
• {c&apos;  |c&apos; ∈ R(c31, c30) ∩ A(c)}, if s is non-frontier
and x = UNARY
• R(e, c030) = ∅ where c030 is the conjunctive node cor-
responding to the gold-standard lexical category of the
</listItem>
<bodyText confidence="0.984899517241379">
first word in the sentence (e is a dummy symbol indi-
cating the bottom of stack).
The base case for Definition 7 is when the gold-
standard lexical category of the first word in the
sentence has been shifted, which creates an empty
shared ancestor set. Furthermore, the shared an-
cestor set is always empty when the stack is a fron-
tier stack.
The dependency oracle algorithm checks the va-
lidity of applicable actions. A SHIFT action is
valid if R(cs&apos;1, cs&apos;0) =� 0 for the resulting stack
s&apos;. A valid REDUCE action consumes s1 and
s0. For the new node, its shared ancestor set is
the subset of the conjunctive nodes in R(cs2, cs1)
which dominate the resulting conjunctive node of
a valid REDUCE action. The UNARY case for a
frontier stack is trivial: any UNARY action ap-
plicable to s in ΦG is valid. For a non-frontier
stack, the UNARY case is similar to REDUCE ex-
cept the resulting shared ancestor set is a subset of
R(cs1, cs0).
We now turn to the problem of finding the
shared ancestor sets. In practice, we do not do this
by traversing ΦG top-down from the conjunctive
nodes in PL(cs0) on-the-fly to find each member of
R. Instead, when we build ΦG in bottom-up topo-
logical order, we pre-compute the set of reachable
disjunctive nodes of each conjunctive node c in
ΦG as:
</bodyText>
<equation confidence="0.986037">
D(c) = δ(c) U (Uc&apos;Eγ(d),dEδ(c)(D(c&apos;)))
</equation>
<bodyText confidence="0.995640875">
Each D is implemented as a hash map, which
allows us to test the membership of one potential
conjunctive node in 0(1) time. For example, a
conjunctive node c E PL(cs0) is reachable from
clex0 if there is a disjunctive node d E D(c) s.t.
clex0 E γ(d). With this implementation, the com-
plexity of checking each valid SHIFT action is then
0(|PL(cs0)|).
</bodyText>
<subsectionHeader confidence="0.994509">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.999980666666667">
We use the averaged perceptron (Collins, 2002)
to train a global linear model and score each ac-
tion. The normal-form model of Zhang and Clark
(2011) uses an early update mechanism (Collins
and Roark, 2004), where decoding is stopped to
update model weights whenever the single gold
action falls outside the beam. In our parser, there
can be multiple gold items in a beam. One option
would be to apply early update whenever at least
</bodyText>
<page confidence="0.997536">
223
</page>
<figure confidence="0.84611075">
Algorithm 3 Dependency Model Training
Input: (y, G) and beam size k
1: w ← 0; B0 ← ∅; i ← 0
2: B0.push(hs, qi0) &gt; the initial item
3: cand ← ∅ &gt; candidate output priority queue
4: gold ← ∅ &gt; gold output priority queue
5: while Bi =6 ∅ do
6: for each hs, qi ∈ Bi do
7: if |q |= 0 then &gt; candidate output
8: cand.push(hs, qi)
9: if s &apos; G then &gt; s is a realization of G
10: gold.push(hs, qi)
11: expand hs, qi into Bi+1
12: Bi+1 ← Bi+1[1 : k] &gt; apply beam
13: if nG =6 ∅, nG ∩ Bi+1 = ∅ and cand[0] &apos;6 G then
14: w ← w + O(nG[0]) − O(Bi+1[0]) &gt; early update
15: return
16: i ← i + 1 &gt; continue to next step
17: if cand[0] &apos;6 G then &gt; final update
18: w ← w + O(gold[0]) − O(cand[0])
</figure>
<bodyText confidence="0.999719818181818">
one of these gold items falls outside the beam.
However, this may not be a true violation of the
gold-standard (Huang et al., 2012). Thus, we use a
relaxed version of early update, in which all gold-
standard actions must fall outside the beam before
an update is performed. This update mechanism is
provably correct under the violation-fixing frame-
work of Huang et al. (2012).
Let (y, G) be a training sentence paired with its
gold-standard dependency structure and let H(s,q)
be the following set for an item (s, q):
</bodyText>
<equation confidence="0.625515833333333">
{(s, q) o (x, c)  |fd((s, q), (x, c), ΦG) = true}
H(s,q) contains all correct items at step i + 1 ob-
tained by expanding (s, q). Let the set of all cor-
rect items at a step i + 1 be:5
�HG = H(s,q)
(s,q)EBi
</equation>
<bodyText confidence="0.999886083333333">
Algorithm 3 shows the pseudocode for training
the dependency model with early update for one
input (y, G). The score of an item (s, q) is calcu-
lated as w · 0((s, q)) with respect to the current
model w, where 0((s, q)) is the feature vector for
the item. At step i, all items are expanded and
added onto the next beam Bi+1, and the top-k re-
tained. Early update is applied when all gold items
first fall outside the beam, and any candidate out-
put is incorrect (line 14). Since there are poten-
tially many gold items, and one gold item is re-
quired for the perceptron update, a decision needs
</bodyText>
<footnote confidence="0.985928">
5In Algorithm 3 we abuse notation by using nG[0] to de-
note the highest scoring gold item in the set.
</footnote>
<bodyText confidence="0.999684714285714">
to be made regarding which gold item to update
against. We choose to reward the highest scoring
gold item, in line with the violation-fixing frame-
work; and penalize the highest scoring incorrect
item, using the standard perceptron update. A fi-
nal update is performed if no more expansions are
possible but the final output is incorrect.
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999964476190476">
We implement our shift-reduce parser on top of the
core C&amp;C code base (Clark and Curran, 2007) and
evaluate it against the shift-reduce parser of Zhang
and Clark (2011) (henceforth Z&amp;C) and the chart-
based normal-form and hybrid models of Clark
and Curran (2007). For all experiments, we use
CCGBank with the standard split: sections 2-21
for training (39,604 sentences), section 00 for de-
velopment (1,913 sentences) and section 23 (2,407
sentences) for testing.
The way that the CCG grammar is implemented
in C&amp;C has some implications for our parser.
First, unlike Z&amp;C, which uses a context-free cover
(Fowler and Penn, 2010) and hence is able to use
all sentences in the training data, we are only able
to use 36,036 sentences. The reason is that the
grammar in C&amp;C does not have complete cover-
age of CCGBank, due to the fact that e.g. not
all rules in CCGBank conform to the combinatory
rules of CCG. Second, our parser uses the unifica-
tion mechanism from C&amp;C to output dependencies
directly, and hence does not need a separate post-
processing step to convert derivations into CCG de-
pendencies, as required by Z&amp;C.
The feature templates of our model consist of
all of those in Z&amp;C, except the ones which re-
quire lexical heads to come from either the left or
right child, as such features are incompatible with
the head passing mechanism used by C&amp;C. Each
Z&amp;C template is defined over a parse item, and
captures various aspects of the stack and queue
context. For example, one template returns the
top category on the stack plus its head word, to-
gether with the first word and its POS tag on the
queue. Another template returns the second cat-
egory on the stack, together with the POS tag of
its head word. Every Z&amp;C feature is defined as
a pair, consisting of an instantiated context tem-
plate and a parse action. In addition, we use all
the CCG predicate-argument dependency features
from Clark and Curran (2007), which contribute to
the score of a REDUCE action when dependencies
</bodyText>
<page confidence="0.995482">
224
</page>
<table confidence="0.99854275">
LP % LR % LF % LSent. % CatAcc. % coverage %
this parser 86.29 84.09 85.18 34.40 92.75 100
Z&amp;C 87.15 82.95 85.00 33.82 92.77 100
C&amp;C (normal-form) 85.22 82.52 83.85 31.63 92.40 100
this parser 86.76 84.90 85.82 34.72 93.20 99.06 (C&amp;C coverage)
Z&amp;C 87.55 83.63 85.54 34.14 93.11 99.06 (C&amp;C coverage)
C&amp;C (hybrid) – – 85.25 – – 99.06 (C&amp;C coverage)
C&amp;C (normal-form) 85.22 84.29 84.76 31.93 92.83 99.06 (C&amp;C coverage)
</table>
<figureCaption confidence="0.9702545">
Figure 4: Labeled precision and recall relative to dependency length on the development set. C&amp;C
normal-form model is used.
</figureCaption>
<tableCaption confidence="0.947943">
Table 1: Accuracy comparison on Section 00 (auto POS).
</tableCaption>
<figure confidence="0.952421566666667">
C&amp;C
Z&amp;C
this parser
0 5 10 15 20 25 30
Dependency length (bins of 5)
(a) precision vs. dependency length
Recall %
90
85
80
75
70
65
60
55
50
C&amp;C
Z&amp;C
this parser
0 5 10 15 20 25 30
Dependency length (bins of 5)
(b) recall vs. dependency length
Precision %
90
85
80
75
70
65
60
</figure>
<bodyText confidence="0.999496555555556">
are realized. Detailed descriptions of all the tem-
plates in our model can be found in the respective
papers. We run 20 training iterations and the re-
sulting model contains 16.5M features with a non-
zero weight.
We use 10-fold cross validation for POS tagging
and supertagging the training data, and automat-
ically assigned POS tags for all experiments. A
probability cut-off value of 0.0001 for the Q pa-
rameter in the supertagger is used for both train-
ing and testing. The Q parameter determines how
many lexical categories are assigned to each word;
Q = 0.0001 is a relatively small value which al-
lows in a large number of categories, compared to
the default value used in Clark and Curran (2007).
For training only, if the gold-standard lexical cat-
egory is not supplied by the supertagger for a par-
ticular word, it is added to the list of categories.
</bodyText>
<subsectionHeader confidence="0.681041">
4.1 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999906111111111">
The beam size was tuned on the development set,
and a value of 128 was found to achieve a rea-
sonable balance of accuracy and speed; hence this
value was used for all experiments. Since C&amp;C al-
ways enforces non-fragmentary output (i.e. it can
only produce spanning analyses), it fails on some
sentences in the development and test sets, and
thus we also evaluate on the reduced sets, follow-
ing Clark and Curran (2007). Our parser does not
fail on any sentences because it permits fragmen-
tary output (those cases where there is more than
one subtree left on the final stack). The results for
Z&amp;C, and the C&amp;C normal-form and hybrid mod-
els, are taken from Zhang and Clark (2011).
Table 1 shows the accuracies of all parsers on
the development set, in terms of labeled precision
and recall over the predicate-argument dependen-
cies in CCGBank. On both the full and reduced
sets, our parser achieves the highest F-score. In
comparison with C&amp;C, our parser shows signif-
icant increases across all metrics, with 0.57%
and 1.06% absolute F-score improvements over
the hybrid and normal-form models, respectively.
Another major improvement over the other two
parsers is in sentence level accuracy, LSent, which
measures the number of sentences for which the
dependency structure is completely correct.
Table 1 also shows that our parser has improved
recall over Z&amp;C at some expense of precision. To
probe this further we compare labeled precision
and recall relative to dependency length, as mea-
sured by the distance between the two words in a
dependency, grouped into bins of 5 values. Fig. 4
shows clearly that Z&amp;C favors precision over re-
call, giving higher precision scores for almost all
dependency lengths compared to our parser. In
</bodyText>
<page confidence="0.994741">
225
</page>
<table confidence="0.999923272727273">
category LP % (o) LP % (z) LP % (c) LR % (o) LR % (z) LR % (c) LF % (o) LF % (z) LF % (c) freq.
N/N 95.53 95.77 95.28 95.83 95.79 95.62 95.68 95.78 95.45 7288
NP/N 96.53 96.70 96.57 97.12 96.59 96.03 96.83 96.65 96.30 4101
(NP\NP)/NP 81.64 83.19 82.17 90.63 89.24 88.90 85.90 86.11 85.40 2379
(NP\NP)/NP 81.70 82.53 81.58 88.91 87.99 85.74 85.15 85.17 83.61 2174
((S\NP)\(S\NP))/NP 77.64 77.60 71.94 72.97 71.58 73.32 75.24 74.47 72.63 1147
((S\NP)\(S\NP))/NP 75.78 76.30 70.92 71.27 70.60 71.93 73.45 73.34 71.42 1058
((S[dcl]\NP)/NP 83.94 85.60 81.57 86.04 84.30 86.37 84.98 84.95 83.90 917
PP/NP 77.06 73.76 75.06 73.63 72.83 70.09 75.31 73.29 72.49 876
((S[dcl]\NP)/NP 82.03 85.32 81.62 83.26 82.00 85.55 82.64 83.63 83.54 872
((S\NP)\(S\NP)) 86.42 84.44 86.85 86.19 86.60 86.73 86.31 85.51 86.79 746
</table>
<tableCaption confidence="0.9422125">
Table 2: Accuracy comparison on most frequent dependency types, for our parser (o), Z&amp;C (z) and C&amp;C
hybrid model (c). Categories in bold indicate the argument slot in the relation.
</tableCaption>
<table confidence="0.999840625">
LP % LR % LF % LSent. % CatAcc. % coverage %
our parser 87.03 85.08 86.04 35.69 93.10 100
Z&amp;C 87.43 83.61 85.48 35.19 93.12 100
C&amp;C (normal-form) 85.58 82.85 84.20 32.90 92.84 100
our parser 87.04 85.16 86.09 35.84 93.13 99.58 (C&amp;C coverage)
Z&amp;C 87.43 83.71 85.53 35.34 93.15 99.58 (C&amp;C coverage)
C&amp;C (hybrid) 86.17 84.74 85.45 32.92 92.98 99.58 (C&amp;C coverage)
C&amp;C (normal-form) 85.48 84.60 85.04 33.08 92.86 99.58 (C&amp;C coverage)
</table>
<tableCaption confidence="0.999899">
Table 3: Accuracy comparison on section 23 (auto POS).
</tableCaption>
<bodyText confidence="0.998966">
terms of recall (Fig. 4b), our parser outperforms
Z&amp;C over all dependency lengths, especially for
longer dependencies (x &gt; 20). When compared
with C&amp;C, the recall of the Z&amp;C parser drops
quickly for dependency lengths over 10. While
our parser also suffers from this problem, it is
less severe and is able to achieve higher recall at
x &gt; 30.
Table 2 compares our parser with Z&amp;C and the
C&amp;C hybrid model, for the most frequent depen-
dency relations. While our parser achieved lower
precision than Z&amp;C, it is more balanced and gives
higher recall for all of the dependency relations ex-
cept the last one, and higher F-score for over half
of them.
Table 3 presents the final test results on Section
23. Again, our parser achieves the highest scores
across all metrics (for both the full and reduced
test sets), except for precision and lexical category
assignment, where Z&amp;C performed better.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999992315789474">
We have presented a dependency model for a shift-
reduce CCG parser, which fully aligns CCG parsing
with the left-to-right, incremental nature of a shift-
reduce parser. Our work is in part inspired by the
dependency models of Clark and Curran (2007)
and, in the use of a dependency oracle, is close
in spirit to that of Goldberg and Nivre (2012). The
difference is that the Goldberg and Nivre parser
builds, and scores, dependency structures directly,
whereas our parser uses a unification mechanism
to create dependencies, and scores the CCG deriva-
tions, allowing great flexibility in terms of what
dependencies can be realized. Another related
work is Yu et al. (2013), which introduced a sim-
ilar technique to deal with spurious ambiguity in
MT. Finally, there may be potential to integrate the
techniques of Auli and Lopez (2011), which cur-
rently represents the state-of-the-art in CCGBank
parsing, into our parser.
</bodyText>
<sectionHeader confidence="0.996739" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999952125">
We thank the anonymous reviewers for their help-
ful comments. Wenduan Xu is fully supported by
the Carnegie Trust and receives additional fund-
ing from the Cambridge Trusts. Stephen Clark
is supported by ERC Starting Grant DisCoTex
(306920) and EPSRC grant EP/I037512/1. Yue
Zhang is supported by Singapore MOE Tier2 grant
T2MOE201301.
</bodyText>
<sectionHeader confidence="0.999048" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981179333333333">
Michael Auli and Adam Lopez. 2011. A compari-
son of loopy belief propagation and dual decompo-
sition for integrated CCG supertagging and parsing.
In Proc. ACL 2011, pages 470–480, Portland, OR.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
</reference>
<page confidence="0.981237">
226
</page>
<reference confidence="0.999948128712871">
PARC DepBank. In Proc. of COLING/ACL, pages
41–48, Sydney, Australia.
Stephen Clark and James R. Curran. 2006. Partial
training for a lexicalized-grammar parser. In Proc.
NAACL-06, pages 144–151, New York, USA.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
Stephen Clark and Julia Hockenmaier. 2002. Evalu-
ating a wide-coverage CCG parser. In Proc. of the
LREC 2002 Beyond Parseval Workshop, pages 60–
66, Las Palmas, Spain.
Stephen Clark, Julia Hockenmaier, and Mark Steed-
man. 2002. Building deep dependency structures
with a wide-coverage CCG parser. In Proc. ACL,
pages 327–334, Philadelphia, PA.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proc. of
ACL, pages 111–118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP, pages 1–8, Philadelphia, USA.
Jason Eisner. 1996. Efficient normal-form parsing for
Combinatory Categorial Grammar. In Proc. ACL,
pages 79–86, Santa Cruz, CA.
Timothy AD Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with Combinatory Catego-
rial Grammar. In Proc. ACL, pages 335–344, Upp-
sala, Sweden.
Giorgio Gallo, Giustino Longo, Stefano Pallottino,
and Sang Nguyen. 1993. Directed hypergraphs
and applications. Discrete applied mathematics,
42(2):177–201.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Proc.
COLING, Mumbai, India.
Yoav Goldberg, Kai Zhao, and Liang Huang. 2013.
Efficient implementation for beam search incremen-
tal parsers. In Proceedings of the Short Papers of
ACL, Sofia, Bulgaria.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Julia Hockenmaier. 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Proceedings of the Ninth Interna-
tional Workshop on Parsing Technology, pages 53–
64, Vancouver, Canada.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proc. ACL, pages 1077–1086, Uppsala, Sweden.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
NAACL, pages 142–151, Montreal, Canada.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the Human Language Technology Confer-
ence, San Diego, CA.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proc. of ACL/HLT, pages 950–958,
Columbus, Ohio.
J. Nivre and M Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of
COLING 2004, pages 64–70, Geneva, Switzerland.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-
los Gomez-Rodriguez. 2010. Evaluation of depen-
dency parsers on unbounded dependencies. In Proc.
of COLING, Beijing, China.
Laura Rimell, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In Proc. EMNLP, pages 813–821, Edin-
burgh, UK.
Nathan Schneider, Brendan O’Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse, Sofia, Bulgaria.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, Mass.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In
Proc. of IWPT, Nancy, France.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable mt training. In Proc. EMNLP, Seat-
tle, Washington, USA.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proc. of EMNLP, Hawaii, USA.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proc. ACL 2011, pages 683–692,
Portland, OR.
</reference>
<page confidence="0.997943">
227
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.314544">
<title confidence="0.999097">Shift-Reduce CCG Parsing with a Dependency Model</title>
<author confidence="0.998955">Wenduan Xu Stephen Clark Yue Zhang</author>
<affiliation confidence="0.93917">University of Cambridge University of Cambridge Singapore University Computer Laboratory Computer Laboratory of Technology and Design</affiliation>
<email confidence="0.359795">wx217@cam.ac.uksc609@cam.ac.ukyuezhang@sutd.edu.sg</email>
<abstract confidence="0.999608368421053">This paper presents the first dependency for a shift-reduce Modelling dependencies is desirable for a number of reasons, including handling the ambiguity of fitting well the theory of and optimizing for structures which are evaluated at test time. We develop a novel training technique using a dependency oracle, in which all derivations are hidden. A challenge arises from the fact that the oracle needs to keep track of exponentially many goldstandard derivations, which is solved by integrating a packed parse forest with the beam-search decoder. Standard CCGBank show the model achieves up to labeled F-score improvements over three competitive models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing.</title>
<date>2011</date>
<booktitle>In Proc. ACL 2011,</booktitle>
<pages>470--480</pages>
<location>Portland, OR.</location>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011. A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing. In Proc. ACL 2011, pages 470–480, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>41--48</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3364" citStr="Briscoe and Carroll, 2006" startWordPosition="498" endWordPosition="502"> provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependency recovery makes sense from an evaluation perspective. In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce parsing applies naturally to CCG (Zhang and Clark, 2011), and the left-to-right, incremental nature of the decoding fits with CCG’s cognitive claims. The discriminative model is global and trained with the structured perceptron. The decoder is based on beam-search 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Lingui</context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank. In Proc. of COLING/ACL, pages 41–48, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Partial training for a lexicalized-grammar parser.</title>
<date>2006</date>
<booktitle>In Proc. NAACL-06,</booktitle>
<pages>144--151</pages>
<location>New York, USA.</location>
<contexts>
<context position="3013" citStr="Clark and Curran (2006)" startWordPosition="445" endWordPosition="448">plying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency models are desirable for a number of reasons. First, modelling dependencies provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependency recovery makes sense from an evaluation perspective. In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce </context>
</contexts>
<marker>Clark, Curran, 2006</marker>
<rawString>Stephen Clark and James R. Curran. 2006. Partial training for a lexicalized-grammar parser. In Proc. NAACL-06, pages 144–151, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="1173" citStr="Clark and Curran, 2007" startWordPosition="168" endWordPosition="171">valuated at test time. We develop a novel training technique using a dependency oracle, in which all derivations are hidden. A challenge arises from the fact that the oracle needs to keep track of exponentially many goldstandard derivations, which is solved by integrating a packed parse forest with the beam-search decoder. Standard CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing, competitive CCG parsing models. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is able to derive typed dependency structures (Hockenmaier, 2003; Clark and Curran, 2007), providing a useful approximation to the underlying predicate-argument relations of “who did what to whom”. To date, CCG remains the most competitive formalism for recovering “deep” dependencies arising from many linguistic phenomena such as raising, control, extraction and coordination (Rimell et al., 2009; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of wh</context>
<context position="2827" citStr="Clark and Curran, 2007" startWordPosition="416" endWordPosition="419">ry of CCG, in which Steedman (2000) argues that the derivation is merely a “trace” of the underlying syntactic process, and that the structure which is built, and predicated over when applying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency models are desirable for a number of reasons. First, modelling dependencies provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependenc</context>
<context position="5200" citStr="Clark and Curran (2007)" startWordPosition="785" endWordPosition="788">, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptron learning with beam-search (Collins and Roark, 2004) can be extended to handle the additional ambiguity, by adapting the “violationfixing” perceptron of Huang et al. (2012). Results on the standard CCGBank tests show that our parser achieves absolute labeled F-score gains of up to 0.5 over the shift-reduce parser of Zhang and Clark (2011); and up to 1.05 and 0.64 over the normal-form and hybrid models of Clark and Curran (2007), respectively. 2 Shift-Reduce with Beam-Search This section describes how shift-reduce techniques can be applied to CCG, following Zhang and Clark (2011). First we describe the deterministic process which a parser would follow when tracing out a single, correct derivation; then we describe how a model of normal-form derivations — or, more accurately, a sequence of shift-reduce actions leading to a normal-form derivation — can be used with beam-search to develop a nondeterministic parser which selects the highest scoring sequence of actions. Note this section only describes a normal-form deriv</context>
<context position="8878" citStr="Clark and Curran (2007)" startWordPosition="1400" endWordPosition="1403">ace and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. We refer to the shift-reduce model of Zhang and Clark (2011) as the normal-form model, where the oracle for each sentence specifies a unique sequence of gold-standard actions which produces the corresponding normal-form derivation. No dependency structures are involved at training and test time, except for evaluation. In the next section, we describe a dependency oracle which considers all sequences of actions producing a goldstandard dependency structure to be correct. 1See Hockenmaier (2003) and Clark and Curran (2007) for a description of CCG rules. 219 Mr. President visited Paris N/N N (S[dcl]\NP)/NP NP &gt; &gt; N S[dcl]\NP &gt;TC &lt; S[dcl] (a) Mr. President visited Paris N/N N (S[dcl]\NP)/NP NP &gt; NP &gt;T S[dcl]/(S[dcl]\NP) S[dcl]/NP S[dcl] &gt; (b) NP N &gt;TC &gt;s Figure 2: Two derivations leading to the same dependency structure. TC denotes type-changing. 3 The Dependency Model Categories in CCG are either basic (such as NP and PP) or complex (such as (S[dcl]\NP)/NP). Each complex category in the lexicon defines one or more predicate-argument relations, which can be realized as a predicate-argument dependency when the co</context>
<context position="11170" citStr="Clark and Curran (2007)" startWordPosition="1770" endWordPosition="1773"> with the object NP using forward application. One final dependency, (visited, (S\NP1 )/NP2, 1, President), is realized when the root node S[dcl] is produced through backward application (&lt;). Fig. 2(b) shows a non-normal-form derivation which uses type-raising (T) and composition (B) (which are not required to derive the correct dependency structure). In this alternative derivation, the dependency (visited, (S\NP1)/NP2, 1, President) is realized using forward composition (B), and (visited, (S\NP1)/NP2, 2, Paris) is realized when the S[dcl] root is produced. The chart-based dependency model of Clark and Curran (2007) treats all derivations as hidden, and defines a probabilistic model for a dependency structure by summing probabilities of all derivations leading to a particular structure. Features are defined over both derivations and CCG predicate-argument dependencies. We follow a similar approach, but rather than define a probabilistic model (which requires summing), we define a linear model over sequences of shiftreduce actions, as for the normal-form shift-reduce model. However, the difference compared to the normal-form model is that we do not assume a single gold-standard sequence of actions. Simila</context>
<context position="12732" citStr="Clark and Curran (2007)" startWordPosition="2023" endWordPosition="2026"> what actions can potentially lead to G, starting with (s, q) and the dependencies already built on s). However, there can be exponentially many valid action sequences for G, which we represent efficiently using a packed parse forest. We show how the forest can be used, during beamsearch decoding, to determine the valid actions for a parse item (Section 3.2). We also show, in Section 3.3, how perceptron training with earlyupdate (Collins and Roark, 2004) can be used in this setting. 3.1 The Oracle Forest A CCG parse forest efficiently represents an exponential number of derivations. Following Clark and Curran (2007) (which builds on Miyao and Tsujii (2002)), and using the same notation, we define a CCG parse forest Φ as a tuple (C, D, R, γ, S), where C is a set of conjunctive 220 Algorithm 1(Clark and Curran, 2007) Input: A packed forest (C, D, R, -y, 6), with dmax(c) and dmax(d) already computed 1: function MAIN 2: for each d,. E R s.t. dmax. (d,.) = |G |do 3: MARK(d,.) 4: procedure MARK(d) 5: mark d as a correct node 6: for each c E -y(d) do 7: if dmax(c) == dmax(d) then 8: mark c as a correct node 9: for each d&apos; E 6(c) do 10: if d&apos; has not been visited then 11: MARK(d&apos;) nodes and D is a set of disjunc</context>
<context position="14614" citStr="Clark and Curran (2007)" startWordPosition="2377" endWordPosition="2381">junctive nodes of a disjunctive node, and the latter returns the disjunctive child nodes of a conjunctive node. The dependency model requires all the conjunctive and disjunctive nodes of Φ that are part of the derivations leading to a gold-standard dependency structure G. We refer to such derivations as correct derivations and the packed forest containing all these derivations as the oracle forest, denoted as ΦG, which is a subset of Φ. It is prohibitive to enumerate all correct derivations, but it is possible to identify, from Φ, all the conjunctive and disjunctive nodes that are part of ΦG. Clark and Curran (2007) gives an algorithm for doing so, which we use here. The main intuition behind the algorithm is that a gold-standard dependency structure decomposes over derivations; thus gold-standard dependencies realized at conjunctive nodes can be counted when Φ is built, and all nodes that are part of ΦG can then be marked out of Φ by traversing it top-down. A key idea in understanding the algo2Under the hypergraph framework (Gallo et al., 1993; Huang and Chiang, 2005), a conjunctive node corresponds to a hyperedge and a disjunctive node corresponds to the head of a hyperedge or hyperedge bundle. rithm i</context>
<context position="28265" citStr="Clark and Curran, 2007" startWordPosition="4985" endWordPosition="4988">s, and one gold item is required for the perceptron update, a decision needs 5In Algorithm 3 we abuse notation by using nG[0] to denote the highest scoring gold item in the set. to be made regarding which gold item to update against. We choose to reward the highest scoring gold item, in line with the violation-fixing framework; and penalize the highest scoring incorrect item, using the standard perceptron update. A final update is performed if no more expansions are possible but the final output is incorrect. 4 Experiments We implement our shift-reduce parser on top of the core C&amp;C code base (Clark and Curran, 2007) and evaluate it against the shift-reduce parser of Zhang and Clark (2011) (henceforth Z&amp;C) and the chartbased normal-form and hybrid models of Clark and Curran (2007). For all experiments, we use CCGBank with the standard split: sections 2-21 for training (39,604 sentences), section 00 for development (1,913 sentences) and section 23 (2,407 sentences) for testing. The way that the CCG grammar is implemented in C&amp;C has some implications for our parser. First, unlike Z&amp;C, which uses a context-free cover (Fowler and Penn, 2010) and hence is able to use all sentences in the training data, we are </context>
<context position="30083" citStr="Clark and Curran (2007)" startWordPosition="5302" endWordPosition="5305">are incompatible with the head passing mechanism used by C&amp;C. Each Z&amp;C template is defined over a parse item, and captures various aspects of the stack and queue context. For example, one template returns the top category on the stack plus its head word, together with the first word and its POS tag on the queue. Another template returns the second category on the stack, together with the POS tag of its head word. Every Z&amp;C feature is defined as a pair, consisting of an instantiated context template and a parse action. In addition, we use all the CCG predicate-argument dependency features from Clark and Curran (2007), which contribute to the score of a REDUCE action when dependencies 224 LP % LR % LF % LSent. % CatAcc. % coverage % this parser 86.29 84.09 85.18 34.40 92.75 100 Z&amp;C 87.15 82.95 85.00 33.82 92.77 100 C&amp;C (normal-form) 85.22 82.52 83.85 31.63 92.40 100 this parser 86.76 84.90 85.82 34.72 93.20 99.06 (C&amp;C coverage) Z&amp;C 87.55 83.63 85.54 34.14 93.11 99.06 (C&amp;C coverage) C&amp;C (hybrid) – – 85.25 – – 99.06 (C&amp;C coverage) C&amp;C (normal-form) 85.22 84.29 84.76 31.93 92.83 99.06 (C&amp;C coverage) Figure 4: Labeled precision and recall relative to dependency length on the development set. C&amp;C normal-form mo</context>
<context position="31722" citStr="Clark and Curran (2007)" startWordPosition="5600" endWordPosition="5603">n be found in the respective papers. We run 20 training iterations and the resulting model contains 16.5M features with a nonzero weight. We use 10-fold cross validation for POS tagging and supertagging the training data, and automatically assigned POS tags for all experiments. A probability cut-off value of 0.0001 for the Q parameter in the supertagger is used for both training and testing. The Q parameter determines how many lexical categories are assigned to each word; Q = 0.0001 is a relatively small value which allows in a large number of categories, compared to the default value used in Clark and Curran (2007). For training only, if the gold-standard lexical category is not supplied by the supertagger for a particular word, it is added to the list of categories. 4.1 Results and Analysis The beam size was tuned on the development set, and a value of 128 was found to achieve a reasonable balance of accuracy and speed; hence this value was used for all experiments. Since C&amp;C always enforces non-fragmentary output (i.e. it can only produce spanning analyses), it fails on some sentences in the development and test sets, and thus we also evaluate on the reduced sets, following Clark and Curran (2007). Ou</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Evaluating a wide-coverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proc. of the LREC 2002 Beyond Parseval Workshop,</booktitle>
<pages>60--66</pages>
<location>Las Palmas,</location>
<marker>Clark, Hockenmaier, 2002</marker>
<rawString>Stephen Clark and Julia Hockenmaier. 2002. Evaluating a wide-coverage CCG parser. In Proc. of the LREC 2002 Beyond Parseval Workshop, pages 60– 66, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Building deep dependency structures with a wide-coverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>327--334</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2509" citStr="Clark et al. (2002)" startWordPosition="370" endWordPosition="373">e existing parsers (Hockenmaier, 2003; Clark and Curran, 2007) is to model derivations directly, restricting the gold-standard to be the normal-form derivations (Eisner, 1996) from CCGBank (Hockenmaier and Steedman, 2007). Modelling dependencies, as a proxy for the semantic interpretation, fits well with the theory of CCG, in which Steedman (2000) argues that the derivation is merely a “trace” of the underlying syntactic process, and that the structure which is built, and predicated over when applying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency models are desirable for a number of reasons. First, modelling dependencies provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtrai</context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>Stephen Clark, Julia Hockenmaier, and Mark Steedman. 2002. Building deep dependency structures with a wide-coverage CCG parser. In Proc. ACL, pages 327–334, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>111--118</pages>
<location>Barcelona,</location>
<contexts>
<context position="4821" citStr="Collins and Roark, 2004" startWordPosition="720" endWordPosition="723">paper is a novel technique for training the parser using a dependency oracle, in which all derivations are hidden. A challenge arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptron learning with beam-search (Collins and Roark, 2004) can be extended to handle the additional ambiguity, by adapting the “violationfixing” perceptron of Huang et al. (2012). Results on the standard CCGBank tests show that our parser achieves absolute labeled F-score gains of up to 0.5 over the shift-reduce parser of Zhang and Clark (2011); and up to 1.05 and 0.64 over the normal-form and hybrid models of Clark and Curran (2007), respectively. 2 Shift-Reduce with Beam-Search This section describes how shift-reduce techniques can be applied to CCG, following Zhang and Clark (2011). First we describe the deterministic process which a parser would </context>
<context position="12567" citStr="Collins and Roark, 2004" startWordPosition="1997" endWordPosition="2000">es corresponding to derivations leading to G). More specifically, the oracle can determine, given G and an item (s, q), what the valid actions are for that item (i.e. what actions can potentially lead to G, starting with (s, q) and the dependencies already built on s). However, there can be exponentially many valid action sequences for G, which we represent efficiently using a packed parse forest. We show how the forest can be used, during beamsearch decoding, to determine the valid actions for a parse item (Section 3.2). We also show, in Section 3.3, how perceptron training with earlyupdate (Collins and Roark, 2004) can be used in this setting. 3.1 The Oracle Forest A CCG parse forest efficiently represents an exponential number of derivations. Following Clark and Curran (2007) (which builds on Miyao and Tsujii (2002)), and using the same notation, we define a CCG parse forest Φ as a tuple (C, D, R, γ, S), where C is a set of conjunctive 220 Algorithm 1(Clark and Curran, 2007) Input: A packed forest (C, D, R, -y, 6), with dmax(c) and dmax(d) already computed 1: function MAIN 2: for each d,. E R s.t. dmax. (d,.) = |G |do 3: MARK(d,.) 4: procedure MARK(d) 5: mark d as a correct node 6: for each c E -y(d) d</context>
<context position="25500" citStr="Collins and Roark, 2004" startWordPosition="4439" endWordPosition="4442">node c in ΦG as: D(c) = δ(c) U (Uc&apos;Eγ(d),dEδ(c)(D(c&apos;))) Each D is implemented as a hash map, which allows us to test the membership of one potential conjunctive node in 0(1) time. For example, a conjunctive node c E PL(cs0) is reachable from clex0 if there is a disjunctive node d E D(c) s.t. clex0 E γ(d). With this implementation, the complexity of checking each valid SHIFT action is then 0(|PL(cs0)|). 3.3 Training We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action. The normal-form model of Zhang and Clark (2011) uses an early update mechanism (Collins and Roark, 2004), where decoding is stopped to update model weights whenever the single gold action falls outside the beam. In our parser, there can be multiple gold items in a beam. One option would be to apply early update whenever at least 223 Algorithm 3 Dependency Model Training Input: (y, G) and beam size k 1: w ← 0; B0 ← ∅; i ← 0 2: B0.push(hs, qi0) &gt; the initial item 3: cand ← ∅ &gt; candidate output priority queue 4: gold ← ∅ &gt; gold output priority queue 5: while Bi =6 ∅ do 6: for each hs, qi ∈ Bi do 7: if |q |= 0 then &gt; candidate output 8: cand.push(hs, qi) 9: if s &apos; G then &gt; s is a realization of G 10</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proc. of ACL, pages 111–118, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="25341" citStr="Collins, 2002" startWordPosition="4413" endWordPosition="4414">ach member of R. Instead, when we build ΦG in bottom-up topological order, we pre-compute the set of reachable disjunctive nodes of each conjunctive node c in ΦG as: D(c) = δ(c) U (Uc&apos;Eγ(d),dEδ(c)(D(c&apos;))) Each D is implemented as a hash map, which allows us to test the membership of one potential conjunctive node in 0(1) time. For example, a conjunctive node c E PL(cs0) is reachable from clex0 if there is a disjunctive node d E D(c) s.t. clex0 E γ(d). With this implementation, the complexity of checking each valid SHIFT action is then 0(|PL(cs0)|). 3.3 Training We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action. The normal-form model of Zhang and Clark (2011) uses an early update mechanism (Collins and Roark, 2004), where decoding is stopped to update model weights whenever the single gold action falls outside the beam. In our parser, there can be multiple gold items in a beam. One option would be to apply early update whenever at least 223 Algorithm 3 Dependency Model Training Input: (y, G) and beam size k 1: w ← 0; B0 ← ∅; i ← 0 2: B0.push(hs, qi0) &gt; the initial item 3: cand ← ∅ &gt; candidate output priority queue 4: gold ← ∅ &gt; gold output priorit</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP, pages 1–8, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Efficient normal-form parsing for Combinatory Categorial Grammar.</title>
<date>1996</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>79--86</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="2065" citStr="Eisner, 1996" startWordPosition="301" endWordPosition="302">rdination (Rimell et al., 2009; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark and Curran, 2007) is to model derivations directly, restricting the gold-standard to be the normal-form derivations (Eisner, 1996) from CCGBank (Hockenmaier and Steedman, 2007). Modelling dependencies, as a proxy for the semantic interpretation, fits well with the theory of CCG, in which Steedman (2000) argues that the derivation is merely a “trace” of the underlying syntactic process, and that the structure which is built, and predicated over when applying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency m</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Efficient normal-form parsing for Combinatory Categorial Grammar. In Proc. ACL, pages 79–86, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy AD Fowler</author>
<author>Gerald Penn</author>
</authors>
<title>Accurate context-free parsing with Combinatory Categorial Grammar. In</title>
<date>2010</date>
<booktitle>Proc. ACL,</booktitle>
<pages>335--344</pages>
<location>Uppsala,</location>
<contexts>
<context position="28796" citStr="Fowler and Penn, 2010" startWordPosition="5070" endWordPosition="5073">implement our shift-reduce parser on top of the core C&amp;C code base (Clark and Curran, 2007) and evaluate it against the shift-reduce parser of Zhang and Clark (2011) (henceforth Z&amp;C) and the chartbased normal-form and hybrid models of Clark and Curran (2007). For all experiments, we use CCGBank with the standard split: sections 2-21 for training (39,604 sentences), section 00 for development (1,913 sentences) and section 23 (2,407 sentences) for testing. The way that the CCG grammar is implemented in C&amp;C has some implications for our parser. First, unlike Z&amp;C, which uses a context-free cover (Fowler and Penn, 2010) and hence is able to use all sentences in the training data, we are only able to use 36,036 sentences. The reason is that the grammar in C&amp;C does not have complete coverage of CCGBank, due to the fact that e.g. not all rules in CCGBank conform to the combinatory rules of CCG. Second, our parser uses the unification mechanism from C&amp;C to output dependencies directly, and hence does not need a separate postprocessing step to convert derivations into CCG dependencies, as required by Z&amp;C. The feature templates of our model consist of all of those in Z&amp;C, except the ones which require lexical head</context>
</contexts>
<marker>Fowler, Penn, 2010</marker>
<rawString>Timothy AD Fowler and Gerald Penn. 2010. Accurate context-free parsing with Combinatory Categorial Grammar. In Proc. ACL, pages 335–344, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Gallo</author>
<author>Giustino Longo</author>
<author>Stefano Pallottino</author>
<author>Sang Nguyen</author>
</authors>
<title>Directed hypergraphs and applications. Discrete applied mathematics,</title>
<date>1993</date>
<pages>42--2</pages>
<contexts>
<context position="15051" citStr="Gallo et al., 1993" startWordPosition="2452" endWordPosition="2455">t is prohibitive to enumerate all correct derivations, but it is possible to identify, from Φ, all the conjunctive and disjunctive nodes that are part of ΦG. Clark and Curran (2007) gives an algorithm for doing so, which we use here. The main intuition behind the algorithm is that a gold-standard dependency structure decomposes over derivations; thus gold-standard dependencies realized at conjunctive nodes can be counted when Φ is built, and all nodes that are part of ΦG can then be marked out of Φ by traversing it top-down. A key idea in understanding the algo2Under the hypergraph framework (Gallo et al., 1993; Huang and Chiang, 2005), a conjunctive node corresponds to a hyperedge and a disjunctive node corresponds to the head of a hyperedge or hyperedge bundle. rithm is that dependencies are created when disjunctive nodes are combined, and hence are associated with, or “live on”, conjunctive nodes in the forest. Following Clark and Curran (2007), we also define the following three values, where the first decomposes only over local rule productions, while the other two decompose over derivations: � * if 3 τ E deps(c), τ E/ G cdeps(c) = |deps(c) |otherwise * if cdeps(c) == * * if dmax(d) == * for so</context>
</contexts>
<marker>Gallo, Longo, Pallottino, Nguyen, 1993</marker>
<rawString>Giorgio Gallo, Giustino Longo, Stefano Pallottino, and Sang Nguyen. 1993. Directed hypergraphs and applications. Discrete applied mathematics, 42(2):177–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>A dynamic oracle for arc-eager dependency parsing.</title>
<date>2012</date>
<booktitle>In Proc. COLING,</booktitle>
<location>Mumbai, India.</location>
<contexts>
<context position="11800" citStr="Goldberg and Nivre (2012)" startWordPosition="1867" endWordPosition="1870">ts all derivations as hidden, and defines a probabilistic model for a dependency structure by summing probabilities of all derivations leading to a particular structure. Features are defined over both derivations and CCG predicate-argument dependencies. We follow a similar approach, but rather than define a probabilistic model (which requires summing), we define a linear model over sequences of shiftreduce actions, as for the normal-form shift-reduce model. However, the difference compared to the normal-form model is that we do not assume a single gold-standard sequence of actions. Similar to Goldberg and Nivre (2012), we define an oracle which determines, for a goldstandard dependency structure, G, what the valid transition sequences are (i.e. those sequences corresponding to derivations leading to G). More specifically, the oracle can determine, given G and an item (s, q), what the valid actions are for that item (i.e. what actions can potentially lead to G, starting with (s, q) and the dependencies already built on s). However, there can be exponentially many valid action sequences for G, which we represent efficiently using a packed parse forest. We show how the forest can be used, during beamsearch de</context>
<context position="36349" citStr="Goldberg and Nivre (2012)" startWordPosition="6391" endWordPosition="6394">e for over half of them. Table 3 presents the final test results on Section 23. Again, our parser achieves the highest scores across all metrics (for both the full and reduced test sets), except for precision and lexical category assignment, where Z&amp;C performed better. 5 Conclusion We have presented a dependency model for a shiftreduce CCG parser, which fully aligns CCG parsing with the left-to-right, incremental nature of a shiftreduce parser. Our work is in part inspired by the dependency models of Clark and Curran (2007) and, in the use of a dependency oracle, is close in spirit to that of Goldberg and Nivre (2012). The difference is that the Goldberg and Nivre parser builds, and scores, dependency structures directly, whereas our parser uses a unification mechanism to create dependencies, and scores the CCG derivations, allowing great flexibility in terms of what dependencies can be realized. Another related work is Yu et al. (2013), which introduced a similar technique to deal with spurious ambiguity in MT. Finally, there may be potential to integrate the techniques of Auli and Lopez (2011), which currently represents the state-of-the-art in CCGBank parsing, into our parser. Acknowledgements We thank </context>
</contexts>
<marker>Goldberg, Nivre, 2012</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2012. A dynamic oracle for arc-eager dependency parsing. In Proc. COLING, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Kai Zhao</author>
<author>Liang Huang</author>
</authors>
<title>Efficient implementation for beam search incremental parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the Short Papers of ACL,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="4168" citStr="Goldberg et al., 2013" startWordPosition="618" endWordPosition="621"> first dependency model for a shiftreduce CCG parser. Shift-reduce parsing applies naturally to CCG (Zhang and Clark, 2011), and the left-to-right, incremental nature of the decoding fits with CCG’s cognitive claims. The discriminative model is global and trained with the structured perceptron. The decoder is based on beam-search 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics (Zhang and Clark, 2008) with the advantage of linear-time decoding (Goldberg et al., 2013). A main contribution of the paper is a novel technique for training the parser using a dependency oracle, in which all derivations are hidden. A challenge arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptro</context>
</contexts>
<marker>Goldberg, Zhao, Huang, 2013</marker>
<rawString>Yoav Goldberg, Kai Zhao, and Liang Huang. 2013. Efficient implementation for beam search incremental parsers. In Proceedings of the Short Papers of ACL, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="2111" citStr="Hockenmaier and Steedman, 2007" startWordPosition="305" endWordPosition="309">09; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark and Curran, 2007) is to model derivations directly, restricting the gold-standard to be the normal-form derivations (Eisner, 1996) from CCGBank (Hockenmaier and Steedman, 2007). Modelling dependencies, as a proxy for the semantic interpretation, fits well with the theory of CCG, in which Steedman (2000) argues that the derivation is merely a “trace” of the underlying syntactic process, and that the structure which is built, and predicated over when applying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency models are desirable for a number of reasons. F</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1148" citStr="Hockenmaier, 2003" startWordPosition="166" endWordPosition="167">uctures which are evaluated at test time. We develop a novel training technique using a dependency oracle, in which all derivations are hidden. A challenge arises from the fact that the oracle needs to keep track of exponentially many goldstandard derivations, which is solved by integrating a packed parse forest with the beam-search decoder. Standard CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing, competitive CCG parsing models. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is able to derive typed dependency structures (Hockenmaier, 2003; Clark and Curran, 2007), providing a useful approximation to the underlying predicate-argument relations of “who did what to whom”. To date, CCG remains the most competitive formalism for recovering “deep” dependencies arising from many linguistic phenomena such as raising, control, extraction and coordination (Rimell et al., 2009; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this </context>
<context position="8850" citStr="Hockenmaier (2003)" startWordPosition="1397" endWordPosition="1398">lore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. We refer to the shift-reduce model of Zhang and Clark (2011) as the normal-form model, where the oracle for each sentence specifies a unique sequence of gold-standard actions which produces the corresponding normal-form derivation. No dependency structures are involved at training and test time, except for evaluation. In the next section, we describe a dependency oracle which considers all sequences of actions producing a goldstandard dependency structure to be correct. 1See Hockenmaier (2003) and Clark and Curran (2007) for a description of CCG rules. 219 Mr. President visited Paris N/N N (S[dcl]\NP)/NP NP &gt; &gt; N S[dcl]\NP &gt;TC &lt; S[dcl] (a) Mr. President visited Paris N/N N (S[dcl]\NP)/NP NP &gt; NP &gt;T S[dcl]/(S[dcl]\NP) S[dcl]/NP S[dcl] &gt; (b) NP N &gt;TC &gt;s Figure 2: Two derivations leading to the same dependency structure. TC denotes type-changing. 3 The Dependency Model Categories in CCG are either basic (such as NP and PP) or complex (such as (S[dcl]\NP)/NP). Each complex category in the lexicon defines one or more predicate-argument relations, which can be realized as a predicate-arg</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better kbest parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>53--64</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="15076" citStr="Huang and Chiang, 2005" startWordPosition="2456" endWordPosition="2459">enumerate all correct derivations, but it is possible to identify, from Φ, all the conjunctive and disjunctive nodes that are part of ΦG. Clark and Curran (2007) gives an algorithm for doing so, which we use here. The main intuition behind the algorithm is that a gold-standard dependency structure decomposes over derivations; thus gold-standard dependencies realized at conjunctive nodes can be counted when Φ is built, and all nodes that are part of ΦG can then be marked out of Φ by traversing it top-down. A key idea in understanding the algo2Under the hypergraph framework (Gallo et al., 1993; Huang and Chiang, 2005), a conjunctive node corresponds to a hyperedge and a disjunctive node corresponds to the head of a hyperedge or hyperedge bundle. rithm is that dependencies are created when disjunctive nodes are combined, and hence are associated with, or “live on”, conjunctive nodes in the forest. Following Clark and Curran (2007), we also define the following three values, where the first decomposes only over local rule productions, while the other two decompose over derivations: � * if 3 τ E deps(c), τ E/ G cdeps(c) = |deps(c) |otherwise * if cdeps(c) == * * if dmax(d) == * for some d E 6(c) � �∈6(c) dmax</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better kbest parsing. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 53– 64, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1077--1086</pages>
<location>Uppsala,</location>
<contexts>
<context position="6101" citStr="Huang and Sagae, 2010" startWordPosition="926" endWordPosition="929"> we describe how a model of normal-form derivations — or, more accurately, a sequence of shift-reduce actions leading to a normal-form derivation — can be used with beam-search to develop a nondeterministic parser which selects the highest scoring sequence of actions. Note this section only describes a normal-form derivation model for shiftreduce parsing. Section 3 explains how we extend the approach to dependency models. The shift-reduce algorithm adapted to CCG is similar to that of shift-reduce dependency parsing (Yamada and Matsumoto, 2003; Nivre and McDonald, 2008; Zhang and Clark, 2008; Huang and Sagae, 2010). Following Zhang and Clark (2011), we define each item in the parser as a pair (s, q), where q is a queue of remaining input, consisting of words and a set of possible lexical categories for each word (with q0 being the front word), and s is the stack that holds subtrees s0, s1,... (with s0 at the top). Subtrees on the stack are partial derivastep stack (sn, ..., s1, s0) queue (q0, q1, ..., qm) action 0 N/N Mr. President visited Paris SHIFT 1 N/N N President visited Paris N visited Paris NP visited Paris NP (S[dcl]\NP)/NP visited Paris NP (S[dcl]\NP)/NP N Paris NP (S[dcl]\NP)/NP NP NP S[dcl]\</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proc. ACL, pages 1077–1086, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>142--151</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="4941" citStr="Huang et al. (2012)" startWordPosition="740" endWordPosition="743">e arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptron learning with beam-search (Collins and Roark, 2004) can be extended to handle the additional ambiguity, by adapting the “violationfixing” perceptron of Huang et al. (2012). Results on the standard CCGBank tests show that our parser achieves absolute labeled F-score gains of up to 0.5 over the shift-reduce parser of Zhang and Clark (2011); and up to 1.05 and 0.64 over the normal-form and hybrid models of Clark and Curran (2007), respectively. 2 Shift-Reduce with Beam-Search This section describes how shift-reduce techniques can be applied to CCG, following Zhang and Clark (2011). First we describe the deterministic process which a parser would follow when tracing out a single, correct derivation; then we describe how a model of normal-form derivations — or, more</context>
<context position="26541" citStr="Huang et al., 2012" startWordPosition="4664" endWordPosition="4667">gold output priority queue 5: while Bi =6 ∅ do 6: for each hs, qi ∈ Bi do 7: if |q |= 0 then &gt; candidate output 8: cand.push(hs, qi) 9: if s &apos; G then &gt; s is a realization of G 10: gold.push(hs, qi) 11: expand hs, qi into Bi+1 12: Bi+1 ← Bi+1[1 : k] &gt; apply beam 13: if nG =6 ∅, nG ∩ Bi+1 = ∅ and cand[0] &apos;6 G then 14: w ← w + O(nG[0]) − O(Bi+1[0]) &gt; early update 15: return 16: i ← i + 1 &gt; continue to next step 17: if cand[0] &apos;6 G then &gt; final update 18: w ← w + O(gold[0]) − O(cand[0]) one of these gold items falls outside the beam. However, this may not be a true violation of the gold-standard (Huang et al., 2012). Thus, we use a relaxed version of early update, in which all goldstandard actions must fall outside the beam before an update is performed. This update mechanism is provably correct under the violation-fixing framework of Huang et al. (2012). Let (y, G) be a training sentence paired with its gold-standard dependency structure and let H(s,q) be the following set for an item (s, q): {(s, q) o (x, c) |fd((s, q), (x, c), ΦG) = true} H(s,q) contains all correct items at step i + 1 obtained by expanding (s, q). Let the set of all correct items at a step i + 1 be:5 �HG = H(s,q) (s,q)EBi Algorithm 3</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proc. NAACL, pages 142–151, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="12773" citStr="Miyao and Tsujii (2002)" startWordPosition="2030" endWordPosition="2033">starting with (s, q) and the dependencies already built on s). However, there can be exponentially many valid action sequences for G, which we represent efficiently using a packed parse forest. We show how the forest can be used, during beamsearch decoding, to determine the valid actions for a parse item (Section 3.2). We also show, in Section 3.3, how perceptron training with earlyupdate (Collins and Roark, 2004) can be used in this setting. 3.1 The Oracle Forest A CCG parse forest efficiently represents an exponential number of derivations. Following Clark and Curran (2007) (which builds on Miyao and Tsujii (2002)), and using the same notation, we define a CCG parse forest Φ as a tuple (C, D, R, γ, S), where C is a set of conjunctive 220 Algorithm 1(Clark and Curran, 2007) Input: A packed forest (C, D, R, -y, 6), with dmax(c) and dmax(d) already computed 1: function MAIN 2: for each d,. E R s.t. dmax. (d,.) = |G |do 3: MARK(d,.) 4: procedure MARK(d) 5: mark d as a correct node 6: for each c E -y(d) do 7: if dmax(c) == dmax(d) then 8: mark c as a correct node 9: for each d&apos; E 6(c) do 10: if d&apos; has not been visited then 11: MARK(d&apos;) nodes and D is a set of disjunctive nodes.2 Conjunctive nodes are indivi</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proceedings of the Human Language Technology Conference, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of ACL/HLT,</booktitle>
<pages>950--958</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6054" citStr="Nivre and McDonald, 2008" startWordPosition="917" endWordPosition="921">en tracing out a single, correct derivation; then we describe how a model of normal-form derivations — or, more accurately, a sequence of shift-reduce actions leading to a normal-form derivation — can be used with beam-search to develop a nondeterministic parser which selects the highest scoring sequence of actions. Note this section only describes a normal-form derivation model for shiftreduce parsing. Section 3 explains how we extend the approach to dependency models. The shift-reduce algorithm adapted to CCG is similar to that of shift-reduce dependency parsing (Yamada and Matsumoto, 2003; Nivre and McDonald, 2008; Zhang and Clark, 2008; Huang and Sagae, 2010). Following Zhang and Clark (2011), we define each item in the parser as a pair (s, q), where q is a queue of remaining input, consisting of words and a set of possible lexical categories for each word (with q0 being the front word), and s is the stack that holds subtrees s0, s1,... (with s0 at the top). Subtrees on the stack are partial derivastep stack (sn, ..., s1, s0) queue (q0, q1, ..., qm) action 0 N/N Mr. President visited Paris SHIFT 1 N/N N President visited Paris N visited Paris NP visited Paris NP (S[dcl]\NP)/NP visited Paris NP (S[dcl]</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proc. of ACL/HLT, pages 950–958, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<pages>64--70</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="8188" citStr="Nivre and Scholz, 2004" startWordPosition="1291" endWordPosition="1294">nitial item (s, q)0 (row 0), which has an empty stack and a full queue, a total of nine actions are applied to produce the complete derivation. Applying beam-search to a statistical shiftreduce parser is a straightforward extension to the deterministic example. At each step, a beam is used to store the top-k highest-scoring items, resulting from expanding all items in the previous beam. An item becomes a candidate output once it has an empty queue, and the parser keeps track of the highest scored candidate output and returns the best one as the final output. Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. We refer to the shift-reduce model of Zhang and Clark (2011) as the normal-form model, where the oracle for each sentence specifies a unique sequence of gold-standard actions which produces the corresponding normal-form derivation. No dependency structures are involved at training and test time, except for evaluation. In the next section, we describe a dependency oracle which considers all sequences of actions producing a goldstanda</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>J. Nivre and M Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of COLING 2004, pages 64–70, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Laura Rimell</author>
<author>Ryan McDonald</author>
<author>Carlos Gomez-Rodriguez</author>
</authors>
<title>Evaluation of dependency parsers on unbounded dependencies.</title>
<date>2010</date>
<booktitle>In Proc. of COLING,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="1503" citStr="Nivre et al., 2010" startWordPosition="219" endWordPosition="222">CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing, competitive CCG parsing models. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is able to derive typed dependency structures (Hockenmaier, 2003; Clark and Curran, 2007), providing a useful approximation to the underlying predicate-argument relations of “who did what to whom”. To date, CCG remains the most competitive formalism for recovering “deep” dependencies arising from many linguistic phenomena such as raising, control, extraction and coordination (Rimell et al., 2009; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark and Curran, 2007) is to model derivations directly, restricting the gold-standard to be the normal-form derivations (Eisner, 1996) from CCGBank (Hockenmaier and Steedma</context>
</contexts>
<marker>Nivre, Rimell, McDonald, Gomez-Rodriguez, 2010</marker>
<rawString>Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos Gomez-Rodriguez. 2010. Evaluation of dependency parsers on unbounded dependencies. In Proc. of COLING, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
</authors>
<title>Unbounded dependency recovery for parser evaluation.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>813--821</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="1482" citStr="Rimell et al., 2009" startWordPosition="215" endWordPosition="218">ch decoder. Standard CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing, competitive CCG parsing models. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is able to derive typed dependency structures (Hockenmaier, 2003; Clark and Curran, 2007), providing a useful approximation to the underlying predicate-argument relations of “who did what to whom”. To date, CCG remains the most competitive formalism for recovering “deep” dependencies arising from many linguistic phenomena such as raising, control, extraction and coordination (Rimell et al., 2009; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark and Curran, 2007) is to model derivations directly, restricting the gold-standard to be the normal-form derivations (Eisner, 1996) from CCGBank (Ho</context>
</contexts>
<marker>Rimell, Clark, Steedman, 2009</marker>
<rawString>Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Unbounded dependency recovery for parser evaluation. In Proc. EMNLP, pages 813–821, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Naomi Saphra</author>
<author>David Bamman</author>
<author>Manaal Faruqui</author>
<author>Noah A Smith</author>
<author>Chris Dyer</author>
<author>Jason Baldridge</author>
</authors>
<title>A framework for (under)specifying dependency syntax without overloading annotators.</title>
<date>2013</date>
<booktitle>In Proc. of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<location>Sofia, Bulgaria.</location>
<marker>Schneider, O’Connor, Saphra, Bamman, Faruqui, Smith, Dyer, Baldridge, 2013</marker>
<rawString>Nathan Schneider, Brendan O’Connor, Naomi Saphra, David Bamman, Manaal Faruqui, Noah A. Smith, Chris Dyer, and Jason Baldridge. 2013. A framework for (under)specifying dependency syntax without overloading annotators. In Proc. of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="1082" citStr="Steedman (2000)" startWordPosition="156" endWordPosition="158">CCG; fitting well with the theory of CCG; and optimizing for structures which are evaluated at test time. We develop a novel training technique using a dependency oracle, in which all derivations are hidden. A challenge arises from the fact that the oracle needs to keep track of exponentially many goldstandard derivations, which is solved by integrating a packed parse forest with the beam-search decoder. Standard CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing, competitive CCG parsing models. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is able to derive typed dependency structures (Hockenmaier, 2003; Clark and Curran, 2007), providing a useful approximation to the underlying predicate-argument relations of “who did what to whom”. To date, CCG remains the most competitive formalism for recovering “deep” dependencies arising from many linguistic phenomena such as raising, control, extraction and coordination (Rimell et al., 2009; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, esp</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis using support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT,</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="6028" citStr="Yamada and Matsumoto, 2003" startWordPosition="913" endWordPosition="916">ich a parser would follow when tracing out a single, correct derivation; then we describe how a model of normal-form derivations — or, more accurately, a sequence of shift-reduce actions leading to a normal-form derivation — can be used with beam-search to develop a nondeterministic parser which selects the highest scoring sequence of actions. Note this section only describes a normal-form derivation model for shiftreduce parsing. Section 3 explains how we extend the approach to dependency models. The shift-reduce algorithm adapted to CCG is similar to that of shift-reduce dependency parsing (Yamada and Matsumoto, 2003; Nivre and McDonald, 2008; Zhang and Clark, 2008; Huang and Sagae, 2010). Following Zhang and Clark (2011), we define each item in the parser as a pair (s, q), where q is a queue of remaining input, consisting of words and a set of possible lexical categories for each word (with q0 being the front word), and s is the stack that holds subtrees s0, s1,... (with s0 at the top). Subtrees on the stack are partial derivastep stack (sn, ..., s1, s0) queue (q0, q1, ..., qm) action 0 N/N Mr. President visited Paris SHIFT 1 N/N N President visited Paris N visited Paris NP visited Paris NP (S[dcl]\NP)/N</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H Yamada and Y Matsumoto. 2003. Statistical dependency analysis using support vector machines. In Proc. of IWPT, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Kai Zhao</author>
</authors>
<title>Max-violation perceptron and forced decoding for scalable mt training.</title>
<date>2013</date>
<booktitle>In Proc. EMNLP,</booktitle>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="36674" citStr="Yu et al. (2013)" startWordPosition="6441" endWordPosition="6444">uce CCG parser, which fully aligns CCG parsing with the left-to-right, incremental nature of a shiftreduce parser. Our work is in part inspired by the dependency models of Clark and Curran (2007) and, in the use of a dependency oracle, is close in spirit to that of Goldberg and Nivre (2012). The difference is that the Goldberg and Nivre parser builds, and scores, dependency structures directly, whereas our parser uses a unification mechanism to create dependencies, and scores the CCG derivations, allowing great flexibility in terms of what dependencies can be realized. Another related work is Yu et al. (2013), which introduced a similar technique to deal with spurious ambiguity in MT. Finally, there may be potential to integrate the techniques of Auli and Lopez (2011), which currently represents the state-of-the-art in CCGBank parsing, into our parser. Acknowledgements We thank the anonymous reviewers for their helpful comments. Wenduan Xu is fully supported by the Carnegie Trust and receives additional funding from the Cambridge Trusts. Stephen Clark is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/I037512/1. Yue Zhang is supported by Singapore MOE Tier2 grant T2MOE201301. </context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao. 2013. Max-violation perceptron and forced decoding for scalable mt training. In Proc. EMNLP, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<location>Hawaii, USA.</location>
<contexts>
<context position="4101" citStr="Zhang and Clark, 2008" startWordPosition="608" endWordPosition="611">e. In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce parsing applies naturally to CCG (Zhang and Clark, 2011), and the left-to-right, incremental nature of the decoding fits with CCG’s cognitive claims. The discriminative model is global and trained with the structured perceptron. The decoder is based on beam-search 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics (Zhang and Clark, 2008) with the advantage of linear-time decoding (Goldberg et al., 2013). A main contribution of the paper is a novel technique for training the parser using a dependency oracle, in which all derivations are hidden. A challenge arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is bu</context>
<context position="6077" citStr="Zhang and Clark, 2008" startWordPosition="922" endWordPosition="925">orrect derivation; then we describe how a model of normal-form derivations — or, more accurately, a sequence of shift-reduce actions leading to a normal-form derivation — can be used with beam-search to develop a nondeterministic parser which selects the highest scoring sequence of actions. Note this section only describes a normal-form derivation model for shiftreduce parsing. Section 3 explains how we extend the approach to dependency models. The shift-reduce algorithm adapted to CCG is similar to that of shift-reduce dependency parsing (Yamada and Matsumoto, 2003; Nivre and McDonald, 2008; Zhang and Clark, 2008; Huang and Sagae, 2010). Following Zhang and Clark (2011), we define each item in the parser as a pair (s, q), where q is a queue of remaining input, consisting of words and a set of possible lexical categories for each word (with q0 being the front word), and s is the stack that holds subtrees s0, s1,... (with s0 at the top). Subtrees on the stack are partial derivastep stack (sn, ..., s1, s0) queue (q0, q1, ..., qm) action 0 N/N Mr. President visited Paris SHIFT 1 N/N N President visited Paris N visited Paris NP visited Paris NP (S[dcl]\NP)/NP visited Paris NP (S[dcl]\NP)/NP N Paris NP (S[d</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proc. of EMNLP, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Shift-reduce CCG parsing.</title>
<date>2011</date>
<booktitle>In Proc. ACL 2011,</booktitle>
<pages>683--692</pages>
<location>Portland, OR.</location>
<contexts>
<context position="3669" citStr="Zhang and Clark, 2011" startWordPosition="547" endWordPosition="550">m Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependency recovery makes sense from an evaluation perspective. In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce parsing applies naturally to CCG (Zhang and Clark, 2011), and the left-to-right, incremental nature of the decoding fits with CCG’s cognitive claims. The discriminative model is global and trained with the structured perceptron. The decoder is based on beam-search 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics (Zhang and Clark, 2008) with the advantage of linear-time decoding (Goldberg et al., 2013). A main contribution of the paper is a novel technique for training the parser using a dependency or</context>
<context position="5109" citStr="Zhang and Clark (2011)" startWordPosition="768" endWordPosition="771">n is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptron learning with beam-search (Collins and Roark, 2004) can be extended to handle the additional ambiguity, by adapting the “violationfixing” perceptron of Huang et al. (2012). Results on the standard CCGBank tests show that our parser achieves absolute labeled F-score gains of up to 0.5 over the shift-reduce parser of Zhang and Clark (2011); and up to 1.05 and 0.64 over the normal-form and hybrid models of Clark and Curran (2007), respectively. 2 Shift-Reduce with Beam-Search This section describes how shift-reduce techniques can be applied to CCG, following Zhang and Clark (2011). First we describe the deterministic process which a parser would follow when tracing out a single, correct derivation; then we describe how a model of normal-form derivations — or, more accurately, a sequence of shift-reduce actions leading to a normal-form derivation — can be used with beam-search to develop a nondeterministic parser which selects th</context>
<context position="8412" citStr="Zhang and Clark (2011)" startWordPosition="1328" endWordPosition="1331">ension to the deterministic example. At each step, a beam is used to store the top-k highest-scoring items, resulting from expanding all items in the previous beam. An item becomes a candidate output once it has an empty queue, and the parser keeps track of the highest scored candidate output and returns the best one as the final output. Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. We refer to the shift-reduce model of Zhang and Clark (2011) as the normal-form model, where the oracle for each sentence specifies a unique sequence of gold-standard actions which produces the corresponding normal-form derivation. No dependency structures are involved at training and test time, except for evaluation. In the next section, we describe a dependency oracle which considers all sequences of actions producing a goldstandard dependency structure to be correct. 1See Hockenmaier (2003) and Clark and Curran (2007) for a description of CCG rules. 219 Mr. President visited Paris N/N N (S[dcl]\NP)/NP NP &gt; &gt; N S[dcl]\NP &gt;TC &lt; S[dcl] (a) Mr. Presiden</context>
<context position="25443" citStr="Zhang and Clark (2011)" startWordPosition="4430" endWordPosition="4433">set of reachable disjunctive nodes of each conjunctive node c in ΦG as: D(c) = δ(c) U (Uc&apos;Eγ(d),dEδ(c)(D(c&apos;))) Each D is implemented as a hash map, which allows us to test the membership of one potential conjunctive node in 0(1) time. For example, a conjunctive node c E PL(cs0) is reachable from clex0 if there is a disjunctive node d E D(c) s.t. clex0 E γ(d). With this implementation, the complexity of checking each valid SHIFT action is then 0(|PL(cs0)|). 3.3 Training We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action. The normal-form model of Zhang and Clark (2011) uses an early update mechanism (Collins and Roark, 2004), where decoding is stopped to update model weights whenever the single gold action falls outside the beam. In our parser, there can be multiple gold items in a beam. One option would be to apply early update whenever at least 223 Algorithm 3 Dependency Model Training Input: (y, G) and beam size k 1: w ← 0; B0 ← ∅; i ← 0 2: B0.push(hs, qi0) &gt; the initial item 3: cand ← ∅ &gt; candidate output priority queue 4: gold ← ∅ &gt; gold output priority queue 5: while Bi =6 ∅ do 6: for each hs, qi ∈ Bi do 7: if |q |= 0 then &gt; candidate output 8: cand.p</context>
<context position="28339" citStr="Zhang and Clark (2011)" startWordPosition="4997" endWordPosition="5000">s 5In Algorithm 3 we abuse notation by using nG[0] to denote the highest scoring gold item in the set. to be made regarding which gold item to update against. We choose to reward the highest scoring gold item, in line with the violation-fixing framework; and penalize the highest scoring incorrect item, using the standard perceptron update. A final update is performed if no more expansions are possible but the final output is incorrect. 4 Experiments We implement our shift-reduce parser on top of the core C&amp;C code base (Clark and Curran, 2007) and evaluate it against the shift-reduce parser of Zhang and Clark (2011) (henceforth Z&amp;C) and the chartbased normal-form and hybrid models of Clark and Curran (2007). For all experiments, we use CCGBank with the standard split: sections 2-21 for training (39,604 sentences), section 00 for development (1,913 sentences) and section 23 (2,407 sentences) for testing. The way that the CCG grammar is implemented in C&amp;C has some implications for our parser. First, unlike Z&amp;C, which uses a context-free cover (Fowler and Penn, 2010) and hence is able to use all sentences in the training data, we are only able to use 36,036 sentences. The reason is that the grammar in C&amp;C d</context>
<context position="32577" citStr="Zhang and Clark (2011)" startWordPosition="5753" endWordPosition="5756">a value of 128 was found to achieve a reasonable balance of accuracy and speed; hence this value was used for all experiments. Since C&amp;C always enforces non-fragmentary output (i.e. it can only produce spanning analyses), it fails on some sentences in the development and test sets, and thus we also evaluate on the reduced sets, following Clark and Curran (2007). Our parser does not fail on any sentences because it permits fragmentary output (those cases where there is more than one subtree left on the final stack). The results for Z&amp;C, and the C&amp;C normal-form and hybrid models, are taken from Zhang and Clark (2011). Table 1 shows the accuracies of all parsers on the development set, in terms of labeled precision and recall over the predicate-argument dependencies in CCGBank. On both the full and reduced sets, our parser achieves the highest F-score. In comparison with C&amp;C, our parser shows significant increases across all metrics, with 0.57% and 1.06% absolute F-score improvements over the hybrid and normal-form models, respectively. Another major improvement over the other two parsers is in sentence level accuracy, LSent, which measures the number of sentences for which the dependency structure is comp</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Shift-reduce CCG parsing. In Proc. ACL 2011, pages 683–692, Portland, OR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>