<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.96741875">
COMPUTATIONAL COMPLEXITY AND
LEXICAL FUNCTIONAL GRAMMAR
Robert C. Berwick
MIT Artificial Intelligence Laboratory, Cambridge, MA
</note>
<sectionHeader confidence="0.992739" genericHeader="abstract">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.996973964912281">
An important goal of modern linguistic theory is to characterize as narrowly
as possible the class of natural languages. An adequate linguistic theory
should be broad enough to cover observed variation in human languages, and
yet narrow enough to account for what might be dubbed &amp;quot;cognitive
demands&amp;quot; -- among these, perhaps, the demands of leamability and
parsability. If cognitive demands are to.carry any real theoretical weight, then
presumably a language may be a (theoretically) possible human language,
and yet be &amp;quot;inaccessible&amp;quot; because it is not learnable or parsable.
Formal results along these lines have already been obtained for certain kinds
of Transformational Generative Grammars: for example, Peters and Ritchie
[1.1, showed that Aspecrs-style unrestricted transformational grammars can
generate any recursively enumerable set; while Rounds [2] [3] extended this
work by demonstrating that modestly restricted transformational grammars
(TGs) can generate languages whose recognition time is provably
exponential. (In Rounds&apos; proof, transformation are subject to a &amp;quot;terminal
length non-decreasing&amp;quot; condition, as suggested by Peters and Myhill.) Thus,
in the worst case TGs generate languages whose recognition is widely
recognized to be computationally intractable. Whether this &amp;quot;worst case&amp;quot;
complexity analysis has any real import for actual linguistic study has been
the subject of some debate (for discussion, see Chomsky [4]; Berwick and
Weinberg [51). Without resolving that controversy here howeser, one tinny
can be said: to make TGs efficiently parsablc one might provide actediknii
constraints. For instance, these additional strictures could be roughly of the
sort advocated in Marcus&apos; work on parsing [6] -- constraints specifying that
TO-based languages must have parsers that meet certain &amp;quot;lecality
conditions&amp;quot;. The Marcus&apos; constraints apparently amount to an extension of
Knuth&apos;s LR(k) locality condition [7] to a (restricted) version of a two-stack
deterministic push-down automaton. (The need tor LR(k)-like restrictions in
order to ensure efficient processability was also recognized by Rounds (21.)
Recently, a new theory of grammar has been advanced with the explictly
stated aim of meeting the dual demands of leamability and parsability -- the
Lexical Functional Grammars (LFGs) of Bresnan [8]. The theory of Lexical
Functional Grammars is claimed to have all the descriptive merits of
transformational grammar, but none of its computational unruliness. In
LFG, there are no transformations (as classically described); the work
formerly ascribed to transformations such as &amp;quot;passive&amp;quot; is shouldered by
information stored in Ilmical entries associated with lexical items. The
elimination of transformational power naturally gives rise to the hope that a
lexically-based system would be computationally simpler than a
transformational one.
An interesting question then is to determine, as has already been done for the
case of certain brands of transformational grammar, just what the &amp;quot;worst
case&amp;quot; computational complexity for the recognition of LFG languages is. If
the recognition time complexity for languages generated by the basic LFG.
theory can be as complex as that for languages generated by a modestly
restricted transformational system, then presumably I.FG will also have to
add additional constraints, beyond those provided in its basic theory, in order
to ensure efficient parsability.
The main result of this paper is to show that certain Lexical Functional
Grammars can generate languages whose recognition time is very likely
computationally intractable, at least according to our current understanding
of what is or is not rapidly solvable. Briefly. the demonstration proceeds by
showing how a problem that is widely conjectured to be computationally
difficult -- namely, whether there exists an assignment of l&apos;s and O&apos;s (or &amp;quot;Ts
and &amp;quot;F&amp;quot;s) to the literals of a Boolean formula in conjunctive normal form that
makes the formula evaluate to &amp;quot;1&amp;quot; fur &amp;quot;true&amp;quot;) -- can be re-expressed as the
problem of recognizing whether a particular string is or is not a member of
the language generated by a certain lexical functional grammar, &apos;Ibis
&amp;quot;reduction&amp;quot; shows that in the worst case the recognition of LFG languages
can be just as hard as the original Boolean satisfiability problem. Since it is
widely conjectured that there cannot be a polynomial-time algorithm for
satisfiability (the problem is NP-complete), there cannot be a polynomial-time
recognition algorithm for LFG&apos;s la general either. Note that this result
sharpens that in Kaplan and Bresnan [8]: there it is shown only that LFG&apos;s
(weakly) generate some subset of the class of context-sensitive languages
(including some strictly context-sensitive languages) and therefore, in the
worst case, exponential time is known to be sufficient (though not necessary)
to recognize any LFG language. The result in [8] thus does not address the
question of how much time, in the worst case, is necessary to recognize LFG
languages. The result of this paper indicates that in the worst case more than
polynomial tune will probably be necessary. (The reason for the hedge
&amp;quot;probably&amp;quot; will become apparent below; it hinges upon the central unsolved
conjecture of current complexity theory.) In short then, this result places the
LFG languages more precisely in the complexity hierarchy.
It also turns out to be instructive to inquire into just min a lexically-based
approach can turn out to be computationally difficult, and how
computational tractability may be guaranteed. Advocates of lexically-based
theories may have thought (and some have explicitly stated) that the
banishment of transformations is a computationally wise move because
transformations are computationally &amp;quot;expensive.&amp;quot; Eliminate the
transformations, so this casual argument goes, and one has eliminated all
computational problems. Intriguingly though, when one examines the proof
to be given below, the computational work done by transformations in older
theories re-emerges in the lexical grammar as the problem of choosing
between alternative categorizations for lexical items -- deciding, in a manner
of speaking, whether a particular terminal item is a Noun or a Verb (as with.
the word kiss in English). This power of choice, coupled with an ability to
express co-occurrence constraints over arbitrary distances across terminal
tokens in a string (as in Subject-Verb number agreement) seems to be all that
is required to make the recognition of LFG languages intractable. The work
done by transformations has been exchanged for work done by lexical
schemas, but the overall computational burden remains roughly the same.
This leaves the question posed in the opening paragraph: just what sorts of
constraints on natural languages are required in order to ensure efficient
parsability? An infonlial argument can be made that Marcus&apos; work [61_
provides a good first attack on just this kind of characterization. Marcus&apos;
claim was that languages easily parsed (not &amp;quot;garden-pained&amp;quot;) by petiole could
be precisely modeled by the languages easily parsed by a certain type of
restricted, deterministic, two-stack parsing machine. But this machine can he
shown to be a (weak) non-canonical extension of the I.R(k) grammars, as
proposed by Knuth [5].
Finally, this paper will discuss the relevance of this technical result for more
down-to-earth computational linguistics. As it turns out, even though general
LFG&apos;s may well be computationally intractable, it is easy to imagine a variety
of additional constraints for LFG theory that provide a way to sidestep
around the reduction argument. All of these additional restrictions amount to
making the LFG theory more restricted, in such a way that the reduction
argument cannot be made to work. For example, one effective restriction is
to stipulate that there can only be a finite stock of features with which to label
lexical items. In any case, the moral of the story is an unsurprising one:
specificity and constraints can absolve a theory of computational
intractability. What may be more surprising is that the requisite locality
constraints seem to be useful for a variety of theories of grammar, from
transformational grammar to lexical functional grammar.
</bodyText>
<page confidence="0.997683">
7
</page>
<listItem confidence="0.570218">
2. A REVIEW QE REDUCTION ARGUMENTS
</listItem>
<bodyText confidence="0.99968175">
The demonstration of the computational complexity of LFGs relies upon the
standard compiexity-theoretic technique of reduction. Because this method
may be unfamiliar to many readers, a short review is presented immediately
below; this is followed by a sketch of the reduction proper.
The idea behind the reduction technique is to take a difficult problem, in this
case, the problem of determining the satisfiability of Boolean formulas in
conjunctive normal form (CNF), and show that the known problem can be
quickly transformed into the problem whose complexity remains to be
determined, in this case, the problem of deciding whether a given string is in
the language generated by a given Lexical Functional Grammar. Before the
reduction proper is reviewed, some definitional groundwork must be
presented. A Boolean formula in conjunctive norma/form is a conjunction of
disjunctions. A formula is satisfiable just in case there exists some assignment
of Ts and Fs (or l&apos;s and O&apos;s) to the literals of the formula Xi that forces the
evaluation of the entire formula to be I; otherwise, the formula is said to be
unsaleable. For example.
</bodyText>
<equation confidence="0.902168">
(X2VX3VX7)A(X3V3Z .
2V XdA (3(3 V )73 VR7)
</equation>
<bodyText confidence="0.982980545454545">
is satisfiable, since the assignment of X2=1 (hence 72= F), X3= F (hence
X3.7). X7= F (3Z7=T). Xi =T (X1= F), and Xy= F makes the whole
formula cvalutc to &apos;7&apos;. The reduction in the proof below uses a somewhat
more restricted format where every term is comprised of the disjunction of
exactly three literals, so-called 3-CNRor &amp;quot;3-SAT&apos;). This restriction entails
no loss of generality (see Hoperoft and Ullman, 191. Chapter 12), since this
restricted format is also NP-complete.
How does a reduction show that the LFG recognition problem must be at
least as hard (computationally speaking) as the original problem of Boolean
satisfiability? &apos;Me answer is that any decision procedure for LFG recognition
could be used as-a correspondingly fast procedure for 3-CNI:, as follows:
</bodyText>
<listItem confidence="0.9587828">
(1) Given an instance of a 3-CNF problem (the question of whether there
exists a satisfying assignment for a given formula in 3-CNE), apply the
transformational algorithm provided by the reduction: this algorithm is itself
assumed to execute quickly, in polynomial time or less. The algorithm
outputs a corresponding LFG decision problem, namely: (i) a lexical
</listItem>
<bodyText confidence="0.936784636363636">
functional grammar and (ii) a string to be tested for membership in the
language generated by the I.FG. The LFG recognition problem represents or
mimics the decision problem for 3-CNI: in the sense that the &amp;quot;yes&apos;&apos; and &amp;quot;no&amp;quot;
answers to both satisfiability problem and membership problem must
coincide (if there is a satisfying assignment, then the corresponding LEG
decision problem should give a &amp;quot;yes&amp;quot; answer. etc.).
(2) Solve the LEG decision problem&amp;quot; the string-LEG pair — output by Step
I.: if the string is in the LFG language, the original formula was satisfiable; if
not. unsatisfiable.
(Note that the grammar and string so constructed depend upon just what
fonnula is under analysis: that is. for each different CNF formula, the
procedure presented above outputs a different LFG grammar and string
combination. In the LFG case it is important to remember that &amp;quot;grammar&amp;quot;
really means &amp;quot;grammar plus lexicon&amp;quot; — as one might expect in a
lexically-based theory. S. Peters has observed that a slightly different
reduction allows one to keep most of the grammar fixed across all possible
input formulas, constructing only different-sized lexicons for each different
CN I&apos; formula; for details, see below.)
To sec how a reduction can tell us something about the &amp;quot;worst case&amp;quot; time or
space complexity required to recognize whether a string is or is not in an LFG
language, suppose for example that the decision procedure for determining
whether a string is in an LFG language takes polynomial time (that is, takes
time nk on a deterministic Turing machine, for some integer k, where n= the
length of the input suing). Then, since the composition of two polynomial
algorithms can be readily shown to take only polynomial time (see 191
Chapter 12), the entire process sketched above, from input of the CNF
formula to the decision about its satistiability, will take only polynomial time.
However. CNF (or 3-CNF) has no known polynomial time algorithm, and
indeed, it is considered exceedingly unlikely that one could exists. Therefore,
it is just as unlikely that LFG recognition could be done (in general) in
polynomial time.
The theory of computational complexity has a much more compact term for
problems like CNF: CNF is NP-comolete. This label is easily deciphered:
</bodyText>
<listItem confidence="0.976269714285714">
(1) CNF is in the clasa NP, that is, the class of languages that can be
recognized by a o-deterministic Turing machine in polynomial time.
(Hence the abbreviation &amp;quot;NP&amp;quot;, for &amp;quot;non-deterministic polynomial&amp;quot;. To see
that CNF is in the class NP, note that one can simply guess all possible
combinations of truth assignments to literals. and check each guess in
polynomial time.)
(2) CNF is complete, that is, all other languages in the class NP can be quickly
</listItem>
<bodyText confidence="0.964709636363636">
reduced to some CNF formula. (Roughly, one shows that Boolean formulas
can be used to &amp;quot;simulate&amp;quot; any valid computation of a non-deterministic
Turing machine.)
Since- the class of problems solvable in polynomial time on a deterministic
Turing machine (conventionally notated. P) is trivially contained in the class
so solved by a nondcterministic Turing machine, the class P must be a subset
of the class NP. A well-known, well-studied, and still open question is whthcr
the class P is a proper subset of the class NP, that is, whether there are
problems solvable in non-deterministic polynomial time that cannot be
solved in deterministic polynomial time. Because all of the several thousand
NP-complete problems now catalogued have so far proved recalcitrant to
deterministic polynomial time solution, it is widely held that P must indeed
be a proper subset of NP, and therefore that the best possible algorithms for
solving NP.complete problems must take more than polynomial time (in
general, the algorithms now known for such problems involve exponential
combinatorial search, in one fashion or another; these are essentially methods&apos;
that do no better than to brutally simulate -- deterministically. of course — a
non-deterministic machine that &amp;quot;guesses&amp;quot; possible answers.)
To repeat the force of the reduction argument then, if ail LFG recognition
problems were solvable in polynomial time, then the ability to quickly reduce
CNF formulas to LFG recognition problems implies that all NP-complete
problems would be solvable in polynomial time, and that the class P= the
class NP. This possibility seems extremely remote. Hence, our assumption
that there is a fast (general) procedure for recognizing whether a string is or is
not in the language generated by an arbitrary LFG grammar roust be faLse.
In the terminology of complexity theory, LFG recognition must be NP-hard
— &amp;quot;as hard as&amp;quot; any other NP problem, including the NP-complete problems.
This means only that LFG recogntion is at least as hani as other NP-complete
problems -- it could still be more difficult (lie in some class that contains the
class NP). If one could also show that the languages generated by LFGs are
in the class NP, then LFGs would be shown to be NP-complete. This paper
stops short of proving this last claim, but simply conjectures that LFGs are in
the class NP.
</bodyText>
<listItem confidence="0.622274">
3. a SKETCH QE niE REDUCTION
</listItem>
<bodyText confidence="0.999860176470588">
To carry out this demonstration in detail, one must explicitly describe the
transformation procedure that takes as input a formula in CNF and outputs a
corresponding LFG decision problem — a string to be tested for membership
in a LFG language and the LFG itself. One must also show that this can be
done quickly, in a number of steps proportional to (at most) the length of the
original formula to some polynomial power. Let us dispose of the last point
first. The string to be tested for membership in the LFG language will simply
be the original formula, sans parentheses and logical symbols: the LFG
recognition problem is to find a well-formed derivation of this string with
respect to the grammar to be provided. Since the actual grammar and suing
one has to write down to &amp;quot;simulate&amp;quot; the CNF problem turn out to be no
worse than linearly larger than the original formula, an upper bound of say,
time n-cubed (where n= length of the original .formula) is more than
sufficient to construct a corresponding LFG: thus the reduction procedure
itself can be done in polynomial time, as required. This paper will therefore
have nothing further to say about the time bound on the transformation
procedure.
</bodyText>
<page confidence="0.987905">
8
</page>
<bodyText confidence="0.987031214285714">
Some caveats are in order before embarking on a proof sketch of this
reduction. First of all, the relevant details of the LFG theory will have to be
covered on-the-fly; see (11) for more discussion.&apos; Also, the grammar that is
output by the reduction procedure will nts look very much like a grammar
for a natural language, although the grammatical devices that will be
employed will in every way be those that are an essential part of the LFG
theory. (namely, feature agreement, the lexical analog of Subject or Object
&amp;quot;control&amp;quot;, lexical ambiguity, and a garden variety context-free grammar.) In
other words, although it is most unlikely that any natural language would
encode the satisfiability problem (and hence be intractable) in just the
manner outlined below, on the other hand, no &amp;quot;exotic&amp;quot; LFG machinery is
used in the reduction. Indeed, some of the more powerful LFG notational
formalisms long-distance binding, existential and negative feature operators
— have not been exploited. (An earlier proof made use of an existential
operator in the feature machinery of LFG, but the reduction presented here
does not)
To make good this demonstration one must set out just what the satisfiability
problem is and what the decision problem for membership in an LFG
language is. Recall that a formula in conjunctive normal form is satisfiable
just in case every conjunctive term evaluates to Ina that is, at least me literal
in sash term is true. The satisfiability problem is to find an assignment of Ts
and F&apos;s to the literals at the bottom (note that the complement of literals is
also permitted) such that the root node at the top gets the value &apos;T&apos; (for
one). How can we get a lexical functional grammar to represent this
problem? What we want is for satisfying assignments to correspond to to
well-formed sentences of some corresponding LFG grammar, and
npn-satisfwing assignments to correspond to sentences that are net
well-formed, according to the LFG grammar.
</bodyText>
<figureCaption confidence="0.744506">
Figure L A Reduction Must Preserve Solutions to the Original Problem
</figureCaption>
<bodyText confidence="0.999232475">
Since one wants the satisfying/non-sadsfying assignments of any particular
formula to map over into well-formed/ill-formed sentences, one must
obviously exploit the LFG machinery for capturing well-formedness
conditions for sentences. First of all, an LFG contains a tax context-free
&apos;rammer. A minimal condition for a sentence (considered as a string) to be in
the language generated by a lexical-functional grammar is that it can be
generated by this base grammar; such a sentence is then said to have a
well-formed constituent structure. For example, if the base rules included
SNP VP; VP V NP, then (glossing over details of Noun Phrase rules)
the sentence John kissed the baby would be well-formed but John the baby
kissed would not. Note that this assumes, as usual, the existence of a lexicon
that provides a categorization for each terminal item, e.g., that baby is of the
category N. kissed is a V, etc. Importantly then.&apos; this well-formedness
condition requires us to provide at least one legitimate name tree for the
candidate sentence that shows how it may be derived from the underlying
LFG base context-free grammar. (There could be more than one legitimate
tree if the underlying grammar is ambiguous.) Note further that the choice of
categorization for a lexical item may be crucial. If baby was assumed to be of
category V. then both sentences above would be ill-formed.
A second major component of the LFG theory is the provision for adding a
set of so-called funaional equations to the base context-free rules. These
equations are used to account for that the co-occurrence restrictions that are
so much a part of natural languages (e.g., Subject-Verb agreement). Roughly,
one is • allowed to associate featual with lexical entries and with the
non-terminals of specified context-free rules; these features have alum. The
equation machinery is used to pass features in certain ways around the parse
tree, and conflicting values for the same feature arc cause for rejecting a
candidate analysis, To take the Subject-Verb agreement example, consider
the sentence the baby is kissing John. The lexical entry for baby (considered
as a Noun) might have the Number feature, with the value singular. The
lexical entry for is might assert that the number feature of the Subject above
it in the parse tree must have the value singular; meanwhile, the feature
values for 5ubiect are automatically found by another rule (associated with
the Noun Phrase portion of SNP VP) that grabs whatever features it finds
below the NP node and copies them up above to the S node. Thus the S node
gets the Subject feature, with whatever value it has passed from baby below --
namely, the value aineular; this accords with the dicates of the verb is, and all
is well. Similarly, in the sentence, the boys in the band is kissing John, boys
passes up the number value plural and this clashes with the verb&apos;s constraint;
as a result this sentence is judged ill-formed:
</bodyText>
<sectionHeader confidence="0.661353" genericHeader="method">
S features: Subject Number.Singular or Plural?
= CLASH!
</sectionHeader>
<figure confidence="0.964420285714286">
,
iNP s
i N&apos;.
Numberplural V*. Number:singular
iI
1
the boys in the band i51 kissing John.
</figure>
<figureCaption confidence="0.961017">
Figure 2. Co-occurrence Restrictions are Enforced by Feature Checking in an
LFG.
</figureCaption>
<bodyText confidence="0.999885027777778">
It is important to note that the feature compatability check requires (1) a
particular constituent structure tree (a parse tree); and (2) an assignment of
terminal items (words) to lexical categories -- e.g., in the first Subject-Verb
agreement example above, baby was assigned to be of the category N, a
Nun. The tree is obviously required because the feature checking
machinery propagates values according to the links specified by the
derivation tree; the assignment of terminal items to categories is crucial
because in most cases the values of features are derived from those listed in
the lexical entry for an item (as the value of the number feature was derived
from the lexical entry for the Noun form of baby). One and the same
terminal item can have two distinct lexical entries, corresponding to distinct
lexical categorizations; for example, baby can be both a Noun and a Verb. If
we had picked baby to be a Verb, and hence had adopted whatever features
are associated with the Verb entry for baby to be propagated up the tree, then
the string that was previously well-formed. the baby is kissing John would
now be considered deviant. If a string is ill-formed under all possible
derivation trees and assignments of features from possible lexical
categorizations, then that string is not-in the language generated by the LFG.
The possibility of multiple derivation trees and lexical categorizations (and
hence multiple feature bundles) for one and the same terminal item plays a
crucial rule in the reduction proof: it is intended to capture the satisfiability
problem of deciding whether to give a literal X, a value of -r or &amp;quot;F&amp;quot;.
Finally, LFG also provides a way to express the familiar patterning of
grammatical relations (e.g.. &amp;quot;Subject&amp;quot;. and &amp;quot;Object&amp;quot;) found in natural
language. For example, transitive verbs must have objects. This fact of life
(expressed in an Aspects-style transformational grammar by subcatcgorization
restrictions) is captured in LFG by specifying a so-called PRED (for
predicate) feature with a Verb; the PRE1) can describe what grammatical
relations like &amp;quot;Subject&amp;quot; and &amp;quot;Object&amp;quot; must be filled in after feature passing
has taken place in order for the analysis to be well-formed. For instance, a
transitive verb like kiss might have the pattern. kiss((SubjectX0bject)), and
thus demand that the Subject and Object (now considered to be &amp;quot;features&amp;quot;)
have some value in the final analysis. The values for Subject and Object
might of course be provided from some other branch of the parse tree, as
provided by the feature propagation machinery; for example, the Object
feature could be filled in from the Noun Phrase part of the VP expansion:
</bodyText>
<figure confidence="0.9255275">
sentence w&apos; IS sentence w IS NOT
in LFG language L(G) in LFG language L(G)
satisfiable non-satisfiable
foirm la w form la w
9
SUBJECT: Sue
S features PRED : &apos;kiss&lt;(SubjectX0bject),
OBJECT: John
Sue / V NP
kis John
</figure>
<figureCaption confidence="0.9989365">
Figure 3. Predicate Templates Can Demand That a Subject or Object be
Filled In.
</figureCaption>
<bodyText confidence="0.912313379310345">
But if the Object were az Med in, then the analysis is declared fine:tonally
incomplete, and is ruled out. This device is used to cast out sentences such as,
the baby kisserl
So much for the LFG machinery that is required for the reduction proof.
(Them are additional capabilities in the LFG theory, such as long-distance
binding, but these will not be called upon in the demonstration below.)
What then does the LFG representation of the satisfiability problem look
like? Basically, there are three parts to the satistiability problem that must be
mimicked by the LFG: (1) the assignment of values to literals, e.g., X2-&gt;&amp;quot;T&amp;quot;;
X4-&gt;&amp;quot;F&amp;quot;; (2) the co-ordination of value assignments across intervening literals
in the formula; e.g., the literal X2 can appear in several different terms, but
one is nut allowed to assign it the value &apos;1&amp;quot; in one term and the value &amp;quot;F&amp;quot; in
another (and the same goes for the complement of a literal: if X2 has the
value &apos;1&amp;quot;, 72 cannot have the value &amp;quot;1&amp;quot;); and (3) satistiability must
correspond to LFG wc11-formedness, i.e., each term has the truth value &amp;quot;1&amp;quot;
just in case at least az literal in the tcnn is assigned —r and all terms must
evaluate to &amp;quot;1&amp;quot;.
Let us now go over how these components may be reproduced in an LFG,
one by one.
(1) Assignments: The input string to be tested for membership in the LFG
will simply be the original formula, sans parentheses and logical symbols; the
terminal items are thus just a string of Xi&apos;s. Recall that the job of checking
the string for well-formedncss involves finding a derivation tree for the string,
solving the ancillary co-occurrence equations (by feature propagation), and
checking for functional completeness. Now, the context-free grammar
constructed by the transformation procedure will be set up so as to generate a
virtual copy of the associated formula, down to the point where literals Xi are
assigned their values of &amp;quot;T&apos; or &amp;quot;F&amp;quot;. If the original CNF form had N terms,
this pan of grammar would look like:
</bodyText>
<subsectionHeader confidence="0.541209666666667">
Sx.s•TI T2 T (one &amp;quot;I&amp;quot;&apos; for each term)
Yj Yk (one triple of Y&apos;s per term)
Several comments are in order here.
</subsectionHeader>
<bodyText confidence="0.6234055">
(1) The context-free base that is built depends upon the original CNF
formula that is input, since the number of terms; a, varies from formula to
formula. In Stanley Peters&apos; improved version of the reduction proof, the
context-free base is fixed for all formulas with the rola:
</bodyText>
<subsectionHeader confidence="0.226054">
SS S&apos;
</subsectionHeader>
<bodyText confidence="0.816357071428571">
T T T or ST T F or T F F or T F Tor...
(remaining twelve expansions that have at least one &amp;quot;1&amp;quot; in each triple)
The Peters grammar works by recursing until the right number of terms is
generated (any sentences that are too long or too short cannot be matched to
the input formula). Thus, the number of terms in the original CNF formula
need not be explicitly encoded into the base grammar.
(2) The subscripts i,j, and k depend on the actual subscripts in the original
formula.
(3) The Y. are acs terminal items, but are non-terminals.
(4) This grammar will have to be slightly modified in order for the reduction
to work, as will become apparent shortly.
Note that so far there are no rules to extend the parse tree down to the level
of terminal items, the X. The next step does this and at the same time adds
the power to choose between &apos;1&amp;quot; and &amp;quot;F&amp;quot; assignments to literals. One
includes in the context-free base grammar Ltii productions deriving each
terminal item Xi, namely, XiT=a Xi and XiF=P Xi, corresponding to an
assignment of &amp;quot;1&amp;quot; or &amp;quot;F&apos; to the formula literal Xi (it is important not to get
confused here between the literals of the formula — these are terminal
elements in the lexical functional grammar — and the literals of the grammar
— the non-terminal symbols.) One must also add, obviously, the rules
Yi=&gt; XiTiXiF. for each i, and rules corresponding to • the negations of
variables, 71T=.47 Note that these are not &amp;quot;exotic&amp;quot; LFG rules: exactly the
same sort of rule is required in the baby case. i.e.. N=&gt; baby or V baby,
corresponding to whether baby is a Noun or a Verb. Now, the lexical entries
for the &amp;quot;XiT&amp;quot; categorization of Xi will look very different from the &amp;quot;Xir
categorization of Xi, just as one might expect the N and V forms for baby to
be different. Here is what the entries for the two categorizations of Xi look
like:
</bodyText>
<table confidence="0.574065375">
XIT (T truth-assignmen0= T
(rassign X)=T
XiF (iassign Xi) =F
The feature assignments for the negation of the literal Xi is simply the dual of
the entries above (since the sense of &amp;quot;T&apos; and &amp;quot;I&amp;quot; is reversed):
TiT (Ttruth-assignment)=T
(fassign X)= F.
(Tassign Xi) =T
</table>
<figureCaption confidence="0.73288025">
The role of the additional &amp;quot;truth-assignment&amp;quot; feature will be explained
below.
Figure 4. Sample Lexical Entries to Reproduce the Assignment of Ts and Fs
to a literal Xi.
</figureCaption>
<bodyText confidence="0.997479454545455">
The upward-directed arrows in the entries reflect the LFG feature
propagation machinery. In the case of the XiT entry, for instance, they say to
&amp;quot;make the Truth-assianment feature of the node above XT have the value
&amp;quot;T&amp;quot;, and make the 2f,4 portion of the Assign feature of the node above have
the value T.&amp;quot; This feature propagation device is what reproduces the
assignment of Ts and Fs to the CNF literals. If we have a triple of such
elements, and at least one of them is expanded out to XT, then the feature
propagation machinery of LFG will mug the common feature names into
one large structure for the node above, reflecting the assignments made;
moreover, the term will get a filled-in truth assignment value just in case at
least one of the expansions selected an XT path:
</bodyText>
<figure confidence="0.491947571428571">
T feature structure: Truth-assignment=r
Assig Xi= f
Xl= F
Xt = Ea_
Yi
XiT X.F XtF
terminal I I
</figure>
<figureCaption confidence="0.751723">
string: Xi X1 Xt
Figure 5. The I.FG Feature Propagation Machinery is Used to Percolate
Feature Assignments from the Lexicon.
ONO
</figureCaption>
<page confidence="0.991626">
10
</page>
<bodyText confidence="0.792204277777778">
(The features are passed transparently through the intervening
Yi nodes via the LFG &amp;quot;copy&amp;quot; device. = I.):
this simply means that all thc features of the node below the node to
which the &amp;quot;copy&amp;quot; up-and-down arrows are attached are to be
the same as those of the node above the up-and-down arrows.)
It is plain that this Mechanism mimics the assignment of values To literals
required by the satistlability problem.
(2) Co-ordination of assignments: One must also guarantee that the X1 value
assigned at one place in the tree is not contradicted by an Xi or Xi elsewhere.
To ensure this, we use the LFG co-occurrence agreement machinery: the
Assign feature-bundle is passed up from each term Ti to the highest node in
the parse tree (one simply adds the (1 =1) notation to each Ti rule in order to
indicate this). The Assign feature at this node will thus contain the unioq of
all assign feature bundles passed up by all terms. If any Xi values conflict,
then the resulting structure is judged ill-formed. Thus, only compatible Xi
assignments are well-formed:
Figure 6, The Feature Compatability Machinery of LFG can Force
Assignments to be Co-ordinated Across Terms.
</bodyText>
<listItem confidence="0.986357">
(3) Preservation of satisfying assignments. Finally, one has to reproduce the
conjunctive character of the 3-CNF problem-- that is, a sentence is Satisfiable
(well-formed) iff each term has at least one literal assigned the value &amp;quot;T&amp;quot;.
Part of the disjunctive character of the problem has already been encoded in
the feature propagation machinery presented so far: if at least one Xi in a
</listItem>
<bodyText confidence="0.999280458333333">
term expands to the lexical entry XiT, then the Inah-assianment feature
gets the value T. This is just as desired. If one, two, or three of the literals Xi
in a term select XiT, then Ti&apos;s truth-assignment feature is T. and the analysis
is well-formed. But how do we rule out the case where au three Xi&apos;s in a term
select the &amp;quot;F&amp;quot; path, XiFI And how do we ensure that all terms have at least
one T below them?
Both of these problems can be solved by resorting to the LFG functional
completeness constraint. The trick will be to add a Elmci feature to a
&amp;quot;dummy&amp;quot; node attached to each term: the sole purpose of this feature will be
to refer to the feature Truth-as:ignment, just as the predicate template for the
transitive verb kiss* mentions the feature Object. Since an analysis is not
well-formed if the &amp;quot;grammatical relations&amp;quot; a Fred mentions are not filled in
from somewhere, this will have the effect of forcing the Trutltassianment
feature to get filled in every term. Sincc the &amp;quot;F&amp;quot; lexical entry does not have a
l&apos;oith-assienment value, if ail the Xi in a term triple select the XiF path (all
the literals are &amp;quot;F&amp;quot;) then on Truth-assignment feature is ever picked up from
the lexical entries, and that term never gets a Truth-assignment feature. This
violates what the predicate template demands, and so the whole analysis is
thrown out, (&apos;l&apos;he ill-formedness is exactly analogous to the case where a
transitive verb never gets an Object) Since this condition is applied to each
term, we have now guaranteed that each term must have at least tme literal
below it that selects the &apos;7&apos; path &amp;quot;just as desired. To actually add the new
predicate template, one simply adds a new (but dummy) branch to each term
with the appropriate predicate constraint attached to it
</bodyText>
<figureCaption confidence="0.987032">
Figure 7. Predicates Can be Used to Force at least one &amp;quot;I&apos; Per Term.
</figureCaption>
<bodyText confidence="0.99736753125">
There is a final subtle point here: one must prevent the Pred and
Truth-assignment features for each term from being passed up to the head
&amp;quot;S&amp;quot; node. The reason is that if these features were passed up, then since the
LFG machinery automatically merges the values of any features with the
same name at the topmost node of the parse tree, the LFG machinery would
force the union of the feature values for Pred and Truth-assignment over all
terms in the analysis tree. The result would be that if au term had at least
one (hence satisfying the Truth-assignment predicate template in at least
one term), then the Pred and Truth-assignment would get filled in at the
topmost node as well. The string below would be well&apos; formed if at least one&apos;
term were &apos;7&apos;, and this would amount to a disjunction of disjunctions (an
&amp;quot;OR&amp;quot; of &amp;quot;OR&amp;quot;s), not quite what is sought. To eliminate this possibility, one
must add a final trick: ma term T1 is given separate Predicate,
Truth-assignment. and Assign features, but only the Assign feature is
propagated to the highest node in the parse tree as such. In contrast, the
Predicate and Truth-assignment features for each term are kept &amp;quot;protected&amp;quot;
from merger by storing them under separate feature headings labelled
T .,T. The means by which just the ASSIGN feature bundle is lifted out is
n
the LFG analogue of the natural language phenomenon of Subject or Object
&amp;quot;control&amp;quot;, whereby just the features of the Subject or Object of a lower clause
are lifted out of the lower clause to become the Subject or Object of a matrix
sentence; the remaining features stay unmergeable because they stay
protected behind the individually labelled terms.
To actually &amp;quot;implement&amp;quot; this in an LFG one can add two new branches to
each Term expansion in the base context-free grammar, as well as two
&amp;quot;control&amp;quot; equation specifications that do the actual work of lifting the
features from a lower clause to the matrix sentence:
Natural language case (from [St pp. 43-45):
The girl persuaded the baby to go.
(part of the) lexical entry for
persuaileat
</bodyText>
<equation confidence="0.465949">
V et VCOMP Subject)= (? Object)
</equation>
<bodyText confidence="0.975683">
The notation (1 VCOMP Subject)=(1 Object) — dubbed a &amp;quot;control
equation&amp;quot; -- means that the features of the Object above the V(erb) node are
to be the same as those of the features of the Subject of the verb complement
(VCOMP). Hence the top-most node of the parse tree eventually has a
feature bundle something like:
object: {bundle of features for NP subject &amp;quot;the girl&amp;quot;}
redicate: &apos;persuade&lt;cr Sublet*? ObjectX?VcomPP.
bject: {bundle of features for NP Object &amp;quot;the baby&amp;quot;)
COPIED
erb
omplement: Subject: [bundle &apos;of features for NP subject &amp;quot;the baby&amp;quot;
&amp;quot;VCOMP&amp;quot;)
Predicate: &apos;go&lt;(TSubject)&gt;&apos;
Note how the Object features have been copied from the Subject
features of the Verb Complement, via the notation described above, but
the Predicate features of the Verb Complement were left behind.
The satisfiability analogue of this machinery is almost identical:
</bodyText>
<figure confidence="0.997409882352941">
features: Assign: [C7 = T or 1
Clash!
X7T X7T
Xs;
(iassign X7)= T (Tassilln X7= F)
Dummy2
lexical entry:
&apos;dummyr: I
Pred)=
&apos;clummy2&lt;(1 Truth-assigrunent)Y Xi
s‘
(iTrudrassignment)=T
featureitred: &apos;dummy2&lt;(TTruth-assignment)1
Truth-assignrnent=T
/
IS XLT •XF:--&amp;quot;RF
t
</figure>
<page confidence="0.991455">
11
</page>
<bodyText confidence="0.991493">
Phrase structure tree: attempt ICJ characterize all and only the natural languages. This discovery
would be on a par with, for example, Peters and Ritchie&apos;s observation that
T. although the context-sensitive phrase structure rules formally advanced in
linguistic theory have the power to generate non-context-free languages, that
A. T.COMP power has apparently never been used in immediate constituent analysis (111.
</bodyText>
<equation confidence="0.4109725">
Dummy2 Y. Y.
j
</equation>
<bodyText confidence="0.981387096774194">
One now attaches a &amp;quot;control equation&amp;quot; to the Ai node that forces the Assign.
feature bundle from the TiCOMP side to be lifted up to get merged into the
Assien feature bundle of the T node (and then, in turn, to become merged at
the topmost node of the tree by the usual full copy up-and-down arrows):
(T TiCOMP Assign)= (T Assign)
Note how this is just like the copying of the Subject features of a Verb
Complement into the Object position of a matrix clause.
4. REI EVA NCE QE COMPLEXITY RESUI T ,1 N I) CONCLUSIONS
The demonstration of the previous section shows that LFGs have enough
power to &amp;quot;simulate&amp;quot; a probably computationally intractable problem. But
what are we to make of this result? On the positive side, a complexity result
such as this one places the LFG theory more precisely in the hierarchy of
complexity classes. If we conjecture, as seems reasonable, that LEG language
recognition is actually in the class NP (that is, LFG recognition can be done
by a non-deterministic Turing machine in polynomial time), then LFG
language recognition is NP-complete. (This conjecture seems reasonable
because a non-deterministic Turing machine should be able to &amp;quot;guess&amp;quot;. all
feature propagation solutions using its non-deterministic power — including
any &amp;quot;long-distance&amp;quot; binding solutions, an LFG device not discussed here.
Since checking candidate solutions is quite rapid — it can be done in a2 time
or less, as described in (81 — recognition should be possible in polynomial
time on such a machine.) Comparing this result to other known language
reneges note that context-sensitive language recognition is in the class
polynomial space (&amp;quot;PSPACE&amp;quot;). since (non-deterministic) linear bounded
automata generate exactly the class of context-sensitive languages.
(Non-deterministic and deterministic polynomial space classes collapse
together, because of Savitch&apos;s well-known result (91 that any function
computable in non-deterministic space N can be computed in deterministic
space N2.) Furthermore, the class NP is clearly a subset of PSPACE (since if
a function uses Space N. it must use at least Time N), and it is suspected, but
not known for certain, that NP is a proper subset of PSPACE. (This being a
form of the P= NP question once again.) Our conclusion is that it is likely
that LEC&apos;s generate a proper subset of the context-sensitive languages. (In (81
it is shown that this includes some strictly context-sensitive languages.) It is
interesting that several other &amp;quot;natural&amp;quot; extensions of the context-free
languages — notably, the class of languages generated by the so-called
&amp;quot;indexed grammars&amp;quot; — also generate a subset of the context-sensitive
languages, including those strictly context-sensitive languages shown to be
generable by LFGs in [81, but arc provably NP-complete (see (21 for proofs).
Indeed, a cursory look at the power of the indexed grammars at least suggests
that they might subsume the machinery of the LFG theory; this would be a
good conjecture to check.
On the other side of the coin, how might one restrict LEG theory further so
as to avoid possible intractability? Several escape hatches immediately come
to mind; these will simply be listed here. Note that all of these &amp;quot;fixes&amp;quot; have
the effect of adding additional constraints to timber restrict the LFG theory.
1. Rule out &amp;quot;worst case&amp;quot; languages as linguistically irrelevant.
The probable computational intractability arises because co-occurrence
restrictions (compatibleas.signment of Xi&apos;s) can be forced across arbitrary
distances in the terminal string in conjunction with lexical ambiguity for each
terminal itcm. If some device can be found in natural languages that filters
out or removes such ambiguity locally (so that the choice of whether an item
is &amp;quot;r or &amp;quot;F&amp;quot; never depends on other items arbitrarily far away in the
terminal string), or if natural languages never employ such kinds of
co-occurrence restrictions, then the reduction is theoretically relevant, but
linguistically irrelevant. Note that such a finding would be a positive
discovery, since one would be able to further restrict the LFG theory in its
2. Add &amp;quot;locality principles&amp;quot; for recognition (or parsing).
One could simply stipulate that LFG languages meet some condition known
to ensure efficient recognizability, e.g., Knutles [71 LR(k) restriction, suitably
extended to the case of context-sensitive languages. (See (101 for more
details.)
</bodyText>
<sectionHeader confidence="0.768561" genericHeader="method">
3. Restrict the lexicon.
</sectionHeader>
<bodyText confidence="0.999916818181818">
The reduction depends crucially upon having an infinite stock of lexical items
and an infinite number of features with which to label them — several for
each literal X This is necessary because as CNF formulas grow larger and
larger, the number of literals can grow arbitrarily large, If, for whatever
reason, the stock of lexical items or feature labels is finite, then the reduction
method must fail after a certain point. This restriction seems ad hoc in the
case of lexical items, but perhaps less so in the case of features. (Speculating,
perhaps features require &amp;quot;grounding&amp;quot; in terms of other language/cognitive
sub-systems -- e.g., a feature might be required to be one of a finite number
of primitive &amp;quot;basis&amp;quot; elements of a hypothetical conceptual or sensori-motor
cognitive system.)
</bodyText>
<sectionHeader confidence="0.776653" genericHeader="method">
1.5,NelL12,12/ MEM
</sectionHeader>
<bodyText confidence="0.997958142857143">
I would like to thank Ron Kaplan, Ray Perrault. Christos Patiadimitriou; and
particularly Stanley Peters for various discussions about the contents of this
paper.
Thisreport describes research done at the Artificial Intelligence I aboratory
of me Massachusetts Institute of Technology. Support for the Laboratory&apos;s
artificial intelligence research is provided in part by the Office of Naval
Res‘arch under Office of Naval Research contract N00014-80-C-0505.
</bodyText>
<sectionHeader confidence="0.967413" genericHeader="method">
11 EFERENCES
</sectionHeader>
<reference confidence="0.998973740740741">
(11 Peters, S. and Ritchic, R. &amp;quot;On the generative power of transformational
grammars.&amp;quot; Information Sciences 6, 1973, pp. 49-81
121 Rounds. W. &amp;quot;Complexity of recognition in intermediate-level languages.&amp;quot;
Proceedings of the 14th Ann. Symp. on Switching Theory and Automata,
1973.
[31 Rounds W. &amp;quot;A grammatical characterization of exponential-time
languages,&amp;quot; Proceedings of the 16th Ann. Symp. on Switching Theory and
Automata, 1975, pp. 135-143.
[41 Chomsky, N. Rules and Representations New York: Columbia University
Press. 1980.
[51 Berwick, R. and Weinberg. A. The Role of Grammars in Models of
Language Use, unpublished MIT report, forthcoming, 1981.
[61 Marcus, M. A Theory of Syntactic Recognition for Natural Language,
Cambridge, MA: MIT Press, 1.980.
(71 Knuth, D. &amp;quot;On the translation of languages from left to right&amp;quot;,
Information and Control, 8, 1965, pp. 607-639.
[81 Kaplan. R. and Bresnan. 1. Lexical-fine:Iona&apos; Grammar: A Formal System
for Grammatical Representation, Cambridge, MA: MIT Cognitive Science
Occasional Paper #13, 1981. (also forthcoming in Bresnan, cd., The Mental
Representation of Grammatical Relations, Cambridge, MA: MIT Press, 1981.
[91 floperoft. 1. and Ullman, J. Introduction to Automata Theory, Languages,
and Computation, Reading, MA: Addison-Wesley, 1979.
[101 Berwick, R. Locality Principles and the Acquisition of Syntactic
Knowledge, MIT PhD. dissertation. 1981 forthcoming.
[111 Peters, S. and Ritchie, R. Context-sensitive immediate constituent
analysir context-free languages revisited, Mathematical Systems Theory, 6:4,
1973, pp. 324-333.
</reference>
<page confidence="0.99846">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003238">
<title confidence="0.989062">COMPUTATIONAL COMPLEXITY AND LEXICAL FUNCTIONAL GRAMMAR</title>
<author confidence="0.99995">Robert C Berwick</author>
<affiliation confidence="0.637732">MIT Artificial Intelligence Laboratory, Cambridge, MA</affiliation>
<abstract confidence="0.996575483985766">An important goal of modern linguistic theory is to characterize as narrowly as possible the class of natural languages. An adequate linguistic theory should be broad enough to cover observed variation in human languages, and yet narrow enough to account for what might be dubbed &amp;quot;cognitive demands&amp;quot; -among these, perhaps, the demands of leamability and parsability. If cognitive demands are to.carry any real theoretical weight, then presumably a language may be a (theoretically) possible human language, and yet be &amp;quot;inaccessible&amp;quot; because it is not learnable or parsable. Formal results along these lines have already been obtained for certain kinds of Transformational Generative Grammars: for example, Peters and Ritchie showed that transformational grammars can generate any recursively enumerable set; while Rounds [2] [3] extended this work by demonstrating that modestly restricted transformational grammars (TGs) can generate languages whose recognition time is provably exponential. (In Rounds&apos; proof, transformation are subject to a &amp;quot;terminal length non-decreasing&amp;quot; condition, as suggested by Peters and Myhill.) Thus, in the worst case TGs generate languages whose recognition is widely recognized to be computationally intractable. Whether this &amp;quot;worst case&amp;quot; complexity analysis has any real import for actual linguistic study has been the subject of some debate (for discussion, see Chomsky [4]; Berwick and Weinberg [51). Without resolving that controversy here howeser, one tinny be said: to make TGs efficiently parsablc one might provide constraints. For instance, these additional strictures could be roughly of the sort advocated in Marcus&apos; work on parsing [6] -constraints specifying that TO-based languages must have parsers that meet certain &amp;quot;lecality conditions&amp;quot;. The Marcus&apos; constraints apparently amount to an extension of Knuth&apos;s LR(k) locality condition [7] to a (restricted) version of a two-stack deterministic push-down automaton. (The need tor LR(k)-like restrictions in order to ensure efficient processability was also recognized by Rounds (21.) Recently, a new theory of grammar has been advanced with the explictly stated aim of meeting the dual demands of leamability and parsability -the Lexical Functional Grammars (LFGs) of Bresnan [8]. The theory of Lexical Functional Grammars is claimed to have all the descriptive merits of transformational grammar, but none of its computational unruliness. In LFG, there are no transformations (as classically described); the work formerly ascribed to transformations such as &amp;quot;passive&amp;quot; is shouldered by information stored in Ilmical entries associated with lexical items. The elimination of transformational power naturally gives rise to the hope that a lexically-based system would be computationally simpler than a transformational one. An interesting question then is to determine, as has already been done for the case of certain brands of transformational grammar, just what the &amp;quot;worst computational complexity for the recognition of LFG languages is. recognition time complexity for languages generated by the basic theory can be as complex as that for languages generated by a modestly restricted transformational system, then presumably I.FG will also have to add additional constraints, beyond those provided in its basic theory, in order to ensure efficient parsability. The main result of this paper is to show that certain Lexical Functional can generate languages whose recognition time likely computationally intractable, at least according to our current understanding of what is or is not rapidly solvable. Briefly. the demonstration proceeds by showing how a problem that is widely conjectured to be computationally difficult -namely, whether there exists an assignment of l&apos;s and O&apos;s (or &amp;quot;Ts and &amp;quot;F&amp;quot;s) to the literals of a Boolean formula in conjunctive normal form that the formula evaluate to &amp;quot;1&amp;quot; fur &amp;quot;true&amp;quot;) -can be re-expressed problem of recognizing whether a particular string is or is not a member of the language generated by a certain lexical functional grammar, &apos;Ibis &amp;quot;reduction&amp;quot; shows that in the worst case the recognition of LFG languages can be just as hard as the original Boolean satisfiability problem. Since it is widely conjectured that there cannot be a polynomial-time algorithm for (the problem is cannot be a polynomial-time algorithm for LFG&apos;s la generaleither. Note that this result sharpens that in Kaplan and Bresnan [8]: there it is shown only that LFG&apos;s (weakly) generate some subset of the class of context-sensitive languages (including some strictly context-sensitive languages) and therefore, in the case, exponential time is known to be sufficient(though not necessary) to recognize any LFG language. The result in [8] thus does not address the of how much time, in the worst case, is recognize LFG languages. The result of this paper indicates that in the worst case more than tune will necessary. (The reason for the hedge &amp;quot;probably&amp;quot; will become apparent below; it hinges upon the central unsolved conjecture of current complexity theory.) In short then, this result places the LFG languages more precisely in the complexity hierarchy. It also turns out to be instructive to inquire into just min a lexically-based approach can turn out to be computationally difficult, and how computational tractability may be guaranteed. Advocates of lexically-based theories may have thought (and some have explicitly stated) that the banishment of transformations is a computationally wise move because transformations are computationally &amp;quot;expensive.&amp;quot; Eliminate the transformations, so this casual argument goes, and one has eliminated all computational problems. Intriguingly though, when one examines the proof to be given below, the computational work done by transformations in older the lexical grammar as the problem of choosing between alternative categorizations for lexical items -deciding, in a manner speaking, whether a particular terminal item is a Noun or a Verb (as word English). This power of choice, coupled with an ability to express co-occurrence constraints over arbitrary distances across terminal tokens in a string (as in Subject-Verb number agreement) seems to be all that is required to make the recognition of LFG languages intractable. The work done by transformations has been exchanged for work done by lexical schemas, but the overall computational burden remains roughly the same. This leaves the question posed in the opening paragraph: just what sorts of on natural languages in order to ensure efficient parsability? An infonlial argument can be made that Marcus&apos; work [61_ provides a good first attack on just this kind of characterization. Marcus&apos; was that languages easily parsed (not &amp;quot;garden-pained&amp;quot;) by petiolecould be precisely modeled by the languages easily parsed by a certain type of restricted, deterministic, two-stack parsing machine. But this machine can he to be a (weak) non-canonicalextension of the I.R(k) grammars, as proposed by Knuth [5]. Finally, this paper will discuss the relevance of this technical result for more computational linguistics. As it turns out, even though general LFG&apos;s may well be computationally intractable, it is easy to imagine a variety of additional constraints for LFG theory that provide a way to sidestep around the reduction argument. All of these additional restrictions amount to making the LFG theory more restricted, in such a way that the reduction argument cannot be made to work. For example, one effective restriction is to stipulate that there can only be a finite stock of features with which to label lexical items. In any case, the moral of the story is an unsurprising one: specificity and constraints can absolve a theory of computational intractability. What may be more surprising is that the requisite locality constraints seem to be useful for a variety of theories of grammar, from transformational grammar to lexical functional grammar. 7 A REVIEWQE ARGUMENTS The demonstration of the computational complexity of LFGs relies upon the compiexity-theoretic technique of reduction.Because this method may be unfamiliar to many readers, a short review is presented immediately below; this is followed by a sketch of the reduction proper. The idea behind the reduction technique is to take a difficult problem, in this case, the problem of determining the satisfiability of Boolean formulas in conjunctive normal form (CNF), and show that the known problem can be quickly transformed into the problem whose complexity remains to be determined, in this case, the problem of deciding whether a given string is in the language generated by a given Lexical Functional Grammar. Before the reduction proper is reviewed, some definitional groundwork must be A formula in conjunctive norma/form a conjunction of A formula is in case there exists some assignment Ts and Fs (or l&apos;s and O&apos;s) to the literals of the formula that forces the evaluation of the entire formula to be I; otherwise, the formula is said to be example. . XdA V satisfiable, since the assignment of (hence F), F (hence F =T and the whole formula cvalutc to &apos;7&apos;. The reduction in the proof below uses a somewhat more restricted format where every term is comprised of the disjunction of exactly three literals, so-called 3-CNRor &amp;quot;3-SAT&apos;). This restriction entails no loss of generality (see Hoperoft and Ullman, 191. Chapter 12), since this restricted format is also NP-complete. How does a reduction show that the LFG recognition problem must be at least as hard (computationally speaking) as the original problem of Boolean satisfiability? &apos;Me answer is that any decision procedure for LFG recognition be used correspondingly fast procedure for 3-CNI:, as follows: Given an instance of a 3-CNF problem (the question of whether satisfying assignment for a given formula in 3-CNE), apply the transformational algorithm provided by the reduction: this algorithm is itself assumed to execute quickly, in polynomial time or less. The algorithm outputs a corresponding LFG decision problem, namely: (i) a lexical functional grammar and (ii) a string to be tested for membership in the language generated by the I.FG. The LFG recognition problem represents or mimics the decision problem for 3-CNI: in the sense that the &amp;quot;yes&apos;&apos; and &amp;quot;no&amp;quot; answers to both satisfiability problem and membership problem must coincide (if there is a satisfying assignment, then the corresponding LEG decision problem should give a &amp;quot;yes&amp;quot; answer. etc.). (2) Solve the LEG decision problem&amp;quot; the string-LEG pair — output by Step I.: if the string is in the LFG language, the original formula was satisfiable; if not. unsatisfiable. (Note that the grammar and string so constructed depend upon just what fonnula is under analysis: that is. for each different CNF formula, the presented above outputs a grammar and string combination. In the LFG case it is important to remember that &amp;quot;grammar&amp;quot; really means &amp;quot;grammar plus lexicon&amp;quot; — as one might expect in a lexically-based theory. S. Peters has observed that a slightly different reduction allows one to keep most of the grammar fixed across all possible input formulas, constructing only different-sized lexicons for each different CN I&apos; formula; for details, see below.) To sec how a reduction can tell us something about the &amp;quot;worst case&amp;quot; time or space complexity required to recognize whether a string is or is not in an LFG suppose for example that the decision procedure whether a string is in an LFG language takes polynomial time (that is, takes on a deterministic Turing machine, for some integer k, where n= the length of the input suing). Then, since the composition of two polynomial algorithms can be readily shown to take only polynomial time (see 191 Chapter 12), the entire process sketched above, from input of the CNF formula to the decision about its satistiability, will take only polynomial time. CNF (or 3-CNF) has no time algorithm, and it is considered that one could exists. Therefore, it is just as unlikely that LFG recognition could be done (in general) in polynomial time. The theory of computational complexity has a much more compact term for like CNF: CNF is NP-comolete.This label is easily deciphered: (1) CNF is in the clasa NP, that is, the class of languages that can be by a o-deterministic Turing machine in polynomialtime. (Hence the abbreviation &amp;quot;NP&amp;quot;, for &amp;quot;non-deterministic polynomial&amp;quot;. To see CNF the class NP, note that one can simply possible combinations of truth assignments to literals. and check each guess in polynomial time.) CNF is complete, that is, all in the class NP can be quickly reduced to some CNF formula. (Roughly, one shows that Boolean formulas can be used to &amp;quot;simulate&amp;quot; any valid computation of a non-deterministic Turing machine.) the class of problems solvable in polynomial time on a deterministic Turing machine (conventionally notated. P) is trivially contained in the class so solved by a nondcterministic Turing machine, the class P must be a subset of the class NP. A well-known, well-studied, and still open question is whthcr class P is a propersubset of the class NP, that is, whether there are problems solvable in non-deterministic polynomial time that cannot be solved in deterministic polynomial time. Because all of the several thousand NP-complete problems now catalogued have so far proved recalcitrant to deterministic polynomial time solution, it is widely held that P must indeed be a proper subset of NP, and therefore that the best possible algorithms for solving NP.complete problems must take more than polynomial time (in general, the algorithms now known for such problems involve exponential combinatorial search, in one fashion or another; these are essentially methods&apos; that do no better than to brutally simulate -deterministically. of course — a non-deterministic machine that &amp;quot;guesses&amp;quot; possible answers.) repeat the force of the reduction argument then, if recognition problems were solvable in polynomial time, then the ability to quickly reduce CNF formulas to LFG recognition problems implies that all NP-complete would in polynomial time, and that the class P= the class NP. This possibility seems extremely remote. Hence, our assumption that there is a fast (general) procedure for recognizing whether a string is or is not in the language generated by an arbitrary LFG grammar roust be faLse. the terminology of complexity theory, LFG recognition must be — &amp;quot;as hard as&amp;quot; any other NP problem, including the NP-complete problems. means only that LFG recogntion is least as hani other NP-complete problems -it could still be more difficult (lie in some class that contains the class NP). If one could also show that the languages generated by LFGs are in the class NP, then LFGs would be shown to be NP-complete. This paper short of proving this last claim, but simply conjectures that LFGs the class NP. a SKETCH QE niE To carry out this demonstration in detail, one must explicitly describe the transformation procedure that takes as input a formula in CNF and outputs a corresponding LFG decision problem — a string to be tested for membership in a LFG language and the LFG itself. One must also show that this can be done quickly, in a number of steps proportional to (at most) the length of the original formula to some polynomial power. Let us dispose of the last point first. The string to be tested for membership in the LFG language will simply be the original formula, sans parentheses and logical symbols: the LFG recognition problem is to find a well-formed derivation of this string with respect to the grammar to be provided. Since the actual grammar and suing write down to &amp;quot;simulate&amp;quot; the CNF problem turn out to be no worse than linearly larger than the original formula, an upper bound of say, n-cubed (where n= length of the original is more than sufficient to construct a corresponding LFG: thus the reduction procedure itself can be done in polynomial time, as required. This paper will therefore have nothing further to say about the time bound on the transformation procedure. 8 Some caveats are in order before embarking on a proof sketch of this reduction. First of all, the relevant details of the LFG theory will have to be covered on-the-fly; see (11) for more discussion.&apos; Also, the grammar that is output by the reduction procedure will nts look very much like a grammar for a natural language, although the grammatical devices that will be will in every way be those that are an essential part the theory. (namely, feature agreement, the lexical analog of Subject or Object &amp;quot;control&amp;quot;, lexical ambiguity, and a garden variety context-free grammar.) In words, although it is most unlikely that any naturallanguage would encode the satisfiability problem (and hence be intractable) in just the manner outlined below, on the other hand, no &amp;quot;exotic&amp;quot; LFG machinery is used in the reduction. Indeed, some of the more powerful LFG notational formalisms long-distance binding, existential and negative feature operators — have not been exploited. (An earlier proof made use of an existential operator in the feature machinery of LFG, but the reduction presented here does not) To make good this demonstration one must set out just what the satisfiability what the decision problem for membership in an LFG is. Recall that a formula in conjunctive normal form just in case every conjunctive term evaluates to Ina that is, at least me literal in sash term is true. The satisfiability problem is to find an assignment of Ts F&apos;s to the literals at the bottom (note that the complementof literals is also permitted) such that the root node at the top gets the value &apos;T&apos; (for one). How can we get a lexical functional grammar to represent this What we want is for satisfyingassignments to correspond to to well-formedsentences of some corresponding LFG grammar, and npn-satisfwingassignments to correspond to sentences that are net well-formed, according to the LFG grammar. Figure L A Reduction Must Preserve Solutions to the Original Problem Since one wants the satisfying/non-sadsfying assignments of any particular formula to map over into well-formed/ill-formed sentences, one must obviously exploit the LFG machinery for capturing well-formedness conditions for sentences. First of all, an LFG contains a tax context-free &apos;rammer.A minimal condition for a sentence (considered as a string) to be in the language generated by a lexical-functional grammar is that it can be generated by this base grammar; such a sentence is then said to have a structure.For example, if the base rules included SNP VP; VP V NP, then (glossing over details of Noun Phrase rules) sentence kissed the baby be well-formed but the baby not. Note that this assumes, as usual, the existence of a lexicon provides a categorization for each terminal item, e.g., that of the kissed a Importantly then.&apos; this well-formedness condition requires us to provide at least one legitimate name tree for the sentence that shows how be derived from the underlying LFG base context-free grammar. (There could be more than one legitimate the grammar Note further that the choice of for a lexical item may be crucial. If assumed to be of category V. then both sentences above would be ill-formed. second major component of the LFG theory provision for adding a of so-called equations the base context-free rules. These equations are used to account for that the co-occurrence restrictions that are so much a part of natural languages (e.g., Subject-Verb agreement). Roughly, • to associate featual with lexical entries and with the of specified context-free rules; these features have alum. machinery to pass features in certain ways around the parse tree, and conflicting values for the same feature arc cause for rejecting a candidate analysis, To take the Subject-Verb agreement example, consider sentence baby is kissing John. lexical entry for a Noun) might have the Numberfeature, with the value singular.The entry for assert that the numberfeature of the Subjectabove in the parse tree must have value singular;meanwhile, the feature for 5ubiectare automatically found by another rule (associated with the Noun Phrase portion of SNP VP) that grabs whatever features it finds below the NP node and copies them up above to the S node. Thus the S node the Subject feature, with whatever value it has passed from -the value aineular;this accords with the dicates of the verb all well. Similarly, in the sentence, boys in the band is kissing John, boys up the number value pluraland this clashes with the verb&apos;s constraint; as a result this sentence is judged ill-formed: S features: Subject Number.Singular or Plural? = CLASH! , i N&apos;. Numberplural V*. Number:singular iI 1 boys in the band kissing John. Figure 2. Co-occurrence Restrictions are Enforced by Feature Checking in an LFG. It is important to note that the feature compatability check requires (1) a particular constituent structure tree (a parse tree); and (2) an assignment of terminal items (words) to lexical categories -e.g., in the first Subject-Verb example above, assigned to be of the category N, a Nun. The tree is obviously required because the feature checking machinery propagates values according to the links specified by the derivation tree; the assignment of terminal items to categories is crucial because in most cases the values of features are derived from those listed in lexical entry for an item (as the value of the numberfeature was derived the lexical entry for the Noun form of and the same terminal item can have two distinct lexical entries, corresponding to distinct categorizations; for example, be both a Noun and a Verb. If had picked be a Verb, and hence had adopted whatever features associated with the Verb entry for be propagated up the tree, then string that was previously well-formed. baby is kissing John be considered deviant. If a string under all possible derivation trees and assignments of features from possible lexical then that string the language generated by the LFG. possibility of multiple derivation trees and (and multiple feature bundles) for one and the same terminal a rule in the reduction proof: it to capture the satisfiability of deciding whether to give a literal X, a value of &amp;quot;F&amp;quot;. Finally, LFG also provides a way to express the familiar patterning of relations (e.g.. and &amp;quot;Object&amp;quot;) found in natural language. For example, transitive verbs must have objects. This fact of life in an grammar by subcatcgorization is captured in LFG by specifying a so-called PRED(for predicate) feature with a Verb; the PRE1) can describe what grammatical relations like &amp;quot;Subject&amp;quot; and &amp;quot;Object&amp;quot; must be filled in after feature passing taken place in for the analysis to be well-formed. For instance, verb like have the pattern. thus demand that the Subject and Object (now considered to be &amp;quot;features&amp;quot;) in the final analysis. The values for Subject and Object might of course be provided from some other branch of the parse tree, as by the feature propagation machinery; for example, the Object could in from the Noun Phrase part of the VP expansion: w&apos; w in LFG language L(G) in LFG language L(G) satisfiable non-satisfiable foirm la w form la w 9 SUBJECT: Sue S features PRED : &apos;kiss&lt;(SubjectX0bject), OBJECT: John Sue / V NP kis John Figure 3. Predicate Templates Can Demand That a Subject or Object be Filled In. if the Object were az Med in, then the analysis is fine:tonally is ruled out. This device is used to cast out sentences such as, the baby kisserl So much for the LFG machinery that is required for the reduction proof. (Them are additional capabilities in the LFG theory, such as long-distance binding, but these will not be called upon in the demonstration below.) What then does the LFG representation of the satisfiability problem look like? Basically, there are three parts to the satistiability problem that must be by the LFG: (1) the assignment of values to literals, e.g., (2) the co-ordination of value assignments across intervening literals the formula; e.g., the literal can appear in several different terms, but one is nut allowed to assign it the value &apos;1&amp;quot; in one term and the value &amp;quot;F&amp;quot; in (and the same goes for the complement of if has the &apos;1&amp;quot;, cannot have the value &amp;quot;1&amp;quot;); and (3) satistiability must correspond to LFG wc11-formedness, i.e., each term has the truth value &amp;quot;1&amp;quot; in case at least az literal in the tcnn is assigned all terms must evaluate to &amp;quot;1&amp;quot;. Let us now go over how these components may be reproduced in an LFG, one by one. (1) Assignments: The input string to be tested for membership in the LFG will simply be the original formula, sans parentheses and logical symbols; the items are thus just a string of Recall that the job of checking the string for well-formedncss involves finding a derivation tree for the string, solving the ancillary co-occurrence equations (by feature propagation), and checking for functional completeness. Now, the context-free grammar constructed by the transformation procedure will be set up so as to generate a copy of the associated formula, down to the point where literals are assigned their values of &amp;quot;T&apos; or &amp;quot;F&amp;quot;. If the original CNF form had N terms, this pan of grammar would look like: (one &amp;quot;I&amp;quot;&apos; for each term) triple of Y&apos;s per term) Several comments are in order here. (1) The context-free base that is built depends upon the original CNF formula that is input, since the number of terms; a, varies from formula to formula. In Stanley Peters&apos; improved version of the reduction proof, the context-free base is fixed for all formulas with the rola: SS S&apos; T T T or ST T F or T F F or T F Tor... (remaining twelve expansions that have at least one &amp;quot;1&amp;quot; in each triple) The Peters grammar works by recursing until the right number of terms is generated (any sentences that are too long or too short cannot be matched to the input formula). Thus, the number of terms in the original CNF formula need not be explicitly encoded into the base grammar. (2) The subscripts i,j, and k depend on the actual subscripts in the original formula. (3) The Y. are acs terminal items, but are non-terminals. (4) This grammar will have to be slightly modified in order for the reduction to work, as will become apparent shortly. that far are no rules to extend the parse tree down to the level of terminal items, the X. The next step does this and at the same time adds the power to choose between &apos;1&amp;quot; and &amp;quot;F&amp;quot; assignments to literals. One includes in the context-free base grammar Ltii productions deriving each item and corresponding to an of &amp;quot;1&amp;quot; or &amp;quot;F&apos; to the formula literal (it is important not to get here between the literals of the are in lexical functional grammar — and the literals of grammar — the non-terminal symbols.) One must also add, obviously, the rules for each i, and rules corresponding to • the negations of that these are not &amp;quot;exotic&amp;quot; LFG rules: exactly the sort of rule is required in the N=&gt; baby baby, to whether a Noun or a Verb. Now, the lexical entries the categorization of will look very different from the of just as one might expect the N and V forms for to different. Here is what the entries for the two categorizations of look like: (T truth-assignmen0= T (rassign X)=T (iassign =F feature assignments for the negation of the literal is simply the dual of the entries above (since the sense of &amp;quot;T&apos; and &amp;quot;I&amp;quot; is reversed): (Ttruth-assignment)=T (fassign X)= F. =T The role of the additional &amp;quot;truth-assignment&amp;quot; feature will be explained below. Figure 4. Sample Lexical Entries to Reproduce the Assignment of Ts and Fs a literal The upward-directed arrows in the entries reflect the LFG feature machinery. In the case of the entry, for instance, they say to the Truth-assianmentfeature of the node aboveXT have the value and make the portion of the Assign feature of the node above have the value T.&amp;quot; This feature propagation device is what reproduces the assignment of Ts and Fs to the CNF literals. If we have a triple of such and at least them is expanded out to then the feature propagation machinery of LFG will mug the common feature names into one large structure for the node above, reflecting the assignments made; moreover, the term will get a filled-in truth assignment value just in case at least one of the expansions selected an XT path: T feature structure: Truth-assignment=r = X.F terminal I I Figure 5. The I.FG Feature Propagation Machinery is Used to Percolate Feature Assignments from the Lexicon. ONO 10 (The features are passed transparently through the intervening nodes via the LFG &amp;quot;copy&amp;quot; device. = I.): this simply means that all thc features of the node below the node to which the &amp;quot;copy&amp;quot; up-and-down arrows are attached are to be the same as those of the node above the up-and-down arrows.) It is plain that this Mechanism mimics the assignment of values To literals required by the satistlability problem. Co-ordination of assignments: One must also guarantee that the value at one place in the tree is not contradicted by an or elsewhere. To ensure this, we use the LFG co-occurrence agreement machinery: the Assignfeature-bundle is passed up from each term to the highest node in parse tree (one simply adds the (1 =1) notation to each rule in order to this). The Assignfeature at this node will thus contain the unioqof assignfeature bundles passed up by all terms. If any values conflict, the resulting structure is judged ill-formed. Thus, only assignments are well-formed: Figure 6, The Feature Compatability Machinery of LFG can Force Assignments to be Co-ordinated Across Terms. (3) Preservation of satisfying assignments. Finally, one has to reproduce the of the 3-CNF problem-that is, a sentence is Satisfiable (well-formed) iff each term has at least one literal assigned the value &amp;quot;T&amp;quot;. Part of the disjunctive character of the problem has already been encoded in feature propagation machinery presented so far: if at least one in a expands to the lexical entry then the Inah-assianmentfeature the value T. This is just as desired. If one, two, or three of the literals a term select then truth-assignment feature is T. and the analysis well-formed. But how do we rule out the case where threein a term the &amp;quot;F&amp;quot; path, And how do we ensure that all terms have at least one T below them? Both of these problems can be solved by resorting to the LFG functional completeness constraint. The trick will be to add a Elmci feature to a &amp;quot;dummy&amp;quot; node attached to each term: the sole purpose of this feature will be refer to the feature Truth-as:ignment,just as the predicate template for the verb the feature Object.Since an analysis is not well-formed if the &amp;quot;grammatical relations&amp;quot; a Fred mentions are not filled in somewhere, this will have the effect of forcing the Trutltassianment to get filled in Sincc the &amp;quot;F&amp;quot; lexical entry does not have a l&apos;oith-assienmentvalue, if ail the in a term triple select the path (all the literals are &amp;quot;F&amp;quot;) then on Truth-assignment feature is ever picked up from the lexical entries, and that term never gets a Truth-assignment feature. This violates what the predicate template demands, and so the whole analysis is thrown out, (&apos;l&apos;he ill-formedness is exactly analogous to the case where a transitive verb never gets an Object) Since this condition is applied to each term, we have now guaranteed that each term must have at least tme literal below it that selects the &apos;7&apos; path &amp;quot;just as desired. To actually add the new predicate template, one simply adds a new (but dummy) branch to each term with the appropriate predicate constraint attached to it Can be Used to Force at least one &amp;quot;I&apos; Per Term. is a final subtle point here: one must the and Truth-assignment features for each term from being passed up to the head &amp;quot;S&amp;quot; node. The reason is that if these features were passed up, then since the machinery automatically values of any features with the same name at the topmost node of the parse tree, the LFG machinery would the union of the feature values for Pred and Truth-assignment over terms in the analysis tree. The result would be that if au term had at least one (hence satisfying the Truth-assignment predicate template in at least one term), then the Pred and Truth-assignment would get filled in at the topmost node as well. The string below would be well&apos; formed if at least one&apos; term were &apos;7&apos;, and this would amount to a disjunction of disjunctions (an &amp;quot;OR&amp;quot; of &amp;quot;OR&amp;quot;s), not quite what is sought. To eliminate this possibility, one add a final trick: ma term is given and Assign features, but Assign feature is propagated to the highest node in the parse tree as such. In contrast, the Predicate and Truth-assignment features for each term are kept &amp;quot;protected&amp;quot; merger by storing them under headings labelled means by which just the ASSIGN feature bundle is lifted out is n the LFG analogue of the natural language phenomenon of Subject or Object whereby features of the Subject or Object of a lower clause are lifted out of the lower clause to become the Subject or Object of a matrix sentence; the remaining features stay unmergeable because they stay protected behind the individually labelled terms. To actually &amp;quot;implement&amp;quot; this in an LFG one can add two new branches to each Term expansion in the base context-free grammar, as well as two &amp;quot;control&amp;quot; equation specifications that do the actual work of lifting the features from a lower clause to the matrix sentence: Natural language case (from [St pp. 43-45): The girl persuaded the baby to go. (part of the) lexical entry for persuaileat VCOMP Subject)= (? Object) The notation (1 VCOMP Subject)=(1 Object) — dubbed a &amp;quot;control equation&amp;quot; -means that the features of the Object above the V(erb) node are to be the same as those of the features of the Subject of the verb complement (VCOMP). Hence the top-most node of the parse tree eventually has a feature bundle something like: object: {bundle of features for NP subject &amp;quot;the girl&amp;quot;} &apos;persuade&lt;cr Sublet*? bject: {bundle of features for NP Object &amp;quot;the baby&amp;quot;) COPIED erb omplement: Subject: [bundle &apos;of features for NP subject &amp;quot;the baby&amp;quot; &amp;quot;VCOMP&amp;quot;) Predicate: &apos;go&lt;(TSubject)&gt;&apos; Note how the Object features have been copied from the Subject features of the Verb Complement, via the notation described above, but the Predicate features of the Verb Complement were left behind.</abstract>
<note confidence="0.808879666666667">The satisfiability analogue of this machinery is almost identical: Assign: = T or 1 Clash! T (Tassilln X7= F) Dummy2 lexical entry: I Pred)= &apos;clummy2&lt;(1 Truth-assigrunent)Y Xi</note>
<abstract confidence="0.988665766666667">(iTrudrassignment)=T featureitred: &apos;dummy2&lt;(TTruth-assignment)1 / t 11 structure tree: attempt all and only the natural languages. This discovery be on a par with, for example, Peters observation that T. although the context-sensitive phrase structure rules formally advanced in linguistic theory have the power to generate non-context-free languages, that A. T.COMP power has apparently never been used in immediate constituent analysis (111. Dummy2 Y. Y. j now attaches a &amp;quot;control equation&amp;quot; to the node that forces the bundle from the side to be lifted up to get merged into the Assienfeature bundle of the T node (and then, in turn, to become merged at the topmost node of the tree by the usual full copy up-and-down arrows): Assign)= (T Assign) Note how this is just like the copying of the Subject features of a Verb Complement into the Object position of a matrix clause. EVA NCEQE RESUI T ,1 N I) CONCLUSIONS The demonstration of the previous section shows that LFGs have enough power to &amp;quot;simulate&amp;quot; a probably computationally intractable problem. But what are we to make of this result? On the positive side, a complexity result such as this one places the LFG theory more precisely in the hierarchy of classes. conjecture, as seems reasonable, that LEG language recognition is actually in the class NP (that is, LFG recognition can be done by a non-deterministic Turing machine in polynomial time), then LFG language recognition is NP-complete. (This conjecture seems reasonable because a non-deterministic Turing machine should be able to &amp;quot;guess&amp;quot;. all feature propagation solutions using its non-deterministic power — including any &amp;quot;long-distance&amp;quot; binding solutions, an LFG device not discussed here. solutions is quite rapid — it can be done in time or less, as described in (81 — recognition should be possible in polynomial time on such a machine.) Comparing this result to other known language renegesnote that context-sensitive language recognition is in the class since (non-deterministic) linear bounded automata generate exactly the class of context-sensitive languages. (Non-deterministic and deterministic polynomial space classes collapse together, because of Savitch&apos;s well-known result (91 that any function computable in non-deterministic space N can be computed in deterministic Furthermore, the class NP is clearly a subset of PSPACE (since if a function uses Space N. it must use at least Time N), and it is suspected, but not known for certain, that NP is a proper subset of PSPACE. (This being a form of the P= NP question once again.) Our conclusion is that it is likely that LEC&apos;s generate a proper subset of the context-sensitive languages. (In (81 it is shown that this includes some strictly context-sensitive languages.) It is interesting that several other &amp;quot;natural&amp;quot; extensions of the context-free languages — notably, the class of languages generated by the so-called &amp;quot;indexed grammars&amp;quot; — also generate a subset of the context-sensitive languages, including those strictly context-sensitive languages shown to be generable by LFGs in [81, but arc provably NP-complete (see (21 for proofs). Indeed, a cursory look at the power of the indexed grammars at least suggests that they might subsume the machinery of the LFG theory; this would be a good conjecture to check. the other side coin, how might one restrict LEG theory further so as to avoid possible intractability? Several escape hatches immediately come to mind; these will simply be listed here. Note that all of these &amp;quot;fixes&amp;quot; have the effect of adding additional constraints to timber restrict the LFG theory. 1. Rule out &amp;quot;worst case&amp;quot; languages as linguistically irrelevant. The probable computational intractability arises because co-occurrence (compatibleas.signment of can be forced across arbitrary distances in the terminal string in conjunction with lexical ambiguity for each terminal itcm. If some device can be found in natural languages that filters out or removes such ambiguity locally (so that the choice of whether an item is &amp;quot;r or &amp;quot;F&amp;quot; never depends on other items arbitrarily far away in the terminal string), or if natural languages never employ such kinds of co-occurrence restrictions, then the reduction is theoretically relevant, but linguistically irrelevant. Note that such a finding would be a positive discovery, since one would be able to further restrict the LFG theory in its 2. Add &amp;quot;locality principles&amp;quot; for recognition (or parsing). One could simply stipulate that LFG languages meet some condition known to ensure efficient recognizability, e.g., Knutles [71 LR(k) restriction, suitably extended to the case of context-sensitive languages. (See (101 for more details.) 3. Restrict the lexicon. The reduction depends crucially upon having an infinite stock of lexical items and an infinite number of features with which to label them — several for each literal X This is necessary because as CNF formulas grow larger and larger, the number of literals can grow arbitrarily large, If, for whatever reason, the stock of lexical items or feature labels is finite, then the reduction must fail after a certain point. This restriction seems hoc the case of lexical items, but perhaps less so in the case of features. (Speculating, perhaps features require &amp;quot;grounding&amp;quot; in terms of other language/cognitive sub-systems -e.g., a feature might be required to be one of a finite number of primitive &amp;quot;basis&amp;quot; elements of a hypothetical conceptual or sensori-motor cognitive system.) MEM I would like to thank Ron Kaplan, Ray Perrault. Christos Patiadimitriou; and particularly Stanley Peters for various discussions about the contents of this paper.</abstract>
<affiliation confidence="0.49962">Thisreport describes research done at the Artificial Intelligence I aboratory Massachusetts Institute of Technology. Support Laboratory&apos;s artificial intelligence research is provided in part by the Office of Naval under Office of Naval Research contract</affiliation>
<note confidence="0.961280428571429">11 EFERENCES (11 Peters, S. and Ritchic, R. &amp;quot;On the generative power of transformational grammars.&amp;quot; Information Sciences 6, 1973, pp. 49-81 121 Rounds. W. &amp;quot;Complexity of recognition in intermediate-level languages.&amp;quot; Proceedings of the 14th Ann. Symp. on Switching Theory and Automata, 1973. [31 Rounds W. &amp;quot;A grammatical characterization of exponential-time languages,&amp;quot; Proceedings of the 16th Ann. Symp. on Switching Theory and Automata, 1975, pp. 135-143. Chomsky, N. and Representations York: Columbia University Press. 1980. Berwick, R. and Weinberg. A. Role of Grammars in Models of Use, MIT report, forthcoming, 1981. [61 Marcus, M. A Theory of Syntactic Recognition for Natural Language, Cambridge, MA: MIT Press, 1.980. (71 Knuth, D. &amp;quot;On the translation of languages from left to right&amp;quot;, Information and Control, 8, 1965, pp. 607-639. Kaplan. R. and Bresnan. 1. Grammar: A Formal System Grammatical Representation, MA: MIT Cognitive Science Occasional Paper #13, 1981. (also forthcoming in Bresnan, cd., The Mental Representation of Grammatical Relations, Cambridge, MA: MIT Press, 1981.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>On the generative power of transformational grammars.&amp;quot;</title>
<date>1973</date>
<journal>Information Sciences</journal>
<volume>6</volume>
<pages>49--81</pages>
<marker>1973</marker>
<rawString>(11 Peters, S. and Ritchic, R. &amp;quot;On the generative power of transformational grammars.&amp;quot; Information Sciences 6, 1973, pp. 49-81</rawString>
</citation>
<citation valid="true">
<authors>
<author>W</author>
</authors>
<title>Complexity of recognition in intermediate-level languages.&amp;quot;</title>
<date>1973</date>
<booktitle>Proceedings of the 14th Ann. Symp. on Switching Theory and Automata,</booktitle>
<marker>W, 1973</marker>
<rawString>121 Rounds. W. &amp;quot;Complexity of recognition in intermediate-level languages.&amp;quot; Proceedings of the 14th Ann. Symp. on Switching Theory and Automata, 1973.</rawString>
</citation>
<citation valid="true">
<title>A grammatical characterization of exponential-time languages,&amp;quot;</title>
<date>1975</date>
<booktitle>Proceedings of the 16th Ann. Symp. on Switching Theory and Automata,</booktitle>
<pages>135--143</pages>
<marker>1975</marker>
<rawString>[31 Rounds W. &amp;quot;A grammatical characterization of exponential-time languages,&amp;quot; Proceedings of the 16th Ann. Symp. on Switching Theory and Automata, 1975, pp. 135-143.</rawString>
</citation>
<citation valid="true">
<title>Rules and Representations</title>
<date>1980</date>
<publisher>Columbia University Press.</publisher>
<location>New York:</location>
<marker>1980</marker>
<rawString>[41 Chomsky, N. Rules and Representations New York: Columbia University Press. 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A</author>
</authors>
<title>The Role of Grammars in Models of Language Use,</title>
<date>1981</date>
<note>unpublished MIT report, forthcoming,</note>
<marker>A, 1981</marker>
<rawString>[51 Berwick, R. and Weinberg. A. The Role of Grammars in Models of Language Use, unpublished MIT report, forthcoming, 1981.</rawString>
</citation>
<citation valid="true">
<title>A Theory of Syntactic Recognition for Natural Language,</title>
<date></date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA:</location>
<marker></marker>
<rawString>[61 Marcus, M. A Theory of Syntactic Recognition for Natural Language, Cambridge, MA: MIT Press, 1.980.</rawString>
</citation>
<citation valid="true">
<title>On the translation of languages from left to right&amp;quot;,</title>
<date>1965</date>
<journal>Information and Control,</journal>
<volume>8</volume>
<pages>607--639</pages>
<marker>1965</marker>
<rawString>(71 Knuth, D. &amp;quot;On the translation of languages from left to right&amp;quot;, Information and Control, 8, 1965, pp. 607-639.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R</author>
<author>Bresnan</author>
</authors>
<title>Lexical-fine:Iona&apos; Grammar: A Formal System for Grammatical Representation,</title>
<date>1981</date>
<journal>MIT Cognitive Science Occasional Paper</journal>
<volume>13</volume>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA:</location>
<note>91 floperoft. 1. and</note>
<marker>R, Bresnan, 1981</marker>
<rawString>[81 Kaplan. R. and Bresnan. 1. Lexical-fine:Iona&apos; Grammar: A Formal System for Grammatical Representation, Cambridge, MA: MIT Cognitive Science Occasional Paper #13, 1981. (also forthcoming in Bresnan, cd., The Mental Representation of Grammatical Relations, Cambridge, MA: MIT Press, 1981. [91 floperoft. 1. and Ullman, J. Introduction to Automata Theory, Languages, and Computation, Reading, MA: Addison-Wesley, 1979.</rawString>
</citation>
<citation valid="true">
<title>Locality Principles and the Acquisition of Syntactic Knowledge, MIT PhD. dissertation.</title>
<date>1981</date>
<booktitle>Mathematical Systems Theory,</booktitle>
<volume>6</volume>
<pages>111</pages>
<marker>1981</marker>
<rawString>[101 Berwick, R. Locality Principles and the Acquisition of Syntactic Knowledge, MIT PhD. dissertation. 1981 forthcoming. [111 Peters, S. and Ritchie, R. Context-sensitive immediate constituent analysir context-free languages revisited, Mathematical Systems Theory, 6:4, 1973, pp. 324-333.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>