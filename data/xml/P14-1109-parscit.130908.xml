<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.99968">
A Semiparametric Gaussian Copula Regression Model
for Predicting Financial Risks from Earnings Calls
</title>
<author confidence="0.999187">
William Yang Wang
</author>
<affiliation confidence="0.865189">
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.470807">
Pittsburgh, PA 15213
</address>
<email confidence="0.997731">
yww@cs.cmu.edu
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999900103448276">
Earnings call summarizes the financial
performance of a company, and it is an
important indicator of the future financial
risks of the company. We quantitatively
study how earnings calls are correlated
with the financial risks, with a special fo-
cus on the financial crisis of 2009. In par-
ticular, we perform a text regression task:
given the transcript of an earnings call, we
predict the volatility of stock prices from
the week after the call is made. We pro-
pose the use of copula: a powerful statis-
tical framework that separately models the
uniform marginals and their complex mul-
tivariate stochastic dependencies, while
not requiring any prior assumptions on the
distributions of the covariate and the de-
pendent variable. By performing probabil-
ity integral transform, our approach moves
beyond the standard count-based bag-of-
words models in NLP, and improves pre-
vious work on text regression by incor-
porating the correlation among local fea-
tures in the form of semiparametric Gaus-
sian copula. In experiments, we show
that our model significantly outperforms
strong linear and non-linear discriminative
baselines on three datasets under various
settings.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998487">
Predicting the risks of publicly listed companies is
of great interests not only to the traders and ana-
lysts on the Wall Street, but also virtually anyone
who has investments in the market (Kogan et al.,
2009). Traditionally, analysts focus on quantita-
tive modeling of historical trading data. Today,
even though earnings calls transcripts are abun-
dantly available, their distinctive communicative
practices (Camiciottoli, 2010), and correlations
with the financial risks, in particular, future stock
</bodyText>
<author confidence="0.864256">
Zhenhao Hua
</author>
<affiliation confidence="0.991189">
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.63002">
Pittsburgh, PA 15213
</address>
<email confidence="0.99275">
zhua@cs.cmu.edu
</email>
<bodyText confidence="0.999743585365853">
performances (Price et al., 2012), are not well
studied in the past.
Earnings calls are conference calls where a
listed company discusses the financial perfor-
mance. Typically, a earnings call contains two
parts: the senior executives first report the oper-
ational outcomes, as well as the current financial
performance, and then discuss their perspectives
on the future of the company. The second part of
the teleconference includes a question answering
session where the floor will be open to investors,
analysts, and other parties for inquiries. The ques-
tion we ask is that, even though each earnings call
has distinct styles, as well as different speakers
and mixed formats, can we use earnings calls to
predict the financial risks of the company in the
limited future?
Given a piece of earnings call transcript, we
investigate a semiparametric approach for auto-
matic prediction of future financial risk1. To do
this, we formulate the problem as a text regres-
sion task, and use a Gaussian copula with prob-
ability integral transform to model the uniform
marginals and their dependencies. Copula mod-
els (Schweizer and Sklar, 1983; Nelsen, 1999)
are often used by statisticians (Genest and Favre,
2007; Liu et al., 2012; Masarotto and Varin, 2012)
and economists (Chen and Fan, 2006) to study the
bivariate and multivariate stochastic dependency
among random variables, but they are very new
to the machine learning (Ghahramani et al., 2012;
Han et al., 2012; Xiang and Neville, 2013; Lopez-
paz et al., 2013) and related communities (Eick-
hoff et al., 2013). To the best of our knowledge,
even though the term “copula” is named for the
resemblance to grammatical copulas in linguistics,
copula models have not been explored in the NLP
community. To evaluate the performance of our
approach, we compare with a standard squared
loss linear regression baseline, as well as strong
baselines such as linear and non-linear support
</bodyText>
<footnote confidence="0.997898666666667">
1In this work, the risk is defined as the measured volatil-
ity of stock prices from the week following the earnings call
teleconference. See details in Section 5.
</footnote>
<page confidence="0.881975">
1155
</page>
<note confidence="0.8341415">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1155–1165,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998309428571429">
vector machines (SVMs) that are widely used in
text regression tasks. By varying different exper-
imental settings on three datasets concerning dif-
ferent periods of the Great Recession from 2006-
2013, we empirically show that our approach sig-
nificantly outperforms the baselines by a wide
margin. Our main contributions are:
</bodyText>
<listItem confidence="0.9658845">
• We are among the first to formally study tran-
scripts of earnings calls to predict financial
risks.
• We propose a novel semiparametric Gaussian
copula model for text regression.
• Our results significantly outperform standard
linear regression and strong SVM baselines.
• By varying the number of dimensions of the
covariates and the size of the training data,
we show that the improvements over the
baselines are robust across different param-
eter settings on three datasets.
</listItem>
<bodyText confidence="0.999871625">
In the next section, we outline related work in
modeling financial reports and text regression. In
Section 3, the details of the semiparametric cop-
ula model are introduced. We then describe the
dataset and dependent variable in this study, and
the experiments are shown in Section 6. We dis-
cuss the results and findings in Section 7 and then
conclude in Section 8.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999969320754717">
Fung et al. (2003) are among the first to study
SVM and text mining methods in the market pre-
diction domain, where they align financial news
articles with multiple time series to simulate the
33 stocks in the Hong Kong Hang Seng Index.
However, text regression in the financial domain
have not been explored until recently. Kogan et
al. (2009) model the SEC-mandated annual re-
ports, and performs linear SVM regression with
E-insensitive loss function to predict the mea-
sured volatility. Another recent study (Wang et
al., 2013) uses exactly the same max-margin re-
gression technique, but with a different focus on
the financial sentiment. Using the same dataset,
Tsai and Wang (2013) reformulate the regression
problem as a text ranking problem. Note that
all these regression studies above investigate the
SEC-mandated annual reports, which are very dif-
ferent from the earnings calls in many aspects such
as length, format, vocabulary, and genre. Most
recently, Xie et al. (2013) have proposed the use
of frame-level semantic features to understand fi-
nancial news, but they treat the stock movement
prediction problem as a binary classification task.
Broadly speaking, our work is also aligned to re-
cent studies that make use of social media data
to predict the stock market (Bollen et al., 2011;
Zhang et al., 2011).
Despite our financial domain, our approach is
more relevant to text regression. Traditional dis-
criminative models, such as linear regression and
linear SVM, have been very popular in various
text regression tasks, such as predicting movie rev-
enues from reviews (Joshi et al., 2010), under-
standing the geographic lexical variation (Eisen-
stein et al., 2010), and predicting food prices from
menus (Chahuneau et al., 2012). The advantage of
these models is that the estimation of the parame-
ters is often simple, the results are easy to inter-
pret, and the approach often yields strong perfor-
mances. While these approaches have merits, they
suffer from the problem of not explicitly model-
ing the correlations and interactions among ran-
dom variables, which in some sense, correspond-
ing to the impractical assumption of independent
and identically distributed (i.i.d) of the data. For
example, when bag-of-word-unigrams are present
in the feature space, it is easier if one does not ex-
plicitly model the stochastic dependencies among
the words, even though doing so might hurt the
predictive power, while the variance from the cor-
relations among the random variables is not ex-
plained.
</bodyText>
<sectionHeader confidence="0.993459" genericHeader="method">
3 Copula Models for Text Regression
</sectionHeader>
<bodyText confidence="0.999933363636364">
In NLP, many statistical machine learning meth-
ods that capture the dependencies among ran-
dom variables, including topic models (Blei et al.,
2003; Lafferty and Blei, 2005; Wang et al., 2012),
always have to make assumptions with the under-
lying distributions of the random variables, and
make use of informative priors. This might be
rather restricting the expressiveness of the model
in some sense (Reisinger et al., 2010). On the
other hand, once such assumptions are removed,
another problem arises — they might be prone to
errors, and suffer from the overfitting issue. There-
fore, coping with the tradeoff between expressive-
ness and overfitting, seems to be rather important
in statistical approaches that capture stochastic de-
pendency.
Our proposed semiparametric copula regression
model takes a different perspective. On one hand,
copula models (Nelsen, 1999) seek to explicitly
model the dependency of random variables by sep-
arating the marginals and their correlations. On
the other hand, it does not make use of any as-
</bodyText>
<page confidence="0.988575">
1156
</page>
<bodyText confidence="0.999988424242424">
sumptions on the distributions of the random vari-
ables, yet, the copula model is still expressive.
This nice property essentially allows us to fuse
distinctive lexical, syntactic, and semantic feature
sets naturally into a single compact model.
From an information-theoretic point of
view (Shannon, 1948), various problems in text
analytics can be formulated as estimating the
probability mass/density functions of tokens
in text. In NLP, many of the probabilistic text
models work in the discrete space (Church and
Gale, 1995; Blei et al., 2003), but our model is
different: since the text features are sparse, we
first perform kernel density estimates to smooth
out the zeroing items, and then calculate the
empirical cumulative distribution function (CDF)
of the random variables. By doing this, we
are essentially performing probability integral
transform— an important statistical technique
that moves beyond the count-based bag-of-words
feature space to marginal cumulative density
functions space. Last but not least, by using
a parametric copula, in our case, the Gaussian
copula, we reduce the computational cost from
fully nonparametric methods, and explicitly
model the correlations among the covariate and
the dependent variable.
In this section, we first briefly look at the
theoretical foundations of copulas, including the
Sklar’s theorem. Then we describe the proposed
semiparametric Gaussian copula text regression
model. The algorithmic implementation of our ap-
proach is introduced at the end of this section.
</bodyText>
<subsectionHeader confidence="0.999503">
3.1 The Theory of Copula
</subsectionHeader>
<bodyText confidence="0.999980857142857">
In the statistics literature, copula is widely known
as a family of distribution function. The idea be-
hind copula theory is that the cumulative distri-
bution function (CDF) of a random vector can be
represented in the form of uniform marginal cu-
mulative distribution functions, and a copula that
connects these marginal CDFs, which describes
the correlations among the input random variables.
However, in order to have a valid multivariate dis-
tribution function regardless of n-dimensional co-
variates, not every function can be used as a copula
function. The central idea behind copula, there-
fore, can be summarize by the Sklar’s theorem and
the corollary.
</bodyText>
<subsectionHeader confidence="0.538229">
Theorem 1 (Sklar’s Theorem (1959)) Let F
</subsectionHeader>
<bodyText confidence="0.9673794">
be the joint cumulative distribution function
of n random variables X1, X2,..., Xn. Let
the corresponding marginal cumulative dis-
tribution functions of the random variable be
F1(x1), F2(x2), ..., Fn(xn). Then, if the marginal
</bodyText>
<equation confidence="0.905590333333333">
functions are continuous, there exists a unique
copula C, such that
F(x1, ..., xn) = C[F1(x1), ..., Fn(xn)]. (1)
</equation>
<bodyText confidence="0.9913038">
Furthermore, if the distributions are continuous,
the multivariate dependency structure and the
marginals might be separated, and the copula can
be considered independent of the marginals (Joe,
1997; Parsa and Klugman, 2011). Therefore, the
copula does not have requirements on the marginal
distributions, and any arbitrary marginals can be
combined and their dependency structure can be
modeled using the copula. The inverse of Sklar’s
Theorem is also true in the following:
Corollary 1 If there exists a copula C : (0, 1)n
and marginal cumulative distribution func-
tions F1(x1), F2(x2), ..., Fn(xn), then
C[F1(x1), ..., Fn(xn)] defines a multivariate
cumulative distribution function.
</bodyText>
<subsectionHeader confidence="0.9973425">
3.2 Semiparametric Gaussian Copula Models
The Non-Parametric Estimation
</subsectionHeader>
<bodyText confidence="0.9999819">
We formulate the copula regression model as fol-
lows. Assume we have n random variables of text
features X1, X2,..., Xn. The problem is that text
features are sparse, so we need to perform non-
parametric kernel density estimation to smooth out
the distribution of each variable. Let f1, f2,..., fn
be the unknown density, we are interested in de-
riving the shape of these functions. Assume we
have m samples, the kernel density estimator can
be defined as:
</bodyText>
<equation confidence="0.984416">
Kh(x − xi) (2)
x − xi �
K (3)
h
</equation>
<bodyText confidence="0.569812">
Here, K(·) is the kernel function, where in our
case, we use the Box kernel2 K(z):
</bodyText>
<equation confidence="0.997394666666667">
1
K(z) = 2, |z |G 1, (4)
= 0,|z |&gt; 1. (5)
</equation>
<bodyText confidence="0.9977565">
Comparing to the Gaussian kernel and other ker-
nels, the Box kernel is simple, and computation-
ally inexpensive. The parameter h is the band-
width for smoothing3.
</bodyText>
<footnote confidence="0.99486675">
2It is also known as the original Parzen windows (Parzen,
1962).
3In our implementation, we use the default h of the Box
kernel in the ksdensity function in Matlab.
</footnote>
<equation confidence="0.991186">
ˆfh(x) = 1
m
�m
i=1
1
�m
i=1
mh
</equation>
<page confidence="0.840115">
1157
</page>
<bodyText confidence="0.917503428571429">
Now, we can derive the empiri-
cal cumulative distribution functions
ˆFX1(ˆf1(X1)),
ˆFX2(ˆf2(X2)),...,ˆFXn(ˆfn(Xn)) of
the smoothed covariates, as well as the dependent
ˆf(y)). The empirical
cumulative distribution functions are defined as:
</bodyText>
<equation confidence="0.89499">
I{xi G ν} (6)
</equation>
<bodyText confidence="0.999978416666667">
where I{·} is the indicator function, and ν in-
dicates the current value that we are evaluating.
Note that the above step is also known as prob-
ability integral transform (Diebold et al., 1997),
which allows us to convert any given continuous
distribution to random variables having a uniform
distribution. This is of crucial importance to mod-
eling text data: instead of using the classic bag-of-
words representation that uses raw counts, we are
now working with uniform marginal CDFs, which
helps coping with the overfitting issue due to noise
and data sparsity.
</bodyText>
<subsectionHeader confidence="0.902138">
The Parametric Copula Estimation
</subsectionHeader>
<bodyText confidence="0.99980625">
Now that we have obtained the marginals, and then
the joint distribution can be constructed by apply-
ing the copula function that models the stochastic
dependencies among marginal CDFs:
</bodyText>
<equation confidence="0.999799">
Fˆ(ˆf1(X1), ..., ˆf1(Xn), ˆf(y)) (7)
= C[ ˆFX � ˆf1(X1)~, ..., ˆFXn � ˆfn(Xn)~, ˆFy � ˆfy(y)~](8)
</equation>
<bodyText confidence="0.9999212">
In this work, we apply the parametric Gaussian
copula to model the correlations among the text
features and the label. Assume xi is the smoothed
version of random variable Xi, and y is the
smoothed label, we have:
</bodyText>
<equation confidence="0.99950075">
F(x1, ..., xn, y) (9)
~ ~
= ΦE Φ−1[Fx (x1)], ..., , Φ−1[Fxn(xn)], Φ−1[Fy(y)]
(10)
</equation>
<bodyText confidence="0.999985">
where ΦΣ is the joint cumulative distribution func-
tion of a multivariate Gaussian with zero mean and
Σ variance. Φ−1 is the inverse CDF of a standard
Gaussian. In this parametric part of the model, the
parameter estimation boils down to the problem of
learning the covariance matrix Σ of this Gaussian
copula. In this work, we perform standard maxi-
mum likelihood estimation for the Σ matrix.
To calibrate the Σ matrix, we make use of
the power of randomness: using the initial Σ
from MLE, we generate random samples from
the Gaussian copula, and then concatenate previ-
ously generated joint of Gaussian inverse marginal
CDFs with the newly generated random copula
numbers, and re-estimate using MLE to derive the
final adjusted Σ. Note that the final Σ matrix has
to be symmetric and positive definite.
</bodyText>
<subsectionHeader confidence="0.92688">
Computational Complexity
</subsectionHeader>
<bodyText confidence="0.996879">
One important question regarding the proposed
semiparametric Gaussian copula model is the cor-
responding computational complexity. This boils
down to the estimation of the Σˆ matrix (Liu et al.,
2012): one only needs to calculate the correla-
tion coefficients of n(n − 1)/2 pairs of random
variables. Christensen (2005) shows that sort-
ing and balanced binary trees can be used to cal-
culate the correlation coefficients with complex-
ity of O(n log n). Therefore, the computational
complexity of MLE for the proposed model is
O(n log n).
</bodyText>
<subsectionHeader confidence="0.922045">
Efficient Approximate Inference
</subsectionHeader>
<bodyText confidence="0.997204">
In this regression task, in order to perform
exact inference of the conditional probability
distribution p(Fy(y)|Fx1(x1), ..., Fxn(xn)),
one needs to solve the mean response
ˆE(Fy(y)|Fx1(x1), ..., Fx1(x1)) from a joint
distribution of high-dimensional Gaussian copula.
Assume in the simple bivariate case of Gaussian
copula regression, the covariance matrix Σ is:
</bodyText>
<equation confidence="0.999468">
Σ = r Σ11 Σ12 1
L Σ22 J
</equation>
<bodyText confidence="0.969428533333333">
We can easily derive the conditional density that
can be used to calculate the expected value of the
CDF of the label:
exp − 2δT ~[Σ22 − ΣT —1E12]-1 − I~ δ
where δ = Φ−1[Fy(y)] − ΣT 12Σ−1
11 Φ−1[Fx1(x1)].
Unfortunately, the exact inference can be in-
tractable in the multivariate case, and approximate
inference, such as Markov Chain Monte Carlo
sampling (Gelfand and Smith, 1990; Pitt et al.,
2006) is often used for posterior inference. In this
work, we propose an efficient sampling method
to derive y given the text features — we sample
Fy(y) s.t. it maximizes the joint high-dimensional
Gaussian copula density:
</bodyText>
<equation confidence="0.92933975">
ˆFy(y) Pz� arg max
Fy(y)∈(0,1) d Eet exp ~−12ΔT (F,-,
� 1 − I~ Δ~
variable y and its CDF ˆFy(
Fˆ(ν) = 1
m
Xm
i=1
C(Fy(y)|Fx1(x1); Σ) = 1
2
|Σ22 − ΣT 12Σ−1
11 Σ12|1
</equation>
<page confidence="0.931138">
1158
</page>
<bodyText confidence="0.970788">
where
</bodyText>
<equation confidence="0.96547975">
Φ−1(Fx1(x1))
...
Φ−1(Fxn(xn))
Φ−1(Fy(y))
</equation>
<bodyText confidence="0.999888333333333">
Again, the reason why we perform approxi-
mated inference is that: exact inference in the
high-dimensional Gaussian copula density is non-
trivial, and might not have analytical solutions,
but approximate inference using maximum den-
sity sampling from the Gaussian copula signifi-
cantly relaxes the complexity of inference. Fi-
nally, to derive ˆy, the last step is to compute the
inverse CDF of ˆFy(y).
</bodyText>
<subsectionHeader confidence="0.984933">
3.3 Algorithmic Implementation
</subsectionHeader>
<bodyText confidence="0.999965">
The algorithmic implementation of our semipara-
metric Gaussian copula text regression model is
shown in Algorithm 1. Basically, the algorithm
can be decomposed into four parts:
</bodyText>
<listItem confidence="0.997449818181818">
• Perform nonparametric Box kernel density
estimates of the covariates and the dependent
variable for smoothing.
• Calculate the empirical cumulative distribu-
tion functions of the smoothed random vari-
ables.
• Estimate the parameters (covariance E) of the
Gaussian copula.
• Infer the predicted value of the dependent
variable by sampling the Gaussian copula
probability density function.
</listItem>
<sectionHeader confidence="0.997029" genericHeader="method">
4 Datasets
</sectionHeader>
<bodyText confidence="0.99976425">
We use three datasets4 of transcribed quarterly
earnings calls from the U.S. stock market, focus-
ing on the period of the Great Recession.
The pre-2009 dataset consists of earnings calls
from the period of 2006-2008, which includes
calls from the beginning of economic downturn,
the outbreak of the subprime mortgage crisis, and
the epidemic of collapses of large financial insti-
tutions. The 2009 dataset contains earnings calls
from the year of 2009, which is a period where the
credit crisis spreads globally, and the Dow Jones
Industrial Average hit the lowest since the begin-
ning of the millennium. The post-2009 dataset in-
cludes earnings calls from the period of 2010 to
2013, which concerns the recovery of global econ-
omy. The detailed statistics is shown in Table 1.
</bodyText>
<footnote confidence="0.772714">
4http://www.cs.cmu.edu/˜yww/data/earningscalls.zip
</footnote>
<figure confidence="0.785435333333333">
Algorithm 1 A Semi-parametric Gaussian Copula
Model Based Text Regression Algorithm
Given:
(1) training data (X(tr), �y(tr));
(2) testing data (X(te), �y(te));
Learning:
</figure>
<equation confidence="0.992332451612903">
for i = 1 → n dimensions do
X(tr)0 i← BoxKDE(X(tr)
i , Xi (tr));
U(tr) i ←EmpiricalCDF(X(tr)0
i );
i ,X(te)
i );
X(te)0 i← BoxKDE(X(tr)
U(te) i ← EmpiricalCDF(X(te)0
i );
end for
y(tr)0 ←BoxKDE(y(tr), y(tr));
v(tr) ← EmpiricalCDF(y(tr)0);
Z(tr) ← GaussianInverseCDF([U(tr) v(tr)]);
Eˆ ← CorrelationCoefficients(Z(tr));
ˆE, n);
r ← MultiVariateGaussianRandNum(0,
Z(tr)0 = GaussianCDF(r);
Eˆ ← CorrelationCoefficients([Z(tr) Z(tr)0]);
Inference:
for j = 1 → m instances do
maxj ← 0;
Yˆ&amp;quot; = 0;
fork = 0.01 → 1 do
Z(te) ← GaussianInverseCDF([U(te) k]);
_ MultiV ariateGaussianPDF(Z(te),ˆΣ)
pj — � ;
n GaussianPDF(Z(te))
if pj ≥ maxj then
maxj = pj;
Yˆ&amp;quot; = k;
</equation>
<table confidence="0.835734125">
end if
end for
end for
yˆ ← InverseCDF(-y(tr), Yˆ&amp;quot;);
Dataset #Calls #Companies #Types #Tokens
Pre-2009 3694 2746 371.5K 28.7M
2009 3474 2178 346.2K 26.4M
Post-2009 3726 2107 377.4K 28.6M
</table>
<tableCaption confidence="0.8684675">
Table 1: Statistics of three datasets. Types: unique
words. Tokens: word tokens.
</tableCaption>
<bodyText confidence="0.999968916666667">
Note that unlike the standard news corpora in
NLP or the SEC-mandated financial report, Tran-
scripts of earnings call is a very special genre
of text. For example, the length of WSJ docu-
ments is typically one to three hundreds (Harman,
1995), but the averaged document length of our
three earnings calls datasets is 7677. Depending
on the amount of interactions in the question an-
swering session, the complexities of the calls vary.
This mixed form of formal statement and informal
speech brought difficulties to machine learning al-
gorithms.
</bodyText>
<sectionHeader confidence="0.979184" genericHeader="method">
5 Measuring Financial Risks
</sectionHeader>
<bodyText confidence="0.999331666666667">
Volatility is an important measure of the financial
risk, and in this work, we focus on predicting the
future volatility following the earnings teleconfer-
</bodyText>
<equation confidence="0.9470934">
⎛
⎜ ⎜ ⎜ ⎝
A =
⎞
⎠ ⎟ ⎟ ⎟
</equation>
<page confidence="0.97914">
1159
</page>
<bodyText confidence="0.999508333333333">
ence call. For each earning call, we have a week of
stock prices of the company after the day on which
the earnings call is made. The Return of Day t is:
</bodyText>
<equation confidence="0.9954015">
1 (13)
xt−1
</equation>
<bodyText confidence="0.9956125">
where xt represents the share price of Day t, and
the Measured Stock Volatility from Day t to t + T:
</bodyText>
<equation confidence="0.987588666666667">
��τ i=0(rt+i − ¯r)2
y(t,t+τ) = (14)
T
</equation>
<bodyText confidence="0.9999006">
Using the stock prices, we can use the equations
above to calculate the measured stock volatility af-
ter the earnings call, which is the standard measure
of risks in finance, and the dependent variable y of
our predictive task.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996185">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9904203125">
In all experiments throughout this section, we use
80-20 train/test splits on all three datasets.
Feature sets:
We have extracted lexical, named entity, syntactic,
and frame-semantics features, most of which have
been shown to perform well in previous work (Xie
et al., 2013). We use the unigrams and bigrams
to represent lexical features, and the Stanford part-
of-speech tagger (Toutanova et al., 2003) to extract
the lexicalized named entity and part-of-speech
features. A probabilistic frame-semantics parser,
SEMAFOR (Das et al., 2010), is used to provide
the FrameNet-style frame-level semantic annota-
tions. For each of the five sets, we collect the top-
100 most frequent features, and end up with a total
of 500 features.
</bodyText>
<sectionHeader confidence="0.558391" genericHeader="method">
Baselines:
</sectionHeader>
<bodyText confidence="0.999886842105263">
The baselines are standard squared-loss linear
regression, linear kernel SVM, and non-linear
(Gaussian) kernel SVM. They are all standard
algorithms in regression problems, and have
been shown to have outstanding performances in
many recent text regression (Kogan et al., 2009;
Chahuneau et al., 2012; Xie et al., 2013; Wang
et al., 2013; Tsai and Wang, 2013). We use
the Statistical Toolbox’s linear regression imple-
mentation in Matlab, and LibSVM (Chang and
Lin, 2011) for training and testing the SVM mod-
els. The hyperparameter C in linear SVM, and
the γ and C hyperparameters in Gaussian SVM
are tuned on the training set using 10-fold cross-
validation. Note that since the kernel density esti-
mation in the proposed copula model is nonpara-
metric, and we only need to learn the E in the
Gaussian copula, there is no hyperparameters that
need to be tuned.
</bodyText>
<subsectionHeader confidence="0.495632">
Evaluation Metrics:
</subsectionHeader>
<bodyText confidence="0.999529875">
Spearman’s correlation (Hogg and Craig, 1994)
and Kendall’s tau (Kendall, 1938) have been
widely used in many regression problems in NLP
(Albrecht and Hwa, 2007; Yogatama et al., 2011;
Wang et al., 2013; Tsai and Wang, 2013), and here
we use them to measure the quality of predicted
values yˆ by comparing to the vector of ground
truth y. In contrast to Pearson’s correlation, Spear-
man’s correlation has no assumptions on the rela-
tionship of the two measured variables. Kendall’s
tau is a nonparametric statistical metric that have
shown to be inexpensive, robust, and represen-
tation independent (Lapata, 2006). We also use
paired two-tailed t-test to measure the statistical
significance between the best and the second best
approaches.
</bodyText>
<subsectionHeader confidence="0.999688">
6.2 Comparing to Various Baselines
</subsectionHeader>
<bodyText confidence="0.99998552631579">
In the first experiment, we compare the proposed
semiparametric Gaussian copula regression model
to three baselines on three datasets with all fea-
tures. The detailed results are shown in Table 2.
On the pre-2009 dataset, we see that the linear re-
gression and linear SVM perform reasonably well,
but the Gaussian kernel SVM performs less well,
probably due to overfitting. The copula model
outperformed all three baselines by a wide mar-
gin on this dataset with both metrics. Similar per-
formances are also obtained in the 2009 dataset,
where the result of linear SVM baseline falls be-
hind. On the post-2009 dataset, none of results
from the linear and non-linear SVM models can
match up with the linear regression model, but
our proposed copula model still improves over all
baselines by a large margin. Comparing to second-
best approaches, all improvements obtained by the
copula model are statistically significant.
</bodyText>
<subsectionHeader confidence="0.999586">
6.3 Varying the Amount of Training Data
</subsectionHeader>
<bodyText confidence="0.999652416666667">
To understand the learning curve of our proposed
copula regression model, we use the 25%, 50%,
75% subsets from the training data, and evaluate
all four models. Figure 1 shows the evaluation
results. From the experiments on the pre-2009
dataset, we see that when the amount of training
data is small (25%), both SVM models have ob-
tained very impressive results. This is not surpris-
ing at all, because as max-margin models, soft-
margin SVM only needs a handful of examples
that come with nonvanishing coefficients (support
vectors) to find a reasonable margin. When in-
</bodyText>
<equation confidence="0.922903">
xt
rt =
</equation>
<page confidence="0.957961">
1160
</page>
<table confidence="0.999364666666667">
Method Pre-2009 2009 Post-2009
Spearman Kendall Spearman Kendall Spearman Kendall
linear regression: 0.377 0.259 0.367 0.252 0.314 0.216
linear SVM: 0.364 0.249 0.242 0.167 0.132 0.091
Gaussian SVM: 0.305 0.207 0.280 0.192 0.152 0.104
Gaussian copula: 0.425* 0.315* 0.422* 0.310* 0.375* 0.282*
</table>
<tableCaption confidence="0.9939125">
Table 2: Comparing the learning algorithms on three datasets with all features. The best result is high-
lighted in bold. * indicates p &lt; .001 comparing to the second best result.
</tableCaption>
<figureCaption confidence="0.976757">
Figure 1: Varying the amount of training data. Left column: pre-2009 dataset. Middle column: 2009
dataset. Right column: post-2009 dataset. Top row: Spearman’s correlation. Bottom row: Kendall’s tau.
</figureCaption>
<bodyText confidence="0.997823">
creasing the amount of training data to 50%, we do
see the proposed copula model catches up quickly,
and lead all baseline methods undoubtably at 75%
training data. On the 2009 dataset, we observe
very similar patterns. Interestingly, the proposed
copula regression model has dominated all meth-
ods for both metrics throughout all proportions of
the “post-2009” earnings calls dataset, where in-
stead of financial crisis, the economic recovery is
the main theme. In contrast to the previous two
datasets, both linear and non-linear SVMs fail to
reach reasonable performances on this dataset.
</bodyText>
<subsectionHeader confidence="0.999701">
6.4 Varying the Amount of Features
</subsectionHeader>
<bodyText confidence="0.999900458333334">
Finally, we investigate the robustness of the pro-
posed semiparametric Gaussian copula regression
model by varying the amount of features in the co-
variate space. To do this, we sample equal amount
of features from each feature set, and concatenate
them into a feature vector. When increasing the
amount of total features from 100 to 400, the re-
sults are shown in Figure 2. On the pre-2009
dataset, we see that the gaps between the best-
perform copula model and the second-best linear
regression model are consistent throughout all fea-
ture sizes. On the 2009 dataset, we see that the
performance of Gaussian copula is aligned with
the linear regression model in terms of Spearman’s
correlation, where the former seems to perform
better in terms of Kendall’s tau. Both linear and
non-linear SVM models do not have any advan-
tages over the proposed approach. On the post-
2009 dataset that concerns economic growth and
recovery, the boundaries among all methods are
very clear. The Spearman’s correlation for both
SVM baselines is less than 0.15 throughout all set-
tings, but copula model is able to achieve 0.4 when
using 400 features. The improvements of copula
</bodyText>
<page confidence="0.995929">
1161
</page>
<figureCaption confidence="0.975532">
Figure 2: Varying the amount of features. Left column: pre-2009 dataset. Middle column: 2009 dataset.
Right column: post-2009 dataset. Top row: Spearman’s correlation. Bottom row: Kendall’s tau.
</figureCaption>
<table confidence="0.997380272727273">
Pre-2009 2009 Post-2009
2008/CD 2008 first quarter
2008 million/CD revenue/NN
third quarter 2008/CD revenue
third million quarter of
third/JJ million in compared to
the third the fourth million in
million/CD fourth quarter Peter/PERSON
capital fourth call
million fourth/JJ first/JJ
FE Trajector entity $/$ million/CD
</table>
<tableCaption confidence="0.9720755">
Table 3: Top-10 features that have positive corre-
lations with stock volatility in three datasets.
</tableCaption>
<bodyText confidence="0.685637666666667">
model over squared loss linear regression model
are increasing, when working with larger feature
spaces.
</bodyText>
<subsectionHeader confidence="0.998575">
6.5 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999991157894737">
Like linear classifiers, by “opening the hood” to
the Gaussian copula regression model, one can ex-
amine features that exhibit high correlations with
the dependent variable. Table 3 shows the top fea-
tures that are positively correlated with the future
stock volatility in the three datasets. On the top
features from the “pre-2009” dataset, which pri-
marily (82%) includes calls from 2008, we can
clearly observe that the word “2008” has strong
correlation with the financial risks. Interestingly,
the phrase “third quarter” and its variations, not
only play an important role in the model, but also
highly correlated to the timeline of the financial
crisis: the Q3 of 2008 is a critical period in the
recession, where Lehman Brothers falls on the
Sept. 15 of 2008, filing $613 billion of debt —
the biggest bankruptcy in U.S. history (Mamudi,
2008). This huge panic soon broke out in vari-
ous financial institutions in the Wall Street. On
the top features from “2009” dataset, again, we see
the word “2008” is still prominent in predicting fi-
nancial risks, indicating the hardship and extended
impacts from the center of the economic crisis.
After examining the transcripts, we found sen-
tences like: “...our specialty lighting business that
we discontinued in the fourth quarter of 2008...”,
“...the exception of fourth quarter revenue which
was $100,000 below our guidance target...”, and
“...to address changing economic conditions and
their impact on our operations, in the fourth quar-
ter we took the painful but prudent step of de-
creasing our headcount by about 5%...”, show-
ing the crucial role that Q4 of 2008 plays in 2009
earnings calls. Interestingly, after the 2008-2009
crisis, in the recovery period, we have observed
new words like “revenue”, indicating the “back-to-
normal” trend of financial environment, and new
features that predict financial volatility.
</bodyText>
<sectionHeader confidence="0.994833" genericHeader="evaluation">
7 Discussions
</sectionHeader>
<bodyText confidence="0.9996685">
In the experimental section, we notice that the
proposed semiparametric Gaussian copula model
has obtained promising results in various setups
on three datasets in this text regression task. The
</bodyText>
<page confidence="0.990031">
1162
</page>
<bodyText confidence="0.999982225">
main questions we ask are: how is the pro-
posed model different from standard text regres-
sion/classification models? What are the advan-
tages of copula-based models, and what makes it
perform so well?
One advantage we see from the copula model
is that it does not require any assumptions on
the marginal distributions. For example, in latent
Dirichlet allocation (Blei et al., 2003), the topic
proportion of a document is always drawn from
a Dirichlet(α) distribution. This is rather re-
stricted, because the possible shapes from a K −1
simplex of Dirichlet is always limited in some
sense. In our copula model, instead of using some
priors, we just calculate the empirical cumulative
distribution function of the random variables, and
model the correlation among them. This is ex-
tremely practical, because in many natural lan-
guage processing tasks, we often have to deal with
features that are extracted from many different do-
mains and signals. By applying the Probability
Integral Transform to raw features in the copula
model, we essentially avoid comparing apples and
oranges in the feature space, which is a common
problem in bag-of-features models in NLP.
The second hypothesis is about the semiparam-
etirc parameterization, which contains the non-
parametric kernel density estimation and the para-
metric Gaussian copula regression components.
The benefit of a semiparametric model is that here
we are not interested in performing completely
nonparametric estimations, where the infinite di-
mensional parameters might bring intractability.
In contrast, by considering the semiparametric
case, we not only obtain some expressiveness from
the nonparametric models, but also reduce the
complexity of the task: we are only interested in
the finite-dimensional components E in the Gaus-
sian copula with O(n log n) complexity, which
is not as computationally difficult as the com-
pletely nonparametric cases. Also, by modeling
the marginals and their correlations seperately, our
approach is cleaner, easy-to-understand, and al-
lows us to have more flexibility to model the un-
certainty of data. Our pilot experiment also aligns
with our hypothesis: when not performing the ker-
nel density estimation part for smoothing out the
marginal distributions, the performances dropped
significantly when sparser features are included.
The third advantage we observe is the power of
modeling the covariance of the random variables.
Traditionally, in statistics, independent and identi-
cally distributed (i.i.d) assumptions among the in-
stances and the random variables are often used in
various models, such that the correlations among
the instances or the variables are often ignored.
However, this might not be practical at all: in im-
age processing, the “cloud” pixel of a pixel show-
ing the blue sky of a picture are more likelihood to
co-occur in the same picture; in natural language
processing, the word “mythical” is more likely to
co-occur with the word “unicorn”, rather than the
word “popcorn”. Therefore, by modeling the cor-
relations among marginal CDFs, the copula model
has gained the insights on the dependency struc-
tures of the random variables, and thus, the perfor-
mance of the regression task is boosted.
In the future, we plan to apply the proposed
approach to large datasets where millions of fea-
tures and millions of instances are involved. Cur-
rently we have not experienced the difficulty when
estimating the Gaussian copula model, but paral-
lel methods might be needed to speedup learning
when significantly more marginal CDFs are in-
volved. The second issue is about overfitting. We
see that when features are rather noisy, we might
need to investigate regularized copula models to
avoid this. Finally, we plan to extend the proposed
approach to text classification and structured pre-
diction problems in NLP.
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999987521739131">
In this work, we have demonstrated that the more
complex quarterly earnings calls can also be used
to predict the measured volatility of the stocks in
the limited future. We propose a novel semipara-
metric Gausian copula regression approach that
models the dependency structure of the language
in the earnings calls. Unlike traditional bag-of-
features models that work discrete features from
various signals, we perform kernel density esti-
mation to smooth out the distribution, and use
probability integral transform to work with CDFs
that are uniform. The copula model deals with
marginal CDFs and the correlation among them
separately, in a cleaner manner that is also flexible
to parameterize. Focusing on the three financial
crisis related datasets, the proposed model signif-
icantly outperform the standard linear regression
method in statistics and strong discriminative sup-
port vector regression baselines. By varying the
size of the training data and the dimensionality of
the covariates, we have demonstrated that our pro-
posed model is relatively robust across different
parameter settings.
</bodyText>
<sectionHeader confidence="0.973193" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.982698666666667">
We thank Alex Smola, Barnab´as P´oczos, Sam
Thomson, Shoou-I Yu, Zi Yang, and anonymous
reviewers for their useful comments.
</bodyText>
<page confidence="0.969041">
1163
</page>
<sectionHeader confidence="0.996116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999366913461539">
Joshua Albrecht and Rebecca Hwa. 2007. Regression
for sentence-level mt evaluation with pseudo refer-
ences. In Proceedings of Annual Meeting of the As-
sociation for Computational Linguistics.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet allocation. Journal of machine
Learning research.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science.
Belinda Camiciottoli. 2010. Earnings calls: Exploring
an emerging financial reporting genre. Discourse &amp;
Communication.
Victor Chahuneau, Kevin Gimpel, Bryan R Routledge,
Lily Scherlis, and Noah A Smith. 2012. Word
salad: Relating food prices and descriptions. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology.
Xiaohong Chen and Yanqin Fan. 2006. Estimation
of copula-based semiparametric time series models.
Journal of Econometrics.
David Christensen. 2005. Fast algorithms for the cal-
culation of kendalls τ. Computational Statistics.
Kenneth Church and William Gale. 1995. Poisson
mixtures. Natural Language Engineering.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Human language technologies: The
2010 annual conference of the North American
chapter of the association for computational linguis-
tics.
Francis X Diebold, Todd A Gunther, and Anthony S
Tay. 1997. Evaluating density forecasts.
Carsten Eickhoff, Arjen P. de Vries, and Kevyn
Collins-Thompson. 2013. Copulas for information
retrieval. In Proceedings of the 36th International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval.
Jacob Eisenstein, Brendan O’Connor, Noah A Smith,
and Eric P Xing. 2010. A latent variable model for
geographic lexical variation. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing.
Pui Cheong Fung, Xu Yu, and Wai Lam. 2003. Stock
prediction: Integrating text mining approach using
real-time news. In Proceedings of IEEE Interna-
tional Conference on Computational Intelligence for
Financial Engineering.
Alan Gelfand and Adrian Smith. 1990. Sampling-
based approaches to calculating marginal densities.
Journal of the American statistical association.
Christian Genest and Anne-Catherine Favre. 2007.
Everything you always wanted to know about copula
modeling but were afraid to ask. Journal of Hydro-
logic Engineering.
Zoubin Ghahramani, Barnab´as P´oczos, and Jeff
Schneider. 2012. Copula-based kernel dependency
measures. In Proceedings of the 29th International
Conference on Machine Learning.
Fang Han, Tuo Zhao, and Han Liu. 2012. Coda: High
dimensional copula discriminant analysis. Journal
of Machine Learning Research.
Donna Harman. 1995. Overview of the second text re-
trieval conference (trec-2). Information Processing
&amp; Management.
Robert V Hogg and Allen Craig. 1994. Introduction to
mathematical statistics.
Harry Joe. 1997. Multivariate models and dependence
concepts.
Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and
Noah A Smith. 2010. Movie reviews and revenues:
An experiment in text regression. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika.
Shimon Kogan, Dimitry Levin, Bryan Routledge, Ja-
cob Sagi, and Noah Smith. 2009. Predicting risk
from financial reports with regression. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
John Lafferty and David Blei. 2005. Correlated topic
models. In Advances in neural information process-
ing systems.
Mirella Lapata. 2006. Automatic evaluation of infor-
mation ordering: Kendall’s tau. Computational Lin-
guistics.
Han Liu, Fang Han, Ming Yuan, John Lafferty, and
Larry Wasserman. 2012. High-dimensional semi-
parametric gaussian copula graphical models. The
Annals of Statistics.
David Lopez-paz, Jose M Hern´andez-lobato, and
Ghahramani Zoubin. 2013. Gaussian process vine
copulas for multivariate dependence. In Proceed-
ings of the 30th International Conference on Ma-
chine Learning.
Sam Mamudi. 2008. Lehman folds with record $613
billion debt. MarketWatch.com.
</reference>
<page confidence="0.907538">
1164
</page>
<reference confidence="0.999747838235294">
Guido Masarotto and Cristiano Varin. 2012. Gaussian
copula marginal regression. Electronic Journal of
Statistics.
Roger B Nelsen. 1999. An introduction to copulas.
Springer Verlag.
Rahul A Parsa and Stuart A Klugman. 2011. Copula
regression. Variance Advancing and Science of Risk.
Emanuel Parzen. 1962. On estimation of a probability
density function and mode. The annals of mathe-
matical statistics.
Michael Pitt, David Chan, and Robert Kohn. 2006.
Efficient bayesian inference for gaussian copula re-
gression models. Biometrika.
McKay Price, James Doran, David Peterson, and Bar-
bara Bliss. 2012. Earnings conference calls and
stock returns: The incremental informativeness of
textual tone. Journal of Banking &amp; Finance.
Joseph Reisinger, Austin Waters, Bryan Silverthorn,
and Raymond J Mooney. 2010. Spherical topic
models. In Proceedings of the 27th International
Conference on Machine Learning.
Berthold Schweizer and Abe Sklar. 1983. Probabilis-
tic metric spaces.
Claude Shannon. 1948. A mathematical theory of
communication. In The Bell System Technical Jour-
nal.
Abe Sklar. 1959. Fonctions de r´epartition a` n dimen-
sions et leurs marges. Universit´e Paris 8.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology.
Ming-Feng Tsai and Chuan-Ju Wang. 2013. Risk
ranking from financial reports. In Advances in In-
formation Retrieval.
William Yang Wang, Elijah Mayfield, Suresh Naidu,
and Jeremiah Dittmar. 2012. Historical analysis
of legal opinions with a sparse mixed-effects latent
variable model. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics.
Chuan-Ju Wang, Ming-Feng Tsai, Tse Liu, and Chin-
Ting Chang. 2013. Financial sentiment analysis for
risk prediction. In Proceedings of the Sixth Interna-
tional Joint Conference on Natural Language Pro-
cessing.
Rongjing Xiang and Jennifer Neville. 2013. Collec-
tive inference for network data with copula latent
markov networks. In Proceedings of the sixth ACM
international conference on Web search and data
mining.
Boyi Xie, Rebecca J. Passonneau, Leon Wu, and
Germ´an G. Creamer. 2013. Semantic frames to pre-
dict stock price movement. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics.
Dani Yogatama, Michael Heilman, Brendan O’Connor,
Chris Dyer, Bryan R Routledge, and Noah A Smith.
2011. Predicting a scientific community’s response
to an article. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Xue Zhang, Hauke Fuehres, and Peter A Gloor. 2011.
Predicting stock market indicators through twitter “i
hope it is not as bad as i fear”. Procedia-Social and
Behavioral Sciences.
</reference>
<page confidence="0.994413">
1165
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938153">
<title confidence="0.99932">A Semiparametric Gaussian Copula Regression for Predicting Financial Risks from Earnings Calls</title>
<author confidence="0.999983">William Yang Wang</author>
<affiliation confidence="0.9999235">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999747">Pittsburgh, PA 15213</address>
<email confidence="0.998322">yww@cs.cmu.edu</email>
<abstract confidence="0.998026066666667">Earnings call summarizes the financial performance of a company, and it is an important indicator of the future financial risks of the company. We quantitatively study how earnings calls are correlated with the financial risks, with a special focus on the financial crisis of 2009. In particular, we perform a text regression task: given the transcript of an earnings call, we predict the volatility of stock prices from the week after the call is made. We prothe use of a powerful statistical framework that separately models the uniform marginals and their complex multivariate stochastic dependencies, while not requiring any prior assumptions on the distributions of the covariate and the devariable. By performing probabilintegral our approach moves beyond the standard count-based bag-ofwords models in NLP, and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula. In experiments, we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joshua Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>Regression for sentence-level mt evaluation with pseudo references.</title>
<date>2007</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23397" citStr="Albrecht and Hwa, 2007" startWordPosition="3764" endWordPosition="3767">lementation in Matlab, and LibSVM (Chang and Lin, 2011) for training and testing the SVM models. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold crossvalidation. Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the E in the Gaussian copula, there is no hyperparameters that need to be tuned. Evaluation Metrics: Spearman’s correlation (Hogg and Craig, 1994) and Kendall’s tau (Kendall, 1938) have been widely used in many regression problems in NLP (Albrecht and Hwa, 2007; Yogatama et al., 2011; Wang et al., 2013; Tsai and Wang, 2013), and here we use them to measure the quality of predicted values yˆ by comparing to the vector of ground truth y. In contrast to Pearson’s correlation, Spearman’s correlation has no assumptions on the relationship of the two measured variables. Kendall’s tau is a nonparametric statistical metric that have shown to be inexpensive, robust, and representation independent (Lapata, 2006). We also use paired two-tailed t-test to measure the statistical significance between the best and the second best approaches. 6.2 Comparing to Vario</context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>Joshua Albrecht and Rebecca Hwa. 2007. Regression for sentence-level mt evaluation with pseudo references. In Proceedings of Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of machine Learning research.</journal>
<contexts>
<context position="8141" citStr="Blei et al., 2003" startWordPosition="1288" endWordPosition="1291">s, which in some sense, corresponding to the impractical assumption of independent and identically distributed (i.i.d) of the data. For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained. 3 Copula Models for Text Regression In NLP, many statistical machine learning methods that capture the dependencies among random variables, including topic models (Blei et al., 2003; Lafferty and Blei, 2005; Wang et al., 2012), always have to make assumptions with the underlying distributions of the random variables, and make use of informative priors. This might be rather restricting the expressiveness of the model in some sense (Reisinger et al., 2010). On the other hand, once such assumptions are removed, another problem arises — they might be prone to errors, and suffer from the overfitting issue. Therefore, coping with the tradeoff between expressiveness and overfitting, seems to be rather important in statistical approaches that capture stochastic dependency. Our p</context>
<context position="9573" citStr="Blei et al., 2003" startWordPosition="1513" endWordPosition="1516">eir correlations. On the other hand, it does not make use of any as1156 sumptions on the distributions of the random variables, yet, the copula model is still expressive. This nice property essentially allows us to fuse distinctive lexical, syntactic, and semantic feature sets naturally into a single compact model. From an information-theoretic point of view (Shannon, 1948), various problems in text analytics can be formulated as estimating the probability mass/density functions of tokens in text. In NLP, many of the probabilistic text models work in the discrete space (Church and Gale, 1995; Blei et al., 2003), but our model is different: since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables. By doing this, we are essentially performing probability integral transform— an important statistical technique that moves beyond the count-based bag-of-words feature space to marginal cumulative density functions space. Last but not least, by using a parametric copula, in our case, the Gaussian copula, we reduce the computational cost from fully nonparametric</context>
<context position="31204" citStr="Blei et al., 2003" startWordPosition="5012" endWordPosition="5015">t financial volatility. 7 Discussions In the experimental section, we notice that the proposed semiparametric Gaussian copula model has obtained promising results in various setups on three datasets in this text regression task. The 1162 main questions we ask are: how is the proposed model different from standard text regression/classification models? What are the advantages of copula-based models, and what makes it perform so well? One advantage we see from the copula model is that it does not require any assumptions on the marginal distributions. For example, in latent Dirichlet allocation (Blei et al., 2003), the topic proportion of a document is always drawn from a Dirichlet(α) distribution. This is rather restricted, because the possible shapes from a K −1 simplex of Dirichlet is always limited in some sense. In our copula model, instead of using some priors, we just calculate the empirical cumulative distribution function of the random variables, and model the correlation among them. This is extremely practical, because in many natural language processing tasks, we often have to deal with features that are extracted from many different domains and signals. By applying the Probability Integral </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
<author>Xiaojun Zeng</author>
</authors>
<title>Twitter mood predicts the stock market.</title>
<date>2011</date>
<journal>Journal of Computational Science.</journal>
<contexts>
<context position="6759" citStr="Bollen et al., 2011" startWordPosition="1067" endWordPosition="1070">(2013) reformulate the regression problem as a text ranking problem. Note that all these regression studies above investigate the SEC-mandated annual reports, which are very different from the earnings calls in many aspects such as length, format, vocabulary, and genre. Most recently, Xie et al. (2013) have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a binary classification task. Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market (Bollen et al., 2011; Zhang et al., 2011). Despite our financial domain, our approach is more relevant to text regression. Traditional discriminative models, such as linear regression and linear SVM, have been very popular in various text regression tasks, such as predicting movie revenues from reviews (Joshi et al., 2010), understanding the geographic lexical variation (Eisenstein et al., 2010), and predicting food prices from menus (Chahuneau et al., 2012). The advantage of these models is that the estimation of the parameters is often simple, the results are easy to interpret, and the approach often yields str</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of Computational Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Belinda Camiciottoli</author>
</authors>
<title>Earnings calls: Exploring an emerging financial reporting genre.</title>
<date>2010</date>
<booktitle>Discourse &amp; Communication.</booktitle>
<contexts>
<context position="1819" citStr="Camiciottoli, 2010" startWordPosition="275" endWordPosition="276">ussian copula. In experiments, we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings. 1 Introduction Predicting the risks of publicly listed companies is of great interests not only to the traders and analysts on the Wall Street, but also virtually anyone who has investments in the market (Kogan et al., 2009). Traditionally, analysts focus on quantitative modeling of historical trading data. Today, even though earnings calls transcripts are abundantly available, their distinctive communicative practices (Camiciottoli, 2010), and correlations with the financial risks, in particular, future stock Zhenhao Hua School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 zhua@cs.cmu.edu performances (Price et al., 2012), are not well studied in the past. Earnings calls are conference calls where a listed company discusses the financial performance. Typically, a earnings call contains two parts: the senior executives first report the operational outcomes, as well as the current financial performance, and then discuss their perspectives on the future of the company. The second part of the teleconference i</context>
</contexts>
<marker>Camiciottoli, 2010</marker>
<rawString>Belinda Camiciottoli. 2010. Earnings calls: Exploring an emerging financial reporting genre. Discourse &amp; Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Chahuneau</author>
<author>Kevin Gimpel</author>
<author>Bryan R Routledge</author>
<author>Lily Scherlis</author>
<author>Noah A Smith</author>
</authors>
<title>Word salad: Relating food prices and descriptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="7201" citStr="Chahuneau et al., 2012" startWordPosition="1136" endWordPosition="1139">lem as a binary classification task. Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market (Bollen et al., 2011; Zhang et al., 2011). Despite our financial domain, our approach is more relevant to text regression. Traditional discriminative models, such as linear regression and linear SVM, have been very popular in various text regression tasks, such as predicting movie revenues from reviews (Joshi et al., 2010), understanding the geographic lexical variation (Eisenstein et al., 2010), and predicting food prices from menus (Chahuneau et al., 2012). The advantage of these models is that the estimation of the parameters is often simple, the results are easy to interpret, and the approach often yields strong performances. While these approaches have merits, they suffer from the problem of not explicitly modeling the correlations and interactions among random variables, which in some sense, corresponding to the impractical assumption of independent and identically distributed (i.i.d) of the data. For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencie</context>
<context position="22660" citStr="Chahuneau et al., 2012" startWordPosition="3638" endWordPosition="3641">he lexicalized named entity and part-of-speech features. A probabilistic frame-semantics parser, SEMAFOR (Das et al., 2010), is used to provide the FrameNet-style frame-level semantic annotations. For each of the five sets, we collect the top100 most frequent features, and end up with a total of 500 features. Baselines: The baselines are standard squared-loss linear regression, linear kernel SVM, and non-linear (Gaussian) kernel SVM. They are all standard algorithms in regression problems, and have been shown to have outstanding performances in many recent text regression (Kogan et al., 2009; Chahuneau et al., 2012; Xie et al., 2013; Wang et al., 2013; Tsai and Wang, 2013). We use the Statistical Toolbox’s linear regression implementation in Matlab, and LibSVM (Chang and Lin, 2011) for training and testing the SVM models. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold crossvalidation. Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the E in the Gaussian copula, there is no hyperparameters that need to be tuned. Evaluation Metrics: Spearman’s correlation </context>
</contexts>
<marker>Chahuneau, Gimpel, Routledge, Scherlis, Smith, 2012</marker>
<rawString>Victor Chahuneau, Kevin Gimpel, Bryan R Routledge, Lily Scherlis, and Noah A Smith. 2012. Word salad: Relating food prices and descriptions. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology.</journal>
<contexts>
<context position="22830" citStr="Chang and Lin, 2011" startWordPosition="3667" endWordPosition="3670">el semantic annotations. For each of the five sets, we collect the top100 most frequent features, and end up with a total of 500 features. Baselines: The baselines are standard squared-loss linear regression, linear kernel SVM, and non-linear (Gaussian) kernel SVM. They are all standard algorithms in regression problems, and have been shown to have outstanding performances in many recent text regression (Kogan et al., 2009; Chahuneau et al., 2012; Xie et al., 2013; Wang et al., 2013; Tsai and Wang, 2013). We use the Statistical Toolbox’s linear regression implementation in Matlab, and LibSVM (Chang and Lin, 2011) for training and testing the SVM models. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold crossvalidation. Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the E in the Gaussian copula, there is no hyperparameters that need to be tuned. Evaluation Metrics: Spearman’s correlation (Hogg and Craig, 1994) and Kendall’s tau (Kendall, 1938) have been widely used in many regression problems in NLP (Albrecht and Hwa, 2007; Yogatama et al., 2011; Wang et </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohong Chen</author>
<author>Yanqin Fan</author>
</authors>
<title>Estimation of copula-based semiparametric time series models.</title>
<date>2006</date>
<journal>Journal of Econometrics.</journal>
<contexts>
<context position="3277" citStr="Chen and Fan, 2006" startWordPosition="504" endWordPosition="507">ixed formats, can we use earnings calls to predict the financial risks of the company in the limited future? Given a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk1. To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies. Copula models (Schweizer and Sklar, 1983; Nelsen, 1999) are often used by statisticians (Genest and Favre, 2007; Liu et al., 2012; Masarotto and Varin, 2012) and economists (Chen and Fan, 2006) to study the bivariate and multivariate stochastic dependency among random variables, but they are very new to the machine learning (Ghahramani et al., 2012; Han et al., 2012; Xiang and Neville, 2013; Lopezpaz et al., 2013) and related communities (Eickhoff et al., 2013). To the best of our knowledge, even though the term “copula” is named for the resemblance to grammatical copulas in linguistics, copula models have not been explored in the NLP community. To evaluate the performance of our approach, we compare with a standard squared loss linear regression baseline, as well as strong baseline</context>
</contexts>
<marker>Chen, Fan, 2006</marker>
<rawString>Xiaohong Chen and Yanqin Fan. 2006. Estimation of copula-based semiparametric time series models. Journal of Econometrics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Christensen</author>
</authors>
<title>Fast algorithms for the calculation of kendalls τ.</title>
<date>2005</date>
<journal>Computational Statistics.</journal>
<contexts>
<context position="15917" citStr="Christensen (2005)" startWordPosition="2542" endWordPosition="2543">aussian copula, and then concatenate previously generated joint of Gaussian inverse marginal CDFs with the newly generated random copula numbers, and re-estimate using MLE to derive the final adjusted Σ. Note that the final Σ matrix has to be symmetric and positive definite. Computational Complexity One important question regarding the proposed semiparametric Gaussian copula model is the corresponding computational complexity. This boils down to the estimation of the Σˆ matrix (Liu et al., 2012): one only needs to calculate the correlation coefficients of n(n − 1)/2 pairs of random variables. Christensen (2005) shows that sorting and balanced binary trees can be used to calculate the correlation coefficients with complexity of O(n log n). Therefore, the computational complexity of MLE for the proposed model is O(n log n). Efficient Approximate Inference In this regression task, in order to perform exact inference of the conditional probability distribution p(Fy(y)|Fx1(x1), ..., Fxn(xn)), one needs to solve the mean response ˆE(Fy(y)|Fx1(x1), ..., Fx1(x1)) from a joint distribution of high-dimensional Gaussian copula. Assume in the simple bivariate case of Gaussian copula regression, the covariance m</context>
</contexts>
<marker>Christensen, 2005</marker>
<rawString>David Christensen. 2005. Fast algorithms for the calculation of kendalls τ. Computational Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>Poisson mixtures. Natural Language Engineering.</title>
<date>1995</date>
<contexts>
<context position="9553" citStr="Church and Gale, 1995" startWordPosition="1509" endWordPosition="1512">ng the marginals and their correlations. On the other hand, it does not make use of any as1156 sumptions on the distributions of the random variables, yet, the copula model is still expressive. This nice property essentially allows us to fuse distinctive lexical, syntactic, and semantic feature sets naturally into a single compact model. From an information-theoretic point of view (Shannon, 1948), various problems in text analytics can be formulated as estimating the probability mass/density functions of tokens in text. In NLP, many of the probabilistic text models work in the discrete space (Church and Gale, 1995; Blei et al., 2003), but our model is different: since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables. By doing this, we are essentially performing probability integral transform— an important statistical technique that moves beyond the count-based bag-of-words feature space to marginal cumulative density functions space. Last but not least, by using a parametric copula, in our case, the Gaussian copula, we reduce the computational cost from</context>
</contexts>
<marker>Church, Gale, 1995</marker>
<rawString>Kenneth Church and William Gale. 1995. Poisson mixtures. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Nathan Schneider</author>
<author>Desai Chen</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic frame-semantic parsing. In Human language technologies: The</title>
<date>2010</date>
<contexts>
<context position="22161" citStr="Das et al., 2010" startWordPosition="3560" endWordPosition="3563">endent variable y of our predictive task. 6 Experiments 6.1 Experimental Setup In all experiments throughout this section, we use 80-20 train/test splits on all three datasets. Feature sets: We have extracted lexical, named entity, syntactic, and frame-semantics features, most of which have been shown to perform well in previous work (Xie et al., 2013). We use the unigrams and bigrams to represent lexical features, and the Stanford partof-speech tagger (Toutanova et al., 2003) to extract the lexicalized named entity and part-of-speech features. A probabilistic frame-semantics parser, SEMAFOR (Das et al., 2010), is used to provide the FrameNet-style frame-level semantic annotations. For each of the five sets, we collect the top100 most frequent features, and end up with a total of 500 features. Baselines: The baselines are standard squared-loss linear regression, linear kernel SVM, and non-linear (Gaussian) kernel SVM. They are all standard algorithms in regression problems, and have been shown to have outstanding performances in many recent text regression (Kogan et al., 2009; Chahuneau et al., 2012; Xie et al., 2013; Wang et al., 2013; Tsai and Wang, 2013). We use the Statistical Toolbox’s linear </context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A Smith. 2010. Probabilistic frame-semantic parsing. In Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis X Diebold</author>
<author>Todd A Gunther</author>
<author>Anthony S Tay</author>
</authors>
<title>Evaluating density forecasts.</title>
<date>1997</date>
<contexts>
<context position="13791" citStr="Diebold et al., 1997" startWordPosition="2190" endWordPosition="2193">original Parzen windows (Parzen, 1962). 3In our implementation, we use the default h of the Box kernel in the ksdensity function in Matlab. ˆfh(x) = 1 m �m i=1 1 �m i=1 mh 1157 Now, we can derive the empirical cumulative distribution functions ˆFX1(ˆf1(X1)), ˆFX2(ˆf2(X2)),...,ˆFXn(ˆfn(Xn)) of the smoothed covariates, as well as the dependent ˆf(y)). The empirical cumulative distribution functions are defined as: I{xi G ν} (6) where I{·} is the indicator function, and ν indicates the current value that we are evaluating. Note that the above step is also known as probability integral transform (Diebold et al., 1997), which allows us to convert any given continuous distribution to random variables having a uniform distribution. This is of crucial importance to modeling text data: instead of using the classic bag-ofwords representation that uses raw counts, we are now working with uniform marginal CDFs, which helps coping with the overfitting issue due to noise and data sparsity. The Parametric Copula Estimation Now that we have obtained the marginals, and then the joint distribution can be constructed by applying the copula function that models the stochastic dependencies among marginal CDFs: Fˆ(ˆf1(X1), </context>
</contexts>
<marker>Diebold, Gunther, Tay, 1997</marker>
<rawString>Francis X Diebold, Todd A Gunther, and Anthony S Tay. 1997. Evaluating density forecasts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carsten Eickhoff</author>
<author>Arjen P de Vries</author>
<author>Kevyn Collins-Thompson</author>
</authors>
<title>Copulas for information retrieval.</title>
<date>2013</date>
<booktitle>In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<marker>Eickhoff, de Vries, Collins-Thompson, 2013</marker>
<rawString>Carsten Eickhoff, Arjen P. de Vries, and Kevyn Collins-Thompson. 2013. Copulas for information retrieval. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A Smith, and Eric P Xing. 2010. A latent variable model for geographic lexical variation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pui Cheong Fung</author>
<author>Xu Yu</author>
<author>Wai Lam</author>
</authors>
<title>Stock prediction: Integrating text mining approach using real-time news.</title>
<date>2003</date>
<booktitle>In Proceedings of IEEE International Conference on Computational Intelligence for Financial Engineering.</booktitle>
<contexts>
<context position="5482" citStr="Fung et al. (2003)" startWordPosition="859" endWordPosition="862">VM baselines. • By varying the number of dimensions of the covariates and the size of the training data, we show that the improvements over the baselines are robust across different parameter settings on three datasets. In the next section, we outline related work in modeling financial reports and text regression. In Section 3, the details of the semiparametric copula model are introduced. We then describe the dataset and dependent variable in this study, and the experiments are shown in Section 6. We discuss the results and findings in Section 7 and then conclude in Section 8. 2 Related Work Fung et al. (2003) are among the first to study SVM and text mining methods in the market prediction domain, where they align financial news articles with multiple time series to simulate the 33 stocks in the Hong Kong Hang Seng Index. However, text regression in the financial domain have not been explored until recently. Kogan et al. (2009) model the SEC-mandated annual reports, and performs linear SVM regression with E-insensitive loss function to predict the measured volatility. Another recent study (Wang et al., 2013) uses exactly the same max-margin regression technique, but with a different focus on the f</context>
</contexts>
<marker>Fung, Yu, Lam, 2003</marker>
<rawString>Pui Cheong Fung, Xu Yu, and Wai Lam. 2003. Stock prediction: Integrating text mining approach using real-time news. In Proceedings of IEEE International Conference on Computational Intelligence for Financial Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Gelfand</author>
<author>Adrian Smith</author>
</authors>
<title>Samplingbased approaches to calculating marginal densities.</title>
<date>1990</date>
<journal>Journal of the American statistical association.</journal>
<contexts>
<context position="16931" citStr="Gelfand and Smith, 1990" startWordPosition="2708" endWordPosition="2711">eds to solve the mean response ˆE(Fy(y)|Fx1(x1), ..., Fx1(x1)) from a joint distribution of high-dimensional Gaussian copula. Assume in the simple bivariate case of Gaussian copula regression, the covariance matrix Σ is: Σ = r Σ11 Σ12 1 L Σ22 J We can easily derive the conditional density that can be used to calculate the expected value of the CDF of the label: exp − 2δT ~[Σ22 − ΣT —1E12]-1 − I~ δ where δ = Φ−1[Fy(y)] − ΣT 12Σ−1 11 Φ−1[Fx1(x1)]. Unfortunately, the exact inference can be intractable in the multivariate case, and approximate inference, such as Markov Chain Monte Carlo sampling (Gelfand and Smith, 1990; Pitt et al., 2006) is often used for posterior inference. In this work, we propose an efficient sampling method to derive y given the text features — we sample Fy(y) s.t. it maximizes the joint high-dimensional Gaussian copula density: ˆFy(y) Pz� arg max Fy(y)∈(0,1) d Eet exp ~−12ΔT (F,-, � 1 − I~ Δ~ variable y and its CDF ˆFy( Fˆ(ν) = 1 m Xm i=1 C(Fy(y)|Fx1(x1); Σ) = 1 2 |Σ22 − ΣT 12Σ−1 11 Σ12|1 1158 where Φ−1(Fx1(x1)) ... Φ−1(Fxn(xn)) Φ−1(Fy(y)) Again, the reason why we perform approximated inference is that: exact inference in the high-dimensional Gaussian copula density is nontrivial, an</context>
</contexts>
<marker>Gelfand, Smith, 1990</marker>
<rawString>Alan Gelfand and Adrian Smith. 1990. Samplingbased approaches to calculating marginal densities. Journal of the American statistical association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Genest</author>
<author>Anne-Catherine Favre</author>
</authors>
<title>Everything you always wanted to know about copula modeling but were afraid to ask.</title>
<date>2007</date>
<journal>Journal of Hydrologic Engineering.</journal>
<contexts>
<context position="3195" citStr="Genest and Favre, 2007" startWordPosition="490" endWordPosition="493">en though each earnings call has distinct styles, as well as different speakers and mixed formats, can we use earnings calls to predict the financial risks of the company in the limited future? Given a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk1. To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies. Copula models (Schweizer and Sklar, 1983; Nelsen, 1999) are often used by statisticians (Genest and Favre, 2007; Liu et al., 2012; Masarotto and Varin, 2012) and economists (Chen and Fan, 2006) to study the bivariate and multivariate stochastic dependency among random variables, but they are very new to the machine learning (Ghahramani et al., 2012; Han et al., 2012; Xiang and Neville, 2013; Lopezpaz et al., 2013) and related communities (Eickhoff et al., 2013). To the best of our knowledge, even though the term “copula” is named for the resemblance to grammatical copulas in linguistics, copula models have not been explored in the NLP community. To evaluate the performance of our approach, we compare w</context>
</contexts>
<marker>Genest, Favre, 2007</marker>
<rawString>Christian Genest and Anne-Catherine Favre. 2007. Everything you always wanted to know about copula modeling but were afraid to ask. Journal of Hydrologic Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Barnab´as P´oczos</author>
<author>Jeff Schneider</author>
</authors>
<title>Copula-based kernel dependency measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning.</booktitle>
<marker>Ghahramani, P´oczos, Schneider, 2012</marker>
<rawString>Zoubin Ghahramani, Barnab´as P´oczos, and Jeff Schneider. 2012. Copula-based kernel dependency measures. In Proceedings of the 29th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Han</author>
<author>Tuo Zhao</author>
<author>Han Liu</author>
</authors>
<title>Coda: High dimensional copula discriminant analysis.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="3452" citStr="Han et al., 2012" startWordPosition="532" endWordPosition="535">etric approach for automatic prediction of future financial risk1. To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies. Copula models (Schweizer and Sklar, 1983; Nelsen, 1999) are often used by statisticians (Genest and Favre, 2007; Liu et al., 2012; Masarotto and Varin, 2012) and economists (Chen and Fan, 2006) to study the bivariate and multivariate stochastic dependency among random variables, but they are very new to the machine learning (Ghahramani et al., 2012; Han et al., 2012; Xiang and Neville, 2013; Lopezpaz et al., 2013) and related communities (Eickhoff et al., 2013). To the best of our knowledge, even though the term “copula” is named for the resemblance to grammatical copulas in linguistics, copula models have not been explored in the NLP community. To evaluate the performance of our approach, we compare with a standard squared loss linear regression baseline, as well as strong baselines such as linear and non-linear support 1In this work, the risk is defined as the measured volatility of stock prices from the week following the earnings call teleconference.</context>
</contexts>
<marker>Han, Zhao, Liu, 2012</marker>
<rawString>Fang Han, Tuo Zhao, and Han Liu. 2012. Coda: High dimensional copula discriminant analysis. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
</authors>
<date>1995</date>
<booktitle>Overview of the second text retrieval conference (trec-2). Information Processing &amp; Management.</booktitle>
<contexts>
<context position="20542" citStr="Harman, 1995" startWordPosition="3285" endWordPosition="3286">_ MultiV ariateGaussianPDF(Z(te),ˆΣ) pj — � ; n GaussianPDF(Z(te)) if pj ≥ maxj then maxj = pj; Yˆ&amp;quot; = k; end if end for end for yˆ ← InverseCDF(-y(tr), Yˆ&amp;quot;); Dataset #Calls #Companies #Types #Tokens Pre-2009 3694 2746 371.5K 28.7M 2009 3474 2178 346.2K 26.4M Post-2009 3726 2107 377.4K 28.6M Table 1: Statistics of three datasets. Types: unique words. Tokens: word tokens. Note that unlike the standard news corpora in NLP or the SEC-mandated financial report, Transcripts of earnings call is a very special genre of text. For example, the length of WSJ documents is typically one to three hundreds (Harman, 1995), but the averaged document length of our three earnings calls datasets is 7677. Depending on the amount of interactions in the question answering session, the complexities of the calls vary. This mixed form of formal statement and informal speech brought difficulties to machine learning algorithms. 5 Measuring Financial Risks Volatility is an important measure of the financial risk, and in this work, we focus on predicting the future volatility following the earnings teleconfer⎛ ⎜ ⎜ ⎜ ⎝ A = ⎞ ⎠ ⎟ ⎟ ⎟ 1159 ence call. For each earning call, we have a week of stock prices of the company after th</context>
</contexts>
<marker>Harman, 1995</marker>
<rawString>Donna Harman. 1995. Overview of the second text retrieval conference (trec-2). Information Processing &amp; Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert V Hogg</author>
<author>Allen Craig</author>
</authors>
<date>1994</date>
<note>Introduction to mathematical statistics.</note>
<contexts>
<context position="23282" citStr="Hogg and Craig, 1994" startWordPosition="3745" endWordPosition="3748"> Xie et al., 2013; Wang et al., 2013; Tsai and Wang, 2013). We use the Statistical Toolbox’s linear regression implementation in Matlab, and LibSVM (Chang and Lin, 2011) for training and testing the SVM models. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold crossvalidation. Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the E in the Gaussian copula, there is no hyperparameters that need to be tuned. Evaluation Metrics: Spearman’s correlation (Hogg and Craig, 1994) and Kendall’s tau (Kendall, 1938) have been widely used in many regression problems in NLP (Albrecht and Hwa, 2007; Yogatama et al., 2011; Wang et al., 2013; Tsai and Wang, 2013), and here we use them to measure the quality of predicted values yˆ by comparing to the vector of ground truth y. In contrast to Pearson’s correlation, Spearman’s correlation has no assumptions on the relationship of the two measured variables. Kendall’s tau is a nonparametric statistical metric that have shown to be inexpensive, robust, and representation independent (Lapata, 2006). We also use paired two-tailed t-t</context>
</contexts>
<marker>Hogg, Craig, 1994</marker>
<rawString>Robert V Hogg and Allen Craig. 1994. Introduction to mathematical statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Joe</author>
</authors>
<title>Multivariate models and dependence concepts.</title>
<date>1997</date>
<contexts>
<context position="11814" citStr="Joe, 1997" startWordPosition="1856" endWordPosition="1857">r’s theorem and the corollary. Theorem 1 (Sklar’s Theorem (1959)) Let F be the joint cumulative distribution function of n random variables X1, X2,..., Xn. Let the corresponding marginal cumulative distribution functions of the random variable be F1(x1), F2(x2), ..., Fn(xn). Then, if the marginal functions are continuous, there exists a unique copula C, such that F(x1, ..., xn) = C[F1(x1), ..., Fn(xn)]. (1) Furthermore, if the distributions are continuous, the multivariate dependency structure and the marginals might be separated, and the copula can be considered independent of the marginals (Joe, 1997; Parsa and Klugman, 2011). Therefore, the copula does not have requirements on the marginal distributions, and any arbitrary marginals can be combined and their dependency structure can be modeled using the copula. The inverse of Sklar’s Theorem is also true in the following: Corollary 1 If there exists a copula C : (0, 1)n and marginal cumulative distribution functions F1(x1), F2(x2), ..., Fn(xn), then C[F1(x1), ..., Fn(xn)] defines a multivariate cumulative distribution function. 3.2 Semiparametric Gaussian Copula Models The Non-Parametric Estimation We formulate the copula regression model</context>
</contexts>
<marker>Joe, 1997</marker>
<rawString>Harry Joe. 1997. Multivariate models and dependence concepts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Dipanjan Das</author>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Movie reviews and revenues: An experiment in text regression.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7063" citStr="Joshi et al., 2010" startWordPosition="1115" endWordPosition="1118">have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a binary classification task. Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market (Bollen et al., 2011; Zhang et al., 2011). Despite our financial domain, our approach is more relevant to text regression. Traditional discriminative models, such as linear regression and linear SVM, have been very popular in various text regression tasks, such as predicting movie revenues from reviews (Joshi et al., 2010), understanding the geographic lexical variation (Eisenstein et al., 2010), and predicting food prices from menus (Chahuneau et al., 2012). The advantage of these models is that the estimation of the parameters is often simple, the results are easy to interpret, and the approach often yields strong performances. While these approaches have merits, they suffer from the problem of not explicitly modeling the correlations and interactions among random variables, which in some sense, corresponding to the impractical assumption of independent and identically distributed (i.i.d) of the data. For exa</context>
</contexts>
<marker>Joshi, Das, Gimpel, Smith, 2010</marker>
<rawString>Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and Noah A Smith. 2010. Movie reviews and revenues: An experiment in text regression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Kendall</author>
</authors>
<title>A new measure of rank correlation.</title>
<date>1938</date>
<journal>Biometrika.</journal>
<contexts>
<context position="23316" citStr="Kendall, 1938" startWordPosition="3752" endWordPosition="3753">i and Wang, 2013). We use the Statistical Toolbox’s linear regression implementation in Matlab, and LibSVM (Chang and Lin, 2011) for training and testing the SVM models. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold crossvalidation. Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the E in the Gaussian copula, there is no hyperparameters that need to be tuned. Evaluation Metrics: Spearman’s correlation (Hogg and Craig, 1994) and Kendall’s tau (Kendall, 1938) have been widely used in many regression problems in NLP (Albrecht and Hwa, 2007; Yogatama et al., 2011; Wang et al., 2013; Tsai and Wang, 2013), and here we use them to measure the quality of predicted values yˆ by comparing to the vector of ground truth y. In contrast to Pearson’s correlation, Spearman’s correlation has no assumptions on the relationship of the two measured variables. Kendall’s tau is a nonparametric statistical metric that have shown to be inexpensive, robust, and representation independent (Lapata, 2006). We also use paired two-tailed t-test to measure the statistical sig</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Maurice Kendall. 1938. A new measure of rank correlation. Biometrika.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimon Kogan</author>
<author>Dimitry Levin</author>
<author>Bryan Routledge</author>
<author>Jacob Sagi</author>
<author>Noah Smith</author>
</authors>
<title>Predicting risk from financial reports with regression.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1600" citStr="Kogan et al., 2009" startWordPosition="246" endWordPosition="249">ansform, our approach moves beyond the standard count-based bag-ofwords models in NLP, and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula. In experiments, we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings. 1 Introduction Predicting the risks of publicly listed companies is of great interests not only to the traders and analysts on the Wall Street, but also virtually anyone who has investments in the market (Kogan et al., 2009). Traditionally, analysts focus on quantitative modeling of historical trading data. Today, even though earnings calls transcripts are abundantly available, their distinctive communicative practices (Camiciottoli, 2010), and correlations with the financial risks, in particular, future stock Zhenhao Hua School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 zhua@cs.cmu.edu performances (Price et al., 2012), are not well studied in the past. Earnings calls are conference calls where a listed company discusses the financial performance. Typically, a earnings call contains two </context>
<context position="5807" citStr="Kogan et al. (2009)" startWordPosition="915" endWordPosition="918">on 3, the details of the semiparametric copula model are introduced. We then describe the dataset and dependent variable in this study, and the experiments are shown in Section 6. We discuss the results and findings in Section 7 and then conclude in Section 8. 2 Related Work Fung et al. (2003) are among the first to study SVM and text mining methods in the market prediction domain, where they align financial news articles with multiple time series to simulate the 33 stocks in the Hong Kong Hang Seng Index. However, text regression in the financial domain have not been explored until recently. Kogan et al. (2009) model the SEC-mandated annual reports, and performs linear SVM regression with E-insensitive loss function to predict the measured volatility. Another recent study (Wang et al., 2013) uses exactly the same max-margin regression technique, but with a different focus on the financial sentiment. Using the same dataset, Tsai and Wang (2013) reformulate the regression problem as a text ranking problem. Note that all these regression studies above investigate the SEC-mandated annual reports, which are very different from the earnings calls in many aspects such as length, format, vocabulary, and gen</context>
<context position="22636" citStr="Kogan et al., 2009" startWordPosition="3634" endWordPosition="3637">, 2003) to extract the lexicalized named entity and part-of-speech features. A probabilistic frame-semantics parser, SEMAFOR (Das et al., 2010), is used to provide the FrameNet-style frame-level semantic annotations. For each of the five sets, we collect the top100 most frequent features, and end up with a total of 500 features. Baselines: The baselines are standard squared-loss linear regression, linear kernel SVM, and non-linear (Gaussian) kernel SVM. They are all standard algorithms in regression problems, and have been shown to have outstanding performances in many recent text regression (Kogan et al., 2009; Chahuneau et al., 2012; Xie et al., 2013; Wang et al., 2013; Tsai and Wang, 2013). We use the Statistical Toolbox’s linear regression implementation in Matlab, and LibSVM (Chang and Lin, 2011) for training and testing the SVM models. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold crossvalidation. Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the E in the Gaussian copula, there is no hyperparameters that need to be tuned. Evaluation Metrics:</context>
</contexts>
<marker>Kogan, Levin, Routledge, Sagi, Smith, 2009</marker>
<rawString>Shimon Kogan, Dimitry Levin, Bryan Routledge, Jacob Sagi, and Noah Smith. 2009. Predicting risk from financial reports with regression. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>David Blei</author>
</authors>
<title>Correlated topic models. In Advances in neural information processing systems.</title>
<date>2005</date>
<contexts>
<context position="8166" citStr="Lafferty and Blei, 2005" startWordPosition="1292" endWordPosition="1295">nse, corresponding to the impractical assumption of independent and identically distributed (i.i.d) of the data. For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained. 3 Copula Models for Text Regression In NLP, many statistical machine learning methods that capture the dependencies among random variables, including topic models (Blei et al., 2003; Lafferty and Blei, 2005; Wang et al., 2012), always have to make assumptions with the underlying distributions of the random variables, and make use of informative priors. This might be rather restricting the expressiveness of the model in some sense (Reisinger et al., 2010). On the other hand, once such assumptions are removed, another problem arises — they might be prone to errors, and suffer from the overfitting issue. Therefore, coping with the tradeoff between expressiveness and overfitting, seems to be rather important in statistical approaches that capture stochastic dependency. Our proposed semiparametric co</context>
</contexts>
<marker>Lafferty, Blei, 2005</marker>
<rawString>John Lafferty and David Blei. 2005. Correlated topic models. In Advances in neural information processing systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics.</title>
<date>2006</date>
<contexts>
<context position="23847" citStr="Lapata, 2006" startWordPosition="3840" endWordPosition="3841">s: Spearman’s correlation (Hogg and Craig, 1994) and Kendall’s tau (Kendall, 1938) have been widely used in many regression problems in NLP (Albrecht and Hwa, 2007; Yogatama et al., 2011; Wang et al., 2013; Tsai and Wang, 2013), and here we use them to measure the quality of predicted values yˆ by comparing to the vector of ground truth y. In contrast to Pearson’s correlation, Spearman’s correlation has no assumptions on the relationship of the two measured variables. Kendall’s tau is a nonparametric statistical metric that have shown to be inexpensive, robust, and representation independent (Lapata, 2006). We also use paired two-tailed t-test to measure the statistical significance between the best and the second best approaches. 6.2 Comparing to Various Baselines In the first experiment, we compare the proposed semiparametric Gaussian copula regression model to three baselines on three datasets with all features. The detailed results are shown in Table 2. On the pre-2009 dataset, we see that the linear regression and linear SVM perform reasonably well, but the Gaussian kernel SVM performs less well, probably due to overfitting. The copula model outperformed all three baselines by a wide margi</context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>Mirella Lapata. 2006. Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han Liu</author>
<author>Fang Han</author>
<author>Ming Yuan</author>
<author>John Lafferty</author>
<author>Larry Wasserman</author>
</authors>
<title>High-dimensional semiparametric gaussian copula graphical models. The Annals of Statistics.</title>
<date>2012</date>
<contexts>
<context position="3213" citStr="Liu et al., 2012" startWordPosition="494" endWordPosition="497">call has distinct styles, as well as different speakers and mixed formats, can we use earnings calls to predict the financial risks of the company in the limited future? Given a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk1. To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies. Copula models (Schweizer and Sklar, 1983; Nelsen, 1999) are often used by statisticians (Genest and Favre, 2007; Liu et al., 2012; Masarotto and Varin, 2012) and economists (Chen and Fan, 2006) to study the bivariate and multivariate stochastic dependency among random variables, but they are very new to the machine learning (Ghahramani et al., 2012; Han et al., 2012; Xiang and Neville, 2013; Lopezpaz et al., 2013) and related communities (Eickhoff et al., 2013). To the best of our knowledge, even though the term “copula” is named for the resemblance to grammatical copulas in linguistics, copula models have not been explored in the NLP community. To evaluate the performance of our approach, we compare with a standard squ</context>
<context position="15799" citStr="Liu et al., 2012" startWordPosition="2521" endWordPosition="2524">Σ matrix, we make use of the power of randomness: using the initial Σ from MLE, we generate random samples from the Gaussian copula, and then concatenate previously generated joint of Gaussian inverse marginal CDFs with the newly generated random copula numbers, and re-estimate using MLE to derive the final adjusted Σ. Note that the final Σ matrix has to be symmetric and positive definite. Computational Complexity One important question regarding the proposed semiparametric Gaussian copula model is the corresponding computational complexity. This boils down to the estimation of the Σˆ matrix (Liu et al., 2012): one only needs to calculate the correlation coefficients of n(n − 1)/2 pairs of random variables. Christensen (2005) shows that sorting and balanced binary trees can be used to calculate the correlation coefficients with complexity of O(n log n). Therefore, the computational complexity of MLE for the proposed model is O(n log n). Efficient Approximate Inference In this regression task, in order to perform exact inference of the conditional probability distribution p(Fy(y)|Fx1(x1), ..., Fxn(xn)), one needs to solve the mean response ˆE(Fy(y)|Fx1(x1), ..., Fx1(x1)) from a joint distribution of</context>
</contexts>
<marker>Liu, Han, Yuan, Lafferty, Wasserman, 2012</marker>
<rawString>Han Liu, Fang Han, Ming Yuan, John Lafferty, and Larry Wasserman. 2012. High-dimensional semiparametric gaussian copula graphical models. The Annals of Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lopez-paz</author>
<author>Jose M Hern´andez-lobato</author>
<author>Ghahramani Zoubin</author>
</authors>
<title>Gaussian process vine copulas for multivariate dependence.</title>
<date>2013</date>
<booktitle>In Proceedings of the 30th International Conference on Machine Learning.</booktitle>
<marker>Lopez-paz, Hern´andez-lobato, Zoubin, 2013</marker>
<rawString>David Lopez-paz, Jose M Hern´andez-lobato, and Ghahramani Zoubin. 2013. Gaussian process vine copulas for multivariate dependence. In Proceedings of the 30th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Mamudi</author>
</authors>
<title>Lehman folds with record $613 billion debt.</title>
<date>2008</date>
<journal>MarketWatch.com.</journal>
<contexts>
<context position="29589" citStr="Mamudi, 2008" startWordPosition="4758" endWordPosition="4759"> with the future stock volatility in the three datasets. On the top features from the “pre-2009” dataset, which primarily (82%) includes calls from 2008, we can clearly observe that the word “2008” has strong correlation with the financial risks. Interestingly, the phrase “third quarter” and its variations, not only play an important role in the model, but also highly correlated to the timeline of the financial crisis: the Q3 of 2008 is a critical period in the recession, where Lehman Brothers falls on the Sept. 15 of 2008, filing $613 billion of debt — the biggest bankruptcy in U.S. history (Mamudi, 2008). This huge panic soon broke out in various financial institutions in the Wall Street. On the top features from “2009” dataset, again, we see the word “2008” is still prominent in predicting financial risks, indicating the hardship and extended impacts from the center of the economic crisis. After examining the transcripts, we found sentences like: “...our specialty lighting business that we discontinued in the fourth quarter of 2008...”, “...the exception of fourth quarter revenue which was $100,000 below our guidance target...”, and “...to address changing economic conditions and their impac</context>
</contexts>
<marker>Mamudi, 2008</marker>
<rawString>Sam Mamudi. 2008. Lehman folds with record $613 billion debt. MarketWatch.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Masarotto</author>
<author>Cristiano Varin</author>
</authors>
<title>Gaussian copula marginal regression.</title>
<date>2012</date>
<journal>Electronic Journal of Statistics.</journal>
<contexts>
<context position="3241" citStr="Masarotto and Varin, 2012" startWordPosition="498" endWordPosition="501">styles, as well as different speakers and mixed formats, can we use earnings calls to predict the financial risks of the company in the limited future? Given a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk1. To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies. Copula models (Schweizer and Sklar, 1983; Nelsen, 1999) are often used by statisticians (Genest and Favre, 2007; Liu et al., 2012; Masarotto and Varin, 2012) and economists (Chen and Fan, 2006) to study the bivariate and multivariate stochastic dependency among random variables, but they are very new to the machine learning (Ghahramani et al., 2012; Han et al., 2012; Xiang and Neville, 2013; Lopezpaz et al., 2013) and related communities (Eickhoff et al., 2013). To the best of our knowledge, even though the term “copula” is named for the resemblance to grammatical copulas in linguistics, copula models have not been explored in the NLP community. To evaluate the performance of our approach, we compare with a standard squared loss linear regression </context>
</contexts>
<marker>Masarotto, Varin, 2012</marker>
<rawString>Guido Masarotto and Cristiano Varin. 2012. Gaussian copula marginal regression. Electronic Journal of Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger B Nelsen</author>
</authors>
<title>An introduction to copulas.</title>
<date>1999</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="3139" citStr="Nelsen, 1999" startWordPosition="483" endWordPosition="484"> for inquiries. The question we ask is that, even though each earnings call has distinct styles, as well as different speakers and mixed formats, can we use earnings calls to predict the financial risks of the company in the limited future? Given a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk1. To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies. Copula models (Schweizer and Sklar, 1983; Nelsen, 1999) are often used by statisticians (Genest and Favre, 2007; Liu et al., 2012; Masarotto and Varin, 2012) and economists (Chen and Fan, 2006) to study the bivariate and multivariate stochastic dependency among random variables, but they are very new to the machine learning (Ghahramani et al., 2012; Han et al., 2012; Xiang and Neville, 2013; Lopezpaz et al., 2013) and related communities (Eickhoff et al., 2013). To the best of our knowledge, even though the term “copula” is named for the resemblance to grammatical copulas in linguistics, copula models have not been explored in the NLP community. T</context>
<context position="8860" citStr="Nelsen, 1999" startWordPosition="1401" endWordPosition="1402">utions of the random variables, and make use of informative priors. This might be rather restricting the expressiveness of the model in some sense (Reisinger et al., 2010). On the other hand, once such assumptions are removed, another problem arises — they might be prone to errors, and suffer from the overfitting issue. Therefore, coping with the tradeoff between expressiveness and overfitting, seems to be rather important in statistical approaches that capture stochastic dependency. Our proposed semiparametric copula regression model takes a different perspective. On one hand, copula models (Nelsen, 1999) seek to explicitly model the dependency of random variables by separating the marginals and their correlations. On the other hand, it does not make use of any as1156 sumptions on the distributions of the random variables, yet, the copula model is still expressive. This nice property essentially allows us to fuse distinctive lexical, syntactic, and semantic feature sets naturally into a single compact model. From an information-theoretic point of view (Shannon, 1948), various problems in text analytics can be formulated as estimating the probability mass/density functions of tokens in text. In</context>
</contexts>
<marker>Nelsen, 1999</marker>
<rawString>Roger B Nelsen. 1999. An introduction to copulas. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul A Parsa</author>
<author>Stuart A Klugman</author>
</authors>
<date>2011</date>
<booktitle>Copula regression. Variance Advancing and Science of Risk.</booktitle>
<contexts>
<context position="11840" citStr="Parsa and Klugman, 2011" startWordPosition="1858" endWordPosition="1861"> and the corollary. Theorem 1 (Sklar’s Theorem (1959)) Let F be the joint cumulative distribution function of n random variables X1, X2,..., Xn. Let the corresponding marginal cumulative distribution functions of the random variable be F1(x1), F2(x2), ..., Fn(xn). Then, if the marginal functions are continuous, there exists a unique copula C, such that F(x1, ..., xn) = C[F1(x1), ..., Fn(xn)]. (1) Furthermore, if the distributions are continuous, the multivariate dependency structure and the marginals might be separated, and the copula can be considered independent of the marginals (Joe, 1997; Parsa and Klugman, 2011). Therefore, the copula does not have requirements on the marginal distributions, and any arbitrary marginals can be combined and their dependency structure can be modeled using the copula. The inverse of Sklar’s Theorem is also true in the following: Corollary 1 If there exists a copula C : (0, 1)n and marginal cumulative distribution functions F1(x1), F2(x2), ..., Fn(xn), then C[F1(x1), ..., Fn(xn)] defines a multivariate cumulative distribution function. 3.2 Semiparametric Gaussian Copula Models The Non-Parametric Estimation We formulate the copula regression model as follows. Assume we hav</context>
</contexts>
<marker>Parsa, Klugman, 2011</marker>
<rawString>Rahul A Parsa and Stuart A Klugman. 2011. Copula regression. Variance Advancing and Science of Risk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel Parzen</author>
</authors>
<title>On estimation of a probability density function and mode. The annals of mathematical statistics.</title>
<date>1962</date>
<contexts>
<context position="13208" citStr="Parzen, 1962" startWordPosition="2095" endWordPosition="2096">mation to smooth out the distribution of each variable. Let f1, f2,..., fn be the unknown density, we are interested in deriving the shape of these functions. Assume we have m samples, the kernel density estimator can be defined as: Kh(x − xi) (2) x − xi � K (3) h Here, K(·) is the kernel function, where in our case, we use the Box kernel2 K(z): 1 K(z) = 2, |z |G 1, (4) = 0,|z |&gt; 1. (5) Comparing to the Gaussian kernel and other kernels, the Box kernel is simple, and computationally inexpensive. The parameter h is the bandwidth for smoothing3. 2It is also known as the original Parzen windows (Parzen, 1962). 3In our implementation, we use the default h of the Box kernel in the ksdensity function in Matlab. ˆfh(x) = 1 m �m i=1 1 �m i=1 mh 1157 Now, we can derive the empirical cumulative distribution functions ˆFX1(ˆf1(X1)), ˆFX2(ˆf2(X2)),...,ˆFXn(ˆfn(Xn)) of the smoothed covariates, as well as the dependent ˆf(y)). The empirical cumulative distribution functions are defined as: I{xi G ν} (6) where I{·} is the indicator function, and ν indicates the current value that we are evaluating. Note that the above step is also known as probability integral transform (Diebold et al., 1997), which allows us</context>
</contexts>
<marker>Parzen, 1962</marker>
<rawString>Emanuel Parzen. 1962. On estimation of a probability density function and mode. The annals of mathematical statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pitt</author>
<author>David Chan</author>
<author>Robert Kohn</author>
</authors>
<title>Efficient bayesian inference for gaussian copula regression models.</title>
<date>2006</date>
<journal>Biometrika.</journal>
<contexts>
<context position="16951" citStr="Pitt et al., 2006" startWordPosition="2712" endWordPosition="2715">ponse ˆE(Fy(y)|Fx1(x1), ..., Fx1(x1)) from a joint distribution of high-dimensional Gaussian copula. Assume in the simple bivariate case of Gaussian copula regression, the covariance matrix Σ is: Σ = r Σ11 Σ12 1 L Σ22 J We can easily derive the conditional density that can be used to calculate the expected value of the CDF of the label: exp − 2δT ~[Σ22 − ΣT —1E12]-1 − I~ δ where δ = Φ−1[Fy(y)] − ΣT 12Σ−1 11 Φ−1[Fx1(x1)]. Unfortunately, the exact inference can be intractable in the multivariate case, and approximate inference, such as Markov Chain Monte Carlo sampling (Gelfand and Smith, 1990; Pitt et al., 2006) is often used for posterior inference. In this work, we propose an efficient sampling method to derive y given the text features — we sample Fy(y) s.t. it maximizes the joint high-dimensional Gaussian copula density: ˆFy(y) Pz� arg max Fy(y)∈(0,1) d Eet exp ~−12ΔT (F,-, � 1 − I~ Δ~ variable y and its CDF ˆFy( Fˆ(ν) = 1 m Xm i=1 C(Fy(y)|Fx1(x1); Σ) = 1 2 |Σ22 − ΣT 12Σ−1 11 Σ12|1 1158 where Φ−1(Fx1(x1)) ... Φ−1(Fxn(xn)) Φ−1(Fy(y)) Again, the reason why we perform approximated inference is that: exact inference in the high-dimensional Gaussian copula density is nontrivial, and might not have ana</context>
</contexts>
<marker>Pitt, Chan, Kohn, 2006</marker>
<rawString>Michael Pitt, David Chan, and Robert Kohn. 2006. Efficient bayesian inference for gaussian copula regression models. Biometrika.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McKay Price</author>
<author>James Doran</author>
<author>David Peterson</author>
<author>Barbara Bliss</author>
</authors>
<title>Earnings conference calls and stock returns: The incremental informativeness of textual tone.</title>
<date>2012</date>
<journal>Journal of Banking &amp; Finance.</journal>
<contexts>
<context position="2028" citStr="Price et al., 2012" startWordPosition="301" endWordPosition="304">ks of publicly listed companies is of great interests not only to the traders and analysts on the Wall Street, but also virtually anyone who has investments in the market (Kogan et al., 2009). Traditionally, analysts focus on quantitative modeling of historical trading data. Today, even though earnings calls transcripts are abundantly available, their distinctive communicative practices (Camiciottoli, 2010), and correlations with the financial risks, in particular, future stock Zhenhao Hua School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 zhua@cs.cmu.edu performances (Price et al., 2012), are not well studied in the past. Earnings calls are conference calls where a listed company discusses the financial performance. Typically, a earnings call contains two parts: the senior executives first report the operational outcomes, as well as the current financial performance, and then discuss their perspectives on the future of the company. The second part of the teleconference includes a question answering session where the floor will be open to investors, analysts, and other parties for inquiries. The question we ask is that, even though each earnings call has distinct styles, as we</context>
</contexts>
<marker>Price, Doran, Peterson, Bliss, 2012</marker>
<rawString>McKay Price, James Doran, David Peterson, and Barbara Bliss. 2012. Earnings conference calls and stock returns: The incremental informativeness of textual tone. Journal of Banking &amp; Finance.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Austin Waters</author>
<author>Bryan Silverthorn</author>
<author>Raymond J Mooney</author>
</authors>
<title>Spherical topic models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 27th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="8418" citStr="Reisinger et al., 2010" startWordPosition="1333" endWordPosition="1336">dencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained. 3 Copula Models for Text Regression In NLP, many statistical machine learning methods that capture the dependencies among random variables, including topic models (Blei et al., 2003; Lafferty and Blei, 2005; Wang et al., 2012), always have to make assumptions with the underlying distributions of the random variables, and make use of informative priors. This might be rather restricting the expressiveness of the model in some sense (Reisinger et al., 2010). On the other hand, once such assumptions are removed, another problem arises — they might be prone to errors, and suffer from the overfitting issue. Therefore, coping with the tradeoff between expressiveness and overfitting, seems to be rather important in statistical approaches that capture stochastic dependency. Our proposed semiparametric copula regression model takes a different perspective. On one hand, copula models (Nelsen, 1999) seek to explicitly model the dependency of random variables by separating the marginals and their correlations. On the other hand, it does not make use of an</context>
</contexts>
<marker>Reisinger, Waters, Silverthorn, Mooney, 2010</marker>
<rawString>Joseph Reisinger, Austin Waters, Bryan Silverthorn, and Raymond J Mooney. 2010. Spherical topic models. In Proceedings of the 27th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berthold Schweizer</author>
<author>Abe Sklar</author>
</authors>
<title>Probabilistic metric spaces.</title>
<date>1983</date>
<contexts>
<context position="3124" citStr="Schweizer and Sklar, 1983" startWordPosition="479" endWordPosition="482">analysts, and other parties for inquiries. The question we ask is that, even though each earnings call has distinct styles, as well as different speakers and mixed formats, can we use earnings calls to predict the financial risks of the company in the limited future? Given a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk1. To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies. Copula models (Schweizer and Sklar, 1983; Nelsen, 1999) are often used by statisticians (Genest and Favre, 2007; Liu et al., 2012; Masarotto and Varin, 2012) and economists (Chen and Fan, 2006) to study the bivariate and multivariate stochastic dependency among random variables, but they are very new to the machine learning (Ghahramani et al., 2012; Han et al., 2012; Xiang and Neville, 2013; Lopezpaz et al., 2013) and related communities (Eickhoff et al., 2013). To the best of our knowledge, even though the term “copula” is named for the resemblance to grammatical copulas in linguistics, copula models have not been explored in the N</context>
</contexts>
<marker>Schweizer, Sklar, 1983</marker>
<rawString>Berthold Schweizer and Abe Sklar. 1983. Probabilistic metric spaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<booktitle>In The Bell System Technical Journal.</booktitle>
<contexts>
<context position="9331" citStr="Shannon, 1948" startWordPosition="1476" endWordPosition="1477">hastic dependency. Our proposed semiparametric copula regression model takes a different perspective. On one hand, copula models (Nelsen, 1999) seek to explicitly model the dependency of random variables by separating the marginals and their correlations. On the other hand, it does not make use of any as1156 sumptions on the distributions of the random variables, yet, the copula model is still expressive. This nice property essentially allows us to fuse distinctive lexical, syntactic, and semantic feature sets naturally into a single compact model. From an information-theoretic point of view (Shannon, 1948), various problems in text analytics can be formulated as estimating the probability mass/density functions of tokens in text. In NLP, many of the probabilistic text models work in the discrete space (Church and Gale, 1995; Blei et al., 2003), but our model is different: since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables. By doing this, we are essentially performing probability integral transform— an important statistical technique that mov</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude Shannon. 1948. A mathematical theory of communication. In The Bell System Technical Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abe Sklar</author>
</authors>
<title>Fonctions de r´epartition a` n dimensions et leurs marges. Universit´e Paris 8.</title>
<date>1959</date>
<marker>Sklar, 1959</marker>
<rawString>Abe Sklar. 1959. Fonctions de r´epartition a` n dimensions et leurs marges. Universit´e Paris 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology.</booktitle>
<contexts>
<context position="22025" citStr="Toutanova et al., 2003" startWordPosition="3542" endWordPosition="3545">tions above to calculate the measured stock volatility after the earnings call, which is the standard measure of risks in finance, and the dependent variable y of our predictive task. 6 Experiments 6.1 Experimental Setup In all experiments throughout this section, we use 80-20 train/test splits on all three datasets. Feature sets: We have extracted lexical, named entity, syntactic, and frame-semantics features, most of which have been shown to perform well in previous work (Xie et al., 2013). We use the unigrams and bigrams to represent lexical features, and the Stanford partof-speech tagger (Toutanova et al., 2003) to extract the lexicalized named entity and part-of-speech features. A probabilistic frame-semantics parser, SEMAFOR (Das et al., 2010), is used to provide the FrameNet-style frame-level semantic annotations. For each of the five sets, we collect the top100 most frequent features, and end up with a total of 500 features. Baselines: The baselines are standard squared-loss linear regression, linear kernel SVM, and non-linear (Gaussian) kernel SVM. They are all standard algorithms in regression problems, and have been shown to have outstanding performances in many recent text regression (Kogan e</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Feng Tsai</author>
<author>Chuan-Ju Wang</author>
</authors>
<title>Risk ranking from financial reports.</title>
<date>2013</date>
<booktitle>In Advances in Information Retrieval.</booktitle>
<contexts>
<context position="6146" citStr="Tsai and Wang (2013)" startWordPosition="968" endWordPosition="971">ining methods in the market prediction domain, where they align financial news articles with multiple time series to simulate the 33 stocks in the Hong Kong Hang Seng Index. However, text regression in the financial domain have not been explored until recently. Kogan et al. (2009) model the SEC-mandated annual reports, and performs linear SVM regression with E-insensitive loss function to predict the measured volatility. Another recent study (Wang et al., 2013) uses exactly the same max-margin regression technique, but with a different focus on the financial sentiment. Using the same dataset, Tsai and Wang (2013) reformulate the regression problem as a text ranking problem. Note that all these regression studies above investigate the SEC-mandated annual reports, which are very different from the earnings calls in many aspects such as length, format, vocabulary, and genre. Most recently, Xie et al. (2013) have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a binary classification task. Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market (Bollen</context>
<context position="22719" citStr="Tsai and Wang, 2013" startWordPosition="3650" endWordPosition="3653">obabilistic frame-semantics parser, SEMAFOR (Das et al., 2010), is used to provide the FrameNet-style frame-level semantic annotations. For each of the five sets, we collect the top100 most frequent features, and end up with a total of 500 features. Baselines: The baselines are standard squared-loss linear regression, linear kernel SVM, and non-linear (Gaussian) kernel SVM. They are all standard algorithms in regression problems, and have been shown to have outstanding performances in many recent text regression (Kogan et al., 2009; Chahuneau et al., 2012; Xie et al., 2013; Wang et al., 2013; Tsai and Wang, 2013). We use the Statistical Toolbox’s linear regression implementation in Matlab, and LibSVM (Chang and Lin, 2011) for training and testing the SVM models. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold crossvalidation. Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the E in the Gaussian copula, there is no hyperparameters that need to be tuned. Evaluation Metrics: Spearman’s correlation (Hogg and Craig, 1994) and Kendall’s tau (Kendall, 1938) ha</context>
</contexts>
<marker>Tsai, Wang, 2013</marker>
<rawString>Ming-Feng Tsai and Chuan-Ju Wang. 2013. Risk ranking from financial reports. In Advances in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Elijah Mayfield</author>
<author>Suresh Naidu</author>
<author>Jeremiah Dittmar</author>
</authors>
<title>Historical analysis of legal opinions with a sparse mixed-effects latent variable model.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8186" citStr="Wang et al., 2012" startWordPosition="1296" endWordPosition="1299"> impractical assumption of independent and identically distributed (i.i.d) of the data. For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained. 3 Copula Models for Text Regression In NLP, many statistical machine learning methods that capture the dependencies among random variables, including topic models (Blei et al., 2003; Lafferty and Blei, 2005; Wang et al., 2012), always have to make assumptions with the underlying distributions of the random variables, and make use of informative priors. This might be rather restricting the expressiveness of the model in some sense (Reisinger et al., 2010). On the other hand, once such assumptions are removed, another problem arises — they might be prone to errors, and suffer from the overfitting issue. Therefore, coping with the tradeoff between expressiveness and overfitting, seems to be rather important in statistical approaches that capture stochastic dependency. Our proposed semiparametric copula regression mode</context>
</contexts>
<marker>Wang, Mayfield, Naidu, Dittmar, 2012</marker>
<rawString>William Yang Wang, Elijah Mayfield, Suresh Naidu, and Jeremiah Dittmar. 2012. Historical analysis of legal opinions with a sparse mixed-effects latent variable model. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuan-Ju Wang</author>
<author>Ming-Feng Tsai</author>
<author>Tse Liu</author>
<author>ChinTing Chang</author>
</authors>
<title>Financial sentiment analysis for risk prediction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="5991" citStr="Wang et al., 2013" startWordPosition="943" endWordPosition="946">scuss the results and findings in Section 7 and then conclude in Section 8. 2 Related Work Fung et al. (2003) are among the first to study SVM and text mining methods in the market prediction domain, where they align financial news articles with multiple time series to simulate the 33 stocks in the Hong Kong Hang Seng Index. However, text regression in the financial domain have not been explored until recently. Kogan et al. (2009) model the SEC-mandated annual reports, and performs linear SVM regression with E-insensitive loss function to predict the measured volatility. Another recent study (Wang et al., 2013) uses exactly the same max-margin regression technique, but with a different focus on the financial sentiment. Using the same dataset, Tsai and Wang (2013) reformulate the regression problem as a text ranking problem. Note that all these regression studies above investigate the SEC-mandated annual reports, which are very different from the earnings calls in many aspects such as length, format, vocabulary, and genre. Most recently, Xie et al. (2013) have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a bina</context>
<context position="22697" citStr="Wang et al., 2013" startWordPosition="3646" endWordPosition="3649">eech features. A probabilistic frame-semantics parser, SEMAFOR (Das et al., 2010), is used to provide the FrameNet-style frame-level semantic annotations. For each of the five sets, we collect the top100 most frequent features, and end up with a total of 500 features. Baselines: The baselines are standard squared-loss linear regression, linear kernel SVM, and non-linear (Gaussian) kernel SVM. They are all standard algorithms in regression problems, and have been shown to have outstanding performances in many recent text regression (Kogan et al., 2009; Chahuneau et al., 2012; Xie et al., 2013; Wang et al., 2013; Tsai and Wang, 2013). We use the Statistical Toolbox’s linear regression implementation in Matlab, and LibSVM (Chang and Lin, 2011) for training and testing the SVM models. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold crossvalidation. Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the E in the Gaussian copula, there is no hyperparameters that need to be tuned. Evaluation Metrics: Spearman’s correlation (Hogg and Craig, 1994) and Kendall’s </context>
</contexts>
<marker>Wang, Tsai, Liu, Chang, 2013</marker>
<rawString>Chuan-Ju Wang, Ming-Feng Tsai, Tse Liu, and ChinTing Chang. 2013. Financial sentiment analysis for risk prediction. In Proceedings of the Sixth International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rongjing Xiang</author>
<author>Jennifer Neville</author>
</authors>
<title>Collective inference for network data with copula latent markov networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the sixth ACM international conference on Web search and data mining.</booktitle>
<contexts>
<context position="3477" citStr="Xiang and Neville, 2013" startWordPosition="536" endWordPosition="539"> automatic prediction of future financial risk1. To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies. Copula models (Schweizer and Sklar, 1983; Nelsen, 1999) are often used by statisticians (Genest and Favre, 2007; Liu et al., 2012; Masarotto and Varin, 2012) and economists (Chen and Fan, 2006) to study the bivariate and multivariate stochastic dependency among random variables, but they are very new to the machine learning (Ghahramani et al., 2012; Han et al., 2012; Xiang and Neville, 2013; Lopezpaz et al., 2013) and related communities (Eickhoff et al., 2013). To the best of our knowledge, even though the term “copula” is named for the resemblance to grammatical copulas in linguistics, copula models have not been explored in the NLP community. To evaluate the performance of our approach, we compare with a standard squared loss linear regression baseline, as well as strong baselines such as linear and non-linear support 1In this work, the risk is defined as the measured volatility of stock prices from the week following the earnings call teleconference. See details in Section 5</context>
</contexts>
<marker>Xiang, Neville, 2013</marker>
<rawString>Rongjing Xiang and Jennifer Neville. 2013. Collective inference for network data with copula latent markov networks. In Proceedings of the sixth ACM international conference on Web search and data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boyi Xie</author>
<author>Rebecca J Passonneau</author>
<author>Leon Wu</author>
<author>Germ´an G Creamer</author>
</authors>
<title>Semantic frames to predict stock price movement.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6443" citStr="Xie et al. (2013)" startWordPosition="1014" endWordPosition="1017">ated annual reports, and performs linear SVM regression with E-insensitive loss function to predict the measured volatility. Another recent study (Wang et al., 2013) uses exactly the same max-margin regression technique, but with a different focus on the financial sentiment. Using the same dataset, Tsai and Wang (2013) reformulate the regression problem as a text ranking problem. Note that all these regression studies above investigate the SEC-mandated annual reports, which are very different from the earnings calls in many aspects such as length, format, vocabulary, and genre. Most recently, Xie et al. (2013) have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a binary classification task. Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market (Bollen et al., 2011; Zhang et al., 2011). Despite our financial domain, our approach is more relevant to text regression. Traditional discriminative models, such as linear regression and linear SVM, have been very popular in various text regression tasks, such as predicting movie revenues from reviews </context>
<context position="21898" citStr="Xie et al., 2013" startWordPosition="3522" endWordPosition="3525">d Stock Volatility from Day t to t + T: ��τ i=0(rt+i − ¯r)2 y(t,t+τ) = (14) T Using the stock prices, we can use the equations above to calculate the measured stock volatility after the earnings call, which is the standard measure of risks in finance, and the dependent variable y of our predictive task. 6 Experiments 6.1 Experimental Setup In all experiments throughout this section, we use 80-20 train/test splits on all three datasets. Feature sets: We have extracted lexical, named entity, syntactic, and frame-semantics features, most of which have been shown to perform well in previous work (Xie et al., 2013). We use the unigrams and bigrams to represent lexical features, and the Stanford partof-speech tagger (Toutanova et al., 2003) to extract the lexicalized named entity and part-of-speech features. A probabilistic frame-semantics parser, SEMAFOR (Das et al., 2010), is used to provide the FrameNet-style frame-level semantic annotations. For each of the five sets, we collect the top100 most frequent features, and end up with a total of 500 features. Baselines: The baselines are standard squared-loss linear regression, linear kernel SVM, and non-linear (Gaussian) kernel SVM. They are all standard </context>
</contexts>
<marker>Xie, Passonneau, Wu, Creamer, 2013</marker>
<rawString>Boyi Xie, Rebecca J. Passonneau, Leon Wu, and Germ´an G. Creamer. 2013. Semantic frames to predict stock price movement. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Yogatama</author>
<author>Michael Heilman</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting a scientific community’s response to an article.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Yogatama, Heilman, O’Connor, Dyer, Routledge, Smith, 2011</marker>
<rawString>Dani Yogatama, Michael Heilman, Brendan O’Connor, Chris Dyer, Bryan R Routledge, and Noah A Smith. 2011. Predicting a scientific community’s response to an article. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xue Zhang</author>
<author>Hauke Fuehres</author>
<author>Peter A Gloor</author>
</authors>
<title>Predicting stock market indicators through twitter “i hope it is not as bad as i fear”. Procedia-Social and Behavioral Sciences.</title>
<date>2011</date>
<contexts>
<context position="6780" citStr="Zhang et al., 2011" startWordPosition="1071" endWordPosition="1074">e regression problem as a text ranking problem. Note that all these regression studies above investigate the SEC-mandated annual reports, which are very different from the earnings calls in many aspects such as length, format, vocabulary, and genre. Most recently, Xie et al. (2013) have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a binary classification task. Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market (Bollen et al., 2011; Zhang et al., 2011). Despite our financial domain, our approach is more relevant to text regression. Traditional discriminative models, such as linear regression and linear SVM, have been very popular in various text regression tasks, such as predicting movie revenues from reviews (Joshi et al., 2010), understanding the geographic lexical variation (Eisenstein et al., 2010), and predicting food prices from menus (Chahuneau et al., 2012). The advantage of these models is that the estimation of the parameters is often simple, the results are easy to interpret, and the approach often yields strong performances. Whi</context>
</contexts>
<marker>Zhang, Fuehres, Gloor, 2011</marker>
<rawString>Xue Zhang, Hauke Fuehres, and Peter A Gloor. 2011. Predicting stock market indicators through twitter “i hope it is not as bad as i fear”. Procedia-Social and Behavioral Sciences.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>