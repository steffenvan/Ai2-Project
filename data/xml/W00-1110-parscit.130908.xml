<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000179">
<title confidence="0.96944">
Automatic summarization of search engine hit lists
</title>
<author confidence="0.949369">
Dragomir R. Radev
</author>
<affiliation confidence="0.790336">
School of Information, University of Michigan
550 E. University St.
</affiliation>
<address confidence="0.740735">
Ann Arbor, MI 48109
radev@umich edu
</address>
<author confidence="0.631076">
Weiguo Fan
</author>
<affiliation confidence="0.908772">
University of Michigan Business School
</affiliation>
<address confidence="0.813382">
701 Tappan St.
Ann Arbor, MI 48109
</address>
<bodyText confidence="0.465536">
wf an@umi ch e du
</bodyText>
<sectionHeader confidence="0.959871" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858285714286">
We present our work on open-domain
multi-document summarization in the
framework of Web search. Our system,
SNS (pronounced &amp;quot;essence&amp;quot;), retrieves
documents related to an unrestricted user
query and summarizes a subset of them as
selected by the user. We present a task-
based extrinsic evaluation of the quality of
the produced multi-document summaries.
The evaluation results show that
summarization quality is relatively high
and does help improve the reading speed
and judge the relevance of the retrieved
URLs.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977909090909">
Online information is increasingly available at
an exponential rate. According to a recent
study by NetSizer (2000), the number of web
hosts has increased from 30 million in
Jan.1998 to 44 million in Jan. 1999, and to
more than 70 million in Jan. 2000. More than
2 million new hosts were added to the Internet
in Feb. 2000, according to this report. Similar
Internet growth results were reported by
Internet Domain Service (IDS, 2000). The
number of web pages on the Internet was 320
million pages in Dec. 1997 as reported by
Lawrence et al. (1997), 800 million in Feb.
1999 (Lawrence et al. 1999), and more than
1,720 million in March, 2000 (Censorware,
2000). The number of pages available on the
Internet almost doubles every year.
To help alleviate the information overload
problem and help users find the information
they need, many search engines emerge. They
build a huge centralized database to index a
portion of the Internet: ranging from 10
million to more than 300 million of web
pages. Search engines do help reduce the
information overload problem by allowing a
user to do a centralized search, but they also
bring up another problem for the user: too
many web pages are returned for a single
query. To find out which documents are
useful, the user often have to sift through
hundreds of pages to find out that only a few
of them are relevant. Moreover, browsing
through the long list of retrieval results is so
tedious that few users would be willing to go
through. That&apos;s why research results have
shown that search engine users often give up
their search in the first try, examining no more
than 10 documents (Jansen et al. 2000). It
would be very helpful if an effective search
engine could be designed to help classify the
retrieved web pages into clusters and provide
more contextual and summary information to
help these users explore the retrieval set more
efficiently.
Recent advances in information retrieval,
natural language processing, computational
linguistics make it easier to build a helpful
search engine based on summaries of hit lists.
We describe in this paper a prototype system,
SNS, which blends the traditional information
retrieval technology with the advanced
document clustering and multi-document
summarization technology in an integrated
framework. The following steps are performed
for a given query:
</bodyText>
<page confidence="0.996857">
99
</page>
<figureCaption confidence="0.999754">
Figure 1: Architecture diagram
</figureCaption>
<bodyText confidence="0.99973">
The general architecture of our system is
shown in Figure 1. User interaction with SNS
can be done in three different modes:
</bodyText>
<listItem confidence="0.9099563">
• Web search mode. The user enters a
general-domain query in the search engine
(MySearch). The result is a set of related
documents (the hit-list). The user then
selects which of the hits should be
summarized. MEAD, the summarization
component produces a cross-document
summary of the documents selected by the
user from the hit list.
• Intranet mode. The user indicates what
collection of documents needs to be
summarized. These documents are not
necessarily extracted from the Web.
• Clustering mode. The user indicates that
either the hit list of the search engine or a
stand-alone document collection needs to
be clustered. CIDR, the clustering
component, creates clusters of documents.
For each cluster, MEAD produces a cross-
document summary.
</listItem>
<bodyText confidence="0.9999113">
Our paper is organized as follows. Sections 2
— 4 describe the system. More specifically:
Section 2 explains how the search engine
operates, Section 3 deals with the clustering
module while Section 4 presents the multi-
document summarizer. Section 5 describes the
user interface of the system. In Section 6, we
present some experimental results. After we
compare our work to related research in
Section 7, we conclude the paper in Section 8.
</bodyText>
<sectionHeader confidence="0.911724" genericHeader="introduction">
2 Search
</sectionHeader>
<bodyText confidence="0.998870625">
The search component of SNS is a
personalized search engine called MySearch.
MySearch utilizes a centralized relational
database to store all the URL indexes and
other related URL information. Spiders are
used to fetch URLs from the Internet. After a
URL is downloaded, the following steps are
applied to index the URL:
</bodyText>
<listItem confidence="0.9549545">
• Parse the HTML file, remove all those
tags
• Apply Porter&apos;s stemming algorithms to
each keyword.
• Remove stop words
• Index each keyword into the database
along with its frequency and position
information.
</listItem>
<bodyText confidence="0.99992832">
The contents of URLs are indexed based on
the locations of the keywords: Anchor, Title,
and Body. This allows weighted retrieval
based on different word positions. For
example, a user can specify that he&apos;d like to
give a weight 5 for the keyword appearing in
the title, 4 for anchor, and 2 for body. This
information can be saved in his personal
profile and used for later weighted ranking.
Besides the weighted search, MySearch also
supports Boolean search and Vector Space
search (Salton, 1989). For the vector space
model, the famous TF-IDF is used for ranking
purpose. We used a modified version of TF-
IDF: log(y.+0.5)*log(N/dj), where y&apos;means the
number of times a term appeared in the
content of an URL, N is the total number of
documents in the text collection, and df stands
for the number of unique URLs in which a
term appears in the entire collection.
A user can choose which search method he
wants to use. He/she can also combine
Boolean search with Vector Space search.
These options are provided to give users more
flexibility to control the retrieval results as
</bodyText>
<page confidence="0.985812">
100
</page>
<bodyText confidence="0.999182214285714">
past research indicated that different ranking
functions give different performances (Salton,
1989).
documents indicates that the term &amp;quot;deny&amp;quot;
appears 40 times in the three documents. The
IDF values are computed from a mixture of
200 MB of news and web-based documents.
A sample search for &amp;quot;Clinton&amp;quot; using the TF-
IDF Vector Space search is shown in Figure 3.
The keyword &amp;quot;Clinton&amp;quot; is highlighted using a
different color to help users get more
contextual information. The retrieval status
value is shown in a bold black font after the
URL title.
</bodyText>
<sectionHeader confidence="0.986596" genericHeader="method">
3 Clustering
</sectionHeader>
<bodyText confidence="0.99958075">
Our system uses two types of clustered input -
either the set of hits that the user has selected
or the output of our own clustering engine -
CIDR (Columbia Intelligent Document
Relater). CIDR is described in (Radev et al.,
1999). It uses an iterative algorithm that
creates as a side product so-called &amp;quot;document
centroids&amp;quot;. The centroids contain the most
highly relevant words to the entire cluster (not
to the user query). We use these words to find
the most salient &amp;quot;themes&amp;quot; in the cluster of
documents.
</bodyText>
<subsectionHeader confidence="0.999715">
3.1 Finding themes within clusters
</subsectionHeader>
<bodyText confidence="0.999937357142857">
One of the underlying assumptions behind
SNS is that when a user selects a set of hits
after reading the single-document summaries
from the hit list retrieved by the system, he or
she performs a cognitive activity whereby he
or she selects documents which appear to be
related to one or more common themes. The
multi-document summarization algorithm
attempts to identify these themes and to
identify the most salient passages from the
selected documents using a pseudo-document
called the cluster centroid which is computed
automatically from the entire list of hits
selected by the user.
</bodyText>
<subsectionHeader confidence="0.99977">
3.2 Computing centroids
</subsectionHeader>
<bodyText confidence="0.99849625">
Figure 2 describes a sample of a cluster
centroid. The TF column indicates the average
term frequency of a given term within the
cluster. E.g., a TF value of 13.33 for three
</bodyText>
<table confidence="0.998275518518518">
Term TF IDF Score
app 20.67 8.90 183.88
lewinsky 34.67 5.25 182.03
currie 15.33 7.60 116.50
ms 32.00 3.06 97.97
january 25.33 3.30 83.60
jordan 18.67 4.06 75.81
referral 9.00 7.43 66.88
magaziner 6.67 10.00 66.64
Deny 13.33 4.92 65.61
Admit 13.00 4.92 63.97
monica 14.67 4.29 62.85
oic 5.67 10.00 56.64
betty 8.00 6.01 48.06
vemon 8.67 5.49 47.54
do 32.67 1.40 45.80
Telephoned 6.67 6.86 45.74
you 36.33 1.19 43.30
i 42.67 0.96 40.84
clinton 16.33 2.23 36.39
jones 11.33 3.17 35.88
OT 32.33 1.09 35.20
gif 3.33 9.30 31.01
white 12.00 2.50 30.01
triPP 4.67 6.23 29.10
ctv 3.00 9.30 27.91
december 7.33 3.71 27.19
</table>
<figureCaption confidence="0.983538">
Figure 2: A sample cluster centroid
</figureCaption>
<sectionHeader confidence="0.983402" genericHeader="method">
4 Centroid-based summarization
</sectionHeader>
<bodyText confidence="0.9999346">
The main technique that we use for
summarization is sentence extraction. We
score individually each sentence within a
cluster and output these that score the highest.
A more detailed description of the summarizer
can be found in (Radev et al., 2000).
The input to the summarization component is
a cluster of documents. These documents can
be either the result of a user query or the
output of CIDR.
The summarizer takes as input a cluster of d
documents with a total of n sentences as well
as a compression ratio parameter r which
indicates how much of the original cluster to
preserve.
</bodyText>
<page confidence="0.994489">
101
</page>
<bodyText confidence="0.9999682">
The output consists of a sequence of [n * r]
sentences from the original documents in the
same order as the input documents. The
highest-ranking sentences are included
according to the scoring formula below:
</bodyText>
<subsectionHeader confidence="0.705829">
Si = WcCi WpPi WfFi
</subsectionHeader>
<bodyText confidence="0.9970336">
In the formula, we, wf are weights. Ci is the
centroid score of the sentence, Pi is the
positional score of the sentence, and Fi is the
score of the sentence according to the overlap
with the first sentence of the document.
</bodyText>
<subsectionHeader confidence="0.996948">
4.1 Centroid value
</subsectionHeader>
<bodyText confidence="0.999559875">
The centroid value Ci for sentence Si is
computed as the sum of the centroid values C„,
of all words in the sentence. For example, the
sentence &amp;quot;President Clinton met with Vernon
Jordon in January&amp;quot; gets a score of 243.34
which is the sum of the individual centroid
values of the words (clinton = 36.39; vernon =
47.54; jordan = 75.81; j anuary = 83.60).
</bodyText>
<equation confidence="0.548761">
Ci = Cw
</equation>
<subsectionHeader confidence="0.992493">
4.2 Positional value
</subsectionHeader>
<bodyText confidence="0.999847428571429">
The positional value is computed as follows:
the first sentence in a document gets the same
score C,„„.,, as the highest-ranking sentence in
the document according to the centroid value.
The score for all sentences within a document
is computed according to the following
formula:
</bodyText>
<equation confidence="0.9839135">
(n—i+1)
Pi= * max(Ci)
</equation>
<bodyText confidence="0.9990444">
For example, if the sentence described above
appears as the third sentence out of 30 in a
document and the largest centroid value of any
sentence in the given document is 917.31, the
positional value P3 will be = 28/30 * 917.31
</bodyText>
<subsectionHeader confidence="0.999201">
4.3 First-sentence overlap
</subsectionHeader>
<bodyText confidence="0.928230444444444">
The overlap value is computed as the inner
product of the sentence vectors for the current
sentence i and the first sentence of the
document. The sentence vectors are the n-
dimensional representations of the words in
each sentence whereby the value at position i
of a sentence vector indicates the number of
occurrences of that word in the sentence.
Fi=SiSi
</bodyText>
<subsectionHeader confidence="0.999112">
4.4 Combining the three parameters
</subsectionHeader>
<bodyText confidence="0.999971916666667">
As indicated in (Radev &amp; al., 2000) we have
experimented with several weighting schemes
for the three parameters (centroid, position,
and first-sentence overlap). Until this moment,
we have not come to the point in which the
three weights wc, w,„ and wf are either
automatically learned or derived from a user
profile. Instead, we have experimented with
various sets of empirically determined values
for the weights. In this paper the results are
based on equal weights for the three
parameters w, = w,, = w1= 1.
</bodyText>
<sectionHeader confidence="0.992388" genericHeader="method">
5 User Interface
</sectionHeader>
<bodyText confidence="0.999945625">
We describe in this section the user interface
for web search mode as described earlier in
Section 1.
One component of our system is the search
engine (MySearch). The detailed design of the
search component is discussed in Section 2.
The result of a sample query &amp;quot;Clinton&amp;quot; to our
search engine is shown starting in Figure 4.
</bodyText>
<page confidence="0.995538">
102
</page>
<figureCaption confidence="0.993134">
Figure 3: Sample user query
</figureCaption>
<figure confidence="0.9901">
-4
WWWOCD.W.P.
SSEARCH/MEAD - Netscape
A Truly Personalized Search Engine
Lcon RPostP1
MySearch
Ab011t MN SPOI
Rank b7 I TFIDF. (deraPr.9_ _
Search results: clinion
Displaying documents 1-10 of total 386 found.
1- court tv online [2.95]
Court TV Online Text of President Clinton&apos;s responses to huliciary
Committee&apos;s 81 questions Full
• httuw.courttv.comteasefiles/clintoncrisis/112798 answerstext.html
43 KB
2. &apos;index (2.87)
index THE STARR SCANDAL A site chronicling the wrongdoinp of the
Shadow Government of Kenneth W
O http://www.geocities. comicapitolinlVsenate/9634/ 27 KB
Please click on &amp;quot;submit&amp;quot; in the frame above to continue. 2:1
Maintained by radev@vnich.edu
sone, Thr iclinton
Results pecparem 1173
SSEARCH
Temporary
Web Interface
• .1P-2.g
• Help
Search:
Weisuo Fan
Summarization:
Dragortrir R. Radev
0 Columbia U.
1998-2000
CU. of Michigan
2000
</figure>
<bodyText confidence="0.997922869565217">
A user has the option to choose a specific
ranking function as well as the number of
retrieval results to be shown in a single screen.
The keyword contained in the query string will
be automatically highlighted in the search
results to provide contextual information for
the user.
The overall interface for SNS is shown in
Figure 4. On the top right of the frame is the
MySearch search engine. When a user
submits a query, the screen in Figure 5
appears. As can be seen from Figure 5, there is
a check box along with each retrieved record.
This allows the user to tell the summarization
engine which documents he/she wants to
summarize. After the user clicks the
summarization button, the summarization
option screen is displayed as shown in bottom
of Figure 6. The summarization option screen
allows a user to specify the summarization
compression ratio. Figure 7 shows the
summarization result for four URLs with the
compression ratio set as 30%.
</bodyText>
<page confidence="0.998457">
103
</page>
<figureCaption confidence="0.997927">
Figure 4: SNS interface (framed)
</figureCaption>
<figure confidence="0.999431014925373">
:
-
WiRitt
y,,15,141:2141734Z
-.SITARCII/Mf Al) N. tscdp,.
ve
•
4.t. •
A
SSEARCII
Temporary
Web interface
A Truly Personalized Search Engine
Loam
&amp;Ara ja.. 1 int 0 n Ras* byIWIDF (detaUt)
Rambo perms:
MySearch
.VmutM0,51,1.
Powered by MALI.. Apache ea a Sue Vasa SPARC saw. RISanch is free torch cages nr•IC• Provided by
the emanate bb at the Uriversity othAchiBusosess Schod.
Any Comsner•• and suggations to the system should be directed to the enemata.
Secede
Weis)* Fan
Summarintiotx
Drasiontir R. Badev
0 Columbia U.
19912-2000
0 U. of Iffichipn
2000
• .01_mg
• If*
imemiumeluseenserameetil
Please click on &amp;quot;submit&amp;quot; in the frame above to continue.
Maintained by radevestmich.edg
o Cohunbia U.
19984000
(Next l&gt;&gt;
&apos;47,-SSLARCHIMEAD - Netscape
aaaa,.
SSEARCH
Temporary
Web Interface
•
• Mt
Sunk
Weiguo Fan
Summons...Minx
DmpmirR.Radm
Starch Top Stories Index To.:
• littlx//www.seattletimes.com/extra/browse/htird9Mdtinet 052297.htng
KB
L. policy news events daily briefina revert details chinese esuionale efforts [1.763
Foley corn News Events Daly Briefing Cox Report Details Chinese
Espionage Efforts Today&apos;s News D...
* hthrfiwww.Dolicv.cominews/d16efidbriefare248.aso 11 KB
10. close foundation policy alba (1.76]
Close Up Foundation U S Policy Toward Cuba Close Up Foundation Special
Topic Page U S Policy Tows...
6 btemliorww.closcup.orgieuba-htin 29 KB
?meat by MySQL, Apache one Sup latsa SPARC server, hlySetatels is free search rag* CergiCe provided by
the •caMtnerCe tab at Use University of Wicklow, Business School.
Any corootests and suggestions to the listen should be directed to thg webrraster.
Please click on &amp;quot;submit&amp;quot; in the frame above to continue.
Maintained by radevaumeh.edu
CU. of Michigan
2000
,NIA0m
</figure>
<figureCaption confidence="0.551839">
&apos;igure 5: Sea.-ch output along with user selection of documents to be summarized
</figureCaption>
<page confidence="0.928179">
104
</page>
<figureCaption confidence="0.9999085">
Figure 6: 6: Selected documents for summarization
Figure 7: Output of the summarizer
</figureCaption>
<figure confidence="0.996780537313433">
SSEARCH/M EAU - Netscape
10. close foundation policy cuba[1.76)
Close Up Foundation US Policy Toward Cuba Close Up Foundation Special
Topic Page US Policy Tows...
0 httrewww.closcuo.oralcuba.htm 19 KB
(Next 1 »I
Summarizing 4 hits:
Rank vim
1isttp//ww.couitb&apos;.com/casefilcslclintoncrisislll2798 answerstact.hUnD
; 4
httn://www.ecommerce.zov
8. http://www.seattlethnes.com/eara/browseihtm197/altinet 052297.html
10 http//www.closcup.ort./cuba.htmi
Summary Length (m percent)
Suppress repeated information (not done YET) 1177-•-3
Order summary sentences by: (not done YET)po irnn
J
Maintained by radevaanich.edu
4
SSEARCH
Temporary
Web Interface
----------
• About
• Li..*
Search:
Weizuo Fan
Summarizatiorc
Drazornir R. Radev
0 Columbia U.
1998-2000
0 U. of Michigan
2000
MEM
SSEARCI1/MEAD - Netscape
Temporary
Web Interface
-------
• About
• p_t*
Search:
Weitruo Fan
Summarizatioa
Drazornir R. Radev
0 Columbia U.
1998-2000
CU. of Michigan
2000
10. close foundation policy cuba [1.76)
SSEARCH
Maintained by radevOwnich.edu
w),fal 4 1
Summary @ 30% of the URLs that you
selected:
httirf/www.courttv.comicasefiles/cfmtonerisis/112798_answerstcct.html
httirJ1www.ecommerce.gov
httpd/www.scaffletjncs.com/extrafarowse/bn7/althict_052297.houl
bttpil/www.closeup.org/euba.htm
Summarizing 1360 sentences @30% = 408 sentences
Sentence Score
Tightening the EmbargoFor almost forty years the United States has not imported any Cuban i63.7.1
Products nor allowed any American food merfical supplies or capital to enter Cuba
other camtry has joined the United States in the trade anbargo against Cuba in fact the
p1.93
Acims-Burton Act angered nations that do business with Cuba
Many people in the United States however do not think that foreign countries&apos; dollars should
:106.84.
</figure>
<page confidence="0.996907">
105
</page>
<bodyText confidence="0.9965435">
The following information is shown in the
summarization result screen in Figure 7:
</bodyText>
<listItem confidence="0.973451333333333">
• The number of sentences in the text of the
set of URLs that the user selected
• The number of sentences in the summary
</listItem>
<bodyText confidence="0.996565">
The sentences representing the themes of those
selected URLs and their relative scores. The
sentences are ordered the same way they appear
in the original set of documents.
</bodyText>
<sectionHeader confidence="0.989016" genericHeader="method">
6 Experimental results
</sectionHeader>
<bodyText confidence="0.993282571428571">
Our system was evaluated using the task-based
extrinsic measure as suggested in (Math et al.
1999). The experiment was set up as follows:
Three sets of documents on different topics were
selected prior to the experiment. The topics and
their corresponding document information are
shown in Table 1.
</bodyText>
<table confidence="0.9824744">
Topic No. Topic No. of Articles Length
S1 Global E-Commerce Framework 3 200k
S2 Introduction to Data Mining 2 100k
S3 Intelligent Agents and their application in 5 160k
Information retrieval
</table>
<tableCaption confidence="0.999637">
Table 1: Evaluation Topics and their corresponding document set information
</tableCaption>
<bodyText confidence="0.961509428571428">
Sentence Score
Inc idea behind .dataminirig :then is ihe::non-...trivial process,ofidentifying valid 494.92
.nOirel potentially usefulfahd:tiltiinatelytinderttaildable patterns in data 18 2 The
term knowledge discovery databases KDD, Was forMalized irt1989 in,reference
. ::. - „ - . -
to theierieral concept ofbeing broad and &apos;high level&apos; in pursuit of seeking
knOWlidge:fr &apos; ,::.data . .
The term data mining is then this high-level application techniques / tools used to 509.11
present and analyze data for decision makers
&apos;Thi§lefiredita mining has been used by sio§ii640:0,&apos;data,analyst 4iirtbemtay:!, 487.92
management information systeMS&apos;eoininunity whereat icpp:jias been mostly used
&apos; by .iiiiticia:1 intelligence and itiic:Iiin6 learning researchers
These are : -the untapped value in large databases consolidation of database 576.60
records tending towards a single customer view concept of an information or data
warehouse from the consolidation of databases dramatic drop in the
cost/performance ratio of hardware systems - for data storage and processing
-Intense competition in anincreasmg saturated marketplace ltreabilitycto custom 486.92
manufacture market and advertise to small markeisegmentS.and individuals 4 and
the market for data mining products is estiniatedt about 50tlinillion-in early
- _ , - . ,
1994 12 Data&apos; :Tx-lir&apos; ling teclinOlogies are charaderii&apos;&apos;&apos;&amp;quot;:&apos; ed by intensive computations on
large vol tunes of data,::-,
Data mining versus traditional database queries Traditional database queries 520.53
contrasts with data mining since these are typified by the simple question such as
what were the sales of orange juice in January 1995 for the Boston area
Data inininobthe other hand thid0. the use of spe ific alg*Ittyns or .carch 500.80
,en esittent-. tits to ,sonrceotit&apos;disceehahle p.itteiiis and trendt,Wilie data and
infers es:fromAtiese pattry■
</bodyText>
<figureCaption confidence="0.986148">
Figure 8: A sample of the summarization result for S2 at 10% compression rate
</figureCaption>
<bodyText confidence="0.997812">
As Table 1 shows, the articles in topic set Si are in S3 are the shortest, with each 32k in average.
longer than both these in S2 and S3. The articles The number of documents in each topic set is
</bodyText>
<page confidence="0.996457">
106
</page>
<bodyText confidence="0.998840444444444">
also different. The variations of document length
and different number of documents in each topic
set will help test the robustness of our
summarization algorithms.
We used SNS to generate both 10% and 20%
summaries for each topic. A sample of the 10%
summary for topic S2 is shown in Figure 8. Four
users were selected for evaluation of these
summarization results. Each user was asked to
read through the set of full articles for each topic
first, followed by its corresponding 10% and
20% summaries. After these 4 users finished
each set, they were asked to assign a readability
score (1-10) for each summary. The higher the
readability score is, the more readable and
meaningful for comprehension is the summary.
The time of reading both full articles and
summaries was tracked and recorded.
</bodyText>
<figure confidence="0.983081930232558">
User I Time User 1 - Time
(Mins) Readability (Mins)
55 (1-10) - 70
7 N/A 10
12 N/A 16
42 N/A 49
6 7
10 12
60 68
7 8
12 14
Item Time ItiTritf—abiltry
I: Global E-Commerce (Mins) (1-10)
ramework (3 articles) 75 N/A
1: 10% SUMfilary 15 9
1: 20% Summar!, 20 S
2: Introduction to Data 55 N/A
ining (2 articles) 10 9
2: 10% Summar!, 14 S
2: 20% Summary 70 N/A
3: Intelligent Agents 13 S
ad their application in 20 9
nformation retrieval (5
rucks)
3: 10% Summary
3: 20% Summary
User 3
ty
(1-10)
NIA
N/A
User 4
Time Readability
(Mins) (1L10)
65 NIA
8
15
46 N/A
6
11
66 NIA
7 8
8 15
</figure>
<tableCaption confidence="0.999426">
Table 2: Summarization evaluation: detailed results
</tableCaption>
<table confidence="0.996772333333333">
10% Summaries 20% Summaries
Speedup in reading time by summary over full article 721 / 105 = 6.87 721 / 171= 4.22
Avg. Readability 7.92 8.42
</table>
<tableCaption confidence="0.982122">
Table 3: Summary of the evaluation results
The detailed evaluation results are shown in
Table 2. Table 3 gives the summary of the Table
2. It&apos;s shown in Table 2 that these four users
</tableCaption>
<bodyText confidence="0.864660142857143">
have different reading speeds. However, their
reading speed is pretty consistent across the 3
topics. The summaries generated by SNS are
also very readable. For example, The average
readability score (which is obtained by
averaging the readability scores assigned by the
four users) for 10% and 20% summaries for
</bodyText>
<page confidence="0.996352">
107
</page>
<bodyText confidence="0.997523071428572">
topic Si, is 8, 8 respectively. For topic S3, the
average readability score for 10% and 20%
summaries is 7.75, and 8.75, respectively.
Similarly, for S2 the average readability score
for 10% and 20% summaries is 8 and 8.5,
respectively. The differences in the average
readability score also suggest that (a) our
summarizer favors longer documents over
shorter documents; (b) 20% summaries are
generally favorable over 10% summaries. The
difference in the readability score between 10%
and 20% summaries is bigger in S3 (diff = 1.0)
than in Si (diff = 0). These interesting findings
raise interesting questions for future research.
As can be seen from Table 3, the 20% summary
achieves better readability score in overall than
the 10% summary. The speedup of the 10%
summary over full articles is 6.87. That is, with
reading material reduced by 900%, the speedup
in reading is only 687%. This suggests that there
may be a little bit difficulty in reading the 10%
summary result. This may be due to the simple
sentence boundary detection algorithm we used.
The feedback from users in the evaluation seems
to confirm the above reason. As more sentences
were included in the 20% summaries, the
speedup in reading (4.22) almost approached the
optimal speedup ratio (5.0)1.
</bodyText>
<sectionHeader confidence="0.999934" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.9994602">
Neto et al. (2000) describes a text mining tool
that performs document clustering and text
summarization. They used the Autoclass
algorithm to perform document clustering and
used TF-ISF (an adaptation of TF-IDF) to
perform sentence ranking and generate the
summarization output. Our work is different
from theirs in that we perform personalized
summarization based on the retrieval result from
a generic personalized web-based search engine.
A more complicated sentence ranking functions
is employed to boost the ranking performance.
The compression ratio for the summary is
customizable by a user. Both single-document
for a single URL and multiple-document
I Since the length of the summary is only 20% of the
original documents, the maximum speedup in terms of
reading time is 1/0.2=5.
summarization for a cluster of URLs are
supported in our system.
More related work can be found in Extractor
web site http://extractor.iit.nrc.ca/. They use
MetaCrawler to perform web-based search and
automatically generate summaries for each
URLs retrieved. They only support single
document summarization in their engine and the
compression rate of the summarizer is also non-
customizable. We not only support both single
and multiple document summarization, but also
allow the user to specify the summarization
compression ratio as well as to get per-cluster
summaries of automatically generated clusters,
which, we believe, are more valuable to online
users and give them more flexibility and control
of the summarization results.
</bodyText>
<sectionHeader confidence="0.975263" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999883818181818">
We described in this paper a prototype system
SNS, which integrates natural language
processing and information retrieval techniques
to perform automatic customized summarization
of search engine results. The user interface and
detailed design of SNS&apos;s components are also
discussed. Task-based extrinsic evaluation
showed that the system is of reasonably high
quality.
The following issues will be addressed in the
future.
</bodyText>
<subsectionHeader confidence="0.9936535">
8.1 Interaction between sentence inclusion
in a summary
</subsectionHeader>
<bodyText confidence="0.999877615384615">
There are two types of interaction (or
reinforcement) between sentences in a summary:
negative and positive.
Negative interaction occurs when the inclusion
of one sentence in the summary indicates that
another sentence should not appear in the
summary. This is particularly relevant to multi-
document summarization as in this case:
negative interaction models the non-inclusion of
redundant information.
The case of positive interaction involves positive
reinforcement between sentences. For example,
if a sentence with a referring expression is to be
</bodyText>
<page confidence="0.99707">
108
</page>
<bodyText confidence="0.9994492">
included in a summary, typically the sentence
containing the antecedent should also be added.
We will investigate specific setups in which
positive and/or ,negative reinforcement between
sentences is practical and useful.
</bodyText>
<subsectionHeader confidence="0.996383">
8.2 Personalization
</subsectionHeader>
<bodyText confidence="0.998895">
We will investigate additional techniques for
producing personalized summaries. Some of the
approaches that we are considering are:
</bodyText>
<listItem confidence="0.94826325">
• Query words: favoring sentences that
include words from the user query in the
Web-based scenario
• Personal preferences and interaction history:
we would favor sentences that match the
user profile (e.g., overlapping with his or her
long-term interests and/or recent queries
logged by the system).
</listItem>
<subsectionHeader confidence="0.761301">
83 Technical limitations
</subsectionHeader>
<bodyText confidence="0.999819666666667">
The current version of our system uses a fairly
basic sentence delimiting component. We will
investigate the user of robust sentence boundary
identification modules in the future.
We will also investigate the possibility of some
limited-form anaphora resolution component.
</bodyText>
<subsectionHeader confidence="0.992181">
8.4 Availability
</subsectionHeader>
<bodyText confidence="0.979442333333333">
A demonstration version of SNS is available at
the following URL:
http://www.si.umich.edu/—radev/ssearch/
</bodyText>
<sectionHeader confidence="0.999374" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99971164">
Carbonell, J. and Goldstein, J. (1998). The use of
MMR, Diversity-Based Reranking for Reordering
Documents and Producing Summaries. Poster
Session, SIGIR&apos;98, Melbourne, Australia.
Censorware (2000).
http://www.censorware.org/web size/.
Extractor (2000). http://extractoriit.nrc.cal.
IDS (2000). Internet Domain Survey.
http://www.isc.orWds/.
Jansen, B. J., Spink, A., and Saracevic, T. (2000).
Real life, real users, and real needs: a study and
analysis of user queries on the web. Information
Processing and Management. 36(2), 207-227.
Lawrence, S., and Giles, C. L. (1997). Searching the
World Wide Web, Science, 280(3), 98-100.
Lawrence, S., and Giles, C. L. (1999). Accessibility of
information on the web, Nature, 400, 107-109.
Mani, I. and Bloedom, E. (1999). Summarizing
similarities and dfferences among related
documents. Information Retrieval 1(1): 35-67.
Mani, I., House, D., Klein, G., Hirschman, L., Obrst,
L., Firmin, T., Chrzanowski, M., and Sundheim, B.
(1998). The TIPSTER SUMMAC Text
Summarization Evaluation. The MITRE
Corporation Technical Report MTR 98W0000138,
McLean, Virginia.
McKeown, K. and D. R. Radev. Generating Summaries
of Multiple News Articles. Proceedings, ACM
Conference on Research and Development in
Information Retrieval SIGIR&apos;95 (Seattle, WA, July
1995).
NetSizer (2000). http://wvvw.netsizer.com/.
Neto, J. L., Santos, A. D., ICaestner, C. A. A., and
Freitas, A. A. (2000). Document clustering and text
summarization. In Proceedings, 4th Int. Conference
on Practical Applications of Knowledge Discovery
and Data Mining (PADD-2000), 41-55. London:
The Practical Application Company.
Radev, D. R., Hatzivassiloglou, V., and McKeown,
K. A Description of the CIDR System as Used for
TDT-2. Proceedings, DARPA Broadcast News
Workshop, (Herndon, VA, February 1999).
Radev, D. R, ling, H., and Stys-Budzikowska, M.
Summarization of multiple documents: clustering,
sentence extraction, and evaluation. Proceedings,
ANLP-NAACL Workshop on Automatic
Summarization, (Seattle, WA, April 2000)
Salton, G. (1989). Automatic Text Processing.
Addison-Wesley Publishing Co., Reading, MA,
1989.
</reference>
<page confidence="0.998956">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.352483">
<title confidence="0.999552">Automatic summarization of search engine hit lists</title>
<author confidence="0.99994">Dragomir R Radev</author>
<affiliation confidence="0.999936">School of Information, University of Michigan</affiliation>
<address confidence="0.9886895">550 E. University St. Ann Arbor, MI 48109</address>
<email confidence="0.870212">radev@umichedu</email>
<author confidence="0.743277">Weiguo</author>
<affiliation confidence="0.999666">University of Michigan Business</affiliation>
<address confidence="0.934371">701 Tappan Ann Arbor, MI</address>
<email confidence="0.682657">wfan@umichedu</email>
<abstract confidence="0.993455866666667">We present our work on open-domain multi-document summarization in the framework of Web search. Our system, SNS (pronounced &amp;quot;essence&amp;quot;), retrieves documents related to an unrestricted user query and summarizes a subset of them as selected by the user. We present a taskbased extrinsic evaluation of the quality of the produced multi-document summaries. The evaluation results show that summarization quality is relatively high and does help improve the reading speed and judge the relevance of the retrieved URLs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. Poster Session, SIGIR&apos;98,</title>
<date>1998</date>
<location>Melbourne, Australia.</location>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Carbonell, J. and Goldstein, J. (1998). The use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. Poster Session, SIGIR&apos;98, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Censorware</author>
</authors>
<title>http://www.censorware.org/web size/. Extractor</title>
<date>2000</date>
<note>http://extractoriit.nrc.cal.</note>
<contexts>
<context position="1473" citStr="Censorware, 2000" startWordPosition="239" endWordPosition="240">e at an exponential rate. According to a recent study by NetSizer (2000), the number of web hosts has increased from 30 million in Jan.1998 to 44 million in Jan. 1999, and to more than 70 million in Jan. 2000. More than 2 million new hosts were added to the Internet in Feb. 2000, according to this report. Similar Internet growth results were reported by Internet Domain Service (IDS, 2000). The number of web pages on the Internet was 320 million pages in Dec. 1997 as reported by Lawrence et al. (1997), 800 million in Feb. 1999 (Lawrence et al. 1999), and more than 1,720 million in March, 2000 (Censorware, 2000). The number of pages available on the Internet almost doubles every year. To help alleviate the information overload problem and help users find the information they need, many search engines emerge. They build a huge centralized database to index a portion of the Internet: ranging from 10 million to more than 300 million of web pages. Search engines do help reduce the information overload problem by allowing a user to do a centralized search, but they also bring up another problem for the user: too many web pages are returned for a single query. To find out which documents are useful, the us</context>
</contexts>
<marker>Censorware, 2000</marker>
<rawString>Censorware (2000). http://www.censorware.org/web size/. Extractor (2000). http://extractoriit.nrc.cal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IDS</author>
</authors>
<title>Internet Domain Survey.</title>
<date>2000</date>
<note>http://www.isc.orWds/.</note>
<contexts>
<context position="1247" citStr="IDS, 2000" startWordPosition="198" endWordPosition="199">The evaluation results show that summarization quality is relatively high and does help improve the reading speed and judge the relevance of the retrieved URLs. 1 Introduction Online information is increasingly available at an exponential rate. According to a recent study by NetSizer (2000), the number of web hosts has increased from 30 million in Jan.1998 to 44 million in Jan. 1999, and to more than 70 million in Jan. 2000. More than 2 million new hosts were added to the Internet in Feb. 2000, according to this report. Similar Internet growth results were reported by Internet Domain Service (IDS, 2000). The number of web pages on the Internet was 320 million pages in Dec. 1997 as reported by Lawrence et al. (1997), 800 million in Feb. 1999 (Lawrence et al. 1999), and more than 1,720 million in March, 2000 (Censorware, 2000). The number of pages available on the Internet almost doubles every year. To help alleviate the information overload problem and help users find the information they need, many search engines emerge. They build a huge centralized database to index a portion of the Internet: ranging from 10 million to more than 300 million of web pages. Search engines do help reduce the i</context>
</contexts>
<marker>IDS, 2000</marker>
<rawString>IDS (2000). Internet Domain Survey. http://www.isc.orWds/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
<author>A Spink</author>
<author>T Saracevic</author>
</authors>
<title>Real life, real users, and real needs: a study and analysis of user queries on the web. Information Processing and Management.</title>
<date>2000</date>
<volume>36</volume>
<issue>2</issue>
<pages>207--227</pages>
<contexts>
<context position="2459" citStr="Jansen et al. 2000" startWordPosition="409" endWordPosition="412">e the information overload problem by allowing a user to do a centralized search, but they also bring up another problem for the user: too many web pages are returned for a single query. To find out which documents are useful, the user often have to sift through hundreds of pages to find out that only a few of them are relevant. Moreover, browsing through the long list of retrieval results is so tedious that few users would be willing to go through. That&apos;s why research results have shown that search engine users often give up their search in the first try, examining no more than 10 documents (Jansen et al. 2000). It would be very helpful if an effective search engine could be designed to help classify the retrieved web pages into clusters and provide more contextual and summary information to help these users explore the retrieval set more efficiently. Recent advances in information retrieval, natural language processing, computational linguistics make it easier to build a helpful search engine based on summaries of hit lists. We describe in this paper a prototype system, SNS, which blends the traditional information retrieval technology with the advanced document clustering and multi-document summar</context>
</contexts>
<marker>Jansen, Spink, Saracevic, 2000</marker>
<rawString>Jansen, B. J., Spink, A., and Saracevic, T. (2000). Real life, real users, and real needs: a study and analysis of user queries on the web. Information Processing and Management. 36(2), 207-227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lawrence</author>
<author>C L Giles</author>
</authors>
<date>1997</date>
<journal>Searching the World Wide Web, Science,</journal>
<volume>280</volume>
<issue>3</issue>
<pages>98--100</pages>
<marker>Lawrence, Giles, 1997</marker>
<rawString>Lawrence, S., and Giles, C. L. (1997). Searching the World Wide Web, Science, 280(3), 98-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lawrence</author>
<author>C L Giles</author>
</authors>
<date>1999</date>
<journal>Accessibility of information on the web, Nature,</journal>
<volume>400</volume>
<pages>107--109</pages>
<marker>Lawrence, Giles, 1999</marker>
<rawString>Lawrence, S., and Giles, C. L. (1999). Accessibility of information on the web, Nature, 400, 107-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>E Bloedom</author>
</authors>
<title>Summarizing similarities and dfferences among related documents.</title>
<date>1999</date>
<journal>Information Retrieval</journal>
<volume>1</volume>
<issue>1</issue>
<pages>35--67</pages>
<marker>Mani, Bloedom, 1999</marker>
<rawString>Mani, I. and Bloedom, E. (1999). Summarizing similarities and dfferences among related documents. Information Retrieval 1(1): 35-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>D House</author>
<author>G Klein</author>
<author>L Hirschman</author>
<author>L Obrst</author>
<author>T Firmin</author>
<author>M Chrzanowski</author>
<author>B Sundheim</author>
</authors>
<date>1998</date>
<booktitle>The TIPSTER SUMMAC Text Summarization Evaluation. The MITRE Corporation</booktitle>
<tech>Technical Report MTR 98W0000138,</tech>
<location>McLean, Virginia.</location>
<marker>Mani, House, Klein, Hirschman, Obrst, Firmin, Chrzanowski, Sundheim, 1998</marker>
<rawString>Mani, I., House, D., Klein, G., Hirschman, L., Obrst, L., Firmin, T., Chrzanowski, M., and Sundheim, B. (1998). The TIPSTER SUMMAC Text Summarization Evaluation. The MITRE Corporation Technical Report MTR 98W0000138, McLean, Virginia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>D R Radev</author>
</authors>
<title>Generating Summaries of Multiple News Articles.</title>
<date>1995</date>
<booktitle>Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR&apos;95</booktitle>
<location>Seattle, WA,</location>
<marker>McKeown, Radev, 1995</marker>
<rawString>McKeown, K. and D. R. Radev. Generating Summaries of Multiple News Articles. Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR&apos;95 (Seattle, WA, July 1995).</rawString>
</citation>
<citation valid="false">
<marker></marker>
<rawString>NetSizer (2000). http://wvvw.netsizer.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Neto</author>
<author>A D Santos</author>
<author>C A A ICaestner</author>
<author>A A Freitas</author>
</authors>
<title>Document clustering and text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings, 4th Int. Conference on Practical Applications of Knowledge Discovery and Data Mining (PADD-2000),</booktitle>
<pages>41--55</pages>
<contexts>
<context position="23979" citStr="Neto et al. (2000)" startWordPosition="3882" endWordPosition="3885">s better readability score in overall than the 10% summary. The speedup of the 10% summary over full articles is 6.87. That is, with reading material reduced by 900%, the speedup in reading is only 687%. This suggests that there may be a little bit difficulty in reading the 10% summary result. This may be due to the simple sentence boundary detection algorithm we used. The feedback from users in the evaluation seems to confirm the above reason. As more sentences were included in the 20% summaries, the speedup in reading (4.22) almost approached the optimal speedup ratio (5.0)1. 7 Related Work Neto et al. (2000) describes a text mining tool that performs document clustering and text summarization. They used the Autoclass algorithm to perform document clustering and used TF-ISF (an adaptation of TF-IDF) to perform sentence ranking and generate the summarization output. Our work is different from theirs in that we perform personalized summarization based on the retrieval result from a generic personalized web-based search engine. A more complicated sentence ranking functions is employed to boost the ranking performance. The compression ratio for the summary is customizable by a user. Both single-docume</context>
</contexts>
<marker>Neto, Santos, ICaestner, Freitas, 2000</marker>
<rawString>Neto, J. L., Santos, A. D., ICaestner, C. A. A., and Freitas, A. A. (2000). Document clustering and text summarization. In Proceedings, 4th Int. Conference on Practical Applications of Knowledge Discovery and Data Mining (PADD-2000), 41-55. London: The Practical Application Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>V Hatzivassiloglou</author>
<author>K McKeown</author>
</authors>
<date>1999</date>
<booktitle>A Description of the CIDR System as Used for TDT-2. Proceedings, DARPA Broadcast News Workshop,</booktitle>
<location>Herndon, VA,</location>
<contexts>
<context position="6910" citStr="Radev et al., 1999" startWordPosition="1138" endWordPosition="1141">three documents. The IDF values are computed from a mixture of 200 MB of news and web-based documents. A sample search for &amp;quot;Clinton&amp;quot; using the TFIDF Vector Space search is shown in Figure 3. The keyword &amp;quot;Clinton&amp;quot; is highlighted using a different color to help users get more contextual information. The retrieval status value is shown in a bold black font after the URL title. 3 Clustering Our system uses two types of clustered input - either the set of hits that the user has selected or the output of our own clustering engine - CIDR (Columbia Intelligent Document Relater). CIDR is described in (Radev et al., 1999). It uses an iterative algorithm that creates as a side product so-called &amp;quot;document centroids&amp;quot;. The centroids contain the most highly relevant words to the entire cluster (not to the user query). We use these words to find the most salient &amp;quot;themes&amp;quot; in the cluster of documents. 3.1 Finding themes within clusters One of the underlying assumptions behind SNS is that when a user selects a set of hits after reading the single-document summaries from the hit list retrieved by the system, he or she performs a cognitive activity whereby he or she selects documents which appear to be related to one or </context>
</contexts>
<marker>Radev, Hatzivassiloglou, McKeown, 1999</marker>
<rawString>Radev, D. R., Hatzivassiloglou, V., and McKeown, K. A Description of the CIDR System as Used for TDT-2. Proceedings, DARPA Broadcast News Workshop, (Herndon, VA, February 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H ling</author>
<author>M Stys-Budzikowska</author>
</authors>
<title>Summarization of multiple documents: clustering, sentence extraction, and evaluation.</title>
<date>2000</date>
<booktitle>Proceedings, ANLP-NAACL Workshop on Automatic Summarization,</booktitle>
<location>Seattle, WA,</location>
<contexts>
<context position="8938" citStr="Radev et al., 2000" startWordPosition="1477" endWordPosition="1480">8.00 6.01 48.06 vemon 8.67 5.49 47.54 do 32.67 1.40 45.80 Telephoned 6.67 6.86 45.74 you 36.33 1.19 43.30 i 42.67 0.96 40.84 clinton 16.33 2.23 36.39 jones 11.33 3.17 35.88 OT 32.33 1.09 35.20 gif 3.33 9.30 31.01 white 12.00 2.50 30.01 triPP 4.67 6.23 29.10 ctv 3.00 9.30 27.91 december 7.33 3.71 27.19 Figure 2: A sample cluster centroid 4 Centroid-based summarization The main technique that we use for summarization is sentence extraction. We score individually each sentence within a cluster and output these that score the highest. A more detailed description of the summarizer can be found in (Radev et al., 2000). The input to the summarization component is a cluster of documents. These documents can be either the result of a user query or the output of CIDR. The summarizer takes as input a cluster of d documents with a total of n sentences as well as a compression ratio parameter r which indicates how much of the original cluster to preserve. 101 The output consists of a sequence of [n * r] sentences from the original documents in the same order as the input documents. The highest-ranking sentences are included according to the scoring formula below: Si = WcCi WpPi WfFi In the formula, we, wf are wei</context>
</contexts>
<marker>Radev, ling, Stys-Budzikowska, 2000</marker>
<rawString>Radev, D. R, ling, H., and Stys-Budzikowska, M. Summarization of multiple documents: clustering, sentence extraction, and evaluation. Proceedings, ANLP-NAACL Workshop on Automatic Summarization, (Seattle, WA, April 2000)</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing.</title>
<date>1989</date>
<publisher>Addison-Wesley Publishing Co.,</publisher>
<location>Reading, MA,</location>
<contexts>
<context position="5541" citStr="Salton, 1989" startWordPosition="905" endWordPosition="906">yword. • Remove stop words • Index each keyword into the database along with its frequency and position information. The contents of URLs are indexed based on the locations of the keywords: Anchor, Title, and Body. This allows weighted retrieval based on different word positions. For example, a user can specify that he&apos;d like to give a weight 5 for the keyword appearing in the title, 4 for anchor, and 2 for body. This information can be saved in his personal profile and used for later weighted ranking. Besides the weighted search, MySearch also supports Boolean search and Vector Space search (Salton, 1989). For the vector space model, the famous TF-IDF is used for ranking purpose. We used a modified version of TFIDF: log(y.+0.5)*log(N/dj), where y&apos;means the number of times a term appeared in the content of an URL, N is the total number of documents in the text collection, and df stands for the number of unique URLs in which a term appears in the entire collection. A user can choose which search method he wants to use. He/she can also combine Boolean search with Vector Space search. These options are provided to give users more flexibility to control the retrieval results as 100 past research in</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton, G. (1989). Automatic Text Processing. Addison-Wesley Publishing Co., Reading, MA, 1989.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>