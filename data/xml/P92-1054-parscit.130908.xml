<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.549950">
<title confidence="0.992291">
Sense-Linking in a Machine Readable Dictionary
</title>
<author confidence="0.998934">
Robert Krovetz
</author>
<affiliation confidence="0.9998695">
Department of Computer Science
University of Massachusetts, Amherst, MA 01003
</affiliation>
<sectionHeader confidence="0.957837" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999904909090909">
Dictionaries contain a rich set of relation-
ships between their senses, but often these
relationships are only implicit. We report
on our experiments to automatically iden-
tify links between the senses in a machine-
readable dictionary. In particular, we au-
tomatically identify instances of zero-affix
morphology, and use that information to
find specific linkages between senses. This
work has provided insight into the perfor-
mance of a stochastic tagger.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999620625">
Machine-readable dictionaries contain a rich set
of relationships between their senses, and indicate
them in a variety of ways. Sometimes the relation-
ship is provided explicitly, such as with a synonym or
antonym reference. More commonly the relationship
is only implicit, and needs to be uncovered through
outside mechanisms. This paper describes our ef-
forts at identifying these links.
The purpose of the research is to obtain a bet-
ter understanding of the relationships between word
meanings, and to provide data for our work on word-
sense disambiguation and information retrieval. Our
hypothesis is that retrieving documents on the basis
of word senses (instead of words) will result in bet-
ter performance. Our approach is to treat the in-
formation associated with dictionary senses (part of
speech, subcategorization, subject area codes, etc.)
as multiple sources of evidence (cf. Krovetz [3]).
This process is fundamentally a divisive one, and
each of the sources of evidence has exceptions (i.e.,
instances in which senses are related in spite of be-
ing separated by part of speech, subcategorization,
or morphology). Identifying related senses will help
us to test the hypothesis that unrelated meanings
will be more effective at separating relevant from
nonrelevant documents than meanings which are re-
lated.
We will first discuss some of the explicit indica-
tions of sense relationships as found in usage notes
and deictic references. We will then describe our
efforts at uncovering the implicit relationships via
stochastic tagging and word collocation.
</bodyText>
<sectionHeader confidence="0.975006" genericHeader="method">
2 Explicit Sense Links
</sectionHeader>
<bodyText confidence="0.992453153846154">
The dictionary we are using in our research,
the Longman Dictionary of Contemporary English
(LDOCE), is a dictionary for learners of English as
a second language. As such, it provides a great
deal of information about word meanings in the
form of example sentences, usage notes, and gram-
mar codes. The Longman dictionary is also unique
among learner&apos;s dictionaries in that its definitions
are generally written using a controlled vocabulary
of approximately 2200 words. When exceptions oc-
cur they are indicated by means of a different font.
For example, consider the definition of the word
gravity:
</bodyText>
<listItem confidence="0.9984405">
• gravity n lb. worrying importance: He
doesn&apos;t understand the gravity of his illness -
see GRAVE2
• grave adj 2. important and needing attention
and (often) worrying: This is grave news — The
sick man&apos;s condition is grave
</listItem>
<bodyText confidence="0.99996046875">
These definitions serve to illustrate how words
can be synonymousl even though they have different
parts of speech. They also indicate how the Long-
man dictionary not only indicates that a word is a
synonym, but sometimes specifies the sense of that
word (indicated in this example by the superscript
following the word `GRAVE&apos;). This is extremely im-
portant because synonymy is not a relation that
holds between words, but between the senses of
words.
Unfortunately these explicit sense indications are
not always consistently provided. For example, the
definition of &apos;marbled&apos; provides an explicit indica-
tion of the appropriate sense of &apos;marble&apos; (the stone
instead of the child&apos;s toy), but this is not done within
the definition of &apos;marbles&apos;.
LDOCE also provides explicit indications of sense
relationships via usage notes. For example, the def-
inition for argument mentions that it derives from
both senses of argue - to quarrel (to have an ar-
gument), and to reason (to present an argument).
The notes also provide advice regarding similar look-
ing variants (e.g., the difference between distinct and
distinctive, or the fact that an attendant is not some-
one who attends a play, concert, or religious ser-
vice). Usage notes can also specify information that
is shared among some word meanings, but not others
(e.g., the note for venture mentions that both verb
and noun carry a connotation of risk, but this isn&apos;t
necessarily true for adventure).
Finally, LDOCE provides explicit connections be-
tween senses via deictic reference (links created by
</bodyText>
<footnote confidence="0.872305">
1We take two words to be synonymous if they have
the same or closely related meanings.
</footnote>
<page confidence="0.995584">
330
</page>
<bodyText confidence="0.999956060606061">
&apos;this&apos;, &apos;these&apos;, &apos;that&apos;, &apos;those&apos;, &apos;its&apos;, &apos;itself&apos;, and &apos;such
a/an&apos;). That is, some of the senses use these words
to refer to a previous sense (e.g., &apos;the fruit of this
tree&apos;, or &apos;a plant bearing these seeds&apos;). These rela-
tionships are important because they allow us to get
a better understanding of the nature of polysemy
(related word meanings). Most of the literature on
polysemy only provides anecdotal examples; it usu-
ally does not provide information about how to de-
termine whether word meanings are related, what
kind of relationships there are, or how frequently
they occur. The grouping of senses in a dictionary
is generally based on part of speech and etymology,
but part of speech is orthogonal to a semantic rela-
tionship (cf. Krovetz [3]), and word senses can be re-
lated etymologically, but be perceived as distinct at
the present time (e.g., the &apos;cardinal&apos; of a church and
&apos;cardinal&apos; numbers are etymologically related). By
examining deictic reference we gain a better under-
standing of senses that are truly related, and it also
helps us to understand how language can be used
creatively (i.e., how senses can be productively ex-
tended). Deictic references are also important in the
design of an algorithm for word-sense disambigua-
tion (e.g., exceptions to subcategorization).
The primary relations we have identified so
far are: substance/product (tree:fruit or wood,
plant:flower or seeds), substance/color (jade, amber,
rust), object/shape (pyramid, globe, lozenge), ani-
mal/food (chicken, lamb, tuna), count-noun/mass-
noun,2 language/people (English, Spanish, Dutch),
animal/skin or fur (crocodile, beaver, rabbit), and
music/dance (waltz, conga, tango).3
</bodyText>
<sectionHeader confidence="0.999064" genericHeader="method">
3 Zero-Affix Morphology
</sectionHeader>
<bodyText confidence="0.999955625">
Deictic reference provides us with different types of
relationships within the same part of speech. We can
also get related senses that differ in part of speech,
and these are referred to as instances of zero-affix
morphology or functional shift. The Longman dic-
tionary explicitly indicates some of these relation-
ships by homographs that have more than one part
of speech. It usually provides an indication of the
relationship by a leading parenthesized expression.
For example, the word bay is defined as N,ADJ, and
the definition reads &apos;(a horse whose color is) reddish-
brown&apos;. However, out of the 41122 homographs de-
fined, there are only 695 that have more than one
part of speech. Another way in which LDOCE pro-
vides these links is by an explicit sense reference for
a word outside the controlled vocabulary; the def-
</bodyText>
<footnote confidence="0.985539857142857">
2These may or may not be related; consider &apos;com-
puter vision&apos; vs. &apos;visions of computers&apos;. The related
senses are usually indicated by the defining formula: &apos;an
example of this&apos;.
3The related senses are sometimes merged into one;
for example, the definition of foxtrot is `(a piece of music
for) a type of formal dance...&apos;
</footnote>
<bodyText confidence="0.997738474576272">
inition of anchor (v) reads: `to lower an anchorl
(1) to keep (a ship) from moving&apos;. This indicates a
reference to sense 1 of the first homograph.
Zero-affix morphology is also present implicitly,
and we conducted an experiment to try to identify
instances of it using a probabilistic tagger [2]. The
hypothesis is that if the word that&apos;s being defined
(the definiendum) occurs within the text of its own
definition, but occurs with a different part of speech,
then it will be an instance of zero-affix morphology.
The question is: How do we tell whether or not we
have an instance of zero-affix morphology when there
is no explicit indication of a suffix? Part of the an-
swer is to rely on subjective judgment, but we can
also support these judgments by making an anal-
ogy with derivational morphology. For example, the
word wad is defined as `to make a wad of&apos;. That is,
the noun bears the semantic relation of formation to
the verb that defines it. This is similar to the effect
that the morpheme -ize has on the noun union. in
order to make the verb unionize (cf. Marchand [5]).
The experiment not only gives us insight into se-
mantic relatedness across part of speech, it also en-
abled us to determine the effectiveness of tagging.
We initially examined the results of the tagger on
all words starting with the letter &apos;W&apos;; this letter was
chosen because it provided a sufficient number of
words for examination, but wasn&apos;t so small as to be
trivial. There were a total of 1141 words that were
processed, which amounted to 1309 homographs and
2471 word senses; of these senses, 209 were identified
by the tagger as containing the definiendum with a
different part of speech. We analyzed these instances
and the result was that only 51 of the 209 instances
were found to be correct (i.e., actual zero-morphs).
The instances that are indicated as correct are
currently based on our subjective judgment; we are
in the process of examining them to identify the type
of semantic relation and any analog to a derivational
suffix. The instances that were not found to be cor-
rect (76 percent of the total) were due to incorrect
tagging; that is, we had a large number of false pos-
itives because the tagger did not correctly identify
the part of speech. We were surprised that the num-
ber of incorrect tags was so high given the perfor-
mance figures cited in the literature (more than a
90 percent accuracy rate). However, the figures re-
ported in the literature were based on word tokens,
and 60 percent of all word tokens have only one part
of speech to begin with. We feel that the perfor-
mance figures should be supplemented with the tag-
ger&apos;s performance on word types as well. Most word
types are rare, and the stochastic methods do not
perform as well on them because they do not have
sufficient information. Church has plans for improv-
ing the smoothing algorithms used in his tagger, and
this would help on these low frequency words. In
addition, we conducted a failure analysis and it in-
dicated that 91% the errors occurred in idiomatic
</bodyText>
<page confidence="0.997134">
331
</page>
<bodyText confidence="0.9998095">
expressions (45 instances) or example sentences (98
instances). We therefore eliminated these from fur-
ther processing and tagged the rest of the dictionary.
We are still in the process of analyzing these results.
</bodyText>
<sectionHeader confidence="0.987959" genericHeader="method">
4 Derivational Morphology
</sectionHeader>
<bodyText confidence="0.999898653846154">
Word collocation is one method that has been pro-
posed as a means for identifying word meanings.
The basic idea is to take two words in context, and
find the definitions that have the most words in com-
mon. This strategy was tried by Lesk using the Ox-
ford Advanced Learner&apos;s Dictionary [4]. For exam-
ple, the word &apos;pine&apos; can have two senses: a tree,
or sadness (as in &apos;pine away&apos;), and the word &apos;cone&apos;
may be a geometric structure, or a fruit of a tree.
Lesk&apos;s program computes the overlap between the
senses of &apos;pine&apos; and &apos;cone&apos;, and finds that the senses
meaning &apos;tree&apos; and &apos;fruit of a tree&apos; have the most
words in common. Lesk gives a success rate of fifty
to seventy percent in disambiguating the words over
a small collection of text. Later work by Becker on
the New OED indicated that Lesk&apos;s algorithm did
not perform as well as expected [1].
The difficulty with the word overlap approach is
that a wide range of vocabulary can be used in defin-
ing a word&apos;s meaning. It is possible that we will be
more likely to have an overlap in a dictionary with
a restricted defining vocabulary. When the senses
to be matched are further restricted to be morpho-
logical variants, the approach seems to work very
well. For example, consider the definitions of the
word &apos;appreciate&apos; and &apos;appreciation&apos;:
</bodyText>
<listItem confidence="0.9890344375">
• appreciate
1. to be thankful or grateful for
2. to understand and enjoy the good qualities
of
3. to understand fully
4. to understand the high worth of
5. (of property, possessions, etc.) to increase
in value
• appreciation
1. judgment, as of the quality, worth, or facts
of something
2. a written account of the worth of something
3. understanding of the qualities or worth of
something
4. grateful feelings
5. rise in value, esp. of land or possessions
</listItem>
<bodyText confidence="0.99998816">
The word overlap approach pairs up sense 1 with
sense 4 (grateful), sense 2 with sense 3 (understand;
qualities), sense 3 with sense 3 (understand), sense 4
with sense 1 (worth), and sense 5 with sense 5 (value;
possessions). The matcher we are using ignores
closed class words, and makes use of a simple mor-
phological analyzer (for inflectional morphology). It
ignores words found in example sentences (prelim-
inary experiments indicated that this didn&apos;t help
and sometimes made matches worse), and it also
ignores typographical codes and usage labels (for-
mal/informal, poetic, literary, etc.). It also doesn&apos;t
try to make matches between word senses that are
idiomatic (these are identified by font codes). We
are currently in the process of determining the effec-
tiveness of the approach. The experiment involves
comparing the morphological variations for a set of
queries used in an information retrieval test collec-
tion. We have manually identified all variations of
the words in the queries as well as the root forms.
Those variants that appear in LDOCE will be com-
pared against all root forms and the result will be
examined to see how well the overlap method was
able to identify the correct sense of the variant with
the correct sense of the root.
</bodyText>
<sectionHeader confidence="0.999448" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999971285714286">
The purpose of this work is to gain a better under-
standing of the relationships between word mean-
ings, and to help in development of an algorithm for
word sense disambiguation. Our approach is based
on treating the information associated with dictio-
nary senses (part of speech, subcategorization, sub-
ject area codes, etc.) as multiple sources of evidence
(cf. Krovetz [3]). This process is fundamentally a
divisive one, and each of the sources of evidence has
exceptions (i.e., instances in which senses are related
in spite of being separated by part of speech, sub-
categorization, or morphology). Identifying the rela-
tionships we have described will help us to determine
these exceptions.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989015">
[1] Becker B., &amp;quot;Sense Disambiguation using the
New Oxford English Dictionary&amp;quot;, Masters The-
sis, University of Waterloo, 1989.
[2] Church K., &amp;quot;A Stochastic Parts Program and
Noun Phrase Parser for Unrestricted Text&amp;quot;,
in Proceedings of the 2nd Conference on Ap-
plied Natural Language Processing, pp. 136-143,
1988.
[3] Krovetz R., &amp;quot;Lexical Acquisition and Informa-
tion Retrieval&amp;quot;, in Lexical Acquisition: Build-
ing the Lexicon Using On-Line Resources, U.
Zernik (ed), pp. 45-64, 1991.
[4] Lesk M., &amp;quot;Automatic Sense Disambiguation Us-
ing Machine Readable Dictionaries: How to tell
a Pine Cone from an Ice Cream Cone&amp;quot;, Proceed-
ings of SIGDOC, pp. 24-26, 1986.
[5] Marchand H, &amp;quot;On a Question of Contrary Anal-
ysis with Derivational Connected but Mor-
phologically Uncharacterized Words&amp;quot;, English
Studies, 44, pp. 176-187, 1963
</reference>
<page confidence="0.998257">
332
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894058">
<title confidence="0.999623">Sense-Linking in a Machine Readable Dictionary</title>
<author confidence="0.999984">Robert Krovetz</author>
<affiliation confidence="0.999966">Department of Computer Science</affiliation>
<address confidence="0.902324">University of Massachusetts, Amherst, MA 01003</address>
<abstract confidence="0.999266833333333">Dictionaries contain a rich set of relationships between their senses, but often these relationships are only implicit. We report on our experiments to automatically identify links between the senses in a machinereadable dictionary. In particular, we automatically identify instances of zero-affix morphology, and use that information to find specific linkages between senses. This work has provided insight into the performance of a stochastic tagger.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Becker</author>
</authors>
<title>Sense Disambiguation using the New Oxford English Dictionary&amp;quot;, Masters Thesis,</title>
<date>1989</date>
<institution>University of Waterloo,</institution>
<contexts>
<context position="11612" citStr="[1]" startWordPosition="1920" endWordPosition="1920">g the Oxford Advanced Learner&apos;s Dictionary [4]. For example, the word &apos;pine&apos; can have two senses: a tree, or sadness (as in &apos;pine away&apos;), and the word &apos;cone&apos; may be a geometric structure, or a fruit of a tree. Lesk&apos;s program computes the overlap between the senses of &apos;pine&apos; and &apos;cone&apos;, and finds that the senses meaning &apos;tree&apos; and &apos;fruit of a tree&apos; have the most words in common. Lesk gives a success rate of fifty to seventy percent in disambiguating the words over a small collection of text. Later work by Becker on the New OED indicated that Lesk&apos;s algorithm did not perform as well as expected [1]. The difficulty with the word overlap approach is that a wide range of vocabulary can be used in defining a word&apos;s meaning. It is possible that we will be more likely to have an overlap in a dictionary with a restricted defining vocabulary. When the senses to be matched are further restricted to be morphological variants, the approach seems to work very well. For example, consider the definitions of the word &apos;appreciate&apos; and &apos;appreciation&apos;: • appreciate 1. to be thankful or grateful for 2. to understand and enjoy the good qualities of 3. to understand fully 4. to understand the high worth of </context>
</contexts>
<marker>[1]</marker>
<rawString>Becker B., &amp;quot;Sense Disambiguation using the New Oxford English Dictionary&amp;quot;, Masters Thesis, University of Waterloo, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text&amp;quot;,</title>
<date>1988</date>
<booktitle>in Proceedings of the 2nd Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="7788" citStr="[2]" startWordPosition="1240" endWordPosition="1240"> may not be related; consider &apos;computer vision&apos; vs. &apos;visions of computers&apos;. The related senses are usually indicated by the defining formula: &apos;an example of this&apos;. 3The related senses are sometimes merged into one; for example, the definition of foxtrot is `(a piece of music for) a type of formal dance...&apos; inition of anchor (v) reads: `to lower an anchorl (1) to keep (a ship) from moving&apos;. This indicates a reference to sense 1 of the first homograph. Zero-affix morphology is also present implicitly, and we conducted an experiment to try to identify instances of it using a probabilistic tagger [2]. The hypothesis is that if the word that&apos;s being defined (the definiendum) occurs within the text of its own definition, but occurs with a different part of speech, then it will be an instance of zero-affix morphology. The question is: How do we tell whether or not we have an instance of zero-affix morphology when there is no explicit indication of a suffix? Part of the answer is to rely on subjective judgment, but we can also support these judgments by making an analogy with derivational morphology. For example, the word wad is defined as `to make a wad of&apos;. That is, the noun bears the seman</context>
</contexts>
<marker>[2]</marker>
<rawString>Church K., &amp;quot;A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text&amp;quot;, in Proceedings of the 2nd Conference on Applied Natural Language Processing, pp. 136-143, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
</authors>
<title>Lexical Acquisition and Information Retrieval&amp;quot;,</title>
<date>1991</date>
<booktitle>in Lexical Acquisition: Building the Lexicon Using On-Line Resources, U. Zernik (ed),</booktitle>
<pages>45--64</pages>
<contexts>
<context position="1512" citStr="[3]" startWordPosition="229" endWordPosition="229">overed through outside mechanisms. This paper describes our efforts at identifying these links. The purpose of the research is to obtain a better understanding of the relationships between word meanings, and to provide data for our work on wordsense disambiguation and information retrieval. Our hypothesis is that retrieving documents on the basis of word senses (instead of words) will result in better performance. Our approach is to treat the information associated with dictionary senses (part of speech, subcategorization, subject area codes, etc.) as multiple sources of evidence (cf. Krovetz [3]). This process is fundamentally a divisive one, and each of the sources of evidence has exceptions (i.e., instances in which senses are related in spite of being separated by part of speech, subcategorization, or morphology). Identifying related senses will help us to test the hypothesis that unrelated meanings will be more effective at separating relevant from nonrelevant documents than meanings which are related. We will first discuss some of the explicit indications of sense relationships as found in usage notes and deictic references. We will then describe our efforts at uncovering the im</context>
<context position="5408" citStr="[3]" startWordPosition="864" endWordPosition="864">., &apos;the fruit of this tree&apos;, or &apos;a plant bearing these seeds&apos;). These relationships are important because they allow us to get a better understanding of the nature of polysemy (related word meanings). Most of the literature on polysemy only provides anecdotal examples; it usually does not provide information about how to determine whether word meanings are related, what kind of relationships there are, or how frequently they occur. The grouping of senses in a dictionary is generally based on part of speech and etymology, but part of speech is orthogonal to a semantic relationship (cf. Krovetz [3]), and word senses can be related etymologically, but be perceived as distinct at the present time (e.g., the &apos;cardinal&apos; of a church and &apos;cardinal&apos; numbers are etymologically related). By examining deictic reference we gain a better understanding of senses that are truly related, and it also helps us to understand how language can be used creatively (i.e., how senses can be productively extended). Deictic references are also important in the design of an algorithm for word-sense disambiguation (e.g., exceptions to subcategorization). The primary relations we have identified so far are: substan</context>
</contexts>
<marker>[3]</marker>
<rawString>Krovetz R., &amp;quot;Lexical Acquisition and Information Retrieval&amp;quot;, in Lexical Acquisition: Building the Lexicon Using On-Line Resources, U. Zernik (ed), pp. 45-64, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to tell a Pine Cone from an Ice Cream Cone&amp;quot;,</title>
<date>1986</date>
<booktitle>Proceedings of SIGDOC,</booktitle>
<pages>24--26</pages>
<contexts>
<context position="11055" citStr="[4]" startWordPosition="1817" endWordPosition="1817">lure analysis and it indicated that 91% the errors occurred in idiomatic 331 expressions (45 instances) or example sentences (98 instances). We therefore eliminated these from further processing and tagged the rest of the dictionary. We are still in the process of analyzing these results. 4 Derivational Morphology Word collocation is one method that has been proposed as a means for identifying word meanings. The basic idea is to take two words in context, and find the definitions that have the most words in common. This strategy was tried by Lesk using the Oxford Advanced Learner&apos;s Dictionary [4]. For example, the word &apos;pine&apos; can have two senses: a tree, or sadness (as in &apos;pine away&apos;), and the word &apos;cone&apos; may be a geometric structure, or a fruit of a tree. Lesk&apos;s program computes the overlap between the senses of &apos;pine&apos; and &apos;cone&apos;, and finds that the senses meaning &apos;tree&apos; and &apos;fruit of a tree&apos; have the most words in common. Lesk gives a success rate of fifty to seventy percent in disambiguating the words over a small collection of text. Later work by Becker on the New OED indicated that Lesk&apos;s algorithm did not perform as well as expected [1]. The difficulty with the word overlap appr</context>
</contexts>
<marker>[4]</marker>
<rawString>Lesk M., &amp;quot;Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to tell a Pine Cone from an Ice Cream Cone&amp;quot;, Proceedings of SIGDOC, pp. 24-26, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Marchand</author>
</authors>
<title>On a Question of Contrary Analysis with Derivational Connected but Morphologically Uncharacterized Words&amp;quot;,</title>
<date>1963</date>
<journal>English Studies,</journal>
<volume>44</volume>
<pages>176--187</pages>
<contexts>
<context position="8571" citStr="[5]" startWordPosition="1383" endWordPosition="1383">e an instance of zero-affix morphology. The question is: How do we tell whether or not we have an instance of zero-affix morphology when there is no explicit indication of a suffix? Part of the answer is to rely on subjective judgment, but we can also support these judgments by making an analogy with derivational morphology. For example, the word wad is defined as `to make a wad of&apos;. That is, the noun bears the semantic relation of formation to the verb that defines it. This is similar to the effect that the morpheme -ize has on the noun union. in order to make the verb unionize (cf. Marchand [5]). The experiment not only gives us insight into semantic relatedness across part of speech, it also enabled us to determine the effectiveness of tagging. We initially examined the results of the tagger on all words starting with the letter &apos;W&apos;; this letter was chosen because it provided a sufficient number of words for examination, but wasn&apos;t so small as to be trivial. There were a total of 1141 words that were processed, which amounted to 1309 homographs and 2471 word senses; of these senses, 209 were identified by the tagger as containing the definiendum with a different part of speech. We </context>
</contexts>
<marker>[5]</marker>
<rawString>Marchand H, &amp;quot;On a Question of Contrary Analysis with Derivational Connected but Morphologically Uncharacterized Words&amp;quot;, English Studies, 44, pp. 176-187, 1963</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>