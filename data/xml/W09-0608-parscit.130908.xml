<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000241">
<title confidence="0.981205">
Class-Based Ordering of Prenominal ModiÞers
</title>
<author confidence="0.952794">
Margaret Mitchell
</author>
<affiliation confidence="0.927693">
Center for Spoken Language Understanding
</affiliation>
<address confidence="0.840687">
Portland, Oregon, U.S.A
</address>
<email confidence="0.998652">
itallow@cslu.ogi.edu
</email>
<sectionHeader confidence="0.997386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975545454545">
This paper introduces a class-based ap-
proach to ordering prenominal modifiers.
Modifiers are grouped into broad classes
based on where they tend to occur prenom-
inally, and a framework is developed to or-
der sets of modifiers based on their classes.
This system is developed to generate sev-
eral orderings for modifiers with more
flexible positional constraints, and lends
itself to bootstrapping for the classification
of previously unseen modifiers.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980642857143">
Ordering prenominal modifiers is a necessary task
in the generation of natural language. For a system
to effectively generate fluent utterances, the sys-
tem must determine the proper order for any given
set of modifiers. The order of modifiers before a
noun affects the meaning and fluency of generated
utterances. Determining ways to order modifiers
prenominally has been an area of considerable re-
search (cf. Shaw and Hatzivassiloglou, 1999; Mal-
ouf, 2000).
In this paper, we establish and evaluate a classi-
fication system that can be used to order prenom-
inal modifiers automatically. This may be im-
plemented in a surface realization component of
a natural language generation system, or may be
used to help specify the ordering of properties that
feed into a referring expression generation algo-
rithm. Predictions of prenominal modifier order-
ing based on these classes are shown to be robust
and accurate.
The work here diverges from the approaches
commonly employed in modifier classification by
assuming no underlying relationship between se-
mantics and prenominal order or morphology and
prenominal order. The approach instead relies
on generalizing empirical evidence from a corpus.
This allows the system to be robust and portable to
a variety of applications, without precluding any
underlying linguistic constraints.
In the next section, we discuss prior work on
this topic, and address the differences in our ap-
proach. Section 3 discusses the relationship be-
tween modifier ordering and referring expression
generation, a principal component of natural lan-
guage generation. Section 4 describes the ideas
behind the modifier classification system. Sec-
tions 5 and 6 present the materials and method-
ology of the current study, with a discussion of the
corpus involved and the basic modules used in the
process. In Section 7 we discuss the results of our
study. Finally, in Section 8, we outline areas for
improvement and possible future work.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.994060681818182">
Discerning the rules governing the ordering of ad-
jectives has been an area of research for quite some
time (see, for example, Panini’s work on San-
skrit grammar ca. 350 BCE). Most approaches as-
sume an underlying relationship between seman-
tics and prenominal position (cf. Whorf, 1945;
Ziff, 1960; Bever, 1970; Danks and Glucksberg,
1971). These approaches can be characterized as
predicting modifier order based on degrees of se-
mantic closeness to the noun. This follows what
is known as Behaghel’s First Law (Behaghel,
1930):
Word groups: What belongs together
mentally is placed close together syntac-
tically.
(Clark and Clark, 1977: 545)
However, there is disagreement on the exact
qualities that affect position. These theories are
also difficult to implement in a generation system,
as they require determining the semantic proper-
ties of each modifier used, relative to the context
in which it occurs. If a modifier classification
</bodyText>
<note confidence="0.9157905">
Proceedings of the 12th European Workshop on Natural Language Generation, pages 50–57,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.996427">
50
</page>
<bodyText confidence="0.997712781609195">
scheme is to be implemented, it should be able to
create a variety of natural, unmarked orders; be ro-
bust enough to handle a wide variety of modifiers;
and be flexible enough to allow different natural
orderings.
Shaw and Hatzivassiloglou (1999) examine this
problem, and develop ways to order all prenominal
modifier types. This includes adjectives as well
as nouns, such as “baseball” in “baseball field”;
gerunds, such as “running” in “running man”; and
past participles, such as “heated” in “heated de-
bate”. The authors devise three different meth-
ods that may be implemented in a generation sys-
tem to order these kinds of prenominal modifiers.
These are the direct evidence method, the transi-
tivity method, and the clustering method.
Briefly, given prenominal modifiers a and b in
a training corpus, the direct evidence method uti-
lizes probabilistic reasoning to determine whether
the frequency count of the ordered sequence
&lt;a,b&gt; or &lt;b,a&gt; is stronger. The transitiv-
ity method makes inferences about unseen order-
ings among prenominal modifiers; given a third
prenominal modifier c, where a precedes b and b
precedes c, the authors can conclude that a pre-
cedes c. In the clustering method, an order sim-
ilarity metric is used to group modifiers together
that share a similar relative order to other modi-
fiers.
Shaw and Hatzivassiloglou achieve their high-
est prediction accuracy of 90.67% using their tran-
sitivity technique on prenominal modifiers from
a medical corpus. However, with their system
trained on the medical corpus and then tested
on the Wall Street Journal corpus (Marcus et al.,
1993), they achieve an overall prediction accuracy
of only 54%. The authors conclude that prenomi-
nal modifier ordering is domain-specific.
Malouf (2000) continues this work, determin-
ing the order for sequences of prenominal adjec-
tives by examining several different statistical and
machine learning techniques. These achieve good
results, ranging from 78.28% to 89.73% accuracy.
Malouf achieves the best results by combining
memory-based learning and positional probabil-
ity, which reaches 91.85% accuracy at predicting
the prenominal adjective orderings in the first 100
million tokens of the BNC. However, this analysis
does not extend to other kinds of prenominal mod-
ifiers. The model is also not tested on a different
domain.
The approach to modifier classification taken
here is similar to the clustering method discussed
by Shaw and Hatzivassiloglou. Modifiers are
grouped into classes based on where they occur
prenominally. This approach differs, however, in
how classes are assigned. In our approach, modi-
fiers are grouped into classes based on the frequen-
cies with which they occur in different prenominal
positions. Classes are built based not on where
modifiers are positioned in respect to other mod-
ifiers, but on where modifiers are positioned in
general. Grouping modifiers into classes based on
prenominal positions mitigates the problems noted
by Shaw and Hatzivassiloglou that ordering pre-
dictions cannot be made (1) when both a and b be-
long to the same class, (2) when either a or b are
not associated to a class that can be ordered with
respect to the other, and (3) when the evidence for
one class preceding the other is equally strong for
both classes.
This approach allows modifiers with strong
positional preferences to be in a class separate
from modifiers with weaker positional prefer-
ences. This also ensures that any prenominal mod-
ifiers a and b seen in the training corpus can be
ordered, regardless of which particular modifiers
they appear with and whether they occur together
in the training data at all. This approach also has
the added benefit of developing modifier classes
that are usable across many different domains.
Further, this method is conceptually simple and
easy to implement. Although this approach is less
context-sensitive than earlier work, we find that it
is highly accurate, with comparable token preci-
sion. We discuss this in greater detail in Sections
6 and 7.
</bodyText>
<sectionHeader confidence="0.957025" genericHeader="method">
3 The Problem of Ordering Prenominal
</sectionHeader>
<subsectionHeader confidence="0.54839">
ModiÞers
</subsectionHeader>
<bodyText confidence="0.954999333333333">
Generating referring expressions in part requires
generating the adjectives, verbs, and nouns that
modify head nouns. In order for these expressions
to clearly convey the intended referent, the mod-
ifiers must appear in an order that sounds natural
and mimics human language use.
For example, consider the alternation given in
Figure 1. Some combinations of modifiers be-
fore a noun are more marked than others, although
all are strictly speaking grammatical. This speaks
to the need for a broad modifier classes to order
prenominal modifiers, where individual modifiers
</bodyText>
<page confidence="0.99438">
51
</page>
<listItem confidence="0.994383333333333">
(1) big beautiful white wooden house 1. Not all modifiers have equally stringent or-
(2) ?white wooden beautiful big house dering preferences.
(3) comfortable red chair
(4) ?red comfortable chair
(5) big rectangular green Chinese silk carpet
(6) ?Chinese big silk green rectangular carpet
</listItem>
<figureCaption confidence="0.99646">
Figure 1: Grammatical Modifier Alternations
(Vendler, 1968: 122)
</figureCaption>
<bodyText confidence="0.98093325">
may be ordered separately as required by particu-
lar contexts.
Along these lines, almost all referring expres-
sion generation algorithms rely on the availability
of a predefined ordering or weighting of properties
(Dale and Reiter, 1995; van Deemter, 2002; Krah-
mer et al., 2003). This requires that for every refer-
ent, an ordered or weighted listing of all the prop-
erties that can apply to it must be created before
referring expression generation begins. In these
models, the order or weights of the input proper-
ties map to the order of the output modifiers.
However, the method used to determine the or-
dering or weighting of properties is an open is-
sue. The difficulty with capturing the ordering of
properties and their corresponding modifiers stems
from the problem of data sparsity. In the example
in Figure 1, the modifier silk may be rare enough in
any corpus that finding it in combination with an-
other modifier, in order to create a generalization
about its ordering constraints, is nearly impossi-
ble. Malouf (2000) examined the first million sen-
tences of the British National Corpus and found
only one sequence of adjectives for every twenty
sentences. With sequences of adjectives occurring
so rarely, the chances of finding information on
any particular sequence is small. The data is just
too sparse.
</bodyText>
<sectionHeader confidence="0.942049" genericHeader="method">
4 Towards a Solution
</sectionHeader>
<bodyText confidence="0.999953">
To create a flexible system capable of predicting a
wide variety of orderings, we used several corpora
to build broad modifier classes. Modifiers are clas-
sified by where they tend to appear prenominally,
and ordering constraints between the classes de-
termine the order for any set of modifiers. This
system incorporates three main ideas:
</bodyText>
<listItem confidence="0.99162725">
2. Modifier ordering preferences can be learned
empirically.
3. Modifiers can be grouped into classes indica-
tive of their ordering preferences.
</listItem>
<bodyText confidence="0.996645">
The classification scheme therefore allows rigid
as well as more loose orders (compare big red
ball and ?red big ball with white ßoppy hat and
ßoppy white hat). It is not based on any mapping
between position and semantics, morphology, or
phonology, but does not exclude any such rela-
tionship in the classification: This classification
scheme builds on what there is direct evidence for,
independent of why each modifier appears where
it does.
To create our model, all simplex noun phrases
(NPs) are extracted from parsed corpora. A sim-
plex NP is defined as a maximal noun phrase
that includes premodifiers such as determiners and
possessives but no post-nominal constituents such
as prepositional phrases or relative clauses (Shaw
and Hatzivassiloglou, 1999: 137). From these
simplex NPs, we extract all those headed by a
noun and preceded by only prenominal modifiers.
This includes modifiers tagged as adjectives (JJ),
nouns (NN), gerunds (VBG), and past participles
(VBN). The counts and relative positions of each
modifier are stored, and these are converted into
position probabilities in vector file format. Modi-
fiers are classified based on the positions in which
they have the highest probabilities of occurring.
Examples of the intermediary files in this pro-
cess are given in Tables 1 and 2. Table 1 lists
modifiers followed by their frequency counts in
each prenominal position. Table 2 lists these mod-
ifiers associated to their classes, with the propor-
tions that determine the class.
wealthy four 2 three 2 two 3 one 1
red four 13 three 35 two 35 one 21
golden four 1 three 5 two 5 one 3
strongest four 5 three 5 two 5 one 5
</bodyText>
<tableCaption confidence="0.8821665">
Table 1: Example Modifier Classification Interme-
diate File: Step 3
</tableCaption>
<sectionHeader confidence="0.998176" genericHeader="method">
5 Materials
</sectionHeader>
<bodyText confidence="0.9952945">
To create the training and test data, we utilize the
Penn Treebank-3 (Marcus et al., 1999) releases of
</bodyText>
<page confidence="0.990697">
52
</page>
<bodyText confidence="0.9751866">
wealthy two two 0.38
red two three three 0.34 two 0.34
golden one two three three 0.33 two 0.33 one 0.29
strongest two three four four 0.33 three 0.33 two 0.33
Table 2: Example Modifier Classification Interme-
diate File: Step 4
the parsed Wall Street Journal corpus, the parsed
Brown corpus, and the parsed Switchboard cor-
pus. The Wall Street Journal corpus is a selec-
tion of over one million words collected from the
Wall Street Journal over a three-year period. The
Brown corpus is over one million words of prose
written in various genres, including mystery, hu-
mor, and popular lore, collected from newspapers
and periodicals in 1961. The Switchboard corpus
is over one million words of spontaneous speech
collected from thousands of five-minute telephone
conversations. Several programs were constructed
to analyze the information provided by these data.
The details of each module are outlined below.
</bodyText>
<subsectionHeader confidence="0.982539">
5.1 Code Modules
</subsectionHeader>
<bodyText confidence="0.95290588">
The following five components were developed (in
Python) for this project.
Modifier Extractor – This program takes as in-
put a parsed corpus, and outputs a list of all
occurrences of all noun phrases in that cor-
pus.
input: Parsed Corpus
output: List of simplex NPs
Modifier Organizer – This program takes as in-
put a list of simplex NPs and filters out words
that appear prenominally and are occasion-
ally mistagged as modifiers. A list of these
filtered words is available in Table 3. This
returns a vector with frequency counts for
all positions in which each observed modifier
occurs.
input: Modifier-rich noun phrases and their
frequencies
output: Vector file with distributional infor-
mation for each modifier position
Modifier Classifier – This program takes as in-
put a vector file with distributional informa-
tion for each modifier’s position, and from
this builds our model by determining the clas-
sification for each modifier.
</bodyText>
<table confidence="0.949800285714286">
about behind on
above in under
after inside out
outside up over
down like past
near through off
the a
</table>
<tableCaption confidence="0.999541">
Table 3: Filtered Mistagged Words
</tableCaption>
<bodyText confidence="0.967935666666667">
input: Vector file with distributional infor-
mation for each modifier position
output: Ordering model: File with each
modifier associated to a class
Prenominal Modifier Ordering Predictor –
This program takes as input two files: an or-
dering model and a list of simplex NPs (for
testing). The program then uses the model
to assign a class to each modifier seen in the
testing data, and predicts the ordering for all
the modifiers that appear prenominally. A
discussion of the ordering decisions is given
below. This program then compares its pre-
dicted ordering of modifiers prenominally to
the observed ordering of modifiers prenom-
inally. It returns precision and recall values
for its predictions.
input: Vector file with each modifier associ-
ated to a class, list of simplex NPs
output: Precision and recall for modifier or-
dering predictions
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="method">
6 Method
</sectionHeader>
<subsectionHeader confidence="0.999795">
6.1 Classification Scheme
</subsectionHeader>
<bodyText confidence="0.9931338">
To develop modifier classes and create our model,
we assume four primary modifier positions. This
assumption is based on the idea that people rarely
produce more than four modifiers before a noun.
This assumption covers 99.70% of our data (see
Table 5). The longest noun phrases for this ex-
periment are therefore those with five words: Four
modifiers followed by a noun.
small smiling white fuzzy bunny
four three two one
</bodyText>
<figureCaption confidence="0.944855">
Figure 2: Example Simplex NP with Prenominal
Positions
</figureCaption>
<bodyText confidence="0.979061">
Each modifier’s class is determined by counting
the frequency of each modifier in each position.
</bodyText>
<page confidence="0.994614">
53
</page>
<table confidence="0.9828388">
Class 1: one Class 6: two-three
Class 2: two Class 7: three-four
Class 3: three Class 8: one-two-three
Class 4: four Class 9: two-three-four
Class 5: one-two
</table>
<tableCaption confidence="0.996934">
Table 4: Modifier Classes
</tableCaption>
<bodyText confidence="0.998883204545455">
This is turned into a probability over all four posi-
tions. All position probabilities ≤ 0.25 (baseline)
are discarded. Those positions that remain deter-
mine the modifier class.
To calculate modifier position for each phrase,
counts were incremented for all feasible positions.
This is a way of sharing evidence among sev-
eral positions. For example, in the phrase clean
wooden spoon, the adjective clean was counted as
occurring in positions two, three, and four, while
the adjective wooden was counted as occurring in
positions one, two, and three.
The classification that emerges after applying
this technique to a large body of data gives rise
to the broad positional preferences of each modi-
fier. In this way, a modifier with a strict positional
preference can emerge as occurring in just one po-
sition; a modifier with a less strict preference can
emerge as occurring in three.
The final class for each modifier is dependent
on the positions the modifier appears in more than
25% of the time. Since there are four possible
positions, 25% is the baseline: A single modifier
preceding a noun has equal probability of being in
each of the four positions. There are nine derivable
modifier classes in this approach, listed in Table 4.
A diagram of how a modifier is associated to a
class is shown in Figure 3. In this example, red
appears in several simplex NPs. In each sequence,
we associate red to its possible positions within
the four prenominal slots. We see that red occurs
in positions one, two and three; two, three, and
four; and three and four. With only this data, red
has a 12.5% probability of being in position one; a
25% probability of being in position two; a 37.5%
probability of being in position three; and a 25%
probability of being in position four. It can there-
fore be classified as belonging to Class 3, the class
for modifiers that tend to occur in position three.
This kind of classification allows the system to
be flexible to the idea that some modifiers exhibit
stringent ordering constraints, while others have
more loose constraints. Some modifiers may al-
ways appear immediately before the noun, while
</bodyText>
<figureCaption confidence="0.8714175">
Figure 3: Constructing the Class of the Modifier
red
</figureCaption>
<bodyText confidence="0.97031725">
others may generally appear close to or far from
the noun. By counting the occurrences of each
modifier in each position, classes for all observed
modifiers may be derived.
The frequencies of all extracted groupings of
prenominal modifiers used to build our model are
shown in Table 5. The frequencies of the extracted
classes are shown in Table 6.
</bodyText>
<subsectionHeader confidence="0.566552">
Mods Count Percentage
</subsectionHeader>
<tableCaption confidence="0.869693">
Table 5: Frequency of Prenominal Modifier
</tableCaption>
<table confidence="0.996575636363636">
Amounts
Class Position Count Percentage
1 one 18 0.23%
2 two 46 0.68%
3 three 62 0.92%
4 four 21 0.31%
5 one-two 329 4.88%
6 two-three 1136 16.86%
7 three-four 261 3.87%
8 one-two-three 2671 39.65%
9 two-three-four 2193 32.55%
</table>
<tableCaption confidence="0.997384">
Table 6: Modifier Class Distribution
</tableCaption>
<bodyText confidence="0.9993888">
Modifiers of Class 8, the class for modifiers that
show a general preference to be closer to the head
noun but do not have a strict positional preference,
make up the largest portion of the data. An exam-
ple of a modifier from Class 8 is golden. The next
</bodyText>
<figure confidence="0.980428346153846">
15856
1770
155
21
1
88.90%
9.92%
0.87%
0.12%
.03%
2
3
4
5
6
54
Class Position Generated Before Class
1 one 2 3 4 5 6 7 8 9
2 two 3 4 6 7 9
3 three 4 7
4 four
5 one-two 2 3 4 6 7 8 9
6 two-three 3 4 7 9
7 three-four 4
8 one-two-three 4 6 7 9
9 two-three-four 4 7
</figure>
<tableCaption confidence="0.802162">
Table 7: Proposed Modifier Ordering
</tableCaption>
<bodyText confidence="0.999961535714286">
largest portion of the data are modifiers of Class 9,
the class for modifiers that show a general prefer-
ence to be farther from the head noun. An exam-
ple of a modifier from Class 9 is strongest. With
these defined, strongest golden arch is predicted
to sound grammatical and unmarked, but ?golden
strongest arch is not.
Some expected patterns also emerge in these
groupings. For example, green, yellow, red and
other colors are determined to be Class 6. Ex-
plained and unexplained are both determined to be
Class 5, and big and small are both determined to
be Class 9.
Once classified, modifiers may be ordered ac-
cording to their classes. The proposed ordering
constraints for these classes are listed in Table 7.
Note that using this classification scheme, the or-
dering of modifiers that belong to the same class
is not predicted. This seems to be reflective of nat-
ural language use. For example, both wealthy and
performing are predicted to be Class 2. This seems
reasonable; whether wealthy performing man or
performing wealthy man is a more natural order-
ing of prenominal modifiers is at least debatable.
The freedom of intra-class positioning allows for
some randomization in the generation of prenom-
inal modifiers, where other factors may be used to
determine the final ordering.
</bodyText>
<subsectionHeader confidence="0.996969">
6.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999928204081633">
In order to test how well the proposed system
works, 10-fold cross-validation was used on the
extracted corpora. The held-out data was selected
as random lines from the corpus, with a list stor-
ing the index of each selected line to ensure no
line was selected more than once. In each trial,
modifier classification was learned from 90% of
the data and the resulting model was used to pre-
dict the prenominal ordering of modifiers in the
remaining 10%.
The modifiers preceding each noun were stored
in unordered groups, and the ordering for each un-
ordered prenominal modifier pair {a,b} was pre-
dicted based on the classes of the modifiers in
our model. The ordering predictions followed the
constraints listed in Table 7. When the class was
known for one modifier but not for the other, the
two modifiers were ordered based on the class of
the known modifier: Modifiers in Classes 1, 2, 5,
and 8 were placed closer to the head noun than the
unknown modifier, while modifiers in Classes 3,
4, 7, and 9 were placed farther from the head noun
than the unknown modifier. If the known modifier
was of Class 6 (occurring in position two-three), a
random guess decided the ordering. This reflects
the idea that Classes 1, 2, 5, and 8 are all classes
for modifiers that broadly prefer to be closer to
the head noun, while Classes 3, 4, 7, and 9 are
all classes for modifiers that broadly prefer to be
farther from the head noun.
In the context of classification tasks, precision
and recall measurements provide useful informa-
tion of system accuracy. Precision, as defined in
(7), is the number of true positives divided by the
number of true positives plus false positives. This
is calculated here as tp/(tp + fp), where tp is the
number of orderings that were correctly predicted,
and fp is the number of orderings not correctly pre-
dicted. This measure provides information about
how accurate the modifier classification is. Recall,
as defined in (8), is the number of true positives
divided by the number of true positives plus false
negatives. This is calculated here as tp/(tp + fn),
where tp is the number of orderings that were cor-
rectly predicted, and fn is the total number of or-
derings that could not be predicted by our system.
This measure provides information about the pro-
portion of modifiers in the training data that can be
correctly ordered.
</bodyText>
<listItem confidence="0.7715964">
(7) Precision = tp/(tp + fp)
tp = number of orderings correctly predicted
fp = number of orderings not correctly
predicted
(8) Recall = tp/(tp + fn)
</listItem>
<bodyText confidence="0.961074">
tp = number of orderings correctly predicted
fn = number of orderings that could not be
predicted
</bodyText>
<page confidence="0.997305">
55
</page>
<table confidence="0.999769666666667">
Precision Recall
Token 89.63% (0.02) 74.14% (0.03)
Type 90.26% (0.02) 67.17% (0.03)
</table>
<tableCaption confidence="0.960417">
Table 8: Precision and Recall for Prenominal
Modifier Ordering
</tableCaption>
<sectionHeader confidence="0.999393" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999814604651163">
Results are shown in Table 8. Our model pre-
dicts the correct order for 89.63% of unordered
modifiers {a,b} for which an ordering decision
can be made, making correct predictions for
74.14% of all unordered modifiers in the test data.
The system also correctly predicts 90.26% of the
unordered modifier {a,b} types in the test data for
which an ordering decision can be made. This
covers 67.17% of the modifier pair types in the
test data. This lower value appears to be due to
the large amount of modifier pairs that are in the
data only once.
The values given are averages over each trial.
The standard deviation for each average is given
in parentheses. On average, 191 modifier pairs
were ordered in each trial, based on the assigned
orders of 273 individual modifiers, with an aver-
age of 23.01% of the modifiers outside of the vo-
cabulary in each trial.
The system precision and recall here are compa-
rable to previously reported results (see Section 2).
Extending our analysis over entire simplex NPs,
where we generate all possible orderings based on
our system constraints, we are able to predict an
average of 94.44% of the sequences for which a
determination can be made. This is a correct pre-
diction for 78.59% of all the simplex NPs in the
data.
Previous attempts have achieved very poor re-
sults when testing their models on a new domain.
We conclude our analysis by testing the accuracy
of our models on different domains. To do this, we
combine two corpora to build our model and then
test this model on the third.
Combining the WSJ corpus and the Brown cor-
pus to build our modifier classes and then testing
on the Switchboard (Swbd) corpus, we achieve
quite promising results. Our token precision is
89.57% and our type precision is 94.17%. How-
ever, our recall values are much lower than those
reported above (63.47% and 58.18%). Other train-
ing and testing combinations follow this pattern:
A model built from the Switchboard corpus and
</bodyText>
<table confidence="0.9997259">
Training Testing Token Token
Corpus Corpus Precision Recall
Brown+WSJ Swbd 89.57% 63.47%
Swbd+WSJ Brown 82.75% 57.14%
Swbd+Brown WSJ 79.82% 39.55%
Training Testing Type Type
Corpus Corpus Precision Recall
Brown+WSJ Swbd 94.17% 58.18%
Swbd+WSJ Brown 87.00% 51.18%
Swbd+Brown WSJ 82.43% 27.16%
</table>
<tableCaption confidence="0.8785455">
Table 9: Precision and Recall for Prenominal
Modifier Ordering of a New Domain
</tableCaption>
<bodyText confidence="0.999665625">
the WSJ corpus achieves 82.75% token precision
and 87% type precision when tested on the Brown
corpus (57.14% token recall, 51.18% type recall),
while a model built from the Switchboard corpus
and the Brown corpus achieves 79.82% token pre-
cision and 82.43% type precision when tested on
the WSJ corpus (39.55% token recall and 27.16%
type recall).
</bodyText>
<sectionHeader confidence="0.999581" genericHeader="conclusions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.975522807692308">
The system precision is comparable to previously
reported results. The results show that order-
ing modifiers based on this classification system
can aid in generating simplex noun phrases with
prenominal modifiers ordered in a way that sounds
natural. We now turn to a discussion of areas for
future work.
It seems reasonable that the classes for previ-
ously unseen modifiers could be developed based
on the known classes of surrounding modifiers.
This system lends itself to bootstrapping, where
a lexical acquisition task that constructed class
probabilities based on the surrounding context
could classify previously unseen modifiers:
grey shining metallic chain
three-four unknown one-two head-noun
Given its position and the classes of the surround-
ing modifiers, unknown could be two-three.
Grouping modifiers into classes that determine
their order also lends itself to incorporation into
generative grammars. For example, Head-driven
Phrase Structure Grammar (Sag et al., 2003),
a constraint-based grammatical framework that
groups lexical items into broader classes, could
utilize the classes proposed here to determine
modifier positions prenominally. Advancing re-
</bodyText>
<page confidence="0.988596">
56
</page>
<bodyText confidence="0.998416571428571">
search in this area could help grow the generative
capabilities of class-based grammars.
It bears mentioning that this same system was
attempted on the Google Web 1T 5-Gram corpus
(Brants and Franz, 2006), where we used WordNet
(Miller et al., 2006) to extract sequences of nouns
preceded by modifiers. The precision and recall
were similar to the values reported here, however,
the proportions of prenominal modifiers belied a
problem in using such a corpus for this approach:
82.56% of our data had two prenominal modifiers,
16.79% had four, but only 0.65% had three. This
pattern was due to the many extracted sequences
of modifiers preceding a noun that were not actu-
ally simplex NPs. That is, the 5-Grams include
many sequences of words in which the final one
has a use as a noun and the earlier ones have uses
as adjectives, but the 5-Gram itself may not be a
noun phrase. We found that many of our extracted
5-Grams were actually lists of words (for example,
Chinese Polish Portuguese Romanian Russian was
observed 115 times). In the future, we would like
to examine ways to use the 5-Gram corpus to sup-
plement our system.
The results reported here are encouraging, and
we hope to continue this work on a parsed version
of the Gutenberg corpus (Hart, 2009). This cor-
pus is a collection of text versions of novels and
other written works, and is available online. Using
a corpus of modifier-rich text such as this would
aid the system in classifying a greater number of
modifiers. Further work should also test how ro-
bust the acquisition of unseen modifiers is using
these classes, and examine implementing this or-
dering system into a language generation system.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999838045454545">
Otto Behaghel. 1930. Von Deutscher Wortstellung,
volume 44. Zeitschrift F¨ur Deutschen, Unterricht.
Thomas G. Bever. 1970. The cognitive basis for lin-
guistic structures. In J. R. Hayes, editor, Cogni-
tion and the Development of Language. Wiley, New
York.
Gemma Boleda and Laura Alonso. 2003. Cluster-
ing adjectives for class acquisition. In Proceedings
of the EACL’03 Student Session, pages 9–16, Bu-
dapest.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram version 1. http://www.ldc.upenn.edu. Lin-
guistic Data Consortium.
H. H. Clark and E. V. Clark. 1976. Psychology
and language: An introduction to psycholinguistics.
Harcourt Brace Jovanovich, New York.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233–263.
M.A.K. Halliday and Christian Matthiessen. 1999.
Construing experience as meaning: a language-
based approach to cognition. Cassell, London.
Michael Hart. 2009. Project Gutenberg collection.
http://www.gutenberg.org. Project Gutenberg.
Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53–72.
Robert Malouf. 2000. The order of prenominal ad-
jectives in natural language generation. In Proceed-
ings of the 38th Annual Meeting of the Association
for Computational Linguistics, pages 85–92, Hong
Kong.
Christopher D. Manning. 1993. Automatic acquisi-
tion of a large subcategorization dictionary from cor-
pora. In Meeting of the Association for Computa-
tional Linguistics, pages 235–242.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313–330.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Linguistic Data Consortium.
George A. Miller, Christiane Fellbaum, Randee Tengi,
Pamela Wakefield, Helen Langone, and Benjamin R.
Haskell. 2006. WordNet: A lexical database for the
english language.
Ivan Sag, Tom Wasow, and Emily Bender. 2003. Syn-
tactic Theory: A Formal Introduction. CSLI Publi-
cations, Stanford University.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proceedings of the
37th Annual Meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 135–143, Morristown, NJ, USA. Association
for Computational Linguistics.
Kees van Deemter. 2002. Generating referring expres-
sions: Boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37–52.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton.
Benjamin Lee Whorf. 1945. Grammatical categories.
Language, 21(1):1–11.
Paul Ziff. 1960. Semantic Analysis. Cornell Univer-
sity Press, Ithaca, New York.
</reference>
<page confidence="0.999146">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.527748">
<title confidence="0.983795">Ordering of Prenominal</title>
<author confidence="0.987084">Margaret</author>
<affiliation confidence="0.998527">Center for Spoken Language</affiliation>
<address confidence="0.718793">Portland, Oregon,</address>
<email confidence="0.999934">itallow@cslu.ogi.edu</email>
<abstract confidence="0.978780083333333">This paper introduces a class-based apto ordering prenominal are grouped into broad classes based on where they tend to occur prenominally, and a framework is developed to orsets of based on their classes. This system is developed to generate sevorderings for with more positional constraints, and lends to bootstrapping for the previously unseen</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Otto Behaghel</author>
</authors>
<title>Von Deutscher Wortstellung,</title>
<date>1930</date>
<volume>44</volume>
<institution>Zeitschrift F¨ur Deutschen, Unterricht.</institution>
<contexts>
<context position="3107" citStr="Behaghel, 1930" startWordPosition="482" endWordPosition="483">y, in Section 8, we outline areas for improvement and possible future work. 2 Related Work Discerning the rules governing the ordering of adjectives has been an area of research for quite some time (see, for example, Panini’s work on Sanskrit grammar ca. 350 BCE). Most approaches assume an underlying relationship between semantics and prenominal position (cf. Whorf, 1945; Ziff, 1960; Bever, 1970; Danks and Glucksberg, 1971). These approaches can be characterized as predicting modifier order based on degrees of semantic closeness to the noun. This follows what is known as Behaghel’s First Law (Behaghel, 1930): Word groups: What belongs together mentally is placed close together syntactically. (Clark and Clark, 1977: 545) However, there is disagreement on the exact qualities that affect position. These theories are also difficult to implement in a generation system, as they require determining the semantic properties of each modifier used, relative to the context in which it occurs. If a modifier classification Proceedings of the 12th European Workshop on Natural Language Generation, pages 50–57, Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics 50 scheme is to be</context>
</contexts>
<marker>Behaghel, 1930</marker>
<rawString>Otto Behaghel. 1930. Von Deutscher Wortstellung, volume 44. Zeitschrift F¨ur Deutschen, Unterricht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Bever</author>
</authors>
<title>The cognitive basis for linguistic structures.</title>
<date>1970</date>
<booktitle>Cognition and the Development of Language.</booktitle>
<editor>In J. R. Hayes, editor,</editor>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="2890" citStr="Bever, 1970" startWordPosition="448" endWordPosition="449">ions 5 and 6 present the materials and methodology of the current study, with a discussion of the corpus involved and the basic modules used in the process. In Section 7 we discuss the results of our study. Finally, in Section 8, we outline areas for improvement and possible future work. 2 Related Work Discerning the rules governing the ordering of adjectives has been an area of research for quite some time (see, for example, Panini’s work on Sanskrit grammar ca. 350 BCE). Most approaches assume an underlying relationship between semantics and prenominal position (cf. Whorf, 1945; Ziff, 1960; Bever, 1970; Danks and Glucksberg, 1971). These approaches can be characterized as predicting modifier order based on degrees of semantic closeness to the noun. This follows what is known as Behaghel’s First Law (Behaghel, 1930): Word groups: What belongs together mentally is placed close together syntactically. (Clark and Clark, 1977: 545) However, there is disagreement on the exact qualities that affect position. These theories are also difficult to implement in a generation system, as they require determining the semantic properties of each modifier used, relative to the context in which it occurs. If</context>
</contexts>
<marker>Bever, 1970</marker>
<rawString>Thomas G. Bever. 1970. The cognitive basis for linguistic structures. In J. R. Hayes, editor, Cognition and the Development of Language. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gemma Boleda</author>
<author>Laura Alonso</author>
</authors>
<title>Clustering adjectives for class acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL’03 Student Session,</booktitle>
<pages>9--16</pages>
<location>Budapest.</location>
<marker>Boleda, Alonso, 2003</marker>
<rawString>Gemma Boleda and Laura Alonso. 2003. Clustering adjectives for class acquisition. In Proceedings of the EACL’03 Student Session, pages 9–16, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram version 1. http://www.ldc.upenn.edu. Linguistic Data Consortium.</title>
<date>2006</date>
<contexts>
<context position="27461" citStr="Brants and Franz, 2006" startWordPosition="4522" endWordPosition="4525">modifiers, unknown could be two-three. Grouping modifiers into classes that determine their order also lends itself to incorporation into generative grammars. For example, Head-driven Phrase Structure Grammar (Sag et al., 2003), a constraint-based grammatical framework that groups lexical items into broader classes, could utilize the classes proposed here to determine modifier positions prenominally. Advancing re56 search in this area could help grow the generative capabilities of class-based grammars. It bears mentioning that this same system was attempted on the Google Web 1T 5-Gram corpus (Brants and Franz, 2006), where we used WordNet (Miller et al., 2006) to extract sequences of nouns preceded by modifiers. The precision and recall were similar to the values reported here, however, the proportions of prenominal modifiers belied a problem in using such a corpus for this approach: 82.56% of our data had two prenominal modifiers, 16.79% had four, but only 0.65% had three. This pattern was due to the many extracted sequences of modifiers preceding a noun that were not actually simplex NPs. That is, the 5-Grams include many sequences of words in which the final one has a use as a noun and the earlier one</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram version 1. http://www.ldc.upenn.edu. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>E V Clark</author>
</authors>
<title>Psychology and language: An introduction to psycholinguistics. Harcourt Brace Jovanovich,</title>
<date>1976</date>
<location>New York.</location>
<marker>Clark, Clark, 1976</marker>
<rawString>H. H. Clark and E. V. Clark. 1976. Psychology and language: An introduction to psycholinguistics. Harcourt Brace Jovanovich, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<pages>18--233</pages>
<contexts>
<context position="8905" citStr="Dale and Reiter, 1995" startWordPosition="1396" endWordPosition="1399">ers, where individual modifiers 51 (1) big beautiful white wooden house 1. Not all modifiers have equally stringent or(2) ?white wooden beautiful big house dering preferences. (3) comfortable red chair (4) ?red comfortable chair (5) big rectangular green Chinese silk carpet (6) ?Chinese big silk green rectangular carpet Figure 1: Grammatical Modifier Alternations (Vendler, 1968: 122) may be ordered separately as required by particular contexts. Along these lines, almost all referring expression generation algorithms rely on the availability of a predefined ordering or weighting of properties (Dale and Reiter, 1995; van Deemter, 2002; Krahmer et al., 2003). This requires that for every referent, an ordered or weighted listing of all the properties that can apply to it must be created before referring expression generation begins. In these models, the order or weights of the input properties map to the order of the output modifiers. However, the method used to determine the ordering or weighting of properties is an open issue. The difficulty with capturing the ordering of properties and their corresponding modifiers stems from the problem of data sparsity. In the example in Figure 1, the modifier silk ma</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science, 18:233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Christian Matthiessen</author>
</authors>
<title>Construing experience as meaning: a languagebased approach to cognition.</title>
<date>1999</date>
<publisher>Cassell,</publisher>
<location>London.</location>
<marker>Halliday, Matthiessen, 1999</marker>
<rawString>M.A.K. Halliday and Christian Matthiessen. 1999. Construing experience as meaning: a languagebased approach to cognition. Cassell, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Hart</author>
</authors>
<date>2009</date>
<booktitle>Project Gutenberg collection. http://www.gutenberg.org. Project Gutenberg.</booktitle>
<marker>Hart, 2009</marker>
<rawString>Michael Hart. 2009. Project Gutenberg collection. http://www.gutenberg.org. Project Gutenberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Sebastiaan van Erk</author>
<author>Andr´e Verleg</author>
</authors>
<title>Graph-based generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg. 2003. Graph-based generation of referring expressions. Computational Linguistics, 29(1):53–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>The order of prenominal adjectives in natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>85--92</pages>
<location>Hong Kong.</location>
<contexts>
<context position="1077" citStr="Malouf, 2000" startWordPosition="157" endWordPosition="159">modifiers with more flexible positional constraints, and lends itself to bootstrapping for the classification of previously unseen modifiers. 1 Introduction Ordering prenominal modifiers is a necessary task in the generation of natural language. For a system to effectively generate fluent utterances, the system must determine the proper order for any given set of modifiers. The order of modifiers before a noun affects the meaning and fluency of generated utterances. Determining ways to order modifiers prenominally has been an area of considerable research (cf. Shaw and Hatzivassiloglou, 1999; Malouf, 2000). In this paper, we establish and evaluate a classification system that can be used to order prenominal modifiers automatically. This may be implemented in a surface realization component of a natural language generation system, or may be used to help specify the ordering of properties that feed into a referring expression generation algorithm. Predictions of prenominal modifier ordering based on these classes are shown to be robust and accurate. The work here diverges from the approaches commonly employed in modifier classification by assuming no underlying relationship between semantics and </context>
<context position="5439" citStr="Malouf (2000)" startWordPosition="851" endWordPosition="852"> can conclude that a precedes c. In the clustering method, an order similarity metric is used to group modifiers together that share a similar relative order to other modifiers. Shaw and Hatzivassiloglou achieve their highest prediction accuracy of 90.67% using their transitivity technique on prenominal modifiers from a medical corpus. However, with their system trained on the medical corpus and then tested on the Wall Street Journal corpus (Marcus et al., 1993), they achieve an overall prediction accuracy of only 54%. The authors conclude that prenominal modifier ordering is domain-specific. Malouf (2000) continues this work, determining the order for sequences of prenominal adjectives by examining several different statistical and machine learning techniques. These achieve good results, ranging from 78.28% to 89.73% accuracy. Malouf achieves the best results by combining memory-based learning and positional probability, which reaches 91.85% accuracy at predicting the prenominal adjective orderings in the first 100 million tokens of the BNC. However, this analysis does not extend to other kinds of prenominal modifiers. The model is also not tested on a different domain. The approach to modifie</context>
<context position="9693" citStr="Malouf (2000)" startWordPosition="1534" endWordPosition="1535">fore referring expression generation begins. In these models, the order or weights of the input properties map to the order of the output modifiers. However, the method used to determine the ordering or weighting of properties is an open issue. The difficulty with capturing the ordering of properties and their corresponding modifiers stems from the problem of data sparsity. In the example in Figure 1, the modifier silk may be rare enough in any corpus that finding it in combination with another modifier, in order to create a generalization about its ordering constraints, is nearly impossible. Malouf (2000) examined the first million sentences of the British National Corpus and found only one sequence of adjectives for every twenty sentences. With sequences of adjectives occurring so rarely, the chances of finding information on any particular sequence is small. The data is just too sparse. 4 Towards a Solution To create a flexible system capable of predicting a wide variety of orderings, we used several corpora to build broad modifier classes. Modifiers are classified by where they tend to appear prenominally, and ordering constraints between the classes determine the order for any set of modif</context>
</contexts>
<marker>Malouf, 2000</marker>
<rawString>Robert Malouf. 2000. The order of prenominal adjectives in natural language generation. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 85–92, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>235--242</pages>
<marker>Manning, 1993</marker>
<rawString>Christopher D. Manning. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. In Meeting of the Association for Computational Linguistics, pages 235–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5292" citStr="Marcus et al., 1993" startWordPosition="828" endWordPosition="831"> makes inferences about unseen orderings among prenominal modifiers; given a third prenominal modifier c, where a precedes b and b precedes c, the authors can conclude that a precedes c. In the clustering method, an order similarity metric is used to group modifiers together that share a similar relative order to other modifiers. Shaw and Hatzivassiloglou achieve their highest prediction accuracy of 90.67% using their transitivity technique on prenominal modifiers from a medical corpus. However, with their system trained on the medical corpus and then tested on the Wall Street Journal corpus (Marcus et al., 1993), they achieve an overall prediction accuracy of only 54%. The authors conclude that prenominal modifier ordering is domain-specific. Malouf (2000) continues this work, determining the order for sequences of prenominal adjectives by examining several different statistical and machine learning techniques. These achieve good results, ranging from 78.28% to 89.73% accuracy. Malouf achieves the best results by combining memory-based learning and positional probability, which reaches 91.85% accuracy at predicting the prenominal adjective orderings in the first 100 million tokens of the BNC. However</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<booktitle>Treebank3. Linguistic Data Consortium.</booktitle>
<contexts>
<context position="12292" citStr="Marcus et al., 1999" startWordPosition="1961" endWordPosition="1964">highest probabilities of occurring. Examples of the intermediary files in this process are given in Tables 1 and 2. Table 1 lists modifiers followed by their frequency counts in each prenominal position. Table 2 lists these modifiers associated to their classes, with the proportions that determine the class. wealthy four 2 three 2 two 3 one 1 red four 13 three 35 two 35 one 21 golden four 1 three 5 two 5 one 3 strongest four 5 three 5 two 5 one 5 Table 1: Example Modifier Classification Intermediate File: Step 3 5 Materials To create the training and test data, we utilize the Penn Treebank-3 (Marcus et al., 1999) releases of 52 wealthy two two 0.38 red two three three 0.34 two 0.34 golden one two three three 0.33 two 0.33 one 0.29 strongest two three four four 0.33 three 0.33 two 0.33 Table 2: Example Modifier Classification Intermediate File: Step 4 the parsed Wall Street Journal corpus, the parsed Brown corpus, and the parsed Switchboard corpus. The Wall Street Journal corpus is a selection of over one million words collected from the Wall Street Journal over a three-year period. The Brown corpus is over one million words of prose written in various genres, including mystery, humor, and popular lore</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank3. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Christiane Fellbaum</author>
<author>Randee Tengi</author>
<author>Pamela Wakefield</author>
<author>Helen Langone</author>
<author>Benjamin R Haskell</author>
</authors>
<title>WordNet: A lexical database for the english language.</title>
<date>2006</date>
<contexts>
<context position="27506" citStr="Miller et al., 2006" startWordPosition="4530" endWordPosition="4533">modifiers into classes that determine their order also lends itself to incorporation into generative grammars. For example, Head-driven Phrase Structure Grammar (Sag et al., 2003), a constraint-based grammatical framework that groups lexical items into broader classes, could utilize the classes proposed here to determine modifier positions prenominally. Advancing re56 search in this area could help grow the generative capabilities of class-based grammars. It bears mentioning that this same system was attempted on the Google Web 1T 5-Gram corpus (Brants and Franz, 2006), where we used WordNet (Miller et al., 2006) to extract sequences of nouns preceded by modifiers. The precision and recall were similar to the values reported here, however, the proportions of prenominal modifiers belied a problem in using such a corpus for this approach: 82.56% of our data had two prenominal modifiers, 16.79% had four, but only 0.65% had three. This pattern was due to the many extracted sequences of modifiers preceding a noun that were not actually simplex NPs. That is, the 5-Grams include many sequences of words in which the final one has a use as a noun and the earlier ones have uses as adjectives, but the 5-Gram its</context>
</contexts>
<marker>Miller, Fellbaum, Tengi, Wakefield, Langone, Haskell, 2006</marker>
<rawString>George A. Miller, Christiane Fellbaum, Randee Tengi, Pamela Wakefield, Helen Langone, and Benjamin R. Haskell. 2006. WordNet: A lexical database for the english language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Sag</author>
<author>Tom Wasow</author>
<author>Emily Bender</author>
</authors>
<title>Syntactic Theory: A Formal Introduction.</title>
<date>2003</date>
<publisher>CSLI Publications,</publisher>
<institution>Stanford University.</institution>
<contexts>
<context position="27065" citStr="Sag et al., 2003" startWordPosition="4464" endWordPosition="4467">odifiers could be developed based on the known classes of surrounding modifiers. This system lends itself to bootstrapping, where a lexical acquisition task that constructed class probabilities based on the surrounding context could classify previously unseen modifiers: grey shining metallic chain three-four unknown one-two head-noun Given its position and the classes of the surrounding modifiers, unknown could be two-three. Grouping modifiers into classes that determine their order also lends itself to incorporation into generative grammars. For example, Head-driven Phrase Structure Grammar (Sag et al., 2003), a constraint-based grammatical framework that groups lexical items into broader classes, could utilize the classes proposed here to determine modifier positions prenominally. Advancing re56 search in this area could help grow the generative capabilities of class-based grammars. It bears mentioning that this same system was attempted on the Google Web 1T 5-Gram corpus (Brants and Franz, 2006), where we used WordNet (Miller et al., 2006) to extract sequences of nouns preceded by modifiers. The precision and recall were similar to the values reported here, however, the proportions of prenominal</context>
</contexts>
<marker>Sag, Wasow, Bender, 2003</marker>
<rawString>Ivan Sag, Tom Wasow, and Emily Bender. 2003. Syntactic Theory: A Formal Introduction. CSLI Publications, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Ordering among premodifiers.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>135--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1062" citStr="Shaw and Hatzivassiloglou, 1999" startWordPosition="153" endWordPosition="156">o generate several orderings for modifiers with more flexible positional constraints, and lends itself to bootstrapping for the classification of previously unseen modifiers. 1 Introduction Ordering prenominal modifiers is a necessary task in the generation of natural language. For a system to effectively generate fluent utterances, the system must determine the proper order for any given set of modifiers. The order of modifiers before a noun affects the meaning and fluency of generated utterances. Determining ways to order modifiers prenominally has been an area of considerable research (cf. Shaw and Hatzivassiloglou, 1999; Malouf, 2000). In this paper, we establish and evaluate a classification system that can be used to order prenominal modifiers automatically. This may be implemented in a surface realization component of a natural language generation system, or may be used to help specify the ordering of properties that feed into a referring expression generation algorithm. Predictions of prenominal modifier ordering based on these classes are shown to be robust and accurate. The work here diverges from the approaches commonly employed in modifier classification by assuming no underlying relationship between</context>
<context position="3937" citStr="Shaw and Hatzivassiloglou (1999)" startWordPosition="609" endWordPosition="612">heories are also difficult to implement in a generation system, as they require determining the semantic properties of each modifier used, relative to the context in which it occurs. If a modifier classification Proceedings of the 12th European Workshop on Natural Language Generation, pages 50–57, Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics 50 scheme is to be implemented, it should be able to create a variety of natural, unmarked orders; be robust enough to handle a wide variety of modifiers; and be flexible enough to allow different natural orderings. Shaw and Hatzivassiloglou (1999) examine this problem, and develop ways to order all prenominal modifier types. This includes adjectives as well as nouns, such as “baseball” in “baseball field”; gerunds, such as “running” in “running man”; and past participles, such as “heated” in “heated debate”. The authors devise three different methods that may be implemented in a generation system to order these kinds of prenominal modifiers. These are the direct evidence method, the transitivity method, and the clustering method. Briefly, given prenominal modifiers a and b in a training corpus, the direct evidence method utilizes proba</context>
<context position="11244" citStr="Shaw and Hatzivassiloglou, 1999" startWordPosition="1778" endWordPosition="1781">e ßoppy hat and ßoppy white hat). It is not based on any mapping between position and semantics, morphology, or phonology, but does not exclude any such relationship in the classification: This classification scheme builds on what there is direct evidence for, independent of why each modifier appears where it does. To create our model, all simplex noun phrases (NPs) are extracted from parsed corpora. A simplex NP is defined as a maximal noun phrase that includes premodifiers such as determiners and possessives but no post-nominal constituents such as prepositional phrases or relative clauses (Shaw and Hatzivassiloglou, 1999: 137). From these simplex NPs, we extract all those headed by a noun and preceded by only prenominal modifiers. This includes modifiers tagged as adjectives (JJ), nouns (NN), gerunds (VBG), and past participles (VBN). The counts and relative positions of each modifier are stored, and these are converted into position probabilities in vector file format. Modifiers are classified based on the positions in which they have the highest probabilities of occurring. Examples of the intermediary files in this process are given in Tables 1 and 2. Table 1 lists modifiers followed by their frequency coun</context>
</contexts>
<marker>Shaw, Hatzivassiloglou, 1999</marker>
<rawString>James Shaw and Vasileios Hatzivassiloglou. 1999. Ordering among premodifiers. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, pages 135–143, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>Generating referring expressions: Boolean extensions of the incremental algorithm.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<marker>van Deemter, 2002</marker>
<rawString>Kees van Deemter. 2002. Generating referring expressions: Boolean extensions of the incremental algorithm. Computational Linguistics, 28(1):37–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Vendler</author>
</authors>
<title>Adjectives and Nominalizations.</title>
<date>1968</date>
<publisher>Mouton.</publisher>
<contexts>
<context position="8664" citStr="Vendler, 1968" startWordPosition="1361" endWordPosition="1362"> alternation given in Figure 1. Some combinations of modifiers before a noun are more marked than others, although all are strictly speaking grammatical. This speaks to the need for a broad modifier classes to order prenominal modifiers, where individual modifiers 51 (1) big beautiful white wooden house 1. Not all modifiers have equally stringent or(2) ?white wooden beautiful big house dering preferences. (3) comfortable red chair (4) ?red comfortable chair (5) big rectangular green Chinese silk carpet (6) ?Chinese big silk green rectangular carpet Figure 1: Grammatical Modifier Alternations (Vendler, 1968: 122) may be ordered separately as required by particular contexts. Along these lines, almost all referring expression generation algorithms rely on the availability of a predefined ordering or weighting of properties (Dale and Reiter, 1995; van Deemter, 2002; Krahmer et al., 2003). This requires that for every referent, an ordered or weighted listing of all the properties that can apply to it must be created before referring expression generation begins. In these models, the order or weights of the input properties map to the order of the output modifiers. However, the method used to determi</context>
</contexts>
<marker>Vendler, 1968</marker>
<rawString>Zeno Vendler. 1968. Adjectives and Nominalizations. Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Lee Whorf</author>
</authors>
<title>Grammatical categories.</title>
<date>1945</date>
<journal>Language,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="2865" citStr="Whorf, 1945" startWordPosition="444" endWordPosition="445">assification system. Sections 5 and 6 present the materials and methodology of the current study, with a discussion of the corpus involved and the basic modules used in the process. In Section 7 we discuss the results of our study. Finally, in Section 8, we outline areas for improvement and possible future work. 2 Related Work Discerning the rules governing the ordering of adjectives has been an area of research for quite some time (see, for example, Panini’s work on Sanskrit grammar ca. 350 BCE). Most approaches assume an underlying relationship between semantics and prenominal position (cf. Whorf, 1945; Ziff, 1960; Bever, 1970; Danks and Glucksberg, 1971). These approaches can be characterized as predicting modifier order based on degrees of semantic closeness to the noun. This follows what is known as Behaghel’s First Law (Behaghel, 1930): Word groups: What belongs together mentally is placed close together syntactically. (Clark and Clark, 1977: 545) However, there is disagreement on the exact qualities that affect position. These theories are also difficult to implement in a generation system, as they require determining the semantic properties of each modifier used, relative to the conte</context>
</contexts>
<marker>Whorf, 1945</marker>
<rawString>Benjamin Lee Whorf. 1945. Grammatical categories. Language, 21(1):1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Ziff</author>
</authors>
<title>Semantic Analysis.</title>
<date>1960</date>
<publisher>Cornell University Press,</publisher>
<location>Ithaca, New York.</location>
<contexts>
<context position="2877" citStr="Ziff, 1960" startWordPosition="446" endWordPosition="447">system. Sections 5 and 6 present the materials and methodology of the current study, with a discussion of the corpus involved and the basic modules used in the process. In Section 7 we discuss the results of our study. Finally, in Section 8, we outline areas for improvement and possible future work. 2 Related Work Discerning the rules governing the ordering of adjectives has been an area of research for quite some time (see, for example, Panini’s work on Sanskrit grammar ca. 350 BCE). Most approaches assume an underlying relationship between semantics and prenominal position (cf. Whorf, 1945; Ziff, 1960; Bever, 1970; Danks and Glucksberg, 1971). These approaches can be characterized as predicting modifier order based on degrees of semantic closeness to the noun. This follows what is known as Behaghel’s First Law (Behaghel, 1930): Word groups: What belongs together mentally is placed close together syntactically. (Clark and Clark, 1977: 545) However, there is disagreement on the exact qualities that affect position. These theories are also difficult to implement in a generation system, as they require determining the semantic properties of each modifier used, relative to the context in which </context>
</contexts>
<marker>Ziff, 1960</marker>
<rawString>Paul Ziff. 1960. Semantic Analysis. Cornell University Press, Ithaca, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>