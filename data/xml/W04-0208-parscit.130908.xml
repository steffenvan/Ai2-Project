<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.992653">
Temporal Discourse Models for Narrative Structure
</title>
<author confidence="0.98531">
Inderjeet MANI
</author>
<affiliation confidence="0.996813">
Department of Linguistics
Georgetown University
</affiliation>
<address confidence="0.898144">
ICC 452
Washington, DC 20057
</address>
<email confidence="0.996344">
im5@georgetown.edu
</email>
<author confidence="0.959944">
James PUSTEJOVSKY
</author>
<affiliation confidence="0.971606">
Department of Computer Science
Brandeis University
</affiliation>
<address confidence="0.7523805">
Volen 258
Waltham, Massachusetts 02254
</address>
<email confidence="0.997774">
jamesp@cs.brandeis.edu
</email>
<sectionHeader confidence="0.992443" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998154">
Getting a machine to understand human
narratives has been a classic challenge for
NLP and AI. This paper proposes a new
representation for the temporal structure of
narratives. The representation is parsimonious,
using temporal relations as surrogates for
discourse relations. The narrative models,
called Temporal Discourse Models, are tree-
structured, where nodes include abstract
events interpreted as pairs of time points and
where the dominance relation is expressed by
temporal inclusion. Annotation examples and
challenges are discussed, along with a report
on progress to date in creating annotated
corpora.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945928571429">
Getting a machine to understand human narratives
has been a classic challenge for NLP and AI.
Central to all narratives is the notion of time and
the unfolding of events. When we understand a
story, in addition to understanding other aspects
such as plot, characters, goals, etc., we are able to
understand the order of happening of events. A
given text may have multiple stories; when we
understand such a text, we are able to tease apart
these distinct stories. Thus, understanding the story
from a text involves building a global model of the
sequences of events in the text, as well as the
structure of nested stories. We refer to such models
as Temporal Discourse Models (TDMs).
Currently, while we have informal
descriptions of the structure of narratives, e.g.,
(Bell 1999), we lack a precise understanding of
this aspect of discourse. What sorts of structural
configurations are observed? What formal
characteristics do they have? For syntactic
processing of natural languages, we have,
arguably, answers to similar questions. However,
for discourse, we have hardly begun to ask the
questions.
One of the problems here is that most of
the information about narrative structure is implicit
in the text. Thus, while linguistic information in
the form of tense, aspect, temporal adverbials and
discourse markers is often present, people use
commonsense knowledge to fill in information.
Consider a simple discourse: Yesterday Holly was
running a marathon when she twisted her ankle.
David had pushed her. Here, aspectual information
indicates that the twisting occurred during the
running, while tense suggests that the pushing
occurs before the twisting. Commonsense
knowledge also suggests that the pushing caused
the twisting.
We can see that even for interpreting such
relatively simple discourses, a system might
require a variety of sources of linguistic
knowledge, including knowledge of tense, aspect,
temporal adverbials, discourse relations, as well as
background knowledge. Of course, other
inferences are clearly possible, e.g., that the
running stopped after the twisting, but when
viewed as defaults, these latter inferences seem to
be more easily violated. The need for
commonsense inferences has motivated
computational approaches that are domain-
specific, using hand-coded knowledge (e.g., Asher
and Lascarides 2003, Hitzeman et al. 1995).
A number of theories have postulated the
existence of various discourse relations that relate
elements in the text to produce a global model of
discourse, e.g., (Mann and Thompson 1988),
(Hobbs 1985), (Hovy 1990) and others. In RST
(Mann and Thompson 1988), (Marcu 2000), these
relations are ultimately between semantic elements
corresponding to discourse units that can be simple
sentences or clauses as well as entire discourses. In
SDRT (Asher and Lascarides 2003), these relations
are between representations of propositional
content, called Discourse Representation
Structures (Kamp and Reyle, 1993).
Despite a considerable amount of very
productive research, annotating such discourse
relations has proved problematic. This is due to the
fact that discourse markers may be absent (i.e.,
implicit) or ambiguous; but more importantly,
because in many cases the precise nature of these
discourse relations is unclear. Although (Marcu et
al. 1999) (Carlson et al. 2001) reported relatively
high levels of inter-annotator agreement, this was
based on an annotation procedure where the
annotators were allowed to iteratively revise the
instructions based on joint discussion.
While we appreciate the importance of
representing rhetorical relations in order to carry
out temporal inferences about event ordering, we
believe that there are substantial advantages in
isolating the temporal aspects and modeling them
separately as TDMs. This greatly simplifies the
representation, which we discuss next.
</bodyText>
<sectionHeader confidence="0.982636" genericHeader="method">
2 Temporal Discourse Models
</sectionHeader>
<bodyText confidence="0.99894725">
A TDM is a tree-structured syntactic model of
global discourse structure, where temporal
relations are used as surrogates for discourse
relations, and where abstract events corresponding
to entire discourses are introduced as nodes in the
We begin by illustrating the basic
intuition. Consider discourse (1), from (Webber
1988):
</bodyText>
<listItem confidence="0.809095833333333">
(1) a. John went into the florist shop.
b. He had promised Mary some flowers.
c. She said she wouldn’t forgive him if he
forgot. d. So he picked out three red roses.
The discourse structure of (1) can be
represented by the tree, T1, shown below.
</listItem>
<bodyText confidence="0.984921366666667">
In addition to T1, we also have the
temporal ordering constraints C1: {Eb &lt; Ec, Ec &lt;
Ea, Ea &lt; Ed}. These are represented separately
from the tree. A TDM is thus a pairing of tree
structures and temporal constraints. More
precisely, a Temporal Discourse Model for a text is
a pair &lt;T, C&gt;, where T is a rooted, unordered,
directed tree with nodes N = E v A, where E is
the set of events mentioned in the text and A is a
set of abstract events, and a parent-child ordering
relation, c (temporal inclusion). A non-leaf node
can be textually mentioned or abstract. Nodes also
have a set of atomic-valued features. Note that the
tree is temporally unordered left to right. C is a set
of temporal ordering constraints using the ordering
relation, &lt; (temporal precedence) as well as (for
states, clarified below) ‘minimal restrictions’ on
the above temporal inclusion relation (expressed as
a cmin).
were abstract, but textually mentioned events can
also create embeddings, as in (2) (example from
(Spejewski 1988)):
(2) a. Edmond made his own Christmas
presents this year. b. First he dried a bunch
of tomatoes in his oven. c. Then he made a
booklet of recipes that use dried tomatoes. d.
He scanned in the recipes from his gourmet
magazines. e. He gave these gifts to his
family.
In (1) the embedding nodes E0 and E1
</bodyText>
<equation confidence="0.965366">
tree.
T2 =
E0 E0
Ea E1 Ed
Eb Ec
</equation>
<bodyText confidence="0.999672333333333">
Here E0 has children Ea, E1, and Ed, and
E1 has children Eb and Ec. The nodes with
alphabetic subscripts are events mentioned in the
text, whereas nodes with numeric subscripts are
abstract events, i.e., events that represent abstract
discourse objects. A node X is a child of node Y iff
X is temporally included in Y. In our scheme,
events are represented as pairs of time points. So,
E0 is an abstract node representing a top-level
story, and E1 is an abstract node representing an
embedded story. Note that the mentioned events
are ordered left to right in text order for notational
convenience, but no temporal ordering is directly
represented in the tree. Since the nodes in this
representation are at a semantic level, the tree
structure is not necessarily isomorphic to a
representation at the text level, although T1
happens to be isomorphic.
</bodyText>
<equation confidence="0.945191666666667">
Eb Ec
Ed
C2 = {Ea &lt; Ee, Eb &lt; Ec}
</equation>
<bodyText confidence="0.99748912195122">
Note that the partial ordering C can be
extended using T and temporal closure axioms
(Setzer and Gaizauskas 2001), (Verhagen 2004), so
that in the case of &lt;T2, C2&gt;, we can infer, for
example, that Eb &lt; Ed, Ed &lt; Ee, and so forth.
In representing states, we take a
conservative approach to the problems of
ramification and change (McCarthy and Hayes
1969). This is the classic problem of recognizing
when states (the effects of actions) change as a
result of actions. Any tensed stative predicate will
be represented as a node in the tree (progressives
are here treated as stative). Consider an example
like John walked home. He was feeling great.
Here we represent the state of feeling great as
being minimally a part of the event of walking,
without committing to whether it extends before or
Ea Ee
after the event. While this is interpreted as an
overloaded temporal inclusion in the TDM tree, a
constraint is added to C indicating that this
inclusion is minimal.
This conservative approach results in
logical incompleteness, however. For example,
given the discourse Max entered the room. He was
wearing a black shirt, the system will not know
whether the shirt was worn after he entered the
room. States are represented as bounded intervals,
and participate in ordering relations with events in
the tree. It is clear that in many cases, a state
should persist throughout the interval spanning
subsequent events. This is not captured by the
current tree representation. Opposition structures
of predicates and gating operations over properties
can be expressed as constraints introduced by
events, however, but at this stage of development,
we have been interested in capturing a coarser
temporal ordering representation, very robustly.
We believe, however, that annotation using the
minimal inclusion relation will allow us to reason
about persistence heuristically in the future.
</bodyText>
<sectionHeader confidence="0.996166" genericHeader="method">
3 Prerequisites
</sectionHeader>
<bodyText confidence="0.999978925">
Prior work on temporal information extraction has
been fairly extensive and is covered in (Mani et al.
2004). Recent research has developed the TimeML
annotation scheme (Pustejovsky et al. 2002)
(Pustejovsky et al. 2004), as well as a corpus of
TimeML-annotated news stories (TimeBank 2004)
and annotation tools that go along with it, such as
the TANGO tool (Pustejovsky et al. 2003).
TimeML flags tensed verbs, adjectives, and
nominals that correspond to events and states,
tagging instances of them with standard TimeML
attributes, including the class of event (perception,
reporting, aspectual, state, etc.), tense (past,
present, future), grammatical aspect (perfective,
progressive, or both), whether it is negated, any
modal operators which govern it, and its
cardinality if the event occurs more than once.
Likewise, time expressions are flagged, and their
values normalized, so that Thursday in He left on
Thursday would get a resolved ISO time value
depending on context (TIMEX2 2004). Finally,
temporal relations between events and time
expressions (e.g., that the leaving occurs during
Thursday) are recorded by means of temporal links
(TLINKs) that express Allen-style interval
relations (Allen 1984).
Several automatic tools have been
developed in conjunction with TimeML, including
event taggers (Pustejovsky et al. 2003), time
expression taggers (Mani and Wilson 2000), and
an exploratory link extractor (Mani et al. 2003).
Temporal reasoning algorithms have also been
developed, that apply transitivity axioms to expand
the links using temporal closure algorithms (Setzer
and Gaizauskas 2001), (Pustejovsky et al. 2003).
However, TimeML is inadequate as a
temporal model of discourse: it constructs no
global representation of the narrative structure,
instead annotating a complex graph that links
primitive events and times.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="method">
4 Related Frameworks
</sectionHeader>
<bodyText confidence="0.999895074074074">
Since the relations in TDMs involve temporal
inclusion and temporal ordering, the mentioned
events can naturally be mapped to other discourse
representations used in computational linguistics.
A TDM tree can be converted to a first-order
temporal logic representation (where temporal
ordering and inclusion operators are added) by
expanding the properties of the nodes. These
properties include any additional predications
made explicitly about the event, e.g., information
from thematic arguments and adjuncts. In other
words, a full predicate argument representation,
e.g., as might be found in the PropBank
(Kingsbury and Palmer 2002), can be associated
with each node.
TDMs can also be mapped to Discourse
Representation Structures (DRS) (which in turn
can be mapped to a logical form). Since TDMs
represent events as pairs of time points (which can
be viewed as intervals), and DRT represents events
as primitives, we can reintroduce time intervals
based on the standard DRT approach (e ⊆ t for
events, e O t for states, except for present tense
states, where t ⊆ e).
Consider an example from the Discourse
Representation Theory (DRT) literature (from
Kamp and Reyle 1993):
</bodyText>
<listItem confidence="0.990289">
(3) a. A man entered the White Hart. b. He
</listItem>
<bodyText confidence="0.8766515">
was wearing a black jacket. c. Bill served
him a beer.
The TDM is &lt;T3, C3&gt; below, with
internal properties of the nodes as shown:
</bodyText>
<equation confidence="0.78741425">
T3 = E0
Ea Ec
Eb
C3 = {Ea &lt; Ec}
node.properties(Ea): enter(Ea, x, y),
man(x), y= theWhiteHart, Ea &lt; n
node.properties(Eb): PROG(wear(Eb, x1,
y1)), black-jacket(y1), x1=x, Eb &lt; n,
node.properties(Ec): serve(Ec, x2, y2, z),
beer(z), x2=Bill, y2=x, Ec &lt; n
From T3: Eb c Ea
From C3: Ea &lt; Ec
</equation>
<bodyText confidence="0.995216333333333">
The DRT representation is shown below
(here we have created variables for the
reference times):
</bodyText>
<equation confidence="0.9493196">
Ea, x, y , Eb, x1, y1, Ec, x2, y2, z,
t1, t2, t3
enter(Ea, x, y), man(x), y=
theWhiteHart
PROG(wear(Eb, x1, y1)), black-
jacket(y1), x1=x
serve(Ec, x2, y2, z), beer(z),
x2=Bill, y2=x
t1 &lt; n, Ea c t1, t2 &lt; n, Eb ο t2, Eb
c Ea, t3 &lt; n, Ec c t3, Ea &lt; Ec
</equation>
<bodyText confidence="0.999927260869566">
Note that we are by no means claiming
that DRSs and TDMs are equivalent. TDMs are
tree-structured and DRSs are not, and the inclusion
relations involving our abstract events, i.e., Ea c
E0 and Ec c E0, are not usually represented in
DRT. Nevertheless, there are many similarities
between TDMs and DRT which are worth
examining for semantic and computational
properties. Furthermore, SDRT (Asher and
Lascarides 2003) extends DRT to include
discourse relations. SDRT and RST both differ
fundamentally from TDMs, since we dispense with
rhetorical relations.
It should be pointed out, nevertheless, that
TDMs, as modeled so far, do not represent
modality and intensional contexts in the tree
structure. (However, information about modality
and negation is stored in the nodes based on
TimeML preprocessing). One way of addressing
this issue is to handle lexically derived modal
subordination (such as believe and want) by
introducing embedded events, linked to the modal
predicate by subordinating relations. For example,
in the sentence John believed that Mary graduated
from Harvard, the complement event is
represented as a subtree linked by a lexical
relation.
DLTAG (Webber et al. 2004) is a model
of discourse structure where explicit or implicit
discourse markers relating only primitive discourse
units. Unlike TDMs, where the nodes in the tree
can contain embedded structures, DLTAG is a
local model of discourse structure; it thus provides
a set of binary relations, rather than a tree Like
TDMs, however, DLTAG models discourse
structure without postulating the existence of
rhetorical relations in the discourse tree. Instead,
the rhetorical relations appear as predicates in the
semantic forms for discourse markers. In this
respect, they differ from TDMs, which do not
commit to specific rhetorical relations.
Spejewski (1994) developed a tree-based
model of the temporal structure of a sequence of
sentences. Her approach is based on relations of
temporal coordination and subordination, and is
thus a major motivation for our own approach.
However, her approach mixes both reference times
and events in the same representation, so that the
parent-child relation sometimes represents
temporal anchoring, and at other times
coordination. In the above example of John walked
home. He was feeling great, her approach would
represent the “reference time” of the state (of
feeling great) as being part of the event of walking
as well as part of the state, resulting in a graph
rather than a strict tree. Note that our approach
uses minimality.
(Hitzeman et al. 1995) developed a
computational approach to distinguish various
temporal threads in discourse. The idea here, based
on the notion of temporal centering, is that there is
one ‘thread’ that the discourse is currently
following. Thus, in (1) above, each utterance is
associated with exactly one of two threads: (i)
going into the florist’s shop and (ii) interacting
with Mary. Hitzeman et al. prefer an utterance to
continue a current thread which has the same tense
or is semantically related to it, so that in (1) above,
utterance d would continue the thread (i) above
based on tense. In place of world knowledge,
however, semantic distance between utterances is
used, presumably based on lexical relationships.
Whether such semantic similarity is effective is a
matter for evaluation, which is not discussed in
their paper. For example, it isn’t clear what would
rule out (1c) as continuing thread (i).
While TDMs do not commit to rhetorical
relations, our expectation is that they can be used
as an intermediate representation for rhetorical
parsing. Thus, when event A in a TDM temporally
precedes its right sibling B, the rhetorical relation
of Narration will typically be inferred. When B
precedes is left sibling A, then Explanation will
typically be inferred. When A temporally includes
a child node B, then Elaboration is typically
inferred, etc. TDMs are thus a useful shallow
representation that can be a useful first step in
deriving rhetorical relations; indeed, rhetorical
relations may be implicit in the human annotation
of such relations, e.g., when explicit discourse
markers like “because” indicate a particular
temporal order.
</bodyText>
<sectionHeader confidence="0.976455" genericHeader="method">
5 Annotation Scheme
</sectionHeader>
<bodyText confidence="0.99995359375">
The annotation scheme involves taking each
document that has been preprocessed with time
expressions and event tags (complying with
TimeML) and then representing TDM parse trees
and temporal ordering constraints (the latter also
compliant with TimeML TLINKS).
Each discourse begins with a root abstract
node. As an annotation convention, (A1) in the
absence of any overt or covert discourse markers
or temporal adverbials, a tense shift will license the
creation of an abstract node, with the event with
the shifted tense being the leftmost daughter of the
abstract node. The abstract node will then be
inserted as the child of the immediately preceding
text node. In addition, convention (A2) states that
in the absence of temporal adverbials and overt or
covert discourse markers, a stative event will
always be placed as a child of the immediately
preceding text event when the latter is non-stative.
Further, convention (A3) states that when the
previous event is stative, in the absence of
temporal adverbials and explicit or implicit
discourse markers, the stative event is a sibling of
the previous stative (as in a scene-setting fragment
of discourse).
We expect that inter-annotator reliability
on TDM trees will be quite high, given the
transparent nature of the tree structure along with
clear annotation conventions. The Appendices
provide examples of annotation, to illustrate the
simplicity of the scheme as well as potential
problems.
</bodyText>
<sectionHeader confidence="0.998231" genericHeader="method">
6 Corpora
</sectionHeader>
<bodyText confidence="0.999954083333333">
We have begun annotating three corpora with
Temporal Discourse Model information. The first
is the Remedia corpus (remedia.com). There are
115 documents in total, grouped into four reading
levels, all of which have been tagged by a human
for time expressions in a separate project by Lisa
Ferro at MITRE. Each document is short, about
237 words on average, and has a small number of
questions after it for reading comprehension.
The Brandeis Reading Corpus is a
collection of 100 K-8 Reading Comprehension
articles, mined from the web and categorized by
level of comprehension difficulty. Articles range
from 50-350 words in length. Complexity of the
reading task is defined in terms of five basic
classes of reading difficulty.
The last is the Canadian Broadcasting
Corporation (cbc4kids.ca). The materials are
current-event stories aimed at an audience of 8-
year-old to 13-year-old students. The stories are
short (average length around 450 words). More
than a thousand articles are available. The
CBC4Kids corpus is already annotated with POS
and parse tree markup.
</bodyText>
<sectionHeader confidence="0.984397" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999977862068966">
Our assumption so far has been that the temporal
structure of narratives is tree-structured and
context-free. Whether the context-free property is
violated or not remains to be seen.
Once the annotation effort is completed,
we plan to use the annotated corpora in statistical
parsing algorithms to construct TDMs. This should
allow features from the corpus to be leveraged
together to make inferences about narrative
structure. While such knowledge source
combination is not by any means guaranteed to
substitute for commonsense knowledge, it at least
allows for the introduction of generic, machine
learning methods for extracting narrative structure
from stories in any domain. Earlier work in a non-
corpus based (Hitzeman et al. 1995) as well as
corpus-based setting (Mani et al. 2003) attests to
the usefulness of combining knowledge sources for
inferring temporal relations. We expect to leverage
similar methods in TDM parsing.
We believe that the temporal aspect of
discourse provides a handle for investigating
discourse structure, thereby simplifying the
problem of discourse structure annotation. It is
therefore of considerable theoretical interest.
Further, being able to understand the structure of
narratives will in turn allow us to summarize them
and answer temporal questions about narrative
structure.
</bodyText>
<sectionHeader confidence="0.991125" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988347656565657">
J. F. Allen. 1984. Towards a General Theory of
Action and Time. Artificial Intelligence 23:
123-154.
N. Asher and A. Lascarides. 2003. Logics of
Conversation. Cambridge University Press.
A. Bell. 1999. News Stories as Narratives. In A.
Jaworski and N. Coupland, The Discourse
Reader, Routledge, London and New York, 236-
251.
L. Carlson, D. Marcu and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In
Proceedings of the 2nd SIGDIAL Workshop on
Discourse and Dialogue, Eurospeech 2001,
Aalborg, Denmark.
B. Grosz, A. Joshi and S. Weinstein. 1995.
Centering: A Framework for Modeling the
Local Coherence of Discourse. Computational
Linguistics 2(21), pp. 203-225
J. Hitzeman, M. Moens and C. Grover. 1995.
Algorithms for Analyzing the Temporal
Structure of Discourse. In Proceedings of the
Annual Meeting of the European Chapter of the
Association for Computational Linguistics,
Utrecht, Netherlands, 1995, 253-260.
J. Hobbs. 1985. On the Coherence and Structure of
Discourse. Report No. CSLI-85-37. Stanford,
California: Center for the Study of Language
and Information, Stanford University.
E. Hovy. 1990. Parsimonious and Profligate
Approaches to the Question of Discourse
Structure Relations. In Proceedings of the Fifth
International Workshop on Natural Language
Generation.
H. Kamp and U. Reyle. 1993. Tense and Aspect.
Part 2, Chapter 5 of From Discourse to Logic,
483-546.
P. Kingsbury and M. Palmer. 2002. From
Treebank to PropBank. In Proceedings of the
3rd International Conference on Language
Resources and Evaluation (LREC-2002), Las
Palmas, Spain.
I.. Mani, B. Schiffman and J. Zhang. 2003.
Inferring Temporal Ordering of Events in News.
Proceedings of the Human Language
Technology Conference, HLT’03.
I. Mani and G. Wilson. 2000. Processing of News.
Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics
(ACL&apos;2000), 69-76.
I. Mani, J. Pustejovsky and R. Gaizauskas. 2004.
The Language of Time: A Reader. Oxford
University Press, to appear.
W. Mann and S. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of
text organization. Text, 8(3): 243-281.
D. Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. The
MIT Press.
D. Marcu, E. Amorrortu and M. Romera. 1999.
Experiments in constructing a corpus of
discourse trees. In Proceedings of the ACL
Workshop on Standards and Tools for Discourse
Tagging, College Park, MD, 48-57.
J. McCarthy and P. Hayes. 1969. Some
philosophical problems from the standpoint of
artificial intelligence. In B.Meltzer and D.
Michie, Eds. Machine Intelligence 4.
J. Pustejovsky, B. Ingria, R. Sauri, J. Castano, J.
Littman, R. Gaizauskas, A. Setzer, G. Katz and
I. Mani. 2004. The Specification Language
TimeML. In I. Mani, J. Pustejovsky and R.
Gaizauskas. The Language of Time: A Reader.
Oxford University Press, to appear.
A. Setzer and R. Gaizauskas. 2001. A Pilot Study
on Annotating Temporal Relations in Text. ACL
2001, Workshop on Temporal and Spatial
Information Processing
B. Spejewski. 1994. Temporal Subordination in
Discourse. .Ph.D. Thesis, University of
Rochester.
J. Pustejovsky, I. Mani, L. Belanger, B. Boguraev,
B. Knippen, J. Littman, A. Rumshisky, A. See,
S. Symonenko, J. Van Guilder, L. Van Guilder,
M. Verhagen, R. Ingria. 2003. TANGO Final
Report. timeml.org.
J. Pustejovsky, L. Belanger, J. Castano, R.
Gaizauskas, P. Hanks, R. Ingria, G. Katz, D.
Radev, A. Rumshisky, A. Sanfilippo, R. Sauri,
B. Sundheim, M. Verhagen. 2002. TERQAS
Final Report. timeml.org.
TIMEBANK. 2004. timeml.org.
TIMEX2. 2004. timex2.mitre.org.
B. Webber. 1998. Tense as Discourse Anaphor.
Computational Linguistics 14(2): 61-73.
B. Webber, M. Stone, A. Joshi and A. Knott. 2003.
Computational Linguistics, 29:4, 545-588.
Appendix A: Examples from (Hitzeman et al.
1995)
</reference>
<listItem confidence="0.883928">
1. (a) John entered the room. (b) Mary stood
up.
</listItem>
<bodyText confidence="0.8779762">
Ea is inserted as left daughter of root. Eb is
attached as sister (an analogue of a Narration
default rhetorical relation).
E0
Ea Eb
</bodyText>
<listItem confidence="0.779956333333333">
C: Ea&lt;Eb
2. (a) John entered the room. (b) Mary was
seated behind the desk.
</listItem>
<bodyText confidence="0.8976385">
Ea is anchored as left daughter of root. Eb is a
tensed stative, and is embedded below Ea.
</bodyText>
<page confidence="0.468355">
E0
</page>
<figure confidence="0.874813904761905">
Ea
Eb
C: Eb ⊆min Ea
Ea E1 E2
3. (a) John fell. (b) Mary pushed him.
Eb Ec
C: Eb&lt;Ea, Ec&lt;Eb
7. (a) John got to work late. (b) He had left
the house at 8. (c) He had eaten a big
E0 breakfast.
Ea Eb
C: Eb&lt;Ea E0
4. (a) John entered the room because (b) Mary
stood up. Ea E1 E2
E0
Ea Eb
C: Eb&lt;Ea
This is due to the “because”-inversion rule.
5. (a) Mary was tired. (b) She was exhausted.
Eb
C: Eb ⊆min Ea
</figure>
<bodyText confidence="0.97109875">
This case, unlike (2), would be an analogue of an
Elaboration relation. Here, other knowledge
sources, such as a centering (Grosz et al. 1995)
could play a role in inferring such a discourse
relation.
Ea is attached as right branching event. Eb is
attached as sister with precedence constraint
relative to Ea coming from the past perfect
marking. Ec is attached as sister with precedence
constraint relative to Eb coming from past perfect.
Losing the key is explained by (or elaborated by)
the description of the key falling through the hole.
Hence, it should be an embedding relation on this
reading. Nevertheless, the current parse is arguably
correct since the falling caused the loss of the key.
E0
</bodyText>
<reference confidence="0.885837692307692">
Eb Ec
C: Eb&lt;Ea, Ec&lt;Eb
Appendix B: Level 200 Story from the Brandeis
Reading Corpus
a. David wants to buy a Christmas present for a
very special person, his mother.
b. David&apos;s father gives him $5.00 a week pocket
money and
c. David puts $2.00 a week into his bank
account.
d. After three months David takes $20.00 out of
his bank account and
e. goes to the shopping mall.
f. He looks and looks for a perfect gift.
g. Suddenly he sees a beautiful brooch in the
shape of his favorite pet.
h. He says to himself
i. &amp;quot;Mother loves jewelry, and
j. the brooch costs only $l7.00.&amp;quot;
k. He buys the brooch and
o. He is very excited and
p. he is looking forward to Christmas morning to
see the joy on his mother&apos;s face.
q. But when his mother opens the present
r. she screams with fright because
s. she sees a spider.
</reference>
<bodyText confidence="0.852124">
Ea, Eb, and Ec are all statively interpreted due to
the presence of modification by frequency
adverbial TIMEX3 expressions (from TimeML),
giving rise to habitual event interpretations. They
are embedded inside an abstract E0 node.
</bodyText>
<figure confidence="0.944769363636364">
E0
Ea
6. (a) Sam rang the bell. (b) He had lost the key.
(c) It had fallen through a hole in his pocket. l. takes it home.
m. He wraps the present in Christmas paper and
n. places it under the tree.
E3
E0
Eq Er Es
Ea Eb Ec
C: Ea ⊆min E0, Eb ⊆min E0, Ec ⊆min E0
</figure>
<bodyText confidence="0.778938451612904">
E1 is created with the recognition of the time
expression “after three months”. Ed is attached as
the left daughter node in E1. Ee is attached as
sister (default Narrative). Similarly for Ef, Eg, and
Eh.
E1
Ed Ee Ef Eg Eh
C: Ed&lt;Ee, Ee&lt;Ef, Ef&lt;Eg, Eg&lt;Eh
The syntactically embedded sentences in (i) and
(j) are recognized as states and are embedded
within Eh.
E1
Ed Ee Ef Eg Eh
Ei Ej
C: Ed&lt;Ee, Ee&lt;Ef, Ef&lt;Eg, Eg&lt;Eh, Ei in Eh, Ej in
Eh
Attachment and narrative order holds for Ek,
El, Em, and En. The states in Eo and Ep will
be embedded under En:
E1
Ed Ee Ef Eg Eh Ek El Em En
Ei Ej Eo Ep
The presence of “when” as a TimeML signal
creates a new abstract event, E3, and the
subsequent ordering relation E3&gt;E2.
Finally, narration continues under E3 with Eq,
Er, and Es, as daughters to E3, with the additional
constraint of “because-inversion”, Es&lt;Er.
C: Eq&lt;Er, Es&lt;Er, Eq&lt;Es
The TDM for the entire article is
represented below:
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.378751">
<title confidence="0.996792">Temporal Discourse Models for Narrative Structure</title>
<author confidence="0.988402">Inderjeet MANI</author>
<affiliation confidence="0.999879">Department of Linguistics Georgetown University</affiliation>
<address confidence="0.9942665">ICC 452 Washington, DC 20057</address>
<email confidence="0.998293">im5@georgetown.edu</email>
<author confidence="0.975972">James</author>
<affiliation confidence="0.738028666666667">Department of Computer Brandeis Volen</affiliation>
<address confidence="0.714857">Waltham, Massachusetts</address>
<email confidence="0.99891">jamesp@cs.brandeis.edu</email>
<abstract confidence="0.9916064375">Getting a machine to understand human narratives has been a classic challenge for NLP and AI. This paper proposes a new representation for the temporal structure of narratives. The representation is parsimonious, using temporal relations as surrogates for discourse relations. The narrative models, called Temporal Discourse Models, are treestructured, where nodes include abstract events interpreted as pairs of time points and where the dominance relation is expressed by temporal inclusion. Annotation examples and challenges are discussed, along with a report on progress to date in creating annotated corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
</authors>
<title>Towards a General Theory of Action and Time.</title>
<date>1984</date>
<journal>Artificial Intelligence</journal>
<volume>23</volume>
<pages>123--154</pages>
<contexts>
<context position="10736" citStr="Allen 1984" startWordPosition="1694" endWordPosition="1695">te, etc.), tense (past, present, future), grammatical aspect (perfective, progressive, or both), whether it is negated, any modal operators which govern it, and its cardinality if the event occurs more than once. Likewise, time expressions are flagged, and their values normalized, so that Thursday in He left on Thursday would get a resolved ISO time value depending on context (TIMEX2 2004). Finally, temporal relations between events and time expressions (e.g., that the leaving occurs during Thursday) are recorded by means of temporal links (TLINKs) that express Allen-style interval relations (Allen 1984). Several automatic tools have been developed in conjunction with TimeML, including event taggers (Pustejovsky et al. 2003), time expression taggers (Mani and Wilson 2000), and an exploratory link extractor (Mani et al. 2003). Temporal reasoning algorithms have also been developed, that apply transitivity axioms to expand the links using temporal closure algorithms (Setzer and Gaizauskas 2001), (Pustejovsky et al. 2003). However, TimeML is inadequate as a temporal model of discourse: it constructs no global representation of the narrative structure, instead annotating a complex graph that link</context>
</contexts>
<marker>Allen, 1984</marker>
<rawString>J. F. Allen. 1984. Towards a General Theory of Action and Time. Artificial Intelligence 23: 123-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Asher</author>
<author>A Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3267" citStr="Asher and Lascarides 2003" startWordPosition="481" endWordPosition="484">ng caused the twisting. We can see that even for interpreting such relatively simple discourses, a system might require a variety of sources of linguistic knowledge, including knowledge of tense, aspect, temporal adverbials, discourse relations, as well as background knowledge. Of course, other inferences are clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated. The need for commonsense inferences has motivated computational approaches that are domainspecific, using hand-coded knowledge (e.g., Asher and Lascarides 2003, Hitzeman et al. 1995). A number of theories have postulated the existence of various discourse relations that relate elements in the text to produce a global model of discourse, e.g., (Mann and Thompson 1988), (Hobbs 1985), (Hovy 1990) and others. In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Stru</context>
<context position="13786" citStr="Asher and Lascarides 2003" startWordPosition="2193" endWordPosition="2196">t1, t2, t3 enter(Ea, x, y), man(x), y= theWhiteHart PROG(wear(Eb, x1, y1)), blackjacket(y1), x1=x serve(Ec, x2, y2, z), beer(z), x2=Bill, y2=x t1 &lt; n, Ea c t1, t2 &lt; n, Eb ο t2, Eb c Ea, t3 &lt; n, Ec c t3, Ea &lt; Ec Note that we are by no means claiming that DRSs and TDMs are equivalent. TDMs are tree-structured and DRSs are not, and the inclusion relations involving our abstract events, i.e., Ea c E0 and Ec c E0, are not usually represented in DRT. Nevertheless, there are many similarities between TDMs and DRT which are worth examining for semantic and computational properties. Furthermore, SDRT (Asher and Lascarides 2003) extends DRT to include discourse relations. SDRT and RST both differ fundamentally from TDMs, since we dispense with rhetorical relations. It should be pointed out, nevertheless, that TDMs, as modeled so far, do not represent modality and intensional contexts in the tree structure. (However, information about modality and negation is stored in the nodes based on TimeML preprocessing). One way of addressing this issue is to handle lexically derived modal subordination (such as believe and want) by introducing embedded events, linked to the modal predicate by subordinating relations. For exampl</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>N. Asher and A. Lascarides. 2003. Logics of Conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bell</author>
</authors>
<title>News Stories as Narratives. In</title>
<date>1999</date>
<pages>236--251</pages>
<location>New York,</location>
<contexts>
<context position="1711" citStr="Bell 1999" startWordPosition="255" endWordPosition="256"> When we understand a story, in addition to understanding other aspects such as plot, characters, goals, etc., we are able to understand the order of happening of events. A given text may have multiple stories; when we understand such a text, we are able to tease apart these distinct stories. Thus, understanding the story from a text involves building a global model of the sequences of events in the text, as well as the structure of nested stories. We refer to such models as Temporal Discourse Models (TDMs). Currently, while we have informal descriptions of the structure of narratives, e.g., (Bell 1999), we lack a precise understanding of this aspect of discourse. What sorts of structural configurations are observed? What formal characteristics do they have? For syntactic processing of natural languages, we have, arguably, answers to similar questions. However, for discourse, we have hardly begun to ask the questions. One of the problems here is that most of the information about narrative structure is implicit in the text. Thus, while linguistic information in the form of tense, aspect, temporal adverbials and discourse markers is often present, people use commonsense knowledge to fill in i</context>
</contexts>
<marker>Bell, 1999</marker>
<rawString>A. Bell. 1999. News Stories as Narratives. In A. Jaworski and N. Coupland, The Discourse Reader, Routledge, London and New York, 236-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue, Eurospeech</booktitle>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="4263" citStr="Carlson et al. 2001" startWordPosition="630" endWordPosition="633">urse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993). Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic. This is due to the fact that discourse markers may be absent (i.e., implicit) or ambiguous; but more importantly, because in many cases the precise nature of these discourse relations is unclear. Although (Marcu et al. 1999) (Carlson et al. 2001) reported relatively high levels of inter-annotator agreement, this was based on an annotation procedure where the annotators were allowed to iteratively revise the instructions based on joint discussion. While we appreciate the importance of representing rhetorical relations in order to carry out temporal inferences about event ordering, we believe that there are substantial advantages in isolating the temporal aspects and modeling them separately as TDMs. This greatly simplifies the representation, which we discuss next. 2 Temporal Discourse Models A TDM is a tree-structured syntactic model </context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>L. Carlson, D. Marcu and M. E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue, Eurospeech 2001, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>A Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: A Framework for Modeling the Local Coherence of Discourse.</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<volume>2</volume>
<issue>21</issue>
<pages>203--225</pages>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B. Grosz, A. Joshi and S. Weinstein. 1995. Centering: A Framework for Modeling the Local Coherence of Discourse. Computational Linguistics 2(21), pp. 203-225</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hitzeman</author>
<author>M Moens</author>
<author>C Grover</author>
</authors>
<title>Algorithms for Analyzing the Temporal Structure of Discourse.</title>
<date>1995</date>
<booktitle>In Proceedings of the Annual Meeting of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>253--260</pages>
<location>Utrecht, Netherlands,</location>
<contexts>
<context position="3290" citStr="Hitzeman et al. 1995" startWordPosition="485" endWordPosition="488">can see that even for interpreting such relatively simple discourses, a system might require a variety of sources of linguistic knowledge, including knowledge of tense, aspect, temporal adverbials, discourse relations, as well as background knowledge. Of course, other inferences are clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated. The need for commonsense inferences has motivated computational approaches that are domainspecific, using hand-coded knowledge (e.g., Asher and Lascarides 2003, Hitzeman et al. 1995). A number of theories have postulated the existence of various discourse relations that relate elements in the text to produce a global model of discourse, e.g., (Mann and Thompson 1988), (Hobbs 1985), (Hovy 1990) and others. In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle,</context>
<context position="15943" citStr="Hitzeman et al. 1995" startWordPosition="2529" endWordPosition="2532">ons of temporal coordination and subordination, and is thus a major motivation for our own approach. However, her approach mixes both reference times and events in the same representation, so that the parent-child relation sometimes represents temporal anchoring, and at other times coordination. In the above example of John walked home. He was feeling great, her approach would represent the “reference time” of the state (of feeling great) as being part of the event of walking as well as part of the state, resulting in a graph rather than a strict tree. Note that our approach uses minimality. (Hitzeman et al. 1995) developed a computational approach to distinguish various temporal threads in discourse. The idea here, based on the notion of temporal centering, is that there is one ‘thread’ that the discourse is currently following. Thus, in (1) above, each utterance is associated with exactly one of two threads: (i) going into the florist’s shop and (ii) interacting with Mary. Hitzeman et al. prefer an utterance to continue a current thread which has the same tense or is semantically related to it, so that in (1) above, utterance d would continue the thread (i) above based on tense. In place of world kno</context>
</contexts>
<marker>Hitzeman, Moens, Grover, 1995</marker>
<rawString>J. Hitzeman, M. Moens and C. Grover. 1995. Algorithms for Analyzing the Temporal Structure of Discourse. In Proceedings of the Annual Meeting of the European Chapter of the Association for Computational Linguistics, Utrecht, Netherlands, 1995, 253-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>On the Coherence and Structure of Discourse.</title>
<date>1985</date>
<tech>Report No. CSLI-85-37.</tech>
<institution>Center for the Study of Language and Information, Stanford University.</institution>
<location>Stanford, California:</location>
<contexts>
<context position="3491" citStr="Hobbs 1985" startWordPosition="519" endWordPosition="520">ations, as well as background knowledge. Of course, other inferences are clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated. The need for commonsense inferences has motivated computational approaches that are domainspecific, using hand-coded knowledge (e.g., Asher and Lascarides 2003, Hitzeman et al. 1995). A number of theories have postulated the existence of various discourse relations that relate elements in the text to produce a global model of discourse, e.g., (Mann and Thompson 1988), (Hobbs 1985), (Hovy 1990) and others. In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993). Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic. This is due to the fact that discourse markers may be absent (i.e., implic</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>J. Hobbs. 1985. On the Coherence and Structure of Discourse. Report No. CSLI-85-37. Stanford, California: Center for the Study of Language and Information, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
</authors>
<title>Parsimonious and Profligate Approaches to the Question of Discourse Structure Relations.</title>
<date>1990</date>
<booktitle>In Proceedings of the Fifth International Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="3504" citStr="Hovy 1990" startWordPosition="521" endWordPosition="522">l as background knowledge. Of course, other inferences are clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated. The need for commonsense inferences has motivated computational approaches that are domainspecific, using hand-coded knowledge (e.g., Asher and Lascarides 2003, Hitzeman et al. 1995). A number of theories have postulated the existence of various discourse relations that relate elements in the text to produce a global model of discourse, e.g., (Mann and Thompson 1988), (Hobbs 1985), (Hovy 1990) and others. In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993). Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic. This is due to the fact that discourse markers may be absent (i.e., implicit) or ambigu</context>
</contexts>
<marker>Hovy, 1990</marker>
<rawString>E. Hovy. 1990. Parsimonious and Profligate Approaches to the Question of Discourse Structure Relations. In Proceedings of the Fifth International Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
<author>U Reyle</author>
</authors>
<date>1993</date>
<booktitle>Tense and Aspect. Part 2, Chapter 5 of From Discourse to Logic,</booktitle>
<pages>483--546</pages>
<contexts>
<context position="3896" citStr="Kamp and Reyle, 1993" startWordPosition="574" endWordPosition="577">an et al. 1995). A number of theories have postulated the existence of various discourse relations that relate elements in the text to produce a global model of discourse, e.g., (Mann and Thompson 1988), (Hobbs 1985), (Hovy 1990) and others. In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993). Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic. This is due to the fact that discourse markers may be absent (i.e., implicit) or ambiguous; but more importantly, because in many cases the precise nature of these discourse relations is unclear. Although (Marcu et al. 1999) (Carlson et al. 2001) reported relatively high levels of inter-annotator agreement, this was based on an annotation procedure where the annotators were allowed to iteratively revise the instructions based on joint discussion. While we appreciate the impo</context>
<context position="12564" citStr="Kamp and Reyle 1993" startWordPosition="1968" endWordPosition="1971">resentation, e.g., as might be found in the PropBank (Kingsbury and Palmer 2002), can be associated with each node. TDMs can also be mapped to Discourse Representation Structures (DRS) (which in turn can be mapped to a logical form). Since TDMs represent events as pairs of time points (which can be viewed as intervals), and DRT represents events as primitives, we can reintroduce time intervals based on the standard DRT approach (e ⊆ t for events, e O t for states, except for present tense states, where t ⊆ e). Consider an example from the Discourse Representation Theory (DRT) literature (from Kamp and Reyle 1993): (3) a. A man entered the White Hart. b. He was wearing a black jacket. c. Bill served him a beer. The TDM is &lt;T3, C3&gt; below, with internal properties of the nodes as shown: T3 = E0 Ea Ec Eb C3 = {Ea &lt; Ec} node.properties(Ea): enter(Ea, x, y), man(x), y= theWhiteHart, Ea &lt; n node.properties(Eb): PROG(wear(Eb, x1, y1)), black-jacket(y1), x1=x, Eb &lt; n, node.properties(Ec): serve(Ec, x2, y2, z), beer(z), x2=Bill, y2=x, Ec &lt; n From T3: Eb c Ea From C3: Ea &lt; Ec The DRT representation is shown below (here we have created variables for the reference times): Ea, x, y , Eb, x1, y1, Ec, x2, y2, z, t1, </context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>H. Kamp and U. Reyle. 1993. Tense and Aspect. Part 2, Chapter 5 of From Discourse to Logic, 483-546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="12024" citStr="Kingsbury and Palmer 2002" startWordPosition="1876" endWordPosition="1879">elations in TDMs involve temporal inclusion and temporal ordering, the mentioned events can naturally be mapped to other discourse representations used in computational linguistics. A TDM tree can be converted to a first-order temporal logic representation (where temporal ordering and inclusion operators are added) by expanding the properties of the nodes. These properties include any additional predications made explicitly about the event, e.g., information from thematic arguments and adjuncts. In other words, a full predicate argument representation, e.g., as might be found in the PropBank (Kingsbury and Palmer 2002), can be associated with each node. TDMs can also be mapped to Discourse Representation Structures (DRS) (which in turn can be mapped to a logical form). Since TDMs represent events as pairs of time points (which can be viewed as intervals), and DRT represents events as primitives, we can reintroduce time intervals based on the standard DRT approach (e ⊆ t for events, e O t for states, except for present tense states, where t ⊆ e). Consider an example from the Discourse Representation Theory (DRT) literature (from Kamp and Reyle 1993): (3) a. A man entered the White Hart. b. He was wearing a b</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>P. Kingsbury and M. Palmer. 2002. From Treebank to PropBank. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>B Schiffman</author>
<author>J Zhang</author>
</authors>
<title>Inferring Temporal Ordering of Events in News.</title>
<date>2003</date>
<booktitle>Proceedings of the Human Language Technology Conference, HLT’03.</booktitle>
<contexts>
<context position="10961" citStr="Mani et al. 2003" startWordPosition="1725" endWordPosition="1728">se, time expressions are flagged, and their values normalized, so that Thursday in He left on Thursday would get a resolved ISO time value depending on context (TIMEX2 2004). Finally, temporal relations between events and time expressions (e.g., that the leaving occurs during Thursday) are recorded by means of temporal links (TLINKs) that express Allen-style interval relations (Allen 1984). Several automatic tools have been developed in conjunction with TimeML, including event taggers (Pustejovsky et al. 2003), time expression taggers (Mani and Wilson 2000), and an exploratory link extractor (Mani et al. 2003). Temporal reasoning algorithms have also been developed, that apply transitivity axioms to expand the links using temporal closure algorithms (Setzer and Gaizauskas 2001), (Pustejovsky et al. 2003). However, TimeML is inadequate as a temporal model of discourse: it constructs no global representation of the narrative structure, instead annotating a complex graph that links primitive events and times. 4 Related Frameworks Since the relations in TDMs involve temporal inclusion and temporal ordering, the mentioned events can naturally be mapped to other discourse representations used in computat</context>
</contexts>
<marker>Mani, Schiffman, Zhang, 2003</marker>
<rawString>I.. Mani, B. Schiffman and J. Zhang. 2003. Inferring Temporal Ordering of Events in News. Proceedings of the Human Language Technology Conference, HLT’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>G Wilson</author>
</authors>
<title>Processing of News.</title>
<date>2000</date>
<booktitle>Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL&apos;2000),</booktitle>
<pages>69--76</pages>
<contexts>
<context position="10907" citStr="Mani and Wilson 2000" startWordPosition="1716" endWordPosition="1719">its cardinality if the event occurs more than once. Likewise, time expressions are flagged, and their values normalized, so that Thursday in He left on Thursday would get a resolved ISO time value depending on context (TIMEX2 2004). Finally, temporal relations between events and time expressions (e.g., that the leaving occurs during Thursday) are recorded by means of temporal links (TLINKs) that express Allen-style interval relations (Allen 1984). Several automatic tools have been developed in conjunction with TimeML, including event taggers (Pustejovsky et al. 2003), time expression taggers (Mani and Wilson 2000), and an exploratory link extractor (Mani et al. 2003). Temporal reasoning algorithms have also been developed, that apply transitivity axioms to expand the links using temporal closure algorithms (Setzer and Gaizauskas 2001), (Pustejovsky et al. 2003). However, TimeML is inadequate as a temporal model of discourse: it constructs no global representation of the narrative structure, instead annotating a complex graph that links primitive events and times. 4 Related Frameworks Since the relations in TDMs involve temporal inclusion and temporal ordering, the mentioned events can naturally be mapp</context>
</contexts>
<marker>Mani, Wilson, 2000</marker>
<rawString>I. Mani and G. Wilson. 2000. Processing of News. Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL&apos;2000), 69-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>J Pustejovsky</author>
<author>R Gaizauskas</author>
</authors>
<title>The Language of Time: A Reader.</title>
<date>2004</date>
<publisher>Oxford University Press,</publisher>
<note>to appear.</note>
<contexts>
<context position="9632" citStr="Mani et al. 2004" startWordPosition="1528" endWordPosition="1531">ning subsequent events. This is not captured by the current tree representation. Opposition structures of predicates and gating operations over properties can be expressed as constraints introduced by events, however, but at this stage of development, we have been interested in capturing a coarser temporal ordering representation, very robustly. We believe, however, that annotation using the minimal inclusion relation will allow us to reason about persistence heuristically in the future. 3 Prerequisites Prior work on temporal information extraction has been fairly extensive and is covered in (Mani et al. 2004). Recent research has developed the TimeML annotation scheme (Pustejovsky et al. 2002) (Pustejovsky et al. 2004), as well as a corpus of TimeML-annotated news stories (TimeBank 2004) and annotation tools that go along with it, such as the TANGO tool (Pustejovsky et al. 2003). TimeML flags tensed verbs, adjectives, and nominals that correspond to events and states, tagging instances of them with standard TimeML attributes, including the class of event (perception, reporting, aspectual, state, etc.), tense (past, present, future), grammatical aspect (perfective, progressive, or both), whether it</context>
</contexts>
<marker>Mani, Pustejovsky, Gaizauskas, 2004</marker>
<rawString>I. Mani, J. Pustejovsky and R. Gaizauskas. 2004. The Language of Time: A Reader. Oxford University Press, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>243--281</pages>
<contexts>
<context position="3477" citStr="Mann and Thompson 1988" startWordPosition="515" endWordPosition="518"> adverbials, discourse relations, as well as background knowledge. Of course, other inferences are clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated. The need for commonsense inferences has motivated computational approaches that are domainspecific, using hand-coded knowledge (e.g., Asher and Lascarides 2003, Hitzeman et al. 1995). A number of theories have postulated the existence of various discourse relations that relate elements in the text to produce a global model of discourse, e.g., (Mann and Thompson 1988), (Hobbs 1985), (Hovy 1990) and others. In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993). Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic. This is due to the fact that discourse markers may be absent</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W. Mann and S. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3): 243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="3562" citStr="Marcu 2000" startWordPosition="531" endWordPosition="532">e clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated. The need for commonsense inferences has motivated computational approaches that are domainspecific, using hand-coded knowledge (e.g., Asher and Lascarides 2003, Hitzeman et al. 1995). A number of theories have postulated the existence of various discourse relations that relate elements in the text to produce a global model of discourse, e.g., (Mann and Thompson 1988), (Hobbs 1985), (Hovy 1990) and others. In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993). Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic. This is due to the fact that discourse markers may be absent (i.e., implicit) or ambiguous; but more importantly, because in many cases the preci</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>E Amorrortu</author>
<author>M Romera</author>
</authors>
<title>Experiments in constructing a corpus of discourse trees.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL Workshop on Standards and Tools for Discourse Tagging,</booktitle>
<pages>48--57</pages>
<location>College Park, MD,</location>
<contexts>
<context position="4241" citStr="Marcu et al. 1999" startWordPosition="626" endWordPosition="629">rresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993). Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic. This is due to the fact that discourse markers may be absent (i.e., implicit) or ambiguous; but more importantly, because in many cases the precise nature of these discourse relations is unclear. Although (Marcu et al. 1999) (Carlson et al. 2001) reported relatively high levels of inter-annotator agreement, this was based on an annotation procedure where the annotators were allowed to iteratively revise the instructions based on joint discussion. While we appreciate the importance of representing rhetorical relations in order to carry out temporal inferences about event ordering, we believe that there are substantial advantages in isolating the temporal aspects and modeling them separately as TDMs. This greatly simplifies the representation, which we discuss next. 2 Temporal Discourse Models A TDM is a tree-struc</context>
</contexts>
<marker>Marcu, Amorrortu, Romera, 1999</marker>
<rawString>D. Marcu, E. Amorrortu and M. Romera. 1999. Experiments in constructing a corpus of discourse trees. In Proceedings of the ACL Workshop on Standards and Tools for Discourse Tagging, College Park, MD, 48-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCarthy</author>
<author>P Hayes</author>
</authors>
<title>Some philosophical problems from the standpoint of artificial intelligence.</title>
<date>1969</date>
<journal>Eds. Machine Intelligence</journal>
<booktitle>In B.Meltzer</booktitle>
<volume>4</volume>
<contexts>
<context position="7977" citStr="McCarthy and Hayes 1969" startWordPosition="1265" endWordPosition="1268">l ordering is directly represented in the tree. Since the nodes in this representation are at a semantic level, the tree structure is not necessarily isomorphic to a representation at the text level, although T1 happens to be isomorphic. Eb Ec Ed C2 = {Ea &lt; Ee, Eb &lt; Ec} Note that the partial ordering C can be extended using T and temporal closure axioms (Setzer and Gaizauskas 2001), (Verhagen 2004), so that in the case of &lt;T2, C2&gt;, we can infer, for example, that Eb &lt; Ed, Ed &lt; Ee, and so forth. In representing states, we take a conservative approach to the problems of ramification and change (McCarthy and Hayes 1969). This is the classic problem of recognizing when states (the effects of actions) change as a result of actions. Any tensed stative predicate will be represented as a node in the tree (progressives are here treated as stative). Consider an example like John walked home. He was feeling great. Here we represent the state of feeling great as being minimally a part of the event of walking, without committing to whether it extends before or Ea Ee after the event. While this is interpreted as an overloaded temporal inclusion in the TDM tree, a constraint is added to C indicating that this inclusion </context>
</contexts>
<marker>McCarthy, Hayes, 1969</marker>
<rawString>J. McCarthy and P. Hayes. 1969. Some philosophical problems from the standpoint of artificial intelligence. In B.Meltzer and D. Michie, Eds. Machine Intelligence 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>B Ingria</author>
<author>R Sauri</author>
<author>J Castano</author>
<author>J Littman</author>
<author>R Gaizauskas</author>
<author>A Setzer</author>
<author>G Katz</author>
<author>I Mani</author>
</authors>
<title>The Specification Language TimeML. In</title>
<date>2004</date>
<publisher>Oxford University Press,</publisher>
<note>to appear.</note>
<contexts>
<context position="9744" citStr="Pustejovsky et al. 2004" startWordPosition="1544" endWordPosition="1547"> predicates and gating operations over properties can be expressed as constraints introduced by events, however, but at this stage of development, we have been interested in capturing a coarser temporal ordering representation, very robustly. We believe, however, that annotation using the minimal inclusion relation will allow us to reason about persistence heuristically in the future. 3 Prerequisites Prior work on temporal information extraction has been fairly extensive and is covered in (Mani et al. 2004). Recent research has developed the TimeML annotation scheme (Pustejovsky et al. 2002) (Pustejovsky et al. 2004), as well as a corpus of TimeML-annotated news stories (TimeBank 2004) and annotation tools that go along with it, such as the TANGO tool (Pustejovsky et al. 2003). TimeML flags tensed verbs, adjectives, and nominals that correspond to events and states, tagging instances of them with standard TimeML attributes, including the class of event (perception, reporting, aspectual, state, etc.), tense (past, present, future), grammatical aspect (perfective, progressive, or both), whether it is negated, any modal operators which govern it, and its cardinality if the event occurs more than once. Likewi</context>
</contexts>
<marker>Pustejovsky, Ingria, Sauri, Castano, Littman, Gaizauskas, Setzer, Katz, Mani, 2004</marker>
<rawString>J. Pustejovsky, B. Ingria, R. Sauri, J. Castano, J. Littman, R. Gaizauskas, A. Setzer, G. Katz and I. Mani. 2004. The Specification Language TimeML. In I. Mani, J. Pustejovsky and R. Gaizauskas. The Language of Time: A Reader. Oxford University Press, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Setzer</author>
<author>R Gaizauskas</author>
</authors>
<title>A Pilot Study on Annotating Temporal Relations</title>
<date>2001</date>
<booktitle>in Text. ACL 2001, Workshop on Temporal and Spatial Information Processing</booktitle>
<contexts>
<context position="7737" citStr="Setzer and Gaizauskas 2001" startWordPosition="1221" endWordPosition="1224">ime points. So, E0 is an abstract node representing a top-level story, and E1 is an abstract node representing an embedded story. Note that the mentioned events are ordered left to right in text order for notational convenience, but no temporal ordering is directly represented in the tree. Since the nodes in this representation are at a semantic level, the tree structure is not necessarily isomorphic to a representation at the text level, although T1 happens to be isomorphic. Eb Ec Ed C2 = {Ea &lt; Ee, Eb &lt; Ec} Note that the partial ordering C can be extended using T and temporal closure axioms (Setzer and Gaizauskas 2001), (Verhagen 2004), so that in the case of &lt;T2, C2&gt;, we can infer, for example, that Eb &lt; Ed, Ed &lt; Ee, and so forth. In representing states, we take a conservative approach to the problems of ramification and change (McCarthy and Hayes 1969). This is the classic problem of recognizing when states (the effects of actions) change as a result of actions. Any tensed stative predicate will be represented as a node in the tree (progressives are here treated as stative). Consider an example like John walked home. He was feeling great. Here we represent the state of feeling great as being minimally a p</context>
<context position="11132" citStr="Setzer and Gaizauskas 2001" startWordPosition="1748" endWordPosition="1751">TIMEX2 2004). Finally, temporal relations between events and time expressions (e.g., that the leaving occurs during Thursday) are recorded by means of temporal links (TLINKs) that express Allen-style interval relations (Allen 1984). Several automatic tools have been developed in conjunction with TimeML, including event taggers (Pustejovsky et al. 2003), time expression taggers (Mani and Wilson 2000), and an exploratory link extractor (Mani et al. 2003). Temporal reasoning algorithms have also been developed, that apply transitivity axioms to expand the links using temporal closure algorithms (Setzer and Gaizauskas 2001), (Pustejovsky et al. 2003). However, TimeML is inadequate as a temporal model of discourse: it constructs no global representation of the narrative structure, instead annotating a complex graph that links primitive events and times. 4 Related Frameworks Since the relations in TDMs involve temporal inclusion and temporal ordering, the mentioned events can naturally be mapped to other discourse representations used in computational linguistics. A TDM tree can be converted to a first-order temporal logic representation (where temporal ordering and inclusion operators are added) by expanding the </context>
</contexts>
<marker>Setzer, Gaizauskas, 2001</marker>
<rawString>A. Setzer and R. Gaizauskas. 2001. A Pilot Study on Annotating Temporal Relations in Text. ACL 2001, Workshop on Temporal and Spatial Information Processing</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Spejewski</author>
</authors>
<title>Temporal Subordination in Discourse.</title>
<date>1994</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="15207" citStr="Spejewski (1994)" startWordPosition="2411" endWordPosition="2412">e where explicit or implicit discourse markers relating only primitive discourse units. Unlike TDMs, where the nodes in the tree can contain embedded structures, DLTAG is a local model of discourse structure; it thus provides a set of binary relations, rather than a tree Like TDMs, however, DLTAG models discourse structure without postulating the existence of rhetorical relations in the discourse tree. Instead, the rhetorical relations appear as predicates in the semantic forms for discourse markers. In this respect, they differ from TDMs, which do not commit to specific rhetorical relations. Spejewski (1994) developed a tree-based model of the temporal structure of a sequence of sentences. Her approach is based on relations of temporal coordination and subordination, and is thus a major motivation for our own approach. However, her approach mixes both reference times and events in the same representation, so that the parent-child relation sometimes represents temporal anchoring, and at other times coordination. In the above example of John walked home. He was feeling great, her approach would represent the “reference time” of the state (of feeling great) as being part of the event of walking as w</context>
</contexts>
<marker>Spejewski, 1994</marker>
<rawString>B. Spejewski. 1994. Temporal Subordination in Discourse. .Ph.D. Thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>I Mani</author>
<author>L Belanger</author>
<author>B Boguraev</author>
<author>B Knippen</author>
<author>J Littman</author>
<author>A Rumshisky</author>
<author>A See</author>
<author>S Symonenko</author>
<author>J Van Guilder</author>
<author>L Van Guilder</author>
<author>M Verhagen</author>
<author>R Ingria</author>
</authors>
<date>2003</date>
<tech>TANGO Final Report. timeml.org.</tech>
<marker>Pustejovsky, Mani, Belanger, Boguraev, Knippen, Littman, Rumshisky, See, Symonenko, Van Guilder, Van Guilder, Verhagen, Ingria, 2003</marker>
<rawString>J. Pustejovsky, I. Mani, L. Belanger, B. Boguraev, B. Knippen, J. Littman, A. Rumshisky, A. See, S. Symonenko, J. Van Guilder, L. Van Guilder, M. Verhagen, R. Ingria. 2003. TANGO Final Report. timeml.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>L Belanger</author>
<author>J Castano</author>
<author>R Gaizauskas</author>
<author>P Hanks</author>
<author>R Ingria</author>
<author>G Katz</author>
<author>D Radev</author>
<author>A Rumshisky</author>
<author>A Sanfilippo</author>
<author>R Sauri</author>
<author>B Sundheim</author>
<author>M Verhagen</author>
</authors>
<date>2002</date>
<tech>TERQAS Final Report. timeml.org.</tech>
<contexts>
<context position="9718" citStr="Pustejovsky et al. 2002" startWordPosition="1540" endWordPosition="1543">. Opposition structures of predicates and gating operations over properties can be expressed as constraints introduced by events, however, but at this stage of development, we have been interested in capturing a coarser temporal ordering representation, very robustly. We believe, however, that annotation using the minimal inclusion relation will allow us to reason about persistence heuristically in the future. 3 Prerequisites Prior work on temporal information extraction has been fairly extensive and is covered in (Mani et al. 2004). Recent research has developed the TimeML annotation scheme (Pustejovsky et al. 2002) (Pustejovsky et al. 2004), as well as a corpus of TimeML-annotated news stories (TimeBank 2004) and annotation tools that go along with it, such as the TANGO tool (Pustejovsky et al. 2003). TimeML flags tensed verbs, adjectives, and nominals that correspond to events and states, tagging instances of them with standard TimeML attributes, including the class of event (perception, reporting, aspectual, state, etc.), tense (past, present, future), grammatical aspect (perfective, progressive, or both), whether it is negated, any modal operators which govern it, and its cardinality if the event occ</context>
</contexts>
<marker>Pustejovsky, Belanger, Castano, Gaizauskas, Hanks, Ingria, Katz, Radev, Rumshisky, Sanfilippo, Sauri, Sundheim, Verhagen, 2002</marker>
<rawString>J. Pustejovsky, L. Belanger, J. Castano, R. Gaizauskas, P. Hanks, R. Ingria, G. Katz, D. Radev, A. Rumshisky, A. Sanfilippo, R. Sauri, B. Sundheim, M. Verhagen. 2002. TERQAS Final Report. timeml.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TIMEBANK</author>
</authors>
<date>2004</date>
<marker>TIMEBANK, 2004</marker>
<rawString>TIMEBANK. 2004. timeml.org. TIMEX2. 2004. timex2.mitre.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
</authors>
<title>Tense as Discourse Anaphor.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>14</volume>
<issue>2</issue>
<pages>61--73</pages>
<marker>Webber, 1998</marker>
<rawString>B. Webber. 1998. Tense as Discourse Anaphor. Computational Linguistics 14(2): 61-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
<author>M Stone</author>
<author>A Joshi</author>
<author>A Knott</author>
</authors>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<pages>545--588</pages>
<marker>Webber, Stone, Joshi, Knott, 2003</marker>
<rawString>B. Webber, M. Stone, A. Joshi and A. Knott. 2003. Computational Linguistics, 29:4, 545-588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Appendix</author>
</authors>
<title>Examples from (Hitzeman et al.</title>
<date>1995</date>
<marker>Appendix, 1995</marker>
<rawString>Appendix A: Examples from (Hitzeman et al. 1995)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eb Ec C EbEa</author>
</authors>
<booktitle>Ec&lt;Eb Appendix B: Level 200 Story from the Brandeis</booktitle>
<location>Reading Corpus</location>
<marker>EbEa, </marker>
<rawString>Eb Ec C: Eb&lt;Ea, Ec&lt;Eb Appendix B: Level 200 Story from the Brandeis Reading Corpus</rawString>
</citation>
<citation valid="false">
<authors>
<author>a David</author>
</authors>
<title>wants to buy a Christmas present for a very special person, his mother.</title>
<marker>David, </marker>
<rawString>a. David wants to buy a Christmas present for a very special person, his mother.</rawString>
</citation>
<citation valid="false">
<authors>
<author>b David&apos;s</author>
</authors>
<title>father gives him $5.00 a week pocket money and c. David puts $2.00 a week into his bank account.</title>
<marker>David&apos;s, </marker>
<rawString>b. David&apos;s father gives him $5.00 a week pocket money and c. David puts $2.00 a week into his bank account.</rawString>
</citation>
<citation valid="false">
<authors>
<author>d</author>
</authors>
<title>After three months David takes $20.00 out of his bank account and e. goes to the shopping mall.</title>
<marker>d, </marker>
<rawString>d. After three months David takes $20.00 out of his bank account and e. goes to the shopping mall.</rawString>
</citation>
<citation valid="false">
<authors>
<author>f</author>
</authors>
<title>He looks and looks for a perfect gift. g. Suddenly he sees a beautiful brooch in the shape of his favorite pet.</title>
<marker>f, </marker>
<rawString>f. He looks and looks for a perfect gift. g. Suddenly he sees a beautiful brooch in the shape of his favorite pet.</rawString>
</citation>
<citation valid="false">
<authors>
<author>h</author>
</authors>
<title>He says to himself i. &amp;quot;Mother loves jewelry, and j. the brooch costs only $l7.00.&amp;quot; k. He buys the brooch and o. He is very excited and p. he is looking forward to Christmas morning to see the joy on his mother&apos;s face.</title>
<marker>h, </marker>
<rawString>h. He says to himself i. &amp;quot;Mother loves jewelry, and j. the brooch costs only $l7.00.&amp;quot; k. He buys the brooch and o. He is very excited and p. he is looking forward to Christmas morning to see the joy on his mother&apos;s face.</rawString>
</citation>
<citation valid="false">
<authors>
<author>q But</author>
</authors>
<title>when his mother opens the present r. she screams with fright because s. she sees a spider.</title>
<marker>But, </marker>
<rawString>q. But when his mother opens the present r. she screams with fright because s. she sees a spider.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>