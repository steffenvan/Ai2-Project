<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000303">
<title confidence="0.91194">
Semeval-2013 Task 8:
Cross-lingual Textual Entailment for Content Synchronization
</title>
<author confidence="0.986736">
Matteo Negri Alessandro Marchetti Yashar Mehdad
</author>
<affiliation confidence="0.845214">
FBK-irst CELCT UBC
Trento, Italy Trento, Italy Vancouver, Canada
</affiliation>
<email confidence="0.97215">
negri@fbk.eu amarchetti@celct.it mehdad@cs.ubc.ca
</email>
<author confidence="0.959265">
Luisa Bentivogli
</author>
<affiliation confidence="0.888784">
FBK-irst
</affiliation>
<address confidence="0.74681">
Trento, Italy
</address>
<email confidence="0.991501">
bentivo@fbk.eu
</email>
<sectionHeader confidence="0.995452" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997948125">
This paper presents the second round of the
task on Cross-lingual Textual Entailment for
Content Synchronization, organized within
SemEval-2013. The task was designed to pro-
mote research on semantic inference over texts
written in different languages, targeting at the
same time a real application scenario. Par-
ticipants were presented with datasets for dif-
ferent language pairs, where multi-directional
entailment relations (“forward”, “backward”,
“bidirectional”, “no entailment”) had to be
identified. We report on the training and test
data used for evaluation, the process of their
creation, the participating systems (six teams,
61 runs), the approaches adopted and the re-
sults achieved.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999846066666667">
The cross-lingual textual entailment task (Mehdad et
al., 2010) addresses textual entailment (TE) recog-
nition (Dagan and Glickman, 2004) under the new
dimension of cross-linguality, and within the new
challenging application scenario of content synchro-
nization. Given two texts in different languages, the
cross-lingual textual entailment (CLTE) task con-
sists of deciding if the meaning of one text can be
inferred from the meaning of the other text. Cross-
linguality represents an interesting direction for re-
search on recognizing textual entailment (RTE), es-
pecially due to its possible application in a vari-
ety of tasks. Among others (e.g. question answer-
ing, information retrieval, information extraction,
and document summarization), multilingual content
</bodyText>
<sectionHeader confidence="0.5172665" genericHeader="introduction">
Danilo Giampiccolo
CELCT
</sectionHeader>
<bodyText confidence="0.973039916666667">
Trento, Italy
giampiccolo@celct.it
synchronization represents a challenging application
scenario to evaluate CLTE recognition components
geared to the identification of sentence-level seman-
tic relations.
Given two documents about the same topic writ-
ten in different languages (e.g. Wikipedia pages),
the content synchronization task consists of au-
tomatically detecting and resolving differences in
the information they provide, in order to produce
aligned, mutually enriched versions of the two docu-
ments (Monz et al., 2011; Bronner et al., 2012). To-
wards this objective, a crucial requirement is to iden-
tify the information in one page that is either equiv-
alent or novel (more informative) with respect to the
content of the other. The task can be naturally cast
as an entailment recognition problem, where bidi-
rectional and unidirectional entailment judgements
for two text fragments are respectively mapped into
judgements about semantic equivalence and novelty.
The task can also be seen as a machine translation
evaluation problem, where judgements about se-
mantic equivalence and novelty depend on the pos-
sibility to fully or partially translate a text fragment
into the other.
The recent advances on monolingual TE on the
one hand, and the methodologies used in Statisti-
cal Machine Translation (SMT) on the other, offer
promising solutions to approach the CLTE task. In
line with a number of systems that model the RTE
task as a similarity problem (i.e. handling similar-
ity scores between T and H as features contributing
to the entailment decision), the standard sentence
and word alignment programs used in SMT offer
a strong baseline for CLTE (Mehdad et al., 2011;
</bodyText>
<page confidence="0.982109">
25
</page>
<note confidence="0.895102">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 25–33, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999916">
Figure 1: Example of SP-EN CLTE pairs.
</figureCaption>
<bodyText confidence="0.998942375">
Mehdad et al., 2012). However, although repre-
senting a solid starting point to approach the prob-
lem, similarity-based techniques are just approx-
imations, open to significant improvements com-
ing from semantic inference at the multilingual
level (e.g. cross-lingual entailment rules such as
“perro”→“animal”). Taken in isolation, similarity-
based techniques clearly fall short of providing an
effective solution to the problem of assigning direc-
tions to the entailment relations (especially in the
complex CLTE scenario, where entailment relations
are multi-directional). Thanks to the contiguity be-
tween CLTE, TE and SMT, the proposed task pro-
vides an interesting scenario to approach the issues
outlined above from different perspectives, and of-
fers large room for mutual improvement.
Building on the success of the first CLTE evalua-
tion organized within SemEval-2012 (Negri et al.,
2012a), the remainder of this paper describes the
second evaluation round organized within SemEval-
2013. The following sections provide an overview
of the datasets used, the participating systems, the
approaches adopted, the achieved results, and the
lessons learned.
</bodyText>
<sectionHeader confidence="0.938438" genericHeader="method">
2 The task
</sectionHeader>
<bodyText confidence="0.9995058">
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgements (see Figure 1 for
Spanish/English examples of each judgement):
</bodyText>
<listItem confidence="0.9588922">
• bidirectional (T1→T2 &amp; T1←T2): the two
fragments entail each other (semantic equiva-
lence);
• forward (T1→T2 &amp; T16←T2): unidirectional
entailment from T1 to T2;
• backward (T16→T2 &amp; T1←T2): unidirectional
entailment from T2 to T1;
• no entailment (T16→T2 &amp; T16←T2): there is
no entailment between T1 and T2 in either di-
rection;
</listItem>
<bodyText confidence="0.9926975">
In this task, both T1 and T2 are assumed to be
true statements. Although contradiction is relevant
from an application-oriented perspective, contradic-
tory pairs are not present in the dataset.
</bodyText>
<sectionHeader confidence="0.99457" genericHeader="method">
3 Dataset description
</sectionHeader>
<bodyText confidence="0.999945">
The CLTE-2013 dataset is composed of four CLTE
corpora created for the following language combi-
nations: Spanish/English (SP-EN), Italian/English
(IT-EN), French/English (FR-EN), German/English
(DE-EN). Each corpus consists of 1,500 sentence
pairs (1,000 for training and 500 for test), balanced
across the four entailment judgements.
In this year’s evaluation, as training set we used
the CLTE-2012 corpus1 that was created for the
SemEval-2012 evaluation exercise2 (including both
training and test sets). The CLTE-2013 test set was
created from scratch, following the methodology de-
scribed in the next section.
</bodyText>
<subsectionHeader confidence="0.999876">
3.1 Data collection and annotation
</subsectionHeader>
<bodyText confidence="0.998782">
To collect the entailment pairs for the 2013 test set
we adopted a slightly modified version of the crowd-
sourcing methodology followed to create the CLTE-
2012 corpus (Negri et al., 2011). The main differ-
ence with last year’s procedure is that we did not
take advantage of crowdsourcing for the whole data
collection process, but only for part of it.
As for CLTE-2012, the collection and annotation
process consists of the following steps:
</bodyText>
<listItem confidence="0.99562525">
1. First, English sentences were manually ex-
tracted from Wikipedia and Wikinews. The se-
lected sentences represent one of the elements
(T1) of each entailment pair;
</listItem>
<footnote confidence="0.9999845">
1http://www.celct.it/resources.php?id page=CLTE
2http://www.cs.york.ac.uk/semeval-2012/task8/
</footnote>
<page confidence="0.998706">
26
</page>
<bodyText confidence="0.981930518518518">
2. Next, each T1 was modified in various ways
in order to obtain a corresponding T2. While
in the CLTE-2012 dataset the whole T2 cre-
ation process was carried out through crowd-
sourcing, for the CLTE-2013 test set we crowd-
sourced only the first phase of T1 modification,
namely the creation of paraphrases. Focusing
on the creation of high quality paraphrases, we
followed the crowdsourcing methodology ex-
perimented in Negri et al. (2012b), in which
a paraphrase is obtained through an itera-
tive modification process of an original sen-
tence, by asking workers to introduce meaning-
preserving lexical and syntactic changes. At
each round of the iteration, new workers are
presented with the output of the previous iter-
ation in order to increase divergence from the
original sentence. At the end of the process,
only the more divergent paraphrases according
to the Lesk score (Lesk, 1986) are selected. As
for the second phase of T2 creation process,
this year it was carried out by expert annota-
tors, who followed the same criteria used last
year for the crowdsourced tasks, i.e. i) remove
information from the input (paraphrased) sen-
tence and ii) add information from sentences
surrounding T1 in the source article;
</bodyText>
<listItem confidence="0.923750421052631">
3. Each T2 was then paired to the original T1, and
the resulting pairs were annotated with one of
the four entailment judgements. In order to re-
duce the correlation between the difference in
sentences’ length and entailment judgements,
only the pairs where the difference between the
number of words in T1 and T2 (length diff) was
below a fixed threshold (10 words) were re-
tained.3 The final result is a monolingual En-
glish dataset annotated with multi-directional
entailment judgements, which are well dis-
tributed over length diff values ranging from 0
to 9;
4. In order to create the cross-lingual datasets,
each English T1 was manually translated into
3Such constraint has been applied in order to focus as much
as possible on semantic aspects of the problem, by reduc-
ing the applicability of simple association rules such as IF
length(T1)&gt;length(T2) THEN T1--+T2.
</listItem>
<bodyText confidence="0.975281411764706">
four different languages (i.e. Spanish, German,
Italian and French) by expert translators;
5. By pairing the translated T1 with the cor-
responding T2 in English, four cross-lingual
datasets were obtained.
To ensure the good quality of the datasets, all the
collected pairs were cross-annotated and filtered to
retain only those pairs with full agreement in the
entailment judgement between two expert annota-
tors. The final result is a multilingual parallel en-
tailment corpus, where T1s are in 5 different lan-
guages (i.e. English, Spanish, German, Italian, and
French), and T2s are in English. It is worth men-
tioning that the monolingual English corpus, a by-
product of our data collection methodology, will be
publicly released as a further contribution to the re-
search community.
</bodyText>
<subsectionHeader confidence="0.999226">
3.2 Dataset statistics
</subsectionHeader>
<bodyText confidence="0.9999875">
As described in section 3.1, the methodology fol-
lowed to create the training and test sets was the
same except for the crowdsourced tasks. This al-
lowed us to obtain two datasets with the same bal-
ance across the entailment judgements, and to keep
under control the distribution of the pairs for differ-
ent length diff values in each language combination.
Training Set. The training set is composed of
1,000 CLTE pairs for each language combina-
tion, balanced across the four entailment judge-
ments (bidirectional, forward, backward, and
no entailment). As shown in Table 1, our data col-
lection procedure led to a dataset where the major-
ity of the pairs falls in the +5 -5 length diff range
for each language pair (67.2% on average across the
four language pairs). This characteristic is partic-
ularly relevant as our assumption is that such data
distribution makes entailment judgements based on
mere surface features such as sentence length inef-
fective, thus encouraging the development of alter-
native, deeper processing strategies.
Test Set. The test set is composed of 500 entail-
ment pairs for each language combination, balanced
across the four entailment judgements. As shown
in Table 2, also in this dataset the majority of the
collected entailment pairs is uniformly distributed
</bodyText>
<page confidence="0.983368">
27
</page>
<figure confidence="0.9993535">
(a) SP-EN (b) IT-EN
(c) FR-EN (d) DE-EN
</figure>
<figureCaption confidence="0.999181">
Figure 2: Pair distribution in the 2013 test set: total number of pairs (y-axis) for different length diff values (x-axis).
</figureCaption>
<table confidence="0.99872">
SP-EN IT-EN FR-EN DE-EN
Forward 104 132 121 179
Backward 202 182 191 123
No entailment 163 173 169 174
Bidirectional 175 199 193 209
ALL 644 686 674 685
% (out of 1,000) 64.4 68.6 67.4 68.5
</table>
<tableCaption confidence="0.999143">
Table 1: Training set pair distribution within the -5/+5
</tableCaption>
<table confidence="0.9467755">
length diff range.
SP-EN IT-EN FR-EN DE-EN
backward 82 89 82 102
bidirectional 89 92 90 106
forward 69 78 76 98
no entailment 71 80 59 100
ALL 311 339 307 406
% (out of 500) 62.2 67.8 61.4 81.2
</table>
<tableCaption confidence="0.75222">
Table 2: Test set pair distribution within the -5/+5
length diff range.
</tableCaption>
<bodyText confidence="0.99198447368421">
in the [-5,+5] length diff range (68.1% on average
across the four language pairs).
However, comparing training and test set for
each language pair, it can be seen that while the
Spanish-English and Italian-English datasets are ho-
mogeneous with respect to the length diff feature,
the French-English and German-English datasets
present noticeable differences between training and
test set. These figures show that, despite the consid-
erable effort spent to produce comparable training
and test sets, the ideal objective of a full homogene-
ity between the datasets for these two languages was
difficult to reach.
Complete details about the distribution of the
pairs in terms of length diff for the four cross-
lingual corpora in the test set are provided in Figure
2. Vertical bars represent, for each length diff value,
the proportion of pairs belonging to the four entail-
ment classes.
</bodyText>
<page confidence="0.997105">
28
</page>
<sectionHeader confidence="0.986618" genericHeader="method">
4 Evaluation metrics and baselines
</sectionHeader>
<bodyText confidence="0.9999485">
Evaluation results have been automatically com-
puted by comparing the entailment judgements re-
turned by each system with those manually assigned
by human annotators in the gold standard. The met-
rics used for systems’ ranking is accuracy over the
whole test set, i.e. the number of correct judge-
ments out of the total number of judgements in the
test set. Additionally, we calculated precision, re-
call, and F1 measures for each of the four entail-
ment judgement categories taken separately. These
scores aim at giving participants the possibility to
gain clearer insights into their system’s behaviour on
the entailment phenomena relevant to the task.
To allow comparison with the CLTE-2012 re-
sults, the same three baselines were calculated on the
CLTE-2013 test set for each language combination.
The first one is the 0.25 accuracy score obtained by
assigning each test pair in the balanced dataset to
one of the four classes. The other two baselines con-
sider the length difference between T1 and T2:
</bodyText>
<listItem confidence="0.950101823529412">
• Composition of binary judgements (Bi-
nary). To calculate this baseline an SVM
classifier is trained to take binary en-
tailment decisions (“YES”, “NO”). The
classifier uses length(T1)/length(T2) and
length(T2)/length(T1) as features respectively
to check for entailment from T1 to T2 and vice-
versa. For each test pair, the unidirectional
judgements returned by the two classifiers are
composed into a single multi-directional judge-
ment (“YES-YES”=“bidirectional”, “YES-
NO”=“forward”, “NO-YES”=“backward”,
“NO-NO”=“no entailment”);
• Multi-class classification (Multi-class). A
single SVM classifier is trained with the same
features to directly assign to each pair one of
the four entailment judgements.
</listItem>
<bodyText confidence="0.9969238">
Both the baselines have been calculated with the
LIBSVM package (Chang and Lin, 2011), using de-
fault parameters. Baseline results are reported in Ta-
ble 3.
Although the four CLTE datasets are derived from
the same monolingual EN-EN corpus, baseline re-
sults present slight differences due to the effect of
translation into different languages. With respect to
last year’s evaluation, we can observe a slight drop
in the binary classification baseline results. This
might be due to the fact that the length distribution
of examples is slightly different this year. How-
ever, there are no significant differences between the
multi-class baseline results of this year in compar-
ison with the previous round results. This might
suggest that multi-class classification is a more ro-
bust approach for recognizing multi-directional en-
tailment relations. Moreover, both baselines failed
in capturing the “no-entailment” examples in all
datasets (F1no−entailment = 0).
</bodyText>
<table confidence="0.9947895">
SP-EN IT-EN FR-EN DE-EN
1-class 0.25 0.25 0.25 0.25
Binary 0.35 0.39 0.37 0.39
Multi-class 0.43 0.44 0.42 0.42
</table>
<tableCaption confidence="0.998038">
Table 3: Baseline accuracy results.
</tableCaption>
<sectionHeader confidence="0.865292" genericHeader="method">
5 Submitted runs and results
</sectionHeader>
<bodyText confidence="0.999868541666667">
Like in the 2012 round of the CLTE task, partici-
pants were allowed to submit up to five runs for each
language combination. A total of twelve teams reg-
istered for participation and downloaded the train-
ing set. Out of them, six4 submitted valid runs.
Five teams produced submissions for all the four
language combinations, while one team participated
only in the DE-EN task. In total, 61 runs have been
submitted and evaluated (16 for DE-EN, and 15 for
each of the other language pairs).
Accuracy results are reported in Table 4. As can
be seen from the table, the performance of the best
systems is quite similar across the four language
combinations, with the best submissions achieving
results in the 43.4-45.8% accuracy interval. Simi-
larly, also average and median results are close to
each other, with a small drop on DE-EN. This drop
might be explained by the difference between the
training and test set with respect to the length diff
feature. Moreover, the performance of DE-EN auto-
matic translation might affect approaches based on
“pivoting”, (i.e. addressing CLTE by automatically
translating T1 in the same language of T2, as de-
scribed in Section 6).
</bodyText>
<footnote confidence="0.971066">
4Including the task organizers.
</footnote>
<page confidence="0.994364">
29
</page>
<table confidence="0.999979809523809">
System name SP-EN IT-EN FR-EN DE-EN
altn run1* 0.428 0.432 0.420 0.388
BUAP run1 0.364 0.358 0.368 0.322
BUAP run2 0.374 0.358 0.364 0.318
BUAP run3 0.380 0.358 0.362 0.316
BUAP run4 0.364 0.388 0.392 0.350
BUAP run5 0.386 0.360 0.372 0.318
celi run1 0.340 0.324 0.334 0.342
celi run2 0.342 0.324 0.340 0.342
ECNUCS run1 0.428 0.426 0.438 0.422
ECNUCS run2 0.404 0.420 0.450 0.436
ECNUCS run3 0.408 0.426 0.458 0.432
ECNUCS run4 0.422 0.416 0.436 0.452
ECNUCS run5 0.392 0.402 0.442 0.426
SoftCard run1 0.434 0.454 0.416 0.414
SoftCard run2 0.432 0.448 0.426 0.402
umelb run1 − − − 0.324
Highest 0.434 0.454 0.458 0.452
Average 0.404 0.404 0.401 0.378
Median 0.428 0.426 0.420 0.369
Lowest 0.342 0.324 0.340 0.324
</table>
<tableCaption confidence="0.974983">
Table 4: CLTE-2013 accuracy results (61 runs) over the
4 language combinations. Highest, average, median and
lowest scores are calculated considering only the best run
for each team (*task organizers’ system).
</tableCaption>
<bodyText confidence="0.995033888888889">
Compared to the results achieved last year, shown
in Table 5, a sensible decrease in the highest scores
can be observed. While in CLTE-2012 the top sys-
tems achieved an accuracy well above 0.5 (with a
maximum of 0.632 in SP-EN), the results for this
year are far below such level (the peak is now at
45,8% for FR-EN). A slight decrease with respect
to 2012 can also be noted for average performances.
However, it’s worth remarking the general increase
of the lowest and median scores, which are less sen-
sitive to isolate outstanding results achieved by sin-
gle teams. This indicates that a progress in CLTE
research has been made building on the lessons
learned after the first round of the initiative.
To better understand the behaviour of each sys-
tem, Table 6 provides separate precision, recall, and
F1 scores for each entailment judgement, calculated
over the best runs of each participating team. In
contrast to CLTE-2012, where the “bidirectional”
and “no entailment” categories consistently proved
to be more problematic than “forward” and “back-
ward” judgements, this year’s results are more ho-
mogeneous across the different classes. Neverthe-
less, on average, the classification of “bidirectional”
pairs is still worse for three language pairs (SP-EN,
IT-EN and FR-EN), and results for “no entailment”
are lower for two of them (SP-EN and DE-EN).
</bodyText>
<table confidence="0.9980292">
SP-EN IT-EN FR-EN DE-EN
Highest 0.632 0.566 0.570 0.558
Average 0.440 0.411 0.408 0.408
Median 0.407 0.350 0.365 0.363
Lowest 0.274 0.326 0.296 0.296
</table>
<tableCaption confidence="0.933835">
Table 5: CLTE-2012 accuracy results. Highest, average,
median and lowest scores are calculated considering only
the best run for each team.
</tableCaption>
<bodyText confidence="0.9999954">
As regards the comparison with the baselines,
this year’s results confirm that the length diff-based
baselines are hard to beat. More specifically, most
of the systems are slightly above the binary classi-
fication baseline (with the exception of the DE-EN
dataset where only two systems out of six outper-
formed it), whereas for all the language combina-
tions the multi-class baseline was beaten only by the
best participating system.
This shows that, despite the effort in keeping the
distribution of the entailment classes uniform across
different length diff values, eliminating the correla-
tion between sentence length and correct entailment
decisions is difficult. As a consequence, although
disregarding semantic aspects of the problem, fea-
tures considering length information are quite ef-
fective in terms of overall accuracy. Such features,
however, perform rather poorly when dealing with
challenging cases (e.g. “no-entailment”), which are
better handled by participating systems.
</bodyText>
<sectionHeader confidence="0.998499" genericHeader="method">
6 Approaches
</sectionHeader>
<bodyText confidence="0.995793333333333">
A rough classification of the approaches adopted by
participants can be made along two orthogonal di-
mensions, namely:
</bodyText>
<listItem confidence="0.995997454545455">
• Pivoting vs. Cross-lingual. Pivoting meth-
ods rely on the automatic translation of one of
the two texts (either single words or the en-
tire sentence) into the language of the other
(typically English) in order perform monolin-
gual TE recognition. Cross-lingual methods
assign entailment judgements without prelim-
inary translation.
• Composition of binary judgements vs.
Multi-class classification. Compositional ap-
proaches map unidirectional (“YES”/“NO”)
</listItem>
<page confidence="0.985421">
30
</page>
<table confidence="0.999921621621622">
SP-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full spa-eng 0.509 0.464 0.485 0.440 0.264 0.330 0.464 0.416 0.439 0.357 0.568 0.438
BUAP spa-eng run5 0.446 0.360 0.398 0.521 0.296 0.378 0.385 0.456 0.418 0.300 0.432 0.354
celi spa-eng run2 0.396 0.352 0.373 0.431 0.400 0.415 0.325 0.328 0.327 0.245 0.288 0.265
ECNUCS spa-eng run1 0.458 0.432 0.444 0.533 0.320 0.400 0.406 0.416 0.411 0.380 0.544 0.447
SoftCard spa-eng run1 0.462 0.344 0.394 0.619 0.480 0.541 0.418 0.472 0.444 0.325 0.440 0.374
AVG. 0.454 0.390 0.419 0.509 0.352 0.413 0.400 0.418 0.408 0.321 0.454 0.376
IT-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full ita-eng 0.448 0.376 0.409 0.417 0.344 0.377 0.512 0.496 0.504 0.374 0.512 0.432
BUAP ita-eng run4 0.418 0.328 0.368 0.462 0.384 0.419 0.379 0.440 0.407 0.327 0.400 0.360
celi ita-eng run1 0.288 0.256 0.271 0.395 0.408 0.402 0.336 0.304 0.319 0.279 0.328 0.301
ECNUCS ita-eng run1 0.422 0.456 0.438 0.592 0.336 0.429 0.440 0.440 0.440 0.349 0.472 0.401
SoftCard ita-eng run1 0.514 0.456 0.483 0.612 0.480 0.538 0.392 0.464 0.425 0.364 0.416 0.388
AVG. 0.418 0.374 0.394 0.496 0.390 0.433 0.412 0.429 0.419 0.339 0.426 0.376
FR-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full fra-eng 0.405 0.392 0.398 0.420 0.296 0.347 0.500 0.440 0.468 0.381 0.552 0.451
BUAP fra-eng run4 0.407 0.472 0.437 0.431 0.376 0.402 0.379 0.376 0.378 0.352 0.344 0.348
celi fra-eng run2 0.394 0.344 0.368 0.364 0.376 0.370 0.352 0.352 0.352 0.263 0.288 0.275
ECNUCS fra-eng run3 0.422 0.432 0.427 0.667 0.352 0.461 0.514 0.432 0.470 0.383 0.616 0.472
SoftCard fra-eng run2 0.477 0.416 0.444 0.556 0.400 0.465 0.412 0.432 0.422 0.335 0.456 0.386
AVG. 0.421 0.411 0.415 0.488 0.360 0.409 0.431 0.406 0.418 0.343 0.451 0.386
DE-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full deu-eng 0.432 0.408 0.420 0.378 0.272 0.316 0.445 0.392 0.417 0.330 0.480 0.391
BUAP deu-eng run4 0.364 0.344 0.354 0.389 0.280 0.326 0.352 0.352 0.352 0.317 0.424 0.363
celi deu-eng run1 0.346 0.352 0.349 0.414 0.424 0.419 0.351 0.264 0.301 0.272 0.328 0.297
ECNUCS deu-eng run4 0.429 0.432 0.430 0.611 0.352 0.447 0.415 0.392 0.403 0.429 0.632 0.511
SoftCard deu-eng run1 0.511 0.368 0.428 0.527 0.384 0.444 0.417 0.400 0.408 0.317 0.504 0.389
umelb deu-eng run1 0.323 0.320 0.321 0.240 0.184 0.208 0.362 0.376 0.369 0.347 0.416 0.378
AVG. 0.401 0.371 0.384 0.426 0.316 0.360 0.390 0.363 0.375 0.335 0.464 0.389
</table>
<tableCaption confidence="0.998656">
Table 6: Precision, recall and F1 scores, calculated for each team’s best run for all the language combinations.
</tableCaption>
<bodyText confidence="0.961379147058823">
entailment decisions taken separately into sin-
gle judgements (similar to the Binary baseline
in Section 4). Methods based on multi-class
classification directly assign one of the four en-
tailment judgements to each test pair (similar to
our Multi-class baseline).
In contrast with CLTE-2012, where the combina-
tion of pivoting and compositional methods was the
option adopted by the majority of the approaches,
this year’s solutions do not show a clear trend. Con-
cerning the former dimension, participating systems
are equally distributed in cross-lingual and pivoting
methods relying on external automatic translation
tools. Regarding the latter dimension, in addition
to compositional and multi-class strategies, also al-
ternative solutions that leverage more sophisticated
meta-classification strategies have been proposed.
Besides the recourse to MT tools (e.g. Google
Translate), other tools and resources used by partic-
ipants include: WordNet, word alignment tools (e.g.
Giza++), part-of-speech taggers (e.g. Stanford POS
Tagger), stemmers (e.g. Snowball), machine learn-
ing libraries (e.g. Weka, SVMlight), parallel corpora
(e.g. Europarl), and stopword lists. More in detail:
ALTN [cross-lingual, compositional] (Turchi
and Negri, 2013) adopts a supervised learning
method based on features that consider word align-
ments between the two sentences obtained with
GIZA++ (Och et al., 2003). Binary entailment
judgements are taken separately, and combined into
final CLTE decisions.
BUAP [pivoting, multi-class and meta-
classifier] (Vilari˜no et al., 2013) adopts a pivoting
method based on translating T1 into the language of
</bodyText>
<page confidence="0.999783">
31
</page>
<bodyText confidence="0.999489">
T2 and vice versa (using Google Translate5). Sim-
ilarity measures (e.g. Jaccard index) and features
based on n-gram overlap, computed at the level of
words and part of speech categories, are used (either
alone or in combination) by different classification
strategies including: multi-class, a meta-classifier
(i.e. combining the output of 2/3/4-class classifiers),
and majority voting.
</bodyText>
<sectionHeader confidence="0.60877" genericHeader="method">
CELI [cross-lingual, meta-classifier]
</sectionHeader>
<bodyText confidence="0.98986696969697">
(Kouylekov, 2013) uses dictionaries for word
matching, and a multilingual corpus extracted
from Wikipedia for term weighting. A variety of
distance measures implemented in the RTE system
EDITS (Kouylekov and Negri, 2010; Negri et
al., 2009) are used to extract features to train a
meta-classifier. Such classifier combines binary
decisions (“YES”/“NO”) taken separately for each
of the four CLTE judgements.
ECNUCS [pivoting, multi-class] (Jiang and
Man, 2013) uses Google Translate to obtain the En-
glish translation of each T1. After a pre-processing
step aimed at maximizing the commonalities be-
tween the two sentences (e.g. abbreviation replace-
ment), a number of features is extracted to train
a multi-class SVM classifier. Such features con-
sider information about sentence length, text sim-
ilarity/difference measures, and syntactic informa-
tion.
SoftCard [pivoting, multi-class] (Jimenez et al.,
2013) after automatic translation with Google Trans-
late, uses SVMs to learn entailment decisions based
on information about the cardinality of: T1, T2, their
intersection and their union. Cardinalities are com-
puted in different ways, considering tokens in T1 and
T2, their IDF, and their similarity.
Umelb [cross-lingual, pivoting, compositional]
(Graham et al., 2013) adopts both pivoting and
cross-lingual approaches. For the latter, GIZA++
was used to compute word alignments between the
input sentences. Word alignment features are used
to train binary SVM classifiers whose decisions are
eventually composed into CLTE judgements.
</bodyText>
<sectionHeader confidence="0.998903" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.8542505">
Following the success of the first round of the Cross-
lingual Textual Entailment for Content Synchroniza-
</bodyText>
<footnote confidence="0.868861">
5http://translate.google.com/
</footnote>
<bodyText confidence="0.999934161290322">
tion task organized within SemEval-2012, a second
evaluation task has been organized within SemEval-
2013. Despite the decrease in the number of partic-
ipants (six teams - four less than in the first round
- submitted a total of 61 runs) the new experience
is still positive. In terms of data, a new test set
has been released, extending the old one with 500
new CLTE pairs. The resulting 1,500 cross-lingual
pairs, aligned over four language combinations (in
addition to the monolingual English version), and
annotated with multiple entailment relations, repre-
sent a significant contribution to the research com-
munity and a solid starting point for further develop-
ments.6 In terms of results, in spite of a significant
decrease of the top scores, the increase of both me-
dian and lower results demonstrates some encour-
aging progress in CLTE research. Such progress is
also demonstrated by the variety of the approaches
proposed. While in the first round most of the
teams adopted more intuitive and “simpler” solu-
tions based on pivoting (i.e. translation of T1 and
T2 in the same language) and compositional entail-
ment decision strategies, this year new ideas and
more complex solutions have emerged. Pivoting and
cross-lingual approaches are equally distributed, and
new classification methods have been proposed. Our
hope is that the large room for improvement, the in-
crease of available data, and the potential of CLTE
as a way to address complex NLP tasks and applica-
tions will motivate further research on the proposed
problem.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997590125">
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-248531). The
authors would also like to acknowledge Pamela
Forner and Giovanni Moretti from CELCT, and the
volunteer translators that contributed to the creation
of the dataset: Giusi Calo, Victoria Diaz, Bianca
Jeremias, Anne Kauffman, Laura L´opez Ortiz, Julie
Mailfait, Laura Mor´an Iglesias, Andreas Schwab.
</bodyText>
<footnote confidence="0.994783666666667">
6Together with the datasets derived from translation of the
RTE data (Negri and Mehdad, 2010), this is the only material
currently available to train and evaluate CLTE systems.
</footnote>
<page confidence="0.998111">
32
</page>
<sectionHeader confidence="0.989899" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999573961165049">
Amit Bronner, Matteo Negri, Yashar Mehdad, Angela
Fahrni, and Christof Monz. 2012. Cosyne: Synchro-
nizing multilingual wiki content. In Proceedings of
WikiSym 2012.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining.
Yvette Graham, Bahar Salehi, and Tim Baldwin. 2013.
Unimelb: Cross-lingual Textual Entailment with Word
Alignment and String Similarity Features. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013).
Zhao Jiang and Lan Man. 2013. ECNUCS: Recognizing
Cross-lingual Textual Entailment Using Multiple Fea-
ture Types.. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013. Soft Cardinality-CLTE: Learning to Iden-
tify Directional Cross-Lingual Entailmens from Car-
dinalities and SMT. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations.
Milen Kouylekov. 2013. Celi: EDITS and Generic Text
Pair Classification. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Michael Lesk. 1986. Automated Sense Disambiguation
Using Machine-readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. In Proceedings
of the 5th annual international conference on Systems
documentation (SIGDOC86).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Christof Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
Cosyne: a framework for multilingual content syn-
chronization of wikis. In Proceedings of WikiSym
2011.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazon´s Me-
chanical Turk.
Matteo Negri, Milen Kouylekov, Bernardo Magnini,
Yashar Mehdad, and Elena Cabrio. 2009. Towards ex-
tensible textual entailment engines: the edits package.
In AI* IA 2009: Emergent Perspectives in Artificial In-
telligence, pages 314–323. Springer.
Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012a.
Semeval-2012 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Matteo Negri, Yashar Mehdad, Alessandro Marchetti,
Danilo Giampiccolo, and Luisa Bentivogli. 2012b.
Chinese Whispers: Cooperative Paraphrase Acqui-
sition. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC12), volume 2, pages 2659–2665.
F. Och, H. Ney, F. Josef, and O. H. Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics.
Marco Turchi and Matteo Negri. 2013. ALTN: Word
Alignment Features for Cross-Lingual Textual Entail-
ment. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
Darnes Vilari˜no, David Pinto, Saul Le´on, Yuridiana
Alem´an, and Helena G´omez-Adorno. 2013. BUAP:
N-gram based Feature Evaluation for the Cross-
Lingual Textual Entailment Task. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
</reference>
<page confidence="0.999377">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.387890">
<title confidence="0.942138">Semeval-2013 Task 8: Cross-lingual Textual Entailment for Content Synchronization</title>
<author confidence="0.999571">Matteo Negri Alessandro Marchetti Yashar Mehdad</author>
<affiliation confidence="0.984554">FBK-irst CELCT UBC</affiliation>
<address confidence="0.937945">Trento, Italy Trento, Italy Vancouver, Canada</address>
<email confidence="0.893098">negri@fbk.euamarchetti@celct.itmehdad@cs.ubc.ca</email>
<affiliation confidence="0.726798">Luisa</affiliation>
<address confidence="0.664145">Trento,</address>
<email confidence="0.998785">bentivo@fbk.eu</email>
<abstract confidence="0.995775647058823">This paper presents the second round of the on Textual Entailment for organized within SemEval-2013. The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (six teams, 61 runs), the approaches adopted and the results achieved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bronner</author>
<author>Matteo Negri</author>
<author>Yashar Mehdad</author>
<author>Angela Fahrni</author>
<author>Christof Monz</author>
</authors>
<title>Cosyne: Synchronizing multilingual wiki content.</title>
<date>2012</date>
<booktitle>In Proceedings of WikiSym</booktitle>
<contexts>
<context position="2348" citStr="Bronner et al., 2012" startWordPosition="321" endWordPosition="324">and document summarization), multilingual content Danilo Giampiccolo CELCT Trento, Italy giampiccolo@celct.it synchronization represents a challenging application scenario to evaluate CLTE recognition components geared to the identification of sentence-level semantic relations. Given two documents about the same topic written in different languages (e.g. Wikipedia pages), the content synchronization task consists of automatically detecting and resolving differences in the information they provide, in order to produce aligned, mutually enriched versions of the two documents (Monz et al., 2011; Bronner et al., 2012). Towards this objective, a crucial requirement is to identify the information in one page that is either equivalent or novel (more informative) with respect to the content of the other. The task can be naturally cast as an entailment recognition problem, where bidirectional and unidirectional entailment judgements for two text fragments are respectively mapped into judgements about semantic equivalence and novelty. The task can also be seen as a machine translation evaluation problem, where judgements about semantic equivalence and novelty depend on the possibility to fully or partially trans</context>
</contexts>
<marker>Bronner, Negri, Mehdad, Fahrni, Monz, 2012</marker>
<rawString>Amit Bronner, Matteo Negri, Yashar Mehdad, Angela Fahrni, and Christof Monz. 2012. Cosyne: Synchronizing multilingual wiki content. In Proceedings of WikiSym 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="14618" citStr="Chang and Lin, 2011" startWordPosition="2268" endWordPosition="2271">uses length(T1)/length(T2) and length(T2)/length(T1) as features respectively to check for entailment from T1 to T2 and viceversa. For each test pair, the unidirectional judgements returned by the two classifiers are composed into a single multi-directional judgement (“YES-YES”=“bidirectional”, “YESNO”=“forward”, “NO-YES”=“backward”, “NO-NO”=“no entailment”); • Multi-class classification (Multi-class). A single SVM classifier is trained with the same features to directly assign to each pair one of the four entailment judgements. Both the baselines have been calculated with the LIBSVM package (Chang and Lin, 2011), using default parameters. Baseline results are reported in Table 3. Although the four CLTE datasets are derived from the same monolingual EN-EN corpus, baseline results present slight differences due to the effect of translation into different languages. With respect to last year’s evaluation, we can observe a slight drop in the binary classification baseline results. This might be due to the fact that the length distribution of examples is slightly different this year. However, there are no significant differences between the multi-class baseline results of this year in comparison with the </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
</authors>
<title>Probabilistic Textual Entailment: Generic Applied Modeling of Language Variability.</title>
<date>2004</date>
<booktitle>In Proceedings of the PASCAL Workshop of Learning Methods for Text Understanding and Mining.</booktitle>
<contexts>
<context position="1154" citStr="Dagan and Glickman, 2004" startWordPosition="151" endWordPosition="154">written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (six teams, 61 runs), the approaches adopted and the results achieved. 1 Introduction The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization. Given two texts in different languages, the cross-lingual textual entailment (CLTE) task consists of deciding if the meaning of one text can be inferred from the meaning of the other text. Crosslinguality represents an interesting direction for research on recognizing textual entailment (RTE), especially due to its possible application in a variety of tasks. Among others (e.g. question answering, information retrieval, information extraction, and document summarization)</context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>Ido Dagan and Oren Glickman. 2004. Probabilistic Textual Entailment: Generic Applied Modeling of Language Variability. In Proceedings of the PASCAL Workshop of Learning Methods for Text Understanding and Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Bahar Salehi</author>
<author>Tim Baldwin</author>
</authors>
<title>Unimelb: Cross-lingual Textual Entailment with Word Alignment and String Similarity Features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="27022" citStr="Graham et al., 2013" startWordPosition="4221" endWordPosition="4224">ment), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measures, and syntactic information. SoftCard [pivoting, multi-class] (Jimenez et al., 2013) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union. Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity. Umelb [cross-lingual, pivoting, compositional] (Graham et al., 2013) adopts both pivoting and cross-lingual approaches. For the latter, GIZA++ was used to compute word alignments between the input sentences. Word alignment features are used to train binary SVM classifiers whose decisions are eventually composed into CLTE judgements. 7 Conclusion Following the success of the first round of the Crosslingual Textual Entailment for Content Synchroniza5http://translate.google.com/ tion task organized within SemEval-2012, a second evaluation task has been organized within SemEval2013. Despite the decrease in the number of participants (six teams - four less than in </context>
</contexts>
<marker>Graham, Salehi, Baldwin, 2013</marker>
<rawString>Yvette Graham, Bahar Salehi, and Tim Baldwin. 2013. Unimelb: Cross-lingual Textual Entailment with Word Alignment and String Similarity Features. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhao Jiang</author>
<author>Lan Man</author>
</authors>
<title>ECNUCS: Recognizing Cross-lingual Textual Entailment Using Multiple Feature Types..</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="26215" citStr="Jiang and Man, 2013" startWordPosition="4103" endWordPosition="4106">es including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers), and majority voting. CELI [cross-lingual, meta-classifier] (Kouylekov, 2013) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. A variety of distance measures implemented in the RTE system EDITS (Kouylekov and Negri, 2010; Negri et al., 2009) are used to extract features to train a meta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two sentences (e.g. abbreviation replacement), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measures, and syntactic information. SoftCard [pivoting, multi-class] (Jimenez et al., 2013) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their int</context>
</contexts>
<marker>Jiang, Man, 2013</marker>
<rawString>Zhao Jiang and Lan Man. 2013. ECNUCS: Recognizing Cross-lingual Textual Entailment Using Multiple Feature Types.. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft Cardinality-CLTE: Learning to Identify Directional Cross-Lingual Entailmens from Cardinalities and SMT.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="26659" citStr="Jimenez et al., 2013" startWordPosition="4168" endWordPosition="4171">ta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two sentences (e.g. abbreviation replacement), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measures, and syntactic information. SoftCard [pivoting, multi-class] (Jimenez et al., 2013) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union. Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity. Umelb [cross-lingual, pivoting, compositional] (Graham et al., 2013) adopts both pivoting and cross-lingual approaches. For the latter, GIZA++ was used to compute word alignments between the input sentences. Word alignment features are used to train binary SVM classifiers whose decisions are eventually c</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2013</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2013. Soft Cardinality-CLTE: Learning to Identify Directional Cross-Lingual Entailmens from Cardinalities and SMT. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
<author>Matteo Negri</author>
</authors>
<title>An opensource package for recognizing textual entailment.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations.</booktitle>
<contexts>
<context position="25974" citStr="Kouylekov and Negri, 2010" startWordPosition="4068" endWordPosition="4071">(using Google Translate5). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers), and majority voting. CELI [cross-lingual, meta-classifier] (Kouylekov, 2013) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. A variety of distance measures implemented in the RTE system EDITS (Kouylekov and Negri, 2010; Negri et al., 2009) are used to extract features to train a meta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two sentences (e.g. abbreviation replacement), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measure</context>
</contexts>
<marker>Kouylekov, Negri, 2010</marker>
<rawString>Milen Kouylekov and Matteo Negri. 2010. An opensource package for recognizing textual entailment. In Proceedings of the ACL 2010 System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
</authors>
<title>Celi: EDITS and Generic Text Pair Classification.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="25772" citStr="Kouylekov, 2013" startWordPosition="4040" endWordPosition="4041">d into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of 31 T2 and vice versa (using Google Translate5). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers), and majority voting. CELI [cross-lingual, meta-classifier] (Kouylekov, 2013) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. A variety of distance measures implemented in the RTE system EDITS (Kouylekov and Negri, 2010; Negri et al., 2009) are used to extract features to train a meta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two senten</context>
</contexts>
<marker>Kouylekov, 2013</marker>
<rawString>Milen Kouylekov. 2013. Celi: EDITS and Generic Text Pair Classification. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automated Sense Disambiguation Using Machine-readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems documentation (SIGDOC86).</booktitle>
<contexts>
<context position="7951" citStr="Lesk, 1986" startWordPosition="1185" endWordPosition="1186">ely the creation of paraphrases. Focusing on the creation of high quality paraphrases, we followed the crowdsourcing methodology experimented in Negri et al. (2012b), in which a paraphrase is obtained through an iterative modification process of an original sentence, by asking workers to introduce meaningpreserving lexical and syntactic changes. At each round of the iteration, new workers are presented with the output of the previous iteration in order to increase divergence from the original sentence. At the end of the process, only the more divergent paraphrases according to the Lesk score (Lesk, 1986) are selected. As for the second phase of T2 creation process, this year it was carried out by expert annotators, who followed the same criteria used last year for the crowdsourced tasks, i.e. i) remove information from the input (paraphrased) sentence and ii) add information from sentences surrounding T1 in the source article; 3. Each T2 was then paired to the original T1, and the resulting pairs were annotated with one of the four entailment judgements. In order to reduce the correlation between the difference in sentences’ length and entailment judgements, only the pairs where the differenc</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automated Sense Disambiguation Using Machine-readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. In Proceedings of the 5th annual international conference on Systems documentation (SIGDOC86).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Towards Cross-Lingual Textual Entailment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT</booktitle>
<contexts>
<context position="1081" citStr="Mehdad et al., 2010" startWordPosition="141" endWordPosition="144">k was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (six teams, 61 runs), the approaches adopted and the results achieved. 1 Introduction The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization. Given two texts in different languages, the cross-lingual textual entailment (CLTE) task consists of deciding if the meaning of one text can be inferred from the meaning of the other text. Crosslinguality represents an interesting direction for research on recognizing textual entailment (RTE), especially due to its possible application in a variety of tasks. Among others (e.g. question answering, i</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2010</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2010. Towards Cross-Lingual Textual Entailment. In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Using Bilingual Parallel Corpora for CrossLingual Textual Entailment.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT</booktitle>
<contexts>
<context position="3474" citStr="Mehdad et al., 2011" startWordPosition="505" endWordPosition="508"> about semantic equivalence and novelty depend on the possibility to fully or partially translate a text fragment into the other. The recent advances on monolingual TE on the one hand, and the methodologies used in Statistical Machine Translation (SMT) on the other, offer promising solutions to approach the CLTE task. In line with a number of systems that model the RTE task as a similarity problem (i.e. handling similarity scores between T and H as features contributing to the entailment decision), the standard sentence and word alignment programs used in SMT offer a strong baseline for CLTE (Mehdad et al., 2011; 25 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 25–33, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Figure 1: Example of SP-EN CLTE pairs. Mehdad et al., 2012). However, although representing a solid starting point to approach the problem, similarity-based techniques are just approximations, open to significant improvements coming from semantic inference at the multilingual level (e.g. cross-lingual entailment rules such as “perro”→“animal”).</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2011</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2011. Using Bilingual Parallel Corpora for CrossLingual Textual Entailment. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="3787" citStr="Mehdad et al., 2012" startWordPosition="548" endWordPosition="551">E task. In line with a number of systems that model the RTE task as a similarity problem (i.e. handling similarity scores between T and H as features contributing to the entailment decision), the standard sentence and word alignment programs used in SMT offer a strong baseline for CLTE (Mehdad et al., 2011; 25 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 25–33, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Figure 1: Example of SP-EN CLTE pairs. Mehdad et al., 2012). However, although representing a solid starting point to approach the problem, similarity-based techniques are just approximations, open to significant improvements coming from semantic inference at the multilingual level (e.g. cross-lingual entailment rules such as “perro”→“animal”). Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional). Thanks to the contiguity between CLTE, TE and SMT, </context>
</contexts>
<marker>Mehdad, Negri, Federico, 2012</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof Monz</author>
<author>Vivi Nastase</author>
<author>Matteo Negri</author>
<author>Angela Fahrni</author>
<author>Yashar Mehdad</author>
<author>Michael Strube</author>
</authors>
<title>Cosyne: a framework for multilingual content synchronization of wikis.</title>
<date>2011</date>
<booktitle>In Proceedings of WikiSym</booktitle>
<contexts>
<context position="2325" citStr="Monz et al., 2011" startWordPosition="317" endWordPosition="320">mation extraction, and document summarization), multilingual content Danilo Giampiccolo CELCT Trento, Italy giampiccolo@celct.it synchronization represents a challenging application scenario to evaluate CLTE recognition components geared to the identification of sentence-level semantic relations. Given two documents about the same topic written in different languages (e.g. Wikipedia pages), the content synchronization task consists of automatically detecting and resolving differences in the information they provide, in order to produce aligned, mutually enriched versions of the two documents (Monz et al., 2011; Bronner et al., 2012). Towards this objective, a crucial requirement is to identify the information in one page that is either equivalent or novel (more informative) with respect to the content of the other. The task can be naturally cast as an entailment recognition problem, where bidirectional and unidirectional entailment judgements for two text fragments are respectively mapped into judgements about semantic equivalence and novelty. The task can also be seen as a machine translation evaluation problem, where judgements about semantic equivalence and novelty depend on the possibility to f</context>
</contexts>
<marker>Monz, Nastase, Negri, Fahrni, Mehdad, Strube, 2011</marker>
<rawString>Christof Monz, Vivi Nastase, Matteo Negri, Angela Fahrni, Yashar Mehdad, and Michael Strube. 2011. Cosyne: a framework for multilingual content synchronization of wikis. In Proceedings of WikiSym 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Yashar Mehdad</author>
</authors>
<title>Creating a Bilingual Entailment Corpus through Translations with Mechanical Turk: $100 for a 10-day Rush.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon´s Mechanical Turk.</booktitle>
<marker>Negri, Mehdad, 2010</marker>
<rawString>Matteo Negri and Yashar Mehdad. 2010. Creating a Bilingual Entailment Corpus through Translations with Mechanical Turk: $100 for a 10-day Rush. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon´s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Milen Kouylekov</author>
<author>Bernardo Magnini</author>
<author>Yashar Mehdad</author>
<author>Elena Cabrio</author>
</authors>
<title>Towards extensible textual entailment engines: the edits package.</title>
<date>2009</date>
<booktitle>In AI* IA 2009: Emergent Perspectives in Artificial Intelligence,</booktitle>
<pages>314--323</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="25995" citStr="Negri et al., 2009" startWordPosition="4072" endWordPosition="4075">Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers), and majority voting. CELI [cross-lingual, meta-classifier] (Kouylekov, 2013) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. A variety of distance measures implemented in the RTE system EDITS (Kouylekov and Negri, 2010; Negri et al., 2009) are used to extract features to train a meta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two sentences (e.g. abbreviation replacement), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measures, and syntactic info</context>
</contexts>
<marker>Negri, Kouylekov, Magnini, Mehdad, Cabrio, 2009</marker>
<rawString>Matteo Negri, Milen Kouylekov, Bernardo Magnini, Yashar Mehdad, and Elena Cabrio. 2009. Towards extensible textual entailment engines: the edits package. In AI* IA 2009: Emergent Perspectives in Artificial Intelligence, pages 314–323. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matto Negri</author>
<author>Luisa Bentivogli</author>
<author>Yashar Mehdad</author>
<author>Danilo Giampiccolo</author>
<author>Alessandro Marchetti</author>
</authors>
<title>Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora.</title>
<date>2011</date>
<booktitle>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="6554" citStr="Negri et al., 2011" startWordPosition="965" endWordPosition="968">consists of 1,500 sentence pairs (1,000 for training and 500 for test), balanced across the four entailment judgements. In this year’s evaluation, as training set we used the CLTE-2012 corpus1 that was created for the SemEval-2012 evaluation exercise2 (including both training and test sets). The CLTE-2013 test set was created from scratch, following the methodology described in the next section. 3.1 Data collection and annotation To collect the entailment pairs for the 2013 test set we adopted a slightly modified version of the crowdsourcing methodology followed to create the CLTE2012 corpus (Negri et al., 2011). The main difference with last year’s procedure is that we did not take advantage of crowdsourcing for the whole data collection process, but only for part of it. As for CLTE-2012, the collection and annotation process consists of the following steps: 1. First, English sentences were manually extracted from Wikipedia and Wikinews. The selected sentences represent one of the elements (T1) of each entailment pair; 1http://www.celct.it/resources.php?id page=CLTE 2http://www.cs.york.ac.uk/semeval-2012/task8/ 26 2. Next, each T1 was modified in various ways in order to obtain a corresponding T2. W</context>
</contexts>
<marker>Negri, Bentivogli, Mehdad, Giampiccolo, Marchetti, 2011</marker>
<rawString>Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Alessandro Marchetti</author>
<author>Yashar Mehdad</author>
<author>Luisa Bentivogli</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="4653" citStr="Negri et al., 2012" startWordPosition="675" endWordPosition="678">ilment rules such as “perro”→“animal”). Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional). Thanks to the contiguity between CLTE, TE and SMT, the proposed task provides an interesting scenario to approach the issues outlined above from different perspectives, and offers large room for mutual improvement. Building on the success of the first CLTE evaluation organized within SemEval-2012 (Negri et al., 2012a), the remainder of this paper describes the second evaluation round organized within SemEval2013. The following sections provide an overview of the datasets used, the participating systems, the approaches adopted, the achieved results, and the lessons learned. 2 The task Given a pair of topically related text fragments (T1 and T2) in different languages, the CLTE task consists of automatically annotating it with one of the following entailment judgements (see Figure 1 for Spanish/English examples of each judgement): • bidirectional (T1→T2 &amp; T1←T2): the two fragments entail each other (semant</context>
<context position="7503" citStr="Negri et al. (2012" startWordPosition="1110" endWordPosition="1113">s. The selected sentences represent one of the elements (T1) of each entailment pair; 1http://www.celct.it/resources.php?id page=CLTE 2http://www.cs.york.ac.uk/semeval-2012/task8/ 26 2. Next, each T1 was modified in various ways in order to obtain a corresponding T2. While in the CLTE-2012 dataset the whole T2 creation process was carried out through crowdsourcing, for the CLTE-2013 test set we crowdsourced only the first phase of T1 modification, namely the creation of paraphrases. Focusing on the creation of high quality paraphrases, we followed the crowdsourcing methodology experimented in Negri et al. (2012b), in which a paraphrase is obtained through an iterative modification process of an original sentence, by asking workers to introduce meaningpreserving lexical and syntactic changes. At each round of the iteration, new workers are presented with the output of the previous iteration in order to increase divergence from the original sentence. At the end of the process, only the more divergent paraphrases according to the Lesk score (Lesk, 1986) are selected. As for the second phase of T2 creation process, this year it was carried out by expert annotators, who followed the same criteria used la</context>
</contexts>
<marker>Negri, Marchetti, Mehdad, Bentivogli, Giampiccolo, 2012</marker>
<rawString>Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa Bentivogli, and Danilo Giampiccolo. 2012a. Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Yashar Mehdad</author>
<author>Alessandro Marchetti</author>
<author>Danilo Giampiccolo</author>
<author>Luisa Bentivogli</author>
</authors>
<title>Chinese Whispers: Cooperative Paraphrase Acquisition.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC12),</booktitle>
<volume>2</volume>
<pages>2659--2665</pages>
<contexts>
<context position="4653" citStr="Negri et al., 2012" startWordPosition="675" endWordPosition="678">ilment rules such as “perro”→“animal”). Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional). Thanks to the contiguity between CLTE, TE and SMT, the proposed task provides an interesting scenario to approach the issues outlined above from different perspectives, and offers large room for mutual improvement. Building on the success of the first CLTE evaluation organized within SemEval-2012 (Negri et al., 2012a), the remainder of this paper describes the second evaluation round organized within SemEval2013. The following sections provide an overview of the datasets used, the participating systems, the approaches adopted, the achieved results, and the lessons learned. 2 The task Given a pair of topically related text fragments (T1 and T2) in different languages, the CLTE task consists of automatically annotating it with one of the following entailment judgements (see Figure 1 for Spanish/English examples of each judgement): • bidirectional (T1→T2 &amp; T1←T2): the two fragments entail each other (semant</context>
<context position="7503" citStr="Negri et al. (2012" startWordPosition="1110" endWordPosition="1113">s. The selected sentences represent one of the elements (T1) of each entailment pair; 1http://www.celct.it/resources.php?id page=CLTE 2http://www.cs.york.ac.uk/semeval-2012/task8/ 26 2. Next, each T1 was modified in various ways in order to obtain a corresponding T2. While in the CLTE-2012 dataset the whole T2 creation process was carried out through crowdsourcing, for the CLTE-2013 test set we crowdsourced only the first phase of T1 modification, namely the creation of paraphrases. Focusing on the creation of high quality paraphrases, we followed the crowdsourcing methodology experimented in Negri et al. (2012b), in which a paraphrase is obtained through an iterative modification process of an original sentence, by asking workers to introduce meaningpreserving lexical and syntactic changes. At each round of the iteration, new workers are presented with the output of the previous iteration in order to increase divergence from the original sentence. At the end of the process, only the more divergent paraphrases according to the Lesk score (Lesk, 1986) are selected. As for the second phase of T2 creation process, this year it was carried out by expert annotators, who followed the same criteria used la</context>
</contexts>
<marker>Negri, Mehdad, Marchetti, Giampiccolo, Bentivogli, 2012</marker>
<rawString>Matteo Negri, Yashar Mehdad, Alessandro Marchetti, Danilo Giampiccolo, and Luisa Bentivogli. 2012b. Chinese Whispers: Cooperative Paraphrase Acquisition. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC12), volume 2, pages 2659–2665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
<author>F Josef</author>
<author>O H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models. Computational Linguistics.</title>
<date>2003</date>
<contexts>
<context position="25092" citStr="Och et al., 2003" startWordPosition="3943" endWordPosition="3946">meta-classification strategies have been proposed. Besides the recourse to MT tools (e.g. Google Translate), other tools and resources used by participants include: WordNet, word alignment tools (e.g. Giza++), part-of-speech taggers (e.g. Stanford POS Tagger), stemmers (e.g. Snowball), machine learning libraries (e.g. Weka, SVMlight), parallel corpora (e.g. Europarl), and stopword lists. More in detail: ALTN [cross-lingual, compositional] (Turchi and Negri, 2013) adopts a supervised learning method based on features that consider word alignments between the two sentences obtained with GIZA++ (Och et al., 2003). Binary entailment judgements are taken separately, and combined into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of 31 T2 and vice versa (using Google Translate5). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifier</context>
</contexts>
<marker>Och, Ney, Josef, Ney, 2003</marker>
<rawString>F. Och, H. Ney, F. Josef, and O. H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>ALTN: Word Alignment Features for Cross-Lingual Textual Entailment.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="24942" citStr="Turchi and Negri, 2013" startWordPosition="3919" endWordPosition="3922">tools. Regarding the latter dimension, in addition to compositional and multi-class strategies, also alternative solutions that leverage more sophisticated meta-classification strategies have been proposed. Besides the recourse to MT tools (e.g. Google Translate), other tools and resources used by participants include: WordNet, word alignment tools (e.g. Giza++), part-of-speech taggers (e.g. Stanford POS Tagger), stemmers (e.g. Snowball), machine learning libraries (e.g. Weka, SVMlight), parallel corpora (e.g. Europarl), and stopword lists. More in detail: ALTN [cross-lingual, compositional] (Turchi and Negri, 2013) adopts a supervised learning method based on features that consider word alignments between the two sentences obtained with GIZA++ (Och et al., 2003). Binary entailment judgements are taken separately, and combined into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of 31 T2 and vice versa (using Google Translate5). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or</context>
</contexts>
<marker>Turchi, Negri, 2013</marker>
<rawString>Marco Turchi and Matteo Negri. 2013. ALTN: Word Alignment Features for Cross-Lingual Textual Entailment. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darnes Vilari˜no</author>
<author>David Pinto</author>
<author>Saul Le´on</author>
<author>Yuridiana Alem´an</author>
<author>Helena G´omez-Adorno</author>
</authors>
<title>BUAP: N-gram based Feature Evaluation for the CrossLingual Textual Entailment Task.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Vilari˜no, Pinto, Le´on, Alem´an, G´omez-Adorno, 2013</marker>
<rawString>Darnes Vilari˜no, David Pinto, Saul Le´on, Yuridiana Alem´an, and Helena G´omez-Adorno. 2013. BUAP: N-gram based Feature Evaluation for the CrossLingual Textual Entailment Task. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>