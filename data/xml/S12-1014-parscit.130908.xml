<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003080">
<title confidence="0.861805">
Unsupervised Disambiguation of Image Captions
</title>
<author confidence="0.993483">
Wesley May, Sanja Fidler, Afsaneh Fazly, Sven Dickinson, and Suzanne Stevenson
</author>
<affiliation confidence="0.9991235">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.569492">
Toronto, Ontario, Canada, M5S 3G4
</address>
<email confidence="0.997836">
twesley,fidler,afsaneh,sven,suzannel@cs.toronto.edu
</email>
<sectionHeader confidence="0.996646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997086">
Given a set of images with related captions,
our goal is to show how visual features can
improve the accuracy of unsupervised word
sense disambiguation when the textual con-
text is very small, as this sort of data is com-
mon in news and social media. We extend
previous work in unsupervised text-only dis-
ambiguation with methods that integrate text
and images. We construct a corpus by using
Amazon Mechanical Turk to caption sense-
tagged images gathered from ImageNet. Us-
ing a Yarowsky-inspired algorithm, we show
that gains can be made over text-only disam-
biguation, as well as multimodal approaches
such as Latent Dirichlet Allocation.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.958965764705883">
We examine the problem of performing unsuper-
vised word sense disambiguation (WSD) in situa-
tions with little text, but where additional informa-
tion is available in the form of an image. Such situ-
ations include captioned newswire photos, and pic-
tures in social media where the textual context is of-
ten no larger than a tweet.
Unsupervised WSD has been shown to work very
well when the target word is embedded in a large
We thank NSERC and U. Toronto for financial support. Fi-
dler and Dickinson were sponsored by the Army Research Lab-
oratory and this research was accomplished in part under Co-
operative Agreement Number W911NF-10-2-0060. The views
and conclusions contained in this document are those of the au-
thors and should not be interpreted as representing the official
policies, either express or implied, of the Army Research Lab-
oratory or the U.S. Government.
</bodyText>
<figureCaption confidence="0.991415">
Figure 1: “The crane was so massive it blocked the sun.”
Which sense of crane? With images the answer is clear.
</figureCaption>
<bodyText confidence="0.999190192307692">
quantity of text (Yarowsky, 1995). However, if the
only available text is “The crane was so massive it
blocked the sun” (see Fig. 1), then text-only dis-
ambiguation becomes much more difficult; a human
could do little more than guess. But if an image is
available, the intended sense is much clearer. We
develop an unsupervised WSD algorithm based on
Yarowsky’s that uses words in a short caption along
with “visual words” from the captioned image to
choose the best of two possible senses of an ambigu-
ous keyword describing the content of the image.
Language-vision integration is a quickly develop-
ing field, and a number of researchers have explored
the possibility of combining text and visual features
in various multimodal tasks. Leong and Mihal-
cea (2011) explored semantic relatedness between
words and images to better exploit multimodal con-
tent. Jamieson et al. (2009) and Feng and Lap-
ata (2010) combined text and vision to perform ef-
fective image annotation. Barnard and colleagues
(2003; 2005) showed that supervised WSD by could
be improved with visual features. Here we show that
unsupervised WSD can similarly be improved. Lo-
eff, Alm and Forsyth (2006) and Saenko and Darrell
(2008) combined visual and textual information to
solve a related task, image sense disambiguation, in
</bodyText>
<page confidence="0.997083">
85
</page>
<note confidence="0.5278815">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 85–89,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999828833333333">
an unsupervised fashion. In Loeff et al.’s work, little
gain was realized when visual features were added
to a great deal of text. We show that these features
have more utility with small textual contexts, and
that, when little text is available, our method is more
suitable than Saenko and Darrell’s.
</bodyText>
<sectionHeader confidence="0.970879" genericHeader="method">
2 Our Algorithm
</sectionHeader>
<bodyText confidence="0.977666">
We model our algorithm after Yarowsky’s (1995) al-
gorithm for unsupervised WSD: Given a set of doc-
uments that contain a certain ambiguous word, the
goal is to label each instance of that word as some
particular sense. A seed set of collocations that
strongly indicate one of the senses is initially used to
label a subset of the data. Yarowsky then finds new
collocations in the labelled data that are strongly as-
sociated with one of the current labels and applies
these to unlabelled data. This process repeats iter-
atively, building a decision list of collocations that
indicate a particular sense with a certain confidence.
In our algorithm (Algorithm 1), we have a docu-
ment collection D of images relevant to an ambigu-
ous keyword k with senses s1 and s2 (though the al-
gorithm is extensible to more than two senses). Such
a collection might result from an internet image
search using an ambiguous word such as “mouse”.
Each Di is an image–caption pair repsented as a
bag-of-words that includes both lexical words from
the caption, and “visual words” from the image. A
visual word is simply an abstract representation that
describes a small portion of an image, such that sim-
ilar portions in other images are represented by the
same visual word (see Section 3.2 for details). Our
seed sets consist of the words in the definitions of s1
and s2 from WordNet (Fellbaum, 1998). Any docu-
ment whose caption contains more words from one
sense definition than the other is initially labelled
with that sense. We then iterate between two steps
that (i) find additional words associated with s1 or
s2 in currently labelled data, and (ii) relabel all data
using the word sense associations discovered so far.
We let V be the entire vocabulary of words across
all documents. We run experiements both with and
without visual words, but when we use visual words,
they are included in V . In the first step, we com-
pute a confidence Ci for each word Vi. This con-
fidence is a log-ratio of the probability of seeing
Vi in documents labelled as s1 as opposed to doc-
uments labelled as s2. That is, a positive Ci indi-
cates greater association with s1, and vice versa. In
the second step we find, for each document Dj, the
word Vi E Dj with the highest magnitude of Ci. If
the magnitude of Ci is above a labelling threshold
τc, then we label this document as s1 or s2 depend-
ing on the sign of Ci. Note that all old labels are dis-
carded before this step, so labelled documents may
become unlabelled, or even differently labelled, as
the algorithm progresses.
Algorithm 1 Proposed Algorithm
D: set of documents D1 ... Dd
V : set of lexical and visual words V1 ... Vv in D
Ci: log-confidence Vi is sense 1 vs. sense 2
S1 and S2: bag of dictionary words for each sense
L1 and L2: documents labelled as sense 1 or 2
for all Di do . Initial labelling using seed set
if jDi n S1j &gt; jDi n S2j then
</bodyText>
<equation confidence="0.85602952631579">
L1 +– L1 U {Di}
else if jDi n S1j &lt; jDi n S2j then
L2 +– L2 U {Di}
end if
end for
repeat
for all i E L.v do . Update word conf.
(P (Vi�L�) )
Ci � log P (Vi�L2)
end for
L1 — 0, L2 — 0 . Update document conf.
for all Di do
. Find word with highest confidence
m +– arg max jCjj
jE1..v,VjEDi
if Cm &gt; τc then
L1 , L1 U {Di}
else if Cm &lt; —τc then
L2 , L2 U {Di}
</equation>
<subsectionHeader confidence="0.574439">
end if
end for
</subsectionHeader>
<bodyText confidence="0.849229">
until no change to L1 or L2
</bodyText>
<sectionHeader confidence="0.876581" genericHeader="method">
3 Creation of the Dataset
</sectionHeader>
<bodyText confidence="0.99944075">
We require a collection of images with associated
captions. We also require sense annotations for
the keyword for each image to use for evalua-
tion. Barnard and Johnson (2005) developed the
</bodyText>
<page confidence="0.984484">
86
</page>
<bodyText confidence="0.995548285714286">
“Music is an important “Keeping your office sup-
means of expression for plies organized is easy, with
many teens.” the right tools.”
“The internet has opened up “When there is no cheese I
the world to people of all will take over the world.”
nationalities.”
blue fish”, as such captions are very unnatural. True
captions generally offer orthogonal information that
is not readily apparent from the image. The key-
word for each image (as specified by ImageNet) was
not presented to the Turkers, so the captions do not
necessarily contain it. Knowledge of the keyword is
presumed to be available to the algorithm in the form
of an image tag, or filename, or the like. We found
that forcing a certain word to be included in the cap-
tion also led to sentences that described the picture
very directly. Sentences were required to be a least
ten words long, and have acceptable grammar and
spelling. We remove stop words from the captions
and lemmatize the remaining words. See Figure 2
for some examples.
</bodyText>
<figureCaption confidence="0.98722">
Figure 2: Example image-caption pairs from our dataset,
for “band” (top) and “mouse” (bottom).
</figureCaption>
<bodyText confidence="0.999984571428571">
ImCor dataset by associating images from the Corel
database with text from the SemCor corpus (Miller
et al., 1993). Loeff et al. (2006) and Saenko and
Darrell (2008) used Yahoo!’s image search to gather
images with their associated web pages. While these
datasets contain images paired with text, the textual
contexts are much larger than typical captions.
</bodyText>
<subsectionHeader confidence="0.998042">
3.1 Captioning Images
</subsectionHeader>
<bodyText confidence="0.999991842105263">
To develop a large set of sense-annotated image–
caption pairs with a focus on caption-sized text, we
turned to ImageNet (Deng et al., 2009). ImageNet is
a database of images that are each associated with
a synset from WordNet. Hundreds of images are
available for each of a number of senses of a wide
variety of common nouns. To gather captions, we
used Amazon Mechanical Turk to collect five sen-
tences for each image. We chose two word senses
for each of 20 polysemous nouns and for each sense
we collected captions for 50 representative images.
For each image we gathered five captions, for a to-
tal of 10,000 captions. As we have five captions for
each image, we split our data into five sets. Each set
has the same images, but each image is paired with
a different caption in each set.
We specified to the Turkers that the sentences
should be relevant to, but should not talk directly
about, the image, as in “In this picture there is a
</bodyText>
<subsectionHeader confidence="0.999423">
3.2 Computing the Visual Words
</subsectionHeader>
<bodyText confidence="0.999986625">
We compute visual words for each image with Ima-
geNet’s feature extractor. This extractor lays down
a grid of overlapping squares onto the image and
computes a SIFT descriptor (Lowe, 2004) for each
square. Each descriptor is a vector that encodes the
edge orientation information in a given square. The
descriptors are computed at three scales: 1x, 0.5x
and 0.25x the original side lengths. These vectors
are clustered with k-means into 1000 clusters, and
the labels of these clusters (arbitrary integers from 1
to 1000) serve as our visual words.
It is common for each image to have a “vocab-
ulary” of over 300 distinct visual words, many of
which only occur once. To denoise the visual data,
we use only those visual words which account for at
least 1% of the total visual words for that image.
</bodyText>
<sectionHeader confidence="0.997469" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99971675">
To show that the addition of visual features improves
the accuracy of sense disambiguation for image–
caption pairs, we run our algorithm both with and
without the visual features. We also compare our re-
sults to three different baseline methods: K-means
(K-M), Latent Dirichlet Allocation (LDA) (Blei et
al., 2003), and an unsupervised WSD algorithm
(PBP) explained below. We use accuracy to measure
performance as it is commonly used by the WSD
community (See Table 1).
For K-means, we set k = 2 as we have two senses,
and represent each document with a V-dimensional
</bodyText>
<page confidence="0.999735">
87
</page>
<tableCaption confidence="0.9409385">
Table 1: Results (Average accuracy across all five sets of
data). Bold indicates best performance for that word.
</tableCaption>
<table confidence="0.999216826086957">
Ours Ours K-M K-M LDA LDA PBP
text w/vis text w/vis text w/vis text
band .80 .82 .66 .65 .64 .56 .73
bank .77 .78 .71 .59 .52 .67 .62
bass .94 .94 .90 .88 .61 .62 .49
chip .90 .90 .73 .58 .57 .66 .75
clip .70 .79 .65 .58 .48 .53 .65
club .80 .84 .80 .81 .61 .73 .63
court .79 .79 .61 .53 .62 .82 .57
crane .62 .67 .76 .76 .52 .54 .66
game .78 .78 .60 .66 .60 .66 .70
hood .74 .73 .73 .70 .51 .45 .55
jack .76 .74 .62 .53 .58 .66 .47
key .81 .92 .79 .54 .57 .70 .50
mold .67 .68 .59 .67 .57 .66 .54
mouse .84 .84 .71 .62 .62 .69 .68
plant .54 .54 .56 .53 .52 .50 .72
press .60 .59 .60 .54 .58 .62 .48
seal .70 .80 .61 .67 .55 .53 .62
speaker .70 .69 .57 .53 .55 .62 .63
squash .89 .95 .84 .92 .55 .67 .79
track .78 .85 .71 .66 .51 .54 .69
avg. .76 .78 .69 .65 .56 .63 .62
</table>
<bodyText confidence="0.999986611111111">
vector, where the ith element is the proportion of
word U in the document. We run K-means both with
and without visual features.
For LDA, we use the dictionary sense model from
Saenko and Darrell (2008). A topic model is learned
where the relatedness of a topic to a sense is based
on the probabilities of that topic generating the seed
words from its dictionary definitions. Analogously
to k-means, we learn a model for text alone, and a
model for text augmented with visual information.
For unsupervised WSD (applied to text only),
we use WordNet::SenseRelate::TargetWord, here-
after PBP (Patwardhan et al., 2007), the highest
scoring unsupervised lexical sample word sense dis-
ambiguation algorithm at SemEval07 (Pradhan et
al., 2007). PBP treats the nearby words around the
target word as a bag, and uses the WordNet hierar-
chy to assign a similarity score between the possible
senses of words in the context, and possible senses
of the target word. As our captions are fairly short,
we use the entire caption as context.
The most important result is the gain in accuracy
after adding visual features. While the average gain
across all words is slight, it is significant at p &lt; 0.02
(using a paired t-test). For 12 of the 20 words, the
visual features improve performance, and in 6 of
those, the improvement is 5–11%.
For some words there is no significant improve-
ment in accuracy, or even a slight decrease. With
words like “bass” or “chip” there is little room to
improve upon the text-only result. For words like
“plant” or “press” it seems the text-only result is not
strong enough to help bootstrap the visual features
in any useful way. In other cases where little im-
provement is seen, the problem may lie with high
intra-class variation, as our visual words are not very
robust features, or with a lack of orthogonality be-
tween the lexical and visual information.
Our algorithm also performs significantly better
than the baseline measurements. K-means performs
surprisingly well compared to the other baselines,
but seems unable to make much sense of the visual
information present. Saenko and Darrell’s (2008)
LDA model makes substansial gains by using vi-
sual features, but does not perform as well on this
task. We suspect that a strict adherence to the seed
words may be to blame: while both this LDA model
and our algorithm use the same seed definitions ini-
tially, our algorithm is free to change its mind about
the usefulness of the words in the definitions as it
progresses, whereas the LDA model has no such
capacity. Indeed, words that are intuitively non-
discriminative, such as “carry”, “lack”, or “late”, are
not uncommon in the definitions we use.
</bodyText>
<sectionHeader confidence="0.992708" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999977785714286">
We present an approach to unsupervised WSD that
works jointly with the visual and textual domains.
We showed that this multimodal approach makes
gains over text-only disambiguation, and outper-
forms previous approaches for WSD (both text-only,
and multimodal), when textual contexts are limited.
This project is still in progress, and there are many
avenues for further study. We do not currently ex-
ploit collocations between lexical and visual infor-
mation. Also, the bag-of-SIFT visual features that
we use, while effective, have little semantic content.
More structured representations over segmented im-
age regions offer greater potential for encoding se-
mantic content (Duygulu et al., 2002).
</bodyText>
<page confidence="0.998431">
88
</page>
<sectionHeader confidence="0.983556" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999266317460317">
Kobus Barnard and Matthew Johnson. 2005. Word
sense disambiguation with pictures. In Artificial In-
telligence, volume 167, pages 13–130.
Kobus Barnard, Matthew Johnson, and David Forsyth.
2003. Word sense disambiguation with pictures.
In Workshop on Learning Word Meaning from Non-
Linguistic Data, Edmonton, Canada.
David M. Blei, Andrew Ng, and Michael I. Jordan. 2003.
Latent dirichlet allocation. In JMLR, volume 3, pages
993–1022.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hierar-
chical image database. In IEEE Conference on Com-
puter Vision and Pattern Recognition.
Pinar Duygulu, Kobus Barnard, Nando de Freitas, and
David Forsyth. 2002. Object recognition as machine
translation: Learning a lexicon for a fixed image vo-
cabulary. In European Conference on Computer Vi-
sion, Copenhagen, Denmark.
Christiane Fellbaum. 1998. Wordnet: An electronic lex-
ical database. In Bradford Books.
Yansong Feng and Mirella Lapata. 2010. Topic models
for image annotation and text illustration. In Annual
Conference of the North American Chapter of the ACL,
pages 831–839, Los Angeles, California.
Michael Jamieson, Afsaneh Fazly, Suzanne Stevenson,
Sven Dickinson, and Sven Wachsmuth. 2009. Using
language to learn structured appearance models for im-
age annotation. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 32(1):148–164.
Chee Wee Leong and Rada Mihalcea. 2011. Measuring
the semantic relatedness between words and images.
In International Conference on Semantic Computing,
Oxford, UK.
Nicolas Loeff, Cecilia Ovesdotter Alm, and David
Forsyth. 2006. Discriminating image senses by clus-
tering with multimodal features. In Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions,
pages 547–554, Sydney, Australia.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2):91–110.
George Miller, Claudia Leacock, Randee Tengi, and Ross
Bunker. 1993. A semantic concordance. In Proceed-
ings of the 3rd DARPA Workshop on Human Language
Technology, pages 303–308.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2007. UMND1: Unsupervised word sense
disambiguation using contextual semantic relatedness.
In Proceedings of SemEval-2007, pages 390–393,
Prague, Czech Republic.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Task 17: English lexical sam-
ple, SRL and all words. In Proceedings of SemEval-
2007, pages 87–92, Prague, Czech Republic.
Kate Saenko and Trevor Darrell. 2008. Unsupervised
learning of visual sense models for polysemous words.
In Proceedings of Neural Information Processing Sys-
tems, Vancouver, Canada.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the ACL, pages
189–196, Cambridge, Massachusetts.
</reference>
<page confidence="0.999752">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.913849">
<title confidence="0.999729">Unsupervised Disambiguation of Image Captions</title>
<author confidence="0.99828">Wesley May</author>
<author confidence="0.99828">Sanja Fidler</author>
<author confidence="0.99828">Afsaneh Fazly</author>
<author confidence="0.99828">Sven Dickinson</author>
<author confidence="0.99828">Suzanne</author>
<affiliation confidence="0.9997325">Department of Computer University of</affiliation>
<address confidence="0.97139">Toronto, Ontario, Canada, M5S</address>
<email confidence="0.999047">twesley,fidler,afsaneh,sven,suzannel@cs.toronto.edu</email>
<abstract confidence="0.9959814375">Given a set of images with related captions, our goal is to show how visual features can improve the accuracy of unsupervised word sense disambiguation when the textual context is very small, as this sort of data is common in news and social media. We extend previous work in unsupervised text-only disambiguation with methods that integrate text and images. We construct a corpus by using Amazon Mechanical Turk to caption sensetagged images gathered from ImageNet. Using a Yarowsky-inspired algorithm, we show that gains can be made over text-only disambiguation, as well as multimodal approaches such as Latent Dirichlet Allocation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kobus Barnard</author>
<author>Matthew Johnson</author>
</authors>
<title>Word sense disambiguation with pictures.</title>
<date>2005</date>
<journal>In Artificial Intelligence,</journal>
<volume>167</volume>
<pages>13--130</pages>
<contexts>
<context position="7147" citStr="Barnard and Johnson (2005)" startWordPosition="1247" endWordPosition="1250">jDi n S1j &gt; jDi n S2j then L1 +– L1 U {Di} else if jDi n S1j &lt; jDi n S2j then L2 +– L2 U {Di} end if end for repeat for all i E L.v do . Update word conf. (P (Vi�L�) ) Ci � log P (Vi�L2) end for L1 — 0, L2 — 0 . Update document conf. for all Di do . Find word with highest confidence m +– arg max jCjj jE1..v,VjEDi if Cm &gt; τc then L1 , L1 U {Di} else if Cm &lt; —τc then L2 , L2 U {Di} end if end for until no change to L1 or L2 3 Creation of the Dataset We require a collection of images with associated captions. We also require sense annotations for the keyword for each image to use for evaluation. Barnard and Johnson (2005) developed the 86 “Music is an important “Keeping your office supmeans of expression for plies organized is easy, with many teens.” the right tools.” “The internet has opened up “When there is no cheese I the world to people of all will take over the world.” nationalities.” blue fish”, as such captions are very unnatural. True captions generally offer orthogonal information that is not readily apparent from the image. The keyword for each image (as specified by ImageNet) was not presented to the Turkers, so the captions do not necessarily contain it. Knowledge of the keyword is presumed to be </context>
</contexts>
<marker>Barnard, Johnson, 2005</marker>
<rawString>Kobus Barnard and Matthew Johnson. 2005. Word sense disambiguation with pictures. In Artificial Intelligence, volume 167, pages 13–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kobus Barnard</author>
<author>Matthew Johnson</author>
<author>David Forsyth</author>
</authors>
<title>Word sense disambiguation with pictures.</title>
<date>2003</date>
<booktitle>In Workshop on Learning Word Meaning from NonLinguistic Data,</booktitle>
<location>Edmonton, Canada.</location>
<marker>Barnard, Johnson, Forsyth, 2003</marker>
<rawString>Kobus Barnard, Matthew Johnson, and David Forsyth. 2003. Word sense disambiguation with pictures. In Workshop on Learning Word Meaning from NonLinguistic Data, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<booktitle>In JMLR,</booktitle>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="10743" citStr="Blei et al., 2003" startWordPosition="1865" endWordPosition="1868">0) serve as our visual words. It is common for each image to have a “vocabulary” of over 300 distinct visual words, many of which only occur once. To denoise the visual data, we use only those visual words which account for at least 1% of the total visual words for that image. 4 Experiments and Results To show that the addition of visual features improves the accuracy of sense disambiguation for image– caption pairs, we run our algorithm both with and without the visual features. We also compare our results to three different baseline methods: K-means (K-M), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and an unsupervised WSD algorithm (PBP) explained below. We use accuracy to measure performance as it is commonly used by the WSD community (See Table 1). For K-means, we set k = 2 as we have two senses, and represent each document with a V-dimensional 87 Table 1: Results (Average accuracy across all five sets of data). Bold indicates best performance for that word. Ours Ours K-M K-M LDA LDA PBP text w/vis text w/vis text w/vis text band .80 .82 .66 .65 .64 .56 .73 bank .77 .78 .71 .59 .52 .67 .62 bass .94 .94 .90 .88 .61 .62 .49 chip .90 .90 .73 .58 .57 .66 .75 clip .70 .79 .65 .58 .48 .53 </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. In JMLR, volume 3, pages 993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>Imagenet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="8776" citStr="Deng et al., 2009" startWordPosition="1521" endWordPosition="1524">es. Figure 2: Example image-caption pairs from our dataset, for “band” (top) and “mouse” (bottom). ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al., 1993). Loeff et al. (2006) and Saenko and Darrell (2008) used Yahoo!’s image search to gather images with their associated web pages. While these datasets contain images paired with text, the textual contexts are much larger than typical captions. 3.1 Captioning Images To develop a large set of sense-annotated image– caption pairs with a focus on caption-sized text, we turned to ImageNet (Deng et al., 2009). ImageNet is a database of images that are each associated with a synset from WordNet. Hundreds of images are available for each of a number of senses of a wide variety of common nouns. To gather captions, we used Amazon Mechanical Turk to collect five sentences for each image. We chose two word senses for each of 20 polysemous nouns and for each sense we collected captions for 50 representative images. For each image we gathered five captions, for a total of 10,000 captions. As we have five captions for each image, we split our data into five sets. Each set has the same images, but each imag</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinar Duygulu</author>
<author>Kobus Barnard</author>
<author>Nando de Freitas</author>
<author>David Forsyth</author>
</authors>
<title>Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary.</title>
<date>2002</date>
<booktitle>In European Conference on Computer Vision,</booktitle>
<location>Copenhagen, Denmark.</location>
<marker>Duygulu, Barnard, de Freitas, Forsyth, 2002</marker>
<rawString>Pinar Duygulu, Kobus Barnard, Nando de Freitas, and David Forsyth. 2002. Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary. In European Conference on Computer Vision, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>Wordnet: An electronic lexical database. In</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="5052" citStr="Fellbaum, 1998" startWordPosition="827" endWordPosition="828">though the algorithm is extensible to more than two senses). Such a collection might result from an internet image search using an ambiguous word such as “mouse”. Each Di is an image–caption pair repsented as a bag-of-words that includes both lexical words from the caption, and “visual words” from the image. A visual word is simply an abstract representation that describes a small portion of an image, such that similar portions in other images are represented by the same visual word (see Section 3.2 for details). Our seed sets consist of the words in the definitions of s1 and s2 from WordNet (Fellbaum, 1998). Any document whose caption contains more words from one sense definition than the other is initially labelled with that sense. We then iterate between two steps that (i) find additional words associated with s1 or s2 in currently labelled data, and (ii) relabel all data using the word sense associations discovered so far. We let V be the entire vocabulary of words across all documents. We run experiements both with and without visual words, but when we use visual words, they are included in V . In the first step, we compute a confidence Ci for each word Vi. This confidence is a log-ratio of </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. Wordnet: An electronic lexical database. In Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for image annotation and text illustration.</title>
<date>2010</date>
<booktitle>In Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>831--839</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="2803" citStr="Feng and Lapata (2010)" startWordPosition="449" endWordPosition="453">se is much clearer. We develop an unsupervised WSD algorithm based on Yarowsky’s that uses words in a short caption along with “visual words” from the captioned image to choose the best of two possible senses of an ambiguous keyword describing the content of the image. Language-vision integration is a quickly developing field, and a number of researchers have explored the possibility of combining text and visual features in various multimodal tasks. Leong and Mihalcea (2011) explored semantic relatedness between words and images to better exploit multimodal content. Jamieson et al. (2009) and Feng and Lapata (2010) combined text and vision to perform effective image annotation. Barnard and colleagues (2003; 2005) showed that supervised WSD by could be improved with visual features. Here we show that unsupervised WSD can similarly be improved. Loeff, Alm and Forsyth (2006) and Saenko and Darrell (2008) combined visual and textual information to solve a related task, image sense disambiguation, in 85 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 85–89, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics an unsupervised fashion. In Loeff et al.’</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Topic models for image annotation and text illustration. In Annual Conference of the North American Chapter of the ACL, pages 831–839, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Jamieson</author>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
<author>Sven Dickinson</author>
<author>Sven Wachsmuth</author>
</authors>
<title>Using language to learn structured appearance models for image annotation.</title>
<date>2009</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="2776" citStr="Jamieson et al. (2009)" startWordPosition="444" endWordPosition="447">available, the intended sense is much clearer. We develop an unsupervised WSD algorithm based on Yarowsky’s that uses words in a short caption along with “visual words” from the captioned image to choose the best of two possible senses of an ambiguous keyword describing the content of the image. Language-vision integration is a quickly developing field, and a number of researchers have explored the possibility of combining text and visual features in various multimodal tasks. Leong and Mihalcea (2011) explored semantic relatedness between words and images to better exploit multimodal content. Jamieson et al. (2009) and Feng and Lapata (2010) combined text and vision to perform effective image annotation. Barnard and colleagues (2003; 2005) showed that supervised WSD by could be improved with visual features. Here we show that unsupervised WSD can similarly be improved. Loeff, Alm and Forsyth (2006) and Saenko and Darrell (2008) combined visual and textual information to solve a related task, image sense disambiguation, in 85 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 85–89, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics an unsupervise</context>
</contexts>
<marker>Jamieson, Fazly, Stevenson, Dickinson, Wachsmuth, 2009</marker>
<rawString>Michael Jamieson, Afsaneh Fazly, Suzanne Stevenson, Sven Dickinson, and Sven Wachsmuth. 2009. Using language to learn structured appearance models for image annotation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(1):148–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chee Wee Leong</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic relatedness between words and images.</title>
<date>2011</date>
<booktitle>In International Conference on Semantic Computing,</booktitle>
<location>Oxford, UK.</location>
<contexts>
<context position="2660" citStr="Leong and Mihalcea (2011)" startWordPosition="426" endWordPosition="430">then text-only disambiguation becomes much more difficult; a human could do little more than guess. But if an image is available, the intended sense is much clearer. We develop an unsupervised WSD algorithm based on Yarowsky’s that uses words in a short caption along with “visual words” from the captioned image to choose the best of two possible senses of an ambiguous keyword describing the content of the image. Language-vision integration is a quickly developing field, and a number of researchers have explored the possibility of combining text and visual features in various multimodal tasks. Leong and Mihalcea (2011) explored semantic relatedness between words and images to better exploit multimodal content. Jamieson et al. (2009) and Feng and Lapata (2010) combined text and vision to perform effective image annotation. Barnard and colleagues (2003; 2005) showed that supervised WSD by could be improved with visual features. Here we show that unsupervised WSD can similarly be improved. Loeff, Alm and Forsyth (2006) and Saenko and Darrell (2008) combined visual and textual information to solve a related task, image sense disambiguation, in 85 First Joint Conference on Lexical and Computational Semantics (*S</context>
</contexts>
<marker>Leong, Mihalcea, 2011</marker>
<rawString>Chee Wee Leong and Rada Mihalcea. 2011. Measuring the semantic relatedness between words and images. In International Conference on Semantic Computing, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Loeff</author>
<author>Cecilia Ovesdotter Alm</author>
<author>David Forsyth</author>
</authors>
<title>Discriminating image senses by clustering with multimodal features.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>547--554</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="8392" citStr="Loeff et al. (2006)" startWordPosition="1460" endWordPosition="1463"> in the form of an image tag, or filename, or the like. We found that forcing a certain word to be included in the caption also led to sentences that described the picture very directly. Sentences were required to be a least ten words long, and have acceptable grammar and spelling. We remove stop words from the captions and lemmatize the remaining words. See Figure 2 for some examples. Figure 2: Example image-caption pairs from our dataset, for “band” (top) and “mouse” (bottom). ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al., 1993). Loeff et al. (2006) and Saenko and Darrell (2008) used Yahoo!’s image search to gather images with their associated web pages. While these datasets contain images paired with text, the textual contexts are much larger than typical captions. 3.1 Captioning Images To develop a large set of sense-annotated image– caption pairs with a focus on caption-sized text, we turned to ImageNet (Deng et al., 2009). ImageNet is a database of images that are each associated with a synset from WordNet. Hundreds of images are available for each of a number of senses of a wide variety of common nouns. To gather captions, we used A</context>
</contexts>
<marker>Loeff, Alm, Forsyth, 2006</marker>
<rawString>Nicolas Loeff, Cecilia Ovesdotter Alm, and David Forsyth. 2006. Discriminating image senses by clustering with multimodal features. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 547–554, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lowe</author>
</authors>
<title>Distinctive image features from scale-invariant keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="9795" citStr="Lowe, 2004" startWordPosition="1706" endWordPosition="1707">For each image we gathered five captions, for a total of 10,000 captions. As we have five captions for each image, we split our data into five sets. Each set has the same images, but each image is paired with a different caption in each set. We specified to the Turkers that the sentences should be relevant to, but should not talk directly about, the image, as in “In this picture there is a 3.2 Computing the Visual Words We compute visual words for each image with ImageNet’s feature extractor. This extractor lays down a grid of overlapping squares onto the image and computes a SIFT descriptor (Lowe, 2004) for each square. Each descriptor is a vector that encodes the edge orientation information in a given square. The descriptors are computed at three scales: 1x, 0.5x and 0.25x the original side lengths. These vectors are clustered with k-means into 1000 clusters, and the labels of these clusters (arbitrary integers from 1 to 1000) serve as our visual words. It is common for each image to have a “vocabulary” of over 300 distinct visual words, many of which only occur once. To denoise the visual data, we use only those visual words which account for at least 1% of the total visual words for that</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David Lowe. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd DARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<contexts>
<context position="8371" citStr="Miller et al., 1993" startWordPosition="1456" endWordPosition="1459">lable to the algorithm in the form of an image tag, or filename, or the like. We found that forcing a certain word to be included in the caption also led to sentences that described the picture very directly. Sentences were required to be a least ten words long, and have acceptable grammar and spelling. We remove stop words from the captions and lemmatize the remaining words. See Figure 2 for some examples. Figure 2: Example image-caption pairs from our dataset, for “band” (top) and “mouse” (bottom). ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al., 1993). Loeff et al. (2006) and Saenko and Darrell (2008) used Yahoo!’s image search to gather images with their associated web pages. While these datasets contain images paired with text, the textual contexts are much larger than typical captions. 3.1 Captioning Images To develop a large set of sense-annotated image– caption pairs with a focus on caption-sized text, we turned to ImageNet (Deng et al., 2009). ImageNet is a database of images that are each associated with a synset from WordNet. Hundreds of images are available for each of a number of senses of a wide variety of common nouns. To gathe</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George Miller, Claudia Leacock, Randee Tengi, and Ross Bunker. 1993. A semantic concordance. In Proceedings of the 3rd DARPA Workshop on Human Language Technology, pages 303–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>UMND1: Unsupervised word sense disambiguation using contextual semantic relatedness.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007,</booktitle>
<pages>390--393</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12499" citStr="Patwardhan et al., 2007" startWordPosition="2213" endWordPosition="2216">6 .63 .62 vector, where the ith element is the proportion of word U in the document. We run K-means both with and without visual features. For LDA, we use the dictionary sense model from Saenko and Darrell (2008). A topic model is learned where the relatedness of a topic to a sense is based on the probabilities of that topic generating the seed words from its dictionary definitions. Analogously to k-means, we learn a model for text alone, and a model for text augmented with visual information. For unsupervised WSD (applied to text only), we use WordNet::SenseRelate::TargetWord, hereafter PBP (Patwardhan et al., 2007), the highest scoring unsupervised lexical sample word sense disambiguation algorithm at SemEval07 (Pradhan et al., 2007). PBP treats the nearby words around the target word as a bag, and uses the WordNet hierarchy to assign a similarity score between the possible senses of words in the context, and possible senses of the target word. As our captions are fairly short, we use the entire caption as context. The most important result is the gain in accuracy after adding visual features. While the average gain across all words is slight, it is significant at p &lt; 0.02 (using a paired t-test). For 1</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2007</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2007. UMND1: Unsupervised word sense disambiguation using contextual semantic relatedness. In Proceedings of SemEval-2007, pages 390–393, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Task 17: English lexical sample, SRL and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval2007,</booktitle>
<pages>87--92</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12620" citStr="Pradhan et al., 2007" startWordPosition="2230" endWordPosition="2233">sual features. For LDA, we use the dictionary sense model from Saenko and Darrell (2008). A topic model is learned where the relatedness of a topic to a sense is based on the probabilities of that topic generating the seed words from its dictionary definitions. Analogously to k-means, we learn a model for text alone, and a model for text augmented with visual information. For unsupervised WSD (applied to text only), we use WordNet::SenseRelate::TargetWord, hereafter PBP (Patwardhan et al., 2007), the highest scoring unsupervised lexical sample word sense disambiguation algorithm at SemEval07 (Pradhan et al., 2007). PBP treats the nearby words around the target word as a bag, and uses the WordNet hierarchy to assign a similarity score between the possible senses of words in the context, and possible senses of the target word. As our captions are fairly short, we use the entire caption as context. The most important result is the gain in accuracy after adding visual features. While the average gain across all words is slight, it is significant at p &lt; 0.02 (using a paired t-test). For 12 of the 20 words, the visual features improve performance, and in 6 of those, the improvement is 5–11%. For some words t</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. Task 17: English lexical sample, SRL and all words. In Proceedings of SemEval2007, pages 87–92, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Saenko</author>
<author>Trevor Darrell</author>
</authors>
<title>Unsupervised learning of visual sense models for polysemous words.</title>
<date>2008</date>
<booktitle>In Proceedings of Neural Information Processing Systems,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="3095" citStr="Saenko and Darrell (2008)" startWordPosition="497" endWordPosition="500">egration is a quickly developing field, and a number of researchers have explored the possibility of combining text and visual features in various multimodal tasks. Leong and Mihalcea (2011) explored semantic relatedness between words and images to better exploit multimodal content. Jamieson et al. (2009) and Feng and Lapata (2010) combined text and vision to perform effective image annotation. Barnard and colleagues (2003; 2005) showed that supervised WSD by could be improved with visual features. Here we show that unsupervised WSD can similarly be improved. Loeff, Alm and Forsyth (2006) and Saenko and Darrell (2008) combined visual and textual information to solve a related task, image sense disambiguation, in 85 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 85–89, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics an unsupervised fashion. In Loeff et al.’s work, little gain was realized when visual features were added to a great deal of text. We show that these features have more utility with small textual contexts, and that, when little text is available, our method is more suitable than Saenko and Darrell’s. 2 Our Algorithm We model our al</context>
<context position="8422" citStr="Saenko and Darrell (2008)" startWordPosition="1465" endWordPosition="1468"> tag, or filename, or the like. We found that forcing a certain word to be included in the caption also led to sentences that described the picture very directly. Sentences were required to be a least ten words long, and have acceptable grammar and spelling. We remove stop words from the captions and lemmatize the remaining words. See Figure 2 for some examples. Figure 2: Example image-caption pairs from our dataset, for “band” (top) and “mouse” (bottom). ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al., 1993). Loeff et al. (2006) and Saenko and Darrell (2008) used Yahoo!’s image search to gather images with their associated web pages. While these datasets contain images paired with text, the textual contexts are much larger than typical captions. 3.1 Captioning Images To develop a large set of sense-annotated image– caption pairs with a focus on caption-sized text, we turned to ImageNet (Deng et al., 2009). ImageNet is a database of images that are each associated with a synset from WordNet. Hundreds of images are available for each of a number of senses of a wide variety of common nouns. To gather captions, we used Amazon Mechanical Turk to colle</context>
<context position="12087" citStr="Saenko and Darrell (2008)" startWordPosition="2147" endWordPosition="2150">60 .66 .60 .66 .70 hood .74 .73 .73 .70 .51 .45 .55 jack .76 .74 .62 .53 .58 .66 .47 key .81 .92 .79 .54 .57 .70 .50 mold .67 .68 .59 .67 .57 .66 .54 mouse .84 .84 .71 .62 .62 .69 .68 plant .54 .54 .56 .53 .52 .50 .72 press .60 .59 .60 .54 .58 .62 .48 seal .70 .80 .61 .67 .55 .53 .62 speaker .70 .69 .57 .53 .55 .62 .63 squash .89 .95 .84 .92 .55 .67 .79 track .78 .85 .71 .66 .51 .54 .69 avg. .76 .78 .69 .65 .56 .63 .62 vector, where the ith element is the proportion of word U in the document. We run K-means both with and without visual features. For LDA, we use the dictionary sense model from Saenko and Darrell (2008). A topic model is learned where the relatedness of a topic to a sense is based on the probabilities of that topic generating the seed words from its dictionary definitions. Analogously to k-means, we learn a model for text alone, and a model for text augmented with visual information. For unsupervised WSD (applied to text only), we use WordNet::SenseRelate::TargetWord, hereafter PBP (Patwardhan et al., 2007), the highest scoring unsupervised lexical sample word sense disambiguation algorithm at SemEval07 (Pradhan et al., 2007). PBP treats the nearby words around the target word as a bag, and </context>
</contexts>
<marker>Saenko, Darrell, 2008</marker>
<rawString>Kate Saenko and Trevor Darrell. 2008. Unsupervised learning of visual sense models for polysemous words. In Proceedings of Neural Information Processing Systems, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the ACL,</booktitle>
<pages>189--196</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1934" citStr="Yarowsky, 1995" startWordPosition="307" endWordPosition="308">is embedded in a large We thank NSERC and U. Toronto for financial support. Fidler and Dickinson were sponsored by the Army Research Laboratory and this research was accomplished in part under Cooperative Agreement Number W911NF-10-2-0060. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either express or implied, of the Army Research Laboratory or the U.S. Government. Figure 1: “The crane was so massive it blocked the sun.” Which sense of crane? With images the answer is clear. quantity of text (Yarowsky, 1995). However, if the only available text is “The crane was so massive it blocked the sun” (see Fig. 1), then text-only disambiguation becomes much more difficult; a human could do little more than guess. But if an image is available, the intended sense is much clearer. We develop an unsupervised WSD algorithm based on Yarowsky’s that uses words in a short caption along with “visual words” from the captioned image to choose the best of two possible senses of an ambiguous keyword describing the content of the image. Language-vision integration is a quickly developing field, and a number of research</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the ACL, pages 189–196, Cambridge, Massachusetts.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>