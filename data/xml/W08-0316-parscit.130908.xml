<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056385">
<title confidence="0.996927">
European Language Translation with Weighted Finite State Transducers:
The CUED MT System for the 2008 ACL Workshop on SMT
</title>
<author confidence="0.963948">
Graeme Blackwood, Adri`a de Gispert, Jamie Brunning, William Byrne
</author>
<affiliation confidence="0.9851535">
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
</affiliation>
<address confidence="0.617987">
Trumpington Street, Cambridge, CB2 1PZ, U.K.
</address>
<email confidence="0.998182">
{gwb24|ad465|jjjb2|wjb31}@cam.ac.uk
</email>
<sectionHeader confidence="0.994806" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.866169538461538">
We describe the Cambridge University En-
gineering Department phrase-based statisti-
cal machine translation system for Spanish-
English and French-English translation in the
ACL 2008 Third Workshop on Statistical Ma-
chine Translation Shared Task. The CUED
system follows a generative model of trans-
lation and is implemented by composition of
component models realised as Weighted Fi-
nite State Transducers, without the use of a
special-purpose decoder. Details of system
tuning for both Europarl and News translation
tasks are provided.
</bodyText>
<sectionHeader confidence="0.998649" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999559586206897">
The Cambridge University Engineering Department
statistical machine translation system follows the
Transducer Translation Model (Kumar and Byrne,
2005; Kumar et al., 2006), a phrase-based generative
model of translation that applies a series of transfor-
mations specified by conditional probability distri-
butions and encoded as Weighted Finite State Trans-
ducers (Mohri et al., 2002).
The main advantages of this approach are its mod-
ularity, which facilitates the development and eval-
uation of each component individually, and its im-
plementation simplicity which allows us to focus on
modeling issues rather than complex decoding and
search algorithms. In addition, no special-purpose
decoder is required since standard WFST operations
can be used to obtain the 1-best translation or a lat-
tice of alternative hypotheses. Finally, the system
architecture readily extends to speech translation, in
which input ASR lattices can be translated in the
same way as for text (Mathias and Byrne, 2006).
This paper reviews the first participation of CUED
in the ACL Workshop on Statistical Machine Trans-
lation in 2008. It is organised as follows. Firstly,
section 2 describes the system architecture and its
main components. Section 3 gives details of the de-
velopment work conducted for this shared task and
results are reported and discussed in section 4. Fi-
nally, in section 5 we summarise our participation in
the task and outline directions for future work.
</bodyText>
<sectionHeader confidence="0.909468" genericHeader="method">
2 The Transducer Translation Model
</sectionHeader>
<bodyText confidence="0.999479428571429">
Under the Transducer Translation Model, the gen-
eration of a target language sentence tJ1 starts with
the generation of a source language sentence sI1 by
the source language model PG(sI1). Next, the source
language sentence is segmented into phrases accord-
ing to the unweighted uniform phrasal segmenta-
tion model PW (uK1 , K|sI1). This source phrase se-
quence generates a reordered target language phrase
sequence according to the phrase translation and re-
ordering model PR(xK1 |uK1 ). Next, target language
phrases are inserted into this sequence according to
the insertion model PD(vR1 |xK1 ,uK1 ). Finally, the
sequence of reordered and inserted target language
phrases are transformed to word sequences tJ1 under
the target phrasal segmentation model Pn(tJ1 |vR1 ).
These component distributions together form a joint
distribution over the source and target language sen-
tences and their possible intermediate phrase se-
quences as P(tJ1 , vR1 , xK1 , uK1 , sI1).
In translation under the generative model, we start
with the target sentence tJ1 in the foreign language
</bodyText>
<page confidence="0.976759">
131
</page>
<note confidence="0.400845">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 131–134,
</note>
<page confidence="0.479096">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.998937333333333">
and search for the best source sentence 4. Encod-
ing each distribution as a WFST leads to a model of
translation as the series of compositions
</bodyText>
<equation confidence="0.96199">
L = G ◦ W ◦ R ◦ 4b ◦ Q ◦ T (1)
</equation>
<bodyText confidence="0.99974075">
in which T is an acceptor for the target language
sentence and L is the word lattice of translations ob-
tained during decoding. The most likely translation
si is the path in L with least cost.
</bodyText>
<subsectionHeader confidence="0.959173">
2.1 TTM Reordering Model
</subsectionHeader>
<bodyText confidence="0.999982">
The TTM reordering model associates a jump se-
quence with each phrase pair. For the experi-
ments described in this paper, the jump sequence
is restricted such that only adjacent phrases can be
swapped; this is the MJ1 reordering model of (Ku-
mar and Byrne, 2005). Although the reordering
probability for each pair of phrases could be esti-
mated from word-aligned parallel data, we here as-
sume a uniform reordering probability p tuned as de-
scribed in section 3.1. Figure 1 shows how the MJ1
reordering model for a pair of phrases x1 and x2 is
implemented as a WFST.
</bodyText>
<equation confidence="0.652036">
x2 : x1
</equation>
<figureCaption confidence="0.999493">
Figure 1: The uniform MJ1 reordering transducer.
</figureCaption>
<bodyText confidence="0.999262571428572">
Spanish and French parallel texts each contain ap-
proximately 5% News Commentary data; the rest
is Europarl data. Various single-reference develop-
ment and test sets were provided for each of the
tracks. However, the 2008 evaluation included a new
News task, for which no corresponding development
set was available.
</bodyText>
<table confidence="0.9960546">
sentences words vocab
FR 1.33M 39.9M 124k
EN 36.4M 106k
ES 1.30M 38.2M 140k
EN 35.7M 106k
</table>
<tableCaption confidence="0.999852">
Table 1: Parallel corpora statistics.
</tableCaption>
<bodyText confidence="0.9999383">
All of the training and system tuning was per-
formed using lower-cased data. Word alignments
were generated using GIZA++ (Och and Ney, 2003)
over a stemmed version of the parallel text. Stems
for each language were obtained using the Snowball
stemmer1. After unioning the Viterbi alignments,
the stems were replaced with their original words,
and phrase-pairs of up to five foreign words in length
were extracted in the usual fashion (Koehn et al.,
2003).
</bodyText>
<subsectionHeader confidence="0.997693">
3.1 System Tuning
</subsectionHeader>
<bodyText confidence="0.9996042">
Minimum error training (Och, 2003) under
BLEU (Papineni et al., 2001) was used to optimise
the feature weights of the decoder with respect
to the dev2006 development set. The following
features are optimized:
</bodyText>
<equation confidence="0.998846666666667">
x : x
1−p / b=0
x1 : x2
p / b=+1
0 1
1 / b=−1
</equation>
<listItem confidence="0.989073">
• Language model scale factor
</listItem>
<sectionHeader confidence="0.992923" genericHeader="method">
3 System Development
</sectionHeader>
<bodyText confidence="0.999332416666667">
CUED participated in two of the WMT shared task
tracks: French→English and Spanish→English. For
both tracks, primary and contrast systems were sub-
mitted. The primary submission was restricted
to only the parallel and language model data dis-
tributed for the shared task. The contrast submission
incorporates large additional quantities of English
monolingual training text for building the second-
pass language model described in section 3.2.
Table 1 summarises the parallel training data, in-
cluding the total number of sentences, total num-
ber of words, and lower-cased vocabulary size. The
</bodyText>
<listItem confidence="0.9999575">
• Word and phrase insertion penalties
• Reordering scale factor
• Insertion scale factor
• Translation model scale factor: u-to-v
• Translation model scale factor: v-to-u
• Three phrase pair count features
</listItem>
<bodyText confidence="0.998273">
The phrase-pair count features track whether each
phrase-pair occurred once, twice, or more than twice
</bodyText>
<footnote confidence="0.99882">
1Available at http://snowball.tartarus.org
</footnote>
<page confidence="0.993153">
132
</page>
<bodyText confidence="0.9997305">
in the parallel text (Bender et al., 2007). All de-
coding and minimum error training operations are
performed with WFSTs and implemented using the
OpenFST libraries (Allauzen et al., 2007).
</bodyText>
<subsectionHeader confidence="0.998723">
3.2 English Language Models
</subsectionHeader>
<bodyText confidence="0.999942909090909">
Separate language models are used when translating
the Europarl and News sets. The models are esti-
mated using SRILM (Stolcke, 2002) and converted
to WFSTs for use in TTM translation. We use the of-
fline approximation in which failure transitions are
replaced with epsilons (Allauzen et al., 2003).
The Europarl language model is a Kneser-
Ney (Kneser and Ney, 1995) smoothed default-
cutoff 5-gram back-off language model estimated
over the concatenation of the Europarl and News
language model training data. The News language
model is created by optimising the interpolation
weights of two component models with respect to
the News Commentary development sets since we
believe these more closely match the newstest2008
domain. The optimised interpolation weights were
0.44 for the Europarl corpus and 0.56 for the much
smaller News Commentary corpus. For our contrast
submission, we rescore the first-pass translation lat-
tices with a large zero-cutoff stupid-backoff (Brants
et al., 2007) language model estimated over approx-
imately five billion words of newswire text.
</bodyText>
<sectionHeader confidence="0.999576" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999424658536585">
Table 2 reports lower-cased BLEU scores for the
French→English and Spanish→English Europarl
and News translation tasks. The NIST scores are
also provided in parentheses. The row labelled
“TTM+MET” shows results obtained after TTM
translation and minimum error training, i.e. our pri-
mary submission constrained to use only the data
distributed for the task. The row labelled “+5gram”
shows translation results obtained after rescoring
with the large zero-cutoff 5-gram language model
described in section 3.2. Since this includes addi-
tional language model data, it represents the CUED
contrast submission.
Translation quality for the ES→EN task is
slightly higher than that of FR→EN. For Europarl
translation, most of the additional English language
model training data incorporated into the 5-gram
rescoring step is out-of-domain and so does not sub-
stantially improve the scores. Rescoring yields an
average gain of just +0.5 BLEU points.
Translation quality is significantly lower in both
language pairs for the new news2008 set. Two fac-
tors may account for this. The first is the change
in domain and the fact that no training or devel-
opment set was available for the News translation
task. Secondly, the use of a much freer translation
in the single News reference, which makes it dif-
ficult to obtain a good BLEU score. However, the
second-pass 5-gram language model rescoring gains
are larger than those observed in the Europarl sets,
with approximately +1.7 BLEU points for each lan-
guage pair. The additional in-domain newswire data
clearly helps to improve translation quality.
Finally, we use a simple 3-gram casing model
trained on the true-case workshop distributed
language model data, and apply the SRILM
disambig tool to restore true-case for our final
submissions. With respect to the lower-cased scores,
true-casing drops around 1.0 BLEU in the Europarl
task, and around 1.7 BLEU in the News Commen-
tary and News tasks.
</bodyText>
<sectionHeader confidence="0.992117" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999895571428572">
We have reviewed the Cambridge University Engi-
neering Department first participation in the work-
shop on machine translation using a phrase-based
SMT system implemented with a simple WFST ar-
chitecture. Results are largely competitive with the
state-of-the-art in this task.
Future work will examine whether further im-
provements can be obtained by incorporating addi-
tional features into MET, such as the word-to-word
Model 1 scores or phrasal segmentation models. The
MJ1 reordering model could also be extended to al-
low for longer-span phrase movement. Minimum
Bayes Risk decoding, which has been applied suc-
cessfully in other tasks, could also be included.
The difference in the gains from 5-gram lattice
rescoring suggests that, particularly for Europarl
translation, it is important to ensure the language
model data is in-domain. Some form of count mix-
ing or alternative language model adaptation tech-
niques may prove useful for unconstrained Europarl
translation.
</bodyText>
<page confidence="0.995899">
133
</page>
<table confidence="0.9998092">
Task dev2006 devtest2006 test2007 test2008 newstest2008
FR→EN TTM+MET 31.92 (7.650) 32.51 (7.719) 32.94 (7.805) 32.83 (7.799) 19.58 (6.108)
+5gram 32.51 (7.744) 32.96 (7.797) 33.33 (7.880) 33.03 (7.856) 21.22 (6.311)
ES→EN TTM+MET 33.11 (7.799) 32.25 (7.649) 32.90 (7.766) 33.11 (7.859) 20.99 (6.308)
+5gram 33.30 (7.835) 32.96 (7.740) 33.55 (7.857) 33.47 (7.893) 22.83 (6.513)
</table>
<tableCaption confidence="0.997288">
Table 2: Translation results for the Europarl and News tasks for various dev sets and the 2008 test sets.
</tableCaption>
<sectionHeader confidence="0.994419" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998220666666667">
This work was supported in part under the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022.
</bodyText>
<sectionHeader confidence="0.998905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999747048387097">
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Meeting of
the Association for Computational Linguistics, pages
557–564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFST: a
general and efficient weighted finite-state transducer
library. In Proceedings of the 9th International Con-
ference on Implementation and Application of Au-
tomata, pages 11–23. Springer.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of the 2007 Automatic
Speech Understanding Workshop, pages 396–401.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858–867.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech, and
Signal Processing, pages 181–184.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference for Computational Lin-
guistics on Human Language Technology, pages 48–
54, Morristown, NJ, USA.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 161–168.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35–75.
Lambert Mathias and William Byrne. 2006. Statistical
phrase-based speech translation. In 2006 IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69–88.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Meeting of the Association for Computational
Linguistics, pages 160–167, Morristown, NJ, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Meeting of the Association for Computational
Linguistics, pages 311–318, Morristown, NJ, USA.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
</reference>
<page confidence="0.99862">
134
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.269535">
<title confidence="0.9924035">European Language Translation with Weighted Finite State The CUED MT System for the 2008 ACL Workshop on SMT</title>
<author confidence="0.884076">Graeme Blackwood</author>
<author confidence="0.884076">Adri`a de_Gispert</author>
<author confidence="0.884076">Jamie Brunning</author>
<author confidence="0.884076">William Machine Intelligence</author>
<affiliation confidence="0.949932">Department of Engineering, Cambridge</affiliation>
<address confidence="0.338377">Trumpington Street, Cambridge, CB2 1PZ,</address>
<abstract confidence="0.999217714285714">We describe the Cambridge University Engineering Department phrase-based statistical machine translation system for Spanish- English and French-English translation in the ACL 2008 Third Workshop on Statistical Machine Translation Shared Task. The CUED system follows a generative model of translation and is implemented by composition of component models realised as Weighted Finite State Transducers, without the use of a special-purpose decoder. Details of system tuning for both Europarl and News translation tasks are provided.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="7314" citStr="Allauzen et al., 2003" startWordPosition="1159" endWordPosition="1162">ther each phrase-pair occurred once, twice, or more than twice 1Available at http://snowball.tartarus.org 132 in the parallel text (Bender et al., 2007). All decoding and minimum error training operations are performed with WFSTs and implemented using the OpenFST libraries (Allauzen et al., 2007). 3.2 English Language Models Separate language models are used when translating the Europarl and News sets. The models are estimated using SRILM (Stolcke, 2002) and converted to WFSTs for use in TTM translation. We use the offline approximation in which failure transitions are replaced with epsilons (Allauzen et al., 2003). The Europarl language model is a KneserNey (Kneser and Ney, 1995) smoothed defaultcutoff 5-gram back-off language model estimated over the concatenation of the Europarl and News language model training data. The News language model is created by optimising the interpolation weights of two component models with respect to the News Commentary development sets since we believe these more closely match the newstest2008 domain. The optimised interpolation weights were 0.44 for the Europarl corpus and 0.56 for the much smaller News Commentary corpus. For our contrast submission, we rescore the fir</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFST: a general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of the 9th International Conference on Implementation and Application of Automata,</booktitle>
<pages>11--23</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6989" citStr="Allauzen et al., 2007" startWordPosition="1107" endWordPosition="1110">mber of sentences, total number of words, and lower-cased vocabulary size. The • Word and phrase insertion penalties • Reordering scale factor • Insertion scale factor • Translation model scale factor: u-to-v • Translation model scale factor: v-to-u • Three phrase pair count features The phrase-pair count features track whether each phrase-pair occurred once, twice, or more than twice 1Available at http://snowball.tartarus.org 132 in the parallel text (Bender et al., 2007). All decoding and minimum error training operations are performed with WFSTs and implemented using the OpenFST libraries (Allauzen et al., 2007). 3.2 English Language Models Separate language models are used when translating the Europarl and News sets. The models are estimated using SRILM (Stolcke, 2002) and converted to WFSTs for use in TTM translation. We use the offline approximation in which failure transitions are replaced with epsilons (Allauzen et al., 2003). The Europarl language model is a KneserNey (Kneser and Ney, 1995) smoothed defaultcutoff 5-gram back-off language model estimated over the concatenation of the Europarl and News language model training data. The News language model is created by optimising the interpolatio</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFST: a general and efficient weighted finite-state transducer library. In Proceedings of the 9th International Conference on Implementation and Application of Automata, pages 11–23. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Bender</author>
<author>Evgeny Matusov</author>
<author>Stefan Hahn</author>
<author>Sasa Hasan</author>
<author>Shahram Khadivi</author>
<author>Hermann Ney</author>
</authors>
<title>The RWTH Arabic-to-English spoken language translation system.</title>
<date>2007</date>
<booktitle>In Proceedings of the</booktitle>
<pages>396--401</pages>
<contexts>
<context position="6844" citStr="Bender et al., 2007" startWordPosition="1085" endWordPosition="1088">text for building the secondpass language model described in section 3.2. Table 1 summarises the parallel training data, including the total number of sentences, total number of words, and lower-cased vocabulary size. The • Word and phrase insertion penalties • Reordering scale factor • Insertion scale factor • Translation model scale factor: u-to-v • Translation model scale factor: v-to-u • Three phrase pair count features The phrase-pair count features track whether each phrase-pair occurred once, twice, or more than twice 1Available at http://snowball.tartarus.org 132 in the parallel text (Bender et al., 2007). All decoding and minimum error training operations are performed with WFSTs and implemented using the OpenFST libraries (Allauzen et al., 2007). 3.2 English Language Models Separate language models are used when translating the Europarl and News sets. The models are estimated using SRILM (Stolcke, 2002) and converted to WFSTs for use in TTM translation. We use the offline approximation in which failure transitions are replaced with epsilons (Allauzen et al., 2003). The Europarl language model is a KneserNey (Kneser and Ney, 1995) smoothed defaultcutoff 5-gram back-off language model estimate</context>
</contexts>
<marker>Bender, Matusov, Hahn, Hasan, Khadivi, Ney, 2007</marker>
<rawString>Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa Hasan, Shahram Khadivi, and Hermann Ney. 2007. The RWTH Arabic-to-English spoken language translation system. In Proceedings of the 2007 Automatic Speech Understanding Workshop, pages 396–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="8004" citStr="Brants et al., 2007" startWordPosition="1263" endWordPosition="1266">oothed defaultcutoff 5-gram back-off language model estimated over the concatenation of the Europarl and News language model training data. The News language model is created by optimising the interpolation weights of two component models with respect to the News Commentary development sets since we believe these more closely match the newstest2008 domain. The optimised interpolation weights were 0.44 for the Europarl corpus and 0.56 for the much smaller News Commentary corpus. For our contrast submission, we rescore the first-pass translation lattices with a large zero-cutoff stupid-backoff (Brants et al., 2007) language model estimated over approximately five billion words of newswire text. 4 Results and Discussion Table 2 reports lower-cased BLEU scores for the French→English and Spanish→English Europarl and News translation tasks. The NIST scores are also provided in parentheses. The row labelled “TTM+MET” shows results obtained after TTM translation and minimum error training, i.e. our primary submission constrained to use only the data distributed for the task. The row labelled “+5gram” shows translation results obtained after rescoring with the large zero-cutoff 5-gram language model described </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="7381" citStr="Kneser and Ney, 1995" startWordPosition="1171" endWordPosition="1174">lable at http://snowball.tartarus.org 132 in the parallel text (Bender et al., 2007). All decoding and minimum error training operations are performed with WFSTs and implemented using the OpenFST libraries (Allauzen et al., 2007). 3.2 English Language Models Separate language models are used when translating the Europarl and News sets. The models are estimated using SRILM (Stolcke, 2002) and converted to WFSTs for use in TTM translation. We use the offline approximation in which failure transitions are replaced with epsilons (Allauzen et al., 2003). The Europarl language model is a KneserNey (Kneser and Ney, 1995) smoothed defaultcutoff 5-gram back-off language model estimated over the concatenation of the Europarl and News language model training data. The News language model is created by optimising the interpolation weights of two component models with respect to the News Commentary development sets since we believe these more closely match the newstest2008 domain. The optimised interpolation weights were 0.44 for the Europarl corpus and 0.56 for the much smaller News Commentary corpus. For our contrast submission, we rescore the first-pass translation lattices with a large zero-cutoff stupid-backof</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5531" citStr="Koehn et al., 2003" startWordPosition="876" endWordPosition="879">onding development set was available. sentences words vocab FR 1.33M 39.9M 124k EN 36.4M 106k ES 1.30M 38.2M 140k EN 35.7M 106k Table 1: Parallel corpora statistics. All of the training and system tuning was performed using lower-cased data. Word alignments were generated using GIZA++ (Och and Ney, 2003) over a stemmed version of the parallel text. Stems for each language were obtained using the Snowball stemmer1. After unioning the Viterbi alignments, the stems were replaced with their original words, and phrase-pairs of up to five foreign words in length were extracted in the usual fashion (Koehn et al., 2003). 3.1 System Tuning Minimum error training (Och, 2003) under BLEU (Papineni et al., 2001) was used to optimise the feature weights of the decoder with respect to the dev2006 development set. The following features are optimized: x : x 1−p / b=0 x1 : x2 p / b=+1 0 1 1 / b=−1 • Language model scale factor 3 System Development CUED participated in two of the WMT shared task tracks: French→English and Spanish→English. For both tracks, primary and contrast systems were submitted. The primary submission was restricted to only the parallel and language model data distributed for the shared task. The </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference for Computational Linguistics on Human Language Technology, pages 48– 54, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="1054" citStr="Kumar and Byrne, 2005" startWordPosition="138" endWordPosition="141">tatistical machine translation system for SpanishEnglish and French-English translation in the ACL 2008 Third Workshop on Statistical Machine Translation Shared Task. The CUED system follows a generative model of translation and is implemented by composition of component models realised as Weighted Finite State Transducers, without the use of a special-purpose decoder. Details of system tuning for both Europarl and News translation tasks are provided. 1 Introduction The Cambridge University Engineering Department statistical machine translation system follows the Transducer Translation Model (Kumar and Byrne, 2005; Kumar et al., 2006), a phrase-based generative model of translation that applies a series of transformations specified by conditional probability distributions and encoded as Weighted Finite State Transducers (Mohri et al., 2002). The main advantages of this approach are its modularity, which facilitates the development and evaluation of each component individually, and its implementation simplicity which allows us to focus on modeling issues rather than complex decoding and search algorithms. In addition, no special-purpose decoder is required since standard WFST operations can be used to o</context>
<context position="4276" citStr="Kumar and Byrne, 2005" startWordPosition="666" endWordPosition="670">rce sentence 4. Encoding each distribution as a WFST leads to a model of translation as the series of compositions L = G ◦ W ◦ R ◦ 4b ◦ Q ◦ T (1) in which T is an acceptor for the target language sentence and L is the word lattice of translations obtained during decoding. The most likely translation si is the path in L with least cost. 2.1 TTM Reordering Model The TTM reordering model associates a jump sequence with each phrase pair. For the experiments described in this paper, the jump sequence is restricted such that only adjacent phrases can be swapped; this is the MJ1 reordering model of (Kumar and Byrne, 2005). Although the reordering probability for each pair of phrases could be estimated from word-aligned parallel data, we here assume a uniform reordering probability p tuned as described in section 3.1. Figure 1 shows how the MJ1 reordering model for a pair of phrases x1 and x2 is implemented as a WFST. x2 : x1 Figure 1: The uniform MJ1 reordering transducer. Spanish and French parallel texts each contain approximately 5% News Commentary data; the rest is Europarl data. Various single-reference development and test sets were provided for each of the tracks. However, the 2008 evaluation included a</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>A weighted finite state transducer translation template model for statistical machine translation.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="1075" citStr="Kumar et al., 2006" startWordPosition="142" endWordPosition="145">slation system for SpanishEnglish and French-English translation in the ACL 2008 Third Workshop on Statistical Machine Translation Shared Task. The CUED system follows a generative model of translation and is implemented by composition of component models realised as Weighted Finite State Transducers, without the use of a special-purpose decoder. Details of system tuning for both Europarl and News translation tasks are provided. 1 Introduction The Cambridge University Engineering Department statistical machine translation system follows the Transducer Translation Model (Kumar and Byrne, 2005; Kumar et al., 2006), a phrase-based generative model of translation that applies a series of transformations specified by conditional probability distributions and encoded as Weighted Finite State Transducers (Mohri et al., 2002). The main advantages of this approach are its modularity, which facilitates the development and evaluation of each component individually, and its implementation simplicity which allows us to focus on modeling issues rather than complex decoding and search algorithms. In addition, no special-purpose decoder is required since standard WFST operations can be used to obtain the 1-best tran</context>
</contexts>
<marker>Kumar, Deng, Byrne, 2006</marker>
<rawString>Shankar Kumar, Yonggang Deng, and William Byrne. 2006. A weighted finite state transducer translation template model for statistical machine translation. Natural Language Engineering, 12(1):35–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lambert Mathias</author>
<author>William Byrne</author>
</authors>
<title>Statistical phrase-based speech translation. In</title>
<date>2006</date>
<booktitle>IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="1894" citStr="Mathias and Byrne, 2006" startWordPosition="268" endWordPosition="271">i et al., 2002). The main advantages of this approach are its modularity, which facilitates the development and evaluation of each component individually, and its implementation simplicity which allows us to focus on modeling issues rather than complex decoding and search algorithms. In addition, no special-purpose decoder is required since standard WFST operations can be used to obtain the 1-best translation or a lattice of alternative hypotheses. Finally, the system architecture readily extends to speech translation, in which input ASR lattices can be translated in the same way as for text (Mathias and Byrne, 2006). This paper reviews the first participation of CUED in the ACL Workshop on Statistical Machine Translation in 2008. It is organised as follows. Firstly, section 2 describes the system architecture and its main components. Section 3 gives details of the development work conducted for this shared task and results are reported and discussed in section 4. Finally, in section 5 we summarise our participation in the task and outline directions for future work. 2 The Transducer Translation Model Under the Transducer Translation Model, the generation of a target language sentence tJ1 starts with the </context>
</contexts>
<marker>Mathias, Byrne, 2006</marker>
<rawString>Lambert Mathias and William Byrne. 2006. Statistical phrase-based speech translation. In 2006 IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<booktitle>In Computer Speech and Language,</booktitle>
<volume>16</volume>
<pages>69--88</pages>
<contexts>
<context position="1285" citStr="Mohri et al., 2002" startWordPosition="173" endWordPosition="176">mplemented by composition of component models realised as Weighted Finite State Transducers, without the use of a special-purpose decoder. Details of system tuning for both Europarl and News translation tasks are provided. 1 Introduction The Cambridge University Engineering Department statistical machine translation system follows the Transducer Translation Model (Kumar and Byrne, 2005; Kumar et al., 2006), a phrase-based generative model of translation that applies a series of transformations specified by conditional probability distributions and encoded as Weighted Finite State Transducers (Mohri et al., 2002). The main advantages of this approach are its modularity, which facilitates the development and evaluation of each component individually, and its implementation simplicity which allows us to focus on modeling issues rather than complex decoding and search algorithms. In addition, no special-purpose decoder is required since standard WFST operations can be used to obtain the 1-best translation or a lattice of alternative hypotheses. Finally, the system architecture readily extends to speech translation, in which input ASR lattices can be translated in the same way as for text (Mathias and Byr</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. In Computer Speech and Language, volume 16, pages 69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5217" citStr="Och and Ney, 2003" startWordPosition="825" endWordPosition="828"> uniform MJ1 reordering transducer. Spanish and French parallel texts each contain approximately 5% News Commentary data; the rest is Europarl data. Various single-reference development and test sets were provided for each of the tracks. However, the 2008 evaluation included a new News task, for which no corresponding development set was available. sentences words vocab FR 1.33M 39.9M 124k EN 36.4M 106k ES 1.30M 38.2M 140k EN 35.7M 106k Table 1: Parallel corpora statistics. All of the training and system tuning was performed using lower-cased data. Word alignments were generated using GIZA++ (Och and Ney, 2003) over a stemmed version of the parallel text. Stems for each language were obtained using the Snowball stemmer1. After unioning the Viterbi alignments, the stems were replaced with their original words, and phrase-pairs of up to five foreign words in length were extracted in the usual fashion (Koehn et al., 2003). 3.1 System Tuning Minimum error training (Och, 2003) under BLEU (Papineni et al., 2001) was used to optimise the feature weights of the decoder with respect to the dev2006 development set. The following features are optimized: x : x 1−p / b=0 x1 : x2 p / b=+1 0 1 1 / b=−1 • Language </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5585" citStr="Och, 2003" startWordPosition="886" endWordPosition="887">1.33M 39.9M 124k EN 36.4M 106k ES 1.30M 38.2M 140k EN 35.7M 106k Table 1: Parallel corpora statistics. All of the training and system tuning was performed using lower-cased data. Word alignments were generated using GIZA++ (Och and Ney, 2003) over a stemmed version of the parallel text. Stems for each language were obtained using the Snowball stemmer1. After unioning the Viterbi alignments, the stems were replaced with their original words, and phrase-pairs of up to five foreign words in length were extracted in the usual fashion (Koehn et al., 2003). 3.1 System Tuning Minimum error training (Och, 2003) under BLEU (Papineni et al., 2001) was used to optimise the feature weights of the decoder with respect to the dev2006 development set. The following features are optimized: x : x 1−p / b=0 x1 : x2 p / b=+1 0 1 1 / b=−1 • Language model scale factor 3 System Development CUED participated in two of the WMT shared task tracks: French→English and Spanish→English. For both tracks, primary and contrast systems were submitted. The primary submission was restricted to only the parallel and language model data distributed for the shared task. The contrast submission incorporates large additional quan</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 160–167, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 40th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5620" citStr="Papineni et al., 2001" startWordPosition="890" endWordPosition="893">4M 106k ES 1.30M 38.2M 140k EN 35.7M 106k Table 1: Parallel corpora statistics. All of the training and system tuning was performed using lower-cased data. Word alignments were generated using GIZA++ (Och and Ney, 2003) over a stemmed version of the parallel text. Stems for each language were obtained using the Snowball stemmer1. After unioning the Viterbi alignments, the stems were replaced with their original words, and phrase-pairs of up to five foreign words in length were extracted in the usual fashion (Koehn et al., 2003). 3.1 System Tuning Minimum error training (Och, 2003) under BLEU (Papineni et al., 2001) was used to optimise the feature weights of the decoder with respect to the dev2006 development set. The following features are optimized: x : x 1−p / b=0 x1 : x2 p / b=+1 0 1 1 / b=−1 • Language model scale factor 3 System Development CUED participated in two of the WMT shared task tracks: French→English and Spanish→English. For both tracks, primary and contrast systems were submitted. The primary submission was restricted to only the parallel and language model data distributed for the shared task. The contrast submission incorporates large additional quantities of English monolingual train</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Meeting of the Association for Computational Linguistics, pages 311–318, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="7150" citStr="Stolcke, 2002" startWordPosition="1134" endWordPosition="1135"> Translation model scale factor: u-to-v • Translation model scale factor: v-to-u • Three phrase pair count features The phrase-pair count features track whether each phrase-pair occurred once, twice, or more than twice 1Available at http://snowball.tartarus.org 132 in the parallel text (Bender et al., 2007). All decoding and minimum error training operations are performed with WFSTs and implemented using the OpenFST libraries (Allauzen et al., 2007). 3.2 English Language Models Separate language models are used when translating the Europarl and News sets. The models are estimated using SRILM (Stolcke, 2002) and converted to WFSTs for use in TTM translation. We use the offline approximation in which failure transitions are replaced with epsilons (Allauzen et al., 2003). The Europarl language model is a KneserNey (Kneser and Ney, 1995) smoothed defaultcutoff 5-gram back-off language model estimated over the concatenation of the Europarl and News language model training data. The News language model is created by optimising the interpolation weights of two component models with respect to the News Commentary development sets since we believe these more closely match the newstest2008 domain. The opt</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>