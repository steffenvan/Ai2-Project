<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.915287">
HPSG-Style Underspecified Japanese Grammar
with Wide Coverage
</title>
<author confidence="0.990116">
MITSUISHI Yutaka% TORISAWA Kentarot, TSUJII Jun&apos;ichitI
</author>
<affiliation confidence="0.984682">
tDepartment of Information Science
Graduate School of Science, University of Tokyo*
</affiliation>
<email confidence="0.301288">
1CCL, UMIST, U.K.
</email>
<sectionHeader confidence="0.983302" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999761421052631">
This paper describes a wide-coverage Japanese
grammar based on HPSG. The aim of this work
is to see the coverage and accuracy attain-
able using an underspecified grammar. Under-
specification, allowed in a typed feature struc-
ture formalism, enables us to write down a
wide-coverage grammar concisely. The gram-
mar we have implemented consists of only 6 ID
schemata, 68 lexical entries (assigned to func-
tional words). and 63 lexical entry templates
(assigned to parts of speech (POSs) ). Further-
more, word-specific constraints such as subcate-
gorization of verbs are not fixed in the gram-
mar. However. this grammar can generate parse
trees for 87% of the 10000 sentences in the
Japanese EDR corpus. The dependency accu-
racy is 78% when a parser uses the heuristic
that every bunsetsul is attached to the nearest
possible one.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999386315789474">
Our purpose is to design a practical Japanese
grammar based on HPSG (Head-driven Phrase
Structure Grammar) (Pollard and Sag, 1994),
with wide coverage and reasonable accuracy for
syntactic structures of real-world texts. In this
paper, &amp;quot;coverage&amp;quot; refers to the percentage of
input sentences for which the grammar returns
at least one parse tree, and &amp;quot;accuracy&amp;quot; refers to
the percentage of bunsetsus which are attached
correctly.
To realize wide coverage and reasonable ac-
curacy, the following steps had been taken:
A) At first we prepared a linguistically valid
but coarse grammar with wide coverage.
B) We then refined the grammar in regard to
accuracy, using practical heuristics which
are not linguistically motivated.
As for A), the first grammar we have con-
structed actually consists of only 68 lexical en-
</bodyText>
<footnote confidence="0.97749775">
* This research is partially founded by the project of
JSPS (JSPS-RFTF96P00502).
1A bunsetsu is a common unit when syntactic struc-
tures in Japanese are discussed.
</footnote>
<bodyText confidence="0.987415380952381">
tries (LEs) for some functional words2, 63 lex-
ical entry templates (LETs) for POSO, and 6
ID schemata. Nevertheless, the coverage of our
grammar was 92% for the Japanese corpus in
the EDR Electronic Dictionary (EDR, 1996),
mainly due to underspecification, which is al-
lowed in HPSG and does not always require de-
tailed grammar descriptions.
As for B), in order to improve accuracy, the
grammar should restrict ambiguity as much as
possible. For this purpose, the grammar needs
more constraints in itself. To reduce ambiguity.
we added additional feature structures which
may not be linguistically valid but be empir-
ically correct. as constraints to i) the original
LEs and LETs. and ii) the ID schemata.
The rest of this paper describes the archi-
tecture of our Japanese grammar (Section 2).
refinement of our grammar (Section 3). exper-
imental results (Section 4). and discussion re-
garding errors (Section 5).
</bodyText>
<sectionHeader confidence="0.765864" genericHeader="method">
2 Architecture of Japanese
Grammar
</sectionHeader>
<bodyText confidence="0.999918352941177">
In this section we describe the architecture of
the HPSG-style Japanese grammar we have de-
veloped. In the HPSG framework, a grammar
consists of (i) immediate dominance schemata
(ID schemata), (ii) principles, and (iii) lexi-
cal entries (LEs). All of them are represented
by typed feature structures (TFSs) (Carpen-
ter, 1992), the fundamental data structures of
HPSG. ID schemata, corresponding to rewrit-
ing rules in CFG, are significant for construct-
ing syntactic structures. The details of our ID
schemata are discussed in Section 2.1. Princi-
ples are constraints between mother and daugh-
ter feature structures:1 LEs, which compose the
lexicon, are detailed constraints on each word.
In our grammar, we do not always assign LEs
to each word. Instead, we assign lexical entry
</bodyText>
<footnote confidence="0.99918925">
2A functional word is assigned one or more LEs.
3A POS is also assigned one or more LETs.
4We omit further explanation about principles here
due to limited space.
</footnote>
<page confidence="0.995881">
876
</page>
<table confidence="0.99967955">
Schema name Explanation Example
Head-complement schema A_pplied when a predicate subcategorizes a Kare ga hashiru. he-sUBJ run
phrase. &apos;He runs.&apos;
Head-relative schema A_pplied when a relative clause modifies a Aruku hitobito.
phrase. walk people
&apos;People who walk.&apos;
Head-marker schema Applied when a marker like a postposition Kanojo ga.
marks a phrase. she -suBJ
&apos;She • • •.&apos;
Head-adjacent schema Applied when a suffix attaches to a word Iku darou.
or a compound word. g
r• o will • • will go.&apos;
Head-compound schema Applied when a compound word is Sinzen Gengo.
constructed. natural language
&apos;Natural language.&apos;
Yukkuri to u.
!1°1iY fly
• • • y slowly.&apos;
Head-modifier schema Applied when a phrase modifies another or
when a coordinate structure is constructed.
</table>
<tableCaption confidence="0.999843">
Table 1: ID schemata in our grammar
</tableCaption>
<bodyText confidence="0.618152">
templates (LETs) to POSs. The details of our
LEs and LETs are discussed in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.970705">
2.1 ID Schemata
</subsectionHeader>
<bodyText confidence="0.966105263157895">
Our grammar includes the 6 ID schemata shown
in Table 1. Although they are similar to the
ones used for English in standard HPSG, there
is a fundamental difference in the treatment of
relative clauses. Our grammar adopts the head-
relative schema to treat relative clauses instead
of the head-filler schema. More specifically, our
grammar does not have SLASH features and does
not use traces. Informally speaking, this is be-
cause SLASH features and traces are really nec-
essary only when there are more than one verb
between the head and the filler (e.g., Sentence
(1) ). But such sentences are rare in real-world
corpora in Japanese. Just using a Head-relative
schema makes our grammar simpler and thus
less ambiguous.
(1) Taro ga aisuru to iu onna.
-SUBJ love -QUOTE say woman
The woman who Taro says that he loves.&apos;
</bodyText>
<subsectionHeader confidence="0.99734">
2.2 Lexical Entries (LEs) and Lexical
Entry Templates (LETs)
</subsectionHeader>
<bodyText confidence="0.999922666666667">
Basically, we assign LETs to POSs. For ex-
ample, common nouns are assigned one LET,
which has general constraints that they can be
complements of predicates, that they can be a
compound noun with other common nouns, and
so on. However, we assign LEs to some single
functional words which behave in a special way.
For example, the verb `suru&apos; can be adjacent to
some nouns unlike other ordinary verbs. The
solution we have adopted is that we assign a
special LE to the verb `suru&apos;.
Our lexicon consists of 68 LEs for some func-
tional words, and 63 LETs for POSs. A func-
tional word is assigned one or more LEs, and a
POS is also assigned one or more LETs.
</bodyText>
<sectionHeader confidence="0.935749" genericHeader="method">
3 Refinement of our Grammar
</sectionHeader>
<bodyText confidence="0.999817115384616">
Our goal in this section is to improve accuracy
without losing coverage. Constraints to improve
accuracy can also be represented by TFSs and
be added to the original grammar components
such as ID schemata, LEs, and LETs.
The basic idea to improve accuracy is that in-
cluding descriptions for rare linguistic phenom-
ena might make it more difficult for our system
to choose the right analyses. Thus, we abandon
some rare linguistic phenomena. This approach
is not always linguistically valid but at least is
practical for real-world corpora.
In this section, we consider some frequent
linguistic phenomena, and explain how we dis-
carded the treatment of rare linguistic phenom-
ena in favor of frequent ones, regarding three
components: (i) the postposition `tva&apos;, (ii) rela-
tive clauses and commas and (iii) nominal suf-
fixes representing time. The way how we aban-
don the treatment of rare linguistic phenomena
is by introducing additional constraints in fea-
ture structures. Regarding (i) and (ii), we intro-
duce `pseudo-principles&apos;, which are unified with
ID schemata in the same way principles are uni-
fied. Regarding (iii), we add some feature struc-
tures to LEs/LETs.
</bodyText>
<subsectionHeader confidence="0.999598">
3.1 Postposition &apos;Wa&apos;
</subsectionHeader>
<bodyText confidence="0.999948">
The main usage of the postposition `tva&apos; is di-
vided into the following two patterns&apos;:
</bodyText>
<listItem confidence="0.9425945">
• If two PPs with the postposition &apos;Ara&apos; ap-
pear consecutively, we treat the first PP as
</listItem>
<footnote confidence="0.758506">
5These patterns are almost similar to the ones in
(Kurohashi and Nagao, 1994).
</footnote>
<page confidence="0.97684">
877
</page>
<figure confidence="0.997736142857143">
(a)
. 1 t
.---1---.
lieu la klda &amp;slit&amp; la 1 nli
(c).
•
Talai la hito ia into samagaskii
</figure>
<figureCaption confidence="0.991090666666667">
Figure 1: (a) Correct / (b) incorrect parse tree for
Sentence (2); (c) correct / (d) incorrect parse tree
for Sentence (3)
</figureCaption>
<bodyText confidence="0.792416">
a complement of a predicate just before the
second ,PP.
</bodyText>
<listItem confidence="0.98835">
• Otherwise, PP with the postposition &apos;we&apos; is
treated as the complement of the last pred-
icate in the sentence.
</listItem>
<figureCaption confidence="0.985786">
Sentences (2) and (3) are examples for these
patterns. respectively. The parse tree for Sen-
tence (2) corresponds to Figure 1(a). but not to
Figure 1(b). and the parse tree for Sentence (3)
corresponds to Figure 1(c). but not to Figure
1(d).
</figureCaption>
<listItem confidence="0.733585307692308">
(2) Taro wa iku ga Jiro wa ika nai.
-TOPIC go but -TOPIC go -NEG
&apos;Though Taro goes, Jiro does not go.&apos;
(3) Tokai wa hito ga ookute sawagashii.
city. -TOPIC people -SUBJ many noisy
&apos;A city is noisy because there are many people.&apos;
Although there are exceptions to the above
patterns (e.g., Sentence (4) &amp; Figure (2) ), they
are rarely observed in real-world corpora. Thus,
we abandon their treatment.
(4) Ude wa nai ga, konjo ga aru.
ability -TOPIC missing but guts -SUBJ exist
&apos;Though he does not have ability, he has guts.&apos;
</listItem>
<bodyText confidence="0.929049666666667">
To deal with the characteristic of &apos;wa&apos;, we in-
troduced the WA feature and the P_WA feature.
Both of them are binary features as follows:
</bodyText>
<figureCaption confidence="0.687748">
Feature Value Meaning
WA + — The phrase contains a/no &apos;Iva&apos;.
P_WA +7— The PP is/isn&apos;t marked by `tva&apos;.
</figureCaption>
<bodyText confidence="0.994354">
We then introduced a &apos;pseudo-principle&apos; for `wa&apos;
in a disjunctive form as below6:
</bodyText>
<listItem confidence="0.38976125">
(A) When applying head-complement schema,
also apply:
6wa_hc and va_hm are DCPs, which are also executed
when the pseudo-principle is applied.
</listItem>
<figure confidence="0.923194444444444">
1 •
1-4-4
I I
Cha ma nai kenjo ga aro.
H_DTRIWA 2
, NH_DTRIP_WA 3
■••■
H_DTRIWA 2
NH_DTRIWA 1
</figure>
<bodyText confidence="0.780928">
wa_tunC,
where
wa_hm(—, —). wa_hm(—, +). wa_hra(+, +).
and so on.
This treatment prunes the parse trees like those
in Figure 1(b, d) as follows:
</bodyText>
<listItem confidence="0.9943635">
• Figure 1(b)
1) At ( 4), the head-complement schema
should be applied, and (A) of the &apos;pseudo-
principle&apos; should also be applied.
2) Since the phrase `iku kedo ashita we ika
nai&apos; contains a. &apos;we&apos;, Ei is +.
3) Since the PP `Kyou wa&apos; is marked by &apos;we&apos;,
is +.
4) wa_hcg, n a fails.
• Figure 1(d)
1) At (#), the head-modifier schema should
be applied, and (B) of the &apos;pseudo-
principle&apos; should also be applied.
2) Since the phrase Tokai wa hito ga ookute&apos;
contains a `tva&apos;, El is +.
3) Since the phrase `sawagashii&apos; contains no
`wa&apos;, El is —.
4) wa_lunCI, fails.
</listItem>
<subsectionHeader confidence="0.998745">
3.2 Relative Clauses and Commas
</subsectionHeader>
<bodyText confidence="0.998038625">
Relative clauses have a tendency to contain no
commas. In Sentence (5), the PP &apos;Nippon de,&apos;
is a complement of the main verb `atta&apos;, not a
complement of `urnareta&apos; in the relative clause
(Figure 3(a) ), though &apos;Nippon de&apos; is preferred
to `umareta&apos; if the comma after &apos;de&apos; does not
exist (Figure 3(b) ). We, therefore, abandon
the treatment of relative clauses containing a
</bodyText>
<figure confidence="0.997627111111111">
(b)*
121
I
I (I)
i 4
I I
4-4-4
1 i I I
Takai va hit. ga eakute saw&amp; &apos;,Alia
</figure>
<figureCaption confidence="0.634907">
Figure 2: Correct parse tree for Sentence (4)
</figureCaption>
<figure confidence="0.977951384615385">
[ WA
DTRS
wa_hc(IiI,
where
—). wa_hc(+, +). wa_hc(—, 4-, +).
(B) When applying head-modifier schema, also
apply:
[ WA
DTRS
El
878
I
Nippon I no sainin nnarota akachan at atta
</figure>
<figureCaption confidence="0.633127">
Figure 3: (a) Correct parse tree for Sentence (5);
</figureCaption>
<figure confidence="0.926965875">
(b) correct parse tree for comma-removed Sentence
(5)
comma.
(5) Nippon de, saikin umareta akachan
Japan -LOC recently be-born-PAST baby
ni atta.
-GOAL meet-PAST
&apos;In Japan I met a baby who was born recently.&apos;
</figure>
<bodyText confidence="0.933145875">
To treat such a tendency of relative clauses.
we first introduced the TOUTEN feature&apos;&apos;&apos;. The
TOUTEN feature is a binary feature which takes
+/— if the phrase contains a/no comma. We
then introduced a &apos;pseudo-principle&apos; for relative
clauses as follows:
(A) When applying head-relative schema, also
apply:
</bodyText>
<equation confidence="0.916014">
DTRSINH_DTRITOUTEN —
</equation>
<bodyText confidence="0.9361505">
(B) When applying other ID schemata, this
pseudo-principle has no effect.
This is to make sure that parse trees for relative
clauses with a comma cannot be produced.
</bodyText>
<subsectionHeader confidence="0.9988535">
3.3 Nominal Suffixes Representing
Time and Commas
</subsectionHeader>
<bodyText confidence="0.9991775">
Noun phrases (NPs) with nominal suffixes such
as nen (year), gatsu (month), and ji (hour) rep-
resent information about time. Such NPs are
sometimes used adverbially, rather than nomi-
nally. Especially NPs with such a nominal suffix
and comma are often used adverbially (Sentence
</bodyText>
<footnote confidence="0.528515166666667">
(6) &amp; Figure 4(a) ), while general NPs with a
comma are used in coordinate structures (Sen-
tence (7) &amp; Figure 4(b) ).
(6) 1995 nen, jishin ga okita.
year earthquake -SUBJ occur-PAST
An earthquake occurred in 1995.
</footnote>
<figure confidence="0.97881">
(a)
t..1-1 I
1916 win. jishin ea oklta.
</figure>
<figureCaption confidence="0.980868">
Figure 4: (a, b) Correct parse trees for Sentences
</figureCaption>
<listItem confidence="0.774117333333333">
(6) and (7) respectively
(7) Kyoto, Nara ni itta.
-GOAL go-PAST
</listItem>
<bodyText confidence="0.933993">
I went to Kyoto and Nara.
In order to restrict the behavior of NPs with
nominal time suffixes and commas to adverbial
usage only, we added the following constraint to
the LE of a comma, constructing a coordinate
structure:
</bodyText>
<subsectionHeader confidence="0.453513">
[ MARKISYNILOCALIN-SUFFIX —
</subsectionHeader>
<bodyText confidence="0.999883">
This prohibits an NP with a nominal suffix from
being marked by a comma for coordination.
</bodyText>
<sectionHeader confidence="0.999231" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.998658">
We implemented our parser and grammar in
LiLFeS (Makin() et al., 1998)8, a feature-
structure description language developed by our
group. We tested randomly selected 10000 sen-
tences from the Japanese EDR corpus (EDR,
1996). The EDR Corpus is a Japanese version
of treebank with morphological, structural, and
semantic information. In our experiments, we
used only the structural information, that is,
parse trees. Both the parse trees in our parser
and the parse trees in the EDR Corpus are first
converted into bunsetsu dependencies, and they
are compared when calculating accuracy. Note
that the internal structures of bunsetsus, e.g.
structures of compound nouns, are not consid-
ered in our evaluations.
We evaluated the following grammars: (a) the
original underspecified grammar, (b) (a) + con-
straint for wa-marked PPs, (c) (a) + constraint
for relative clauses with a comma, (d) (a) + con-
straint for nominal time suffixes with a comma,
and (e) (a) + all the three constraints. We eval-
uated those grammars by the following three
measurements:
Coverage The percentage of the sentences
that generate at least one parse tree.
Partial Accuracy The percentage of the cor-
rect dependencies between bunsetsus (ex-
cepting the last obvious dependency) for
the parsable sentences.
Total Accuracy The percentage of the correct
dependencies between bunsetsus (excepting
the last dependency) over all sentences.
</bodyText>
<figure confidence="0.9171615">
saokin mmarota
(a)
(b)
1
lyoto, nun ni it.
LiLFeS will soon be published on its homepage,
</figure>
<footnote confidence="0.526471">
TA touten stands for a comma in Japanese. http://wwv.is.s.u-tokyo.ac.jp/mak/lilfes/
</footnote>
<page confidence="0.985313">
879
</page>
<table confidence="0.9998">
Coverage Partial Total
Accuracy Accuracy
91.87% 74.20% 72.61%
88.37% 77.50% 74.65%
90.75% 74.98% 73.11%
91.87% 74.41% 72.80%
87.37% 77.77% 74.65%
</table>
<tableCaption confidence="0.990415">
Table 2: Experimental results for 10000 sentences
</tableCaption>
<bodyText confidence="0.989692105263158">
from the Japanese EDR Corpus: (a—e) are grammars
respectively corresponding to Section 2 (a), Section
2 + Subsection 3.1 (b), Section 2 + Subsection 3.2
(c), Section 2 + Subsection 3.3 (d), and Section 2 +
Section 3 (e).
When calculating total accuracy, the depen-
dencies for unparsable sentences are predicted
so that every bunsetsu is attached to the near-
est bunsetsu. In other words, total accuracy
can be regarded as a weighted average of partial
accuracy and baseline accuracy.
Table 2 lists the results of our experiments.
Comparison of the results between (a) and (b—
d) shows that all the three constraints improve
partial accuracy and total accuracy with
little coverage loss. And grammar (e) using the
combination of the three constraints still works
with no side effect.
We also measured average parsing time per
sentence for the original grammar (a) and the
fully augmented grammar (e). The parser we
adopted is a naive CKY-style parser. Table 3
gives the average parsing time per sentence for
those 2 grammars. Pseudo-principles and fur-
ther constraints on LEs/LETs also make pars-
ing more time-efficient. Even though they are
sometimes considered to be slow in practical ap-
plication because of their heavy feature struc-
tures, actually we found them to improve speed.
In (Torisawa and Tsujii, 1996), an efficient
HPSG parser is proposed, and our preliminary
experiments show that the parsing time of the
efficient parser is about three times shorter than
that of the naive one. Thus, the average parsing
time per sentence will be about 300 msec., and
we believe our grammar will achive a practical
speed. Other techniques to speed-up the parser
are proposed in (Makin° et al., 1998).
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.940888571428571">
This section focuses on the behavior of commas.
Out of randomly selected 119 errors in experi-
ment (e), 34 errors are considered to have been
caused by the insufficient treatment of commas.
Especially the fatal errors (28 errors) oc-
curred due to the nature of commas. To put it
Average parsing time per sentence
</bodyText>
<figure confidence="0.984039">
(a) 1277 (msec)
(e) 838 (msec)
</figure>
<tableCaption confidence="0.989371">
Table 3: The average parsing time per sentence
</tableCaption>
<bodyText confidence="0.999985583333333">
in another way, a phrase with a comma, some-
times, is attached to a phrase farther than the
nearest possible phrase. In (Kurohashi and Na-
ga°, 1994), the parser always attaches a phrase
with a comma to the second nearest possible
phrase. We need to introduce such a constraint
into our grammar.
Though the grammar (e) had the pseudo-
principle prohibiting relative clauses containing
commas, there were still 6 relative clauses con-
taining commas. This can be fixed by investi-
gating the nature of relative clauses.
</bodyText>
<sectionHeader confidence="0.999223" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999953">
We have introduced an underspecified Japanese
grammar using the HPSG framework. The
techniques for improving accuracy were easy to
include into our grammar due to the HPSG
framework. Experimental results have shown
that our grammar has wide coverage with rea-
sonable accuracy.
Though the pseudo-principles and further
constraints on LEs/LETs that we have intro-
duced contribute to accuracy, they are too
strong and therefore cause some coverage loss.
One way we could prevent coverage loss is by
introducing preferences for feature structures.
</bodyText>
<sectionHeader confidence="0.997191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.884562095238095">
Bob Carpenter. 1992. The Logic of Typed Fea-
ture Structures. Cambridge University Press.
EDR (Japan Electronic Dictionary Research In-
stitute, Ltd.). 1996. EDR electronic dictio-
nary version 1.5 technical guide.
Sadao Kurohashi and Makoto Nagao. 1994. A
syntactic analysis method of long japanese
sentences based on the detection of conjunc-
tive structures. Computational Linguistics,
20(4):507-534.
Takaki Makino, Minoru Yoshida, Kentaro Tori-
sawa, and Tsujii Jun&apos;ichi. 1998. LiLFeS — to-
wards a practical HPSG parser. In COLING-
A CL &apos;98, August.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. The Uni-
versity of Chicago Press.
Kentaro Torisawa and Jun&apos;ichi Tsujii. 1996.
Computing phrasal-signs in HPSG prior to
parsing. In COLING-96, pages 949-955, Au-
gust.
</reference>
<page confidence="0.997847">
880
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.583616">
<title confidence="0.994878">HPSG-Style Underspecified Japanese Grammar with Wide Coverage</title>
<author confidence="0.939608">MITSUISHI Yutaka TORISAWA Kentarot</author>
<author confidence="0.939608">TSUJII Jun&apos;ichitI</author>
<affiliation confidence="0.879542">tDepartment of Information Science Graduate School of Science, University of Tokyo* UMIST, U.K.</affiliation>
<abstract confidence="0.99880595">This paper describes a wide-coverage Japanese grammar based on HPSG. The aim of this work is to see the coverage and accuracy attainable using an underspecified grammar. Underspecification, allowed in a typed feature structure formalism, enables us to write down a grammar concisely. The gramhave implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words). and 63 lexical entry templates (assigned to parts of speech (POSs) ). Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the gramthis grammar can generate parse for 87% of the in the EDR corpus. dependency accuis 78% a parser uses the heuristic every attached to the nearest possible one.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures.</title>
<date>1992</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3301" citStr="Carpenter, 1992" startWordPosition="522" endWordPosition="524">) the original LEs and LETs. and ii) the ID schemata. The rest of this paper describes the architecture of our Japanese grammar (Section 2). refinement of our grammar (Section 3). experimental results (Section 4). and discussion regarding errors (Section 5). 2 Architecture of Japanese Grammar In this section we describe the architecture of the HPSG-style Japanese grammar we have developed. In the HPSG framework, a grammar consists of (i) immediate dominance schemata (ID schemata), (ii) principles, and (iii) lexical entries (LEs). All of them are represented by typed feature structures (TFSs) (Carpenter, 1992), the fundamental data structures of HPSG. ID schemata, corresponding to rewriting rules in CFG, are significant for constructing syntactic structures. The details of our ID schemata are discussed in Section 2.1. Principles are constraints between mother and daughter feature structures:1 LEs, which compose the lexicon, are detailed constraints on each word. In our grammar, we do not always assign LEs to each word. Instead, we assign lexical entry 2A functional word is assigned one or more LEs. 3A POS is also assigned one or more LETs. 4We omit further explanation about principles here due to l</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Bob Carpenter. 1992. The Logic of Typed Feature Structures. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<title>EDR electronic dictionary version 1.5 technical guide.</title>
<date>1996</date>
<institution>EDR (Japan Electronic Dictionary Research Institute, Ltd.).</institution>
<marker>1996</marker>
<rawString>EDR (Japan Electronic Dictionary Research Institute, Ltd.). 1996. EDR electronic dictionary version 1.5 technical guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>A syntactic analysis method of long japanese sentences based on the detection of conjunctive structures.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--4</pages>
<contexts>
<context position="7812" citStr="Kurohashi and Nagao, 1994" startWordPosition="1282" endWordPosition="1285"> suffixes representing time. The way how we abandon the treatment of rare linguistic phenomena is by introducing additional constraints in feature structures. Regarding (i) and (ii), we introduce `pseudo-principles&apos;, which are unified with ID schemata in the same way principles are unified. Regarding (iii), we add some feature structures to LEs/LETs. 3.1 Postposition &apos;Wa&apos; The main usage of the postposition `tva&apos; is divided into the following two patterns&apos;: • If two PPs with the postposition &apos;Ara&apos; appear consecutively, we treat the first PP as 5These patterns are almost similar to the ones in (Kurohashi and Nagao, 1994). 877 (a) . 1 t .---1---. lieu la klda &amp;slit&amp; la 1 nli (c). • Talai la hito ia into samagaskii Figure 1: (a) Correct / (b) incorrect parse tree for Sentence (2); (c) correct / (d) incorrect parse tree for Sentence (3) a complement of a predicate just before the second ,PP. • Otherwise, PP with the postposition &apos;we&apos; is treated as the complement of the last predicate in the sentence. Sentences (2) and (3) are examples for these patterns. respectively. The parse tree for Sentence (2) corresponds to Figure 1(a). but not to Figure 1(b). and the parse tree for Sentence (3) corresponds to Figure 1(c)</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. A syntactic analysis method of long japanese sentences based on the detection of conjunctive structures. Computational Linguistics, 20(4):507-534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaki Makino</author>
<author>Minoru Yoshida</author>
<author>Kentaro Torisawa</author>
<author>Tsujii Jun&apos;ichi</author>
</authors>
<title>LiLFeS — towards a practical HPSG parser.</title>
<date>1998</date>
<booktitle>In COLINGA CL &apos;98,</booktitle>
<marker>Makino, Yoshida, Torisawa, Jun&apos;ichi, 1998</marker>
<rawString>Takaki Makino, Minoru Yoshida, Kentaro Torisawa, and Tsujii Jun&apos;ichi. 1998. LiLFeS — towards a practical HPSG parser. In COLINGA CL &apos;98, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>HeadDriven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="1192" citStr="Pollard and Sag, 1994" startWordPosition="182" endWordPosition="185">only 6 ID schemata, 68 lexical entries (assigned to functional words). and 63 lexical entry templates (assigned to parts of speech (POSs) ). Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the grammar. However. this grammar can generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus. The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsul is attached to the nearest possible one. 1 Introduction Our purpose is to design a practical Japanese grammar based on HPSG (Head-driven Phrase Structure Grammar) (Pollard and Sag, 1994), with wide coverage and reasonable accuracy for syntactic structures of real-world texts. In this paper, &amp;quot;coverage&amp;quot; refers to the percentage of input sentences for which the grammar returns at least one parse tree, and &amp;quot;accuracy&amp;quot; refers to the percentage of bunsetsus which are attached correctly. To realize wide coverage and reasonable accuracy, the following steps had been taken: A) At first we prepared a linguistically valid but coarse grammar with wide coverage. B) We then refined the grammar in regard to accuracy, using practical heuristics which are not linguistically motivated. As for A</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. HeadDriven Phrase Structure Grammar. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Computing phrasal-signs in HPSG prior to parsing.</title>
<date>1996</date>
<booktitle>In COLING-96,</booktitle>
<pages>949--955</pages>
<contexts>
<context position="15806" citStr="Torisawa and Tsujii, 1996" startWordPosition="2638" endWordPosition="2641">s. And grammar (e) using the combination of the three constraints still works with no side effect. We also measured average parsing time per sentence for the original grammar (a) and the fully augmented grammar (e). The parser we adopted is a naive CKY-style parser. Table 3 gives the average parsing time per sentence for those 2 grammars. Pseudo-principles and further constraints on LEs/LETs also make parsing more time-efficient. Even though they are sometimes considered to be slow in practical application because of their heavy feature structures, actually we found them to improve speed. In (Torisawa and Tsujii, 1996), an efficient HPSG parser is proposed, and our preliminary experiments show that the parsing time of the efficient parser is about three times shorter than that of the naive one. Thus, the average parsing time per sentence will be about 300 msec., and we believe our grammar will achive a practical speed. Other techniques to speed-up the parser are proposed in (Makin° et al., 1998). 5 Discussion This section focuses on the behavior of commas. Out of randomly selected 119 errors in experiment (e), 34 errors are considered to have been caused by the insufficient treatment of commas. Especially t</context>
</contexts>
<marker>Torisawa, Tsujii, 1996</marker>
<rawString>Kentaro Torisawa and Jun&apos;ichi Tsujii. 1996. Computing phrasal-signs in HPSG prior to parsing. In COLING-96, pages 949-955, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>