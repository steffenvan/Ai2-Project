<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999736">
A Unified Model for Soft Linguistic Reordering Constraints
in Statistical Machine Translation
</title>
<author confidence="0.997398">
Junhui Li† Yuval Marton‡ Philip Resnik† Hal Daum´e III†
</author>
<affiliation confidence="0.997853">
†UMIACS, University of Maryland, College Park, MD
</affiliation>
<email confidence="0.919812">
{lijunhui, resnik, hal}@umiacs.umd.edu
</email>
<affiliation confidence="0.71739">
‡Microsoft Corp., City Center Plaza, Bellevue, WA
</affiliation>
<email confidence="0.992851">
yumarton@microsoft.com
</email>
<sectionHeader confidence="0.993779" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998715">
This paper explores a simple and effec-
tive unified framework for incorporating
soft linguistic reordering constraints into a
hierarchical phrase-based translation sys-
tem: 1) a syntactic reordering model
that explores reorderings for context free
grammar rules; and 2) a semantic re-
ordering model that focuses on the re-
ordering of predicate-argument structures.
We develop novel features based on both
models and use them as soft constraints
to guide the translation process. Ex-
periments on Chinese-English translation
show that the reordering approach can sig-
nificantly improve a state-of-the-art hier-
archical phrase-based translation system.
However, the gain achieved by the seman-
tic reordering model is limited in the pres-
ence of the syntactic reordering model,
and we therefore provide a detailed analy-
sis of the behavior differences between the
two.
</bodyText>
<sectionHeader confidence="0.999126" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999750767857143">
Reordering models in statistical machine transla-
tion (SMT) model the word order difference when
translating from one language to another. The
popular distortion or lexicalized reordering mod-
els in phrase-based SMT make good local pre-
dictions by focusing on reordering on word level,
while the synchronous context free grammars in
hierarchical phrase-based (HPB) translation mod-
els are capable of handling non-local reordering
on the translation phrase level. However, reorder-
ing, especially without any help of external knowl-
edge, remains a great challenge because an ac-
curate reordering is usually beyond these word
level or translation phrase level reordering mod-
els’ ability. In addition, often these translation
models fail to respect linguistically-motivated syn-
tax and semantics. As a result, they tend to pro-
duce translations containing both syntactic and se-
mantic reordering confusions. In this paper our
goal is to take advantage of syntactic and seman-
tic parsing to improve translation quality. Rather
than introducing reordering models on either the
word level or the translation phrase level, we pro-
pose a unified approach to modeling reordering on
the linguistic unit level, e.g., syntactic constituents
and semantic roles. The reordering unit falls into
multiple granularities, from single words to more
complex constituents and semantic roles, and of-
ten crosses translation phrases. To show the ef-
fectiveness of our reordering models, we integrate
both syntactic constituent reordering models and
semantic role reordering models into a state-of-
the-art HPB system (Chiang, 2007; Dyer et al.,
2010). We further contrast it with a stronger base-
line, already including fine-grained soft syntac-
tic constraint features (Marton and Resnik, 2008;
Chiang et al., 2008). The general ideas, however,
are applicable to other translation models, e.g.,
phrase-based model, as well.
Our syntactic constituent reordering model con-
siders context free grammar (CFG) rules in the
source language and predicts the reordering of
their elements on the target side, using word align-
ment information. Due to the fact that a con-
stituent, especially a long one, usually maps into
multiple discontinuous blocks in the target lan-
guage, there is more than one way to describe the
monotonicity or swapping patterns; we therefore
design two reordering models: one is based on the
leftmost aligned target word and the other based
on the rightmost target word.
While recently there has also been some encour-
aging work on incorporating semantic structure
(or, more specifically, predicate-argument struc-
ture: PAS) reordering in SMT, it is still an open
question whether semantic structure reordering
</bodyText>
<page confidence="0.796667">
1123
</page>
<note confidence="0.83121">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1123–1133,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.996487888888889">
strongly overlaps with syntactic structure reorder-
ing, since the semantic structure is closely tied to
syntax. To this end, we employ the same reorder-
ing framework as syntactic constituent reordering
and focus on semantic roles in a PAS. We then an-
alyze the differences between the syntactic and se-
mantic features.
The contributions of this paper include the fol-
lowing:
</bodyText>
<listItem confidence="0.8940335">
• We introduce novel soft reordering con-
straints, using syntactic constituents or se-
mantic roles, composed over word alignment
information in translation rules used during
decoding time;
• We introduce a unified framework to incor-
porate syntactic and semantic reordering con-
straints;
• We provide a detailed analysis providing in-
sight into why the semantic reordering model
is significantly less effective when syntactic
reordering features are also present.
</listItem>
<bodyText confidence="0.999790666666667">
The rest of the paper is organized as follows.
Section 2 provides an overview of HPB transla-
tion model. Section 3 describes the details of our
unified reordering models. Section 4 gives our ex-
perimental results and Section 5 discusses the be-
havior difference between syntactic constituent re-
ordering and semantic role reordering. Section 6
reviews related work and, finally Section 7 con-
cludes the paper.
</bodyText>
<sectionHeader confidence="0.884796" genericHeader="method">
2 HPB Translation Model: an Overview
</sectionHeader>
<bodyText confidence="0.99987725">
In HPB models (Chiang, 2007), synchronous rules
take the form X —* (&apos;y, α, —), where X is the non-
terminal symbol, &apos;y and α are strings of lexical
items and non-terminals in the source and target
side, respectively, and — indicates the one-to-one
correspondence between non-terminals in &apos;y and α.
Each such rule is associated with a set of transla-
tion model features {OZ}, such as phrase transla-
tion probability p (α  |&apos;y) and its inverse p (&apos;y  |α),
the lexical translation probability plea (α  |&apos;y) and
its inverse plea (&apos;y  |α), and a rule penalty that af-
fects preference for longer or shorter derivations.
Two other widely used features are a target lan-
guage model feature and a target word penalty.
Given a derivation d, its translation log-
probability is estimated as:
</bodyText>
<equation confidence="0.939452333333333">
log P (d) ∝ � Aiφi (d) (1)
i
PAS
</equation>
<figure confidence="0.41605">
A0 TMP Pre A1
(NP) (NP) (VBD) (NP)
Applicants yesterday filled the forms
</figure>
<figureCaption confidence="0.9866515">
Figure 1: Example of predicate-argument struc-
ture.
</figureCaption>
<bodyText confidence="0.99783">
where AZ is the corresponding weight of feature OZ.
See (Chiang, 2007) for more details.
</bodyText>
<sectionHeader confidence="0.980453" genericHeader="method">
3 Unified Linguistic Reordering Models
</sectionHeader>
<bodyText confidence="0.999924945945946">
As mentioned earlier, the linguistic reordering unit
is the syntactic constituent for syntactic reorder-
ing, and the semantic role for semantic reordering.
The syntactic reordering model takes a CFG rule
(e.g., VP —* VP PP PP) and models the reorder-
ing of the constituents on the left hand side by ex-
amining their translation or visit order according
to the target language. For the semantic reorder-
ing model, it takes a PAS and models its reorder-
ing on the target side. Figure 1 shows an example
of a PAS where the predicate (Pre) has two core
arguments (A0 and A1) and one adjunct (TMP).
Note that we refer all core arguments, adjuncts,
and predicates as semantic roles; thus we say the
PAS in Figure 1 has 4 roles. According to the an-
notation principles in (Chinese) PropBank (Palmer
et al., 2005; Xue and Palmer, 2009), all the roles
in a PAS map to a corresponding constituent in the
parse tree, and these constituents (e.g., NPs and
VBD in Figure 1) do not overlap with each other.
Next, we use a CFG rule to describe our syn-
tactic reordering model. Treating the two forms
of reorderings in a unified way, the semantic re-
ordering model is obtainable by regarding a PAS
as a CFG rule and considering a semantic role as a
constituent.
Because the translation of a source constituent
might result in multiple discontinuous blocks,
there can be several ways to describe or group
the reordering patterns. Therefore, we design
two general constituent reordering sub-models.
One is based on the leftmost aligned word (left-
most reordering model) and the other is based on
the rightmost aligned word (rightmost reordering
model), as follows. Figure 2 shows the model-
ing steps for the leftmost reordering model. Fig-
ure 2(a) is an example of a CFG rule in the source
</bodyText>
<page confidence="0.923906">
1124
</page>
<figure confidence="0.865137416666666">
XP
XP1 XP2 XP3 XP4
f3 f4 f5 f6 f7 f8
XP1 XP2 XP3 XP4
...
...
...
...
... e2 e3 e4 e5 e6 e7 e8 e9 ... e2 e3 e5
(a) a CFG rule and its alignment (b) leftmost aligned target words
XP1 XP2 XP3 XP4 XP1 XP2 XP3 XP4
1 4 2 3 DM DS M
</figure>
<figureCaption confidence="0.850749">
(c) visit order (d) reordering types
Figure 2: Modeling process illustration for leftmost reordering model.
</figureCaption>
<bodyText confidence="0.570402636363636">
parse tree and its word alignment links to the target
language. Note that constituent XP4, which covers
word f8, has no alignment. Then for each XPi, we
find the leftmost target word which is aligned to a
source word covered by XPi. Figure 2(b) shows
that the leftmost target words for XP1, XP2, and
XP3 are e2, e5, and e3, respectively, while XP4
has no aligned target word. Then we get visit
order V = {vi} for {XPi} in the transformation
from Figure 2(b) to Figure 2(c), with the follow-
ing strategies for special cases:
</bodyText>
<listItem confidence="0.9899759">
• if the first constituent XP1 is unaligned, we
add a NULL word at the beginning of the tar-
get side and link XP1 to the NULL word;
• if a constituent XPi (i &gt; 1) is unaligned, we
add a link to the target word which is aligned
to XPi−1, e.g., XP4 will be linked to e3; and
• if k constituents XPm1 ... XPmk (m1 &lt;
... &lt; mk) are linked to the same target word,
then vmi = vmi+1 − 1, e.g., since XP3 and
XP4 are both linked to e3, then v3 = v4 − 1.
</listItem>
<bodyText confidence="0.992581333333333">
Finally Figure 2(d) converts the visit order V =
{v1, ... vn} into a sequence of leftmost reordering
types LRT = {lrt1, ... , lrtn−1}. For every two
adjacent constituents XPi and XPi+1 with corre-
sponding visit order vi and vi+1, their reordering
could be one of the following:
</bodyText>
<listItem confidence="0.99995375">
• Monotone (M) if vi+1 = vi + 1;
• Discontinuous Monotone (DM) if vi+1 &gt; vi + 1;
• Swap (S) if vi+1 = vi − 1;
• Discontinuous Swap (DS) if vi+1 &lt; vi − 1.
</listItem>
<bodyText confidence="0.741044363636364">
Up to this point, we have generated a se-
quence of leftmost reordering types LRT =
{lrt1,...,lrtn−1} for a given CFG rule cfg:
XP → XP1 ... XPn. The leftmost reordering
model takes the following form:
scorelrt (cfg) = Pl (lrt1, ... , lrtn−1  |0 (cfg))
(2)
where 0 (cfg) indicates the surrounding context of
the CFG. By assuming that any two reordering
types in LRT = {lrt1, ... , lrtn−1} are indepen-
dent of each other, we reformulate Eq. 2 into:
</bodyText>
<equation confidence="0.9691125">
scorelrt (cfg) = n−1� Pl (lrti  |0 (cfg)) (3)
i=1
</equation>
<bodyText confidence="0.986648857142857">
Similarly, the sequence of rightmost reordering
types RRT can be decided for a CFG rule XP →
XP1 ... XPn.
Accordingly, for a PAS pas: PAS → R1 ... Rn,
we can obtain its sequences of leftmost and right-
most reordering types by using the same way de-
scribed above.
</bodyText>
<subsectionHeader confidence="0.999619">
3.1 Probability Estimation
</subsectionHeader>
<bodyText confidence="0.9947908">
In order to predict either the leftmost or right-
most reordering type for two adjacent constituents,
we use a maximum entropy classifier to esti-
mate the probability of the reordering type rt E
{M, DM, S, DS} as follows:
</bodyText>
<equation confidence="0.982584">
P (rt  |0 (cfg)) =
</equation>
<bodyText confidence="0.8030384">
exp (Ek Bkfk (rt, 0 (cfg))) (4)
Ertl exp (Ek Bkfi (rt0, 0 (cfg)))
where fk are binary features, Bk are the weights of
these features. Most of our features fk are syntax-
based. For XPi and XPi+1 in cfg, the features
</bodyText>
<page confidence="0.987786">
1125
</page>
<table confidence="0.999438285714286">
#Index Feature
cf1 L(XPi) &amp; L(XPi+1) &amp; L(XP)
cf2
for each XPj (j &lt; i)
L(XPi) &amp; L(XPi+1) &amp; L(XP) &amp; L(XPj)
cf3 for each XPj (j &gt; i + 1) L(XPi) &amp; L(XPi+1) &amp; L(XP) &amp; L(XPj)
cf4 L(XPi) &amp; L(XPi+1) &amp; P(XPi)
cf5 L(XPi) &amp; L(XPi+1) &amp; H(XPi)
cf6 L(XPi) &amp; L(XPi+1) &amp; P(XPi+1)
cf7 L(XPi) &amp; L(XPi+1) &amp; H(XPi+1)
cf8 L(XPi) &amp; L(XPi+1) &amp; S(XPi)
cf9 L(XPi) &amp; L(XPi+1) &amp; S(XPi+1)
cf10 L(XPi) &amp; L(XP)
cf11 L(XPi+1) &amp; L(XP)
</table>
<tableCaption confidence="0.9983">
Table 1: Features adopted in the syntactic leftmost
</tableCaption>
<bodyText confidence="0.989794333333333">
and rightmost reordering models. G (XP) returns
the syntactic category of XP, e.g., NP, VP, PP etc.;
x (XP) returns the head word of XP; P (XP) re-
turns the POS tagger of the head word; S (XP)
returns the translation status of XP on the target
language: un. if it is untranslated; cont. if it is
a continuous block; and discont. if it maps into
multiple discontinuous blocks.
are aimed to examine which of them should be
translated first. Therefore, most features share two
common components: the syntactic categories of
XPi and XPi+1. Table 1 shows the features used in
syntactic leftmost and rightmost reordering mod-
els. Note that we use the same features for both.
Although the semantic reordering model is
structured in precisely the same way, we use dif-
ferent feature sets to predict the reordering be-
tween two semantic roles. Given the two adjacent
roles Ri and Ri+1 in a PAS pas, Table 2 shows the
features that are used in the semantic leftmost and
rightmost reordering models.
</bodyText>
<subsectionHeader confidence="0.998196">
3.2 Integrating into the HPB Model
</subsectionHeader>
<bodyText confidence="0.999922733333333">
For models with syntactic reordering, we add two
new features (i.e., one for the leftmost reorder-
ing model and the other for the rightmost reorder-
ing model) into the log-linear translation model in
Eq. 1. Unlike the conventional phrase and lexi-
cal translation features, whose values are phrase
pair-determined and thus can be calculated offline,
the value of the reordering features can only be
obtained during decoding time, and requires word
alignment information as well. Before we present
the algorithm integrating the reordering models,
we define the following functions by assuming
XPi and XPi+1 are the constituent pair of interest
in CFG rule cfg, H is the translation hypothesis
and a is its word alignment:
</bodyText>
<table confidence="0.813078285714286">
#Index Feature
rf1 R(Ri) &amp; R(Ri+1) &amp; P(pas)
R(Ri) &amp; R(Ri+1)
rf2 for each Rj (j &lt; i)
R(Ri) &amp; R(Ri+1) &amp; R(Rj) &amp; P(pas)
R(Ri) &amp; R(Ri+1) &amp; R(Rj)
rf3 for each Rj (j &gt; i + 1)
R(Ri) &amp; R(Ri+1) &amp; R(Rj) &amp; P(pas)
R(Ri) &amp; R(Ri+1) &amp; R(Rj)
rf4 R(Ri) &amp; R(Ri+1) &amp; P(Ri)
rf5 R(Ri) &amp; R(Ri+1) &amp; H(Ri)
rf6 R(Ri) &amp; R(Ri+1) &amp; L(Ri)
rf7 R(Ri) &amp; R(Ri+1) &amp; P(Ri+1)
rf8 R(Ri) &amp; R(Ri+1) &amp; H(Ri+1)
rf9 R(Ri) &amp; R(Ri+1) &amp; L(Ri+1)
rf10 R(Ri) &amp; R(Ri+1) &amp; S(Ri)
rf11 R(Ri) &amp; R(Ri+1) &amp; S(Ri+1)
rf12 R(Ri) &amp; P(pas)
R(Ri)
rf13 R(Ri+1) &amp; P(pas)
R(Ri+1)
</table>
<tableCaption confidence="0.907544">
Table 2: Features adopted in the semantic leftmost
</tableCaption>
<bodyText confidence="0.992527666666667">
and rightmost reordering models. P (pas) returns
the predicate content of pas; R (R) returns the role
type of R, e.g., Pred, A0, TMP, etc. For features
rf1, rf2, rf3, rf12 and rf13, we include another ver-
sion which excludes the predicate content P(pas)
for reasons of sparsity.
</bodyText>
<listItem confidence="0.962113">
• F1 (w1, w2, XP): returns true if constituent XP is
within the span from word w1 to w2; otherwise returns
false.
• F2 (H, cfg, XPi, XPi+1) returns true if the reordering
of the pair hXPi, XPi+1i in rule cfg has not been calcu-
lated yet; otherwise returns false.
• F3 (H, a, XPi, XPi+1) returns the leftmost and right-
most reordering types for the constituent pair hXPi,
XPi+1i, given alignment a, according to Section 3.
• F4 (rt, cfg, XPi, XPi+1) returns the probability of
leftmost reordering type rt for the constituent pair
hXPi, XPi+1i in rule cfg.
• F5 (rt, cfg, XPi, XPi+1) returns the probability of
rightmost reordering type rt for the constituent pair
hXPi, XPi+1i in rule cfg.
</listItem>
<bodyText confidence="0.999589272727273">
Algorithm 1 integrates the syntactic leftmost
and rightmost reordering models into a CKY-style
decoder whenever a new hypothesis is generated.
Given a hypothesis H with its alignment a, it tra-
verses all CFG rules in the parse tree and sees if
two adjacent constituents are conditioned to trig-
ger the reordering models (lines 2-4). For each
pair of constituents, it first extracts its leftmost and
rightmost reordering types (line 6) and then gets
their respective probabilities returned by the max-
imum entropy classifiers defined in Section 3.1
</bodyText>
<page confidence="0.985673">
1126
</page>
<construct confidence="0.639159333333333">
Algorithm 1: Integrating the syntactic reordering models
into a CKY-style decoder
Input: Sentence f in the source language
</construct>
<figure confidence="0.624290411764706">
Parse tree t of f
All CFG rules {cfg} in t
Hypothesis H spanning from word w1 to w2
Alignment a of H
Output: Log-Probabilities of the syntactic leftmost
and rightmost reordering models
1. set l prob = rprob = 0.0
2. foreach cfg in {cfg}
3. foreach pair XPi and XPi+1 in cfg
4. if F1 (w1, w2, XPi) =false or
F1 (w1, w2, XPi+1) =false or
F2 (H, cfg, XPi, XPi+1) =false
5. continue
6. (l type, r type) = F3 (H, a, XPi, XPi+1)
7. l prob += log F4 (l type, cfg, XPi, XPi+1)
8. r prob += log F5 (r type, cfg, XPi, XPi+1)
9. return (l prob, r prob)
</figure>
<bodyText confidence="0.997453666666667">
(lines 7-8). Then the algorithm returns two log-
probabilities of the syntactic reordering models.
Note that Function F1 returns true if hypothesis
H fully covers, or fully contains, constituent XPi,
regardless of the reordering type of XPi. Do not
confuse any parsing tag XPi with the nameless
variables Xi in Hiero or cdec rules.
For the semantic reordering models, we also
add two new features into the log-linear transla-
tion model. To get the two semantic reordering
model feature values, we simply use Algorithm 1
and its associated functions from F1 to F5 replac-
ing a CFG rule cfg with a PAS pas, and a con-
stituent XPi with a semantic role Ri. Algorithm 1
therefore permits a unified treatment of syntactic
and PAS-based reordering, even though it is ex-
pressed in terms of syntactic reordering here for
ease of presentation.
</bodyText>
<sectionHeader confidence="0.999743" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9997374">
We have presented our unified approach to in-
corporating syntactic and semantic soft reorder-
ing constraints in an HPB system. In this section,
we test its effectiveness in Chinese-English trans-
lation.
</bodyText>
<subsectionHeader confidence="0.970248">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999979108695653">
For training we use 1.6M sentence pairs of the
non-UN and non-HK Hansards portions of NIST
MT training corpora, segmented with the Stan-
ford segmenter (Tseng et al., 2005). The En-
glish data is lowercased, tokenized and aligned
with GIZA++ (Och and Ney, 2000) to obtain bidi-
rectional alignments, which are symmetrized us-
ing the grow-diag-final-and method (Koehn et al.,
2003). We train a 4-gram LM on the English
side of the corpus with 600M additional words
from non-NYT and non-LAT, randomly selected
portions of the Gigaword v4 corpus, using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996). We use the HPB decoder cdec (Dyer et
al., 2010), with Mr. Mira (Eidelman et al., 2013),
which is a k-best variant of MIRA (Chiang et al.,
2008), to tune the parameters of the system.
We use NIST MT 06 dataset (1664 sentence
pairs) for tuning, and NIST MT 03, 05, and 08
datasets (919, 1082, and 1357 sentence pairs, re-
spectively) for evaluation.1 We use BLEU (Pap-
ineni et al., 2002) for both tuning and evaluation.
To obtain syntactic parse trees and semantic
roles on the tuning and test datasets, we first
parse the source sentences with the Berkeley
Parser (Petrov and Klein, 2007), trained on the
Chinese Treebank 7.0 (Xue et al., 2005). We
then pass the parses to a Chinese semantic role
labeler (Li et al., 2010), trained on the Chinese
PropBank 3.0 (Xue and Palmer, 2009), to anno-
tate semantic roles for all verbal predicates (part-
of-speech tag VV, VE, or VC).
Our basic baseline system employs 19 basic
features: a language model feature, 7 transla-
tion model features, word penalty, unknown word
penalty, the glue rule, date, number and 6 pass-
through features. Our stronger baseline employs,
in addition, the fine-grained syntactic soft con-
straint features of Marton and Resnik (2008), here-
after MR08. The syntactic soft constraint features
include both MR08 exact-matching and cross-
boundary constraints (denoted XP= and XP+).
Since the syntactic parses of the tuning and test
data contain 29 types of constituent labels and 35
types of POS tags, we have 29 types of XP+ fea-
tures and 64 types of XP= features.
</bodyText>
<subsectionHeader confidence="0.99775">
4.2 Model Training
</subsectionHeader>
<bodyText confidence="0.9995355">
To train the syntactic and semantic reordering
models, we use a gold alignment dataset.2 It con-
tains 7,870 sentences with 191,364 Chinese words
and 261,399 English words. We first run syn-
</bodyText>
<footnote confidence="0.871977">
1http://www.itl.nist.gov/iad/mig//tests/mt
2This dataset includes LDC2006E86, and newswire
parts of LDC2012T16, LDC2012T20, LDC2012T24, and
LDC2013T05. Indeed, the reordering models can also be
trained on the MT training data with its automatic alignment.
However, our preliminary experiments showed that the re-
ordering models trained on gold alignment yielded higher im-
provement.
</footnote>
<page confidence="0.91085">
1127
</page>
<table confidence="0.99984525">
Reordering Syntactic Semantic
Type
l-m r-m l-m r-m
M 73.5 80.6 63.8 67.9
DM 3.9 3.3 14.0 12.0
S 19.5 13.2 13.1 10.7
DS 3.2 3.0 9.1 9.5
#instance 199,234 66,757
</table>
<tableCaption confidence="0.936700333333333">
Table 3: Reordering type distribution over the re-
ordering model’s training data. Hereafter, l-m and
r-m are for leftmost and rightmost, respectively.
</tableCaption>
<bodyText confidence="0.999227763157895">
tactic parsing and semantic role labeling on the
Chinese sentences, then train the models by us-
ing MaxEnt toolkit with L1 regularizer (Tsuruoka
et al., 2009).3 Table 3 shows the reordering type
distribution over the training data. Interestingly,
about 17% of the syntactic instances and 16% of
the semantic instances differ in their leftmost and
rightmost reordering types, indicating that the left-
most/rightmost distinction is informative. We also
see that the number of semantic instances is about
1/3 of that of syntactic instances, but the entropy
of the semantic reordering classes is higher, indi-
cating the reordering of semantic roles is harder
than that of syntactic constituents.
A deeper examination of the reordering model’s
training data reveals that some constituent pairs
and semantic role pairs have a preference for a
specific reordering type (monotone or swap). In
order to understand how well the MR08 system
respects their reordering preference, we use the
gold alignment dataset LDC2006E86, in which
the source sentences are from the Chinese Tree-
bank, and thus both the gold parse trees and gold
predicate-argument structures are available. Ta-
ble 4 presents examples comparing the reordering
distribution between gold alignment and the out-
put of the MR08 system. For example, the first
row shows that based on the gold alignment, for
(PP,VP), 16% are in monotone and 76% are in
swap reordering. However, our MR08 system out-
puts 46% of them in monotone and and 50% in
swap reordering. Hence, the reordering accuracy
for (PP,VP) is 54%. Table 4 also shows that the
semantic reordering between core arguments and
predicates (e.g., (Pred, A1), (A0, Pred)) has a less
ambiguous pattern than that between adjuncts and
other roles (e.g., (LOC,Pred), (A0,TMP)), indicat-
ing the higher reordering flexibility of adjuncts.
</bodyText>
<footnote confidence="0.943588">
3http://www.logos.ic.i.u-tokyo.ac.jp/∼tsuruoka/maxent/
</footnote>
<table confidence="0.999786277777778">
Const. Pair Gold MR08 output
M S M S acc.
PP VP 16 76 46 50 54
NP LC 26 74 58 42 50
DNP NP 24 72 78 19 39
CP NP 26 67 84 10 33
NP DEG 39 61 31 69 66
... ... ...
all 81 13 79 14 80
Role Pair Gold MR08 output
M S M S acc.
Pred A1 84 6 82 9 72
A0 Pred 82 11 79 8 75
LOC Pred 17 30 36 25 49
A0 TMP 35 25 61 6 45
TMP Pred 30 22 49 19 43
... ... ...
all 63 13 73 9 64
</table>
<tableCaption confidence="0.81963825">
Table 4: Examples of the reordering distribution
(%) of gold alignment and the MR08 system out-
put. For simplicity, we only focus on (M)onotone
and (S)wap based on leftmost reordering.
</tableCaption>
<subsectionHeader confidence="0.988731">
4.3 Translation Experiment Results
</subsectionHeader>
<bodyText confidence="0.94597275">
Our first group of experiments investigates
whether the syntactic reordering models are able
to improve translation quality in terms of BLEU.
To this end, we respectively add our syntactic re-
ordering models into both the baseline and MR08
systems. The effect is shown in the rows of “+ syn-
reorder” in Table 5. From the table, we have the
following two observations.
• Although the HPB model is capable of
handling non-local phrase reordering using
synchronous context free grammars, both
our syntactic leftmost reordering model and
rightmost model are still able to achieve im-
provement over both the baseline and MR08.
This suggests that our syntactic reordering
features interact well with the MR08 syntac-
tic soft constraints: the XP+ and XP= fea-
tures focus on a single constituent each, while
our reordering features focus on a pair of con-
stituents each.
</bodyText>
<listItem confidence="0.724094166666667">
• There is no clear indication of whether the
leftmost reordering model works better than
the other. In addition, integrating both the
leftmost and rightmost reordering models has
limited improvement over a single reordering
model.
</listItem>
<bodyText confidence="0.9930725">
Our second group of experiments is to vali-
date the semantic reordering models. Results are
</bodyText>
<page confidence="0.981584">
1128
</page>
<table confidence="0.999729666666667">
System Tuning Test
MT06 MT03 MT05 MT08 Avg.
Baseline 34.1 36.1 32.3 27.4 31.9
+ l-m 35.2 36.9$ 33.6$ 28.4$ 33.0
syn- r-m 35.2 37.2$ 33.7$ 28.6$ 33.2
reorder both 35.6 37.1$ 33.6$ 28.8$ 33.1
+ l-m 34.4 36.7$ 33.0$ 27.81 32.5
sem- r-m 34.5 36.7$ 33.1$ 27.8$ 32.5
reorder both 34.5 37.0$ 33.6$ 27.71 32.8
+syn+sem 35.5 37.3$ 33.7$ 29.0$ 33.3
MR08 35.6 37.4 34.2 28.7 33.4
+ l-m 36.0 38.2$ 35.0$ 29.2$ 34.1
syn- r-m 36.0 38.1$ 34.8$ 29.2$ 34.0
reorder both 35.9 38.2$ 35.3$ 29.5$ 34.3
+ l-m 35.8 37.61 34.7$ 28.7 33.7
sem- r-m 35.8 37.4 34.51 28.8 33.6
reorder both 35.8 37.61 34.7$ 28.8 33.7
+syn+sem 36.1 38.4$ 35.2$ 29.5$ 34.4
</table>
<tableCaption confidence="0.994438">
Table 5: System performance in BLEU scores.
</tableCaption>
<bodyText confidence="0.8106098">
‡/†: significant over baseline or MR08 at 0.01
/ 0.05, respectively, as tested by bootstrap re-
sampling (Koehn, 2004)
shown in the rows of “+ sem-reorder” in Table 5.
Here we observe:
</bodyText>
<listItem confidence="0.985265222222222">
• The semantic reordering models also achieve
significant gain of 0.8 BLEU on average over
the baseline system, demonstrating the ef-
fectiveness of PAS-based reordering. How-
ever, the gain diminishes to 0.3 BLEU on the
MR08 system.
• The syntactic reordering models outperform
the semantic reordering models on both the
baseline and MR08 systems.
</listItem>
<bodyText confidence="0.999832166666667">
Finally, we integrate both the syntactic and se-
mantic reordering models into the final system.
The two models collectively achieve a gain of up
to 1.4 BLEU over the baseline and 1.0 BLEU over
MR08 on average, which is shown in the rows of
“+syn+sem” in Table 5.
</bodyText>
<sectionHeader confidence="0.999299" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999154">
The trend of the results, summarized as perfor-
mance gain over the baseline and MR08 systems
averaged over all test sets, is presented in Table 6.
The syntactic reordering models outperform the
semantic reordering models, and the gain achieved
by the semantic reordering models is limited in the
presence of the MR08 syntactic features. In this
section, we look at MR08 system and the systems
improving it to explore the behavior differences
between the two reordering models.
Coverage analysis: Our statistics show that
syntactic reordering features (either leftmost or
Table 6: Performance gain in BLEU over baseline
and MR08 systems averaged over all test sets.
rightmost) are called 24 times per sentence on av-
erage. This is compared to only 9 times per sen-
tence for semantic reordering features. This is not
surprising since the semantic reordering features
are exclusively attached to predicates, and the span
set of the semantic roles is a strict subset of the
span set of the syntactic constituents; only 22% of
syntactic constituents are semantic roles. On aver-
age, a sentences has 4 PASs and each PAS contains
3 semantic roles. Of all the semantic role pairs,
44% are in the same CFG rules, indicating that this
part of semantic reordering has overlap with syn-
tactic reordering. Therefore, the PAS model has
fewer opportunities to influence reordering.
Reordering accuracy analysis: The reordering
type distribution on the reordering model training
data in Table 3 suggests that semantic reordering
is more difficult than syntactic reordering. To val-
idate this conjecture on our translation test data,
we compare the reordering performance among
the MR08 system, the improved systems and the
maximum entropy classifiers. For the test set, we
have four reference translations. We run GIZA++
on the data combination of our translation train-
ing data and test data to get the alignment for the
test data and each reference translation. Once we
have the (semi-)gold alignment, we compute the
gold reordering types between two adjacent syn-
tactic constituents or semantic roles. Then we
evaluate the automatic reordering outputs gener-
ated from both our translation systems and max-
imum entropy classifiers. Table 7 shows the ac-
curacy averaged over the four gold reordering sets
(the four reference translations). It shows that 1)
as expected, our classifiers do worse on the harder
semantic reordering prediction than syntactic re-
ordering prediction; 2) thanks to the high accu-
racy obtained by the maxent classifiers, integrat-
ing either the syntactic or the semantic reorder-
ing constraints results in better reordering perfor-
mance from both syntactic and semantic perspec-
tives; 3) in terms of the mutual impact, the syn-
tactic reordering models help improving seman-
tic reordering more than the semantic reordering
</bodyText>
<figure confidence="0.95553475">
System
Baseline MR08
+syn-reorder
+sem-reorder
1.2 0.9
0.8 0.3
+ both
1.4 1.0
</figure>
<page confidence="0.944682">
1129
</page>
<table confidence="0.999606857142857">
System Syntactic Semantic
l-m r-m l-m r-m
MR08 75.0 78.0 66.3 68.5
+syn-reorder 78.4 80.9 69.0 70.2
+sem-reorder 76.0 78.8 70.7 72.7
+both 78.6 81.7 70.6 72.1
Maxent Classifier 80.7 85.6 70.9 73.5
</table>
<tableCaption confidence="0.987423">
Table 7: Reordering accuracy on four gold sets.
</tableCaption>
<table confidence="0.9997392">
System Syntactic Semantic
l-m r-m l-m r-m
+syn-reorder 1.2 1.2 - -
+sem-reorder - - 0.7 0.9
+both 1.2 1.0 0.5 0.4
</table>
<tableCaption confidence="0.999706">
Table 8: Reordering feature weights.
</tableCaption>
<bodyText confidence="0.9998276">
models help improving syntactic reordering; and
4) the rightmost models have a learnability advan-
tage over the leftmost models, achieving higher
accuracy across the board.
Feature weight analysis: Table 8 shows the
syntactic and semantic reordering feature weights.
It shows that the semantic feature weights de-
crease in the presence of the syntactic features, in-
dicating that the decoder learns to trust semantic
features less in the presence of the more accurate
syntactic features. This is consistent with our ob-
servation that semantic reordering is harder than
syntactic reordering, as seen in Tables 3 and 7.
Potential improvement analysis: Table 7 also
shows that our current maximum entropy classi-
fiers have room for improvement, especially for
semantic reordering. In order to explore the error
propagation from the classifiers themselves and
explore the upper bound for improvement from the
reordering models, we perform an “oracle” study,
letting the classifiers be aware of the “gold” re-
ordering type between two syntactic constituents
or two semantic roles, and returning a higher prob-
ability for the gold reordering type and a smaller
one for the others (i.e., we set 0.9 for the gold
</bodyText>
<table confidence="0.998582384615385">
System MT 03 MT 05 MT 08 Avg.
Non- MR08 37.4 34.2 28.7 33.4
Oracle
+syn- 38.2 35.3 29.5 34.3
reorder 37.6 34.7 28.8 33.7
+sem-
reorder
+ both 38.4 35.2 29.5 34.4
Oracle +syn- 39.2 35.9 29.6 34.9
reorder 37.9 34.8 28.9 33.9
+sem-
reorder
+ both 39.1 36.0 29.8 35.0
</table>
<tableCaption confidence="0.832025">
Table 9: Performance (BLEU score) comparison
between non-oracle and oracle experiments.
</tableCaption>
<bodyText confidence="0.999956476190476">
reordering type, and let the other non-gold three
types share 0.1). Again, to get the gold reorder-
ing type, we run GIZA++ to get the alignment for
tuning/test source sentences and each of four ref-
erence translations. We report the averaged per-
formance by using the gold reordering type ex-
tracted from the four reference translations. Ta-
ble 9 compares the performance between the non-
oracle and oracle settings. We clearly see that us-
ing gold syntactic reordering types significantly
improves the performance (e.g., 34.9 vs. 33.4 on
average) and there is still some room for improve-
ment by building a better maximum entropy clas-
sifiers (e.g., 34.9 vs. 34.3). To our surprise, how-
ever, the improvement achieved by gold semantic
reordering types is still small (e.g., 33.9 vs. 33.4),
suggesting that the potential improvement of se-
mantic reordering models is much more limited.
And we again see that the improvement achieved
by semantic reordering models is limited in the
presence of the syntactic reordering models.
</bodyText>
<sectionHeader confidence="0.99988" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999806428571429">
Syntax-based reordering: Some previous work
pre-ordered words in the source sentences, so that
the word order of source and target sentences is
similar. The reordering rules were either manu-
ally designed (Collins et al., 2005; Wang et al.,
2007; Xu et al., 2009; Lee et al., 2010) or auto-
matically learned (Xia and McCord, 2004; Gen-
zel, 2010; Visweswariah et al., 2010; Khalilov
and Sima’an, 2011; Lerner and Petrov, 2013), us-
ing syntactic parses. Li et al. (2007) focused on
finding the n-best pre-ordered source sentences by
predicting the reordering of sibling constituents,
while Yang et al. (2012) obtained word order by
using a reranking approach to reposition nodes in
syntactic parse trees. Both are close to our work;
however, our model generates reordering features
that are integrated into the log-linear translation
model during decoding.
Another approach in previous work added soft
constraints as weighted features in the SMT de-
coder to reward good reorderings and penalize bad
ones. Marton and Resnik (2008) employed soft
syntactic constraints with weighted binary features
and no MaxEnt model. They did not explicitly
target reordering (beyond applying constraints on
HPB rules). Although employing linguistically
motivated labels in SCFG is capable of captur-
ing constituent reorderings (Chiang, 2010; Mylon-
</bodyText>
<page confidence="0.979305">
1130
</page>
<bodyText confidence="0.99976525">
akis and Sima’an, 2011), the rules are sparser than
SCFG with nameless non-terminals (i.e., Xs) and
soft constraints. Ge (2010) presented a syntax-
driven maximum entropy reordering model that
predicted the source word translation order. Gao
et al. (2011) employed dependency trees to predict
the translation order of a word and its head word.
Huang et al. (2013) predicted the translation order
of two source words.4 Our work, which shares this
approach, differs from their work primarily in that
our syntactic reordering models are based on the
constituent level, rather than the word level.
</bodyText>
<subsectionHeader confidence="0.451157">
Semantics-based reordering: Semantics-
</subsectionHeader>
<bodyText confidence="0.993759857142857">
based reordering has also seen an increase
in activity recently. In the pre-ordering ap-
proach, Wu et al. (2011) automatically learned
pre-ordering rules from PAS. In the soft con-
straint or reordering model approach, Liu and
Gildea (2010) modeled the reordering/deletion
of source-side semantic roles in a tree-to-string
translation model. Xiong et al. (2012) and Li et
al. (2013) predicted the translation order between
either two arguments or an argument and its
predicate. Instead of decomposing a PAS into
individual units, Zhai et al. (2013) constructed
a classifier for each source side PAS. Finally in
the post-processing approach category, Wu and
Fung (2009) performed semantic role labeling
on translation output and reordered arguments to
maximize the cross-lingual match of the semantic
frames between the source sentence and the target
translation. To our knowledge, their semantic
reordering models were PAS-specific. In contrast,
our model is universal and can be easily adopted
to model the reordering of other linguistic units
(e.g., syntactic constituents). Moreover, we
have studied the effectiveness of the semantic
reordering model in different scenarios.
Non-syntax-based reorderings in HPB: Re-
cently we have also seen work on lexicalized re-
ordering models without syntactic information in
HPB (Setiawan et al., 2009; Huck et al., 2013;
Nguyen and Vogel, 2013). The non-syntax-
based reordering approach models the reorder-
ing of translation words/phrases while the syntax-
based approach models the reordering of syn-
tactic constituents. Although there are overlaps
between translation phrases and syntactic con-
stituents, it is reasonable to think that the two re-
4Note that they obtained the translation order of source
word pairs by predicting the reordering of adjacent con-
stituents, which was quite close to our work.
ordering approaches can work together well and
even complement each other, as the linguistic pat-
terns they capture differ substantially. Setiawan
et al. (2013) modeled the orientation decisions
between anchors and two neighboring multi-unit
chunks which might cross phrase or rule bound-
aries. Last, we also note that recent work on non-
syntax-based reorderings in (flat) phrase-based
models (Cherry, 2013; Feng et al., 2013) can also
be potentially adopted to hpb models.
</bodyText>
<sectionHeader confidence="0.990334" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99998124">
In this paper, we have presented a unified reorder-
ing framework to incorporate soft linguistic con-
straints (of syntactic or semantic nature) into the
HPB translation model. The syntactic reordering
models take CFG rules and model their reordering
on the target side, while the semantic reordering
models work with PAS. Experiments on Chinese-
English translation show that the reordering ap-
proach can significantly improve a state-of-the-art
hierarchical phrase-based translation system. We
have also discussed the differences between the
two linguistic reordering models.
There are many directions in which this work
can be continued. First, the syntactic reordering
model can be extended to model reordering among
constituents that cross CFG rules. Second, al-
though we do not see obvious gain from the se-
mantic reordering model when the syntactic model
is adopted, it might be beneficial to further jointly
consider the two reordering models, focusing on
where each one does well. Third, to better exam-
ine the overlap or synergy between our approach
and the non-syntax-based reordering approach, we
will conduct direct comparisons and combinations
with the latter.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999736916666667">
This research was supported in part by the
BOLT program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0012-
12-C-0015. Any opinions, findings, conclusions
or recommendations expressed in this paper are
those of the authors and do not necessarily re-
flect the view of DARPA. The authors would like
to thank three anonymous reviewers for providing
helpful comments, and also acknowledge Ke Wu,
Vladimir Eidelman, Hua He, Doug Oard, Yuening
Hu, Jordan Boyd-Graber, and Jyothi Vinjumur for
useful discussions.
</bodyText>
<page confidence="0.991364">
1131
</page>
<sectionHeader confidence="0.989913" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999423911764706">
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of ACL 1996, pages 310–
318.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of HLT-NAACL 2013, pages 22–31.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008, pages 224–233.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of ACL 2010,
pages 1443–1452.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL 2005, pages
531–540.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL 2010 System Demonstra-
tions, pages 7–12.
Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip
Resnik, and Jimmy Lin. 2013. Mr. mira: Open-
source large-margin structured learning on mapre-
duce. In Proceedings of ACL 2013 System Demon-
strations, pages 199–204.
Minwei Feng, Jan-Thorsten Peter, and Hermann Ney.
2013. Advancements in reordering models for sta-
tistical machine translation. In Proceedings of ACL
2013, pages 322–332.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hier-
archical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857–868.
Niyu Ge. 2010. A direct syntax-driven reordering
model for phrase-based machine translation. In Pro-
ceedings of HLT-NAACL 2010, pages 849–857.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of COLING 2010, pages 376–
384.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In Proceedings of
EMNLP 2013, pages 556–566.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A phrase orientation model for
hierarchical machine translation. In Proceedings of
WMT 2013, pages 452–463.
Maxim Khalilov and Khalil Sima’an. 2011. Context-
sensitive syntactic source-reordering by statistical
transduction. In Proceedings of IJCNLP 2011,
pages 38–46.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 48–54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent reordering and syntax models for
English-to-Japanese statistical machine translation.
In Proceedings of COLING 2010, pages 626–634.
Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of EMNLP 2013, pages 513–523.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proceedings of ACL 2007,
pages 720–727.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of Chinese. In
Proceedings ofACL 2010, pages 1108–1117.
Junhui Li, Philip Resnik, and Hal Daum´e III. 2013.
Modeling syntactic and semantic structures in hier-
archical phrase-based translation. In Proceedings of
HLT-NAACL 2013, pages 540–549.
Ding Liu and Daniel Gildea. 2010. Semantic role
features for machine translation. In Proceedings of
COLING 2010, pages 716–724.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of ACL-HLT 2008, pages
1003–1011.
Markos Mylonakis and Khalil Sima’an. 2011. Learn-
ing hierarchical translation structure with linguistic
annotations. In Proceedings of ACL 2011, pages
642–652.
ThuyLinh Nguyen and Stephan Vogel. 2013. Integrat-
ing phrase-based reordering features into a chart-
based decoder for machine translation. In Proceed-
ings of ACL 2013, pages 1587–1596.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings ofACL
2000, pages 440–447.
</reference>
<page confidence="0.891499">
1132
</page>
<reference confidence="0.999939421052632">
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL 2002, pages 311–318.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL 2007, pages 404–411.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function
words in hierarchical phrase-based translation. In
Proceedings of ACL-IJCNLP 2009, pages 324–332.
Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin
Shen. 2013. Two-neighbor orientation model with
cross-boundary global contexts. In Proceedings of
ACL 2013, pages 1264–1274.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
168–171.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumula-
tive penalty. In Proceedings of ACL-IJCNLP 2009,
pages 477–485.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of COLING 2010, pages
1119–1127.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of EMNLP
2007, pages 737–745.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
HLT-NAACL 2009: short papers, pages 13–16.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proceedings of IJCNLP 2011, pages 29–
37.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of COLING 2004, pages
508–514.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of ACL 2012, pages 902–
911.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of HLT-NAACL 2009, pages 245–253.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural Lan-
guage Engineering, 15(1):143–172.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
ACL 2012, pages 912–920.
Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2013. Handling ambiguities of bilingual
predicate-argument structures for statistical machine
translation. In Proceedings of ACL 2013, pages
1127–1136.
</reference>
<page confidence="0.983481">
1133
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.474647">
<title confidence="0.9995435">A Unified Model for Soft Linguistic Reordering in Statistical Machine Translation</title>
<author confidence="0.999041">Yuval Philip Hal Daum´e</author>
<affiliation confidence="0.762805">University of Maryland, College Park, resnik,</affiliation>
<address confidence="0.950553">Corp., City Center Plaza, Bellevue,</address>
<email confidence="0.999906">yumarton@microsoft.com</email>
<abstract confidence="0.998429173913044">This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system: 1) a syntactic reordering model that explores reorderings for context free grammar rules; and 2) a semantic reordering model that focuses on the reordering of predicate-argument structures. We develop novel features based on both models and use them as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system. However, the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model, and we therefore provide a detailed analysis of the behavior differences between the two.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>310--318</pages>
<contexts>
<context position="17740" citStr="Chen and Goodman, 1996" startWordPosition="3047" endWordPosition="3050">4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on t</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of ACL 1996, pages 310– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Improved reordering for phrasebased translation using sparse features.</title>
<date>2013</date>
<booktitle>In Proceedings of HLT-NAACL 2013,</booktitle>
<pages>22--31</pages>
<contexts>
<context position="35352" citStr="Cherry, 2013" startWordPosition="5923" endWordPosition="5924">tituents, it is reasonable to think that the two re4Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule boundaries. Last, we also note that recent work on nonsyntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models. 7 Conclusion and Future Work In this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the target side, while the semantic reordering models work with PAS. Experiments on ChineseEnglish translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system. We have also disc</context>
</contexts>
<marker>Cherry, 2013</marker>
<rawString>Colin Cherry. 2013. Improved reordering for phrasebased translation using sparse features. In Proceedings of HLT-NAACL 2013, pages 22–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>224--233</pages>
<contexts>
<context position="2965" citStr="Chiang et al., 2008" startWordPosition="433" endWordPosition="436">ing on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-ofthe-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well. Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous blocks in the target language, there is more than one way to describe the monotonicity or swapping patterns; we therefore design two reordering models: one is based on the leftmost a</context>
<context position="17885" citStr="Chiang et al., 2008" startWordPosition="3075" endWordPosition="3078">with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese </context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of EMNLP 2008, pages 224–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2782" citStr="Chiang, 2007" startWordPosition="406" endWordPosition="407">rove translation quality. Rather than introducing reordering models on either the word level or the translation phrase level, we propose a unified approach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-ofthe-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well. Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous</context>
<context position="5374" citStr="Chiang, 2007" startWordPosition="807" endWordPosition="808"> providing insight into why the semantic reordering model is significantly less effective when syntactic reordering features are also present. The rest of the paper is organized as follows. Section 2 provides an overview of HPB translation model. Section 3 describes the details of our unified reordering models. Section 4 gives our experimental results and Section 5 discusses the behavior difference between syntactic constituent reordering and semantic role reordering. Section 6 reviews related work and, finally Section 7 concludes the paper. 2 HPB Translation Model: an Overview In HPB models (Chiang, 2007), synchronous rules take the form X —* (&apos;y, α, —), where X is the nonterminal symbol, &apos;y and α are strings of lexical items and non-terminals in the source and target side, respectively, and — indicates the one-to-one correspondence between non-terminals in &apos;y and α. Each such rule is associated with a set of translation model features {OZ}, such as phrase translation probability p (α |&apos;y) and its inverse p (&apos;y |α), the lexical translation probability plea (α |&apos;y) and its inverse plea (&apos;y |α), and a rule penalty that affects preference for longer or shorter derivations. Two other widely used f</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="32472" citStr="Chiang, 2010" startWordPosition="5486" endWordPosition="5487">are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word lev</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of ACL 2010, pages 1443–1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>531--540</pages>
<contexts>
<context position="31381" citStr="Collins et al., 2005" startWordPosition="5316" endWordPosition="5319">(e.g., 34.9 vs. 34.3). To our surprise, however, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translati</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL 2005, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="2802" citStr="Dyer et al., 2010" startWordPosition="408" endWordPosition="411">on quality. Rather than introducing reordering models on either the word level or the translation phrase level, we propose a unified approach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-ofthe-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well. Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous blocks in the targe</context>
<context position="17789" citStr="Dyer et al., 2010" startWordPosition="3057" endWordPosition="3060">tence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We th</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of ACL 2010 System Demonstrations, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Ke Wu</author>
<author>Ferhan Ture</author>
<author>Philip Resnik</author>
<author>Jimmy Lin</author>
</authors>
<title>Mr. mira: Opensource large-margin structured learning on mapreduce.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013 System Demonstrations,</booktitle>
<pages>199--204</pages>
<contexts>
<context position="17828" citStr="Eidelman et al., 2013" startWordPosition="3064" endWordPosition="3067">K Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semanti</context>
</contexts>
<marker>Eidelman, Wu, Ture, Resnik, Lin, 2013</marker>
<rawString>Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip Resnik, and Jimmy Lin. 2013. Mr. mira: Opensource large-margin structured learning on mapreduce. In Proceedings of ACL 2013 System Demonstrations, pages 199–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwei Feng</author>
<author>Jan-Thorsten Peter</author>
<author>Hermann Ney</author>
</authors>
<title>Advancements in reordering models for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013,</booktitle>
<pages>322--332</pages>
<contexts>
<context position="35372" citStr="Feng et al., 2013" startWordPosition="5925" endWordPosition="5928">s reasonable to think that the two re4Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule boundaries. Last, we also note that recent work on nonsyntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models. 7 Conclusion and Future Work In this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the target side, while the semantic reordering models work with PAS. Experiments on ChineseEnglish translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system. We have also discussed the difference</context>
</contexts>
<marker>Feng, Peter, Ney, 2013</marker>
<rawString>Minwei Feng, Jan-Thorsten Peter, and Hermann Ney. 2013. Advancements in reordering models for statistical machine translation. In Proceedings of ACL 2013, pages 322–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Gao</author>
<author>Philipp Koehn</author>
<author>Alexandra Birch</author>
</authors>
<title>Soft dependency constraints for reordering in hierarchical phrase-based translation.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP 2011,</booktitle>
<pages>857--868</pages>
<contexts>
<context position="32737" citStr="Gao et al. (2011)" startWordPosition="5525" endWordPosition="5528">eorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and </context>
</contexts>
<marker>Gao, Koehn, Birch, 2011</marker>
<rawString>Yang Gao, Philipp Koehn, and Alexandra Birch. 2011. Soft dependency constraints for reordering in hierarchical phrase-based translation. In Proceedings of EMNLP 2011, pages 857–868.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
</authors>
<title>A direct syntax-driven reordering model for phrase-based machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>849--857</pages>
<contexts>
<context position="32611" citStr="Ge (2010)" startWordPosition="5508" endWordPosition="5509">ing. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu </context>
</contexts>
<marker>Ge, 2010</marker>
<rawString>Niyu Ge. 2010. A direct syntax-driven reordering model for phrase-based machine translation. In Proceedings of HLT-NAACL 2010, pages 849–857.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically learning sourceside reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING 2010,</booktitle>
<pages>376--384</pages>
<contexts>
<context position="31497" citStr="Genzel, 2010" startWordPosition="5340" endWordPosition="5342">e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT d</context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically learning sourceside reordering rules for large scale machine translation. In Proceedings of COLING 2010, pages 376– 384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
</authors>
<title>Factored soft source syntactic constraints for hierarchical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP 2013,</booktitle>
<pages>556--566</pages>
<contexts>
<context position="32845" citStr="Huang et al. (2013)" startWordPosition="5544" endWordPosition="5547">ed binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation </context>
</contexts>
<marker>Huang, Devlin, Zbib, 2013</marker>
<rawString>Zhongqiang Huang, Jacob Devlin, and Rabih Zbib. 2013. Factored soft source syntactic constraints for hierarchical machine translation. In Proceedings of EMNLP 2013, pages 556–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Joern Wuebker</author>
<author>Felix Rietig</author>
<author>Hermann Ney</author>
</authors>
<title>A phrase orientation model for hierarchical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of WMT 2013,</booktitle>
<pages>452--463</pages>
<contexts>
<context position="34468" citStr="Huck et al., 2013" startWordPosition="5787" endWordPosition="5790">uments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture diff</context>
</contexts>
<marker>Huck, Wuebker, Rietig, Ney, 2013</marker>
<rawString>Matthias Huck, Joern Wuebker, Felix Rietig, and Hermann Ney. 2013. A phrase orientation model for hierarchical machine translation. In Proceedings of WMT 2013, pages 452–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxim Khalilov</author>
<author>Khalil Sima’an</author>
</authors>
<title>Contextsensitive syntactic source-reordering by statistical transduction.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP 2011,</booktitle>
<pages>38--46</pages>
<marker>Khalilov, Sima’an, 2011</marker>
<rawString>Maxim Khalilov and Khalil Sima’an. 2011. Contextsensitive syntactic source-reordering by statistical transduction. In Proceedings of IJCNLP 2011, pages 38–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>48--54</pages>
<contexts>
<context position="17516" citStr="Koehn et al., 2003" startWordPosition="3010" endWordPosition="3013">tion. 4 Experiments We have presented our unified approach to incorporating syntactic and semantic soft reordering constraints in an HPB system. In this section, we test its effectiveness in Chinese-English translation. 4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="24601" citStr="Koehn, 2004" startWordPosition="4215" endWordPosition="4216">3.6$ 28.8$ 33.1 + l-m 34.4 36.7$ 33.0$ 27.81 32.5 sem- r-m 34.5 36.7$ 33.1$ 27.8$ 32.5 reorder both 34.5 37.0$ 33.6$ 27.71 32.8 +syn+sem 35.5 37.3$ 33.7$ 29.0$ 33.3 MR08 35.6 37.4 34.2 28.7 33.4 + l-m 36.0 38.2$ 35.0$ 29.2$ 34.1 syn- r-m 36.0 38.1$ 34.8$ 29.2$ 34.0 reorder both 35.9 38.2$ 35.3$ 29.5$ 34.3 + l-m 35.8 37.61 34.7$ 28.7 33.7 sem- r-m 35.8 37.4 34.51 28.8 33.6 reorder both 35.8 37.61 34.7$ 28.8 33.7 +syn+sem 36.1 38.4$ 35.2$ 29.5$ 34.4 Table 5: System performance in BLEU scores. ‡/†: significant over baseline or MR08 at 0.01 / 0.05, respectively, as tested by bootstrap resampling (Koehn, 2004) shown in the rows of “+ sem-reorder” in Table 5. Here we observe: • The semantic reordering models also achieve significant gain of 0.8 BLEU on average over the baseline system, demonstrating the effectiveness of PAS-based reordering. However, the gain diminishes to 0.3 BLEU on the MR08 system. • The syntactic reordering models outperform the semantic reordering models on both the baseline and MR08 systems. Finally, we integrate both the syntactic and semantic reordering models into the final system. The two models collectively achieve a gain of up to 1.4 BLEU over the baseline and 1.0 BLEU o</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
<author>Bing Zhao</author>
<author>Xiaoqian Luo</author>
</authors>
<title>Constituent reordering and syntax models for English-to-Japanese statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>626--634</pages>
<contexts>
<context position="31436" citStr="Lee et al., 2010" startWordPosition="5328" endWordPosition="5331">vement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous </context>
</contexts>
<marker>Lee, Zhao, Luo, 2010</marker>
<rawString>Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010. Constituent reordering and syntax models for English-to-Japanese statistical machine translation. In Proceedings of COLING 2010, pages 626–634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Lerner</author>
<author>Slav Petrov</author>
</authors>
<title>Source-side classifier preordering for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP 2013,</booktitle>
<pages>513--523</pages>
<contexts>
<context position="31578" citStr="Lerner and Petrov, 2013" startWordPosition="5351" endWordPosition="5354">mantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008)</context>
</contexts>
<marker>Lerner, Petrov, 2013</marker>
<rawString>Uri Lerner and Slav Petrov. 2013. Source-side classifier preordering for machine translation. In Proceedings of EMNLP 2013, pages 513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Minghui Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Yi Guan</author>
</authors>
<title>A probabilistic approach to syntax-based reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>720--727</pages>
<contexts>
<context position="31620" citStr="Li et al. (2007)" startWordPosition="5359" endWordPosition="5362"> we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with </context>
</contexts>
<marker>Li, Li, Zhang, Li, Zhou, Guan, 2007</marker>
<rawString>Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. In Proceedings of ACL 2007, pages 720–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Guodong Zhou</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Joint syntactic and semantic parsing of Chinese.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL 2010,</booktitle>
<pages>1108--1117</pages>
<contexts>
<context position="18460" citStr="Li et al., 2010" startWordPosition="3177" endWordPosition="3180">est variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word penalty, unknown word penalty, the glue rule, date, number and 6 passthrough features. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), hereafter MR08. The syntactic soft constraint features include both MR08 exact-matching and crossboundary constraints (denot</context>
</contexts>
<marker>Li, Zhou, Ng, 2010</marker>
<rawString>Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010. Joint syntactic and semantic parsing of Chinese. In Proceedings ofACL 2010, pages 1108–1117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Philip Resnik</author>
<author>Hal Daum´e</author>
</authors>
<title>Modeling syntactic and semantic structures in hierarchical phrase-based translation.</title>
<date>2013</date>
<booktitle>In Proceedings of HLT-NAACL 2013,</booktitle>
<pages>540--549</pages>
<marker>Li, Resnik, Daum´e, 2013</marker>
<rawString>Junhui Li, Philip Resnik, and Hal Daum´e III. 2013. Modeling syntactic and semantic structures in hierarchical phrase-based translation. In Proceedings of HLT-NAACL 2013, pages 540–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Semantic role features for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>716--724</pages>
<contexts>
<context position="33350" citStr="Liu and Gildea (2010)" startWordPosition="5621" endWordPosition="5624">. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and t</context>
</contexts>
<marker>Liu, Gildea, 2010</marker>
<rawString>Ding Liu and Daniel Gildea. 2010. Semantic role features for machine translation. In Proceedings of COLING 2010, pages 716–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="2943" citStr="Marton and Resnik, 2008" startWordPosition="429" endWordPosition="432">roach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-ofthe-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well. Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous blocks in the target language, there is more than one way to describe the monotonicity or swapping patterns; we therefore design two reordering models: one is b</context>
<context position="18934" citStr="Marton and Resnik (2008)" startWordPosition="3253" endWordPosition="3256">and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word penalty, unknown word penalty, the glue rule, date, number and 6 passthrough features. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), hereafter MR08. The syntactic soft constraint features include both MR08 exact-matching and crossboundary constraints (denoted XP= and XP+). Since the syntactic parses of the tuning and test data contain 29 types of constituent labels and 35 types of POS tags, we have 29 types of XP+ features and 64 types of XP= features. 4.2 Model Training To train the syntactic and semantic reordering models, we use a gold alignment dataset.2 It contains 7,870 sentences with 191,364 Chinese words and 261,399 English words. We first run syn1http://www.itl.nist.gov/iad/mig//tests/mt 2This dataset includes LD</context>
<context position="32178" citStr="Marton and Resnik (2008)" startWordPosition="5444" endWordPosition="5447">Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of ACL-HLT 2008, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Learning hierarchical translation structure with linguistic annotations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL 2011,</booktitle>
<pages>642--652</pages>
<marker>Mylonakis, Sima’an, 2011</marker>
<rawString>Markos Mylonakis and Khalil Sima’an. 2011. Learning hierarchical translation structure with linguistic annotations. In Proceedings of ACL 2011, pages 642–652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ThuyLinh Nguyen</author>
<author>Stephan Vogel</author>
</authors>
<title>Integrating phrase-based reordering features into a chartbased decoder for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013,</booktitle>
<pages>1587--1596</pages>
<contexts>
<context position="34493" citStr="Nguyen and Vogel, 2013" startWordPosition="5791" endWordPosition="5794">the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawa</context>
</contexts>
<marker>Nguyen, Vogel, 2013</marker>
<rawString>ThuyLinh Nguyen and Stephan Vogel. 2013. Integrating phrase-based reordering features into a chartbased decoder for machine translation. In Proceedings of ACL 2013, pages 1587–1596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>440--447</pages>
<contexts>
<context position="17400" citStr="Och and Ney, 2000" startWordPosition="2993" endWordPosition="2996">ic and PAS-based reordering, even though it is expressed in terms of syntactic reordering here for ease of presentation. 4 Experiments We have presented our unified approach to incorporating syntactic and semantic soft reordering constraints in an HPB system. In this section, we test its effectiveness in Chinese-English translation. 4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03,</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings ofACL 2000, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="7198" citStr="Palmer et al., 2005" startWordPosition="1125" endWordPosition="1128"> model takes a CFG rule (e.g., VP —* VP PP PP) and models the reordering of the constituents on the left hand side by examining their translation or visit order according to the target language. For the semantic reordering model, it takes a PAS and models its reordering on the target side. Figure 1 shows an example of a PAS where the predicate (Pre) has two core arguments (A0 and A1) and one adjunct (TMP). Note that we refer all core arguments, adjuncts, and predicates as semantic roles; thus we say the PAS in Figure 1 has 4 roles. According to the annotation principles in (Chinese) PropBank (Palmer et al., 2005; Xue and Palmer, 2009), all the roles in a PAS map to a corresponding constituent in the parse tree, and these constituents (e.g., NPs and VBD in Figure 1) do not overlap with each other. Next, we use a CFG rule to describe our syntactic reordering model. Treating the two forms of reorderings in a unified way, the semantic reordering model is obtainable by regarding a PAS as a CFG rule and considering a semantic role as a constituent. Because the translation of a source constituent might result in multiple discontinuous blocks, there can be several ways to describe or group the reordering pat</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="18124" citStr="Papineni et al., 2002" startWordPosition="3118" endWordPosition="3122"> et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of ACL 2002, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL</booktitle>
<pages>404--411</pages>
<contexts>
<context position="18326" citStr="Petrov and Klein, 2007" startWordPosition="3152" endWordPosition="3155">ey smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word penalty, unknown word penalty, the glue rule, date, number and 6 passthrough features. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resni</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLTNAACL 2007, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Min Yen Kan</author>
<author>Haizhou Li</author>
<author>Philip Resnik</author>
</authors>
<title>Topological ordering of function words in hierarchical phrase-based translation.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP</booktitle>
<pages>324--332</pages>
<contexts>
<context position="34449" citStr="Setiawan et al., 2009" startWordPosition="5783" endWordPosition="5786">utput and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic pattern</context>
</contexts>
<marker>Setiawan, Kan, Li, Resnik, 2009</marker>
<rawString>Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip Resnik. 2009. Topological ordering of function words in hierarchical phrase-based translation. In Proceedings of ACL-IJCNLP 2009, pages 324–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Bowen Zhou</author>
<author>Bing Xiang</author>
<author>Libin Shen</author>
</authors>
<title>Two-neighbor orientation model with cross-boundary global contexts.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013,</booktitle>
<pages>1264--1274</pages>
<contexts>
<context position="35108" citStr="Setiawan et al. (2013)" startWordPosition="5885" endWordPosition="5888">, 2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule boundaries. Last, we also note that recent work on nonsyntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models. 7 Conclusion and Future Work In this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the targe</context>
</contexts>
<marker>Setiawan, Zhou, Xiang, Shen, 2013</marker>
<rawString>Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin Shen. 2013. Two-neighbor orientation model with cross-boundary global contexts. In Proceedings of ACL 2013, pages 1264–1274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>168--171</pages>
<contexts>
<context position="17313" citStr="Tseng et al., 2005" startWordPosition="2978" endWordPosition="2981">Pi with a semantic role Ri. Algorithm 1 therefore permits a unified treatment of syntactic and PAS-based reordering, even though it is expressed in terms of syntactic reordering here for ease of presentation. 4 Experiments We have presented our unified approach to incorporating syntactic and semantic soft reordering constraints in an HPB system. In this section, we test its effectiveness in Chinese-English translation. 4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of </context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 168–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP</booktitle>
<pages>477--485</pages>
<contexts>
<context position="20319" citStr="Tsuruoka et al., 2009" startWordPosition="3472" endWordPosition="3475">its automatic alignment. However, our preliminary experiments showed that the reordering models trained on gold alignment yielded higher improvement. 1127 Reordering Syntactic Semantic Type l-m r-m l-m r-m M 73.5 80.6 63.8 67.9 DM 3.9 3.3 14.0 12.0 S 19.5 13.2 13.1 10.7 DS 3.2 3.0 9.1 9.5 #instance 199,234 66,757 Table 3: Reordering type distribution over the reordering model’s training data. Hereafter, l-m and r-m are for leftmost and rightmost, respectively. tactic parsing and semantic role labeling on the Chinese sentences, then train the models by using MaxEnt toolkit with L1 regularizer (Tsuruoka et al., 2009).3 Table 3 shows the reordering type distribution over the training data. Interestingly, about 17% of the syntactic instances and 16% of the semantic instances differ in their leftmost and rightmost reordering types, indicating that the leftmost/rightmost distinction is informative. We also see that the number of semantic instances is about 1/3 of that of syntactic instances, but the entropy of the semantic reordering classes is higher, indicating the reordering of semantic roles is harder than that of syntactic constituents. A deeper examination of the reordering model’s training data reveals</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In Proceedings of ACL-IJCNLP 2009, pages 477–485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Jiri Navratil</author>
<author>Jeffrey Sorensen</author>
<author>Vijil Chenthamarakshan</author>
<author>Nandakishore Kambhatla</author>
</authors>
<title>Syntax based reordering with automatically derived rules for improved statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>1119--1127</pages>
<contexts>
<context position="31524" citStr="Visweswariah et al., 2010" startWordPosition="5343" endWordPosition="5346"> 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reord</context>
</contexts>
<marker>Visweswariah, Navratil, Sorensen, Chenthamarakshan, Kambhatla, 2010</marker>
<rawString>Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nandakishore Kambhatla. 2010. Syntax based reordering with automatically derived rules for improved statistical machine translation. In Proceedings of COLING 2010, pages 1119–1127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>737--745</pages>
<contexts>
<context position="31400" citStr="Wang et al., 2007" startWordPosition="5320" endWordPosition="5323"> To our surprise, however, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during dec</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLP 2007, pages 737–745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Semantic roles for smt: A hybrid two-pass model.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>13--16</pages>
<contexts>
<context position="33778" citStr="Wu and Fung (2009)" startWordPosition="5686" endWordPosition="5689">ctivity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reorde</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>Dekai Wu and Pascale Fung. 2009. Semantic roles for smt: A hybrid two-pass model. In Proceedings of HLT-NAACL 2009: short papers, pages 13–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Extracting pre-ordering rules from predicate-argument structures.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP 2011,</booktitle>
<pages>29--37</pages>
<contexts>
<context position="33224" citStr="Wu et al. (2011)" startWordPosition="5602" endWordPosition="5605">10) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translati</context>
</contexts>
<marker>Wu, Sudoh, Duh, Tsukada, Nagata, 2011</marker>
<rawString>Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011. Extracting pre-ordering rules from predicate-argument structures. In Proceedings of IJCNLP 2011, pages 29– 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>508--514</pages>
<contexts>
<context position="31483" citStr="Xia and McCord, 2004" startWordPosition="5336" endWordPosition="5339">types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted feature</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In Proceedings of COLING 2004, pages 508–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Modeling the translation of predicate-argument structure for smt.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012,</booktitle>
<pages>902--911</pages>
<contexts>
<context position="33471" citStr="Xiong et al. (2012)" startWordPosition="5637" endWordPosition="5640">ted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is un</context>
</contexts>
<marker>Xiong, Zhang, Li, 2012</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Modeling the translation of predicate-argument structure for smt. In Proceedings of ACL 2012, pages 902– 911.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a dependency parser to improve smt for subject-object-verb languages.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>245--253</pages>
<contexts>
<context position="31417" citStr="Xu et al., 2009" startWordPosition="5324" endWordPosition="5327">owever, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another ap</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In Proceedings of HLT-NAACL 2009, pages 245–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="7221" citStr="Xue and Palmer, 2009" startWordPosition="1129" endWordPosition="1132">le (e.g., VP —* VP PP PP) and models the reordering of the constituents on the left hand side by examining their translation or visit order according to the target language. For the semantic reordering model, it takes a PAS and models its reordering on the target side. Figure 1 shows an example of a PAS where the predicate (Pre) has two core arguments (A0 and A1) and one adjunct (TMP). Note that we refer all core arguments, adjuncts, and predicates as semantic roles; thus we say the PAS in Figure 1 has 4 roles. According to the annotation principles in (Chinese) PropBank (Palmer et al., 2005; Xue and Palmer, 2009), all the roles in a PAS map to a corresponding constituent in the parse tree, and these constituents (e.g., NPs and VBD in Figure 1) do not overlap with each other. Next, we use a CFG rule to describe our syntactic reordering model. Treating the two forms of reorderings in a unified way, the semantic reordering model is obtainable by regarding a PAS as a CFG rule and considering a semantic role as a constituent. Because the translation of a source constituent might result in multiple discontinuous blocks, there can be several ways to describe or group the reordering patterns. Therefore, we de</context>
<context position="18520" citStr="Xue and Palmer, 2009" startWordPosition="3187" endWordPosition="3190">parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word penalty, unknown word penalty, the glue rule, date, number and 6 passthrough features. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), hereafter MR08. The syntactic soft constraint features include both MR08 exact-matching and crossboundary constraints (denoted XP= and XP+). Since the syntactic parses of the tuning an</context>
</contexts>
<marker>Xue, Palmer, 2009</marker>
<rawString>Nianwen Xue and Martha Palmer. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="18382" citStr="Xue et al., 2005" startWordPosition="3162" endWordPosition="3165">cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word penalty, unknown word penalty, the glue rule, date, number and 6 passthrough features. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), hereafter MR08. The syntactic soft constraint </context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Dongdong Zhang</author>
<author>Nenghai Yu</author>
</authors>
<title>A ranking-based approach to word reordering for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012,</booktitle>
<pages>912--920</pages>
<contexts>
<context position="31758" citStr="Yang et al. (2012)" startWordPosition="5379" endWordPosition="5382"> 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Althou</context>
</contexts>
<marker>Yang, Li, Zhang, Yu, 2012</marker>
<rawString>Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu. 2012. A ranking-based approach to word reordering for statistical machine translation. In Proceedings of ACL 2012, pages 912–920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifei Zhai</author>
<author>Jiajun Zhang</author>
<author>Yu Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Handling ambiguities of bilingual predicate-argument structures for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013,</booktitle>
<pages>1127--1136</pages>
<contexts>
<context position="33658" citStr="Zhai et al. (2013)" startWordPosition="5668" endWordPosition="5671"> level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering </context>
</contexts>
<marker>Zhai, Zhang, Zhou, Zong, 2013</marker>
<rawString>Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing Zong. 2013. Handling ambiguities of bilingual predicate-argument structures for statistical machine translation. In Proceedings of ACL 2013, pages 1127–1136.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>