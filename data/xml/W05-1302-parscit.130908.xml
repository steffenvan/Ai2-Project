<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.973639">
Adaptive String Similarity Metrics for Biomedical Reference Resolution
</title>
<author confidence="0.839437">
Ben Wellner
</author>
<affiliation confidence="0.42452">
The MITRE Corporation
</affiliation>
<address confidence="0.6227125">
202 Burlington Rd
Bedford MA 01730
</address>
<email confidence="0.993639">
wellner@mitre.org
</email>
<author confidence="0.993952">
Jos´e Castafnoand James Pustejovsky
</author>
<affiliation confidence="0.9712255">
Computer Science Department
Brandeis University
</affiliation>
<address confidence="0.692964">
Waltham MA 02454
</address>
<email confidence="0.999275">
jcastano,jamesp@cs.brandeis.edu
</email>
<sectionHeader confidence="0.995662" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99950175">
In this paper we present the evaluation
of a set of string similarity metrics used
to resolve the mapping from strings to
concepts in the UMLS MetaThesaurus.
String similarity is conceived as a single
component in a full Reference Resolution
System that would resolve such a map-
ping. Given this qualification, we obtain
positive results achieving 73.6 F-measure
(76.1 precision and 71.4 recall) for the
task of assigning the correct UMLS con-
cept to a given string. Our results demon-
strate that adaptive string similarity meth-
ods based on Conditional Random Fields
outperform standard metrics in this do-
main.
</bodyText>
<sectionHeader confidence="0.999753" genericHeader="keywords">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.999975">
1.1 String Similarity and Reference Resolution
</subsectionHeader>
<bodyText confidence="0.999819">
String similarity/matching algorithms are used as a
component in reference resolution algorithms. We
use reference resolution in a broad sense, which in-
cludes any of the following aspects:
</bodyText>
<listItem confidence="0.737891166666667">
a. Intra-document noun phrase reference resolu-
tion.
b. Cross-document or corpus reference resolution.
c. Resolution of entities found in a corpus with
databases, dictionaries or other external knowl-
edge sources. This is also called semantic inte-
</listItem>
<bodyText confidence="0.999255375">
gration, e.g., (Li et al., 2005), reference ground-
ing, e.g., (Kim and Park, 2004) or normaliza-
tion, e.g., (Pustejovsky et al., 2002; Morgan et
al., 2004).
The last two aspects of reference resolution are
particularly important for information extraction,
and the interaction of reference resolution with in-
formation extraction techniques (see for example
Bagga (1998)). The extraction of a particular set of
entities from a corpus requires reference resolution
for the set of entities extracted (e.g., the EDT task in
ACE1), and it is apparent that there is more variation
in the cross-document naming conventions than in a
single document.
The importance of edit distance algorithms has
already been noticed, (M¨uller et al., 2002) and the
importance of string similarity techniques in the
biomedical domain has also been acknowledged,
e.g., (Yang et al., 2004).
String similarity/matching algorithms have also
been used extensively in related problems such as
Name databases and similar problems in structured
data, see (Li et al., 2005) and references mentioned
therein.
The problem of determining whether two similar
strings may denotate the same entity is particularly
challenging in the biomedical literature. It has al-
ready been noticed (Cohen et al., 2002) that there
is great variation in the naming conventions, and
noun phrase constructions in the literature. It has
also been noticed that bio-databases are hardly ever
updated with the names in the literature (Blaschke
</bodyText>
<footnote confidence="0.312456">
lhttp://www.nist.gov/speech/tests/ace/
</footnote>
<page confidence="0.949497">
9
</page>
<note confidence="0.6325175">
Proceedings of the ACL-ISMB Workshop on Linking Biolo ical Literature, Ontologies and Databases: Mining
Biological Semantics, pages 9–16, Detroit, June 2005. @c 2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999935125">
et al., 2003). A further complication is that the ac-
tual mentions found in text are more complex than
just names - including descriptors, in particular. Fi-
nally, ambiguity (where multiple entities have the
same name) is very pervasive in biomedicine.
In this paper we investigate the use of several
string similarity methods to group together string
mentions that might refer to the same entity or con-
cept. Specifically, we consider the sub-problem of
assigning an unseen mention to one of a set of exist-
ing unique entities or concepts, each with an associ-
ated set of known synonyms. As our aim here is fo-
cusing on improving string matching, we have pur-
posely factored out the problem of ambiguity (to the
extent possible) by using the UMLS MetaThesaurus
as our data source, which is largly free of strings that
refer to multiple entities. Thus, our work here can be
viewed an important piece in a larger normalization
or reference resolution system that resolves ambigu-
ity (which includes filtering out mentions that don’t
refer to any entity of interest).
The experiments reported on in this paper evalu-
ate a suite of robust string similarity techniques. Our
results demonstrate considerable improvement to be
gained by using adaptive string similarity metrics
based on Conditional Random Fields customized to
the domain at hand. The resulting best metric, we
term SoftTFIDF-CRF, achieves 73.6 F-measure on
the task of assigning a given string to the correct
concept. Additionally, our experiments demonstrate
a tradeoff between efficiency and recall based on-
gram indexing.
</bodyText>
<sectionHeader confidence="0.995322" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.9017165">
2.1 Entity Extraction and Reference
Resolution in the Biomedical Domain
</subsectionHeader>
<bodyText confidence="0.999921590909091">
Most of the work related to reference resolution in
this domain has been done in the following areas: a)
Intra-document Reference resolution, e.g (Casta˜no
et al., 2002; Lin and Liang, 2004) b) Intra-document
Named entity recognition (e.g Biocreative Task 1A
(Blaschke et al., 2003), and others), also called clas-
sification of biological names (Torii et al., 2004) c)
Intra-document alias extraction d) cross-document
Acronym-expansion extraction, e.g., (Pustejovsky
et al., 2001). e) Protein names resolution against
database entries in SwissProt, protein name ground-
ing, in the context of a relation extraction task
(Kim and Park, 2004). One constraint in these ap-
proaches is that they use several patterns for the
string matching problem. The results of the protein
name grounding are 59% precision and 40% recall.
The Biocreative Task 1B task challenged systems
to ground entities found in article abstracts which
contain mentions of genes in Fly, Mouse and Yeast
databases. A central component in this task was re-
solving ambiguity as many gene names refer to mul-
tiple genes.
</bodyText>
<subsectionHeader confidence="0.999988">
2.2 String Similarity and Ambiguity
</subsectionHeader>
<bodyText confidence="0.99897835">
In this subsection consider the string similarity is-
sues that are present in the biology domain in par-
ticular. The task we consider is to associate a string
with an existing entity, represented by a set of known
strings. Although the issue of ambiguity is present
in the examples we give, it cannot be resolved by
using string similarity methods alone, but instead by
methods that take into account the context in which
those strings occur.
The protein name p21 is ambiguous at least
between two entities, mentioned as p21-ras and
p21/Waf in the literature. A biologist can look at
a set of descriptions and decide whether the strings
are ambiguous or correspond to any of these two (or
any other entity).
The following is an example of such a mapping,
where R corresponds to p21-ras, W to p21(Waf) and
G to another entity (the gene). Also it can be noticed
that some of the mappings include subcases (e.g.,
R.1).2
</bodyText>
<subsectionHeader confidence="0.630079">
String Form Entity
</subsectionHeader>
<tableCaption confidence="0.996756">
Table 1: A possible mapping from strings to entities.
</tableCaption>
<footnote confidence="0.610913">
2All the examples were taken from the MEDLINE corpus.
</footnote>
<figure confidence="0.9703075">
p21
ras-p21 protein
p21(Waf1/Cip1)
cyclin-dependent kinase-I p21(Waf-1)
normal ras p21 protein
pure v-Kirsten (Ki)-ras p21
wild type p21
synthetic peptide P21
p21 promoter
transforming protein v-p21
v-p21
p21CIP1/WAF1
protein p21 WAF1/CIP1/Sd:1
R
R/W
W
W
R
R.1
R/W
R/W.2
G
R.3
R.3
W
W
</figure>
<page confidence="0.980825">
10
</page>
<bodyText confidence="0.999264666666667">
If we want to use an external knowlege source to
produce such a mapping, we can try to map it to con-
cepts in the UMLS Methatesaurus and entries in the
SwissProt database.
These two entities correspond to the concepts
C0029007 (p21-Ras) and C0288472 (p21-Waf) in
the UMLS Methathesaurus. There are 27 strings or
names in the UMLS that map to C0288472 (Table
2):
</bodyText>
<table confidence="0.952580357142857">
oncoprotein p21 CAP20
CDK2-associated protein 20 kDa MDA 6
Cdk2 inhibitor WAF1 CIP1
Cdk-interacting protein cdn1 protein
CDK-Interacting Protein 1 CDKN1A
CDKN1 protein Cip1 protein
Cip-1 protein mda-6 protein
Cyclin-Dependent Kinase Inhibitor 1A p21
p21 cell cycle regulator p21(cip1)
p21 cyclin kinase inhibitor p21(waf1-cip1)
Pic-1 protein (cyclin) p21-WAF1
senescent cell-derived inhibitor protein 1 protein p21
CDKN1Aprotein WAF1 protein
WAF-1 Protein
</table>
<tableCaption confidence="0.692538666666667">
Table 2: UMLS strings corresponding to C0288472
There are 8 strings that map to concept C0029007
(Table 3).
</tableCaption>
<table confidence="0.994115">
Proto-Oncogene Protein p21(ras) p21(c-ras)
p21 RAS Family Protein p21 RAS Protein
Proto-Oncogene Protein ras c-ras Protein
ras Proto-Oncogene Product p21 p21(ras)
</table>
<tableCaption confidence="0.999863">
Table 3: UMLS strings corresponding to C0029007
</tableCaption>
<bodyText confidence="0.998828655172414">
It can be observed that there is only one exact
match: p21 in C0288472 and Table 1. It should
be noted that p21, is not present in the UMLS as a
possible string for C0029007. There are other close
matches like p21(Waf1/Cip1) (which seems very
frequent) and p21(waf1-cip1).
An expression like The inhibitor of cyclin-
dependent kinases WAF1 gene product p21 has
a high similarity with Cyclin-Dependent Kinase
Inhibitor 1 A and The cyclin-dependent kinase-I
p21(Waf-1) partially matches Cyclin-Dependent Ki-
nase
However there are other mappings which look
quite difficult unless some context is given to pro-
vide additional clues (e.g., v-p21).
The SwissProt entries CDN1A FELCA,
CDN1A HUMAN and CDN1A MOUSE are
related to p21(Waf). They have the following set of
common description names:
Cyclin-dependent kinase inhibitor 1, p21, CDK-
interacting protein 1.3
There is only one entry in SwissProt related to p21-
ras: Q9PSS8 PLAFE: with the description name
P21-ras protein and a related gene name: Ki-ras.
It should be noted that SwissProt classifies, as dif-
ferent entities, the proteins that refer to different or-
ganisms. The UMLS MetaThesaurus, on the other
hand, does not make this distinction. Neither is this
distinction always present in the literature.
</bodyText>
<sectionHeader confidence="0.968441" genericHeader="method">
3 Methods for Computing String
Similarity
</sectionHeader>
<bodyText confidence="0.999969384615385">
A central component in the process of normaliza-
tion or reference resolution is computing string sim-
ilarity between two strings. Methods for measuring
string similarity can generally be broken down into
character-based and token-based approaches.
Character-based approaches typically consist of
the edit-distance metric and variants thereof. Edit
distance considers the number of edit operations (ad-
dition, substitution and deletion) required to trans-
form a string into another string . The Leven-
stein distance assigns unit cost to all edit operations.
Other variations allow arbitrary costs or special costs
for starting and continuing a “gap” (i.e., a long se-
quence of adds or deletes).
Token-based approaches include the Jaccard sim-
ilarity metric and the TF/IDF metric. The meth-
ods consider the (possibly weighted) overlap be-
tween the tokens of two strings. Hybrid token and
character-based are best represented by SoftTFIDF,
which includes not only exact token matches but
also close matches (using edit-distance, for exam-
ple). Another approach is to perform the Jaccard
similarity (or TF/IDF) between the-grams of the
two strings instead of the tokens. See Cohen et
al. (2003) for a detailed overview and comparison
of some of these methods on different data sets.
</bodyText>
<footnote confidence="0.584344142857143">
3There are two more description names for the human and
mouse entries. The SwissProt database has also associated
Gene names to those entries which are related to some of the
possible names that we find in the literature. Those gene names
are: CDKN1A, CAP20, CDKN1, CIP1, MDA6, PIC1, SDI1,
WAF1, Cdkn1a, Cip1, Waf1. It can be seen that those names are
incorporated in the UMLS as protein names.
</footnote>
<page confidence="0.997717">
11
</page>
<bodyText confidence="0.9999775">
Recent work has also focused on automatic meth-
ods for adapting these string similarity measures
to specific data sets using machine learning. Such
approaches include using classifiers to weight var-
ious fields for matching database records (Cohen
and Richman, 2001). (Belenko and Mooney, 2003)
presents a generative, Hidden Markov Model for
string similarity.
</bodyText>
<sectionHeader confidence="0.956414" genericHeader="method">
4 An Adaptive String Similarity Model
</sectionHeader>
<bodyText confidence="0.997741153846154">
Conditional Random Fields (CRF) are a recent, in-
creasingly popular approach to sequence labeling
problems. Informally, a CRF bears resemblance to
a Hidden Markov Model (HMM) in which, for each
input position in a sequence, there is an observed
variable and a corresponding hidden variable. Like
HMMs, CRFs are able to model (Markov) depen-
dencies between the hidden (predicted) variables.
However, because CRFs are conditional, discrimina-
tively trained models, they can incorporate arbitrary
overlapping (non-independent) features over the en-
tire input space — just like a discriminative classi-
fier.
CRFs are log-linear models that compute the
probability of a state sequence,
where the are arbitrary feature functions, the
are the model parameters and is a normaliza-
tion function.
Training a CRF amounts to finding the that
maximize the conditional log-likelihood of the data.
Given a trained CRF, the inference problem in-
volves finding the most likely state sequence given
a sequence of observations. This is done using a
slightly modified version of the Viterbi algorithm
(See Lafferty et al. (2001) more for details on
CRFs).
</bodyText>
<subsectionHeader confidence="0.795715">
4.1 CRFs for String Similarity
</subsectionHeader>
<bodyText confidence="0.998188">
CRFs can be used to measure string similarity by
viewing the observed sequence,, and the state se-
quence,, as sequences of characters. In practice
we are presented with two strings, , and of pos-
sibly differing lengths. A necessary first step is to
align the two strings by applying the Levenstein dis-
tance procedure as described earlier. This produces
a series of edit operations where each operation has
one of three possible forms: 1) (addition), 2)
(substitution) and 3) (deletion). The
observed and hidden sequences are then derived by
reading off the terms on the right and left-hand sides
of the operations, respectively. Thus, the possible
state values include all the characters in our domain
plus the special null character,.
</bodyText>
<table confidence="0.723620571428572">
Feature Description Variables
State uni-gram
State bi-gram
Obs. uni-gram; state uni-gram
Obs. bi-gram; state uni-gram
Obs. is punctuation and state uni-gram
Obs. is a number and state uni-gram
</table>
<tableCaption confidence="0.99855">
Table 4: Features used for string similarity
</tableCaption>
<bodyText confidence="0.999886727272727">
We employ a set of relatively simple features in
our string similarity model described in Table 4. One
motivation for keeping the set of features simple was
to determine the utility of string similarity CRFs
without spending effort designing domain-specific
features; this is a primary motivation for taking a
machine learning approach in the first place. Addi-
tionally, we have found that more specific, discrimi-
nating features (e.g., observation tri-grams with state
bi-grams) tend to reduce the performance of the
CRF on this domain - in some cases considerably.
</bodyText>
<subsectionHeader confidence="0.974344">
4.2 Practical Considerations
</subsectionHeader>
<bodyText confidence="0.963823888888889">
We discuss a few practical concerns with using
CRFs for string similarity.
The first issue is how to scale CRFs to this task.
The inference complexity for CRFs is where
is the size of the vocabulary of states andis the
number of input positions. In our setting, the num-
ber of state variable values is very large - one for
each character in our alphabet (which is on the or-
der of 40 or more including digits and punctuation).
Moreover, we typically have very large training sets
largely due to the fact that training pairs are
derivable from an equivalence class of size.
Given this situation, standard training for CRFs
becomes unwieldy, since it involves performing in-
ference over the entire data set repeatedly (typically
a few hundred iterations are required to converge).
,
given an observed sequence, as:
</bodyText>
<page confidence="0.963923">
12
</page>
<bodyText confidence="0.999947529411764">
As such, we resort to an approximation: Voted Per-
ceptron training (Collins, 2002). Voted Perceptron
training does not involve maximizing log-likelihood,
but instead updates parameters via stochastic gradi-
ent descent with a small number of passes over the
data.
Another consideration that arises is given a pair
of strings, which one should be considered the “ob-
served” sequence and which one the “hidden” se-
quence.
Another consideration that arises is given a pair
of strings, which string should be considered the
“observed” sequence and which the “hidden” se-
quence?4 We have taken to always selecting the
longest string as the “observed” string, as it appears
most natural, though that decision is somewhat arbi-
trary.
A last observation is that the probability assigned
to a pair of strings by the model will be reduced ge-
ometrically for longer string pairs (since the prob-
ability is computed as a product ofterms, where
is the length of the sequence). We have taken to
normalizing the probabilities by the length of the se-
quence roughly following the approach of (Belenko
and Mooney, 2003).
A final point here is that it is possible to use
Viterbi decoding to find the-best hidden strings
given only the observed string. This provides a
mechanism to generate domain-specific string alter-
ations for a given string ranked by their probability.
The advantage of this approach is that such alter-
ations can be used to expand a synonym list; exact
matching can then be used greatly increasing effi-
ciency. Work is ongoing in this area.
</bodyText>
<sectionHeader confidence="0.985245" genericHeader="method">
5 Matching Procedure
</sectionHeader>
<bodyText confidence="0.999797">
Our matching procedure in this paper is set in the
context of finding the concept or entity (each with
some existing set of known strings) that a given
string,, is referring to. In many settings, such as the
BioCreative Task 1B task mentioned above, it is nec-
essary to match large numbers of strings against the
lexicon - potentially every possible phrase in a large
</bodyText>
<footnote confidence="0.5268496">
4Note that a standard use for models such as this is to find the
most likely hidden sequence given only the observed sequence.
In our setting here we are provided the hidden sequence and
wish to compute it’s (log-)probability given the observed se-
quence.
</footnote>
<bodyText confidence="0.99786444">
number of documents. As such, very fast matching
times (typically on the order of milliseconds) are re-
quired.
Our method can be broken down into two steps.
We first select a reasonable candidate set of strings
(associated with a concept or lexical entry),
, reasonably similar to the given string
using an efficient method. We then use one of a
number of string similarity metrics on all the pairs:
The set of candidate strings, is deter-
mined by the-gram match ratio, which we define
as:
where such thatis a-gram of .
This set is retrieved very quickly by creating a-
gram index: a mapping between each-gram and
the strings (entries) in which it occurs. At query
time, the given string is broken into-grams and
the sets corresonding to each-gram are retrieved
from the index. A straightforward computation finds
those entries that have a certain number of-grams
in common with the query string from which the
ratio can be readily computed.
Depending on the setting, three options are possi-
ble given the returned set of candidates for a string
:
</bodyText>
<listItem confidence="0.978586428571428">
1. Consider and equivalent where is the
most similar string
2. Consider andequivalent whereis the
most similar string and , for
some threshold
3. Consider andequivalent for allwhere
, for some threshold
</listItem>
<bodyText confidence="0.9998425">
In the experiments in this paper, we use the first
criterion since for a given string, we know that it
should be assigned to exactly one concept (see be-
low).
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.998946">
6.1 Data and Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999943333333333">
We used the UMLS MetaThesaurus for all our ex-
periments for three reasons: 1) the UMLS repre-
sents a wide-range of important biomedical concepts
</bodyText>
<page confidence="0.996177">
13
</page>
<figure confidence="0.760635142857143">
Precision Recall F-measure
SoftTFIDF-CRF(
SoftTFIDF-Lev(
CRF( )
-gram Best(
Levenstein( )
TFIDF( )
</figure>
<table confidence="0.826950333333333">
) 0.761 0.714 0.736
) 0.742 0.697 0.718
0.729 0.705 0.717
) 0.714 0.658 0.685
0.710 0.622 0.663
0.730 0.576 0.644
</table>
<bodyText confidence="0.999821202898551">
for many applications and 2) the size of the UMLS
(compared with BioCreative Task 1B, for example)
promotes statistically significant results as well as
sufficient training data 3) the problem of ambiguity
(multiple concepts with the same name) is largely
absent in the UMLS.
The UMLS is a taxonomy of medical and clini-
cal concepts consisting of 1,938,701 lexical entries
(phrase strings) where each entry belongs to one (or,
in very rarely, more than one) of 887,688 concepts.
We prepared the data by first selecting only those
lexical entries belonging to a concept containing 12
or more entries. This resulted in a total of 129,463
entries belonging to 7,993 concepts. We then di-
vided this data into a training set of 95,167 entries
and test set of 34,296 entries where roughly 70% of
the entries for each concept were placed in the train-
ing set and 30% in the test set. Thus, the training
set and test set both contained some string entries
for each of the 7,993 concepts. While restricting the
number of entries to 12 or more was somewhat arbi-
trary, this allowed for at least 7 (70% of 12) entries
in the training data for each concept, providing suf-
ficient training data.
The task was to assign the correct concept identi-
fier to each of the lexical entries in the test set. This
was carried out by finding the most similar string
entry in the training data and returning the con-
cept identifier associated with that entry. Since each
test instance must be assigned to exactly one con-
cept, our system simply ranked the candidate strings
based on the string similarity metric
used. We compared the results for different maxi-
mum-gram match ratios. Recall that the-gram
match mechanism is essentially a filter; higher val-
ues correspond to larger candidate pools of strings
considered by the string similarity metrics.
We used six different string similarity metrics
that were applied to the same set of candidate re-
sults returned by the-gram matching procedure
for each test string. These were TFIDF, Lev-
enstein, q-gram-Best, CRF, SoftTFIDF-Lev and
SoftTFIDF-CRF. TFIDF and Levenstein were de-
scribed earlier. The q-gram-Best metric simply se-
lects the match with the lowest-gram match ratio
returned by the-gram match procedure described
Table 5: Maximum F-measure attained for each
string similarity metric, with corresponding preci-
sion and recall values. The numbers in parentheses
indicate the-gram match value for which the high-
est F-measure was attained.
above5. The SoftTFIDF-Lev model is the Soft-
TFIDF metric described earlier where the secondary
metric for similarity between pairs of tokens is the
Levenstein distance.
The CRF metric is the CRF string similarity
model applied to the entire strings. This model was
trained on pairs of strings that belonged to the same
concept in the training data, resulting in 130,504
string pair training instances. The SoftTFIDF-CRF
metric is the SoftTFIDF method where the sec-
ondary metric is the CRF string similarity model.
This CRF model was trained on pairs of tokens (not
entire phrases). We derived pairs of tokens by find-
ing the most similar pairs of tokens (similarity was
determined here by Levenstein distance) between
strings belonging to the same concept in the training
data. This resulted in 336,930 string pairs as training
instances.
</bodyText>
<subsectionHeader confidence="0.929378">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999693153846154">
We computed the precision, recall and F-measure
for each of the string similarity metrics across dif-
ferent-gram match ratios shown in Fig. 1. Both
a precision and recall error is introduced when the
top-returned concept id is incorrect; just a recall er-
ror occurs when no concept id is returned at all - i.e.
when the-gram match procedure returns the empty
set of candidate strings. This is more likely to occur
when for lowervalues and explains the poor recall
in those cases. In addition, we computed the mean
reciprocal rank of each of the methods. This is com-
puted using the ranked, ordered list of the concepts
returned by each method. This scoring method as-
</bodyText>
<footnote confidence="0.997189">
5This is essentially the Jaccard similarity metric over-
grams instead of tokens
</footnote>
<page confidence="0.997126">
14
</page>
<figure confidence="0.989880176470588">
q−gram Match Ratio
SoRTFIDF−CRF
SoftTFIDF−Lev
CRF
q−gram Best
Levenstein
TFIDF
0.2 0.3 0.4 0.5 0.6
q−gram Match Ratio
SoRTFIDF−CRF
SoftTFIDF−Lev
CRF
q−gram Best
Levenstein
TFIDF
0.2 0.3 0.4 0.5 0.6
q−gram Match Ratio
</figure>
<figureCaption confidence="0.942127">
Figure 1: Precision, Recall, F-measure and Mean
Reciprocal Rank comparisions for each string simi-
larity metric across different-gram match ratios.
</figureCaption>
<bodyText confidence="0.997887933333333">
signs a score of for each test instance where
is the position in the ranked list at which the correct
concept is found. For example, by returning the cor-
rect concept as the 4th element in the ranked list, a
method is awarded . The mean recip-
rocal rank is just the average score over all the test
elements.
As can be seen, the SoftTFIDF-CRF string-
similarity metric out-performs all the other meth-
ods on this data set. This approach is robust to
both word order variations and character-level dif-
ferences, the latter with the benefit of being adapted
to the domain. Word order is clearly a critical fac-
tor in this domain though the CRF metric, entirely
character-based, does surprisingly well - much bet-
ter than the Levenstein distance. The q-gram-Best
metric, being able to handle word order variations
and character-level differences, performs fairly.
The graphs illustrate a tradeoff between efficiency
and accuracy (recall). Lower-gram match ratios
return fewer candidates with correspondingly fewer
pairwise string similarities to compute. Precision ac-
tually peaks with a-gram match ratio of around
0.2. Recall tapers off even up to high q-gram lev-
els for all metrics, indicating that nearly 30% of
the test instances are probably too difficult for any
string similarity metric. Error analysis indicates that
these cases tend to be entries involving synonymous
“nicknames”. Acquiring such synonyms requires
other machinery, e.g., (Yu and Agichtein, 2003).
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.974092">
We have explored a set of string similarity metrics
in the biological domain in the service of reference
resolution. String similarity is only one parameter to
be considered in this task. We presented encourag-
ing results for assigning strings to UMLS concepts
based solely on string similarity metrics — demon-
strating that adaptive string similarity metrics show
significant promise for biomedical text processing.
Further progress will require a system that 1) uti-
lizes context of occurrence of respective strings for
handling ambiguity and 2) further improves recall
&apos;Inspection of the data indicates that the purely character-
based methods are more robust than one might think. There are
at least 8 strings to match against for a concept and it is likely
that at least one of them will have similar word order to the test
string.
</bodyText>
<figure confidence="0.988730130434782">
0.2 0.3 0.4 0.5 0.6
Recall
0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80
SoRTFIDF−CRF
SoftTFIDF−Lev
CRF
q−gram Best
Levenstein
TFIDF
SoRTFIDF−CRF
SoftTFIDF−Lev
CRF
q−gram Best
Levenstein
TFIDF
0.1 0.2 0.3 0.4 0.5 0.6
q−gram Match Ratio
Precision
0.5 0.6 0.7 0.8
F−mmsum
0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80
Mean Reciprical Score
0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80
</figure>
<page confidence="0.96132">
15
</page>
<bodyText confidence="0.998315">
through expanded synonyms.
Future work should also consider the depen-
dent nature (via transitivity) of reference resolution.
Comparing a test string against all (current) mem-
bers of an equivalence class and considering multi-
ple, similar test instances simultaneously (McCal-
lum and Wellner, 2003) are two directions to pursue
in this vein.
</bodyText>
<sectionHeader confidence="0.998431" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.90800225">
We thank Dave Harris, Alex Morgan, Lynette Hirschman and
Marc Colosimo for useful discussions and comments. This
work was supported in part by MITRE Sponsored Research
51MSR123-A5.
</bodyText>
<sectionHeader confidence="0.992089" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999506541176471">
A. Bagga. 1998. Coreference, cross-document coreference,
and information extraction methodologies. Ph.D. thesis,
Duke University. Supervisor-Alan W. Biermann.
M. Belenko and R. Mooney. 2003. Adaptive duplicate detec-
tion using learnable string similarity measures. In Proceed-
ings of the 9th ACM SIGKDD International Conference on
Knowledge Discovery and Datamining, pages 39–48, Wash-
ington D.C.
C. Blaschke, L. Hirschman, A. Yeh, and A. Valencia. 2003.
Critical assessment of information extraction systems in bi-
ology. Comparative and Functional Genomics, pages 674–
677.
J. Casta˜no, J. Zhang, and J. Pustejovsky. 2002. Anaphora reso-
lution in biomedical literature. In International Symposium
on Reference Resolution, Alicante, Spain.
William Cohen and Jacob Richman. 2001. Learning to match
and cluster entity names. In ACM SIGIR-2001 Workshop
on Mathematical/Formal Methods in Information Retrieval,
New Orleans, LA, September.
K. Bretonnel Cohen, Andrew Dolbey, George Acquaah-
Mensah, and Lawrence Hunter. 2002. Contrast and vari-
ability in gene names. In Proceedings of the Workshop on
Natural Language Processing in the Biomedical Domain,
pages 14–20, Philadelphia, July. Association for Computa-
tional Linguistics.
W. Cohen, P. Ravikumar, and S. Fienburg. 2003. A comparison
of string metrics for matching names and records. In KDD
Workshop on Data Cleaning and Object Consolidation.
Michael Collins. 2002. Discriminative training methods for
hidden markove models: Theory and experiments with per-
ceptron algorithms. In EMNLP 2002.
Yu H, Hatzivassiloglou V, Friedman C, Rzhetsky A, and Wilbur
W. 2002. Automatic extraction of gene and protein syn-
onyms from medline and journal articles. In Proc AMIA
Symposium, pages 919–23.
Jung-Jae Kim and Jong C. Park. 2004. Bioar: Anaphora res-
olution for relating protein names to proteome database en-
tries. In Sanda Harabagiu and David Farwell, editors, ACL
2004: Workshop on Reference Resolution and its Applica-
tions, pages 79–86, Barcelona, Spain, July. Association for
Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. 18th Interna-
tional Conf. on Machine Learning, pages 282–289. Morgan
Kaufmann, San Francisco, CA.
X. Li, P. Morie, and D. Roth. 2005. Semantic integration
in text: From ambiguous names to identifiable entities. AI
Magazine. Special Issue on Semantic Integration.
Y. Lin and T. Liang. 2004. Pronominal and sortal anaphora
resolution for biomedical literature. In Proceedings of RO-
CLINGXVI: Conference on Computational Linguistics and
Speech Processing, Taipei, Taiwan.
Andrew McCallum and Ben Wellner. 2003. Toward con-
ditional models of identity uncertainty with application to
proper noun coreference. In Proceedings of the IJCAI-2003
Workshop on Information Integration on the Web, pages 79–
86, Acapulco, Mexico, August.
A. Morgan, L. Hirschman, M. Colosimo, A. Yeh, and
J. Colombe. 2004. Gene name identification and normaliza-
tion using a model organism database. Journal of Biomedi-
calInformatics, (6):396–410.
Christoph M¨uller, Stefan Rapp, and Michael Strube. 2002. Ap-
plying co-training to reference resolution. In ACL, pages
352–359.
J. Pustejovsky, J. Casta˜no, B. Cochran, M. Kotecki, and
M. Morrell. 2001. Automatic extraction of acronym-
meaning pairs from medline databases. In Proceedings of
Medinfo, London.
J. Pustejovsky, J. Casta˜no, J. Zhang, R. Sauri, and W. Luo.
2002. Medstract: creating large-scale information servers
from biomedical texts. In Proceedings of the Workshop on
Natural Language Processing in the Biomedical Domain,
pages 85–92, Philadelphia, July. Association for Computa-
tional Linguistics.
M. Torii, S. Kamboj, and K. Vijay-Shanker. 2004. Using name-
internal and contextual features to classify biological terms.
Journal of Biomedical Informatics, pages 498–511.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2004. Improving
noun phrase coreference resolution by matching strings. In
Proceedings of 1st Internation Joint Conference of Natural
Language Processing, pages 326–333.
H. Yu and E. Agichtein. 2003. Extracting synonymous gene
and protein terms from biological literature. Bioinformatics,
pages 340–349.
</reference>
<page confidence="0.998673">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.438482">
<title confidence="0.999893">Adaptive String Similarity Metrics for Biomedical Reference Resolution</title>
<author confidence="0.999991">Ben Wellner</author>
<affiliation confidence="0.999256">The MITRE Corporation</affiliation>
<address confidence="0.99716">202 Burlington Rd</address>
<author confidence="0.508053">Bedford MA</author>
<email confidence="0.974288">wellner@mitre.org</email>
<author confidence="0.998325">Jos´e Castafnoand James Pustejovsky</author>
<affiliation confidence="0.999893">Computer Science Department Brandeis University</affiliation>
<address confidence="0.999506">Waltham MA 02454</address>
<email confidence="0.999842">jcastano,jamesp@cs.brandeis.edu</email>
<abstract confidence="0.992944941176471">In this paper we present the evaluation of a set of string similarity metrics used to resolve the mapping from strings to the UMLS MetaThesaurus. String similarity is conceived as a single component in a full Reference Resolution System that would resolve such a mapping. Given this qualification, we obtain positive results achieving 73.6 F-measure (76.1 precision and 71.4 recall) for the task of assigning the correct UMLS concept to a given string. Our results demonstrate that adaptive string similarity methods based on Conditional Random Fields outperform standard metrics in this domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bagga</author>
</authors>
<title>Coreference, cross-document coreference, and information extraction methodologies.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Duke University. Supervisor-Alan</institution>
<contexts>
<context position="1771" citStr="Bagga (1998)" startWordPosition="258" endWordPosition="259">a. Intra-document noun phrase reference resolution. b. Cross-document or corpus reference resolution. c. Resolution of entities found in a corpus with databases, dictionaries or other external knowledge sources. This is also called semantic integration, e.g., (Li et al., 2005), reference grounding, e.g., (Kim and Park, 2004) or normalization, e.g., (Pustejovsky et al., 2002; Morgan et al., 2004). The last two aspects of reference resolution are particularly important for information extraction, and the interaction of reference resolution with information extraction techniques (see for example Bagga (1998)). The extraction of a particular set of entities from a corpus requires reference resolution for the set of entities extracted (e.g., the EDT task in ACE1), and it is apparent that there is more variation in the cross-document naming conventions than in a single document. The importance of edit distance algorithms has already been noticed, (M¨uller et al., 2002) and the importance of string similarity techniques in the biomedical domain has also been acknowledged, e.g., (Yang et al., 2004). String similarity/matching algorithms have also been used extensively in related problems such as Name </context>
</contexts>
<marker>Bagga, 1998</marker>
<rawString>A. Bagga. 1998. Coreference, cross-document coreference, and information extraction methodologies. Ph.D. thesis, Duke University. Supervisor-Alan W. Biermann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Belenko</author>
<author>R Mooney</author>
</authors>
<title>Adaptive duplicate detection using learnable string similarity measures.</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Datamining,</booktitle>
<pages>39--48</pages>
<location>Washington D.C.</location>
<contexts>
<context position="11633" citStr="Belenko and Mooney, 2003" startWordPosition="1809" endWordPosition="1812">ries. The SwissProt database has also associated Gene names to those entries which are related to some of the possible names that we find in the literature. Those gene names are: CDKN1A, CAP20, CDKN1, CIP1, MDA6, PIC1, SDI1, WAF1, Cdkn1a, Cip1, Waf1. It can be seen that those names are incorporated in the UMLS as protein names. 11 Recent work has also focused on automatic methods for adapting these string similarity measures to specific data sets using machine learning. Such approaches include using classifiers to weight various fields for matching database records (Cohen and Richman, 2001). (Belenko and Mooney, 2003) presents a generative, Hidden Markov Model for string similarity. 4 An Adaptive String Similarity Model Conditional Random Fields (CRF) are a recent, increasingly popular approach to sequence labeling problems. Informally, a CRF bears resemblance to a Hidden Markov Model (HMM) in which, for each input position in a sequence, there is an observed variable and a corresponding hidden variable. Like HMMs, CRFs are able to model (Markov) dependencies between the hidden (predicted) variables. However, because CRFs are conditional, discriminatively trained models, they can incorporate arbitrary over</context>
<context position="16369" citStr="Belenko and Mooney, 2003" startWordPosition="2570" endWordPosition="2573"> strings, which string should be considered the “observed” sequence and which the “hidden” sequence?4 We have taken to always selecting the longest string as the “observed” string, as it appears most natural, though that decision is somewhat arbitrary. A last observation is that the probability assigned to a pair of strings by the model will be reduced geometrically for longer string pairs (since the probability is computed as a product ofterms, where is the length of the sequence). We have taken to normalizing the probabilities by the length of the sequence roughly following the approach of (Belenko and Mooney, 2003). A final point here is that it is possible to use Viterbi decoding to find the-best hidden strings given only the observed string. This provides a mechanism to generate domain-specific string alterations for a given string ranked by their probability. The advantage of this approach is that such alterations can be used to expand a synonym list; exact matching can then be used greatly increasing efficiency. Work is ongoing in this area. 5 Matching Procedure Our matching procedure in this paper is set in the context of finding the concept or entity (each with some existing set of known strings) </context>
</contexts>
<marker>Belenko, Mooney, 2003</marker>
<rawString>M. Belenko and R. Mooney. 2003. Adaptive duplicate detection using learnable string similarity measures. In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Datamining, pages 39–48, Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Blaschke</author>
<author>L Hirschman</author>
<author>A Yeh</author>
<author>A Valencia</author>
</authors>
<title>Critical assessment of information extraction systems in biology. Comparative and Functional Genomics,</title>
<date>2003</date>
<pages>674--677</pages>
<contexts>
<context position="5078" citStr="Blaschke et al., 2003" startWordPosition="770" endWordPosition="773">n at hand. The resulting best metric, we term SoftTFIDF-CRF, achieves 73.6 F-measure on the task of assigning a given string to the correct concept. Additionally, our experiments demonstrate a tradeoff between efficiency and recall based ongram indexing. 2 Background 2.1 Entity Extraction and Reference Resolution in the Biomedical Domain Most of the work related to reference resolution in this domain has been done in the following areas: a) Intra-document Reference resolution, e.g (Casta˜no et al., 2002; Lin and Liang, 2004) b) Intra-document Named entity recognition (e.g Biocreative Task 1A (Blaschke et al., 2003), and others), also called classification of biological names (Torii et al., 2004) c) Intra-document alias extraction d) cross-document Acronym-expansion extraction, e.g., (Pustejovsky et al., 2001). e) Protein names resolution against database entries in SwissProt, protein name grounding, in the context of a relation extraction task (Kim and Park, 2004). One constraint in these approaches is that they use several patterns for the string matching problem. The results of the protein name grounding are 59% precision and 40% recall. The Biocreative Task 1B task challenged systems to ground entiti</context>
</contexts>
<marker>Blaschke, Hirschman, Yeh, Valencia, 2003</marker>
<rawString>C. Blaschke, L. Hirschman, A. Yeh, and A. Valencia. 2003. Critical assessment of information extraction systems in biology. Comparative and Functional Genomics, pages 674– 677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Casta˜no</author>
<author>J Zhang</author>
<author>J Pustejovsky</author>
</authors>
<title>Anaphora resolution in biomedical literature.</title>
<date>2002</date>
<booktitle>In International Symposium on Reference Resolution,</booktitle>
<location>Alicante,</location>
<marker>Casta˜no, Zhang, Pustejovsky, 2002</marker>
<rawString>J. Casta˜no, J. Zhang, and J. Pustejovsky. 2002. Anaphora resolution in biomedical literature. In International Symposium on Reference Resolution, Alicante, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cohen</author>
<author>Jacob Richman</author>
</authors>
<title>Learning to match and cluster entity names.</title>
<date>2001</date>
<booktitle>In ACM SIGIR-2001 Workshop on Mathematical/Formal Methods in Information Retrieval,</booktitle>
<location>New Orleans, LA,</location>
<contexts>
<context position="11605" citStr="Cohen and Richman, 2001" startWordPosition="1805" endWordPosition="1808">for the human and mouse entries. The SwissProt database has also associated Gene names to those entries which are related to some of the possible names that we find in the literature. Those gene names are: CDKN1A, CAP20, CDKN1, CIP1, MDA6, PIC1, SDI1, WAF1, Cdkn1a, Cip1, Waf1. It can be seen that those names are incorporated in the UMLS as protein names. 11 Recent work has also focused on automatic methods for adapting these string similarity measures to specific data sets using machine learning. Such approaches include using classifiers to weight various fields for matching database records (Cohen and Richman, 2001). (Belenko and Mooney, 2003) presents a generative, Hidden Markov Model for string similarity. 4 An Adaptive String Similarity Model Conditional Random Fields (CRF) are a recent, increasingly popular approach to sequence labeling problems. Informally, a CRF bears resemblance to a Hidden Markov Model (HMM) in which, for each input position in a sequence, there is an observed variable and a corresponding hidden variable. Like HMMs, CRFs are able to model (Markov) dependencies between the hidden (predicted) variables. However, because CRFs are conditional, discriminatively trained models, they ca</context>
</contexts>
<marker>Cohen, Richman, 2001</marker>
<rawString>William Cohen and Jacob Richman. 2001. Learning to match and cluster entity names. In ACM SIGIR-2001 Workshop on Mathematical/Formal Methods in Information Retrieval, New Orleans, LA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bretonnel Cohen</author>
<author>Andrew Dolbey</author>
<author>George AcquaahMensah</author>
<author>Lawrence Hunter</author>
</authors>
<title>Contrast and variability in gene names.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Natural Language Processing in the Biomedical Domain,</booktitle>
<pages>14--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia,</location>
<contexts>
<context position="2668" citStr="Cohen et al., 2002" startWordPosition="395" endWordPosition="398">importance of edit distance algorithms has already been noticed, (M¨uller et al., 2002) and the importance of string similarity techniques in the biomedical domain has also been acknowledged, e.g., (Yang et al., 2004). String similarity/matching algorithms have also been used extensively in related problems such as Name databases and similar problems in structured data, see (Li et al., 2005) and references mentioned therein. The problem of determining whether two similar strings may denotate the same entity is particularly challenging in the biomedical literature. It has already been noticed (Cohen et al., 2002) that there is great variation in the naming conventions, and noun phrase constructions in the literature. It has also been noticed that bio-databases are hardly ever updated with the names in the literature (Blaschke lhttp://www.nist.gov/speech/tests/ace/ 9 Proceedings of the ACL-ISMB Workshop on Linking Biolo ical Literature, Ontologies and Databases: Mining Biological Semantics, pages 9–16, Detroit, June 2005. @c 2005 Association for Computational Linguistics et al., 2003). A further complication is that the actual mentions found in text are more complex than just names - including descript</context>
</contexts>
<marker>Cohen, Dolbey, AcquaahMensah, Hunter, 2002</marker>
<rawString>K. Bretonnel Cohen, Andrew Dolbey, George AcquaahMensah, and Lawrence Hunter. 2002. Contrast and variability in gene names. In Proceedings of the Workshop on Natural Language Processing in the Biomedical Domain, pages 14–20, Philadelphia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
<author>P Ravikumar</author>
<author>S Fienburg</author>
</authors>
<title>A comparison of string metrics for matching names and records.</title>
<date>2003</date>
<booktitle>In KDD Workshop on Data Cleaning and Object Consolidation.</booktitle>
<contexts>
<context position="10854" citStr="Cohen et al. (2003)" startWordPosition="1681" endWordPosition="1684">iations allow arbitrary costs or special costs for starting and continuing a “gap” (i.e., a long sequence of adds or deletes). Token-based approaches include the Jaccard similarity metric and the TF/IDF metric. The methods consider the (possibly weighted) overlap between the tokens of two strings. Hybrid token and character-based are best represented by SoftTFIDF, which includes not only exact token matches but also close matches (using edit-distance, for example). Another approach is to perform the Jaccard similarity (or TF/IDF) between the-grams of the two strings instead of the tokens. See Cohen et al. (2003) for a detailed overview and comparison of some of these methods on different data sets. 3There are two more description names for the human and mouse entries. The SwissProt database has also associated Gene names to those entries which are related to some of the possible names that we find in the literature. Those gene names are: CDKN1A, CAP20, CDKN1, CIP1, MDA6, PIC1, SDI1, WAF1, Cdkn1a, Cip1, Waf1. It can be seen that those names are incorporated in the UMLS as protein names. 11 Recent work has also focused on automatic methods for adapting these string similarity measures to specific data </context>
</contexts>
<marker>Cohen, Ravikumar, Fienburg, 2003</marker>
<rawString>W. Cohen, P. Ravikumar, and S. Fienburg. 2003. A comparison of string metrics for matching names and records. In KDD Workshop on Data Cleaning and Object Consolidation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markove models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="15358" citStr="Collins, 2002" startWordPosition="2406" endWordPosition="2407">e variable values is very large - one for each character in our alphabet (which is on the order of 40 or more including digits and punctuation). Moreover, we typically have very large training sets largely due to the fact that training pairs are derivable from an equivalence class of size. Given this situation, standard training for CRFs becomes unwieldy, since it involves performing inference over the entire data set repeatedly (typically a few hundred iterations are required to converge). , given an observed sequence, as: 12 As such, we resort to an approximation: Voted Perceptron training (Collins, 2002). Voted Perceptron training does not involve maximizing log-likelihood, but instead updates parameters via stochastic gradient descent with a small number of passes over the data. Another consideration that arises is given a pair of strings, which one should be considered the “observed” sequence and which one the “hidden” sequence. Another consideration that arises is given a pair of strings, which string should be considered the “observed” sequence and which the “hidden” sequence?4 We have taken to always selecting the longest string as the “observed” string, as it appears most natural, thoug</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markove models: Theory and experiments with perceptron algorithms. In EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
<author>C Friedman</author>
<author>A Rzhetsky</author>
<author>W Wilbur</author>
</authors>
<title>Automatic extraction of gene and protein synonyms from medline and journal articles.</title>
<date>2002</date>
<booktitle>In Proc AMIA Symposium,</booktitle>
<pages>919--23</pages>
<marker>Yu, Hatzivassiloglou, Friedman, Rzhetsky, Wilbur, 2002</marker>
<rawString>Yu H, Hatzivassiloglou V, Friedman C, Rzhetsky A, and Wilbur W. 2002. Automatic extraction of gene and protein synonyms from medline and journal articles. In Proc AMIA Symposium, pages 919–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung-Jae Kim</author>
<author>Jong C Park</author>
</authors>
<title>Bioar: Anaphora resolution for relating protein names to proteome database entries.</title>
<date>2004</date>
<booktitle>In Sanda Harabagiu and</booktitle>
<pages>79--86</pages>
<editor>David Farwell, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1485" citStr="Kim and Park, 2004" startWordPosition="215" endWordPosition="218">tperform standard metrics in this domain. 1 Introduction 1.1 String Similarity and Reference Resolution String similarity/matching algorithms are used as a component in reference resolution algorithms. We use reference resolution in a broad sense, which includes any of the following aspects: a. Intra-document noun phrase reference resolution. b. Cross-document or corpus reference resolution. c. Resolution of entities found in a corpus with databases, dictionaries or other external knowledge sources. This is also called semantic integration, e.g., (Li et al., 2005), reference grounding, e.g., (Kim and Park, 2004) or normalization, e.g., (Pustejovsky et al., 2002; Morgan et al., 2004). The last two aspects of reference resolution are particularly important for information extraction, and the interaction of reference resolution with information extraction techniques (see for example Bagga (1998)). The extraction of a particular set of entities from a corpus requires reference resolution for the set of entities extracted (e.g., the EDT task in ACE1), and it is apparent that there is more variation in the cross-document naming conventions than in a single document. The importance of edit distance algorith</context>
<context position="5434" citStr="Kim and Park, 2004" startWordPosition="821" endWordPosition="824">lated to reference resolution in this domain has been done in the following areas: a) Intra-document Reference resolution, e.g (Casta˜no et al., 2002; Lin and Liang, 2004) b) Intra-document Named entity recognition (e.g Biocreative Task 1A (Blaschke et al., 2003), and others), also called classification of biological names (Torii et al., 2004) c) Intra-document alias extraction d) cross-document Acronym-expansion extraction, e.g., (Pustejovsky et al., 2001). e) Protein names resolution against database entries in SwissProt, protein name grounding, in the context of a relation extraction task (Kim and Park, 2004). One constraint in these approaches is that they use several patterns for the string matching problem. The results of the protein name grounding are 59% precision and 40% recall. The Biocreative Task 1B task challenged systems to ground entities found in article abstracts which contain mentions of genes in Fly, Mouse and Yeast databases. A central component in this task was resolving ambiguity as many gene names refer to multiple genes. 2.2 String Similarity and Ambiguity In this subsection consider the string similarity issues that are present in the biology domain in particular. The task we</context>
</contexts>
<marker>Kim, Park, 2004</marker>
<rawString>Jung-Jae Kim and Jong C. Park. 2004. Bioar: Anaphora resolution for relating protein names to proteome database entries. In Sanda Harabagiu and David Farwell, editors, ACL 2004: Workshop on Reference Resolution and its Applications, pages 79–86, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="12839" citStr="Lafferty et al. (2001)" startWordPosition="1994" endWordPosition="1997">e arbitrary overlapping (non-independent) features over the entire input space — just like a discriminative classifier. CRFs are log-linear models that compute the probability of a state sequence, where the are arbitrary feature functions, the are the model parameters and is a normalization function. Training a CRF amounts to finding the that maximize the conditional log-likelihood of the data. Given a trained CRF, the inference problem involves finding the most likely state sequence given a sequence of observations. This is done using a slightly modified version of the Viterbi algorithm (See Lafferty et al. (2001) more for details on CRFs). 4.1 CRFs for String Similarity CRFs can be used to measure string similarity by viewing the observed sequence,, and the state sequence,, as sequences of characters. In practice we are presented with two strings, , and of possibly differing lengths. A necessary first step is to align the two strings by applying the Levenstein distance procedure as described earlier. This produces a series of edit operations where each operation has one of three possible forms: 1) (addition), 2) (substitution) and 3) (deletion). The observed and hidden sequences are then derived by re</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282–289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>P Morie</author>
<author>D Roth</author>
</authors>
<title>Semantic integration in text: From ambiguous names to identifiable entities. AI Magazine. Special Issue on Semantic Integration.</title>
<date>2005</date>
<contexts>
<context position="1436" citStr="Li et al., 2005" startWordPosition="207" endWordPosition="210"> methods based on Conditional Random Fields outperform standard metrics in this domain. 1 Introduction 1.1 String Similarity and Reference Resolution String similarity/matching algorithms are used as a component in reference resolution algorithms. We use reference resolution in a broad sense, which includes any of the following aspects: a. Intra-document noun phrase reference resolution. b. Cross-document or corpus reference resolution. c. Resolution of entities found in a corpus with databases, dictionaries or other external knowledge sources. This is also called semantic integration, e.g., (Li et al., 2005), reference grounding, e.g., (Kim and Park, 2004) or normalization, e.g., (Pustejovsky et al., 2002; Morgan et al., 2004). The last two aspects of reference resolution are particularly important for information extraction, and the interaction of reference resolution with information extraction techniques (see for example Bagga (1998)). The extraction of a particular set of entities from a corpus requires reference resolution for the set of entities extracted (e.g., the EDT task in ACE1), and it is apparent that there is more variation in the cross-document naming conventions than in a single d</context>
</contexts>
<marker>Li, Morie, Roth, 2005</marker>
<rawString>X. Li, P. Morie, and D. Roth. 2005. Semantic integration in text: From ambiguous names to identifiable entities. AI Magazine. Special Issue on Semantic Integration.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lin</author>
<author>T Liang</author>
</authors>
<title>Pronominal and sortal anaphora resolution for biomedical literature.</title>
<date>2004</date>
<booktitle>In Proceedings of ROCLINGXVI: Conference on Computational Linguistics and Speech Processing,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4986" citStr="Lin and Liang, 2004" startWordPosition="757" endWordPosition="760">ptive string similarity metrics based on Conditional Random Fields customized to the domain at hand. The resulting best metric, we term SoftTFIDF-CRF, achieves 73.6 F-measure on the task of assigning a given string to the correct concept. Additionally, our experiments demonstrate a tradeoff between efficiency and recall based ongram indexing. 2 Background 2.1 Entity Extraction and Reference Resolution in the Biomedical Domain Most of the work related to reference resolution in this domain has been done in the following areas: a) Intra-document Reference resolution, e.g (Casta˜no et al., 2002; Lin and Liang, 2004) b) Intra-document Named entity recognition (e.g Biocreative Task 1A (Blaschke et al., 2003), and others), also called classification of biological names (Torii et al., 2004) c) Intra-document alias extraction d) cross-document Acronym-expansion extraction, e.g., (Pustejovsky et al., 2001). e) Protein names resolution against database entries in SwissProt, protein name grounding, in the context of a relation extraction task (Kim and Park, 2004). One constraint in these approaches is that they use several patterns for the string matching problem. The results of the protein name grounding are 59</context>
</contexts>
<marker>Lin, Liang, 2004</marker>
<rawString>Y. Lin and T. Liang. 2004. Pronominal and sortal anaphora resolution for biomedical literature. In Proceedings of ROCLINGXVI: Conference on Computational Linguistics and Speech Processing, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In Proceedings of the IJCAI-2003 Workshop on Information Integration on the Web,</booktitle>
<pages>79--86</pages>
<location>Acapulco, Mexico,</location>
<marker>McCallum, Wellner, 2003</marker>
<rawString>Andrew McCallum and Ben Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In Proceedings of the IJCAI-2003 Workshop on Information Integration on the Web, pages 79– 86, Acapulco, Mexico, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Morgan</author>
<author>L Hirschman</author>
<author>M Colosimo</author>
<author>A Yeh</author>
<author>J Colombe</author>
</authors>
<title>Gene name identification and normalization using a model organism database.</title>
<date>2004</date>
<journal>Journal of BiomedicalInformatics,</journal>
<pages>6--396</pages>
<contexts>
<context position="1557" citStr="Morgan et al., 2004" startWordPosition="227" endWordPosition="230">ilarity and Reference Resolution String similarity/matching algorithms are used as a component in reference resolution algorithms. We use reference resolution in a broad sense, which includes any of the following aspects: a. Intra-document noun phrase reference resolution. b. Cross-document or corpus reference resolution. c. Resolution of entities found in a corpus with databases, dictionaries or other external knowledge sources. This is also called semantic integration, e.g., (Li et al., 2005), reference grounding, e.g., (Kim and Park, 2004) or normalization, e.g., (Pustejovsky et al., 2002; Morgan et al., 2004). The last two aspects of reference resolution are particularly important for information extraction, and the interaction of reference resolution with information extraction techniques (see for example Bagga (1998)). The extraction of a particular set of entities from a corpus requires reference resolution for the set of entities extracted (e.g., the EDT task in ACE1), and it is apparent that there is more variation in the cross-document naming conventions than in a single document. The importance of edit distance algorithms has already been noticed, (M¨uller et al., 2002) and the importance o</context>
</contexts>
<marker>Morgan, Hirschman, Colosimo, Yeh, Colombe, 2004</marker>
<rawString>A. Morgan, L. Hirschman, M. Colosimo, A. Yeh, and J. Colombe. 2004. Gene name identification and normalization using a model organism database. Journal of BiomedicalInformatics, (6):396–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
<author>Stefan Rapp</author>
<author>Michael Strube</author>
</authors>
<title>Applying co-training to reference resolution.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>352--359</pages>
<marker>M¨uller, Rapp, Strube, 2002</marker>
<rawString>Christoph M¨uller, Stefan Rapp, and Michael Strube. 2002. Applying co-training to reference resolution. In ACL, pages 352–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>J Casta˜no</author>
<author>B Cochran</author>
<author>M Kotecki</author>
<author>M Morrell</author>
</authors>
<title>Automatic extraction of acronymmeaning pairs from medline databases.</title>
<date>2001</date>
<booktitle>In Proceedings of Medinfo,</booktitle>
<location>London.</location>
<marker>Pustejovsky, Casta˜no, Cochran, Kotecki, Morrell, 2001</marker>
<rawString>J. Pustejovsky, J. Casta˜no, B. Cochran, M. Kotecki, and M. Morrell. 2001. Automatic extraction of acronymmeaning pairs from medline databases. In Proceedings of Medinfo, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>J Casta˜no</author>
<author>J Zhang</author>
<author>R Sauri</author>
<author>W Luo</author>
</authors>
<title>Medstract: creating large-scale information servers from biomedical texts.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Natural Language Processing in the Biomedical Domain,</booktitle>
<pages>85--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia,</location>
<marker>Pustejovsky, Casta˜no, Zhang, Sauri, Luo, 2002</marker>
<rawString>J. Pustejovsky, J. Casta˜no, J. Zhang, R. Sauri, and W. Luo. 2002. Medstract: creating large-scale information servers from biomedical texts. In Proceedings of the Workshop on Natural Language Processing in the Biomedical Domain, pages 85–92, Philadelphia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Torii</author>
<author>S Kamboj</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Using nameinternal and contextual features to classify biological terms.</title>
<date>2004</date>
<journal>Journal of Biomedical Informatics,</journal>
<pages>498--511</pages>
<contexts>
<context position="5160" citStr="Torii et al., 2004" startWordPosition="783" endWordPosition="786">on the task of assigning a given string to the correct concept. Additionally, our experiments demonstrate a tradeoff between efficiency and recall based ongram indexing. 2 Background 2.1 Entity Extraction and Reference Resolution in the Biomedical Domain Most of the work related to reference resolution in this domain has been done in the following areas: a) Intra-document Reference resolution, e.g (Casta˜no et al., 2002; Lin and Liang, 2004) b) Intra-document Named entity recognition (e.g Biocreative Task 1A (Blaschke et al., 2003), and others), also called classification of biological names (Torii et al., 2004) c) Intra-document alias extraction d) cross-document Acronym-expansion extraction, e.g., (Pustejovsky et al., 2001). e) Protein names resolution against database entries in SwissProt, protein name grounding, in the context of a relation extraction task (Kim and Park, 2004). One constraint in these approaches is that they use several patterns for the string matching problem. The results of the protein name grounding are 59% precision and 40% recall. The Biocreative Task 1B task challenged systems to ground entities found in article abstracts which contain mentions of genes in Fly, Mouse and Ye</context>
</contexts>
<marker>Torii, Kamboj, Vijay-Shanker, 2004</marker>
<rawString>M. Torii, S. Kamboj, and K. Vijay-Shanker. 2004. Using nameinternal and contextual features to classify biological terms. Journal of Biomedical Informatics, pages 498–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>G Zhou</author>
<author>J Su</author>
<author>C L Tan</author>
</authors>
<title>Improving noun phrase coreference resolution by matching strings.</title>
<date>2004</date>
<booktitle>In Proceedings of 1st Internation Joint Conference of Natural Language Processing,</booktitle>
<pages>326--333</pages>
<contexts>
<context position="2266" citStr="Yang et al., 2004" startWordPosition="335" endWordPosition="338">xtraction, and the interaction of reference resolution with information extraction techniques (see for example Bagga (1998)). The extraction of a particular set of entities from a corpus requires reference resolution for the set of entities extracted (e.g., the EDT task in ACE1), and it is apparent that there is more variation in the cross-document naming conventions than in a single document. The importance of edit distance algorithms has already been noticed, (M¨uller et al., 2002) and the importance of string similarity techniques in the biomedical domain has also been acknowledged, e.g., (Yang et al., 2004). String similarity/matching algorithms have also been used extensively in related problems such as Name databases and similar problems in structured data, see (Li et al., 2005) and references mentioned therein. The problem of determining whether two similar strings may denotate the same entity is particularly challenging in the biomedical literature. It has already been noticed (Cohen et al., 2002) that there is great variation in the naming conventions, and noun phrase constructions in the literature. It has also been noticed that bio-databases are hardly ever updated with the names in the l</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2004</marker>
<rawString>X. Yang, G. Zhou, J. Su, and C. L. Tan. 2004. Improving noun phrase coreference resolution by matching strings. In Proceedings of 1st Internation Joint Conference of Natural Language Processing, pages 326–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>E Agichtein</author>
</authors>
<title>Extracting synonymous gene and protein terms from biological literature.</title>
<date>2003</date>
<journal>Bioinformatics,</journal>
<pages>340--349</pages>
<contexts>
<context position="25148" citStr="Yu and Agichtein, 2003" startWordPosition="4025" endWordPosition="4028"> performs fairly. The graphs illustrate a tradeoff between efficiency and accuracy (recall). Lower-gram match ratios return fewer candidates with correspondingly fewer pairwise string similarities to compute. Precision actually peaks with a-gram match ratio of around 0.2. Recall tapers off even up to high q-gram levels for all metrics, indicating that nearly 30% of the test instances are probably too difficult for any string similarity metric. Error analysis indicates that these cases tend to be entries involving synonymous “nicknames”. Acquiring such synonyms requires other machinery, e.g., (Yu and Agichtein, 2003). 7 Conclusions We have explored a set of string similarity metrics in the biological domain in the service of reference resolution. String similarity is only one parameter to be considered in this task. We presented encouraging results for assigning strings to UMLS concepts based solely on string similarity metrics — demonstrating that adaptive string similarity metrics show significant promise for biomedical text processing. Further progress will require a system that 1) utilizes context of occurrence of respective strings for handling ambiguity and 2) further improves recall &apos;Inspection of </context>
</contexts>
<marker>Yu, Agichtein, 2003</marker>
<rawString>H. Yu and E. Agichtein. 2003. Extracting synonymous gene and protein terms from biological literature. Bioinformatics, pages 340–349.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>