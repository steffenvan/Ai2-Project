<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006707">
<note confidence="0.805034">
PROJECT APRIL — A PROGRESS REPORT
</note>
<author confidence="0.920135">
Robin Haigh, Geoffrey Sampson, Eric Atwell
</author>
<affiliation confidence="0.9472625">
Centre for Computer Analysis of Language and Speech,
University of Leeds,
</affiliation>
<address confidence="0.78283">
Leeds LS2 9JT, UK
</address>
<sectionHeader confidence="0.640606" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999882655172414">
Parsing techniques based on rules defining
grammaticality are difficult to use with authentic
inputs, which are often grammatically messy.
Instead, the APRIL system seeks a labelled tree
structure which maximizes a numerical measure
of conformity to statistical norms derived from a
sample of parsed text. No distinction between
legal and illegal trees arises: any labelled tree
has a value. Because the search space is large
and has an irregular geometry, APRIL seeks the
best tree using simulated annealing, a stochastic
optimization technique. Beginning with an arbi-
trary tree, many randomly-generated local
modifications are considered and adopted or
rejected according to their effect on tree-value:
acceptance decisions are made probabilistically,
subject to a bias against adverse moves which is
very weak at the outset but is made to increase
as the random walk through the search space
continues. This enables the system to converge
on the global optimum without getting trapped
in local optima. Performance of an early ver-
sion of the APRIL system on authentic inputs is
yielding analyses with a mean accuracy of
75.3% using a schedule which increases pro-
cessing linearly with sentence-length;
modifications currently being implemented
should eliminate a high proportion of the
remaining errors.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.992679944444445">
Project APRIL (Annealing Parser for Realis-
tic Input Language) is constructing a software
system that uses the stochastic optimization
technique known as &amp;quot;simulated annealing&amp;quot;
(Kirkpatrick et al. 1983, van Laarhoven &amp; Aarts
1987) to parse authentic English inputs by seek-
ing labelled tree-structures that maximize a
measure of plausibility defined in terms of
empirical statistics on parse-tree configurations
drawn from a database of manually parsed
English text. This approach is a response to the
fact that &amp;quot;real-life&amp;quot; English, such as the
material in the Lancaster-Oslo/Bergen Corpus
on which our research focuses, does not appear
to conform to a fixed set of grammatical rules.
(On the LOB Corpus and the research back-
ground from which Project APRIL emerged, see
Garside et al. (1987). A crude pilot version of
the APRIL system was described in Sampson
(1986).)
Orthodox computational linguistics is
heavily influenced by a concept of language
according to which the set of all strings over the
vocabulary of the language is partitioned into a
class of grammatical strings, which possess ana-
lyses all parts of which conform to a finite set
of rules defining the language, and a class of
strings which are ungrammatical and for which
the question of their grammatical structure
accordingly does not arise. Even systems which
set out to handle &amp;quot;deviant&amp;quot; sentences com-
monly do so by referring them to particular
&amp;quot;non-deviant&amp;quot; sentences of which they are
deemed to be distortions. In our work with
authentic texts, however, we find the &amp;quot;gramma-
ticality&amp;quot; concept unhelpful. It frequently hap-
pens that a word-sequence occurs which violates
some recognized rule of English grammar, yet
any reader can understand the passage without
difficulty, and it often seems unlikely that most
readers would notice the violation. Further-
more, a problem which is probably even more
troublesome for the rule-based approach is that
there is an apparently endless diversity of con-
structions that no-one would be likely to
describe as ungrammatical or deviant. Impres-
sionistically it appears that any attempt to state
a finite set of rules covering everything that
occurs in authentic English text is doomed to go
on adding more rules as long as more text is
examined; Sampson (1987) adduced objective
evidence supporting this impression.
Our approach, therefore, is to define a func-
tion which associates a figure of merit with any
</bodyText>
<page confidence="0.998161">
104
</page>
<bodyText confidence="0.999592785714286">
possible tree having labels drawn from a recog-
nized alphabet of grammatical category-
symbols; any input sentence is parsed by seek-
ing the highest-valued tree possible for that sen-
tence. The analysis process works the same
way, whether the input is impeccably grammati-
cal or quite bizarre. No contrast between legal
and illegal labelled trees arises: a tree which
would ordinarily be described as thoroughly ille-
gal is in our terms just a tree whose figure of
merit is relatively very poor.
This conception of parsing as optimization
of a function defined for all inputs seems to us
not implausible as a model of how people
understand language. But that is not our con-
cern; what matters to us is that this model
seems very fruitful for automatic language-
processing systems. It has a theoretical disad-
vantage by comparison with rule-based
approaches: if an input is perfectly grammatical
but contains many out-of-the-way (Le. low fre-
quency) constructions, the correct analysis may
be assigned a low figure of merit relative to
some alternative analysis which treats the sen-
tence as an imperfect approximation to a struc-
ture composed of high-frequency constructions.
However, our experience is that, in authentic
English, &amp;quot;trick sentences&amp;quot; of this kind tend to
be much rarer than textbooks of theoretical
linguistics might lead one to imagine. Against
this drawback our approach balances the advan-
tage of robustness. No input, no matter how
bizarre, can can cause our system simply to fail
to return any analysis. Our sponsors, the Royal
Signals and Radar Establishment (an agency of
the U.K. Ministry of Defence)1 are principally
interested in speech analysis, and arguably this
robustness should be even more advantageous
for spoken language, which makes little use of
constructions that are legitimate but recherché,
while it contains a great deal that is sloppy or
incorrect.
</bodyText>
<sectionHeader confidence="0.70876" genericHeader="method">
PARSING SCHEME
</sectionHeader>
<bodyText confidence="0.960521894736842">
Any automatic parser needs some external
standard against which its output is judged. Our
&amp;quot;target&amp;quot; parses are those given by a scheme
previously evolved for analysis of LOB Corpus
material, which is sketched in Garside et aL
1 Project APRIL has been spcsisored since De-
cember 1986 under ccatract MOD2062/0128(RSRE):
we are grateful to the Ministry of Defence for permis-
sion to publish this paper.
(1987, chap. 7) and laid down in minute detail
in unpublished documentation. This scheme
was applied in manually parsing sentences total-
ling ca 50,000 words drawn from the various
LOB genres: this TreeBank, as we call it, also
serves as our source of grammatical statistics.
A major objective in the definition of the pars-
ing scheme and the construction of the
TreeBank was consistency: wherever alternative
analyses of a complex construction might be
suggested (as a matter of analytic style as
opposed to genuine ambiguity in sense), the
scheme aims to stipulate which of the alterna-
tives is to be used. It is this need to ensure the
greatest possible consistency which sets a practi-
cal limit to the size of the available database;
producing the TreeBank took most of one
teacher&apos;s research time for two years.
The parses yielded by the TreeBank scheme
art immediate-constituent analyses of conven-
tional type: they were designed so far as possi-
ble to be theoretically uncontroversial. They
were not designed to be especially convenient
for stochastic parsing, which we had not at that
time thought of.
The prior existence of the TreeBank is also
the reason why we are working with written
language rather than speech: at present we have
no equivalent resource for spoken English.
</bodyText>
<sectionHeader confidence="0.9983625" genericHeader="method">
THE PRINCIPLES OF SIMULATED
ANNEALING
</sectionHeader>
<bodyText confidence="0.999953238095238">
To explain how APRIL works, two chief
issues must be clarified. One is the simulated
annealing technique used to locate the highest-
valued tree in the set of possible labelled trees;
the other is the function used to evaluate any
such tree.
We will begin by explaining the technique
of simulated annealing. This technique uses
stochastic (randomizing) methods to locate good
solutions; it is now widely exploited, in domains
where combinatorial explosion makes the search
space too vast for exhaustive examination,
where no algorithm is available which leads sys-
tematically to the optimal solution, and where
there is a considerable degree of &amp;quot;frustration&amp;quot;
in the sense of Toulouse (1977), meaning that a
seeming improvement in one feature of a solu-
tion often at the same time worsens some other
feature of the solution, so that the problem can-
not be decomposed into small subproblems
which can each be optimized separately. (Com-
</bodyText>
<page confidence="0.998743">
105
</page>
<bodyText confidence="0.999993253521127">
pare how, in parsing, deciding to attach a con-
stituent A as a daughter of a constituent B may
be a relatively attractive way of &amp;quot;using up&amp;quot; A,
at the cost of making B a less plausible consti-
tuent than it would be without A.)
One simple optimization technique, iterative
improvement, begins by selecting a solution
arbitrarily and then makes a long series of small
modifications, drawn from a class of
modifications which is defined in such a way
that any point in the solution-space can be
reached from any other point by a chain of
modifications each belonging to the class. At
each step the value of the solution obtained by
making some such change is compared with the
value of the current solution. The change is
accepted and the new solution becomes current
if it is an improvement; otherwise the change is
rejected, the existing solution retained, and an
alternative modification is tried. The process
terminates on reaching a solution superior to
each of its neighbours, i.e. when none of the
available modifications is an improvement.
As it stands, such a technique is useless for
parsing. It is too easy for the system to become
trapped at a point which is better than its
immediate neighbours but which is by no means
the best solution overall, i.e. at a local but not a
global optimum.
Simulated annealing is a variant which deals
with this difficulty by using a more sophisti-
cated rule for deciding whether to accept or
reject a modification. In the variant we use, a
favourable step is always accepted; but an
unfavourable step is rejected only if the loss of
merit resulting from the step exceeds a certain
threshold. This acceptance threshold is ran-
domly generated at each step from a biassed
distribution; it may at any time be very high or
very low, but its mean value is made to
decrease in accordance with some defined
schedule as the iteration proceeds, so that ini-
tially almost all moves are accepted, good or
bad, but moves which are severely detrimental
soon start to be rejected, and in the later stages
almost all detrimental moves are avoided. This
scheme was originally devised as a simulation
of the thermodynamic processes involved in the
slow cooling of certain materials, hence the
name &amp;quot;simulated annealing&amp;quot;. Accepting
modifications which worsen the current tree is at
first sight a surprising idea, but such moves
prevent the system getting stuck and instead
open up new possibilities; at the same time,
there is an inexorable overall trend towards
improvement. As a result, the system tends to
seek out high-valued areas of the solution space
initially in terms of gross features, and later in
terms of progressively finer detail. Again, the
process terminates at a local optimum, but not
before exploring the possibilities so thoroughly
that this is in general the global optimum. With
certain simplifying assumptions, it has been
shown mathematically that the global optimum
is always found (Lundy &amp; Mees, 1986): in prac-
tice, the procedure appears to work well under
rather less stringent conditions than those
demanded by mathematical treatments that have
so far appeared, and our application does in fact
take several liberties with the &amp;quot;pure&amp;quot; algorithm
as set out in the literature.
</bodyText>
<sectionHeader confidence="0.994085" genericHeader="method">
ANNEALING PARSE-TREES
</sectionHeader>
<bodyText confidence="0.999968666666667">
To apply simulated annealing to a given
problem, it is necessary to define (a) a space of
possible solutions, (b) a class of solution
modifications which provides a route from any
point in the space to any other, and (c) an
annealing schedule (i.e. an initial value for the
mean acceptance threshold, a specification of
the rate at which this mean is reduced, and a
criterion for terminating the process).
</bodyText>
<subsectionHeader confidence="0.680421">
Solution space
</subsectionHeader>
<bodyText confidence="0.990846666666667">
• For us, the solution space for an input sen-
tence n words long is the set of all rooted
labelled trees having n leaves, in which the leaf
nodes are labelled with the word-class codes
corresponding to the words of the sentence (for
test inputs drawn from LOB, these are the codes
given in the Tagged version of the LOB corpus)
and the non-terminal nodes have labels drawn
from the set of grammatical-category labels
specified in the parsing scheme. The root node
of a tree is assigned a fixed label, but any other
non-terminal node may bear any category label.
</bodyText>
<subsectionHeader confidence="0.613367">
Move set
</subsectionHeader>
<bodyText confidence="0.9999806">
A set of possible parse-tree modifications
allowing any tree to be reached from any other
can be defined as follows. To generate a
modification, pick a non-terminal node of the
current tree at random. Choose at random one
of the move-types Merge or Hive. If Merge is
chosen, delete the chosen node by replacing it,
in its mother&apos;s daughter-sequence, with its own
daughter-sequence. If the move-type is Hive,
choose a random continuous subsequence of the
</bodyText>
<page confidence="0.991781">
106
</page>
<bodyText confidence="0.994174536585366">
node&apos;s daughter-sequence, and replace that
subsequence by a new node having the subse-
quence as its own daughter-sequence; assign a
label drawn from the non-terminal alphabet to
the new node. It is easy to see that the class of
Merge and Hive moves allows at least one route
from any tree to any other tree over the same
leaf-sequence: repeated Merging will ultimately
turn any tree into the &amp;quot;flat tree&amp;quot; in which every
leaf is directly dominated by the root, and since
Merge and Hive moves mirror one another, if it
is possible to get from any tree to the flat tree it
is equally possible to get from the flat tree to
any tree. (In reality, there will be numerous
alternative routes between a given pair of trees,
most of which will not pass through the flat
tree.)
New labels for nodes created by Hive moves
are chosen randomly, with a bias determined by
the labels of the daughter-sequence. This bias
attempts to increase the frequency with which
correct labels are chosen, without limiting the
choice to the label which is best for the
daughter-sequence considered in isolation,
which may not of course be the best in context.
An early version of APRIL limited itself to
just the Merge and Hive moves. However, a
good move-set for annealing should not only
permit any solution to be reached from any
other solution, but should also be such that
paths exist between good trees which do not
involve passing through much inferior inter-
mediate stages. (See for example the remarks
on depth in Lundy &amp; Mees (1986).) To
strengthen this tendency in our system it has
proved desirable to add a third class of Reattach
moves to the move-set To generate a Reattach
move, choose randomly any non-root node in
the current tree, eliminate the arc linking the
chosen node to its mother, and insert an arc
linking it to a node randomly chosen from the
set of nodes topologically capable of being its
mother. Currently, we are exploring the cost-
effectiveness of adding a fourth move-type,
which relabels a randomly-chosen node without
changing the tree shape; a task for the future is
to investigate how best to determine the propor-
tions in which different move-types are gen-
erated.
Schedule
The annealing schedule is ultimately a
compromise between processing time and qual-
ity of results: although the process can be
speeded up at will, inevitably speeding up too
much will make the system more likely to con-
verge on a false solution when presented with a
difficult sentence. Optimizing the schedule is a
topic to which much attention has been paid in
the literature of simulated annealing, but it
seems fair to say that the discussion remains
inconclusive. Since it does not in general bear
on the specifically linguistic aspects of our pro-
ject, we have deferred detailed consideration of
this issue. We intend however to look at the
variation in rate with respect to type of input,
exploiting the division of the TreeBank (like its
parent LOB Corpus) into genres: we would
expect that the simple if sometimes messy sen-
tences of dialogue in fiction, for instance, can be
dealt with more quickly than the precise but tor-
tuous grammar of legal prose.
At present, then, we reduce the acceptance
threshold at a constant rate which errs on the
slow side; we expect that important advances in
efficiency will result from improvements in the
schedule, but such improvements may be over-
taken by other developments to be described in
later sections. The rate of decrease of the
acceptance threshold is varied inversely with the
length of the sentence, with the consequence
that the run time varies roughly linearly with
sentence length.
</bodyText>
<sectionHeader confidence="0.98662" genericHeader="method">
EVALUATING PARSE-TREES
</sectionHeader>
<bodyText confidence="0.999986227272727">
The function of the evaluation system is to
assign a value to any labelled tree whatsoever,
in such a way that the correct parse-tree for any
given sentence is the highest-valued tree which
can be drawn over the sentence, and the values
of other trees over the same sentence reflect
their relative merit (though comparisons of
values between trees drawn over different sen-
tences are not requited to be meaningful).
An advantage of the annealing technique is
that in principle it makes no demands on the
form of evaluation: in particular, we are not
constrained by the nature of the parsing algo-
rithm to assume that the grammar of English is
context-free or has any other special property.
Nevertheless, we have found it convenient in
our early work to start with a context-free
assumption and work forward from that
With this assumption, a tree can be treated
as a set of productions m—).11 dz... d.
corresponding to the various nodes in the tree,
where in is a non-terminal label and each di is
</bodyText>
<page confidence="0.99512">
107
</page>
<bodyText confidence="0.999195912500001">
either a non-terminal label or a wordtag, and we
can assign to any such production a probability
representing the frequency of such productions,
as a proportion of all productions having m as
mother-label; the value assigned to the entire
wee will be the product of the probabilities of
its productions.
The statistic required for any production,
then, is an estimate of its probability of
occurrence, and this may be derived from its
frequency in the manually-parsed TreeBank.
(To avoid circularity, sentences in the TreeBank
which are to be used to test the performance of
the parser are excluded from the frequency
counts.) Clearly, with a database of this size,
the figures obtained as production probabilities
will be distorted by sampling effects. In gen-
eral, even quite large sampling errors have little
influence on results, since the frequency con-
trasts between alternative tree-structures tend to
be of a higher order of magnitude, but
difficulties arise with very low frequency pro-
ductions: in particular, as an important special
case, many quite normal productions will fail to
occur at all in the TreeBank, and are thus not
distinguished in our raw data from virtually-
impossible productions. But it seems reasonable
to infer probability estimates for unobserved
productions from those of similar, observed pro-
ductions, and more generally to smooth the raw
frequency observations using statistical tech-
niques (see for instance Good (1953)). (One
consequence of such smoothing is that no pro-
duction is ever assigned a probability of zero.)
A natural response by linguists would be to say
that a relationship of &amp;quot;similarity&amp;quot; between pro-
ductions needs to be defined in terms of subtle,
complex theoretical issues. However, so far we
have been impressed by results obtainable in
practice using very crude similarity relation-
Our current evaluation method is only
slightly more elaborate than the technique
described in Sampson (1986), whereby the pro-
bability of a production was derived exclusively
from the observed frequencies of the various
pairwise transitions between daughter-labels
within the production (that is, for any produc-
tion m —&gt;do d d,,41, where do and d,..1 are
boundary symbols, the estimated probability was
the product of the observed frequencies of the
various transitions m—+... d, d1+1 ... (0 n)
with zeroes replaced by small positive values).
This approach was suggested by the success of
the CLAWS system for grammatically disambi-
guating words in context (Garside et al. 1987,
chap. 3), which uses an essentially Markovian
model, and by the success of Markovian tech-
niques in automatic speech understanding
research from the Harpy project onwards (e.g.
Lea 1980, Cravero et al. 1984).
Subsequent versions of APRft. have begun
to incorporate an evaluation measure which
makes limited use of non-Markovian relation-
ships. Each label in the non-terminal alphabet
is associated with a transition network, each arc
of which is assigned a probability as well as a
(non-terminal or terminal) label: the probability
estimate for a node labelled m is the product of
the probabilities of the consecutive arcs in the
transition network for m which carry the labels
of the node&apos;s daughter-sequence. Unlike the
FSAs commonly used in computational linguis-
tics, ours are required to accept any label-
sequence: a &amp;quot;crazy&amp;quot; sequence will be assigned
a low but non-zero value. Indeed our networks
make no attempt to reflect subtle nuances of
grammaticality; they diverge from Markovian
networks only to represent a limited number of
fundamental issues that are lost in a pure Mar-
kovian system.
</bodyText>
<sectionHeader confidence="0.99667" genericHeader="method">
APRIL IN ACTION
</sectionHeader>
<bodyText confidence="0.9999924">
It is rather difficult to convey non-
mathematically a feel for the way in which the
system converges from an arbitrary tree to the
correct tree by a sequence of random moves. In
the earliest stages, labelled nodes are being
created, moved and destroyed at a rapid rate in
all regions of the wee, but after a while it starts
to become apparent that certain local features
are tending to persist. These tend to be the
most strongly marked features grammatically,
such as constituents comprising a single pro-
noun or an auxiliary verb. While such a feature
persists, surrounding developments are con-
strained by it other new nodes can be created if
they are compatible, but new nodes which
would conflict cannot appear. Thus the gram-
matical words form a skeleton on which the
phrases and clauses can start to hang, and we
find there is a perceptible gradually increasing
tendency for the tree to consist of nodes and
substructures which fit together well into a
coherent whole. Speaking anthropomorphically,
the system tends to make the simplest and most
clear-cut decisions first, and the more subtle
decisions later. But the strength of the system
</bodyText>
<page confidence="0.998385">
108
</page>
<bodyText confidence="0.999516666666667">
lies in the fact that no such decision is final:
each is constantly being reappraised in the light
of developments in its surroundings.
</bodyText>
<sectionHeader confidence="0.933239" genericHeader="method">
CURRENT PERFORMANCE
</sectionHeader>
<bodyText confidence="0.999890466019418">
In order to assess APRLL&apos;s performance we
need an objective way to compare output with
target parses, i.e. a measure of similarity
between pairs of distinct trees over the same
sequence of leaf nodes. We know of no stan-
dard measure for this, but we have evolved one
that seems natural and fair. For each word of
input we compare the chains of node-labels
between leaf and root in the two trees, and com-
pute the number of labels which match each
other and occur in the same order in the two
chains as a proportion of all labels in both
chains; then we average over the words. (We
omit discussion of a refinement included in
order to ensure that only fully-identical tree-
pairs receive 100% scores.) With respect to our
parsing technique, this performance measure is
conservative, since averaging over words means
that high-level nodes, dominating many words,
contribute more than low-level nodes to overall
scores, but APRIL tends to discover structure in
a broadly bottom-up fashion.
At the time of writing, our latest results
were those of a test run carried out in early
February 1988, 14 months into a 36-month pro-
ject, over 50 LOB sentences drawn from techni-
cal prose and fiction, with mean, minimum, and
maximum lengths of 22.4, 3, and 140 words
respectively. (Note that our parsing scheme,
and therefore our word-counts, treat punctuation
marks as separate &amp;quot;words&amp;quot;.) The alphabet of
non-terminal labels from which APRIL chooses
when labelling new nodes included virtually all
the distinctions required by our scheme in an
adequately parsed output; and it included
several of the more significant phrase-
subcategory distinctions whose role in the
scheme is to guide the parser towards the
correct output rather than to appear in the out-
put (Garside et al. 1987, p. 89). Altogether the
non-terminal alphabet included 113 distinct
labels.
For a 22-word sentence, the number of dis-
tinct trees with labels drawn from a 113-
member alphabet (and obeying the restrictions
our scheme places on the occurrence of nodes
with only single daughters) is about 5x101°3.
To put this in perspective, finding a particular
labelled tree in a search space of this size is like
finding a single atom of gold in a solid cube of
gold a thousand million light-years on a side.
Mean score of the 50 output analyses was
75.3%. This is not yet good enough for incor-
poration into practical language-processing
application software, but bearing in mind the
preliminary nature of the current version of the
system we are heartened by how good the
scores already are. Furthermore, above about
15 words there appears to be no correlation
between sentence-length and output score,
offering a measure of support for our decision
to use an annealing schedule which increases
processing time roughly linearly with input
length. Kirkpatrick et al. (1983) suggest that
linear processing is adequate for simulated
annealing in other domains, but orthodox deter-
ministic approaches to computational linguistics
do not permit linear parsing except for highly
artificial well-behaved languages.
The parse-trees produced in this test run typ-
ically show a substantially correct overall struc-
ture, with isolated local areas of difficulty where
some deviant analysis has been preferred, com-
monly a constituent wrongly labelled or a con-
stituent attached to the surrounding tree at the
wrong level. An encouraging point is that a
number of these errors relate to debatable gram-
matical issues and might not be seen as errors at
all. In the years when our target parsing
scheme was being evolved, we worried about
the idiomatic construction to try and (do some-
thing]: should try and Verb be grouped as a
constituent equivalent to a single verb? We
finally decided not: we chose to analyse such
sequences as co-ordinated clauses. But, where
the test sentences include the sequence I want to
try and find properties that..., APRIL has
parsed: I want (Ti to (VB&amp; try and find] proper-
ties that...J—the analysis which we came close
to choosing as correct
A sentence which raises less trivial issues is
illustrated (this is from text E23 in the LOB
Corpus). We show the manual parse in the
TreeBank (Fig.!), and APRIL&apos;s current output
(Fig. 2), which contains two errors. First, the
final phrase of the human mind should be
=ached as a postmodifier of mysteries. At this
stage no distinction was made in word-tagging
between of and other prepositions: there is how-
ever a strong tendency (though no absolute rule,
of course) for an of phrase following a noun to
be a postmodifier of the noun, and it is
correspondingly rare for such a phrase to be an
</bodyText>
<page confidence="0.993649">
109
</page>
<figure confidence="0.9984992">
Cr
z
1=-Ecazt-E - 3 g°- 12 3 LZE 3 :&apos;,;E -§:i? 3 cl 2,E•
&lt; n.
4 73&apos;51122.2 g -2. 2 &apos;2 S 2 - 2 STj •
o E E
TD a
-0
co
4 7211122 Cl 5 -2.2= 2 2 - 2 :2-5 2 5 2 •
to
o E co co — g E
2.3 7420.:i.:
0. U3
a.
9.
1
a.
a.
a.
</figure>
<page confidence="0.988397">
110
</page>
<bodyText confidence="0.999988240740741">
immediate constituent of a clause. Distinguish-
ing of from other prepositions will enable the
evaluation system to incorporate a representa-
tion of this piece of statistical evidence in its
transition probabilities, whereupon this error
should be avoided.
Secondly, APRIL has rejected the interpreta-
tion of the clause beginning representing... as a
postrnodifier of tulle, and has chosen to make
this clause appositional to the clause beginning
placing... (our scheme represents apposition in a
manner akin to subordination). This error can
be avoided if we note the strong tendency in
English (again, not an absolute rule) that
postmodifiers of any kind are most often
attached to the nearest element that they can
logically postmodify, that is, that the chain-
structure typified in Fig. 1 is preferred to the
embedding-structure in Fig. 2. A preliminary
statistical analysis of the TreeBank appears to
support the conjecture—developed from the
hypothesis formulated by Yngve (1960)—that
&amp;quot;the greater the depth of a non-terminal consti-
tuent, the greater the probability that either (a)
this constituent is the last daughter of its
mother, or (b) the next daughter of its mother is
a punctuation mark.&amp;quot; (We adapt Yngve&apos;s
notion of depth to non-binary trees.) With this
formulation it is relatively easy to incorporate
into our evaluation system the necessary adjust-
ments to our transition probabilities, so that
trees of the more common type will tend to be
preferred; but note that nothing prevents an
overriding local consideration from leading the
parser to prefer, in any given case, an analysis
that departs from this general principle. When
this is done, the initial context-free assumption
will have been abandoned, to the extent that
depths of constituents are taken into account as
well as their labels, but no change is needed in
the parsing algorithm.
The erroneous parsings in this example flout
no rules of syntax that we can formulate and
seem to involve no impossible productions, so
they could be regarded as valid alternatives in a
syntactically ambiguous sentence: a generative
grammar could be expected to generate this sen-
tence in several different ways, of which
APRIL&apos;s would be one. However, as our
methods improve we find that more and more
sentences which are in principle ambiguous
have the same reading selected by purely
statistical-syntactic considerations as is preferred
by human readers, who also have access to
</bodyText>
<sectionHeader confidence="0.673584" genericHeader="evaluation">
semantic and pragmatic considerations.
FUTURE DEVELOPMENTS
</sectionHeader>
<bodyText confidence="0.999995764705883">
Apart from improving the evaluation system
as already discussed, we plan in the near future
to adapt APRIL SO that it accepts raw text rather
than sequences of word-class codes as input,
choosing tags for grammatically ambiguous
words as part of the same optimization process
by which higher structure is discovered. The
availability of the (probabilistic but determinis-
tic) CLAWS word-tagging system meant that
this was not seen as an initial priority. Raw
text input involves a number of problems relat-
ing to orthographic matters such as capitaliza-
tion and hyphenated words, but these problems
have essentially been solved by our Lancaster
colleagues (Garside et al., chap. 8). We also
intend soon to move from the current static sys-
tem whose inputs are isolated sentences to a
dynamic system within which annealing will
take place in a window that scans across con-
tinuous text, with the system discovering
sentence-boundaries for itself along with lower-
level structure. (If our system is in due course
adapted to parse spoken rather than written
input, it is clear that all constituent boundaries
including those of sentences would need to be
discovered rather than given, and a corollary
appears to be that the processing time needed
for any length of input must increase only
linearly with input length.) As adumbrated in
Sampson (1986), we expect to make the
dynamic annealing parser more efficient by
exploiting the insight of Marcus (1980) that
backtracking .is rarely needed in natural
language parsing: a gradient of processing inten-
sity will be imposed on the annealing window,
with most processing occurring in the &amp;quot;newest&amp;quot;
parts of the current tree where valuable moves
are most likely to be found.
However, simulated annealing is necessarily
costly in terms of amount of processing needed.
(The schedule used for the run discussed above
involved on the order of 30,000 steps generated
per input word.) Particularly with a view to
applications such as real-time speech analysis, it
would be desirable to find a way of exploiting
parallel processing in order to minimize the
time needed for parse-tree optimization.
Parallelizing our approach to parsing is not a
straightforward matter: one cannot, for instance,
simply associate a process with each node of a
tree, since there is no natural identity relation-
</bodyText>
<page confidence="0.99555">
111
</page>
<bodyText confidence="0.999972514285714">
ship between nodes in different trees within the
solution space for an input. However, we have
evolved an algorithm for concurrent tree anneal-
ing which we believe should be efficient, and a
research proposal currently under consideration
will implement this algorithm, using a transputer
array which is about to be installed by a consor-
tium of Leeds departments. In view of the
widespread occurrence of hierarchical structures
in cognitive science, we hope that a successful
solution to the problem of parallel tree-
optimization should be of interest to workers in
other areas, such as image processing, as well as
to linguists.
Lastly, a reasonable criticism of our wort so
far is that our target parses are those defined by
a purely &amp;quot;surfacy&amp;quot; parsing scheme. For some
speech-processing applications surface parsing is
adequate, but for many purposes deeper
language analyses are needed. We see no issue
of principle hindering the extension of our
methods to deep parsing, but at present there is
a serious practical hindrance: our techniques can
only be applied after a target parsing scheme
has been specified in sufficient detail to
prescribe unambiguous analyses for all
phenomena occurring in authentic English, and
then applied manually to a large enough quan-
tity of text to yield usable statistics. A second
currently-pending research proposal plans to
convert the Gothenburg Corpus (Ellegird 1978),
which consists of relatively deep manual pars-
ings of 128,000 words of the Brown Corpus of
American English, into a database usable for
this purpose.
</bodyText>
<sectionHeader confidence="0.999696" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.995856142857143">
Craver°, M., et al. 1984. &amp;quot;Syntax driven
recognition of connected words by Markov
models&amp;quot;. Proceedings of the 1984 IEEE Inter-
national Conference on Acoustics, Speech and
Signal Processing.
Elleggrd, A. 1978. The Syntactic Structure of
English Texts. Gothenburg Studies in English,
43.
Garside, R. G., et al., eds. 1987. The Computa-
tional Analysis of English. Longman.
Good, I. J. 1953. &amp;quot;The population frequencies
of species and the estimation of population
parameters&amp;quot;. Biometrika 40.237-64.
Kirkpatrick, S. E., et al. 1983. &amp;quot;Optimization
by Simulated Annealing&amp;quot;. Science 220.671-80.
van Laarhoven, P. J. M., &amp; E. H. L. Aarts.
1987. Simulated Annealing: Theory and Appli-
cations. D. Reidel.
Lea, R. G„ ed. 1980. Trends in Speech Recog-
nition. Prentice-Hall.
Lundy, M. and A. Mees. 1986. &amp;quot;Convergence
of an annealing algorithm&amp;quot;. Mathematical Pro-
gramming 34.111-24.
Marcus, M. P. 1980. A Theory of Syntactic
Recognition for Natural Language. MIT Press.
Sampson, G. R. 1986. &amp;quot;A stochastic approach
to parsing&amp;quot;. Proceedings of the I I th Interna-
tional Conference on Computational Linguistics
(COLING &apos;86), pp. 151-5. [GRS wishes to take
this opportunity to apologize for the inadvertent
near-coincidence of title between this paper and
an important 1984 paper by T. Fujisaki.]
Sampson, G. R. 1987. &amp;quot;Evidence against the
&apos;grammatical&apos;/&apos;ungrammatical&apos; distinction&amp;quot;. In
W. Meijs, ed., Corpus Linguistics and Beyond.
Rodopi.
Toulouse, G. 1977. &amp;quot;Theory of the frustration
effect in spin glasses. I.&amp;quot; Communications on
Physics, 2.115-119.
Yngve, V. 1960. &amp;quot;A model and an hypothesis
for language structure&amp;quot;. Proceedings of the
American Philosophical Society, 104.444-66.
</reference>
<page confidence="0.998287">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987177">
<title confidence="0.998274">PROJECT APRIL — A PROGRESS REPORT</title>
<author confidence="0.999967">Robin Haigh</author>
<author confidence="0.999967">Geoffrey Sampson</author>
<author confidence="0.999967">Eric Atwell</author>
<affiliation confidence="0.999936">Centre for Computer Analysis of Language and Speech, University of Leeds,</affiliation>
<address confidence="0.998991">Leeds LS2 9JT, UK</address>
<abstract confidence="0.999643633333333">Parsing techniques based on rules defining grammaticality are difficult to use with authentic inputs, which are often grammatically messy. Instead, the APRIL system seeks a labelled tree structure which maximizes a numerical measure of conformity to statistical norms derived from a sample of parsed text. No distinction between legal and illegal trees arises: any labelled tree has a value. Because the search space is large and has an irregular geometry, APRIL seeks the best tree using simulated annealing, a stochastic optimization technique. Beginning with an arbitrary tree, many randomly-generated local modifications are considered and adopted or rejected according to their effect on tree-value: acceptance decisions are made probabilistically, subject to a bias against adverse moves which is very weak at the outset but is made to increase as the random walk through the search space continues. This enables the system to converge on the global optimum without getting trapped in local optima. Performance of an early version of the APRIL system on authentic inputs is yielding analyses with a mean accuracy of 75.3% using a schedule which increases processing linearly with sentence-length; modifications currently being implemented should eliminate a high proportion of the remaining errors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Craver°</author>
</authors>
<title>Syntax driven recognition of connected words by Markov models&amp;quot;.</title>
<date>1984</date>
<booktitle>Proceedings of the 1984 IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<marker>Craver°, 1984</marker>
<rawString>Craver°, M., et al. 1984. &amp;quot;Syntax driven recognition of connected words by Markov models&amp;quot;. Proceedings of the 1984 IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Elleggrd</author>
</authors>
<title>The Syntactic Structure of English Texts.</title>
<date>1978</date>
<journal>Gothenburg Studies in English,</journal>
<volume>43</volume>
<marker>Elleggrd, 1978</marker>
<rawString>Elleggrd, A. 1978. The Syntactic Structure of English Texts. Gothenburg Studies in English, 43.</rawString>
</citation>
<citation valid="true">
<date>1987</date>
<booktitle>The Computational Analysis of English.</booktitle>
<editor>Garside, R. G., et al., eds.</editor>
<publisher>Longman.</publisher>
<contexts>
<context position="2284" citStr="(1987)" startWordPosition="348" endWordPosition="348">trick et al. 1983, van Laarhoven &amp; Aarts 1987) to parse authentic English inputs by seeking labelled tree-structures that maximize a measure of plausibility defined in terms of empirical statistics on parse-tree configurations drawn from a database of manually parsed English text. This approach is a response to the fact that &amp;quot;real-life&amp;quot; English, such as the material in the Lancaster-Oslo/Bergen Corpus on which our research focuses, does not appear to conform to a fixed set of grammatical rules. (On the LOB Corpus and the research background from which Project APRIL emerged, see Garside et al. (1987). A crude pilot version of the APRIL system was described in Sampson (1986).) Orthodox computational linguistics is heavily influenced by a concept of language according to which the set of all strings over the vocabulary of the language is partitioned into a class of grammatical strings, which possess analyses all parts of which conform to a finite set of rules defining the language, and a class of strings which are ungrammatical and for which the question of their grammatical structure accordingly does not arise. Even systems which set out to handle &amp;quot;deviant&amp;quot; sentences commonly do so by refe</context>
<context position="3758" citStr="(1987)" startWordPosition="587" endWordPosition="587">e of English grammar, yet any reader can understand the passage without difficulty, and it often seems unlikely that most readers would notice the violation. Furthermore, a problem which is probably even more troublesome for the rule-based approach is that there is an apparently endless diversity of constructions that no-one would be likely to describe as ungrammatical or deviant. Impressionistically it appears that any attempt to state a finite set of rules covering everything that occurs in authentic English text is doomed to go on adding more rules as long as more text is examined; Sampson (1987) adduced objective evidence supporting this impression. Our approach, therefore, is to define a function which associates a figure of merit with any 104 possible tree having labels drawn from a recognized alphabet of grammatical categorysymbols; any input sentence is parsed by seeking the highest-valued tree possible for that sentence. The analysis process works the same way, whether the input is impeccably grammatical or quite bizarre. No contrast between legal and illegal labelled trees arises: a tree which would ordinarily be described as thoroughly illegal is in our terms just a tree whose</context>
</contexts>
<marker>1987</marker>
<rawString>Garside, R. G., et al., eds. 1987. The Computational Analysis of English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters&amp;quot;.</title>
<date>1953</date>
<journal>Biometrika</journal>
<pages>40--237</pages>
<contexts>
<context position="19232" citStr="Good (1953)" startWordPosition="3172" endWordPosition="3173">the frequency contrasts between alternative tree-structures tend to be of a higher order of magnitude, but difficulties arise with very low frequency productions: in particular, as an important special case, many quite normal productions will fail to occur at all in the TreeBank, and are thus not distinguished in our raw data from virtuallyimpossible productions. But it seems reasonable to infer probability estimates for unobserved productions from those of similar, observed productions, and more generally to smooth the raw frequency observations using statistical techniques (see for instance Good (1953)). (One consequence of such smoothing is that no production is ever assigned a probability of zero.) A natural response by linguists would be to say that a relationship of &amp;quot;similarity&amp;quot; between productions needs to be defined in terms of subtle, complex theoretical issues. However, so far we have been impressed by results obtainable in practice using very crude similarity relationOur current evaluation method is only slightly more elaborate than the technique described in Sampson (1986), whereby the probability of a production was derived exclusively from the observed frequencies of the various</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, I. J. 1953. &amp;quot;The population frequencies of species and the estimation of population parameters&amp;quot;. Biometrika 40.237-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Kirkpatrick</author>
</authors>
<title>Optimization by Simulated Annealing&amp;quot;.</title>
<date>1983</date>
<journal>Science</journal>
<pages>220--671</pages>
<marker>Kirkpatrick, 1983</marker>
<rawString>Kirkpatrick, S. E., et al. 1983. &amp;quot;Optimization by Simulated Annealing&amp;quot;. Science 220.671-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J M van Laarhoven</author>
<author>E H L Aarts</author>
</authors>
<date>1987</date>
<booktitle>Simulated Annealing: Theory</booktitle>
<marker>van Laarhoven, Aarts, 1987</marker>
<rawString>van Laarhoven, P. J. M., &amp; E. H. L. Aarts. 1987. Simulated Annealing: Theory and Applications. D. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R G„ ed Lea</author>
</authors>
<title>Trends in Speech Recognition.</title>
<date>1980</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="20475" citStr="Lea 1980" startWordPosition="3369" endWordPosition="3370">ter-labels within the production (that is, for any production m —&gt;do d d,,41, where do and d,..1 are boundary symbols, the estimated probability was the product of the observed frequencies of the various transitions m—+... d, d1+1 ... (0 n) with zeroes replaced by small positive values). This approach was suggested by the success of the CLAWS system for grammatically disambiguating words in context (Garside et al. 1987, chap. 3), which uses an essentially Markovian model, and by the success of Markovian techniques in automatic speech understanding research from the Harpy project onwards (e.g. Lea 1980, Cravero et al. 1984). Subsequent versions of APRft. have begun to incorporate an evaluation measure which makes limited use of non-Markovian relationships. Each label in the non-terminal alphabet is associated with a transition network, each arc of which is assigned a probability as well as a (non-terminal or terminal) label: the probability estimate for a node labelled m is the product of the probabilities of the consecutive arcs in the transition network for m which carry the labels of the node&apos;s daughter-sequence. Unlike the FSAs commonly used in computational linguistics, ours are requir</context>
</contexts>
<marker>Lea, 1980</marker>
<rawString>Lea, R. G„ ed. 1980. Trends in Speech Recognition. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lundy</author>
<author>A Mees</author>
</authors>
<title>Convergence of an annealing algorithm&amp;quot;.</title>
<date>1986</date>
<journal>Mathematical Programming</journal>
<pages>34--111</pages>
<contexts>
<context position="11397" citStr="Lundy &amp; Mees, 1986" startWordPosition="1847" endWordPosition="1850">ea, but such moves prevent the system getting stuck and instead open up new possibilities; at the same time, there is an inexorable overall trend towards improvement. As a result, the system tends to seek out high-valued areas of the solution space initially in terms of gross features, and later in terms of progressively finer detail. Again, the process terminates at a local optimum, but not before exploring the possibilities so thoroughly that this is in general the global optimum. With certain simplifying assumptions, it has been shown mathematically that the global optimum is always found (Lundy &amp; Mees, 1986): in practice, the procedure appears to work well under rather less stringent conditions than those demanded by mathematical treatments that have so far appeared, and our application does in fact take several liberties with the &amp;quot;pure&amp;quot; algorithm as set out in the literature. ANNEALING PARSE-TREES To apply simulated annealing to a given problem, it is necessary to define (a) a space of possible solutions, (b) a class of solution modifications which provides a route from any point in the space to any other, and (c) an annealing schedule (i.e. an initial value for the mean acceptance threshold, a </context>
<context position="14653" citStr="Lundy &amp; Mees (1986)" startWordPosition="2410" endWordPosition="2413"> This bias attempts to increase the frequency with which correct labels are chosen, without limiting the choice to the label which is best for the daughter-sequence considered in isolation, which may not of course be the best in context. An early version of APRIL limited itself to just the Merge and Hive moves. However, a good move-set for annealing should not only permit any solution to be reached from any other solution, but should also be such that paths exist between good trees which do not involve passing through much inferior intermediate stages. (See for example the remarks on depth in Lundy &amp; Mees (1986).) To strengthen this tendency in our system it has proved desirable to add a third class of Reattach moves to the move-set To generate a Reattach move, choose randomly any non-root node in the current tree, eliminate the arc linking the chosen node to its mother, and insert an arc linking it to a node randomly chosen from the set of nodes topologically capable of being its mother. Currently, we are exploring the costeffectiveness of adding a fourth move-type, which relabels a randomly-chosen node without changing the tree shape; a task for the future is to investigate how best to determine th</context>
</contexts>
<marker>Lundy, Mees, 1986</marker>
<rawString>Lundy, M. and A. Mees. 1986. &amp;quot;Convergence of an annealing algorithm&amp;quot;. Mathematical Programming 34.111-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="31474" citStr="Marcus (1980)" startWordPosition="5207" endWordPosition="5208">in a window that scans across continuous text, with the system discovering sentence-boundaries for itself along with lowerlevel structure. (If our system is in due course adapted to parse spoken rather than written input, it is clear that all constituent boundaries including those of sentences would need to be discovered rather than given, and a corollary appears to be that the processing time needed for any length of input must increase only linearly with input length.) As adumbrated in Sampson (1986), we expect to make the dynamic annealing parser more efficient by exploiting the insight of Marcus (1980) that backtracking .is rarely needed in natural language parsing: a gradient of processing intensity will be imposed on the annealing window, with most processing occurring in the &amp;quot;newest&amp;quot; parts of the current tree where valuable moves are most likely to be found. However, simulated annealing is necessarily costly in terms of amount of processing needed. (The schedule used for the run discussed above involved on the order of 30,000 steps generated per input word.) Particularly with a view to applications such as real-time speech analysis, it would be desirable to find a way of exploiting paral</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M. P. 1980. A Theory of Syntactic Recognition for Natural Language. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Sampson</author>
</authors>
<title>A stochastic approach to parsing&amp;quot;.</title>
<date>1986</date>
<booktitle>Proceedings of the I I th International Conference on Computational Linguistics (COLING &apos;86),</booktitle>
<pages>151--5</pages>
<contexts>
<context position="2359" citStr="Sampson (1986)" startWordPosition="360" endWordPosition="361">nglish inputs by seeking labelled tree-structures that maximize a measure of plausibility defined in terms of empirical statistics on parse-tree configurations drawn from a database of manually parsed English text. This approach is a response to the fact that &amp;quot;real-life&amp;quot; English, such as the material in the Lancaster-Oslo/Bergen Corpus on which our research focuses, does not appear to conform to a fixed set of grammatical rules. (On the LOB Corpus and the research background from which Project APRIL emerged, see Garside et al. (1987). A crude pilot version of the APRIL system was described in Sampson (1986).) Orthodox computational linguistics is heavily influenced by a concept of language according to which the set of all strings over the vocabulary of the language is partitioned into a class of grammatical strings, which possess analyses all parts of which conform to a finite set of rules defining the language, and a class of strings which are ungrammatical and for which the question of their grammatical structure accordingly does not arise. Even systems which set out to handle &amp;quot;deviant&amp;quot; sentences commonly do so by referring them to particular &amp;quot;non-deviant&amp;quot; sentences of which they are deemed t</context>
<context position="19722" citStr="Sampson (1986)" startWordPosition="3250" endWordPosition="3251">ions, and more generally to smooth the raw frequency observations using statistical techniques (see for instance Good (1953)). (One consequence of such smoothing is that no production is ever assigned a probability of zero.) A natural response by linguists would be to say that a relationship of &amp;quot;similarity&amp;quot; between productions needs to be defined in terms of subtle, complex theoretical issues. However, so far we have been impressed by results obtainable in practice using very crude similarity relationOur current evaluation method is only slightly more elaborate than the technique described in Sampson (1986), whereby the probability of a production was derived exclusively from the observed frequencies of the various pairwise transitions between daughter-labels within the production (that is, for any production m —&gt;do d d,,41, where do and d,..1 are boundary symbols, the estimated probability was the product of the observed frequencies of the various transitions m—+... d, d1+1 ... (0 n) with zeroes replaced by small positive values). This approach was suggested by the success of the CLAWS system for grammatically disambiguating words in context (Garside et al. 1987, chap. 3), which uses an essenti</context>
<context position="31368" citStr="Sampson (1986)" startWordPosition="5190" endWordPosition="5191">atic system whose inputs are isolated sentences to a dynamic system within which annealing will take place in a window that scans across continuous text, with the system discovering sentence-boundaries for itself along with lowerlevel structure. (If our system is in due course adapted to parse spoken rather than written input, it is clear that all constituent boundaries including those of sentences would need to be discovered rather than given, and a corollary appears to be that the processing time needed for any length of input must increase only linearly with input length.) As adumbrated in Sampson (1986), we expect to make the dynamic annealing parser more efficient by exploiting the insight of Marcus (1980) that backtracking .is rarely needed in natural language parsing: a gradient of processing intensity will be imposed on the annealing window, with most processing occurring in the &amp;quot;newest&amp;quot; parts of the current tree where valuable moves are most likely to be found. However, simulated annealing is necessarily costly in terms of amount of processing needed. (The schedule used for the run discussed above involved on the order of 30,000 steps generated per input word.) Particularly with a view </context>
</contexts>
<marker>Sampson, 1986</marker>
<rawString>Sampson, G. R. 1986. &amp;quot;A stochastic approach to parsing&amp;quot;. Proceedings of the I I th International Conference on Computational Linguistics (COLING &apos;86), pp. 151-5. [GRS wishes to take this opportunity to apologize for the inadvertent near-coincidence of title between this paper and an important 1984 paper by T. Fujisaki.]</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Sampson</author>
</authors>
<title>Evidence against the &apos;grammatical&apos;/&apos;ungrammatical&apos; distinction&amp;quot;.</title>
<date>1987</date>
<booktitle>Corpus Linguistics and Beyond. Rodopi.</booktitle>
<editor>In W. Meijs, ed.,</editor>
<contexts>
<context position="3758" citStr="Sampson (1987)" startWordPosition="586" endWordPosition="587">ized rule of English grammar, yet any reader can understand the passage without difficulty, and it often seems unlikely that most readers would notice the violation. Furthermore, a problem which is probably even more troublesome for the rule-based approach is that there is an apparently endless diversity of constructions that no-one would be likely to describe as ungrammatical or deviant. Impressionistically it appears that any attempt to state a finite set of rules covering everything that occurs in authentic English text is doomed to go on adding more rules as long as more text is examined; Sampson (1987) adduced objective evidence supporting this impression. Our approach, therefore, is to define a function which associates a figure of merit with any 104 possible tree having labels drawn from a recognized alphabet of grammatical categorysymbols; any input sentence is parsed by seeking the highest-valued tree possible for that sentence. The analysis process works the same way, whether the input is impeccably grammatical or quite bizarre. No contrast between legal and illegal labelled trees arises: a tree which would ordinarily be described as thoroughly illegal is in our terms just a tree whose</context>
</contexts>
<marker>Sampson, 1987</marker>
<rawString>Sampson, G. R. 1987. &amp;quot;Evidence against the &apos;grammatical&apos;/&apos;ungrammatical&apos; distinction&amp;quot;. In W. Meijs, ed., Corpus Linguistics and Beyond. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Toulouse</author>
</authors>
<title>Theory of the frustration effect in spin glasses. I.&amp;quot;</title>
<date>1977</date>
<journal>Communications on Physics,</journal>
<pages>2--115</pages>
<contexts>
<context position="8206" citStr="Toulouse (1977)" startWordPosition="1307" endWordPosition="1308">he simulated annealing technique used to locate the highestvalued tree in the set of possible labelled trees; the other is the function used to evaluate any such tree. We will begin by explaining the technique of simulated annealing. This technique uses stochastic (randomizing) methods to locate good solutions; it is now widely exploited, in domains where combinatorial explosion makes the search space too vast for exhaustive examination, where no algorithm is available which leads systematically to the optimal solution, and where there is a considerable degree of &amp;quot;frustration&amp;quot; in the sense of Toulouse (1977), meaning that a seeming improvement in one feature of a solution often at the same time worsens some other feature of the solution, so that the problem cannot be decomposed into small subproblems which can each be optimized separately. (Com105 pare how, in parsing, deciding to attach a constituent A as a daughter of a constituent B may be a relatively attractive way of &amp;quot;using up&amp;quot; A, at the cost of making B a less plausible constituent than it would be without A.) One simple optimization technique, iterative improvement, begins by selecting a solution arbitrarily and then makes a long series o</context>
</contexts>
<marker>Toulouse, 1977</marker>
<rawString>Toulouse, G. 1977. &amp;quot;Theory of the frustration effect in spin glasses. I.&amp;quot; Communications on Physics, 2.115-119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Yngve</author>
</authors>
<title>A model and an hypothesis for language structure&amp;quot;.</title>
<date>1960</date>
<journal>Proceedings of the American Philosophical Society,</journal>
<pages>104--444</pages>
<contexts>
<context position="28507" citStr="Yngve (1960)" startWordPosition="4729" endWordPosition="4730">e, and has chosen to make this clause appositional to the clause beginning placing... (our scheme represents apposition in a manner akin to subordination). This error can be avoided if we note the strong tendency in English (again, not an absolute rule) that postmodifiers of any kind are most often attached to the nearest element that they can logically postmodify, that is, that the chainstructure typified in Fig. 1 is preferred to the embedding-structure in Fig. 2. A preliminary statistical analysis of the TreeBank appears to support the conjecture—developed from the hypothesis formulated by Yngve (1960)—that &amp;quot;the greater the depth of a non-terminal constituent, the greater the probability that either (a) this constituent is the last daughter of its mother, or (b) the next daughter of its mother is a punctuation mark.&amp;quot; (We adapt Yngve&apos;s notion of depth to non-binary trees.) With this formulation it is relatively easy to incorporate into our evaluation system the necessary adjustments to our transition probabilities, so that trees of the more common type will tend to be preferred; but note that nothing prevents an overriding local consideration from leading the parser to prefer, in any given c</context>
</contexts>
<marker>Yngve, 1960</marker>
<rawString>Yngve, V. 1960. &amp;quot;A model and an hypothesis for language structure&amp;quot;. Proceedings of the American Philosophical Society, 104.444-66.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>