<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.869717">
How Is Meaning Grounded in Dictionary Definitions?
</title>
<author confidence="0.372084">
A. Blondin Mass´e
</author>
<note confidence="0.902400666666667">
Laboratoire de combinatoire et d’informatique math´ematique
Universit´e du Qu´ebec a` Montr´eal
Montr´eal (QC), CANADA H3C 3P8
</note>
<email confidence="0.798704">
alexandre.blondin.masse@gmail.com
</email>
<author confidence="0.970456">
G. Chicoisne, Y. Gargouri, S. Harnad, O. Picard
</author>
<affiliation confidence="0.901619">
Institut des sciences cognitives
</affiliation>
<address confidence="0.566065">
Universit´e du Qu´ebec a` Montr´eal
Montr´eal (QC), CANADA H3C 3P8
</address>
<email confidence="0.9684725">
chicoisne.guillaume@uqam.ca, yassinegargouri@hotmail.com
harnad@ecs.soton.ac.uk, olivierpicard18@hotmail.com
</email>
<author confidence="0.964776">
O. Marcotte
</author>
<affiliation confidence="0.922424">
Groupe d’´etudes et de recherche en analyse des d´ecisions (GERAD) and UQ `AM
HEC Montr´eal
Montr´eal (Qu´ebec) Canada H3T 2A7
</affiliation>
<email confidence="0.913208">
Odile.Marcotte@gerad.ca
</email>
<sectionHeader confidence="0.996751" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999696142857143">
Meaning cannot be based on dictionary defini-
tions all the way down: at some point the cir-
cularity of definitions must be broken in some
way, by grounding the meanings of certain
words in sensorimotor categories learned from
experience or shaped by evolution. This is the
“symbol grounding problem”. We introduce
the concept of a reachable set — a larger vo-
cabulary whose meanings can be learned from
a smaller vocabulary through definition alone,
as long as the meanings of the smaller vocabu-
lary are themselves already grounded. We pro-
vide simple algorithms to compute reachable
sets for any given dictionary.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974538461538">
We know from the 19th century philosopher-
mathematician Frege that the referent and the meaning
(or “sense”) of a word (or phrase) are not the same
thing: two different words or phrases can refer to the
very same object without having the same meaning
(Frege, 1948): “George W. Bush” and “the current
president of the United States of America” have the
same referent but a different meaning. So do “human
females” and “daughters”. And “things that are bigger
than a breadbox” and “things that are not the size of a
breadbox or smaller”.
A word’s “extension” is the set of things to which it
refers, and its “intension” is the rule for defining what
</bodyText>
<footnote confidence="0.903042">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999561705882353">
things fall within its extension.. A word’s meaning is
hence something closer to a rule for picking out its ref-
erent. Is the dictionary definition of a word, then, its
meaning?
Clearly, if we do not know the meaning of a word,
we look up its definition in a dictionary. But what if
we do not know the meaning of any of the words in its
dictionary definition? And what if we don’t know the
meanings of the words in the definitions of the words
defining those words, and so on? This is a problem of
infinite regress, called the “symbol grounding problem”
(Harnad, 1990; Harnad, 2003): the meanings of words
in dictionary definitions are, in and of themselves, un-
grounded. The meanings of some of the words, at least,
have to be grounded by some means other than dictio-
nary definition look-up.
How are word meanings grounded? Almost certainly
in the sensorimotor capacity to pick out their referents
(Harnad, 2005). Knowing what to do with what is not
a matter of definition but of adaptive sensorimotor in-
teraction between autonomous, behaving systems and
categories of “objects” (including individuals, kinds,
events, actions, traits and states). Our embodied sen-
sorimotor systems can also be described as applying in-
formation processing rules to inputs in order to generate
the right outputs, just as a thermostat defending a tem-
perature of 20 degrees can be. But this dynamic process
is in no useful way analogous to looking up a definition
in a dictionary.
We will not be discussing sensorimotor grounding
(Barsalou, 2008; Glenberg &amp; Robertson, 2002; Steels,
2007) in this paper. We will assume some sort of
grounding as given: when we consult a dictionary, we
already know the meanings of at least some words,
</bodyText>
<page confidence="0.990014">
17
</page>
<note confidence="0.695071">
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 17–24
Manchester, August 2008
</note>
<bodyText confidence="0.99887168">
somehow. A natural first hypothesis is that the ground-
ing words ought to be more concrete, referring to things
that are closer to our overt sensorimotor experience,
and learned earlier, but that remains to be tested (Clark,
2003). Apart from the question of the boundary condi-
tions of grounding, however, there are basic questions
to be asked about the structure of word meanings in dic-
tionary definition space.
In the path from a word, to the definition of that word,
to the definition of the words in the definition of that
word, and so on, through what sort of a structure are
we navigating (Ravasz &amp; Barabasi, 2003; Steyvers &amp;
Tenenbaum, 2005)? Meaning is compositional: A def-
inition is composed of words, combined according to
syntactic rules to form a proposition (with a truth value:
true or false). For example, the word to be defined w
(the “definiendum”) might mean w1 &amp; w2 &amp; ... &amp; wn,
where the wi are other words (the “definientes”) in its
definition. Rarely does that proposition provide the full
necessary and sufficient conditions for identifying the
referent of the word, w, but the approximation must at
least be close enough to allow most people, armed with
the definition, to understand and use the defined word
most of the time, possibly after looking up a few of its
definientes d,,,, but without having to cycle through the
entire dictionary, and without falling into circularity or
infinite regress.
If enough of the definientes are grounded, then there
is no problem of infinite regress. But we can still ask
the question: What is the size of the grounding vocab-
ulary? and what words does it contain? What is the
length and shape of the path that would be taken in a
recursive definitional search, from a word, to its defi-
nition, to the definition of the words in its definition,
and so on? Would it eventually cycle through the entire
dictionary? Or would there be disjoint subsets?
This paper raises more questions than it answers, but
it develops the formal groundwork for a new means of
finding the answers to questions about how word mean-
ing is explicitly represented in real dictionaries — and
perhaps also about how it is implicitly represented in the
“mental lexicon” that each of us has in our brain (Hauk
et al., 2008).
The remainder of this paper is organized as follows:
In Section 2, we introduce the graph-theoretical defi-
nitions and notations used for formulating the symbol
grounding problem in Section 3. Sections 4 and 5 deal
with the implication of this approach in cognitive sci-
ences and show in what ways grounding kernels may
be useful.
</bodyText>
<sectionHeader confidence="0.976478" genericHeader="introduction">
2 Definitions and Notations
</sectionHeader>
<bodyText confidence="0.988670777777778">
In this section, we give mathematical definitions for
the dictionary-related terminology, relate them to natu-
ral language dictionaries and supply the pertinent graph
theoretical definitions. Additional details are given to
ensure mutual comprehensibility to specialists in the
three disciplines involved (mathematics, linguistics and
psychology). Complete introductions to graph theory
and discrete mathematics are provided in (Bondy &amp;
Murty, 1978; Rosen, 2007).
</bodyText>
<subsectionHeader confidence="0.92124">
2.1 Relations and Functions
</subsectionHeader>
<bodyText confidence="0.9582651875">
Let A be any set. A binary relation on A is any subset
R of A x A. We write xRy if (x, y) E R. The relation
R is said to be (1) reflexive if for all x E A, we have
xRx, (2) symmetric if for all x, y E A such that xRy,
we have yRx and (3) transitive if for all x, y, z E A
such that xRy and yRz, we have xRz. The relation R
is an equivalence relation if it is reflexive, symmetric
and transitive. For any x E A, the equivalence class of
x, designated by [x], is given by [x] = {y E A  |xRy}.
It is easy to show that [x] = [y] if and only if xRy and
that the set of all equivalence classes forms a partition
of A.
Let A be any set, f : A → A a function and k a
positive integer. We designate by fk the function f o
f o ... o f (k times), where o denotes the composition
offunctions.
</bodyText>
<subsectionHeader confidence="0.980763">
2.2 Dictionaries
</subsectionHeader>
<bodyText confidence="0.999800176470588">
At its most basic level, a dictionary is a set of associ-
ated pairs: a word and its defznition, along with some
disambiguating parameters. The word1 to be defined,
w, is called the defzniendum (plural: defznienda) while
the finite nonempty set of words that defines w, d,,,, is
called the set of defznientes of w (singular: defzniens).
Each dictionary entry accordingly consists of a
definiendum w followed by its set of definientes
d,,,. A dictionary D then consists of a finite set
of pairs (w, d,,,) where w is a word and d,,, =
{w1, w2, ... , wn}, where n &gt; 1, is its definition, satis-
fying the property that for all (w, d,,,) E D and for all
d E d,,,, there exists (w&apos;, d,,,,) E D such that d = w&apos;. A
pair (w, d,,,) is called an entry of D. In other words, a
dictionary is a finite set of words, each of which is de-
fined, and each of its defining words is likewise defined
somewhere in the dictionary.
</bodyText>
<subsectionHeader confidence="0.995765">
2.3 Graphs
</subsectionHeader>
<bodyText confidence="0.999945285714286">
A directed graph is a pair G = (V, E) such that V is
a finite set of vertices and E C_ V x V is a finite set
of arcs. Given V &apos; C_ V , the subgraph induced by V &apos;,
designated by G[V &apos;], is the graph G[V &apos;] = (V &apos;, E&apos;)
where E&apos; = E n (V &apos; x V &apos;). For any v E V , N−(v)
and N+(v) designate, respectively, the set of incoming
and outgoing neighbors of v, i.e.
</bodyText>
<equation confidence="0.9998985">
N−(v) = {u E V  |(u, v) E E}
N+(v) = {u E V  |(v, u) E E}.
</equation>
<bodyText confidence="0.933453">
We write deg−(v) = |N−(v) |and deg+(v) =
|N+(v)|, respectively. A path of G is a sequence
</bodyText>
<footnote confidence="0.99705">
1In the context of this mathematical analysis, we will use
“word” to mean a finite string of uninterrupted letters having
some associated meaning.
</footnote>
<page confidence="0.999413">
18
</page>
<bodyText confidence="0.982899">
(v1, v2, ... , vn), where n is a positive integer, vi ∈ V
for i = 1, 2, ... , n and (vi, vi+1) ∈ E, for i =
1, 2, ... , n − 1. A uv-path is a path starting with u
and ending with v. Finally, we say that a uv-path is a
cycle if u = v.
Given a directed graph G = (V, E) and u, v ∈ V , we
write u → v if there exists a uv-path in G. We define a
relation ∼ as
u ∼ v ⇔ u → v and v → u.
It is an easy exercise to show that ∼ is an equivalence
relation. The equivalence classes of V with respect to ∼
are called the strongly connected components of G. In
other words, in a directed graph, it might be possible to
go directly from point A to point B, without being able
to get back from point B to point A (as in a city with
only one-way streets). Strongly connected components,
however, are subgraphs in which whenever it is possible
to go from point A to point B, it is also possible to come
back from point B to point A (the way back may be
different).
There is a very natural way of representing defini-
tional relations using graph theory, thus providing a for-
mal tool for analyzing grounding properties of dictio-
naries: words can be represented as vertices, with arcs
representing definitional relations, i.e. there is an arc
(u, v) between two words u and v if the word u appears
in the definition of the word v. More formally, for every
dictionary D, its associated graph G = (V, E) is given
by
</bodyText>
<equation confidence="0.988024666666667">
V = {w  |∃dw such that (w, dw) ∈ D},
E = {(v, w)  |∃dw such that (w, dw) ∈ D and
v ∈ dw}.
</equation>
<bodyText confidence="0.978384636363636">
Note that every vertex v of G satisfies deg−G(v) &gt; 0,
but it is possible to have deg+G(v) = 0. In other words,
whereas every word has a definition, some words are
not used in any definition.
Example 1. Let D be the dictionary whose definitions
are given in Table 1. Note that every word appearing
in some definition is likewise defined in D (this is one
of the criteria for D to be a dictionary). The associated
graph G of D is represented in Figure 1. Note that
(not, good, eatable, fruit) is a path of G while (good,
bad, good) is a cycle (as well as a path) of G.
</bodyText>
<sectionHeader confidence="0.7328355" genericHeader="method">
3 A Graph-Theoretical Formulation of
the Problem
</sectionHeader>
<bodyText confidence="0.9999625">
We are now ready to formulate the symbol grounding
problem from a mathematical point of view.
</bodyText>
<subsectionHeader confidence="0.998071">
3.1 Reachable and Grounding Sets
</subsectionHeader>
<bodyText confidence="0.997744692307692">
Given a dictionary D of n words and a person x who
knows m out of these n words, assume that the only
way x can learn new words is by consulting the dic-
tionary definitions. Can all n words be learned by x
Word Definition Word Definition
apple red fruit bad not good
banana yellow fruit color dark or light
dark not light eatable good
fruit eatable thing good not bad
light not dark not not
or or red dark color
thing thing tomato red fruit
yellow light color
</bodyText>
<tableCaption confidence="0.995817">
Table 1: Definitions of the dictionary D
</tableCaption>
<figure confidence="0.914421">
thing
dark
</figure>
<figureCaption confidence="0.999774">
Figure 1: Graph representation of the dictionary D.
</figureCaption>
<bodyText confidence="0.989317666666667">
through dictionary look-up alone? If not, then exactly
what subset of words can be learned by x through dic-
tionary look-up alone?
For this purpose, let G = (V, E) be a directed graph
and consider the following application, where 2V de-
notes the collection of all subsets of V :
</bodyText>
<equation confidence="0.999445">
RG : 2V 7−→ 2V
U 7−→ U ∪ {v ∈ V  |N−(v) ⊆ U}.
</equation>
<bodyText confidence="0.997726">
When the context is clear, we omit the subscript G.
Also we let Rk denote the kth power of R. We say
that v ∈ V is k-reachable from U if v ∈ Rk(U) and
k is a nonnegative integer. It is easy to show that there
exists an integer k such that R`(U) = Rk(U), for every
integer B &gt; k. More precisely, we have the following
definitions:
</bodyText>
<construct confidence="0.987335">
Definition 2. Let G = (V, E) be a directed graph, U
a subset of V, and k an integer such that R`(U) =
Rk(U) for all B &gt; k. The set Rk(U) is called the reach-
able set from U and is denoted by R*(U). Moreover, if
R*(U) = V, then we say that U is a grounding set of
G.
</construct>
<bodyText confidence="0.999474166666667">
We say that G is p-groundable if there exists U ⊆ V
such that |U |= p and U is a grounding set of G. The
grounding number of a graph G is the smallest integer
p such that G is p-groundable.
Reachable sets can be computed very simply using a
breadth-first-search type algorithm, as shown by Algo-
</bodyText>
<figure confidence="0.997901769230769">
tomato
fruit
eatable
apple
banana
good
yellow
or
red
bad
not
light
color
</figure>
<page confidence="0.887109">
19
</page>
<construct confidence="0.7789775">
rithm 1.
Algorithm 1 Computing reachable sets
</construct>
<listItem confidence="0.983725125">
1: function REACHABLESET(G, U)
2: R ← U
3: repeat
4: S ← {v ∈ V  |N−G(v) ⊆ R} − R
5: R ← R ∪ S
6: until S = ∅
7: return R
8: end function
</listItem>
<bodyText confidence="0.9777195">
We now present some examples of reachable sets and
grounding sets.
</bodyText>
<equation confidence="0.943637">
Example 3. Consider the dictionary D and the graph
G of Example 1. Let U = {bad, light, not, thing}. Note
that
R0(U) = U
R1(U) = U ∪ {dark, good},
R2(U) = R1(U) ∪ {eatable}
R3(U) = R2(U) ∪ {fruit}
R4(U) = R3(U)
</equation>
<bodyText confidence="0.89706375">
so that R*(U) = {bad, dark, eatable, fruit, good, light,
not, thing} (see Figure 2). In particular, this means
that the word “eatable” is 2-reachable (but not 1-
reachable) from U and all words in U are 0-reachable
from U. Moreover, we observe that U is not a ground-
ing set of G (“color”, for example, is unreachable). On
the other hand, the set U&apos; = U ∪ {or} is a grounding
set of G, so that G is 5-groundable.
</bodyText>
<figureCaption confidence="0.994686">
Figure 2: The set R*(U) (the words in squares) ob-
tained from U
</figureCaption>
<subsectionHeader confidence="0.995451">
3.2 The Minimum Grounding Set Problem
</subsectionHeader>
<bodyText confidence="0.999909222222222">
Given a dictionary and its associated graph G, we are
interested in finding minimum grounding sets of G.
(Note that in general, there is more than one grounding
set of minimum cardinality.) This is related to a natural
decision problem: we designate by k-GS the problem
of deciding whether G is k-groundable. We show that
k-GS is closely related to the problem of finding mini-
mum feedback vertex sets. First, we recall the definition
of a feedback vertex set.
</bodyText>
<construct confidence="0.6676105">
Definition 4. Let G = (V, E) be a directed graph and
U a subset of V . We say that U is a feedback vertex set
of G iffor every cycle C of G, we have U ∩ C =6 ∅. In
other words, U covers every cycle of G.
</construct>
<bodyText confidence="0.9996616">
The minimum feedback vertex set problem is the
problem of finding a feedback vertex set of G of mini-
mum cardinality. To show that feedback vertex sets and
grounding sets are the same, we begin by stating two
simple lemmas.
</bodyText>
<construct confidence="0.7764885">
Lemma 5. Let G = (V, E) be a directed graph, C a
cycle of G and U ⊆ V a grounding set of G. Then
</construct>
<equation confidence="0.497452">
U ∩ C =6∅.
</equation>
<bodyText confidence="0.704257294117647">
Proof. By contradiction, assume that U ∩ C = ∅ and,
for all v ∈ C, there exists an integer k such that v be-
longs to Rk(U). Let B be the smallest index in the set
{k  |∃u ∈ C such that u ∈ Rk(U)}. Let u be a vertex
in C ∩ R`(U) and w the predecessor of u in C. Since
U ∩ C = ∅, k must be greater than 0 and w a member
of R`−1(U), contradicting the minimality of B.
Lemma 6. Every directed acyclic graph G is 0-
groundable.
Proof. We prove the statement by induction on |V |.
BASIS. If |V  |= 1, then |E |= 0, so that the only vertex
v of G satisfies N−G (v) = ∅. Hence R(∅) = V .
INDUCTION. Let v be a vertex such that deg+(v) = 0.
Such a vertex exists since G is acyclic. Moreover,
let G&apos; be the (acyclic) graph obtained from G by re-
moving vertex v and all its incident arcs. By the in-
duction hypothesis, there exists an integer k such that
</bodyText>
<equation confidence="0.999249333333333">
RkG,(∅) = V − {v}. Therefore, V − {v} ⊆ RkG(∅) so
that Rk+1
G (∅) = V .
</equation>
<bodyText confidence="0.790647">
The next theorem follows easily from Lemmas 5 and
</bodyText>
<sectionHeader confidence="0.423013" genericHeader="method">
6.
</sectionHeader>
<construct confidence="0.43338825">
Theorem 7. Let G = (V, E) be a directed graph and
U ⊆ V . Then U is a grounding set of G if and only if
U is a feedback vertex set of G.
Proof. (⇒) Let C be a cycle of G. By Lemma 5, U ∩
C =6 ∅, so that U is a minimum feedback vertex set
of G. (⇐) Let G&apos; be the graph obtained from G by
removing U. Then G&apos; is acyclic and ∅ is a grounding
set of G&apos;. Therefore, U ∪ ∅ = U is a grounding set of
</construct>
<figure confidence="0.9595938">
G.
Corollary 8. k-GS is NP-complete.
0
thing
dark
bad
good
0
2
eatable
1
not
0
banana
yellow
fruit
light
3
0
1
color
or
apple
tomato
red
</figure>
<page confidence="0.861554">
20
</page>
<bodyText confidence="0.999865952380952">
Proof. Denote by k-FVS the problem of deciding
whether a directed graph G admits a feedback vertex
set of cardinality at most k. This problem is known
to be NP-complete and has been widely studied (Karp,
1972; Garey &amp; Johnson, 1979). It follows directly from
Theorem 7 that k-GS is NP-complete as well since the
problems are equivalent.
The fact that problems k-GS and k-FVS are equiv-
alent is not very surprising. Indeed, roughly speaking,
the minimum grounding problem consists of finding a
minimum set large enough to enable the reader to learn
(reach) all the words of the dictionary. On the other
hand, the minimum feedback vertex set problem con-
sists of finding a minimum set large enough to break the
circularity of the definitions in the dictionary. Hence,
the problems are the same, even if they are stated dif-
ferently.
Although the problem is NP-complete in general, we
show that there is a simple way of reducing the com-
plexity of the problem by considering the strongly con-
nected components.
</bodyText>
<subsectionHeader confidence="0.999151">
3.3 Decomposing the Problem
</subsectionHeader>
<bodyText confidence="0.987645125">
Let G = (V, E) be a directed graph and G1, G2, ...,
Gm the subgraphs induced by its strongly connected
components, where m ≥ 1. In particular, there are
no cycles of G containing vertices in different strongly
connected components. Since the minimum ground-
ing set problem is equivalent to the minimum feed-
back vertex set problem, this means that when seeking
a minimum grounding set of G, we can restrict our-
selves to seeking minimum grounding sets of Gi, for
i = 1, 2, ... , m. More precisely, we have the following
proposition.
Proposition 9. Let G = (V, E) be a directed graph
with m strongly connected components, with m ≥ 1,
and let Gi = (Vi, Ei) be the subgraph induced by its
i-th strongly connected component, where 1 ≤ i ≤ m.
Moreover, let Ui be a minimum grounding set of Gi,
for i = 1, 2, ... , m. Then U = Um1 Ui is a minimum
grounding set of G.
Proof. First, we show that U is a grounding set of G.
Let C be a cycle of G. Then C is completely contained
in some strongly connected component of G, say Gj,
where 1 ≤ j ≤ m. But Uj ⊆ U is a grounding set of
Gj, therefore Uj ∩C =6 ∅ so that U ∩C =6 ∅. It remains
to show that U is a minimum grounding set of G. By
contradiction, assume that there exists a grounding set
U0 of G, with |U0 |&lt; |U |and let U0i = U0 ∩ Vi. Then
there exists an index j, with 1 ≤ j ≤ m, such that
|U0j |&lt; |Uj|, contradicting the minimality of |Uj|.
Note that this proposition may be very useful for
graphs having many small strongly connected compo-
nents. Indeed, by using Tar an’s Algorithm (Tar an,
1972), the strongly connected components can be com-
puted in linear time. We illustrate this reduction by an
example.
Example 10. Consider again the dictionary D and the
graph G of Example 1. The strongly connected com-
ponents of G are encircled in Figure 3 and minimum
grounding sets (represented by words in squares) for
each of them are easilyfound. Thus the grounding num-
ber of G is 5.
</bodyText>
<figureCaption confidence="0.991402">
Figure 3: The strongly connected components and a
minimum grounding set of G
</figureCaption>
<subsectionHeader confidence="0.988691">
3.4 The Grounding Kernel
</subsectionHeader>
<bodyText confidence="0.931444933333333">
In Example 10, we have seen that there exist some
strongly connected components consisting of only one
vertex without any loop. In particular, there exist
vertices with no successor, i.e. vertices v such that
N+G (v) = 0. For instance, this is the case of the words
“apple”, “banana” and “tomato”, which are not used in
any definition in the dictionary. Removing these three
words, we notice that “fruit”, “red” and “yellow” are
in the same situation and they can be removed as well.
Pursuing the same idea, we can now remove the words
“color” and “eatable”. At this point, we cannot remove
any further words. The set of remaining words is called
the grounding kernel of the graph G. More formally,
we have the following definition..
Definition 11. Let D be a dictionary, G = (V, E) its
associated graph and G1 = (V1, E1), G2 = (V2, E2),
..., Gm = (Vm, Em) the subgraphs induced by the
strongly connected components of G, where m ≥ 1. Let
V 0 be the set of vertices u such that {u} is a strongly
connected component without any loop (i.e., (u, u) is
not an arc of G). For any u, let N∗(u) denote the set
of vertices v such that G contains a uv-path. Then the
grounding kernel of G, denoted by KG, is the set V −
{u  |u ∈ V 0 and N∗(u) ⊆ V 0}.
Clearly, every dictionary D admits a grounding ker-
nel, as shown by Algorithm 2. Moreover, the ground-
ing kernel is a grounding set of its associated graph G
and every minimum grounding set of G is a subset of
the grounding kernel. Therefore, in studying the sym-
bol grounding problem in dictionaries, we can restrict
</bodyText>
<figure confidence="0.997137533333333">
bad
good
eatable
not
banana
yellow
light
fruit
thing
dark
color
or
apple
tomato
red
</figure>
<page confidence="0.947335">
21
</page>
<bodyText confidence="0.620633">
Algorithm 2 Computing the grounding kernel
</bodyText>
<listItem confidence="0.978969555555555">
1: function GROUNDINGKERNEL(G)
2: G&apos; +– G
3: repeat
4: Let W be the set of vertices of G&apos;
5: U +– {v E W  |N+G,(v) = 0}
6: G&apos; +– G&apos;[W − U]
7: until U = 0
8: return G&apos;
9: end function
</listItem>
<bodyText confidence="0.958009461538462">
ourselves to the grounding kernel of the graph G corre-
sponding to D. This phenomenon is interesting because
every dictionary contains many words that can be recur-
sively removed without compromising the understand-
ing of the other definitions. Formally, this property re-
lates to the level of a word: we will say of a word w
that it is of level k if it is k-reachable from KG but not
i-reachable from KG, for any i &lt; k. In particular, level
0 indicates that the word is part of the grounding kernel.
A similar concept has been studied in (Changizi, 2008).
Example 12. Continuing Example 10 and from what
we have seen so far, itfollows that the grounding kernel
of G is given by
</bodyText>
<equation confidence="0.728398">
KG = {bad, dark, good, light, not, or, thing}.
</equation>
<bodyText confidence="0.997183333333333">
Level 1 words are “color” and “eatable”, level 2 words
are “fruit”, “red” and “yellow”, and level 3 words are
“apple”, “banana” and “tomato”.
</bodyText>
<sectionHeader confidence="0.8309755" genericHeader="method">
4 Grounding Sets and the Mental
Lexicon
</sectionHeader>
<bodyText confidence="0.999888890243903">
In Section 3, we introduced all the necessary terminol-
ogy to study the symbol grounding problem using graph
theory and digital dictionaries. In this section, we ex-
plain how this model can be useful and on what assump-
tions it is based.
A dictionary is a formal symbol system. The pre-
ceding section showed how formal methods can be
applied to this system in order to extract formal fea-
tures. In cognitive science, this is the basis of com-
putationalism (or cognitivism or “disembodied cogni-
tion” (Pylyshyn, 1984)), according to which cognition,
too, is a formal symbol system – one that can be stud-
ied and explained independently of the hardware (or,
insofar as it concerns humans, the wetware) on which
it is implemented. However, pure computationalism
is vulnerable to the problem of the grounding of sym-
bols too (Harnad, 1990). Some of this can be reme-
died by the competing paradigm of embodied cogni-
tion (Barsalou, 2008; Glenberg &amp; Robertson, 2002;
Steels, 2007), which draws on dynamical (noncompu-
tational) systems theory to ground cognition in senso-
rimotor experience. Although computationalism and
symbol grounding provide the background context for
our investigations and findings, the present paper does
not favor any particular theory of mental representation
of meaning.
A dictionary is a symbol system that relates words to
words in such a way that the meanings of the definienda
are conveyed via the definientes. The user is intended to
arrive at an understanding of an unknown word through
an understanding of its definition. What was formally
demonstrated in Section 3 agrees with common sense:
although one can learn new word meanings from a dic-
tionary, the entire dictionary cannot be learned in this
way because of circular references in the definitions
(cycles, in graph theoretic terminology). Information
– nonverbal information – must come from outside the
system to ground at least some of its symbols by some
means other than just formal definition (Cangelosi &amp;
Harnad, 2001). For humans, the two options are learned
sensorimotor grounding and innate grounding. (Al-
though the latter is no doubt important, our current fo-
cus is more on the former.)
The need for information from outside the dictio-
nary is formalized in Section 3. Apart from confirming
the need for such external grounding, we take a sym-
metric stance: In natural language, some word mean-
ings — especially highly abstract ones, such as those
of mathematical or philosophical terms — are not or
cannot be acquired through direct sensorimotor ground-
ing. They are acquired through the composition of pre-
viously known words. The meaning of some of those
words, or of the words in their respective definitions,
must in turn have been grounded through direct senso-
rimotor experience.
To state this in another way: Meaning is not just for-
mal definitions all the way down; nor is it just sensori-
motor experience all the way up. The two extreme poles
of that continuum are sensorimotor induction at one
pole (trial and error experience with corrective feed-
back; observation, pointing, gestures, imitation, etc.),
and symbolic instruction (definitions, descriptions, ex-
planation, verbal examples etc.) at the other pole. Be-
ing able to identify from their lexicological structure
which words were acquired one way or the other would
provide us with important clues about the cognitive pro-
cesses underlying language and the mental representa-
tion of meaning.
To compare the word meanings acquired via sensori-
motor induction with word meanings acquired via sym-
bolic instruction (definitions), we first need access to
the encoding of that knowledge. In this component
of our research, our hypothesis is that the representa-
tional structure of word meanings in dictionaries shares
some commonalities with the representational structure
of word meanings in the human brain (Hauk et al.,
2008). We are thus trying to extract from dictionar-
ies the grounding kernel (and eventually a minimum
grounding set, which in general is a proper subset of
this kernel), from which the rest of the dictionary can be
reached through definitions alone. We hypothesize that
this kernel, identified through formal structural analy-
</bodyText>
<page confidence="0.98668">
22
</page>
<bodyText confidence="0.999249">
sis, will exhibit properties that are also reflected in the
mental lexicon. In parallel ongoing studies, we are find-
ing that the words in the grounding kernel are indeed
(1) more frequent in oral and written usage, (2) more
concrete, (3) more readily imageable, and (4) learned
earlier or at a younger age. We also expect they will be
(5) more universal (across dictionaries, languages and
cultures) (Chicoisne et al., 2008).
</bodyText>
<sectionHeader confidence="0.9421275" genericHeader="method">
5 Grounding Kernels in Natural
Language Dictionaries
</sectionHeader>
<bodyText confidence="0.999995888888889">
In earlier research (Clark, 2003), we have been ana-
lyzing two special dictionaries: the Longman’s Dic-
tionary of Contemporary English (LDOCE) (Procter,
1978) and the Cambridge International Dictionary of
English (CIDE) (Procter, 1995). Both are officially
described as being based upon a defining vocabulary:
a set of 2000 words which are purportedly the only
words used in all the definitions of the dictionary, in-
cluding the definitions of the defining vocabulary itself.
A closer analysis of this defining vocabulary, however,
has revealed that it is not always faithful to these con-
straints: A significant number of words used in the def-
initions turn out not to be in the defining vocabulary.
Hence it became evident that we would ourselves have
to generate a grounding kernel (roughly equivalent to
the defining vocabulary) from these dictionaries.
The method presented in this paper makes it possi-
ble, given the graph structure of a dictionary, to extract
a grounding kernel therefrom. Extracting this struc-
ture in turn confronts us with two further problems:
morphology and polysemy. Neither of these problems
has a definite algorithmic solution. Morphology can
be treated through stemming and associated look-up
lists for the simplest cases (i.e., was → to be, and chil-
dren → child), but more elaborate or complicated cases
would require syntactic analysis or, ultimately, human
evaluation. Polysemy is usually treated through statisti-
cal analysis of the word context (as in Latent Semantic
Analysis) (Kintsch, 2007) or human evaluation. Indeed,
a good deal of background knowledge is necessary to
analyse an entry such as: “dominant: the fifth note of a
musical scale of eight notes” (the LDOCE notes 16 dif-
ferent meanings of scale and 4 for dominant, and in our
example, none of these words are used with their most
frequent meaning).
Correct disambiguation of a dictionary is time-
consuming work, as the most effective way to do it
for now is through consensus among human evaluators.
Fortunately, a fully disambiguated version of the Word-
Net database (Fellbaum,1998; Fellbaum, 2005) has just
become available. We expect the grounding kernel of
WordNet to be of greater interest than the defining vo-
cabulary of either CIDE or LDOCE (or what we extract
from them and disambiguate automatically, and imper-
fectly) for our analysis.
</bodyText>
<sectionHeader confidence="0.999677" genericHeader="conclusions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.947643175438597">
The main purpose of this paper was to introduce a for-
mal approach to the symbol grounding problem based
on the computational analysis of digital dictionaries.
Ongoing and future work includes the following:
The minimum grounding set problem. We have seen
that the problem of finding a minimum grounding set is
NP-complete for general graphs. However, graphs as-
sociated with dictionaries have a very specific structure.
We intend to describe a class of graphs including those
specific graphs and to try to design a polynomial-time
algorithm to solve the problem. Another approach is
to design approximation algorithms, yielding a solution
close to the optimal solution, with some known guaran-
tee.
Grounding sets satisfying particular constraints. Let
D be a dictionary, G = (V, E) its associated graph,
and U ⊆ V any subset of vertices satisfying a given
property P. We can use Algorithm 1 to test whether
or not U is a grounding set. In particular, it would be
interesting to test different sets U satisfying different
cognitive constraints.
Relaxing the grounding conditions. In this paper
we imposed strong conditions on the learning of new
words: One must know all the words of the definition
fully in order to learn a new word from them. This is
not realistic, because we all know one can often under-
stand a definition without knowing every single word
in it. Hence one way to relax these conditions would
be to modify the learning rule so that one need only un-
derstand at least r% of the definition, where r is some
number between 0 and 100. Another variation would
be to assign weights to words to take into account their
morphosyntactic and semantic properties (rather than
just treating them as an unordered list, as in the present
analysis). Finally, we could consider “quasi-grounding
sets”, whose associated reachable set consists of r% of
the whole dictionary.
Disambiguation of definitional relations. Analyzing
real dictionaries raises, in its full generality, the prob-
lem of word and text disambiguation in free text; this
is a very difficult problem. For example, if the word
“make” appears in a definition, we do not know which
of its many senses is intended — nor even what its
grammatical category is. To our knowledge, the only
available dictionary that endeavors to provide fully dis-
ambiguated definitions is the just-released version of
WordNet. On the other hand, dictionary definitions
have a very specific grammatical structure, presumably
simpler and more limited than the general case of free
text. It might hence be feasible to develop automatic
disambiguation algorithms specifically dedicated to the
special case of dictionary definitions.
Concluding Remark: Definition can reach the sense
(sometimes), but only the senses can reach the referent.
Research funded by Canada Research Chair in Cog-
nitive Sciences, SSHRC (S. Harnad)and NSERC (S.
Harnad &amp; O. Marcotte)
</bodyText>
<page confidence="0.998591">
23
</page>
<sectionHeader confidence="0.998317" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999086807692308">
Barsalou, L. (2008) Grounded Cognition. Annual Re-
view of Psychology (in press).
Bondy, J.A. &amp; U.S.R. Murty. (1978) Graph theory
with applications. Macmillan, New York.
Cangelosi, A. &amp; Harnad, S. (2001) The Adap-
tive Advantage of Symbolic Theft Over Sensorimotor
Toil: Grounding Language in Perceptual Categories.
Evol. of Communication 4(1) 117-142.
Changizi, M.A. (2008) Economically organized hier-
archies in WordNet and the Oxford English Dictio-
nary. Cognitive Systems Research (in press).
Chicoisne G., A. Blondin-Mass´e, O. Picard, S. Har-
nad (2008) Grounding Abstract Word Definitions
In Prior Concrete Experience. 6th Int. Conf. on the
Mental Lexicon, Banff, Alberta.
Clark G. (2003) Recursion Through Dictionary Defi-
nition Space: Concrete Versus Abstract Words. (U.
Southampton Tech Report).
Fellbaum, C. (1998) WordNet: An electronic lexical
database. Cambridge: MIT Press.
Fellbaum, C. (2005) Theories of human semantic rep-
resentation of the mental lexicon. In: Cruse, D. A.
(Ed.), Handbook of Linguistics and Communication
Science, Berlin, Germany: Walter de Gruyter, 1749-
1758.
Frege G. (1948) Sense and Reference. The Philosoph-
ical Review 57 (3) 209-230.
Garey, M.R. &amp; D.S. Johnson (1979) Computers
and Intractability: A Guide to the Theory of NP-
completeness. W.H. Freeman, New York.
Glenberg A.M. &amp; D.A. Robertson (2002) Symbol
Grounding and Meaning: A Comparison of High-
Dimensional and Embodied Theories of Meaning.
Journal of Memory and Language 43 (3) 379-401.
Harnad, S. (1990) The Symbol Grounding Problem.
Physica D 42:335-346.
Harnad, S. (2003) Symbol-Grounding Problem. En-
cylopedia of Cognitive Science. Nature Publishing
Group. Macmillan.
Harnad, S. (2005) To Cognize is to Categorize: Cog-
nition is Categorization. In Lefebvre, C. and Cohen,
H. (Eds.), Handbook of Categorization. Elsevier.
Hauk, O., M.H. Davis, F. Kherif, F. Pulverm¨uller.
(2008) Imagery or meaning? Evidence for a se-
mantic origin of category-specific brain activity in
metabolic imaging. European Journal of Neuro-
science 27 (7) 1856-1866.
Karp, R.M. (1972) Reducibility among combinato-
rial problems. In: R.E. Miller, J.W. Thatcher (Eds.),
Complexity of Computer Computations, Plenum
Press, New York, 1972, pp. 85-103.
Kintsch, W. (2007) Meaning in Context. In T.K.
Landauer, D.S. McNamara, S. Dennis &amp; W. Kintsch
(Eds.), Handbook of Latent Semantic Analysis. Erl-
baum.
Procter, P. (1978) Longman Dictionary of Contempo-
rary English. Longman Group Ltd., Essex, UK.
Procter, P. (1995) Cambridge International Dictionary
of English (CIDE). Cambridge University Press.
Pylyshyn, Z. W. (1984) Computation and Cognition:
Towards a Foundation for Cognitive Science. Cam-
bridge: MIT Press.
Ravasz, E. &amp; Barabasi, A. L. (2003) Hierarchical or-
ganization in complex networks. Physical Review E
67, 026112.
Rosen, K.H. (2007) Discrete mathematics and its ap-
plications, 6th ed. McGraw-Hill.
Steels, L. (2007) The symbol grounding problem is
solved, so what’s next? In De Vega, M. and G. Glen-
berg and A. Graesser (Eds.), Symbols, embodiment
and meaning. Academic Press, North Haven.
Steyvers, M. &amp; Tenenbaum J.B. (2005) The large-
scale structure of semantic networks: statistical
analyses and a model of semantic growth. Cognitive
Science, 29(1) 41-78.
Tarjan, R. (1972) Depth-first search and linear graph
algorithms. SIAM Journal on Computing. 1 (2) 146-
160.
</reference>
<page confidence="0.999164">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.061395">
<title confidence="0.999904">How Is Meaning Grounded in Dictionary Definitions?</title>
<author confidence="0.999836">A Blondin</author>
<affiliation confidence="0.9950705">Laboratoire de combinatoire et d’informatique Universit´e du Qu´ebec a`</affiliation>
<address confidence="0.662731">Montr´eal (QC), CANADA H3C</address>
<email confidence="0.996346">alexandre.blondin.masse@gmail.com</email>
<author confidence="0.420003">G Chicoisne</author>
<author confidence="0.420003">Y Gargouri</author>
<author confidence="0.420003">S Harnad</author>
<author confidence="0.420003">O</author>
<affiliation confidence="0.8735225">Institut des sciences Universit´e du Qu´ebec a`</affiliation>
<address confidence="0.774258">Montr´eal (QC), CANADA H3C</address>
<email confidence="0.9054275">chicoisne.guillaume@uqam.ca,harnad@ecs.soton.ac.uk,olivierpicard18@hotmail.com</email>
<note confidence="0.6398962">O. Groupe d’´etudes et de recherche en analyse des d´ecisions (GERAD) and UQ HEC Montr´eal (Qu´ebec) Canada H3T Odile.Marcotte@gerad.ca</note>
<abstract confidence="0.999048">Meaning cannot be based on dictionary definitions all the way down: at some point the circularity of definitions must be broken in some way, by grounding the meanings of certain words in sensorimotor categories learned from experience or shaped by evolution. This is the “symbol grounding problem”. We introduce concept of a — a larger vocabulary whose meanings can be learned from a smaller vocabulary through definition alone, as long as the meanings of the smaller vocabulary are themselves already grounded. We provide simple algorithms to compute reachable sets for any given dictionary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Barsalou</author>
</authors>
<title>Grounded Cognition. Annual Review of Psychology</title>
<date>2008</date>
<note>(in press).</note>
<contexts>
<context position="3634" citStr="Barsalou, 2008" startWordPosition="568" endWordPosition="569">5). Knowing what to do with what is not a matter of definition but of adaptive sensorimotor interaction between autonomous, behaving systems and categories of “objects” (including individuals, kinds, events, actions, traits and states). Our embodied sensorimotor systems can also be described as applying information processing rules to inputs in order to generate the right outputs, just as a thermostat defending a temperature of 20 degrees can be. But this dynamic process is in no useful way analogous to looking up a definition in a dictionary. We will not be discussing sensorimotor grounding (Barsalou, 2008; Glenberg &amp; Robertson, 2002; Steels, 2007) in this paper. We will assume some sort of grounding as given: when we consult a dictionary, we already know the meanings of at least some words, 17 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 17–24 Manchester, August 2008 somehow. A natural first hypothesis is that the grounding words ought to be more concrete, referring to things that are closer to our overt sensorimotor experience, and learned earlier, but that remains to be tested (Clark, 2003). Apart from the question of the</context>
<context position="23733" citStr="Barsalou, 2008" startWordPosition="4470" endWordPosition="4471">w formal methods can be applied to this system in order to extract formal features. In cognitive science, this is the basis of computationalism (or cognitivism or “disembodied cognition” (Pylyshyn, 1984)), according to which cognition, too, is a formal symbol system – one that can be studied and explained independently of the hardware (or, insofar as it concerns humans, the wetware) on which it is implemented. However, pure computationalism is vulnerable to the problem of the grounding of symbols too (Harnad, 1990). Some of this can be remedied by the competing paradigm of embodied cognition (Barsalou, 2008; Glenberg &amp; Robertson, 2002; Steels, 2007), which draws on dynamical (noncomputational) systems theory to ground cognition in sensorimotor experience. Although computationalism and symbol grounding provide the background context for our investigations and findings, the present paper does not favor any particular theory of mental representation of meaning. A dictionary is a symbol system that relates words to words in such a way that the meanings of the definienda are conveyed via the definientes. The user is intended to arrive at an understanding of an unknown word through an understanding of</context>
</contexts>
<marker>Barsalou, 2008</marker>
<rawString>Barsalou, L. (2008) Grounded Cognition. Annual Review of Psychology (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Bondy</author>
<author>U S R Murty</author>
</authors>
<title>Graph theory with applications.</title>
<date>1978</date>
<publisher>Macmillan,</publisher>
<location>New York.</location>
<contexts>
<context position="7031" citStr="Bondy &amp; Murty, 1978" startWordPosition="1128" endWordPosition="1131">ion 3. Sections 4 and 5 deal with the implication of this approach in cognitive sciences and show in what ways grounding kernels may be useful. 2 Definitions and Notations In this section, we give mathematical definitions for the dictionary-related terminology, relate them to natural language dictionaries and supply the pertinent graph theoretical definitions. Additional details are given to ensure mutual comprehensibility to specialists in the three disciplines involved (mathematics, linguistics and psychology). Complete introductions to graph theory and discrete mathematics are provided in (Bondy &amp; Murty, 1978; Rosen, 2007). 2.1 Relations and Functions Let A be any set. A binary relation on A is any subset R of A x A. We write xRy if (x, y) E R. The relation R is said to be (1) reflexive if for all x E A, we have xRx, (2) symmetric if for all x, y E A such that xRy, we have yRx and (3) transitive if for all x, y, z E A such that xRy and yRz, we have xRz. The relation R is an equivalence relation if it is reflexive, symmetric and transitive. For any x E A, the equivalence class of x, designated by [x], is given by [x] = {y E A |xRy}. It is easy to show that [x] = [y] if and only if xRy and that the </context>
</contexts>
<marker>Bondy, Murty, 1978</marker>
<rawString>Bondy, J.A. &amp; U.S.R. Murty. (1978) Graph theory with applications. Macmillan, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cangelosi</author>
<author>S Harnad</author>
</authors>
<title>The Adaptive Advantage of Symbolic Theft Over Sensorimotor Toil: Grounding Language in Perceptual Categories.</title>
<date>2001</date>
<journal>Evol. of Communication</journal>
<volume>4</volume>
<issue>1</issue>
<pages>117--142</pages>
<contexts>
<context position="24809" citStr="Cangelosi &amp; Harnad, 2001" startWordPosition="4636" endWordPosition="4639">of the definienda are conveyed via the definientes. The user is intended to arrive at an understanding of an unknown word through an understanding of its definition. What was formally demonstrated in Section 3 agrees with common sense: although one can learn new word meanings from a dictionary, the entire dictionary cannot be learned in this way because of circular references in the definitions (cycles, in graph theoretic terminology). Information – nonverbal information – must come from outside the system to ground at least some of its symbols by some means other than just formal definition (Cangelosi &amp; Harnad, 2001). For humans, the two options are learned sensorimotor grounding and innate grounding. (Although the latter is no doubt important, our current focus is more on the former.) The need for information from outside the dictionary is formalized in Section 3. Apart from confirming the need for such external grounding, we take a symmetric stance: In natural language, some word meanings — especially highly abstract ones, such as those of mathematical or philosophical terms — are not or cannot be acquired through direct sensorimotor grounding. They are acquired through the composition of previously kno</context>
</contexts>
<marker>Cangelosi, Harnad, 2001</marker>
<rawString>Cangelosi, A. &amp; Harnad, S. (2001) The Adaptive Advantage of Symbolic Theft Over Sensorimotor Toil: Grounding Language in Perceptual Categories. Evol. of Communication 4(1) 117-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Changizi</author>
</authors>
<title>Economically organized hierarchies in WordNet and the Oxford English Dictionary. Cognitive Systems Research</title>
<date>2008</date>
<note>(in press).</note>
<contexts>
<context position="22460" citStr="Changizi, 2008" startWordPosition="4249" endWordPosition="4250"> 6: G&apos; +– G&apos;[W − U] 7: until U = 0 8: return G&apos; 9: end function ourselves to the grounding kernel of the graph G corresponding to D. This phenomenon is interesting because every dictionary contains many words that can be recursively removed without compromising the understanding of the other definitions. Formally, this property relates to the level of a word: we will say of a word w that it is of level k if it is k-reachable from KG but not i-reachable from KG, for any i &lt; k. In particular, level 0 indicates that the word is part of the grounding kernel. A similar concept has been studied in (Changizi, 2008). Example 12. Continuing Example 10 and from what we have seen so far, itfollows that the grounding kernel of G is given by KG = {bad, dark, good, light, not, or, thing}. Level 1 words are “color” and “eatable”, level 2 words are “fruit”, “red” and “yellow”, and level 3 words are “apple”, “banana” and “tomato”. 4 Grounding Sets and the Mental Lexicon In Section 3, we introduced all the necessary terminology to study the symbol grounding problem using graph theory and digital dictionaries. In this section, we explain how this model can be useful and on what assumptions it is based. A dictionary</context>
</contexts>
<marker>Changizi, 2008</marker>
<rawString>Changizi, M.A. (2008) Economically organized hierarchies in WordNet and the Oxford English Dictionary. Cognitive Systems Research (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Chicoisne</author>
<author>A Blondin-Mass´e</author>
<author>O Picard</author>
<author>S Harnad</author>
</authors>
<title>Grounding Abstract Word Definitions In</title>
<date>2008</date>
<booktitle>Prior Concrete Experience. 6th Int. Conf. on the Mental Lexicon,</booktitle>
<location>Banff, Alberta.</location>
<marker>Chicoisne, Blondin-Mass´e, Picard, Harnad, 2008</marker>
<rawString>Chicoisne G., A. Blondin-Mass´e, O. Picard, S. Harnad (2008) Grounding Abstract Word Definitions In Prior Concrete Experience. 6th Int. Conf. on the Mental Lexicon, Banff, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Clark</author>
</authors>
<title>Recursion Through Dictionary Definition Space: Concrete Versus Abstract Words.</title>
<date>2003</date>
<tech>(U. Southampton Tech Report).</tech>
<contexts>
<context position="4202" citStr="Clark, 2003" startWordPosition="659" endWordPosition="660">ssing sensorimotor grounding (Barsalou, 2008; Glenberg &amp; Robertson, 2002; Steels, 2007) in this paper. We will assume some sort of grounding as given: when we consult a dictionary, we already know the meanings of at least some words, 17 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 17–24 Manchester, August 2008 somehow. A natural first hypothesis is that the grounding words ought to be more concrete, referring to things that are closer to our overt sensorimotor experience, and learned earlier, but that remains to be tested (Clark, 2003). Apart from the question of the boundary conditions of grounding, however, there are basic questions to be asked about the structure of word meanings in dictionary definition space. In the path from a word, to the definition of that word, to the definition of the words in the definition of that word, and so on, through what sort of a structure are we navigating (Ravasz &amp; Barabasi, 2003; Steyvers &amp; Tenenbaum, 2005)? Meaning is compositional: A definition is composed of words, combined according to syntactic rules to form a proposition (with a truth value: true or false). For example, the word </context>
<context position="27507" citStr="Clark, 2003" startWordPosition="5072" endWordPosition="5073">gh definitions alone. We hypothesize that this kernel, identified through formal structural analy22 sis, will exhibit properties that are also reflected in the mental lexicon. In parallel ongoing studies, we are finding that the words in the grounding kernel are indeed (1) more frequent in oral and written usage, (2) more concrete, (3) more readily imageable, and (4) learned earlier or at a younger age. We also expect they will be (5) more universal (across dictionaries, languages and cultures) (Chicoisne et al., 2008). 5 Grounding Kernels in Natural Language Dictionaries In earlier research (Clark, 2003), we have been analyzing two special dictionaries: the Longman’s Dictionary of Contemporary English (LDOCE) (Procter, 1978) and the Cambridge International Dictionary of English (CIDE) (Procter, 1995). Both are officially described as being based upon a defining vocabulary: a set of 2000 words which are purportedly the only words used in all the definitions of the dictionary, including the definitions of the defining vocabulary itself. A closer analysis of this defining vocabulary, however, has revealed that it is not always faithful to these constraints: A significant number of words used in </context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Clark G. (2003) Recursion Through Dictionary Definition Space: Concrete Versus Abstract Words. (U. Southampton Tech Report).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge:</location>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (1998) WordNet: An electronic lexical database. Cambridge: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>Theories of human semantic representation of the mental lexicon. In: Cruse,</title>
<date>2005</date>
<journal>Walter de Gruyter,</journal>
<booktitle>Handbook of Linguistics and Communication Science,</booktitle>
<pages>1749--1758</pages>
<editor>D. A. (Ed.),</editor>
<location>Berlin, Germany:</location>
<contexts>
<context position="29566" citStr="Fellbaum, 2005" startWordPosition="5400" endWordPosition="5401">s in Latent Semantic Analysis) (Kintsch, 2007) or human evaluation. Indeed, a good deal of background knowledge is necessary to analyse an entry such as: “dominant: the fifth note of a musical scale of eight notes” (the LDOCE notes 16 different meanings of scale and 4 for dominant, and in our example, none of these words are used with their most frequent meaning). Correct disambiguation of a dictionary is timeconsuming work, as the most effective way to do it for now is through consensus among human evaluators. Fortunately, a fully disambiguated version of the WordNet database (Fellbaum,1998; Fellbaum, 2005) has just become available. We expect the grounding kernel of WordNet to be of greater interest than the defining vocabulary of either CIDE or LDOCE (or what we extract from them and disambiguate automatically, and imperfectly) for our analysis. 6 Future Work The main purpose of this paper was to introduce a formal approach to the symbol grounding problem based on the computational analysis of digital dictionaries. Ongoing and future work includes the following: The minimum grounding set problem. We have seen that the problem of finding a minimum grounding set is NP-complete for general graphs</context>
</contexts>
<marker>Fellbaum, 2005</marker>
<rawString>Fellbaum, C. (2005) Theories of human semantic representation of the mental lexicon. In: Cruse, D. A. (Ed.), Handbook of Linguistics and Communication Science, Berlin, Germany: Walter de Gruyter, 1749-1758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Frege</author>
</authors>
<title>Sense and Reference.</title>
<date>1948</date>
<journal>The Philosophical Review</journal>
<volume>57</volume>
<issue>3</issue>
<pages>209--230</pages>
<contexts>
<context position="1549" citStr="Frege, 1948" startWordPosition="224" endWordPosition="225"> grounding problem”. We introduce the concept of a reachable set — a larger vocabulary whose meanings can be learned from a smaller vocabulary through definition alone, as long as the meanings of the smaller vocabulary are themselves already grounded. We provide simple algorithms to compute reachable sets for any given dictionary. 1 Introduction We know from the 19th century philosophermathematician Frege that the referent and the meaning (or “sense”) of a word (or phrase) are not the same thing: two different words or phrases can refer to the very same object without having the same meaning (Frege, 1948): “George W. Bush” and “the current president of the United States of America” have the same referent but a different meaning. So do “human females” and “daughters”. And “things that are bigger than a breadbox” and “things that are not the size of a breadbox or smaller”. A word’s “extension” is the set of things to which it refers, and its “intension” is the rule for defining what © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. things fall within its extension.. A w</context>
</contexts>
<marker>Frege, 1948</marker>
<rawString>Frege G. (1948) Sense and Reference. The Philosophical Review 57 (3) 209-230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Garey</author>
<author>D S Johnson</author>
</authors>
<title>Computers and Intractability: A Guide to the Theory of NPcompleteness.</title>
<date>1979</date>
<publisher>W.H. Freeman,</publisher>
<location>New York.</location>
<contexts>
<context position="17215" citStr="Garey &amp; Johnson, 1979" startWordPosition="3253" endWordPosition="3256"> (⇒) Let C be a cycle of G. By Lemma 5, U ∩ C =6 ∅, so that U is a minimum feedback vertex set of G. (⇐) Let G&apos; be the graph obtained from G by removing U. Then G&apos; is acyclic and ∅ is a grounding set of G&apos;. Therefore, U ∪ ∅ = U is a grounding set of G. Corollary 8. k-GS is NP-complete. 0 thing dark bad good 0 2 eatable 1 not 0 banana yellow fruit light 3 0 1 color or apple tomato red 20 Proof. Denote by k-FVS the problem of deciding whether a directed graph G admits a feedback vertex set of cardinality at most k. This problem is known to be NP-complete and has been widely studied (Karp, 1972; Garey &amp; Johnson, 1979). It follows directly from Theorem 7 that k-GS is NP-complete as well since the problems are equivalent. The fact that problems k-GS and k-FVS are equivalent is not very surprising. Indeed, roughly speaking, the minimum grounding problem consists of finding a minimum set large enough to enable the reader to learn (reach) all the words of the dictionary. On the other hand, the minimum feedback vertex set problem consists of finding a minimum set large enough to break the circularity of the definitions in the dictionary. Hence, the problems are the same, even if they are stated differently. Alth</context>
</contexts>
<marker>Garey, Johnson, 1979</marker>
<rawString>Garey, M.R. &amp; D.S. Johnson (1979) Computers and Intractability: A Guide to the Theory of NPcompleteness. W.H. Freeman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Glenberg</author>
<author>D A Robertson</author>
</authors>
<title>Symbol Grounding and Meaning: A Comparison of HighDimensional and Embodied Theories of Meaning.</title>
<date>2002</date>
<journal>Journal of Memory and Language</journal>
<volume>43</volume>
<issue>3</issue>
<pages>379--401</pages>
<contexts>
<context position="3662" citStr="Glenberg &amp; Robertson, 2002" startWordPosition="570" endWordPosition="573"> to do with what is not a matter of definition but of adaptive sensorimotor interaction between autonomous, behaving systems and categories of “objects” (including individuals, kinds, events, actions, traits and states). Our embodied sensorimotor systems can also be described as applying information processing rules to inputs in order to generate the right outputs, just as a thermostat defending a temperature of 20 degrees can be. But this dynamic process is in no useful way analogous to looking up a definition in a dictionary. We will not be discussing sensorimotor grounding (Barsalou, 2008; Glenberg &amp; Robertson, 2002; Steels, 2007) in this paper. We will assume some sort of grounding as given: when we consult a dictionary, we already know the meanings of at least some words, 17 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 17–24 Manchester, August 2008 somehow. A natural first hypothesis is that the grounding words ought to be more concrete, referring to things that are closer to our overt sensorimotor experience, and learned earlier, but that remains to be tested (Clark, 2003). Apart from the question of the boundary conditions of grou</context>
<context position="23761" citStr="Glenberg &amp; Robertson, 2002" startWordPosition="4472" endWordPosition="4475"> can be applied to this system in order to extract formal features. In cognitive science, this is the basis of computationalism (or cognitivism or “disembodied cognition” (Pylyshyn, 1984)), according to which cognition, too, is a formal symbol system – one that can be studied and explained independently of the hardware (or, insofar as it concerns humans, the wetware) on which it is implemented. However, pure computationalism is vulnerable to the problem of the grounding of symbols too (Harnad, 1990). Some of this can be remedied by the competing paradigm of embodied cognition (Barsalou, 2008; Glenberg &amp; Robertson, 2002; Steels, 2007), which draws on dynamical (noncomputational) systems theory to ground cognition in sensorimotor experience. Although computationalism and symbol grounding provide the background context for our investigations and findings, the present paper does not favor any particular theory of mental representation of meaning. A dictionary is a symbol system that relates words to words in such a way that the meanings of the definienda are conveyed via the definientes. The user is intended to arrive at an understanding of an unknown word through an understanding of its definition. What was fo</context>
</contexts>
<marker>Glenberg, Robertson, 2002</marker>
<rawString>Glenberg A.M. &amp; D.A. Robertson (2002) Symbol Grounding and Meaning: A Comparison of HighDimensional and Embodied Theories of Meaning. Journal of Memory and Language 43 (3) 379-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harnad</author>
</authors>
<title>The Symbol Grounding Problem.</title>
<date>1990</date>
<journal>Physica D</journal>
<pages>42--335</pages>
<contexts>
<context position="2677" citStr="Harnad, 1990" startWordPosition="414" endWordPosition="415">icenses/by-nc-sa/3.0/). Some rights reserved. things fall within its extension.. A word’s meaning is hence something closer to a rule for picking out its referent. Is the dictionary definition of a word, then, its meaning? Clearly, if we do not know the meaning of a word, we look up its definition in a dictionary. But what if we do not know the meaning of any of the words in its dictionary definition? And what if we don’t know the meanings of the words in the definitions of the words defining those words, and so on? This is a problem of infinite regress, called the “symbol grounding problem” (Harnad, 1990; Harnad, 2003): the meanings of words in dictionary definitions are, in and of themselves, ungrounded. The meanings of some of the words, at least, have to be grounded by some means other than dictionary definition look-up. How are word meanings grounded? Almost certainly in the sensorimotor capacity to pick out their referents (Harnad, 2005). Knowing what to do with what is not a matter of definition but of adaptive sensorimotor interaction between autonomous, behaving systems and categories of “objects” (including individuals, kinds, events, actions, traits and states). Our embodied sensori</context>
<context position="23639" citStr="Harnad, 1990" startWordPosition="4453" endWordPosition="4454">umptions it is based. A dictionary is a formal symbol system. The preceding section showed how formal methods can be applied to this system in order to extract formal features. In cognitive science, this is the basis of computationalism (or cognitivism or “disembodied cognition” (Pylyshyn, 1984)), according to which cognition, too, is a formal symbol system – one that can be studied and explained independently of the hardware (or, insofar as it concerns humans, the wetware) on which it is implemented. However, pure computationalism is vulnerable to the problem of the grounding of symbols too (Harnad, 1990). Some of this can be remedied by the competing paradigm of embodied cognition (Barsalou, 2008; Glenberg &amp; Robertson, 2002; Steels, 2007), which draws on dynamical (noncomputational) systems theory to ground cognition in sensorimotor experience. Although computationalism and symbol grounding provide the background context for our investigations and findings, the present paper does not favor any particular theory of mental representation of meaning. A dictionary is a symbol system that relates words to words in such a way that the meanings of the definienda are conveyed via the definientes. The</context>
</contexts>
<marker>Harnad, 1990</marker>
<rawString>Harnad, S. (1990) The Symbol Grounding Problem. Physica D 42:335-346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harnad</author>
</authors>
<title>Symbol-Grounding Problem. Encylopedia of Cognitive Science.</title>
<date>2003</date>
<publisher>Nature Publishing Group. Macmillan.</publisher>
<contexts>
<context position="2692" citStr="Harnad, 2003" startWordPosition="416" endWordPosition="417">sa/3.0/). Some rights reserved. things fall within its extension.. A word’s meaning is hence something closer to a rule for picking out its referent. Is the dictionary definition of a word, then, its meaning? Clearly, if we do not know the meaning of a word, we look up its definition in a dictionary. But what if we do not know the meaning of any of the words in its dictionary definition? And what if we don’t know the meanings of the words in the definitions of the words defining those words, and so on? This is a problem of infinite regress, called the “symbol grounding problem” (Harnad, 1990; Harnad, 2003): the meanings of words in dictionary definitions are, in and of themselves, ungrounded. The meanings of some of the words, at least, have to be grounded by some means other than dictionary definition look-up. How are word meanings grounded? Almost certainly in the sensorimotor capacity to pick out their referents (Harnad, 2005). Knowing what to do with what is not a matter of definition but of adaptive sensorimotor interaction between autonomous, behaving systems and categories of “objects” (including individuals, kinds, events, actions, traits and states). Our embodied sensorimotor systems c</context>
</contexts>
<marker>Harnad, 2003</marker>
<rawString>Harnad, S. (2003) Symbol-Grounding Problem. Encylopedia of Cognitive Science. Nature Publishing Group. Macmillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harnad</author>
</authors>
<title>To Cognize is to Categorize: Cognition is Categorization. In</title>
<date>2005</date>
<publisher>Elsevier.</publisher>
<contexts>
<context position="3022" citStr="Harnad, 2005" startWordPosition="470" endWordPosition="471">he meaning of any of the words in its dictionary definition? And what if we don’t know the meanings of the words in the definitions of the words defining those words, and so on? This is a problem of infinite regress, called the “symbol grounding problem” (Harnad, 1990; Harnad, 2003): the meanings of words in dictionary definitions are, in and of themselves, ungrounded. The meanings of some of the words, at least, have to be grounded by some means other than dictionary definition look-up. How are word meanings grounded? Almost certainly in the sensorimotor capacity to pick out their referents (Harnad, 2005). Knowing what to do with what is not a matter of definition but of adaptive sensorimotor interaction between autonomous, behaving systems and categories of “objects” (including individuals, kinds, events, actions, traits and states). Our embodied sensorimotor systems can also be described as applying information processing rules to inputs in order to generate the right outputs, just as a thermostat defending a temperature of 20 degrees can be. But this dynamic process is in no useful way analogous to looking up a definition in a dictionary. We will not be discussing sensorimotor grounding (Ba</context>
</contexts>
<marker>Harnad, 2005</marker>
<rawString>Harnad, S. (2005) To Cognize is to Categorize: Cognition is Categorization. In Lefebvre, C. and Cohen, H. (Eds.), Handbook of Categorization. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Hauk</author>
<author>M H Davis</author>
<author>F Kherif</author>
<author>F Pulverm¨uller</author>
</authors>
<title>Imagery or meaning? Evidence for a semantic origin of category-specific brain activity in metabolic imaging.</title>
<date>2008</date>
<journal>European Journal of Neuroscience</journal>
<volume>27</volume>
<issue>7</issue>
<pages>1856--1866</pages>
<marker>Hauk, Davis, Kherif, Pulverm¨uller, 2008</marker>
<rawString>Hauk, O., M.H. Davis, F. Kherif, F. Pulverm¨uller. (2008) Imagery or meaning? Evidence for a semantic origin of category-specific brain activity in metabolic imaging. European Journal of Neuroscience 27 (7) 1856-1866.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Karp</author>
</authors>
<title>Reducibility among combinatorial problems. In:</title>
<date>1972</date>
<journal>Complexity of Computer Computations,</journal>
<pages>85--103</pages>
<editor>R.E. Miller, J.W. Thatcher (Eds.),</editor>
<publisher>Plenum Press,</publisher>
<location>New York,</location>
<contexts>
<context position="17191" citStr="Karp, 1972" startWordPosition="3251" endWordPosition="3252">of G. Proof. (⇒) Let C be a cycle of G. By Lemma 5, U ∩ C =6 ∅, so that U is a minimum feedback vertex set of G. (⇐) Let G&apos; be the graph obtained from G by removing U. Then G&apos; is acyclic and ∅ is a grounding set of G&apos;. Therefore, U ∪ ∅ = U is a grounding set of G. Corollary 8. k-GS is NP-complete. 0 thing dark bad good 0 2 eatable 1 not 0 banana yellow fruit light 3 0 1 color or apple tomato red 20 Proof. Denote by k-FVS the problem of deciding whether a directed graph G admits a feedback vertex set of cardinality at most k. This problem is known to be NP-complete and has been widely studied (Karp, 1972; Garey &amp; Johnson, 1979). It follows directly from Theorem 7 that k-GS is NP-complete as well since the problems are equivalent. The fact that problems k-GS and k-FVS are equivalent is not very surprising. Indeed, roughly speaking, the minimum grounding problem consists of finding a minimum set large enough to enable the reader to learn (reach) all the words of the dictionary. On the other hand, the minimum feedback vertex set problem consists of finding a minimum set large enough to break the circularity of the definitions in the dictionary. Hence, the problems are the same, even if they are </context>
</contexts>
<marker>Karp, 1972</marker>
<rawString>Karp, R.M. (1972) Reducibility among combinatorial problems. In: R.E. Miller, J.W. Thatcher (Eds.), Complexity of Computer Computations, Plenum Press, New York, 1972, pp. 85-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<title>Meaning in Context. In</title>
<date>2007</date>
<publisher>Erlbaum.</publisher>
<contexts>
<context position="28997" citStr="Kintsch, 2007" startWordPosition="5305" endWordPosition="5306">ven the graph structure of a dictionary, to extract a grounding kernel therefrom. Extracting this structure in turn confronts us with two further problems: morphology and polysemy. Neither of these problems has a definite algorithmic solution. Morphology can be treated through stemming and associated look-up lists for the simplest cases (i.e., was → to be, and children → child), but more elaborate or complicated cases would require syntactic analysis or, ultimately, human evaluation. Polysemy is usually treated through statistical analysis of the word context (as in Latent Semantic Analysis) (Kintsch, 2007) or human evaluation. Indeed, a good deal of background knowledge is necessary to analyse an entry such as: “dominant: the fifth note of a musical scale of eight notes” (the LDOCE notes 16 different meanings of scale and 4 for dominant, and in our example, none of these words are used with their most frequent meaning). Correct disambiguation of a dictionary is timeconsuming work, as the most effective way to do it for now is through consensus among human evaluators. Fortunately, a fully disambiguated version of the WordNet database (Fellbaum,1998; Fellbaum, 2005) has just become available. We </context>
</contexts>
<marker>Kintsch, 2007</marker>
<rawString>Kintsch, W. (2007) Meaning in Context. In T.K. Landauer, D.S. McNamara, S. Dennis &amp; W. Kintsch (Eds.), Handbook of Latent Semantic Analysis. Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Procter</author>
</authors>
<title>Longman Dictionary of Contemporary English.</title>
<date>1978</date>
<publisher>Longman Group Ltd.,</publisher>
<location>Essex, UK.</location>
<contexts>
<context position="27630" citStr="Procter, 1978" startWordPosition="5090" endWordPosition="5091">perties that are also reflected in the mental lexicon. In parallel ongoing studies, we are finding that the words in the grounding kernel are indeed (1) more frequent in oral and written usage, (2) more concrete, (3) more readily imageable, and (4) learned earlier or at a younger age. We also expect they will be (5) more universal (across dictionaries, languages and cultures) (Chicoisne et al., 2008). 5 Grounding Kernels in Natural Language Dictionaries In earlier research (Clark, 2003), we have been analyzing two special dictionaries: the Longman’s Dictionary of Contemporary English (LDOCE) (Procter, 1978) and the Cambridge International Dictionary of English (CIDE) (Procter, 1995). Both are officially described as being based upon a defining vocabulary: a set of 2000 words which are purportedly the only words used in all the definitions of the dictionary, including the definitions of the defining vocabulary itself. A closer analysis of this defining vocabulary, however, has revealed that it is not always faithful to these constraints: A significant number of words used in the definitions turn out not to be in the defining vocabulary. Hence it became evident that we would ourselves have to gene</context>
</contexts>
<marker>Procter, 1978</marker>
<rawString>Procter, P. (1978) Longman Dictionary of Contemporary English. Longman Group Ltd., Essex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Procter</author>
</authors>
<title>Cambridge International Dictionary of English (CIDE).</title>
<date>1995</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="27707" citStr="Procter, 1995" startWordPosition="5100" endWordPosition="5101">udies, we are finding that the words in the grounding kernel are indeed (1) more frequent in oral and written usage, (2) more concrete, (3) more readily imageable, and (4) learned earlier or at a younger age. We also expect they will be (5) more universal (across dictionaries, languages and cultures) (Chicoisne et al., 2008). 5 Grounding Kernels in Natural Language Dictionaries In earlier research (Clark, 2003), we have been analyzing two special dictionaries: the Longman’s Dictionary of Contemporary English (LDOCE) (Procter, 1978) and the Cambridge International Dictionary of English (CIDE) (Procter, 1995). Both are officially described as being based upon a defining vocabulary: a set of 2000 words which are purportedly the only words used in all the definitions of the dictionary, including the definitions of the defining vocabulary itself. A closer analysis of this defining vocabulary, however, has revealed that it is not always faithful to these constraints: A significant number of words used in the definitions turn out not to be in the defining vocabulary. Hence it became evident that we would ourselves have to generate a grounding kernel (roughly equivalent to the defining vocabulary) from </context>
</contexts>
<marker>Procter, 1995</marker>
<rawString>Procter, P. (1995) Cambridge International Dictionary of English (CIDE). Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z W Pylyshyn</author>
</authors>
<title>Computation and Cognition: Towards a Foundation for Cognitive Science.</title>
<date>1984</date>
<publisher>MIT Press.</publisher>
<location>Cambridge:</location>
<contexts>
<context position="23322" citStr="Pylyshyn, 1984" startWordPosition="4400" endWordPosition="4401">d “yellow”, and level 3 words are “apple”, “banana” and “tomato”. 4 Grounding Sets and the Mental Lexicon In Section 3, we introduced all the necessary terminology to study the symbol grounding problem using graph theory and digital dictionaries. In this section, we explain how this model can be useful and on what assumptions it is based. A dictionary is a formal symbol system. The preceding section showed how formal methods can be applied to this system in order to extract formal features. In cognitive science, this is the basis of computationalism (or cognitivism or “disembodied cognition” (Pylyshyn, 1984)), according to which cognition, too, is a formal symbol system – one that can be studied and explained independently of the hardware (or, insofar as it concerns humans, the wetware) on which it is implemented. However, pure computationalism is vulnerable to the problem of the grounding of symbols too (Harnad, 1990). Some of this can be remedied by the competing paradigm of embodied cognition (Barsalou, 2008; Glenberg &amp; Robertson, 2002; Steels, 2007), which draws on dynamical (noncomputational) systems theory to ground cognition in sensorimotor experience. Although computationalism and symbol </context>
</contexts>
<marker>Pylyshyn, 1984</marker>
<rawString>Pylyshyn, Z. W. (1984) Computation and Cognition: Towards a Foundation for Cognitive Science. Cambridge: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ravasz</author>
<author>A L Barabasi</author>
</authors>
<title>Hierarchical organization in complex networks.</title>
<date>2003</date>
<journal>Physical Review E</journal>
<volume>67</volume>
<pages>026112</pages>
<contexts>
<context position="4591" citStr="Ravasz &amp; Barabasi, 2003" startWordPosition="727" endWordPosition="730">2008 somehow. A natural first hypothesis is that the grounding words ought to be more concrete, referring to things that are closer to our overt sensorimotor experience, and learned earlier, but that remains to be tested (Clark, 2003). Apart from the question of the boundary conditions of grounding, however, there are basic questions to be asked about the structure of word meanings in dictionary definition space. In the path from a word, to the definition of that word, to the definition of the words in the definition of that word, and so on, through what sort of a structure are we navigating (Ravasz &amp; Barabasi, 2003; Steyvers &amp; Tenenbaum, 2005)? Meaning is compositional: A definition is composed of words, combined according to syntactic rules to form a proposition (with a truth value: true or false). For example, the word to be defined w (the “definiendum”) might mean w1 &amp; w2 &amp; ... &amp; wn, where the wi are other words (the “definientes”) in its definition. Rarely does that proposition provide the full necessary and sufficient conditions for identifying the referent of the word, w, but the approximation must at least be close enough to allow most people, armed with the definition, to understand and use the </context>
</contexts>
<marker>Ravasz, Barabasi, 2003</marker>
<rawString>Ravasz, E. &amp; Barabasi, A. L. (2003) Hierarchical organization in complex networks. Physical Review E 67, 026112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K H Rosen</author>
</authors>
<title>Discrete mathematics and its applications, 6th ed.</title>
<date>2007</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="7045" citStr="Rosen, 2007" startWordPosition="1132" endWordPosition="1133"> 5 deal with the implication of this approach in cognitive sciences and show in what ways grounding kernels may be useful. 2 Definitions and Notations In this section, we give mathematical definitions for the dictionary-related terminology, relate them to natural language dictionaries and supply the pertinent graph theoretical definitions. Additional details are given to ensure mutual comprehensibility to specialists in the three disciplines involved (mathematics, linguistics and psychology). Complete introductions to graph theory and discrete mathematics are provided in (Bondy &amp; Murty, 1978; Rosen, 2007). 2.1 Relations and Functions Let A be any set. A binary relation on A is any subset R of A x A. We write xRy if (x, y) E R. The relation R is said to be (1) reflexive if for all x E A, we have xRx, (2) symmetric if for all x, y E A such that xRy, we have yRx and (3) transitive if for all x, y, z E A such that xRy and yRz, we have xRz. The relation R is an equivalence relation if it is reflexive, symmetric and transitive. For any x E A, the equivalence class of x, designated by [x], is given by [x] = {y E A |xRy}. It is easy to show that [x] = [y] if and only if xRy and that the set of all equ</context>
</contexts>
<marker>Rosen, 2007</marker>
<rawString>Rosen, K.H. (2007) Discrete mathematics and its applications, 6th ed. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Steels</author>
</authors>
<title>The symbol grounding problem is solved, so what’s next? In</title>
<date>2007</date>
<publisher>Academic Press, North Haven.</publisher>
<contexts>
<context position="3677" citStr="Steels, 2007" startWordPosition="574" endWordPosition="575">tter of definition but of adaptive sensorimotor interaction between autonomous, behaving systems and categories of “objects” (including individuals, kinds, events, actions, traits and states). Our embodied sensorimotor systems can also be described as applying information processing rules to inputs in order to generate the right outputs, just as a thermostat defending a temperature of 20 degrees can be. But this dynamic process is in no useful way analogous to looking up a definition in a dictionary. We will not be discussing sensorimotor grounding (Barsalou, 2008; Glenberg &amp; Robertson, 2002; Steels, 2007) in this paper. We will assume some sort of grounding as given: when we consult a dictionary, we already know the meanings of at least some words, 17 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 17–24 Manchester, August 2008 somehow. A natural first hypothesis is that the grounding words ought to be more concrete, referring to things that are closer to our overt sensorimotor experience, and learned earlier, but that remains to be tested (Clark, 2003). Apart from the question of the boundary conditions of grounding, however,</context>
<context position="23776" citStr="Steels, 2007" startWordPosition="4476" endWordPosition="4477">em in order to extract formal features. In cognitive science, this is the basis of computationalism (or cognitivism or “disembodied cognition” (Pylyshyn, 1984)), according to which cognition, too, is a formal symbol system – one that can be studied and explained independently of the hardware (or, insofar as it concerns humans, the wetware) on which it is implemented. However, pure computationalism is vulnerable to the problem of the grounding of symbols too (Harnad, 1990). Some of this can be remedied by the competing paradigm of embodied cognition (Barsalou, 2008; Glenberg &amp; Robertson, 2002; Steels, 2007), which draws on dynamical (noncomputational) systems theory to ground cognition in sensorimotor experience. Although computationalism and symbol grounding provide the background context for our investigations and findings, the present paper does not favor any particular theory of mental representation of meaning. A dictionary is a symbol system that relates words to words in such a way that the meanings of the definienda are conveyed via the definientes. The user is intended to arrive at an understanding of an unknown word through an understanding of its definition. What was formally demonstr</context>
</contexts>
<marker>Steels, 2007</marker>
<rawString>Steels, L. (2007) The symbol grounding problem is solved, so what’s next? In De Vega, M. and G. Glenberg and A. Graesser (Eds.), Symbols, embodiment and meaning. Academic Press, North Haven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
<author>J B Tenenbaum</author>
</authors>
<title>The largescale structure of semantic networks: statistical analyses and a model of semantic growth.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>41--78</pages>
<contexts>
<context position="4620" citStr="Steyvers &amp; Tenenbaum, 2005" startWordPosition="731" endWordPosition="734">irst hypothesis is that the grounding words ought to be more concrete, referring to things that are closer to our overt sensorimotor experience, and learned earlier, but that remains to be tested (Clark, 2003). Apart from the question of the boundary conditions of grounding, however, there are basic questions to be asked about the structure of word meanings in dictionary definition space. In the path from a word, to the definition of that word, to the definition of the words in the definition of that word, and so on, through what sort of a structure are we navigating (Ravasz &amp; Barabasi, 2003; Steyvers &amp; Tenenbaum, 2005)? Meaning is compositional: A definition is composed of words, combined according to syntactic rules to form a proposition (with a truth value: true or false). For example, the word to be defined w (the “definiendum”) might mean w1 &amp; w2 &amp; ... &amp; wn, where the wi are other words (the “definientes”) in its definition. Rarely does that proposition provide the full necessary and sufficient conditions for identifying the referent of the word, w, but the approximation must at least be close enough to allow most people, armed with the definition, to understand and use the defined word most of the time</context>
</contexts>
<marker>Steyvers, Tenenbaum, 2005</marker>
<rawString>Steyvers, M. &amp; Tenenbaum J.B. (2005) The largescale structure of semantic networks: statistical analyses and a model of semantic growth. Cognitive Science, 29(1) 41-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tarjan</author>
</authors>
<title>Depth-first search and linear graph algorithms.</title>
<date>1972</date>
<journal>SIAM Journal on Computing.</journal>
<volume>1</volume>
<issue>2</issue>
<pages>146--160</pages>
<marker>Tarjan, 1972</marker>
<rawString>Tarjan, R. (1972) Depth-first search and linear graph algorithms. SIAM Journal on Computing. 1 (2) 146-160.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>