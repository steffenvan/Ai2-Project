<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005482">
<title confidence="0.99841">
Active Learning for the Identification of Nonliteral Language �
</title>
<author confidence="0.98814">
Julia Birke and Anoop Sarkar
</author>
<affiliation confidence="0.993315">
School of Computing Science, Simon Fraser University
</affiliation>
<address confidence="0.94654">
Burnaby, BC, V5A 1S6, Canada
</address>
<email confidence="0.990117">
jbirke@alumni.sfu.ca, anoop@cs.sfu.ca
</email>
<sectionHeader confidence="0.993716" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999446">
In this paper we present an active learn-
ing approach used to create an annotated
corpus of literal and nonliteral usages
of verbs. The model uses nearly unsu-
pervised word-sense disambiguation and
clustering techniques. We report on exper-
iments in which a human expert is asked
to correct system predictions in different
stages of learning: (i) after the last iter-
ation when the clustering step has con-
verged, or (ii) during each iteration of the
clustering algorithm. The model obtains
an f-score of 53.8% on a dataset in which
literal/nonliteral usages of 25 verbs were
annotated by human experts. In compari-
son, the same model augmented with ac-
tive learning obtains 64.91%. We also
measure the number of examples required
when model confidence is used to select
examples for human correction as com-
pared to random selection. The results of
this active learning system have been com-
piled into a freely available annotated cor-
pus of literal/nonliteral usage of verbs in
context.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926125">
In this paper, we propose a largely automated
method for creating an annotated corpus of literal vs.
nonliteral usages of verbs. For example, given the
verb “pour”, we would expect our method to iden-
tify the sentence “Custom demands that cognac be
poured from a freshly opened bottle” as literal, and
the sentence “Salsa and rap music pour out of the
windows” as nonliteral, which, indeed, it does.
</bodyText>
<footnote confidence="0.95678025">
&amp;quot;This research was partially supported by NSERC, Canada
(RGPIN: 264905). We would like to thank Bill Dolan, Fred
Popowich, Dan Fass, Katja Markert, Yudong Liu, and the
anonymous reviewers for their comments.
</footnote>
<page confidence="0.997266">
21
</page>
<bodyText confidence="0.999940536585366">
We reduce the problem of nonliteral language
recognition to one of word-sense disambiguation
(WSD) by redefining literal and nonliteral as two
different senses of the same word, and we adapt
an existing similarity-based word-sense disambigua-
tion method to the task of separating usages of verbs
into literal and nonliteral clusters. Note that treat-
ing this task as similar to WSD only means that
we use features from the local context around the
verb to identify it as either literal or non-literal. It
does not mean that we can use a classifier trained on
WSD annotated corpora to solve this issue, or use
any existing WSD classification technique that re-
lies on supervised learning. We do not have any an-
notated data to train such a classifier, and indeed our
work is focused on building such a dataset. Indeed
our work aims to first discover reliable seed data
and then bootstrap a literal/nonliteral identification
model. Also, we cannot use any semi-supervised
learning algorithm for WSD which relies on reliably
annotated seed data since we do not possess any reli-
ably labeled data (except for our test data set). How-
ever we do exploit a noisy source of seed data in
a nearly unsupervised approach augmented with ac-
tive learning. Noisy data containing example sen-
tences of literal and nonliteral usage of verbs is used
in our model to cluster a particular instance of a verb
into one class or the other. This paper focuses on the
use of active learning using this model. We suggest
that this approach produces a large saving of effort
compared to creating such an annotated corpus man-
ually.
An active learning approach to machine learn-
ing is one in which the learner has the ability to
influence the selection of at least a portion of its
training data. In our approach, a clustering algo-
rithm for literal/nonliteral recognition tries to anno-
tate the examples that it can, while in each iteration
it sends a small set of examples to a human expert
to annotate, which in turn provides additional ben-
efit to the bootstrapping process. Our active learn-
</bodyText>
<note confidence="0.535718">
Proceedings of the Workshop on Computational Approaches to Figurative Language, pages 21–28,
Rochester, NY, April 26, 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998705875">
ing method is similar to the Uncertainty Sampling
algorithm of (Lewis &amp; Gale, 1994) but in our case
interacts with iterative clustering. As we shall see,
some of the crucial criticisms leveled against un-
certainty sampling and in favor of Committee-based
sampling (Engelson &amp; Dagan, 1996) do not apply in
our case, although the latter may still be more accu-
rate in our task.
</bodyText>
<sectionHeader confidence="0.922688" genericHeader="method">
2 Literal vs. Nonliteral Identification
</sectionHeader>
<bodyText confidence="0.99997845882353">
For the purposes of this paper we will take the sim-
plified view that literal is anything that falls within
accepted selectional restrictions (“he was forced to
eat his spinach” vs. “he was forced to eat his words”)
or our knowledge of the world (“the sponge ab-
sorbed the water” vs. “the company absorbed the
loss”). Nonliteral is then anything that is “not lit-
eral”, including most tropes, such as metaphors, id-
ioms, as well as phrasal verbs and other anomalous
expressions that cannot really be seen as literal. We
aim to automatically discover the contrast between
the standard set of selectional restrictions for the lit-
eral usage of verbs and the non-standard set which
we assume will identify the nonliteral usage.
Our identification model for literal vs. nonliteral
usage of verbs is described in detail in a previous
publication (Birke &amp; Sarkar, 2006). Here we pro-
vide a brief description of the model so that the use
of this model in our proposed active learning ap-
proach can be explained.
Since we are attempting to reduce the problem
of literal/nonliteral recognition to one of word-sense
disambiguation, we use an existing similarity-based
word-sense disambiguation algorithm developed by
(Karov &amp; Edelman, 1998), henceforth KE. The KE
algorithm is based on the principle of attraction:
similarities are calculated between sentences con-
taining the word we wish to disambiguate (the target
word) and collections of seed sentences (feedback
sets). It requires a target set – the set of sentences
containing the verbs to be classified into literal or
nonliteral – and the seed sets: the literal feedback
set and the nonliteral feedback set. A target set sen-
tence is considered to be attracted to the feedback set
containing the sentence to which it shows the highest
similarity. Two sentences are similar if they contain
similar words and two words are similar if they are
contained in similar sentences. The resulting transi-
tive similarity allows us to defeat the knowledge ac-
quisition bottleneck – i.e. the low likelihood of find-
ing all possible usages of a word in a single corpus.
Note that the KE algorithm concentrates on similari-
ties in the way sentences use the target literal or non-
literal word, not on similarities in the meanings of
the sentences themselves.
Algorithms 1 and 2 summarize our approach.
Note that p* s) is the unigram probability of word
w in sentence s, normalized by the total number of
words in s. We omit some details about the algo-
rithm here which do not affect our discussion about
active learning. These details are provided in a pre-
vious publication (Birke &amp; Sarkar, 2006).
As explained before, our model requires a target
set and two seed sets: the literal feedback set and
the nonliteral feedback set. We do not explain the
details of how these feedback sets were constructed
in this paper, however, it is important to note that the
feedback sets themselves are noisy and not carefully
vetted by human experts. The literal feedback set
was built from WSJ newswire text, and for the non-
literal feedback set, we use expressions from vari-
ous datasets such as the Wayne Magnuson English
Idioms Sayings &amp; Slang and George Lakoff’s Con-
ceptual Metaphor List, as well as example sentences
from these sources. These datasets provide lists of
verbs that may be used in a nonliteral usage, but we
cannot explicitly provide only those sentences that
contain nonliteral use of that verb in the nonliteral
feedback set. In particular, knowing that an expres-
sion can be used nonliterally does not mean that you
can tell when it is being used nonliterally. In fact
even the literal feedback set has noise from nonlit-
eral uses of verbs in the news articles. To deal with
this issue (Birke &amp; Sarkar, 2006) provides automatic
methods to clean up the feedback sets during the
clustering algorithm. Note that the feedback sets are
not cleaned up by human experts, however the test
data is carefully annotated by human experts (details
about inter-annotator agreement on the test set are
provided below). The test set is not large enough to
be split up into a training and test set that can support
learning using a supervised learning method.
The sentences in the target set and feedback sets
were augmented with some shallow syntactic in-
formation such as part of speech tags provided
</bodyText>
<page confidence="0.97788">
22
</page>
<bodyText confidence="0.721355">
Algorithm 1 KE-train: (Karov &amp; Edelman, 1998) algorithm adapted to literal/nonliteral identification
</bodyText>
<listItem confidence="0.950139818181818">
Require: S: the set of sentences containing the target word (each sentence is classified as literal/nonliteral)
Require: G: the set of literal seed sentences
Require: N: the set of nonliteral seed sentences
Require: W: the set of words/features, w E s means w is in sentence s, s 3 w means s contains w
Require: E: threshold that determines the stopping condition
1: w-sim0(wx, wy) := 1 if wx = wy, 0 otherwise
2: s-simI0(sx, sy) := 1, for all sx, sy E S x S where sx = sy, 0 otherwise
3: i := 0
4: while (true) do
5: s-simLi+1(sx, sy) := Ewx∈sx p(wx, sx) maxwy∈sy w-simi(wx, wy), for all sx, sy E S x G
6: s-simNi+1(sx, sy) := Ewx∈sx p(wx, sx) maxwy∈sy w-simi(wx, wy), for all sx, sy E S x N
7: for wx, wy E W x W do
y3wy
8: w-simi+1(wx, wy) :={ else 0 EsXE)wX p(wX, SX) maxs,3wy {s-simiLi (sxssy ), s-simNi (sx, sy)}
9: end for
10: if bwx, maxwy{w-simi+1(wx, wy) − w-simi(wx, wy)} &lt; E then
11: break # algorithm converges in 1 � steps.
12: end if
13: i := i + 1
14: end while
by a statistical tagger (Ratnaparkhi, 1996) and Su-
perTags (Bangalore &amp; Joshi, 1999).
</listItem>
<bodyText confidence="0.999731891304348">
This model was evaluated on 25 target verbs:
absorb, assault, die, drag, drown, escape,
examine, fill, fix, flow, grab, grasp, kick,
knock, lend, miss, pass, rest, ride, roll,
smooth, step, stick, strike, touch
The verbs were carefully chosen to have vary-
ing token frequencies (we do not simply learn on
frequently occurring verbs). As a result, the tar-
get sets contain from 1 to 115 manually annotated
sentences for each verb to enable us to measure ac-
curacy. The annotations were not provided to the
learning algorithm: they were only used to evaluate
the test data performance. The first round of anno-
tations was done by the first annotator. The second
annotator was given no instructions besides a few
examples of literal and nonliteral usage (not cov-
ering all target verbs). The authors of this paper
were the annotators. Our inter-annotator agreement
on the annotations used as test data in the experi-
ments in this paper is quite high. κ (Cohen) and κ
(S&amp;C) on a random sample of 200 annotated exam-
ples annotated by two different annotators was found
to be 0.77. As per ((Di Eugenio &amp; Glass, 2004), cf.
refs therein), the standard assessment for κ values is
that tentative conclusions on agreement exists when
.67 &lt; κ &lt; .8, and a definite conclusion on agree-
ment exists when κ &gt; .8.
In the case of a larger scale annotation effort, hav-
ing the person leading the effort provide one or two
examples of literal and nonliteral usages for each tar-
get verb to each annotator would almost certainly
improve inter-annotator agreement.
The algorithms were evaluated based on how
accurately they clustered the hand-annotated sen-
tences. Sentences that were attracted to neither clus-
ter or were equally attracted to both were put in the
opposite set from their label, making a failure to
cluster a sentence an incorrect clustering.
Evaluation results were recorded as recall, preci-
sion, and f-score values. Literal recall is defined as
(correct literals in literal cluster /total correct liter-
als). Literal precision is defined as (correct literals
in literal cluster /size of literal cluster). If there are
no literals, literal recall is 100%; literal precision is
100% if there are no nonliterals in the literal clus-
ter and 0% otherwise. The f-score is defined as (2 �
</bodyText>
<page confidence="0.887366">
23
</page>
<bodyText confidence="0.994653833333333">
Algorithm 2 KE-test: classifying literal/nonliteral cluster – i.e. those sentences where attraction to
either feedback set, or the absolute difference of
the two attractions, falls below a given threshold.
The number of sentences falling under this threshold
varies considerably from word to word, so we ad-
ditionally impose a predetermined cap on the num-
ber of sentences that can ultimately be sent to the
human. Based on an experiment on a held-out set
separate from our target set of sentences, sending
a maximum of 30% of the original set was deter-
mined to be optimal in terms of eventual accuracy
obtained. We impose an order on the candidate sen-
tences using similarity values. This allows the origi-
nal sentences with the least similarity to either feed-
back set to be sent to the human first. Further, we
alternate positive similarity (or absolute difference)
values and values of zero. Note that sending ex-
amples that score zero to the human may not help
attract new sentences to either of the feedback sets
(since scoring zero means that the sentence was not
attracted to any of the sentences). However, human
help may be the only chance these sentences have to
be clustered at all.
After the human provides an identification for a
particular example we move the sentence not only
into the correct cluster, but also into the correspond-
ing feedback set so that other sentences might be
attracted to this certifiably correctly classified sen-
tence.
The second question is when to send the sentences
to the human. We can send all the examples after
the first iteration, after some intermediate iteration,
distributed across iterations, or at the end. Sending
everything after the first iteration is best for coun-
teracting false attractions before they become en-
trenched and for allowing future iterations to learn
from the human decisions. Risks include sending
sentences to the human before our model has had
a chance to make potentially correct decision about
them, counteracting any saving of effort. (Karov &amp;
Edelman, 1998) state that the results are not likely
to change much after the third iteration and we have
confirmed this independently: similarity values con-
tinue to change until convergence, but cluster al-
legiance tends not to. Sending everything to the
human after the third iteration could therefore en-
tail some of the damage control of sending every-
thing after the first iteration while giving the model
</bodyText>
<listItem confidence="0.960848428571429">
1: For any sentence sx E S
2: if saxs-simL(sx,sy) &gt; mIs-simN(sx,sy)
then
3: tag sx as literal
4: else
5: tag sx as nonliteral
6: end if
</listItem>
<bodyText confidence="0.993723263157895">
precision · recall) / (precision + recall). Nonliteral
precision and recall are defined similarly. Average
precision is the average of literal and nonliteral pre-
cision; similarly for average recall. For overall per-
formance, we take the f-score of average precision
and average recall.
We calculated two baselines for each word. The
first was a simple majority-rules baseline (assign
each word to the sense which is dominant which
is always literal in our dataset). Due to the imbal-
ance of literal and nonliteral examples, this baseline
ranges from 60.9% to 66.7% for different verbs with
an average of 63.6%. Keep in mind though that us-
ing this baseline, the f-score for the nonliteral set
will always be 0% – which is the problem we are
trying to solve in this work. We calculated a second
baseline using a simple attraction algorithm. Each
sentence in the target set is attracted to the feedback
set with which it has the most words in common.
For the baseline and for our own model, sentences
attracted to neither, or equally to both sets are put
in the opposite cluster to which they belong. This
second baseline obtains a f-score of 29.36% while
the weakly supervised model without active learn-
ing obtains an f-score of 53.8%. Results for each
verb are shown in Figure 1.
3 Active Learning
The model described thus far is weakly supervised.
The main proposal in this paper is to push the re-
sults further by adding in an active learning compo-
nent, which puts the model described in Section 2 in
the position of helping a human expert with the lit-
eral/nonliteral clustering task. The two main points
to consider are: what to send to the human annotator,
and when to send it.
We always send sentences from the undecided
24
a chance to do its best. Another possibility is to
send the sentences in small doses in order to gain
some bootstrapping benefit at each iteration i.e. the
certainty measures will improve with each bit of hu-
man input, so at each iteration more appropriate sen-
tences will be sent to the human. Ideally, this would
produce a compounding of benefits. On the other
hand, it could produce a compounding of risks. A fi-
nal possibility is to wait until the last iteration in the
hope that our model has correctly clustered every-
thing else and those correctly labeled examples do
not need to be examined by the human. This imme-
diately destroys any bootstrapping possibilities for
the current run, although it still provides benefits for
iterative augmentation runs (see Section 4).
A summary of our results in shown in Figure 1.
The last column in the graph shows the average
across all the target verbs. We now discuss the vari-
ous active learning experiments we performed using
our model and a human expert annotator.
</bodyText>
<subsectionHeader confidence="0.991625">
3.1 Experiment 1
</subsectionHeader>
<bodyText confidence="0.99999855">
Experiments were performed to determine the best
time to send up to 30% of the sentences to the human
annotator. Sending everything after the first iteration
produced an average accuracy of 66.8%; sending ev-
erything after the third iteration, 65.2%; sending a
small amount at each iteration, 60.8%; sending ev-
erything after the last iteration, 64.9%. Going just by
the average accuracy, the first iteration option seems
optimal. However, several of the individual word re-
sults fell catastrophically below the baseline, mainly
due to original sentences having been moved into a
feedback set too early, causing false attraction. This
risk was compounded in the distributed case, as pre-
dicted. The third iteration option gave slightly bet-
ter results (0.3%) than the last iteration option, but
since the difference was minor, we opted for the sta-
bility of sending everything after the last iteration.
These results show an improvement of 11.1% over
the model from Section 2. Individual results for each
verb are given in Figure 1.
</bodyText>
<subsectionHeader confidence="0.973556">
3.2 Experiment 2
</subsectionHeader>
<bodyText confidence="0.9999902">
In a second experiment, rather than letting our model
select the sentences to send to the human, we se-
lected them randomly. We found no significant dif-
ference in the results. For the random model to out-
perform the non-random one it would have to select
only sentences that our model would have clustered
incorrectly; to do worse it would have to select only
sentences that our model could have handled on its
own. The likelihood of the random choices coming
exclusively from these two sets is low.
</bodyText>
<subsectionHeader confidence="0.992417">
3.3 Experiment 3
</subsectionHeader>
<bodyText confidence="0.99999595">
Our third experiment considers the effort-savings of
using our literal/nonliteral identification model. The
main question must be whether the 11.1% accuracy
gain of active learning is worth the effort the hu-
man must contribute. In our experiments, the hu-
man annotator is given at most 30% of the sentences
to classify manually. It is expected that the human
will classify these correctly and any additional ac-
curacy gain is contributed by the model. Without
semi-supervised learning, we might expect that if
the human were to manually classify 30% of the sen-
tences chosen at random, he would have 30% of the
sentences classified correctly. However, in order to
be able to compare the human-only scenario to the
active learning scenario, we must find what the av-
erage f-score of the manual process is. The f-score
depends on the distribution of literal and nonliteral
sentences in the original set. For example, in a set
of 100 sentences, if there are exactly 50 of each, and
of the 30 chosen for manual annotation, half come
from the literal set and half come from the nonlit-
eral set, the f-score will be exactly 30%. We could
compare our performance to this, but that would be
unfair to the manual process since the sets on which
we did our evaluation were by no means balanced.
We base a hypothetical scenario on the heavy imbal-
ance often seen in our evaluation sets, and suggest
a situation where 96 of our 100 sentences are literal
and only 4 are nonliteral. If it were to happen that all
4 of the nonliteral sentences were sent to the human,
we would get a very high f-score, due to a perfect
recall score for the nonliteral cluster and a perfect
precision score for the literal cluster. If none of the
four nonliteral sentences were sent to the human, the
scores for the nonliteral cluster would be disastrous.
This situation is purely hypothetical, but should ac-
count for the fact that 30 out of 100 sentences an-
notated by a human will not necessarily result in an
average f-score of 30%: in fact, averaging the re-
sults of the three sitatuations described above results
</bodyText>
<page confidence="0.99812">
25
</page>
<figureCaption confidence="0.81422325">
Figure 1: Active Learning evaluation results. Baseline refers to the second baseline from Section 2. Semi-
supervised: Trust Seed Data refers to the standard KE model that trusts the seed data. Optimal Semi-
supervised refers to the augmented KE model described in (Birke &amp; Sarkar, 2006). Active Learning refers
to the model proposed in this paper.
</figureCaption>
<bodyText confidence="0.999974925925926">
in an avarage f-score of nearly 36.9%. This is 23%
higher than the 30% of the balanced case, which is
1.23 times higher. For this reason, we give the hu-
man scores a boost by assuming that whatever the
human annotates in the manual scenario will result
in an f-score that is 1.23 times higher. For our ex-
periment, we take the number of sentences that our
active learning method sent to the human for each
word – note that this is not always 30% of the to-
tal number of sentences – and multiply that by 1.23
– to give the human the benefit of the doubt, so to
speak. Still we find that using active learning gives
us an avarage accuracy across all words of 64.9%,
while we get only 21.7% with the manual process.
This means that for the same human effort, using
the weakly supervised classifier produced a three-
fold improvement in accuracy. Looking at this con-
versely, this means that in order to obtain an ac-
curacy of 64.9%, by a purely manual process, the
human would have to classify nearly 53.6% of the
sentences, as opposed to the 17.7% he needs to do
using active learning. This is an effort-savings of
about 35%. To conclude, we claim that our model
combined with active learning is a helpful tool for
a literal/nonliteral clustering project. It can save the
human significant effort while still producing rea-
sonable results.
</bodyText>
<sectionHeader confidence="0.924682" genericHeader="method">
4 Annotated corpus built using active
</sectionHeader>
<subsectionHeader confidence="0.554047">
learning
</subsectionHeader>
<bodyText confidence="0.9998925">
In this section we discuss the development of an an-
notated corpus of literal/nonliteral usages of verbs
in context. First, we examine iterative augmenta-
tion. Then we discuss the structure and contents of
the annotated corpus and the potential for expansion.
After an initial run for a particular target word, we
have the cluster results plus a record of the feedback
sets augmented with the newly clustered sentences.
</bodyText>
<page confidence="0.992635">
26
</page>
<note confidence="0.711299333333333">
***pour***
*nonliteral cluster*
wsj04:7878 N As manufacturers get bigger, they are likely to
pour more money into the battle for shelf space , raising the
ante for new players ./.
wsj25:3283 N Salsa and rap music pour out of the windows./.
wsj06:300 U Investors hungering for safety and high yields
are pouring record sums into single-premium , interest-earning
annuities ./.
</note>
<figure confidence="0.635945666666667">
*literal cluster*
wsj59:3286 L Custom demands that cognac be poured from a
freshly opened bottle./.
</figure>
<figureCaption confidence="0.9836055">
Figure 2: Excerpt from our annotated corpus of lit-
eral/nonliteral usages of verbs in context.
</figureCaption>
<bodyText confidence="0.999286161290323">
Each feedback set sentence is saved with a weight,
with newly clustered sentences receiving a weight
of 1.0. Subsequent runs may be done to augment
the initial clusters. For these runs, we use the the
output identification over the examples from our ini-
tial run as feedback sets. New sentences for cluster-
ing are treated like a regular target set. Running the
algorithm in this way produces new clusters and a
re-weighted model augmented with newly clustered
sentences. There can be as many runs as desired;
hence iterative augmentation.
We used the iterative augmentation process to
build a small annotated corpus consisting of the tar-
get words from Table 1, as well as another 25 words
drawn from the examples of previously published
work (see Section 5). It is important to note that
in building the annotated corpus, we used the Ac-
tive Learning component as described in this paper,
which improved our average f-score from 53.8% to
64.9% on the original 25 target words, and we ex-
pect also improved performance on the remainder of
the words in the annotated corpus.
An excerpt from the annotated corpus is shown in
Figure 2. Each entry includes an ID number and a
Nonliteral, Literal, or Unannotated tag. Annotations
are from testing or from active learning during anno-
tated corpus construction. The corpus is available at
http://www.cs.sfu.ca/∼anoop/students/jbirke/. Fur-
ther unsupervised expansion of the existing clusters
as well as the production of additional clusters is a
possibility.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="method">
5 Previous Work
</sectionHeader>
<bodyText confidence="0.99974368">
To our knowledge there has not been any previous
work done on taking a model for literal/nonliteral
language and augmenting it with an active learning
approach which allows human expert knowledge to
become part of the learning process.
Our approach to active learning is similar to the
Uncertainty Sampling approach of (Lewis &amp; Gale,
1994) and (Fujii et. al., 1998) in that we pick those
examples that we could not classify due to low con-
fidence in the labeling at a particular point. We
employ a resource-limited version in which only
a small fixed sample is ever annotated by a hu-
man. Some of the criticisms leveled against un-
certainty sampling and in favor of Committee-based
sampling (Engelson &amp; Dagan, 1996) (and see refs
therein) do not apply in our case.
Our similarity measure is based on two views of
sentence- and word-level similarity and hence we
get an estimate of appropriate identification rather
than just correct classification. As a result, by em-
bedding an Uncertainty Sampling active learning
model within a two-view clustering algorithm, we
gain the same advantages as other uncertainty sam-
pling methods obtain when used in bootstrapping
methods (e.g. (Fujii et. al., 1998)). Other machine
learning approaches that derive from optimal exper-
iment design are not appropriate in our case because
we do not yet have a strong predictive (or generative)
model of the literal/nonliteral distinction.
Our machine learning model only does identifi-
cation of verb usage as literal or nonliteral but it
can be seen as a first step towards the use of ma-
chine learning for more sophisticated metaphor and
metonymy processing tasks on larger text corpora.
Rule-based systems – some using a type of interlin-
gua (Russell, 1976); others using complicated net-
works and hierarchies often referred to as metaphor
maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992)
– must be largely hand-coded and generally work
well on an enumerable set of metaphors or in lim-
ited domains. Dictionary-based systems use exist-
ing machine-readable dictionaries and path lengths
between words as one of their primary sources
for metaphor processing information (e.g. (Dolan,
1995)). Corpus-based systems primarily extract or
learn the necessary metaphor-processing informa-
tion from large corpora, thus avoiding the need for
manual annotation or metaphor-map construction.
Examples of such systems are (Murata et. al., 2000;
Nissim &amp; Markert, 2003; Mason, 2004).
</bodyText>
<page confidence="0.99271">
27
</page>
<bodyText confidence="0.9997502">
Nissim &amp; Markert (2003) approach metonymy
resolution with machine learning methods, “which
[exploit] the similarity between examples of con-
ventional metonymy” ((Nissim &amp; Markert, 2003),
p. 56). They see metonymy resolution as a classi-
fication problem between the literal use of a word
and a number of pre-defined metonymy types. They
use similarities between possibly metonymic words
(PMWs) and known metonymies as well as context
similarities to classify the PMWs.
Mason (2004) presents CorMet, “a corpus-based
system for discovering metaphorical mappings be-
tween concepts” ((Mason, 2004), p. 23). His system
finds the selectional restrictions of given verbs in
particular domains by statistical means. It then finds
metaphorical mappings between domains based on
these selectional preferences. By finding seman-
tic differences between the selectional preferences,
it can “articulate the higher-order structure of con-
ceptual metaphors” ((Mason, 2004), p. 24), finding
mappings like LIQUID-*MONEY.
Metaphor processing has even been ap-
proached with connectionist systems storing
world-knowledge as probabilistic dependencies
(Narayanan, 1999).
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999769692307693">
In this paper we presented a system for separating
literal and nonliteral usages of verbs through statis-
tical word-sense disambiguation and clustering tech-
niques. We used active learning to combine the pre-
dictions of this system with a human expert anno-
tator in order to boost the overall accuracy of the
system by 11.1%. We used the model together with
active learning and iterative augmentation, to build
an annotated corpus which is publicly available, and
is a resource of literal/nonliteral usage clusters that
we hope will be useful not only for future research in
the field of nonliteral language processing, but also
as training data for other statistical NLP tasks.
</bodyText>
<sectionHeader confidence="0.999268" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998738736842105">
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging:
an approach to almost parsing. Comput. Linguist. 25, 2 (Jun.
1999), 237-265.
Julia Birke and Anoop Sarkar. 2006. In Proceedings of the 11th
Conference of the European Chapter of the Association for
Computational Linguistics, EACL-2006. Trento, Italy. April
3-7.
Barbara Di Eugenio and Michael Glass. 2004. The kappa
statistic: a second look. Comput. Linguist. 30, 1 (Mar. 2004),
95-101.
William B. Dolan. 1995. Metaphor as an emergent property
of machine-readable dictionaries. In Proceedings of Repre-
sentation and Acquisition of Lexical Knowledge: Polysemy,
Ambiguity, and Generativity (March 1995, Stanford Univer-
sity, CA). AAAI 1995 Spring Symposium Series, 27-29.
Sean P. Engelson and Ido Dagan. 1996. In Proc. of 34th Meet-
ing of the ACL. 319–326.
Dan Fass. 1997. Processing metonymy and metaphor. Green-
wich, CT: Ablex Publishing Corporation.
Atsushi Fujii, Takenobu Tokunaga, Kentaro Inui and Hozumi
Tanaka. 1998. Selective sampling for example-based word
sense disambiguation. Comput. Linguist. 24, 4 (Dec. 1998),
573–597.
Yael Karov and Shimon Edelman. 1998. Similarity-based word
sense disambiguation. Comput. Linguist. 24, 1 (Mar. 1998),
41-59.
David D. Lewis and William A. Gale. 1994. A sequential algo-
rithm for training text classifiers. In Proc. of SIGIR-94.
James H. Martin. 1990. A computational model of metaphor
interpretation. Toronto, ON: Academic Press, Inc.
James H. Martin. 1992. Computer understanding of conven-
tional metaphoric language. Cognitive Science 16, 2 (1992),
233-270.
Zachary J. Mason. 2004. CorMet: a computational, corpus-
based conventional metaphor extraction system. Comput.
Linguist. 30, 1 (Mar. 2004), 23-44.
Masaki Murata, Qing Ma, Atsumu Yamamoto, and Hitoshi Isa-
hara. 2000. Metonymy interpretation using x no y exam-
ples. In Proceedings of SNLP2000 (Chiang Mai, Thailand,
10 May 2000).
Srini Narayanan. 1999. Moving right along: a computational
model of metaphoric reasoning about events. In Proceed-
ings of the 16th National Conference on Artificial Intelli-
gence and the 11th IAAI Conference (Orlando, US, 1999).
121-127.
Malvina Nissim and Katja Markert. 2003. Syntactic features
and word similarity for supervised metonymy resolution. In
Proceedings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL-03) (Sapporo, Japan,
2003). 56-63.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Empirical Methods
in Natural Language Processing Conference (University of
Pennsylvania, May 17-18 1996).
Sylvia W. Russell. 1976. Computer understanding of
metaphorically used verbs. American Journal of Computa-
tional Linguistics, Microfiche 44.
</reference>
<page confidence="0.999069">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.870364">
<title confidence="0.999946">Learning for the Identification of Nonliteral Language</title>
<author confidence="0.999876">Julia Birke</author>
<author confidence="0.999876">Anoop Sarkar</author>
<affiliation confidence="0.999436">School of Computing Science, Simon Fraser</affiliation>
<address confidence="0.999608">Burnaby, BC, V5A 1S6, Canada</address>
<email confidence="0.996001">jbirke@alumni.sfu.ca,anoop@cs.sfu.ca</email>
<abstract confidence="0.994740653846154">In this paper we present an active learning approach used to create an annotated corpus of literal and nonliteral usages of verbs. The model uses nearly unsupervised word-sense disambiguation and clustering techniques. We report on experiments in which a human expert is asked to correct system predictions in different stages of learning: (i) after the last iteration when the clustering step has converged, or (ii) during each iteration of the clustering algorithm. The model obtains an f-score of 53.8% on a dataset in which literal/nonliteral usages of 25 verbs were annotated by human experts. In comparison, the same model augmented with active learning obtains 64.91%. We also the examples required when model confidence is used to select examples for human correction as compared to random selection. The results of this active learning system have been compiled into a freely available annotated corpus of literal/nonliteral usage of verbs in context.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: an approach to almost parsing.</title>
<date>1999</date>
<journal>Comput. Linguist.</journal>
<volume>25</volume>
<pages>237--265</pages>
<contexts>
<context position="9941" citStr="Bangalore &amp; Joshi, 1999" startWordPosition="1684" endWordPosition="1687">or all sx, sy E S x S where sx = sy, 0 otherwise 3: i := 0 4: while (true) do 5: s-simLi+1(sx, sy) := Ewx∈sx p(wx, sx) maxwy∈sy w-simi(wx, wy), for all sx, sy E S x G 6: s-simNi+1(sx, sy) := Ewx∈sx p(wx, sx) maxwy∈sy w-simi(wx, wy), for all sx, sy E S x N 7: for wx, wy E W x W do y3wy 8: w-simi+1(wx, wy) :={ else 0 EsXE)wX p(wX, SX) maxs,3wy {s-simiLi (sxssy ), s-simNi (sx, sy)} 9: end for 10: if bwx, maxwy{w-simi+1(wx, wy) − w-simi(wx, wy)} &lt; E then 11: break # algorithm converges in 1 � steps. 12: end if 13: i := i + 1 14: end while by a statistical tagger (Ratnaparkhi, 1996) and SuperTags (Bangalore &amp; Joshi, 1999). This model was evaluated on 25 target verbs: absorb, assault, die, drag, drown, escape, examine, fill, fix, flow, grab, grasp, kick, knock, lend, miss, pass, rest, ride, roll, smooth, step, stick, strike, touch The verbs were carefully chosen to have varying token frequencies (we do not simply learn on frequently occurring verbs). As a result, the target sets contain from 1 to 115 manually annotated sentences for each verb to enable us to measure accuracy. The annotations were not provided to the learning algorithm: they were only used to evaluate the test data performance. The first round o</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: an approach to almost parsing. Comput. Linguist. 25, 2 (Jun. 1999), 237-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Birke</author>
<author>Anoop Sarkar</author>
</authors>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, EACL-2006.</booktitle>
<pages>3--7</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="5326" citStr="Birke &amp; Sarkar, 2006" startWordPosition="877" endWordPosition="880">e world (“the sponge absorbed the water” vs. “the company absorbed the loss”). Nonliteral is then anything that is “not literal”, including most tropes, such as metaphors, idioms, as well as phrasal verbs and other anomalous expressions that cannot really be seen as literal. We aim to automatically discover the contrast between the standard set of selectional restrictions for the literal usage of verbs and the non-standard set which we assume will identify the nonliteral usage. Our identification model for literal vs. nonliteral usage of verbs is described in detail in a previous publication (Birke &amp; Sarkar, 2006). Here we provide a brief description of the model so that the use of this model in our proposed active learning approach can be explained. Since we are attempting to reduce the problem of literal/nonliteral recognition to one of word-sense disambiguation, we use an existing similarity-based word-sense disambiguation algorithm developed by (Karov &amp; Edelman, 1998), henceforth KE. The KE algorithm is based on the principle of attraction: similarities are calculated between sentences containing the word we wish to disambiguate (the target word) and collections of seed sentences (feedback sets). I</context>
<context position="7082" citStr="Birke &amp; Sarkar, 2006" startWordPosition="1172" endWordPosition="1175">n bottleneck – i.e. the low likelihood of finding all possible usages of a word in a single corpus. Note that the KE algorithm concentrates on similarities in the way sentences use the target literal or nonliteral word, not on similarities in the meanings of the sentences themselves. Algorithms 1 and 2 summarize our approach. Note that p* s) is the unigram probability of word w in sentence s, normalized by the total number of words in s. We omit some details about the algorithm here which do not affect our discussion about active learning. These details are provided in a previous publication (Birke &amp; Sarkar, 2006). As explained before, our model requires a target set and two seed sets: the literal feedback set and the nonliteral feedback set. We do not explain the details of how these feedback sets were constructed in this paper, however, it is important to note that the feedback sets themselves are noisy and not carefully vetted by human experts. The literal feedback set was built from WSJ newswire text, and for the nonliteral feedback set, we use expressions from various datasets such as the Wayne Magnuson English Idioms Sayings &amp; Slang and George Lakoff’s Conceptual Metaphor List, as well as example</context>
<context position="21444" citStr="Birke &amp; Sarkar, 2006" startWordPosition="3646" endWordPosition="3649">t to the human, the scores for the nonliteral cluster would be disastrous. This situation is purely hypothetical, but should account for the fact that 30 out of 100 sentences annotated by a human will not necessarily result in an average f-score of 30%: in fact, averaging the results of the three sitatuations described above results 25 Figure 1: Active Learning evaluation results. Baseline refers to the second baseline from Section 2. Semisupervised: Trust Seed Data refers to the standard KE model that trusts the seed data. Optimal Semisupervised refers to the augmented KE model described in (Birke &amp; Sarkar, 2006). Active Learning refers to the model proposed in this paper. in an avarage f-score of nearly 36.9%. This is 23% higher than the 30% of the balanced case, which is 1.23 times higher. For this reason, we give the human scores a boost by assuming that whatever the human annotates in the manual scenario will result in an f-score that is 1.23 times higher. For our experiment, we take the number of sentences that our active learning method sent to the human for each word – note that this is not always 30% of the total number of sentences – and multiply that by 1.23 – to give the human the benefit o</context>
</contexts>
<marker>Birke, Sarkar, 2006</marker>
<rawString>Julia Birke and Anoop Sarkar. 2006. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, EACL-2006. Trento, Italy. April 3-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The kappa statistic: a second look.</title>
<date>2004</date>
<journal>Comput. Linguist.</journal>
<volume>30</volume>
<pages>95--101</pages>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>Barbara Di Eugenio and Michael Glass. 2004. The kappa statistic: a second look. Comput. Linguist. 30, 1 (Mar. 2004), 95-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
</authors>
<title>Metaphor as an emergent property of machine-readable dictionaries.</title>
<date>1995</date>
<booktitle>In Proceedings of Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity</booktitle>
<pages>27--29</pages>
<institution>Stanford University,</institution>
<contexts>
<context position="27546" citStr="Dolan, 1995" startWordPosition="4663" endWordPosition="4664">he use of machine learning for more sophisticated metaphor and metonymy processing tasks on larger text corpora. Rule-based systems – some using a type of interlingua (Russell, 1976); others using complicated networks and hierarchies often referred to as metaphor maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionary-based systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction. Examples of such systems are (Murata et. al., 2000; Nissim &amp; Markert, 2003; Mason, 2004). 27 Nissim &amp; Markert (2003) approach metonymy resolution with machine learning methods, “which [exploit] the similarity between examples of conventional metonymy” ((Nissim &amp; Markert, 2003), p. 56). They see metonymy resolution as a classification problem between the literal use of a word and a number of pre-defined me</context>
</contexts>
<marker>Dolan, 1995</marker>
<rawString>William B. Dolan. 1995. Metaphor as an emergent property of machine-readable dictionaries. In Proceedings of Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity (March 1995, Stanford University, CA). AAAI 1995 Spring Symposium Series, 27-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean P Engelson</author>
<author>Ido Dagan</author>
</authors>
<date>1996</date>
<booktitle>In Proc. of 34th Meeting of the ACL.</booktitle>
<pages>319--326</pages>
<contexts>
<context position="4341" citStr="Engelson &amp; Dagan, 1996" startWordPosition="710" endWordPosition="713"> iteration it sends a small set of examples to a human expert to annotate, which in turn provides additional benefit to the bootstrapping process. Our active learnProceedings of the Workshop on Computational Approaches to Figurative Language, pages 21–28, Rochester, NY, April 26, 2007. c�2007 Association for Computational Linguistics ing method is similar to the Uncertainty Sampling algorithm of (Lewis &amp; Gale, 1994) but in our case interacts with iterative clustering. As we shall see, some of the crucial criticisms leveled against uncertainty sampling and in favor of Committee-based sampling (Engelson &amp; Dagan, 1996) do not apply in our case, although the latter may still be more accurate in our task. 2 Literal vs. Nonliteral Identification For the purposes of this paper we will take the simplified view that literal is anything that falls within accepted selectional restrictions (“he was forced to eat his spinach” vs. “he was forced to eat his words”) or our knowledge of the world (“the sponge absorbed the water” vs. “the company absorbed the loss”). Nonliteral is then anything that is “not literal”, including most tropes, such as metaphors, idioms, as well as phrasal verbs and other anomalous expressions</context>
<context position="26098" citStr="Engelson &amp; Dagan, 1996" startWordPosition="4432" endWordPosition="4435">nonliteral language and augmenting it with an active learning approach which allows human expert knowledge to become part of the learning process. Our approach to active learning is similar to the Uncertainty Sampling approach of (Lewis &amp; Gale, 1994) and (Fujii et. al., 1998) in that we pick those examples that we could not classify due to low confidence in the labeling at a particular point. We employ a resource-limited version in which only a small fixed sample is ever annotated by a human. Some of the criticisms leveled against uncertainty sampling and in favor of Committee-based sampling (Engelson &amp; Dagan, 1996) (and see refs therein) do not apply in our case. Our similarity measure is based on two views of sentence- and word-level similarity and hence we get an estimate of appropriate identification rather than just correct classification. As a result, by embedding an Uncertainty Sampling active learning model within a two-view clustering algorithm, we gain the same advantages as other uncertainty sampling methods obtain when used in bootstrapping methods (e.g. (Fujii et. al., 1998)). Other machine learning approaches that derive from optimal experiment design are not appropriate in our case because</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Sean P. Engelson and Ido Dagan. 1996. In Proc. of 34th Meeting of the ACL. 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Fass</author>
</authors>
<title>Processing metonymy and metaphor.</title>
<date>1997</date>
<publisher>Ablex Publishing Corporation.</publisher>
<location>Greenwich, CT:</location>
<contexts>
<context position="27220" citStr="Fass, 1997" startWordPosition="4614" endWordPosition="4615">aches that derive from optimal experiment design are not appropriate in our case because we do not yet have a strong predictive (or generative) model of the literal/nonliteral distinction. Our machine learning model only does identification of verb usage as literal or nonliteral but it can be seen as a first step towards the use of machine learning for more sophisticated metaphor and metonymy processing tasks on larger text corpora. Rule-based systems – some using a type of interlingua (Russell, 1976); others using complicated networks and hierarchies often referred to as metaphor maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionary-based systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction. Examples of such systems are (Murata et. al., 2000; Nissim &amp; Markert, 2003; Mason,</context>
</contexts>
<marker>Fass, 1997</marker>
<rawString>Dan Fass. 1997. Processing metonymy and metaphor. Greenwich, CT: Ablex Publishing Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
</authors>
<title>Takenobu Tokunaga, Kentaro Inui and Hozumi Tanaka.</title>
<date>1998</date>
<journal>Comput. Linguist.</journal>
<volume>24</volume>
<pages>573--597</pages>
<marker>Fujii, 1998</marker>
<rawString>Atsushi Fujii, Takenobu Tokunaga, Kentaro Inui and Hozumi Tanaka. 1998. Selective sampling for example-based word sense disambiguation. Comput. Linguist. 24, 4 (Dec. 1998), 573–597.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yael Karov</author>
<author>Shimon Edelman</author>
</authors>
<title>Similarity-based word sense disambiguation.</title>
<date>1998</date>
<journal>Comput. Linguist.</journal>
<volume>24</volume>
<pages>41--59</pages>
<contexts>
<context position="5691" citStr="Karov &amp; Edelman, 1998" startWordPosition="934" endWordPosition="937">strictions for the literal usage of verbs and the non-standard set which we assume will identify the nonliteral usage. Our identification model for literal vs. nonliteral usage of verbs is described in detail in a previous publication (Birke &amp; Sarkar, 2006). Here we provide a brief description of the model so that the use of this model in our proposed active learning approach can be explained. Since we are attempting to reduce the problem of literal/nonliteral recognition to one of word-sense disambiguation, we use an existing similarity-based word-sense disambiguation algorithm developed by (Karov &amp; Edelman, 1998), henceforth KE. The KE algorithm is based on the principle of attraction: similarities are calculated between sentences containing the word we wish to disambiguate (the target word) and collections of seed sentences (feedback sets). It requires a target set – the set of sentences containing the verbs to be classified into literal or nonliteral – and the seed sets: the literal feedback set and the nonliteral feedback set. A target set sentence is considered to be attracted to the feedback set containing the sentence to which it shows the highest similarity. Two sentences are similar if they co</context>
<context position="8824" citStr="Karov &amp; Edelman, 1998" startWordPosition="1469" endWordPosition="1472">ovides automatic methods to clean up the feedback sets during the clustering algorithm. Note that the feedback sets are not cleaned up by human experts, however the test data is carefully annotated by human experts (details about inter-annotator agreement on the test set are provided below). The test set is not large enough to be split up into a training and test set that can support learning using a supervised learning method. The sentences in the target set and feedback sets were augmented with some shallow syntactic information such as part of speech tags provided 22 Algorithm 1 KE-train: (Karov &amp; Edelman, 1998) algorithm adapted to literal/nonliteral identification Require: S: the set of sentences containing the target word (each sentence is classified as literal/nonliteral) Require: G: the set of literal seed sentences Require: N: the set of nonliteral seed sentences Require: W: the set of words/features, w E s means w is in sentence s, s 3 w means s contains w Require: E: threshold that determines the stopping condition 1: w-sim0(wx, wy) := 1 if wx = wy, 0 otherwise 2: s-simI0(sx, sy) := 1, for all sx, sy E S x S where sx = sy, 0 otherwise 3: i := 0 4: while (true) do 5: s-simLi+1(sx, sy) := Ewx∈s</context>
<context position="14240" citStr="Karov &amp; Edelman, 1998" startWordPosition="2404" endWordPosition="2407">s certifiably correctly classified sentence. The second question is when to send the sentences to the human. We can send all the examples after the first iteration, after some intermediate iteration, distributed across iterations, or at the end. Sending everything after the first iteration is best for counteracting false attractions before they become entrenched and for allowing future iterations to learn from the human decisions. Risks include sending sentences to the human before our model has had a chance to make potentially correct decision about them, counteracting any saving of effort. (Karov &amp; Edelman, 1998) state that the results are not likely to change much after the third iteration and we have confirmed this independently: similarity values continue to change until convergence, but cluster allegiance tends not to. Sending everything to the human after the third iteration could therefore entail some of the damage control of sending everything after the first iteration while giving the model 1: For any sentence sx E S 2: if saxs-simL(sx,sy) &gt; mIs-simN(sx,sy) then 3: tag sx as literal 4: else 5: tag sx as nonliteral 6: end if precision · recall) / (precision + recall). Nonliteral precision and r</context>
</contexts>
<marker>Karov, Edelman, 1998</marker>
<rawString>Yael Karov and Shimon Edelman. 1998. Similarity-based word sense disambiguation. Comput. Linguist. 24, 1 (Mar. 1998), 41-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proc. of SIGIR-94.</booktitle>
<contexts>
<context position="4137" citStr="Lewis &amp; Gale, 1994" startWordPosition="678" endWordPosition="681">influence the selection of at least a portion of its training data. In our approach, a clustering algorithm for literal/nonliteral recognition tries to annotate the examples that it can, while in each iteration it sends a small set of examples to a human expert to annotate, which in turn provides additional benefit to the bootstrapping process. Our active learnProceedings of the Workshop on Computational Approaches to Figurative Language, pages 21–28, Rochester, NY, April 26, 2007. c�2007 Association for Computational Linguistics ing method is similar to the Uncertainty Sampling algorithm of (Lewis &amp; Gale, 1994) but in our case interacts with iterative clustering. As we shall see, some of the crucial criticisms leveled against uncertainty sampling and in favor of Committee-based sampling (Engelson &amp; Dagan, 1996) do not apply in our case, although the latter may still be more accurate in our task. 2 Literal vs. Nonliteral Identification For the purposes of this paper we will take the simplified view that literal is anything that falls within accepted selectional restrictions (“he was forced to eat his spinach” vs. “he was forced to eat his words”) or our knowledge of the world (“the sponge absorbed th</context>
<context position="25725" citStr="Lewis &amp; Gale, 1994" startWordPosition="4366" endWordPosition="4369">sting or from active learning during annotated corpus construction. The corpus is available at http://www.cs.sfu.ca/∼anoop/students/jbirke/. Further unsupervised expansion of the existing clusters as well as the production of additional clusters is a possibility. 5 Previous Work To our knowledge there has not been any previous work done on taking a model for literal/nonliteral language and augmenting it with an active learning approach which allows human expert knowledge to become part of the learning process. Our approach to active learning is similar to the Uncertainty Sampling approach of (Lewis &amp; Gale, 1994) and (Fujii et. al., 1998) in that we pick those examples that we could not classify due to low confidence in the labeling at a particular point. We employ a resource-limited version in which only a small fixed sample is ever annotated by a human. Some of the criticisms leveled against uncertainty sampling and in favor of Committee-based sampling (Engelson &amp; Dagan, 1996) (and see refs therein) do not apply in our case. Our similarity measure is based on two views of sentence- and word-level similarity and hence we get an estimate of appropriate identification rather than just correct classific</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proc. of SIGIR-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James H Martin</author>
</authors>
<title>A computational model of metaphor interpretation.</title>
<date>1990</date>
<publisher>Academic Press, Inc.</publisher>
<location>Toronto, ON:</location>
<contexts>
<context position="27234" citStr="Martin, 1990" startWordPosition="4616" endWordPosition="4617">erive from optimal experiment design are not appropriate in our case because we do not yet have a strong predictive (or generative) model of the literal/nonliteral distinction. Our machine learning model only does identification of verb usage as literal or nonliteral but it can be seen as a first step towards the use of machine learning for more sophisticated metaphor and metonymy processing tasks on larger text corpora. Rule-based systems – some using a type of interlingua (Russell, 1976); others using complicated networks and hierarchies often referred to as metaphor maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionary-based systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction. Examples of such systems are (Murata et. al., 2000; Nissim &amp; Markert, 2003; Mason, 2004). 27 Nis</context>
</contexts>
<marker>Martin, 1990</marker>
<rawString>James H. Martin. 1990. A computational model of metaphor interpretation. Toronto, ON: Academic Press, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James H Martin</author>
</authors>
<title>Computer understanding of conventional metaphoric language.</title>
<date>1992</date>
<journal>Cognitive Science</journal>
<volume>16</volume>
<pages>233--270</pages>
<contexts>
<context position="27249" citStr="Martin, 1992" startWordPosition="4618" endWordPosition="4619">imal experiment design are not appropriate in our case because we do not yet have a strong predictive (or generative) model of the literal/nonliteral distinction. Our machine learning model only does identification of verb usage as literal or nonliteral but it can be seen as a first step towards the use of machine learning for more sophisticated metaphor and metonymy processing tasks on larger text corpora. Rule-based systems – some using a type of interlingua (Russell, 1976); others using complicated networks and hierarchies often referred to as metaphor maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionary-based systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction. Examples of such systems are (Murata et. al., 2000; Nissim &amp; Markert, 2003; Mason, 2004). 27 Nissim &amp; Markert (</context>
</contexts>
<marker>Martin, 1992</marker>
<rawString>James H. Martin. 1992. Computer understanding of conventional metaphoric language. Cognitive Science 16, 2 (1992), 233-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zachary J Mason</author>
</authors>
<title>CorMet: a computational, corpusbased conventional metaphor extraction system.</title>
<date>2004</date>
<journal>Comput. Linguist.</journal>
<volume>30</volume>
<pages>23--44</pages>
<contexts>
<context position="27826" citStr="Mason, 2004" startWordPosition="4702" endWordPosition="4703">, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionary-based systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction. Examples of such systems are (Murata et. al., 2000; Nissim &amp; Markert, 2003; Mason, 2004). 27 Nissim &amp; Markert (2003) approach metonymy resolution with machine learning methods, “which [exploit] the similarity between examples of conventional metonymy” ((Nissim &amp; Markert, 2003), p. 56). They see metonymy resolution as a classification problem between the literal use of a word and a number of pre-defined metonymy types. They use similarities between possibly metonymic words (PMWs) and known metonymies as well as context similarities to classify the PMWs. Mason (2004) presents CorMet, “a corpus-based system for discovering metaphorical mappings between concepts” ((Mason, 2004), p. 2</context>
</contexts>
<marker>Mason, 2004</marker>
<rawString>Zachary J. Mason. 2004. CorMet: a computational, corpusbased conventional metaphor extraction system. Comput. Linguist. 30, 1 (Mar. 2004), 23-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Qing Ma</author>
<author>Atsumu Yamamoto</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Metonymy interpretation using x no y examples.</title>
<date>2000</date>
<booktitle>In Proceedings of SNLP2000</booktitle>
<location>Chiang Mai,</location>
<marker>Murata, Ma, Yamamoto, Isahara, 2000</marker>
<rawString>Masaki Murata, Qing Ma, Atsumu Yamamoto, and Hitoshi Isahara. 2000. Metonymy interpretation using x no y examples. In Proceedings of SNLP2000 (Chiang Mai, Thailand, 10 May 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
</authors>
<title>Moving right along: a computational model of metaphoric reasoning about events.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th IAAI Conference</booktitle>
<pages>121--127</pages>
<location>Orlando, US,</location>
<contexts>
<context position="28971" citStr="Narayanan, 1999" startWordPosition="4862" endWordPosition="4863">iscovering metaphorical mappings between concepts” ((Mason, 2004), p. 23). His system finds the selectional restrictions of given verbs in particular domains by statistical means. It then finds metaphorical mappings between domains based on these selectional preferences. By finding semantic differences between the selectional preferences, it can “articulate the higher-order structure of conceptual metaphors” ((Mason, 2004), p. 24), finding mappings like LIQUID-*MONEY. Metaphor processing has even been approached with connectionist systems storing world-knowledge as probabilistic dependencies (Narayanan, 1999). 6 Conclusion In this paper we presented a system for separating literal and nonliteral usages of verbs through statistical word-sense disambiguation and clustering techniques. We used active learning to combine the predictions of this system with a human expert annotator in order to boost the overall accuracy of the system by 11.1%. We used the model together with active learning and iterative augmentation, to build an annotated corpus which is publicly available, and is a resource of literal/nonliteral usage clusters that we hope will be useful not only for future research in the field of n</context>
</contexts>
<marker>Narayanan, 1999</marker>
<rawString>Srini Narayanan. 1999. Moving right along: a computational model of metaphoric reasoning about events. In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th IAAI Conference (Orlando, US, 1999). 121-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malvina Nissim</author>
<author>Katja Markert</author>
</authors>
<title>Syntactic features and word similarity for supervised metonymy resolution.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03)</booktitle>
<pages>56--63</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="27812" citStr="Nissim &amp; Markert, 2003" startWordPosition="4698" endWordPosition="4701">etaphor maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionary-based systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction. Examples of such systems are (Murata et. al., 2000; Nissim &amp; Markert, 2003; Mason, 2004). 27 Nissim &amp; Markert (2003) approach metonymy resolution with machine learning methods, “which [exploit] the similarity between examples of conventional metonymy” ((Nissim &amp; Markert, 2003), p. 56). They see metonymy resolution as a classification problem between the literal use of a word and a number of pre-defined metonymy types. They use similarities between possibly metonymic words (PMWs) and known metonymies as well as context similarities to classify the PMWs. Mason (2004) presents CorMet, “a corpus-based system for discovering metaphorical mappings between concepts” ((Maso</context>
</contexts>
<marker>Nissim, Markert, 2003</marker>
<rawString>Malvina Nissim and Katja Markert. 2003. Syntactic features and word similarity for supervised metonymy resolution. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03) (Sapporo, Japan, 2003). 56-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-ofspeech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference (University of Pennsylvania,</booktitle>
<contexts>
<context position="9901" citStr="Ratnaparkhi, 1996" startWordPosition="1679" endWordPosition="1680">herwise 2: s-simI0(sx, sy) := 1, for all sx, sy E S x S where sx = sy, 0 otherwise 3: i := 0 4: while (true) do 5: s-simLi+1(sx, sy) := Ewx∈sx p(wx, sx) maxwy∈sy w-simi(wx, wy), for all sx, sy E S x G 6: s-simNi+1(sx, sy) := Ewx∈sx p(wx, sx) maxwy∈sy w-simi(wx, wy), for all sx, sy E S x N 7: for wx, wy E W x W do y3wy 8: w-simi+1(wx, wy) :={ else 0 EsXE)wX p(wX, SX) maxs,3wy {s-simiLi (sxssy ), s-simNi (sx, sy)} 9: end for 10: if bwx, maxwy{w-simi+1(wx, wy) − w-simi(wx, wy)} &lt; E then 11: break # algorithm converges in 1 � steps. 12: end if 13: i := i + 1 14: end while by a statistical tagger (Ratnaparkhi, 1996) and SuperTags (Bangalore &amp; Joshi, 1999). This model was evaluated on 25 target verbs: absorb, assault, die, drag, drown, escape, examine, fill, fix, flow, grab, grasp, kick, knock, lend, miss, pass, rest, ride, roll, smooth, step, stick, strike, touch The verbs were carefully chosen to have varying token frequencies (we do not simply learn on frequently occurring verbs). As a result, the target sets contain from 1 to 115 manually annotated sentences for each verb to enable us to measure accuracy. The annotations were not provided to the learning algorithm: they were only used to evaluate the </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy part-ofspeech tagger. In Proceedings of the Empirical Methods in Natural Language Processing Conference (University of Pennsylvania, May 17-18 1996).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvia W Russell</author>
</authors>
<title>Computer understanding of metaphorically used verbs.</title>
<date>1976</date>
<journal>American Journal of Computational Linguistics, Microfiche</journal>
<volume>44</volume>
<contexts>
<context position="27116" citStr="Russell, 1976" startWordPosition="4598" endWordPosition="4599">methods obtain when used in bootstrapping methods (e.g. (Fujii et. al., 1998)). Other machine learning approaches that derive from optimal experiment design are not appropriate in our case because we do not yet have a strong predictive (or generative) model of the literal/nonliteral distinction. Our machine learning model only does identification of verb usage as literal or nonliteral but it can be seen as a first step towards the use of machine learning for more sophisticated metaphor and metonymy processing tasks on larger text corpora. Rule-based systems – some using a type of interlingua (Russell, 1976); others using complicated networks and hierarchies often referred to as metaphor maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionary-based systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metap</context>
</contexts>
<marker>Russell, 1976</marker>
<rawString>Sylvia W. Russell. 1976. Computer understanding of metaphorically used verbs. American Journal of Computational Linguistics, Microfiche 44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>