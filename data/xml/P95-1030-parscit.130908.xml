<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000041">
<title confidence="0.996148">
New Techniques for Context Modeling
</title>
<author confidence="0.966238">
Eric Sven Ristad and Robert G. Thomas
</author>
<affiliation confidence="0.9731665">
Department of Computer Science
Princeton University
</affiliation>
<email confidence="0.949018">
{ristad,rgt}Ocs.princeton.edu
</email>
<sectionHeader confidence="0.992587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999841857142857">
We introduce three new techniques for sta-
tistical language models: extension mod-
eling, nonmonotonic contexts, and the di-
vergence heuristic. Together these tech-
niques result in language models that have
few states, even fewer parameters, and low
message entropies.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936888888889">
Current approaches to automatic speech and hand-
writing transcription demand a strong language
model with a small number of states and an even
smaller number of parameters. If the model entropy
is high, then transcription results are abysmal. If
there are too many states, then transcription be-
comes computationally infeasible. And if there are
too many parameters; then &amp;quot;overfitting&amp;quot; occurs and
predictive performance degrades.
In this paper we introduce three new techniques
for statistical language models: extension modeling,
nonmonotonic contexts, and the divergence heuris-
tic. Together these techniques result in language
models that have few states, even fewer parameters,
and low message entropies. For example, our tech-
niques achieve a message entropy of 1.97 bits/char
on the Brown corpus using only 89,325 parameters.
By modestly increasing the number of model param-
eters in a principled manner, our techniques are able
to further reduce the message entropy of the Brown
Corpus to 1.91 bits/char.1 In contrast, the charac-
ter 4-gram model requires 250 times as many pa-
rameters in order to achieve a message entropy of
only 2.47 bits/char. Given the logarithmic nature
of codelengths, a savings of 0.5 bits/char is quite
significant. The fact that our model performs signif-
icantly better using vastly fewer parameters argues
</bodyText>
<footnote confidence="0.975925833333333">
1The only change to our model selection procedure is
to replace the incremental cost formula AL,p(w,E&apos;, cr)
with a constant cost of 2 bits/extension. This small
change reduces the test message entropy from 1.97 to
1.91 bits/char but it also quadruples the number of
model parameters and triples the total codelength.
</footnote>
<bodyText confidence="0.998118931818182">
that it is a much better probability model of natural
language text.
Our first two techniques — nonmonotonic contexts
and extension modeling — are generalizations of the
traditional context model (Cleary and Witten 1984;
Rissanen 1983,1986). Our third technique — the di-
vergence heuristic — is an incremental model selec-
tion criterion based directly on Rissanen&apos;s (1978)
minimum description length (MDL) principle. The
MDL principle states that the best model is the sim-
plest model that provides a compact description of
the observed data.
In the traditional context model, every prefix and
every suffix of a context is also a context. Three
consequences follow from this property. The first
consequence is that the context dictionary is un-
necessarily large because most of these contexts are
redundant. The second consequence is to attenu-
ate the benefits of context blending, because most
contexts are equivalent to their maximal proper suf-
fixes. The third consequence is that the length of the
longest candidate context can increase by at most
one symbol at each time step, which impairs the
model&apos;s ability to model complex sources. In a non-
monotonic model, this constraint is relaxed to allow
compact dictionaries, discontinuous backoff, and ar-
bitrary context switching.
The traditional context model maps every history
to a unique context. All symbols are predicted us-
ing that context, and those predictions are estimated
using the same set of histories. In contrast, an exten-
sion model maps every history to a set of contexts,
one for each symbol in the alphabet. Each symbol is
predicted in its own context, and the model&apos;s current
predictions need not be estimated using the same
set of histories. This is a form of parameter tying
that increases the accuracy of the model&apos;s predic-
tions while reducing the number of free parameters
in the model.
As a result of these two generalizations, nonmono-
tonic extension models can outperform their equiv-
alent context models using significantly fewer pa-
rameters. For example, an order 3 n-gram (ie., the
4-gram) requires more than 51 times as many con-
</bodyText>
<page confidence="0.990457">
220
</page>
<bodyText confidence="0.999963576923077">
texts and 787 times as many parameters as the order
3 nonmonotonic extension model, yet already per-
forms worse on the Brown corpus by 0.08 bits/char.
Our third contribution is the divergence heuris-
tic, which adds a more specific context to the model
only when it reduces the codelength of the past data
more than it increases the codelength of the model.
In contrast, the traditional selection heuristic adds a
more specific context to the model only if it&apos;s entropy
is less than the entropy of the more general context
(Rissanen 1983,1986). The traditional minimum en-
tropy heuristic is a special case of the more effective
and more powerful divergence heuristic. The diver-
gence heuristic allows our models to generalize from
the training corpus to the testing corpus, even for
nonstationary sources such as the Brown corpus.
The remainder of our article is organized into
three sections. In section 2, we formally define the
class of extension models and present a heuristic
model selection algorithm for that model class based
on the divergence criterion. Next, in section 3, we
demonstrate the efficacy of our techniques on the
Brown Corpus, an eclectic collection of English prose
containing approximately one million words of text.
Section 4 discusses possible improvements to the
model class.
</bodyText>
<sectionHeader confidence="0.996694" genericHeader="method">
2 Extension Model Class
</sectionHeader>
<bodyText confidence="0.999903923076923">
This section consists of four parts. In 2.1, we for-
mally define the class of extension models and prove
that they satisfy the axioms of probability. In 2.2,
we show to estimate the parameters of an exten-
sion model using Moffat&apos;s (1990) &amp;quot;method C.&amp;quot; In 2.3,
we provide codelength formulas for our model class,
based on efficient enumerative codes. These code-
length formulas will be used to match the complexity
of the model to the complexity of the data. In 2.4,
we present a heuristic model selection algorithm that
adds parameters to an extension model only when
they reduce the codelength of the data more than
they increase the codelength of the model.
</bodyText>
<subsectionHeader confidence="0.988981">
2.1 Model Class Definition
</subsectionHeader>
<bodyText confidence="0.998617666666667">
Formally, an extension model cb = (ED, E, )) con-
sists of a finite alphabet E, lEl = m, a dictionary
D of contexts, D C E*, a set of available context
extensions E, EC Dx E, and a probability func-
tion A : E [0, 1]. For every context w in D, E(w)
is the set of symbols available in the context w and
)(ow) is the conditional probability of the symbol
a in the context w. Note that EaEE A(Cf1W) &lt; 1 for
all contexts w in the dictionary D.
The probability1:3(h10) of a string h given the
model 0, h E En, is calculated as a chain of con-
ditional probabilities (1)
</bodyText>
<equation confidence="0.975439">
/3010) -= i3(hnlhi • hn—i, 0)1301 • • • h.--110) (1)
</equation>
<bodyText confidence="0.8675605">
while the conditional probability &amp;quot;5(o-1h, 0) of a single
symbol a after the history h is defined as (2).
</bodyText>
<equation confidence="0.8144835">
A(crih) if (h, cr) E E
73(alh, 0) ==-. 6(h)73(o-lh2h3 .hrt Cb) otherwise
</equation>
<bodyText confidence="0.9745205">
(2)
The expansion factor 6(h) ensures that 13(•Ih, 0) is a
probability function if 25(-Ih2 ...hn,0) is a probabil-
ity function.
</bodyText>
<equation confidence="0.807315111111111">
(2h.) .1h. h)
6(h) -27 i15(E— (Ah()EIh (3)
1—
Note that E(h) represents a set of symbols, and
so by a slight abuse of notation A(E(h)lh) denotes
EaEE(h) A(olh), ie., the sum of A(crilz) over all a in
E(h).
Example 1. Let E = {0,1}, D = { c, &amp;quot;0&amp;quot; }, E(c)
= 10,11, E(&amp;quot;0&amp;quot;) = {0}. Suppose A(01e) = -12‘, A(11e)
</equation>
<bodyText confidence="0.974208592592593">
= 1, and A(01&amp;quot;0&amp;quot;) =1. Then 6(110&amp;quot;) = =
and /3(11&amp;quot;0&amp;quot;,0) = 6(&amp;quot;0&amp;quot;) A(11e) =
The fundamental difference between a context
model and an extension model lies in the inputs
to the context selection rule, not its outputs. The
traditional context model includes a selection rule
s : E* D whose only input is the history. In con-
trast, an extension model includes a selection rule
s : E* x E --+ D whose inputs include the past
history and the symbol to be predicted. This dis-
tinction is preserved even if we generalize the selec-
tion rule to select a set of candidate contexts. Un-
der such a generalization, the context model would
map every history to a set of candidate contexts,
ie., s : E* 2D, while an extension model would
map every history and symbol to a set of candidate
contexts, ie., s : E* x E 2D .
Our extension selection rule s : E* x E D is de-
fined implicitly by the set E of extensions currently
in the model. The recursion in (2) says that each
symbol should be predicted in its longest candidate
context, while the expansion factor 6(h) says that
longer contexts in the model should be trusted more
than shorter contexts when combining the predic-
tions from different contexts.
An extension model 0 is valid if it satisfies the
following constraints:
</bodyText>
<listItem confidence="0.891336">
a. e E DA E(c) E
b. Vw E D [EE(w) A(crlw) &lt; 1]
c. Vw E D [E(w), E =EaEE(w) A(criw) = 1]
(4)
</listItem>
<bodyText confidence="0.9944495">
These constraints suffice to ensure that the model 15
defines a probability function. Constraint (4a) states
that every symbol has the empty string as a context.
This guarantees that every symbol will always have
at least one context in every history and that the re-
cursion in (2) will terminate. Constraint (4b) states
that the sum of the probabilities of the extensions
E(w) available in in a given context w cannot sum
</bodyText>
<page confidence="0.99615">
221
</page>
<bodyText confidence="0.981041">
to more than unity. The third constraint (4c) states
that the sum of the probabilities of the extensions
E(w) must sum exactly to unity when every symbol
is available in that context (ie., when E(w) = E).
</bodyText>
<equation confidence="0.752420833333333">
Lemma 2.1 Vy E E* Vcr E E
[ /3(Eiy) = 1 = 13(Elory) = 1]
Proof. By the definition of b(oy).
Theorem 1 If an extension model cb is valid, then
Vn EsEE„ /5(510) = 1.
Proof. By induction on n. For the base case,
</equation>
<bodyText confidence="0.999635">
n = 1 and the statement is true by the definition of
validity (constraints 4a and 4c). The induction step
is true by lemma 2.1 and definition (1). 0
</bodyText>
<subsectionHeader confidence="0.968038">
2.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.997924153846154">
Let us now estimate the conditional probabilities
A(•I-) required for an extension model. Traditionally,
these conditional probabilities are estimated using
string frequencies obtained from a training corpus.
Let c(o-lw) be the number of times that the symbol o-
followed the string w in the training corpus, and let
c(w) be the sum EG,EE c(o-lw) of all its conditional
frequencies.
Following Moffat (1990), we first partition the
conditional event space E in a given context w
into two subevents: the symbols q(w) that have
previously occurred in context w and those that
q(w) that have not. Formally, q(w) :
c(olw) &gt; 0} and q(w) E — q(w). We estimate
Ac(q(w)lw) as c(w)/(c(w) #(w)) and Ac(4(w)lw)
as #(w)/(c(w) #(w)) where #(w) is the to-
tal weight assigned to the novel events q(w) in
the context w. Currently, we calculate #(w)
as minaq(w)l , I(w)I) so that highly variable con-
texts receive more flattening, but no novel symbol
in q(w) receives more than unity weight. Next,
Ac(o-lq(w), w) is estimated as c(o-Itv)Ic(w) for the
previously seen symbols a- E q(w) and Ac(crI4(w), w)
is estimated uniformly as 1/ 1(w)1 for the novel sym-
bols a- E q(w). Combining these estimates, we ob-
tain our overall estimate (5).
</bodyText>
<equation confidence="0.979454333333333">
Ac(01w) = c(01w) if o- E q(w)
otherwise
c(w) #(w)
#(w)
1q1(w) I (c(w) #(w))
(5)
</equation>
<bodyText confidence="0.999103074074074">
Unlike Moffat, our estimate (5) does not use escape
probabilities or any other form of context blending.
All novel events q(w) in the context w are assigned
uniform probability. This is suboptimal but simpler.
We note that our frequencies are incorrect when
used in an extension model that contains contexts
that are proper suffixes of each other. In such a sit-
uation, the shorter context is only used when the
longer context was not used. Let y and xy be two
distinct contexts in a model cb. Then the context y
will never be used when the history is E*xy. There-
fore, our estimate of A(• ly) should be conditioned on
the fact that the longer context xy did not occur.
The interaction between candidate contexts can be-
come quite complex, and we consider this problem
in other work (Ristad and Thomas, 1995).
Parameter estimation is only a small part of the
overall model estimation problem. Not only do we
have to estimate the parameters for a model, we have
to find the right parameters to use! To do this, we
proceed in two steps. First, in section 2.3, we use
the minimum description length (MDL) principle to
quantify the total merit of a model with respect to
a training corpus. Next, in section 2.4, we use our
MDL codelengths to derive a practical model selec-
tion algorithm with which to find a good model in
the vast class of all extension models.
</bodyText>
<subsectionHeader confidence="0.994597">
2.3 Codelength Formulas
</subsectionHeader>
<bodyText confidence="0.9818700625">
The goal of this section is to establish the proper ten-
sion between model complexity and data complexity,
in the fundamental units of information. Although
the MDL framework obliges us to propose particu-
lar encodings for the model and the data, our goal
is not to actually encode the data or the model.
Given an extension model 0 and a text corpus T,
IT = t, we define the total codelength L(T, 014)
relative to the model class (11. using a 2-part code.
L(T, 014) = L(01(1)) + L(TIO (I))
Since conditioning on the model class 4) is always
understood, we will henceforth suppress it in our
notation.
Firstly, we will encode the text T using the prob-
ability model 0 and an arithmetic code, obtaining
the following codelength.
</bodyText>
<equation confidence="0.813252">
L (TI0) = —log ATI0)
</equation>
<bodyText confidence="0.9977679">
Next, we encode the model q5 in three parts: the con-
text dictionary as L(D), the extensions as L(EID),
and the conditional frequencies c(+) as L(cID, E).
The dictionary D of contexts forms a suffix tree
containing ni vertices with branching factor i. The
tree contains n = in2 internal vertices and
no leaf vertices. There are (no + n1 nm —
1)!/no!ni! ...nm! such trees (Knuth, 1986:587). Ac-
cordingly, this tree may be encoded with an enumer-
ative code using L(D) bits.
</bodyText>
<equation confidence="0.993450857142857">
L(D) = Lz(n)+ log n m — 1 )
m — 1
+ log (no + ni nm — 1)!
+ nilog( n)
-I- log( n+ILDJI— 1
Pi — 1 )
no!ni!
</equation>
<page confidence="0.984437">
222
</page>
<bodyText confidence="0.999031">
where [DJ is the set of all contexts in D that are
proper suffixes of another context in D. The first
term encodes the number n of internal vertices using
the Elias code. The second term encodes the counts
{n1, n2, , nrn}. Given the frequencies of these in-
ternal vertices, we may calculate the number no of
leaf vertices as no = 1+ n2 + 2n3 + 3n4 + + (m —
1)nm. The third term encodes the actual tree (with-
out labels) using an enumerative code. The fourth
term assigns labels (ie., symbols from E) to the edges
in the tree. At this point the decoder knows all con-
texts which are not proper suffixes of other contexts,
ie., D — LDJ • The fourth term encodes the magni-
tude of LDJ as an integer bounded by the number n
of internal vertices in the suffix tree. The fifth term
identifies the contexts LDJ as interior vertices in the
tree that are proper suffices of another context in D.
Now we encode the symbols available in each con-
text. Let mi be the number of contexts that have
exactly i extensions, ie., mi = w 1E (w)1 =
Observe that Erli mi
</bodyText>
<equation confidence="0.99824225">
L(E(D) = log ( (DI rn
m — 1 ) + log (
+ mi log ( )
i=1
</equation>
<bodyText confidence="0.999944">
The first term represents the encoding of {mi} while
the second term represents the encoding 1E(w)1 for
each w in D. The third term represents the encoding
of E(w) as a subset of E for each w in D.
Finally, we encode the frequencies c(u1w) used to
estimate the model parameters
</bodyText>
<equation confidence="0.9921782">
L(c(D, E) = L z(c(c)) + E log ( c(w)ctti)rwl )
we],
+ E log ( c(w) IE(w)I
1E(w) 1
wED
</equation>
<bodyText confidence="0.999910636363636">
where 11 consists of all contexts that have y as their
maximal proper suffix, ie., all contexts that y imme-
diately dominates, and Lyj is the maximal proper
suffix of y in D, ie., the unique context that imme-
diately dominates y. The first term encodes 171 with
an Elias code and the second term recursively parti-
tions c(w) into c( ) for every context w. The third
term partitions the context frequency c(w) into the
available extensions c(E(w)(w) and the &amp;quot;unallocated
frequency&amp;quot; c(E—E(w)(w) = c(w)— c(E(w)(w) in the
context w.
</bodyText>
<subsectionHeader confidence="0.999561">
2.4 Model Selection
</subsectionHeader>
<bodyText confidence="0.9999743125">
The final component of our contribution is a model
selection algorithm for the extension model class (I).
Our algorithm repeatedly refines the accuracy of our
model in increasingly long contexts. Adding a new
parameter to the model will decrease the codelength
of the data and increase the codelength of the model.,
Accordingly, we add a new parameter to the model
only if doing so will decrease the total codelength of
the data and the model.
The incremental cost and benefit of adding a sin-
gle parameter to a given context cannot be accu-
rately approximated in isolation from any other pa-
rameters that might be added to that context. Ac-
cordingly, the incremental cost of adding the sei E&apos;
of extensions to the context w is defined as (6) while
the incremental benefit is defined as (7).
</bodyText>
<equation confidence="0.99603">
AL0(w, E&apos;) = L(0 U ({w} x E&apos;)) — L(0) (6)
ALT(w,E&apos;) L(T(q5) — L(TIO U ({w} x E&apos;)) (7)
</equation>
<bodyText confidence="0.999692666666667">
Keeping only significant terms that are monoton-
ically nondecreasing, we approximate the incremen-
tal cost AL,p(w, E&apos;) as
</bodyText>
<equation confidence="0.876948">
101 )
+ log c(l_w_1) + log ( c(w) (El)
(E&apos;1 )
</equation>
<bodyText confidence="0.999955333333333">
The first term represents the incremental increase
in the size of the context dictionary D. The second
term represents the cost of encoding the candidate
extensions E(w) = E&apos;. The third term represents
(an upper bound on) the cost of encoding c(w). The
fourth term represents the cost of encoding c(&apos; 1w)
for E(w). Only the second and fourth terms are
signfi cant.
Let us now consider the incremental benefit of
adding the extensions E&apos; to a given context w. The
addition of a single parameter (w, o-) to the model
(/) will immediately change A(cr(w), by definition of
the model class. Any change to A(•(w) will also
change the expansion factor Ow) in that context,
which may in turn change the conditional probabili-
ties 13(E— E(w)(w , 0) of symbols not available in that
context. Thus the incremental benefit of adding the
extensions E&apos; to the context w may be calculated as
</bodyText>
<equation confidence="0.979564333333333">
1 — A(E11w)
ALT(w,E&apos;) = c(E — E&apos;(w) log
1 — P(E&apos;lw,
A(crliw)
+ E c(cr&apos; (w) log 23(0.1w, 0)
aiEE&apos;
</equation>
<bodyText confidence="0.999960090909091">
The first term represents the incremental benefit (in
bits) for evaluating E — E&apos; in the context w using
the more accurate expansion factor 6(w). The sec-
ond term represents the incremental benefit (in bits)
of using the direct estimate A(cr&apos;(w) instead of the
model probability 3(o&apos; (w, 0) in the context w. Note
that A(cr&apos; w may be more or less than 15(0-&apos;(w, cb).
Now the incremental cost and benefit of adding
a single extension (w, u) to a model that already
contains the extensions (w, E&apos;) may be defined as
follows.
</bodyText>
<equation confidence="0.898904666666667">
AL0(w, E&apos;, o-) =A.L,p(w,E&apos; U {u}) — 1L0(w,E&apos;)
= 1,01.
AL0(w, E&apos;) log (D( + log (
</equation>
<page confidence="0.987705">
223
</page>
<bodyText confidence="0.9940861875">
ALT(w, E&apos;, ALT(w,r U {a}) — ALT(w, E&apos;)
Let us now use these incremental cost/benefit for-
mulas to design a simple heuristic estimation algo-
rithm for the extension model. The algorithm con-
sists of two subroutines. Refine(D,E,n) augments
the model with all individually profitable extensions
of contexts of length n. It rests on the assump-
tion that adding a new context does not change
the model&apos;s performance in the shorter contexts.
Extend(w) determines all profitable extensions of the
candidate context w, if any exist. Since it is not
feasible to evaluate the incremental profit of every
subset of E, Extend(w) uses a greedy heuristic that
repeatedly augments the set of profitable extensions
of w by the single most profitable extension until it
is not longer profitable to do so.
</bodyText>
<equation confidence="0.642399">
Refine(D,E,n)
</equation>
<listItem confidence="0.974956625">
1. Dr, := {}; := ;
2. Cr,. := {w : w E C.-1E A c(w) &gt; cmin};
3. if ((n &gt; nmaz) V (1C,1 = 0)) then return;
4. for w E C7,
5. S := Extend(w);
6. if 1,51 &gt; 0 then Dr, :=D U {w}; E(w) := S;
7. D D U Dr,; E := E U En;
8. Refine(D,E,n 1);
</listItem>
<bodyText confidence="0.884148">
Cri is the set of candidate contexts of length n,
obtained from the training corpus. De., is the set of
profitable contexts of length n, while En is the set
of profitable extensions of those contexts.
Extend(w)
</bodyText>
<listItem confidence="0.98748">
1. S:={};
2. a := argmax,EE {AL(w, {a})}
3. while (AL(w, S, a-) &gt; 0)
4. S := S U {o-};
5. a := argmax,EE_s {AL(w, S, a)}
6. return(S);
</listItem>
<bodyText confidence="0.999920676470588">
The loop in lines 3-5 repeatedly finds the single
most profitable symbol cr with which to augment
the set S of profitable extensions. The incremental
profit ALL .) is the incremental benefit ALT(• • -)
minus the incremental cost AL4,(...).
Our breadth-first search considers shorter con-
texts before longer ones, and consequently the deci-
sion to add a profitable context y may significantly
decrease the benefit of a more profitable context xy,
particularly when c(xy) c(y). For example, con-
sider a source with two hidden states. In the first
state, the source generates the alphabet E = {0, 1, 2}
uniformly. In the second state, the source generates
the string &amp;quot;012&amp;quot; with certainty. With appropriate
state transition probabilities, the source generates
strings where c(0) c(1) c(2), c(211)/c(1)
c(21e)/c(e), and c(2101)/c(01) &gt; c(211)/c(1). In such
a situation, the best context model includes the con-
texts &amp;quot;0&amp;quot; and &amp;quot;01&amp;quot; along with the empty context
e. However, the divergence heuristic will first deter-
mine that the context &amp;quot;1&amp;quot; is profitable relative to the
empty context, and add it to the model. Now the
profitability of the better context &amp;quot;01&amp;quot; is reduced,
and the divergence heuristic may therefore not in-
clude it in the model. This problem is best solved
with a best first search. Our current implementation
uses a breadth first search to limit the computational
complexity of model selection.
Finally, we note that our parameter estimation
techniques and model selection criteria are compara-
ble in computational complexity to Rissanen&apos;s con-
text models (1983, 1986). For that reason, extension
models should be amendable to efficient online im-
plementation.
</bodyText>
<sectionHeader confidence="0.993925" genericHeader="method">
3 Empirical Results
</sectionHeader>
<bodyText confidence="0.999967488372093">
By means of the following experiments, we hope
to demonstrate the utility of our context modeling
techniques. All results are based on the Brown cor-
pus, an eclectic collection of English prose drawn
from 500 sources across 15 genres (Francis and
Kucera, 1982). The irregular and nonstationary na-
ture of this corpus poses an exacting test for sta-
tistical language models. We use the first 90% of
each file in the corpus to estimate our models, and
then use the remaining 10% of each file in the corpus
to evaluate the models. Each file contains approx-
imately 2000 words. Due to limited computational
resources, we set nmax = 10, cr., = 8, and restrict
our our alphabet size to 70 (ie., all printing ascii
characters, ignoring case distinction).
Our results are summarized in the following ta-
ble. Message entropy (in bits/symbol) is for the
testing corpus only, as per traditional model vali-
dation methodology. The nonmonotonic extension
model (NEM) outperforms all other models for all
orders using vastly fewer parameters. Its perfor-
mance all the more impressive when we consider that
no context blending or escaping is performed, even
for novel events.
We note that the test message entropy of the n-
gram model class is minimized by the 5-gram at 2.38
bits/char. This result for the 5-gram is not honest
because knowledge of the test set was used to select
the optimal model order. Jelinek and Mercer (1980)
have shown to interpolate n-grams of different or-
der using mixing parameters that are conditioned
on the history. Such interpolated Markov sources
are considerably more powerful than traditional n-
grams but contain even more parameters.
The best reported results on the Brown Corpus
are 1.75 bits/char using a large interpolated trigram
word model whose parameters are estimated using
over 600,000,000 words of proprietary training data
(Brown et.al., 1992). The use of proprietary training
data means that these results are not independently
repeatable. In contrast, our results were obtained
using only 900,000 words of generally available train-
ing data and may be independently verified by any-
</bodyText>
<page confidence="0.999061">
224
</page>
<tableCaption confidence="0.960087">
Table 1: Results for the nonmonotonic extension
</tableCaption>
<bodyText confidence="0.99696548">
model (NEM), the nonmonotonic context model
(NCM), Rissanen&apos;s (1983,1986) monotonic context
models (MCM1 , MCM2) and the n-gram model. All
models are order 7. The rightmost column contains
test message entropy in bits/symbol. NEM outper-
forms all other model classes for all orders using sig-
nificantly fewer parameters. It is possible to reduce
the test message entropy of the NEM and NCM to
1.91 and 1.99, respectively, by quadrupling the num-
ber of model parameters.
one with the inclination to do so. The amount of
training data is known to be a significant factor in
model performance. Given a sufficiently rich dictio-
nary of words and a sufficiently large training corpus,
a model of word sequences is likely to outperform an
otherwise equivalent model of character sequences.
For these three reasons — repeatability, training cor-
pus size, and the advantage of word models over
character models — the results reported by Brown
et.al (1992) are not directly comparable to those re-
ported here.
Section 3.1 compares the statistical efficiency of
the various context model classes. Next, sec-
tion 3.2 anecodatally examines the complex interac-
tions among the parameters of an extension model.
</bodyText>
<subsectionHeader confidence="0.999525">
3.1 Model Class Comparison
</subsectionHeader>
<bodyText confidence="0.999870606060606">
Given the tremendous risk of overfitting, the most
important property of a model class is arguably its
statistical efficiency. Informally, statistical efficiency
measures the effectiveness of individual parameters
in a given model class. A high efficiency indicates
that our model class provides a good description of
the data. Conversely, a low efficiency indicates that
the model class does not adequately describe the ob-
served data.
In this section, we compare the statistical effi-
ciency of three model classes: context models, ex-
tension models, and fixed-length Markov processes
(ie., n-grams). Our model class comparison is based
on three criteria of statistical efficiency: total code-
length, bits/parameter on the test message, and
bits/order on the test message. The context and
extension models are all of order 9, and were es-
timated using the true incremental benefit and a
range of fixed incremental costs (between 5 and 25
bits/extension for the extension model and between
25 and 150 bits/context for the context model).
According to the first criteria of statistical effi-
ciency, the best model is the one that achieves the
smallest total codelength L(T, 0) of the training cor-
pus T and model 0 using the fewest parameters.
This criteria measures the statistical efficiency of a
model class according to the MDL framework, where
we would like each parameter to be as cheap as pos-
sible and do as much work as possible. Figure 1
graphs the number of model parameters required to
achieve a given total codelength for the training cor-
pus and model. The extension model class is the
overwhelming winner.
</bodyText>
<figure confidence="0.834386857142857">
Number of Parameters vs. Crodelangth
--••— extension model
- - s- - context model
• • 2,3,4 ngrem
30
25
10
</figure>
<figureCaption confidence="0.999826">
Figure 1: The relationship between the number of
</figureCaption>
<bodyText confidence="0.980050115384615">
model parameters and the total codelength L(T, 0)
of the training corpus T and the model 0. By this
criteria of statistical efficiency, the extension models
completely dominate context models and n-grams.
According to the second criteria of statistical effi-
ciency, the best model is the one that achieves the
lowest test message entropy using the fewest param-
eters. This criteria measures the statistical efficiency
of a model class according to traditional model vali-
dation methodology, tempered by a healthy concern
for overfitting. Figure 2 graphs the number of model
parameters required to achieve a given test message
entropy for each of the three model classes. Again,
the extension model class is the clear winner. (This
is particularly striking when the number of parame-
ters is plotted on a linear scale.) For example, one of
our extension models saves 0.98 bits/char over the
trigram while using less than 1/3 as many param-
eters. Given the logarithmic nature of codelength
and the scarcity of training data, this is a significant
improvement.
According to the third criteria of statistical effi-
ciency, the best model is one that achieves the low-
est test message entropy for a given model order.
This criteria is widely used in the language model-
ing community, in part because model order is typi-
</bodyText>
<figure confidence="0.937904851851852">
Model
NEM
NCM
MGM&apos;
MCM2
n-gr am
Parameters
89,325
687,276
88,945,904
88,945,904
506,352,021,176,052
Entropy
1.97
2.19
2.43
3.12
3.74
100000 1000030 100300,
Number of Parametem
225
Number of Parameters vs. Message Entropy
extension modal
- - - context model
x.-.. 3,4 gram
104 105 106 to&apos; ton
Number of Parameters
</figure>
<figureCaption confidence="0.8783865">
Figure 2: The relationship between the number of
model parameters and test message entropy. The
most striking fact about this graph is the tremen-
dous efficiency of the extension model.
</figureCaption>
<bodyText confidence="0.996515">
cally — although not necessarily — related to both
the number of model parameters and the amount of
computation required to estimate the model. Fig-
ure 3 compares model order to test message entropy
for each of the three model classes. As the order
of the models increases from 0 (ie., unigram) to 10,
we naturally expect the test message entropy to ap-
proach a lower bound, which is itself bounded below
by the true source entropy. By this criteria, the ex-
tension model class is better than the context model
class, and both are significantly better than the n-
gram.
</bodyText>
<figure confidence="0.9852595">
6 8 10
Model Order
</figure>
<figureCaption confidence="0.936154333333333">
Figure 3: The relationship between model order and
test message entropy. The extension model class is
the clear winner by this criteria as well.
</figureCaption>
<subsectionHeader confidence="0.987228">
3.2 Anecdotes
</subsectionHeader>
<bodyText confidence="0.971979142857143">
It is also worthwhile to interpret the parameters of
the extension model estimated from the Brown Cor-
pus, to better understand the interaction between
our model class and our heuristic model selection al-
gorithm. According to the divergence heuristic, the
decision to add an extension (w, cr) is made relative
to that context&apos;s maximal proper suffix Lw j in D as
well as any other extensions in the context w. An
extension (w, cr) will be added only if the direct es-
timate of its conditional probability is significantly
different from its conditional probability in its maxi-
mal proper suffix after scaling by the expansion fac-
tor in the context w, ie., if \(ow) is significantly
different than b(w)73(al [w j).
This is illusrated by the three contexts and six
extensions shown immediately below, where +E(w)
includes all symbols in E(w) that are more likely
in w than they were in Ltd and —E(w) includes all
symbols in E(w) that are less likely in w than they
were in Lw i.
&amp;quot;blish&amp;quot;
&amp;quot;ouestablish&amp;quot;
&amp;quot;euestablish&amp;quot;
The substring blish is most often followed by the
characters &apos;e&apos;, `i&apos;, and &apos;m&apos;, corresponding to the rel-
atively frequent word forms publish{ed,er,ing} and
establish{ed,ing,ment}. Accordingly, the context
&amp;quot;blish&amp;quot; has three positive extensions {e, i,m}, of
which e has by far the greatest probability. The
context &amp;quot;blish&amp;quot; is the maximal proper suffix of two
other contexts in the model, &amp;quot;ouestablish&amp;quot; and
&amp;quot;euestablish&amp;quot;.
The substring o establish occurs most frequently
in the gerund to establish, which is nearly always
followed by a space. Accordingly, the context
&amp;quot;ouestablish&amp;quot; has a single positive extension &amp;quot;u&amp;quot;.
The substring o establish is also found before the
characters &apos;m&apos;, &apos;e&apos;, and `i&apos; in sequences such as
to establishments, {who,ratio,also} established, and
{ to,into,also} establishing. Accordingly, the context
&amp;quot;ouestablish&amp;quot; does not have any negative exten-
sions.
In contrast, the substring e establish is overwhelm-
ingly followed by the character &apos;m&apos;, rarely followed
by &apos;e&apos;, and never followed by either `i&apos; or space. For
this reason, the context &amp;quot;euestablish&amp;quot; has a sin-
gle positive extension {m} corresponding to the great
frequency of the string the establishment. This con-
text also has single negative extension {e}, corre-
sponding to the fact that the character &apos;e&apos; is still pos-
sible in the context &amp;quot;euestablish&amp;quot; but considerably
less likely than in that context&apos;s maximal proper suf-
fix &amp;quot;blish&amp;quot;.
Since `i&apos; is reasonably likely in the context
&amp;quot;blish&amp;quot; but completely unlikely in the context
&amp;quot;euestablish&amp;quot;, we may well wonder why the model
</bodyText>
<figure confidence="0.970166076923077">
Model Order vs. Message Entropy
....... .
- extension model
- - e- - context model
3.0
2
2.4
2.2
3
2.0
1.8
4.4
42
4.0
3.8
3.6
3.4
t 32
3.0
&amp; 2.8
2.6
2.4
22
2.0
1.8
—E(w)
</figure>
<page confidence="0.996702">
226
</page>
<bodyText confidence="0.999969181818182">
does not include the negative extension `i&apos; in addi-
tion to `e&apos; or even instead of `e&apos;. This puzzle is ex-
plained by the expansion factor as follows. Since
the substring e establish is only followed by &apos;in&apos; and
`e&apos;, the expansion factor b(&amp;quot;euestablish&amp;quot;) is essen-
tially zero after &apos;m&apos; and `e&apos; are added to that con-
text, and therefore /3(E — {m, e} I &amp;quot;euestablish&amp;quot;)
is also essentially zero. Thus, T and space are
both assigned nearly zero probability in the con-
text &amp;quot;euestablish&amp;quot;, simply because &apos;m&apos; and `e&apos; get
nearly all the probability in that context.
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999773454545455">
In ongoing work, we are investigating extension mix-
ture models as well as improved model selection al-
gorithms. An extension mixture model is an exten-
sion model whose A(crlw) parameters are estimated
by linearly interpolating the empirical probability
estimates for all extensions that dominate w with
respect to cr, ie., all extensions whose symbol is a
and whose context is a suffix of w. Extension mix-
ing allows us to remove the uniform flattening of
zero frequency symbols in our parameter estimates
(5). Preliminary results are promising. The idea of
context mixing is due to Jelinek and Mercer (1980).
Our results highlight the fundamental tension be-
tween model complexity and data complexity. If the
model complexity does not match the data complex-
ity, then both the total codelength of the past obser-
vations and the predictive error increase. In other
words, simply increasing the number of parameters
in the model does not necessarily increase predictive
power of the model. Therefore, it is necessary to
have a a fine-grained model along with a heuristic
model selection algorithm to guide the expansion of
the model in a principled manner.
Acknowledgements. Thanks to Andrew Appel,
Carl de Marken, and Dafna Scheinvald for their cri-
tique. The paper has benefited from discussions with
the participants of DCC95. Both authors are par-
tially supported by Young Investigator Award IRI-
0258517 to the first author from the National Science
Foundation. The second author is additionally sup-
ported by a tuition award from the Princeton Uni-
versity Research Board. The research was partially
supported by NSF SGER IRI-9217208.
</bodyText>
<sectionHeader confidence="0.999229" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997401121212121">
BROWN, P., PIETRA, V. D., PIETRA, S. D., LAI,
J., AND MERCER, R. An estimate of an upper
bound for the entropy of English. Computational
Linguistics 18 (1992), 31-40.
CLEARY, J., AND WITTEN, I. Data compression
using adaptive coding and partial string matching.
IEEE Trans. Comm. COM-32, 4 (1984), 396-402.
FRANCIS, W. N., AND KUCERA, H. Frequency
analysis of English usage: lexicon and grammar.
Houghton Mifflin, Boston, 1982.
JELINEK, F., AND MERCER, R. L. Interpolated es-
timation of Markov source parameters from sparse
data. In Pattern Recognition in Practice (Amster-
dam, May 21-23 1980), E. S. Gelsema and L. N.
Kanal, Eds., North Holland, pp. 381-397.
KNUTH, D. E. The Art of Computer Programming,
1 ed., vol. 1. Addison-Wesley, Reading, MA, 1968.
MOFFAT, A. Implementing the PPM data compre-
sion scheme. IEEE Trans. Communications 38,
11 (1990), 1917-1921.
RISSANEN, J. Modeling by shortest data descrip-
tion. Automatica 14 (1978), 465-471.
RISSANEN, J. A universal data compression system.
IEEE Trans. Information Theory IT-29, 5 (1983),
656-664.
RISSANEN, J. Complexity of strings in the class of
Markov sources. IEEE Trans. Information The-
ory IT-32, 4 (1986), 526-532.
RISTAD, E. S., AND THOMAS, R. G. Context
models in the MDL framework. In Proceedings of
5th Data Compression Conference (Los Alamitos,
CA, March 28-30 1995), J. Storer and M. Cohn,
Eds., IEEE Computer Society Press, pp. 62-71.
</reference>
<page confidence="0.997889">
227
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.990690">
<title confidence="0.999905">New Techniques for Context Modeling</title>
<author confidence="0.999921">Eric Sven Ristad</author>
<author confidence="0.999921">Robert G Thomas</author>
<affiliation confidence="0.999953">Department of Computer Science Princeton University</affiliation>
<email confidence="0.999064">ristadOcs.princeton.edu</email>
<email confidence="0.999064">rgtOcs.princeton.edu</email>
<abstract confidence="0.99897875">We introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic. Together these techniques result in language models that have few states, even fewer parameters, and low message entropies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P BROWN</author>
<author>V D PIETRA</author>
<author>S D PIETRA</author>
<author>J LAI</author>
<author>R MERCER</author>
</authors>
<title>An estimate of an upper bound for the entropy of English.</title>
<date>1992</date>
<journal>Computational Linguistics</journal>
<volume>18</volume>
<pages>31--40</pages>
<marker>BROWN, PIETRA, PIETRA, LAI, MERCER, 1992</marker>
<rawString>BROWN, P., PIETRA, V. D., PIETRA, S. D., LAI, J., AND MERCER, R. An estimate of an upper bound for the entropy of English. Computational Linguistics 18 (1992), 31-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J CLEARY</author>
<author>I WITTEN</author>
</authors>
<title>Data compression using adaptive coding and partial string matching.</title>
<date>1984</date>
<journal>IEEE Trans. Comm.</journal>
<volume>32</volume>
<pages>396--402</pages>
<marker>CLEARY, WITTEN, 1984</marker>
<rawString>CLEARY, J., AND WITTEN, I. Data compression using adaptive coding and partial string matching. IEEE Trans. Comm. COM-32, 4 (1984), 396-402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N FRANCIS</author>
<author>H KUCERA</author>
</authors>
<title>Frequency analysis of English usage: lexicon and grammar.</title>
<date>1982</date>
<location>Houghton Mifflin, Boston,</location>
<marker>FRANCIS, KUCERA, 1982</marker>
<rawString>FRANCIS, W. N., AND KUCERA, H. Frequency analysis of English usage: lexicon and grammar. Houghton Mifflin, Boston, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F JELINEK</author>
<author>R L MERCER</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Pattern Recognition in Practice</booktitle>
<pages>381--397</pages>
<location>Amsterdam,</location>
<marker>JELINEK, MERCER, 1980</marker>
<rawString>JELINEK, F., AND MERCER, R. L. Interpolated estimation of Markov source parameters from sparse data. In Pattern Recognition in Practice (Amsterdam, May 21-23 1980), E. S. Gelsema and L. N. Kanal, Eds., North Holland, pp. 381-397.</rawString>
</citation>
<citation valid="true">
<date>1968</date>
<booktitle>The Art of Computer Programming,</booktitle>
<volume>1</volume>
<editor>KNUTH, D. E.</editor>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA,</location>
<marker>1968</marker>
<rawString>KNUTH, D. E. The Art of Computer Programming, 1 ed., vol. 1. Addison-Wesley, Reading, MA, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A MOFFAT</author>
</authors>
<title>Implementing the PPM data compresion scheme.</title>
<date>1990</date>
<journal>IEEE Trans. Communications</journal>
<volume>38</volume>
<pages>1917--1921</pages>
<marker>MOFFAT, 1990</marker>
<rawString>MOFFAT, A. Implementing the PPM data compresion scheme. IEEE Trans. Communications 38, 11 (1990), 1917-1921.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J RISSANEN</author>
</authors>
<title>Modeling by shortest data description.</title>
<date>1978</date>
<journal>Automatica</journal>
<volume>14</volume>
<pages>465--471</pages>
<marker>RISSANEN, 1978</marker>
<rawString>RISSANEN, J. Modeling by shortest data description. Automatica 14 (1978), 465-471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J RISSANEN</author>
</authors>
<title>A universal data compression system.</title>
<date>1983</date>
<journal>IEEE Trans. Information Theory</journal>
<volume>29</volume>
<pages>656--664</pages>
<marker>RISSANEN, 1983</marker>
<rawString>RISSANEN, J. A universal data compression system. IEEE Trans. Information Theory IT-29, 5 (1983), 656-664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J RISSANEN</author>
</authors>
<title>Complexity of strings in the class of Markov sources.</title>
<date>1986</date>
<journal>IEEE Trans. Information Theory</journal>
<volume>32</volume>
<pages>526--532</pages>
<marker>RISSANEN, 1986</marker>
<rawString>RISSANEN, J. Complexity of strings in the class of Markov sources. IEEE Trans. Information Theory IT-32, 4 (1986), 526-532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S RISTAD</author>
<author>R G THOMAS</author>
</authors>
<title>Context models in the MDL framework.</title>
<date>1995</date>
<booktitle>In Proceedings of 5th Data Compression Conference</booktitle>
<pages>62--71</pages>
<publisher>IEEE Computer Society Press,</publisher>
<location>Los Alamitos, CA,</location>
<marker>RISTAD, THOMAS, 1995</marker>
<rawString>RISTAD, E. S., AND THOMAS, R. G. Context models in the MDL framework. In Proceedings of 5th Data Compression Conference (Los Alamitos, CA, March 28-30 1995), J. Storer and M. Cohn, Eds., IEEE Computer Society Press, pp. 62-71.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>