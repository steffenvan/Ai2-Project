<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.4087595">
Towards Uniform Processing of Constraint-based Categorial
Grammars
</title>
<author confidence="0.153895">
Gertjan van Noord
</author>
<affiliation confidence="0.350186">
Lehrstuhl für Computerlinguistik
Universitat des Saarlandes
</affiliation>
<address confidence="0.8849725">
Im Stadtwald 15
D-6600 Saarbriicken 11, FRG
</address>
<email confidence="0.995979">
vannoord@coli.uni-sb.de
</email>
<sectionHeader confidence="0.996545" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928222222222">
A class of constraint-based categorial grammars is
proposed in which the construction of both logical
forms and strings is specified completely lexically.
Such grammars allow the construction of a uni-
form algorithm for both parsing and generation.
Termination of the algorithm can be guaranteed
if lexical entries adhere to a constraint, that can
be seen as a computationally motivated version
of GB&apos;s projection principle.
</bodyText>
<sectionHeader confidence="0.98897" genericHeader="keywords">
1 Motivations
</sectionHeader>
<bodyText confidence="0.999959101694916">
In constraint-based approaches to grammar the
semantic intermtation of phrases is often de-
fined in the lexical entries. These lexical en-
tries specify their semantic interpretation, taking
into account the semantics of the arguments they
subcategorize for (specified in their subcat list).
The grammar rules simply percolate the seman-
tics upwards; by the selection of the arguments,
this semantic formula then gets further instanti-
ated (Moore, 1989). Hence in such approaches it
can be said that all semantic formulas are &apos;pro-
jected from the lexicon&apos; (Zeevat et al., 1987).
Such an organization of a grammar is the starting
point of a class of generation algorithms that have
become popular recently (Calder et al., 1989;
Shieber et al., 1989; Shieber et aL, 1990). These
semantic-head-driven algorithms are both geared
towards the input semantic representation and
the information contained in lexical entries. If
the above sketched approach to semantic inter-
pretation is followed systematically, it is possible
to show that such a semantic-head-driven gen-
eration algorithm terminates (Dymetman et al.,
1990).
In van Noord (1991) I define a head-driven
parser (based on Kay (1989)) for a class of
constraint-based grammars in which the con-
struction of strings may use more complex op-
erations that simple context-free concatenation.
Again, this algorithm is geared towards the in-
put (string) and the information found in lexi-
cal entries. In this paper I investigate an ap-
proach where the construction of strings is de-
fined lexically. Grammar rules simply percolate
strings upwards. Such an approach seems feasible
if we allow for powerful constraints to be defined.
The head-corner parser knows about strings and
performs operations on them; in the types of
grammars defined here these operations are re-
placed by general constraint-solving techniques
(Holifeld and Smolka, 1988; Tuda et a/., 1989;
Damas et al., 1991). Therefore, it becomes pos-
sible to view both the head-driven generator and
the head-driven parser as one and the same algo-
rithm.
For this uniform algorithm to terminate, we
generalize the constraint proposed by Dymetman
et al. (1990) to both semantic interpretations
and strings. That is, for each lexical entry we
require that its string and its semantics is larger
than the string and the semantics associated with
each of its arguments. The following picture then
emerges. The depth of a derivation tree is de-
termined by the subcat list of the ultimate head
of the tree. Furthermore, the string and the se-
mantic representation of each of the non heads in
the derivation tree is determined by the subcat
list as well. A specific condition on the relation
between elements in the subcat list and their se-
</bodyText>
<page confidence="0.997488">
12
</page>
<bodyText confidence="0.999504533333333">
mantics and string representation ensures termi-
nation. This condition on lexical entries can be
seen as a lexicalized and computationally moti-
vated version of GB&apos;s projection principle.
Word-order domains. The string associated
with a linguistic object (sign) is defined in terms
of its word-order domain (Reape, 1989; Reape,
1990a). I take a word-order domain as a sequence
of signs. Each of these signs is associated with a
word-order domain recursively, or with a sequence
of words. A word-order domain is thus a tree.
Linear precedence rules are defined that constrain
possible orderings of signs in such a word-order
domain. Surface strings are a direct function of
word-order domains. In the lexicon, the word-
order domain of a lexical entry is defined by shar-
ing parts of this domain with the arguments it
subcategorizes for. Word-order domains are per-
colated upward. Hence word-order domains are
constructed in a derivation by gradual instantia-
tions (hence strings are constructued in a deriva-
tion by gradual instantiation as well). Note that
this implies that an unsaturated sign is not asso-
ciated with one string, but merely with a set of
possible strings (this is similar to the semantic in-
terpretation of unsaturated signs (Moore, 1989)).
In lexical entries, word order domains are defined
using Reape&apos;s sequence union operation (Reape,
1990a). Hence the grammars are not only based
on context-free string concatenation.
</bodyText>
<sectionHeader confidence="0.969079" genericHeader="introduction">
2 Constraint-based versions
</sectionHeader>
<subsectionHeader confidence="0.661359">
of categorial grammar
</subsectionHeader>
<bodyText confidence="0.999708666666667">
The formalism I assume consists of definite clau-
ses over constraint languages in the manner of
Hohfeld and Smolka (1988). The constraint lan-
guage at least consists of the path equations
known from PATR II (Shieber, 1989), augmented
with variables. I write such a definite clause as:
</bodyText>
<equation confidence="0.699975">
P • • 4,1,4).
</equation>
<bodyText confidence="0.999771666666667">
where p, qi are atoms and 0 is a (conjunction of)
constraint(s). The path equations are written as
in PATR II, but each path starts with a variable:
</bodyText>
<equation confidence="0.891399">
(Xi c
or
(X111 4,) Xi .1n)
</equation>
<bodyText confidence="0.99996375">
where Xk are variables, c is a constant, 1,1&apos; are
attributes. I also use some more powerful con-
straints that are written as atoms.
This formalism is used to define what possible
&apos;signs&apos; are, by the definition of the unary predi-
cate sign/1. There is only one nonunit clause for
this predicate. The idea is that unit clauses for
sign/1 are lexical entries, and the one nonunit
clause defines the (binary) application rule. I as-
sume that lexical entries are specified for their
arguments in their `subcat list&apos; (sc). In the ap-
plication rule a head selects the first (f) element
from its subcat list, and the tail (r) of the subcat
list is the subcat list of the mother; the semantics
(sem) and strings (phon) are shared between the
head and the mother.
</bodyText>
<equation confidence="0.992977466666666">
sign(X0):— sign(X1), sign(X2),
(Xo synsem sem) --L. (X1 synsem sem),
(X0 phon) (Xi phon),
(X0 synsem sc) (X1 synsem sc r),
(X1 synsem sc (X2).
I write such rules using matrix notation as fol-
lows; string(X) represents the value Y, where
string(X,Y).
X0 : synsem
[sem : 2
Sc:
phon
synsem ssec7(gD I ]
Lphon :
X2: B
</equation>
<bodyText confidence="0.980572368421053">
The grammar also consists of a number of lex-
ical entries. Each of these lexical entries is speci-
fied for its subcat list, and for each subcat element
the semantics and word-order domain is specified,
such that they satisfy a termination condition to
be defined in the following section. For exam-
ple, this condition is satisfied if the semantics of
each element in the subcat list is a proper sub-
part of the semantics of the entry, and each ele-
ment of the subcat list is a proper subpart of the
word-order domain of the entry. The phonology
of a sign is defined with respect to the word-order
domain with the predicate &apos;string&apos;. This predi-
cate simply defines a left-to-right depth-first tra-
verse&apos; of a word-order domain and picks up all
the strings at the terminals. It should be noted
that the way strings are computed from the word-
order domains implies that the string of a node
X1:
</bodyText>
<page confidence="0.925327">
13
</page>
<bodyText confidence="0.541642">
synsem: El
</bodyText>
<equation confidence="0.985024727272727">
syn : vp
synsem: [
syn : np
Sc: ()
sem : fl
sem : schla f en(D
[synsem :
Eldom : ( , dom: 0
phon: (schlii f 0
phon : string(E1)
El
</equation>
<figureCaption confidence="0.999884">
Figure 1: The German verb `schlift&apos;
</figureCaption>
<bodyText confidence="0.981923318181818">
not necessarily is the concatenation of the strings
of its daughter nodes. In fact, the relation be-
tween the strings of nodes is defined indirectly
via the word-order domains.
The word-order domains are sequences of signs.
One of these signs is the sign corresponding to the
lexical entry itself. However, the domain of this
sign is empty, but other values can be shared.
Hence the entry for an intransitive German verb
such as `schlift&apos; (sleeps) is defined as in figure 1.
I introduce some syntactic sugaring to make
such entries readable. Firstly, X Pi will stand for
synsem:
Furthermore, in lexical entries the synsem part is
shared with the synsem part of an element of the
word order domain, that is furthermore specified
for the empty domain and some string. I will
write: &lt; string &gt;&gt; in a lexical entry to stand for
the sign whose synsem value is shared with the
synsem of the lexical entry itself; its dom value
is () and its phon value is string. The foregoing
entry is abreviated as:
</bodyText>
<construct confidence="0.913764">
synse : sem : schla f en(E,)
m [syn : vp
sc : (EIN PI)
dom: El , &lt; schlii f t &gt;)
phon : string(
</construct>
<bodyText confidence="0.999941703703704">
Note that in this entry we merely stipulate that
the verb preceded by the subject constitutes the
word-order domain of the entire phrase. How-
ever, we may also use more complex constraints
to define word-order constraints. In particular,
as already stated above, LP constraints are de-
fined which holds for word-order domains. I use
the sequence-union predicate (abbreviated su)
defined by Reape as a possible constraint as well.
This predicate is motivated by clause union and
scrambling phenomena in German. A linguisti-
cally motivated example of the use of this con-
straint can be found in section 4. The predicate
su(A, B, C) is true in case the elements of the list
C is the multi set union of the elements of the lists
A and B; moreover, a &lt; bin either A or B if a &lt;
b in C. I also use the notation X Uo Y to denote
the value Seq, where su(X,Y,Seq). For exam-
ple, suaa, d, e],[b,c, f],[a,b,c,d,e, f]); [a, clUo [b]
stands for [a, c, b] ,[a , b , c] or [b, a, c]. In fact, I as-
sume that this predicate is also used in the simple
cases, in order to be able to spel out generaliza-
tions in the linear precedence constraints. Hence
the entry for `schlafen&apos; is defined as follows, where
I write lp(X) to indicate that the lp constraints
should be satisfied for X. I have nothing to say
about the definition of these constraints.
</bodyText>
<equation confidence="0.9956">
syn : vp
synsem: [ sem : schlafen(D
Sc: (EN Pi)
odorn : (0) Uo (&lt; schlii ft &gt;)
phon : string(Ip(1))
</equation>
<bodyText confidence="0.999701">
In the following I (implicitly) assume that for each
lexical entry the following holds:
</bodyText>
<equation confidence="0.907117">
phon : string(lp(1)) ]
</equation>
<sectionHeader confidence="0.981378" genericHeader="method">
3 Uniform Processing
</sectionHeader>
<bodyText confidence="0.998898">
In van Noord (1991) I define a parsing strat-
egy, called &apos;head-corner parsing&apos; for a class of
</bodyText>
<equation confidence="0.79070875">
[syn : xp ]]
sem :
Sc:
II
</equation>
<page confidence="0.983194">
14
</page>
<bodyText confidence="0.999976735294117">
grammars allowing more complex constraints on
strings than context-free concatenation. Reape
defines generalizations of the shift-reduce parser
and the CYK parser (Reape, 1990b), for the
same class of grammars. For generation head-
driven generators can be used (van Noord, 1989;
Calder et al., 1989; Shieber et al., 1990). Alter-
natively I propose a generalization of these head-
driven parsing— and generation algorithms. The
generalized algorithm can be used both for pars-
ing and generation. Hence we obtain a uniform
algorithm for both processes. Shieber (1988) ar-
gues for a uniform architecture for parsing in
generation. In his proposal, both processes are
(different) instantiations of a parameterized algo-
rithm. The algorithm I define is not parameter-
ized in this sense, but really uses the same code in
both directions. Some of the specific properties of
the head-driven generator on the one hand, and
the head-driven parser on the other hand, follow
from general constraint-solving techniques. We
thus obtain a uniform algorithm that is suitable
for linguistic processing. This result should be
compared with other uniform scheme&apos;s such as
SLD-resolution or some implementations of type
inference (Zajac, 1991, this volume) which clearly
are also uniform but faces severe problems in the
case of lexicalist grammars, as such scheme&apos;s do
not take into account the specific nature of lexi-
calist grammars (Shieber et al., 1990).
Algorithm. The algorithm is written in the
same formalism as the grammar and thus con-
stitutes a meta-interpreter. The definite clauses
of the object-grammar are represented as
</bodyText>
<equation confidence="0.627533">
lexical_entry(X):— (1).
</equation>
<bodyText confidence="0.878478588235294">
for the unit clauses
sign(X):-4).
and
rule(H, M, A) :—
for the rule
sig n(M) sign(H), sign(A), (1).
The associated interpreter is a Prolog like top-
down backtrack interpreter where term unifi-
cation is replaced by more general constraint-
solving techniques, (Hafeld and Smolka, 1988;
Tuda et aL, 1989; Damas ei al., 1991). The
meta-interpreter defines a head-driven bottom-up
strategy with top-down prediction (figure 2), and
is a generalization of the head-driven generator
(van Noord, 1989; Calder et al., 1989; van Noord,
1990a) and the head-corner parser (Kay, 1989;
van Noord, 1991).
</bodyText>
<equation confidence="0.790643">
prove(T):—
lexical_entry(L), connect(L,T),
(T phon) = (L phon),
(T synsem sem) = (L synsem sem).
connect(T,T).
connect(S,T):—
rule(S, M, A), prove(A),
connect(M,T).
</equation>
<figureCaption confidence="0.999453">
Figure 2: The uniform algorithm
</figureCaption>
<bodyText confidence="0.96899271875">
In the formalism defined in the preceding sec-
tion there are two possible ways where non-
termination may come in, in the constraints or
in the definite relations over these constraints. In
this paper I am only concerned with the second
type of non-termination, that is, I simply assume
that the constraint language is decidable (Hohfeld
and Smolka, 1988). 1 For the grammar sketched
in the foregoing section we can define a very nat-
ural condition on lexical entries that guarantees
us termination of both parsing and generation,
provided the constraint language we use is decid-
able.
The basic idea is that for a given semantic rep-
resentation or (string constraining a) word-order
domain, the derivation tree that derives these rep-
resentations has a finite depth. Lexical entries
are specified for (at least) Sc, phon and sem. The
constraint merely states that the values of these
attributes are dependent. It is not possible for
one value to &apos;grow&apos; unless the values of the other
attributes grow as well. Therefore the constraint
we propose can be compared with GB&apos;s projec-
tion principle if we regard each of the attributes
to define a &apos;level of description&apos;. Termination can
then be guaranteed because derivation trees are
restricted in depth by the value of the sc at-
tribute.
In order to define a condition to guarantee ter-
mination we need to be specific about the inter-
&apos;This is the case if we only have PATR equations; but
probably not if we use uo, string/2and /p/2 unlimited.
</bodyText>
<page confidence="0.996858">
15
</page>
<bodyText confidence="0.98866005">
pretation of a lexical entry. Following Shieber
(1989) I assume that the interpretation of a set
of path equations is defined in terms of directed
graphs; the interpretation of a lexical entry is a
set of such graphs. The &apos;size&apos; of a graph simply is
defined as the number of nodes the graph consists
of. We require that for each graph in the interpre-
tation of a lexical entry, the size of the subgraph
at sem is strictly larger than each of the sizes of
the sem part of the (subgraphs corresponding to
the) elements of the subcat list. I require that
for each graph in the interpretation of a lexical
entry, the size of phon is strictly larger than each
of the sizes of (subgraphs corresponding to) the
phon parts of the elements of the subcat lists.
Summarizing, all lexical entries should satisfy the
following condition:
Termination condition. For each interpreta-
tion L of a lexical entry, if E is an element of L&apos;s
subcat list (i.e. (L synsern se rt f) E), then:
</bodyText>
<construct confidence="0.4249085">
size[(E phon)] &lt; size[(L phon)]
size[(E synsem sem)] &lt; size[(L synsem sem)]
</construct>
<bodyText confidence="0.982809888888889">
The most straightforward way to satisfy this con-
dition is for an element of a subcat list to share
its semantics with a proper part of the semantics
of the lexical entry, and to include the elements
of the subcat list in its word-order domain.
Possible inputs. In order to prove termination
of the algorithm we need to make some assump-
tions about possible inputs. For a discussion cf.
van Noord (1990b) and also Thompson (1991,
this volume). The input to parsing and gener-
ation is specified as the goal
?— sign(X0),
where 4 restricts the variable Xo. We re-
quire that for each interpretation of X0 there
is a maximum for parsing of size[(X0 phon)],
and that there is a maximum for generation of
size[(X0 synsem sem)].
If the input has a maximum size for either se-
mantics or phonology, then the uniform algorithm
terminates (assuming the constraint language is
decidable), because each recursive call to &apos;prove&apos;
will necessarily be a &apos;smaller&apos; problem, and as
the order on semantics and word-order domains
is well-founded, there is a &apos;smallest&apos; problem. As
a lexical entry specifies the length of its subcat
list, there is only a finite number of embeddings
of the &apos;connect&apos; clause possible.
</bodyText>
<sectionHeader confidence="0.965406" genericHeader="method">
4 Some examples
</sectionHeader>
<bodyText confidence="0.999505041666667">
Verb raising. First I show how Reape&apos;s anal-
ysis of Dutch and German verb raising construc-
tions can be incorporated in the current grammar
(Reape, 1989; Reape, 1990a). For a linguistic dis-
cussion of verb-raising constructions the reader is
referred to Reape&apos;s papers. A verb raiser such as
the German verb &apos;versprechen&apos; (promise) selects
three arguments, a vp, an object up and a subject
up. The word-order domain of the vp is unioned
into the word order domain of versprechen. This
is necessary because in German the arguments of
the embedded vp can in fact occur left from the
other arguments of versprechen, as in:
es i ihmi jemandk zu leseni versprocheni hatk
(it him someone to read promised had
i.e. someome had promised him to read it.
Hence, the lexical entry for the raising verb &apos;ver-
sprechen&apos; is defined as in figure 3. The word-order
domain of &apos;versprechen&apos; simply is the sequence
union of the word-order domain of its vp object,
with the up object, the subject, and versprechen
itself. This allows any of the permuations (al-
lowed by the LP constraints) of the up object,
versprechen, the subject, and the elements of
the domain of the vp object (which may contain
signs that have been unioned in recursively).
Seperable prefixes. The current framework
offers an interesting account of seperable prefix
verbs in German and Dutch. For an overview of
alternative accounts of such verbs, see Uszkoreit
(1987)[chapter 4]. At first sight, such verbs may
seem problematic for the current approach be-
cause their prefixes seem not to have any seman-
tic content. However, in my analysis a seperable
prefix is lexically specified as part of the word-
order domain of the verb. Hence a particle is not
identified as an element of the subcat list. Fig-
ure 4 might be the encoding of the German verb
&apos;anrufen&apos; (call up). Note that this analysis con-
forms to the condition of the foregoing section,
because the particle is not on the subcat list. The
advantages of this analysis can be summarized as
follows.
Firstly, there is no need for a feature system
to link verbs with the correct prefixes, as eg.
in Uszkoreit&apos;s proposal. Instead, the correspon-
dence is directly stated in the lexical entry of the
particle verb which seems to me a very desirable
</bodyText>
<page confidence="0.959998">
16
</page>
<equation confidence="0.721630533333333">
synsem :
dorn
: (&lt; versprechen &gt;) U0
sern : versprechen( 4 IDE)
synsem : [
[ ss eY mn :&amp;quot;
: 11
6
Sc: (,
Sc: (N
dom :
II
,E
(0). u,
1 NP5, IN P4)
</equation>
<figureCaption confidence="0.999414">
Figure 3: The German verb `versprechen&apos;
</figureCaption>
<bodyText confidence="0.996166794117647">
result.
Secondly, the analysis predicts that particles
can &apos;move away&apos; from the verb in case the verb
is sequence-unioned into a larger word-order do-
main. This prediction is correct. The clearest
examples are possibly from Dutch. In Dutch, the
particle of a verb can be placed (nearly) anywhere
in the verb cluster, as long as it precedes its ma-
trix verb:
*dat jan marie piet heeft willen zien bellen op
dat jan marie piet heeft willen zien op bellen
dat jan marie piet heeft willen op zien bellen
dat jan marie piet heeft op willen zien bellen
dat jan marie piet op heeft willen zien bellen
that john mary pete up has want see call
(i.e. john wanted to see mary call up pete)
The fact that the particle is not allowed to follow
its head word is easily explained by the (indepen-
dently motivated) LP constraint that arguments
of a verb precede the verb. Hence these curious
facts follow immediately in our analysis (the anal-
ysis makes the same prediction for German, but
because of the different order of German verbs,
this prediction can not be tested).
Thirdly, Uszkoreit argues that a theory of
seperable prefixes should also account for the
&apos;systematic orthographic insecurity felt by native
speakers&apos; i.e. whether or not they should write
the prefix and the verb as one word. The current
approach can be seen as one such explanation: in
the lexical entry for a seperable prefix verb the
verb and prefix are already there, on the other
hand each of the words is in a different part of
the word-order domain.
</bodyText>
<sectionHeader confidence="0.989558" genericHeader="method">
5 HP S G Markers
</sectionHeader>
<bodyText confidence="0.999975166666667">
In newer versions of HPSG (Pollard and Sag,
1991) a special &apos;marker&apos; category is assumed for
which our projection principle does not seem to
work. For example, complementizers are ana-
lyzed as markers. They are not taken to be the
head of a phrase, but merely &apos;mark&apos; a sentence
for some features. On the other hand, a spe-
cial principle is assumed such that markers do in
fact select for certain type of constituents. In the
present framework a simple approach would be to
analyze such markers as functors, i.e. heads, that
have one element in their subcat list:
</bodyText>
<equation confidence="0.9950275">
[syn : vp_sub
synsem : sem :
Sc: ( 2 V P_FINO
dom : (&lt; dass &gt; ,D
</equation>
<bodyText confidence="0.970377294117647">
However, the termination condition defined in
the third section can not always be satisfied be-
cause these markers usually do not have much
semantic content (as in the preceding example).
Furthermore these markers may also be phoneti-
cally empty, for example in the HPSG-2 analysis
of infinite vp&apos;s that occur independently such an
empty marker is assumed. Such an entry would
look presumably as follows, where it is assumed
that the empty marker constitutes no element of
its own domain:
d o m : (E)
It seems, then, that analyses that rely on such
marker categories can not be defined in the cur-
rent framework. On the other hand, however,
such markers have a very restricted distribution,
and are never recursive. Therefore, a slight mod-
</bodyText>
<equation confidence="0.76674475">
syn : vp _in f _sub
sem : 1
Sc: (
[synsem :[
</equation>
<page confidence="0.936658">
17
</page>
<table confidence="0.520998666666667">
synsem: syn : vp
sem : anru f en(ril)
Sc:
</table>
<figureCaption confidence="0.999568">
Figure 4: The verb `anrufen&apos;
</figureCaption>
<equation confidence="0.85006225">
isynsem : syn : part
dom : (&lt;ruft &gt;) U0 (E)
U0 ( [dom :
phon: (an)
</equation>
<bodyText confidence="0.999985225806452">
ification of the termination condition can be de-
fined that take into account such marker cate-
gories. To make this feasible we need a constraint
that markers can not apply arbitrarily. In HPSG-
2 the distribution of the English complementizer
&apos;that&apos; is limited by the introduction of a special
binary feature whose single purpose is to disallow
sentences such as `john said that that that mary
loves pete&apos;. It is possible to generalize this to dis-
allow any marker to be repeatedly applied in some
domain. The &apos;seed&apos; of a lexical entry is this entry
itself; the seed of a rule is the seed of the head of
this rule unless this head is a marker in which case
the seed is defined as the seed of the argument.
In a derivation tree, no marker may be applied
more than once to the same seed. This &apos;don&apos;t
stutter&apos; principle then subsumes the feature ma-
chinery introduced in HPSG-2, and parsing and
generation terminates for the resulting system.
Given such a system for marker categories,
we need to adapt our algorithm. I assume lex-
ical entries are divided (eg. using some user-
defined predicate) into markers and not mark-
ers; markers are defined with the predicate
marker(Sign,Name) where Name is a unique
identifier. Other lexical entries are encoded as
before. marktypes(L) is the list of all marker
identifiers. The idea simply is that markers are
applied top-down, keeping track of the markers
that have already been used. The revised algo-
rithm is given in figure 5.
</bodyText>
<sectionHeader confidence="0.996019" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.679944">
I am supported by SFB 314, Project N3 BiLD.
</bodyText>
<construct confidence="0.738798307692308">
prove(T): —
marktypes(M), prove(T, M).
prove(T, M) : —
marker(L, Name), del(Name, M, M2),
rule(L,T, A), prove(A, M2).
prove(T, M) : —
lexical _entry(L), connect(L,T),
(T phon) = (L phon),
(T synsem sem) (L synsem sem).
connect(T,T).
connect(S,T):—
rule(S, M, A), prove(A),
connect(M,T).
</construct>
<figureCaption confidence="0.998964">
Figure 5: The algorithm including markers
</figureCaption>
<sectionHeader confidence="0.989687" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.931936285714286">
Jonathan Calder, Mike Reape, and Henk Zeevat.
An algorithm for generation in unification cat-
egorial grammar. In Fourth Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 233-240, Manch-
ester, 1989.
Luis Damas, Nelma Moreira, and Giovanni B.
Varile. The formal and processing models of
CLG. In Fifth Conference of the European
Chapter of the Association for Computational
Linguistics, Berlin, 1991.
Marc Dymetman, Pierre Isabelle, and Francois
Perrault. A symmetrical approach to parsing
and generation. In Proceedings of the 13th In-
</reference>
<page confidence="0.994295">
18
</page>
<reference confidence="0.997507034090909">
ternational Conference on Computational Lin-
guistics (COLING), Helsinki, 1990.
Markus HOHeld and Gert Smolka. Definite rela-
tions over constraint languages. Technical re-
port, 1988. LILOG Report 53; to appear in
Journal of Logic Programming.
Martin Kay. Head driven parsing. In Proceedings
of Workshop on Parsing Technologies, Pitts-
burgh, 1989.
Robert C. Moore. Unification-based semantic
interpretation. In 27th Annual Meeting of
the Association for Computational Linguistics,
Vancouver, 1989.
Carl Pollard and Ivan Sag. Information Based
Syntax and Semantics, Volume 2. Center for
the Study of Language and Information Stan-
ford, 1991. to appear.
Mike R,eape. A logical treatment of semi-free
word order and bounded discontinuous con-
stituency. In Fourth Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics, UMIST Manchester, 1989.
Mike R,eape. Getting things in order. In Proceed-
ings of the Symposium on Discontinuous Con-
stituency, ITK Tilburg, 1990.
Mike Reape. Parsing bounded discontinous con-
stituents: Generalisations of the shift-reduce
and CKY algorithms, 1990. Paper presented
at the first CLIN meeting, October 26, OTS
Utrecht.
Stuart M. Shieber, Gertjan van Noord, Robert C.
Moore, and Fernando C.N. Pereira. A
semantic-head-driven generation algorithm for
unification based formalisms. In 27th Annual
Meeting of the Association for Computational
Linguistics, Vancouver, 1989.
Stuart M. Shieber, Gertjan van Noord, Robert C.
Moore, and Fernando C.N. Pereira. Semantic-
head-driven generation. Computational Lin-
guistics, 16(1), 1990.
Stuart M. Shieber. A uniform architecture for
parsing and generation. In Proceedings of the
12th International Conference on Computa-
tional Linguistics (COLING), Budapest, 1988.
Stuart M. Shieber. Parsing and Type Inference
for Natural and Computer Languages. PhD
thesis, Menlo Park, 1989. Technical note 460.
Henry S. Thompson. Generation and transla-
tion - towards a formalism-independent char-
acterization. In Proceedings of ACL workshop
Reversible Grammar in Natural Language Pro-
cessing, Berkeley, 1991.
Hirosi Tuda, Kôiti Hasida, and Hidetosi Sirai.
JPSG parser on constraint logic programming.
In Fourth Conference of the European Chapter
of the Association for Computational Linguis-
tics, Manchester, 1989.
Hans Uszkoreit. Word Order and Constituent
Structure in German. CSLI Stanford, 1987.
Gertjan van Noord. BUG: A directed bottom-
up generator for unification based formalisms.
Working Papers in Natural Language Process-
ing, Katholieke Universiteit Leuven, Stichting
Taaltechnologie Utrecht, 4, 1989.
Gertjan van Noord. An overview of head-
driven bottom-up generation. In Robert Dale,
Chris Mellish, and Michael Zock, editors, Cur-
rent Research in Natural Language Generation.
Academic Press, 1990.
Gertjan van Noord. Reversible unification-based
machine translation. In Proceedings of the
13th International Conference on Computa-
tional Linguistics (COLING), Helsinki, 1990.
Gertjan van Noord. Head corner parsing for dis-
continuous constituency. In 29th Annual Meet-
ing of the Association for Computational Lin-
guistics, Berkeley, 1991.
RAmi Zajac. A uniform architecture for parsing,
generation and transfer. In Proceedings of ACL
workshop Reversible Grammar in Natural Lan-
guage Processing, Berkeley, 1991.
IIenk Zeevat, Ewan Klein, and Jo Calder. Unifi-
cation categorial grammar. In Nicholas Had-
dock, Ewan Klein, and Glyn Morrill, edi-
tors, Categorial Grammar, Unification Gram-
mar and Parsing. Centre for Cognitive Science,
University of Edinburgh, 1987. Volume 1 of
Working Papers in Cognitive Science.
</reference>
<page confidence="0.999332">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.013446">
<title confidence="0.9988595">Towards Uniform Processing of Constraint-based Categorial Grammars</title>
<author confidence="0.747894">Gertjan van_Noord Lehrstuhl für</author>
<affiliation confidence="0.969797">Universitat des</affiliation>
<author confidence="0.545471">Im Stadtwald D- Saarbriicken</author>
<email confidence="0.977151">vannoord@coli.uni-sb.de</email>
<abstract confidence="0.987747670454546">A class of constraint-based categorial grammars is proposed in which the construction of both logical forms and strings is specified completely lexically. Such grammars allow the construction of a uniform algorithm for both parsing and generation. Termination of the algorithm can be guaranteed if lexical entries adhere to a constraint, that can be seen as a computationally motivated version of GB&apos;s projection principle. 1 Motivations In constraint-based approaches to grammar the intermtation phrases is often defined in the lexical entries. These lexical entries specify their semantic interpretation, taking into account the semantics of the arguments they for (specified in their list). The grammar rules simply percolate the semantics upwards; by the selection of the arguments, this semantic formula then gets further instantiated (Moore, 1989). Hence in such approaches it can be said that all semantic formulas are &apos;profrom the lexicon&apos; (Zeevat 1987). Such an organization of a grammar is the starting point of a class of generation algorithms that have popular recently (Calder 1989; et al., 1989; Shieber aL, These semantic-head-driven algorithms are both geared towards the input semantic representation and the information contained in lexical entries. If the above sketched approach to semantic interpretation is followed systematically, it is possible show that such a semantic-head-driven genalgorithm terminates (Dymetman al., 1990). In van Noord (1991) I define a head-driven parser (based on Kay (1989)) for a class of constraint-based grammars in which the construction of strings may use more complex operations that simple context-free concatenation. Again, this algorithm is geared towards the input (string) and the information found in lexical entries. In this paper I investigate an approach where the construction of strings is defined lexically. Grammar rules simply percolate strings upwards. Such an approach seems feasible if we allow for powerful constraints to be defined. The head-corner parser knows about strings and performs operations on them; in the types of grammars defined here these operations are replaced by general constraint-solving techniques (Holifeld and Smolka, 1988; Tuda et a/., 1989; al., Therefore, it becomes possible to view both the head-driven generator and the head-driven parser as one and the same algorithm. For this uniform algorithm to terminate, we generalize the constraint proposed by Dymetman et al. (1990) to both semantic interpretations and strings. That is, for each lexical entry we require that its string and its semantics is larger than the string and the semantics associated with each of its arguments. The following picture then emerges. The depth of a derivation tree is determined by the subcat list of the ultimate head of the tree. Furthermore, the string and the semantic representation of each of the non heads in the derivation tree is determined by the subcat list as well. A specific condition on the relation elements in the subcat list and their se- 12 mantics and string representation ensures termination. This condition on lexical entries can be seen as a lexicalized and computationally motivated version of GB&apos;s projection principle. Word-order domains. The string associated a linguistic object is in terms its domain 1989; Reape, 1990a). I take a word-order domain as a sequence of signs. Each of these signs is associated with a word-order domain recursively, or with a sequence of words. A word-order domain is thus a tree. Linear precedence rules are defined that constrain possible orderings of signs in such a word-order domain. Surface strings are a direct function of word-order domains. In the lexicon, the wordorder domain of a lexical entry is defined by sharing parts of this domain with the arguments it subcategorizes for. Word-order domains are percolated upward. Hence word-order domains are constructed in a derivation by gradual instantiations (hence strings are constructued in a derivation by gradual instantiation as well). Note that this implies that an unsaturated sign is not associated with one string, but merely with a set of strings (this is similar to the interpretation of unsaturated signs (Moore, 1989)). In lexical entries, word order domains are defined Reape&apos;s union (Reape, 1990a). Hence the grammars are not only based on context-free string concatenation. 2 Constraint-based versions of categorial grammar The formalism I assume consists of definite clauses over constraint languages in the manner of Hohfeld and Smolka (1988). The constraint language at least consists of the path equations known from PATR II (Shieber, 1989), augmented variables. I write such a definite clause P • • 4,1,4). p, atoms and 0 is a (conjunction of) constraint(s). The path equations are written as in PATR II, but each path starts with a variable: or 4,) variables, a constant, are attributes. I also use some more powerful constraints that are written as atoms. This formalism is used to define what possible &apos;signs&apos; are, by the definition of the unary predicate sign/1. There is only one nonunit clause for this predicate. The idea is that unit clauses for sign/1 are lexical entries, and the one nonunit clause defines the (binary) application rule. I assume that lexical entries are specified for their arguments in their `subcat list&apos; (sc). In the aprule a head selects the first its subcat list, and the tail the subcat list is the subcat list of the mother; the semantics strings (phon) are shared between the head and the mother. sign(X2), (Xo synsem sem) --L. (X1 synsem sem), (X0 phon) (Xi phon), synsem sc) synsem sc r), (X1 synsem sc (X2). I write such rules using matrix notation as folthe value string(X,Y). X0 : synsem : phon : The grammar also consists of a number of lexical entries. Each of these lexical entries is specified for its subcat list, and for each subcat element the semantics and word-order domain is specified, such that they satisfy a termination condition to be defined in the following section. For example, this condition is satisfied if the semantics of each element in the subcat list is a proper subpart of the semantics of the entry, and each element of the subcat list is a proper subpart of the word-order domain of the entry. The phonology of a sign is defined with respect to the word-order domain with the predicate &apos;string&apos;. This predicate simply defines a left-to-right depth-first traverse&apos; of a word-order domain and picks up all the strings at the terminals. It should be noted that the way strings are computed from the wordorder domains implies that the string of a node 13 syn : vp : Sc: () : fl sem : schla f en(D [synsem : : ( , dom: 0 phon: (schlii f 0 phon : string(E1) El Figure 1: The German verb `schlift&apos; not necessarily is the concatenation of the strings of its daughter nodes. In fact, the relation between the strings of nodes is defined indirectly via the word-order domains. The word-order domains are sequences of signs. One of these signs is the sign corresponding to the lexical entry itself. However, the domain of this sign is empty, but other values can be shared. Hence the entry for an intransitive German verb such as `schlift&apos; (sleeps) is defined as in figure 1. I introduce some syntactic sugaring to make entries readable. Firstly, will stand for synsem: in lexical entries the is with the of an element of the word order domain, that is furthermore specified for the empty domain and some string. I will &lt; &gt;&gt; in lexical entry to stand for sign whose is shared with the the lexical entry itself; its () and its is foregoing entry is abreviated as: : : f en(E,) : vp : (EIN &lt; schlii f t &gt;) phon : string( Note that in this entry we merely stipulate that the verb preceded by the subject constitutes the word-order domain of the entire phrase. However, we may also use more complex constraints to define word-order constraints. In particular, as already stated above, LP constraints are defined which holds for word-order domains. I use the sequence-union predicate (abbreviated su) defined by Reape as a possible constraint as well. This predicate is motivated by clause union and scrambling phenomena in German. A linguistically motivated example of the use of this constraint can be found in section 4. The predicate B, C) true in case the elements of the list the multi set union of the elements of the lists and a &lt; either A or a &lt; in C. also use the notation Y to denote value examsuaa, d, f],[a,b,c,d,e, f]); [a, clUo [b] for [a, b] ,[a , b , c] fact, I assume that this predicate is also used in the simple cases, in order to be able to spel out generalizations in the linear precedence constraints. Hence the entry for `schlafen&apos; is defined as follows, where write indicate that the lp constraints be satisfied for have nothing to say about the definition of these constraints. syn : vp synsem: [ sem : schlafen(D (&lt; ft &gt;) phon : string(Ip(1)) In the following I (implicitly) assume that for each lexical entry the following holds: phon : string(lp(1)) ] 3 Uniform Processing In van Noord (1991) I define a parsing strategy, called &apos;head-corner parsing&apos; for a class of : xp ]] sem : Sc: II 14 grammars allowing more complex constraints on strings than context-free concatenation. Reape defines generalizations of the shift-reduce parser and the CYK parser (Reape, 1990b), for the same class of grammars. For generation headdriven generators can be used (van Noord, 1989; et al., 1989; Shieber al., Alternatively I propose a generalization of these headdriven parsing— and generation algorithms. The generalized algorithm can be used both for parsing and generation. Hence we obtain a uniform algorithm for both processes. Shieber (1988) argues for a uniform architecture for parsing in generation. In his proposal, both processes are (different) instantiations of a parameterized algorithm. The algorithm I define is not parameterized in this sense, but really uses the same code in both directions. Some of the specific properties of the head-driven generator on the one hand, and the head-driven parser on the other hand, follow from general constraint-solving techniques. We thus obtain a uniform algorithm that is suitable for linguistic processing. This result should be compared with other uniform scheme&apos;s such as SLD-resolution or some implementations of type inference (Zajac, 1991, this volume) which clearly are also uniform but faces severe problems in the case of lexicalist grammars, as such scheme&apos;s do not take into account the specific nature of lexigrammars (Shieber al., algorithm is written in the same formalism as the grammar and thus constitutes a meta-interpreter. The definite clauses of the object-grammar are represented as for the unit clauses and M, A) for the rule sig n(M) sign(H), sign(A), (1). The associated interpreter is a Prolog like topdown backtrack interpreter where term unification is replaced by more general constraintsolving techniques, (Hafeld and Smolka, 1988; 1989; Damas al., The meta-interpreter defines a head-driven bottom-up strategy with top-down prediction (figure 2), and is a generalization of the head-driven generator Noord, 1989; Calder 1989; van Noord, 1990a) and the head-corner parser (Kay, 1989; van Noord, 1991). prove(T):— lexical_entry(L), connect(L,T), (T phon) = (L phon), (T synsem sem) = (L synsem sem). connect(T,T). connect(S,T):— rule(S, M, A), prove(A), connect(M,T). Figure 2: The uniform algorithm In the formalism defined in the preceding section there are two possible ways where nontermination may come in, in the constraints or in the definite relations over these constraints. In this paper I am only concerned with the second type of non-termination, that is, I simply assume that the constraint language is decidable (Hohfeld Smolka, 1988). 1For the grammar sketched in the foregoing section we can define a very natural condition on lexical entries that guarantees us termination of both parsing and generation, provided the constraint language we use is decidable. The basic idea is that for a given semantic representation or (string constraining a) word-order domain, the derivation tree that derives these representations has a finite depth. Lexical entries specified for (at least) phon and sem. constraint merely states that the values of these attributes are dependent. It is not possible for one value to &apos;grow&apos; unless the values of the other attributes grow as well. Therefore the constraint we propose can be compared with GB&apos;s projection principle if we regard each of the attributes to define a &apos;level of description&apos;. Termination can then be guaranteed because derivation trees are restricted in depth by the value of the sc attribute. In order to define a condition to guarantee terwe need to be specific about the inter- &apos;This is the case if we only have PATR equations; but not if we use uo, unlimited. 15 pretation of a lexical entry. Following Shieber (1989) I assume that the interpretation of a set of path equations is defined in terms of directed graphs; the interpretation of a lexical entry is a set of such graphs. The &apos;size&apos; of a graph simply is defined as the number of nodes the graph consists of. We require that for each graph in the interpretation of a lexical entry, the size of the subgraph strictly larger than each of the sizes of of the (subgraphs corresponding to the) elements of the subcat list. I require that for each graph in the interpretation of a lexical the size of strictly larger than each of the sizes of (subgraphs corresponding to) the of the elements of the subcat lists. Summarizing, all lexical entries should satisfy the following condition: condition. each interpretaa lexical entry, if is element of L&apos;s list (i.e. synsern se E), size[(E phon)] &lt; size[(L synsem sem)] &lt; size[(L synsem The most straightforward way to satisfy this condition is for an element of a subcat list to share its semantics with a proper part of the semantics of the lexical entry, and to include the elements of the subcat list in its word-order domain. inputs. order to prove termination of the algorithm we need to make some assumptions about possible inputs. For a discussion cf. van Noord (1990b) and also Thompson (1991, this volume). The input to parsing and generation is specified as the goal 4restricts the variable We require that for each interpretation of X0 there a maximum for parsing of phon)], and that there is a maximum for generation of synsem sem)]. If the input has a maximum size for either semantics or phonology, then the uniform algorithm terminates (assuming the constraint language is decidable), because each recursive call to &apos;prove&apos; will necessarily be a &apos;smaller&apos; problem, and as the order on semantics and word-order domains is well-founded, there is a &apos;smallest&apos; problem. As a lexical entry specifies the length of its subcat list, there is only a finite number of embeddings of the &apos;connect&apos; clause possible. 4 Some examples raising. I show how Reape&apos;s analysis of Dutch and German verb raising constructions can be incorporated in the current grammar (Reape, 1989; Reape, 1990a). For a linguistic discussion of verb-raising constructions the reader is referred to Reape&apos;s papers. A verb raiser such as the German verb &apos;versprechen&apos; (promise) selects arguments, a object a subject word-order domain of the unioned the word order domain of is necessary because in German the arguments of embedded in fact occur left from the arguments of in: es i ihmi jemandk zu leseni versprocheni hatk (it him someone to read promised had i.e. someome had promised him to read it. Hence, the lexical entry for the raising verb &apos;versprechen&apos; is defined as in figure 3. The word-order domain of &apos;versprechen&apos; simply is the sequence union of the word-order domain of its vp object, the the subject, and itself. This allows any of the permuations (alby the LP constraints) of the subject, and the elements of domain of the (which may contain signs that have been unioned in recursively). prefixes. current framework offers an interesting account of seperable prefix verbs in German and Dutch. For an overview of alternative accounts of such verbs, see Uszkoreit (1987)[chapter 4]. At first sight, such verbs may seem problematic for the current approach because their prefixes seem not to have any semantic content. However, in my analysis a seperable prefix is lexically specified as part of the wordorder domain of the verb. Hence a particle is not identified as an element of the subcat list. Figure 4 might be the encoding of the German verb &apos;anrufen&apos; (call up). Note that this analysis conforms to the condition of the foregoing section, because the particle is not on the subcat list. The advantages of this analysis can be summarized as follows. Firstly, there is no need for a feature system to link verbs with the correct prefixes, as eg. in Uszkoreit&apos;s proposal. Instead, the correspondence is directly stated in the lexical entry of the particle verb which seems to me a very desirable 16 synsem : dorn : (&lt; versprechen &gt;) U0 : versprechen( IDE) synsem : [ :&amp;quot; : 11 6 Sc: (, II ,E (0). u, Figure 3: The German verb `versprechen&apos; result. Secondly, the analysis predicts that particles can &apos;move away&apos; from the verb in case the verb is sequence-unioned into a larger word-order domain. This prediction is correct. The clearest examples are possibly from Dutch. In Dutch, the particle of a verb can be placed (nearly) anywhere in the verb cluster, as long as it precedes its matrix verb: *dat jan marie piet heeft willen zien bellen op dat jan marie piet heeft willen zien op bellen dat jan marie piet heeft willen op zien bellen dat jan marie piet heeft op willen zien bellen dat jan marie piet op heeft willen zien bellen that john mary pete up has want see call (i.e. john wanted to see mary call up pete) The fact that the particle is not allowed to follow its head word is easily explained by the (independently motivated) LP constraint that arguments of a verb precede the verb. Hence these curious facts follow immediately in our analysis (the analysis makes the same prediction for German, but because of the different order of German verbs, this prediction can not be tested). Thirdly, Uszkoreit argues that a theory of seperable prefixes should also account for the &apos;systematic orthographic insecurity felt by native speakers&apos; i.e. whether or not they should write the prefix and the verb as one word. The current approach can be seen as one such explanation: in the lexical entry for a seperable prefix verb the verb and prefix are already there, on the other hand each of the words is in a different part of the word-order domain. 5 HP S G Markers In newer versions of HPSG (Pollard and Sag, 1991) a special &apos;marker&apos; category is assumed for which our projection principle does not seem to work. For example, complementizers are analyzed as markers. They are not taken to be the head of a phrase, but merely &apos;mark&apos; a sentence for some features. On the other hand, a special principle is assumed such that markers do in fact select for certain type of constituents. In the present framework a simple approach would be to analyze such markers as functors, i.e. heads, that have one element in their subcat list: : synsem : sem : 2 V P_FINO : (&lt; dass &gt; However, the termination condition defined in the third section can not always be satisfied because these markers usually do not have much semantic content (as in the preceding example). Furthermore these markers may also be phonetically empty, for example in the HPSG-2 analysis of infinite vp&apos;s that occur independently such an empty marker is assumed. Such an entry would look presumably as follows, where it is assumed that the empty marker constitutes no element of its own domain: d o m : (E) It seems, then, that analyses that rely on such marker categories can not be defined in the current framework. On the other hand, however, such markers have a very restricted distribution, are never recursive. Therefore, a slight modsyn : vp _in f _sub sem : 1 Sc: ( [synsem :[ 17 synsem: syn : vp sem : anru f en(ril) Sc: Figure 4: The verb `anrufen&apos; : syn : part : (&lt;ruft &gt;) (E) ( : ification of the termination condition can be defined that take into account such marker categories. To make this feasible we need a constraint that markers can not apply arbitrarily. In HPSG-</abstract>
<intro confidence="0.686366">2 the distribution of the English complementizer</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Calder</author>
<author>Mike Reape</author>
<author>Henk Zeevat</author>
</authors>
<title>An algorithm for generation in unification categorial grammar.</title>
<date>1989</date>
<booktitle>In Fourth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>233--240</pages>
<location>Manchester,</location>
<contexts>
<context position="1379" citStr="Calder et al., 1989" startWordPosition="201" endWordPosition="204">he lexical entries. These lexical entries specify their semantic interpretation, taking into account the semantics of the arguments they subcategorize for (specified in their subcat list). The grammar rules simply percolate the semantics upwards; by the selection of the arguments, this semantic formula then gets further instantiated (Moore, 1989). Hence in such approaches it can be said that all semantic formulas are &apos;projected from the lexicon&apos; (Zeevat et al., 1987). Such an organization of a grammar is the starting point of a class of generation algorithms that have become popular recently (Calder et al., 1989; Shieber et al., 1989; Shieber et aL, 1990). These semantic-head-driven algorithms are both geared towards the input semantic representation and the information contained in lexical entries. If the above sketched approach to semantic interpretation is followed systematically, it is possible to show that such a semantic-head-driven generation algorithm terminates (Dymetman et al., 1990). In van Noord (1991) I define a head-driven parser (based on Kay (1989)) for a class of constraint-based grammars in which the construction of strings may use more complex operations that simple context-free co</context>
<context position="10533" citStr="Calder et al., 1989" startWordPosition="1798" endWordPosition="1801">n(D Sc: (EN Pi) odorn : (0) Uo (&lt; schlii ft &gt;) phon : string(Ip(1)) In the following I (implicitly) assume that for each lexical entry the following holds: phon : string(lp(1)) ] 3 Uniform Processing In van Noord (1991) I define a parsing strategy, called &apos;head-corner parsing&apos; for a class of [syn : xp ]] sem : Sc: II 14 grammars allowing more complex constraints on strings than context-free concatenation. Reape defines generalizations of the shift-reduce parser and the CYK parser (Reape, 1990b), for the same class of grammars. For generation headdriven generators can be used (van Noord, 1989; Calder et al., 1989; Shieber et al., 1990). Alternatively I propose a generalization of these headdriven parsing— and generation algorithms. The generalized algorithm can be used both for parsing and generation. Hence we obtain a uniform algorithm for both processes. Shieber (1988) argues for a uniform architecture for parsing in generation. In his proposal, both processes are (different) instantiations of a parameterized algorithm. The algorithm I define is not parameterized in this sense, but really uses the same code in both directions. Some of the specific properties of the head-driven generator on the one h</context>
<context position="12376" citStr="Calder et al., 1989" startWordPosition="2085" endWordPosition="2088">tes a meta-interpreter. The definite clauses of the object-grammar are represented as lexical_entry(X):— (1). for the unit clauses sign(X):-4). and rule(H, M, A) :— for the rule sig n(M) sign(H), sign(A), (1). The associated interpreter is a Prolog like topdown backtrack interpreter where term unification is replaced by more general constraintsolving techniques, (Hafeld and Smolka, 1988; Tuda et aL, 1989; Damas ei al., 1991). The meta-interpreter defines a head-driven bottom-up strategy with top-down prediction (figure 2), and is a generalization of the head-driven generator (van Noord, 1989; Calder et al., 1989; van Noord, 1990a) and the head-corner parser (Kay, 1989; van Noord, 1991). prove(T):— lexical_entry(L), connect(L,T), (T phon) = (L phon), (T synsem sem) = (L synsem sem). connect(T,T). connect(S,T):— rule(S, M, A), prove(A), connect(M,T). Figure 2: The uniform algorithm In the formalism defined in the preceding section there are two possible ways where nontermination may come in, in the constraints or in the definite relations over these constraints. In this paper I am only concerned with the second type of non-termination, that is, I simply assume that the constraint language is decidable </context>
</contexts>
<marker>Calder, Reape, Zeevat, 1989</marker>
<rawString>Jonathan Calder, Mike Reape, and Henk Zeevat. An algorithm for generation in unification categorial grammar. In Fourth Conference of the European Chapter of the Association for Computational Linguistics, pages 233-240, Manchester, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Damas</author>
<author>Nelma Moreira</author>
<author>Giovanni B Varile</author>
</authors>
<title>The formal and processing models of CLG.</title>
<date>1991</date>
<booktitle>In Fifth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Berlin,</location>
<contexts>
<context position="2576" citStr="Damas et al., 1991" startWordPosition="386" endWordPosition="389">simple context-free concatenation. Again, this algorithm is geared towards the input (string) and the information found in lexical entries. In this paper I investigate an approach where the construction of strings is defined lexically. Grammar rules simply percolate strings upwards. Such an approach seems feasible if we allow for powerful constraints to be defined. The head-corner parser knows about strings and performs operations on them; in the types of grammars defined here these operations are replaced by general constraint-solving techniques (Holifeld and Smolka, 1988; Tuda et a/., 1989; Damas et al., 1991). Therefore, it becomes possible to view both the head-driven generator and the head-driven parser as one and the same algorithm. For this uniform algorithm to terminate, we generalize the constraint proposed by Dymetman et al. (1990) to both semantic interpretations and strings. That is, for each lexical entry we require that its string and its semantics is larger than the string and the semantics associated with each of its arguments. The following picture then emerges. The depth of a derivation tree is determined by the subcat list of the ultimate head of the tree. Furthermore, the string a</context>
</contexts>
<marker>Damas, Moreira, Varile, 1991</marker>
<rawString>Luis Damas, Nelma Moreira, and Giovanni B. Varile. The formal and processing models of CLG. In Fifth Conference of the European Chapter of the Association for Computational Linguistics, Berlin, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Dymetman</author>
<author>Pierre Isabelle</author>
<author>Francois Perrault</author>
</authors>
<title>A symmetrical approach to parsing and generation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Helsinki,</location>
<contexts>
<context position="1768" citStr="Dymetman et al., 1990" startWordPosition="256" endWordPosition="259">aid that all semantic formulas are &apos;projected from the lexicon&apos; (Zeevat et al., 1987). Such an organization of a grammar is the starting point of a class of generation algorithms that have become popular recently (Calder et al., 1989; Shieber et al., 1989; Shieber et aL, 1990). These semantic-head-driven algorithms are both geared towards the input semantic representation and the information contained in lexical entries. If the above sketched approach to semantic interpretation is followed systematically, it is possible to show that such a semantic-head-driven generation algorithm terminates (Dymetman et al., 1990). In van Noord (1991) I define a head-driven parser (based on Kay (1989)) for a class of constraint-based grammars in which the construction of strings may use more complex operations that simple context-free concatenation. Again, this algorithm is geared towards the input (string) and the information found in lexical entries. In this paper I investigate an approach where the construction of strings is defined lexically. Grammar rules simply percolate strings upwards. Such an approach seems feasible if we allow for powerful constraints to be defined. The head-corner parser knows about strings </context>
</contexts>
<marker>Dymetman, Isabelle, Perrault, 1990</marker>
<rawString>Marc Dymetman, Pierre Isabelle, and Francois Perrault. A symmetrical approach to parsing and generation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING), Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus HOHeld</author>
<author>Gert Smolka</author>
</authors>
<title>Definite relations over constraint languages.</title>
<date>1988</date>
<journal>Journal of Logic Programming.</journal>
<tech>Technical report,</tech>
<note>LILOG Report 53; to appear in</note>
<marker>HOHeld, Smolka, 1988</marker>
<rawString>Markus HOHeld and Gert Smolka. Definite relations over constraint languages. Technical report, 1988. LILOG Report 53; to appear in Journal of Logic Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Head driven parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of Workshop on Parsing Technologies,</booktitle>
<location>Pittsburgh,</location>
<contexts>
<context position="1840" citStr="Kay (1989)" startWordPosition="271" endWordPosition="272">). Such an organization of a grammar is the starting point of a class of generation algorithms that have become popular recently (Calder et al., 1989; Shieber et al., 1989; Shieber et aL, 1990). These semantic-head-driven algorithms are both geared towards the input semantic representation and the information contained in lexical entries. If the above sketched approach to semantic interpretation is followed systematically, it is possible to show that such a semantic-head-driven generation algorithm terminates (Dymetman et al., 1990). In van Noord (1991) I define a head-driven parser (based on Kay (1989)) for a class of constraint-based grammars in which the construction of strings may use more complex operations that simple context-free concatenation. Again, this algorithm is geared towards the input (string) and the information found in lexical entries. In this paper I investigate an approach where the construction of strings is defined lexically. Grammar rules simply percolate strings upwards. Such an approach seems feasible if we allow for powerful constraints to be defined. The head-corner parser knows about strings and performs operations on them; in the types of grammars defined here t</context>
<context position="12433" citStr="Kay, 1989" startWordPosition="2096" endWordPosition="2097">are represented as lexical_entry(X):— (1). for the unit clauses sign(X):-4). and rule(H, M, A) :— for the rule sig n(M) sign(H), sign(A), (1). The associated interpreter is a Prolog like topdown backtrack interpreter where term unification is replaced by more general constraintsolving techniques, (Hafeld and Smolka, 1988; Tuda et aL, 1989; Damas ei al., 1991). The meta-interpreter defines a head-driven bottom-up strategy with top-down prediction (figure 2), and is a generalization of the head-driven generator (van Noord, 1989; Calder et al., 1989; van Noord, 1990a) and the head-corner parser (Kay, 1989; van Noord, 1991). prove(T):— lexical_entry(L), connect(L,T), (T phon) = (L phon), (T synsem sem) = (L synsem sem). connect(T,T). connect(S,T):— rule(S, M, A), prove(A), connect(M,T). Figure 2: The uniform algorithm In the formalism defined in the preceding section there are two possible ways where nontermination may come in, in the constraints or in the definite relations over these constraints. In this paper I am only concerned with the second type of non-termination, that is, I simply assume that the constraint language is decidable (Hohfeld and Smolka, 1988). 1 For the grammar sketched in</context>
</contexts>
<marker>Kay, 1989</marker>
<rawString>Martin Kay. Head driven parsing. In Proceedings of Workshop on Parsing Technologies, Pittsburgh, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Unification-based semantic interpretation.</title>
<date>1989</date>
<booktitle>In 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Vancouver,</location>
<contexts>
<context position="1108" citStr="Moore, 1989" startWordPosition="156" endWordPosition="157">hm can be guaranteed if lexical entries adhere to a constraint, that can be seen as a computationally motivated version of GB&apos;s projection principle. 1 Motivations In constraint-based approaches to grammar the semantic intermtation of phrases is often defined in the lexical entries. These lexical entries specify their semantic interpretation, taking into account the semantics of the arguments they subcategorize for (specified in their subcat list). The grammar rules simply percolate the semantics upwards; by the selection of the arguments, this semantic formula then gets further instantiated (Moore, 1989). Hence in such approaches it can be said that all semantic formulas are &apos;projected from the lexicon&apos; (Zeevat et al., 1987). Such an organization of a grammar is the starting point of a class of generation algorithms that have become popular recently (Calder et al., 1989; Shieber et al., 1989; Shieber et aL, 1990). These semantic-head-driven algorithms are both geared towards the input semantic representation and the information contained in lexical entries. If the above sketched approach to semantic interpretation is followed systematically, it is possible to show that such a semantic-head-dr</context>
<context position="4632" citStr="Moore, 1989" startWordPosition="728" endWordPosition="729">ce strings are a direct function of word-order domains. In the lexicon, the wordorder domain of a lexical entry is defined by sharing parts of this domain with the arguments it subcategorizes for. Word-order domains are percolated upward. Hence word-order domains are constructed in a derivation by gradual instantiations (hence strings are constructued in a derivation by gradual instantiation as well). Note that this implies that an unsaturated sign is not associated with one string, but merely with a set of possible strings (this is similar to the semantic interpretation of unsaturated signs (Moore, 1989)). In lexical entries, word order domains are defined using Reape&apos;s sequence union operation (Reape, 1990a). Hence the grammars are not only based on context-free string concatenation. 2 Constraint-based versions of categorial grammar The formalism I assume consists of definite clauses over constraint languages in the manner of Hohfeld and Smolka (1988). The constraint language at least consists of the path equations known from PATR II (Shieber, 1989), augmented with variables. I write such a definite clause as: P • • 4,1,4). where p, qi are atoms and 0 is a (conjunction of) constraint(s). The</context>
</contexts>
<marker>Moore, 1989</marker>
<rawString>Robert C. Moore. Unification-based semantic interpretation. In 27th Annual Meeting of the Association for Computational Linguistics, Vancouver, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Information Based Syntax and Semantics, Volume 2. Center for the Study of Language and Information Stanford,</title>
<date>1991</date>
<note>to appear.</note>
<contexts>
<context position="20389" citStr="Pollard and Sag, 1991" startWordPosition="3483" endWordPosition="3486">or German, but because of the different order of German verbs, this prediction can not be tested). Thirdly, Uszkoreit argues that a theory of seperable prefixes should also account for the &apos;systematic orthographic insecurity felt by native speakers&apos; i.e. whether or not they should write the prefix and the verb as one word. The current approach can be seen as one such explanation: in the lexical entry for a seperable prefix verb the verb and prefix are already there, on the other hand each of the words is in a different part of the word-order domain. 5 HP S G Markers In newer versions of HPSG (Pollard and Sag, 1991) a special &apos;marker&apos; category is assumed for which our projection principle does not seem to work. For example, complementizers are analyzed as markers. They are not taken to be the head of a phrase, but merely &apos;mark&apos; a sentence for some features. On the other hand, a special principle is assumed such that markers do in fact select for certain type of constituents. In the present framework a simple approach would be to analyze such markers as functors, i.e. heads, that have one element in their subcat list: [syn : vp_sub synsem : sem : Sc: ( 2 V P_FINO dom : (&lt; dass &gt; ,D However, the terminatio</context>
</contexts>
<marker>Pollard, Sag, 1991</marker>
<rawString>Carl Pollard and Ivan Sag. Information Based Syntax and Semantics, Volume 2. Center for the Study of Language and Information Stanford, 1991. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mike</author>
<author>eape</author>
</authors>
<title>A logical treatment of semi-free word order and bounded discontinuous constituency.</title>
<date>1989</date>
<booktitle>In Fourth Conference of the European Chapter of the Association for Computational Linguistics, UMIST Manchester,</booktitle>
<marker>Mike, eape, 1989</marker>
<rawString>Mike R,eape. A logical treatment of semi-free word order and bounded discontinuous constituency. In Fourth Conference of the European Chapter of the Association for Computational Linguistics, UMIST Manchester, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mike</author>
<author>eape</author>
</authors>
<title>Getting things in order.</title>
<date>1990</date>
<booktitle>In Proceedings of the Symposium on Discontinuous Constituency, ITK</booktitle>
<location>Tilburg,</location>
<marker>Mike, eape, 1990</marker>
<rawString>Mike R,eape. Getting things in order. In Proceedings of the Symposium on Discontinuous Constituency, ITK Tilburg, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
</authors>
<title>Parsing bounded discontinous constituents: Generalisations of the shift-reduce and CKY algorithms,</title>
<date>1990</date>
<booktitle>Paper presented at the first CLIN meeting, October 26, OTS</booktitle>
<location>Utrecht.</location>
<contexts>
<context position="3715" citStr="Reape, 1990" startWordPosition="576" endWordPosition="577"> subcat list of the ultimate head of the tree. Furthermore, the string and the semantic representation of each of the non heads in the derivation tree is determined by the subcat list as well. A specific condition on the relation between elements in the subcat list and their se12 mantics and string representation ensures termination. This condition on lexical entries can be seen as a lexicalized and computationally motivated version of GB&apos;s projection principle. Word-order domains. The string associated with a linguistic object (sign) is defined in terms of its word-order domain (Reape, 1989; Reape, 1990a). I take a word-order domain as a sequence of signs. Each of these signs is associated with a word-order domain recursively, or with a sequence of words. A word-order domain is thus a tree. Linear precedence rules are defined that constrain possible orderings of signs in such a word-order domain. Surface strings are a direct function of word-order domains. In the lexicon, the wordorder domain of a lexical entry is defined by sharing parts of this domain with the arguments it subcategorizes for. Word-order domains are percolated upward. Hence word-order domains are constructed in a derivation</context>
<context position="10411" citStr="Reape, 1990" startWordPosition="1779" endWordPosition="1780">satisfied for X. I have nothing to say about the definition of these constraints. syn : vp synsem: [ sem : schlafen(D Sc: (EN Pi) odorn : (0) Uo (&lt; schlii ft &gt;) phon : string(Ip(1)) In the following I (implicitly) assume that for each lexical entry the following holds: phon : string(lp(1)) ] 3 Uniform Processing In van Noord (1991) I define a parsing strategy, called &apos;head-corner parsing&apos; for a class of [syn : xp ]] sem : Sc: II 14 grammars allowing more complex constraints on strings than context-free concatenation. Reape defines generalizations of the shift-reduce parser and the CYK parser (Reape, 1990b), for the same class of grammars. For generation headdriven generators can be used (van Noord, 1989; Calder et al., 1989; Shieber et al., 1990). Alternatively I propose a generalization of these headdriven parsing— and generation algorithms. The generalized algorithm can be used both for parsing and generation. Hence we obtain a uniform algorithm for both processes. Shieber (1988) argues for a uniform architecture for parsing in generation. In his proposal, both processes are (different) instantiations of a parameterized algorithm. The algorithm I define is not parameterized in this sense, b</context>
<context position="16544" citStr="Reape, 1990" startWordPosition="2801" endWordPosition="2802">r semantics or phonology, then the uniform algorithm terminates (assuming the constraint language is decidable), because each recursive call to &apos;prove&apos; will necessarily be a &apos;smaller&apos; problem, and as the order on semantics and word-order domains is well-founded, there is a &apos;smallest&apos; problem. As a lexical entry specifies the length of its subcat list, there is only a finite number of embeddings of the &apos;connect&apos; clause possible. 4 Some examples Verb raising. First I show how Reape&apos;s analysis of Dutch and German verb raising constructions can be incorporated in the current grammar (Reape, 1989; Reape, 1990a). For a linguistic discussion of verb-raising constructions the reader is referred to Reape&apos;s papers. A verb raiser such as the German verb &apos;versprechen&apos; (promise) selects three arguments, a vp, an object up and a subject up. The word-order domain of the vp is unioned into the word order domain of versprechen. This is necessary because in German the arguments of the embedded vp can in fact occur left from the other arguments of versprechen, as in: es i ihmi jemandk zu leseni versprocheni hatk (it him someone to read promised had i.e. someome had promised him to read it. Hence, the lexical en</context>
</contexts>
<marker>Reape, 1990</marker>
<rawString>Mike Reape. Parsing bounded discontinous constituents: Generalisations of the shift-reduce and CKY algorithms, 1990. Paper presented at the first CLIN meeting, October 26, OTS Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Gertjan van Noord</author>
<author>Robert C Moore</author>
<author>Fernando C N Pereira</author>
</authors>
<title>A semantic-head-driven generation algorithm for unification based formalisms.</title>
<date>1989</date>
<booktitle>In 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Vancouver,</location>
<marker>Shieber, van Noord, Moore, Pereira, 1989</marker>
<rawString>Stuart M. Shieber, Gertjan van Noord, Robert C. Moore, and Fernando C.N. Pereira. A semantic-head-driven generation algorithm for unification based formalisms. In 27th Annual Meeting of the Association for Computational Linguistics, Vancouver, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Gertjan van Noord</author>
<author>Robert C Moore</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Semantichead-driven generation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<marker>Shieber, van Noord, Moore, Pereira, 1990</marker>
<rawString>Stuart M. Shieber, Gertjan van Noord, Robert C. Moore, and Fernando C.N. Pereira. Semantichead-driven generation. Computational Linguistics, 16(1), 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>A uniform architecture for parsing and generation.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Budapest,</location>
<contexts>
<context position="10796" citStr="Shieber (1988)" startWordPosition="1841" endWordPosition="1842"> parsing&apos; for a class of [syn : xp ]] sem : Sc: II 14 grammars allowing more complex constraints on strings than context-free concatenation. Reape defines generalizations of the shift-reduce parser and the CYK parser (Reape, 1990b), for the same class of grammars. For generation headdriven generators can be used (van Noord, 1989; Calder et al., 1989; Shieber et al., 1990). Alternatively I propose a generalization of these headdriven parsing— and generation algorithms. The generalized algorithm can be used both for parsing and generation. Hence we obtain a uniform algorithm for both processes. Shieber (1988) argues for a uniform architecture for parsing in generation. In his proposal, both processes are (different) instantiations of a parameterized algorithm. The algorithm I define is not parameterized in this sense, but really uses the same code in both directions. Some of the specific properties of the head-driven generator on the one hand, and the head-driven parser on the other hand, follow from general constraint-solving techniques. We thus obtain a uniform algorithm that is suitable for linguistic processing. This result should be compared with other uniform scheme&apos;s such as SLD-resolution </context>
</contexts>
<marker>Shieber, 1988</marker>
<rawString>Stuart M. Shieber. A uniform architecture for parsing and generation. In Proceedings of the 12th International Conference on Computational Linguistics (COLING), Budapest, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Parsing and Type Inference for Natural and Computer Languages.</title>
<date>1989</date>
<tech>PhD thesis,</tech>
<institution>Menlo Park,</institution>
<note>Technical note 460.</note>
<contexts>
<context position="5087" citStr="Shieber, 1989" startWordPosition="797" endWordPosition="798">is not associated with one string, but merely with a set of possible strings (this is similar to the semantic interpretation of unsaturated signs (Moore, 1989)). In lexical entries, word order domains are defined using Reape&apos;s sequence union operation (Reape, 1990a). Hence the grammars are not only based on context-free string concatenation. 2 Constraint-based versions of categorial grammar The formalism I assume consists of definite clauses over constraint languages in the manner of Hohfeld and Smolka (1988). The constraint language at least consists of the path equations known from PATR II (Shieber, 1989), augmented with variables. I write such a definite clause as: P • • 4,1,4). where p, qi are atoms and 0 is a (conjunction of) constraint(s). The path equations are written as in PATR II, but each path starts with a variable: (Xi c or (X111 4,) Xi .1n) where Xk are variables, c is a constant, 1,1&apos; are attributes. I also use some more powerful constraints that are written as atoms. This formalism is used to define what possible &apos;signs&apos; are, by the definition of the unary predicate sign/1. There is only one nonunit clause for this predicate. The idea is that unit clauses for sign/1 are lexical e</context>
<context position="14186" citStr="Shieber (1989)" startWordPosition="2387" endWordPosition="2388"> for one value to &apos;grow&apos; unless the values of the other attributes grow as well. Therefore the constraint we propose can be compared with GB&apos;s projection principle if we regard each of the attributes to define a &apos;level of description&apos;. Termination can then be guaranteed because derivation trees are restricted in depth by the value of the sc attribute. In order to define a condition to guarantee termination we need to be specific about the inter&apos;This is the case if we only have PATR equations; but probably not if we use uo, string/2and /p/2 unlimited. 15 pretation of a lexical entry. Following Shieber (1989) I assume that the interpretation of a set of path equations is defined in terms of directed graphs; the interpretation of a lexical entry is a set of such graphs. The &apos;size&apos; of a graph simply is defined as the number of nodes the graph consists of. We require that for each graph in the interpretation of a lexical entry, the size of the subgraph at sem is strictly larger than each of the sizes of the sem part of the (subgraphs corresponding to the) elements of the subcat list. I require that for each graph in the interpretation of a lexical entry, the size of phon is strictly larger than each </context>
</contexts>
<marker>Shieber, 1989</marker>
<rawString>Stuart M. Shieber. Parsing and Type Inference for Natural and Computer Languages. PhD thesis, Menlo Park, 1989. Technical note 460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry S Thompson</author>
</authors>
<title>Generation and translation - towards a formalism-independent characterization.</title>
<date>1991</date>
<booktitle>In Proceedings of ACL workshop Reversible Grammar in Natural Language Processing,</booktitle>
<location>Berkeley,</location>
<contexts>
<context position="15601" citStr="Thompson (1991" startWordPosition="2642" endWordPosition="2643"> interpretation L of a lexical entry, if E is an element of L&apos;s subcat list (i.e. (L synsern se rt f) E), then: size[(E phon)] &lt; size[(L phon)] size[(E synsem sem)] &lt; size[(L synsem sem)] The most straightforward way to satisfy this condition is for an element of a subcat list to share its semantics with a proper part of the semantics of the lexical entry, and to include the elements of the subcat list in its word-order domain. Possible inputs. In order to prove termination of the algorithm we need to make some assumptions about possible inputs. For a discussion cf. van Noord (1990b) and also Thompson (1991, this volume). The input to parsing and generation is specified as the goal ?— sign(X0), where 4 restricts the variable Xo. We require that for each interpretation of X0 there is a maximum for parsing of size[(X0 phon)], and that there is a maximum for generation of size[(X0 synsem sem)]. If the input has a maximum size for either semantics or phonology, then the uniform algorithm terminates (assuming the constraint language is decidable), because each recursive call to &apos;prove&apos; will necessarily be a &apos;smaller&apos; problem, and as the order on semantics and word-order domains is well-founded, there</context>
</contexts>
<marker>Thompson, 1991</marker>
<rawString>Henry S. Thompson. Generation and translation - towards a formalism-independent characterization. In Proceedings of ACL workshop Reversible Grammar in Natural Language Processing, Berkeley, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirosi Tuda</author>
<author>Kôiti Hasida</author>
<author>Hidetosi Sirai</author>
</authors>
<title>JPSG parser on constraint logic programming.</title>
<date>1989</date>
<booktitle>In Fourth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Manchester,</location>
<marker>Tuda, Hasida, Sirai, 1989</marker>
<rawString>Hirosi Tuda, Kôiti Hasida, and Hidetosi Sirai. JPSG parser on constraint logic programming. In Fourth Conference of the European Chapter of the Association for Computational Linguistics, Manchester, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Uszkoreit</author>
</authors>
<title>Word Order and Constituent Structure in German.</title>
<date>1987</date>
<publisher>CSLI Stanford,</publisher>
<contexts>
<context position="17796" citStr="Uszkoreit (1987)" startWordPosition="3012" endWordPosition="3013">n&apos; is defined as in figure 3. The word-order domain of &apos;versprechen&apos; simply is the sequence union of the word-order domain of its vp object, with the up object, the subject, and versprechen itself. This allows any of the permuations (allowed by the LP constraints) of the up object, versprechen, the subject, and the elements of the domain of the vp object (which may contain signs that have been unioned in recursively). Seperable prefixes. The current framework offers an interesting account of seperable prefix verbs in German and Dutch. For an overview of alternative accounts of such verbs, see Uszkoreit (1987)[chapter 4]. At first sight, such verbs may seem problematic for the current approach because their prefixes seem not to have any semantic content. However, in my analysis a seperable prefix is lexically specified as part of the wordorder domain of the verb. Hence a particle is not identified as an element of the subcat list. Figure 4 might be the encoding of the German verb &apos;anrufen&apos; (call up). Note that this analysis conforms to the condition of the foregoing section, because the particle is not on the subcat list. The advantages of this analysis can be summarized as follows. Firstly, there </context>
</contexts>
<marker>Uszkoreit, 1987</marker>
<rawString>Hans Uszkoreit. Word Order and Constituent Structure in German. CSLI Stanford, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>BUG: A directed bottomup generator for unification based formalisms. Working Papers in Natural Language Processing, Katholieke Universiteit Leuven, Stichting Taaltechnologie Utrecht,</title>
<date>1989</date>
<volume>4</volume>
<marker>van Noord, 1989</marker>
<rawString>Gertjan van Noord. BUG: A directed bottomup generator for unification based formalisms. Working Papers in Natural Language Processing, Katholieke Universiteit Leuven, Stichting Taaltechnologie Utrecht, 4, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>An overview of headdriven bottom-up generation.</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation.</booktitle>
<editor>In Robert Dale, Chris Mellish, and Michael Zock, editors,</editor>
<publisher>Academic Press,</publisher>
<marker>van Noord, 1990</marker>
<rawString>Gertjan van Noord. An overview of headdriven bottom-up generation. In Robert Dale, Chris Mellish, and Michael Zock, editors, Current Research in Natural Language Generation. Academic Press, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Reversible unification-based machine translation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Helsinki,</location>
<marker>van Noord, 1990</marker>
<rawString>Gertjan van Noord. Reversible unification-based machine translation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING), Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Head corner parsing for discontinuous constituency.</title>
<date>1991</date>
<booktitle>In 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Berkeley,</location>
<marker>van Noord, 1991</marker>
<rawString>Gertjan van Noord. Head corner parsing for discontinuous constituency. In 29th Annual Meeting of the Association for Computational Linguistics, Berkeley, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RAmi Zajac</author>
</authors>
<title>A uniform architecture for parsing, generation and transfer.</title>
<date>1991</date>
<booktitle>In Proceedings of ACL workshop Reversible Grammar in Natural Language Processing,</booktitle>
<location>Berkeley,</location>
<contexts>
<context position="11450" citStr="Zajac, 1991" startWordPosition="1942" endWordPosition="1943">ing in generation. In his proposal, both processes are (different) instantiations of a parameterized algorithm. The algorithm I define is not parameterized in this sense, but really uses the same code in both directions. Some of the specific properties of the head-driven generator on the one hand, and the head-driven parser on the other hand, follow from general constraint-solving techniques. We thus obtain a uniform algorithm that is suitable for linguistic processing. This result should be compared with other uniform scheme&apos;s such as SLD-resolution or some implementations of type inference (Zajac, 1991, this volume) which clearly are also uniform but faces severe problems in the case of lexicalist grammars, as such scheme&apos;s do not take into account the specific nature of lexicalist grammars (Shieber et al., 1990). Algorithm. The algorithm is written in the same formalism as the grammar and thus constitutes a meta-interpreter. The definite clauses of the object-grammar are represented as lexical_entry(X):— (1). for the unit clauses sign(X):-4). and rule(H, M, A) :— for the rule sig n(M) sign(H), sign(A), (1). The associated interpreter is a Prolog like topdown backtrack interpreter where ter</context>
</contexts>
<marker>Zajac, 1991</marker>
<rawString>RAmi Zajac. A uniform architecture for parsing, generation and transfer. In Proceedings of ACL workshop Reversible Grammar in Natural Language Processing, Berkeley, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IIenk Zeevat</author>
<author>Ewan Klein</author>
<author>Jo Calder</author>
</authors>
<title>Unification categorial grammar.</title>
<date>1987</date>
<booktitle>of Working Papers in Cognitive Science.</booktitle>
<volume>1</volume>
<editor>In Nicholas Haddock, Ewan Klein, and Glyn Morrill, editors,</editor>
<institution>Categorial Grammar, Unification Grammar and Parsing. Centre for Cognitive Science, University of Edinburgh,</institution>
<contexts>
<context position="1231" citStr="Zeevat et al., 1987" startWordPosition="176" endWordPosition="179">sion of GB&apos;s projection principle. 1 Motivations In constraint-based approaches to grammar the semantic intermtation of phrases is often defined in the lexical entries. These lexical entries specify their semantic interpretation, taking into account the semantics of the arguments they subcategorize for (specified in their subcat list). The grammar rules simply percolate the semantics upwards; by the selection of the arguments, this semantic formula then gets further instantiated (Moore, 1989). Hence in such approaches it can be said that all semantic formulas are &apos;projected from the lexicon&apos; (Zeevat et al., 1987). Such an organization of a grammar is the starting point of a class of generation algorithms that have become popular recently (Calder et al., 1989; Shieber et al., 1989; Shieber et aL, 1990). These semantic-head-driven algorithms are both geared towards the input semantic representation and the information contained in lexical entries. If the above sketched approach to semantic interpretation is followed systematically, it is possible to show that such a semantic-head-driven generation algorithm terminates (Dymetman et al., 1990). In van Noord (1991) I define a head-driven parser (based on K</context>
</contexts>
<marker>Zeevat, Klein, Calder, 1987</marker>
<rawString>IIenk Zeevat, Ewan Klein, and Jo Calder. Unification categorial grammar. In Nicholas Haddock, Ewan Klein, and Glyn Morrill, editors, Categorial Grammar, Unification Grammar and Parsing. Centre for Cognitive Science, University of Edinburgh, 1987. Volume 1 of Working Papers in Cognitive Science.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>