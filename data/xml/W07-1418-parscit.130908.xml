<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003202">
<title confidence="0.970162">
Semantic and Logical Inference Model for Textual Entailment
</title>
<author confidence="0.997636">
Dan Roth
</author>
<affiliation confidence="0.998563">
University of Illinois
</affiliation>
<address confidence="0.772646">
at Urbana-Champaign
Urbana, IL 61801
</address>
<email confidence="0.998371">
danr@cs.uiuc.edu
</email>
<author confidence="0.995737">
Mark Sammons
</author>
<affiliation confidence="0.998443">
University of Illinois
</affiliation>
<address confidence="0.8255995">
at Urbana-Champaign
Urbana, IL 61801
</address>
<email confidence="0.999595">
mssammon@uiuc.edu
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999484">
We compare two approaches to the problem
of Textual Entailment: SLIM, a composi-
tional approach modeling the task based on
identifying relations in the entailment pair,
and BoLI, a lexical matching algorithm.
SLIM’s framework incorporates a range of
resources that solve local entailment prob-
lems. A search-based inference procedure
unifies these resources, permitting them to
interact flexibly. BoLI uses WordNet and
other lexical similarity resources to detect
correspondence between related words in
the Hypothesis and the Text. In this pa-
per we describe both systems in some detail
and evaluate their performance on the 3rd
PASCAL RTE Challenge. While the lex-
ical method outperforms the relation-based
approach, we argue that the relation-based
model offers better long-term prospects for
entailment recognition.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988825">
We compare two Textual Entailment recognition
systems applied to the 3rd PASCAL RTE challenge.
Both systems model the entailment task in terms
of determining whether the Hypothesis can be “ex-
plained” by the Text.
The first system, BoLI (Bag of Lexical Items)
uses WordNet and (optionally) other word similarity
resources to compare individual words in the Hy-
pothesis with the words in the Text.
The second system, the Semantic and Logical
Inference Model (SLIM) system, uses a relational
model, and follows the model-theory-based ap-
proach of (Braz et al., 2005).
SLIM uses a suite of resources to modify the orig-
inal entailment pair by augmenting or simplifying
either or both the Text and Hypothesis. Terms re-
lating to quantification, modality and negation are
detected and removed from the graphical represen-
tation of the entailment pair and resolved with an
entailment module that handles basic logic.
In this study we describe the BoLI and SLIM sys-
tems and evaluate their performance on the 3rd PAS-
CAL RTE Challenge corpora. We discuss some ex-
amples and possible improvements for each system.
</bodyText>
<sectionHeader confidence="0.9635085" genericHeader="method">
2 System Description: Bag of Lexical
Items (BoLI)
</sectionHeader>
<bodyText confidence="0.999962277777778">
The BoLI system compares each word in the text
with a word in the hypothesis. If a word is found in
the Text that entails a word in the Hypothesis, that
word is considered “explained”. If the percentage of
the Hypothesis that can be explained is above a cer-
tain threshold, the Text is considered to entail the
Hypothesis. This threshold is determined using a
training set (in this case, the Development corpus),
by determining the percentage match for each entail-
ment pair and selecting the threshold that results in
the highest overall accuracy.
BoLI uses an extended set of stopwords includ-
ing auxiliary verbs, articles, exclamations, and dis-
course markers in order to improve the distinction
between Text and Hypothesis. Negation and modal-
ity are not explicitly handled.
The BoLI system can be changed by varying
the comparison resources it uses. The available
</bodyText>
<page confidence="0.983408">
107
</page>
<bodyText confidence="0.960523789473684">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 107–112,
Prague, June 2007. c�2007 Association for Computational Linguistics
resources are: WordNet-derived (Fellbaum, 1998)
synonymy, meronymy, membership, and hyper-
nymy; a filtered version of Dekang Lin’s word sim-
ilarity list (Lin, 1998) (only the ten highest-scored
entries for each word); and a resource based on a
lexical comparison of WordNet glosses.
We tried three main versions; one that used the
four WordNet- derived resources ( ); a second
that adds to the first system the Dekang Lin resource
( ); and a third that added to the second sys-
tem the Gloss resource ( ). We ran them on
the Development corpus, and determined the thresh-
old that gave the highest overall score. We then
used the highest-scoring version and the correspond-
ing threshold to determine labels for the Test cor-
pus. The results and thresholds for each variation
are given in table 1.
</bodyText>
<sectionHeader confidence="0.997598" genericHeader="method">
3 System Description: Semantic and
Logical Inference Model (SLIM)
</sectionHeader>
<bodyText confidence="0.999849347826087">
The SLIM system approaches the problem of entail-
ment via relations: the goal is to recognize the rela-
tions in the Text and Hypothesis, and use these to de-
termine whether the Text entails the Hypothesis. A
word in the Hypothesis is considered “covered” by
a relation if it appears in that relation in some form
(either directly or via abstraction). For the Text to
entail the Hypothesis, sufficient relations in the Hy-
pothesis must be entailed by relations in the Text to
cover the underlying text.
The term “Relation” is used here to describe a
predicate-argument structure where the predicate is
represented by a verb (which may be inferred from a
nominalized form), and the arguments by strings of
text from the original sentence. These constituents
may be (partially) abstracted by replacing tokens
in some constituent with attributes attached to that
or a related constituent (for example, modal terms
may be dropped and represented with an attribute
attached to the appropriate predicate).
Relations may take other relations as arguments.
Examples include “before” and “after” (when both
arguments are events) and complement structures.
</bodyText>
<subsectionHeader confidence="0.99492">
3.1 Representation
</subsectionHeader>
<bodyText confidence="0.977204789473684">
The system compares the Text to the Hypothesis us-
ing a “blackboard” representation of the two text
fragments (see figure 1). Different types of anno-
tation are specified on different layers, all of which
are “visible” to the comparison algorithm. All lay-
ers map to the original representation of the text, and
each annotated constituent corresponds to some ini-
tial subset of this original representation. This al-
lows multiple representations of the same surface
form to be entertained.
Figure 1 shows some of the layers in this data
structure for a simple entailment pair: the origi-
nal text in the WORD layer; the relations induced
from this text in the PREDICATE layer; and for
the Text, a Coreference constituent aligned with the
word “he” in the COREFERENCE layer. Note that
the argument labels for “give” in the Text indicate
that “he” is the theme/indirect object of the predi-
cate “give”.
</bodyText>
<figureCaption confidence="0.943458">
Figure 1: “Blackboard” Representation of Entail-
ment Pairs in SLIM
</figureCaption>
<bodyText confidence="0.999981733333334">
At compare-time, the coref constituent “The Pres-
ident” will be considered as a substitute for “he”
when comparing the relation in the Hypothesis with
the second relation in the Text. (The dashed lines
indicate that the coverage of the coreference con-
stituent is just that of the argument consisting of the
word “he”.) The relation comparator has access to
a list of rules mapping between verbs and their ar-
gument types; this will allow it to recognize that the
relation “give” can entail “receive”, subject to the
constraint that the agent of “give” must be the patient
of “receive”, and vice versa. This, together with the
coreference constituent in the Text that aligns with
the argument “he”, will allow the system to recog-
nize that the Text entails the Hypothesis.
</bodyText>
<figure confidence="0.99848576">
WORD
The President was happy that he was given the award .
COREF
The President
TEXT
ARG1
PRED
ARG2
he
ARG2
give
PRED
the award
ARG1
PREDICATE
happy that he was given the award
The President be
HYPOTHESIS
The President received an award .
The President receive an award
ARG0
PRED
ARG1
WORD
PREDICATE
</figure>
<page confidence="0.957704">
108
</page>
<subsectionHeader confidence="0.995296">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.99809">
The SLIM entailment system applies sequences of
transformations to the original entailment pair in or-
der to modify one or both members of the pair to
make it easier to determine whether the Text entails
the Hypothesis. The resources that make these trans-
formations are referred to here as “operators”. Each
operator is required to use Purposeful Inference: be-
fore making a change to either entailment pair mem-
ber, they must take the other member into account.
For example, the conjunction expander will generate
only those expansions in a text fragment that match
structures in the paired text fragment more closely.
This constrains the number of transformations con-
sidered and can reduce the amount of noise intro-
duced by these operators.
Each such operator serves one of three purposes:
</bodyText>
<listItem confidence="0.890285636363636">
1. ANNOTATE. Make some implicit property of
the meaning of the sentence explicit.
2. SIMPLIFY/TRANSFORM. Remove or alter
some section of the Text in order to improve
annotation accuracy or make it more similar to
the Hypothesis.
3. COMPARE. Compare (some elements of) the
two members of the entailment pair and as-
sign a score that correlates to how successfully
(those elements of) the Hypothesis can be sub-
sumed by the Text.
</listItem>
<bodyText confidence="0.999980277777778">
The system’s operators are applied to an entail-
ment pair, potentially generating a number of new
versions of that entailment pair. They may then be
applied to these new versions. It is likely that only
a subset of the operators will fire. It is also possible
that multiple operators may affect overlapping sec-
tions of one or both members of the entailment pair,
and so the resulting perturbations of the original pair
may be sensitive to the order of application.
To explore these different subsets/orderings, the
system is implemented as a search process over the
different operators. The search terminates as soon
as a satisfactory entailment score is returned by the
comparison operator for a state reached by applying
transformation operators, or after some limit to the
depth of the search is reached. If entailment is de-
termined to hold, the set of operations that generated
the terminal state constitutes a proof of the solution.
</bodyText>
<subsectionHeader confidence="0.97085">
3.2.1 Constraining the Search
</subsectionHeader>
<bodyText confidence="0.9999901">
To control the search to allow for the interdepen-
dence of certain operators, each operator may spec-
ify a set of pre- and post-conditions. Pre-conditions
specify which operators must have fired to provide
the necessary input for the current operator. Post-
conditions typically indicate whether or not it is de-
sirable to re-annotate the resulting entailment pair
(e.g. after an operation that appends a new relation
to an entailment pair member), or whether the Com-
parator should be called to check for entailment.
</bodyText>
<subsectionHeader confidence="0.998527">
3.3 System Resources: Annotation
</subsectionHeader>
<bodyText confidence="0.995994085714286">
The SLIM system uses a number of standard an-
notation resources – Part-of-Speech, Shallow- and
Full syntactic parsing, Named Entity tagging, and
Semantic Role Labelling – but also has a number
of more specialized resources intended to recognize
implicit predicates from the surface representation
in the text, and append these relations to the original
text. These resources are listed below with a brief
description of each.
Apposition Detector. Uses full parse information
to detect appositive constructions, adding a relation
that makes the underlying meaning explicit. It uses
a set of rules specifying subtree structure and phrase
labels.
Complex Noun Phrase Relation Detector. An-
alyzes long noun phrases and annotates them with
their implicit relations. It applies a few general
rules expressed at the shallow parse and named en-
tity level.
Modality and Negation Annotator. Abstracts
modifiers of relations representing modality or nega-
tion into attributes attached to the relation.
Discourse Structure Annotator. Scans the rela-
tion structure (presently only at the sentence level)
to determine negation and modality of relations em-
bedded in factive and other constructions. It marks
the embedded relations accordingly, and where pos-
sible, discards the embedding relation.
Coreference Annotator. Uses Named Entity
information to map pronouns to possible replace-
ments.
Nominalization Rewriter. Detects certain com-
mon nominalized verb structures and makes the re-
lation explicit. The present version applies a small
set of very general rules instantiated with a list of
</bodyText>
<page confidence="0.99836">
109
</page>
<bodyText confidence="0.99393775">
embedding verbs and a mapping from nominalized
to verbal forms.
threshold that give the best performance on the de-
velopment set.
</bodyText>
<subsectionHeader confidence="0.964619">
3.4 System Resources:
</subsectionHeader>
<bodyText confidence="0.973156166666667">
Simplification/Transformation
The simplification resources all demonstrate pur-
poseful inference, as described in section 3.2.
Idiom Catcher. Identifies and replaces sequences
of words corresponding to a list of known idioms,
simplifying sentence structure. It can recognize a
range of surface representations for each idiom.
Phrasal Verb Replacer. Checks for phrasal verb
constructions, including those where the particle is
distant from the main verb, replacing them with sin-
gle verbs of equivalent meaning.
Conjunction Expander. Uses full parse informa-
tion to detect and rewrite conjunctive argument and
predicate structures by expanding them.
Multi-Word Expression Contractor. Scans both
members of the entailment pair for compound noun
phrases that can be replaced by just the head of the
phrase.
</bodyText>
<subsectionHeader confidence="0.979805">
3.5 System Resources: Main Comparator
</subsectionHeader>
<bodyText confidence="0.99994308">
All comparator resources are combined in a single
operator for simplicity. This comparator uses the
blackboard architecture described in 3.1.
The main comparator compares each relation in
the Hypothesis to each relation in the Text, return-
ing “True” if sufficient relations in the Hypothesis
are entailed by relations in the Text to cover the un-
derlying representation of the Hypothesis.
For a relation in the Text to entail a relation in the
Hypothesis, the Text predicate must entail the Hy-
pothesis predicate, and all arguments of the Hypoth-
esis relation must be entailed by arguments of the
Text relation. This entailment check also accounts
for attributes such as negation and modality.
As part of this process, a set of rules that map be-
tween predicate- argument structures (some hand-
written, most derived from VerbNet) are applied
on-the-fly to the pair of relations being compared.
These rules specify a mapping between predicates
and a set of constraints that apply to the mappings
between arguments of the predicates. For example,
the agent of the relation “sell” should be the theme
of the relation “buy”, and vice versa.
When comparing the arguments of predicates, the
system uses BoLI with the same configuration and
</bodyText>
<subsectionHeader confidence="0.996936">
3.6 Comparison to Similar Approaches
</subsectionHeader>
<bodyText confidence="0.9999777">
Like (de Marneffe et al., 2005), SLIM’s represen-
tation abstracts away terms relating to negation,
modality and quantification. However, it uses them
as part of the comparison process, not as features
to be used in a classifier. In contrast to (Braz
et al., 2005), SLIM considers versions of the en-
tailment pair with and without simplifications of-
fered by preprocessing modules, rather than reason-
ing only about the simplified version; and rather
than formulating the subsumption (entailment) prob-
lem as a hierarchical linear program or classification
problem, SLIM defers local entailment decisions to
its modules and returns a positive label for a con-
stituent only if these resources return a positive la-
bel for all subconstituents. Finally, SLIM returns an
overall positive label if all words in the Hypothesis
can be ’explained’ by relations detected in the Hy-
pothesis and matched in the Text, rather than requir-
ing all detected relations in the Text to be entailed
by relations in the Hypothesis.
</bodyText>
<sectionHeader confidence="0.992092" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999822772727273">
Table 3 presents the peformance of the BoLI and
SLIM systems on the 3rd PASCAL RTE Challenge.
The version of SLIM used for the Development cor-
pus was incomplete, as several modules (Multi-word
Expression, Conjunction, and Apposition) were still
being completed at that time. Table 1 indicates the
performance of different versions of the BoLI sys-
tem on the Development corpus as described in sec-
tion 2.
To investigate the improvement of performance
for the SLIM system relative to the available re-
sources, we conducted a limited ablation study. Ta-
ble 2 shows the performance for different ver-
sions of the SLIM system on 100 entailment pairs
each from the IE and QA subtasks of the Test cor-
pus. The “full” (f) system includes all available re-
sources. The “intermediate” (i) system excludes the
resources we consider most likely to introduce er-
rors, the Multiword Expression module and the most
general Nominalization rewrite rules in the Nom-
inalization Rewriter. The “strict” (s) system also
omits the Apposition and Complex Noun Phrase
</bodyText>
<page confidence="0.999247">
110
</page>
<tableCaption confidence="0.9772585">
Table 1: Accuracy and corresponding threshold for
versions of BoLI on the Development corpus.
</tableCaption>
<table confidence="0.99908675">
TASK Accuracy Threshold
0.675 0.667
0.650 0.833
0.655 0.833
</table>
<tableCaption confidence="0.988473">
Table 2: Results for different versions of SLIM on
subsets of the Test and Develoment corpora.
</tableCaption>
<table confidence="0.9989172">
System SLIM s SLIM i SLIM f
Dev IE - - 0.650
Dev QA - - 0.660
Test IE 0.480 0.480 0.470
Test QA 0.680 0.710 0.710
</table>
<bodyText confidence="0.9997636">
modules. To give a sense of how well the complete
SLIM system does on the Development corpus, the
results for the full SLIM system on equal-sized sub-
sets of the IE and QA subtasks of the Development
corpus are also shown.
</bodyText>
<sectionHeader confidence="0.99806" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9999057">
From Table 3, it is clear that BoLI outperforms
SLIM in every subtask.
The ablation study in Table 2 shows that adding
new resources to SLIM has mixed benefits; from
the samples we used for evaluation, the intermediate
system would be the best balance between module
coverage and module accuracy.
In the rest of this section, we analyze the re-
sults and each system’s behavior on several exam-
ples from the corpus.
</bodyText>
<subsectionHeader confidence="0.989515">
5.1 BoLI
</subsectionHeader>
<bodyText confidence="0.984784045454545">
There is a significant drop in performance of the
BoLI from the Development corpus to the Test cor-
pus, indicating that the threshold somewhat overfit-
ted to the data used to train it. The performance drop
when adding the gloss and Dekang Lin word simi-
larity resources is not necessarily surprising, as these
resources are clearly noisy, and so may increase sim-
ilarity based on inappropriate word pairs.
In the following example, the word similarity is
high, but the structure of the two text fragments
gives the relevant words different overall meaning
(here, that one subset of the matched words does not
apply to the other):
id=26 Text: Born in Kingston-upon-Thames, Surrey, Brock-
well played his county cricket for the very strong Surrey side of
the last years of the 19th century.
Hypothesis: Brockwell was born in the last years of the 19th
century.
From this example it is clear that in addition to
the role of noise from these additional resources, the
structure of text plays a major role in meaning, and
this is exactly what BoLI cannot capture.
</bodyText>
<subsectionHeader confidence="0.962477">
5.2 SLIM
</subsectionHeader>
<bodyText confidence="0.915884228571428">
The ablation study for the SLIM system shows a
trade-off between precision and recall for some re-
sources. In this instance, adding resources improves
performance significantly, but including noisy re-
sources also implies a ceiling on overall perfor-
mance will ultimately be reached.
The following example shows the potentially
noisy possessive rewrite operator permitting suc-
cessful entailment:
id=19 Text: During Reinsdorf’s 24 seasons as chairman of
the White Sox, the team has captured Americal League divi-
sion championships three times, including an AL Central title
in 2000.
Transformed Text: During Reinsdorf have 24 seasons as chair-
man of the White Sox ...
Hypothesis: Reinsdorf was chairman of the White Sox for 24
seasons.
There are a number of examples where relaxed
operators result in false positives, but where the neg-
ative label is debatable. In the next example, the ap-
position module adds a new relation and the Nomi-
nalization Rewriter detects the hypothesis using this
new relation:
id=102 Hypothesis: He was initially successful, negotiating
a 3/4 of 1 percent royalty on all cars sold by the Association of
Licensed Automobile Manufacturers, the ALAM.
Transformed Text: ... Association of Licensed Automobile
Manufacturers is the ALAM.
Hypothesis: The ALAM manufactured cars.
Finally, some modules did not fire as they should;
for example 15, the conjunction module did not ex-
pand the conjunction over predicates. For example
24, the nominalization rewriter did not detect “plays
in the NHL” from “is a NHL player”. In example 35,
the apposition module did not detect that “Harriet
</bodyText>
<page confidence="0.999205">
111
</page>
<tableCaption confidence="0.9980145">
Table 3: Results for SLIM and BoLI on the Pascal Development and Test Corpora. Results marked with an
asterisk indicate not all system resources were available at the time the system was run.
</tableCaption>
<table confidence="0.9965995">
Corpus Development Test
Subtask IE IR QA SUM OVERALL IE IR QA SUM OVERALL
BoLI 0.560 0.700 0.790 0.690 0.675 0.510 0.710 0.830 0.575 0.656
SLIM 0.580* 0.595* 0.650* 0.545* 0.593* 0.485 0.6150 0.715 0.575 0.5975
</table>
<bodyText confidence="0.8585442">
Lane, niece of President James” could be rewritten.
Of course, there are also many examples where
the SLIM system simply does not have appropriate
resources (e.g. numerical reasoning, coreference re-
quiring semantic categorization).
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999961884615385">
While BoLI outperforms SLIM on the PASCAL
RTE 3 task, there is no clear way to improve BoLI.
It is clear that for the PASCAL corpora, the distribu-
tions over word similarity between entailment pair
members in positive and negative examples are dif-
ferent, allowing this simple approach to perform rel-
atively well, but there is no guarantee that this is gen-
erally the case, and it is easy to create an adversar-
ial corpus on which BoLI performs very badly (e.g.,
exchanging arguments or predicates of different cre-
lations in the Text), no matter how good the word-
level entailment resources are. This approach also
offers no possibility of a meaningful explanation of
the entailment decision.
SLIM, on the other hand, by offering a framework
to which new resources can be added in a principled
way, can be extended to cover new entailment phe-
nomena in an incremental, local (i.e. compositional)
way. The results of the limited ablation study sup-
port this conclusion, though the poor performance
on the IE task indicates the problems with using
lower-precision, higher-recall resources.
Overall, we find the results for the SLIM system
very encouraging, as they support the underlying
concept of incremental improvement, and this offers
a clear path toward better performance.
</bodyText>
<subsectionHeader confidence="0.935217">
6.1 Acknowledgements
</subsectionHeader>
<bodyText confidence="0.97405325">
We gratefully acknowledge the work on SLIM
modules by Ming-Wei Chang, Michael Connor,
Quang Do, Alex Klementiev, Lev Ratinov, and
Vivek Srikumar. This work was funded by
the Advanced Research and Development Activity
(ARDA)’s Advanced Question Answering for Intel-
ligence (AQUAINT) program, a grant from Boeing,
and a gift from Google.
</bodyText>
<sectionHeader confidence="0.997857" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999731">
Johan Bos and Katja Markert. 2005. When logical infer-
ence helps determining textual entailment (and when it
doesn’t). In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognizing Textual Entailment.
R. Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sam-
mons. 2005. Knowledge representation for seman-
tic entailment and question-answering. In IJCAI-05
Workshop on Knowledge and Reasoning for Question
Answering.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christopher
Manning. 2005. Learning to distinguish valid textual
entailments. In Proceedings of the Second PASCAL
Challenges Workshop on Recognizing Textual Entail-
ment.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Sophia Katrenko and Peter Adriaans. 2005. Using
maximal embedded syntactic subtrees for textual en-
tailment recognition. In Proceedings of the Second
PASCAL Challenges Workshop on Recognizing Tex-
tual Entailment.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proc. of the International Conference on
Machine Learning (ICML).
Marta Tatu, Brandon Iles, John Slavick, Adrian Novis-
chi, and Dan Moldovan. 2005. Cogex at the second
recognizing textual entailment challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognizing Textual Entailment.
</reference>
<page confidence="0.998287">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.498101">
<title confidence="0.999858">Semantic and Logical Inference Model for Textual Entailment</title>
<author confidence="0.996264">Dan</author>
<affiliation confidence="0.798145">University of at</affiliation>
<address confidence="0.858799">Urbana, IL</address>
<email confidence="0.99916">danr@cs.uiuc.edu</email>
<author confidence="0.996412">Mark</author>
<affiliation confidence="0.9956985">University of at Urbana-Champaign</affiliation>
<address confidence="0.966436">Urbana, IL 61801</address>
<email confidence="0.999585">mssammon@uiuc.edu</email>
<abstract confidence="0.997819238095238">We compare two approaches to the problem of Textual Entailment: SLIM, a compositional approach modeling the task based on identifying relations in the entailment pair, and BoLI, a lexical matching algorithm. SLIM’s framework incorporates a range of resources that solve local entailment problems. A search-based inference procedure unifies these resources, permitting them to interact flexibly. BoLI uses WordNet and other lexical similarity resources to detect correspondence between related words in the Hypothesis and the Text. In this paper we describe both systems in some detail and evaluate their performance on the 3rd PASCAL RTE Challenge. While the lexical method outperforms the relation-based approach, we argue that the relation-based model offers better long-term prospects for entailment recognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>When logical inference helps determining textual entailment (and when it doesn’t).</title>
<date>2005</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</booktitle>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. When logical inference helps determining textual entailment (and when it doesn’t). In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Braz</author>
<author>R Girju</author>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>M Sammons</author>
</authors>
<title>Knowledge representation for semantic entailment and question-answering.</title>
<date>2005</date>
<booktitle>In IJCAI-05 Workshop on Knowledge and Reasoning for Question Answering.</booktitle>
<contexts>
<context position="1638" citStr="Braz et al., 2005" startWordPosition="239" endWordPosition="242">erm prospects for entailment recognition. 1 Introduction We compare two Textual Entailment recognition systems applied to the 3rd PASCAL RTE challenge. Both systems model the entailment task in terms of determining whether the Hypothesis can be “explained” by the Text. The first system, BoLI (Bag of Lexical Items) uses WordNet and (optionally) other word similarity resources to compare individual words in the Hypothesis with the words in the Text. The second system, the Semantic and Logical Inference Model (SLIM) system, uses a relational model, and follows the model-theory-based approach of (Braz et al., 2005). SLIM uses a suite of resources to modify the original entailment pair by augmenting or simplifying either or both the Text and Hypothesis. Terms relating to quantification, modality and negation are detected and removed from the graphical representation of the entailment pair and resolved with an entailment module that handles basic logic. In this study we describe the BoLI and SLIM systems and evaluate their performance on the 3rd PASCAL RTE Challenge corpora. We discuss some examples and possible improvements for each system. 2 System Description: Bag of Lexical Items (BoLI) The BoLI syste</context>
<context position="14124" citStr="Braz et al., 2005" startWordPosition="2252" endWordPosition="2255"> mapping between predicates and a set of constraints that apply to the mappings between arguments of the predicates. For example, the agent of the relation “sell” should be the theme of the relation “buy”, and vice versa. When comparing the arguments of predicates, the system uses BoLI with the same configuration and 3.6 Comparison to Similar Approaches Like (de Marneffe et al., 2005), SLIM’s representation abstracts away terms relating to negation, modality and quantification. However, it uses them as part of the comparison process, not as features to be used in a classifier. In contrast to (Braz et al., 2005), SLIM considers versions of the entailment pair with and without simplifications offered by preprocessing modules, rather than reasoning only about the simplified version; and rather than formulating the subsumption (entailment) problem as a hierarchical linear program or classification problem, SLIM defers local entailment decisions to its modules and returns a positive label for a constituent only if these resources return a positive label for all subconstituents. Finally, SLIM returns an overall positive label if all words in the Hypothesis can be ’explained’ by relations detected in the H</context>
</contexts>
<marker>Braz, Girju, Punyakanok, Roth, Sammons, 2005</marker>
<rawString>R. Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sammons. 2005. Knowledge representation for semantic entailment and question-answering. In IJCAI-05 Workshop on Knowledge and Reasoning for Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Daniel Cer</author>
<author>Anna Rafferty</author>
<author>Christopher Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</booktitle>
<marker>de Marneffe, MacCartney, Grenager, Cer, Rafferty, Manning, 2005</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, Trond Grenager, Daniel Cer, Anna Rafferty, and Christopher Manning. 2005. Learning to distinguish valid textual entailments. In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3290" citStr="Fellbaum, 1998" startWordPosition="507" endWordPosition="508"> each entailment pair and selecting the threshold that results in the highest overall accuracy. BoLI uses an extended set of stopwords including auxiliary verbs, articles, exclamations, and discourse markers in order to improve the distinction between Text and Hypothesis. Negation and modality are not explicitly handled. The BoLI system can be changed by varying the comparison resources it uses. The available 107 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 107–112, Prague, June 2007. c�2007 Association for Computational Linguistics resources are: WordNet-derived (Fellbaum, 1998) synonymy, meronymy, membership, and hypernymy; a filtered version of Dekang Lin’s word similarity list (Lin, 1998) (only the ten highest-scored entries for each word); and a resource based on a lexical comparison of WordNet glosses. We tried three main versions; one that used the four WordNet- derived resources ( ); a second that adds to the first system the Dekang Lin resource ( ); and a third that added to the second system the Gloss resource ( ). We ran them on the Development corpus, and determined the threshold that gave the highest overall score. We then used the highest-scoring version</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sophia Katrenko</author>
<author>Peter Adriaans</author>
</authors>
<title>Using maximal embedded syntactic subtrees for textual entailment recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</booktitle>
<marker>Katrenko, Adriaans, 2005</marker>
<rawString>Sophia Katrenko and Peter Adriaans. 2005. Using maximal embedded syntactic subtrees for textual entailment recognition. In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="3405" citStr="Lin, 1998" startWordPosition="525" endWordPosition="526">t of stopwords including auxiliary verbs, articles, exclamations, and discourse markers in order to improve the distinction between Text and Hypothesis. Negation and modality are not explicitly handled. The BoLI system can be changed by varying the comparison resources it uses. The available 107 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 107–112, Prague, June 2007. c�2007 Association for Computational Linguistics resources are: WordNet-derived (Fellbaum, 1998) synonymy, meronymy, membership, and hypernymy; a filtered version of Dekang Lin’s word similarity list (Lin, 1998) (only the ten highest-scored entries for each word); and a resource based on a lexical comparison of WordNet glosses. We tried three main versions; one that used the four WordNet- derived resources ( ); a second that adds to the first system the Dekang Lin resource ( ); and a third that added to the second system the Gloss resource ( ). We ran them on the Development corpus, and determined the threshold that gave the highest overall score. We then used the highest-scoring version and the corresponding threshold to determine labels for the Test corpus. The results and thresholds for each varia</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An information-theoretic definition of similarity. In Proc. of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Brandon Iles</author>
<author>John Slavick</author>
<author>Adrian Novischi</author>
<author>Dan Moldovan</author>
</authors>
<title>Cogex at the second recognizing textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</booktitle>
<marker>Tatu, Iles, Slavick, Novischi, Moldovan, 2005</marker>
<rawString>Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi, and Dan Moldovan. 2005. Cogex at the second recognizing textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>