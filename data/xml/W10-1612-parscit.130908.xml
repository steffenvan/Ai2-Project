<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010225">
<title confidence="0.85019">
IRASubcat, a highly customizable, language independent tool for the
acquisition of verbal subcategorization information from corpus
</title>
<author confidence="0.521301">
Ivana Romina Altamirano and Laura Alonso i Alemany
</author>
<affiliation confidence="0.620956">
Grupo de Procesamiento de Lenguaje Natural
Secci´on de Ciencias de la Computaci´on
</affiliation>
<address confidence="0.706605">
Facultad de Matem´atica, Astronom´ıa y F´ısica
Universidad Nacional de C´ordoba
C´ordoba, Argentina
</address>
<email confidence="0.995909">
romina.altamirano@gmail.com,alemany@famaf.unc.edu.ar
</email>
<sectionHeader confidence="0.993808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999003">
IRASubcat is a language-independent tool to
acquire information about the subcategoriza-
tion of verbs from corpus. The tool can extract
information from corpora annotated at vari-
ous levels, including almost raw text, where
only verbs are identified. It can also ag-
gregate information from a pre-existing lex-
icon with verbal subcategorization informa-
tion. The system is highly customizable, and
works with XML as input and output format.
IRASubcat identifies patterns of constituents
in the corpus, and associates patterns with
verbs if their association strength is over a fre-
quency threshold and passes the likelihood ra-
tio hypothesis test. It also implements a proce-
dure to identify verbal constituents that could
be playing the role of an adjunct in a pattern.
Thresholds controlling frequency and identi-
fication of adjuncts can be customized by the
user, or else they are given a default value.
</bodyText>
<sectionHeader confidence="0.990813" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.9998355">
Characterizing the behavior of verbs as nuclear or-
ganizers of clauses (the so-called subcategorization
information) is crucial to obtain deep analyses of
natural language. For example, it can significantly
reduce structural ambiguities in parsing (Carroll et
al., 1999; Carroll and Fang, 2004), help in word
sense disambiguation or improve information ex-
traction (Surdeanu et al., 2003). However, the usual
construction of linguistic resources for verbal sub-
categorization involves many expert hours, and it is
usually prone to low coverage and inconsistencies
across human experts.
Corpora can be very useful to alleviate the prob-
lems of low coverage and inconsistencies. Verbs
can be characterized by their behavior in a big cor-
pus of the language. Thus, lexicographers only need
to validate, correct or complete this digested infor-
mation about the behavior of verbs. Moreover, the
starting information can have higher coverage and
be more unbiased than if it is manually constructed.
That’s why automatic acquisition of subcategoriza-
tion frames has been an active research area since
the mid-90s (Manning, 1993; Brent, 1993; Briscoe
and Carroll, 1997).
However, most of the approaches have been ad-
hoc for particular languages or particular settings,
like a determined corpus with a given kind of an-
notation, be it manual or automatic. To our knowl-
edge, there is no system to acquire subcategorization
information from corpora that is flexible enough to
work with different languages and levels of annota-
tion of corpora.
We present IRASubcat, a tool that acquires in-
formation about the behaviour of verbs from cor-
pora. It is aimed to address a variety of situations
and needs, ranging from rich annotated corpora to
virtually raw text (because the tags to study can be
selected in the configuration file). The characteri-
zation of linguistic patterns associated to verbs will
be correspondingly rich. The tool allows to cus-
tomize most of the aspects of its functioning, to
adapt to different requirements of the users. More-
over, IRASubcat is platform-independent and open
source, available for download at http://www.
irasubcat.com.ar.
IRASubcat input is a corpus (in xml format) with
examples of the verbs one wants to characterize, and
its output is a lexicon where each verb is associated
with the patterns of linguistic constituents that re-
flect its behavior in the given corpus, an approxima-
</bodyText>
<page confidence="0.977028">
84
</page>
<bodyText confidence="0.9735115">
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 84–91, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
tion to its subcategorization frame. Such association
is established when the verb and pattern co-occur in
corpus significantly enough to pass a frequency test
and a hypothesis test.
In the following section we discuss some previ-
ous work in the area of subcategorization acquisi-
tion from corpora. Then, Section 3 presents the
main functionality of the tool, and describe its us-
age. Section 4 details the parameters that can be cus-
tomized to adapt to different experimental settings.
In Section 5 we outline the functionality that iden-
tifies constituents that are likely to be adjuncts and
not arguments, and in Section 6 we describe the pro-
cedures to determine whether a given pattern is ac-
tually part of the subcategorization frame of a verb.
Section 7 presents some results of applying IRASub-
cat to two very different corpora. Finally, we present
some conclusions and the lines of future work.
</bodyText>
<sectionHeader confidence="0.99578" genericHeader="related work">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999851">
We review here some previous work related to ac-
quisition of subcategorization information from cor-
pora, focussing on the constraints of the approach
and corpora to learn with. We specially mention ap-
proaches for languages other than English.
The foundational work of (Brent, 1993) was based
on plain text (2.6 million words of the Wall Street
Journal (WSJ, 1994)). Since the corpus had no an-
notation, verbs were found by heuristics. He de-
tected six frame types and filtered associations be-
tween verbs and frames with the binomial hypothe-
sis test. This approach obtained 73.85% f-score in
an evaluation with human judges.
Also in 1993, (Ushioda et al., 1993) exploited also
the WSJ corpus but only the part that was annotated
with part-of-speech tags, with 600.000 words. He
studied also six frame types and did not distinguish-
ing arguments and adjuncts.
The same year, (Manning, 1993) used 4 million
words of the New York Times (Sandhaus, ), selected
only clauses with auxiliary verbs and automatically
analyzed them with a finite-state parser. He defined
19 frame types, and reported an f-score of 58.20%.
Various authors developed approaches assuming a
full syntactic analysis, which was usually annotated
manually in corpora (Briscoe and Carroll, 1997;
Kinyon and Prolo, 2002). Others associated syn-
tactic analyses to corpora with automatic parsers
(O’Donovan et al., 2005).
Various approaches were also found for languages
other than English. For German, (Eckle-Kohler,
1999) studied the behaviour of 6305 verbs on auto-
matically POS-tagged corpus data. He defined lin-
guistic heuristics by regular expression queries over
the usage of 244 frame types.
(Wauschkuhn, 1999) studied 1044 German verbs.
He extracted maximum of 2000 example sentences
for each verb from a corpus, and analyzed them
with partial (as opposed to full) syntactic analysis.
He found valency patterns, which were grouped in
order to extract the most frequent pattern combi-
nations, resulting in a verb-frame lexicon with 42
frame types.
(Schulte im Walde, 2000) worked with 18.7 mil-
lion words of German corpus, found 38 frame types.
She used the Duden das Stilw¨orterbuch(AG, 2001)
to evaluate results and reported f-score 57,24% with
PP and 62,30% without.
Many other approaches have been pursued for
various languages: (de Lima, 2002) for Portuguese,
(Georgala, 2003) for Greek, (Sarkar and Zeman,
2000) for Czech, (Spranger and Heid, 2003) for
Dutch, (Chesley and Salmon-Alt, 2006) for French
or (Chrupala, 2003) for Spanish, to name a few.
</bodyText>
<sectionHeader confidence="0.92135" genericHeader="method">
3 General description of the tool
</sectionHeader>
<bodyText confidence="0.999993333333334">
IRASubcat takes as input a corpus in XML format.
This corpus is expected to have some kind of anno-
tation associated to its elements, which will enrich
the description of the patterns associated to verbs.
The minimal required annotation is that verbs are
marked. If no other information is available, the
form of words will be used to build the patterns. If
the corpus has rich annotation for its elements, the
system can build the patterns with the value of at-
tributes or with a combination of them, and also with
combinations with lexical items. The only require-
ments are that verbs are marked, and that all linguis-
tic units to be considered to build the patterns are
siblings in the XML tree.
The output of IRASubcat is a lexicon, also in
XML format, where each of the verbs under inspec-
tion is associated to a set of subcategorization pat-
terns. A given pattern is associated to a given verb
</bodyText>
<page confidence="0.998655">
85
</page>
<bodyText confidence="0.999971611111111">
if the evidence found in the corpus passes certain
tests. Thresholds for these tests are defined by the
user, so that precision can be priorized over recall or
the other way round. In all cases, information about
the evidence found and the result of each test is pro-
vided, so that it can be easily assessed whether the
threshold for each test has the expected effects, and
it can be modified accordingly.
The lexicon also provides information about fre-
quencies of occurrence for verbs, patterns, and their
co-occurrences in corpus.
Moreover, IRASubcat is capable of integrating
the output lexicon with a pre-existing one, merg-
ing information about verbs and patterns with infor-
mation that had been previously extracted, possibly
from a different corpus or even from a hand-built
lexicon. The only requirement is that the lexicon is
in the same format as IRASubcat output lexicon.
</bodyText>
<sectionHeader confidence="0.98384" genericHeader="method">
4 A highly customizable tool
</sectionHeader>
<bodyText confidence="0.9999181">
IRASubcat has been designed to be adaptable in a
variety of settings. The user can set the conditions
for many aspects of the tool, in order to extract dif-
ferent kinds of information for different representa-
tional purposes or from corpora with different kinds
of annotation. For example, the system accepts a
wide range of levels of annotation in the input cor-
pus, and it is language independent. To guarantee
that any language can be dealt with, the corpus needs
to be codified in UTF-8 format, in which virtually
any existing natural language can be codified.
If the user does not know how to customize these
parameters, she can resort to the default values that
are automatically provided by the system for each of
them. The only information that needs to be speci-
fied in any case is the name of the tag marking verbs,
the name of the parent tag for the linguistic units that
characterize patterns and, of course, the input cor-
pus.
The parameters of the system are as follows:
</bodyText>
<listItem confidence="0.978217571428571">
• The user can provide a list of verbs to be de-
scribed, so that any other verb will not be con-
sidered. If no list is provided, all words marked
as verb in the corpus will be described.
• The scope of patterns can be specified as a win-
dow of n words around the words marked as
verbs, where n is a number specified by the
</listItem>
<bodyText confidence="0.950968">
user. It can also be specified that all elements
that are siblings of the verb in the XML tree are
considered, which is equivalent to considering
all elements in the scope of the clause, if that is
the parent node of the verb in an annotated cor-
pus. By default, a window of 3 sibling nodes at
each side of the verb is considered.
</bodyText>
<listItem confidence="0.986468736842105">
• It can be specified that patterns are completed
by a dummy symbol if the context of occur-
rence of the verb does not provide enough lin-
guistic elements to fill the specified window
length, for example, at the end of a sentence.
By default, no dummy symbol is used.
• It can be specified whether the order of occur-
rence of linguistic units should be taken into
account to characterize the pattern or not, de-
pending of the meaning of word order in the
language under study. By default, order is not
considered.
• We can provide a list of the attributes of lin-
guistic units that we want to study, for example,
syntactic function, morphological category, etc.
Attributes should be expressed as an XML at-
tribute of the unit. It can also be specified that
no attribute of the unit is considered, but only
its content, which is usually the surface form of
the unit. By default, an attribute named “sint”
will be considered.
• We can specify whether the content of linguis-
tic units will be considered to build patterns.
As in the previous case, the content is usually
the surface form of the unit (lexical form). By
default, content is not considered.
• A mark describing the position of the verb can
be introduced in patterns. By default it is not
considered, to be coherent with the default op-
tion of ignoring word order.
• It can be specified that, after identifying possi-
ble adjuncts, patterns with the same arguments
are collapsed into the same pattern, with all
their characterizing features (number of occur-
rences, etc.). By default, patterns are not col-
lapsed.
• The number of iterations that are carried out on
patterns to identify adjuncts can be customized,
</listItem>
<page confidence="0.873882">
86
</page>
<bodyText confidence="0.5922565">
by default it is not considered because by de-
fault patterns are not collapsed.
</bodyText>
<listItem confidence="0.997995692307692">
• The user can specify a minimal number of oc-
currences of a verb to be described. By default,
the minimal frequency is 0, so all verbs that oc-
cur in the corpus are described.
• A minimal number of occurrences of a pattern
can also be specified, with the default as 0.
• The user can specify whether the Log-
Likelihood Ratio hypothesis test will be ap-
plied to test whether the association between a
verb and a pattern cannot be considered a prod-
uct of chance. By defect, the test is used (and
the output will be 90, 95, 99 or 99.5 when the
co-ocurrence have that confiability) .
</listItem>
<sectionHeader confidence="0.92504" genericHeader="method">
5 Identification of adjuncts
</sectionHeader>
<bodyText confidence="0.999964740740741">
One of the most interesting capabilities of IRASub-
cat is the identification of possible adjuncts. Ad-
juncts are linguistic units that do not make part of
the core of a subcategorization pattern (Fillmore,
1968). They are optional constituents in the con-
stituent structure governed by a verb. Since they are
optional, we assume they can be recognized because
the same pattern can occur with or without them
without a significant difference. IRASubcat imple-
ments a procedure to identify these units by their op-
tionality, described in what follows. An example of
this procedure is shown in Figure 1.
First, all patterns of a verb are represented in a
trie. A trie is a tree-like structure where patterns are
represented as paths in the trie. In our case, the root
is empty and each node represents a constituent of a
pattern, so that a pattern is represented by concate-
nating all nodes that are crossed when following a
path from the root. Each node is associated with a
number expressing the number of occurrences of the
pattern that is constructed from the root to that node.
Constituents are ordered by frequency, so that more
frequent constituents are closer to the root.
In this structure, it is easy to identify constituents
that are optional, because they are topologically lo-
cated at the leaves of the trie and the number of oc-
currences of the optional node is much smaller than
the number of occurrences of its immediately pre-
ceding node.
We have experimented with different ratios be-
tween the frequency of the pattern with and without
the constituent to identify adjuncts. We have found
that adjuncts are usually characterized by occurring
in leaves of the trie at least for 80% of the patterns
of the verb.
Once a constituent is identified as an adjunct, it
is removed from all patterns that contain it within
the verb that is being characterized at the moment.
A new trie is built without the adjunct, and so new
adjuncts may be identified. This procedure can be it-
erated until no constituent is found to be optional, or
until a user-defined number of iterations is reached.
When an adjunct is removed, the original pat-
tern is preserved, so that the user can see whether
a given pattern occurred with constituents that have
been classified as adjuncts, and precisely which con-
stituents.
When this data structure is created, the sequential
ordering of constituents is lost, in case it had been
preserved in the starting patterns. If the mark sig-
nalling the position of the verb had been introduced,
it is also lost. However, order and position of the
verb can be recovered in the final patterns, after ad-
juncts have been identified.
</bodyText>
<sectionHeader confidence="0.947395" genericHeader="method">
6 Associating patterns to verbs
</sectionHeader>
<bodyText confidence="0.99991065">
One of the critical aspects of subcategorization ac-
quisition is the association of verbs and patterns.
How often must a pattern occur with a verb to make
part of the subcategorization frame of the verb? To
deal with this problem, different approaches have
been taken, going from simple co-occurrence count
to various kinds of hypothesis testing (Korhonen et
al., 2000).
To determine whether a verb and a pattern are as-
sociated, IRASubcat provides a co-occurrence fre-
quency threshold, that can be tuned by the user, and
a hypothesis test, the Likelihood Ratio test (Dun-
ning, 1993). We chose to implement this test, and
not others like the binomial that have been exten-
sively used in subcategorization acquisition, because
the Likelihood Ratio is specially good at modeling
unfrequent events.
To perform this test, the null hypothesis is that the
distribution of an observed pattern ’Mj’ is indepen-
dent of of the distribution of verb ’Vi’.
</bodyText>
<page confidence="0.999146">
87
</page>
<figureCaption confidence="0.999862">
Figure 1: Example of application of the procedure to identify adjuncts.
</figureCaption>
<figure confidence="0.9421183125">
1. A starting set of patterns:
[NP DirObj PP-with], [NP DirObj], [NP DirObj], [NP DirObj
PP-with],[NP DirObj] y [NP DirObj PP-for]
2. Pattern constituents are ordered by frequency:
NP &gt; DirObj &gt; PP-with &gt; PP-for
3. Constituents in patterns are ordered by their relative frequency:
[NP DirObj PP-with]
[NP DirObj]
[NP DirObj]
[NP DirObj PP-with]
[NP DirObj]
[NP DirObj PP-for]
4. A trie is built with patterns:
[NP DirObj] -&gt;3
[NP DirObj PP-with] -&gt;2
[NP DirObj PP-for] -&gt;1
</figure>
<listItem confidence="0.85540325">
5. Leafs in the trie are “DirObj”,“PP-with” and “PP-for”. Since DirObj also occurs in the trie in a
position other than leaf, it will not be considered as an adjunct in this iteration. In contrast, both
PP-with and PP-for fulfill the conditions to be considered adjuncts, so we prune the patterns the trie,
which will now have the single pattern, which forms a trie with 2 adjuncts (with information about
the number of occurrences of each adjunct constituent):
[NP DirObj {PP-with:1 PP-for:2}]
6. If the trie has been modified in this iteration, we go back to 2. If no modification has been operated,
the procedure ends.
</listItem>
<page confidence="0.998403">
88
</page>
<bodyText confidence="0.999936">
Moreover, the user can also specify a minimum
number of occurrences of a verb to be taken into
consideration, thus ruling out verbs for which there
is not enough evidence in the corpus to obtain reli-
able subcategorization information.
</bodyText>
<sectionHeader confidence="0.517571" genericHeader="method">
7 Examples of applications
</sectionHeader>
<bodyText confidence="0.999861710526316">
We have applied IRASubcat to two very different
corpora in order to test its functionalities.
We have applied it to the SenSem corpus
(Castellon et al., 2006), a corpus with 100 sentences
for each of the 250 most frequent verbs of Span-
ish, manually annotated with information of verbal
sense, syntactical function and semantic role of sen-
tence constituents, among other information. From
all the available information, we specified as input
parameter for IRASubcat to consider only the syn-
tactic function of sentence constituents. Thus, the
expected output was the syntactic aspect of subcat-
egorization frames of verbs. We worked with the
verbal sense as the unit.
We compared the patterns associated to each ver-
bal sense by IRASubcat with the subcategorization
frames manually associated to the verbs at the a lex-
ical data base of SenSem verbs1. We manually in-
spected the results for the 20 most frequent verbal
senses. Results can be seen at Table 1. We found
that the frequency threshold was the best filter to as-
sociate patterns and verbs, obtaining an f-measure
of 74%. When hypothesis tests were used as a crite-
rion to filter out associations of patterns with verbal
senses, performance dropped, as can be seen in the
lower rows of Table 1.
We also applied IRASubcat to an unannotated
corpus of Russian. The corpus was automatically
POS-tagged with TreeTagger (Schmid, 1994). We
applied IRASubcat to work with parts of speech to
build the patterns.
We manually inspected the patterns associated to
prototypical intransitive (“sleep”), transitive (“eat”)
and ditransitive (“give”) verbs. We found that pat-
terns which were more strongly associated to verbs
corresponded to their prototypical behaviour. For
example, the patterns associated to the verb “eat”
reflect the presence of a subject and a direct object:
</bodyText>
<footnote confidence="0.9958265">
1The lexical data base of SenSem verbs can be found at
http://grial.uab.es/adquisicio/.
</footnote>
<table confidence="0.9962455">
Pattern occurrences % Likelihood
Ratio Test
[’V’, ’Nn’] 5 99
[’V’, ’C’] 5 95
[’V’, ’R’] 4 did not pass
[’V’, ’Nn’, ’C’, ’Q’] 3 95
[’V’, ’V’, ’Nn’, ’Nn’] 3 99
[’V’, ’Nn’, ’Na’] 3 99,5
[’Nn’, ’C’] 3 90
[’V’, ’Nn’, ’Nn’] 3 99
[’V’, ’R’, ’Q’] 2 95
[’V’, ’Nn’, ’An’] 2 99
</table>
<bodyText confidence="0.8337185">
For more details on evaluation, see (Altamirano,
2009).
</bodyText>
<sectionHeader confidence="0.996585" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999989083333333">
We have presented a highly flexible tool to acquire
verbal subcategorization information from corpus,
independently of the language and level of annota-
tion of the corpus. It is capable of identifying ad-
juncts and performs different tests to associate pat-
terns with verbs. Thresholds for these tests can be
set by the user, as well as a series of other sys-
tem parameters. Moreover, the system is platform-
independent and open-source2.
We are currently carrying out experiments to as-
sess the utility of the tool with two very different
corpora: the SenSem corpus of Spanish, where sen-
tences have been manually annotated with informa-
tion about the category, function and role of the ar-
guments of each verb, and also a raw corpus of Rus-
sian, for which only automatic part-of-speech tag-
ging is available. Preliminary results indicate that,
when parameters are properly set, IRASubcat is ca-
pable of identifying reliable subcategorization infor-
mation in corpus.
As future work, we plan to integrate evaluation
capabilities into the tool, so that it can provide pre-
cision and recall figures if a gold standard subcate-
gorization lexicon is provided.
</bodyText>
<sectionHeader confidence="0.998304" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.981423">
This research has been partially funded by projects
KNOW, TIN2006-15049-C03-01 and Representa-
tion of Semantic Knowledge TIN2009-14715-C04-
03 of the Spanish Ministry of Education and Cul-
</bodyText>
<footnote confidence="0.9974595">
2IRASubcat is available for download at http://www.
irasubcat.com.ar
</footnote>
<page confidence="0.997052">
89
</page>
<table confidence="0.998020833333333">
applied filter Precision Recall F-measure
Frequency .79 .70 .74
likelihood ratio 90% .42 .46 .39
likelihood ratio 95% .38 .42 .32
likelihood ratio 99% .31 .36 .22
likelihood ratio 99.5% .25 .28 .14
</table>
<tableCaption confidence="0.99976875">
Table 1: Performance of IRASubcat to acquire subcategorization information from the SenSem corpus, for the 20
most frequent verbal senses, as compared with manual association of subcategorization patterns with verbal senses.
Performance with different filters is detailed: only the most frequent patterns are considered, or only patterns passing
a hypothesis test are considered.
</tableCaption>
<bodyText confidence="0.999148333333333">
ture, and by project PAE-PICT-2007-02290, funded
by the National Agency for the Promotion of Sci-
ence and Technology in Argentina.
</bodyText>
<sectionHeader confidence="0.998937" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999881763888889">
Bibliographisches Institut &amp; F. A. Brockhaus AG, editor.
2001. Duden das Stilw¨orterbuch. Dudenverlag.
I. Romina Altamirano. 2009. Irasubcat: Un sistema
para adquisici´on autom´atica de marcos de subcatego-
rizaci´on de piezas l´exicas a partir de corpus. Master’s
thesis, Facultad de Matem´atica, Astronomia y Fisica,
Universidad Nacional de C´ordoba, Argentina.
Michael R. Brent. 1993. From grammar to lexicon: un-
supervised learning of lexical syntax. Comput. Lin-
guist., 19(2):243–262.
Ted Briscoe and John Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. pages 356–
363.
J. Carroll and A. Fang. 2004. The automatic acquisition
of verb subcategorisations and their impact on the per-
formance of an HPSG parser. In Proceedings of the 1st
International Joint Conference on Natural Language
Processing (IJCNLP), pages 107–114.
J. Carroll, G. Minnen, and T. Briscoe. 1999. Corpus
annotation for parser evaluation. In Proceedings of the
EACL-99 Post-conference Workshop on Linguistical ly
Interpreted Corpora, pages 35–41, Bergen, Norway.
Irene Castell´on, Ana Fern´andez-Montraveta, Gl`oria
V´azquez, Laura Alonso, and Joanan Capilla. 2006.
The SENSEM corpus: a corpus annotated at the syntac-
tic and semantic level. In 5th International Conference
on Language Resources and Evaluation (LREC 2006).
Paula Chesley and Susanne Salmon-Alt. 2006. Au-
tomatic extraction of subcategorization frames for
french.
Grzegorz Chrupala. 2003. Acquiring verb subcatego-
rization from spanish corpora. Master’s thesis, Uni-
versitat de Barcelona.
Erika de Lima. 2002. The automatic acquisition
of lexical information from portuguese text corpora.
Master’s thesis, Institut f¨ur Maschinelle Sprachverar-
beitung, Universit¨at Stuttgart.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. COMPUTATIONAL LIN-
GUISTICS.
Judith Eckle-Kohler. 1999. Linguistic knowledge for au-
tomatic lexicon acquisition from german text corpora.
Charles J. Fillmore. 1968. The case for case. In E. Bach
and R. T. Harms, editors, Universals in Linguistic The-
ory. Holt, Rinehart, and Winston, New York.
Effi Georgala. 2003. A statistical grammar model for
modern greek: The context-free grammar.
Alexandra Kinyon and Carlos A. Prolo. 2002. Identify-
ing verb arguments and their syntactic function in the
penn treebank. pages 1982–1987.
Anna Korhonen, Genevieve Gorrell, and Diana Mc-
Carthy. 2000. Statistical filtering and subcategoriza-
tion frame acquisition. In Proceedings of the 2000
Joint SIGDAT conference on Empirical methods in
natural language processing and very large corpora,
pages 199–206, Morristown, NJ, USA. Association for
Computational Linguistics.
Christopher D. Manning. 1993. Automatic acquisition
of a large subcategorization dictionary from corpora.
pages 235–242.
Ruth O’Donovan, Michael Burke, Aoife Cahill, Josef van
Genabith, and Andy Way. 2005. Large-scale induc-
tion and evaluation of lexical resources from the penn-
ii and penn-iii treebanks. volume 31, pages 329–365.
Evan Sandhaus, editor. New York Times.
Anoop Sarkar and Daniel Zeman. 2000. Automatic ex-
traction of subcategorization frames for czech. pages
691–697.
H. Schmid. 1994. Probabilistic part–of–speech tagging
using decision trees. In Proceedings of the Conference
on New Methods in Language Processing, pages 44–
49, Manchester, UK.
</reference>
<page confidence="0.97718">
90
</page>
<reference confidence="0.999197631578947">
Sabine Schulte im Walde. 2000. Clustering verbs se-
mantically according to their alternation behaviour. In
COLING’00, pages 747–753.
Kristina Spranger and Ulrich Heid. 2003. A dutch chun-
ker as a basis for the extraction of linguistic knowl-
edge.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate arguments struc-
tures for information extraction. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2003).
Akira Ushioda, David A. Evans, Ted Gibson, and Alex
Waibel. 1993. The automatic acquisition of frequen-
cies of verb subcategorization frames from tagged cor-
pora. pages 95–106.
Oliver Wauschkuhn. 1999. Automatische extraktion von
verbvalenzen aus deutschen text korpora. Master’s
thesis, Universit¨at Stuttgart.
WSJ, editor. 1994. Wall Street Journal.
</reference>
<page confidence="0.999166">
91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.377058">
<title confidence="0.9985215">IRASubcat, a highly customizable, language independent tool for acquisition of verbal subcategorization information from corpus</title>
<author confidence="0.9472">Romina Altamirano Alonso i</author>
<affiliation confidence="0.78167725">Grupo de Procesamiento de Lenguaje Secci´on de Ciencias de la Facultad de Matem´atica, Astronom´ıa y Universidad Nacional de</affiliation>
<address confidence="0.807747">C´ordoba,</address>
<email confidence="0.997079">romina.altamirano@gmail.com,alemany@famaf.unc.edu.ar</email>
<abstract confidence="0.998685047619048">IRASubcat is a language-independent tool to acquire information about the subcategorization of verbs from corpus. The tool can extract information from corpora annotated at various levels, including almost raw text, where only verbs are identified. It can also aggregate information from a pre-existing lexicon with verbal subcategorization information. The system is highly customizable, and works with XML as input and output format. IRASubcat identifies patterns of constituents in the corpus, and associates patterns with verbs if their association strength is over a frequency threshold and passes the likelihood ratio hypothesis test. It also implements a procedure to identify verbal constituents that could be playing the role of an adjunct in a pattern. Thresholds controlling frequency and identification of adjuncts can be customized by the user, or else they are given a default value.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2001</date>
<booktitle>Duden das Stilw¨orterbuch. Dudenverlag.</booktitle>
<editor>Bibliographisches Institut &amp; F. A. Brockhaus AG, editor.</editor>
<marker>2001</marker>
<rawString>Bibliographisches Institut &amp; F. A. Brockhaus AG, editor. 2001. Duden das Stilw¨orterbuch. Dudenverlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Romina Altamirano</author>
</authors>
<title>Irasubcat: Un sistema para adquisici´on autom´atica de marcos de subcategorizaci´on de piezas l´exicas a partir de corpus. Master’s thesis, Facultad de Matem´atica, Astronomia y Fisica, Universidad Nacional de</title>
<date>2009</date>
<location>C´ordoba, Argentina.</location>
<contexts>
<context position="20495" citStr="Altamirano, 2009" startWordPosition="3422" endWordPosition="3423">tterns which were more strongly associated to verbs corresponded to their prototypical behaviour. For example, the patterns associated to the verb “eat” reflect the presence of a subject and a direct object: 1The lexical data base of SenSem verbs can be found at http://grial.uab.es/adquisicio/. Pattern occurrences % Likelihood Ratio Test [’V’, ’Nn’] 5 99 [’V’, ’C’] 5 95 [’V’, ’R’] 4 did not pass [’V’, ’Nn’, ’C’, ’Q’] 3 95 [’V’, ’V’, ’Nn’, ’Nn’] 3 99 [’V’, ’Nn’, ’Na’] 3 99,5 [’Nn’, ’C’] 3 90 [’V’, ’Nn’, ’Nn’] 3 99 [’V’, ’R’, ’Q’] 2 95 [’V’, ’Nn’, ’An’] 2 99 For more details on evaluation, see (Altamirano, 2009). 8 Conclusions and Future Work We have presented a highly flexible tool to acquire verbal subcategorization information from corpus, independently of the language and level of annotation of the corpus. It is capable of identifying adjuncts and performs different tests to associate patterns with verbs. Thresholds for these tests can be set by the user, as well as a series of other system parameters. Moreover, the system is platformindependent and open-source2. We are currently carrying out experiments to assess the utility of the tool with two very different corpora: the SenSem corpus of Spani</context>
</contexts>
<marker>Altamirano, 2009</marker>
<rawString>I. Romina Altamirano. 2009. Irasubcat: Un sistema para adquisici´on autom´atica de marcos de subcategorizaci´on de piezas l´exicas a partir de corpus. Master’s thesis, Facultad de Matem´atica, Astronomia y Fisica, Universidad Nacional de C´ordoba, Argentina.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>From grammar to lexicon: unsupervised learning of lexical syntax.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2482" citStr="Brent, 1993" startWordPosition="365" endWordPosition="366">lly prone to low coverage and inconsistencies across human experts. Corpora can be very useful to alleviate the problems of low coverage and inconsistencies. Verbs can be characterized by their behavior in a big corpus of the language. Thus, lexicographers only need to validate, correct or complete this digested information about the behavior of verbs. Moreover, the starting information can have higher coverage and be more unbiased than if it is manually constructed. That’s why automatic acquisition of subcategorization frames has been an active research area since the mid-90s (Manning, 1993; Brent, 1993; Briscoe and Carroll, 1997). However, most of the approaches have been adhoc for particular languages or particular settings, like a determined corpus with a given kind of annotation, be it manual or automatic. To our knowledge, there is no system to acquire subcategorization information from corpora that is flexible enough to work with different languages and levels of annotation of corpora. We present IRASubcat, a tool that acquires information about the behaviour of verbs from corpora. It is aimed to address a variety of situations and needs, ranging from rich annotated corpora to virtuall</context>
<context position="5167" citStr="Brent, 1993" startWordPosition="798" endWordPosition="799">ncts and not arguments, and in Section 6 we describe the procedures to determine whether a given pattern is actually part of the subcategorization frame of a verb. Section 7 presents some results of applying IRASubcat to two very different corpora. Finally, we present some conclusions and the lines of future work. 2 Previous Work We review here some previous work related to acquisition of subcategorization information from corpora, focussing on the constraints of the approach and corpora to learn with. We specially mention approaches for languages other than English. The foundational work of (Brent, 1993) was based on plain text (2.6 million words of the Wall Street Journal (WSJ, 1994)). Since the corpus had no annotation, verbs were found by heuristics. He detected six frame types and filtered associations between verbs and frames with the binomial hypothesis test. This approach obtained 73.85% f-score in an evaluation with human judges. Also in 1993, (Ushioda et al., 1993) exploited also the WSJ corpus but only the part that was annotated with part-of-speech tags, with 600.000 words. He studied also six frame types and did not distinguishing arguments and adjuncts. The same year, (Manning, 1</context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Michael R. Brent. 1993. From grammar to lexicon: unsupervised learning of lexical syntax. Comput. Linguist., 19(2):243–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<pages>356--363</pages>
<contexts>
<context position="2510" citStr="Briscoe and Carroll, 1997" startWordPosition="367" endWordPosition="370">low coverage and inconsistencies across human experts. Corpora can be very useful to alleviate the problems of low coverage and inconsistencies. Verbs can be characterized by their behavior in a big corpus of the language. Thus, lexicographers only need to validate, correct or complete this digested information about the behavior of verbs. Moreover, the starting information can have higher coverage and be more unbiased than if it is manually constructed. That’s why automatic acquisition of subcategorization frames has been an active research area since the mid-90s (Manning, 1993; Brent, 1993; Briscoe and Carroll, 1997). However, most of the approaches have been adhoc for particular languages or particular settings, like a determined corpus with a given kind of annotation, be it manual or automatic. To our knowledge, there is no system to acquire subcategorization information from corpora that is flexible enough to work with different languages and levels of annotation of corpora. We present IRASubcat, a tool that acquires information about the behaviour of verbs from corpora. It is aimed to address a variety of situations and needs, ranging from rich annotated corpora to virtually raw text (because the tags</context>
<context position="6141" citStr="Briscoe and Carroll, 1997" startWordPosition="954" endWordPosition="957">1993, (Ushioda et al., 1993) exploited also the WSJ corpus but only the part that was annotated with part-of-speech tags, with 600.000 words. He studied also six frame types and did not distinguishing arguments and adjuncts. The same year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus, ), selected only clauses with auxiliary verbs and automatically analyzed them with a finite-state parser. He defined 19 frame types, and reported an f-score of 58.20%. Various authors developed approaches assuming a full syntactic analysis, which was usually annotated manually in corpora (Briscoe and Carroll, 1997; Kinyon and Prolo, 2002). Others associated syntactic analyses to corpora with automatic parsers (O’Donovan et al., 2005). Various approaches were also found for languages other than English. For German, (Eckle-Kohler, 1999) studied the behaviour of 6305 verbs on automatically POS-tagged corpus data. He defined linguistic heuristics by regular expression queries over the usage of 244 frame types. (Wauschkuhn, 1999) studied 1044 German verbs. He extracted maximum of 2000 example sentences for each verb from a corpus, and analyzed them with partial (as opposed to full) syntactic analysis. He fo</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Ted Briscoe and John Carroll. 1997. Automatic extraction of subcategorization from corpora. pages 356– 363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>A Fang</author>
</authors>
<title>The automatic acquisition of verb subcategorisations and their impact on the performance of an HPSG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the 1st International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>107--114</pages>
<contexts>
<context position="1649" citStr="Carroll and Fang, 2004" startWordPosition="234" endWordPosition="237">sses the likelihood ratio hypothesis test. It also implements a procedure to identify verbal constituents that could be playing the role of an adjunct in a pattern. Thresholds controlling frequency and identification of adjuncts can be customized by the user, or else they are given a default value. 1 Introduction and Motivation Characterizing the behavior of verbs as nuclear organizers of clauses (the so-called subcategorization information) is crucial to obtain deep analyses of natural language. For example, it can significantly reduce structural ambiguities in parsing (Carroll et al., 1999; Carroll and Fang, 2004), help in word sense disambiguation or improve information extraction (Surdeanu et al., 2003). However, the usual construction of linguistic resources for verbal subcategorization involves many expert hours, and it is usually prone to low coverage and inconsistencies across human experts. Corpora can be very useful to alleviate the problems of low coverage and inconsistencies. Verbs can be characterized by their behavior in a big corpus of the language. Thus, lexicographers only need to validate, correct or complete this digested information about the behavior of verbs. Moreover, the starting </context>
</contexts>
<marker>Carroll, Fang, 2004</marker>
<rawString>J. Carroll and A. Fang. 2004. The automatic acquisition of verb subcategorisations and their impact on the performance of an HPSG parser. In Proceedings of the 1st International Joint Conference on Natural Language Processing (IJCNLP), pages 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>T Briscoe</author>
</authors>
<title>Corpus annotation for parser evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of the EACL-99 Post-conference Workshop on Linguistical ly Interpreted Corpora,</booktitle>
<pages>35--41</pages>
<location>Bergen,</location>
<contexts>
<context position="1624" citStr="Carroll et al., 1999" startWordPosition="230" endWordPosition="233">uency threshold and passes the likelihood ratio hypothesis test. It also implements a procedure to identify verbal constituents that could be playing the role of an adjunct in a pattern. Thresholds controlling frequency and identification of adjuncts can be customized by the user, or else they are given a default value. 1 Introduction and Motivation Characterizing the behavior of verbs as nuclear organizers of clauses (the so-called subcategorization information) is crucial to obtain deep analyses of natural language. For example, it can significantly reduce structural ambiguities in parsing (Carroll et al., 1999; Carroll and Fang, 2004), help in word sense disambiguation or improve information extraction (Surdeanu et al., 2003). However, the usual construction of linguistic resources for verbal subcategorization involves many expert hours, and it is usually prone to low coverage and inconsistencies across human experts. Corpora can be very useful to alleviate the problems of low coverage and inconsistencies. Verbs can be characterized by their behavior in a big corpus of the language. Thus, lexicographers only need to validate, correct or complete this digested information about the behavior of verbs</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1999</marker>
<rawString>J. Carroll, G. Minnen, and T. Briscoe. 1999. Corpus annotation for parser evaluation. In Proceedings of the EACL-99 Post-conference Workshop on Linguistical ly Interpreted Corpora, pages 35–41, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Castell´on</author>
<author>Ana Fern´andez-Montraveta</author>
<author>Gl`oria V´azquez</author>
<author>Laura Alonso</author>
<author>Joanan Capilla</author>
</authors>
<title>The SENSEM corpus: a corpus annotated at the syntactic and semantic level.</title>
<date>2006</date>
<booktitle>In 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>Castell´on, Fern´andez-Montraveta, V´azquez, Alonso, Capilla, 2006</marker>
<rawString>Irene Castell´on, Ana Fern´andez-Montraveta, Gl`oria V´azquez, Laura Alonso, and Joanan Capilla. 2006. The SENSEM corpus: a corpus annotated at the syntactic and semantic level. In 5th International Conference on Language Resources and Evaluation (LREC 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Chesley</author>
<author>Susanne Salmon-Alt</author>
</authors>
<title>Automatic extraction of subcategorization frames for french.</title>
<date>2006</date>
<contexts>
<context position="7344" citStr="Chesley and Salmon-Alt, 2006" startWordPosition="1140" endWordPosition="1143"> syntactic analysis. He found valency patterns, which were grouped in order to extract the most frequent pattern combinations, resulting in a verb-frame lexicon with 42 frame types. (Schulte im Walde, 2000) worked with 18.7 million words of German corpus, found 38 frame types. She used the Duden das Stilw¨orterbuch(AG, 2001) to evaluate results and reported f-score 57,24% with PP and 62,30% without. Many other approaches have been pursued for various languages: (de Lima, 2002) for Portuguese, (Georgala, 2003) for Greek, (Sarkar and Zeman, 2000) for Czech, (Spranger and Heid, 2003) for Dutch, (Chesley and Salmon-Alt, 2006) for French or (Chrupala, 2003) for Spanish, to name a few. 3 General description of the tool IRASubcat takes as input a corpus in XML format. This corpus is expected to have some kind of annotation associated to its elements, which will enrich the description of the patterns associated to verbs. The minimal required annotation is that verbs are marked. If no other information is available, the form of words will be used to build the patterns. If the corpus has rich annotation for its elements, the system can build the patterns with the value of attributes or with a combination of them, and al</context>
</contexts>
<marker>Chesley, Salmon-Alt, 2006</marker>
<rawString>Paula Chesley and Susanne Salmon-Alt. 2006. Automatic extraction of subcategorization frames for french.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupala</author>
</authors>
<title>Acquiring verb subcategorization from spanish corpora.</title>
<date>2003</date>
<tech>Master’s thesis,</tech>
<institution>Universitat de Barcelona.</institution>
<contexts>
<context position="7375" citStr="Chrupala, 2003" startWordPosition="1147" endWordPosition="1148">s, which were grouped in order to extract the most frequent pattern combinations, resulting in a verb-frame lexicon with 42 frame types. (Schulte im Walde, 2000) worked with 18.7 million words of German corpus, found 38 frame types. She used the Duden das Stilw¨orterbuch(AG, 2001) to evaluate results and reported f-score 57,24% with PP and 62,30% without. Many other approaches have been pursued for various languages: (de Lima, 2002) for Portuguese, (Georgala, 2003) for Greek, (Sarkar and Zeman, 2000) for Czech, (Spranger and Heid, 2003) for Dutch, (Chesley and Salmon-Alt, 2006) for French or (Chrupala, 2003) for Spanish, to name a few. 3 General description of the tool IRASubcat takes as input a corpus in XML format. This corpus is expected to have some kind of annotation associated to its elements, which will enrich the description of the patterns associated to verbs. The minimal required annotation is that verbs are marked. If no other information is available, the form of words will be used to build the patterns. If the corpus has rich annotation for its elements, the system can build the patterns with the value of attributes or with a combination of them, and also with combinations with lexic</context>
</contexts>
<marker>Chrupala, 2003</marker>
<rawString>Grzegorz Chrupala. 2003. Acquiring verb subcategorization from spanish corpora. Master’s thesis, Universitat de Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erika de Lima</author>
</authors>
<title>The automatic acquisition of lexical information from portuguese text corpora. Master’s thesis, Institut f¨ur Maschinelle Sprachverarbeitung,</title>
<date>2002</date>
<location>Universit¨at Stuttgart.</location>
<marker>de Lima, 2002</marker>
<rawString>Erika de Lima. 2002. The automatic acquisition of lexical information from portuguese text corpora. Master’s thesis, Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>COMPUTATIONAL LINGUISTICS.</journal>
<contexts>
<context position="16475" citStr="Dunning, 1993" startWordPosition="2753" endWordPosition="2755">d. 6 Associating patterns to verbs One of the critical aspects of subcategorization acquisition is the association of verbs and patterns. How often must a pattern occur with a verb to make part of the subcategorization frame of the verb? To deal with this problem, different approaches have been taken, going from simple co-occurrence count to various kinds of hypothesis testing (Korhonen et al., 2000). To determine whether a verb and a pattern are associated, IRASubcat provides a co-occurrence frequency threshold, that can be tuned by the user, and a hypothesis test, the Likelihood Ratio test (Dunning, 1993). We chose to implement this test, and not others like the binomial that have been extensively used in subcategorization acquisition, because the Likelihood Ratio is specially good at modeling unfrequent events. To perform this test, the null hypothesis is that the distribution of an observed pattern ’Mj’ is independent of of the distribution of verb ’Vi’. 87 Figure 1: Example of application of the procedure to identify adjuncts. 1. A starting set of patterns: [NP DirObj PP-with], [NP DirObj], [NP DirObj], [NP DirObj PP-with],[NP DirObj] y [NP DirObj PP-for] 2. Pattern constituents are ordered</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. COMPUTATIONAL LINGUISTICS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Eckle-Kohler</author>
</authors>
<title>Linguistic knowledge for automatic lexicon acquisition from german text corpora.</title>
<date>1999</date>
<contexts>
<context position="6366" citStr="Eckle-Kohler, 1999" startWordPosition="988" endWordPosition="989">me year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus, ), selected only clauses with auxiliary verbs and automatically analyzed them with a finite-state parser. He defined 19 frame types, and reported an f-score of 58.20%. Various authors developed approaches assuming a full syntactic analysis, which was usually annotated manually in corpora (Briscoe and Carroll, 1997; Kinyon and Prolo, 2002). Others associated syntactic analyses to corpora with automatic parsers (O’Donovan et al., 2005). Various approaches were also found for languages other than English. For German, (Eckle-Kohler, 1999) studied the behaviour of 6305 verbs on automatically POS-tagged corpus data. He defined linguistic heuristics by regular expression queries over the usage of 244 frame types. (Wauschkuhn, 1999) studied 1044 German verbs. He extracted maximum of 2000 example sentences for each verb from a corpus, and analyzed them with partial (as opposed to full) syntactic analysis. He found valency patterns, which were grouped in order to extract the most frequent pattern combinations, resulting in a verb-frame lexicon with 42 frame types. (Schulte im Walde, 2000) worked with 18.7 million words of German cor</context>
</contexts>
<marker>Eckle-Kohler, 1999</marker>
<rawString>Judith Eckle-Kohler. 1999. Linguistic knowledge for automatic lexicon acquisition from german text corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>The case for case.</title>
<date>1968</date>
<booktitle>Universals in Linguistic Theory.</booktitle>
<editor>In E. Bach and R. T. Harms, editors,</editor>
<location>Holt, Rinehart, and Winston, New York.</location>
<contexts>
<context position="13433" citStr="Fillmore, 1968" startWordPosition="2233" endWordPosition="2234">urrences of a pattern can also be specified, with the default as 0. • The user can specify whether the LogLikelihood Ratio hypothesis test will be applied to test whether the association between a verb and a pattern cannot be considered a product of chance. By defect, the test is used (and the output will be 90, 95, 99 or 99.5 when the co-ocurrence have that confiability) . 5 Identification of adjuncts One of the most interesting capabilities of IRASubcat is the identification of possible adjuncts. Adjuncts are linguistic units that do not make part of the core of a subcategorization pattern (Fillmore, 1968). They are optional constituents in the constituent structure governed by a verb. Since they are optional, we assume they can be recognized because the same pattern can occur with or without them without a significant difference. IRASubcat implements a procedure to identify these units by their optionality, described in what follows. An example of this procedure is shown in Figure 1. First, all patterns of a verb are represented in a trie. A trie is a tree-like structure where patterns are represented as paths in the trie. In our case, the root is empty and each node represents a constituent o</context>
</contexts>
<marker>Fillmore, 1968</marker>
<rawString>Charles J. Fillmore. 1968. The case for case. In E. Bach and R. T. Harms, editors, Universals in Linguistic Theory. Holt, Rinehart, and Winston, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Effi Georgala</author>
</authors>
<title>A statistical grammar model for modern greek: The context-free grammar.</title>
<date>2003</date>
<contexts>
<context position="7229" citStr="Georgala, 2003" startWordPosition="1124" endWordPosition="1125">00 example sentences for each verb from a corpus, and analyzed them with partial (as opposed to full) syntactic analysis. He found valency patterns, which were grouped in order to extract the most frequent pattern combinations, resulting in a verb-frame lexicon with 42 frame types. (Schulte im Walde, 2000) worked with 18.7 million words of German corpus, found 38 frame types. She used the Duden das Stilw¨orterbuch(AG, 2001) to evaluate results and reported f-score 57,24% with PP and 62,30% without. Many other approaches have been pursued for various languages: (de Lima, 2002) for Portuguese, (Georgala, 2003) for Greek, (Sarkar and Zeman, 2000) for Czech, (Spranger and Heid, 2003) for Dutch, (Chesley and Salmon-Alt, 2006) for French or (Chrupala, 2003) for Spanish, to name a few. 3 General description of the tool IRASubcat takes as input a corpus in XML format. This corpus is expected to have some kind of annotation associated to its elements, which will enrich the description of the patterns associated to verbs. The minimal required annotation is that verbs are marked. If no other information is available, the form of words will be used to build the patterns. If the corpus has rich annotation for</context>
</contexts>
<marker>Georgala, 2003</marker>
<rawString>Effi Georgala. 2003. A statistical grammar model for modern greek: The context-free grammar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Kinyon</author>
<author>Carlos A Prolo</author>
</authors>
<title>Identifying verb arguments and their syntactic function in the penn treebank.</title>
<date>2002</date>
<pages>1982--1987</pages>
<contexts>
<context position="6166" citStr="Kinyon and Prolo, 2002" startWordPosition="958" endWordPosition="961">) exploited also the WSJ corpus but only the part that was annotated with part-of-speech tags, with 600.000 words. He studied also six frame types and did not distinguishing arguments and adjuncts. The same year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus, ), selected only clauses with auxiliary verbs and automatically analyzed them with a finite-state parser. He defined 19 frame types, and reported an f-score of 58.20%. Various authors developed approaches assuming a full syntactic analysis, which was usually annotated manually in corpora (Briscoe and Carroll, 1997; Kinyon and Prolo, 2002). Others associated syntactic analyses to corpora with automatic parsers (O’Donovan et al., 2005). Various approaches were also found for languages other than English. For German, (Eckle-Kohler, 1999) studied the behaviour of 6305 verbs on automatically POS-tagged corpus data. He defined linguistic heuristics by regular expression queries over the usage of 244 frame types. (Wauschkuhn, 1999) studied 1044 German verbs. He extracted maximum of 2000 example sentences for each verb from a corpus, and analyzed them with partial (as opposed to full) syntactic analysis. He found valency patterns, whi</context>
</contexts>
<marker>Kinyon, Prolo, 2002</marker>
<rawString>Alexandra Kinyon and Carlos A. Prolo. 2002. Identifying verb arguments and their syntactic function in the penn treebank. pages 1982–1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
<author>Genevieve Gorrell</author>
<author>Diana McCarthy</author>
</authors>
<title>Statistical filtering and subcategorization frame acquisition.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora,</booktitle>
<pages>199--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16264" citStr="Korhonen et al., 2000" startWordPosition="2716" endWordPosition="2719">starting patterns. If the mark signalling the position of the verb had been introduced, it is also lost. However, order and position of the verb can be recovered in the final patterns, after adjuncts have been identified. 6 Associating patterns to verbs One of the critical aspects of subcategorization acquisition is the association of verbs and patterns. How often must a pattern occur with a verb to make part of the subcategorization frame of the verb? To deal with this problem, different approaches have been taken, going from simple co-occurrence count to various kinds of hypothesis testing (Korhonen et al., 2000). To determine whether a verb and a pattern are associated, IRASubcat provides a co-occurrence frequency threshold, that can be tuned by the user, and a hypothesis test, the Likelihood Ratio test (Dunning, 1993). We chose to implement this test, and not others like the binomial that have been extensively used in subcategorization acquisition, because the Likelihood Ratio is specially good at modeling unfrequent events. To perform this test, the null hypothesis is that the distribution of an observed pattern ’Mj’ is independent of of the distribution of verb ’Vi’. 87 Figure 1: Example of applic</context>
</contexts>
<marker>Korhonen, Gorrell, McCarthy, 2000</marker>
<rawString>Anna Korhonen, Genevieve Gorrell, and Diana McCarthy. 2000. Statistical filtering and subcategorization frame acquisition. In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora, pages 199–206, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<pages>235--242</pages>
<contexts>
<context position="2469" citStr="Manning, 1993" startWordPosition="363" endWordPosition="364"> and it is usually prone to low coverage and inconsistencies across human experts. Corpora can be very useful to alleviate the problems of low coverage and inconsistencies. Verbs can be characterized by their behavior in a big corpus of the language. Thus, lexicographers only need to validate, correct or complete this digested information about the behavior of verbs. Moreover, the starting information can have higher coverage and be more unbiased than if it is manually constructed. That’s why automatic acquisition of subcategorization frames has been an active research area since the mid-90s (Manning, 1993; Brent, 1993; Briscoe and Carroll, 1997). However, most of the approaches have been adhoc for particular languages or particular settings, like a determined corpus with a given kind of annotation, be it manual or automatic. To our knowledge, there is no system to acquire subcategorization information from corpora that is flexible enough to work with different languages and levels of annotation of corpora. We present IRASubcat, a tool that acquires information about the behaviour of verbs from corpora. It is aimed to address a variety of situations and needs, ranging from rich annotated corpor</context>
<context position="5771" citStr="Manning, 1993" startWordPosition="900" endWordPosition="901">ent, 1993) was based on plain text (2.6 million words of the Wall Street Journal (WSJ, 1994)). Since the corpus had no annotation, verbs were found by heuristics. He detected six frame types and filtered associations between verbs and frames with the binomial hypothesis test. This approach obtained 73.85% f-score in an evaluation with human judges. Also in 1993, (Ushioda et al., 1993) exploited also the WSJ corpus but only the part that was annotated with part-of-speech tags, with 600.000 words. He studied also six frame types and did not distinguishing arguments and adjuncts. The same year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus, ), selected only clauses with auxiliary verbs and automatically analyzed them with a finite-state parser. He defined 19 frame types, and reported an f-score of 58.20%. Various authors developed approaches assuming a full syntactic analysis, which was usually annotated manually in corpora (Briscoe and Carroll, 1997; Kinyon and Prolo, 2002). Others associated syntactic analyses to corpora with automatic parsers (O’Donovan et al., 2005). Various approaches were also found for languages other than English. For German, (Eckle-Kohler, 1999) stud</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>Christopher D. Manning. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. pages 235–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruth O’Donovan</author>
<author>Michael Burke</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Large-scale induction and evaluation of lexical resources from the pennii and penn-iii treebanks.</title>
<date>2005</date>
<volume>31</volume>
<pages>329--365</pages>
<marker>O’Donovan, Burke, Cahill, van Genabith, Way, 2005</marker>
<rawString>Ruth O’Donovan, Michael Burke, Aoife Cahill, Josef van Genabith, and Andy Way. 2005. Large-scale induction and evaluation of lexical resources from the pennii and penn-iii treebanks. volume 31, pages 329–365.</rawString>
</citation>
<citation valid="false">
<editor>Evan Sandhaus, editor.</editor>
<location>New York Times.</location>
<marker></marker>
<rawString>Evan Sandhaus, editor. New York Times.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
<author>Daniel Zeman</author>
</authors>
<title>Automatic extraction of subcategorization frames for czech.</title>
<date>2000</date>
<pages>691--697</pages>
<contexts>
<context position="7265" citStr="Sarkar and Zeman, 2000" startWordPosition="1128" endWordPosition="1131">h verb from a corpus, and analyzed them with partial (as opposed to full) syntactic analysis. He found valency patterns, which were grouped in order to extract the most frequent pattern combinations, resulting in a verb-frame lexicon with 42 frame types. (Schulte im Walde, 2000) worked with 18.7 million words of German corpus, found 38 frame types. She used the Duden das Stilw¨orterbuch(AG, 2001) to evaluate results and reported f-score 57,24% with PP and 62,30% without. Many other approaches have been pursued for various languages: (de Lima, 2002) for Portuguese, (Georgala, 2003) for Greek, (Sarkar and Zeman, 2000) for Czech, (Spranger and Heid, 2003) for Dutch, (Chesley and Salmon-Alt, 2006) for French or (Chrupala, 2003) for Spanish, to name a few. 3 General description of the tool IRASubcat takes as input a corpus in XML format. This corpus is expected to have some kind of annotation associated to its elements, which will enrich the description of the patterns associated to verbs. The minimal required annotation is that verbs are marked. If no other information is available, the form of words will be used to build the patterns. If the corpus has rich annotation for its elements, the system can build </context>
</contexts>
<marker>Sarkar, Zeman, 2000</marker>
<rawString>Anoop Sarkar and Daniel Zeman. 2000. Automatic extraction of subcategorization frames for czech. pages 691–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part–of–speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="19649" citStr="Schmid, 1994" startWordPosition="3281" endWordPosition="3282">s manually associated to the verbs at the a lexical data base of SenSem verbs1. We manually inspected the results for the 20 most frequent verbal senses. Results can be seen at Table 1. We found that the frequency threshold was the best filter to associate patterns and verbs, obtaining an f-measure of 74%. When hypothesis tests were used as a criterion to filter out associations of patterns with verbal senses, performance dropped, as can be seen in the lower rows of Table 1. We also applied IRASubcat to an unannotated corpus of Russian. The corpus was automatically POS-tagged with TreeTagger (Schmid, 1994). We applied IRASubcat to work with parts of speech to build the patterns. We manually inspected the patterns associated to prototypical intransitive (“sleep”), transitive (“eat”) and ditransitive (“give”) verbs. We found that patterns which were more strongly associated to verbs corresponded to their prototypical behaviour. For example, the patterns associated to the verb “eat” reflect the presence of a subject and a direct object: 1The lexical data base of SenSem verbs can be found at http://grial.uab.es/adquisicio/. Pattern occurrences % Likelihood Ratio Test [’V’, ’Nn’] 5 99 [’V’, ’C’] 5 9</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Probabilistic part–of–speech tagging using decision trees. In Proceedings of the Conference on New Methods in Language Processing, pages 44– 49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Clustering verbs semantically according to their alternation behaviour.</title>
<date>2000</date>
<booktitle>In COLING’00,</booktitle>
<pages>747--753</pages>
<contexts>
<context position="6921" citStr="Walde, 2000" startWordPosition="1076" endWordPosition="1077">ages other than English. For German, (Eckle-Kohler, 1999) studied the behaviour of 6305 verbs on automatically POS-tagged corpus data. He defined linguistic heuristics by regular expression queries over the usage of 244 frame types. (Wauschkuhn, 1999) studied 1044 German verbs. He extracted maximum of 2000 example sentences for each verb from a corpus, and analyzed them with partial (as opposed to full) syntactic analysis. He found valency patterns, which were grouped in order to extract the most frequent pattern combinations, resulting in a verb-frame lexicon with 42 frame types. (Schulte im Walde, 2000) worked with 18.7 million words of German corpus, found 38 frame types. She used the Duden das Stilw¨orterbuch(AG, 2001) to evaluate results and reported f-score 57,24% with PP and 62,30% without. Many other approaches have been pursued for various languages: (de Lima, 2002) for Portuguese, (Georgala, 2003) for Greek, (Sarkar and Zeman, 2000) for Czech, (Spranger and Heid, 2003) for Dutch, (Chesley and Salmon-Alt, 2006) for French or (Chrupala, 2003) for Spanish, to name a few. 3 General description of the tool IRASubcat takes as input a corpus in XML format. This corpus is expected to have so</context>
</contexts>
<marker>Walde, 2000</marker>
<rawString>Sabine Schulte im Walde. 2000. Clustering verbs semantically according to their alternation behaviour. In COLING’00, pages 747–753.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Spranger</author>
<author>Ulrich Heid</author>
</authors>
<title>A dutch chunker as a basis for the extraction of linguistic knowledge.</title>
<date>2003</date>
<contexts>
<context position="7302" citStr="Spranger and Heid, 2003" startWordPosition="1134" endWordPosition="1137">hem with partial (as opposed to full) syntactic analysis. He found valency patterns, which were grouped in order to extract the most frequent pattern combinations, resulting in a verb-frame lexicon with 42 frame types. (Schulte im Walde, 2000) worked with 18.7 million words of German corpus, found 38 frame types. She used the Duden das Stilw¨orterbuch(AG, 2001) to evaluate results and reported f-score 57,24% with PP and 62,30% without. Many other approaches have been pursued for various languages: (de Lima, 2002) for Portuguese, (Georgala, 2003) for Greek, (Sarkar and Zeman, 2000) for Czech, (Spranger and Heid, 2003) for Dutch, (Chesley and Salmon-Alt, 2006) for French or (Chrupala, 2003) for Spanish, to name a few. 3 General description of the tool IRASubcat takes as input a corpus in XML format. This corpus is expected to have some kind of annotation associated to its elements, which will enrich the description of the patterns associated to verbs. The minimal required annotation is that verbs are marked. If no other information is available, the form of words will be used to build the patterns. If the corpus has rich annotation for its elements, the system can build the patterns with the value of attrib</context>
</contexts>
<marker>Spranger, Heid, 2003</marker>
<rawString>Kristina Spranger and Ulrich Heid. 2003. A dutch chunker as a basis for the extraction of linguistic knowledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate arguments structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="1742" citStr="Surdeanu et al., 2003" startWordPosition="248" endWordPosition="251">onstituents that could be playing the role of an adjunct in a pattern. Thresholds controlling frequency and identification of adjuncts can be customized by the user, or else they are given a default value. 1 Introduction and Motivation Characterizing the behavior of verbs as nuclear organizers of clauses (the so-called subcategorization information) is crucial to obtain deep analyses of natural language. For example, it can significantly reduce structural ambiguities in parsing (Carroll et al., 1999; Carroll and Fang, 2004), help in word sense disambiguation or improve information extraction (Surdeanu et al., 2003). However, the usual construction of linguistic resources for verbal subcategorization involves many expert hours, and it is usually prone to low coverage and inconsistencies across human experts. Corpora can be very useful to alleviate the problems of low coverage and inconsistencies. Verbs can be characterized by their behavior in a big corpus of the language. Thus, lexicographers only need to validate, correct or complete this digested information about the behavior of verbs. Moreover, the starting information can have higher coverage and be more unbiased than if it is manually constructed.</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate arguments structures for information extraction. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Ushioda</author>
<author>David A Evans</author>
<author>Ted Gibson</author>
<author>Alex Waibel</author>
</authors>
<title>The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora.</title>
<date>1993</date>
<pages>95--106</pages>
<contexts>
<context position="5544" citStr="Ushioda et al., 1993" startWordPosition="861" endWordPosition="864">lated to acquisition of subcategorization information from corpora, focussing on the constraints of the approach and corpora to learn with. We specially mention approaches for languages other than English. The foundational work of (Brent, 1993) was based on plain text (2.6 million words of the Wall Street Journal (WSJ, 1994)). Since the corpus had no annotation, verbs were found by heuristics. He detected six frame types and filtered associations between verbs and frames with the binomial hypothesis test. This approach obtained 73.85% f-score in an evaluation with human judges. Also in 1993, (Ushioda et al., 1993) exploited also the WSJ corpus but only the part that was annotated with part-of-speech tags, with 600.000 words. He studied also six frame types and did not distinguishing arguments and adjuncts. The same year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus, ), selected only clauses with auxiliary verbs and automatically analyzed them with a finite-state parser. He defined 19 frame types, and reported an f-score of 58.20%. Various authors developed approaches assuming a full syntactic analysis, which was usually annotated manually in corpora (Briscoe and Carroll, 1997; K</context>
</contexts>
<marker>Ushioda, Evans, Gibson, Waibel, 1993</marker>
<rawString>Akira Ushioda, David A. Evans, Ted Gibson, and Alex Waibel. 1993. The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora. pages 95–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Wauschkuhn</author>
</authors>
<title>Automatische extraktion von verbvalenzen aus deutschen text korpora. Master’s thesis,</title>
<date>1999</date>
<institution>Universit¨at Stuttgart.</institution>
<contexts>
<context position="6560" citStr="Wauschkuhn, 1999" startWordPosition="1018" endWordPosition="1019">19 frame types, and reported an f-score of 58.20%. Various authors developed approaches assuming a full syntactic analysis, which was usually annotated manually in corpora (Briscoe and Carroll, 1997; Kinyon and Prolo, 2002). Others associated syntactic analyses to corpora with automatic parsers (O’Donovan et al., 2005). Various approaches were also found for languages other than English. For German, (Eckle-Kohler, 1999) studied the behaviour of 6305 verbs on automatically POS-tagged corpus data. He defined linguistic heuristics by regular expression queries over the usage of 244 frame types. (Wauschkuhn, 1999) studied 1044 German verbs. He extracted maximum of 2000 example sentences for each verb from a corpus, and analyzed them with partial (as opposed to full) syntactic analysis. He found valency patterns, which were grouped in order to extract the most frequent pattern combinations, resulting in a verb-frame lexicon with 42 frame types. (Schulte im Walde, 2000) worked with 18.7 million words of German corpus, found 38 frame types. She used the Duden das Stilw¨orterbuch(AG, 2001) to evaluate results and reported f-score 57,24% with PP and 62,30% without. Many other approaches have been pursued fo</context>
</contexts>
<marker>Wauschkuhn, 1999</marker>
<rawString>Oliver Wauschkuhn. 1999. Automatische extraktion von verbvalenzen aus deutschen text korpora. Master’s thesis, Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>editor WSJ</author>
</authors>
<date>1994</date>
<publisher>Wall Street Journal.</publisher>
<contexts>
<context position="5249" citStr="WSJ, 1994" startWordPosition="813" endWordPosition="814">ther a given pattern is actually part of the subcategorization frame of a verb. Section 7 presents some results of applying IRASubcat to two very different corpora. Finally, we present some conclusions and the lines of future work. 2 Previous Work We review here some previous work related to acquisition of subcategorization information from corpora, focussing on the constraints of the approach and corpora to learn with. We specially mention approaches for languages other than English. The foundational work of (Brent, 1993) was based on plain text (2.6 million words of the Wall Street Journal (WSJ, 1994)). Since the corpus had no annotation, verbs were found by heuristics. He detected six frame types and filtered associations between verbs and frames with the binomial hypothesis test. This approach obtained 73.85% f-score in an evaluation with human judges. Also in 1993, (Ushioda et al., 1993) exploited also the WSJ corpus but only the part that was annotated with part-of-speech tags, with 600.000 words. He studied also six frame types and did not distinguishing arguments and adjuncts. The same year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus, ), selected only clause</context>
</contexts>
<marker>WSJ, 1994</marker>
<rawString>WSJ, editor. 1994. Wall Street Journal.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>