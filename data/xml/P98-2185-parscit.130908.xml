<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000341">
<title confidence="0.9984595">
An Interactive Domain Independent Approach to Robust
Dialogue Interpretation
</title>
<author confidence="0.936066">
Carolyn Penstein Rosé
</author>
<affiliation confidence="0.928788">
LRDC 520, University of Pittsburgh
</affiliation>
<address confidence="0.7537685">
3939 Ohara St.,
Pittsburgh PA, 15260
</address>
<email confidence="0.99929">
rosecp@pitt.edu
</email>
<author confidence="0.923834">
Lori S. Levin
</author>
<affiliation confidence="0.7679685">
Carnegie Mellon University
Center for Machine Translation
</affiliation>
<address confidence="0.842891">
Pittsburgh, PA 15213
</address>
<email confidence="0.966913">
lslOcs.cmu.edu
</email>
<sectionHeader confidence="0.996882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998365">
We discuss an interactive approach to robust inter-
pretation in a large scale speech-to-speech transla-
tion system. Where other interactive approaches to
robust interpretation have depended upon domain
dependent repair rules, the approach described here
operates efficiently without any such hand-coded re-
pair knowledge and yields a 37% reduction in error
rate over a corpus of noisy sentences.
</bodyText>
<sectionHeader confidence="0.999409" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998967132075472">
In this paper we discuss ROSE, an interactive ap-
proach to robust interpretation developed in the
context of the JANUS speech-to-speech translation
system (Lavie et al., 1996). Previous interactive
approaches to robust interpretation have either re-
quired excessive amounts of interaction (Rosé and
Waibel, 1994), depended upon domain dependent
repair rules (Van Noord, 1996; Danieli and Gerbino,
1995), or relied on the minimum distance parsing
approach (Hipp, 1992; Smith, 1992; Lehman, 1989)
which has been shown to be intractable in a large-
scale system (Rose and Lavie, 1997). In contrast,
the ROSE approach operates efficiently without any
hand-coded repair knowledge. An empirical evalu-
ation demonstrates the efficacy of this domain in-
dependent approach. A further evaluation demon-
strates that the ROSE approach combines easily
with available domain knowledge in order to improve
the quality of the interaction.
The ROSE approach is based on a model of hu-
man communication between speakers of different
languages with a small shared language base. Hu-
mans who share a very small language base are
able to communicate when the need arises by sim-
plifying their speech patterns and negotiating un-
til they manage to transmit their ideas to one an-
other (Hatch, 1983). As the speaker is speaking,
the listener &amp;quot;casts his net&amp;quot; in order to catch those
fragments of speech that are comprehensible to him,
which he then attempts to fit together semantically.
His subsequent negotiation with the speaker builds
upon this partial understanding. Similarly, ROSE
repairs extragrammatical input in two phases. The
first phase, Repair Hypothesis Formation, is respon-
sible for assembling a set of hypotheses about the
meaning of the ungrammatical utterance. In the
second phase, Interaction with the User, the sys-
tern generates a set of queries, negotiating with the
speaker in order to narrow down to a single best
meaning representation hypothesis.
This approach was evaluated in the context of
the JANUS multi-lingual machine translation sys-
tem. First, the system obtains a meaning represen-
tation for a sentence uttered in the source language.
Then the resulting meaning representation structure
is mapped onto a sentence in the target language
using GENKIT (Tomita and Nyberg, 1988) with a
sentence level generation grammar. Currently, the
translation system deals with the scheduling domain
where two speakers attempt to schedule a meeting
together over the phone. This paper focuses on
the Interaction phase. Details about the Hypoth-
esis Formation phase are found in (Rose, 1997).
</bodyText>
<sectionHeader confidence="0.995059" genericHeader="method">
2 Interactive Repair In Depth
</sectionHeader>
<bodyText confidence="0.999800909090909">
As mentioned above, ROSE repairs extragrammat-
ical input in two phases. The first phase, Repair
Hypothesis Formation, is responsible for assembling
a ranked set of ten or fewer hypotheses about the
meaning of the ungrammatical utterance expressed
in the source language. This phase is itself divided
into two stages, Partial Parsing and Combination.
The Partial Parsing stage is similar to the concept
of the listener &amp;quot;casting his net&amp;quot; for comprehensi-
ble fragments of speech. A robust skipping parser
(Lavie, 1995) is used to obtain an analysis for islands
of the speaker&apos;s sentence. In the Combination stage,
the fragments from the partial parse are assembled
into a ranked set of alternative meaning represen-
tation hypotheses. A genetic programming (Koza,
1992; Koza, 1994) approach is used to search for
different ways to combine the fragments in order to
avoid requiring any hand-crafted repair rules. Our
genetic programming approach has been shown pre-
viously to be orders of magnitude more efficient than
the minimum distance parsing approach (Rose and
Lavie, 1997). In the second phase, Interaction with
</bodyText>
<page confidence="0.993648">
1129
</page>
<bodyText confidence="0.999959034482759">
the user, the system generates a set of queries, nego-
tiating with the speaker in order to narrow down to
a single best meaning representation hypothesis. Or,
if it determines based on the user&apos;s responses to its
queries that none of its hypotheses are acceptable,
it requests a rephrase.
Inspired by (Clark and Wilkes-Gibbs, 1986; Clark
and Schaefer, 1989), the goal of the Interaction
Phase is to minimize collaborative effort between
the system and the speaker while maintaining a high
level of interpretation accuracy. It uses this princi-
ple in determining which portions of the speaker&apos;s
utterance to question. Thus, it focuses its interac-
tion on those portions of the speaker&apos;s meaning that
it is particularly uncertain about. In its question-
ing, it attempts to display the state of the system&apos;s
understanding, acknowledging information conveyed
by the speaker as it becomes clear. The interaction
process can be summarized as follows: The system
first assesses the state of its understanding of what
the speaker has said by extracting features that dis-
tinguish the top set of hypotheses from one another.
It then builds upon this understanding by cycling
through the following four step process: selecting a
feature; generating a natural language query from
this feature; updating its list of alternative hypothe-
ses based on the user&apos;s answer; and finally updating
its list of distinguishing features based on the re-
maining set of alternative hypotheses.
</bodyText>
<subsectionHeader confidence="0.998731">
2.1 Extracting Distinguishing Features
</subsectionHeader>
<bodyText confidence="0.997938482758621">
In the example in Figure 1, the Hypothesis Forma-
tion phase produces three alternative hypotheses.
The hypotheses are ranked using a trained evalua-
tion function, but the hypothesis ranked first is not
guaranteed to be best. In this case, the hypothe-
sis ranked as second is the best hypothesis. The
hypotheses are expressed in a frame-based feature
structure representation. Above each hypothesis is
the corresponding text generated by the system for
the associated feature structure.
In order for the system to return the correct hy-
pothesis, it must use interaction to narrow down the
list of alternatives to the best single one. The first
task of the Interaction Mechanism is to determine
what the system knows about what the speaker has
said and what it is not certain about. It does this
by comparing the top set of repair hypotheses and
extracting a set of features that distinguish them
from one another. The set of distinguishing features
corresponding to the example set of alternative hy-
potheses can be found in Figure 2.
The meaning representation&apos;s recursive structure
is made up of frames with slots that can be filled ei-
ther with other frames or with atomic fillers. These
compositional structures can be thought of as trees,
with the top level frame being the root of the tree
and branches attached through slots. The features
Sentence: What did you say &apos;bout what
was your schedule for the twenty sixth of May?
</bodyText>
<figure confidence="0.9530090625">
Alternative Hypotheses:
&amp;quot;What will be scheduled for the twenty-
sixth of May&amp;quot;
((frame *schedule)
(what ((frame *what)(wh +)))
(when ((frame *simple-time)(day 26)(month 5))))
&amp;quot;You will schedule what for the twenty-
sixth of May?&amp;quot;
((frame *schedule)
(who ((frame *you)))
(what ((frame *what)(wh +)))
(when ((frame *simple-time)(day 26)(month 5))))
&amp;quot;Will schedule for the twenty-sixth
of May&amp;quot;
((frame *schedule)
(when ((frame *simple-time)(day 26)(month 5))))
</figure>
<figureCaption confidence="0.997119">
Figure 1: Example Alternative Hypotheses
</figureCaption>
<figure confidence="0.992605555555556">
((f *schedule)(s who))
((f *you))
((f *schedule)(s what))
((f *schedule)(s what)(f *what))
((f *what))
((f +))
((f *what)(s wh)(f +))
((f *schedule)(s who)(f *you))
((f *schedule)(s what)(f *what)(s wh)(f +))
</figure>
<figureCaption confidence="0.999517">
Figure 2: Distinguishing Features
</figureCaption>
<bodyText confidence="0.9999373125">
used in the system to distinguish alternative mean-
ing representation structures from one another spec-
ify paths down this tree structure. Thus, the distin-
guishing features that are extracted are always an-
chored in a frame or atomic filler, marked by an f
in Figure 2. Within a feature, a frame may be fol-
lowed by a slot, marked by an s. And a slot may
be followed by a frame or atomic filler, and so on.
These features are generated by comparing the set.
of feature structures returned from the Hypothesis
Formation phase. No knowledge about what the fea-
tures mean is needed in order to generate or use
these features. Thus, the feature based approach
is completely domain independent. It can be used
without modification with any frame-based meaning
representation.
</bodyText>
<page confidence="0.94737">
1130
</page>
<bodyText confidence="0.999896178571429">
When a feature is applied to a meaning repre-
sentation structure, a value is obtained. Thus, fea-
tures can be used to assign meaning representation
structures to classes according to what value is ob-
tained for each when the feature is applied. For
example, the feature ((f *schedule) (s who) (f
*you)), distinguishes structures that contain the
filler *you in the who slot in the *schedule frame
from those that do not. When it is applied to struc-
tures that contain the specified frame in the specified
slot, it returns true. When it is applied to structures
that do not, it returns false. Thus, it groups the
first and third hypotheses in one class, and the sec-
ond hypothesis in another class. Because the value
of a feature that ends in a frame or atomic filler
can have either true or false as its value, these are
called yes/no features. When a feature that ends in
a slot, such as ( (f *schedule) (s who)), is applied
to a feature structure, the value is the filler in the
specified slot. These features are called wh-features.
Each feature is associated with a question that
the system could ask the user. The purpose of the
generated question is to determine what the value
of the feature should be. The system can then keep
those hypotheses that are consistent with that fea-
ture value and eliminate from consideration the rest.
Generating a natural language question from a fea-
ture is discussed in section 2.3.
</bodyText>
<subsectionHeader confidence="0.999959">
2.2 Selecting a Feature
</subsectionHeader>
<bodyText confidence="0.999951625">
Once a set of features is extracted, the system en-
ters a loop in which it selects a feature from the
list, generates a query, and then updates the list of
alternative hypotheses and remaining distinguishing
features based on the user&apos;s response. It attempts
to ask the most informative questions first in order
to limit the number of necessary questions. It uses
the following four criteria in making its selection:
</bodyText>
<listItem confidence="0.99357775">
• Askable: Is it possible to ask a natural ques-
tion from it?
• Evaluatable: Does it ask about a single repair
or set of repairs that always occur together?
• In Focus: Does it involve information from the
common ground?
• Most Informative: Is it likely to result in the
greatest search space reduction?
</listItem>
<bodyText confidence="0.999849046511628">
First, the set of features is narrowed down to those
features that represent askable questions. For exam-
ple, it is not natural to ask about the filler of a par-
ticular slot in a particular frame if it is not known
whether the ideal meaning representation structure
contains that frame. Also, it is awkward to gen-
erate a wh-question based on a feature of length
greater than two. For example, a question corre-
sponding to ((f *how) (s what) (f *interval) (s
end)) might be phrased something like &amp;quot;flow is the
time ending when?&amp;quot;. So even-lengthed features more
than two elements long are also eliminated at this
stage.
The next criterion considered by the Interaction
phase is evaluatability. In order for a Yes/No ques-
tion to be evaluatable, it must confirm only a single
repair action. Otherwise, if the user responds with
&amp;quot;No&amp;quot; it cannot be determined whether the user is
rejecting both repair actions or only one of them.
Next, the set of features is narrowed down to those
that can easily be identified as being in focus. In or-
der to do this, the system prefers to use features
that overlap with structures that all of the alter-
native hypotheses have in common. Thus, the sys-
tem encodes as much common ground knowledge in
each question as possible. The structures that all
of the alternative hypotheses share are called non-
controversial substructures. As the negotiation con-
tinues, these tend to be structures that have been
confirmed through interaction. Including these sub-
structures has the effect of having questions tend
to follow in a natural succession. It also has the
other desirable effect that the system&apos;s state of un-
derstanding the speaker&apos;s sentence is indicated to
the speaker.
The final piece of information used in selecting
between those remaining features is the expected
search reduction. The expected search reduction in-
dicates how much the search space can be expected
to be reduced once the answer to the corresponding
question is obtained from the user. Equation 1 is
for calculating Sr , the expected search reduction of
feature number f.
</bodyText>
<sectionHeader confidence="0.555869" genericHeader="method">
11 f
</sectionHeader>
<bodyText confidence="0.472169">
f
</bodyText>
<equation confidence="0.558903">
= La( ) X (L —l1) (1)
</equation>
<bodyText confidence="0.99985195">
L is the number of alternative hypotheses. As
mention above, each feature can be used to assign
the hypotheses to equivalence classes. /i,f is the
number of alternative hypotheses in the ith equiv-
alence class of feature f. If the value for feature f
associated with the class of length /i,f is the correct
value, /i,f will be the new size of the search space.
In this case, the actual search reduction will be the
current number of hypotheses, L, minus the number
of alternative hypotheses in the resulting set, /i,f.
Intuitively, the expected search reduction of a fea-
ture is the sum over all of a feature&apos;s equivalence
classes of the percentage of hypotheses in that class
times the reduction in the search space assuming the
associated value for that feature is correct.
The first three criteria select a subset of the cur-
rent distinguishing features which the final crite-
rion then ranks. Note that all of these criteria can
be evaluated without the system having any under-
standing about what the features actually mean.
</bodyText>
<page confidence="0.963462">
1131
</page>
<table confidence="0.9928182">
Selected Feature:
((f *schedule)(s wha.t)(f *what)(s wh)(f +))
Non-controversial Structures if Answer
to Question is Yes:
((when ((month 5)(day 26)(frame *simple-time)))
(frame *schedule)
(what ((wh +)(frame *what))))
Question Structure:
((when ((month 5)(day 26)(frame *simple-time)))
(frame *schedule)
(what ((wh +)(frame *what))))
Question Text:
Was something like WHAT WILL BE
SCHEDULED FOR THE TWENTY-SIXTH
OF MAY part of what you meant?
</table>
<figureCaption confidence="0.4800515">
Figure 3: Query Text Generation
2.3 Generating Query Text
</figureCaption>
<bodyText confidence="0.99998624137931">
The selected feature is used to generate a query for
the user. First, a skeleton structure is built from
the feature, with top level frame equivalent to the
frame at the root of the feature. Then the skeleton
structure is filled out with the non-controversial sub-
structures. If the question is a Yes/No question, it
includes all of the substructures that would be non-
controversial assuming the answer to the question
is &amp;quot;Yes&amp;quot;. Since information confirmed by the pre-
vious question is now considered non-controversial,
the result of the previous interaction is made evident
in how the current question is phrased. An exam-
ple of a question generated with this process can be
found in Figure 3.
If the selected feature is a wh-feature, i.e., if it is
an even lengthed feature, the question is generated
in the form of a wh-question. Otherwise the text
is generated declaratively and the generated text is
inserted into the following formula: &amp;quot;Was something
like XXX part of what you meant?&amp;quot;, where XXX is
filled in with the generated text. The set of alter-
native answers based on the set of alternative hy-
potheses is presented to the user. For wh-questions,
a final alternative, &amp;quot;None of these alternatives are
acceptable&amp;quot;, is made available. Again, no particu-
lar domain knowledge is necessary for the purpose
of generating query text from features since the sen-
tence level generation component from the system
can be used as is.
</bodyText>
<subsectionHeader confidence="0.972582">
2.4 Processing the User&apos;s Response
</subsectionHeader>
<bodyText confidence="0.999739666666667">
Once the user has responded with the correct value
for the feature, only the alternative hypotheses that
have that value for that feature are kept, and the rest
</bodyText>
<figure confidence="0.910616909090909">
&amp;quot;What will be scheduled for the twenty-
sixth of May&amp;quot;
((what ((frame *what)(wh +)))
(when ((frame *simple-time)(day 26)(month 5)))
(frame *schedule))))
&amp;quot;You will schedule what for the twenty-
sixth of May?&amp;quot;
((what ((frame *what)(wh +)))
(frame *schedule)
(when ((frame *simple-time)(day 26)(month 5)))
(who ((frame *you))))
</figure>
<figureCaption confidence="0.993577">
Figure 4: Remaining Hypotheses
</figureCaption>
<figure confidence="0.846410666666667">
((f *schedule)(s who))
((f *you))
((f *schedule)(s who)(f *you))
</figure>
<figureCaption confidence="0.999866">
Figure 5: Remaining Distinguishing Features
</figureCaption>
<bodyText confidence="0.9999319">
are eliminated. In the case of a wh-question, if the
user selects &amp;quot;None of these alternatives are accept-
able&amp;quot;, all of the alternative hypothesized structures
are eliminated and a rephrase is requested. After
this step, all of the features that no longer parti-
tion the search space into equivalence classes are also
eliminated. In the example, assume the answer to
the generated question in Figure 3 was &amp;quot;Yes&amp;quot;. Thus,
the result is that two of the original three hypothe-
ses are remaining, displayed in Figure 4, and the re-
maining set of features that still partition the search
space can be found in Figure 5.
If one or more distinguishing features remain, the
cycle begins again by selecting a feature, generating
a question, and so on until the system narrows down
to the final result. If the user does not answer posi-
tively to any of the system&apos;s questions by the time it
runs out of distinguishing features regarding a par-
ticular sentence, the system loses confidence in its
set of hypotheses and requests a rephrase.
</bodyText>
<sectionHeader confidence="0.96633" genericHeader="method">
3 Using Discourse Information
</sectionHeader>
<bodyText confidence="0.999924818181818">
Though discourse processing is not essential to
the ROSE approach, discourse information has
been found to be useful in robust interpretation
(Ramshaw, 1994; Smith, 1992). In this section we
discuss how discourse information can be used for
focusing the interaction between system and user on
the task level rather than on the literal meaning of
the user&apos;s utterance.
A plan-based discourse processor (Rose et al.,
1995) provides contextual expectations that guide
the system in the manner in which it formulates
</bodyText>
<page confidence="0.996008">
1132
</page>
<figure confidence="0.896882148148148">
Sentence: What about any time but the ten to twelve
slot on Tuesday the thirtieth?
Hypothesis 1:
&amp;quot;How about from ten o&apos;clock till twelve o&apos;clock
Tuesday the thirtieth any time&amp;quot;
((frame *how)
(when (*multiple*
((end ((frame *simple-time) (hour 12)))
(start ((frame *simple-time) (hour 10)))
(incl-excl inclusive)
(frame *interval))
((frame *simple-time)
(day 30)
(day-of-week tuesday))
((specifier any) (name time)
(frame *special-time)))))
Hypothesis 2:
&amp;quot;From ten o&apos;clock till Tuesday the thirtieth
twelve o&apos;clock&amp;quot;
((frame *interval)
(incl-excl inclusive)
(start ((frame *simple-time) (hour 10)))
(end (*multiple*
((frame *simple -time)
(day 30)
(day-of-week tuesday))
((frame *simple-time) (hour 12)))))
</figure>
<subsectionHeader confidence="0.541309">
Selected Feature: ((f *how)(s when)(f *interval))
</subsectionHeader>
<bodyText confidence="0.918043333333333">
Query Without discourse: Was something like
&amp;quot;how about from ten o&apos;clock till twelve &apos;clock&amp;quot; part
of what you meant?
Query With discourse: Are you suggesting that
Tuesday November the thirtieth from ten a.m. till
twelve a.m. is a good time to meet?
</bodyText>
<figureCaption confidence="0.999629">
Figure 6: Modified Question Generation
</figureCaption>
<bodyText confidence="0.99992909859155">
queries to the user. By computing a structure for
the dialogue, the discourse processor is able to iden-
tify the speech act performed by each sentence. Ad-
ditionally, it augments temporal expressions from
context. Based on this information, it computes
the constraints on the speaker&apos;s schedule expressed
by each sentence. Each constraint associates a sta-
tus with a particular speaker&apos;s schedule for time
slots within the time indicated by the temporal
expression. There are seven possible statuses, in-
cluding accepted, suggested, preferred, neutral,
dispref erred, busy, and rejected.
As discussed above, the Interaction Mechanism
uses features that distinguish between alternative
hypotheses to divide the set of alternative repair hy-
potheses into classes. Each member within the same
class has the same value for the associated feature.
By comparing computed status and augmented tem-
poral information for alternative repair hypotheses
within the same class, it is possible to determine
what common implications for the task each member
or most of the members in the associated class have.
Thus, it is possible to compute what implications
for the task are associated with the corresponding
value for the feature. By comparing this common in-
formation across classes, it is possible to determine
whether the feature makes a consistent distinction
on the task level. If so, it is possible to take this
distinguishing information and use it for refocusing
the associated question on the task level rather than
on the level of the sentence&apos;s literal meaning.
In the example in Figure 6, the parser is not able
to correctly process the &amp;quot;but&amp;quot;, causing it to miss
the fact that the speaker intended any other time
besides ten to twelve rather than particularly ten to
twelve. Two alternative hypotheses are constructed
during the Hypothesis Formation phase. However,
neither hypothesis correctly represents the meaning
of the sentence. In this case, the purpose of the
interaction is to indicate to the system that neither
of the hypotheses are correct and that a rephrase is
needed. This will be accomplished when the user
answers negatively to the system&apos;s query since the
user will not have responded positively to any of the
system&apos;s queries regarding this sentence.
The system selects the feature ( (f *how) (s
when) (f *interval)) to distinguish the two hy-
potheses from one another. Its generated query is
thus &amp;quot;Was something like HOW ABOUT FROM
TEN OCLOCK TILL TWELVE °CLOCK part of
what you meant?&amp;quot;. The discourse processor returns
a different result for each of these two representa-
tions. In particular, only the first hypothesis con-
tains enough information for the discourse proces-
sor to compute any scheduling constraints since it
contains both a temporal expression and a top level
semantic frame. It would create a constraint associ-
ating the status of suggested with a representation
for Tuesday the thirtieth from ten o&apos;clock till twelve
o&apos;clock. The other hypothesis contains date infor-
mation but no status information. Based on this
difference, the system can generate a query asking
whether or not the user expressed this constraint. Its
query is &amp;quot;Are you -suggesting that Tuesday, Novem-
ber the thirtieth from ten a.m. till twelve a.m. is a
good time to meet?&amp;quot; The suggested status is as-
sociated with a template that looks like &amp;quot;Are you
suggesting that XXX is a good time to meet?&amp;quot; The
XXX is then filled in with the text generated from
the temporal expression using the regular system
generation grammar.
</bodyText>
<sectionHeader confidence="0.997272" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.904966">
An empirical evaluation was conducted in or-
der to determine how much improvement can be
gained with limited amounts of interaction in the
</bodyText>
<page confidence="0.927123">
1133
</page>
<table confidence="0.9977345">
Bad Okay Perfect Total Acceptable
Parser 85.0% 12.0% 3.0% 15.0%
Top Hypothesis 64.0% 28.0% 8.0% 36.0%
1 Question 61.39% 28.71% 9.9% 38,61
2 Questions 59.41% 28.71% 11.88% 40.59%
3 Questions 53.47% 32.67% 13.86% 46.53%
</table>
<figureCaption confidence="0.99814">
Figure 7: Translation Quality As Maximum Number of Questions Increases
</figureCaption>
<bodyText confidence="0.999712272727273">
domain independent ROSE approach. This evalu-
ation is an end-to-end evaluation where a sentence
expressed in the source language is parsed into a
language independent meaning representation using
the ROSE approach. This meaning representation
is then mapped onto a sentence in the target lan-
guage. In this case both the source language and
the target language are English. An additional eval-
uation demonstrates the improvement in interaction
quality that can be gained by introducing available
domain information.
</bodyText>
<subsectionHeader confidence="0.993062">
4.1 Domain Independent Repair
</subsectionHeader>
<bodyText confidence="0.999989015151515">
First. the system automatically selected 100 sen-
tences from a previously unseen corpus of 500 sen-
tences. These 100 sentences are the first 100 sen-
tences in the set that a parse quality heuristic sim-
ilar to that described in (Lavie, 1995) indicated to
be of low quality. The parse quality heuristic evalu-
ates how much skipping was necessary in the parser
in order to arrive at a partial parse and how well
the parser&apos;s analysis scores statistically. It should
be kept in mind, then, that this testing corpus is
composed of 100 of the most difficult sentences from
the original corpus.
The goal of the evaluation was to compute aver-
age performance per question asked and to compare
this with the performance with using only the partial
parser as well as with using only the Hypothesis For-
mation phase. In each case performance was mea-
sured in terms of a. translation quality score assigned
by an independent human judge to the generated
natural language target text. Scores of Bad, Okay,
and Perfect were assigned. A score of Bad indicates
that the translation does not capture the original
meaning of the input sentence. Okay indicates that
the translation captures the meaning of the -input
sentence, but not in a completely natural manner.
Perfect indicates both that the resulting translation
captures the meaning of the original sentence and
that it does so in a smooth and fluent manner.
Eight native speakers of English who had never
previously used the translation system participated
in this evaluation to interact with the system. For
each sentence, the participants were presented with
the original sentence and with three or fewer ques-
tions to answer. The parse result, the result of re-
pair without interaction, and the result for each user
after each question were recorded in order to be
graded later by the independent judge mentioned
above. Note that this evaluation was conducted on
the nosiest portion of the corpus, not on an aver-
age set of naturally occurring utterances. While this
evaluation indicates that repair without interaction
yields an acceptable result in only 36% of these dif-
ficult cases, in an evaluation over the entire corpus,
it. was determined to return an acceptable result in
78% of the cases.
A global parameter was set such that the system
never asked more than a maximum of three ques-
tions. This limitation was placed on the system in
order to keep the task from becoming too tedious
and time consuming for the users. It was estimated
that three questions was approximately the maxi-
mum number of questions that users would be will-
ing to answer per sentence.
The results are presented in Figure 7. Repair
without interaction achieves a 25% reduction in er-
ror rate. Since the partial parser only produced suf-
ficient chunks for building an acceptable repair hy-
pothesis in about 26% of the cases where it did not
produce an acceptable hypothesis by itself, the max-
imum reduction in error rate was 26%. Thus, a 25%
reduction in error rate without interaction is a very
positive result. Additionally, interaction increases
the system&apos;s average translation quality above that
of repair without interaction. With three questions,
the system achieves a 37% reduction in error rate
over partial parsing alone.
</bodyText>
<subsectionHeader confidence="0.993221">
4.2 Discourse Based Interaction
</subsectionHeader>
<bodyText confidence="0.9997484">
In a final evaluation, the quality of questions based
only on feature information was compared with that
of questions focused on the task level using discourse
information. The discourse processor was only able
to provide sufficient information for reformulating
22% of the questions in terms of the task. The rea-
son is that this discourse processor only provides in-
formation for reformulating questions distinguishing
between meaning representations that differ in terms
of status and augmented temporal information.
Four independent human judges were asked to
grade pairs of questions, assigning a score between
1 and 5 for relevance and form and indicating which
question they would prefer to answer. They were
instructed to think of relevance in terms of how use-
</bodyText>
<page confidence="0.984994">
1134
</page>
<bodyText confidence="0.999970947368421">
ful they expected the question would be in helping a
computer understand the sentence the question was
intended to clarify. For form, they were instructed
to evaluate how natural and smooth sounding the
generated question was.
Interaction without discourse received on average
2.7 for form and 2.4 for relevance. Interaction with
discourse, on the other hand, received 4.1 for form
and 3.7 for relevance. Subjects preferred the dis-
course influenced question in 73.6% of the cases, ex-
pressed no preference in 14.8% of the cases, and pre-
ferred interaction without discourse in 11.6% of the
cases. Though the discourse influenced question was
not preferred universely, this evaluation supports the
claim that. humans prefer to receive clarifications on
the task level and indicates that further exploration
in using discourse information in repair, and partic-
ularly in interaction, is a promising avenue for future
research.
</bodyText>
<sectionHeader confidence="0.983552" genericHeader="conclusions">
5 Conclusions and Current
Directions
</sectionHeader>
<bodyText confidence="0.999997">
This paper presents a domain independent, interac-
tive approach to robust interpretation. Where other
interactive approaches to robust interpretation have
depended upon domain dependent repair rules, the
approach described here operates efficiently without
any such hand-coded repair knowledge. An empir-
ical evaluation demonstrates that limited amounts
of focused interaction allow this repair approach to
achieve a 37% reduction in error rate over a corpus of
noisy sentences. A further evaluation demonstrates
that this domain independent approach combines
easily with available domain knowledge in order to
improve the quality of the interaction. Introducing
discourse information yields a preferable query in
74% of the cases where discourse information ap-
plies. Interaction in the current ROSE approach
is limited to confirming hypotheses about how the
fragments of the partial parse can be combined and
requesting rephrases. It would be interesting to gen-
erate and test hypotheses about information missing
from the partial parse, perhaps using information
predicted by the discourse context.
</bodyText>
<sectionHeader confidence="0.998138" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992548803030303">
H. H. Clark and E. F. Schaefer. 1989. Contributing
to discourse. Cognitive Science, 13:259-294.
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring
as a collaborative process. Cognition, 22:1-39.
M. Danieli and E. Gerbino. 1995. Metrics for evalu-
ating dialogue strategies in a spoken language sys-
tem. In Working Notes of the AAAI Spring Sym-
posium on Empirical Methods in Discourse Inter-
pretation and Generation.
E. Hatch. 1983. Simplified input and second
language acquisition. In R. Andersen, editor,
Pidginization and Creolization as Language Ac-
quisition. Newbury House Publishers.
D. R. Hipp. 1992. Design and Development of
Spoken Natural-Language Dialog Parsing Systems.
Ph.D. thesis, Dept. of Computer Science, Duke
University.
J. Koza. 1992. Genetic Programming: On the Pro-
gramming of Computers by Means of Natural Se-
lection. MIT Press.
J. Koza. 1994. Genetic Programming II. MIT Press.
A. Lavie, D. Gates, M. Gayalda, L. Mayfield, and
A. Waibel L. Levin. 1996. Multi-lingual transla-
tion of spontaneously spoken language in a limited
domain. In Proceedings of COLING 96, Kopen-
hagen.
A. Lavie. 1995. A Grammar Based Robust Parser
For Spontaneous Speech. Ph.D. thesis, School of
Computer Science, Carnegie Mellon University.
J. F. Lehman. 1989. Adaptive Parsing: Self-
Extending Natural Language Interfaces. Ph.D.
thesis, School of Computer Science, Carnegie Mel-
lon University.
L. A. Ramshaw. 1994. Correcting real-world
spelling errors using a model of the problem-
solving context. Computational Intelligence,
10(2).
C. P. Rose and A. Lavie. 1997. An efficient distribu-
tion of labor in a two stage robust interpretation
process. In Proceedings of the Second Conference
on Empirical Methods in Natural Language Pro-
cessing.
C. P. Rosé and A. Waibel. 1994. Recovering from
parser failures: A hybrid statistical/symbolic ap-
proach. In Proceedings of The Balancing Act:
Combining Symbolic and Statistical Approaches to
Language workshop at the 32nd Annual Meeting of
the ACL.
C. P. Rose, B. Di Eugenio, L. S. Levin, and C. Van
Ess-Dykema. 1995. Discourse processing of dia-
logues with multiple threads. In Proceedings of
the ACL.
C. P. Rose. 1997. Robust Interactive Dialogue Inter-
pretation. Ph.D. thesis, School of Computer Sci-
ence, Carnegie Mellon University.
R. Smith. 1992. A Computational Model of
Expectation-Driven Mixed-Initiative Dialog Pro-
cessing. Ph.D. thesis, CS Dept., Duke University.
M. Tomita and E. H. Nyberg. 1988. Generation kit
and transformation kit version 3.2: User&apos;s man-
ual. Technical Report CMU-CMT-88-MEMO,
Carnegie Mellon University, Pittsburgh, PA.
G. Van Noord. 1996. Robust parsing with the head-
corner parser. In Proceedings of the Eight Euro-
pean Summer School In Logic, Language and In-
formation, Prague, Czech Republic.
</reference>
<page confidence="0.994413">
1135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.782153">
<title confidence="0.998816">An Interactive Domain Independent Approach to Robust Dialogue Interpretation</title>
<author confidence="0.986593">Carolyn Penstein Rosé</author>
<affiliation confidence="0.979129">LRDC 520, University of Pittsburgh</affiliation>
<address confidence="0.9799705">3939 Ohara St., Pittsburgh PA, 15260</address>
<email confidence="0.856718">rosecp@pitt.edu</email>
<author confidence="0.997811">Lori S Levin</author>
<affiliation confidence="0.9964825">Carnegie Mellon University Center for Machine Translation</affiliation>
<address confidence="0.998647">Pittsburgh, PA 15213</address>
<email confidence="0.998856">lslOcs.cmu.edu</email>
<abstract confidence="0.999248222222222">We discuss an interactive approach to robust interpretation in a large scale speech-to-speech translation system. Where other interactive approaches to robust interpretation have depended upon domain dependent repair rules, the approach described here operates efficiently without any such hand-coded repair knowledge and yields a 37% reduction in error rate over a corpus of noisy sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>E F Schaefer</author>
</authors>
<title>Contributing to discourse.</title>
<date>1989</date>
<journal>Cognitive Science,</journal>
<pages>13--259</pages>
<contexts>
<context position="4786" citStr="Clark and Schaefer, 1989" startWordPosition="735" endWordPosition="738">in order to avoid requiring any hand-crafted repair rules. Our genetic programming approach has been shown previously to be orders of magnitude more efficient than the minimum distance parsing approach (Rose and Lavie, 1997). In the second phase, Interaction with 1129 the user, the system generates a set of queries, negotiating with the speaker in order to narrow down to a single best meaning representation hypothesis. Or, if it determines based on the user&apos;s responses to its queries that none of its hypotheses are acceptable, it requests a rephrase. Inspired by (Clark and Wilkes-Gibbs, 1986; Clark and Schaefer, 1989), the goal of the Interaction Phase is to minimize collaborative effort between the system and the speaker while maintaining a high level of interpretation accuracy. It uses this principle in determining which portions of the speaker&apos;s utterance to question. Thus, it focuses its interaction on those portions of the speaker&apos;s meaning that it is particularly uncertain about. In its questioning, it attempts to display the state of the system&apos;s understanding, acknowledging information conveyed by the speaker as it becomes clear. The interaction process can be summarized as follows: The system firs</context>
</contexts>
<marker>Clark, Schaefer, 1989</marker>
<rawString>H. H. Clark and E. F. Schaefer. 1989. Contributing to discourse. Cognitive Science, 13:259-294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>D Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.</title>
<date>1986</date>
<journal>Cognition,</journal>
<pages>22--1</pages>
<contexts>
<context position="4759" citStr="Clark and Wilkes-Gibbs, 1986" startWordPosition="731" endWordPosition="734">ways to combine the fragments in order to avoid requiring any hand-crafted repair rules. Our genetic programming approach has been shown previously to be orders of magnitude more efficient than the minimum distance parsing approach (Rose and Lavie, 1997). In the second phase, Interaction with 1129 the user, the system generates a set of queries, negotiating with the speaker in order to narrow down to a single best meaning representation hypothesis. Or, if it determines based on the user&apos;s responses to its queries that none of its hypotheses are acceptable, it requests a rephrase. Inspired by (Clark and Wilkes-Gibbs, 1986; Clark and Schaefer, 1989), the goal of the Interaction Phase is to minimize collaborative effort between the system and the speaker while maintaining a high level of interpretation accuracy. It uses this principle in determining which portions of the speaker&apos;s utterance to question. Thus, it focuses its interaction on those portions of the speaker&apos;s meaning that it is particularly uncertain about. In its questioning, it attempts to display the state of the system&apos;s understanding, acknowledging information conveyed by the speaker as it becomes clear. The interaction process can be summarized </context>
</contexts>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition, 22:1-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Danieli</author>
<author>E Gerbino</author>
</authors>
<title>Metrics for evaluating dialogue strategies in a spoken language system.</title>
<date>1995</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation.</booktitle>
<contexts>
<context position="1112" citStr="Danieli and Gerbino, 1995" startWordPosition="153" endWordPosition="156">ended upon domain dependent repair rules, the approach described here operates efficiently without any such hand-coded repair knowledge and yields a 37% reduction in error rate over a corpus of noisy sentences. 1 Introduction In this paper we discuss ROSE, an interactive approach to robust interpretation developed in the context of the JANUS speech-to-speech translation system (Lavie et al., 1996). Previous interactive approaches to robust interpretation have either required excessive amounts of interaction (Rosé and Waibel, 1994), depended upon domain dependent repair rules (Van Noord, 1996; Danieli and Gerbino, 1995), or relied on the minimum distance parsing approach (Hipp, 1992; Smith, 1992; Lehman, 1989) which has been shown to be intractable in a largescale system (Rose and Lavie, 1997). In contrast, the ROSE approach operates efficiently without any hand-coded repair knowledge. An empirical evaluation demonstrates the efficacy of this domain independent approach. A further evaluation demonstrates that the ROSE approach combines easily with available domain knowledge in order to improve the quality of the interaction. The ROSE approach is based on a model of human communication between speakers of dif</context>
</contexts>
<marker>Danieli, Gerbino, 1995</marker>
<rawString>M. Danieli and E. Gerbino. 1995. Metrics for evaluating dialogue strategies in a spoken language system. In Working Notes of the AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hatch</author>
</authors>
<title>Simplified input and second language acquisition. In</title>
<date>1983</date>
<booktitle>Pidginization and Creolization as Language Acquisition.</booktitle>
<editor>R. Andersen, editor,</editor>
<publisher>Newbury House Publishers.</publisher>
<contexts>
<context position="1976" citStr="Hatch, 1983" startWordPosition="296" endWordPosition="297">d repair knowledge. An empirical evaluation demonstrates the efficacy of this domain independent approach. A further evaluation demonstrates that the ROSE approach combines easily with available domain knowledge in order to improve the quality of the interaction. The ROSE approach is based on a model of human communication between speakers of different languages with a small shared language base. Humans who share a very small language base are able to communicate when the need arises by simplifying their speech patterns and negotiating until they manage to transmit their ideas to one another (Hatch, 1983). As the speaker is speaking, the listener &amp;quot;casts his net&amp;quot; in order to catch those fragments of speech that are comprehensible to him, which he then attempts to fit together semantically. His subsequent negotiation with the speaker builds upon this partial understanding. Similarly, ROSE repairs extragrammatical input in two phases. The first phase, Repair Hypothesis Formation, is responsible for assembling a set of hypotheses about the meaning of the ungrammatical utterance. In the second phase, Interaction with the User, the systern generates a set of queries, negotiating with the speaker in </context>
</contexts>
<marker>Hatch, 1983</marker>
<rawString>E. Hatch. 1983. Simplified input and second language acquisition. In R. Andersen, editor, Pidginization and Creolization as Language Acquisition. Newbury House Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Hipp</author>
</authors>
<title>Design and Development of Spoken Natural-Language Dialog Parsing Systems.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Dept. of Computer Science, Duke University.</institution>
<contexts>
<context position="1176" citStr="Hipp, 1992" startWordPosition="165" endWordPosition="166">fficiently without any such hand-coded repair knowledge and yields a 37% reduction in error rate over a corpus of noisy sentences. 1 Introduction In this paper we discuss ROSE, an interactive approach to robust interpretation developed in the context of the JANUS speech-to-speech translation system (Lavie et al., 1996). Previous interactive approaches to robust interpretation have either required excessive amounts of interaction (Rosé and Waibel, 1994), depended upon domain dependent repair rules (Van Noord, 1996; Danieli and Gerbino, 1995), or relied on the minimum distance parsing approach (Hipp, 1992; Smith, 1992; Lehman, 1989) which has been shown to be intractable in a largescale system (Rose and Lavie, 1997). In contrast, the ROSE approach operates efficiently without any hand-coded repair knowledge. An empirical evaluation demonstrates the efficacy of this domain independent approach. A further evaluation demonstrates that the ROSE approach combines easily with available domain knowledge in order to improve the quality of the interaction. The ROSE approach is based on a model of human communication between speakers of different languages with a small shared language base. Humans who s</context>
</contexts>
<marker>Hipp, 1992</marker>
<rawString>D. R. Hipp. 1992. Design and Development of Spoken Natural-Language Dialog Parsing Systems. Ph.D. thesis, Dept. of Computer Science, Duke University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Koza</author>
</authors>
<title>Genetic Programming: On the Programming of Computers by Means of Natural Selection.</title>
<date>1992</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4076" citStr="Koza, 1992" startWordPosition="622" endWordPosition="623">nked set of ten or fewer hypotheses about the meaning of the ungrammatical utterance expressed in the source language. This phase is itself divided into two stages, Partial Parsing and Combination. The Partial Parsing stage is similar to the concept of the listener &amp;quot;casting his net&amp;quot; for comprehensible fragments of speech. A robust skipping parser (Lavie, 1995) is used to obtain an analysis for islands of the speaker&apos;s sentence. In the Combination stage, the fragments from the partial parse are assembled into a ranked set of alternative meaning representation hypotheses. A genetic programming (Koza, 1992; Koza, 1994) approach is used to search for different ways to combine the fragments in order to avoid requiring any hand-crafted repair rules. Our genetic programming approach has been shown previously to be orders of magnitude more efficient than the minimum distance parsing approach (Rose and Lavie, 1997). In the second phase, Interaction with 1129 the user, the system generates a set of queries, negotiating with the speaker in order to narrow down to a single best meaning representation hypothesis. Or, if it determines based on the user&apos;s responses to its queries that none of its hypothese</context>
</contexts>
<marker>Koza, 1992</marker>
<rawString>J. Koza. 1992. Genetic Programming: On the Programming of Computers by Means of Natural Selection. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Koza</author>
</authors>
<title>Genetic Programming II.</title>
<date>1994</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4089" citStr="Koza, 1994" startWordPosition="624" endWordPosition="625">ten or fewer hypotheses about the meaning of the ungrammatical utterance expressed in the source language. This phase is itself divided into two stages, Partial Parsing and Combination. The Partial Parsing stage is similar to the concept of the listener &amp;quot;casting his net&amp;quot; for comprehensible fragments of speech. A robust skipping parser (Lavie, 1995) is used to obtain an analysis for islands of the speaker&apos;s sentence. In the Combination stage, the fragments from the partial parse are assembled into a ranked set of alternative meaning representation hypotheses. A genetic programming (Koza, 1992; Koza, 1994) approach is used to search for different ways to combine the fragments in order to avoid requiring any hand-crafted repair rules. Our genetic programming approach has been shown previously to be orders of magnitude more efficient than the minimum distance parsing approach (Rose and Lavie, 1997). In the second phase, Interaction with 1129 the user, the system generates a set of queries, negotiating with the speaker in order to narrow down to a single best meaning representation hypothesis. Or, if it determines based on the user&apos;s responses to its queries that none of its hypotheses are accepta</context>
</contexts>
<marker>Koza, 1994</marker>
<rawString>J. Koza. 1994. Genetic Programming II. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>D Gates</author>
<author>M Gayalda</author>
<author>L Mayfield</author>
<author>A Waibel L Levin</author>
</authors>
<title>Multi-lingual translation of spontaneously spoken language in a limited domain.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING 96,</booktitle>
<location>Kopenhagen.</location>
<contexts>
<context position="886" citStr="Lavie et al., 1996" startWordPosition="122" endWordPosition="125">gh, PA 15213 lslOcs.cmu.edu Abstract We discuss an interactive approach to robust interpretation in a large scale speech-to-speech translation system. Where other interactive approaches to robust interpretation have depended upon domain dependent repair rules, the approach described here operates efficiently without any such hand-coded repair knowledge and yields a 37% reduction in error rate over a corpus of noisy sentences. 1 Introduction In this paper we discuss ROSE, an interactive approach to robust interpretation developed in the context of the JANUS speech-to-speech translation system (Lavie et al., 1996). Previous interactive approaches to robust interpretation have either required excessive amounts of interaction (Rosé and Waibel, 1994), depended upon domain dependent repair rules (Van Noord, 1996; Danieli and Gerbino, 1995), or relied on the minimum distance parsing approach (Hipp, 1992; Smith, 1992; Lehman, 1989) which has been shown to be intractable in a largescale system (Rose and Lavie, 1997). In contrast, the ROSE approach operates efficiently without any hand-coded repair knowledge. An empirical evaluation demonstrates the efficacy of this domain independent approach. A further evalu</context>
</contexts>
<marker>Lavie, Gates, Gayalda, Mayfield, Levin, 1996</marker>
<rawString>A. Lavie, D. Gates, M. Gayalda, L. Mayfield, and A. Waibel L. Levin. 1996. Multi-lingual translation of spontaneously spoken language in a limited domain. In Proceedings of COLING 96, Kopenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
</authors>
<title>A Grammar Based Robust Parser For Spontaneous Speech.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="3828" citStr="Lavie, 1995" startWordPosition="583" endWordPosition="584">ails about the Hypothesis Formation phase are found in (Rose, 1997). 2 Interactive Repair In Depth As mentioned above, ROSE repairs extragrammatical input in two phases. The first phase, Repair Hypothesis Formation, is responsible for assembling a ranked set of ten or fewer hypotheses about the meaning of the ungrammatical utterance expressed in the source language. This phase is itself divided into two stages, Partial Parsing and Combination. The Partial Parsing stage is similar to the concept of the listener &amp;quot;casting his net&amp;quot; for comprehensible fragments of speech. A robust skipping parser (Lavie, 1995) is used to obtain an analysis for islands of the speaker&apos;s sentence. In the Combination stage, the fragments from the partial parse are assembled into a ranked set of alternative meaning representation hypotheses. A genetic programming (Koza, 1992; Koza, 1994) approach is used to search for different ways to combine the fragments in order to avoid requiring any hand-crafted repair rules. Our genetic programming approach has been shown previously to be orders of magnitude more efficient than the minimum distance parsing approach (Rose and Lavie, 1997). In the second phase, Interaction with 112</context>
<context position="24078" citStr="Lavie, 1995" startWordPosition="3878" endWordPosition="3879">aning representation using the ROSE approach. This meaning representation is then mapped onto a sentence in the target language. In this case both the source language and the target language are English. An additional evaluation demonstrates the improvement in interaction quality that can be gained by introducing available domain information. 4.1 Domain Independent Repair First. the system automatically selected 100 sentences from a previously unseen corpus of 500 sentences. These 100 sentences are the first 100 sentences in the set that a parse quality heuristic similar to that described in (Lavie, 1995) indicated to be of low quality. The parse quality heuristic evaluates how much skipping was necessary in the parser in order to arrive at a partial parse and how well the parser&apos;s analysis scores statistically. It should be kept in mind, then, that this testing corpus is composed of 100 of the most difficult sentences from the original corpus. The goal of the evaluation was to compute average performance per question asked and to compare this with the performance with using only the partial parser as well as with using only the Hypothesis Formation phase. In each case performance was measured</context>
</contexts>
<marker>Lavie, 1995</marker>
<rawString>A. Lavie. 1995. A Grammar Based Robust Parser For Spontaneous Speech. Ph.D. thesis, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Lehman</author>
</authors>
<title>Adaptive Parsing: SelfExtending Natural Language Interfaces.</title>
<date>1989</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="1204" citStr="Lehman, 1989" startWordPosition="169" endWordPosition="170">ch hand-coded repair knowledge and yields a 37% reduction in error rate over a corpus of noisy sentences. 1 Introduction In this paper we discuss ROSE, an interactive approach to robust interpretation developed in the context of the JANUS speech-to-speech translation system (Lavie et al., 1996). Previous interactive approaches to robust interpretation have either required excessive amounts of interaction (Rosé and Waibel, 1994), depended upon domain dependent repair rules (Van Noord, 1996; Danieli and Gerbino, 1995), or relied on the minimum distance parsing approach (Hipp, 1992; Smith, 1992; Lehman, 1989) which has been shown to be intractable in a largescale system (Rose and Lavie, 1997). In contrast, the ROSE approach operates efficiently without any hand-coded repair knowledge. An empirical evaluation demonstrates the efficacy of this domain independent approach. A further evaluation demonstrates that the ROSE approach combines easily with available domain knowledge in order to improve the quality of the interaction. The ROSE approach is based on a model of human communication between speakers of different languages with a small shared language base. Humans who share a very small language b</context>
</contexts>
<marker>Lehman, 1989</marker>
<rawString>J. F. Lehman. 1989. Adaptive Parsing: SelfExtending Natural Language Interfaces. Ph.D. thesis, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
</authors>
<title>Correcting real-world spelling errors using a model of the problemsolving context.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="17933" citStr="Ramshaw, 1994" startWordPosition="2913" endWordPosition="2914">d in Figure 5. If one or more distinguishing features remain, the cycle begins again by selecting a feature, generating a question, and so on until the system narrows down to the final result. If the user does not answer positively to any of the system&apos;s questions by the time it runs out of distinguishing features regarding a particular sentence, the system loses confidence in its set of hypotheses and requests a rephrase. 3 Using Discourse Information Though discourse processing is not essential to the ROSE approach, discourse information has been found to be useful in robust interpretation (Ramshaw, 1994; Smith, 1992). In this section we discuss how discourse information can be used for focusing the interaction between system and user on the task level rather than on the literal meaning of the user&apos;s utterance. A plan-based discourse processor (Rose et al., 1995) provides contextual expectations that guide the system in the manner in which it formulates 1132 Sentence: What about any time but the ten to twelve slot on Tuesday the thirtieth? Hypothesis 1: &amp;quot;How about from ten o&apos;clock till twelve o&apos;clock Tuesday the thirtieth any time&amp;quot; ((frame *how) (when (*multiple* ((end ((frame *simple-time) (</context>
</contexts>
<marker>Ramshaw, 1994</marker>
<rawString>L. A. Ramshaw. 1994. Correcting real-world spelling errors using a model of the problemsolving context. Computational Intelligence, 10(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rose</author>
<author>A Lavie</author>
</authors>
<title>An efficient distribution of labor in a two stage robust interpretation process.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1289" citStr="Rose and Lavie, 1997" startWordPosition="183" endWordPosition="186">a corpus of noisy sentences. 1 Introduction In this paper we discuss ROSE, an interactive approach to robust interpretation developed in the context of the JANUS speech-to-speech translation system (Lavie et al., 1996). Previous interactive approaches to robust interpretation have either required excessive amounts of interaction (Rosé and Waibel, 1994), depended upon domain dependent repair rules (Van Noord, 1996; Danieli and Gerbino, 1995), or relied on the minimum distance parsing approach (Hipp, 1992; Smith, 1992; Lehman, 1989) which has been shown to be intractable in a largescale system (Rose and Lavie, 1997). In contrast, the ROSE approach operates efficiently without any hand-coded repair knowledge. An empirical evaluation demonstrates the efficacy of this domain independent approach. A further evaluation demonstrates that the ROSE approach combines easily with available domain knowledge in order to improve the quality of the interaction. The ROSE approach is based on a model of human communication between speakers of different languages with a small shared language base. Humans who share a very small language base are able to communicate when the need arises by simplifying their speech patterns</context>
<context position="4385" citStr="Rose and Lavie, 1997" startWordPosition="669" endWordPosition="672">ible fragments of speech. A robust skipping parser (Lavie, 1995) is used to obtain an analysis for islands of the speaker&apos;s sentence. In the Combination stage, the fragments from the partial parse are assembled into a ranked set of alternative meaning representation hypotheses. A genetic programming (Koza, 1992; Koza, 1994) approach is used to search for different ways to combine the fragments in order to avoid requiring any hand-crafted repair rules. Our genetic programming approach has been shown previously to be orders of magnitude more efficient than the minimum distance parsing approach (Rose and Lavie, 1997). In the second phase, Interaction with 1129 the user, the system generates a set of queries, negotiating with the speaker in order to narrow down to a single best meaning representation hypothesis. Or, if it determines based on the user&apos;s responses to its queries that none of its hypotheses are acceptable, it requests a rephrase. Inspired by (Clark and Wilkes-Gibbs, 1986; Clark and Schaefer, 1989), the goal of the Interaction Phase is to minimize collaborative effort between the system and the speaker while maintaining a high level of interpretation accuracy. It uses this principle in determi</context>
</contexts>
<marker>Rose, Lavie, 1997</marker>
<rawString>C. P. Rose and A. Lavie. 1997. An efficient distribution of labor in a two stage robust interpretation process. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rosé</author>
<author>A Waibel</author>
</authors>
<title>Recovering from parser failures: A hybrid statistical/symbolic approach.</title>
<date>1994</date>
<booktitle>In Proceedings of The Balancing Act: Combining Symbolic and Statistical Approaches to Language workshop at the 32nd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1022" citStr="Rosé and Waibel, 1994" startWordPosition="140" endWordPosition="143">anslation system. Where other interactive approaches to robust interpretation have depended upon domain dependent repair rules, the approach described here operates efficiently without any such hand-coded repair knowledge and yields a 37% reduction in error rate over a corpus of noisy sentences. 1 Introduction In this paper we discuss ROSE, an interactive approach to robust interpretation developed in the context of the JANUS speech-to-speech translation system (Lavie et al., 1996). Previous interactive approaches to robust interpretation have either required excessive amounts of interaction (Rosé and Waibel, 1994), depended upon domain dependent repair rules (Van Noord, 1996; Danieli and Gerbino, 1995), or relied on the minimum distance parsing approach (Hipp, 1992; Smith, 1992; Lehman, 1989) which has been shown to be intractable in a largescale system (Rose and Lavie, 1997). In contrast, the ROSE approach operates efficiently without any hand-coded repair knowledge. An empirical evaluation demonstrates the efficacy of this domain independent approach. A further evaluation demonstrates that the ROSE approach combines easily with available domain knowledge in order to improve the quality of the interac</context>
</contexts>
<marker>Rosé, Waibel, 1994</marker>
<rawString>C. P. Rosé and A. Waibel. 1994. Recovering from parser failures: A hybrid statistical/symbolic approach. In Proceedings of The Balancing Act: Combining Symbolic and Statistical Approaches to Language workshop at the 32nd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rose</author>
<author>B Di Eugenio</author>
<author>L S Levin</author>
<author>C Van Ess-Dykema</author>
</authors>
<title>Discourse processing of dialogues with multiple threads.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<marker>Rose, Di Eugenio, Levin, Van Ess-Dykema, 1995</marker>
<rawString>C. P. Rose, B. Di Eugenio, L. S. Levin, and C. Van Ess-Dykema. 1995. Discourse processing of dialogues with multiple threads. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rose</author>
</authors>
<title>Robust Interactive Dialogue Interpretation.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="3283" citStr="Rose, 1997" startWordPosition="498" endWordPosition="499">ated in the context of the JANUS multi-lingual machine translation system. First, the system obtains a meaning representation for a sentence uttered in the source language. Then the resulting meaning representation structure is mapped onto a sentence in the target language using GENKIT (Tomita and Nyberg, 1988) with a sentence level generation grammar. Currently, the translation system deals with the scheduling domain where two speakers attempt to schedule a meeting together over the phone. This paper focuses on the Interaction phase. Details about the Hypothesis Formation phase are found in (Rose, 1997). 2 Interactive Repair In Depth As mentioned above, ROSE repairs extragrammatical input in two phases. The first phase, Repair Hypothesis Formation, is responsible for assembling a ranked set of ten or fewer hypotheses about the meaning of the ungrammatical utterance expressed in the source language. This phase is itself divided into two stages, Partial Parsing and Combination. The Partial Parsing stage is similar to the concept of the listener &amp;quot;casting his net&amp;quot; for comprehensible fragments of speech. A robust skipping parser (Lavie, 1995) is used to obtain an analysis for islands of the speak</context>
</contexts>
<marker>Rose, 1997</marker>
<rawString>C. P. Rose. 1997. Robust Interactive Dialogue Interpretation. Ph.D. thesis, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Smith</author>
</authors>
<title>A Computational Model of Expectation-Driven Mixed-Initiative Dialog Processing.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>CS Dept., Duke University.</institution>
<contexts>
<context position="1189" citStr="Smith, 1992" startWordPosition="167" endWordPosition="168">ithout any such hand-coded repair knowledge and yields a 37% reduction in error rate over a corpus of noisy sentences. 1 Introduction In this paper we discuss ROSE, an interactive approach to robust interpretation developed in the context of the JANUS speech-to-speech translation system (Lavie et al., 1996). Previous interactive approaches to robust interpretation have either required excessive amounts of interaction (Rosé and Waibel, 1994), depended upon domain dependent repair rules (Van Noord, 1996; Danieli and Gerbino, 1995), or relied on the minimum distance parsing approach (Hipp, 1992; Smith, 1992; Lehman, 1989) which has been shown to be intractable in a largescale system (Rose and Lavie, 1997). In contrast, the ROSE approach operates efficiently without any hand-coded repair knowledge. An empirical evaluation demonstrates the efficacy of this domain independent approach. A further evaluation demonstrates that the ROSE approach combines easily with available domain knowledge in order to improve the quality of the interaction. The ROSE approach is based on a model of human communication between speakers of different languages with a small shared language base. Humans who share a very s</context>
<context position="17947" citStr="Smith, 1992" startWordPosition="2915" endWordPosition="2916">If one or more distinguishing features remain, the cycle begins again by selecting a feature, generating a question, and so on until the system narrows down to the final result. If the user does not answer positively to any of the system&apos;s questions by the time it runs out of distinguishing features regarding a particular sentence, the system loses confidence in its set of hypotheses and requests a rephrase. 3 Using Discourse Information Though discourse processing is not essential to the ROSE approach, discourse information has been found to be useful in robust interpretation (Ramshaw, 1994; Smith, 1992). In this section we discuss how discourse information can be used for focusing the interaction between system and user on the task level rather than on the literal meaning of the user&apos;s utterance. A plan-based discourse processor (Rose et al., 1995) provides contextual expectations that guide the system in the manner in which it formulates 1132 Sentence: What about any time but the ten to twelve slot on Tuesday the thirtieth? Hypothesis 1: &amp;quot;How about from ten o&apos;clock till twelve o&apos;clock Tuesday the thirtieth any time&amp;quot; ((frame *how) (when (*multiple* ((end ((frame *simple-time) (hour 12))) (st</context>
</contexts>
<marker>Smith, 1992</marker>
<rawString>R. Smith. 1992. A Computational Model of Expectation-Driven Mixed-Initiative Dialog Processing. Ph.D. thesis, CS Dept., Duke University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
<author>E H Nyberg</author>
</authors>
<title>Generation kit and transformation kit version 3.2: User&apos;s manual.</title>
<date>1988</date>
<tech>Technical Report CMU-CMT-88-MEMO,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2984" citStr="Tomita and Nyberg, 1988" startWordPosition="450" endWordPosition="453">esponsible for assembling a set of hypotheses about the meaning of the ungrammatical utterance. In the second phase, Interaction with the User, the systern generates a set of queries, negotiating with the speaker in order to narrow down to a single best meaning representation hypothesis. This approach was evaluated in the context of the JANUS multi-lingual machine translation system. First, the system obtains a meaning representation for a sentence uttered in the source language. Then the resulting meaning representation structure is mapped onto a sentence in the target language using GENKIT (Tomita and Nyberg, 1988) with a sentence level generation grammar. Currently, the translation system deals with the scheduling domain where two speakers attempt to schedule a meeting together over the phone. This paper focuses on the Interaction phase. Details about the Hypothesis Formation phase are found in (Rose, 1997). 2 Interactive Repair In Depth As mentioned above, ROSE repairs extragrammatical input in two phases. The first phase, Repair Hypothesis Formation, is responsible for assembling a ranked set of ten or fewer hypotheses about the meaning of the ungrammatical utterance expressed in the source language.</context>
</contexts>
<marker>Tomita, Nyberg, 1988</marker>
<rawString>M. Tomita and E. H. Nyberg. 1988. Generation kit and transformation kit version 3.2: User&apos;s manual. Technical Report CMU-CMT-88-MEMO, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Van Noord</author>
</authors>
<title>Robust parsing with the headcorner parser.</title>
<date>1996</date>
<booktitle>In Proceedings of the Eight European Summer School In Logic, Language and Information,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Van Noord, 1996</marker>
<rawString>G. Van Noord. 1996. Robust parsing with the headcorner parser. In Proceedings of the Eight European Summer School In Logic, Language and Information, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>