<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.192044">
<title confidence="0.951486">
Sussx: WSD using Automatically Acquired Predominant Senses
</title>
<author confidence="0.99827">
Rob Koeling and Diana McCarthy
</author>
<affiliation confidence="0.994019">
Department of Informatics
University of Sussex
</affiliation>
<address confidence="0.992944">
Brighton BN1 9QJ, UK
</address>
<email confidence="0.999641">
robk,dianam@sussex.ac.uk
</email>
<sectionHeader confidence="0.999638" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999024228571428">
We introduced a method for discovering the predom-
inant sense of words automatically using raw (unla-
belled) text in (McCarthy et al., 2004) and partici-
pated with this system in SENSEVAL3. Since then,
we worked on further developing ideas to improve
upon the base method. In the current paper we tar-
get two areas where we believe there is potential for
improvement. In the first one we address the fine-
grained structure of WordNet’s (WN) sense inven-
tory (i.e. the topic of the task in this particular track).
The second issue we address here, deals with topic
domain specilisation of the base method.
Error analysis tought us that the method is sensi-
tive to the fine-grained nature of WN. When two dis-
tinct senses in the WN sense inventory are closely re-
lated, the method often has difficulties discriminat-
ing between the two senses. If, for example, sense 1
and sense 7 for a word are closely related, choosing
sense 7 in stead of sense 1 has serious consequences
if you are using a first-sense heuristic (considering
the highly skewed distribution of word senses). We
expect that applying our method on a coarser grained
sense inventory might help us resolve some of the
more unfortunate errors.
(Magnini et al., 2002) have shown that informa-
tion about the domain of a document is very useful
for WSD. This is because many concepts are spe-
cific to particular domains, and for many words their
most likely meaning in context is strongly correlated
to the domain of the document they appear in. Thus,
since word sense distributions are skewed and de-
pend on the domain at hand we would like to explore
if we can estimate the most likely sense of a word
for each domain of application and exploit this in
a WSD system.
</bodyText>
<sectionHeader confidence="0.973282" genericHeader="method">
2 Predominant Sense Acquisition
</sectionHeader>
<bodyText confidence="0.999962964285714">
We use the method described in (McCarthy et al.,
2004) for finding predominant senses from raw text.
The method uses a thesaurus obtained from the
text by parsing, extracting grammatical relations and
then listing each word (w) with its top k nearest
neighbours, where k is a constant. Like (McCarthy
et al., 2004) we use k = 50 and obtain our thesaurus
using the distributional similarity metric described
by (Lin, 1998) and we use WordNet (WN) as our
sense inventory. The senses of a word w are each
assigned a ranking score which sums over the dis-
tributional similarity scores of the neighbours and
weights each neighbour’s score by a WN Similarity
score (Patwardhan and Pedersen, 2003) between the
sense of w and the sense of the neighbour that max-
imises the WN Similarity score. This weight is nor-
malised by the sum of such WN similarity scores be-
tween all senses of w and the senses of the neighbour
that maximises this score. We use the WN Similarity
jcn score (Jiang and Conrath, 1997) since this gave
reasonable results for (McCarthy et al., 2004) and it
is efficient at run time given precompilation of fre-
quency information. The jcn measure needs word
frequency information, which we obtained from the
British National Corpus (BNC) (Leech, 1992). The
distributional thesaurus was constructed using sub-
ject, direct object adjective modifier and noun mod-
ifier relations.
</bodyText>
<page confidence="0.991169">
314
</page>
<bodyText confidence="0.7484415">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 314–317,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.964874" genericHeader="method">
3 Coarse Sense Inventory Adaptation
</sectionHeader>
<bodyText confidence="0.999993166666667">
We contrasted ranking of the original WordNet
senses with ranking produced using the coarse
grained mapping between WordNet senses and the
clusters provided for this task. In the first, which we
refer to as fine-grained training (SUSSX-FR), we use
the original method as described in section 2 using
WordNet 2.1 as our sense inventory. For the second
method which we refer to as coarse-grained train-
ing (SUSSX-CR), we use the clusters of the target
word as our senses. The distributional similarity of
each neighbour is apportioned to these clusters us-
ing the maximum WordNet similarity between any
of the WordNet senses in the cluster and any of the
senses of the neighbour. This WordNet similarity is
normalised as in the original method, but for the de-
nominator we use the sum of the WordNet similarity
scores between this neighbour and all clusters of the
target word.
</bodyText>
<sectionHeader confidence="0.989775" genericHeader="method">
4 Domain Adaptation
</sectionHeader>
<bodyText confidence="0.999988523809524">
The topic domain of a document has a strong influ-
ence on the sense distribution of words. Unfortu-
nately, it is not feasible to produce large manually
sense-annotated corpora for every domain of inter-
est. Since the method described in section 2 works
with raw text, we can specialize our sense rank-
ings for a particular topic domain, simply by feed-
ing a domain specific corpus to the algorithm. Pre-
vious experiments have shown that unsupervised es-
timation of the predominant sense of certain words
using corpora whose domain has been determined
by hand outperforms estimates based on domain-
independent text for a subset of words and even
outperforms the estimates based on counting oc-
currences in an annotated corpus (Koeling et al.,
2005). A later experiment (using SENSEVAL2 and
3 data) showed that using domain specific predomi-
nant senses can slightly improve the results for some
domains (Koeling et al., 2007). However, a firm idea
of when domain specilisation should be considered
could not (yet) be given.
</bodyText>
<subsectionHeader confidence="0.999534">
4.1 Creating the Domain Corpora
</subsectionHeader>
<bodyText confidence="0.995276020833333">
In order to estimate topic domain specific sense
rankings, we need to specify what we consider ’do-
mains’ and we need to collect corpora of texts for
these domains. We decided to use text classifica-
tion for determining the topic domain and adopted
the domain hierarchy as defined for the topic domain
extension for WN (Subject Field Codes or WordNet
Domains (WN-DOMAINS) (Magnini et al., 2002)).
Domains In WN-DOMAINS the Princeton English
WordNet is augmented with domain labels. Ev-
ery synset in WN’s sense inventory is annotated
with at least one domain label, selected from a set
of about 200 labels hierarchically organized (based
on the Dewey Decimal Classification (Diekema, )).
Each synset of Wordnet 1.6 was labeled with one
or more labels. The label ’factotum’ was assigned
if any other was inadequate. The first level con-
sists of 5 main categories (e.g. ’doctrines’ and ’so-
cial science’) and ’factotum’. ’doctrines’, forexam-
ple, has subcategories such as ’art’, ’religion’ and
’psychology’. Some subcategories are divided in
sub-subcategories, e.g. ’dance’, ’music’ or ’theatre’
are subcategories of ’art’.
Classifier We extracted bags of domain-specific
words from WordNet for all the defined domains by
collecting all the word senses (synsets) and corre-
sponding glosses associated with a certain domain
label. These bags of words define the domains and
we used them to train a Support Vector Machine
(SVM) text classifier using ’TwentyOne’1.
The classifier distinguishes between 48 classes
(first and second level of the WN-DOMAINS hierar-
chy). When a document is evaluated by the classi-
fier, it returns a list of all the classes (domains) it
recognizes and an associated confidence score re-
flecting the certainty that the document belongs to
that particular domain.
Corpora We used the Gigaword English Corpus
as our data source. This corpus is a comprehen-
sive archive of newswire text data that has been
acquired over several years by the Linguistic Data
Consortium, at the University of Pennsylvania. For
the experiments described in this paper, we use the
first 20 months worth of data of all four sources
(Agence France Press English Service, Associated
Press Worldstream English Service, The New York
Times Newswire Service and The Xinhua News
Agency English Service). There are 4 different types
</bodyText>
<footnote confidence="0.986933">
&apos;TwentyOne Classifier is an Irion Technologies product:
www.irion.ml/products/english/products classify.html
</footnote>
<page confidence="0.984374">
315
</page>
<table confidence="0.999546333333333">
Doc.Id. Class Conf. Score
d001 Medicine (Economy) 0.75 (0.75)
d002 Economy (Politics) 0.76 (0.74)
d003 Transport (Biology) 0.75 (0.68)
d004 Comp-Sci (Architecture) 0.81 (0.68)
d005 Psychology (Art) 0.78 (0.74)
</table>
<tableCaption confidence="0.972561333333333">
Table 1: Output of the classifier for the 5 docu-
ments. The classifiers second choice is given be-
tween brackets.
</tableCaption>
<bodyText confidence="0.940464888888889">
of documents identified in the corpus. The vast ma-
jority of the documents are of type ’story’. We are
using all the data.
The five documents were fed to the classifier. The
results are given in table 1. Unfortunately, only one
document (d004) was considered to be a clear-cut
example of a particular domain by the classifier (i.e.
a high score is given to the first class and a much
lower score to the following classes).
</bodyText>
<subsectionHeader confidence="0.995235">
4.2 Domain rankings
</subsectionHeader>
<bodyText confidence="0.999993117647059">
We created domain corpora by feeding the Giga-
Word documents to the classifier and adding each
document to the domain corpus corresponding to
the classifier’s first choice. The five corpora we
needed for these documents were parsed using
RASP (Briscoe and Carroll, 2002) and the result-
ing grammatical relations were used to create a dis-
tributional similarity thesaurus, which in turn was
used for computing the predominant senses (see sec-
tion 2). The only pre-processing we performed
was stripping the XML codes from the documents.
No other filtering was undertaken. This resulted in
five sets of sense inventories with domain-dependent
sense rankings. Each of them has a slightly different
set of words. The words they have in common do
have the same senses, but not necessarily the same
estimated most frequently used sense.
</bodyText>
<sectionHeader confidence="0.999787" genericHeader="method">
5 Results from Semeval
</sectionHeader>
<bodyText confidence="0.99773396">
Coarse Disambiguation of coarse-grained senses is
obviously an easier task than fine grained training.
We had hoped that the coarse-grained training might
show superior performance by removing the noise
created by related but less frequent senses. Since
the mapping between fine-grained senses and clus-
ters is used anyway in the scorer the noise from
related senses does not seem to be an issue. Re-
lated senses are scored correctly. Indeed the per-
formance of the fine-grained training is superior to
that of the coarse-grained training. We believe this
is because predominant meanings have more related
senses. There are therefore more chances that the
distributional similarity of the neighbours will get
apportioned to one of the related senses when there
are more related senses. The coarse grained rank-
ing would have an advantage on occasions when in
the fine-grained ranking the credit between related
senses is split and an unrelated sense ends up with
a higher ranking score. Since the coarse-grained
ranker lumps the credit for related sense together it
would be at an advantage. Clearly this doesn’t hap-
pen enough in the data to outweigh the beneficial
effect of the number of related senses compensating
for other noise in the data.
</bodyText>
<table confidence="0.999523166666667">
Doc.Id. Class SUSSX-FR SUSSX-C-WD
d001 Medicine 0.556 0.560
d002 Economy 0.508 0.515
d003 Transport 0.487 0.454
d004 Comp-Sci 0.407 0.424
d005 Psychology 0.356 0.372
</table>
<tableCaption confidence="0.992303">
Table 2: Impact of domain specialisation for each of
</tableCaption>
<bodyText confidence="0.98505875">
the five documents (F1 scores).
Domain Unfortuately, the system specialised for
domain (SUSSX-C-WD) did not improve the results
over the 5 documents significantly. However, if we
look at the contributions made by each document,
we might learn something about the relation beteen
the output of the classifier and the impact on the
WSDresults. Table 2 shows the per-document results
for the systems SUSSX-FR and SUSSX-C-WD. The
first two documents show very little difference with
the domain independent results. The documents
’d004’ and ’d005’ show a small but clear improved
performance for the domain results. Unfortunately,
document ’d003’ displays a very disappointing drop
of more than 3% in performance, and cancels out all
the gains made by the last two documents.
The output of the classifier seems to be indica-
tive of the results for all documents except ’d003’.
The classifier doesn’t seem to find enough evidence
for a marked preference for a particular domain
</bodyText>
<page confidence="0.998179">
316
</page>
<bodyText confidence="0.99994975">
for documents ’d001’ and ’d002’. This could be
an indication that there is no strong domain ef-
fect to be expected. The strong preference for the
’computer science’ domain for ’d004’ is reflected in
good performance of SUSSX-C-WD and even though
the confidence scores for the first 2 alternatives of
’d005’ are fairly close, there is a clear drop in con-
fidence score for the third alternative, which might
indicate that the topic of this document is related to
both first choices of the classifier. It will be interest-
ing to evaluate the results for ’d005’ using the ’Art’
sense rankings. One would expect those results to be
similar to the results found here. Finally, the results
for ’d003’ are hard to explain. We will need to do an
extensive error analysis as soon as the gold-standard
is available.
</bodyText>
<sectionHeader confidence="0.999758" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99765304">
In this paper we investigated two directions where
we expect potential for improving the performance
of our method for acquiring predominant senses.
In order to fully appreciate what the effects of the
coarse grained sense inventory are (i.e. whether
some of the more unfortunate errors are resolved),
we will have to do an extensive error analysis as
soon as the gold standard becomes available. Con-
sidering the fairly low number of attempted tokens
(only 72.8% of the tokens are attempted), we are at
a disadvantage compared to systems that back-off to
(for example) the first sense in WN. However, we are
well pleased with the high precision (71.7%) of the
method SUSSX-FR, considering this is a completely
unsupervised method. There seems to be potential
gains for domain adaptation, but applying it to each
document does not seem to be advisable. More re-
search needs to be done to identify in which cases a
performance boost can be expected. Five documents
is not enough to fully investigate the matter. At the
moment we are performing a larger scale experiment
with the documents in SemCor. These documents
seem to cover a fairly wide range of domains (ac-
cording to our text classifier) and many domains are
represented by several documents.
</bodyText>
<sectionHeader confidence="0.996446" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.936523857142857">
This work was funded by UK EPSRC project
EP/C537262 “Ranking Word Senses for Disam-
biguation: Models and Applications”, and by a UK
Royal Society Dorothy Hodgkin Fellowship to the
second author. We would also like to thank Piek
Vossen for giving us access to the Irion Technolo-
gies text categoriser.
</bodyText>
<sectionHeader confidence="0.997054" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999249820512821">
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
ofLREC-2002, pages 1499–1504, Las Palmas de Gran
Canaria.
Anne Diekema. http://www.oclc.org/dewey/.
Jay Jiang and David Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In In-
ternational Conference on Research in Computational
Linguistics, Taiwan.
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition. In Proceedings of the Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing.,
pages 419–426, Vancouver, Canada.
Rob Koeling, Diana McCarthy, and John Carroll. 2007.
Text categorization for improved priors of word mean-
ing. In Proceedings of the Eighth International Con-
ference on Intelligent Text Processing and Compu-
tational Linguistics (Cicling 2007), pages 241–252,
Mexico City, Mexico.
Geoffrey Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1–13.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL 98,
Montreal, Canada.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,
and Alfio Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Language
Engineering, 8(4):359–373.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 280–287, Barcelona, Spain.
Siddharth Patwardhan and Ted Pedersen.
2003. The cpan wordnet::similarity package.
http://search.cpan.org/˜sid/WordNet-Similarity/.
</reference>
<page confidence="0.998616">
317
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928971">
<title confidence="0.999909">Sussx: WSD using Automatically Acquired Predominant Senses</title>
<author confidence="0.999977">Rob Koeling</author>
<author confidence="0.999977">Diana McCarthy</author>
<affiliation confidence="0.999853">Department of Informatics University of Sussex</affiliation>
<address confidence="0.999318">Brighton BN1 9QJ, UK</address>
<email confidence="0.929925">robk,dianam@sussex.ac.uk</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings ofLREC-2002,</booktitle>
<pages>1499--1504</pages>
<institution>Las Palmas de Gran Canaria.</institution>
<contexts>
<context position="8930" citStr="Briscoe and Carroll, 2002" startWordPosition="1457" endWordPosition="1460"> ’story’. We are using all the data. The five documents were fed to the classifier. The results are given in table 1. Unfortunately, only one document (d004) was considered to be a clear-cut example of a particular domain by the classifier (i.e. a high score is given to the first class and a much lower score to the following classes). 4.2 Domain rankings We created domain corpora by feeding the GigaWord documents to the classifier and adding each document to the domain corpus corresponding to the classifier’s first choice. The five corpora we needed for these documents were parsed using RASP (Briscoe and Carroll, 2002) and the resulting grammatical relations were used to create a distributional similarity thesaurus, which in turn was used for computing the predominant senses (see section 2). The only pre-processing we performed was stripping the XML codes from the documents. No other filtering was undertaken. This resulted in five sets of sense inventories with domain-dependent sense rankings. Each of them has a slightly different set of words. The words they have in common do have the same senses, but not necessarily the same estimated most frequently used sense. 5 Results from Semeval Coarse Disambiguatio</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Ted Briscoe and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings ofLREC-2002, pages 1499–1504, Las Palmas de Gran Canaria.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anne Diekema</author>
</authors>
<note>http://www.oclc.org/dewey/.</note>
<marker>Diekema, </marker>
<rawString>Anne Diekema. http://www.oclc.org/dewey/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Jiang</author>
<author>David Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="2938" citStr="Jiang and Conrath, 1997" startWordPosition="502" endWordPosition="505">ibutional similarity metric described by (Lin, 1998) and we use WordNet (WN) as our sense inventory. The senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbour’s score by a WN Similarity score (Patwardhan and Pedersen, 2003) between the sense of w and the sense of the neighbour that maximises the WN Similarity score. This weight is normalised by the sum of such WN similarity scores between all senses of w and the senses of the neighbour that maximises this score. We use the WN Similarity jcn score (Jiang and Conrath, 1997) since this gave reasonable results for (McCarthy et al., 2004) and it is efficient at run time given precompilation of frequency information. The jcn measure needs word frequency information, which we obtained from the British National Corpus (BNC) (Leech, 1992). The distributional thesaurus was constructed using subject, direct object adjective modifier and noun modifier relations. 314 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 314–317, Prague, June 2007. c�2007 Association for Computational Linguistics 3 Coarse Sense Inventory Adaptation We c</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay Jiang and David Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.,</booktitle>
<pages>419--426</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5165" citStr="Koeling et al., 2005" startWordPosition="859" endWordPosition="862">asible to produce large manually sense-annotated corpora for every domain of interest. Since the method described in section 2 works with raw text, we can specialize our sense rankings for a particular topic domain, simply by feeding a domain specific corpus to the algorithm. Previous experiments have shown that unsupervised estimation of the predominant sense of certain words using corpora whose domain has been determined by hand outperforms estimates based on domainindependent text for a subset of words and even outperforms the estimates based on counting occurrences in an annotated corpus (Koeling et al., 2005). A later experiment (using SENSEVAL2 and 3 data) showed that using domain specific predominant senses can slightly improve the results for some domains (Koeling et al., 2007). However, a firm idea of when domain specilisation should be considered could not (yet) be given. 4.1 Creating the Domain Corpora In order to estimate topic domain specific sense rankings, we need to specify what we consider ’domains’ and we need to collect corpora of texts for these domains. We decided to use text classification for determining the topic domain and adopted the domain hierarchy as defined for the topic d</context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing., pages 419–426, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Text categorization for improved priors of word meaning.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eighth International Conference on Intelligent Text Processing and Computational Linguistics (Cicling</booktitle>
<pages>241--252</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="5340" citStr="Koeling et al., 2007" startWordPosition="887" endWordPosition="890"> rankings for a particular topic domain, simply by feeding a domain specific corpus to the algorithm. Previous experiments have shown that unsupervised estimation of the predominant sense of certain words using corpora whose domain has been determined by hand outperforms estimates based on domainindependent text for a subset of words and even outperforms the estimates based on counting occurrences in an annotated corpus (Koeling et al., 2005). A later experiment (using SENSEVAL2 and 3 data) showed that using domain specific predominant senses can slightly improve the results for some domains (Koeling et al., 2007). However, a firm idea of when domain specilisation should be considered could not (yet) be given. 4.1 Creating the Domain Corpora In order to estimate topic domain specific sense rankings, we need to specify what we consider ’domains’ and we need to collect corpora of texts for these domains. We decided to use text classification for determining the topic domain and adopted the domain hierarchy as defined for the topic domain extension for WN (Subject Field Codes or WordNet Domains (WN-DOMAINS) (Magnini et al., 2002)). Domains In WN-DOMAINS the Princeton English WordNet is augmented with doma</context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2007</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2007. Text categorization for improved priors of word meaning. In Proceedings of the Eighth International Conference on Intelligent Text Processing and Computational Linguistics (Cicling 2007), pages 241–252, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
</authors>
<title>100 million words of English:</title>
<date>1992</date>
<journal>the British National Corpus. Language Research,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="3201" citStr="Leech, 1992" startWordPosition="545" endWordPosition="546">ilarity score (Patwardhan and Pedersen, 2003) between the sense of w and the sense of the neighbour that maximises the WN Similarity score. This weight is normalised by the sum of such WN similarity scores between all senses of w and the senses of the neighbour that maximises this score. We use the WN Similarity jcn score (Jiang and Conrath, 1997) since this gave reasonable results for (McCarthy et al., 2004) and it is efficient at run time given precompilation of frequency information. The jcn measure needs word frequency information, which we obtained from the British National Corpus (BNC) (Leech, 1992). The distributional thesaurus was constructed using subject, direct object adjective modifier and noun modifier relations. 314 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 314–317, Prague, June 2007. c�2007 Association for Computational Linguistics 3 Coarse Sense Inventory Adaptation We contrasted ranking of the original WordNet senses with ranking produced using the coarse grained mapping between WordNet senses and the clusters provided for this task. In the first, which we refer to as fine-grained training (SUSSX-FR), we use the original method</context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>Geoffrey Leech. 1992. 100 million words of English: the British National Corpus. Language Research, 28(1):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL 98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="2366" citStr="Lin, 1998" startWordPosition="400" endWordPosition="401"> domain at hand we would like to explore if we can estimate the most likely sense of a word for each domain of application and exploit this in a WSD system. 2 Predominant Sense Acquisition We use the method described in (McCarthy et al., 2004) for finding predominant senses from raw text. The method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word (w) with its top k nearest neighbours, where k is a constant. Like (McCarthy et al., 2004) we use k = 50 and obtain our thesaurus using the distributional similarity metric described by (Lin, 1998) and we use WordNet (WN) as our sense inventory. The senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbour’s score by a WN Similarity score (Patwardhan and Pedersen, 2003) between the sense of w and the sense of the neighbour that maximises the WN Similarity score. This weight is normalised by the sum of such WN similarity scores between all senses of w and the senses of the neighbour that maximises this score. We use the WN Similarity jcn score (Jiang and Conrath, 1997) since this gave reasonable </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL 98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
<author>Giovanni Pezzulo</author>
<author>Alfio Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="1416" citStr="Magnini et al., 2002" startWordPosition="231" endWordPosition="234">. Error analysis tought us that the method is sensitive to the fine-grained nature of WN. When two distinct senses in the WN sense inventory are closely related, the method often has difficulties discriminating between the two senses. If, for example, sense 1 and sense 7 for a word are closely related, choosing sense 7 in stead of sense 1 has serious consequences if you are using a first-sense heuristic (considering the highly skewed distribution of word senses). We expect that applying our method on a coarser grained sense inventory might help us resolve some of the more unfortunate errors. (Magnini et al., 2002) have shown that information about the domain of a document is very useful for WSD. This is because many concepts are specific to particular domains, and for many words their most likely meaning in context is strongly correlated to the domain of the document they appear in. Thus, since word sense distributions are skewed and depend on the domain at hand we would like to explore if we can estimate the most likely sense of a word for each domain of application and exploit this in a WSD system. 2 Predominant Sense Acquisition We use the method described in (McCarthy et al., 2004) for finding pred</context>
<context position="5863" citStr="Magnini et al., 2002" startWordPosition="974" endWordPosition="977">ecific predominant senses can slightly improve the results for some domains (Koeling et al., 2007). However, a firm idea of when domain specilisation should be considered could not (yet) be given. 4.1 Creating the Domain Corpora In order to estimate topic domain specific sense rankings, we need to specify what we consider ’domains’ and we need to collect corpora of texts for these domains. We decided to use text classification for determining the topic domain and adopted the domain hierarchy as defined for the topic domain extension for WN (Subject Field Codes or WordNet Domains (WN-DOMAINS) (Magnini et al., 2002)). Domains In WN-DOMAINS the Princeton English WordNet is augmented with domain labels. Every synset in WN’s sense inventory is annotated with at least one domain label, selected from a set of about 200 labels hierarchically organized (based on the Dewey Decimal Classification (Diekema, )). Each synset of Wordnet 1.6 was labeled with one or more labels. The label ’factotum’ was assigned if any other was inadequate. The first level consists of 5 main categories (e.g. ’doctrines’ and ’social science’) and ’factotum’. ’doctrines’, forexample, has subcategories such as ’art’, ’religion’ and ’psych</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo, and Alfio Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(4):359–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>280--287</pages>
<location>Barcelona,</location>
<contexts>
<context position="1999" citStr="McCarthy et al., 2004" startWordPosition="337" endWordPosition="340">ortunate errors. (Magnini et al., 2002) have shown that information about the domain of a document is very useful for WSD. This is because many concepts are specific to particular domains, and for many words their most likely meaning in context is strongly correlated to the domain of the document they appear in. Thus, since word sense distributions are skewed and depend on the domain at hand we would like to explore if we can estimate the most likely sense of a word for each domain of application and exploit this in a WSD system. 2 Predominant Sense Acquisition We use the method described in (McCarthy et al., 2004) for finding predominant senses from raw text. The method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word (w) with its top k nearest neighbours, where k is a constant. Like (McCarthy et al., 2004) we use k = 50 and obtain our thesaurus using the distributional similarity metric described by (Lin, 1998) and we use WordNet (WN) as our sense inventory. The senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbour’s score by a WN Similarity sc</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 280–287, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>The cpan wordnet::similarity package.</title>
<date>2003</date>
<note>http://search.cpan.org/˜sid/WordNet-Similarity/.</note>
<contexts>
<context position="2634" citStr="Patwardhan and Pedersen, 2003" startWordPosition="444" endWordPosition="447">finding predominant senses from raw text. The method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word (w) with its top k nearest neighbours, where k is a constant. Like (McCarthy et al., 2004) we use k = 50 and obtain our thesaurus using the distributional similarity metric described by (Lin, 1998) and we use WordNet (WN) as our sense inventory. The senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbour’s score by a WN Similarity score (Patwardhan and Pedersen, 2003) between the sense of w and the sense of the neighbour that maximises the WN Similarity score. This weight is normalised by the sum of such WN similarity scores between all senses of w and the senses of the neighbour that maximises this score. We use the WN Similarity jcn score (Jiang and Conrath, 1997) since this gave reasonable results for (McCarthy et al., 2004) and it is efficient at run time given precompilation of frequency information. The jcn measure needs word frequency information, which we obtained from the British National Corpus (BNC) (Leech, 1992). The distributional thesaurus wa</context>
</contexts>
<marker>Patwardhan, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. 2003. The cpan wordnet::similarity package. http://search.cpan.org/˜sid/WordNet-Similarity/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>