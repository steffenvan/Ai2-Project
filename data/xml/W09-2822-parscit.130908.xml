<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.433485">
<title confidence="0.9867705">
WLV: A confidence-based machine learning method for the
GREC-NEG’09 task
</title>
<author confidence="0.983554">
Constantin Or˘asan
</author>
<affiliation confidence="0.959374">
RIILP
University of Wolverhampton, UK
</affiliation>
<email confidence="0.991298">
C.Orasan@wlv.ac.uk
</email>
<author confidence="0.921207">
Iustin Dornescu
</author>
<affiliation confidence="0.919732">
RIILP
University of Wolverhampton, UK
</affiliation>
<email confidence="0.995952">
I.Dornescu@wlv.ac.uk
</email>
<sectionHeader confidence="0.993847" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998825">
This article presents the machine learning
approach used by the University of
Wolverhampton in the GREC-NEG’09
task. A classifier based on J48 decision
tree and a meta-classifier were used to
produce two runs. Evaluation on the
development set shows that the meta-
classifier achieves a better performance.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999164">
The solution adopted by the University of
Wolverhampton to solve the GREC-NEG task
relies on machine learning. To this end, we
assumed that it is possible to learn which is the
correct form for a referential expression given the
context in which it appears. The remainder of the
paper is structured as follows: Section 2 presents
the method used in this paper. Section 3 presents
the evaluation results on the development set. The
paper finishes with conclusions.
</bodyText>
<sectionHeader confidence="0.953072" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999932117647059">
The method used to solve the GREC-NEG task
was inspired by the machine learning approaches
employed for coreference resolution. In these
methods, pairs of entities are classified as
coreferential or not on the basis of a set of features
(Mitkov, 2002). In the same manner, each REF
element from the text to be processed is paired
with all the REFEX elements in its chain and
machine learning is used to determine the lexical
form of which candidate REFEX element can be
used in the given context. To achieve this, a set of
features was derived after a corpus investigation.
As can be seen, some of these features are
similar to those used by resolution algorithms
(e.g. distance between entities), whilst others are
specific for the task (e.g. empty markers). The
features used for a (REF, REFEX) pair are:
</bodyText>
<listItem confidence="0.977393514285714">
• Whether the REF element is the first mention
in the chain. We noticed that in most cases
it corresponds to the longest REFEX element
in the plain case.
• Whether the REFEX element is the longest
string.
• Whether the REF element is the first word in
the sentence as this word is very likely to be
the subject (i.e. nominative or plain case).
• Whether the words before the REF element
can signal a possible empty element.
Example of such phrases are “, but” and “and
then”. These phrases were extracted after
analysing the training corpus.
• The distance in sentences to the previous
REF element in the chain. This feature was
used because a pronoun is more likely to
be used when several mentions are in the
same sentence, whilst full noun phrases are
normally used if the mentions are far away or
in different paragraphs.
• The REG08-TYPE of the REFEX tags
that were assigned by the program to the
previous 2 REF elements in the chain. This
information can prove useful in conjunction
with the previous feature.
• The part-of-speech tags of the four words
before and three words after the REF element
as a way to indicate the context in which the
element appears.
• A compatibility feature which indicates pairs
of SYNFUNC and CASE that are highly
correlated. This correlation was determined
by extracting the most frequent SYNFUNC
and CASE pairs from the training corpus.
</listItem>
<page confidence="0.961763">
107
</page>
<note confidence="0.9945045">
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 107–108,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<listItem confidence="0.99777">
• The size of the chain in elements as longer
chains are more likely to contain pronouns.
• The values of SEMCAT, SYNCAT and
SYNFUNC attributes of REF element
and REG08-TYPE and CASE of REFEX
element.
• The number of words in the REFEX value.
• Whether REF is in the first chain of the
document.
</listItem>
<bodyText confidence="0.999950833333333">
The last two features were introduced in order
to discriminate between candidate REFEX values
that have the same type and case. For example,
the number of words proved very useful when
selecting genitive case names and chi-squared
statistic ranks it as one of the best features together
with the compatibility feature, information about
previous elements in the chain and the longest
REFEX candidate.
Before the features are calculated, the text is
split into sentences and enriched with part-of-
speech information using the OpenNLP library.
1 The instances are fed into a binary classifier
that indicates whether the (REF, REFEX) pair is
good (i.e. the REFEX element is a good filler for
the REF element). Since each pair is classified
independently, it is possible to have zero, one or
more good REFEX candidates for a given REF.
Therefore, the system uses the confidence returned
by the classifier to rank the candidates and selects
the one that has the highest probability of being
good, regardless of the class assigned by the
classifier. In this way the system selects exactly
one REFEX for each REF.
</bodyText>
<sectionHeader confidence="0.996406" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999862">
The method proposed in this paper was evaluated
using two classifiers, both trained on the same
set of features. The first classifier is the standard
J48 decision tree algorithm implemented in Weka
(Witten and Frank, 2005). The run that used this
classifier is referred to in the rest of the paper as
standard run. Given the large number of negative
examples present in our training data, a meta-
classifier that is cost-sensitive was used for the
second run. In our case, the meta-classifier relies
on J48 and reweights training instances according
to the total cost assigned to each class. After
</bodyText>
<footnote confidence="0.955062">
1http://opennlp.sourceforge.net/
</footnote>
<bodyText confidence="0.999863">
experimenting with different cost matrices, we
decided to assign a cost of 3 to false negatives
and 1 to false positives, in this way biasing the
classifier towards a higher recall for YES answers.
The results obtained using this meta-classifier are
referred to as biased run. Our results on the
development set are presented in Table 1.
</bodyText>
<table confidence="0.999469727272727">
Measure Standard Biased
classification accuracy 94.40% 92.09%
total pairs 907 907
reg08 type matches 621 728
reg08 type accuracy 68.46% 80.26%
reg08 type precision 68.46% 80.26%
reg08 type recall 66.20% 77.61%
string matches 568 667
string accuracy 62.62% 73.53%
mean edit distance 0.845 0.613
mean normalised edit distance 0.351 0.239
</table>
<tableCaption confidence="0.955103">
Table 1: The evaluation results on the
development set
</tableCaption>
<bodyText confidence="0.998913">
The first row in the table presents the accuracy
of the classifier on the training data using 10-fold
cross-validation. The very high accuracy is due
to the large number of negative instances in the
training data: assigning all the instances to the
class NO achieves a baseline accuracy of 88.96%.
The rest of the table presents the accuracy of the
system on the development set using the script
provided by the GREC-NEG organisers. As can
be seen, the best results are obtained by the biased
classifier despite performing worse at the level
of classification accuracy. This can be explained
by the fact that we do not use the output of the
classifier directly, instead using the classification
confidence.
</bodyText>
<sectionHeader confidence="0.999769" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.997744">
This paper has presented our participation in the
GREC-NEG task with a machine learning system.
Currently the system tries to predict whether a
(REF, REFEX) pair is valid, but in the future
we plan to approach the task by using machine
learning methods to determine the values of
REG08-TYPE and CASE attributes.
</bodyText>
<sectionHeader confidence="0.999388" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9947784">
Ruslan Mitkov. 2002. Anaphora resolution.
Longman.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann Publishers.
</reference>
<page confidence="0.998362">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.751821">
<title confidence="0.9688395">WLV: A confidence-based machine learning method for GREC-NEG’09 task</title>
<author confidence="0.935149">Constantin</author>
<affiliation confidence="0.999905">University of Wolverhampton,</affiliation>
<email confidence="0.991581">C.Orasan@wlv.ac.uk</email>
<affiliation confidence="0.951684">Iustin University of Wolverhampton,</affiliation>
<email confidence="0.997932">I.Dornescu@wlv.ac.uk</email>
<abstract confidence="0.991516444444444">This article presents the machine learning approach used by the University of Wolverhampton in the GREC-NEG’09 task. A classifier based on J48 decision tree and a meta-classifier were used to produce two runs. Evaluation on the development set shows that the metaclassifier achieves a better performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Anaphora resolution.</title>
<date>2002</date>
<publisher>Longman.</publisher>
<contexts>
<context position="1277" citStr="Mitkov, 2002" startWordPosition="194" endWordPosition="195">To this end, we assumed that it is possible to learn which is the correct form for a referential expression given the context in which it appears. The remainder of the paper is structured as follows: Section 2 presents the method used in this paper. Section 3 presents the evaluation results on the development set. The paper finishes with conclusions. 2 Method The method used to solve the GREC-NEG task was inspired by the machine learning approaches employed for coreference resolution. In these methods, pairs of entities are classified as coreferential or not on the basis of a set of features (Mitkov, 2002). In the same manner, each REF element from the text to be processed is paired with all the REFEX elements in its chain and machine learning is used to determine the lexical form of which candidate REFEX element can be used in the given context. To achieve this, a set of features was derived after a corpus investigation. As can be seen, some of these features are similar to those used by resolution algorithms (e.g. distance between entities), whilst others are specific for the task (e.g. empty markers). The features used for a (REF, REFEX) pair are: • Whether the REF element is the first menti</context>
</contexts>
<marker>Mitkov, 2002</marker>
<rawString>Ruslan Mitkov. 2002. Anaphora resolution. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="5013" citStr="Witten and Frank, 2005" startWordPosition="831" endWordPosition="834">air is classified independently, it is possible to have zero, one or more good REFEX candidates for a given REF. Therefore, the system uses the confidence returned by the classifier to rank the candidates and selects the one that has the highest probability of being good, regardless of the class assigned by the classifier. In this way the system selects exactly one REFEX for each REF. 3 Evaluation The method proposed in this paper was evaluated using two classifiers, both trained on the same set of features. The first classifier is the standard J48 decision tree algorithm implemented in Weka (Witten and Frank, 2005). The run that used this classifier is referred to in the rest of the paper as standard run. Given the large number of negative examples present in our training data, a metaclassifier that is cost-sensitive was used for the second run. In our case, the meta-classifier relies on J48 and reweights training instances according to the total cost assigned to each class. After 1http://opennlp.sourceforge.net/ experimenting with different cost matrices, we decided to assign a cost of 3 to false negatives and 1 to false positives, in this way biasing the classifier towards a higher recall for YES answ</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>