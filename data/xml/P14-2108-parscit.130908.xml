<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004754">
<title confidence="0.9963145">
The Penn Parsed Corpus of Modern British English:
First Parsing Results and Analysis
</title>
<author confidence="0.788897">
Seth Kulick Anthony Kroch and Beatrice Santorini
Linguistic Data Consortium Dept. of Linguistics
</author>
<affiliation confidence="0.999621">
University of Pennsylvania University of Pennsylvania
</affiliation>
<email confidence="0.999016">
skulick@ldc.upenn.edu {kroch,beatrice}@ling.upenn.edu
</email>
<sectionHeader confidence="0.993897" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999615">
This paper presents the first results on
parsing the Penn Parsed Corpus of Mod-
ern British English (PPCMBE), a million-
word historical treebank with an annota-
tion style similar to that of the Penn Tree-
bank (PTB). We describe key features of
the PPCMBE annotation style that differ
from the PTB, and present some exper-
iments with tree transformations to bet-
ter compare the results to the PTB. First
steps in parser analysis focus on problem-
atic structures created by the parser.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998608">
We present the first parsing results for the
Penn Parsed Corpus of Modern British English
(PPCMBE) (Kroch et al., 2010), showing that it
can be parsed at a few points lower in F-score than
the Penn Treebank (PTB) (Marcus et al., 1999).
We discuss some of the differences in annotation
style and source material that make a direct com-
parison problematic. Some first steps at analysis
of the parsing results indicate aspects of the anno-
tation style that are difficult for the parser, and also
show that the parser is creating structures that are
not present in the training material.
The PPCMBE is a million-word treebank cre-
ated for researching changes in English syntax. It
covers the years 1700-1914 and is the most mod-
ern in the series of treebanks created for histori-
cal research.1 Due to the historical nature of the
PPCMBE, it shares some of the characteristics of
treebanks based on modern unedited text (Bies et
al., 2012), such as spelling variation.
</bodyText>
<footnote confidence="0.905755">
1The other treebanks in the series cover Early Modern En-
glish (Kroch et al., 2004) (1.8 million words), Middle Eng-
lish (Kroch and Taylor, 2000) (1.2 million words), and Early
English Correspondence (Taylor et al., 2006) (2.2 million
words).
</footnote>
<bodyText confidence="0.9996308">
The size of the PPCMBE is roughly the same
as the WSJ section of the PTB, and its annotation
style is similar to that of the PTB, but with dif-
ferences, particularly with regard to coordination
and NP structure. However, except for Lin et al.
(2012), we have found no discussion of this corpus
in the literature.2 There is also much additional
material annotated in this style, increasing the im-
portance of analyzing parser performance on this
annotation style.3
</bodyText>
<sectionHeader confidence="0.913296" genericHeader="method">
2 Corpus description
</sectionHeader>
<bodyText confidence="0.9992604">
The PPCMBE4 consists of 101 files, but we leave
aside 7 files that consist of legal material with very
different properties than the rest of the corpus.
The remaining 94 files contain 1,018,736 tokens
(words).
</bodyText>
<subsectionHeader confidence="0.927018">
2.1 Part-of-speech tags
</subsectionHeader>
<bodyText confidence="0.999795">
The PPCMBE uses a part-of-speech (POS) tag set
containing 248 POS tags, in contrast to the 45 tags
used by the PTB. The more complex tag set is
mainly due to the desire to tag orthographic vari-
ants consistently throughout the series of historical
corpora. For example “gentlemen” and its ortho-
graphic variant “gen’l’men” are tagged with the
complex tag ADJ+NS (adjective and plural noun)
on the grounds that in earlier time periods, the lex-
ical item is spelled and tagged as two orthographic
words (“gentle”/ADJ and “men”/NS).
While only 81 of the 248 tags are “simple” (i.e.,
not associated with lexical merging or splitting),
</bodyText>
<footnote confidence="0.996927181818182">
2Lin et al. (2012) report some results on POS tagging us-
ing their own mapping to different tags, but no parsing results.
3Aside from the corpora listed in fn. 1, there are also
historical corpora of Old English (Taylor et al., 2003), Ice-
landic (Wallenberg et al., 2011), French (Martineau and oth-
ers, 2009), and Portuguese (Galves and Faria, 2010), totaling
4.5 million words.
4We are working with a pre-release copy of the next re-
vision of the official version. Some annotation errors in the
currently available version have been corrected, but the dif-
ferences are relatively minor.
</footnote>
<page confidence="0.875766">
662
</page>
<note confidence="0.458988">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 662–667,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.99900475">
Type # Tags # Tokens % coverage
Simple 81 1,005,243 98.7%
Complex 167 13,493 1.3%
Total 248 1,018,736 100.0%
</table>
<tableCaption confidence="0.9787695">
Table 1: Distribution of POS tags. Complex tags
indicate lexical merging or splitting.
</tableCaption>
<figureCaption confidence="0.989607">
Figure 1: Coordination in the PPCMBE (1a) and
the PTB (1b).
</figureCaption>
<bodyText confidence="0.999619428571428">
they cover the vast majority of the words in the
corpus, as summarized in Table 1. Of these 81
tags, some are more specialized than in the PTB,
accounting for the increased number of tags com-
pared to the PTB. For instance, for historical con-
sistency, words like “one” and “else” each have
their own tag.
</bodyText>
<subsectionHeader confidence="0.999654">
2.2 Syntactic annotation
</subsectionHeader>
<bodyText confidence="0.9999188">
As mentioned above, the syntactic annotation
guidelines do not differ radically from those of the
PTB. There are some important differences, how-
ever, which we highlight in the following three
subsections.
</bodyText>
<subsectionHeader confidence="0.912995">
2.2.1 Coordination
</subsectionHeader>
<bodyText confidence="0.998476285714286">
A coordinating conjunction and conjunct form a
CONJP, as shown in (1a) in Figure 1. (1b) shows
the corresponding annotation in the PTB.
In a conjoined NP, if part of a first conjunct
potentially scopes over two or more conjuncts
(shared pre-modifiers), the first conjunct has no
phrasal node in the PPCMBE, and the label of the
</bodyText>
<listItem confidence="0.933372">
(2) (a) NP (b) NP
</listItem>
<figureCaption confidence="0.606774">
Figure 3: (3a) shows that a PP is sister to the
</figureCaption>
<bodyText confidence="0.987358222222222">
noun in the PPCMBE, in contrast to the adjunction
structure in the PTB (3b). (4ab) show that clausal
complements and modifiers of a noun are distin-
guished by function tags, rather than structurally
as in the PTB, which would adjoin the CP in (a),
but not in (b).
subsequent conjuncts becomes NX instead of NP,
as shown in (2a) in Figure 2. The corresponding
PTB annotation is flat, as in (2b).5
</bodyText>
<subsectionHeader confidence="0.860377">
2.2.2 Noun Phrase structure
</subsectionHeader>
<bodyText confidence="0.99969452631579">
Neither the PPCMBE nor the PTB distinguish be-
tween PP complements and modifiers of nouns.
However, the PPCMBE annotates both types of
dependents as sisters of the noun, while the PTB
adjoins both types. For instance in (3a) in Fig-
ure 3, the modifier PP is a sister to the noun in
the PPCMBE, while in (3b), the complement PP
is adjoined in the PTB.
Clausal complements and modifiers are also
both treated as sisters to the noun in the PPCMBE.
In this case, though, the complement/modifier dis-
tinction is encoded by a function tag. For exam-
ple, in (4a) and (4b), the status of the CPs as mod-
ifier and complement is indicated by their func-
tion tags: REL for relative clause and THT “that”
complement. In the PTB, the distinction would be
encoded structurally; the relative clause would be
adjoined, whereas the “that” complement would
not.
</bodyText>
<figure confidence="0.997997172413793">
(1) (a) NP
(b) NP
NP
a Ham
CONJP
and NP
a Hare
NP
a Ham
and NP
a Hare
(3) (a) NP
(b) NP
(4) (a) NP
(b) NP
a conviction CP-THT
that..
The Spiders CP-REL
which have..
NP
a teacher
PP
of chemistry
The back PP
of this Spider
their husbands CONJP
or NX
fathers
their husbands or fathers
</figure>
<figureCaption confidence="0.980042666666667">
Figure 2: (2a) is an example of coordination with
a shared pre-modifier in the PPCMBE, and (2b)
shows the corresponding annotation in the PTB.
</figureCaption>
<subsectionHeader confidence="0.963765">
2.2.3 Clausal structure
</subsectionHeader>
<bodyText confidence="0.999954">
The major difference in the clausal structure as
compared to the PTB is the absence of a VP level6,
yielding flatter trees than in the PTB. An example
clause is shown in (5) in Figure 4.
</bodyText>
<footnote confidence="0.99783825">
5Similar coordination structures exist for categories other
than NP, although NP is by far the most common.
6This is due to the changing headedness of VP in the over-
all series of English historical corpora.
</footnote>
<page confidence="0.994527">
663
</page>
<table confidence="0.754337833333333">
Section # Files Token count %
Train 81 890,150 87.4%
Val 4 38,670 3.8%
Dev 4 39,527 3.9%
Test 5 50,389 4.9%
Total 94 1,018,736 100.0%
</table>
<figure confidence="0.984761333333333">
(5) IP
was shot PP
with NP
three Arrows
NP-SBJ
The poor fellow
</figure>
<figureCaption confidence="0.9743498">
Figure 4: An example of clausal structure, without
VP.
Figure 5: (6a) shows how (3a) is transformed in
the “reduced +NPs” version to include a level of
NP recursion, and (6b) shows the same for (4a).
</figureCaption>
<sectionHeader confidence="0.984587" genericHeader="method">
3 Corpus transformations
</sectionHeader>
<bodyText confidence="0.999972666666667">
We refer to the pre-release version of the corpus
described in Section 2 as the “Release” version,
and experiment with three other corpus versions.
</bodyText>
<subsectionHeader confidence="0.975865">
3.1 Reduced
</subsectionHeader>
<bodyText confidence="0.9999512">
As mentioned earlier, the PPCMBE’s relatively
large POS tag set aims to maximize annotation
consistency across the entire time period covered
by the historical corpora, beginning with Middle
English. Since we are concerned here with pars-
ing just the PPCMBE, we simplified the tag set.
The complex tags are simplified in a fully deter-
ministic way, based on the trees and the tags. For
example, the POS tag for “gentleman”, originally
ADJ+N is changed to N. The P tag is split, so that
it is either left as P, if a preposition, or changed
to CONJS, if a subordinating conjunction. The re-
duced tag set contains 76 tags. We call the version
of the corpus with the reduced tag set the “Re-
duced” version.
</bodyText>
<subsectionHeader confidence="0.999007">
3.2 Reduced+NPs
</subsectionHeader>
<bodyText confidence="0.999630777777778">
As discussed in Section 2.2.2, noun modifiers are
sisters to the noun, instead of being adjoined, as in
the PTB. As a result, there are fewer NP brackets
in the PPCMBE than there would be if the PTB-
style were followed. To evaluate the effect of the
difference in annotation guidelines on the parsing
score, we added PTB-style NP brackets to the re-
duced corpus described in Section 3.1. For ex-
ample, (3a) in Figure 3 is transformed into (6a)
</bodyText>
<tableCaption confidence="0.98659">
Table 2: Token count and data split for PPCMBE
</tableCaption>
<bodyText confidence="0.9997975">
in Figure 5, and likewise (4a) is transformed into
(6b). However, (4b) remains as it is, because the
following CP in that case is a complement, as in-
dicated by the THT function tag. This is a signif-
icant transformation of the corpus, adding 43,884
NPs to the already-existing 291,422.
</bodyText>
<subsectionHeader confidence="0.992867">
3.3 Reduced+NPs+VPs
</subsectionHeader>
<bodyText confidence="0.999996">
We carry out a similar transformation to add VP
nodes to the IPs in the Reduced+NPs version,
making them more like the clausal structures in
the PTB. This added 169,877 VP nodes to the cor-
pus (there are 131,671 IP nodes, some of which
contain more than one auxiliary verb).
It is worth emphasizing that the brackets added
in Sections 3.2 and 3.3 add no information, since
they are added automatically. They are added only
to roughly compensate for the difference in anno-
tation styles between the PPCMBE and the PTB.
</bodyText>
<sectionHeader confidence="0.992576" genericHeader="method">
4 Data split
</sectionHeader>
<bodyText confidence="0.999739736842105">
We split the data into four sections, as shown in
Table 2. The validation section consists of the four
files beginning with “a” or “v” (spanning the years
1711-1860), the development section consists of
the four files beginning with “l” (1753-1866), the
test section consists of the five files beginning with
“f” (1749-1900), and the training section consists
of the remaining 81 files (1712-1913). The data
split sizes used here for the PPCMBE closely ap-
proximate that used for the PTB, as described in
Petrov et al. (2006).7 For this first work, we used
a split that was roughly the same as far as time-
spans across the four sections. In future work, we
will do a more proper cross-validation evaluation.
Table 3 shows the average sentence length and
percentage of sentences of length &lt;= 40 in the
PPCMBE and PTB. The PPCMBE sentences are
a bit longer on average, and fewer are of length
&lt;= 40. However, the match is close enough that
</bodyText>
<footnote confidence="0.998897">
7Sections 2-21 for Training Section 1 for Val, 22 for Dev
and 23 for Test.
</footnote>
<figure confidence="0.9961009">
(6) (a) NP
(b)NP
NP
The back
PP
of this Spider
NP
The Spiders
CP-REL
which have..
</figure>
<page confidence="0.987444">
664
</page>
<table confidence="0.995129333333333">
Gold Tags Parser Tags
all &lt;=40 all &lt;=40
Corpus Prec Rec F Prec Rec F Prec Rec F Prec Rec F Tags
1 Rl/Dev 83.7 83.7 83.7 86.3 86.4 86.3 83.8 83.1 83.4 86.2 85.8 86.0 96.9
2 Rd/Dev 84.9 84.5 84.7 86.6 86.7 86.7 84.5 83.7 84.1 86.5 86.2 86.3 96.9
3 Rd/Tst 85.8 85.2 85.5 87.9 87.3 87.6 84.8 83.9 84.3 86.7 85.8 86.2 97.1
4 RdNPs/Dev 87.1 86.3 86.7 88.9 88.5 88.7 86.3 85.1 85.7 88.4 87.6 88.0 96.9
5 RdNPsVPs/Dev 87.2 87.0 87.1 89.5 89.4 89.5 86.3 85.7 86.0 88.6 88.2 88.4 97.0
6 PTB/23 90.3 89.8 90.1 90.9 90.4 90.6 90.0 89.5 89.8 90.6 90.1 90.3 96.9
</table>
<tableCaption confidence="0.80251525">
Table 4: Parsing results with Berkeley Parser. The corpus versions used are Release (Rl), Reduced (Rd),
Reduced+NPs (RdNPs), and Reduced+NPs+VPs (RdNPsVPs). Results are shown for the parser forced
to use the gold POS tags from the corpus, and with the parser supplying its own tags. For the latter case,
the tagging accuracy is shown in the last column.
</tableCaption>
<table confidence="0.9996646">
Corpus Section Avg. len % &lt;= 40
PPCMBE Dev 24.1 85.5
Test 21.2 89.9
PTB Dev 23.6 92.9
Test 23.5 91.3
</table>
<tableCaption confidence="0.999649">
Table 3: Average sentence length and percentage
</tableCaption>
<bodyText confidence="0.79794075">
of sentences of length &lt;=40 in the PPCMBE and
PTB.
we will report the parsing results for sentences of
length &lt;= 40 and all sentences, as with the PTB.
</bodyText>
<sectionHeader confidence="0.971389" genericHeader="method">
5 Parsing Experiments
</sectionHeader>
<bodyText confidence="0.986322235294118">
The PPCMBE is a phrase-structure corpus, and so
we parse with the Berkeley parser (Petrov et al.,
2008) and score using the standard evalb program
(Sekine and Collins, 2008). We used the Train and
Val sections for training, with the parser using the
Val section for fine-tuning parameters (Petrov et
al., 2006). Since the Berkeley parser is capable
of doing its own POS tagging, we ran it using the
gold tags or supplying its own tags. Table 4 shows
the results for both modes.8
Consider first the results for the Dev section
with the parser using the gold tags. The score
for all sentences increases from 83.7 for the Re-
lease corpus (row 1) to 84.7 for the Reduced cor-
pus (row 2), reflecting the POS tag simplifications
in the Reduced corpus. The score goes up by a fur-
ther 2.0 to 86.7 (row 2 to 4) for the Reduced+NPs
corpus and up again by 0.4 to 87.1 (row 5) for
the Reduced+NPs+VPs corpus, showing the ef-
8We modified the evalb parameter file to exclude punctu-
ation in PPCMBE, just as for PTB. The results are based on a
single run for each corpus/section. We expect some variance
to occur, and in future work will average results over several
runs of the training/Dev cycle, following Petrov et al. (2006).
fects of the extra NP and VP brackets. We evalu-
ated the Test section on the Reduced corpus (row
3), with a result 0.8 higher than the Dev (85.5 in
row 3 compared to 84.7 in row 2). The score for
sentences of length &lt;= 40 (a larger percentage
of the PPCMBE than the PTB) is 2.4 higher than
the score for all sentences, with both the gold and
parser tags (row 5).
The results with the parser choosing its own
POS tags naturally go down, with the Test section
suffering more. In general, the PPCMBE is af-
fected by the lack of gold tags more than the PTB.
In sum, the parser results show that the
PPCMBE can be parsed at a level approaching that
of the PTB. We are not proposing that the current
version be replaced by the Reduced+NPs+VPs
version, on the grounds that the latter gets the
highest score. Our goal was to determine whether
the parsing results fell in the same general range
as for the PTB by roughly compensating for the
difference in annotation style. The results in Table
4 show that this is the case.
As a final note, the PPCMBE consists of
unedited data spanning more than 200 years, while
the PTB is edited newswire, and so to some extent
there would almost certainly be some difference in
score.
</bodyText>
<sectionHeader confidence="0.978053" genericHeader="method">
6 Parser Analysis
</sectionHeader>
<bodyText confidence="0.999961857142857">
We are currently developing techniques to better
understand the types of errors is making, which
have already led to interesting results. The parser
is creating some odd structures that violate basic
well-formedness conditions of clauses. Tree (7a)
in Figure 6 is a tree from from the “Reduced” cor-
pus, in which the verb “formed” projects to IP,
</bodyText>
<page confidence="0.991398">
665
</page>
<figure confidence="0.988501">
(7) (a) IP-SUB
NP-SBJ had been formed PP
ADVP-TMP acting
(b) IP now
causes
</figure>
<figureCaption confidence="0.999991">
Figure 6: Examples of issues with parser output
</figureCaption>
<bodyText confidence="0.999988923076923">
with two auxiliary verbs (“had” and “been”). In
the corresponding parser output (7b), the parser
misses the reduced relative RRC, turning “acting”
into the rightmost verb in the IP. The parser is cre-
ating an IP with two main verbs - an ungrammati-
cal structure that is not attested in the gold.
It might be thought that the parser is having
trouble with the flat-IP annotation style, but the
parser posits incorrect structures that are not at-
tested in the gold even in the Reduced+NPs+VPs
version of the corpus. Tree (8a) shows a fragment
of a gold tree from the corpus, with the VPs ap-
propriately inserted. The parser output (8b) has
an extra IP above “teaching”. The POS tags for
“be” (BE) and “teaching“ (VAG) do not appear in
this configuration at all in the training material. In
general, the parser seems to be getting confused
as to when such an IP should appear. We hypoth-
esized that this is due to confusion with infiniti-
val clauses, which can have an unary-branching IP
over a VP, as in the gold tree (9). We retrained the
parser, directing it to retain the INF function tag
that appears in infinitival clauses as in (9). Over-
all, the evalb score went down slightly, but it did
fix cases such as (8b). We do not yet know why the
overall score went down, but what’s surprising is
one would have thought that IP-INF is recoverable
from the absence of a tensed verb.
Preliminary analysis shows that the CONJP
structures are also difficult for the parser. Since
these are structures that are different than the
PTB9, we were particularly interested in them.
Cases where the CONJP is missing an overt co-
ordinating cord (such as “and”), are particularly
difficult, not surprisingly. These can appear as in-
termediate conjuncts in a string of conjuncts, with
the structure (CONJP word). The shared pre-
modifier structure described in (2a) is also difficult
for the parser.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999961476190476">
We have presented the first results on parsing the
PPCMBE and discussed some significant annota-
tion style differences from the PTB. Adjusting for
two major differences that are a matter of anno-
tation convention, we showed that the PPCMBE
can be parsed at approximately the same level of
accuracy as the PTB. The first steps in an inves-
tigation of the parser differences show that the
parser is generating structures that violate basic
well-formedness conditions of the annotation.
For future work, we will carry out a more se-
rious analysis of the parser output, trying to more
properly account for the differences in bracketing
structure between the PPCMBE and PTB. There
is also a great deal of data annotated in the style
of the PPCMBE, as indicated in footnotes 1 and
3, and we are interested in how the parser per-
forms on these, especially comparing the results
on the modern English corpora to the older histor-
ical ones, which will have greater issues of ortho-
graphic and tokenization complications.
</bodyText>
<sectionHeader confidence="0.998299" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9979555">
This work was supported by National Science
Foundation Grant # BCS-114749. We would like
to thank Ann Bies, Justin Mott, and Mark Liber-
man for helpful discussions.
</bodyText>
<footnote confidence="0.9628185">
9The CONJP nonterminal in the PTB serves a different
purpose than in the PPCMBE and is much more limited.
</footnote>
<figure confidence="0.997583346153846">
by NP
causes RRC
the earth’s crust
NP had been formed PP
by NP
ADVP acting
now
the earth’s crust
(9) IP
It VP
is IP-INF
VP
to VP
be VP
observed
(b) VP
would VP
be IP
VP
teaching NP
the doctrine
the doctrine
(8) (a) VP
would VP
be VP
teaching NP
</figure>
<page confidence="0.990213">
666
</page>
<sectionHeader confidence="0.995043" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999708421875">
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. LDC2012T13. Lin-
guistic Data Consortium.
Charlotte Galves and Pabol Faria. 2010. Tycho
Brahe Parsed Corpus of Historical Portuguese.
http://www.tycho.iel.unicamp.br/
˜tycho/corpus/en/index.html.
Anthony Kroch and Ann Taylor. 2000. Penn-
Helsinki Parsed Corpus of Middle English, second
edition. http://www.ling.upenn.edu/
hist-corpora/PPCME2-RELEASE-3/
index.html.
Anthony Kroch, Beatrice Santorini, and Ariel
Diertani. 2004. Penn-Helsinki Parsed Cor-
pus of Early Modern English. http:
//www.ling.upenn.edu/hist-corpora/
PPCEME-RELEASE-2/index.html.
Anthony Kroch, Beatrice Santorini, and Ariel Dier-
tani. 2010. Penn Parsed Corpus of Modern
British English. http://www.ling.upenn.
edu/hist-corpora/PPCMBE-RELEASE-1/
index.html.
Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the google books ngram
corpus. In Proceedings of the ACL 2012 System
Demonstrations, pages 169–174, Jeju Island, Korea,
July. Association for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
LDC99T42, Linguistic Data Consortium, Philadel-
phia.
France Martineau et al. 2009. Mod´eliser le change-
ment: les voies du franc¸ais, a Parsed Corpus of His-
torical French.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL, pages 433–440, Sydney, Australia,
July. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2008. The Berkeley Parser.
https://code.google.com/p/berkeleyparser/.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Ann Taylor, Anthony Warner, Susan Pintzuk,
and Frank Beths. 2003. The York-Toronto-
Helsinki Parsed Corpus of Old English Prose.
Distributed through the Oxford Text Archive.
http://www-users.york.ac.uk/
˜lang22/YCOE/YcoeHome.htm.
Ann Taylor, Arja Nurmi, Anthony Warner, Susan
Pintzuk, and Terttu Nevalainen. 2006. Parsed
Corpus of Early English Correspondence. Com-
piled by the CEEC Project Team. York: Uni-
versity of York and Helsinki: University of
Helsinki. Distributed through the Oxford Text
Archive. http://www-users.york.ac.uk/
˜lang22/PCEEC-manual/index.htm.
Joel Wallenberg, Anton Karl Ingason, Einar Freyr
Sigursson, and Eirkur Rgnvaldsson. 2011.
Icelandic Parsed Historical Corpus (IcePaHC)
version 0.4. http://www.linguist.is/
icelandic_treebank.
</reference>
<page confidence="0.997142">
667
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.952108">
<title confidence="0.998525">The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis</title>
<author confidence="0.991068">Kulick Anthony Kroch Santorini</author>
<affiliation confidence="0.992811">Linguistic Data Consortium Dept. of Linguistics University of Pennsylvania University of</affiliation>
<abstract confidence="0.997937538461538">This paper presents the first results on parsing the Penn Parsed Corpus of Modern British English (PPCMBE), a millionword historical treebank with an annotation style similar to that of the Penn Treebank (PTB). We describe key features of the PPCMBE annotation style that differ from the PTB, and present some experiments with tree transformations to better compare the results to the PTB. First steps in parser analysis focus on problematic structures created by the parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Justin Mott</author>
<author>Colin Warner</author>
<author>Seth Kulick</author>
</authors>
<date>2012</date>
<booktitle>English Web Treebank. LDC2012T13. Linguistic Data Consortium.</booktitle>
<contexts>
<context position="1719" citStr="Bies et al., 2012" startWordPosition="275" endWordPosition="278">that make a direct comparison problematic. Some first steps at analysis of the parsing results indicate aspects of the annotation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material. The PPCMBE is a million-word treebank created for researching changes in English syntax. It covers the years 1700-1914 and is the most modern in the series of treebanks created for historical research.1 Due to the historical nature of the PPCMBE, it shares some of the characteristics of treebanks based on modern unedited text (Bies et al., 2012), such as spelling variation. 1The other treebanks in the series cover Early Modern English (Kroch et al., 2004) (1.8 million words), Middle English (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words). The size of the PPCMBE is roughly the same as the WSJ section of the PTB, and its annotation style is similar to that of the PTB, but with differences, particularly with regard to coordination and NP structure. However, except for Lin et al. (2012), we have found no discussion of this corpus in the literature.2 There is also mu</context>
</contexts>
<marker>Bies, Mott, Warner, Kulick, 2012</marker>
<rawString>Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 2012. English Web Treebank. LDC2012T13. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlotte Galves</author>
<author>Pabol Faria</author>
</authors>
<title>Tycho Brahe Parsed Corpus of Historical Portuguese.</title>
<date>2010</date>
<note>http://www.tycho.iel.unicamp.br/ ˜tycho/corpus/en/index.html.</note>
<contexts>
<context position="3682" citStr="Galves and Faria, 2010" startWordPosition="603" endWordPosition="606"> ADJ+NS (adjective and plural noun) on the grounds that in earlier time periods, the lexical item is spelled and tagged as two orthographic words (“gentle”/ADJ and “men”/NS). While only 81 of the 248 tags are “simple” (i.e., not associated with lexical merging or splitting), 2Lin et al. (2012) report some results on POS tagging using their own mapping to different tags, but no parsing results. 3Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Icelandic (Wallenberg et al., 2011), French (Martineau and others, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words. 4We are working with a pre-release copy of the next revision of the official version. Some annotation errors in the currently available version have been corrected, but the differences are relatively minor. 662 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 662–667, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Type # Tags # Tokens % coverage Simple 81 1,005,243 98.7% Complex 167 13,493 1.3% Total 248 1,018,736 100.0% Table 1: Distribution of POS tags. Comp</context>
</contexts>
<marker>Galves, Faria, 2010</marker>
<rawString>Charlotte Galves and Pabol Faria. 2010. Tycho Brahe Parsed Corpus of Historical Portuguese. http://www.tycho.iel.unicamp.br/ ˜tycho/corpus/en/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Kroch</author>
<author>Ann Taylor</author>
</authors>
<title>PennHelsinki Parsed Corpus of Middle English,</title>
<date>2000</date>
<note>second edition. http://www.ling.upenn.edu/ hist-corpora/PPCME2-RELEASE-3/ index.html.</note>
<contexts>
<context position="1892" citStr="Kroch and Taylor, 2000" startWordPosition="304" endWordPosition="307">r, and also show that the parser is creating structures that are not present in the training material. The PPCMBE is a million-word treebank created for researching changes in English syntax. It covers the years 1700-1914 and is the most modern in the series of treebanks created for historical research.1 Due to the historical nature of the PPCMBE, it shares some of the characteristics of treebanks based on modern unedited text (Bies et al., 2012), such as spelling variation. 1The other treebanks in the series cover Early Modern English (Kroch et al., 2004) (1.8 million words), Middle English (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words). The size of the PPCMBE is roughly the same as the WSJ section of the PTB, and its annotation style is similar to that of the PTB, but with differences, particularly with regard to coordination and NP structure. However, except for Lin et al. (2012), we have found no discussion of this corpus in the literature.2 There is also much additional material annotated in this style, increasing the importance of analyzing parser performance on this annotation style.3 2 Corpus description The PPCMBE4 consist</context>
</contexts>
<marker>Kroch, Taylor, 2000</marker>
<rawString>Anthony Kroch and Ann Taylor. 2000. PennHelsinki Parsed Corpus of Middle English, second edition. http://www.ling.upenn.edu/ hist-corpora/PPCME2-RELEASE-3/ index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Kroch</author>
<author>Beatrice Santorini</author>
<author>Ariel Diertani</author>
</authors>
<title>Penn-Helsinki Parsed Corpus of Early Modern English.</title>
<date>2004</date>
<note>http: //www.ling.upenn.edu/hist-corpora/ PPCEME-RELEASE-2/index.html.</note>
<contexts>
<context position="1831" citStr="Kroch et al., 2004" startWordPosition="294" endWordPosition="297"> of the annotation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material. The PPCMBE is a million-word treebank created for researching changes in English syntax. It covers the years 1700-1914 and is the most modern in the series of treebanks created for historical research.1 Due to the historical nature of the PPCMBE, it shares some of the characteristics of treebanks based on modern unedited text (Bies et al., 2012), such as spelling variation. 1The other treebanks in the series cover Early Modern English (Kroch et al., 2004) (1.8 million words), Middle English (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words). The size of the PPCMBE is roughly the same as the WSJ section of the PTB, and its annotation style is similar to that of the PTB, but with differences, particularly with regard to coordination and NP structure. However, except for Lin et al. (2012), we have found no discussion of this corpus in the literature.2 There is also much additional material annotated in this style, increasing the importance of analyzing parser performance on thi</context>
</contexts>
<marker>Kroch, Santorini, Diertani, 2004</marker>
<rawString>Anthony Kroch, Beatrice Santorini, and Ariel Diertani. 2004. Penn-Helsinki Parsed Corpus of Early Modern English. http: //www.ling.upenn.edu/hist-corpora/ PPCEME-RELEASE-2/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Kroch</author>
<author>Beatrice Santorini</author>
<author>Ariel Diertani</author>
</authors>
<title>Penn Parsed Corpus of Modern British English.</title>
<date>2010</date>
<note>http://www.ling.upenn. edu/hist-corpora/PPCMBE-RELEASE-1/ index.html.</note>
<contexts>
<context position="909" citStr="Kroch et al., 2010" startWordPosition="135" endWordPosition="138">Abstract This paper presents the first results on parsing the Penn Parsed Corpus of Modern British English (PPCMBE), a millionword historical treebank with an annotation style similar to that of the Penn Treebank (PTB). We describe key features of the PPCMBE annotation style that differ from the PTB, and present some experiments with tree transformations to better compare the results to the PTB. First steps in parser analysis focus on problematic structures created by the parser. 1 Introduction We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (Kroch et al., 2010), showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) (Marcus et al., 1999). We discuss some of the differences in annotation style and source material that make a direct comparison problematic. Some first steps at analysis of the parsing results indicate aspects of the annotation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material. The PPCMBE is a million-word treebank created for researching changes in English syntax. It covers the years 1700-1914 and is the most mo</context>
</contexts>
<marker>Kroch, Santorini, Diertani, 2010</marker>
<rawString>Anthony Kroch, Beatrice Santorini, and Ariel Diertani. 2010. Penn Parsed Corpus of Modern British English. http://www.ling.upenn. edu/hist-corpora/PPCMBE-RELEASE-1/ index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Lin</author>
<author>Jean-Baptiste Michel</author>
<author>Erez Aiden Lieberman</author>
<author>Jon Orwant</author>
<author>Will Brockman</author>
<author>Slav Petrov</author>
</authors>
<title>Syntactic annotations for the google books ngram corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>169--174</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2238" citStr="Lin et al. (2012)" startWordPosition="364" endWordPosition="367"> shares some of the characteristics of treebanks based on modern unedited text (Bies et al., 2012), such as spelling variation. 1The other treebanks in the series cover Early Modern English (Kroch et al., 2004) (1.8 million words), Middle English (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words). The size of the PPCMBE is roughly the same as the WSJ section of the PTB, and its annotation style is similar to that of the PTB, but with differences, particularly with regard to coordination and NP structure. However, except for Lin et al. (2012), we have found no discussion of this corpus in the literature.2 There is also much additional material annotated in this style, increasing the importance of analyzing parser performance on this annotation style.3 2 Corpus description The PPCMBE4 consists of 101 files, but we leave aside 7 files that consist of legal material with very different properties than the rest of the corpus. The remaining 94 files contain 1,018,736 tokens (words). 2.1 Part-of-speech tags The PPCMBE uses a part-of-speech (POS) tag set containing 248 POS tags, in contrast to the 45 tags used by the PTB. The more comple</context>
</contexts>
<marker>Lin, Michel, Lieberman, Orwant, Brockman, Petrov, 2012</marker>
<rawString>Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman, Jon Orwant, Will Brockman, and Slav Petrov. 2012. Syntactic annotations for the google books ngram corpus. In Proceedings of the ACL 2012 System Demonstrations, pages 169–174, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<booktitle>Treebank-3. LDC99T42, Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="1024" citStr="Marcus et al., 1999" startWordPosition="157" endWordPosition="160">), a millionword historical treebank with an annotation style similar to that of the Penn Treebank (PTB). We describe key features of the PPCMBE annotation style that differ from the PTB, and present some experiments with tree transformations to better compare the results to the PTB. First steps in parser analysis focus on problematic structures created by the parser. 1 Introduction We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (Kroch et al., 2010), showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) (Marcus et al., 1999). We discuss some of the differences in annotation style and source material that make a direct comparison problematic. Some first steps at analysis of the parsing results indicate aspects of the annotation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material. The PPCMBE is a million-word treebank created for researching changes in English syntax. It covers the years 1700-1914 and is the most modern in the series of treebanks created for historical research.1 Due to the historical nature of the PPCMBE, it sh</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank-3. LDC99T42, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>France Martineau</author>
</authors>
<title>Mod´eliser le changement: les voies du franc¸ais, a Parsed Corpus of Historical French.</title>
<date>2009</date>
<marker>Martineau, 2009</marker>
<rawString>France Martineau et al. 2009. Mod´eliser le changement: les voies du franc¸ais, a Parsed Corpus of Historical French.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="10588" citStr="Petrov et al. (2006)" startWordPosition="1813" endWordPosition="1816">compensate for the difference in annotation styles between the PPCMBE and the PTB. 4 Data split We split the data into four sections, as shown in Table 2. The validation section consists of the four files beginning with “a” or “v” (spanning the years 1711-1860), the development section consists of the four files beginning with “l” (1753-1866), the test section consists of the five files beginning with “f” (1749-1900), and the training section consists of the remaining 81 files (1712-1913). The data split sizes used here for the PPCMBE closely approximate that used for the PTB, as described in Petrov et al. (2006).7 For this first work, we used a split that was roughly the same as far as timespans across the four sections. In future work, we will do a more proper cross-validation evaluation. Table 3 shows the average sentence length and percentage of sentences of length &lt;= 40 in the PPCMBE and PTB. The PPCMBE sentences are a bit longer on average, and fewer are of length &lt;= 40. However, the match is close enough that 7Sections 2-21 for Training Section 1 for Val, 22 for Dev and 23 for Test. (6) (a) NP (b)NP NP The back PP of this Spider NP The Spiders CP-REL which have.. 664 Gold Tags Parser Tags all &lt;</context>
<context position="12697" citStr="Petrov et al., 2006" startWordPosition="2204" endWordPosition="2207">n Avg. len % &lt;= 40 PPCMBE Dev 24.1 85.5 Test 21.2 89.9 PTB Dev 23.6 92.9 Test 23.5 91.3 Table 3: Average sentence length and percentage of sentences of length &lt;=40 in the PPCMBE and PTB. we will report the parsing results for sentences of length &lt;= 40 and all sentences, as with the PTB. 5 Parsing Experiments The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008) and score using the standard evalb program (Sekine and Collins, 2008). We used the Train and Val sections for training, with the parser using the Val section for fine-tuning parameters (Petrov et al., 2006). Since the Berkeley parser is capable of doing its own POS tagging, we ran it using the gold tags or supplying its own tags. Table 4 shows the results for both modes.8 Consider first the results for the Dev section with the parser using the gold tags. The score for all sentences increases from 83.7 for the Release corpus (row 1) to 84.7 for the Reduced corpus (row 2), reflecting the POS tag simplifications in the Reduced corpus. The score goes up by a further 2.0 to 86.7 (row 2 to 4) for the Reduced+NPs corpus and up again by 0.4 to 87.1 (row 5) for the Reduced+NPs+VPs corpus, showing the ef8</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of COLING-ACL, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
</authors>
<location>Romain Thibaux, and</location>
<marker>Petrov, Barrett, </marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>The Berkeley Parser.</title>
<date>2008</date>
<note>https://code.google.com/p/berkeleyparser/.</note>
<marker>Klein, 2008</marker>
<rawString>Dan Klein. 2008. The Berkeley Parser. https://code.google.com/p/berkeleyparser/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Michael Collins</author>
</authors>
<date>2008</date>
<note>Evalb. http://nlp.cs.nyu.edu/evalb/.</note>
<contexts>
<context position="12560" citStr="Sekine and Collins, 2008" startWordPosition="2181" endWordPosition="2184">m the corpus, and with the parser supplying its own tags. For the latter case, the tagging accuracy is shown in the last column. Corpus Section Avg. len % &lt;= 40 PPCMBE Dev 24.1 85.5 Test 21.2 89.9 PTB Dev 23.6 92.9 Test 23.5 91.3 Table 3: Average sentence length and percentage of sentences of length &lt;=40 in the PPCMBE and PTB. we will report the parsing results for sentences of length &lt;= 40 and all sentences, as with the PTB. 5 Parsing Experiments The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008) and score using the standard evalb program (Sekine and Collins, 2008). We used the Train and Val sections for training, with the parser using the Val section for fine-tuning parameters (Petrov et al., 2006). Since the Berkeley parser is capable of doing its own POS tagging, we ran it using the gold tags or supplying its own tags. Table 4 shows the results for both modes.8 Consider first the results for the Dev section with the parser using the gold tags. The score for all sentences increases from 83.7 for the Release corpus (row 1) to 84.7 for the Reduced corpus (row 2), reflecting the POS tag simplifications in the Reduced corpus. The score goes up by a furthe</context>
</contexts>
<marker>Sekine, Collins, 2008</marker>
<rawString>Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Taylor</author>
<author>Anthony Warner</author>
<author>Susan Pintzuk</author>
<author>Frank Beths</author>
</authors>
<title>The York-TorontoHelsinki Parsed Corpus of Old English Prose. Distributed through the Oxford Text Archive.</title>
<date>2003</date>
<note>http://www-users.york.ac.uk/ ˜lang22/YCOE/YcoeHome.htm.</note>
<contexts>
<context position="3567" citStr="Taylor et al., 2003" startWordPosition="585" endWordPosition="588">orical corpora. For example “gentlemen” and its orthographic variant “gen’l’men” are tagged with the complex tag ADJ+NS (adjective and plural noun) on the grounds that in earlier time periods, the lexical item is spelled and tagged as two orthographic words (“gentle”/ADJ and “men”/NS). While only 81 of the 248 tags are “simple” (i.e., not associated with lexical merging or splitting), 2Lin et al. (2012) report some results on POS tagging using their own mapping to different tags, but no parsing results. 3Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Icelandic (Wallenberg et al., 2011), French (Martineau and others, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words. 4We are working with a pre-release copy of the next revision of the official version. Some annotation errors in the currently available version have been corrected, but the differences are relatively minor. 662 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 662–667, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Type # Tags # Tokens % coverage S</context>
</contexts>
<marker>Taylor, Warner, Pintzuk, Beths, 2003</marker>
<rawString>Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003. The York-TorontoHelsinki Parsed Corpus of Old English Prose. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/ ˜lang22/YCOE/YcoeHome.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Taylor</author>
<author>Arja Nurmi</author>
<author>Anthony Warner</author>
<author>Susan Pintzuk</author>
<author>Terttu Nevalainen</author>
</authors>
<title>Parsed Corpus of Early English Correspondence. Compiled by the CEEC Project Team.</title>
<date>2006</date>
<institution>University of York and Helsinki: University of Helsinki.</institution>
<location>York:</location>
<note>http://www-users.york.ac.uk/ ˜lang22/PCEEC-manual/index.htm.</note>
<contexts>
<context position="1968" citStr="Taylor et al., 2006" startWordPosition="315" endWordPosition="318"> the training material. The PPCMBE is a million-word treebank created for researching changes in English syntax. It covers the years 1700-1914 and is the most modern in the series of treebanks created for historical research.1 Due to the historical nature of the PPCMBE, it shares some of the characteristics of treebanks based on modern unedited text (Bies et al., 2012), such as spelling variation. 1The other treebanks in the series cover Early Modern English (Kroch et al., 2004) (1.8 million words), Middle English (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words). The size of the PPCMBE is roughly the same as the WSJ section of the PTB, and its annotation style is similar to that of the PTB, but with differences, particularly with regard to coordination and NP structure. However, except for Lin et al. (2012), we have found no discussion of this corpus in the literature.2 There is also much additional material annotated in this style, increasing the importance of analyzing parser performance on this annotation style.3 2 Corpus description The PPCMBE4 consists of 101 files, but we leave aside 7 files that consist of legal material wi</context>
</contexts>
<marker>Taylor, Nurmi, Warner, Pintzuk, Nevalainen, 2006</marker>
<rawString>Ann Taylor, Arja Nurmi, Anthony Warner, Susan Pintzuk, and Terttu Nevalainen. 2006. Parsed Corpus of Early English Correspondence. Compiled by the CEEC Project Team. York: University of York and Helsinki: University of Helsinki. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/ ˜lang22/PCEEC-manual/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Wallenberg</author>
</authors>
<title>Anton Karl Ingason, Einar Freyr Sigursson, and Eirkur Rgnvaldsson.</title>
<date>2011</date>
<note>http://www.linguist.is/ icelandic_treebank.</note>
<marker>Wallenberg, 2011</marker>
<rawString>Joel Wallenberg, Anton Karl Ingason, Einar Freyr Sigursson, and Eirkur Rgnvaldsson. 2011. Icelandic Parsed Historical Corpus (IcePaHC) version 0.4. http://www.linguist.is/ icelandic_treebank.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>