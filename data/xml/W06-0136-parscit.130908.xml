<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001514">
<title confidence="0.989555">
N-gram Based Two-Step Algorithm for Word Segmentation
</title>
<author confidence="0.994734">
Dong-Hee Lim
</author>
<affiliation confidence="0.997733">
Dept. of Computer Science
Kookmin University
</affiliation>
<address confidence="0.892235">
Seoul 136-702, Korea
</address>
<email confidence="0.998227">
nlp@cs.kookmin.ac.kr
</email>
<author confidence="0.967715">
Kyu-Baek, Hwang
</author>
<affiliation confidence="0.991665">
School of Computing
Soongsil University
</affiliation>
<address confidence="0.899309">
Seoul 156-743, Korea
</address>
<email confidence="0.998253">
kbhwang@ssu.ac.kr
</email>
<author confidence="0.98448">
Seung-Shik Kang
</author>
<affiliation confidence="0.997716">
Dept. of Computer Science
Kookmin University
</affiliation>
<address confidence="0.89252">
Seoul 136-702, Korea
</address>
<email confidence="0.998791">
sskang@kookmin.ac.kr
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998449">
This paper describes an n-gram based
reinforcement approach to the closed
track of word segmentation in the third
Chinese word segmentation bakeoff.
Character n-gram features of unigram,
bigram, and trigram are extracted from
the training corpus and its frequencies are
counted. We investigated a step-by-step
methodology by using the n-gram statis-
tics. In the first step, relatively definite
segmentations are fixed by the tight
threshold value. The remaining tags are
decided by considering the left or right
space tags that are already fixed in the
first step. Definite and loose segmenta-
tion are performed simply based on the
bigram and trigram statistics. In order to
overcome the data sparseness problem of
bigram data, unigram is used for the
smoothing.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999928866666667">
Word segmentation has been one of the very
important problems in the Chinese language
processing. It is a necessary in the information
retrieval system for the Korean language (Kang
and Woo, 2001; Lee et al, 2002). Though Korean
words are separated by white spaces, many web
users often do not set a space in a sentence when
they write a query at the search engine. Another
necessity of automatic word segmentation is the
index term extraction from a sentence that in-
cludes word spacing errors.
The motivation of this research is to investi-
gate a practical word segmentation system for the
Korean language. While we develop the system,
we found that ngram-based algorithm was ex-
actly applicable to the Chinese word segmenta-
tion and we have participated the bakeoff (Kang
and Lim, 2005). The bakeoff result is not satis-
fiable, but it is acceptable because our method is
language independent that does not consider the
characteristics of the Chinese language. We do
not use any language dependent features except
the average length of Chinese words.
Another advantage of our approach is that it
can express the ambiguous word boundaries that
are error-prone. So, there are a good possibility
of improving the performance if language de-
pendent functionalities are added such as proper
name, numeric expression recognizer, and the
postprocessing of single character words.1
</bodyText>
<sectionHeader confidence="0.996302" genericHeader="method">
2 N-gram Features
</sectionHeader>
<bodyText confidence="0.999477384615385">
The n-gram features in this work are similar to
the previous one in the second bakeoff. The basic
segmentation in (Kang and Lim, 2005) has per-
formed by bigram features together with space
tags, and the trigram features has been used as a
postprocessing of correcting the segmentation
errors. Trigrams for postprocessing are the ones
that are highly biased to one type of the four tag
features of “AiBjC”.2 In addition, unigram fea-
tures are used for smoothing the bigram, where
bigram is not found in the training corpora. In
this current work, we extended the n-gram fea-
tures to a trigram.
</bodyText>
<listItem confidence="0.999629333333333">
(a) trigram: AiBjC
(b) bigram: iAjBk
(c) unigram: iAj
</listItem>
<bodyText confidence="0.996064333333333">
In the above features, AB and ABC are a
Chinese character sequence of bigram and tri-
gram, respectively. The subscripts i, j, and k
</bodyText>
<footnote confidence="0.9365666">
1 Single character words in Korean are not so common,
compared to the Chinese language. We can control the
occurrence of them through an additional processing.
2 We applied the trigrams for error correction in which one
of the trigram feature occupies 95% or more.
</footnote>
<page confidence="0.978343">
197
</page>
<bodyText confidence="0.989613103448276">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 197–200,
Sydney, July 2006. c�2006 Association for Computational Linguistics
denote word space tags, where the tags are
marked as 1(space tag) and 0(non-space tag). For
the unigram iAj, four types of tag features are
calculated in the training corpora and their fre-
quencies are stored. In the same way, eight types
of bigram features and four types of trigram
features are constructed. If we take all the inside
and outside space-tags of ABC, there are sixteen
types of trigram features hAiBjCk for h,i,j,k = 0 or
1. It will cause a data sparseness problem, espe-
cially for small-sized training corpora. In order to
avoid the data sparseness problem, we ignored
the outside-space tags h and k and constructed
four types of trigram features of AiBjC.
Table 1 shows the number of n-gram features
for each corpora. The total number of unique
trigrams for CITYU corpus is 1,341,612 in which
104,852 trigrams occurred more than three times.
It is less than one tenth of the total number of
trigrams. N-gram feature is a compound feature
of &lt;character, space-tag&gt; combination. Trigram
classes are distinguished by the space-tag context,
trigram class hAiBjCk is named as t4-trigram or
C3T4.3 It is simplified into four classes of C3T2
trigrams of AiBjC, in consideration of the mem-
ory space savings and the data sparseness prob-
lem.
</bodyText>
<tableCaption confidence="0.994088">
Table 1. The number of n-gram features
</tableCaption>
<table confidence="0.901064666666667">
Trigram Bigram Unigram
freq&gt;1 freq&gt;2 freq&gt;3 freq&gt;4 freq&gt;1 freq&gt;1
cityu 1341612 329764 165360 104852 404411 5112
ckip 2951274 832836 444012 296372 717432 6121
msra 986338 252656 132456 86391 303443 4767
upuc 463253 96860 45775 28210 177140 4293
</table>
<sectionHeader confidence="0.945672" genericHeader="method">
3 Word Segmentation Algorithm
</sectionHeader>
<bodyText confidence="0.9884285">
Word segmentation is defined as to choose the
best tag-sequence for a sentence.
</bodyText>
<equation confidence="0.882052666666667">
ˆ
T = arg max P(T  |S)
Ter
</equation>
<bodyText confidence="0.883848">
where
</bodyText>
<equation confidence="0.980533">
T t t
= 1, 2, ... , and = 1 2 ...
t n S c , c , , c
</equation>
<bodyText confidence="0.744177833333333">
3 ‘Cn’ refers to the number of characters and ‘Tn’ refers to
the number of spae-tag. According to this notation, iAjBk
and iAj are expressed as C2T3 and C1T2, respectively.
More specifically at each character position, the
algorithm determines a space-tag ‘0’ or ‘1’ by
using the word spacing features.
</bodyText>
<subsectionHeader confidence="0.997592">
3.1 The Features
</subsectionHeader>
<bodyText confidence="0.9979210625">
We investigated a two step algorithm of de-
termining space tags in each character position of
a sentence using by context dependent n-gram
features. It is based on the assumption that space
tags depend on the left and right context of
characters together with the space tags that it
accompanies. Let tici be a current &lt;space tag,
character&gt; pair in a sentence.4
... ti-2ci-2 ti-1ci-1 tici ti+1ci+1 ti+2ci+2 ...
In our previous work of (Lim and Kang, 2005),
n-gram features (a) and (b) are used. These fea-
tures are used to determine the space tag ti. In this
work, core n-gram feature is a C3T2 classes of
trigram features ci-2ti-1ci-1tici, ci-1ticiti+1ci+1. In
addition, a simple character trigram with no
space tag “ticici+1ci+2” is added.
</bodyText>
<listItem confidence="0.8733024">
(a) unigram:
ti-1ci-1ti, ticiti+1
(b) bigram:
ti-2ci-2ti-1ci-1ti, ti-1ci-1ticiti+1, ticiti+1ci+1ti+2
(c) trigram:
</listItem>
<bodyText confidence="0.9720398">
ci-2ti-1ci-1tici, ci-1ticiti+1ci+1, ticici+1ci+2
Extended n-gram features with space tags are
effective when left or right tags are fixed. Sup-
pose that ti-1 and ti+1 are definitely set to 0 in a
bigram context “ti-1ci-1ticiti+1”, then a feature
“0ci-1tici0”(ti = 0 or 1) is applied, instead of a
simple feature “ci-1tici”. However, none of the
space tags are fixed in the beginning that simple
character n-gram features with no space tag are
used.5
</bodyText>
<subsectionHeader confidence="0.985299">
3.2 Two-step Algorithm
</subsectionHeader>
<bodyText confidence="0.999576857142857">
The basic idea of our method is a cross checking
the n-gram features in the space position by using
three trigram features. For a character sequence
“ci-2ci-1ticici+1ci+2”, we can set a space mark ‘1’ to
ti, if P(ti=1) is greater than P(ti=0) in all the three
trigram features ci-2ci-1tici, ci-1ticici+1, and tici
ci+1ci+2. Because no space tags are determined in
</bodyText>
<footnote confidence="0.5539334">
4 Tag ti is located before the character, not after the character
that is common in other tagging problem like POS-tagging.
5 Simple n-grams with no space tags are calculated from the
extended n-grams.
n
</footnote>
<page confidence="0.995294">
198
</page>
<bodyText confidence="0.999576333333333">
the beginning, word segmentation is performed
in two steps. In the first step, simple n-gram
features are applied with strong threshold values
(tlow1 and thigh1 in Table 2). The space tags with
high confidence are determined and the remain-
ing space tags will be set in the next step.
</bodyText>
<tableCaption confidence="0.997529">
Table 2. Strong and weak threshold values6
</tableCaption>
<table confidence="0.995079">
tlow1 thigh1 tlow2 thigh2 tfinal
cityu 0.36 0.69 0.46 0.51 0.48
ckip 0.37 0.69 0.49 0.51 0.49
msra 0.33 0.68 0.46 0.47 0.46
upuc 0.38 0.69 0.45 0.47 0.47
</table>
<bodyText confidence="0.999851421052631">
In the second step, extended bigram features
are applied if any one of the left or right space
tags is fixed in the first step. Otherwise, simple
bigram probability will be applied, too. In this
step, extended bigram features are applied with
weak threshold values tlow2 and thigh2. The space
tags are determined by the final threshold tfinal, if
it was not determined by weak threshold values.
Considering the fact that average length of Chi-
nese words is about 1.6, the threshold values are
lowered or highered.7
In the final step, error correction is performed
by 4-gram error correction dictionary. It is con-
structed by running the training corpus and
comparing the result to the answer. Error correc-
tion data format is 4-gram. If a 4-gram ci-2ci-1cici+1
is found in a sentence, then tag ti is modified
unconditionally as is specified in the 4-gram
dictionary.
</bodyText>
<sectionHeader confidence="0.998246" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999592333333333">
We evaluated our system in the closed task on all
four corpora. Table 3 shows the final results in
bakeoff 2006. We expect that Roov will be im-
proved if any unknown word processing is per-
formed. Riv can also be improved if lexicon is
applied to correct the segmentation errors.
</bodyText>
<tableCaption confidence="0.982">
Table 3. Final results in bakeoff 2006
</tableCaption>
<table confidence="0.9971648">
R P F Roov Riv
cityu 0.950 0.949 0.949 0.638 0.963
ckip 0.937 0.933 0.935 0.547 0.954
msra 0.933 0.939 0.936 0.526 0.948
upuc 0.915 0.896 0.905 0.565 0.949
</table>
<page confidence="0.845276">
6 Threshold values are optimized for each training corpus.
7 The average length of Korean words is 3.2 characters.
</page>
<subsectionHeader confidence="0.99025">
4.1 Step-by-step Analysis
</subsectionHeader>
<bodyText confidence="0.994541428571428">
In order to analyze the effectiveness of each step,
we counted the number of space positions for
sentence by sentence. If the number of characters
in a sentence is n, then the number of words
positions is (n-1) because we ignored the first tag
t0 for c0. Table 4 shows the number of space
positions in four test corpora.
</bodyText>
<tableCaption confidence="0.99796">
Table 4. The number of space positions
</tableCaption>
<table confidence="0.993873166666666">
# of space # of spaces # of non-
positions spaces
cityu 356,791 212,662 144,129
ckip 135,123 80,387 54,736
msra 168,236 95,995 72,241
upuc 251,418 149,747 101,671
</table>
<bodyText confidence="0.999429">
As we expressed in section 3, we assumed that
trigram with space tag information will deter-
mine most of the space tags. Table 5 shows the
application rate with strong threshold values. As
we expected, around 93.8%^95.9% of total space
tags are set in step-1 with the error rate
1.5%^2.8%.
</bodyText>
<tableCaption confidence="0.788813">
Table 5. N-gram results with strong threshold
</tableCaption>
<table confidence="0.5333916">
# of applied (%) # of errors (%)
cityu 342,035 (95.9%) 5,024 (1.5%)
ckip 128,081 (94.8%) 2,818 (2.2%)
msra 160,437 (95.4%) 3,155 (2.0%)
upuc 235,710 (93.8%) 6,601 (2.8%)
</table>
<bodyText confidence="0.9873498">
Table 6 shows the application rate of n-gram
with weak threshold values in step-2. The space
tags that are not determined in step-1 are set in
the second step. The error rate in step-2 is
24.3%^30.1%.
</bodyText>
<tableCaption confidence="0.680947">
Table 6. N-gram results with weak threshold
</tableCaption>
<table confidence="0.9839292">
# of applied (%) # of errors (%)
cityu 14,756 (4.1%) 3,672 (24.9%)
ckip 7,042 (5.2%) 1,710 (24.3%)
msra 7,799 (4.6%) 2,349 (30.1%)
upuc 15,708 (6.3%) 4,565 (29.1%)
</table>
<page confidence="0.99532">
199
</page>
<subsectionHeader confidence="0.995284">
4.2 4-gram Error Correction
</subsectionHeader>
<bodyText confidence="0.999848363636364">
We examined the effectiveness of 4-gram error
correction. The number of 4-grams that is ex-
tracted from training corpora is about 10,000 to
15,000. We counted the number of space tags
that are modified by 4-gram error correction
dictionary. Table 7 shows the number of modi-
fied space tags and the negative effects of 4-gram
error correction. Table 8 shows the results before
error correction. When compared with the final
results in Table 3, F-measure is slightly lower
than the final results.
</bodyText>
<tableCaption confidence="0.998892">
Table 7. Modified space tags by error correction
</tableCaption>
<table confidence="0.999733666666667">
# of modified Modification
space tags (%) errors (%)
cityu 418 (0.1%) 47 (11.2%)
ckip 320 (0.2%) 94 (29.4%)
msra 778 (0.5%) 153 (19.7%)
upuc 178 (0.1%) 61 (34.3%)
</table>
<tableCaption confidence="0.996234">
Table 8. Results before error correction
</tableCaption>
<table confidence="0.9976306">
R P F
cityu 0.948 0.947 0.948
ckip 0.935 0.931 0.933
msra 0.930 0.930 0.930
upuc 0.915 0.895 0.905
</table>
<sectionHeader confidence="0.998973" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999993736842105">
We described a two-step word segmentation
algorithm as a result of the closed track in bake-
off 2006. The algorithm is based on the cross
validation of the word spacing probability by
using n-gram features of &lt;character, space-tag&gt;.
One of the advantages of our system is that it can
show the self-confidence score for ambiguous or
feature-conflict cases. We have not applied any
language dependent resources or functionalities
such as lexicons, numeric expressions, and
proper name recognition. We expect that our
approach will be helpful for the detection of
error-prone tags and the construction of error
correction dictionaries when we develop a prac-
tical system. Furthermore, the proposed algo-
rithm has been applied to the Korean language
and we achieved a good improvement on proper
names, though overall performance is similar to
the previous method.
</bodyText>
<sectionHeader confidence="0.995577" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99979525">
This work was supported by the Korea Science
and Engineering Foundation(KOSEF) through
Advaned Information Technology Research
Center(AITrc).
</bodyText>
<sectionHeader confidence="0.998467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999971790697674">
Asahara, M., C. L. Go, X. Wang, and Y. Matsumoto,
Combining Segmenter and Chunker for Chinese
Word Segmentation, Proceedings of the 2nd
SIGHAN Workshop on Chinese Language Proc-
essing, pp.144-147, 2003.
Chen, A., Chinese Word Segmentation Using Mini-
mal Linguistic Knowledge, SIGHAN 2003,
pp.148-151, 2003.
Gao, J., M. Li, and C.N. Huang, Improved
Source-Channel Models for Chinese Word Seg-
mentation, ACL 2003, pp.272-279, 2003.
Kang, S. S. and C. W. Woo, Automatic Segmentation
of Words using Syllable Bigram Statistics, Pro-
ceedings of NLPRS&apos;2001, pp.729-732, 2001.
Kang, S. S. and D. H. Lim, Data-driven Language
Independent Word Segmentation Using Charac-
ter-Level Information, Proceedings of the 4th
SIGHAN Workshop on Chinese Language Proc-
essing, pp.158-160, 2005.
Lee D. G, S. Z. Lee, and H. C. Rim, H. S. Lim,
Automatic Word Spacing Using Hidden Markov
Model for Refining Korean Text Corpora, Proc. of
the 3rd Workshop on Asian Language Resources
and International Standardization, pp.51-57, 2002.
Maosong, S., S. Dayang, and B. K. Tsou, Chinese
Word Segmentation without Using Lexicon and
Hand-crafted Training Data, Proceedings of the
17th International Conference on Computational
Linguistics (Coling’98), pp.1265-1271, 1998.
Nakagawa, T., Chinese and Japanese Word Segmen-
tation Using Word-Level and Character-Level In-
formation, COLING’04., pp.466-472, 2004.
Ng, H.T. and J.K. Low, Chinese Part-of-Speech Tag-
ging: One-at-a-Time or All-at-Once? Word-Based
or Character-Based, EMNLP’04, pp.277-284,
2004.
Shim, K. S., Automated Word-Segmentation for
Korean using Mutual Information of Syllables,
Journal of KISS: Software and Applications,
pp.991-1000, 1996.
Sproat, R. and T. Emerson, The First International
Chinese Word Segmentation Bakeoff, SIGHAN
2003.
</reference>
<page confidence="0.99662">
200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.236558">
<title confidence="0.998529">N-gram Based Two-Step Algorithm for Word Segmentation</title>
<author confidence="0.676517">Dong-Hee</author>
<affiliation confidence="0.865855">Dept. of Computer Kookmin</affiliation>
<address confidence="0.901229">Seoul 136-702, Korea</address>
<email confidence="0.958751">nlp@cs.kookmin.ac.kr</email>
<author confidence="0.975766">Hwang Kyu-Baek</author>
<affiliation confidence="0.9998785">School of Computing Soongsil University</affiliation>
<address confidence="0.994841">Seoul 156-743, Korea</address>
<email confidence="0.985696">kbhwang@ssu.ac.kr</email>
<author confidence="0.622476">Seung-Shik</author>
<affiliation confidence="0.8636115">Dept. of Computer Kookmin</affiliation>
<address confidence="0.893703">Seoul 136-702, Korea</address>
<email confidence="0.978518">sskang@kookmin.ac.kr</email>
<abstract confidence="0.998522952380952">This paper describes an n-gram based reinforcement approach to the closed track of word segmentation in the third Chinese word segmentation bakeoff. Character n-gram features of unigram, bigram, and trigram are extracted from the training corpus and its frequencies are counted. We investigated a step-by-step methodology by using the n-gram statistics. In the first step, relatively definite segmentations are fixed by the tight threshold value. The remaining tags are decided by considering the left or right space tags that are already fixed in the first step. Definite and loose segmentation are performed simply based on the bigram and trigram statistics. In order to overcome the data sparseness problem of bigram data, unigram is used for the smoothing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Asahara</author>
<author>C L Go</author>
<author>X Wang</author>
<author>Y Matsumoto</author>
</authors>
<title>Combining Segmenter and Chunker for Chinese Word Segmentation,</title>
<date>2003</date>
<booktitle>Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>144--147</pages>
<marker>Asahara, Go, Wang, Matsumoto, 2003</marker>
<rawString>Asahara, M., C. L. Go, X. Wang, and Y. Matsumoto, Combining Segmenter and Chunker for Chinese Word Segmentation, Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing, pp.144-147, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chen</author>
</authors>
<title>Chinese Word Segmentation Using Minimal Linguistic Knowledge,</title>
<date>2003</date>
<pages>148--151</pages>
<location>SIGHAN</location>
<marker>Chen, 2003</marker>
<rawString>Chen, A., Chinese Word Segmentation Using Minimal Linguistic Knowledge, SIGHAN 2003, pp.148-151, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>M Li</author>
<author>C N Huang</author>
</authors>
<title>Improved Source-Channel Models for Chinese Word Segmentation,</title>
<date>2003</date>
<pages>272--279</pages>
<location>ACL</location>
<marker>Gao, Li, Huang, 2003</marker>
<rawString>Gao, J., M. Li, and C.N. Huang, Improved Source-Channel Models for Chinese Word Segmentation, ACL 2003, pp.272-279, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Kang</author>
<author>C W Woo</author>
</authors>
<title>Automatic Segmentation of Words using Syllable Bigram Statistics,</title>
<date>2001</date>
<booktitle>Proceedings of NLPRS&apos;2001,</booktitle>
<pages>729--732</pages>
<contexts>
<context position="1332" citStr="Kang and Woo, 2001" startWordPosition="192" endWordPosition="195"> In the first step, relatively definite segmentations are fixed by the tight threshold value. The remaining tags are decided by considering the left or right space tags that are already fixed in the first step. Definite and loose segmentation are performed simply based on the bigram and trigram statistics. In order to overcome the data sparseness problem of bigram data, unigram is used for the smoothing. 1 Introduction Word segmentation has been one of the very important problems in the Chinese language processing. It is a necessary in the information retrieval system for the Korean language (Kang and Woo, 2001; Lee et al, 2002). Though Korean words are separated by white spaces, many web users often do not set a space in a sentence when they write a query at the search engine. Another necessity of automatic word segmentation is the index term extraction from a sentence that includes word spacing errors. The motivation of this research is to investigate a practical word segmentation system for the Korean language. While we develop the system, we found that ngram-based algorithm was exactly applicable to the Chinese word segmentation and we have participated the bakeoff (Kang and Lim, 2005). The bake</context>
</contexts>
<marker>Kang, Woo, 2001</marker>
<rawString>Kang, S. S. and C. W. Woo, Automatic Segmentation of Words using Syllable Bigram Statistics, Proceedings of NLPRS&apos;2001, pp.729-732, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Kang</author>
<author>D H Lim</author>
</authors>
<title>Data-driven Language Independent Word Segmentation Using Character-Level Information,</title>
<date>2005</date>
<booktitle>Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>158--160</pages>
<contexts>
<context position="1922" citStr="Kang and Lim, 2005" startWordPosition="293" endWordPosition="296"> language (Kang and Woo, 2001; Lee et al, 2002). Though Korean words are separated by white spaces, many web users often do not set a space in a sentence when they write a query at the search engine. Another necessity of automatic word segmentation is the index term extraction from a sentence that includes word spacing errors. The motivation of this research is to investigate a practical word segmentation system for the Korean language. While we develop the system, we found that ngram-based algorithm was exactly applicable to the Chinese word segmentation and we have participated the bakeoff (Kang and Lim, 2005). The bakeoff result is not satisfiable, but it is acceptable because our method is language independent that does not consider the characteristics of the Chinese language. We do not use any language dependent features except the average length of Chinese words. Another advantage of our approach is that it can express the ambiguous word boundaries that are error-prone. So, there are a good possibility of improving the performance if language dependent functionalities are added such as proper name, numeric expression recognizer, and the postprocessing of single character words.1 2 N-gram Featur</context>
</contexts>
<marker>Kang, Lim, 2005</marker>
<rawString>Kang, S. S. and D. H. Lim, Data-driven Language Independent Word Segmentation Using Character-Level Information, Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing, pp.158-160, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Lee</author>
<author>S Z Lee</author>
<author>H C Rim</author>
<author>H S Lim</author>
</authors>
<title>Automatic Word Spacing Using Hidden Markov Model for Refining Korean Text Corpora,</title>
<date>2002</date>
<booktitle>Proc. of the 3rd Workshop on Asian Language Resources and International Standardization,</booktitle>
<pages>51--57</pages>
<contexts>
<context position="1350" citStr="Lee et al, 2002" startWordPosition="196" endWordPosition="199">relatively definite segmentations are fixed by the tight threshold value. The remaining tags are decided by considering the left or right space tags that are already fixed in the first step. Definite and loose segmentation are performed simply based on the bigram and trigram statistics. In order to overcome the data sparseness problem of bigram data, unigram is used for the smoothing. 1 Introduction Word segmentation has been one of the very important problems in the Chinese language processing. It is a necessary in the information retrieval system for the Korean language (Kang and Woo, 2001; Lee et al, 2002). Though Korean words are separated by white spaces, many web users often do not set a space in a sentence when they write a query at the search engine. Another necessity of automatic word segmentation is the index term extraction from a sentence that includes word spacing errors. The motivation of this research is to investigate a practical word segmentation system for the Korean language. While we develop the system, we found that ngram-based algorithm was exactly applicable to the Chinese word segmentation and we have participated the bakeoff (Kang and Lim, 2005). The bakeoff result is not </context>
</contexts>
<marker>Lee, Lee, Rim, Lim, 2002</marker>
<rawString>Lee D. G, S. Z. Lee, and H. C. Rim, H. S. Lim, Automatic Word Spacing Using Hidden Markov Model for Refining Korean Text Corpora, Proc. of the 3rd Workshop on Asian Language Resources and International Standardization, pp.51-57, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Maosong</author>
<author>S Dayang</author>
<author>B K Tsou</author>
</authors>
<title>Chinese Word Segmentation without Using Lexicon and Hand-crafted Training Data,</title>
<date>1998</date>
<booktitle>Proceedings of the 17th International Conference on Computational Linguistics (Coling’98),</booktitle>
<pages>1265--1271</pages>
<marker>Maosong, Dayang, Tsou, 1998</marker>
<rawString>Maosong, S., S. Dayang, and B. K. Tsou, Chinese Word Segmentation without Using Lexicon and Hand-crafted Training Data, Proceedings of the 17th International Conference on Computational Linguistics (Coling’98), pp.1265-1271, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nakagawa</author>
</authors>
<title>Chinese and Japanese Word Segmentation Using Word-Level and Character-Level Information,</title>
<date>2004</date>
<volume>04</volume>
<pages>466--472</pages>
<marker>Nakagawa, 2004</marker>
<rawString>Nakagawa, T., Chinese and Japanese Word Segmentation Using Word-Level and Character-Level Information, COLING’04., pp.466-472, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>J K Low</author>
</authors>
<title>Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once? Word-Based or Character-Based,</title>
<date>2004</date>
<volume>04</volume>
<pages>277--284</pages>
<marker>Ng, Low, 2004</marker>
<rawString>Ng, H.T. and J.K. Low, Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once? Word-Based or Character-Based, EMNLP’04, pp.277-284, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Shim</author>
</authors>
<title>Automated Word-Segmentation for Korean using Mutual Information of Syllables,</title>
<date>1996</date>
<journal>Journal of KISS: Software and Applications,</journal>
<pages>991--1000</pages>
<marker>Shim, 1996</marker>
<rawString>Shim, K. S., Automated Word-Segmentation for Korean using Mutual Information of Syllables, Journal of KISS: Software and Applications, pp.991-1000, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>T Emerson</author>
</authors>
<date>2003</date>
<booktitle>The First International Chinese Word Segmentation Bakeoff, SIGHAN</booktitle>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Sproat, R. and T. Emerson, The First International Chinese Word Segmentation Bakeoff, SIGHAN 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>