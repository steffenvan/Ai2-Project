<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.912339">
Mining the Web for Bilingual Text
</title>
<author confidence="0.978271">
Philip Resnik*
</author>
<affiliation confidence="0.9980785">
Dept. of Linguistics/Institute for Advanced Computer Studies
University of Maryland, College Park, MD 20742
</affiliation>
<email confidence="0.992338">
resnik@umiacs.umd.edu
</email>
<sectionHeader confidence="0.9973" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998979454545454">
STRAND (Resnik, 1998) is a language-
independent system for automatic discovery
of text in parallel translation on the World
Wide Web. This paper extends the prelim-
inary STRAND results by adding automatic
language identification, scaling up by orders
of magnitude, and formally evaluating perfor-
mance. The most recent end-product is an au-
tomatically acquired parallel corpus comprising
2491 English-French document pairs, approxi-
mately 1.5 million words per language.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964611111111">
Text in parallel translation is a valuable re-
source in natural language processing. Sta-
tistical methods in machine translation (e.g.
(Brown et al., 1990)) typically rely on large
quantities of bilingual text aligned at the doc-
ument or sentence level, and a number of
approaches in the burgeoning field of cross-
language information retrieval exploit parallel
corpora either in place of or in addition to map-
pings between languages based on information
from bilingual dictionaries (Davis and Dunning,
1995; Landauer and Littman, 1990; Hull and
Oard, 1997; Oard, 1997). Despite the utility of
such data, however, sources of bilingual text are
subject to such limitations as licensing restric-
tions, usage fees, restricted domains or genres,
and dated text (such as 1980&apos;s Canadian poli-
tics); or such sources simply may not exist for
</bodyText>
<footnote confidence="0.56940825">
* This work was supported by Department of De-
fense contract MDA90496C1250, DARPA/ITO Con-
tract N66001-97-C-8540, and a research grant from Sun
Microsystems Laboratories. The author gratefully ac-
knowledges the comments of the anonymous reviewers,
helpful discussions with Dan Melamed and Doug Oard,
and the assistance of Jeff Allen in the French-English
experimental evaluation.
</footnote>
<bodyText confidence="0.999036">
language pairs of interest.
Although the majority of Web content is in
English, it also shows great promise as a source
of multilingual content. Using figures from
the Babel survey of multilinguality on the Web
(http : f/www. isoc . org/), it is possible to esti-
mate that as of June, 1997, there were on the or-
der of 63000 primarily non-English Web servers,
ranging over 14 languages. Moreover, a follow-
up investigation of the non-English servers sug-
gests that nearly a third contain some useful
cross-language data, such as parallel English on
the page or links to parallel English pages —
the follow-up also found pages in five languages
not identified by the Babel study (Catalan, Chi-
nese, Hungarian, Icelandic, and Arabic; Michael
Littman, personal communication). Given the
continued explosive increase in the size of the
Web, the trend toward business organizations
that cross national boundaries, and high levels
of competition for consumers in a global mar-
ketplace, it seems impossible not to view mul-
tilingual content on the Web as an expanding
resource. Moreover, it is a dynamic resource,
changing in content as the world changes. For
example, Diekema et al., in a presentation at the
1998 TREC-7 conference (Voorhees and Har-
man, 1998), observed that the performance of
their cross-language information retrieval was
hurt by lexical gaps such as Bosnial Bosnie —
this illustrates a highly topical missing pair in
their static lexical resource (which was based on
WordNet 1.5). And Gey et al., also at TREC-7,
observed that in doing cross-language retrieval
using commercial machine translation systems,
gaps in the lexicon (their example was acupunc-
ture&apos; Akupunktur) could make the difference be-
tween precision of 0.08 and precision of 0.83 on
individual queries.
Resnik (1998) presented an algorithm called
</bodyText>
<page confidence="0.948839">
527
</page>
<table confidence="0.508099333333333">
Candidate Pair Candidate Pair Candidate Pair
Generation Evaluation Filtering
(structural) (language dependent)
</table>
<figureCaption confidence="0.999694">
Figure 1: The STRAND architecture
</figureCaption>
<bodyText confidence="0.998354321428571">
STRAND (Structural Translation Recognition for
Acquiring Natural Data) designed to explore
the Web as a source of parallel text, demon-
strating its potential with a small-scale evalu-
ation based on the author&apos;s judgments. After
briefly reviewing the STRAND architecture and
preliminary results (Section 2), this paper goes
beyond that preliminary work in two significant
ways. First, the framework is extended to in-
clude a filtering stage that uses automatic lan-
guage identification to eliminate an important
class of false positives: documents that appear
structurally to be parallel translations but are in
fact not in the languages of interest. The system
is then run on a somewhat larger scale and eval-
uated formally for English and Spanish using
measures of agreement with independent human
judges, precision, and recall (Section 3). Sec-
ond, the algorithm is scaled up more seriously to
generate large numbers of parallel documents,
this time for English and French, and again sub-
jected to formal evaluation (Section 4). The
concrete end result reported here is an automat-
ically acquired English-French parallel corpus
of Web documents comprising 2491 document
pairs, approximately 1.5 million words per lan-
guage (without markup), containing little or no
noise.
</bodyText>
<sectionHeader confidence="0.991333" genericHeader="method">
2 STRAND Preliminaries
</sectionHeader>
<bodyText confidence="0.996084213114754">
This section is a brief summary of the STRAND
system and previously reported preliminary re-
sults (Resnik, 1998).
The STRAND architecture is organized as a
pipeline, beginning with a candidate generation
stage that (over-)generates candidate pairs of
documents that might be parallel translations.
(See Figure 1.) The first implementation of the
generation stage used a query to the Altavista
search engine to generate pages that could be
viewed as &amp;quot;parents&amp;quot; of pages in parallel transla-
tion, by asking for pages containing one portion
of anchor text (the readable material in a hy-
perlink) containing the string &amp;quot;English&amp;quot; within
a fixed distance of another anchor text contain-
ing the string &amp;quot;Spanish&amp;quot;. (The matching pro-
cess was case-insensitive.) This generated many
good pairs of pages, such as those pointed to by
hyperlinks reading Click here for English ver-
sion and Click here for Spanish version, as well
as many bad pairs, such as university pages con-
taining links to English Literature in close prox-
imity to Spanish Literature.
The candidate generation stage is followed
by a candidate evaluation stage that represents
the core of the approach, filtering out bad can-
didates from the set of generated page pairs.
It employs a structural recognition algorithm
exploiting the fact that Web pages in parallel
translation are invariably very similar in the
way they are structured — hence the &apos;s&apos; in
STRAND. For example, see Figure 2.
The structural recognition algorithm first
runs both documents through a transducer
that reduces each to a linear sequence of
tokens corresponding to HTML markup
elements, interspersed with tokens repre-
senting undifferentiated &amp;quot;chunks&amp;quot; of text.
For example, the transducer would replace
the HTML source text &lt;TITLE&gt;ACL &apos;99
Conference Home Page&lt;/TITLE&gt; with the
three tokens [BEGIN: TITLE], [Chunk:24], and
[END: TITLE]. The number inside the chunk
token is the length of the text chunk, not
counting whitespace; from this point on only
the length of the text chunks is used, and
therefore the structural filtering algorithm is
completely language independent.
Given the transducer&apos;s output for each doc-
ument, the structural filtering stage aligns the
two streams of tokens by applying a standard,
widely available dynamic programming algo-
rithm for finding an optimal alignment between
two linear sequences.1 This alignment matches
identical markup tokens to each other as much
as possible, identifies runs of unmatched tokens
that appear to exist only in one sequence but
not the other, and marks pairs of non-identical
tokens that were forced to be matched to each
other in order to obtain the best alignment pos-
&apos;Known to many programmers as diff.
</bodyText>
<page confidence="0.992346">
528
</page>
<subsectionHeader confidence="0.65938">
141
Highlights of Best Practices
</subsectionHeader>
<bodyText confidence="0.699679">
1111111111111110111111
</bodyText>
<subsectionHeader confidence="0.362009">
Seminar on Self-Regulation
</subsectionHeader>
<note confidence="0.765983">
On Friday. 25 October 1996. 40 numb. MA. regula.y community almnded Al. Regulatory B.
</note>
<tableCaption confidence="0.714326875">
Practices Send. designed to help depanmenu kern about metbregulation as an alternating to
rellohn, P.M.. As modem. fa the mai.. Zane Brown Director General. Consume. Produd
Directorate. Waled the serail. by dessibing sons of tls eapatien.s Industry Canada has fued with
math.6 introdmi, fftethetive ...Tim &amp;live, (MD) mechanism. such as voluntary coda*.
industry selbansugemem He oded that a forthcoming papa ..ASD. would explore Rich topics as
winds ASH. provide the nam appropriate mechanism and what are die chededges asmcised with
than.
Volattary Coda
&amp;quot;A voluntary code is a sd =nth:dined commit.. . explicitly part of a legislative or
regulatory regime daigned 6influence, slap, control cc RIGOUR Me behaviour of those who agreed
to meet them Th.y do non&amp;quot;. continued Brian Glabb. Senior Analyst. Resales, Affairs Dimmest
Treasury Board Secretatis, government. &apos; right to regukte. They amply offer the p.icipsits
an alternative to being regulated by the gameness:
As regulatioe dams under increasing public scnniny. goverment. worldwide are Wang to voluntary
approaches to coropkInem and end substitute far. regulation. While voluntary codes have • number of
advantages, including:
</tableCaption>
<listItem confidence="0.99094825">
• the potential to be developed more quickly than laws
• the Iowa conversion met to peep. and nut its planet
• the ability to avoulaunadmional problems that plague regulatory initiatives: and
• the comparative ease with which they cm be modified to nua changing circa....
</listItem>
<figure confidence="0.929452666666667">
A. key question which muounds than is &amp;quot;Can IN Fannie&apos; regulate theroxelves 6096 W..
goventmentr
101
• Faits saillants des pradques exemplaires
11111M11111111111111A
Seminaire sur Pautoreglementadon
</figure>
<bodyText confidence="0.916326894736842">
Le vmdredi 25 cacao 1996. 40 membres nit de la Seeman:anon oat assisth au stninaiste sr les
psalm. exemplars en maniac de reglen.tation qui visait I aida la rnininbto 6.. familiarises avec
P.ereglemsitation can. NINA. de rechange asa progr.unes de rtglanentasion. L&apos;animanur. M.
Zane BMW., aired:MP gannet Direction 40. 61,.. de wasoonnation. owes la Mance en mkt.
It, aphasia dtIndusthe Caned. pe rune de &apos;Introduction de nif animus de diversification 496 .6*..
A. an.ation da services comma les coda Yokota= l&apos; snort, lons.tion de l&apos;indastrie. a add
que pseud. mochainement un document stu I. divenifirstse des modes de postai. da service qui
.intrait de divers suj.. comas les anfmnisnes de premvion de rethange des semi. les phi.
aggroprifs que A. problemes ...leafs par Nunn
Cada volontalms
Us code volontaire un ensemble d&apos;engageme. scernaliffs • ne faisant pas eaplidtanent pink d&apos;un
Mgirrio lOgialatif ou rEglementaire - cone0 pom lateen.. faxonnex conneller ou israluer is
canpartement de ceux qui la ont pan II. Istiliminent pax a pnnsuivi 6.9.. GINS analyse
thincipal. Affair.. Ag). .6*.. au Racentariat du Conseil du Treece. Adroit du gaivemement de
Makatea.. n, alma simplanent aux mMiciparas une solution de red.., I). reglemenmtioe p..).
gomernernmat
Au moment a) Is thglementation f objes true examen adru du public. ks gouvornemerns I lishelle
town. van des approchen volonmirsat en ..5,190,14 A. la rigkoausion et mans
comae submits i celle-d. les codes volontaires prima40 tin cumin menthe &amp;manages. notanstent
</bodyText>
<listItem confidence="0.872962">
• le passibilith dit. flab.. plus rapidem. pie dee lam
• dd. cads 40p.A. iffRienat A. PriPaation et de ml.,., Sumas
• ils mecums devil..).. trobthrora d. costrheace qui f sweat A. initiatives de reglens.tiont
• la fuiliti relative avec laquelle ils peones etre motbrans at tomtits des nouvelke droonstrams.
</listItem>
<figureCaption confidence="0.989342">
Figure 2: Structural similarity in parallel translations on the Web
</figureCaption>
<bodyText confidence="0.998198690476191">
sible.2 At this point, if there were too many
unmatched tokens, the candidate pair is taken
to be prima facie unacceptable and immediately
filtered out.
Otherwise, the algorithm extracts from the
alignment those pairs of chunk tokens that were
matched to each other in order to obtain the
best alignments.3 It then computes the corre-
lation between the lengths of these non-markup
text chunks. As is well known, there is a re-
liably linear relationship in the lengths of text
translations — small pieces of source text trans-
late to small pieces of target text, medium to
medium, and large to large. Therefore we can
apply a standard statistical hypothesis test, and
if p &lt; .05 we can conclude that the lengths are
reliably correlated and accept the page pair as
likely to be translations of each other. Other-
wise, this candidate page pair is filtered out.4
2An anonymous reviewer observes that diff has no
preference for aligning chunks of similar lengths, which
in some cases might lead to a poor alignment when a
good one exists. This could result in a failure to identify
true translations and is worth investigating further.
3Chunk tokens with exactly equal lengths are ex-
cluded; see (Resnik, 1998) for reasons and other details
of the algorithm.
&apos;The level of significance (p &lt; .05) was the ini-
tial selection during algorithm development, and never
changed. This, the unmatched-tokens threshold for
prima facie rejection due to mismatches (20%), and the
maximum distance between hyperlinks in the genera-
In the preliminary evaluation, I generated a
test set containing 90 English-Spanish candi-
date pairs, using the candidate generation stage
as just described. I evaluated these candi-
dates by hand, identifying 24 as true translation
pairs.5 Of these 24, STRAND identified 15 as true
translation pairs, for a recall of 62.5%. Perhaps
more important, it only generated 2 additional
translation pairs incorrectly, for a precision of
15/17 = 88.2%.
</bodyText>
<sectionHeader confidence="0.872587" genericHeader="method">
3 Adding Language Identification
</sectionHeader>
<bodyText confidence="0.8787664">
In the original STRAND architecture, addi-
tional filtering stages were envisaged as pos-
sible (see Figure 1), including such language-
dependent processes as automatic language
identification and content-based comparison of
structually aligned document segments using
cognate matching or existing bilingual dictio-
naries. Such stages were initially avoided in
order to keep the system simple, lightweight,
and independent of linguistic resources. How-
tion stage (10 lines), are parameters of the algorithm
that were determined during development using a small
amount of arbitrarily selected French-English data down-
loaded from the Web. These values work well in prac-
tice and have not been varied systematically; their values
were fixed in advance of the preliminary evaluation and
have not been changed since.
5 The complete test set and my judgments
for this preliminary evaluation can be found at
http://umiacs.umd.edu/n•resnikiamta98/.
</bodyText>
<page confidence="0.993818">
529
</page>
<figureCaption confidence="0.987313">
Figure 3: Structurally similar pages that are not translations
</figureCaption>
<figure confidence="0.392826111111111">
Costeet-tvou textditml
Just For Kids- BookBag Order Processin
View Bookbag I Search Catalog I Order lc&amp;
About U.
Pleam he aWay that limy of our boob mu* ferrt arrive Irma the warebous, so oeveral additiceal dap
Madd be farmed km your delivery times. Poe fad. &amp;berry eater UPS OVERNIGHT iii1O the
Special lystmetiose boa. We trill coma ,018 9.009. ebigping mot (bayed cm weight).
Vkwins Purchases
(310••■•./.8,..11,00,1/*MfOr
C..topm...1•••••••••••■■■,.....araygol .11•11•■••••12•71.011111..1.1..........*
ol.gis.1.1=•••.
&lt;1.1■14■•••14.•••■,■••••••■••••■■••■••1,111.1*
661 7SM 716 he 167.4
t 06071051 17.1*.90o.IOiIIo. to Seem $ 5.0 549
Tem Ohm. $ 5.0
.........1101.1a11.1•■•41...,16•••/...1.11.1....•••••••■■■•••••••.••■•.r
Check Out
a$00 Qoareitim Empty Bookliag
Clidt }hoe to rears to die creaks 011.
A u glare.
ICU Hap a. Mean Prosper IL 00056
Phew or PA Oeilen 1117.14178.1
0. the Perk Pet c Imiejlistporlirlasom
aeloacielleuzeb.211ceo•
O. aka hew fee est Coeval, Nue
§.
Just For Kids- BookBag Order 1Processin
View Swam I_ Surd. Catalog I Order Itio
Ast
About U.
Specials
Please be aviere that many of our book. caul limt arrive from the werehe0e, so revival acidities.&apos; drys
should be factored iato yem delivery Mum Fee fasten delivery ewe UPS OVERNIGHT into**
Special ilISMICIIMI• box. We MU med. you with the shipping cod (timed co weight).
Viewing Purchase.
CIA* cum pet pa. Am 110nr *W. I Ode. ciao el ore p • twei sees
am. awry two...a* am wade...ow m weewedwrwecce
Milli. le elior ate We vec above ow
Remit OM 110 Ado/ Mee leaTer
06070551 swedebkog0sh-Coe0110ruma lac Ellem 1149 56
09091065 6661776116060 Marne Nam A166 1470 1020
Tag dim. • 176
me*. b... elar.
6761. 61766 ....66.6.6•66.66666.6 6.6
Check Out
Quer Ottattibes Cowry Bookliag
Click Here to navy 10 300 mak; pme.
Au Per 13/1
ICU le Hop Lc Me. Pram IL PPM
Phew cc PA Orden re U7 4111.18,
0. the Parie Mee leek onnalealocleitacee
Pcuil treariarerlijoeiporbtaroa
0 Item ler etc Commas eye
Aft,
</figure>
<bodyText confidence="0.999712053571429">
ever, in conducting an error analysis for the pre-
liminary evaluation, and further exploring the
characteristics of parallel Web pages, it became
evident that such processing would be impor-
tant in addressing one large class of potential
false positives. Figure 3 illustrates: it shows
two documents that are generated by looking
for &amp;quot;parent&amp;quot; pages containing hyperlinks to En-
glish and Spanish, which pass the structural fil-
ter with flying colors. The problem is poten-
tially acute if the generation stage happens to
yield up many pairs of pages that come from on-
line catalogues or other Web sites having large
numbers of pages with a conventional structure.
There is, of course, an obvious solution that
will handle most such cases: making sure that
the two pages are actually written in the lan-
guages they are supposed to be written in. In
order to filter out candidate page pairs that
fail this test, statistical language identification
based on character n-grams was added to the
system (Dunning, 1994). Although this does
introduce a need for language-specific training
data for the two languages under consideration,
it is a very mild form of language dependence:
Dunning and others have shown that when
classifying strings on the order of hundreds or
thousands of characters, which is typical of the
non-markup text in Web pages, it is possible
to discriminate languages with accuracy in the
high 90% range for many or most language pairs
given as little as 50k characters per language as
training material.
For the language filtering stage of STRAND,
the following criterion was adopted: given two
documents d1 and d2 that are supposed to be
in languages L1 and L2, keep the document
pair iff Pr(Lildi) &gt; Pr(L214) and Pr(L21d2) &gt;
Pr(Li I d2). For English and Spanish, this trans-
lates as a simple requirement that the &amp;quot;English&amp;quot;
page look more like English than Spanish, and
that the &amp;quot;Spanish&amp;quot; page look more like Spanish
than English. Language identification is per-
formed on the plain-text versions of the pages.
Character 5-gram models for languages under
consideration are constructed using 100k char-
acters of training data from the European Cor-
pus Initiative (Ed), available from the Linguis-
tic Data Consortium (LDC).
In a formal evaluation, STRAND with the new
language identification stage was run for English
and Spanish, starting from the top 1000 hits
yielded up by Altavista in the candidate gen-
eration stage, leading to a set of 913 candidate
pairs. A test set of 179 items was generated for
annotation by human judges, containing:
</bodyText>
<listItem confidence="0.9998022">
• All the pairs marked GOOD (i.e. transla-
tions) by STRAND (61); these are the pairs
that passed both the structural and lan-
guage identification filter.
• All the pairs filtered out via language iden-
</listItem>
<page confidence="0.918568">
530
</page>
<equation confidence="0.471096">
tification (73)
</equation>
<listItem confidence="0.993617">
• A random sample of the pairs filtered out
structurally (45)
</listItem>
<bodyText confidence="0.999945972222222">
It was impractical to manually evaluate all pairs
filtered out structurally, owing to the time re-
quired for judgments and the desire for two in-
dependent judgments per pair in order to assess
inter-judge reliability.
The two judges were both native speakers of
Spanish with high proficiency in English, nei-
ther previously familiar with the project. They
worked independently, using a Web browser to
access test pairs in a fashion that allowed them
to view pairs side by side. The judges were
told they were helping to evaluate a system that
identifies pages on the Web that are translations
of each other, and were instructed to make de-
cisions according to the following criterion:
Is this pair of pages intended to show
the same material to two different
users, one a reader of English and the
other a reader of Spanish?
The phrasing of the criterion required some con-
sideration, since in previous experience with hu-
man judges and translations I have found that
judges are frequently unhappy with the qual-
ity of the translations they are looking at. For
present purposes it was required neither that
the document pair represent a perfect transla-
tion (whatever that might be), nor even nec-
essarily a good one: STRAND was being tested
not on its ability to determine translation qual-
ity, which might or might not be a criterion for
inclusion in a parallel corpus, but rather its abil-
ity to facilitate the task of locating page pairs
that one might reasonably include in a corpus
undifferentiated by quality (or potentially post-
filtered manually).
The judges were permitted three responses:
</bodyText>
<listItem confidence="0.999970333333333">
• Yes: translations of each other
• No: not translations of each other
• Unable to tell
</listItem>
<bodyText confidence="0.91353575">
When computing evaluation measures, page
pairs classified in the third category by a hu-
man judge, for whatever reason, were excluded
from consideration.
</bodyText>
<table confidence="0.985095">
Comparison N Pr(Agree) K
J1, J2: 106 0.85 0.70
J1, STRAND: 165 0.91 0.79
J2, STRAND: 113 0.81 0.61
J1 n J2, STRAND: 90 0.91 0.82
</table>
<tableCaption confidence="0.999678">
Table 1: English-Spanish evaluation
</tableCaption>
<bodyText confidence="0.988731022727273">
Table 1 shows agreement measures between
the two judges, between STRAND and each
individual judge, and the agreement between
STRAND and the intersection of the two judges&apos;
annotations — that is, STRAND evaluated
against only those cases where the two judges
agreed, which are therefore the items we can re-
gard with the highest confidence. The table also
shows Cohen&apos;s lc, an agreement measure that
corrects for chance agreement (Carletta, 1996);
the most important value in the table is the
value of 0.7 for the two human judges, which
can be interpreted as sufficiently high to indi-
cate that the task is reasonably well defined.
(As a rule of thumb, classification tasks with
ic &lt; 0.6 are generally thought of as suspect in
this regard.) The value of N is the number of
pairs that were included, after excluding those
for which the human judgement in the compar-
ison was undecided.
Since the cases where the two judges agreed
can be considered the most reliable, these were
used as the basis for the computation of recall
and precision. For this reason, and because
the human-judged set included only a sample
of the full set evaluated by STRAND, it was nec-
essary to extrapolate from the judged (by both
judges) set to the full set in order to compute
recall/precision figures; hence these figures are
reported as estimates. Precision is estimated
as the proportion of pages judged GOOD by
STRAND that were also judged to be good (i.e.
&amp;quot;yes&amp;quot;) by both judges — this figure is 92.1%
Recall is estimated as the number of pairs that
should have been judged GOOD by STRAND
(i.e. that recieved a &amp;quot;yes&amp;quot; from both judges)
that STRAND indeed marked GOOD — this fig-
ure is 47.3%.
These results can be read as saying that of ev-
ery 10 document pairs included by STRAND in
a parallel corpus acquired fully automatically
from the Web, fewer than 1 pair on average was
included in error. Equivalently, one could say
that the resulting corpus contains only about
</bodyText>
<page confidence="0.994585">
531
</page>
<bodyText confidence="0.999836681818182">
8% noise. Moreover, at least for the confidently
judged cases, STRAND is in agreement with the
combined human judgment more often than the
human judges agree with each other. The recall
figure indicates that for every true translation
pair it accepts, STRAND must also incorrectly re-
ject a true translation pair. Alternatively, this
can be interpreted as saying that the filtering
process has the system identifying about half
of the pairs it could in principle have found
given the candidates produced by the genera-
tion stage. Error analysis suggests that recall
could be increased (at a possible cost to pre-
cision) by making structural filtering more in-
telligent; for example, ignoring some types of
markup (such as italics) when computing align-
ments. However, I presume that if the number
M of translation pairs on the Web is large, then
half of M is also large. Therefore I focus on in-
creasing the total yield by attempting to bring
the number of generated candidate pairs closer
to M, as described in the next section.
</bodyText>
<sectionHeader confidence="0.87243" genericHeader="method">
4 Scaling Up Candidate Generation
</sectionHeader>
<bodyText confidence="0.99958325925926">
The preliminary experiments and the new ex-
periment reported in the previous section made
use of the Altavista search engine to locate &amp;quot;par-
ent&amp;quot; pages, pointing off to multiple language
versions of the same text. However, the same
basic mechanism is easily extended to locate
&amp;quot;sibling&amp;quot; pages: cases where the page in one
language contains a link directly to the trans-
lated page in the other language. Exploration
of the Web suggests that parent pages and sib-
ling pages cover the major relationships between
parallel translations on the Web. Some sites
with bilingual text are arranged according to a
third principle: they contain a completely sep-
arate monolingual sub-tree for each language,
with only the single top-level home page point-
ing off to the root page of single-language ver-
sion of the site. As a first step in increasing
the number of generated candidate page pairs,
STRAND was extended to permit both parent
and sibling search criteria. Relating monolin-
gual sub-trees is an issue for future work.
In principle, using Altavista queries for
the candidate generation stage should enable
STRAND to locate every page pair in the Al-
tavista index that meets the search criteria.
This likely to be an upper bound on the can-
</bodyText>
<table confidence="0.9384604">
Comparison N Pr(Agree)
J1, J2: 267 0.98 0.95
J1, STRAND: 273 0.84 0.65
J2, STRAND: 315 0.85 0.63
J1 n J2, STRAND: 261 0.86 0.68
</table>
<tableCaption confidence="0.997561">
Table 2: English-French evaluation
</tableCaption>
<bodyText confidence="0.999695795454545">
didates that can be obtained without building
a Web crawler dedicated to the task, since one
of Altavista&apos;s distinguishing features is the size
of its index. In practice, however, the user inter-
face for Altavista appears to limit the number
of hits returned to about the first 1000. It was
possible to break this barrier by using a feature
of Altavista&apos;s &amp;quot;Advanced Search&amp;quot;: including a
range of dates in a query&apos;s selection criteria.
Having already redesigned the STRAND gener-
ation component to permit multiple queries (in
order to allow search for both parent and sibling
pages), each query in the query set was trans-
formed into a set of mutually exclusive queries
based on a one-day range; for example, one ver-
sion of a query would restrict the result to pages
last updated on 30 November 1998, the next 29
November 1998, and so forth.
Although the solution granularity was not
perfect — searches for some days still bumped
up against the 1000-hit maximum — use of both
parent and sibling queries with date-range re-
stricted queries increased the productivity of
the candidate generation component by an or-
der of magnitude. The scaled-up system was
run for English-French document pairs in late
November, 1998, and the generation component
produced 16763 candidate page pairs (with du-
plicates removed), an 18-fold increase over the
previous experiment. After eliminating 3153
page pairs that were either exact duplicates
or irretrievable, STRAND&apos;S structural filtering
removed 9820 candidate page pairs, and the
language identification component removed an-
other 414. The remaining pairs identified as
GOOD — i.e. those that STRAND considered
to be parallel translations — comprise a paral-
lel corpus of 3376 document pairs.
A formal evaluation, conducted in the same
fashion as the previous experiment, yields the
agreement data in Table 2. Using the cases
where the two human judgments agree as
ground truth, precision of the system is esti-
mated at 79.5%, and recall at 70.3%.
</bodyText>
<page confidence="0.988611">
532
</page>
<table confidence="0.917331">
Comparison N Pr(Agree) K
J1, J2: 267 0.98 0.95
J1, STRAND: 273 0.88 0.70
J2, STRAND: 315 0.88 0.69
J1 (1 J2, STRAND: 261 0.90 0.75
</table>
<tableCaption confidence="0.971547">
Table 3: English-French evaluation with stricter
language ID criterion
</tableCaption>
<bodyText confidence="0.992699685714286">
A look at STRAND&apos;s errors quickly identifies
the major source of error as a shortcoming of
the language identification module: its implicit
assumption that every document is either in En-
glish or in French. This assumption was vi-
olated by a set of candidates in the test set,
all from the same site, that pair Dutch pages
with French. The language identification cri-
terion adopted in the previous section requires
only that the Dutch pages look more like En-
glish than like French, which in most cases is
true. This problem is easily resolved by train-
ing the existing language identification compo-
nent with a wider range of languages, and then
adopting a stricter filtering criterion requiring
that Pr(Englishldi) &gt; Pr(Lidi) for every lan-
guage L in that range, and that d2 meet the
corresponding requirement for French.&apos; Doing
so leads to the results in Table 3.
This translates into an estimated 100% pre-
cision against 64.1% recall, with a yield of 2491
documents, approximately 1.5 million words per
language as counted after removal of HTML
markup. That is, with a reasonable though
admittedly post-hoc revision of the language
identification criterion, comparison with human
subjects suggests the acquired corpus is non-
trivial and essentially noise free, and moreover,
that the system excludes only a third of the
pages that should have been kept. Naturally
this will need to be verified in a new evaluation
on fresh data.
6Language ID across a wide range of languages is
not difficult to obtain. E.g. see the 13-language set
of the freely available CMU stochastic language iden-
</bodyText>
<footnote confidence="0.753240555555556">
tifier (http://www.cs.cmu.eduh,dougb/ident.html),
the 18-language set of the Sun Language ID Engine
(http://www.sunlabs.com/research/ila/demo/index.html),
or the 31-language set of the XRCE Language
Identifier (http://www.rxrc.xerox.com/research/
mlit/Tools/guesser.html). Here I used the language ID
method of the previous section trained with profiles
of Danish, Dutch, English, French, German, Italian,
Norwegian, Portuguese, Spanish, and Swedish.
</footnote>
<sectionHeader confidence="0.997296" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999982020408163">
This paper places acquisition of parallel text
from the Web on solid empirical footing, mak-
ing a number of contributions that go beyond
the preliminary study. The system has been
extended with automated language identifica-
tion, and scaled up to the point where a non-
trivial parallel corpus of English and French can
be produced completely automatically from the
World Wide Web. In the process, it was discov-
ered that the most lightweight use of language
identification, restricted to just the the language
pair of interest, needed to be revised in favor of a
strategy that includes identification over a wide
range of languages. Rigorous evaluation using
human judges suggests that the technique pro-
duces an extremely clean corpus — noise esti-
mated at between 0 and 8% — even without hu-
man intervention, requiring no more resources
per language than a relatively small sample of
text used to train automatic language identifi-
cation.
Two directions for future work are appar-
ent. First, experiments need to be done using
languages that are less common on the Web.
Likely first pairs to try include English-Korean,
English-Italian, and English-Greek. Inspection
of Web sites — those with bilingual text identi-
fied by STRAND and those without — suggests
that the strategy of using Altavista to generate
candidate pairs could be improved upon signifi-
cantly by adding a true Web crawler to &amp;quot;mine&amp;quot;
sites where bilingual text is known to be avail-
able, e.g. sites uncovered by a first pass of the
system using the Altavista engine. I would con-
jecture that for English-French there is an order
of magnitude more bilingual text on the Web
than that uncovered in this early stage of re-
search.
A second natural direction is the applica-
tion of Web-based parallel text in applications
such as lexical acquisition and cross-language
information retrieval — especially since a side-
effect of the core STRAND algorithm is aligned
&amp;quot;chunks&amp;quot;, i.e. non-markup segments found to
correspond to each other based on alignment
of the markup. Preliminary experiments using
even small amounts of these data suggest that
standard techniques, such as cross-language lex-
ical association, can uncover useful data.
</bodyText>
<page confidence="0.998005">
533
</page>
<sectionHeader confidence="0.998347" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999325416666667">
P. Brown, J. Cocke, S. Della Pietra, V. Della
Pietra, F. Jelinek, R. Mercer, and P. Roossin.
1990. A statistical approach to ma-
chine translation. Computational Linguistics,
16(2):79-85.
Jean Carletta. 1996. Assessing agreement
on classification tasks: the Kappa statis-
tic. Computational Linguistics, 22(2):249-
254, June.
Mark Davis and Ted Dunning. 1995. A TREC
evaluation of query translation methods for
multi-lingual text retrieval. In Fourth Text
Retrieval Conference (TREC-4). NIST.
Ted Dunning. 1994. Statistical identification of
language. Computing Research Laboratory
Technical Memo MCCS 94-273, New Mexico
State University, Las Cruces, New Mexico.
David A. Hull and Douglas W. Oard. 1997.
Symposium on cross-language text and
speech retrieval. Technical Report SS-97-04,
American Association for Artificial Intelli-
gence, Menlo Park, CA, March.
Thomas K. Landauer and Michael L. Littman.
1990. Fully automatic cross-language docu-
ment retrieval using latent semantic indexing.
In Proceedings of the Sixth Annual Confer-
ence of the UW Centre for the New Oxford
English Dictionary and Text Research, pages
pages 31-38, UW Centre for the New OED
and Text Research, Waterloo, Ontario, Octo-
ber.
Douglas W. Oard. 1997. Cross-language text
retrieval research in the USA. In Third
DEL OS Workshop. European Research Con-
sortium for Informatics and Mathematics
March.
Philip Resnik. 1998. Parallel strands: A pre-
liminary investigation into mining the web for
bilingual text. In Proceedings of the Third
Conference of the Association for Machine
Translation in the Americas, AMTA-98, in
Lecture Notes in Artificial Intelligence, 1529,
Langhorne, PA, October 28-31.
E. M. Voorhees and D. K. Harman. 1998.
The seventh Text REtrieval Conference
(TREC-7). NIST special publication,
Gaithersburg, Maryland, November 9-11.
http://trec.nist.gov/pubs.html.
</reference>
<page confidence="0.998316">
534
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.540927">
<title confidence="0.999937">Mining the Web for Bilingual Text</title>
<author confidence="0.999846">Philip Resnik</author>
<affiliation confidence="0.999932">Dept. of Linguistics/Institute for Advanced Computer Studies</affiliation>
<address confidence="0.589751">University of Maryland, College Park, MD 20742</address>
<email confidence="0.999774">resnik@umiacs.umd.edu</email>
<abstract confidence="0.992047916666667">STRAND (Resnik, 1998) is a languageindependent system for automatic discovery of text in parallel translation on the World Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance. The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>F Jelinek</author>
<author>R Mercer</author>
<author>P Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="822" citStr="Brown et al., 1990" startWordPosition="114" endWordPosition="117"> 1998) is a languageindependent system for automatic discovery of text in parallel translation on the World Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance. The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language. 1 Introduction Text in parallel translation is a valuable resource in natural language processing. Statistical methods in machine translation (e.g. (Brown et al., 1990)) typically rely on large quantities of bilingual text aligned at the document or sentence level, and a number of approaches in the burgeoning field of crosslanguage information retrieval exploit parallel corpora either in place of or in addition to mappings between languages based on information from bilingual dictionaries (Davis and Dunning, 1995; Landauer and Littman, 1990; Hull and Oard, 1997; Oard, 1997). Despite the utility of such data, however, sources of bilingual text are subject to such limitations as licensing restrictions, usage fees, restricted domains or genres, and dated text (</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Mercer, Roossin, 1990</marker>
<rawString>P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, R. Mercer, and P. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the Kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="21761" citStr="Carletta, 1996" startWordPosition="3438" endWordPosition="3439">tion. Comparison N Pr(Agree) K J1, J2: 106 0.85 0.70 J1, STRAND: 165 0.91 0.79 J2, STRAND: 113 0.81 0.61 J1 n J2, STRAND: 90 0.91 0.82 Table 1: English-Spanish evaluation Table 1 shows agreement measures between the two judges, between STRAND and each individual judge, and the agreement between STRAND and the intersection of the two judges&apos; annotations — that is, STRAND evaluated against only those cases where the two judges agreed, which are therefore the items we can regard with the highest confidence. The table also shows Cohen&apos;s lc, an agreement measure that corrects for chance agreement (Carletta, 1996); the most important value in the table is the value of 0.7 for the two human judges, which can be interpreted as sufficiently high to indicate that the task is reasonably well defined. (As a rule of thumb, classification tasks with ic &lt; 0.6 are generally thought of as suspect in this regard.) The value of N is the number of pairs that were included, after excluding those for which the human judgement in the comparison was undecided. Since the cases where the two judges agreed can be considered the most reliable, these were used as the basis for the computation of recall and precision. For thi</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the Kappa statistic. Computational Linguistics, 22(2):249-254, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davis</author>
<author>Ted Dunning</author>
</authors>
<title>A TREC evaluation of query translation methods for multi-lingual text retrieval.</title>
<date>1995</date>
<booktitle>In Fourth Text Retrieval Conference (TREC-4).</booktitle>
<publisher>NIST.</publisher>
<contexts>
<context position="1172" citStr="Davis and Dunning, 1995" startWordPosition="169" endWordPosition="172">llel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language. 1 Introduction Text in parallel translation is a valuable resource in natural language processing. Statistical methods in machine translation (e.g. (Brown et al., 1990)) typically rely on large quantities of bilingual text aligned at the document or sentence level, and a number of approaches in the burgeoning field of crosslanguage information retrieval exploit parallel corpora either in place of or in addition to mappings between languages based on information from bilingual dictionaries (Davis and Dunning, 1995; Landauer and Littman, 1990; Hull and Oard, 1997; Oard, 1997). Despite the utility of such data, however, sources of bilingual text are subject to such limitations as licensing restrictions, usage fees, restricted domains or genres, and dated text (such as 1980&apos;s Canadian politics); or such sources simply may not exist for * This work was supported by Department of Defense contract MDA90496C1250, DARPA/ITO Contract N66001-97-C-8540, and a research grant from Sun Microsystems Laboratories. The author gratefully acknowledges the comments of the anonymous reviewers, helpful discussions with Dan </context>
</contexts>
<marker>Davis, Dunning, 1995</marker>
<rawString>Mark Davis and Ted Dunning. 1995. A TREC evaluation of query translation methods for multi-lingual text retrieval. In Fourth Text Retrieval Conference (TREC-4). NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Statistical identification of language.</title>
<date>1994</date>
<journal>Computing Research Laboratory</journal>
<tech>Technical Memo MCCS 94-273,</tech>
<institution>New Mexico State University, Las Cruces,</institution>
<location>New Mexico.</location>
<contexts>
<context position="17500" citStr="Dunning, 1994" startWordPosition="2725" endWordPosition="2726">sh, which pass the structural filter with flying colors. The problem is potentially acute if the generation stage happens to yield up many pairs of pages that come from online catalogues or other Web sites having large numbers of pages with a conventional structure. There is, of course, an obvious solution that will handle most such cases: making sure that the two pages are actually written in the languages they are supposed to be written in. In order to filter out candidate page pairs that fail this test, statistical language identification based on character n-grams was added to the system (Dunning, 1994). Although this does introduce a need for language-specific training data for the two languages under consideration, it is a very mild form of language dependence: Dunning and others have shown that when classifying strings on the order of hundreds or thousands of characters, which is typical of the non-markup text in Web pages, it is possible to discriminate languages with accuracy in the high 90% range for many or most language pairs given as little as 50k characters per language as training material. For the language filtering stage of STRAND, the following criterion was adopted: given two </context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Statistical identification of language. Computing Research Laboratory Technical Memo MCCS 94-273, New Mexico State University, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Hull</author>
<author>Douglas W Oard</author>
</authors>
<title>Symposium on cross-language text and speech retrieval.</title>
<date>1997</date>
<tech>Technical Report SS-97-04,</tech>
<institution>American Association for Artificial Intelligence,</institution>
<location>Menlo Park, CA,</location>
<contexts>
<context position="1221" citStr="Hull and Oard, 1997" startWordPosition="177" endWordPosition="180">airs, approximately 1.5 million words per language. 1 Introduction Text in parallel translation is a valuable resource in natural language processing. Statistical methods in machine translation (e.g. (Brown et al., 1990)) typically rely on large quantities of bilingual text aligned at the document or sentence level, and a number of approaches in the burgeoning field of crosslanguage information retrieval exploit parallel corpora either in place of or in addition to mappings between languages based on information from bilingual dictionaries (Davis and Dunning, 1995; Landauer and Littman, 1990; Hull and Oard, 1997; Oard, 1997). Despite the utility of such data, however, sources of bilingual text are subject to such limitations as licensing restrictions, usage fees, restricted domains or genres, and dated text (such as 1980&apos;s Canadian politics); or such sources simply may not exist for * This work was supported by Department of Defense contract MDA90496C1250, DARPA/ITO Contract N66001-97-C-8540, and a research grant from Sun Microsystems Laboratories. The author gratefully acknowledges the comments of the anonymous reviewers, helpful discussions with Dan Melamed and Doug Oard, and the assistance of Jeff</context>
</contexts>
<marker>Hull, Oard, 1997</marker>
<rawString>David A. Hull and Douglas W. Oard. 1997. Symposium on cross-language text and speech retrieval. Technical Report SS-97-04, American Association for Artificial Intelligence, Menlo Park, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Michael L Littman</author>
</authors>
<title>Fully automatic cross-language document retrieval using latent semantic indexing.</title>
<date>1990</date>
<booktitle>In Proceedings of the Sixth Annual Conference of the UW Centre for the New Oxford English Dictionary and Text Research,</booktitle>
<pages>31--38</pages>
<location>Waterloo, Ontario,</location>
<contexts>
<context position="1200" citStr="Landauer and Littman, 1990" startWordPosition="173" endWordPosition="176">91 English-French document pairs, approximately 1.5 million words per language. 1 Introduction Text in parallel translation is a valuable resource in natural language processing. Statistical methods in machine translation (e.g. (Brown et al., 1990)) typically rely on large quantities of bilingual text aligned at the document or sentence level, and a number of approaches in the burgeoning field of crosslanguage information retrieval exploit parallel corpora either in place of or in addition to mappings between languages based on information from bilingual dictionaries (Davis and Dunning, 1995; Landauer and Littman, 1990; Hull and Oard, 1997; Oard, 1997). Despite the utility of such data, however, sources of bilingual text are subject to such limitations as licensing restrictions, usage fees, restricted domains or genres, and dated text (such as 1980&apos;s Canadian politics); or such sources simply may not exist for * This work was supported by Department of Defense contract MDA90496C1250, DARPA/ITO Contract N66001-97-C-8540, and a research grant from Sun Microsystems Laboratories. The author gratefully acknowledges the comments of the anonymous reviewers, helpful discussions with Dan Melamed and Doug Oard, and t</context>
</contexts>
<marker>Landauer, Littman, 1990</marker>
<rawString>Thomas K. Landauer and Michael L. Littman. 1990. Fully automatic cross-language document retrieval using latent semantic indexing. In Proceedings of the Sixth Annual Conference of the UW Centre for the New Oxford English Dictionary and Text Research, pages pages 31-38, UW Centre for the New OED and Text Research, Waterloo, Ontario, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas W Oard</author>
</authors>
<title>Cross-language text retrieval research in the USA.</title>
<date>1997</date>
<booktitle>In Third DEL OS Workshop. European Research Consortium for Informatics and Mathematics</booktitle>
<contexts>
<context position="1221" citStr="Oard, 1997" startWordPosition="179" endWordPosition="180">roximately 1.5 million words per language. 1 Introduction Text in parallel translation is a valuable resource in natural language processing. Statistical methods in machine translation (e.g. (Brown et al., 1990)) typically rely on large quantities of bilingual text aligned at the document or sentence level, and a number of approaches in the burgeoning field of crosslanguage information retrieval exploit parallel corpora either in place of or in addition to mappings between languages based on information from bilingual dictionaries (Davis and Dunning, 1995; Landauer and Littman, 1990; Hull and Oard, 1997; Oard, 1997). Despite the utility of such data, however, sources of bilingual text are subject to such limitations as licensing restrictions, usage fees, restricted domains or genres, and dated text (such as 1980&apos;s Canadian politics); or such sources simply may not exist for * This work was supported by Department of Defense contract MDA90496C1250, DARPA/ITO Contract N66001-97-C-8540, and a research grant from Sun Microsystems Laboratories. The author gratefully acknowledges the comments of the anonymous reviewers, helpful discussions with Dan Melamed and Doug Oard, and the assistance of Jeff</context>
</contexts>
<marker>Oard, 1997</marker>
<rawString>Douglas W. Oard. 1997. Cross-language text retrieval research in the USA. In Third DEL OS Workshop. European Research Consortium for Informatics and Mathematics March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Parallel strands: A preliminary investigation into mining the web for bilingual text.</title>
<date>1998</date>
<booktitle>In Proceedings of the Third Conference of the Association for Machine Translation in the Americas, AMTA-98, in Lecture Notes in Artificial Intelligence,</booktitle>
<location>1529, Langhorne, PA,</location>
<contexts>
<context position="3663" citStr="Resnik (1998)" startWordPosition="567" endWordPosition="568"> presentation at the 1998 TREC-7 conference (Voorhees and Harman, 1998), observed that the performance of their cross-language information retrieval was hurt by lexical gaps such as Bosnial Bosnie — this illustrates a highly topical missing pair in their static lexical resource (which was based on WordNet 1.5). And Gey et al., also at TREC-7, observed that in doing cross-language retrieval using commercial machine translation systems, gaps in the lexicon (their example was acupuncture&apos; Akupunktur) could make the difference between precision of 0.08 and precision of 0.83 on individual queries. Resnik (1998) presented an algorithm called 527 Candidate Pair Candidate Pair Candidate Pair Generation Evaluation Filtering (structural) (language dependent) Figure 1: The STRAND architecture STRAND (Structural Translation Recognition for Acquiring Natural Data) designed to explore the Web as a source of parallel text, demonstrating its potential with a small-scale evaluation based on the author&apos;s judgments. After briefly reviewing the STRAND architecture and preliminary results (Section 2), this paper goes beyond that preliminary work in two significant ways. First, the framework is extended to include a</context>
<context position="5242" citStr="Resnik, 1998" startWordPosition="805" endWordPosition="806">t human judges, precision, and recall (Section 3). Second, the algorithm is scaled up more seriously to generate large numbers of parallel documents, this time for English and French, and again subjected to formal evaluation (Section 4). The concrete end result reported here is an automatically acquired English-French parallel corpus of Web documents comprising 2491 document pairs, approximately 1.5 million words per language (without markup), containing little or no noise. 2 STRAND Preliminaries This section is a brief summary of the STRAND system and previously reported preliminary results (Resnik, 1998). The STRAND architecture is organized as a pipeline, beginning with a candidate generation stage that (over-)generates candidate pairs of documents that might be parallel translations. (See Figure 1.) The first implementation of the generation stage used a query to the Altavista search engine to generate pages that could be viewed as &amp;quot;parents&amp;quot; of pages in parallel translation, by asking for pages containing one portion of anchor text (the readable material in a hyperlink) containing the string &amp;quot;English&amp;quot; within a fixed distance of another anchor text containing the string &amp;quot;Spanish&amp;quot;. (The match</context>
<context position="12750" citStr="Resnik, 1998" startWordPosition="1987" endWordPosition="1988">to large. Therefore we can apply a standard statistical hypothesis test, and if p &lt; .05 we can conclude that the lengths are reliably correlated and accept the page pair as likely to be translations of each other. Otherwise, this candidate page pair is filtered out.4 2An anonymous reviewer observes that diff has no preference for aligning chunks of similar lengths, which in some cases might lead to a poor alignment when a good one exists. This could result in a failure to identify true translations and is worth investigating further. 3Chunk tokens with exactly equal lengths are excluded; see (Resnik, 1998) for reasons and other details of the algorithm. &apos;The level of significance (p &lt; .05) was the initial selection during algorithm development, and never changed. This, the unmatched-tokens threshold for prima facie rejection due to mismatches (20%), and the maximum distance between hyperlinks in the generaIn the preliminary evaluation, I generated a test set containing 90 English-Spanish candidate pairs, using the candidate generation stage as just described. I evaluated these candidates by hand, identifying 24 as true translation pairs.5 Of these 24, STRAND identified 15 as true translation pa</context>
</contexts>
<marker>Resnik, 1998</marker>
<rawString>Philip Resnik. 1998. Parallel strands: A preliminary investigation into mining the web for bilingual text. In Proceedings of the Third Conference of the Association for Machine Translation in the Americas, AMTA-98, in Lecture Notes in Artificial Intelligence, 1529, Langhorne, PA, October 28-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
<author>D K Harman</author>
</authors>
<date>1998</date>
<booktitle>The seventh Text REtrieval Conference (TREC-7). NIST special publication,</booktitle>
<location>Gaithersburg, Maryland,</location>
<note>http://trec.nist.gov/pubs.html.</note>
<contexts>
<context position="3121" citStr="Voorhees and Harman, 1998" startWordPosition="481" endWordPosition="485">s in five languages not identified by the Babel study (Catalan, Chinese, Hungarian, Icelandic, and Arabic; Michael Littman, personal communication). Given the continued explosive increase in the size of the Web, the trend toward business organizations that cross national boundaries, and high levels of competition for consumers in a global marketplace, it seems impossible not to view multilingual content on the Web as an expanding resource. Moreover, it is a dynamic resource, changing in content as the world changes. For example, Diekema et al., in a presentation at the 1998 TREC-7 conference (Voorhees and Harman, 1998), observed that the performance of their cross-language information retrieval was hurt by lexical gaps such as Bosnial Bosnie — this illustrates a highly topical missing pair in their static lexical resource (which was based on WordNet 1.5). And Gey et al., also at TREC-7, observed that in doing cross-language retrieval using commercial machine translation systems, gaps in the lexicon (their example was acupuncture&apos; Akupunktur) could make the difference between precision of 0.08 and precision of 0.83 on individual queries. Resnik (1998) presented an algorithm called 527 Candidate Pair Candidat</context>
</contexts>
<marker>Voorhees, Harman, 1998</marker>
<rawString>E. M. Voorhees and D. K. Harman. 1998. The seventh Text REtrieval Conference (TREC-7). NIST special publication, Gaithersburg, Maryland, November 9-11. http://trec.nist.gov/pubs.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>