<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.103778">
<affiliation confidence="0.381330333333333">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</affiliation>
<equation confidence="0.982729693548387">
climb\2#1 158 a_hundred\5 1 ab-
sorb\2 1 advance\2 1 ... walk\2 1
want\1 1 warn\2 1 warped\5 1 way\1
2 west\3 1 whip\1 2 whir\1 1
wraithlike\5 1
climb\2#1 45 abruptly\4 1 absence\1
1 ... stop\2 1 switch_off\2 1
there\4 1 tube\1 1 two\5 1 unex-
pectedly\4 1 water\1 1
...
climb\2#2 33 adjust\2 1 almost\4 1
arrange\2 1 ... procedure\1 1 re-
vetment\1 1 run\2 1 sky\1 1
snatch\2 1 spread\2 1 stand\2 1
truck\1 1 various\5 1 wait\2 1
wing\1 1
...
climb\2#3 3 average\2 1 feel\2 1
report\2 1
climb\2#1 10 arise\2 1 come_up\2 1
go\2 1 go_up\2 1 lift\2 1 locomote\2
1 move\2 1 move_up\2 1 rise\2 1
travel\2 1
climb\2#1 3 climb_up\2 1 go_up\2 1
mount\2 1
climb\2#1 5 mountaineer\2 1 ramp\2 1
ride\2 1 scale\2 1 twine\2 1
climb\2#2 7 clamber\2 1 scramble\2 1
shin\2 1 shinny\2 1 skin\2 1 sput-
ter\2 1 struggle\2 1
climb\2#5 2 go up\2 1 rise\2 1
...
α
{ }
−
α
α + = α −
( ) ( ) ( )
α
+ = + ⋅α
⋅ −
[ ]
≠
≠α &lt;α &lt;
α
climb\2#1 1921 a\1#0 0.01883
aarseth\1#0 0.03259 abelard\1#0 ...
yorkshire\1#0 0.03950 young\3#0
0.00380 zero\1#0 0.01449
climb\2#2 235 act\1#0 -0.11558
alone\4#0 -0.07754 ... windy\3#0 -
0.00922 worker\1#0 -0.02738 year\1#0
-0.03715 zacchaeus\1#0 -0.02344
climb\2#3 1148 abchasicus\1#0
0.04127 able\3#0 -0.00945 ...
young\3#0 -0.00275 zero\1#0 -0.00010
climb\2#4 258 age\1#0 -0.04180 air-
space\1#0 -0.02862 alone\4#0 -
0.01920 apple\1#0 -0.04242 ...
world\1#0 -0.14184 year\1#0 -0.04113
young\3#0 -0.04831 zero\1#0 -0.06230
...
</equation>
<bodyText confidence="0.98833725">
lish all word task, we have only used the complete
contexts of both SemCor and WordNet resources.
The cornus has been tagged and lemmatized using
the Tree-tagger (Schmid, 1994).
</bodyText>
<figureCaption confidence="0.998246">
Figure 4. The network architecture
</figureCaption>
<bodyText confidence="0.999807071428571">
Once the training has finished, the testing be-
gins. The test is very simple. We establish the
similarity between a given vector of the corpus
evaluation with all the codebook vectors of its do-
main, and the highest similarity value corresponds
to the disambiguated sense (winner sense). If it is
not possible to find a sense (it is impossible to ob-
tain the cosine similarity value), we assign by de-
fault the most frequent sense (e.g. the first sense in
WordNet).
The official results achieved by the University of
Jaen system are presented in Table 1 for English
lexical sample task, and in Table 2 for English all
words.
</bodyText>
<table confidence="0.993641333333333">
ELS Precision Recall Coverage
Fine-grained 0.613 0.613 99.95%
Coarse-grained 0.695 0.695 99.95%
</table>
<tableCaption confidence="0.996806">
Table 1. Official results for ELS.
</tableCaption>
<table confidence="0.998917666666667">
EA W Precision Recall Coverage
With U 0.590 0.590 100%
Without U 0.601 0.588 97.795%
</table>
<tableCaption confidence="0.999501">
Table 2. Official results for EAW.
</tableCaption>
<sectionHeader confidence="0.667891" genericHeader="abstract">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.997173714285714">
This paper presents a new approach based on
neural networks to disambiguate the word senses.
We have used the LVQ algorithm to train a neural
network to carry out the English lexical sample and
English all words tasks. We have integrated two
linguistic resources in the corpus provided by the
organization: WordNet and SemCor.
</bodyText>
<sectionHeader confidence="0.988521" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987154904761905">
Fellbaum, C. 1998. WordNet: An Electronic Lexi-
cal Database. The MIT Press
Kohonen, T. 1995. Self-Organization and Associa-
tive Memory. 2nd Ed, Springer.Verlag, Berlin.
Kohonen, T., J. Hynninen, J. Kangas, J. Laak-
sonen, K. Torkkola. 1996. Technical Report,
LVQ_PAK: The Learning Vector Quantization
Program Package. Helsinki University of Tech-
nology, Laboratory of Computer and Information
Science, FIN-02150 Espoo, Finland.
Miller G., C. Leacock, T. Randee, R. Bunker.
1993. A Semantic Concordance. Proc. of the 3rd
DARPA Workshop on Human Language Tech-
nology.
Salton, G. &amp; McGill, M.J. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill,
New York.
Schmid, H., 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of International Conference on New Methods in
Language Processing.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000008">
<note confidence="0.7637685">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics 1 absorb\2 1 advance\2 1 ... walk\2 1 want\1 1 warn\2 1 warped\5 1 way\1 2 west\3 1 whip\1 2 whir\1 1 wraithlike\5 1 1 absence\1 1 ... stop\2 1 switch_off\2 1 there\4 1 tube\1 1 two\5 1 unexpectedly\4 1 water\1 1</note>
<abstract confidence="0.805292357142857">1 almost\4 1 arrange\2 1 ... procedure\1 1 revetment\1 1 run\2 1 sky\1 1 snatch\2 1 spread\2 1 stand\2 1 truck\1 1 various\5 1 wait\2 1 wing\1 1 ... 1 feel\2 1 report\2 1 1 come_up\2 1 go\2 1 go_up\2 1 lift\2 1 locomote\2 1 move\2 1 move_up\2 1 rise\2 1 travel\2 1 1 go_up\2 1 mount\2 1 1 ramp\2 1 ride\2 1 scale\2 1 twine\2 1 1 scramble\2 1 shin\2 1 shinny\2 1 skin\2 1 sputter\2 1 struggle\2 1 climb\2#5 2 go up\2 1 rise\2 1 ... α { } − α = ) ( ) ) α = + ⋅ − [ ] ≠ α 0.01883 aarseth\1#0 0.03259 abelard\1#0 ... yorkshire\1#0 0.03950 young\3#0 0.00380 zero\1#0 0.01449 -0.11558 alone\4#0 -0.07754 ... windy\3#0 - 0.00922 worker\1#0 -0.02738 year\1#0 -0.03715 zacchaeus\1#0 -0.02344 0.04127 able\3#0 -0.00945 ... young\3#0 -0.00275 zero\1#0 -0.00010 -0.04180 airspace\1#0 -0.02862 alone\4#0 - 0.01920 apple\1#0 -0.04242 ... world\1#0 -0.14184 year\1#0 -0.04113 young\3#0 -0.04831 zero\1#0 -0.06230 ... lish all word task, we have only used the complete contexts of both SemCor and WordNet resources. The cornus has been tagged and lemmatized using the Tree-tagger (Schmid, 1994). Figure 4. The network architecture Once the training has finished, the testing begins. The test is very simple. We establish the similarity between a given vector of the corpus evaluation with all the codebook vectors of its domain, and the highest similarity value corresponds to the disambiguated sense (winner sense). If it is not possible to find a sense (it is impossible to obtain the cosine similarity value), we assign by default the most frequent sense (e.g. the first sense in WordNet). The official results achieved by the University of Jaen system are presented in Table 1 for English lexical sample task, and in Table 2 for English all words.</abstract>
<note confidence="0.7086824">ELS Precision Recall Coverage Fine-grained 0.613 0.613 99.95% Coarse-grained 0.695 0.695 99.95% Table 1. Official results for ELS. Precision Recall Coverage With U 0.590 0.590 100% Without U 0.601 0.588 97.795% Table 2. Official results for EAW. 5 Conclusion This paper presents a new approach based on</note>
<abstract confidence="0.991194333333333">neural networks to disambiguate the word senses. We have used the LVQ algorithm to train a neural network to carry out the English lexical sample and English all words tasks. We have integrated two linguistic resources in the corpus provided by the organization: WordNet and SemCor.</abstract>
<note confidence="0.6721895">References Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. The MIT Press Kohonen, T. 1995. Self-Organization and Associative Memory. 2nd Ed, Springer.Verlag, Berlin. Kohonen, T., J. Hynninen, J. Kangas, J. Laak-</note>
<keyword confidence="0.32468475">sonen, K. Torkkola. 1996. Technical Report, LVQ_PAK: The Learning Vector Quantization Program Package. Helsinki University of Technology, Laboratory of Computer and Information</keyword>
<note confidence="0.6341408">Science, FIN-02150 Espoo, Finland. Miller G., C. Leacock, T. Randee, R. Bunker. 1993. A Semantic Concordance. Proc. of the 3rd DARPA Workshop on Human Language Technology.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press</publisher>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. The MIT Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
</authors>
<title>Self-Organization and Associative Memory. 2nd Ed,</title>
<date>1995</date>
<publisher>Springer.Verlag,</publisher>
<location>Berlin.</location>
<marker>Kohonen, 1995</marker>
<rawString>Kohonen, T. 1995. Self-Organization and Associative Memory. 2nd Ed, Springer.Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
<author>J Hynninen</author>
<author>J Kangas</author>
<author>J Laaksonen</author>
<author>K Torkkola</author>
</authors>
<date>1996</date>
<tech>Technical Report, LVQ_PAK:</tech>
<institution>The Learning Vector Quantization Program Package. Helsinki University of Technology, Laboratory of Computer and Information Science,</institution>
<location>FIN-02150 Espoo, Finland.</location>
<marker>Kohonen, Hynninen, Kangas, Laaksonen, Torkkola, 1996</marker>
<rawString>Kohonen, T., J. Hynninen, J. Kangas, J. Laaksonen, K. Torkkola. 1996. Technical Report, LVQ_PAK: The Learning Vector Quantization Program Package. Helsinki University of Technology, Laboratory of Computer and Information Science, FIN-02150 Espoo, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>C Leacock</author>
<author>T Randee</author>
<author>R Bunker</author>
</authors>
<title>A Semantic Concordance.</title>
<date>1993</date>
<booktitle>Proc. of the 3rd DARPA Workshop on Human Language Technology.</booktitle>
<marker>Miller, Leacock, Randee, Bunker, 1993</marker>
<rawString>Miller G., C. Leacock, T. Randee, R. Bunker. 1993. A Semantic Concordance. Proc. of the 3rd DARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. &amp; McGill, M.J. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="1776" citStr="Schmid, 1994" startWordPosition="302" endWordPosition="303">g\3#0 0.00380 zero\1#0 0.01449 climb\2#2 235 act\1#0 -0.11558 alone\4#0 -0.07754 ... windy\3#0 - 0.00922 worker\1#0 -0.02738 year\1#0 -0.03715 zacchaeus\1#0 -0.02344 climb\2#3 1148 abchasicus\1#0 0.04127 able\3#0 -0.00945 ... young\3#0 -0.00275 zero\1#0 -0.00010 climb\2#4 258 age\1#0 -0.04180 airspace\1#0 -0.02862 alone\4#0 - 0.01920 apple\1#0 -0.04242 ... world\1#0 -0.14184 year\1#0 -0.04113 young\3#0 -0.04831 zero\1#0 -0.06230 ... lish all word task, we have only used the complete contexts of both SemCor and WordNet resources. The cornus has been tagged and lemmatized using the Tree-tagger (Schmid, 1994). Figure 4. The network architecture Once the training has finished, the testing begins. The test is very simple. We establish the similarity between a given vector of the corpus evaluation with all the codebook vectors of its domain, and the highest similarity value corresponds to the disambiguated sense (winner sense). If it is not possible to find a sense (it is impossible to obtain the cosine similarity value), we assign by default the most frequent sense (e.g. the first sense in WordNet). The official results achieved by the University of Jaen system are presented in Table 1 for English l</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, H., 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of International Conference on New Methods in Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>