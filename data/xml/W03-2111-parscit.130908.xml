<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.076022">
<title confidence="0.9988465">
Using Wizard-of-Oz simulations to bootstrap Reinforcement-Learning-
based dialog management systems
</title>
<author confidence="0.998711">
Jason D. Williams Steve Young
</author>
<affiliation confidence="0.999848">
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, United Kingdom
</affiliation>
<email confidence="0.983097">
{jdw30,sjy}@eng.cam.ac.uk
</email>
<sectionHeader confidence="0.997139" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99985725">
This paper describes a method for “boot-
strapping” a Reinforcement Learning-
based dialog manager using a Wizard-of-
Oz trial. The state space and action set
are discovered through the annotation,
and an initial policy is generated using a
Supervised Learning algorithm. The
method is tested and shown to create an
initial policy which performs significantly
better and with less effort than a hand-
crafted policy, and can be generated using
a small number of dialogs.
</bodyText>
<sectionHeader confidence="0.993994" genericHeader="keywords">
1 Introduction and motivation
</sectionHeader>
<bodyText confidence="0.99994">
Recent work has successfully applied Rein-
forcement Learning (RL) to learning dialog strat-
egy from experience, typically formulating the
problem as a Markov Decision Process (MDP).
(Walker et al., 1998; Singh et al., 2002; Levin et
al., 2000). Despite successes, several open
questions remain, especially the issue of how to
create (or “bootstrap”) the initial system prior to
data becoming available from on-line operation.
This paper proceeds as follows. Section 2 out-
lines the core elements of an MDP and issues re-
lated to applying an MDP to dialog management.
Sections 3 and 4 detail a method for addressing
these issues, and the procedure used to test the
method, respectively. Sections 5-7 present the re-
sults, a discussion, and conclusions, respectively.
</bodyText>
<sectionHeader confidence="0.996992" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999929666666667">
An MDP is composed of a state space, an action
set, and a policy which maps each state to one ac-
tion. Introducing a reward function allows us to
create or refine the policy using RL. (Sutton and
Barto, 1998).
When the MDP framework is applied to dialog
management, the state space is usually constructed
from vector components including information
state, dialog history, recognition confidence, data-
base status, etc. In most of the work to date both
the state space and action set are hand selected, in
part to ensure a limited state space, and to ensure
training can proceed using a tractable number of
dialogs. However, hand selection becomes im-
practical as system size increases, and automatic
generation/selection of these elements is currently
an open problem, closely related to the problem of
exponential state space size.
</bodyText>
<sectionHeader confidence="0.982054" genericHeader="method">
3 A method for bootstrapping RL-based
systems
</sectionHeader>
<bodyText confidence="0.99983475">
Here we propose a method for “bootstrapping” an
MDP-based system; specifically, we address the
choice of the state representation and action set,
and the creation of an initial policy.
</bodyText>
<subsectionHeader confidence="0.99904">
3.1 Step 1: Conduct Wizard-of-Oz dialogs
</subsectionHeader>
<bodyText confidence="0.9999307">
The method commences with “talking wizard”
interactions in which either the wizard’s voice is
disguised, or a Text-to-speech engine is used. We
choose human/wizard rather than human/human
dialogs as people behave differently toward (what
they perceive to be) machines and other people as
discussed in Jönsson and Dahlbick, 1988 and also
validated in Moore and Browning, 1992. The dia-
log, including wizard’s interaction with back-end
data sources is recorded and transcribed.
</bodyText>
<subsectionHeader confidence="0.999055">
3.2 Step 2: Exclude out-of-domain turns
</subsectionHeader>
<bodyText confidence="0.9999765">
The wizard will likely handle a broader set of re-
quests than the system will ultimately be able to
cover; thus some turns must be excluded. Step 2
begins by formulating a list of tasks which are to
be included in the final system’s repertoire; turns
dealing with tasks outside this repertoire are la-
beled out-of-domain (OOD) and excluded.
This step takes an approach which is analogous
to, but more simplistic than “Dialogue Distilling”
(Larsson et al., 2000) which changes, adds and re-
moves portions of turns or whole turns. Here rules
simply stipulate whether to keep a whole turn.
</bodyText>
<subsectionHeader confidence="0.8853275">
3.3 Step 3: Enumerate action set and state
space
</subsectionHeader>
<bodyText confidence="0.9898624">
Next, the in-domain turns are annotated with dia-
log acts. Based on these, an action set is enumer-
ated, and a set of state parameters and their
possible values to form a vector describing the
state space is determined, including:
</bodyText>
<listItem confidence="0.974558444444444">
• Information state (e.g., departure-city, arri-
val-city) from the user and database.
• The confidence/confirmation status of in-
formation state variables.
• Expressed user goal and/or system goal.
• Low-level turn information (e.g., yes/no re-
sponses, backchannel, “thank you”, etc.).
• Status of database interactions (e.g., when a
form can be submitted or has been returned).
</listItem>
<bodyText confidence="0.999842611111111">
A variety of dialog-act tagging taxonomies ex-
ist in the literature. Here we avoid a tagging sys-
tem that relies on a stack or other recursive
structure (for example, a goal or game stack) as it
is not immediately clear how to represent a recur-
sive structure in a state space.
In practice, many information state components
are much less important than their corresponding
confirmation status, and can be removed.
Even with this reduction, the state space will be
massive – probably too large to ever visit all states.
We propose using a parameterized value function -
- i.e., a value function that shares parameters
across states (including states previously unob-
served). One special case of this is state tying, in
which a group of states share the same value func-
tion; an alternative is to use a Supervised Learning
algorithm to estimate a value function.
</bodyText>
<subsectionHeader confidence="0.99123">
3.4 Step 4: Form an initial policy
</subsectionHeader>
<bodyText confidence="0.999970368421053">
For each turn in the corpus, a vector is created rep-
resenting the current dialog state plus the subse-
quent wizard action. Taking the action as the class
variable, Supervised Learning (SL) is used to build
a classifier which functions as the initial policy.
Depending on the type of SL algorithm used, it
may be possible to produce a prioritized list of ac-
tions rather than a single classification; in this case,
this list can form an initial list of actions permitted
in a given state.
As noted by Levin et al. (2000), supervised
learning is not appropriate for optimizing dialog
strategy because of the temporal/environmental
nature of dialog. Here we do not assert that the
SL-learned policy will be optimal – simply that it
can be easily created, that it will be significantly
better than random guessing, and better and
cheaper to produce than creating a cursory hand-
crafted strategy.
</bodyText>
<subsectionHeader confidence="0.995861">
3.5 Limitations of the method
</subsectionHeader>
<bodyText confidence="0.986735">
This method has several obvious limitations:
</bodyText>
<listItem confidence="0.984626857142857">
• Because a talking, perfect-hearing wizard is
used, no/little account is taken of the recog-
nition errors to be expected with automated
speech recognition (ASR).
• Excluding too much in Step 2 may exclude
actions or state components which would
have ultimately produced a superior system.
</listItem>
<sectionHeader confidence="0.988474" genericHeader="method">
4 Experimental design
</sectionHeader>
<bodyText confidence="0.999809384615384">
The proposed approach has been tested using the
Autoroute corpus of 166 dialogs, in which a talk-
ing wizard answered questions about driving direc-
tions in the UK (Moore and Browning, 1992).
A small set of in-domain tasks was enumerated
(e.g., gathering route details, outputting summary
information about a route, disambiguation of place
names, etc.), and turns which did not deal with
these tasks were labeled OOD and excluded. The
latter included gathering the caller’s name and lo-
cation (“UserID”), the most common OOD type.
The corpus was annotated using an XML
schema to provide the following:
</bodyText>
<listItem confidence="0.916011043478261">
• 15 information components were created
(e.g., from, to, time, car-type).
• Each information component was given a
status: C (Confirmed), U (Unconfirmed),
and NULL (Not known).
• Up to 5 routes may be under discussion at
once – the state tracked the route under dis-
cussion (RUD), total number of routes (TR),
and all information and status components
for each route.
• A component called flow tracked single-
turn dialog flow information from the caller
(e.g., yes, no, thank-you, silence).
• A component called goal tracked the (most
recent) goal expressed by the user (e.g.,
plan-route, how-far). Goal is empty
unless explicitly set by the caller, and only
one goal is tracked at a time. No attempt is
made to indicate if/when a goal has been
satisfied.
33 action types were identified, some of which
take information slots as parameters (e.g., wh-
question, implicit-confirmation) .
</listItem>
<bodyText confidence="0.99982532">
The corpus gave no indication of database in-
teractions other than what can be inferred from the
dialog transcripts. One common wizard action
asked the caller to “please wait” when the wizard
was waiting for a database response. To account
for this, we provided an additional state component
which indicated whether the database was working
called db-request, which was set to true
whenever the action taken was please-wait
and false otherwise. Other less common database
interactions occurred when town names were am-
biguous or not found, and no attempt was made to
incorporate this information into the state represen-
tation.
The state space was constructed using only the
status of the information slots (not the values); of
the 15, 4 were occasionally expressed (e.g., day of
the week) but not used to complete the transaction
and were therefore excluded from the state space.
Two turns of wizard action history were also in-
corporated. This formulation of the state space
leads to approximately 1033 distinct states.
For evaluation of the method, a hand-crafted
policy of 30 rules mapping states to actions was
created by inspecting the dialogs.1
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.968393666666667">
Table 1 shows in-domain vs. out-of-domain wizard
and caller turns. Figures 1 through 4 show counts
of flow values, goal values, action values, and state
</bodyText>
<footnote confidence="0.79987">
1 It was not clear in what situations some of the actions should
be used, so some (rare) actions were not covered by the rules.
</footnote>
<bodyText confidence="0.931041">
components, respectively. The most common ac-
tion type was “please-wait” (14.6% of actions).
</bodyText>
<table confidence="0.99909475">
Criteria States Visits
Visited only 1182 1182
once (85.7%) (45.9%)
Visited more 96 353
than once (7.0%) (13.7%)
without a con-
flict
Visited more 101 1041
than once with (7.3%) (40.3%)
conflict
TOTAL 1379 2576
(100%) (100%)
</table>
<tableCaption confidence="0.955944">
Table 2: “Conflicts” by state and visits
</tableCaption>
<table confidence="0.997876555555556">
Estimated action probabilities Visits
p(action taken  |state) &gt; p(any 774 (74.3%)
other action  |state)
p(action taken  |state) = p(one 119 (11.4%)
or more other actions  |state) &gt;
p(all remaining actions  |state)
p(action taken  |state) &lt; 148 (14.2%)
p(another action  |state)
TOTAL 1041 (100%)
</table>
<tableCaption confidence="0.99373">
Table 3: Estimated probabilities in “conflict” states
</tableCaption>
<table confidence="0.99980025">
Engine Class Precision
jBNC Action-type only 72.7%
Action-type &amp; parameters 66.7%
C4.5 Action-type only 79.1%
Action-type &amp; parameters 72.9%
Hand- Action-type only 58.4%
craft
Action-type &amp; parameters 53.9%
</table>
<tableCaption confidence="0.99989">
Table 4: Results from SL training and evaluation
</tableCaption>
<bodyText confidence="0.998661142857143">
In some cases, the wizard took different actions
in the same state; we labeled this situation a “con-
flict.” Table 2 shows the number of distinct states
that were encountered and, for states visited more
than once, whether conflicting actions were se-
lected. Of states with conflicts, Table 3 shows
probabilities estimated from the corpus.
</bodyText>
<table confidence="0.997159333333333">
Turn Total In OOD: OOD:
type domain User ID Other
Wiz- 3155 2410 594 151
ard (100%) (76.4%) (18.8 %) (4.8%)
Caller 2466 1713 561 192
(100%) (69.5%) (22.7 %) (7.8%)
</table>
<tableCaption confidence="0.999023">
Table 1: In-domain and Out-of-domain (OOD) turns
</tableCaption>
<bodyText confidence="0.999831583333333">
The interaction data was then submitted to 2 SL
pattern classifiers – c4.5 using decision-trees
(Quinlan, 1992) and jBNC using Naïve Bayesians
(Sacha, 2003). Table 4 shows both algorithms’ 10-
fold cross validation classification error rates
classifying (1) the action type, and (2) the action
type with parameters, as well as the results for the
hand-crafted policy.
Figure 5 show the 10-fold cross validation clas-
sification error rates for varying amounts of train-
ing data for the two SL algorithms classifying
action-type and parameters.
</bodyText>
<sectionHeader confidence="0.999382" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99995815">
The majority of the data collected was “usable”:
although 26.7% of turns were excluded, 20.5% of
these were due to a well-defined task not under
study here (user identification), and only 6.1% fell
outside of designated tasks. That said, it may be
desirable to impose a minimum threshold on how
many times a flow, goal, or action must be ob-
served before adding it to the state space or action
set given the “long tails” of these elements.
About half of the turns took place in states
which were visited only once. This confirms that
massive amounts of data would be needed to ob-
serve all valid dialog states, and suggests dialogs
do not confine themselves to familiar states.
Within a given state, the wizard’s behavior is
stochastic, occasionally deviating from an other-
wise static policy. Some of this behavior results
from database information not included in the cor-
pus and state space; in other cases, the wizard is
making apparently random choices.
</bodyText>
<figure confidence="0.994656">
Dialogs containing Action
200
150
100
50
0
Action ID
</figure>
<figureCaption confidence="0.999992">
Figure 1: Dialogs containing flow components
Figure 3: Dialogs containing action types
</figureCaption>
<figure confidence="0.9994625">
1 2 3 4 5 6 7 8 9 10 11 12
Flow component ID
200
150
100
50
0
Dialogs containing Flow ID
Dialogs containing 200
component 150
100
50
0
Component ID
</figure>
<figureCaption confidence="0.999991">
Figure 2: Dialogs containing goal components
Figure 4: Dialogs containing information components
</figureCaption>
<bodyText confidence="0.9963545">
Figure 5 implies that a relatively small number
of dialogs (several hundred turns, or about 30-40
dialogs) contain the vast majority of information
relevant to SL algorithms – less than expected.
Correctly predicting the wizard’s action in 72.9%
of turns is significantly better than the 58.4% cor-
rect prediction rate from the handcrafted policy.
When a caller allows the system to retain initia-
tive, the policy learned by the c4.5 algorithm han-
dled enquiries about single trips perfectly. Policy
</bodyText>
<figure confidence="0.96732275">
1 2 3 4 5 6 7 8 9 10 11 12 13
Goal component ID
Dialogs containing Goal
16
14
12
10
4
</figure>
<page confidence="0.84312525">
2
6
0
8
</page>
<bodyText confidence="0.999946461538462">
errors start to occur as the user takes more initia-
tive, entering less well observed states.
Hand examination of a small number of mis-
classified actions indicate that about half of the
actions were “reasonable” – e.g., including an extra
item in a confirmation. Hand examination also
confirmed that the wizard’s non-deterministic be-
havior and lack of database information resulted in
misclassifications.
Other sources of mis-classifications derived
primarily from under-account of the user’s goal
and other deficiencies in the expressiveness of the
state space.
</bodyText>
<sectionHeader confidence="0.996105" genericHeader="conclusions">
7 Conclusion &amp; future work
</sectionHeader>
<bodyText confidence="0.999980363636364">
This work has proposed a method for determining
many of the basic elements of a RL-based spoken
dialog system with minimal input from dialog de-
signers using a “talking wizard.” The viability of
the model has been tested with an existing corpus
and shown to perform significantly better than a
hand-crafted policy and with less effort to create.
Future research will explore refining this ap-
proach vis-à-vis user goal, applying this method to
actual RL-based systems and finding suitable
methods for parameterized value functions
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999351333333333">
A. Jönsson and N. Dahlbick. 1988. Talking to A Com-
puter is Not Like Talking To Your Best Friend.
Proceedings of the Scandinavian Conference on
ceedings of the Scandinavian Conference on
Artificial Intelligence &apos;88, pp. 53-68.
Staffan Larsson, Arne Jönsson and Lena Santamarta.
2000. Using the process of distilling dialogues to
understand dialogue systems. ICSLP 2000, Beijing.
Ester Levin, Roberto Pieraccini and Wieland Eckert.
2000. A Stochastic Model of Human-Machine Inter-
action for Learning Dialogue Structures. IEEE
Trans on Speech and Audio Processing 8(1):11-23.
R. K. Moore and S. R. Browning. 1992. Results of an
exercise to collect ‘genuine’ spoken enquiries using
Wizard of Oz techniques. Proc. of the Inst. of Acous-
tics.
Ross Quinlan. 1992. C4.5 Release 8. (Software pack-
age). http://www.cse.unsw.edu.au/~quinlan/
Jarek P. Sacha. 2003. jBNC version 1.0. (Software
package). http://sourceforge.net/projects/jbnc/.
Satinder Singh, Diane Litman, Michael Kearns, Marilyn
Walker. 2002. Optimizing Dialogue Management
with Reinforcement Learning: Experiments with the
NJFun System. Journal of Artificial Intelligence Re-
search, vol 16, 105-133.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: an Introduction. The MIT
Press, Cambridge, Massachusetts, USA.
Marilyn A. Walker, Jeanne C. Fromer, Shrikanth Nara-
yanan. 1998. Learning Optimal Dialogue Strate-
gies: A Case Study of a Spoken Dialogue Agent for
Email. Proc. 36th Annual Meeting of the ACM and
17th Int’l Conf. on Comp. Linguistics, 1345--1352.
</reference>
<figure confidence="0.961868666666667">
0 500 1000 1500 2000 2500
Training examples (dialog turns)
80.0%
70.0%
c4.5
Naive Bayes
30.0%
20.0%
Classification errors (%)
60.0%
50.0%
40.0%
</figure>
<figureCaption confidence="0.999803">
Figure 5: Classification errors vs. training samples for action-type &amp; parameters
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924156">
<title confidence="0.998815">Using Wizard-of-Oz simulations to bootstrap Reinforcement-Learningbased dialog management systems</title>
<author confidence="0.999924">Jason D Williams Steve Young</author>
<affiliation confidence="0.94959">Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, United Kingdom</affiliation>
<email confidence="0.983754">jdw30@eng.cam.ac.uk</email>
<email confidence="0.983754">sjy@eng.cam.ac.uk</email>
<abstract confidence="0.999301846153846">This paper describes a method for “bootstrapping” a Reinforcement Learningbased dialog manager using a Wizard-of- Oz trial. The state space and action set are discovered through the annotation, and an initial policy is generated using a Supervised Learning algorithm. method is tested and shown to create an initial policy which performs significantly better and with less effort than a handcrafted policy, and can be generated using a small number of dialogs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Jönsson</author>
<author>N Dahlbick</author>
</authors>
<title>Talking to A Computer is Not Like Talking To Your Best Friend.</title>
<date>1988</date>
<booktitle>Proceedings of the Scandinavian Conference on ceedings of the Scandinavian Conference on Artificial Intelligence &apos;88,</booktitle>
<pages>53--68</pages>
<contexts>
<context position="2947" citStr="Jönsson and Dahlbick, 1988" startWordPosition="454" endWordPosition="457">onential state space size. 3 A method for bootstrapping RL-based systems Here we propose a method for “bootstrapping” an MDP-based system; specifically, we address the choice of the state representation and action set, and the creation of an initial policy. 3.1 Step 1: Conduct Wizard-of-Oz dialogs The method commences with “talking wizard” interactions in which either the wizard’s voice is disguised, or a Text-to-speech engine is used. We choose human/wizard rather than human/human dialogs as people behave differently toward (what they perceive to be) machines and other people as discussed in Jönsson and Dahlbick, 1988 and also validated in Moore and Browning, 1992. The dialog, including wizard’s interaction with back-end data sources is recorded and transcribed. 3.2 Step 2: Exclude out-of-domain turns The wizard will likely handle a broader set of requests than the system will ultimately be able to cover; thus some turns must be excluded. Step 2 begins by formulating a list of tasks which are to be included in the final system’s repertoire; turns dealing with tasks outside this repertoire are labeled out-of-domain (OOD) and excluded. This step takes an approach which is analogous to, but more simplistic th</context>
</contexts>
<marker>Jönsson, Dahlbick, 1988</marker>
<rawString>A. Jönsson and N. Dahlbick. 1988. Talking to A Computer is Not Like Talking To Your Best Friend. Proceedings of the Scandinavian Conference on ceedings of the Scandinavian Conference on Artificial Intelligence &apos;88, pp. 53-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
<author>Arne Jönsson</author>
<author>Lena Santamarta</author>
</authors>
<title>Using the process of distilling dialogues to understand dialogue systems. ICSLP</title>
<date>2000</date>
<location>Beijing.</location>
<contexts>
<context position="3594" citStr="Larsson et al., 2000" startWordPosition="560" endWordPosition="563">re and Browning, 1992. The dialog, including wizard’s interaction with back-end data sources is recorded and transcribed. 3.2 Step 2: Exclude out-of-domain turns The wizard will likely handle a broader set of requests than the system will ultimately be able to cover; thus some turns must be excluded. Step 2 begins by formulating a list of tasks which are to be included in the final system’s repertoire; turns dealing with tasks outside this repertoire are labeled out-of-domain (OOD) and excluded. This step takes an approach which is analogous to, but more simplistic than “Dialogue Distilling” (Larsson et al., 2000) which changes, adds and removes portions of turns or whole turns. Here rules simply stipulate whether to keep a whole turn. 3.3 Step 3: Enumerate action set and state space Next, the in-domain turns are annotated with dialog acts. Based on these, an action set is enumerated, and a set of state parameters and their possible values to form a vector describing the state space is determined, including: • Information state (e.g., departure-city, arrival-city) from the user and database. • The confidence/confirmation status of information state variables. • Expressed user goal and/or system goal. •</context>
</contexts>
<marker>Larsson, Jönsson, Santamarta, 2000</marker>
<rawString>Staffan Larsson, Arne Jönsson and Lena Santamarta. 2000. Using the process of distilling dialogues to understand dialogue systems. ICSLP 2000, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ester Levin</author>
<author>Roberto Pieraccini</author>
<author>Wieland Eckert</author>
</authors>
<title>A Stochastic Model of Human-Machine Interaction for Learning Dialogue Structures.</title>
<date>2000</date>
<booktitle>IEEE Trans on Speech and Audio Processing</booktitle>
<pages>8--1</pages>
<contexts>
<context position="984" citStr="Levin et al., 2000" startWordPosition="141" endWordPosition="144">trial. The state space and action set are discovered through the annotation, and an initial policy is generated using a Supervised Learning algorithm. The method is tested and shown to create an initial policy which performs significantly better and with less effort than a handcrafted policy, and can be generated using a small number of dialogs. 1 Introduction and motivation Recent work has successfully applied Reinforcement Learning (RL) to learning dialog strategy from experience, typically formulating the problem as a Markov Decision Process (MDP). (Walker et al., 1998; Singh et al., 2002; Levin et al., 2000). Despite successes, several open questions remain, especially the issue of how to create (or “bootstrap”) the initial system prior to data becoming available from on-line operation. This paper proceeds as follows. Section 2 outlines the core elements of an MDP and issues related to applying an MDP to dialog management. Sections 3 and 4 detail a method for addressing these issues, and the procedure used to test the method, respectively. Sections 5-7 present the results, a discussion, and conclusions, respectively. 2 Background An MDP is composed of a state space, an action set, and a policy wh</context>
<context position="5782" citStr="Levin et al. (2000)" startWordPosition="933" endWordPosition="936"> use a Supervised Learning algorithm to estimate a value function. 3.4 Step 4: Form an initial policy For each turn in the corpus, a vector is created representing the current dialog state plus the subsequent wizard action. Taking the action as the class variable, Supervised Learning (SL) is used to build a classifier which functions as the initial policy. Depending on the type of SL algorithm used, it may be possible to produce a prioritized list of actions rather than a single classification; in this case, this list can form an initial list of actions permitted in a given state. As noted by Levin et al. (2000), supervised learning is not appropriate for optimizing dialog strategy because of the temporal/environmental nature of dialog. Here we do not assert that the SL-learned policy will be optimal – simply that it can be easily created, that it will be significantly better than random guessing, and better and cheaper to produce than creating a cursory handcrafted strategy. 3.5 Limitations of the method This method has several obvious limitations: • Because a talking, perfect-hearing wizard is used, no/little account is taken of the recognition errors to be expected with automated speech recognitio</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>Ester Levin, Roberto Pieraccini and Wieland Eckert. 2000. A Stochastic Model of Human-Machine Interaction for Learning Dialogue Structures. IEEE Trans on Speech and Audio Processing 8(1):11-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R K Moore</author>
<author>S R Browning</author>
</authors>
<title>Results of an exercise to collect ‘genuine’ spoken enquiries using Wizard of Oz techniques.</title>
<date>1992</date>
<booktitle>Proc. of the Inst. of Acoustics.</booktitle>
<contexts>
<context position="2994" citStr="Moore and Browning, 1992" startWordPosition="462" endWordPosition="465">apping RL-based systems Here we propose a method for “bootstrapping” an MDP-based system; specifically, we address the choice of the state representation and action set, and the creation of an initial policy. 3.1 Step 1: Conduct Wizard-of-Oz dialogs The method commences with “talking wizard” interactions in which either the wizard’s voice is disguised, or a Text-to-speech engine is used. We choose human/wizard rather than human/human dialogs as people behave differently toward (what they perceive to be) machines and other people as discussed in Jönsson and Dahlbick, 1988 and also validated in Moore and Browning, 1992. The dialog, including wizard’s interaction with back-end data sources is recorded and transcribed. 3.2 Step 2: Exclude out-of-domain turns The wizard will likely handle a broader set of requests than the system will ultimately be able to cover; thus some turns must be excluded. Step 2 begins by formulating a list of tasks which are to be included in the final system’s repertoire; turns dealing with tasks outside this repertoire are labeled out-of-domain (OOD) and excluded. This step takes an approach which is analogous to, but more simplistic than “Dialogue Distilling” (Larsson et al., 2000)</context>
<context position="6727" citStr="Moore and Browning, 1992" startWordPosition="1083" endWordPosition="1086">oduce than creating a cursory handcrafted strategy. 3.5 Limitations of the method This method has several obvious limitations: • Because a talking, perfect-hearing wizard is used, no/little account is taken of the recognition errors to be expected with automated speech recognition (ASR). • Excluding too much in Step 2 may exclude actions or state components which would have ultimately produced a superior system. 4 Experimental design The proposed approach has been tested using the Autoroute corpus of 166 dialogs, in which a talking wizard answered questions about driving directions in the UK (Moore and Browning, 1992). A small set of in-domain tasks was enumerated (e.g., gathering route details, outputting summary information about a route, disambiguation of place names, etc.), and turns which did not deal with these tasks were labeled OOD and excluded. The latter included gathering the caller’s name and location (“UserID”), the most common OOD type. The corpus was annotated using an XML schema to provide the following: • 15 information components were created (e.g., from, to, time, car-type). • Each information component was given a status: C (Confirmed), U (Unconfirmed), and NULL (Not known). • Up to 5 r</context>
</contexts>
<marker>Moore, Browning, 1992</marker>
<rawString>R. K. Moore and S. R. Browning. 1992. Results of an exercise to collect ‘genuine’ spoken enquiries using Wizard of Oz techniques. Proc. of the Inst. of Acoustics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Quinlan</author>
</authors>
<date>1992</date>
<booktitle>C4.5 Release 8. (Software package). http://www.cse.unsw.edu.au/~quinlan/ Jarek</booktitle>
<note>jBNC version 1.0. (Software package). http://sourceforge.net/projects/jbnc/.</note>
<contexts>
<context position="11076" citStr="Quinlan, 1992" startWordPosition="1787" endWordPosition="1788"> in the same state; we labeled this situation a “conflict.” Table 2 shows the number of distinct states that were encountered and, for states visited more than once, whether conflicting actions were selected. Of states with conflicts, Table 3 shows probabilities estimated from the corpus. Turn Total In OOD: OOD: type domain User ID Other Wiz- 3155 2410 594 151 ard (100%) (76.4%) (18.8 %) (4.8%) Caller 2466 1713 561 192 (100%) (69.5%) (22.7 %) (7.8%) Table 1: In-domain and Out-of-domain (OOD) turns The interaction data was then submitted to 2 SL pattern classifiers – c4.5 using decision-trees (Quinlan, 1992) and jBNC using Naïve Bayesians (Sacha, 2003). Table 4 shows both algorithms’ 10- fold cross validation classification error rates classifying (1) the action type, and (2) the action type with parameters, as well as the results for the hand-crafted policy. Figure 5 show the 10-fold cross validation classification error rates for varying amounts of training data for the two SL algorithms classifying action-type and parameters. 6 Discussion The majority of the data collected was “usable”: although 26.7% of turns were excluded, 20.5% of these were due to a well-defined task not under study here (</context>
</contexts>
<marker>Quinlan, 1992</marker>
<rawString>Ross Quinlan. 1992. C4.5 Release 8. (Software package). http://www.cse.unsw.edu.au/~quinlan/ Jarek P. Sacha. 2003. jBNC version 1.0. (Software package). http://sourceforge.net/projects/jbnc/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satinder Singh</author>
<author>Diane Litman</author>
<author>Michael Kearns</author>
<author>Marilyn Walker</author>
</authors>
<title>Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>16</volume>
<pages>105--133</pages>
<contexts>
<context position="963" citStr="Singh et al., 2002" startWordPosition="137" endWordPosition="140">using a Wizard-ofOz trial. The state space and action set are discovered through the annotation, and an initial policy is generated using a Supervised Learning algorithm. The method is tested and shown to create an initial policy which performs significantly better and with less effort than a handcrafted policy, and can be generated using a small number of dialogs. 1 Introduction and motivation Recent work has successfully applied Reinforcement Learning (RL) to learning dialog strategy from experience, typically formulating the problem as a Markov Decision Process (MDP). (Walker et al., 1998; Singh et al., 2002; Levin et al., 2000). Despite successes, several open questions remain, especially the issue of how to create (or “bootstrap”) the initial system prior to data becoming available from on-line operation. This paper proceeds as follows. Section 2 outlines the core elements of an MDP and issues related to applying an MDP to dialog management. Sections 3 and 4 detail a method for addressing these issues, and the procedure used to test the method, respectively. Sections 5-7 present the results, a discussion, and conclusions, respectively. 2 Background An MDP is composed of a state space, an action</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>Satinder Singh, Diane Litman, Michael Kearns, Marilyn Walker. 2002. Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System. Journal of Artificial Intelligence Research, vol 16, 105-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Reinforcement Learning: an Introduction.</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="1724" citStr="Sutton and Barto, 1998" startWordPosition="266" endWordPosition="269">l system prior to data becoming available from on-line operation. This paper proceeds as follows. Section 2 outlines the core elements of an MDP and issues related to applying an MDP to dialog management. Sections 3 and 4 detail a method for addressing these issues, and the procedure used to test the method, respectively. Sections 5-7 present the results, a discussion, and conclusions, respectively. 2 Background An MDP is composed of a state space, an action set, and a policy which maps each state to one action. Introducing a reward function allows us to create or refine the policy using RL. (Sutton and Barto, 1998). When the MDP framework is applied to dialog management, the state space is usually constructed from vector components including information state, dialog history, recognition confidence, database status, etc. In most of the work to date both the state space and action set are hand selected, in part to ensure a limited state space, and to ensure training can proceed using a tractable number of dialogs. However, hand selection becomes impractical as system size increases, and automatic generation/selection of these elements is currently an open problem, closely related to the problem of expone</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: an Introduction. The MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Jeanne C Fromer</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email.</title>
<date>1998</date>
<booktitle>Proc. 36th Annual Meeting of the ACM and 17th Int’l Conf. on Comp. Linguistics,</booktitle>
<pages>1345--1352</pages>
<contexts>
<context position="943" citStr="Walker et al., 1998" startWordPosition="133" endWordPosition="136">based dialog manager using a Wizard-ofOz trial. The state space and action set are discovered through the annotation, and an initial policy is generated using a Supervised Learning algorithm. The method is tested and shown to create an initial policy which performs significantly better and with less effort than a handcrafted policy, and can be generated using a small number of dialogs. 1 Introduction and motivation Recent work has successfully applied Reinforcement Learning (RL) to learning dialog strategy from experience, typically formulating the problem as a Markov Decision Process (MDP). (Walker et al., 1998; Singh et al., 2002; Levin et al., 2000). Despite successes, several open questions remain, especially the issue of how to create (or “bootstrap”) the initial system prior to data becoming available from on-line operation. This paper proceeds as follows. Section 2 outlines the core elements of an MDP and issues related to applying an MDP to dialog management. Sections 3 and 4 detail a method for addressing these issues, and the procedure used to test the method, respectively. Sections 5-7 present the results, a discussion, and conclusions, respectively. 2 Background An MDP is composed of a st</context>
</contexts>
<marker>Walker, Fromer, Narayanan, 1998</marker>
<rawString>Marilyn A. Walker, Jeanne C. Fromer, Shrikanth Narayanan. 1998. Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email. Proc. 36th Annual Meeting of the ACM and 17th Int’l Conf. on Comp. Linguistics, 1345--1352.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>