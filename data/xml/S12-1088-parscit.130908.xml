<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.078006">
<title confidence="0.998436">
UNITOR: Combining Semantic Text Similarity functions
through SV Regression
</title>
<author confidence="0.998615">
Danilo Croce, Paolo Annesi, Valerio Storch and Roberto Basili
</author>
<affiliation confidence="0.9973755">
Department of Enterprise Engineering
University of Roma, Tor Vergata
</affiliation>
<address confidence="0.646453">
00133 Roma, Italy
</address>
<email confidence="0.997908">
icroce,annesi,storch,basilil@info.uniroma2.it
</email>
<sectionHeader confidence="0.993857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999797277777778">
This paper presents the UNITOR system that
participated to the SemEval 2012 Task 6: Se-
mantic Textual Similarity (STS). The task is
here modeled as a Support Vector (SV) regres-
sion problem, where a similarity scoring func-
tion between text pairs is acquired from exam-
ples. The semantic relatedness between sen-
tences is modeled in an unsupervised fashion
through different similarity functions, each
capturing a specific semantic aspect of the
STS, e.g. syntactic vs. lexical or topical vs.
paradigmatic similarity. The SV regressor ef-
fectively combines the different models, learn-
ing a scoring function that weights individual
scores in a unique resulting STS. It provides a
highly portable method as it does not depend
on any manually built resource (e.g. WordNet)
nor controlled, e.g. aligned, corpus.
</bodyText>
<sectionHeader confidence="0.998969" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999665509803922">
Semantic Textual Similarity (STS) measures the de-
gree of semantic equivalence between two phrases
or texts. An effective method to compute similar-
ity between short texts or sentences has many appli-
cations in Natural Language Processing (Mihalcea
et al., 2006) and related areas such as Information
Retrieval, e.g. to improve the effectiveness of a se-
mantic search engine (Sahami and Heilman, 2006),
or databases, where text similarity can be used in
schema matching to solve semantic heterogeneity
(Islam and Inkpen, 2008).
STS is here modeled as a Support Vector (SV) re-
gression problem, where a SV regressor learns the
similarity function over text pairs. Regression learn-
ing has been already applied to different NLP tasks.
In (Pang and Lee, 2005) it is applied to Opinion
Mining, in particular to the rating-inference prob-
lem, wherein one must determine an author evalua-
tion with respect to a multi-point scale. In (Albrecht
and Hwa, 2007) a method is proposed for develop-
ing sentence-level MT evaluation metrics using re-
gression learning without directly relying on human
reference translations. In (Biadsy et al., 2008) it has
been used to rank candidate sentences for the task
of producing biographies from Wikipedia. Finally,
in (Becker et al., 2011) SV regressor has been used
to rank questions within their context in the multi-
modal tutorial dialogue problem.
In this paper, the semantic relatedness between
two sentences is modeled as a combination of dif-
ferent similarity functions, each describing the anal-
ogy between the two texts according to a specific
semantic perspective: in this way, we aim at captur-
ing syntactic and lexical equivalences between sen-
tences and exploiting either topical relatedness or
paradigmatic similarity between individual words.
The variety of semantic evidences that a system can
employ here grows quickly, according to the genre
and complexity of the targeted sentences. We thus
propose to combine such a body of evidence to learn
a comprehensive scoring function y = f(x) over in-
dividual measures from labeled data through SV re-
gression: y is the gold similarity score (provided by
human annotators), while x is the vector of the dif-
ferent individual scores, provided by the chosen sim-
ilarity functions. The regressor objective is to learn
the proper combination of different functions redun-
dantly applied in an unsupervised fashion, without
involving any in-depth description of the target do-
main or prior knowledge. The resulting function se-
lects and filters the most useful information and it
</bodyText>
<page confidence="0.963685">
597
</page>
<note confidence="0.526865">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 597–602,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999544375">
is a highly portable method. In fact, it does not de-
pend on manually built resources (e.g. WordNet),
but mainly exploits distributional analysis of unla-
beled corpora.
In Section 2, the employed similarity functions
are described and the application of SV regression
is presented. Finally, Section 3 discusses results on
the SemEval 2012 - Task 6.
</bodyText>
<sectionHeader confidence="0.94199" genericHeader="method">
2 Combining different similarity function
through SV regression
</sectionHeader>
<bodyText confidence="0.999882">
This section describes the UNITOR systems partic-
ipating to the SemEval 2012 Task 6: in Section 2.1
the different similarity functions between sentence
pairs are discussed, while Section 2.2 describes how
the SV regression learning is applied.
</bodyText>
<subsectionHeader confidence="0.952488">
2.1 STS functions
</subsectionHeader>
<bodyText confidence="0.981394275862069">
Each STS depends on a variety of linguistic aspects
in data, e.g. syntactic or lexical information. While
their supervised combination can be derived through
SV regression, different unsupervised estimators of
STS exist.
Lexical Overlap (LO). A basic similarity function
is first employed as the lexical overlap between sen-
tences, i.e. the cardinality of the set of words occur-
ring in both sentences.
Document-oriented similarity based on Latent
Semantic Analysis (LSA). This function captures
latent semantic topics through LSA. The adjacency
terms-by-documents matrix is first acquired through
the distributional analysis of a corpus and reduced
through the application of Singular Value Decom-
position (SVD), as described in (Landauer and Du-
mais, 1997). In this work, the individual sentences
are assumed as pseudo documents and represented
by vectors in the lower dimensional LSA space. The
cosine similarity between vectors of a sentence pair
is the metric hereafter referred to as topical similar-
ity.
Compositional Distributional Semantics (CDS).
Lexical similarity can also be extended to account
for syntactic compositions between words. This
makes sentence similarity to depend on the set of in-
dividual compounds, e.g. subject-verb relationship
instances. While basic lexical information can still
be obtained by distributional analysis, phrase level
</bodyText>
<figureCaption confidence="0.99936">
Figure 1: Example of dependency graph
</figureCaption>
<bodyText confidence="0.999958487179487">
similarity can be here modeled as a specific func-
tion of the co-occurring words, i.e. a complex alge-
braic composition of their corresponding word vec-
tors. Differently from the document-oriented case
used in the LSA function, base lexical vectors are
here derived from co-occurrence counts in a word
space, built according to the method discussed in
(Sahlgren, 2006; Croce and Previtali, 2010). In or-
der to keep dimensionality as low as possible, SVD
is also applied here (Annesi et al., 2012). The result
is that every noun, verb, adjective and adverb is then
projected in the reduced word space and then dif-
ferent composition functions can be applied as dis-
cussed in (Mitchell and Lapata, 2010) or (Annesi et
al., 2012).
Convolution kernel-based similarity. The similar-
ity function is here the Smoothed Partial Tree Ker-
nel (SPTK) proposed in (Croce et al., 2011). This
convolution kernel estimates the similarity between
sentences, according to the syntactic and lexical in-
formation in both sentences. Syntactic representa-
tion of a sentence like “A man is riding a bicycle” is
derived from the dependency parse tree, as shown
in Fig. 1. It allows to define different tree struc-
tures over which the SPTK operates. First, a tree
including only lexemes, where edges encode their
dependencies, is generated and called Lexical Only
Centered Tree (LOCT), see Fig. 2. Then, we add
to each lexical node two leftmost children, encod-
ing the grammatical function and the POS-Tag re-
spectively: it is the so-called Lexical Centered Tree
(LCT), see Fig. 3. Finally, we generate the Gram-
matical Relation Centered Tree (GRCT), see Fig.
4, by setting grammatical relation as non-terminal
nodes, while PoS-Tags are pre-terminals and fathers
of their associated lexemes. Each tree representation
provides a different kernel function so that three dif-
ferent SPTK similarity scores, i.e. LOCT, LCT and
GRCT, are here obtained.
</bodyText>
<page confidence="0.978736">
598
</page>
<figure confidence="0.947181">
be::v
ride::v
bic
ycl
</figure>
<page confidence="0.99097">
599
</page>
<bodyText confidence="0.999802086956522">
is reduced to k = 250. Novel sentences are im-
mersed in the reduced space, as described in (Lan-
dauer and Dumais, 1997) and the LSA-based simi-
larity between two sentences is estimated according
the cosine similarity.
To estimate the Compositional Distributional Se-
mantics (CDS) based function, a co-occurrence
Word Space is first acquired through the distribu-
tional analysis of the UKWaC corpus (Baroni et al.,
2009), i.e. a Web document collection made of
about 2 billion tokens. UKWaC is larger than the
Europarl corpus and we expect it makes available
a more general lexical representation suited for all
datasets. An approach similar to the one described in
(Croce and Previtali, 2010) has been adopted for the
acquisition of the word space. First, all words occur-
ring more than 200 times (i.e. the targets) are rep-
resented through vectors. The original space dimen-
sions are generated from the set of the 20,000 most
frequent words (i.e. features) in the UKWaC cor-
pus. One dimension describes the Pointwise Mutual
Information score between one feature as it occurs
on a left or right window of 3 tokens around a target.
Left contexts of targets are treated differently from
the right ones, in order to also capture asymmetric
syntactic behaviors (e.g., useful for verbs): 40,000
dimensional vectors are thus derived for each target.
The particularly small window size allows to better
capture paradigmatic relations between targets, e.g.
hyponymy or synonymy. Again, the SVD reduction
is applied to the original matrix with a k = 250.
Once lexical vectors are available, a compositional
similarity measure can be obtained by combining
the word vectors according to a CDS operator, e.g.
(Mitchell and Lapata, 2010) or (Annesi et al., 2012).
In this work, the adopted compositional representa-
tion is the additive operator between lexical vectors,
as described in (Mitchell and Lapata, 2010) and the
similarity function between two sentences is the co-
sine similarity between their corresponding compo-
sitional vectors. Moreover, two additive operators
that only sum over nouns and verbs are also adopted,
denoted by CDSV and CDSN, respectively.
The estimation of the semantically Smoothed Par-
tial Tree Kernel (SPTK) is made available by an ex-
tended version of SVM-LightTK software1 (Mos-
</bodyText>
<footnote confidence="0.935321">
1http://disi.unitn.it/moschitti/Tree-Kernel.htm
</footnote>
<bodyText confidence="0.999834466666667">
chitti, 2006) implementing the smooth matching be-
tween tree nodes. The tree representation described
in Sec. 2.1 allows to define 3 different kernels, i.e.
SPTKLOCT, SPTKLCT and SPTKGRCT. Similarity
between lexical nodes is estimated as the cosine sim-
ilarity in the co-occurrence Word Space described
above, as in (Croce et al., 2011).
In all corpus analysis and experiments, sentences
are processed with the LTH dependency parser, de-
scribed in (Johansson and Nugues, 2007), for Part-
of-speech tagging and lemmatization. Dependency
parsing of datasets is required for the SPTK appli-
cation. Finally, SVM-LightTK is employed for the
SV regression learning to combine specific similar-
ity functions.
</bodyText>
<subsectionHeader confidence="0.8960625">
3.2 Evaluating the impact of unsupervised
models
</subsectionHeader>
<bodyText confidence="0.999906481481481">
Table 1 compares the Pearson Correlation of differ-
ent similarity functions described in Section 2.1, i.e.
mainly the results of the unsupervised approaches,
against the challenge training data. Regarding to
MSRvid dataset, the topical similarity (LSA func-
tion) achieves the best result, i.e. 0.748. Paradig-
matic lexical information as in CDS, CDSN and LO
provides also good results, confirming the impact of
lexical generalization. However, only nouns seem
to contribute significantly, as for the poor results of
CDSV suggest. As the dataset is characterized by
short sentences with negligible syntactic differences,
SPTK-based kernels are not discriminant. On the
contrary, the SPTKLCT achieves the best result in
the MSRpar dataset, where paraphrasing phenom-
ena are peculiar. Notice that the other SPTK kernels
are not equivalently performant, in line with previ-
ous results on question classification and semantic
role labeling (Croce et al., 2011). Lexical informa-
tion provides a crucial contribution also for LO, al-
though the contribution of topical or paradigmatic
generalization seems negligible over MSRpar. Fi-
nally, in the SMTeuroparl, longer sentences are the
norm and length seems to compromise the perfor-
mance of LO. The best results seem to require the
lexical and syntactic information provided by CDS
and SPTK.
</bodyText>
<page confidence="0.992231">
600
</page>
<table confidence="0.9996731">
Models Dataset
MSRvid MSRpar SMTeuroparl
CDS .652 .393 .681
CDSv .630 .234 .485
CDSv .219 .317 .264
LSA .748 .344 .477
SPTKLOCT .300 .251 .611
SPTKLCT .297 .464 .622
SPTKCRCT .278 .255 .626
LO .560 .446 .248
</table>
<tableCaption confidence="0.999814">
Table 1: Unsupervised results over the training dataset
</tableCaption>
<subsectionHeader confidence="0.99837">
3.3 Evaluating the role of SV regression
</subsectionHeader>
<bodyText confidence="0.999950666666667">
The SV regressors have been trained over a feature
space that enumerates the different similarity func-
tions: one feature is provided by the LSA function,
three by the CDS, i.e. CDS, CDSN and CDSV ,
three by SPTK, i.e. SPTKLOCT, SPTKLCT and
SPTKCRCT and one by LO, i.e. the number of
words in common. Two more features are obtained
by the sentence lengths of a pair, i.e. the number
of words in the first and second sentence, respec-
tively. Table 2 shows Pearson Correlation results
when the regressor is trained according a 10-fold
cross validation schema. First, all possible feature
combinations are attempted for the SV regression,
so that every subset of the 10 features is evaluated.
Results of the best feature combination are shown in
column bestfeat: for MSRvid, the best performance
is achieved when all 10 features are considered; in
MSRpar, SPTK combined with LO is sufficient; fi-
nally, in the SMTeuroparl the combination is LO,
CDS and SPTK. In column allfeat results achieved
by considering all features are reported. Last col-
umn specifies the performance increase with respect
to the corresponding best results in the unsupervised
settings.
Results of the regressors are always higher with
respect to the unsupervised settings, with up to a
35% improvement for the MSRpar, i.e. the most
complex domain. Moreover, differences when best
and all features are employed are negligible. It
means that SV regressor allows to automatically
combine and select the most informative similarity
aspects, confirming the applicability of the proposed
redundant approach to STS.
</bodyText>
<table confidence="0.999375833333333">
Dataset Gain
Experiments
bestfeat allfeat
MSRvid .789 .789 5,0%
MSRpar .615 .612 32,4%
SMTeuroparl .692 .691 1,6%
</table>
<tableCaption confidence="0.992473">
Table 2: SV regressor results over the training dataset
</tableCaption>
<subsectionHeader confidence="0.939793">
3.4 Results over the SemEval Task 6
</subsectionHeader>
<bodyText confidence="0.999965789473684">
According to the above evidence, we participated to
the SemEval challenge with three different systems.
Sys, - Best Features. Scores between pairs from a
specific dataset are obtained by applying a regressor
trained over pairs from the same dataset. It means
that, for example, the test pairs from the MSRvid
dataset are processed with a regressor trained over
the MSRvid training data. Moreover, the most rep-
resentative similarity function estimated for the col-
lection is employed: the feature combination provid-
ing the best correlation results over training pairs is
adopted for the test. The same is applied to MSRpar
and SMTeuroparl. No selection is adopted for the
Surprise data and training data for all the domains
are used, as described in Sys3.
Sys2 - All Features. Relatedness scores between
pairs from a specific dataset are obtained using a re-
gressor trained using pairs from the same dataset.
Differently from the Sys,, the similarity function
here is employed within the SV regressors trained
over all 10 similarity functions (i.e. all features).
Sys3 - All features and All domains. The SV re-
gressor is trained using training pairs from all col-
lections and over all 10 features. It means that one
single model is trained and employed to score all
test data. This approach is also used for the Surprise
data, i.e. the OnWN and SMTnews datasets.
Table 3 reports the general outcome for the UN-
ITOR systems. Rank of the individual scores with
respect to the other systems participating to the chal-
lenge is reported in parenthesis. This allows to draw
some conclusions. First, the proposed system ranks
around the 12 and 13 system positions (out of 89
systems), and the 6th group. The adoption of all pro-
posed features suggests that more evidence is better,
as it can be properly modeled by regression. It seems
generally better suited for the variety of semantic
phenomena observed in the tests. Regressors seem
</bodyText>
<page confidence="0.994833">
601
</page>
<table confidence="0.9997473">
Dataset Results
BL Sys, Sys2 Sys3
MSRvid .299 .821 .821 .802
MSRpar .433 .569 .576 .468
SMTeuroparl .454 .516 .510 .457
surp.OnWN .586 .659
surp.SMTnews .390 .471
ALL .311 .747 (13) .747 (12) .628 (40)
ALLnrm .673 .829 (12) .830 (11) .815 (21)
Mean .436 .632 (10) .632 ( 9) .594 (28)
</table>
<tableCaption confidence="0.999947">
Table 3: Results over the challenge test dataset
</tableCaption>
<bodyText confidence="0.999763909090909">
to be robust enough to select the proper features and
make the feature selection step (through collection
specific cross-validation) useless. Collection spe-
cific training seems useful, as Sys3 achieves lower
results, basically due to the significant stylistic dif-
ferences across the collections. However, the good
level of accuracy achieved over the surprise data sets
(between 11% and 17% performance gain with re-
spect to the baselines) confirms the large applica-
bility of the overall technique: our system in fact
does not depend on any manually coded resource
(e.g. WordNet) nor on any controlled (e.g. parallel
or aligned) corpus. Future work includes the study
of the learning rate and its correlation with differ-
ent and richer similarity functions, e.g. CDS as in
(Annesi et al., 2012).
Acknowledgements This research is partially
supported by the European Community’s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant numbers 262491 (INSEARCH). Many
thanks to the reviewers for their valuable sugges-
tions.
</bodyText>
<sectionHeader confidence="0.99868" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999096646153846">
Joshua Albrecht and Rebecca Hwa. 2007. Regression for
sentence-level mt evaluation with pseudo references.
In Proceedings ofACL, pages 296–303, Prague, Czech
Republic, June.
Paolo Annesi, Valerio Storch, and Roberto Basili. 2012.
Space projections as distributional models for seman-
tic composition. In CICLing (1), Lecture Notes in
Computer Science, pages 323–335. Springer.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Lee Becker, Martha Palmer, Sarel van Vuuren, and
Wayne Ward. 2011. Evaluating questions in context.
Fadi Biadsy, Julia Hirschberg, and Elena Filatova. 2008.
An unsupervised approach to biography production
using wikipedia. In ACL, pages 807–815.
Danilo Croce and Daniele Previtali. 2010. Manifold
learning for the semi-supervised induction of framenet
predicates: An empirical investigation. In Proceed-
ings of the GEMS 2010 Workshop, pages 7–16, Upp-
sala, Sweden.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Trans. Knowl. Discov. Data,
2:10:1–10:25, July.
Richard Johansson and Pierre Nugues. 2007. Semantic
structure extraction using nonprojective dependency
trees. In Proceedings of SemEval-2007, Prague, Czech
Republic, June 23-24.
P. Koehn. 2002. Europarl: A multilingual corpus for
evaluation of machine translation. Draft.
Thomas K Landauer and Susan T. Dumais. 1997. A so-
lution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, pages 211–240.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In In AAAI06.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML’06, pages 318–329.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, WWW ’06,
pages 377–386, New York, NY, USA. ACM.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199–222, August.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer–Verlag, New York.
</reference>
<page confidence="0.998178">
602
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.501573">
<title confidence="0.9942245">UNITOR: Combining Semantic Text Similarity through SV Regression</title>
<author confidence="0.996337">Danilo Croce</author>
<author confidence="0.996337">Paolo Annesi</author>
<author confidence="0.996337">Valerio Storch</author>
<author confidence="0.996337">Roberto</author>
<affiliation confidence="0.917196">Department of Enterprise University of Roma, Tor</affiliation>
<address confidence="0.999875">00133 Roma, Italy</address>
<abstract confidence="0.975060842105263">This paper presents the UNITOR system that participated to the SemEval 2012 Task 6: Semantic Textual Similarity (STS). The task is here modeled as a Support Vector (SV) regression problem, where a similarity scoring function between text pairs is acquired from examples. The semantic relatedness between sentences is modeled in an unsupervised fashion through different similarity functions, each capturing a specific semantic aspect of the STS, e.g. syntactic vs. lexical or topical vs. paradigmatic similarity. The SV regressor effectively combines the different models, learning a scoring function that weights individual scores in a unique resulting STS. It provides a highly portable method as it does not depend on any manually built resource (e.g. WordNet) nor controlled, e.g. aligned, corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joshua Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>Regression for sentence-level mt evaluation with pseudo references.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>296--303</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2039" citStr="Albrecht and Hwa, 2007" startWordPosition="308" endWordPosition="311">rove the effectiveness of a semantic search engine (Sahami and Heilman, 2006), or databases, where text similarity can be used in schema matching to solve semantic heterogeneity (Islam and Inkpen, 2008). STS is here modeled as a Support Vector (SV) regression problem, where a SV regressor learns the similarity function over text pairs. Regression learning has been already applied to different NLP tasks. In (Pang and Lee, 2005) it is applied to Opinion Mining, in particular to the rating-inference problem, wherein one must determine an author evaluation with respect to a multi-point scale. In (Albrecht and Hwa, 2007) a method is proposed for developing sentence-level MT evaluation metrics using regression learning without directly relying on human reference translations. In (Biadsy et al., 2008) it has been used to rank candidate sentences for the task of producing biographies from Wikipedia. Finally, in (Becker et al., 2011) SV regressor has been used to rank questions within their context in the multimodal tutorial dialogue problem. In this paper, the semantic relatedness between two sentences is modeled as a combination of different similarity functions, each describing the analogy between the two text</context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>Joshua Albrecht and Rebecca Hwa. 2007. Regression for sentence-level mt evaluation with pseudo references. In Proceedings ofACL, pages 296–303, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Annesi</author>
<author>Valerio Storch</author>
<author>Roberto Basili</author>
</authors>
<title>Space projections as distributional models for semantic composition.</title>
<date>2012</date>
<booktitle>In CICLing (1), Lecture Notes in Computer Science,</booktitle>
<pages>323--335</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6372" citStr="Annesi et al., 2012" startWordPosition="972" endWordPosition="975"> basic lexical information can still be obtained by distributional analysis, phrase level Figure 1: Example of dependency graph similarity can be here modeled as a specific function of the co-occurring words, i.e. a complex algebraic composition of their corresponding word vectors. Differently from the document-oriented case used in the LSA function, base lexical vectors are here derived from co-occurrence counts in a word space, built according to the method discussed in (Sahlgren, 2006; Croce and Previtali, 2010). In order to keep dimensionality as low as possible, SVD is also applied here (Annesi et al., 2012). The result is that every noun, verb, adjective and adverb is then projected in the reduced word space and then different composition functions can be applied as discussed in (Mitchell and Lapata, 2010) or (Annesi et al., 2012). Convolution kernel-based similarity. The similarity function is here the Smoothed Partial Tree Kernel (SPTK) proposed in (Croce et al., 2011). This convolution kernel estimates the similarity between sentences, according to the syntactic and lexical information in both sentences. Syntactic representation of a sentence like “A man is riding a bicycle” is derived from t</context>
<context position="9557" citStr="Annesi et al., 2012" startWordPosition="1489" endWordPosition="1492">. Left contexts of targets are treated differently from the right ones, in order to also capture asymmetric syntactic behaviors (e.g., useful for verbs): 40,000 dimensional vectors are thus derived for each target. The particularly small window size allows to better capture paradigmatic relations between targets, e.g. hyponymy or synonymy. Again, the SVD reduction is applied to the original matrix with a k = 250. Once lexical vectors are available, a compositional similarity measure can be obtained by combining the word vectors according to a CDS operator, e.g. (Mitchell and Lapata, 2010) or (Annesi et al., 2012). In this work, the adopted compositional representation is the additive operator between lexical vectors, as described in (Mitchell and Lapata, 2010) and the similarity function between two sentences is the cosine similarity between their corresponding compositional vectors. Moreover, two additive operators that only sum over nouns and verbs are also adopted, denoted by CDSV and CDSN, respectively. The estimation of the semantically Smoothed Partial Tree Kernel (SPTK) is made available by an extended version of SVM-LightTK software1 (Mos1http://disi.unitn.it/moschitti/Tree-Kernel.htm chitti, </context>
</contexts>
<marker>Annesi, Storch, Basili, 2012</marker>
<rawString>Paolo Annesi, Valerio Storch, and Roberto Basili. 2012. Space projections as distributional models for semantic composition. In CICLing (1), Lecture Notes in Computer Science, pages 323–335. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="8231" citStr="Baroni et al., 2009" startWordPosition="1272" endWordPosition="1275">r associated lexemes. Each tree representation provides a different kernel function so that three different SPTK similarity scores, i.e. LOCT, LCT and GRCT, are here obtained. 598 be::v ride::v bic ycl 599 is reduced to k = 250. Novel sentences are immersed in the reduced space, as described in (Landauer and Dumais, 1997) and the LSA-based similarity between two sentences is estimated according the cosine similarity. To estimate the Compositional Distributional Semantics (CDS) based function, a co-occurrence Word Space is first acquired through the distributional analysis of the UKWaC corpus (Baroni et al., 2009), i.e. a Web document collection made of about 2 billion tokens. UKWaC is larger than the Europarl corpus and we expect it makes available a more general lexical representation suited for all datasets. An approach similar to the one described in (Croce and Previtali, 2010) has been adopted for the acquisition of the word space. First, all words occurring more than 200 times (i.e. the targets) are represented through vectors. The original space dimensions are generated from the set of the 20,000 most frequent words (i.e. features) in the UKWaC corpus. One dimension describes the Pointwise Mutua</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Becker</author>
<author>Martha Palmer</author>
<author>Sarel van Vuuren</author>
<author>Wayne Ward</author>
</authors>
<title>Evaluating questions in context.</title>
<date>2011</date>
<marker>Becker, Palmer, van Vuuren, Ward, 2011</marker>
<rawString>Lee Becker, Martha Palmer, Sarel van Vuuren, and Wayne Ward. 2011. Evaluating questions in context.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fadi Biadsy</author>
<author>Julia Hirschberg</author>
<author>Elena Filatova</author>
</authors>
<title>An unsupervised approach to biography production using wikipedia. In</title>
<date>2008</date>
<booktitle>ACL,</booktitle>
<pages>807--815</pages>
<contexts>
<context position="2221" citStr="Biadsy et al., 2008" startWordPosition="335" endWordPosition="338">and Inkpen, 2008). STS is here modeled as a Support Vector (SV) regression problem, where a SV regressor learns the similarity function over text pairs. Regression learning has been already applied to different NLP tasks. In (Pang and Lee, 2005) it is applied to Opinion Mining, in particular to the rating-inference problem, wherein one must determine an author evaluation with respect to a multi-point scale. In (Albrecht and Hwa, 2007) a method is proposed for developing sentence-level MT evaluation metrics using regression learning without directly relying on human reference translations. In (Biadsy et al., 2008) it has been used to rank candidate sentences for the task of producing biographies from Wikipedia. Finally, in (Becker et al., 2011) SV regressor has been used to rank questions within their context in the multimodal tutorial dialogue problem. In this paper, the semantic relatedness between two sentences is modeled as a combination of different similarity functions, each describing the analogy between the two texts according to a specific semantic perspective: in this way, we aim at capturing syntactic and lexical equivalences between sentences and exploiting either topical relatedness or par</context>
</contexts>
<marker>Biadsy, Hirschberg, Filatova, 2008</marker>
<rawString>Fadi Biadsy, Julia Hirschberg, and Elena Filatova. 2008. An unsupervised approach to biography production using wikipedia. In ACL, pages 807–815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Daniele Previtali</author>
</authors>
<title>Manifold learning for the semi-supervised induction of framenet predicates: An empirical investigation.</title>
<date>2010</date>
<booktitle>In Proceedings of the GEMS 2010 Workshop,</booktitle>
<pages>7--16</pages>
<location>Uppsala,</location>
<contexts>
<context position="6272" citStr="Croce and Previtali, 2010" startWordPosition="953" endWordPosition="956">e similarity to depend on the set of individual compounds, e.g. subject-verb relationship instances. While basic lexical information can still be obtained by distributional analysis, phrase level Figure 1: Example of dependency graph similarity can be here modeled as a specific function of the co-occurring words, i.e. a complex algebraic composition of their corresponding word vectors. Differently from the document-oriented case used in the LSA function, base lexical vectors are here derived from co-occurrence counts in a word space, built according to the method discussed in (Sahlgren, 2006; Croce and Previtali, 2010). In order to keep dimensionality as low as possible, SVD is also applied here (Annesi et al., 2012). The result is that every noun, verb, adjective and adverb is then projected in the reduced word space and then different composition functions can be applied as discussed in (Mitchell and Lapata, 2010) or (Annesi et al., 2012). Convolution kernel-based similarity. The similarity function is here the Smoothed Partial Tree Kernel (SPTK) proposed in (Croce et al., 2011). This convolution kernel estimates the similarity between sentences, according to the syntactic and lexical information in both </context>
<context position="8504" citStr="Croce and Previtali, 2010" startWordPosition="1317" endWordPosition="1320">duced space, as described in (Landauer and Dumais, 1997) and the LSA-based similarity between two sentences is estimated according the cosine similarity. To estimate the Compositional Distributional Semantics (CDS) based function, a co-occurrence Word Space is first acquired through the distributional analysis of the UKWaC corpus (Baroni et al., 2009), i.e. a Web document collection made of about 2 billion tokens. UKWaC is larger than the Europarl corpus and we expect it makes available a more general lexical representation suited for all datasets. An approach similar to the one described in (Croce and Previtali, 2010) has been adopted for the acquisition of the word space. First, all words occurring more than 200 times (i.e. the targets) are represented through vectors. The original space dimensions are generated from the set of the 20,000 most frequent words (i.e. features) in the UKWaC corpus. One dimension describes the Pointwise Mutual Information score between one feature as it occurs on a left or right window of 3 tokens around a target. Left contexts of targets are treated differently from the right ones, in order to also capture asymmetric syntactic behaviors (e.g., useful for verbs): 40,000 dimens</context>
</contexts>
<marker>Croce, Previtali, 2010</marker>
<rawString>Danilo Croce and Daniele Previtali. 2010. Manifold learning for the semi-supervised induction of framenet predicates: An empirical investigation. In Proceedings of the GEMS 2010 Workshop, pages 7–16, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured lexical similarity via convolution kernels on dependency trees.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="6743" citStr="Croce et al., 2011" startWordPosition="1034" endWordPosition="1037">s are here derived from co-occurrence counts in a word space, built according to the method discussed in (Sahlgren, 2006; Croce and Previtali, 2010). In order to keep dimensionality as low as possible, SVD is also applied here (Annesi et al., 2012). The result is that every noun, verb, adjective and adverb is then projected in the reduced word space and then different composition functions can be applied as discussed in (Mitchell and Lapata, 2010) or (Annesi et al., 2012). Convolution kernel-based similarity. The similarity function is here the Smoothed Partial Tree Kernel (SPTK) proposed in (Croce et al., 2011). This convolution kernel estimates the similarity between sentences, according to the syntactic and lexical information in both sentences. Syntactic representation of a sentence like “A man is riding a bicycle” is derived from the dependency parse tree, as shown in Fig. 1. It allows to define different tree structures over which the SPTK operates. First, a tree including only lexemes, where edges encode their dependencies, is generated and called Lexical Only Centered Tree (LOCT), see Fig. 2. Then, we add to each lexical node two leftmost children, encoding the grammatical function and the PO</context>
<context position="10483" citStr="Croce et al., 2011" startWordPosition="1625" endWordPosition="1628">rs that only sum over nouns and verbs are also adopted, denoted by CDSV and CDSN, respectively. The estimation of the semantically Smoothed Partial Tree Kernel (SPTK) is made available by an extended version of SVM-LightTK software1 (Mos1http://disi.unitn.it/moschitti/Tree-Kernel.htm chitti, 2006) implementing the smooth matching between tree nodes. The tree representation described in Sec. 2.1 allows to define 3 different kernels, i.e. SPTKLOCT, SPTKLCT and SPTKGRCT. Similarity between lexical nodes is estimated as the cosine similarity in the co-occurrence Word Space described above, as in (Croce et al., 2011). In all corpus analysis and experiments, sentences are processed with the LTH dependency parser, described in (Johansson and Nugues, 2007), for Partof-speech tagging and lemmatization. Dependency parsing of datasets is required for the SPTK application. Finally, SVM-LightTK is employed for the SV regression learning to combine specific similarity functions. 3.2 Evaluating the impact of unsupervised models Table 1 compares the Pearson Correlation of different similarity functions described in Section 2.1, i.e. mainly the results of the unsupervised approaches, against the challenge training da</context>
<context position="11843" citStr="Croce et al., 2011" startWordPosition="1827" endWordPosition="1830"> as in CDS, CDSN and LO provides also good results, confirming the impact of lexical generalization. However, only nouns seem to contribute significantly, as for the poor results of CDSV suggest. As the dataset is characterized by short sentences with negligible syntactic differences, SPTK-based kernels are not discriminant. On the contrary, the SPTKLCT achieves the best result in the MSRpar dataset, where paraphrasing phenomena are peculiar. Notice that the other SPTK kernels are not equivalently performant, in line with previous results on question classification and semantic role labeling (Croce et al., 2011). Lexical information provides a crucial contribution also for LO, although the contribution of topical or paradigmatic generalization seems negligible over MSRpar. Finally, in the SMTeuroparl, longer sentences are the norm and length seems to compromise the performance of LO. The best results seem to require the lexical and syntactic information provided by CDS and SPTK. 600 Models Dataset MSRvid MSRpar SMTeuroparl CDS .652 .393 .681 CDSv .630 .234 .485 CDSv .219 .317 .264 LSA .748 .344 .477 SPTKLOCT .300 .251 .611 SPTKLCT .297 .464 .622 SPTKCRCT .278 .255 .626 LO .560 .446 .248 Table 1: Unsu</context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured lexical similarity via convolution kernels on dependency trees. In Proceedings of EMNLP, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic text similarity using corpus-based word similarity and string similarity.</title>
<date>2008</date>
<journal>ACM Trans. Knowl. Discov. Data,</journal>
<pages>2--10</pages>
<contexts>
<context position="1618" citStr="Islam and Inkpen, 2008" startWordPosition="237" endWordPosition="240">d on any manually built resource (e.g. WordNet) nor controlled, e.g. aligned, corpus. 1 Introduction Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two phrases or texts. An effective method to compute similarity between short texts or sentences has many applications in Natural Language Processing (Mihalcea et al., 2006) and related areas such as Information Retrieval, e.g. to improve the effectiveness of a semantic search engine (Sahami and Heilman, 2006), or databases, where text similarity can be used in schema matching to solve semantic heterogeneity (Islam and Inkpen, 2008). STS is here modeled as a Support Vector (SV) regression problem, where a SV regressor learns the similarity function over text pairs. Regression learning has been already applied to different NLP tasks. In (Pang and Lee, 2005) it is applied to Opinion Mining, in particular to the rating-inference problem, wherein one must determine an author evaluation with respect to a multi-point scale. In (Albrecht and Hwa, 2007) a method is proposed for developing sentence-level MT evaluation metrics using regression learning without directly relying on human reference translations. In (Biadsy et al., 20</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic text similarity using corpus-based word similarity and string similarity. ACM Trans. Knowl. Discov. Data, 2:10:1–10:25, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Semantic structure extraction using nonprojective dependency trees.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007,</booktitle>
<pages>23--24</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10622" citStr="Johansson and Nugues, 2007" startWordPosition="1646" endWordPosition="1649">Smoothed Partial Tree Kernel (SPTK) is made available by an extended version of SVM-LightTK software1 (Mos1http://disi.unitn.it/moschitti/Tree-Kernel.htm chitti, 2006) implementing the smooth matching between tree nodes. The tree representation described in Sec. 2.1 allows to define 3 different kernels, i.e. SPTKLOCT, SPTKLCT and SPTKGRCT. Similarity between lexical nodes is estimated as the cosine similarity in the co-occurrence Word Space described above, as in (Croce et al., 2011). In all corpus analysis and experiments, sentences are processed with the LTH dependency parser, described in (Johansson and Nugues, 2007), for Partof-speech tagging and lemmatization. Dependency parsing of datasets is required for the SPTK application. Finally, SVM-LightTK is employed for the SV regression learning to combine specific similarity functions. 3.2 Evaluating the impact of unsupervised models Table 1 compares the Pearson Correlation of different similarity functions described in Section 2.1, i.e. mainly the results of the unsupervised approaches, against the challenge training data. Regarding to MSRvid dataset, the topical similarity (LSA function) achieves the best result, i.e. 0.748. Paradigmatic lexical informati</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Semantic structure extraction using nonprojective dependency trees. In Proceedings of SemEval-2007, Prague, Czech Republic, June 23-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2002</date>
<publisher>Draft.</publisher>
<marker>Koehn, 2002</marker>
<rawString>P. Koehn. 2002. Europarl: A multilingual corpus for evaluation of machine translation. Draft.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review,</title>
<date>1997</date>
<pages>211--240</pages>
<contexts>
<context position="5237" citStr="Landauer and Dumais, 1997" startWordPosition="797" endWordPosition="801">nation can be derived through SV regression, different unsupervised estimators of STS exist. Lexical Overlap (LO). A basic similarity function is first employed as the lexical overlap between sentences, i.e. the cardinality of the set of words occurring in both sentences. Document-oriented similarity based on Latent Semantic Analysis (LSA). This function captures latent semantic topics through LSA. The adjacency terms-by-documents matrix is first acquired through the distributional analysis of a corpus and reduced through the application of Singular Value Decomposition (SVD), as described in (Landauer and Dumais, 1997). In this work, the individual sentences are assumed as pseudo documents and represented by vectors in the lower dimensional LSA space. The cosine similarity between vectors of a sentence pair is the metric hereafter referred to as topical similarity. Compositional Distributional Semantics (CDS). Lexical similarity can also be extended to account for syntactic compositions between words. This makes sentence similarity to depend on the set of individual compounds, e.g. subject-verb relationship instances. While basic lexical information can still be obtained by distributional analysis, phrase l</context>
<context position="7934" citStr="Landauer and Dumais, 1997" startWordPosition="1227" endWordPosition="1231">the grammatical function and the POS-Tag respectively: it is the so-called Lexical Centered Tree (LCT), see Fig. 3. Finally, we generate the Grammatical Relation Centered Tree (GRCT), see Fig. 4, by setting grammatical relation as non-terminal nodes, while PoS-Tags are pre-terminals and fathers of their associated lexemes. Each tree representation provides a different kernel function so that three different SPTK similarity scores, i.e. LOCT, LCT and GRCT, are here obtained. 598 be::v ride::v bic ycl 599 is reduced to k = 250. Novel sentences are immersed in the reduced space, as described in (Landauer and Dumais, 1997) and the LSA-based similarity between two sentences is estimated according the cosine similarity. To estimate the Compositional Distributional Semantics (CDS) based function, a co-occurrence Word Space is first acquired through the distributional analysis of the UKWaC corpus (Baroni et al., 2009), i.e. a Web document collection made of about 2 billion tokens. UKWaC is larger than the Europarl corpus and we expect it makes available a more general lexical representation suited for all datasets. An approach similar to the one described in (Croce and Previtali, 2010) has been adopted for the acqu</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T. Dumais. 1997. A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, pages 211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity. In</title>
<date>2006</date>
<booktitle>In AAAI06.</booktitle>
<contexts>
<context position="1355" citStr="Mihalcea et al., 2006" startWordPosition="196" endWordPosition="199">tactic vs. lexical or topical vs. paradigmatic similarity. The SV regressor effectively combines the different models, learning a scoring function that weights individual scores in a unique resulting STS. It provides a highly portable method as it does not depend on any manually built resource (e.g. WordNet) nor controlled, e.g. aligned, corpus. 1 Introduction Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two phrases or texts. An effective method to compute similarity between short texts or sentences has many applications in Natural Language Processing (Mihalcea et al., 2006) and related areas such as Information Retrieval, e.g. to improve the effectiveness of a semantic search engine (Sahami and Heilman, 2006), or databases, where text similarity can be used in schema matching to solve semantic heterogeneity (Islam and Inkpen, 2008). STS is here modeled as a Support Vector (SV) regression problem, where a SV regressor learns the similarity function over text pairs. Regression learning has been already applied to different NLP tasks. In (Pang and Lee, 2005) it is applied to Opinion Mining, in particular to the rating-inference problem, wherein one must determine a</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In In AAAI06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="6575" citStr="Mitchell and Lapata, 2010" startWordPosition="1007" endWordPosition="1010">ring words, i.e. a complex algebraic composition of their corresponding word vectors. Differently from the document-oriented case used in the LSA function, base lexical vectors are here derived from co-occurrence counts in a word space, built according to the method discussed in (Sahlgren, 2006; Croce and Previtali, 2010). In order to keep dimensionality as low as possible, SVD is also applied here (Annesi et al., 2012). The result is that every noun, verb, adjective and adverb is then projected in the reduced word space and then different composition functions can be applied as discussed in (Mitchell and Lapata, 2010) or (Annesi et al., 2012). Convolution kernel-based similarity. The similarity function is here the Smoothed Partial Tree Kernel (SPTK) proposed in (Croce et al., 2011). This convolution kernel estimates the similarity between sentences, according to the syntactic and lexical information in both sentences. Syntactic representation of a sentence like “A man is riding a bicycle” is derived from the dependency parse tree, as shown in Fig. 1. It allows to define different tree structures over which the SPTK operates. First, a tree including only lexemes, where edges encode their dependencies, is g</context>
<context position="9532" citStr="Mitchell and Lapata, 2010" startWordPosition="1484" endWordPosition="1487">dow of 3 tokens around a target. Left contexts of targets are treated differently from the right ones, in order to also capture asymmetric syntactic behaviors (e.g., useful for verbs): 40,000 dimensional vectors are thus derived for each target. The particularly small window size allows to better capture paradigmatic relations between targets, e.g. hyponymy or synonymy. Again, the SVD reduction is applied to the original matrix with a k = 250. Once lexical vectors are available, a compositional similarity measure can be obtained by combining the word vectors according to a CDS operator, e.g. (Mitchell and Lapata, 2010) or (Annesi et al., 2012). In this work, the adopted compositional representation is the additive operator between lexical vectors, as described in (Mitchell and Lapata, 2010) and the similarity function between two sentences is the cosine similarity between their corresponding compositional vectors. Moreover, two additive operators that only sum over nouns and verbs are also adopted, denoted by CDSV and CDSN, respectively. The estimation of the semantically Smoothed Partial Tree Kernel (SPTK) is made available by an extended version of SVM-LightTK software1 (Mos1http://disi.unitn.it/moschitti</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of ECML’06,</booktitle>
<pages>318--329</pages>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of ECML’06, pages 318–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1846" citStr="Pang and Lee, 2005" startWordPosition="276" endWordPosition="279">compute similarity between short texts or sentences has many applications in Natural Language Processing (Mihalcea et al., 2006) and related areas such as Information Retrieval, e.g. to improve the effectiveness of a semantic search engine (Sahami and Heilman, 2006), or databases, where text similarity can be used in schema matching to solve semantic heterogeneity (Islam and Inkpen, 2008). STS is here modeled as a Support Vector (SV) regression problem, where a SV regressor learns the similarity function over text pairs. Regression learning has been already applied to different NLP tasks. In (Pang and Lee, 2005) it is applied to Opinion Mining, in particular to the rating-inference problem, wherein one must determine an author evaluation with respect to a multi-point scale. In (Albrecht and Hwa, 2007) a method is proposed for developing sentence-level MT evaluation metrics using regression learning without directly relying on human reference translations. In (Biadsy et al., 2008) it has been used to rank candidate sentences for the task of producing biographies from Wikipedia. Finally, in (Becker et al., 2011) SV regressor has been used to rank questions within their context in the multimodal tutoria</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Timothy D Heilman</author>
</authors>
<title>A webbased kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web, WWW ’06,</booktitle>
<pages>377--386</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1493" citStr="Sahami and Heilman, 2006" startWordPosition="218" endWordPosition="221">ng function that weights individual scores in a unique resulting STS. It provides a highly portable method as it does not depend on any manually built resource (e.g. WordNet) nor controlled, e.g. aligned, corpus. 1 Introduction Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two phrases or texts. An effective method to compute similarity between short texts or sentences has many applications in Natural Language Processing (Mihalcea et al., 2006) and related areas such as Information Retrieval, e.g. to improve the effectiveness of a semantic search engine (Sahami and Heilman, 2006), or databases, where text similarity can be used in schema matching to solve semantic heterogeneity (Islam and Inkpen, 2008). STS is here modeled as a Support Vector (SV) regression problem, where a SV regressor learns the similarity function over text pairs. Regression learning has been already applied to different NLP tasks. In (Pang and Lee, 2005) it is applied to Opinion Mining, in particular to the rating-inference problem, wherein one must determine an author evaluation with respect to a multi-point scale. In (Albrecht and Hwa, 2007) a method is proposed for developing sentence-level MT</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Mehran Sahami and Timothy D. Heilman. 2006. A webbased kernel function for measuring the similarity of short text snippets. In Proceedings of the 15th international conference on World Wide Web, WWW ’06, pages 377–386, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University.</institution>
<contexts>
<context position="6244" citStr="Sahlgren, 2006" startWordPosition="951" endWordPosition="952">is makes sentence similarity to depend on the set of individual compounds, e.g. subject-verb relationship instances. While basic lexical information can still be obtained by distributional analysis, phrase level Figure 1: Example of dependency graph similarity can be here modeled as a specific function of the co-occurring words, i.e. a complex algebraic composition of their corresponding word vectors. Differently from the document-oriented case used in the LSA function, base lexical vectors are here derived from co-occurrence counts in a word space, built according to the method discussed in (Sahlgren, 2006; Croce and Previtali, 2010). In order to keep dimensionality as low as possible, SVD is also applied here (Annesi et al., 2012). The result is that every noun, verb, adjective and adverb is then projected in the reduced word space and then different composition functions can be applied as discussed in (Mitchell and Lapata, 2010) or (Annesi et al., 2012). Convolution kernel-based similarity. The similarity function is here the Smoothed Partial Tree Kernel (SPTK) proposed in (Croce et al., 2011). This convolution kernel estimates the similarity between sentences, according to the syntactic and </context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model. Ph.D. thesis, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>2004</date>
<journal>Statistics and Computing,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>Smola, Sch¨olkopf, 2004</marker>
<rawString>Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial on support vector regression. Statistics and Computing, 14(3):199–222, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer–Verlag,</publisher>
<location>New York.</location>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer–Verlag, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>