<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000280">
<title confidence="0.99595">
INAOE_UPV-CORE: Extracting Word Associations from
Document Corpora to estimate Semantic Textual Similarity
</title>
<author confidence="0.791158666666667">
Fernando SÃ¡nchez-Vega
Manuel Montes-y-GÃ³mez
Luis Villasefior-Pineda
</author>
<affiliation confidence="0.37957475">
Laboratorio de TecnologÃ­as del Lenguaje,
Instituto Nacional de AstrofÃ­sica, Ã“ptica y
ElectrÃ³nica (INAOE), Mexico.
{fer.callotl,mmontesg,villasen}
</affiliation>
<email confidence="0.575462">
@inaoep.mx
</email>
<sectionHeader confidence="0.994771" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999852071428571">
This paper presents three methods to evaluate
the Semantic Textual Similarity (STS). The
first two methods do not require labeled train-
ing data; instead, they automatically extract
semantic knowledge in the form of word asso-
ciations from a given reference corpus. Two
kinds of word associations are considered: co-
occurrence statistics and the similarity of
word contexts. The third method was done in
collaboration with groups from the Universi-
ties of Paris 13, Matanzas and Alicante. It
uses several word similarity measures as fea-
tures in order to construct an accurate predic-
tion model for the STS.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901888888889">
Even with the current progress of the natural lan-
guage processing, evaluating the semantic text
similarity is an extremely challenging task. Due to
the existence of multiple semantic relations among
words, the measuring of text similarity is a multi-
factorial and highly complex task (Turney, 2006).
Despite the difficulty of this task, it remains as
one of the most attractive research topics for the
NLP community. This is because the evaluation of
text similarity is commonly used as an internal
module in many different tasks, such as, informa-
tion retrieval, question answering, document sum-
marization, etc. (Resnik, 1999). Moreover, most of
these tasks require determining the â€œsemanticâ€
similarity of texts showing stylistic differences or
using polysemicwords (Hliaoutakis et al., 2006).
The most popular approach to evaluate the se-
mantic similarity of words and texts consists in
</bodyText>
<subsectionHeader confidence="0.904786">
Paolo Rosso
</subsectionHeader>
<bodyText confidence="0.998344552631579">
Natural Language Engineering Lab., ELiRF,
Universitat PolitÃ¨cnica de ValÃ¨ncia, Spain
prosso@dsic.upv.es
using the semantic knowledge expressed in ontolo-
gies (Resnik, 1999); commonly, WorldNet is used
for this purpose (Fellbaum, 2005). Unfortunately,
despite the great effort that has been the creation of
WordNet, it is still far to cover all existing words
and senses (Curran, 2003).Therefore, the semantic
similarity methods that use this resource tend to
reduce their applicability to a restricted domain
and to a specific language.
We recognize the necessity of having and using
manually-constructed semantic-knowledge sources
in order to get precise assessments of the semantic
similarity of texts, but, in turn, we also consider
that it is possible to obtain good estimations of
these similarities using less-expensive, and perhaps
broader, information sources. In particular our
proposal is to automatically extract the semantic
knowledge from large amounts of raw data sam-
ples i.e. document corpora without labels.
In this paper we describe two different strategies
to compute the semantic similarity of words from a
reference corpus. The first strategy uses word co-
occurrence statistics. It determines that two words
are associated (in meaning) if they tend to be used
together, in the same documents or contexts. The
second strategy measures the similarity of words
by taking into consideration second order word co-
occurrences. It defines two words as associated if
they are used in similar contexts (i.e., if they co-
occur with similar words). The following section
describes the implementation of these two strate-
gies for our participation at the STS-SEM 2013
task, as well as their combination with the meas-
ures designed by the groups from the Universities
of Matanzas, Alicante and Paris 13.
</bodyText>
<page confidence="0.969805">
229
</page>
<note confidence="0.436012">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 229â€“233, Atlanta, Georgia, June 13-14, 2013. cï¿½2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.785199" genericHeader="method">
2 Participation in STS-SEM2013
</sectionHeader>
<bodyText confidence="0.999683114285714">
The Semantic Textual Similarity (STS) task con-
sists of estimated the value of semantic similarity
between two texts,D1 and D2 for now on.
As we mentioned previously, our participation in
the STS task of SEM 2013 considered two differ-
ent approaches that aimed to take advantage of the
language knowledge latent in a given reference
corpus. By applying simple statistics we obtained a
semantic similarity measure between words, and
then we used this semantic word similarity (SWS)
to get a sentence level similarity estimation. We
explored two alternatives for measuring the seman-
tic similarity of words, the first one, called
SWSoï¿½ï¿½,.,. , uses the co-occurrence of words in a
limited context1,and the second, SWScontext , com-
pares the contexts of the words using the vector
model and cosine similarity to achieve this com-
parison. It is important to point out that using the
vector space model directly, without any spatial
transformation as those used by other approaches2,
we could get greater control in the selection of the
features used for the extraction of knowledge from
the corpus. It is also worth mentioning that we
applied a stemming procedure to the sentences to
be compared as well as to all documents from the
reference corpus. We represented the texts D1 and
D2 by bags of tokens, which means that our ap-
proaches did not take into account the word order.
Following we present our baseline method, then,
we introduce the two proposed methods as well as
a method done in collaboration with other groups.
The idea of this shared-method is to enhance the
estimation of the semantic textual similarity by
combining different and diverse strategies for
computing word similarities.
</bodyText>
<subsectionHeader confidence="0.72777">
2.1 STS-baseline method
</subsectionHeader>
<bodyText confidence="0.9952255">
Given textsD1 and D2, their textual similarity is
given by:
STS âˆ’ Baseline = MIN(SIM(D1, D2 ), SIM(D2, D1))
where
</bodyText>
<footnote confidence="0.972075">
1 In the experiments we considered a window (context) formed
of 15 surrounding words.
2Such as Latent Semantic Analysis (LSA) (Turney, 2005).
</footnote>
<equation confidence="0.998739">
SIM(Di,DQ = 1Y 1(tk E Dj)
|Di  |.
</equation>
<bodyText confidence="0.9994925">
This measure is based on a direct matching of to-
kens. It simply counts the number of tokens from
one text Di that also exist in the other text Dj . Be-
cause STS is a symmetrical attribute, unlike Tex-
tual Entailment (Agirre et al., 2012), we designed
it as a symmetric measure. We assumed that the
relationship between both texts is at least equal to
their smaller asymmetric similarity.
</bodyText>
<subsectionHeader confidence="0.999364">
2.2 The proposed STS methods
</subsectionHeader>
<bodyText confidence="0.999995352941176">
These methods incorporate semantic knowledge
extracted from a reference corpus. They aim to
take advantage of the latent semantic knowledge
from a large document collection. Because the
extracted knowledge from the reference corpus is
at word level, these methods for STS use the same
basic â€“word matchingâ€“ strategy for comparing the
sentences like the baseline method. Nevertheless,
they allow a soft matching between words by in-
corporating information about their semantic simi-
larity.
The following formula shows the proposed
modification to the SIM function in order to incor-
porate information of the semantic word similarity
(SWS). This modification allowed us not only to
match words with exactly the same stem but also
to link different but semantically related words.
</bodyText>
<equation confidence="0.798938333333333">
SIM(Di, Dj = I MAX U SWS(tm, tn)
tnEDj
tm EDi
</equation>
<bodyText confidence="0.99998475">
We propose two different strategies to compute
the semantic word similarity (SWS), STSoc, and
STScontex . The following subsections describe in
detail these two strategies.
</bodyText>
<subsectionHeader confidence="0.689745">
2.2.1 STS based on word co-occurrence
</subsectionHeader>
<bodyText confidence="0.999682125">
SWSoc, uses a reference corpus to get a numeri-
cal approximation of the semantic similarity be-
tween two terms tiand tj (when these terms have
not the same stem). As shown in the following
formula, SWS,,,, takes values between 0 and 1;
0 indicates that it does not exist any text sample in
the corpus that contains both terms, whereas, 1
indicates that they always occur together.
</bodyText>
<page confidence="0.769777">
230
</page>
<bodyText confidence="0.961021625">
ğ‘†ğ‘Šğ‘†ğ‘œğ‘ğ‘ğ‘¢ğ‘Ÿ ğ‘¡ğ‘–, ğ‘¡ğ‘— = ğ‘¡ğ‘– = ğ‘¡ğ‘— 1 â€¢ UMCC_DLSI (Universidad de Matanzas Cami-
#(ğ‘¡ğ‘–, ğ‘¡ğ‘—) lo Cienfuegos, Cuba, in conjuction with the
ğ‘œğ‘¡â„ğ‘’ğ‘Ÿ ğ‘€ğ¼ğ‘(#(ğ‘¡ğ‘–), #(ğ‘¡ğ‘— )) Departamento de Lenguajes y Sistemas In-
formÃ¡ticos, Universidad de Alicante, Spain).
where# ğ‘¡ğ‘–, ğ‘¡ğ‘— is the number of times that ğ‘¡ğ‘– and
ğ‘¡ğ‘— co-occur and # ğ‘¡ğ‘– and # ğ‘¡ğ‘— are the number of
times that terms ğ‘¡ğ‘– and ğ‘¡ğ‘— occur in the reference
corpus respectively.
</bodyText>
<subsubsectionHeader confidence="0.545776">
2.2.2 STS based on context similarity
</subsubsectionHeader>
<bodyText confidence="0.999836444444445">
ğ‘†ğ‘Šğ‘†ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ is based on the idea that two terms are
semantically closer if they tend to be used in simi-
lar contexts. This measure uses the well-known
vector space model and cosine similarity to com-
pare the termsâ€™ contexts. In a first step, we created
a context vector for each term, which captures all
the terms that appear around it in the whole refer-
ence corpus. Then, we computed the semantic
similarity of two terms by the following formula.
</bodyText>
<equation confidence="0.7539095">
ğ‘¡ğ‘–= ğ‘¡ğ‘— 1
ğ‘œğ‘¡â„ğ‘’ğ‘Ÿ ğ‘†ğ¼ğ‘€ğ¶ğ‘‚ğ‘† ğ‘‡ ğ‘–,ğ‘‡ ğ‘—
</equation>
<bodyText confidence="0.9999875">
where the cosine similarity, SIMCOS, is calcu-
lated on the vectors ğ‘‡ ğ‘– and ğ‘‡ ğ‘— corresponding to the
vector space model representation of terms ğ‘¡ğ‘– and
ğ‘¡ğ‘—, as indicated in the following equation:
</bodyText>
<equation confidence="0.9962975">
ğ‘†ğ¼ğ‘€ğ¶ğ‘‚ğ‘†(ğ‘‡ ğ‘–,ğ‘‡ ğ‘—) = ğ‘˜ âˆˆ |ğ‘‰ |ğ‘¡ğ‘–ğ‘˜ âˆ™ ğ‘¡ğ‘—ğ‘˜
|ğ‘‡ ğ‘– |âˆ™ |T |
</equation>
<bodyText confidence="0.999960833333333">
It is important to point out that SIMCOS is cal-
culated on a â€œpredefinedâ€ vocabulary of interest;
the appropriate selection of this vocabulary helps
to get a better representation of terms, and, conse-
quently, a more accurate estimation of their seman-
tic similarities.
</bodyText>
<subsectionHeader confidence="0.993008">
2.3 STS based on a combination of measures
</subsectionHeader>
<bodyText confidence="0.99741925">
In addition to our main methods we also developed
a method that combines our SWS measures with
measures proposed by other two research groups,
namely:
</bodyText>
<listItem confidence="0.8297935">
â€¢ LIPN (Laboratoire d&apos;Informatique de Paris-
Nord, UniversitÃ© Paris 13, France).
</listItem>
<bodyText confidence="0.998845076923077">
The main motivation for this collaboration was to
investigate the relevance of using diverse strategies
for computing word similarities and the effective-
ness of their combination for estimating the seman-
tic similarity of texts.
The proposed method used a set of measures
provided by each one of the groups. These meas-
ures were employed as features to obtained a pre-
diction model for the STS. Table 1 summarizes the
used measures. For the generation and fitting of the
model we used three approaches: linear regression,
a Gaussian process and a multilayer neural net-
work.
</bodyText>
<table confidence="0.999825222222222">
Description Team # Mean Best
Rank Rank
Based on IR measures LIPN 2 2.0 1
Based on distance on WordNet LIPN 2 8.5 2
STS-Context INA OE-1 34 4.0 4
Complexity of the sentences UPV 12 27.8 5
STS-Occur INAOE- 1 7.0 7
Based on the alignment of UPV 4 40.9 18
particulars POS. INA OE-1 1 20.0 20
n-gram overlap UPV 1 42.6 27
Based on Edit distance UMCC_ 1 29.0 29
Syntactic dependencies overlap DLSI 42.0 42
Levenshteinâ€™s distance LIPN 57.0 57
Named entity overlap UMCC_
DLSI
LIPN
LIPN
LIPN
</table>
<tableCaption confidence="0.959406">
Table 1. General description of the features used by the shared me-
thod. The second column indicates the source team for each group of
features; the third column indicates the number of used features from
each group; the last two columns show the information gain rank of
each group of features over the training set.
</tableCaption>
<sectionHeader confidence="0.987277" genericHeader="method">
3 Implementation considerations
</sectionHeader>
<bodyText confidence="0.999669333333333">
The extraction of knowledge for the computation
of the SWS was performed over the Reuters-21578
collection. This collection was selected because it
is a well-known corpus and also because it in-
cludes documents covering a wide range of topics.
Due to time and space restrictions we could not
consider all the vocabulary from the reference cor-
pus; the vocabulary selection was conducted by
taking the best 20,000 words according to the tran-
</bodyText>
<equation confidence="0.84504">
ğ‘†ğ‘Šğ‘†ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ ğ‘¡ğ‘–, ğ‘¡ğ‘— =
</equation>
<page confidence="0.981533">
231
</page>
<bodyText confidence="0.999965777777778">
sition point method (Pinto et al., 2006). This me-
thod selects the terms associated to the main topics
of the corpus, which presumably contain more
information for estimating the semantic similarity
of words. We also preserved the vocabulary from
the evaluation samples, provided they also occur in
the reference corpus. The size of the vocabulary
used in the experiments and the size of the corpus
and test set vocabularies are shown in Table 2.
</bodyText>
<table confidence="0.892821333333333">
Experimentâ€™s Selected Ref. Corpus Evaluation
Vocabulary Vocabulary Vocabulary Vocabulary
26724 20000 31213 11491
</table>
<tableCaption confidence="0.998661">
Table 2. Number of different stems from each of the
considered vocabularies
</tableCaption>
<sectionHeader confidence="0.995653" genericHeader="evaluation">
4 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.9990898">
The methods proposed by our group do not require
to be trained, i.e., they do not require tagged data,
only a reference corpus, therefore, it was possible
to evaluate them on the whole training set available
this year. Table 3 shows their results on this set.
</bodyText>
<table confidence="0.73015325">
Method Correlation
STS-Baseline 0.455
STS-Occur 0.500
STS-Contex 0.511
</table>
<tableCaption confidence="0.99611">
Table 3. Correlation values of the proposed methods and
our baseline method with human judgments.
</tableCaption>
<bodyText confidence="0.999570043478261">
Results in Table 3 show that the use of the co-
occurrence information improves the correlation
with human judgments. It also shows that the use
of context information further improves the results.
One surprising finding was the competitive per-
formance of our baseline method; it is considerably
better than the previous yearâ€™s baseline result
(0.31).
In order to evaluate the method done in collabo-
ration with LIPN and UMCC_DLSI, we carried
out several experiments using the features provided
by each group independently and in conjunction
with the others. The experiments were performed
over the whole training set by means of two-fold
cross-validation. The individual and global results
are shown in Table 4.
As shown in Table 4, the result corresponding to
the combination of all features clearly outper-
formed the results obtained by using each teamÂ´s
features independently. Moreover, the best combi-
nation of features, containing selected features
from the three teams, obtained a correlation value
very close to last year&apos;s winner result.
</bodyText>
<table confidence="0.980239866666667">
Featured by Group Perdition Model Correlation
LIPN Gaussian Process 0.587
LIPN Lineal Regression 0.701
LIPN Multilayer-NN 0.756
UMCC_DLSI Gaussian Process 0.388
UMCC_DLSI Lineal Regression 0.388
UMCC_DLSI Multilayer-NN 0.382
INAOE-UPV Gaussian Process 0.670
INAOE-UPV Lineal Regression 0.674
INAOE-UPV Multilayer-NN 0.550
ALL Gaussian Process 0.770
ALL Lineal Regression 0.777
ALL Multilayer-NN 0.633
SELECTED-SET Multilayer-NN 0.808
log-linear regression 0.823
</table>
<tableCaption confidence="0.9939225">
Table 4. Results obtained by the different subsets of
features, from the different participating groups.
</tableCaption>
<subsectionHeader confidence="0.997025">
4.1 Officials Runs
</subsectionHeader>
<bodyText confidence="0.99998848">
For the official runs (refer to Table 5) we submit-
ted the results corresponding to the STSO,,ï¿½,- and
STScontext methods. We also submitted a result
from the method done in collaboration with LIPN
and UMCC_DLSI. Due to time restrictions we
were not able to submit the results from our best
configuration; we submitted the results for the
linear regression model using all the features
(second best result from Table 4).Table 5 shows
the results in the four evaluation sub-collections;
Headlines comes from news headlines, OnWN
and FNWN contain pair senses definitions from
WordNet and other resources, finally, SMT are
translations from automatic machine translations
and from the reference human translations.
As shown in Table 5, the performances of the
two proposed methods by our group were very
close. We hypothesize that this result could be
caused by the use of a larger vocabulary for the
computation of co-occurrence statistics than for the
calculation of the context similarities. We had to
use a smaller vocabulary for the later because its
higher computational cost.
Finally, Table 5 also shows that the method
done in collaboration with the other groups ob-
</bodyText>
<figure confidence="0.495215666666667">
LAST YEARÂ´S
WINNER
Simple
</figure>
<page confidence="0.976764">
232
</page>
<bodyText confidence="0.9998944375">
tained our best results, confirming that using more
information about the semantic similarity of words
allows improving the estimation of the semantic
similarity of texts. The advantage of this approach
over the two proposed methods was especially
clear on the OnWN and FNWN datasets, which
were created upon WordNet information. Some-
how this result was predictable since several meas-
ures from this â€œshare-methodâ€ use WordNet
information to compute the semantic similarity of
words. However, this pattern was not the same for
the other two (WordNet unrelated) datasets. In
these other two collections, the average perfor-
mance of our two proposed methods, without using
any expensive and manually constructed resource,
improved by 4% the results from the share-method.
</bodyText>
<table confidence="0.990212">
Method Headlines OnWN FNWN SMT MEAN
STS-Occur 0.639 0.324 0.271 0.349 0.433
STS-Contex 0.639 0.326 0.266 0.345 0.431
Collaboration 0.646 0.629 0.409 0.304 0.508
</table>
<tableCaption confidence="0.99132">
Table 4. Correlation values from our official runs over the
four sub-datasets.
</tableCaption>
<sectionHeader confidence="0.999226" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999414615384616">
The main conclusion of this experiment is that it is
possible to extract useful knowledge from raw
corpora for evaluating the semantic similarity of
texts. Other important conclusion is that the com-
bination of methods (or word semantic similarity
measures) helps improving the accuracy of STS.
As future work we plan to carry out a detailed
analysis of the used measures, with the aim of de-
termining their complementariness and a better
way for combining them. We also plan to evaluate
the impact of the size and vocabulary richness of
the reference corpus on the accuracy of the pro-
posed STS methods.
</bodyText>
<sectionHeader confidence="0.9981" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999803142857143">
This work was done under partial support of
CONACyT project Grants: 134186, and Scholar-
ship 224483. This work is the result of the collabo-
ration in the framework of the WIQEI IRSES
project (Grant No. 269180) within the FP 7 Marie
Curie. The work of the last author was in the
framework the DIANA-APPLICATIONS-Finding
Hidden Knowledge in Texts: Applications
(TIN2012-38603-C02-01) project, and the
VLC/CAMPUS Microcluster on Multimodal Inte-
raction in Intelligent Systems. We also thank the
teams from the Universities of Paris 13, Matanzas
and Alicante for their willingness to collaborate
with us in this evalaution exercise.
</bodyText>
<sectionHeader confidence="0.998466" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887581395349">
AngelosHliaoutakis, GiannisVarelas, EpimeneidisVout-
sakis, Euripides G. M. Petrakis, EvangelosMilios,
2006, Information Retrieval by Semantic Similarity,
Intern. Journal on Semantic Web and Information
Systems: Special Issue of Multimedia Semantics
(IJSWIS), 3(3): 55â€“73.
Carmen Banea, Samer Hassan, Michael Mohler and
RadaMihalcea, 2012, UNT: A Supervised Synergistic
Approach to Semantic Text Similarity, SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics, Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation (SemEval
2012), Montreal, Vol. 2: 635-642.
Christiane Fellbaum,2005, WordNet and wordnets,
Encyclopedia of Language and Linguistics, Second
Ed., Oxford, Elsevier: 665-670.
David Pinto, Hector JimÃ©nez H. and Paolo Rosso. Clus-
tering abstracts of scientific texts using the Transi-
tion Point technique, Proc. 7th Int. Conf. on Comput.
Linguistics and Intelligent Text Processing, CICL-
ing-2006, Springer-Verlag, LNCS(3878): 536-546.
EnekoAgirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre, SemEval-2012 Task 6: A Pilot on Seman-
tic Textual Similarity. SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics, Proceedings of the Sixth International Workshop
on Semantic Evaluation (SemEval2012), Montreal,
Vol. 2: 386-393.
James Richard Curran, 2003, Doctoral Thesis: From
Distributional to Semantic Similarity, Institute for
Communicating and Collaborative Systems, School
of Informatics, University of Edinburgh.
Peter D. Turney, 2005, Measuring semantic similarity
by latent relational analysis, IJCAI&apos;05 Proceedings of
the 19th international joint conference on Artificial
intelligence, Edinburgh, Scotland: 1136-1141
Peter D. Turney, 2006, Similarity of Semantic Relations,
Computational Linguistics, Vol. 32, No. 3: 379-416.
Philip Resnik, 1999, Semantic Similarity in a Taxono-
my: An Information-Based Measure and its Applica-
tion to Problems of Ambiguity in Natural Language,
Journal of Artificial Intelligence Research, Vol. 11:
95-130.
</reference>
<page confidence="0.998956">
233
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.203647">
<title confidence="0.960104">INAOE_UPV-CORE: Extracting Word Associations Document Corpora to estimate Semantic Textual Similarity</title>
<author confidence="0.755474">Fernando Manuel</author>
<affiliation confidence="0.66693525">Luis Laboratorio de TecnologÃ­as del Instituto Nacional de AstrofÃ­sica, Ã“ptica ElectrÃ³nica (INAOE),</affiliation>
<email confidence="0.946337">@inaoep.mx</email>
<abstract confidence="0.993589666666667">This paper presents three methods to evaluate the Semantic Textual Similarity (STS). The first two methods do not require labeled training data; instead, they automatically extract semantic knowledge in the form of word associations from a given reference corpus. Two kinds of word associations are considered: cooccurrence statistics and the similarity of word contexts. The third method was done in collaboration with groups from the Universities of Paris 13, Matanzas and Alicante. It uses several word similarity measures as features in order to construct an accurate prediction model for the STS.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>GiannisVarelas AngelosHliaoutakis</author>
<author>Euripides G M Petrakis EpimeneidisVoutsakis</author>
<author>EvangelosMilios</author>
</authors>
<date>2006</date>
<booktitle>Information Retrieval by Semantic Similarity, Intern. Journal on Semantic Web and Information Systems: Special Issue of Multimedia Semantics (IJSWIS),</booktitle>
<volume>3</volume>
<issue>3</issue>
<pages>55--73</pages>
<marker>AngelosHliaoutakis, EpimeneidisVoutsakis, EvangelosMilios, 2006</marker>
<rawString>AngelosHliaoutakis, GiannisVarelas, EpimeneidisVoutsakis, Euripides G. M. Petrakis, EvangelosMilios, 2006, Information Retrieval by Semantic Similarity, Intern. Journal on Semantic Web and Information Systems: Special Issue of Multimedia Semantics (IJSWIS), 3(3): 55â€“73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Samer Hassan</author>
<author>Michael Mohler</author>
<author>RadaMihalcea</author>
</authors>
<title>UNT: A Supervised Synergistic Approach to Semantic Text Similarity,</title>
<date>2012</date>
<booktitle>SEM 2012: The First Joint Conference on Lexical and Computational Semantics, Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012),</booktitle>
<volume>2</volume>
<pages>635--642</pages>
<location>Montreal,</location>
<marker>Banea, Hassan, Mohler, RadaMihalcea, 2012</marker>
<rawString>Carmen Banea, Samer Hassan, Michael Mohler and RadaMihalcea, 2012, UNT: A Supervised Synergistic Approach to Semantic Text Similarity, SEM 2012: The First Joint Conference on Lexical and Computational Semantics, Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Vol. 2: 635-642.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet and wordnets,</title>
<booktitle>Encyclopedia of Language and Linguistics, Second Ed.,</booktitle>
<pages>665--670</pages>
<location>Oxford, Elsevier:</location>
<marker>Fellbaum, </marker>
<rawString>Christiane Fellbaum,2005, WordNet and wordnets, Encyclopedia of Language and Linguistics, Second Ed., Oxford, Elsevier: 665-670.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David Pinto</author>
<author>Hector JimÃ©nez H</author>
<author>Paolo Rosso</author>
</authors>
<title>Clustering abstracts of scientific texts using the Transition Point technique,</title>
<booktitle>Proc. 7th Int. Conf. on Comput. Linguistics and Intelligent Text Processing, CICLing-2006,</booktitle>
<volume>3878</volume>
<pages>536--546</pages>
<publisher>Springer-Verlag,</publisher>
<marker>Pinto, H, Rosso, </marker>
<rawString>David Pinto, Hector JimÃ©nez H. and Paolo Rosso. Clustering abstracts of scientific texts using the Transition Point technique, Proc. 7th Int. Conf. on Comput. Linguistics and Intelligent Text Processing, CICLing-2006, Springer-Verlag, LNCS(3878): 536-546.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel Cer EnekoAgirre</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<booktitle>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. SEM 2012: The First Joint Conference on Lexical and Computational Semantics, Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval2012),</booktitle>
<volume>2</volume>
<pages>386--393</pages>
<location>Montreal,</location>
<marker>EnekoAgirre, Diab, Gonzalez-Agirre, </marker>
<rawString>EnekoAgirre, Daniel Cer, Mona Diab and Aitor Gonzalez-Agirre, SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. SEM 2012: The First Joint Conference on Lexical and Computational Semantics, Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval2012), Montreal, Vol. 2: 386-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Richard Curran</author>
</authors>
<title>Doctoral Thesis: From Distributional to Semantic Similarity, Institute for Communicating and Collaborative Systems,</title>
<date>2003</date>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="2240" citStr="Curran, 2003" startWordPosition="325" endWordPosition="326">rmining the â€œsemanticâ€ similarity of texts showing stylistic differences or using polysemicwords (Hliaoutakis et al., 2006). The most popular approach to evaluate the semantic similarity of words and texts consists in Paolo Rosso Natural Language Engineering Lab., ELiRF, Universitat PolitÃ¨cnica de ValÃ¨ncia, Spain prosso@dsic.upv.es using the semantic knowledge expressed in ontologies (Resnik, 1999); commonly, WorldNet is used for this purpose (Fellbaum, 2005). Unfortunately, despite the great effort that has been the creation of WordNet, it is still far to cover all existing words and senses (Curran, 2003).Therefore, the semantic similarity methods that use this resource tend to reduce their applicability to a restricted domain and to a specific language. We recognize the necessity of having and using manually-constructed semantic-knowledge sources in order to get precise assessments of the semantic similarity of texts, but, in turn, we also consider that it is possible to obtain good estimations of these similarities using less-expensive, and perhaps broader, information sources. In particular our proposal is to automatically extract the semantic knowledge from large amounts of raw data sample</context>
</contexts>
<marker>Curran, 2003</marker>
<rawString>James Richard Curran, 2003, Doctoral Thesis: From Distributional to Semantic Similarity, Institute for Communicating and Collaborative Systems, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Measuring semantic similarity by latent relational analysis,</title>
<date>2005</date>
<booktitle>IJCAI&apos;05 Proceedings of the 19th international joint conference on Artificial intelligence,</booktitle>
<pages>1136--1141</pages>
<location>Edinburgh, Scotland:</location>
<contexts>
<context position="5902" citStr="Turney, 2005" startWordPosition="906" endWordPosition="907">e word order. Following we present our baseline method, then, we introduce the two proposed methods as well as a method done in collaboration with other groups. The idea of this shared-method is to enhance the estimation of the semantic textual similarity by combining different and diverse strategies for computing word similarities. 2.1 STS-baseline method Given textsD1 and D2, their textual similarity is given by: STS âˆ’ Baseline = MIN(SIM(D1, D2 ), SIM(D2, D1)) where 1 In the experiments we considered a window (context) formed of 15 surrounding words. 2Such as Latent Semantic Analysis (LSA) (Turney, 2005). SIM(Di,DQ = 1Y 1(tk E Dj) |Di |. This measure is based on a direct matching of tokens. It simply counts the number of tokens from one text Di that also exist in the other text Dj . Because STS is a symmetrical attribute, unlike Textual Entailment (Agirre et al., 2012), we designed it as a symmetric measure. We assumed that the relationship between both texts is at least equal to their smaller asymmetric similarity. 2.2 The proposed STS methods These methods incorporate semantic knowledge extracted from a reference corpus. They aim to take advantage of the latent semantic knowledge from a lar</context>
</contexts>
<marker>Turney, 2005</marker>
<rawString>Peter D. Turney, 2005, Measuring semantic similarity by latent relational analysis, IJCAI&apos;05 Proceedings of the 19th international joint conference on Artificial intelligence, Edinburgh, Scotland: 1136-1141</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<date>2006</date>
<journal>Similarity of Semantic Relations, Computational Linguistics,</journal>
<volume>32</volume>
<pages>379--416</pages>
<contexts>
<context position="1255" citStr="Turney, 2006" startWordPosition="178" endWordPosition="179">nsidered: cooccurrence statistics and the similarity of word contexts. The third method was done in collaboration with groups from the Universities of Paris 13, Matanzas and Alicante. It uses several word similarity measures as features in order to construct an accurate prediction model for the STS. 1 Introduction Even with the current progress of the natural language processing, evaluating the semantic text similarity is an extremely challenging task. Due to the existence of multiple semantic relations among words, the measuring of text similarity is a multifactorial and highly complex task (Turney, 2006). Despite the difficulty of this task, it remains as one of the most attractive research topics for the NLP community. This is because the evaluation of text similarity is commonly used as an internal module in many different tasks, such as, information retrieval, question answering, document summarization, etc. (Resnik, 1999). Moreover, most of these tasks require determining the â€œsemanticâ€ similarity of texts showing stylistic differences or using polysemicwords (Hliaoutakis et al., 2006). The most popular approach to evaluate the semantic similarity of words and texts consists in Paolo Ross</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney, 2006, Similarity of Semantic Relations, Computational Linguistics, Vol. 32, No. 3: 379-416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language,</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>11</volume>
<pages>95--130</pages>
<contexts>
<context position="1583" citStr="Resnik, 1999" startWordPosition="230" endWordPosition="231">e current progress of the natural language processing, evaluating the semantic text similarity is an extremely challenging task. Due to the existence of multiple semantic relations among words, the measuring of text similarity is a multifactorial and highly complex task (Turney, 2006). Despite the difficulty of this task, it remains as one of the most attractive research topics for the NLP community. This is because the evaluation of text similarity is commonly used as an internal module in many different tasks, such as, information retrieval, question answering, document summarization, etc. (Resnik, 1999). Moreover, most of these tasks require determining the â€œsemanticâ€ similarity of texts showing stylistic differences or using polysemicwords (Hliaoutakis et al., 2006). The most popular approach to evaluate the semantic similarity of words and texts consists in Paolo Rosso Natural Language Engineering Lab., ELiRF, Universitat PolitÃ¨cnica de ValÃ¨ncia, Spain prosso@dsic.upv.es using the semantic knowledge expressed in ontologies (Resnik, 1999); commonly, WorldNet is used for this purpose (Fellbaum, 2005). Unfortunately, despite the great effort that has been the creation of WordNet, it is still </context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik, 1999, Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language, Journal of Artificial Intelligence Research, Vol. 11: 95-130.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>