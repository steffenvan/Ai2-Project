<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004835">
<title confidence="0.9986495">
Hedge Classification with Syntactic Dependency Features based on an
Ensemble Classifier
</title>
<author confidence="0.999746">
Yi Zheng, Qifeng Dai, Qiming Luo, Enhong Chen
</author>
<affiliation confidence="0.9981275">
Department of Computer Science,
University of Science and Technology of China, Hefei, China.
</affiliation>
<email confidence="0.9883555">
{xiaoe, dqf2008}@mail.ustc.edu.cn
{luoq, cheneh}@ustc.edu.cn
</email>
<sectionHeader confidence="0.993775" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959">
We present our CoNLL-2010 Shared Task
system in the paper. The system operates in
three steps: sequence labeling, syntactic de-
pendency parsing, and classification. We have
participated in the Shared Task 1. Our experi-
mental results measured by the in-domain and
cross-domain F-scores on the biological do-
main are 81.11% and 67.99%, and on the
Wikipedia domain 55.48% and 55.41%.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996886885246">
The goals of the Shared Task (Farkas et al., 2010)
are: (1) learning to detect sentences containing
uncertainty and (2) learning to resolve the in-
sentence scope of hedge cues. We have partici-
pated in the in-domain and cross-domain chal-
lenges of Task 1. Specifically, the aim of Task 1
is to identify sentences in texts that contain unre-
liable or uncertain information, and it is formu-
lated as a binary classification problem.
Similar to Morante et al. (2009), we use the
BIO-cue labels for all tokens in a sentence to
predict whether a token is the first one of a hedge
cue (B-cue), inside a hedge cue (I-cue), or out-
side of a hedge cue (O-cue). Thus we formulate
the problem at the token level, and our task is to
label tokens in every sentence with BIO-cue. Fi-
nally, sentences that contain at least one B-cue or
I-cue are considered as uncertain.
Our system operates in three steps: sequence
labeling, syntactic dependency parsing, and clas-
sification. Sequence labeling is a preprocessing
step for splitting sentence into tokens and obtain-
ing features of tokens. Then a syntactic depend-
ency parser is applied to obtain the dependency
information of tokens. Finally, we employ an
ensemble classifier based on combining CRF
(conditional random field) and MaxEnt (maxi-
mum entropy) classifiers to label each token with
the BIO-cue.
Our experiments are conducted on two train-
ing data sets: one is the abstracts and full articles
from BioScope (biomedical domain) corpus
(Vincze et al., 2008)1, the other one is paragraphs
from Wikipedia possibly containing weasel in-
formation. Both training data sets have been an-
notated manually for hedge/weasel cues. The
annotation of weasel/hedge cues is carried out at
the phrase level. Sentences containing at least
one hedge/weasel cue are considered as uncertain,
while sentences with no hedge/weasel cues are
considered as factual. The results show that em-
ploying the ensemble classifier outperforms the
single classifier system on the Wikipedia data set,
and using the syntactic dependency information
in the feature set outperform the system without
syntactic dependency information on the biologi-
cal data set (in-domain).
In related work, Szarvas (2008) extended the
methodology of Medlock and Briscoe (2007),
and presented a hedge detection method in bio-
medical texts with a weakly supervised selection
of keywords. Ganter and Strube (2009) proposed
an approach for automatic detection of sentences
containing linguistic hedges using Wikipedia
weasel tags and syntactic patterns.
The remainder of this paper is organized as
follows. Section 2 presents the technical details
of our system. Section 3 presents experimental
results and performance analysis. Section 4 pre-
sents our discussion of the experiments. Section
5 concludes the paper and proposes future work.
</bodyText>
<sectionHeader confidence="0.973688" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9990305">
This section describes the implementation of our
system.
</bodyText>
<subsectionHeader confidence="0.945601">
2.1 Information Flow of Our System
</subsectionHeader>
<bodyText confidence="0.999318333333333">
Common classification systems consist of two
steps: feature set construction and classification.
The feature set construction process of our sys-
</bodyText>
<footnote confidence="0.988759">
1 http://www.inf.u-szeged.hu/rgai/bioscope
</footnote>
<page confidence="0.943035">
151
</page>
<bodyText confidence="0.7175786">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 151–156,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
tem consists of sequence labeling and syntactic
dependency parsing. Figure 1 shows the main
information flow of our system.
</bodyText>
<figureCaption confidence="0.998178">
Figure 1: The main information flow of our sys-
</figureCaption>
<bodyText confidence="0.868592">
tem
</bodyText>
<subsectionHeader confidence="0.999846">
2.2 Sequence labeling
</subsectionHeader>
<bodyText confidence="0.9999695">
The sequence labeling step consists of the fol-
lowing consecutive stages: (1) tokenizing, (2)
chunking, (3) POS-tagging, (4) lemmatizing.
Firstly, the PTBTokenizer2 is employed to split
sentence into tokens. Then, tokens are labeled
with BIO-tags by the OpenNLP3 chunker. Fi-
nally, Stanford Parser4 is used to obtain the POS
and lemma of tokens.
</bodyText>
<subsectionHeader confidence="0.999569">
2.3 Syntactic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999975714285714">
In the syntactic dependency parsing stage, we
use the Stanford Parser again to obtain depend-
ency information of tokens. Based on the Stan-
ford typed dependencies manual (Marneffe and
Manning 2008), we have decided to obtain the
tree dependencies structure. During the process
of parsing, we found that the parser may fail due
</bodyText>
<footnote confidence="0.988065">
2 a tokenizer from Stanford Parser.
3 http://www.opennlp.org/
4 http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<bodyText confidence="0.998315571428571">
to either empty sentences or very long sentences.
To deal with very long sentences, we decided to
allocate more memory. To deal with empty sen-
tences, we decided to simply label them as cer-
tain ones because there are only a few empty
sentences in the training and test data sets and we
could ignore their influence.
</bodyText>
<subsectionHeader confidence="0.926664">
2.4 Features
</subsectionHeader>
<bodyText confidence="0.992908625">
After sequence labeling and syntactic depend-
ency parsing, we obtain candidate features. In
our system, all the features belong to the follow-
ing five categories: (1) token features, (2) de-
pendency features, (3) neighbor features, (4) data
features, (5) bigram and trigram features.
Token features of the current token are listed
below:
</bodyText>
<listItem confidence="0.998653388888889">
• token: the current token.
• index: index of the current token in the sen-
tence
• pos: POS of the current token.
• lemma: lemma of the current token.
• chunk: BIO-chunk tags of the current token.
Dependency features of the current token are
listed below:
• parent_index: the index of the parent token
of the current token.
• parent_token: the parent token of the current
token.
• parent_lemma: the lemma of the parent token
of the current token.
• parent_pos: the POS of the parent token of
the current token.
• parent_relation: the dependency relation of
the current token and its parent token.
</listItem>
<bodyText confidence="0.9380865">
Neighbor features of the current token include
token, lemma, pos, chunk tag of three tokens to
the right and three to the left.
Data features of current token are listed below:
</bodyText>
<listItem confidence="0.9621751">
• type: indicating documentPart5 type of the
sentence which contains the current token,
such as Text, SectionTitle and so on.
• domain: distinguishing the Wikipedia and
biological domain.
• abstract_article: indicating document type of
the sentence which contains the current token,
abstract or article.
5 documentPart, SectionTitle, Text and so on are tags
in the training and test data sets.
</listItem>
<figure confidence="0.998836083333333">
CRF
Lemmatizing
POS-tagging
Syntactic
Dependency
Parsing
Tokenizing
Chunking
Merging
Start
End
MaxEnt
</figure>
<page confidence="0.979684">
152
</page>
<bodyText confidence="0.9612415">
We empirically selected some bigram features
and trigram features as listed below:
</bodyText>
<listItem confidence="0.998799428571428">
• left_token_2+left_token_1
• left_token_1+token
• token+right_token_1
• right_token_1+right_token_2
• left_token_2+left_token_1+token
• left_token_1+token+right_token_1
• token+right_token_1+right_token_2
</listItem>
<bodyText confidence="0.999732333333333">
These are the complete set of features for our
system. If the value of a feature is empty, we set
it to a default value. In the ensemble classifier,
we have selected different features for each indi-
vidual classifier. Details of this are described in
the next subsection.
</bodyText>
<subsectionHeader confidence="0.856651">
2.5 Classification
</subsectionHeader>
<bodyText confidence="0.999987761904762">
In our system, we have combined CRF++6 and
OpenNLP MaxEnt7 classifiers into an ensemble
classifier. The set of features for each classifier
are shown in the column named “system” of Ta-
ble 6. And the two classifiers are used in training
and prediction separately, based on their individ-
ual set of features. Then we merge the results in
this way: for each token, if the two predictions
for it are both O-cue, then we label the token
with an O-cue; otherwise, we label the token
with a B-cue (one of the predictions is B-cue) or
an I-cue (no B-cue in the predictions). The moti-
vation of the ensemble classifier approach is
based on the observation of our internal experi-
ments using 10-fold cross validation, which we
describe in Section 3. In addition, the parameters
of OpenNLP MaxEnt classifier are all set to de-
fault values (number of iterations is 100, cutoff is
0 and without smoothing). For CRF++, we only
set the option “-f” as 3 and the option “-c” as 1.5,
and the others are set to default values.
</bodyText>
<sectionHeader confidence="0.997845" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.9998645">
We have participated in four subtasks, biological
in-domain challenge (Bio-in-domain), biological
cross-domain challenge (Bio-cross-domain),
Wikipedia in-domain challenge (Wiki-in-
domain) and Wikipedia cross-domain challenge
(Wiki-cross-domain). In all the experiments, TP,
FP, FN and F-Score for the uncertainty class are
used as the performance measures. We have
</bodyText>
<footnote confidence="0.9896735">
6 http://crfpp.sourceforge.net/
7 http://maxent.sourceforge.net/
</footnote>
<bodyText confidence="0.999271">
tested our system with the test data set and ob-
tained official results as shown in Table 1. In
addition, we have performed several internal ex-
periments on the training data set and several
experiments on the test data set, which we de-
scribe in the next two subsections. The feature
sets used for each subtask in our system are
shown in Table 6, where each column denotes a
feature set named after the title of the column
(“System”, “dep”, ...). Actually, for different
subtasks, we make use of the same feature set
named “system”.
</bodyText>
<table confidence="0.9997146">
SubTask TP FP FN F-Score
Bio-in-domain 717 261 73 81.11
Bio-cross-domain 566 309 224 67.99
Wiki-in-domain 974 303 1260 55.48
Wiki-cross-domain 991 352 1243 55.41
</table>
<tableCaption confidence="0.999003">
Table 1: Official results of our system.
</tableCaption>
<subsectionHeader confidence="0.963873">
3.1 Internal Experiments
</subsectionHeader>
<bodyText confidence="0.9999764">
Initially we only used a single classifier instead
of an ensemble classifier. We performed 10-fold
cross validation experiments on the training data
set at the sentence level with different feature
sets. The results of these experiments are shown
in Table 2 and Table 3.
In internal experiments, we mainly focus on
the results of different models and different fea-
ture sets. In Table 2 and Table 3, CRF and ME
(MaxEnt) indicate the two classifiers; ENSMB
stands for the ensemble classifier obtained by
combining CRF and MaxEnt classifiers; the three
words “dep”, “neighbor” and “together” indicate
the feature sets for different experiments shown
in Table 6, and “together” is the union set of
“dep” and “neighbor”.
The results of ME and CRF experiments (third
column of Table 2 and Table 3) show that the
individual classifier wrongly predicts many un-
certain sentences as certain ones. The number of
such errors is much greater than the number of
errors of predicting certain ones as uncertain. In
other words, FN is greater than FP in our ex-
periments and the recall ratio is very low, espe-
cially for the Wikipedia data set.
</bodyText>
<page confidence="0.997922">
153
</page>
<table confidence="0.999944090909091">
Experiment Biological in-domain Biological cross-domain
TP FP FN F-Score TP FP FN F-Score
ME-dep 244 28 34 88.73 220 24 58 84.29
CRF-dep 244 20 34 90.04 230 19 48 87.29
ENSMB-dep 248 32 30 88.89 235 28 43 86.88
ME-neighbor 229 14 49 87.91 211 12 67 84.23
CRF-neighbor 244 16 34 90.71 228 21 50 86.53
ENSMB-neighbor 247 22 31 90.31 241 26 37 88.44
ME-together 234 11 44 89.48 205 12 73 82.83
CRF-together 247 13 31 91.82 234 21 44 87.80
ENSMB-together 253 17 25 92.36 242 26 36 88.64
</table>
<tableCaption confidence="0.991294">
Table 2: Results of internal experiments on the biological training data set.
</tableCaption>
<table confidence="0.999953636363636">
Experiment Wikipedia in-domain Wikipedia cross-domain
TP FP FN F-Score TP FP FN F-Score
ME-dep 131 91 117 55.74 145 108 103 57.88
CRF-dep 108 51 140 53.07 115 60 133 54.37
ENSMB-dep 148 103 100 59.32 153 119 95 58.85
ME-neighbor 106 52 142 52.22 130 77 118 57.14
CRF-neighbor 123 44 125 59.28 123 72 125 55.53
ENSMB-neighbor 145 71 103 62.50 154 116 94 59.46
ME-together 100 57 148 49.38 117 69 131 53.92
CRF-together 125 54 123 58.55 127 67 121 57.47
ENSMB-together 141 83 107 59.75 146 104 102 58.63
</table>
<tableCaption confidence="0.997617">
Table 3: Results of internal experiments on the Wikipedia training data set.
</tableCaption>
<table confidence="0.999903">
Experiment Biological in-domain Biological cross-domain
TP FP FN F-Score TP FP FN F-Score
System-ME 650 159 140 81.30 518 265 272 65.86
System-CRF 700 197 90 82.99 464 97 326 68.69
System-ENSMB 717 261 73 81.11 566 309 224 67.99
</table>
<tableCaption confidence="0.998266">
Table 4: Results of additional experiment of biological test data set.
</tableCaption>
<table confidence="0.9999028">
Experiment Wikipedia in-domain Wikipedia cross-domain
TP FP FN F-Score TP FP FN F-Score
System-ME 794 235 1440 48.67 798 284 1436 48.13
System-CRF 721 112 1513 47.02 747 153 1487 47.67
System-ENSMB 974 303 1260 55.48 991 352 1243 55.41
</table>
<tableCaption confidence="0.999779">
Table 5: Results of additional experiment of Wikipedia test data set.
</tableCaption>
<bodyText confidence="0.999638173913044">
Based on this analysis, we propose an ensem-
ble classifier approach to decrease FN in order to
improve the recall ratio. The results of the en-
semble classifier show that: along with the de-
creasing of FN, FP and TP are both increasing.
Although the recall ratio increases, the precision
ratio decreases at the same time. Therefore, the
ensemble classifier approach is a trade-off be-
tween precision and recall. For data sets with low
recall ratio, such as Wikipedia, the ensemble
classifier outperforms each single classifier in
terms of F-score, just as the ME, CRF and
ENSMB experiments show in Table 2 and Table
3.
In addition, we have performed simple feature
selection in the internal experiments. The com-
parison of “dep”, “neighbor” and “together” ex-
periments shown in Table 2 demonstrates that
the dependency and neighbor features are both
beneficial only for the biological in-domain ex-
periment. This may be because that sentences of
the biological data are more regular than those of
the Wikipedia data.
</bodyText>
<subsectionHeader confidence="0.999759">
3.2 Additional experiments on test data set
</subsectionHeader>
<bodyText confidence="0.991953">
We have also performed experiments on the test
data set, and the results are shown in Table 4 and
Table 5. With the same set of features of our sys-
</bodyText>
<page confidence="0.999492">
154
</page>
<bodyText confidence="0.997775588235294">
tem as shown in Table 6, we have performed
three experiments: System-ME (ME denotes
MaxEnt classifier), System-CRF (CRF denotes
CRF classifier) and System-ENSMB (ENSMB
denotes ensemble classifier), where “System”
denotes the feature set in Table 6. The meanings
of these words are similar to internal experiments.
As Table 4 and Table 5 show, for the Wikipe-
dia test data set, the ensemble classifier outper-
forms each single classifier in terms of F-score
by improving the recall ratio with a larger extent
than the extent of the decreasing of the precision
ratio. For the biological test data set, the ensem-
ble classifier outperforms System-ME but under-
performs System-CRF. This may be due to the
relatively high values of the precision and recall
ratios already obtained by each single classifier.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999975896551724">
The features in our experiments are selected em-
pirically, and the performance of our system
could be improved with more elaborate feature
selection. From the experimental results, we ob-
serve that there are still many uncertain sen-
tences predicted as certain ones. This indicates
that the ability of learning uncertain information
with the current classifiers and feature sets needs
to be improved. We had the plan of exploring the
ensemble classifier by combining CRF, MaxEnt
and SVM (Support Vector Machine), but it was
given up due to limited time. In addition, we
were not able to complete experiments with
MaxEnt classifier based on bigram and trigram
features due to limited time. Actually only two
labels I and O are needed for Task 1. We have
not done the experiments with only I and O la-
bels, and we plan to do it in the future.
According to our observation, the low F-score
on the Wikipedia data set is due to many uncer-
tain phrases. By contrast, for the biological data
set, the uncertain information consists of mostly
single words rather than phrases. It is difficult for
a classifier to learn uncertain information con-
sisting of 3 words or more. As we have observed,
these uncertain phrases follow several patterns.
A hybrid approach based on rule-based and sta-
tistical approaches to recognize them seems to be
a promising.
</bodyText>
<sectionHeader confidence="0.994841" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999593266666667">
Our CoNLL-2010 Shared Task system operates
in three steps: sequence labeling, syntactic de-
pendency parsing, and classification. The results
show that employing the ensemble classifier out-
performs each single classifier for the Wikipedia
data set, and using the syntactic dependency in-
formation in the feature set outperform the sys-
tem without syntactic dependency information
for the biological data set (in-domain). Our final
system achieves promising results. Due to lim-
ited time, we have only performed simple feature
selection empirically. In the future, we plan to
explore more elaborate feature selection and ex-
plore ensemble classifier by combining more
classifiers.
</bodyText>
<sectionHeader confidence="0.997545" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998740857142857">
The work was supported by the National Natural
Science Foundation of China (No.60775037), the
National High Technology Research and Devel-
opment Program of China (863 Program,
No.2009 AA01Z123), and Specialized Research
Fund for the Doctoral Program of Higher Educa-
tion (No.20093402110017).
</bodyText>
<sectionHeader confidence="0.998849" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998793482758621">
Richárd Farkas, Veronika Vincze, György Móra,
János Csirik, and György Szarvas. 2010. The
CoNLL-2010 Shared Task: Learning to Detect
Hedges and their Scope in Natural Language Text.
In Proceedings of CoNLL-2010: Shared Task,
pages 1–12.
Viola Ganter, and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 173–176.
Marie-Catherine de Marneffe, and Christopher D.
Manning. 2008. Stanford typed dependencies
manual, September 2008.
Ben Medlock, and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proc. of ACL 2007, pages 992–999.
Roser Morante, and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proc. of the BioNLP 2009 Workshop, pages
28–36.
György Szarvas. 2008. Hedge classification in bio-
medical texts with a weakly supervised selection of
keywords. In Proc. of ACL 2008, pages 281–289.
Veronika Vincze, György Szarvas, Richárd Farkas,
György Móra, and János Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for un-
certainty, negation and their scopes. BMC Bioin-
formatics, 9(Suppl 11):S9.
</reference>
<page confidence="0.998781">
155
</page>
<figureCaption confidence="0.93809">
Feature System Dep Neighbor Together
token mc mc mc mc
index m m m m
pos mc mc mc mc
lemma mc mc mc mc
chunk mc mc mc
parent_index mc mc mc
parent_token mc mc
parent_lemma mc mc mc
parent_relation mc mc mc
parent_pos mc mc mc
</figureCaption>
<table confidence="0.835141117647059">
left_token_1 c c c
left_lemma_1 mc mc mc
left_pos_1 mc mc mc
left_chunk_1 mc mc
left_token_2 c c c
left_lemma_2 c mc mc
left_pos_2 mc mc mc
left_chunk_2 mc mc
left_token_3
left_lemma_3 mc m m
left_pos_3 mc m m
left_chunk_3 m m
right_token_1 c c c
right_lemma_1 mc mc mc
right _pos_1 mc mc mc
right _chunk_1 mc mc
right_token_2 c c c
right _lemma_2 mc mc mc
right _pos_2 c mc mc
right _chunk_2 mc mc
right_token_3
right _lemma_3 c m m
right _pos_3 mc m m
right _chunk_3 m m
type m mc mc mc
domain m mc mc mc
abstract_article m mc mc mc
left_token_2+left_token_1 c c c
left_token_1+token c c c
token+right_token_1 c c c
right_token_1+right_token_2 c c c
left_token_2+left_token_1+token c c c
left_token_1+token+right_token_1 c c c
token+right_token_1+right_token_2 c c c
</table>
<tableCaption confidence="0.6772005">
Table 6: Features selected for different experiments. The symbol m indicates MaxEnt classifier and c indicates
CRF classifier.
</tableCaption>
<page confidence="0.99646">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.564274">
<title confidence="0.9978745">Hedge Classification with Syntactic Dependency Features based on Ensemble Classifier</title>
<author confidence="0.997666">Yi Zheng</author>
<author confidence="0.997666">Qifeng Dai</author>
<author confidence="0.997666">Qiming Luo</author>
<author confidence="0.997666">Enhong</author>
<affiliation confidence="0.9521955">Department of Computer Science, University of Science and Technology of China, Hefei, China.</affiliation>
<email confidence="0.875212">xiaoe@ustc.edu.cn</email>
<email confidence="0.875212">{luoq@ustc.edu.cn</email>
<email confidence="0.875212">cheneh@ustc.edu.cn</email>
<abstract confidence="0.9775779">We present our CoNLL-2010 Shared Task system in the paper. The system operates in three steps: sequence labeling, syntactic dependency parsing, and classification. We have participated in the Shared Task 1. Our experimental results measured by the in-domain and cross-domain F-scores on the biological domain are 81.11% and 67.99%, and on the Wikipedia domain 55.48% and 55.41%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richárd Farkas</author>
<author>Veronika Vincze</author>
<author>György Móra</author>
<author>János Csirik</author>
<author>György Szarvas</author>
</authors>
<title>The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL-2010: Shared Task,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="741" citStr="Farkas et al., 2010" startWordPosition="105" endWordPosition="108"> Enhong Chen Department of Computer Science, University of Science and Technology of China, Hefei, China. {xiaoe, dqf2008}@mail.ustc.edu.cn {luoq, cheneh}@ustc.edu.cn Abstract We present our CoNLL-2010 Shared Task system in the paper. The system operates in three steps: sequence labeling, syntactic dependency parsing, and classification. We have participated in the Shared Task 1. Our experimental results measured by the in-domain and cross-domain F-scores on the biological domain are 81.11% and 67.99%, and on the Wikipedia domain 55.48% and 55.41%. 1 Introduction The goals of the Shared Task (Farkas et al., 2010) are: (1) learning to detect sentences containing uncertainty and (2) learning to resolve the insentence scope of hedge cues. We have participated in the in-domain and cross-domain challenges of Task 1. Specifically, the aim of Task 1 is to identify sentences in texts that contain unreliable or uncertain information, and it is formulated as a binary classification problem. Similar to Morante et al. (2009), we use the BIO-cue labels for all tokens in a sentence to predict whether a token is the first one of a hedge cue (B-cue), inside a hedge cue (I-cue), or outside of a hedge cue (O-cue). Thus</context>
</contexts>
<marker>Farkas, Vincze, Móra, Csirik, Szarvas, 2010</marker>
<rawString>Richárd Farkas, Veronika Vincze, György Móra, János Csirik, and György Szarvas. 2010. The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of CoNLL-2010: Shared Task, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viola Ganter</author>
<author>Michael Strube</author>
</authors>
<title>Finding hedges by chasing weasels: Hedge detection using Wikipedia tags and shallow linguistic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>173--176</pages>
<contexts>
<context position="3074" citStr="Ganter and Strube (2009)" startWordPosition="480" endWordPosition="483">ge/weasel cue are considered as uncertain, while sentences with no hedge/weasel cues are considered as factual. The results show that employing the ensemble classifier outperforms the single classifier system on the Wikipedia data set, and using the syntactic dependency information in the feature set outperform the system without syntactic dependency information on the biological data set (in-domain). In related work, Szarvas (2008) extended the methodology of Medlock and Briscoe (2007), and presented a hedge detection method in biomedical texts with a weakly supervised selection of keywords. Ganter and Strube (2009) proposed an approach for automatic detection of sentences containing linguistic hedges using Wikipedia weasel tags and syntactic patterns. The remainder of this paper is organized as follows. Section 2 presents the technical details of our system. Section 3 presents experimental results and performance analysis. Section 4 presents our discussion of the experiments. Section 5 concludes the paper and proposes future work. 2 System Description This section describes the implementation of our system. 2.1 Information Flow of Our System Common classification systems consist of two steps: feature se</context>
</contexts>
<marker>Ganter, Strube, 2009</marker>
<rawString>Viola Ganter, and Michael Strube. 2009. Finding hedges by chasing weasels: Hedge detection using Wikipedia tags and shallow linguistic features. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2008</date>
<note>Stanford typed dependencies manual,</note>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe, and Christopher D. Manning. 2008. Stanford typed dependencies manual, September 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>992--999</pages>
<contexts>
<context position="2941" citStr="Medlock and Briscoe (2007)" startWordPosition="459" endWordPosition="462">ly for hedge/weasel cues. The annotation of weasel/hedge cues is carried out at the phrase level. Sentences containing at least one hedge/weasel cue are considered as uncertain, while sentences with no hedge/weasel cues are considered as factual. The results show that employing the ensemble classifier outperforms the single classifier system on the Wikipedia data set, and using the syntactic dependency information in the feature set outperform the system without syntactic dependency information on the biological data set (in-domain). In related work, Szarvas (2008) extended the methodology of Medlock and Briscoe (2007), and presented a hedge detection method in biomedical texts with a weakly supervised selection of keywords. Ganter and Strube (2009) proposed an approach for automatic detection of sentences containing linguistic hedges using Wikipedia weasel tags and syntactic patterns. The remainder of this paper is organized as follows. Section 2 presents the technical details of our system. Section 3 presents experimental results and performance analysis. Section 4 presents our discussion of the experiments. Section 5 concludes the paper and proposes future work. 2 System Description This section describe</context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock, and Ted Briscoe. 2007. Weakly supervised learning for hedge classification in scientific literature. In Proc. of ACL 2007, pages 992–999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the scope of hedge cues in biomedical texts.</title>
<date>2009</date>
<booktitle>In Proc. of the BioNLP 2009 Workshop,</booktitle>
<pages>28--36</pages>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante, and Walter Daelemans. 2009. Learning the scope of hedge cues in biomedical texts. In Proc. of the BioNLP 2009 Workshop, pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>György Szarvas</author>
</authors>
<title>Hedge classification in biomedical texts with a weakly supervised selection of keywords.</title>
<date>2008</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>281--289</pages>
<contexts>
<context position="2886" citStr="Szarvas (2008)" startWordPosition="453" endWordPosition="454">aining data sets have been annotated manually for hedge/weasel cues. The annotation of weasel/hedge cues is carried out at the phrase level. Sentences containing at least one hedge/weasel cue are considered as uncertain, while sentences with no hedge/weasel cues are considered as factual. The results show that employing the ensemble classifier outperforms the single classifier system on the Wikipedia data set, and using the syntactic dependency information in the feature set outperform the system without syntactic dependency information on the biological data set (in-domain). In related work, Szarvas (2008) extended the methodology of Medlock and Briscoe (2007), and presented a hedge detection method in biomedical texts with a weakly supervised selection of keywords. Ganter and Strube (2009) proposed an approach for automatic detection of sentences containing linguistic hedges using Wikipedia weasel tags and syntactic patterns. The remainder of this paper is organized as follows. Section 2 presents the technical details of our system. Section 3 presents experimental results and performance analysis. Section 4 presents our discussion of the experiments. Section 5 concludes the paper and proposes </context>
</contexts>
<marker>Szarvas, 2008</marker>
<rawString>György Szarvas. 2008. Hedge classification in biomedical texts with a weakly supervised selection of keywords. In Proc. of ACL 2008, pages 281–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>György Szarvas</author>
<author>Richárd Farkas</author>
<author>György Móra</author>
<author>János Csirik</author>
</authors>
<title>The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<pages>11--9</pages>
<contexts>
<context position="2179" citStr="Vincze et al., 2008" startWordPosition="346" endWordPosition="349"> in three steps: sequence labeling, syntactic dependency parsing, and classification. Sequence labeling is a preprocessing step for splitting sentence into tokens and obtaining features of tokens. Then a syntactic dependency parser is applied to obtain the dependency information of tokens. Finally, we employ an ensemble classifier based on combining CRF (conditional random field) and MaxEnt (maximum entropy) classifiers to label each token with the BIO-cue. Our experiments are conducted on two training data sets: one is the abstracts and full articles from BioScope (biomedical domain) corpus (Vincze et al., 2008)1, the other one is paragraphs from Wikipedia possibly containing weasel information. Both training data sets have been annotated manually for hedge/weasel cues. The annotation of weasel/hedge cues is carried out at the phrase level. Sentences containing at least one hedge/weasel cue are considered as uncertain, while sentences with no hedge/weasel cues are considered as factual. The results show that employing the ensemble classifier outperforms the single classifier system on the Wikipedia data set, and using the syntactic dependency information in the feature set outperform the system witho</context>
</contexts>
<marker>Vincze, Szarvas, Farkas, Móra, Csirik, 2008</marker>
<rawString>Veronika Vincze, György Szarvas, Richárd Farkas, György Móra, and János Csirik. 2008. The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC Bioinformatics, 9(Suppl 11):S9.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>