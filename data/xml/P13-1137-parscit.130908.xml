<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9983795">
Domain-Independent Abstract Generation
for Focused Meeting Summarization
</title>
<author confidence="0.999255">
Lu Wang
</author>
<affiliation confidence="0.9963275">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.643841">
Ithaca, NY 14853
</address>
<email confidence="0.998526">
luwang@cs.cornell.edu
</email>
<sectionHeader confidence="0.993883" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999270733333333">
We address the challenge of generating natu-
ral language abstractive summaries for spoken
meetings in a domain-independent fashion.
We apply Multiple-Sequence Alignment to in-
duce abstract generation templates that can be
used for different domains. An Overgenerate-
and-Rank strategy is utilized to produce and
rank candidate abstracts. Experiments us-
ing in-domain and out-of-domain training on
disparate corpora show that our system uni-
formly outperforms state-of-the-art supervised
extract-based approaches. In addition, human
judges rate our system summaries significantly
higher than compared systems in fluency and
overall quality.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894083333334">
Meetings are a common way to collaborate,
share information and exchange opinions. Con-
sequently, automatically generated meeting sum-
maries could be of great value to people and busi-
nesses alike by providing quick access to the es-
sential content of past meetings. Focused meet-
ing summaries have been proposed as particularly
useful; in contrast to summaries of a meeting as
a whole, they refer to summaries of a specific as-
pect of a meeting, such as the DECISIONS reached,
PROBLEMS discussed, PROGRESS made or AC-
TION ITEMS that emerged (Carenini et al., 2011).
Our goal is to provide an automatic summariza-
tion system that can generate abstract-style fo-
cused meeting summaries to help users digest the
vast amount of meeting content in an easy manner.
Existing meeting summarization systems re-
main largely extractive: their summaries are com-
prised exclusively of patchworks of utterances se-
lected directly from the meetings to be summa-
rized (Riedhammer et al., 2010; Bui et al., 2009;
Xie et al., 2008). Although relatively easy to con-
struct, extractive approaches fall short of produc-
ing concise and readable summaries, largely due
</bodyText>
<figure confidence="0.312649833333333">
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
C: Looking at what we’ve got, we we want an LCD dis-
play with a spinning wheel.
B: You have to have some push-buttons, don’t you?
C: Just spinning and not scrolling, I would say.
B: I think the spinning wheel is definitely very now.
A: but since LCDs seems to be uh a definite yes,
C: We’re having push-buttons on the outside
</figure>
<note confidence="0.550615">
C: and then on the inside an LCD with spinning wheel,
</note>
<sectionHeader confidence="0.574619" genericHeader="introduction">
Decision Abstract (Summary):
</sectionHeader>
<bodyText confidence="0.77233">
The remote will have push buttons outside, and an LCD
and spinning wheel inside.
A: and um I’m not sure about the buttons being in the
shape of fruit though.
D: Maybe make it like fruity colours or something.
C: The power button could be like a big apple or some-
thing.
D: Um like I’m just thinking bright colours.
</bodyText>
<subsectionHeader confidence="0.910433">
Problem Abstract (Summary):
</subsectionHeader>
<bodyText confidence="0.919118">
How to incorporate a fruit and vegetable theme into the
remote.
</bodyText>
<figureCaption confidence="0.99555425">
Figure 1: Clips from the AMI meeting corpus (Mc-
cowan et al., 2005). A, B, C and D refer to distinct
speakers. Also shown is the gold-standard (manual)
abstract (summary) for the decision and the problem.
</figureCaption>
<bodyText confidence="0.994299055555556">
to the noisy, fragmented, ungrammatical and un-
structured text of meeting transcripts (Murray et
al., 2010b; Liu and Liu, 2009).
In contrast, human-written meeting summaries
are typically in the form of abstracts — distilla-
tions of the original conversation written in new
language. A user study from Murray et al. (2010b)
showed that people demonstrate a strong prefer-
ence for abstractive summaries over extracts when
the text to be summarized is conversational. Con-
sider, for example, the two types of focused sum-
mary along with their associated dialogue snippets
in Figure 1. We can see that extracts are likely to
include unnecessary and noisy information from
the meeting transcripts. On the contrary, the man-
ually composed summaries (abstracts) are more
compact and readable, and are written in a dis-
tinctly non-conversational style.
</bodyText>
<page confidence="0.931754">
1395
</page>
<note confidence="0.913121">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1395–1405,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.953671205479452">
To address the limitations of extract-based sum-
maries, we propose a complete and fully automatic
domain-independent abstract generation frame-
work for focused meeting summarization. Fol-
lowing existing language generation research (An-
geli et al., 2010; Konstas and Lapata, 2012), we
first perform content selection: given the dia-
logue acts relevant to one element of the meet-
ing (e.g. a single decision or problem), we train
a classifier to identify summary-worthy phrases.
Next, we develop an “overgenerate-and-rank”
strategy (Walker et al., 2001; Heilman and Smith,
2010) for surface realization, which generates and
ranks candidate sentences for the abstract. Af-
ter redundancy reduction, the full meeting abstract
can thus comprise the focused summary for each
meeting element. As described in subsequent sec-
tions, the generation framework allows us to iden-
tify and reformulate the important information for
the focused summary. Our contributions are as fol-
lows:
• To the best of our knowledge, our system is
the first fully automatic system to generate
natural language abstracts for spoken meet-
ings.
• We present a novel template extraction al-
gorithm, based on Multiple Sequence Align-
ment (MSA) (Durbin et al., 1998), to induce
domain-independent templates that guide ab-
stract generation. MSA is commonly used
in bioinformatics to identify equivalent frag-
ments of DNAs (Durbin et al., 1998) and
has also been employed for learning para-
phrases (Barzilay and Lee, 2003).
• Although our framework requires labeled
training data for each type of focused sum-
mary (decisions, problems, etc.), we also
make initial tries for domain adaptation so
that our summarization method does not need
human-written abstracts for each new meet-
ing domain (e.g. faculty meetings, theater
group meetings, project group meetings).
We instantiate the abstract generation frame-
work on two corpora from disparate domains
— the AMI Meeting Corpus (Mccowan et al.,
2005) and ICSI Meeting Corpus (Janin et al.,
2003) — and produce systems to generate fo-
cused summaries with regard to four types of
meeting elements: DECISIONs, PROBLEMs, AC-
TION ITEMSs, and PROGRESS. Automatic eval-
uation (using ROUGE (Lin and Hovy, 2003) and
BLEU (Papineni et al., 2002)) against manually
generated focused summaries shows that our sum-
marizers uniformly and statistically significantly
outperform two baseline systems as well as a
state-of-the-art supervised extraction-based sys-
tem. Human evaluation also indicates that the
abstractive summaries produced by our systems
are more linguistically appealing than those of
the utterance-level extraction-based system, pre-
ferring them over summaries from the extraction-
based system of comparable semantic correctness
(62.3% vs. 37.7%).
Finally, we examine the generality of our model
across domains for two types of focused summa-
rization — decisions and problems — by train-
ing the summarizer on out-of-domain data (i.e. the
AMI corpus for use on the ICSI meeting data,
and vice versa). The resulting systems yield re-
sults comparable to those from the same system
trained on in-domain data, and statistically signif-
icantly outperform supervised extractive summa-
rization approaches trained on in-domain data.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999808458333333">
Most research on spoken dialogue summariza-
tion attempts to generate summaries for full dia-
logues (Carenini et al., 2011). Only recently has
the task of focused summarization been studied.
Supervised methods are investigated to identify
key phrases or utterances for inclusion in the de-
cision summary (Fern´andez et al., 2008; Bui et
al., 2009). Based on Fern´andez et al. (2008), a
relation representation is proposed by Wang and
Cardie (2012) to form structured summaries; we
adopt this representation here for content selec-
tion.
Our research is also in line with generating ab-
stractive summaries for conversations. Extrac-
tive approaches (Murray et al., 2005; Xie et al.,
2008; Galley, 2006) have been investigated exten-
sively in conversation summarization. Murray et
al. (2010a) present an abstraction system consist-
ing of interpretation and transformation steps. Ut-
terances are mapped to a simple conversation on-
tology in the interpretation step according to their
type, such as a decision or problem. Then an in-
teger linear programming approach is employed
to select the utterances that cover more entities as
</bodyText>
<page confidence="0.984258">
1396
</page>
<figure confidence="0.769294450704225">
Relation
Extraction
Relation Instances:
&lt;want, an LCD display with a spinning
wheel&gt;
&lt;an LCD display, with a spinning
wheel&gt;
&lt;have, some push-buttons&gt;
&lt;having, push-buttons on the outside&gt;
&lt;push-buttons, on the outside&gt;
&lt;an LCD, with spinning wheel&gt;
... (other possibilities)
Content Selection
Learned Templates
Template
Filling
&lt;want, an LCD display with a spinning wheel&gt;
• The team will want an LCD display with a
spinning wheel.
• The team with work with an LCD display
with a spinning wheel.
• The group decide to use an LCD display with
a spinning wheel.
... (other possibilities)
&lt;push-buttons, on the outside&gt;
• Push-buttons are going to be on the outside.
• Push-buttons on the outside will be used.
• There will be push-buttons on the outside.
... (other possibilities)
... (all possible abstracts per relation
instance)
Surface Realization
Statistical
Ranking
One-Best
Abstract:
The group decide to
use an LCD display
with a spinning
wheel.
One-Best
Abstract:
There will be push-
buttons on the
outside.
... (one-best abstract
per relation instance)
Post-
Selection
Final Summary:
The group decide to
use an LCD display with
a spinning wheel.
There will be push-
buttons on the outside.
Dialogue Acts:
C: Looking at what we&apos;ve got,
we we want [an LCD display
with a spinning wheel].
B: You have to have some
push-buttons, don&apos;t you?
C: Just spinning and not
scrolling , I would say .
B: I think the spinning wheel is
definitely very now.
A: but since LCDs seems to be
uh a definite yes,
C: We&apos;re having push-buttons
[on the outside]
C: and then on the inside an
LCD with spinning wheel,
</figure>
<figureCaption confidence="0.987011">
Figure 2: The abstract generation framework. It takes as input a cluster of meeting-item-specific dialogue acts,
</figureCaption>
<bodyText confidence="0.904768576923077">
from which one focused summary is constructed. Sample relation instances are denoted in bold (The indicators
are further italicized and the arguments are in [brackets]). Summary-worthy relation instances are identified by
content selection module (see Section 4) and then filled into the learned templates individually. A statistical ranker
subsequently selects one best abstract per relation instance (see Section 5.2). The post-selection component reduces
the redundancy and outputs the final summary (see Section 5.3).
determined by an external ontology. Liu and Liu
(2009) apply sentence compression on extracted
summary utterances. Though some of the unnec-
essary words are dropped, the resulting compres-
sions can still be ungrammatical and unstructured.
This work is also broadly related to ex-
pert system-based language generation (Reiter
and Dale, 2000) and concept-to-text generation
tasks (Angeli et al., 2010; Konstas and Lapata,
2012), where the generation process is decom-
posed into content selection (or text planning) and
surface realization. For instance, Angeli et al.
(2010) learn from structured database records and
parallel textual descriptions. They generate texts
based on a series of decisions made to select the
records, fields, and proper templates for render-
ing. Those techniques that are tailored to specific
domains (e.g. weather forecasts or sportcastings)
cannot be directly applied to the conversational
data, as their input is well-structured and the tem-
plates learned are domain-specific.
</bodyText>
<sectionHeader confidence="0.997207" genericHeader="method">
3 Framework
</sectionHeader>
<bodyText confidence="0.999973862068966">
Our domain-independent abstract generation
framework produces a summarizer that gener-
ates a grammatical abstract from a cluster of
meeting-element-related dialogue acts (DAs) —
all utterances associated with a single decision,
problem, action item or progress step of interest.
Note that identifying these DA clusters is a diffi-
cult task in itself (Bui et al., 2009). Accordingly,
our experiments evaluate two conditions — one
in which we assume that they are perfectly iden-
tified, and one in which we identify the clusters
automatically.
The summarizer consists of two major compo-
nents and is depicted in Figure 2. Given the DA
cluster to be summarized, the Content Selection
module identifies a set of summary-worthy rela-
tion instances represented as indicator-argument
pairs (i.e. these constitute a finer-grained represen-
tation than DAs). The Surface Realization compo-
nent then generates a short summary in three steps.
In the first step, each relation instance is filled into
templates with disparate structures that are learned
automatically from the training set (Template Fill-
ing). A statistical ranker then selects one best ab-
stract per relation instance (Statistical Ranking).
Finally, selected abstracts are processed for redun-
dancy removal in Post-Selection. Detailed descrip-
tions for each individual step are provided in Sec-
tions 4 and 5.
</bodyText>
<sectionHeader confidence="0.971135" genericHeader="method">
4 Content Selection
</sectionHeader>
<bodyText confidence="0.9998505625">
Phrase-based content selection approaches have
been shown to support better meeting sum-
maries (Fern´andez et al., 2008). Therefore, we
chose a content selection representation of a finer
granularity than an utterance: we identify relation
instances that can both effectively detect the cru-
cial content and incorporate enough syntactic in-
formation to facilitate the downstream surface re-
alization.
More specifically, our relation instances are
based on information extraction methods that
identify a lexical indicator (or trigger) that evokes
a relation of interest and then employ syntac-
tic information, often in conjunction with se-
mantic constraints, to find the argument con-
stituent(or target phrase) to be extracted. Rela-
</bodyText>
<page confidence="0.96693">
1397
</page>
<bodyText confidence="0.99934085">
tion instances, then, are represented by indicator-
argument pairs (Chen et al., 2011). For example,
in the DA cluster of Figure 2, (want, an LCD dis-
play with a spinning wheel) and (push-buttons, on
the outside) are two relation instances.
Relation Instance Extraction We adopt and
extend the syntactic constraints from Wang and
Cardie (2012) to identify all relation instances in
the input utterances; the summary-worthy ones
will be selected by a discriminative classifier.
Constituent and dependency parses are obtained
by the Stanford parser (Klein and Manning, 2003).
Both the indicator and argument take the form of
constituents in the parse tree. We restrict the el-
igible indicator to be a noun or verb; the eligi-
ble arguments is a noun phrase (NP), prepositional
phrase (PP) or adjectival phrase (ADJP). A valid
indicator-argument pair should have at least one
content word and satisfy one of the following con-
straints:
</bodyText>
<listItem confidence="0.67969975">
• When the indicator is a noun, the argument
has to be a modifier or complement of the in-
dicator.
• When the indicator is a verb, the argument
</listItem>
<bodyText confidence="0.972572375">
has to be the subject or the object if it is an
NP, or a modifier or complement of the indi-
cator if it is a PP/ADJP.
We view relation extraction as a binary classifi-
cation problem rather than a clustering task (Chen
et al., 2011). All relation instances can be cate-
gorized as summary-worthy or not, but only the
summary-worthy ones are used for abstract gen-
eration. A discriminative classifier is trained for
this purpose based on Support Vector Machines
(SVMs) (Joachims, 1998) with an RBF kernel.
For training data construction, we consider a re-
lation instance to be a positive example if it shares
any content word with its corresponding abstracts,
and a negative example otherwise. The features
used are shown in Table 1.
</bodyText>
<sectionHeader confidence="0.9699" genericHeader="method">
5 Surface Realization
</sectionHeader>
<bodyText confidence="0.98500875862069">
In this section, we describe surface realization,
which renders the relation instances into natural
language abstracts. This process begins with tem-
plate extraction (Section 5.1). Once the templates
are learned, the relation instances from Section 4
are filled into the templates to generate an abstract
(see Section 5.2). Redundancy handling is dis-
cussed in Section 5.3.
Basic Features
number of words/content words
portion of content words/stopwords
number of content words in indicator/argument
number of content words that are also in previous DA
indicator/argument only contains stopword?
number of new nouns
Content Features
has capitalized word?
has proper noun?
TF/IDF/TFIDF min/max/average
Discourse Features
main speaker or not?
is in an adjacency pair (AP)?
is in the source/target of the AP?
number of source/target DA in the AP
is the target of the AP a positive/negative/neutral response?
is the source of the AP a question?
Syntax Features
indicator/argument constituent tag
dependency relation of indicator and argument
</bodyText>
<tableCaption confidence="0.568982">
Table 1: Features for content selection. Most are
adapted from previous work (Galley, 2006; Xie et al.,
</tableCaption>
<bodyText confidence="0.9790825">
2008; Wang and Cardie, 2012). Every basic or con-
tent feature is concatenated with the constituent tags of
indicator and argument to compose a new one. Main
speakers include the most talkative speaker (who has
said the most words) and other speakers whose word
count is more than 20% of the most talkative one (Xie
et al., 2008). Adjacency pair (AP) (Galley, 2006) is
an important conversational analysis concept; each AP
consists of a source utterance and a target utterance pro-
duced by different speakers.
</bodyText>
<subsectionHeader confidence="0.994745">
5.1 Template Extraction
</subsectionHeader>
<bodyText confidence="0.999874681818182">
Sentence Clustering. Template extraction starts
with clustering the sentences that constitute the
manually generated abstracts in the training data
according to their lexical and structural similarity.
From each cluster, multiple-sequence alignment
techniques are employed to capture the recurring
patterns.
Intuitively, desirable templates are those that
can be applied in different domains to generate
the same type of focused summary (e.g. decision
or problem summaries). We do not want sen-
tences to be clustered only because they describe
the same domain-specific details (e.g. they are all
about “data collection”), which will lead to frag-
mented templates that are not reusable for new do-
mains. We therefore replace all appearances of
dates, numbers, and proper names with generic la-
bels. We also replace words that appear in both
the abstract and supporting dialogue acts by a la-
bel indicating its phrase type. For any noun phrase
with its head word abstracted, the whole phrase is
also replaced with “NP”.
</bodyText>
<page confidence="0.95027">
1398
</page>
<figure confidence="0.996504916666667">
1) The group were not sure whether to [include]VP [a recharger for the remote]NP .
2) The group were not sure whether to use [plastic and rubber or titanium for the case]NP .
3) The group were not sure whether [the remote control]NP should include [functions for
controlling video]NP .
4) They were not sure how much [a recharger]NP would cost to make .
... (Other abstracts)
Generic Label Replacement + Clustering
1) The group were not sure whether to VP NP .
2) The group were not sure whether to use NP .
3) The group were not sure whether NP should include NP .
4) They were not sure how much NP would cost to make .
Template Examples:
</figure>
<figureCaption confidence="0.992477111111111">
Fine T1: The group were not sure whether to SLOTVP NP . (1, 2)
Fine T2: The group were not sure whether NP SLOTVP SLOTVP NP . (3)
Fine T3: SLOTNP were not sure SLOTWHADJP SLOTWHADJP NP SLOTVP SLOTVP SLOTVP SLOTVP
SLOTVP . (4)
Coarse T1: SLOTNP SLOTNP were not sure SLOTSBAR SLOTVP SLOTVP SLOTNP . (1, 2)
Coarse T2: SLOTNP SLOTNP were not sure SLOTSBAR SLOTNP SLOTVP SLOTVP SLOTNP . (3)
Coarse T3: SLOTNP were not sure SLOTWHADJP SLOTWHADJP SLOTNP SLOTVP SLOTVP SLOTVP
SLOTVP . (4)
Figure 3: Example of template extraction by Multiple-
</figureCaption>
<bodyText confidence="0.96131092">
Sequence Alignment for problem abstracts from AMI.
Backbone nodes shared by at least 50% sentences are
shaded. The grammatical errors exist in the original
abstracts.
Following Barzilay and Lee (2003), we ap-
proach the sentence clustering task by hierarchical
complete-link clustering with a similarity metric
based on word n-gram overlap (n = 1, 2, 3). Clus-
ters with fewer than three abstracts are removed1.
Learning the Templates via MSA. For learn-
ing the structural patterns among the abstracts,
Multiple-Sequence Alignment (MSA) is first com-
puted for each cluster. MSA takes as input multi-
ple sentences and one scoring function to measure
the similarity between any two words. For inser-
tions or deletions, a gap cost is also added. MSA
can thus find the best way to align the sequences
with insertions or deletions in accordance with the
scorer. However, computing an optimal MSA is
NP-complete (Wang and Jiang, 1994), thus we
implement an approximate algorithm (Needleman
and Wunsch, 1970) that iteratively aligns two se-
quences each time and treats the resulting align-
ment as a new sequence2. Figure 3 demonstrates
an MSA computed from a sample cluster of ab-
</bodyText>
<footnote confidence="0.797363666666667">
1Clustering stops when the similarity between any pair-
wise clusters is below 5. This is applied to every type of sum-
marization. We tune the parameter on a small held-out devel-
opment set by manually evaluating the induced templates. No
significant change is observed within a small range.
2We adopt the scoring function for MSA from Barzilay
and Lee (2003), where aligning two identical words scores
1, inserting a gap scores −0.01, and aligning two different
words scores −0.5.
</footnote>
<bodyText confidence="0.99994228">
stracts. The MSA is represented in the form of
word lattice, from which we can detect the struc-
tural similarities shared by the sentences.
To transform the resulting MSAs into templates,
we need to decide whether a word in the sentence
should be retained to comprise the template or ab-
stracted. The backbone nodes in an MSA are iden-
tified as the ones shared by more than 50%3 of the
cluster’s sentences (shaded in gray in Figure 3).
We then create a FINE template for each sentence
by abstracting the non-backbone words, i.e. re-
placing each of those words with a generic token
(last step in Figure 3). We also create a COARSE
template that only preserves the nodes shared by
all of the cluster’s sentences. By using the op-
erations above, domain-independent patterns are
thus identified and domain-specific details are re-
moved.
Note that we do not explicitly evaluate the qual-
ity of the learned templates, which would require
a significant amount of manual evaluation. In-
stead, they are evaluated extrinsically. We encode
the templates as features (Angeli et al., 2010) that
could be selected or ignored in the succeeding ab-
stract ranking model.
</bodyText>
<subsectionHeader confidence="0.998128">
5.2 Template Filling
</subsectionHeader>
<bodyText confidence="0.997902727272727">
An Overgenerate-and-Rank Approach. Since
filling the relation instances into templates of dis-
tinct structures may result in abstracts of vary-
ing quality, we rank the abstracts based on the
features of the template, the transformation con-
ducted, and the generated abstract. This is realized
by the Overgenerate-and-Rank strategy (Walker et
al., 2001; Heilman and Smith, 2010). It takes as
input a set of relation instances (from the same
cluster) R = {(indi, argi)}Ni=1 that are produced
by content selection component, a set of templates
T = {tj}Mj=1 that are represented as parsing trees,
a transformation function F (described below),
and a statistical ranker S for ranking the generated
abstracts, for which we defer description later in
this Section.
For each (indi, argi), the overgenerate-and-
rank approach fills it into each template in T by
applying F to generate all possible abstracts. Then
the ranker S selects the best abstract absi. Post-
selection is conducted on the abstracts {absi}Ni=1
to form the final summary.
</bodyText>
<footnote confidence="0.989551">
3See Barzilay and Lee (2003) for a detailed discussion
about the choice of 50% according to pigeonhole principle.
</footnote>
<table confidence="0.816125857142857">
MSA
use
The group were not sure whether to VP NP
start They
NP should include end
how much would cost to make
Template Induction
</table>
<page confidence="0.98634">
1399
</page>
<bodyText confidence="0.990839615384615">
The transformation function F models the
constituent-level transformations of relation in-
stances and their mappings to the parse trees of
templates. With the intuition that people will reuse
the relation instances from the transcripts albeit
not necessarily in their original form to write the
abstracts, we consider three major types of map-
ping operations for the indicator or argument in the
source pair, namely, Full-Constituent Mapping,
Sub-Constituent Mapping, and Removal. Full-
Constituent Mapping denotes that a source con-
stituent is mapped directly to a target constituent
of the template parse tree with the same tag. Sub-
Constituent Mapping encodes more complex and
flexible transformations in that a sub-constituent
of the source is mapped to a target constituent
with the same tag. This operation applies when
the source has a tag of PP or ADJP, in which case
its sub-constituent, if any, with a tag of NP, VP or
ADJP can be mapped to the target constituent with
the same tag. For instance, an argument “with a
spinning wheel” (PP) can be mapped to an NP in a
template because it has a sub-constituent “a spin-
ning wheel” (NP). Removal means a source is not
mapped to any constituent in the template.
Formally, F is defined as:
</bodyText>
<equation confidence="0.999201166666667">
F(hindsrc, argsrci, t) =
{hindtran
k , argtran
k , indtar
k , argtar
k i}K k=1
</equation>
<bodyText confidence="0.9691698">
where (indsrc, argsrc) E R is a relation in-
stance (source pair); t E T is a template; indtran
k
and argtran
k is the transformed pair of indsrc and
argsrc; indtar
k and argtar
k are constituents in t, and
they compose one target pair for (indsrc, argsrc).
We require that indsrc and argsrc are not removed
at the same time. Moreover, for valid indtar
k and
argtar
k , the words subsumed by them should be all
abstracted in the template, and they do not overlap
in the parse tree.
To obtain the realized abstract, we traverse the
parse tree of the filled template in pre-order. The
words subsumed by the leaf nodes are thus col-
lected sequentially.
</bodyText>
<subsectionHeader confidence="0.507893">
Learning a Statistical Ranker. We utilize a dis-
</subsectionHeader>
<bodyText confidence="0.9237541875">
criminative ranker based on Support Vector Re-
gression (SVR) (Smola and Sch¨olkopf, 2004) to
rank the generated abstracts. Given the train-
ing data that includes clusters of gold-standard
summary-worthy relation instances, associated ab-
stracts they support, and the parallel templates for
each abstract, training samples for the ranker are
Basic Features
number of words in indsrc/argsrc
number of new nouns in indsrc/argsrc
indtran
k /argtran konly has stopword?
number of new nouns in indtran
k /argtran
k
Structure Features
</bodyText>
<figureCaption confidence="0.923971285714286">
constituent tag of indsrc/argsrc
constituent tag of indsrc with constituent tag of indtar
constituent tag of argsrc with constituent tag of argtar
transformation of indsrc/argsrc combined with constituent tag
dependency relation of indsrc and argsrc
dependency relation of indtar and argtar
above 2 features have same value?
</figureCaption>
<table confidence="0.990296761904762">
Template Features
template type (fine/coarse)
realized template (e.g. “the group decided to”)
number of words in template
the template has verb?
Realization Features
realization has verb?
realization starts with verb?
realization has adjacent verbs/NPs?
indsrc precedes/succeeds argsrc?
indtar precedes/succeeds argtar?
above 2 features have same value?
Language Model Features
log pLM (first word in indtran kprevious 1/2 words)
log pLM (realization)
log pLM (first word in argtran kIprevious 1/2 words)
log pLM (realization)/length
log pLM (next word last 1/2 words in indtran
k )
log pLM (next word last 1/2 words in argtran
k )
</table>
<tableCaption confidence="0.9774365">
Table 2: Features for abstracts ranking. The language
model features are based on a 5-gram language model
trained on Gigaword (Graff, 2003) by SRILM (Stolcke,
2002).
</tableCaption>
<bodyText confidence="0.993417666666667">
constructed according to the transformation func-
tion F mentioned above. Each sample is repre-
sented as:
</bodyText>
<equation confidence="0.8694725">
(hindsrc, argsrci, hindtran k ,argtran k ,indtar k ,argtar
k i, t, a)
</equation>
<bodyText confidence="0.907392">
where (indsrc, argsrc) is the source pair,
(indtran
</bodyText>
<equation confidence="0.96633075">
k , argtran
k ) is the transformed pair,
(indtar
k , argtar
</equation>
<bodyText confidence="0.887221">
k ) is the target pair in template t,
and a is the abstract parallel to t.
We first find (indtar,abs
</bodyText>
<equation confidence="0.7214128">
k , argtar,abs
k ), which
is the corresponding constituent pair of
(indtar
k , argtar
</equation>
<bodyText confidence="0.919049636363637">
k ) in a. Then we identify
the summary-worthy words subsumed by
(indtran
k , argtran
k ) that also appear in a. If those
words are all subsumed by (indtar,abs
k , argtar,abs
k ),
then it is considered to be a positive sample, and
a negative sample otherwise. Table 2 displays the
features used in abstract ranking.
</bodyText>
<subsectionHeader confidence="0.980669">
5.3 Post-Selection: Redundancy Handling.
</subsectionHeader>
<bodyText confidence="0.999253">
Post-selection aims to maximize the information
coverage and minimize the redundancy of the
summary. Given the generated abstracts A =
</bodyText>
<page confidence="0.990917">
1400
</page>
<figureCaption confidence="0.859925">
Algorithm 1: Greedy algorithm for post-
selection to generate the final summary.
</figureCaption>
<bodyText confidence="0.999958333333333">
{absi}Ni=1, we use a greedy algorithm (Lin and
Bilmes, 2010) to select a subset A&apos;, where A&apos; ⊆ A,
to form the final summary. We define wij as
the unigram similarity between abstracts absi and
absj, C(absi) as the number of words in absi. We
employ the following objective function:
</bodyText>
<equation confidence="0.9827245">
f(A, G) = � �absj G wi,j, G C A
absi A\G
</equation>
<bodyText confidence="0.998882">
Algorithm 1 sequentially finds an abstract with
the greatest ratio of objective function gain to
length, and add it to the summary if the gain is
non-negative.
</bodyText>
<sectionHeader confidence="0.998562" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9968135">
Corpora. Two disparate corpora are used for
evaluation. The AMI meeting corpus (Mccowan
et al., 2005) contains 139 scenario-driven meet-
ings, where groups of four people participate in
a series of four meetings for a fictitious project of
designing remote control. The ICSI meeting cor-
pus (Janin et al., 2003) consists of 75 naturally oc-
curring meetings, each of them has 4 to 10 par-
ticipants. Compared to the fabricated topics in
AMI, the conversations in ICSI tend to be special-
ized and technical, e.g. discussion about speech
and language technology. We use 57 meetings in
ICSI and 139 meetings in AMI that include a short
(usually one-sentence), manually constructed ab-
stract summarizing each important output for ev-
ery meeting. Decision and problem summaries are
annotated for both corpora. AMI has extra ac-
tion item summaries, and ICSI has progress sum-
maries. The set of dialogue acts that support each
abstract are annotated as such.
System Inputs. We consider two system input
settings. In the True Clusterings setting, we
use the annotations to create perfect partitions of
the DAs for input to the system; in the System
</bodyText>
<figureCaption confidence="0.7545455">
Figure 4: Content selection evaluation by using
ROUGE-SU4 (multiplied by 100). SVM-DA and
SVM-TOKEN denotes for supervised extract-based
methods with SVMs on utterance- and token-level.
Summaries for decision, problem, action item, and
progress are generated and evaluated for AMI and ICSI
(with names in parentheses). X-axis shows the number
of meetings used for training.
</figureCaption>
<bodyText confidence="0.999401909090909">
Clusterings setting, we employ a hierarchical ag-
glomerative clustering algorithm used for this task
in (Wang and Cardie, 2011). DAs are grouped ac-
cording to a classifier trained beforehand.
Baselines and Comparisons. We compare our
system with (1) two unsupervised baselines, (2)
two supervised extractive approaches, and (3) an
oracle derived from the gold standard abstracts.
Baselines. As in Riedhammer et al. (2010), the
LONGEST DA in each cluster is selected as the
summary. The second baseline picks the clus-
ter prototype (i.e. the DA with the largest TF-
IDF similarity with the cluster centroid) as the
summary according to Wang and Cardie (2011).
Although it is possible that important content is
spread over multiple DAs, both baselines allow
us to determine summary quality when summaries
are restricted to a single utterance.
Supervised Learning. We also compare our
approach to two supervised extractive sum-
marization methods — Support Vector Ma-
chines (Joachims, 1998) trained with the same fea-
</bodyText>
<figure confidence="0.974292285714286">
Input : relation instances R = {(indi, argi�}Ni=1,
generated abstracts A = {absi}Ni=1, objective
function f , cost function C
Output: final abstract G
G Φ (empty set);
U A;
while U =� Φ do
abs arg max f(A,GUabsi)−f(A,G);
abs�,EU C(absi)
if f(A, G U abs) − f(A, G) ≥ 0 then
G G U abs;
end
U U \ abs;
end
</figure>
<page confidence="0.989692">
1401
</page>
<bodyText confidence="0.999962">
tures as our system (see Table 1) to identify the im-
portant DAs (no syntax features) (Xie et al., 2008;
Sandu et al., 2010) or tokens (Fern´andez et al.,
2008) to include into the summary4.
Oracle. We compute an oracle consisting of the
words from the DA cluster that also appear in the
associated abstract to reflect the gap between the
best possible extracts and the human abstracts.
</bodyText>
<sectionHeader confidence="0.999712" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.993249888888889">
Content Selection Evaluation. We first employ
ROUGE (Lin and Hovy, 2003) to evaluate the
content selection component with respect to the
human written abstracts. ROUGE computes the
ngram overlapping between the system summaries
with the reference summaries, and has been used
for both text and speech summarization (Dang,
2005; Xie et al., 2008). We report ROUGE-2 (R-
2) and ROUGE-SU4 (R-SU4) that are shown to
correlate with human evaluation reasonably well.
In AMI, four meetings of different functions are
carried out in each group5. 35 meetings for “con-
ceptual design” are randomly selected for testing.
For ICSI, we reserve 12 meetings for testing.
The R-SU4 scores for each system are displayed
in Figure 4 and show that our system uniformly
outperforms the baselines and supervised systems.
The learning curve of our system is relatively flat,
which means not many training meetings are re-
quired to reach a usable performance level.
Note that the ROUGE scores are relative low
when the reference summaries are human ab-
stracts, even for evaluation among abstracts pro-
duced by different annotators (Dang, 2005). The
intrinsic difference of styles between dialogue and
human abstract further lowers the scores. But the
trend is still respected among the systems.
Abstract Generation Evaluation. To evaluate
the full abstract generation system, the BLEU
score (Papineni et al., 2002) (the precision of uni-
grams and bigrams with a brevity penalty) is com-
puted with human abstracts as reference. BLEU
has a fairly good agreement with human judge-
ment and has been used to evaluate a variety of
language generation systems (Angeli et al., 2010;
Konstas and Lapata, 2012).
</bodyText>
<footnote confidence="0.997152">
4We use SVMlight (Joachims, 1999) with RBF kernel by
default parameters for SVM-based classifiers and regressor.
5The four types of meetings in AMI are: project kick-off
(35 meetings), functional design (35 meetings), conceptual
design (35 meetings), and detailed design (34 meetings).
</footnote>
<figureCaption confidence="0.95854875">
Figure 5: Full abstract generation system evaluation
by using BLEU (multiplied by 100). SVM-DA de-
notes for supervised extractive methods with SVMs on
utterance-level.
</figureCaption>
<bodyText confidence="0.99984736">
We are not aware of any existing work gen-
erating abstractive summaries for conversations.
Therefore, we compare our full system against
a supervised utterance-level extractive method
based on SVMs along with the baselines. The
BLEU scores in Figure 5 show that our system im-
proves the scores consistently over the baselines
and the SVM-based approach.
Domain Adaptation Evaluation. We further
examine our system in domain adaptation sce-
narios for decision and problem summarization,
where we train the system on AMI for use on ICSI,
and vice versa. Table 3 indicates that, with both
true clusterings and system clusterings, our sys-
tem trained on out-of-domain data achieves com-
parable performance with the same system trained
on in-domain data. In most experiments, it also
significantly outperforms the baselines and the
extract-based approaches (p &lt; 0.05).
Human Evaluation. We randomly select 15 de-
cision and 15 problem DA clusters (true cluster-
ings). We evaluate fluency (is the text gram-
matical?) and semantic correctness (does the
summary convey the gist of the DAs in the clus-
ter?) for OUR SYSTEM trained on IN-domain data
</bodyText>
<page confidence="0.9829">
1402
</page>
<table confidence="0.999658277777778">
System (True Clusterings) AMI Decision ICSI Decision AMI Problem ICSI Problem
R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU
CENTROID DA 1.3 3.0 7.7 1.8 3.5 3.8 1.0 2.7 4.2 1.0 2.3 2.8
LONGEST DA 1.6 3.3 7.0 2.8 4.7 6.5 1.0 3.0 3.6 1.2 3.4 4.6
SVM-DA (IN) 3.4 4.7 9.7 3.4 4.5 5.7 1.4 2.4 5.0 1.6 3.4 3.4
SVM-DA (OUT) 2.7 4.2 6.6 3.1 4.2 4.6 1.4 2.2 2.5 1.3 3.0 4.6
OUR SYSTEM (IN) 4.5 6.2 11.6 4.9 7.1 10.0 3.1 4.8 7.2 4.0 5.9 6.0
OUR SYSTEM (OUT) 4.6 6.1 10.3 4.8 6.4 7.8 3.5 4.7 6.2 3.0 5.5 5.3
ORACLE 7.5 12.0 22.8 9.9 14.9 20.2 6.6 11.3 18.9 6.4 12.6 13.0
System (System Clusterings) AMI Decision ICSI Decision AMI Problem ICSI Problem
R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU
CENTROID DA 1.4 3.3 3.8 1.4 2.1 2.0 0.8 2.8 2.9 0.9 2.3 1.8
LONGEST DA 1.4 3.3 5.7 1.7 3.4 5.5 0.8 3.2 4.1 0.9 3.4 4.4
SVM-DA (IN) 2.6 4.6 10.5 3.5 6.5 7.1 1.8 3.7 4.9 1.8 4.0 4.6
SVM-DA (OUT) 3.4 5.8 10.3 2.7 4.8 6.3 2.1 3.8 4.3 1.5 3.8 3.5
OUR SYSTEM (IN) 3.5 5.4 11.7 4.4 7.4 9.1 3.3 4.6 9.5 2.3 4.2 7.4
OUR SYSTEM (OUT) 3.9 6.4 11.4 4.1 5.1 8.4 3.6 5.6 8.9 1.8 4.0 6.8
ORACLE 6.4 12.0 15.1 8.2 15.2 17.6 6.5 13.0 20.9 5.5 11.9 15.5
</table>
<tableCaption confidence="0.991471">
Table 3: Domain adaptation evaluation. Systems trained on out-of-domain data are denoted with “(OUT)”, oth-
erwise with “(IN)”. ROUGE and BLEU scores are multiplied by 100. Our systems that statistically significantly
outperform all the other approaches (except ORACLE) are in bold (p &lt; 0.05, paired t-test). The numbers in italics
show the significant improvement over the baselines by our systems.
</tableCaption>
<table confidence="0.9998554">
System Fluency Semantic Length
Mean S.D. Mean S.D.
OUR SYSTEM (IN) 3.67 0.85 3.27 1.03 23.65
OUR SYSTEM (OUT) 3.58 0.90 3.25 1.16 24.17
SVM-DA (IN) 3.36 0.84 3.44 1.26 38.83
</table>
<tableCaption confidence="0.71556775">
Table 4: Human evaluation results of Fluency and Se-
mantic correctness for the generated abstracts. The rat-
ings are on 1 (worst) to 5 (best) scale. The average
Length of the abstracts for each system is also listed.
</tableCaption>
<bodyText confidence="0.999987">
and OUT-of-domain data, and for the utterance-
level extraction system (SVM-DA) trained on in-
domain data. Each cluster of DAs along with three
randomly ordered summaries are presented to the
judges. Five native speaking Ph.D. students (none
are authors) performed the task.
We carry out an one-way Analysis of Variance
which shows significant differences in score as a
function of system (p &lt; 0.05, paired t-test). Re-
sults in Table 4 demonstrate that our system sum-
maries are significantly more compact and fluent
than the extract-based method (p &lt; 0.05) while
semantic correctness is comparable.
The judges also rank the three summaries in
terms of the overall quality in content, concise-
ness and grammaticality. An inter-rater agreement
of Fleiss’s κ = 0.45 (moderate agreement (Landis
and Koch, 1977)) was computed. Judges selected
our system as the best system in 62.3% scenarios
(IN-DOMAIN: 35.6%, OUT-OF-DOMAIN: 26.7%).
Sample summaries are exhibited in Figure 6.
</bodyText>
<sectionHeader confidence="0.998306" genericHeader="evaluation">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999555">
We presented a domain-independent abstract gen-
eration framework for focused meeting summa-
rization. Experimental results on two disparate
meeting corpora show that our system can uni-
</bodyText>
<sectionHeader confidence="0.309866" genericHeader="conclusions">
Decision Summary:
</sectionHeader>
<bodyText confidence="0.740211083333333">
Human: The remote will have push buttons outside, and
an LCD and spinning wheel inside.
Our System (In): The group decide to use an LCD dis-
play with a spinning wheel. There will be push-buttons on
the outside.
Our System (Out): LCD display is going to be with a
spinning wheel. It is necessary having push-buttons on
the outside.
SVM-DA: Looking at what we’ve got, we we want an
LCD display with a spinning wheel. Just spinning and not
scrolling, I would say. I think the spinning wheel is defi-
nitely very now. We’re having push-buttons on the outside
</bodyText>
<subsectionHeader confidence="0.712021">
Problem Summary:
</subsectionHeader>
<bodyText confidence="0.896404875">
Human: How to incorporate a fruit and vegetable theme
into the remote.
Our System (In): Whether to include the shape of fruit.
The team had to thinking bright colors.
Our System (Out): It is unclear that the buttons being in
the shape of fruit.
SVM-DA: and um Im not sure about the buttons being in
the shape of fruit though.
Figure 6: Sample decision and problem sum-
maries generated by various systems for examples
in Figure 1.
formly outperform the state-of-the-art supervised
extraction-based systems in both automatic and
manual evaluation. Our system also exhibits an
ability to train on out-of-domain data to generate
abstracts for a new target domain.
</bodyText>
<sectionHeader confidence="0.997585" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.985978833333333">
This work was supported in part by National Sci-
ence Foundation Grant IIS-0968450 and a gift
from Boeing. We thank Moontae Lee, Myle Ott,
Yiye Ruan, Chenhao Tan, and the ACL reviewers
for valuable suggestions and advice on various as-
pects of this work.
</bodyText>
<page confidence="0.982239">
1403
</page>
<sectionHeader confidence="0.989302" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824018518518">
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 502–512, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology - Volume 1, NAACL
’03, pages 16–23, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Trung H. Bui, Matthew Frampton, John Dowding, and
Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical mod-
els and semantic similarity. In Proceedings of the
SIGDIAL 2009 Conference: The 10th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, SIGDIAL ’09, pages 235–243, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for Mining and Summarizing Text
Conversations. Morgan &amp; Claypool Publishers.
Harr Chen, Edward Benson, Tahira Naseem, and
Regina Barzilay. 2011. In-domain relation discov-
ery with meta-constraints via posterior regulariza-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ’11,
pages 530–540, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hoa T. Dang. 2005. Overview of DUC 2005. In Doc-
ument Understanding Conference.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press, July.
Raquel Fern´andez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Pe-
ters. 2008. Identifying relevant phrases to sum-
marize decisions in spoken meetings. In INTER-
SPEECH, pages 78–81.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’06, pages 364–372, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
David Graff. 2003. English Gigaword.
Michael Heilman and Noah A. Smith. 2010. Good
question! statistical ranking for question generation.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
’10, pages 609–617, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The icsi meeting corpus.
volume 1, pages I–364–I–367 vol.1.
Thorsten Joachims. 1998. Text categorization with
suport vector machines: Learning with many rele-
vant features. In Proceedings of the 10th European
Conference on Machine Learning, ECML ’98, pages
137–142, London, UK, UK. Springer-Verlag.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making large-scale support vector ma-
chine learning practical, pages 169–184. MIT Press,
Cambridge, MA, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ’12, pages 369–378, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
J R Landis and G G Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159–174.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 912–920, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology - Volume 1, pages 71–78.
Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: can it be done by sen-
tence compression? In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
’09, pages 261–264, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.911554">
1404
</page>
<reference confidence="0.999530271604938">
I. Mccowan, G. Lathoud, M. Lincoln, A. Lisowska,
W. Post, D. Reidsma, and P. Wellner. 2005. The ami
meeting corpus. In In: Proceedings Measuring Be-
havior 2005, 5th International Conference on Meth-
ods and Techniques in Behavioral Research. L.P.J.J.
Noldus, F. Grieco, L.W.S. Loijens and P.H. Zimmer-
man (Eds.), Wageningen: Noldus Information Tech-
nology.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
INTERSPEECH, pages 593–596.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010a. Interpretation and transformation for ab-
stracting conversations. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics, HLT ’10, pages 894–902,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Gabriel Murray, Giuseppe Carenini, and Raymond T.
Ng. 2010b. Generating and validating abstracts of
meeting conversations: a user study. In INLG.
S. B. Needleman and C. D. Wunsch. 1970. A general
method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal of
molecular biology, 48(3):443–453, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge Univer-
sity Press, New York, NY, USA.
Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-T¨ur. 2010. Long story short - global unsu-
pervised models for keyphrase based meeting sum-
marization. Speech Commun., 52(10):801–815, Oc-
tober.
Oana Sandu, Giuseppe Carenini, Gabriel Murray, and
Raymond Ng. 2010. Domain adaptation to sum-
marize human conversations. In Proceedings of the
2010 Workshop on Domain Adaptation for Natural
Language Processing, DANLP 2010, pages 16–22,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199–222, August.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 2, pages 901–904, Denver, USA.
Marilyn A. Walker, Owen Rambow, and Monica Ro-
gati. 2001. Spot: a trainable sentence planner.
In Proceedings of the second meeting of the North
American Chapter of the Association for Com-
putational Linguistics on Language technologies,
NAACL ’01, pages 1–8, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Lu Wang and Claire Cardie. 2011. Summarizing de-
cisions in spoken meetings. In Proceedings of the
Workshop on Automatic Summarization for Different
Genres, Media, and Languages, WASDGML ’11,
pages 16–24, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lu Wang and Claire Cardie. 2012. Focused meet-
ing summarization via unsupervised relation extrac-
tion. In Proceedings of the 13th Annual Meeting of
the Special Interest Group on Discourse and Dia-
logue, SIGDIAL ’12, pages 304–313, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Lusheng Wang and Tao Jiang. 1994. On the complex-
ity of multiple sequence alignment. Journal of Com-
putational Biology, 1(4):337–348.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extrac-
tive meeting summarization. In in Proc. of IEEE
Spoken Language Technology (SLT.
</reference>
<page confidence="0.992829">
1405
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.766811">
<title confidence="0.9987795">Domain-Independent Abstract for Focused Meeting Summarization</title>
<author confidence="0.995778">Lu</author>
<affiliation confidence="0.96308">Department of Computer Cornell</affiliation>
<address confidence="0.875826">Ithaca, NY</address>
<email confidence="0.999676">luwang@cs.cornell.edu</email>
<abstract confidence="0.9974278125">We address the challenge of generating natural language abstractive summaries for spoken meetings in a domain-independent fashion. apply Alignment induce abstract generation templates that can be for different domains. An Overgenerateis utilized to produce and rank candidate abstracts. Experiments using in-domain and out-of-domain training on disparate corpora show that our system uniformly outperforms state-of-the-art supervised extract-based approaches. In addition, human judges rate our system summaries significantly higher than compared systems in fluency and overall quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A simple domain-independent probabilistic approach to generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>502--512</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4363" citStr="Angeli et al., 2010" startWordPosition="678" endWordPosition="682"> the meeting transcripts. On the contrary, the manually composed summaries (abstracts) are more compact and readable, and are written in a distinctly non-conversational style. 1395 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1395–1405, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows u</context>
<context position="11079" citStr="Angeli et al., 2010" startWordPosition="1727" endWordPosition="1730">mplates individually. A statistical ranker subsequently selects one best abstract per relation instance (see Section 5.2). The post-selection component reduces the redundancy and outputs the final summary (see Section 5.3). determined by an external ontology. Liu and Liu (2009) apply sentence compression on extracted summary utterances. Though some of the unnecessary words are dropped, the resulting compressions can still be ungrammatical and unstructured. This work is also broadly related to expert system-based language generation (Reiter and Dale, 2000) and concept-to-text generation tasks (Angeli et al., 2010; Konstas and Lapata, 2012), where the generation process is decomposed into content selection (or text planning) and surface realization. For instance, Angeli et al. (2010) learn from structured database records and parallel textual descriptions. They generate texts based on a series of decisions made to select the records, fields, and proper templates for rendering. Those techniques that are tailored to specific domains (e.g. weather forecasts or sportcastings) cannot be directly applied to the conversational data, as their input is well-structured and the templates learned are domain-specif</context>
<context position="22183" citStr="Angeli et al., 2010" startWordPosition="3524" endWordPosition="3527">a FINE template for each sentence by abstracting the non-backbone words, i.e. replacing each of those words with a generic token (last step in Figure 3). We also create a COARSE template that only preserves the nodes shared by all of the cluster’s sentences. By using the operations above, domain-independent patterns are thus identified and domain-specific details are removed. Note that we do not explicitly evaluate the quality of the learned templates, which would require a significant amount of manual evaluation. Instead, they are evaluated extrinsically. We encode the templates as features (Angeli et al., 2010) that could be selected or ignored in the succeeding abstract ranking model. 5.2 Template Filling An Overgenerate-and-Rank Approach. Since filling the relation instances into templates of distinct structures may result in abstracts of varying quality, we rank the abstracts based on the features of the template, the transformation conducted, and the generated abstract. This is realized by the Overgenerate-and-Rank strategy (Walker et al., 2001; Heilman and Smith, 2010). It takes as input a set of relation instances (from the same cluster) R = {(indi, argi)}Ni=1 that are produced by content sele</context>
<context position="33618" citStr="Angeli et al., 2010" startWordPosition="5415" endWordPosition="5418">abstracts, even for evaluation among abstracts produced by different annotators (Dang, 2005). The intrinsic difference of styles between dialogue and human abstract further lowers the scores. But the trend is still respected among the systems. Abstract Generation Evaluation. To evaluate the full abstract generation system, the BLEU score (Papineni et al., 2002) (the precision of unigrams and bigrams with a brevity penalty) is computed with human abstracts as reference. BLEU has a fairly good agreement with human judgement and has been used to evaluate a variety of language generation systems (Angeli et al., 2010; Konstas and Lapata, 2012). 4We use SVMlight (Joachims, 1999) with RBF kernel by default parameters for SVM-based classifiers and regressor. 5The four types of meetings in AMI are: project kick-off (35 meetings), functional design (35 meetings), conceptual design (35 meetings), and detailed design (34 meetings). Figure 5: Full abstract generation system evaluation by using BLEU (multiplied by 100). SVM-DA denotes for supervised extractive methods with SVMs on utterance-level. We are not aware of any existing work generating abstractive summaries for conversations. Therefore, we compare our fu</context>
</contexts>
<marker>Angeli, Liang, Klein, 2010</marker>
<rawString>Gabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 502–512, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5584" citStr="Barzilay and Lee, 2003" startWordPosition="871" endWordPosition="874"> us to identify and reformulate the important information for the focused summary. Our contributions are as follows: • To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings. • We present a novel template extraction algorithm, based on Multiple Sequence Alignment (MSA) (Durbin et al., 1998), to induce domain-independent templates that guide abstract generation. MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al., 1998) and has also been employed for learning paraphrases (Barzilay and Lee, 2003). • Although our framework requires labeled training data for each type of focused summary (decisions, problems, etc.), we also make initial tries for domain adaptation so that our summarization method does not need human-written abstracts for each new meeting domain (e.g. faculty meetings, theater group meetings, project group meetings). We instantiate the abstract generation framework on two corpora from disparate domains — the AMI Meeting Corpus (Mccowan et al., 2005) and ICSI Meeting Corpus (Janin et al., 2003) — and produce systems to generate focused summaries with regard to four types o</context>
<context position="19675" citStr="Barzilay and Lee (2003)" startWordPosition="3104" endWordPosition="3107">TVP SLOTVP NP . (3) Fine T3: SLOTNP were not sure SLOTWHADJP SLOTWHADJP NP SLOTVP SLOTVP SLOTVP SLOTVP SLOTVP . (4) Coarse T1: SLOTNP SLOTNP were not sure SLOTSBAR SLOTVP SLOTVP SLOTNP . (1, 2) Coarse T2: SLOTNP SLOTNP were not sure SLOTSBAR SLOTNP SLOTVP SLOTVP SLOTNP . (3) Coarse T3: SLOTNP were not sure SLOTWHADJP SLOTWHADJP SLOTNP SLOTVP SLOTVP SLOTVP SLOTVP . (4) Figure 3: Example of template extraction by MultipleSequence Alignment for problem abstracts from AMI. Backbone nodes shared by at least 50% sentences are shaded. The grammatical errors exist in the original abstracts. Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n = 1, 2, 3). Clusters with fewer than three abstracts are removed1. Learning the Templates via MSA. For learning the structural patterns among the abstracts, Multiple-Sequence Alignment (MSA) is first computed for each cluster. MSA takes as input multiple sentences and one scoring function to measure the similarity between any two words. For insertions or deletions, a gap cost is also added. MSA can thus find the best way to align the sequences with insert</context>
<context position="20992" citStr="Barzilay and Lee (2003)" startWordPosition="3323" endWordPosition="3326">ete (Wang and Jiang, 1994), thus we implement an approximate algorithm (Needleman and Wunsch, 1970) that iteratively aligns two sequences each time and treats the resulting alignment as a new sequence2. Figure 3 demonstrates an MSA computed from a sample cluster of ab1Clustering stops when the similarity between any pairwise clusters is below 5. This is applied to every type of summarization. We tune the parameter on a small held-out development set by manually evaluating the induced templates. No significant change is observed within a small range. 2We adopt the scoring function for MSA from Barzilay and Lee (2003), where aligning two identical words scores 1, inserting a gap scores −0.01, and aligning two different words scores −0.5. stracts. The MSA is represented in the form of word lattice, from which we can detect the structural similarities shared by the sentences. To transform the resulting MSAs into templates, we need to decide whether a word in the sentence should be retained to comprise the template or abstracted. The backbone nodes in an MSA are identified as the ones shared by more than 50%3 of the cluster’s sentences (shaded in gray in Figure 3). We then create a FINE template for each sent</context>
<context position="23336" citStr="Barzilay and Lee (2003)" startWordPosition="3708" endWordPosition="3711">same cluster) R = {(indi, argi)}Ni=1 that are produced by content selection component, a set of templates T = {tj}Mj=1 that are represented as parsing trees, a transformation function F (described below), and a statistical ranker S for ranking the generated abstracts, for which we defer description later in this Section. For each (indi, argi), the overgenerate-andrank approach fills it into each template in T by applying F to generate all possible abstracts. Then the ranker S selects the best abstract absi. Postselection is conducted on the abstracts {absi}Ni=1 to form the final summary. 3See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. MSA use The group were not sure whether to VP NP start They NP should include end how much would cost to make Template Induction 1399 The transformation function F models the constituent-level transformations of relation instances and their mappings to the parse trees of templates. With the intuition that people will reuse the relation instances from the transcripts albeit not necessarily in their original form to write the abstracts, we consider three major types of mapping operations for the indicator or ar</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: an unsupervised approach using multiple-sequence alignment. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 16–23, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trung H Bui</author>
<author>Matthew Frampton</author>
<author>John Dowding</author>
<author>Stanley Peters</author>
</authors>
<title>Extracting decisions from multi-party dialogue using directed graphical models and semantic similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’09,</booktitle>
<pages>235--243</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1810" citStr="Bui et al., 2009" startWordPosition="265" endWordPosition="268">ing as a whole, they refer to summaries of a specific aspect of a meeting, such as the DECISIONS reached, PROBLEMS discussed, PROGRESS made or ACTION ITEMS that emerged (Carenini et al., 2011). Our goal is to provide an automatic summarization system that can generate abstract-style focused meeting summaries to help users digest the vast amount of meeting content in an easy manner. Existing meeting summarization systems remain largely extractive: their summaries are comprised exclusively of patchworks of utterances selected directly from the meetings to be summarized (Riedhammer et al., 2010; Bui et al., 2009; Xie et al., 2008). Although relatively easy to construct, extractive approaches fall short of producing concise and readable summaries, largely due Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 cardie@cs.cornell.edu C: Looking at what we’ve got, we we want an LCD display with a spinning wheel. B: You have to have some push-buttons, don’t you? C: Just spinning and not scrolling, I would say. B: I think the spinning wheel is definitely very now. A: but since LCDs seems to be uh a definite yes, C: We’re having push-buttons on the outside C: and then on the ins</context>
<context position="7681" citStr="Bui et al., 2009" startWordPosition="1190" endWordPosition="1193">e ICSI meeting data, and vice versa). The resulting systems yield results comparable to those from the same system trained on in-domain data, and statistically significantly outperform supervised extractive summarization approaches trained on in-domain data. 2 Related Work Most research on spoken dialogue summarization attempts to generate summaries for full dialogues (Carenini et al., 2011). Only recently has the task of focused summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation</context>
<context position="12061" citStr="Bui et al., 2009" startWordPosition="1873" endWordPosition="1876">ring. Those techniques that are tailored to specific domains (e.g. weather forecasts or sportcastings) cannot be directly applied to the conversational data, as their input is well-structured and the templates learned are domain-specific. 3 Framework Our domain-independent abstract generation framework produces a summarizer that generates a grammatical abstract from a cluster of meeting-element-related dialogue acts (DAs) — all utterances associated with a single decision, problem, action item or progress step of interest. Note that identifying these DA clusters is a difficult task in itself (Bui et al., 2009). Accordingly, our experiments evaluate two conditions — one in which we assume that they are perfectly identified, and one in which we identify the clusters automatically. The summarizer consists of two major components and is depicted in Figure 2. Given the DA cluster to be summarized, the Content Selection module identifies a set of summary-worthy relation instances represented as indicator-argument pairs (i.e. these constitute a finer-grained representation than DAs). The Surface Realization component then generates a short summary in three steps. In the first step, each relation instance </context>
</contexts>
<marker>Bui, Frampton, Dowding, Peters, 2009</marker>
<rawString>Trung H. Bui, Matthew Frampton, John Dowding, and Stanley Peters. 2009. Extracting decisions from multi-party dialogue using directed graphical models and semantic similarity. In Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’09, pages 235–243, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Gabriel Murray</author>
<author>Raymond Ng</author>
</authors>
<title>Methods for Mining and Summarizing Text Conversations.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1386" citStr="Carenini et al., 2011" startWordPosition="197" endWordPosition="200">compared systems in fluency and overall quality. 1 Introduction Meetings are a common way to collaborate, share information and exchange opinions. Consequently, automatically generated meeting summaries could be of great value to people and businesses alike by providing quick access to the essential content of past meetings. Focused meeting summaries have been proposed as particularly useful; in contrast to summaries of a meeting as a whole, they refer to summaries of a specific aspect of a meeting, such as the DECISIONS reached, PROBLEMS discussed, PROGRESS made or ACTION ITEMS that emerged (Carenini et al., 2011). Our goal is to provide an automatic summarization system that can generate abstract-style focused meeting summaries to help users digest the vast amount of meeting content in an easy manner. Existing meeting summarization systems remain largely extractive: their summaries are comprised exclusively of patchworks of utterances selected directly from the meetings to be summarized (Riedhammer et al., 2010; Bui et al., 2009; Xie et al., 2008). Although relatively easy to construct, extractive approaches fall short of producing concise and readable summaries, largely due Claire Cardie Department o</context>
<context position="7458" citStr="Carenini et al., 2011" startWordPosition="1155" endWordPosition="1158">.3% vs. 37.7%). Finally, we examine the generality of our model across domains for two types of focused summarization — decisions and problems — by training the summarizer on out-of-domain data (i.e. the AMI corpus for use on the ICSI meeting data, and vice versa). The resulting systems yield results comparable to those from the same system trained on in-domain data, and statistically significantly outperform supervised extractive summarization approaches trained on in-domain data. 2 Related Work Most research on spoken dialogue summarization attempts to generate summaries for full dialogues (Carenini et al., 2011). Only recently has the task of focused summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated exte</context>
</contexts>
<marker>Carenini, Murray, Ng, 2011</marker>
<rawString>Giuseppe Carenini, Gabriel Murray, and Raymond Ng. 2011. Methods for Mining and Summarizing Text Conversations. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harr Chen</author>
<author>Edward Benson</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>In-domain relation discovery with meta-constraints via posterior regularization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>530--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13880" citStr="Chen et al., 2011" startWordPosition="2147" endWordPosition="2150">ity than an utterance: we identify relation instances that can both effectively detect the crucial content and incorporate enough syntactic information to facilitate the downstream surface realization. More specifically, our relation instances are based on information extraction methods that identify a lexical indicator (or trigger) that evokes a relation of interest and then employ syntactic information, often in conjunction with semantic constraints, to find the argument constituent(or target phrase) to be extracted. Rela1397 tion instances, then, are represented by indicatorargument pairs (Chen et al., 2011). For example, in the DA cluster of Figure 2, (want, an LCD display with a spinning wheel) and (push-buttons, on the outside) are two relation instances. Relation Instance Extraction We adopt and extend the syntactic constraints from Wang and Cardie (2012) to identify all relation instances in the input utterances; the summary-worthy ones will be selected by a discriminative classifier. Constituent and dependency parses are obtained by the Stanford parser (Klein and Manning, 2003). Both the indicator and argument take the form of constituents in the parse tree. We restrict the eligible indicat</context>
</contexts>
<marker>Chen, Benson, Naseem, Barzilay, 2011</marker>
<rawString>Harr Chen, Edward Benson, Tahira Naseem, and Regina Barzilay. 2011. In-domain relation discovery with meta-constraints via posterior regularization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 530–540, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa T Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2005</date>
<booktitle>In Document Understanding Conference.</booktitle>
<contexts>
<context position="32303" citStr="Dang, 2005" startWordPosition="5203" endWordPosition="5204"> al., 2010) or tokens (Fern´andez et al., 2008) to include into the summary4. Oracle. We compute an oracle consisting of the words from the DA cluster that also appear in the associated abstract to reflect the gap between the best possible extracts and the human abstracts. 7 Results Content Selection Evaluation. We first employ ROUGE (Lin and Hovy, 2003) to evaluate the content selection component with respect to the human written abstracts. ROUGE computes the ngram overlapping between the system summaries with the reference summaries, and has been used for both text and speech summarization (Dang, 2005; Xie et al., 2008). We report ROUGE-2 (R2) and ROUGE-SU4 (R-SU4) that are shown to correlate with human evaluation reasonably well. In AMI, four meetings of different functions are carried out in each group5. 35 meetings for “conceptual design” are randomly selected for testing. For ICSI, we reserve 12 meetings for testing. The R-SU4 scores for each system are displayed in Figure 4 and show that our system uniformly outperforms the baselines and supervised systems. The learning curve of our system is relatively flat, which means not many training meetings are required to reach a usable perfor</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>Hoa T. Dang. 2005. Overview of DUC 2005. In Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Durbin</author>
<author>Sean R Eddy</author>
<author>Anders Krogh</author>
<author>Graeme Mitchison</author>
</authors>
<title>Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.</title>
<date>1998</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="5333" citStr="Durbin et al., 1998" startWordPosition="832" endWordPosition="835">ich generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows us to identify and reformulate the important information for the focused summary. Our contributions are as follows: • To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings. • We present a novel template extraction algorithm, based on Multiple Sequence Alignment (MSA) (Durbin et al., 1998), to induce domain-independent templates that guide abstract generation. MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al., 1998) and has also been employed for learning paraphrases (Barzilay and Lee, 2003). • Although our framework requires labeled training data for each type of focused summary (decisions, problems, etc.), we also make initial tries for domain adaptation so that our summarization method does not need human-written abstracts for each new meeting domain (e.g. faculty meetings, theater group meetings, project group meetings). We insta</context>
</contexts>
<marker>Durbin, Eddy, Krogh, Mitchison, 1998</marker>
<rawString>Richard Durbin, Sean R. Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Fern´andez</author>
<author>Matthew Frampton</author>
<author>John Dowding</author>
<author>Anish Adukuzhiyil</author>
<author>Patrick Ehlen</author>
<author>Stanley Peters</author>
</authors>
<title>Identifying relevant phrases to summarize decisions in spoken meetings.</title>
<date>2008</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>78--81</pages>
<marker>Fern´andez, Frampton, Dowding, Adukuzhiyil, Ehlen, Peters, 2008</marker>
<rawString>Raquel Fern´andez, Matthew Frampton, John Dowding, Anish Adukuzhiyil, Patrick Ehlen, and Stanley Peters. 2008. Identifying relevant phrases to summarize decisions in spoken meetings. In INTERSPEECH, pages 78–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
</authors>
<title>A skip-chain conditional random field for ranking meeting utterances by importance.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>364--372</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8030" citStr="Galley, 2006" startWordPosition="1247" endWordPosition="1248">or full dialogues (Carenini et al., 2011). Only recently has the task of focused summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation step according to their type, such as a decision or problem. Then an integer linear programming approach is employed to select the utterances that cover more entities as 1396 Relation Extraction Relation Instances: &lt;want, an LCD display with a spinning wheel&gt; &lt;an LCD display, with a spinning wheel&gt; &lt;have, some push-buttons&gt; &lt;having, push-buttons </context>
<context position="16739" citStr="Galley, 2006" startWordPosition="2611" endWordPosition="2612">t words that are also in previous DA indicator/argument only contains stopword? number of new nouns Content Features has capitalized word? has proper noun? TF/IDF/TFIDF min/max/average Discourse Features main speaker or not? is in an adjacency pair (AP)? is in the source/target of the AP? number of source/target DA in the AP is the target of the AP a positive/negative/neutral response? is the source of the AP a question? Syntax Features indicator/argument constituent tag dependency relation of indicator and argument Table 1: Features for content selection. Most are adapted from previous work (Galley, 2006; Xie et al., 2008; Wang and Cardie, 2012). Every basic or content feature is concatenated with the constituent tags of indicator and argument to compose a new one. Main speakers include the most talkative speaker (who has said the most words) and other speakers whose word count is more than 20% of the most talkative one (Xie et al., 2008). Adjacency pair (AP) (Galley, 2006) is an important conversational analysis concept; each AP consists of a source utterance and a target utterance produced by different speakers. 5.1 Template Extraction Sentence Clustering. Template extraction starts with cl</context>
</contexts>
<marker>Galley, 2006</marker>
<rawString>Michel Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 364–372, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<date>2003</date>
<note>English Gigaword.</note>
<contexts>
<context position="27187" citStr="Graff, 2003" startWordPosition="4343" endWordPosition="4344">atures realization has verb? realization starts with verb? realization has adjacent verbs/NPs? indsrc precedes/succeeds argsrc? indtar precedes/succeeds argtar? above 2 features have same value? Language Model Features log pLM (first word in indtran kprevious 1/2 words) log pLM (realization) log pLM (first word in argtran kIprevious 1/2 words) log pLM (realization)/length log pLM (next word last 1/2 words in indtran k ) log pLM (next word last 1/2 words in argtran k ) Table 2: Features for abstracts ranking. The language model features are based on a 5-gram language model trained on Gigaword (Graff, 2003) by SRILM (Stolcke, 2002). constructed according to the transformation function F mentioned above. Each sample is represented as: (hindsrc, argsrci, hindtran k ,argtran k ,indtar k ,argtar k i, t, a) where (indsrc, argsrc) is the source pair, (indtran k , argtran k ) is the transformed pair, (indtar k , argtar k ) is the target pair in template t, and a is the abstract parallel to t. We first find (indtar,abs k , argtar,abs k ), which is the corresponding constituent pair of (indtar k , argtar k ) in a. Then we identify the summary-worthy words subsumed by (indtran k , argtran k ) that also ap</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. English Gigaword.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Good question! statistical ranking for question generation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>609--617</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4685" citStr="Heilman and Smith, 2010" startWordPosition="729" endWordPosition="732">13. c�2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows us to identify and reformulate the important information for the focused summary. Our contributions are as follows: • To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings. • We present a novel template extraction algorithm, based on Multipl</context>
<context position="22655" citStr="Heilman and Smith, 2010" startWordPosition="3596" endWordPosition="3599"> require a significant amount of manual evaluation. Instead, they are evaluated extrinsically. We encode the templates as features (Angeli et al., 2010) that could be selected or ignored in the succeeding abstract ranking model. 5.2 Template Filling An Overgenerate-and-Rank Approach. Since filling the relation instances into templates of distinct structures may result in abstracts of varying quality, we rank the abstracts based on the features of the template, the transformation conducted, and the generated abstract. This is realized by the Overgenerate-and-Rank strategy (Walker et al., 2001; Heilman and Smith, 2010). It takes as input a set of relation instances (from the same cluster) R = {(indi, argi)}Ni=1 that are produced by content selection component, a set of templates T = {tj}Mj=1 that are represented as parsing trees, a transformation function F (described below), and a statistical ranker S for ranking the generated abstracts, for which we defer description later in this Section. For each (indi, argi), the overgenerate-andrank approach fills it into each template in T by applying F to generate all possible abstracts. Then the ranker S selects the best abstract absi. Postselection is conducted on</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Good question! statistical ranking for question generation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 609–617, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The icsi meeting corpus.</title>
<date>2003</date>
<volume>1</volume>
<pages>364--367</pages>
<contexts>
<context position="6104" citStr="Janin et al., 2003" startWordPosition="952" endWordPosition="955"> (Durbin et al., 1998) and has also been employed for learning paraphrases (Barzilay and Lee, 2003). • Although our framework requires labeled training data for each type of focused summary (decisions, problems, etc.), we also make initial tries for domain adaptation so that our summarization method does not need human-written abstracts for each new meeting domain (e.g. faculty meetings, theater group meetings, project group meetings). We instantiate the abstract generation framework on two corpora from disparate domains — the AMI Meeting Corpus (Mccowan et al., 2005) and ICSI Meeting Corpus (Janin et al., 2003) — and produce systems to generate focused summaries with regard to four types of meeting elements: DECISIONs, PROBLEMs, ACTION ITEMSs, and PROGRESS. Automatic evaluation (using ROUGE (Lin and Hovy, 2003) and BLEU (Papineni et al., 2002)) against manually generated focused summaries shows that our summarizers uniformly and statistically significantly outperform two baseline systems as well as a state-of-the-art supervised extraction-based system. Human evaluation also indicates that the abstractive summaries produced by our systems are more linguistically appealing than those of the utterance-</context>
<context position="29073" citStr="Janin et al., 2003" startWordPosition="4670" endWordPosition="4673">bsj, C(absi) as the number of words in absi. We employ the following objective function: f(A, G) = � �absj G wi,j, G C A absi A\G Algorithm 1 sequentially finds an abstract with the greatest ratio of objective function gain to length, and add it to the summary if the gain is non-negative. 6 Experimental Setup Corpora. Two disparate corpora are used for evaluation. The AMI meeting corpus (Mccowan et al., 2005) contains 139 scenario-driven meetings, where groups of four people participate in a series of four meetings for a fictitious project of designing remote control. The ICSI meeting corpus (Janin et al., 2003) consists of 75 naturally occurring meetings, each of them has 4 to 10 participants. Compared to the fabricated topics in AMI, the conversations in ICSI tend to be specialized and technical, e.g. discussion about speech and language technology. We use 57 meetings in ICSI and 139 meetings in AMI that include a short (usually one-sentence), manually constructed abstract summarizing each important output for every meeting. Decision and problem summaries are annotated for both corpora. AMI has extra action item summaries, and ICSI has progress summaries. The set of dialogue acts that support each </context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The icsi meeting corpus. volume 1, pages I–364–I–367 vol.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with suport vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the 10th European Conference on Machine Learning, ECML ’98,</booktitle>
<pages>137--142</pages>
<publisher>UK. Springer-Verlag.</publisher>
<location>London, UK,</location>
<contexts>
<context position="15343" citStr="Joachims, 1998" startWordPosition="2398" endWordPosition="2399"> the indicator is a noun, the argument has to be a modifier or complement of the indicator. • When the indicator is a verb, the argument has to be the subject or the object if it is an NP, or a modifier or complement of the indicator if it is a PP/ADJP. We view relation extraction as a binary classification problem rather than a clustering task (Chen et al., 2011). All relation instances can be categorized as summary-worthy or not, but only the summary-worthy ones are used for abstract generation. A discriminative classifier is trained for this purpose based on Support Vector Machines (SVMs) (Joachims, 1998) with an RBF kernel. For training data construction, we consider a relation instance to be a positive example if it shares any content word with its corresponding abstracts, and a negative example otherwise. The features used are shown in Table 1. 5 Surface Realization In this section, we describe surface realization, which renders the relation instances into natural language abstracts. This process begins with template extraction (Section 5.1). Once the templates are learned, the relation instances from Section 4 are filled into the templates to generate an abstract (see Section 5.2). Redunda</context>
<context position="31247" citStr="Joachims, 1998" startWordPosition="5016" endWordPosition="5017">rd abstracts. Baselines. As in Riedhammer et al. (2010), the LONGEST DA in each cluster is selected as the summary. The second baseline picks the cluster prototype (i.e. the DA with the largest TFIDF similarity with the cluster centroid) as the summary according to Wang and Cardie (2011). Although it is possible that important content is spread over multiple DAs, both baselines allow us to determine summary quality when summaries are restricted to a single utterance. Supervised Learning. We also compare our approach to two supervised extractive summarization methods — Support Vector Machines (Joachims, 1998) trained with the same feaInput : relation instances R = {(indi, argi�}Ni=1, generated abstracts A = {absi}Ni=1, objective function f , cost function C Output: final abstract G G Φ (empty set); U A; while U =� Φ do abs arg max f(A,GUabsi)−f(A,G); abs�,EU C(absi) if f(A, G U abs) − f(A, G) ≥ 0 then G G U abs; end U U \ abs; end 1401 tures as our system (see Table 1) to identify the important DAs (no syntax features) (Xie et al., 2008; Sandu et al., 2010) or tokens (Fern´andez et al., 2008) to include into the summary4. Oracle. We compute an oracle consisting of the words from the DA cluster tha</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with suport vector machines: Learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning, ECML ’98, pages 137–142, London, UK, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Advances in kernel methods. chapter Making large-scale support vector machine learning practical,</title>
<date>1999</date>
<pages>169--184</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="33680" citStr="Joachims, 1999" startWordPosition="5426" endWordPosition="5427">nt annotators (Dang, 2005). The intrinsic difference of styles between dialogue and human abstract further lowers the scores. But the trend is still respected among the systems. Abstract Generation Evaluation. To evaluate the full abstract generation system, the BLEU score (Papineni et al., 2002) (the precision of unigrams and bigrams with a brevity penalty) is computed with human abstracts as reference. BLEU has a fairly good agreement with human judgement and has been used to evaluate a variety of language generation systems (Angeli et al., 2010; Konstas and Lapata, 2012). 4We use SVMlight (Joachims, 1999) with RBF kernel by default parameters for SVM-based classifiers and regressor. 5The four types of meetings in AMI are: project kick-off (35 meetings), functional design (35 meetings), conceptual design (35 meetings), and detailed design (34 meetings). Figure 5: Full abstract generation system evaluation by using BLEU (multiplied by 100). SVM-DA denotes for supervised extractive methods with SVMs on utterance-level. We are not aware of any existing work generating abstractive summaries for conversations. Therefore, we compare our full system against a supervised utterance-level extractive meth</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Advances in kernel methods. chapter Making large-scale support vector machine learning practical, pages 169–184. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14365" citStr="Klein and Manning, 2003" startWordPosition="2222" endWordPosition="2225">onstituent(or target phrase) to be extracted. Rela1397 tion instances, then, are represented by indicatorargument pairs (Chen et al., 2011). For example, in the DA cluster of Figure 2, (want, an LCD display with a spinning wheel) and (push-buttons, on the outside) are two relation instances. Relation Instance Extraction We adopt and extend the syntactic constraints from Wang and Cardie (2012) to identify all relation instances in the input utterances; the summary-worthy ones will be selected by a discriminative classifier. Constituent and dependency parses are obtained by the Stanford parser (Klein and Manning, 2003). Both the indicator and argument take the form of constituents in the parse tree. We restrict the eligible indicator to be a noun or verb; the eligible arguments is a noun phrase (NP), prepositional phrase (PP) or adjectival phrase (ADJP). A valid indicator-argument pair should have at least one content word and satisfy one of the following constraints: • When the indicator is a noun, the argument has to be a modifier or complement of the indicator. • When the indicator is a verb, the argument has to be the subject or the object if it is an NP, or a modifier or complement of the indicator if </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 423– 430, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Konstas</author>
<author>Mirella Lapata</author>
</authors>
<title>Conceptto-text generation via discriminative reranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>369--378</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4390" citStr="Konstas and Lapata, 2012" startWordPosition="683" endWordPosition="686">pts. On the contrary, the manually composed summaries (abstracts) are more compact and readable, and are written in a distinctly non-conversational style. 1395 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1395–1405, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows us to identify and reformula</context>
<context position="11106" citStr="Konstas and Lapata, 2012" startWordPosition="1731" endWordPosition="1734"> A statistical ranker subsequently selects one best abstract per relation instance (see Section 5.2). The post-selection component reduces the redundancy and outputs the final summary (see Section 5.3). determined by an external ontology. Liu and Liu (2009) apply sentence compression on extracted summary utterances. Though some of the unnecessary words are dropped, the resulting compressions can still be ungrammatical and unstructured. This work is also broadly related to expert system-based language generation (Reiter and Dale, 2000) and concept-to-text generation tasks (Angeli et al., 2010; Konstas and Lapata, 2012), where the generation process is decomposed into content selection (or text planning) and surface realization. For instance, Angeli et al. (2010) learn from structured database records and parallel textual descriptions. They generate texts based on a series of decisions made to select the records, fields, and proper templates for rendering. Those techniques that are tailored to specific domains (e.g. weather forecasts or sportcastings) cannot be directly applied to the conversational data, as their input is well-structured and the templates learned are domain-specific. 3 Framework Our domain-</context>
<context position="33645" citStr="Konstas and Lapata, 2012" startWordPosition="5419" endWordPosition="5422">valuation among abstracts produced by different annotators (Dang, 2005). The intrinsic difference of styles between dialogue and human abstract further lowers the scores. But the trend is still respected among the systems. Abstract Generation Evaluation. To evaluate the full abstract generation system, the BLEU score (Papineni et al., 2002) (the precision of unigrams and bigrams with a brevity penalty) is computed with human abstracts as reference. BLEU has a fairly good agreement with human judgement and has been used to evaluate a variety of language generation systems (Angeli et al., 2010; Konstas and Lapata, 2012). 4We use SVMlight (Joachims, 1999) with RBF kernel by default parameters for SVM-based classifiers and regressor. 5The four types of meetings in AMI are: project kick-off (35 meetings), functional design (35 meetings), conceptual design (35 meetings), and detailed design (34 meetings). Figure 5: Full abstract generation system evaluation by using BLEU (multiplied by 100). SVM-DA denotes for supervised extractive methods with SVMs on utterance-level. We are not aware of any existing work generating abstractive summaries for conversations. Therefore, we compare our full system against a supervi</context>
</contexts>
<marker>Konstas, Lapata, 2012</marker>
<rawString>Ioannis Konstas and Mirella Lapata. 2012. Conceptto-text generation via discriminative reranking. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 369–378, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="37972" citStr="Landis and Koch, 1977" startWordPosition="6166" endWordPosition="6169">ed to the judges. Five native speaking Ph.D. students (none are authors) performed the task. We carry out an one-way Analysis of Variance which shows significant differences in score as a function of system (p &lt; 0.05, paired t-test). Results in Table 4 demonstrate that our system summaries are significantly more compact and fluent than the extract-based method (p &lt; 0.05) while semantic correctness is comparable. The judges also rank the three summaries in terms of the overall quality in content, conciseness and grammaticality. An inter-rater agreement of Fleiss’s κ = 0.45 (moderate agreement (Landis and Koch, 1977)) was computed. Judges selected our system as the best system in 62.3% scenarios (IN-DOMAIN: 35.6%, OUT-OF-DOMAIN: 26.7%). Sample summaries are exhibited in Figure 6. 8 Conclusion We presented a domain-independent abstract generation framework for focused meeting summarization. Experimental results on two disparate meeting corpora show that our system can uniDecision Summary: Human: The remote will have push buttons outside, and an LCD and spinning wheel inside. Our System (In): The group decide to use an LCD display with a spinning wheel. There will be push-buttons on the outside. Our System </context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J R Landis and G G Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>912--920</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="28321" citStr="Lin and Bilmes, 2010" startWordPosition="4538" endWordPosition="4541"> we identify the summary-worthy words subsumed by (indtran k , argtran k ) that also appear in a. If those words are all subsumed by (indtar,abs k , argtar,abs k ), then it is considered to be a positive sample, and a negative sample otherwise. Table 2 displays the features used in abstract ranking. 5.3 Post-Selection: Redundancy Handling. Post-selection aims to maximize the information coverage and minimize the redundancy of the summary. Given the generated abstracts A = 1400 Algorithm 1: Greedy algorithm for postselection to generate the final summary. {absi}Ni=1, we use a greedy algorithm (Lin and Bilmes, 2010) to select a subset A&apos;, where A&apos; ⊆ A, to form the final summary. We define wij as the unigram similarity between abstracts absi and absj, C(absi) as the number of words in absi. We employ the following objective function: f(A, G) = � �absj G wi,j, G C A absi A\G Algorithm 1 sequentially finds an abstract with the greatest ratio of objective function gain to length, and add it to the summary if the gain is non-negative. 6 Experimental Setup Corpora. Two disparate corpora are used for evaluation. The AMI meeting corpus (Mccowan et al., 2005) contains 139 scenario-driven meetings, where groups of</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 912–920, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>71--78</pages>
<contexts>
<context position="6308" citStr="Lin and Hovy, 2003" startWordPosition="985" endWordPosition="988">oblems, etc.), we also make initial tries for domain adaptation so that our summarization method does not need human-written abstracts for each new meeting domain (e.g. faculty meetings, theater group meetings, project group meetings). We instantiate the abstract generation framework on two corpora from disparate domains — the AMI Meeting Corpus (Mccowan et al., 2005) and ICSI Meeting Corpus (Janin et al., 2003) — and produce systems to generate focused summaries with regard to four types of meeting elements: DECISIONs, PROBLEMs, ACTION ITEMSs, and PROGRESS. Automatic evaluation (using ROUGE (Lin and Hovy, 2003) and BLEU (Papineni et al., 2002)) against manually generated focused summaries shows that our summarizers uniformly and statistically significantly outperform two baseline systems as well as a state-of-the-art supervised extraction-based system. Human evaluation also indicates that the abstractive summaries produced by our systems are more linguistically appealing than those of the utterance-level extraction-based system, preferring them over summaries from the extractionbased system of comparable semantic correctness (62.3% vs. 37.7%). Finally, we examine the generality of our model across d</context>
<context position="32049" citStr="Lin and Hovy, 2003" startWordPosition="5163" endWordPosition="5166">mpty set); U A; while U =� Φ do abs arg max f(A,GUabsi)−f(A,G); abs�,EU C(absi) if f(A, G U abs) − f(A, G) ≥ 0 then G G U abs; end U U \ abs; end 1401 tures as our system (see Table 1) to identify the important DAs (no syntax features) (Xie et al., 2008; Sandu et al., 2010) or tokens (Fern´andez et al., 2008) to include into the summary4. Oracle. We compute an oracle consisting of the words from the DA cluster that also appear in the associated abstract to reflect the gap between the best possible extracts and the human abstracts. 7 Results Content Selection Evaluation. We first employ ROUGE (Lin and Hovy, 2003) to evaluate the content selection component with respect to the human written abstracts. ROUGE computes the ngram overlapping between the system summaries with the reference summaries, and has been used for both text and speech summarization (Dang, 2005; Xie et al., 2008). We report ROUGE-2 (R2) and ROUGE-SU4 (R-SU4) that are shown to correlate with human evaluation reasonably well. In AMI, four meetings of different functions are carried out in each group5. 35 meetings for “conceptual design” are randomly selected for testing. For ICSI, we reserve 12 meetings for testing. The R-SU4 scores fo</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>From extractive to abstractive meeting summaries: can it be done by sentence compression?</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLIJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>261--264</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3207" citStr="Liu and Liu, 2009" startWordPosition="507" endWordPosition="510">ons being in the shape of fruit though. D: Maybe make it like fruity colours or something. C: The power button could be like a big apple or something. D: Um like I’m just thinking bright colours. Problem Abstract (Summary): How to incorporate a fruit and vegetable theme into the remote. Figure 1: Clips from the AMI meeting corpus (Mccowan et al., 2005). A, B, C and D refer to distinct speakers. Also shown is the gold-standard (manual) abstract (summary) for the decision and the problem. to the noisy, fragmented, ungrammatical and unstructured text of meeting transcripts (Murray et al., 2010b; Liu and Liu, 2009). In contrast, human-written meeting summaries are typically in the form of abstracts — distillations of the original conversation written in new language. A user study from Murray et al. (2010b) showed that people demonstrate a strong preference for abstractive summaries over extracts when the text to be summarized is conversational. Consider, for example, the two types of focused summary along with their associated dialogue snippets in Figure 1. We can see that extracts are likely to include unnecessary and noisy information from the meeting transcripts. On the contrary, the manually compose</context>
<context position="10738" citStr="Liu and Liu (2009)" startWordPosition="1677" endWordPosition="1680">r of meeting-item-specific dialogue acts, from which one focused summary is constructed. Sample relation instances are denoted in bold (The indicators are further italicized and the arguments are in [brackets]). Summary-worthy relation instances are identified by content selection module (see Section 4) and then filled into the learned templates individually. A statistical ranker subsequently selects one best abstract per relation instance (see Section 5.2). The post-selection component reduces the redundancy and outputs the final summary (see Section 5.3). determined by an external ontology. Liu and Liu (2009) apply sentence compression on extracted summary utterances. Though some of the unnecessary words are dropped, the resulting compressions can still be ungrammatical and unstructured. This work is also broadly related to expert system-based language generation (Reiter and Dale, 2000) and concept-to-text generation tasks (Angeli et al., 2010; Konstas and Lapata, 2012), where the generation process is decomposed into content selection (or text planning) and surface realization. For instance, Angeli et al. (2010) learn from structured database records and parallel textual descriptions. They genera</context>
</contexts>
<marker>Liu, Liu, 2009</marker>
<rawString>Fei Liu and Yang Liu. 2009. From extractive to abstractive meeting summaries: can it be done by sentence compression? In Proceedings of the ACLIJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 261–264, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mccowan</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The ami meeting corpus. In In:</title>
<date>2005</date>
<booktitle>Proceedings Measuring Behavior 2005, 5th International Conference on Methods and Techniques in Behavioral Research. L.P.J.J.</booktitle>
<contexts>
<context position="2943" citStr="Mccowan et al., 2005" startWordPosition="463" endWordPosition="467">be uh a definite yes, C: We’re having push-buttons on the outside C: and then on the inside an LCD with spinning wheel, Decision Abstract (Summary): The remote will have push buttons outside, and an LCD and spinning wheel inside. A: and um I’m not sure about the buttons being in the shape of fruit though. D: Maybe make it like fruity colours or something. C: The power button could be like a big apple or something. D: Um like I’m just thinking bright colours. Problem Abstract (Summary): How to incorporate a fruit and vegetable theme into the remote. Figure 1: Clips from the AMI meeting corpus (Mccowan et al., 2005). A, B, C and D refer to distinct speakers. Also shown is the gold-standard (manual) abstract (summary) for the decision and the problem. to the noisy, fragmented, ungrammatical and unstructured text of meeting transcripts (Murray et al., 2010b; Liu and Liu, 2009). In contrast, human-written meeting summaries are typically in the form of abstracts — distillations of the original conversation written in new language. A user study from Murray et al. (2010b) showed that people demonstrate a strong preference for abstractive summaries over extracts when the text to be summarized is conversational.</context>
<context position="6059" citStr="Mccowan et al., 2005" startWordPosition="944" endWordPosition="947">matics to identify equivalent fragments of DNAs (Durbin et al., 1998) and has also been employed for learning paraphrases (Barzilay and Lee, 2003). • Although our framework requires labeled training data for each type of focused summary (decisions, problems, etc.), we also make initial tries for domain adaptation so that our summarization method does not need human-written abstracts for each new meeting domain (e.g. faculty meetings, theater group meetings, project group meetings). We instantiate the abstract generation framework on two corpora from disparate domains — the AMI Meeting Corpus (Mccowan et al., 2005) and ICSI Meeting Corpus (Janin et al., 2003) — and produce systems to generate focused summaries with regard to four types of meeting elements: DECISIONs, PROBLEMs, ACTION ITEMSs, and PROGRESS. Automatic evaluation (using ROUGE (Lin and Hovy, 2003) and BLEU (Papineni et al., 2002)) against manually generated focused summaries shows that our summarizers uniformly and statistically significantly outperform two baseline systems as well as a state-of-the-art supervised extraction-based system. Human evaluation also indicates that the abstractive summaries produced by our systems are more linguist</context>
<context position="28866" citStr="Mccowan et al., 2005" startWordPosition="4636" endWordPosition="4639"> final summary. {absi}Ni=1, we use a greedy algorithm (Lin and Bilmes, 2010) to select a subset A&apos;, where A&apos; ⊆ A, to form the final summary. We define wij as the unigram similarity between abstracts absi and absj, C(absi) as the number of words in absi. We employ the following objective function: f(A, G) = � �absj G wi,j, G C A absi A\G Algorithm 1 sequentially finds an abstract with the greatest ratio of objective function gain to length, and add it to the summary if the gain is non-negative. 6 Experimental Setup Corpora. Two disparate corpora are used for evaluation. The AMI meeting corpus (Mccowan et al., 2005) contains 139 scenario-driven meetings, where groups of four people participate in a series of four meetings for a fictitious project of designing remote control. The ICSI meeting corpus (Janin et al., 2003) consists of 75 naturally occurring meetings, each of them has 4 to 10 participants. Compared to the fabricated topics in AMI, the conversations in ICSI tend to be specialized and technical, e.g. discussion about speech and language technology. We use 57 meetings in ICSI and 139 meetings in AMI that include a short (usually one-sentence), manually constructed abstract summarizing each impor</context>
</contexts>
<marker>Mccowan, Lathoud, Lincoln, Lisowska, Post, Reidsma, Wellner, 2005</marker>
<rawString>I. Mccowan, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and P. Wellner. 2005. The ami meeting corpus. In In: Proceedings Measuring Behavior 2005, 5th International Conference on Methods and Techniques in Behavioral Research. L.P.J.J. Noldus, F. Grieco, L.W.S. Loijens and P.H. Zimmerman (Eds.), Wageningen: Noldus Information Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Steve Renals</author>
<author>Jean Carletta</author>
</authors>
<title>Extractive summarization of meeting recordings.</title>
<date>2005</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>593--596</pages>
<contexts>
<context position="7997" citStr="Murray et al., 2005" startWordPosition="1239" endWordPosition="1242">zation attempts to generate summaries for full dialogues (Carenini et al., 2011). Only recently has the task of focused summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation step according to their type, such as a decision or problem. Then an integer linear programming approach is employed to select the utterances that cover more entities as 1396 Relation Extraction Relation Instances: &lt;want, an LCD display with a spinning wheel&gt; &lt;an LCD display, with a spinning wheel&gt; &lt;have, some pus</context>
</contexts>
<marker>Murray, Renals, Carletta, 2005</marker>
<rawString>Gabriel Murray, Steve Renals, and Jean Carletta. 2005. Extractive summarization of meeting recordings. In INTERSPEECH, pages 593–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
</authors>
<title>Interpretation and transformation for abstracting conversations. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>894--902</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3186" citStr="Murray et al., 2010" startWordPosition="503" endWordPosition="506">ot sure about the buttons being in the shape of fruit though. D: Maybe make it like fruity colours or something. C: The power button could be like a big apple or something. D: Um like I’m just thinking bright colours. Problem Abstract (Summary): How to incorporate a fruit and vegetable theme into the remote. Figure 1: Clips from the AMI meeting corpus (Mccowan et al., 2005). A, B, C and D refer to distinct speakers. Also shown is the gold-standard (manual) abstract (summary) for the decision and the problem. to the noisy, fragmented, ungrammatical and unstructured text of meeting transcripts (Murray et al., 2010b; Liu and Liu, 2009). In contrast, human-written meeting summaries are typically in the form of abstracts — distillations of the original conversation written in new language. A user study from Murray et al. (2010b) showed that people demonstrate a strong preference for abstractive summaries over extracts when the text to be summarized is conversational. Consider, for example, the two types of focused summary along with their associated dialogue snippets in Figure 1. We can see that extracts are likely to include unnecessary and noisy information from the meeting transcripts. On the contrary,</context>
<context position="8116" citStr="Murray et al. (2010" startWordPosition="1257" endWordPosition="1260"> summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation step according to their type, such as a decision or problem. Then an integer linear programming approach is employed to select the utterances that cover more entities as 1396 Relation Extraction Relation Instances: &lt;want, an LCD display with a spinning wheel&gt; &lt;an LCD display, with a spinning wheel&gt; &lt;have, some push-buttons&gt; &lt;having, push-buttons on the outside&gt; &lt;push-buttons, on the outside&gt; &lt;an LCD, with spinning wheel&gt; ... (othe</context>
</contexts>
<marker>Murray, Carenini, Ng, 2010</marker>
<rawString>Gabriel Murray, Giuseppe Carenini, and Raymond Ng. 2010a. Interpretation and transformation for abstracting conversations. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 894–902, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
</authors>
<title>Generating and validating abstracts of meeting conversations: a user study.</title>
<date>2010</date>
<booktitle>In INLG.</booktitle>
<contexts>
<context position="3186" citStr="Murray et al., 2010" startWordPosition="503" endWordPosition="506">ot sure about the buttons being in the shape of fruit though. D: Maybe make it like fruity colours or something. C: The power button could be like a big apple or something. D: Um like I’m just thinking bright colours. Problem Abstract (Summary): How to incorporate a fruit and vegetable theme into the remote. Figure 1: Clips from the AMI meeting corpus (Mccowan et al., 2005). A, B, C and D refer to distinct speakers. Also shown is the gold-standard (manual) abstract (summary) for the decision and the problem. to the noisy, fragmented, ungrammatical and unstructured text of meeting transcripts (Murray et al., 2010b; Liu and Liu, 2009). In contrast, human-written meeting summaries are typically in the form of abstracts — distillations of the original conversation written in new language. A user study from Murray et al. (2010b) showed that people demonstrate a strong preference for abstractive summaries over extracts when the text to be summarized is conversational. Consider, for example, the two types of focused summary along with their associated dialogue snippets in Figure 1. We can see that extracts are likely to include unnecessary and noisy information from the meeting transcripts. On the contrary,</context>
<context position="8116" citStr="Murray et al. (2010" startWordPosition="1257" endWordPosition="1260"> summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation step according to their type, such as a decision or problem. Then an integer linear programming approach is employed to select the utterances that cover more entities as 1396 Relation Extraction Relation Instances: &lt;want, an LCD display with a spinning wheel&gt; &lt;an LCD display, with a spinning wheel&gt; &lt;have, some push-buttons&gt; &lt;having, push-buttons on the outside&gt; &lt;push-buttons, on the outside&gt; &lt;an LCD, with spinning wheel&gt; ... (othe</context>
</contexts>
<marker>Murray, Carenini, Ng, 2010</marker>
<rawString>Gabriel Murray, Giuseppe Carenini, and Raymond T. Ng. 2010b. Generating and validating abstracts of meeting conversations: a user study. In INLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Needleman</author>
<author>C D Wunsch</author>
</authors>
<title>A general method applicable to the search for similarities in the amino acid sequence of two proteins.</title>
<date>1970</date>
<journal>Journal of molecular biology,</journal>
<volume>48</volume>
<issue>3</issue>
<contexts>
<context position="20468" citStr="Needleman and Wunsch, 1970" startWordPosition="3233" endWordPosition="3236"> fewer than three abstracts are removed1. Learning the Templates via MSA. For learning the structural patterns among the abstracts, Multiple-Sequence Alignment (MSA) is first computed for each cluster. MSA takes as input multiple sentences and one scoring function to measure the similarity between any two words. For insertions or deletions, a gap cost is also added. MSA can thus find the best way to align the sequences with insertions or deletions in accordance with the scorer. However, computing an optimal MSA is NP-complete (Wang and Jiang, 1994), thus we implement an approximate algorithm (Needleman and Wunsch, 1970) that iteratively aligns two sequences each time and treats the resulting alignment as a new sequence2. Figure 3 demonstrates an MSA computed from a sample cluster of ab1Clustering stops when the similarity between any pairwise clusters is below 5. This is applied to every type of summarization. We tune the parameter on a small held-out development set by manually evaluating the induced templates. No significant change is observed within a small range. 2We adopt the scoring function for MSA from Barzilay and Lee (2003), where aligning two identical words scores 1, inserting a gap scores −0.01,</context>
</contexts>
<marker>Needleman, Wunsch, 1970</marker>
<rawString>S. B. Needleman and C. D. Wunsch. 1970. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of molecular biology, 48(3):443–453, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6341" citStr="Papineni et al., 2002" startWordPosition="991" endWordPosition="994">itial tries for domain adaptation so that our summarization method does not need human-written abstracts for each new meeting domain (e.g. faculty meetings, theater group meetings, project group meetings). We instantiate the abstract generation framework on two corpora from disparate domains — the AMI Meeting Corpus (Mccowan et al., 2005) and ICSI Meeting Corpus (Janin et al., 2003) — and produce systems to generate focused summaries with regard to four types of meeting elements: DECISIONs, PROBLEMs, ACTION ITEMSs, and PROGRESS. Automatic evaluation (using ROUGE (Lin and Hovy, 2003) and BLEU (Papineni et al., 2002)) against manually generated focused summaries shows that our summarizers uniformly and statistically significantly outperform two baseline systems as well as a state-of-the-art supervised extraction-based system. Human evaluation also indicates that the abstractive summaries produced by our systems are more linguistically appealing than those of the utterance-level extraction-based system, preferring them over summaries from the extractionbased system of comparable semantic correctness (62.3% vs. 37.7%). Finally, we examine the generality of our model across domains for two types of focused s</context>
<context position="33362" citStr="Papineni et al., 2002" startWordPosition="5370" endWordPosition="5373">baselines and supervised systems. The learning curve of our system is relatively flat, which means not many training meetings are required to reach a usable performance level. Note that the ROUGE scores are relative low when the reference summaries are human abstracts, even for evaluation among abstracts produced by different annotators (Dang, 2005). The intrinsic difference of styles between dialogue and human abstract further lowers the scores. But the trend is still respected among the systems. Abstract Generation Evaluation. To evaluate the full abstract generation system, the BLEU score (Papineni et al., 2002) (the precision of unigrams and bigrams with a brevity penalty) is computed with human abstracts as reference. BLEU has a fairly good agreement with human judgement and has been used to evaluate a variety of language generation systems (Angeli et al., 2010; Konstas and Lapata, 2012). 4We use SVMlight (Joachims, 1999) with RBF kernel by default parameters for SVM-based classifiers and regressor. 5The four types of meetings in AMI are: project kick-off (35 meetings), functional design (35 meetings), conceptual design (35 meetings), and detailed design (34 meetings). Figure 5: Full abstract gener</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building natural language generation systems.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11021" citStr="Reiter and Dale, 2000" startWordPosition="1719" endWordPosition="1722">on module (see Section 4) and then filled into the learned templates individually. A statistical ranker subsequently selects one best abstract per relation instance (see Section 5.2). The post-selection component reduces the redundancy and outputs the final summary (see Section 5.3). determined by an external ontology. Liu and Liu (2009) apply sentence compression on extracted summary utterances. Though some of the unnecessary words are dropped, the resulting compressions can still be ungrammatical and unstructured. This work is also broadly related to expert system-based language generation (Reiter and Dale, 2000) and concept-to-text generation tasks (Angeli et al., 2010; Konstas and Lapata, 2012), where the generation process is decomposed into content selection (or text planning) and surface realization. For instance, Angeli et al. (2010) learn from structured database records and parallel textual descriptions. They generate texts based on a series of decisions made to select the records, fields, and proper templates for rendering. Those techniques that are tailored to specific domains (e.g. weather forecasts or sportcastings) cannot be directly applied to the conversational data, as their input is w</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Korbinian Riedhammer</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Long story short - global unsupervised models for keyphrase based meeting summarization.</title>
<date>2010</date>
<journal>Speech Commun.,</journal>
<volume>52</volume>
<issue>10</issue>
<marker>Riedhammer, Favre, Hakkani-T¨ur, 2010</marker>
<rawString>Korbinian Riedhammer, Benoit Favre, and Dilek Hakkani-T¨ur. 2010. Long story short - global unsupervised models for keyphrase based meeting summarization. Speech Commun., 52(10):801–815, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oana Sandu</author>
<author>Giuseppe Carenini</author>
<author>Gabriel Murray</author>
<author>Raymond Ng</author>
</authors>
<title>Domain adaptation to summarize human conversations.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, DANLP 2010,</booktitle>
<pages>16--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="31704" citStr="Sandu et al., 2010" startWordPosition="5106" endWordPosition="5109"> single utterance. Supervised Learning. We also compare our approach to two supervised extractive summarization methods — Support Vector Machines (Joachims, 1998) trained with the same feaInput : relation instances R = {(indi, argi�}Ni=1, generated abstracts A = {absi}Ni=1, objective function f , cost function C Output: final abstract G G Φ (empty set); U A; while U =� Φ do abs arg max f(A,GUabsi)−f(A,G); abs�,EU C(absi) if f(A, G U abs) − f(A, G) ≥ 0 then G G U abs; end U U \ abs; end 1401 tures as our system (see Table 1) to identify the important DAs (no syntax features) (Xie et al., 2008; Sandu et al., 2010) or tokens (Fern´andez et al., 2008) to include into the summary4. Oracle. We compute an oracle consisting of the words from the DA cluster that also appear in the associated abstract to reflect the gap between the best possible extracts and the human abstracts. 7 Results Content Selection Evaluation. We first employ ROUGE (Lin and Hovy, 2003) to evaluate the content selection component with respect to the human written abstracts. ROUGE computes the ngram overlapping between the system summaries with the reference summaries, and has been used for both text and speech summarization (Dang, 2005;</context>
</contexts>
<marker>Sandu, Carenini, Murray, Ng, 2010</marker>
<rawString>Oana Sandu, Giuseppe Carenini, Gabriel Murray, and Raymond Ng. 2010. Domain adaptation to summarize human conversations. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, DANLP 2010, pages 16–22, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>2004</date>
<journal>Statistics and Computing,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>Smola, Sch¨olkopf, 2004</marker>
<rawString>Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial on support vector regression. Statistics and Computing, 14(3):199–222, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, USA.</location>
<contexts>
<context position="27212" citStr="Stolcke, 2002" startWordPosition="4347" endWordPosition="4348">verb? realization starts with verb? realization has adjacent verbs/NPs? indsrc precedes/succeeds argsrc? indtar precedes/succeeds argtar? above 2 features have same value? Language Model Features log pLM (first word in indtran kprevious 1/2 words) log pLM (realization) log pLM (first word in argtran kIprevious 1/2 words) log pLM (realization)/length log pLM (next word last 1/2 words in indtran k ) log pLM (next word last 1/2 words in argtran k ) Table 2: Features for abstracts ranking. The language model features are based on a 5-gram language model trained on Gigaword (Graff, 2003) by SRILM (Stolcke, 2002). constructed according to the transformation function F mentioned above. Each sample is represented as: (hindsrc, argsrci, hindtran k ,argtran k ,indtar k ,argtar k i, t, a) where (indsrc, argsrc) is the source pair, (indtran k , argtran k ) is the transformed pair, (indtar k , argtar k ) is the target pair in template t, and a is the abstract parallel to t. We first find (indtar,abs k , argtar,abs k ), which is the corresponding constituent pair of (indtar k , argtar k ) in a. Then we identify the summary-worthy words subsumed by (indtran k , argtran k ) that also appear in a. If those words</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of ICSLP, volume 2, pages 901–904, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Owen Rambow</author>
<author>Monica Rogati</author>
</authors>
<title>Spot: a trainable sentence planner.</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL ’01,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4659" citStr="Walker et al., 2001" startWordPosition="725" endWordPosition="728">lgaria, August 4-9 2013. c�2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows us to identify and reformulate the important information for the focused summary. Our contributions are as follows: • To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings. • We present a novel template extraction a</context>
<context position="22629" citStr="Walker et al., 2001" startWordPosition="3592" endWordPosition="3595">emplates, which would require a significant amount of manual evaluation. Instead, they are evaluated extrinsically. We encode the templates as features (Angeli et al., 2010) that could be selected or ignored in the succeeding abstract ranking model. 5.2 Template Filling An Overgenerate-and-Rank Approach. Since filling the relation instances into templates of distinct structures may result in abstracts of varying quality, we rank the abstracts based on the features of the template, the transformation conducted, and the generated abstract. This is realized by the Overgenerate-and-Rank strategy (Walker et al., 2001; Heilman and Smith, 2010). It takes as input a set of relation instances (from the same cluster) R = {(indi, argi)}Ni=1 that are produced by content selection component, a set of templates T = {tj}Mj=1 that are represented as parsing trees, a transformation function F (described below), and a statistical ranker S for ranking the generated abstracts, for which we defer description later in this Section. For each (indi, argi), the overgenerate-andrank approach fills it into each template in T by applying F to generate all possible abstracts. Then the ranker S selects the best abstract absi. Pos</context>
</contexts>
<marker>Walker, Rambow, Rogati, 2001</marker>
<rawString>Marilyn A. Walker, Owen Rambow, and Monica Rogati. 2001. Spot: a trainable sentence planner. In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL ’01, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Wang</author>
<author>Claire Cardie</author>
</authors>
<title>Summarizing decisions in spoken meetings.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, WASDGML ’11,</booktitle>
<pages>16--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="30394" citStr="Wang and Cardie, 2011" startWordPosition="4880" endWordPosition="4883">lusterings setting, we use the annotations to create perfect partitions of the DAs for input to the system; in the System Figure 4: Content selection evaluation by using ROUGE-SU4 (multiplied by 100). SVM-DA and SVM-TOKEN denotes for supervised extract-based methods with SVMs on utterance- and token-level. Summaries for decision, problem, action item, and progress are generated and evaluated for AMI and ICSI (with names in parentheses). X-axis shows the number of meetings used for training. Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in (Wang and Cardie, 2011). DAs are grouped according to a classifier trained beforehand. Baselines and Comparisons. We compare our system with (1) two unsupervised baselines, (2) two supervised extractive approaches, and (3) an oracle derived from the gold standard abstracts. Baselines. As in Riedhammer et al. (2010), the LONGEST DA in each cluster is selected as the summary. The second baseline picks the cluster prototype (i.e. the DA with the largest TFIDF similarity with the cluster centroid) as the summary according to Wang and Cardie (2011). Although it is possible that important content is spread over multiple D</context>
</contexts>
<marker>Wang, Cardie, 2011</marker>
<rawString>Lu Wang and Claire Cardie. 2011. Summarizing decisions in spoken meetings. In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, WASDGML ’11, pages 16–24, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Wang</author>
<author>Claire Cardie</author>
</authors>
<title>Focused meeting summarization via unsupervised relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12,</booktitle>
<pages>304--313</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7781" citStr="Wang and Cardie (2012)" startWordPosition="1206" endWordPosition="1209">rom the same system trained on in-domain data, and statistically significantly outperform supervised extractive summarization approaches trained on in-domain data. 2 Related Work Most research on spoken dialogue summarization attempts to generate summaries for full dialogues (Carenini et al., 2011). Only recently has the task of focused summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation step according to their type, such as a decision or problem. Then an integer linear programming app</context>
<context position="14136" citStr="Wang and Cardie (2012)" startWordPosition="2189" endWordPosition="2192">d on information extraction methods that identify a lexical indicator (or trigger) that evokes a relation of interest and then employ syntactic information, often in conjunction with semantic constraints, to find the argument constituent(or target phrase) to be extracted. Rela1397 tion instances, then, are represented by indicatorargument pairs (Chen et al., 2011). For example, in the DA cluster of Figure 2, (want, an LCD display with a spinning wheel) and (push-buttons, on the outside) are two relation instances. Relation Instance Extraction We adopt and extend the syntactic constraints from Wang and Cardie (2012) to identify all relation instances in the input utterances; the summary-worthy ones will be selected by a discriminative classifier. Constituent and dependency parses are obtained by the Stanford parser (Klein and Manning, 2003). Both the indicator and argument take the form of constituents in the parse tree. We restrict the eligible indicator to be a noun or verb; the eligible arguments is a noun phrase (NP), prepositional phrase (PP) or adjectival phrase (ADJP). A valid indicator-argument pair should have at least one content word and satisfy one of the following constraints: • When the ind</context>
<context position="16781" citStr="Wang and Cardie, 2012" startWordPosition="2617" endWordPosition="2620">s DA indicator/argument only contains stopword? number of new nouns Content Features has capitalized word? has proper noun? TF/IDF/TFIDF min/max/average Discourse Features main speaker or not? is in an adjacency pair (AP)? is in the source/target of the AP? number of source/target DA in the AP is the target of the AP a positive/negative/neutral response? is the source of the AP a question? Syntax Features indicator/argument constituent tag dependency relation of indicator and argument Table 1: Features for content selection. Most are adapted from previous work (Galley, 2006; Xie et al., 2008; Wang and Cardie, 2012). Every basic or content feature is concatenated with the constituent tags of indicator and argument to compose a new one. Main speakers include the most talkative speaker (who has said the most words) and other speakers whose word count is more than 20% of the most talkative one (Xie et al., 2008). Adjacency pair (AP) (Galley, 2006) is an important conversational analysis concept; each AP consists of a source utterance and a target utterance produced by different speakers. 5.1 Template Extraction Sentence Clustering. Template extraction starts with clustering the sentences that constitute the</context>
</contexts>
<marker>Wang, Cardie, 2012</marker>
<rawString>Lu Wang and Claire Cardie. 2012. Focused meeting summarization via unsupervised relation extraction. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12, pages 304–313, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lusheng Wang</author>
<author>Tao Jiang</author>
</authors>
<title>On the complexity of multiple sequence alignment.</title>
<date>1994</date>
<journal>Journal of Computational Biology,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="20395" citStr="Wang and Jiang, 1994" startWordPosition="3223" endWordPosition="3226">ty metric based on word n-gram overlap (n = 1, 2, 3). Clusters with fewer than three abstracts are removed1. Learning the Templates via MSA. For learning the structural patterns among the abstracts, Multiple-Sequence Alignment (MSA) is first computed for each cluster. MSA takes as input multiple sentences and one scoring function to measure the similarity between any two words. For insertions or deletions, a gap cost is also added. MSA can thus find the best way to align the sequences with insertions or deletions in accordance with the scorer. However, computing an optimal MSA is NP-complete (Wang and Jiang, 1994), thus we implement an approximate algorithm (Needleman and Wunsch, 1970) that iteratively aligns two sequences each time and treats the resulting alignment as a new sequence2. Figure 3 demonstrates an MSA computed from a sample cluster of ab1Clustering stops when the similarity between any pairwise clusters is below 5. This is applied to every type of summarization. We tune the parameter on a small held-out development set by manually evaluating the induced templates. No significant change is observed within a small range. 2We adopt the scoring function for MSA from Barzilay and Lee (2003), w</context>
</contexts>
<marker>Wang, Jiang, 1994</marker>
<rawString>Lusheng Wang and Tao Jiang. 1994. On the complexity of multiple sequence alignment. Journal of Computational Biology, 1(4):337–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Yang Liu</author>
<author>Hui Lin</author>
</authors>
<title>Evaluating the effectiveness of features and sampling in extractive meeting summarization.</title>
<date>2008</date>
<booktitle>In in Proc. of IEEE Spoken Language Technology (SLT.</booktitle>
<contexts>
<context position="1829" citStr="Xie et al., 2008" startWordPosition="269" endWordPosition="272">ey refer to summaries of a specific aspect of a meeting, such as the DECISIONS reached, PROBLEMS discussed, PROGRESS made or ACTION ITEMS that emerged (Carenini et al., 2011). Our goal is to provide an automatic summarization system that can generate abstract-style focused meeting summaries to help users digest the vast amount of meeting content in an easy manner. Existing meeting summarization systems remain largely extractive: their summaries are comprised exclusively of patchworks of utterances selected directly from the meetings to be summarized (Riedhammer et al., 2010; Bui et al., 2009; Xie et al., 2008). Although relatively easy to construct, extractive approaches fall short of producing concise and readable summaries, largely due Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 cardie@cs.cornell.edu C: Looking at what we’ve got, we we want an LCD display with a spinning wheel. B: You have to have some push-buttons, don’t you? C: Just spinning and not scrolling, I would say. B: I think the spinning wheel is definitely very now. A: but since LCDs seems to be uh a definite yes, C: We’re having push-buttons on the outside C: and then on the inside an LCD with spi</context>
<context position="8015" citStr="Xie et al., 2008" startWordPosition="1243" endWordPosition="1246">nerate summaries for full dialogues (Carenini et al., 2011). Only recently has the task of focused summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation step according to their type, such as a decision or problem. Then an integer linear programming approach is employed to select the utterances that cover more entities as 1396 Relation Extraction Relation Instances: &lt;want, an LCD display with a spinning wheel&gt; &lt;an LCD display, with a spinning wheel&gt; &lt;have, some push-buttons&gt; &lt;having</context>
<context position="16757" citStr="Xie et al., 2008" startWordPosition="2613" endWordPosition="2616">re also in previous DA indicator/argument only contains stopword? number of new nouns Content Features has capitalized word? has proper noun? TF/IDF/TFIDF min/max/average Discourse Features main speaker or not? is in an adjacency pair (AP)? is in the source/target of the AP? number of source/target DA in the AP is the target of the AP a positive/negative/neutral response? is the source of the AP a question? Syntax Features indicator/argument constituent tag dependency relation of indicator and argument Table 1: Features for content selection. Most are adapted from previous work (Galley, 2006; Xie et al., 2008; Wang and Cardie, 2012). Every basic or content feature is concatenated with the constituent tags of indicator and argument to compose a new one. Main speakers include the most talkative speaker (who has said the most words) and other speakers whose word count is more than 20% of the most talkative one (Xie et al., 2008). Adjacency pair (AP) (Galley, 2006) is an important conversational analysis concept; each AP consists of a source utterance and a target utterance produced by different speakers. 5.1 Template Extraction Sentence Clustering. Template extraction starts with clustering the sente</context>
<context position="31683" citStr="Xie et al., 2008" startWordPosition="5102" endWordPosition="5105">re restricted to a single utterance. Supervised Learning. We also compare our approach to two supervised extractive summarization methods — Support Vector Machines (Joachims, 1998) trained with the same feaInput : relation instances R = {(indi, argi�}Ni=1, generated abstracts A = {absi}Ni=1, objective function f , cost function C Output: final abstract G G Φ (empty set); U A; while U =� Φ do abs arg max f(A,GUabsi)−f(A,G); abs�,EU C(absi) if f(A, G U abs) − f(A, G) ≥ 0 then G G U abs; end U U \ abs; end 1401 tures as our system (see Table 1) to identify the important DAs (no syntax features) (Xie et al., 2008; Sandu et al., 2010) or tokens (Fern´andez et al., 2008) to include into the summary4. Oracle. We compute an oracle consisting of the words from the DA cluster that also appear in the associated abstract to reflect the gap between the best possible extracts and the human abstracts. 7 Results Content Selection Evaluation. We first employ ROUGE (Lin and Hovy, 2003) to evaluate the content selection component with respect to the human written abstracts. ROUGE computes the ngram overlapping between the system summaries with the reference summaries, and has been used for both text and speech summa</context>
</contexts>
<marker>Xie, Liu, Lin, 2008</marker>
<rawString>Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating the effectiveness of features and sampling in extractive meeting summarization. In in Proc. of IEEE Spoken Language Technology (SLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>