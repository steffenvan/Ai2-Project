<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000442">
<note confidence="0.827234">
MOVEMENT IN ACTIVE PRODUCTION NETWORKS
Mark A. Jones
Alan S. Driscoll
AT&amp;T Bell Laboratories
Murray Hill, New Jersey 07974
</note>
<sectionHeader confidence="0.676385" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999353888888889">
We describe how movement is handled in a class of
computational devices called active production networks
(APNs). The APN model is a parallel, activation-based
framework that has been applied to other aspects of
natural language processing. The model is briefly defined,
the notation and mechanism for movement is explained,
and then several examples are given which illustrate how
various conditions on movement can naturally be explained
in terms of limitations of the APN device.
</bodyText>
<sectionHeader confidence="0.987505" genericHeader="introduction">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.930108933333333">
Movement is an important phenomenon in natural
languages. Recently, proposals such as Gazdar&apos;s derived
rules (Gazdar, 1982) and Pereira&apos;s extraposition grammars
(Pereira, 1983) have attempted to find minimal extensions
to the context-free framework that would allow the descrip-
tion of movement. In this paper, we describe a class of
computational devices for natural language processing,
called active production networks (APNs), and explore
how certain kinds of movement are handled. In particular,
we are concerned with left extraposition, such as Subject.
auxiliary Inversion, Wh-movement, and NP holes in rela-
tive clauses. In these cases, the extraposed constituent
leaves a trace which is inserted at a later point in the pro-
cessing. This paper builds on the research reported in
Jones (1983) and Jones (forthcoming).
</bodyText>
<sectionHeader confidence="0.988096" genericHeader="method">
2. ACTIVE PRODUCTION NETWORKS
</sectionHeader>
<subsectionHeader confidence="0.829199">
2.1 The Device
</subsectionHeader>
<bodyText confidence="0.933978636363637">
Our contention is that only a class of parallel devices
will prove to be powerful enough to allow broad contextual
priming, to pursue alternative hypotheses, and to explain
the paradox that the performance of a sequential system
often degrades with new knowledge, whereas human per-
formance usually improves with learning and experience.&apos;
There are a number of new parallel processing (connection-
ist) models which are sympathetic to this view—Anderson
(1983). Feldman and Ballard (1982), Waltz and Pollack
(1985), McClelland and Rumelhart (1981, 1982), and
Fahlman, Hinton and Sejnowski (1983).
Many of the connectionist models use iterative relaxa-
tion techniques with networks containing excitatory and
inhibitory links. They have primarily been used as best-fit
categorizers in large recognition spaces, and it is not yet
clear how they will implement the rule-governed behavior
of parsers or problem solvers. Rule-based systems need a
strong notion of an operating state, and they depend
heavily on appropriate variable binding schemes for opera-
tions such as matching (e.g.. unification) and recursion.
The APN model directly supports a rule-based interpreta-
tion, while retaining much of the general flavor of
I. lbs human ability to perform computationally expensive operations using
reetney slow, parallel Meese reinforce MO oche.
connectionism. An active production network is a rule-
oriented, distributed processing system based on the follow-
ing principles:
1. Each node in the network executes a uniform activa-
tion algorithm and assumes states in response to mes-
sages (such as expectation, inhibition, and activation)
that arrive locally; the node can, in turn, relay mes-
sages, initiate messages, and spawn new instances to
process message activity. Although the patterns that
define a node&apos;s behavior may be quite idiosyncratic or
specialized, the algorithm that interprets the pattern
is the same for each node in the network.
2. Messages are relatively simple. They have an associ-
ated time, strength, and purpose (e.g., to post an
expectation). They do not encode complex structures
such as entire binding lists, parse trees, feature lists,
or meaning representations.2 Consequently, no struc-
ture is explicitly built; the &amp;quot;result&amp;quot; of a computation
consists entirely of the activation trace and the new
state of the network.
Figure 1 gives an artificial: but comprehensive example
of an APN grammar in graphical form. The grammar
generates the strings—a, b, acd. ace. bcd, bce. fg and gf—
and illustrates many of the pattern language features and
grammar writing paradigms. The network responds to
sources which activate the network at its leaves. Activa-
tion messages spread &amp;quot;upward&amp;quot; through the network. At
conjunctive nodes (seri and and), expectation messages are
posted for the legal continuations of the pattern; inhibition
messages are sent down previous links when new activa-
tions are recorded.
</bodyText>
<figureCaption confidence="0.897277">
Figure I. A Sample APN
</figureCaption>
<bodyText confidence="0.854676142857143">
In parsing applications, partially instantiated nodes are
viewed as phrase structure rules whose next constituent is
expected. The sources primarily arise from exogenous
2. For • similar contortionist view, see Feldman and Ballard (1982) or
Waltz and Pollack (1985). A comparison of marker posing. value
passing and unrestricted message pawing systems is given in Fahken.
Hinson and SMnowski (1983).
</bodyText>
<page confidence="0.998711">
161
</page>
<tableCaption confidence="0.663296642857143">
strobings of the network by external inputs. In generation
or problem solving applications, partially instantiated nodes
are viewed as partially satisfied goals which have outstand-
ing subgoals whose solutions are desired. The sources in
this case are endogenously generated. The compatibility of
these two views not only allows the same network to be
used for both parsing and generation, but also permits
processes to share in the interaction of internal and exter-
nal sources of information. This compatibility, somewhat
surprisingly, turned out to be crucial to our treatment of
movement, but it is also clearly desirable for other aspects
of natural language processing in which parsing and prob-
lem solving interact (e.g., reference resolution and infer-
ence).
</tableCaption>
<subsectionHeader confidence="0.998253">
2.2 ft Patten Language
</subsectionHeader>
<bodyText confidence="0.9992922">
Each node in an APN is defined by a pattern, written
in the pattern language of Figure 2. A pattern describes
the messages to which a node responds, and the new mes-
sages and internal states that are produced. Each subpat-
tern of the form (S v binding-pat) in the pattern for node
N is a variable binding site; a variable binding takes place
when an instance of a node in binding-pat activates a
reference to variable v of node N. Implicitly, a pattern
defines the set of states and, state transitions for a node.
The ? (optionality). + (repetition) and • (optional repeti-
tion) operators do not extend the expressiveness of the
language, but have been added for convenience. They can
be replaced in preprocessing by equivalent expressions.&apos;
Formal semantic definitions of the message passing
behavior for each primitive operator have been specified.
</bodyText>
<figure confidence="0.984697818181818">
pattern binding-site
I (seq pattern ...)
I (and pattern ...)
I (or pattern ...)
I (?pattern)
I (+ binding-site)
I (• binding-site)
binding-site (S var binding-pattern)
binding-pattern ::•• node
I (and binding-pattern ...)
I (or binding-pattern ...)
</figure>
<figureCaption confidence="0.99992">
Figure 2. The APN Pattern Language
</figureCaption>
<bodyText confidence="0.894253476190476">
An important distinction that the pattern language
makes is in the synchronicity of activation signals. The
pattern (and (S vi X) (S v2 Y)) requires that the activa-
tion from X and Y emanate from distinct network sources,
while the pattern (S v (and X Y)) insists that instances of
X and Y are activated from the same source. In the
3. The ClUICt chaos of operators in the pattern language is a Nomenhat
separate LION from the specthr.ation of the APN machine.,
1. The current APN motel allocates sources sequentiegy. The term
synchnxicity reflects the fact that the 1011f03 identity of two activatioe
menages can be locally computed from their time of arrivaL Thin corks
U long es the &apos;envenom promos rens feat enough to condition dm network
batwing serial source activations. Alternatively, activation manages could
carry the source identity as ea additional parameter; in this cue, source
activations me overlap. but at the possible risk of in inconsistent
expectation =rummest. For relatively independent teas, the overlap
may not ecee • problem.
graphical representation of an APN, synchrony is indicated
by a short tail above the subpattern expression; the
definition of U in Figure I illustrates both conventions:
(and (S vi (and TJ)) (S v2 g)).
</bodyText>
<subsectionHeader confidence="0.999963">
3.3 As Example
</subsectionHeader>
<bodyText confidence="0.996999636363636">
Figure 3 shows the stages in parsing the string acd. An
exogenous source Exog-srcil first activates a, which is not
currently supported by a source and. hence, is in an inac-
tive state. The activation of an inactive or inhibited node
gives rise to a new instance (00) to record the binding.
The instance is effectively a new node in the network, and
derives its pattern from the spawning node. The activation
spreads upward to the other instances shown in Figure
3(a). The labels on each node indicate the current activa-
tion level, represented as an integer between 0 and 9,
inclusive.
</bodyText>
<figure confidence="0.994693">
P0(9)
)
-
Q0(9) c 5
a0(9)
Exog-src0 ( 9 ) lExog-src
(a) trace structure after a
P0(4)
Q0 c0(4)
I
a0
Exog-src0 Exog-srcl ( 9 ) d e f
Exog-src
(b) trace structure after ac
P0( 9 )
RO ( 9)
c0 SO(9)
Exog-srcl
</figure>
<figureCaption confidence="0.994528">
(c) trace structure after acd
Figure 3. Stages in Parsing acd
</figureCaption>
<figure confidence="0.7090098">
10
a0
Exog-src0
dO ( 9 )
Exog-src2 ( 9 )
</figure>
<page confidence="0.982157">
162
</page>
<bodyText confidence="0.999806285714286">
The activation of a node causes its pattern to be
(re)instantiated and a variable to be (re)bound. For exam-
ple. in the activation of RO, the pattern (seq (S vi 0 (S
v2 CD is replaced by (seq (S vi (or Q QO)) (S v2 C)), and
the variable vi is bound to QO. For simplicity, only the
active links are shown in Figure 3. RO posts an expecta-
tion message for node C which can further its pattern.
The source Exog-src0 is said to be supporting the activa-
tion of nodes a0, Q0. RO and PO above it, and the expecta-
tions or inhibitions that are generated by these nodes. For
the current paper we will assume that exogenous sources
remain fully on for the duration of the sentence.&apos;
In Figure 3(b), another exogenous source Exog-srcl
activates c, which furthers the pattern for RO. RO sends an
inhibition message to QO, posts expectations for S. and
relays an activation message to PO, which rebinds its vari-
able to RO and assumes a new activation value. Figure
3(c) shows the final situation after d has been activated.
The synchronous conjunction of SO is satisfied by TO and
dO. RO is fully satisfied (activation value of 9), and PO is
re-satisfied.
</bodyText>
<subsectionHeader confidence="0.987664">
2.4 Gramm %Wag Paradigm
</subsectionHeader>
<bodyText confidence="0.998019315789474">
The APN in Figure I illustrates several grammar writ-
ing paradigms. The situation in which an initial prefix
string (a or b) satisfies a constituent (P), but can be fol-
lowed by optional suffix strings (cd or cc) occurs frequently
in natural language grammars. For example, noun phrase
heads in English have optional prenominal and postnominal
modifiers. The synchronous disjunction at P allows the
local role of a or b to change, while preserving its interpre-
tation as part of a P. It is also simple to encode optional
prefixes.
Another common situation in natural language gram-
mars is specialization of a constituent based on some inter-
nal feature. Noun phrases in English, for example, can be
specialized by case; verb phrases can be specialized as par-
ticipial, tensed or infinitive. In Figure I, node S is a spe-
cialization which represents &amp;quot;Ts with d-ness or e-ness. but
not f-ness.&amp;quot; The specialization is constructed by a synchro-
nous conjunction of features that arise from subtrees some-
where below the node to be specialized.
The APN model also provides for node outputs to be
partitioned into independent classes for the purposes of the
activation algorithm. The nodes in the classes form levels
in the network and represent orthogonal systems of
classification. The cascading of expectations from different
levels can implement context-sensitive behaviors such as
feature agreement and semantic sclectional restrictions.
This is described in Jones (forthcoming). In the next sec-
tion, we will introduce a grammar writing paradigm to
represent movement, another type of non-context-free
behavior.
S. It is interesting to speculate on the consequences of various relaxations of
this auumption. Fundamental iiMilatiON in the allocation at sources may
be related to limitations in shunt term memory (or buffer space in
deterministic mode* toe Marcus, 1980). Linguistic canatraints based on
constituent length could be related to source decay. Some syntactic
garden path behavior might be related to accelerated source decay caused
by inhibitioo from • competing ltypotbesie Anything more ha.. footnote
is premature at this time.
</bodyText>
<sectionHeader confidence="0.992198" genericHeader="method">
3. MOVEMENT
</sectionHeader>
<bodyText confidence="0.999505769230769">
From the APN perspective, movement (limited here to
left-extraposition) necessitates the endogenous reactivation
of a trace that was created earlier in the process. To cap-
ture the trace so that expectations for its reactivation can
be posted, we use the following type of rule: (seq (S vi
X... ) (S v2 ... (and X X-src Y) ...). When an instance,
XO, first activates this rule, vi is bound to XO: the second
occurrence X in the rule is constrained to match instances
of X0, and expectations for X0, X-src and Y are created.
No new exogenous source can satisfy the synchronous con-
junction; only an endogenous X-src can. The rule is simi-
lar to the notion of an X followed by a Y with an X hole in
it (cf. Gazdar, 1982).
</bodyText>
<figureCaption confidence="0.995144">
Figure 4. A Grammar for Relative Clauses
</figureCaption>
<bodyText confidence="0.993780368421053">
Figure 4 defines a grammar with an NP hole in a rela-
tive clause; other types of left-extraposition are handled
analogously. Our treatment of relatives is adapted from
Chomsky and Lasnik (1977). The movement rule for f is:
(seq (S vi (and Comp Rel (or Exog-src PRO-ire)) IS v2
(and Re( Re! -arc S))). The rule restricts the first instance
of Re to arise either from an exogenous relative pronoun
such as which or from an endogenously generated (phono-
logically null) pronoun PRO. The second variable is
satisfied when Re! -arc simultaneously reactivates a trace of
the Rel instance and inserts an NP-trace into an S.
It is instructive to consider how phonologically null pro-
nouns are inserted before we discuss how movement occurs
by trace insertion. The phrase, [NP the mouse ig PRO;
that ...II, illustrates how a relative pronoun PRO is
inserted. Figure 5(a) shows the network after parsing the
cat. When the complementizer that appears next in the
input, PRO-arc receives inhibition (marked by downward
arrows in Figure 5(b)) from Rel-Comp0. Non-exogenous
</bodyText>
<figure confidence="0.9219275">
MP—trace CNP
a the ca
</figure>
<page confidence="0.997033">
163
</page>
<bodyText confidence="0.977630873786408">
sources such as PRO-sre and Rel-src are activated in con-
texts in which they are expected and then receive inhibi-
tion. Figure 5(c) shows the resulting network after PRO-
src has been activated. The inserted pronoun behaves pre-
cisely as an input pronoun with respect to subsequent
movement.
The trace generation necessary for movement uses the
same insertion mechanism described above. Figures 6(a)-
(d) illustrate various stages in parsing the phrase, [Np the
cat E,Sr which, Es It ranlli. In Figure 6(a), after parsing
the cat which, synchronous expectations are posted for an
S which contains a reactivation of the Re10 trace by Rel-
src. The signal sent to S by Rel-src will be in the form of
an NP (through NP-trace).
Figure 6(b) shows how the input of ran produces inhi-
bition on Rel-src from Si. The inhibition on Rel-src
causes it to activate (just as in the null pronoun insertion)
to try to satisfy the current contextual expectations. Fig-
ure 6(c) shows the network after Rel-src has activated to
supply the trace. The only remaining problem is that
Rd-sic is actively inhibiting itself through SO.&apos; When
Re/-sic activates again, new instances are created for the
inhibited nodes as they are re-activated: the uninhibited
nodes are simply rebound. The final structure is shown in
Figure 6(d).
It is interesting that the network automatically enforces
the restriction that the relative pronoun, complementizer
and subject of the embedded sentence cannot all be miss-
ing. PRO must be generated before its trace can be
inserted as the subject. Furthermore, since expectations
are strongest for the first link of a sequence, expectations
will be much weaker for the VP in the relative clause
(under S under ..§.) than for the top-level VP under SO.
The fact that the device blocks certain structures,
without explicit well-formedness constraints, is quite
significant. Wherever possible, we would like to account
for the complexity of the data through the composite
behavior of a universal device and a simple, general gram-
mar. We consider the description of a device which embo-
dies the appropriate principles more parsimonious than a
list of complex conditions and filters, and, to the extent
that its architecture is independently motivated by process-
ing (i.e., performance) considerations, of greater theoretical
interest.&apos;
As we have seen, certain interpretations can be
suppressed by expectations from elsewhere in the network.
Furthermore, the occurrence of traces and empty consti-
tuents is severely constrained because they must be sup-
plied by endogenous sources, which can only support a sin-
gle constituent at any given time. For NP movement.
these two properties of the device, taken together.
effectively enforce Ross&apos;s Complex NP Constraint (Ross,
1967), which states that, -No element contained in a
6. Anotha way of stating this 4 that the noe-synchronicity of the two
variables in the pattern hu been &apos;,elated. The self-inbibitioe of • source
occurs in other contexts in the API.) framework. men fat exogenous
somas. Is networks that comae. left•razursive cycles or ambiguous
attachments (e.g.. PP attachment), will-inhibition caw arias naturally as
the exult of necessary non-determinism. RCHIC11■1160111 of • &apos;elf-inhibited
somas effectively preserve the non-synchrookity of patterns.
7. The work of Marcus (19110) is in this same spirit.
sentence dominated by an NP with a lexical head noun
may be moved out of that NP by a transformation.&amp;quot;
To see why this constraint is enforced, consider the two
kinds of sentences that an NP with a lexical head noun
might dominate. If the embedded sentence is a relative
clause, as in, [NP the rat if which/ Es the cat 47 whichi
Es ti chased tin likes fishill, then Rd-sic cannot support
both traces. If the embedded sentence is a noun comple-
ment (not shown in Figure 4), as in, [NP the rat If
which, Es he read a report if that Is the cat chased
1/11111, then there is only one trace in the intended
interpretation, but there is nondeterminism during parsing
between the noun complement and the relative clause
interpretation. The interference causes the trace to be
bound to the innermost relative pronoun in the relative
clause interpretation! Thus, the combined properties of
the device and grammar consistently block those structures
which violate the Complex NP Constraint. Our prelim-
inary findings for other types of movement (e.g., Subject-
auxiliary Inversion, Wh-movement, and Raising) indicate
that they also have natural APN explanations.
4. IMPLEMENTATION sad FUTURE DIRECTIONS
Although the research described in this summary is pri-
marily of a theoretic nature, the basic ideas involved in
using APNs for recognition and generation are being
implemented and tested in Zetalisp on a Symbolics Lisp
Machine. We have also hand-simulated data on movement
from the literature to design the theory and algorithms
presented in this paper. We are currently designing net-
works for a broad coverage syntactic grammar of English
and for additional, cascaded levels for NP role mapping
and case frames. The model has also been adapted as a
general, context-driven problem solver, although more work
remains to be done.
We are considering ways of integrating iterative relaxa-
tion techniques with the rule-based framework of APNs.
This is particularly necessary in helping the network to
identify expectation coalition.s. In Figure 5(a), for exam-
ple. there should be virtually no expectations for Rd-sic,
since it cannot satisfy any of the dominating synchronous
conjunctions. Some type of non-activating feedback from
the sources seems to be necessary.
</bodyText>
<sectionHeader confidence="0.986447" genericHeader="conclusions">
5. SUMMARY
</sectionHeader>
<bodyText confidence="0.999722538461539">
Recent linguistic theories have attempted to induce
general principles (e.g., CN PC, Subjacency, and the Struc-
ture Preserving Hypothesis) from the detailed structural
descriptions of earlier transformational theories (Chomsky,
1981). Our research can be viewed as an attempt to
induce the machine that embodies these principles. In this
paper, we have described a class of candidate machines,
called active production networks, and outlined how they
handle movement as a natural way in which machine and
grammar interact.
The APN framework was initially developed as a plau-
sible cognitive model for language processing, which would
have real-time processing behavior, and extensive
</bodyText>
<note confidence="0.8502635">
L Due to recency consideraticris such relate to expectant= streagth. traces
are bound IN • way that reserves meting.
</note>
<page confidence="0.993479">
164
</page>
<figure confidence="0.996504923076923">
SOC 4)
-— ••••••• ,
NPO(9) VP
1 I
CNPO (9 ) ...
Datil- ■■■■■ M.M•
I •■••
■•■•••• It--&amp;quot;Ill)i er S
..... ..... ■
a......
the) catO (9 )
I 1
Ex09-src0 Exog-src 1 (9)
.&amp;quot;6
/ which Pao \ that for
/ts A
/ .,_J,&apos; . . . . I )PRO-srcl I / &apos;
/ ,/ jIttel-srcl -., ./... &apos;
(a) trace structure after the cat
WIComo
/ I .1--
/ IN/ Rel-Comp
/ Po 1 1&apos; Z.:71110 I &amp;quot;went zer
NPO (9 ) VP
CNPO (9 )
.0‹ 115:c omp0 ( 4 )
)c. I
_ Rai -Como° ( 4 )
R• 1*-J\ (intent zer0 ( 9 )
\V. 1
PR3,A that0(9)
&amp;quot;PRO-srcl Exog-src2(9)
Op° NO
the° cat 0
Exog-Isrc0 Exctilg-src 1
!Rat -arcl
(b) trace structure after the cat ... that
50( 2)
cmPo(4)
Oet0 NO
1
the° catO
1 1
Exog-src0 Exog-srct
VP
Como I &apos;went i ter° 1
/ I
I
tmatO /
Exog-src2 /
......./
-- .......„
</figure>
<figureCaption confidence="0.9845105">
(c) trace structure after the cat PRO that
Figure 5. Relative Pronoun Insertion
</figureCaption>
<figure confidence="0.991568571428571">
Rs 10 (9 )
PRO° (9 )
I Re -srcl
30(4)
Conto0(9)
NP VP
Re i -Como° (9 ) NP-trac• CNP
</figure>
<bodyText confidence="0.99964575">
contextual processing and learning capabilities based on a
formal notion of expectations. That movement also seems
naturally expressible in a way that is consistent with
current linguistic theories is quite intriguing.
</bodyText>
<sectionHeader confidence="0.999698" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.9993214375">
Anderson, J. R. (1983). The Architecture of Cognition,
Harvard University Press, Cambridge.
Chomsky, N. (1981). Lectures on Government and Bind-
ing, Foris Publications, Dordrecht.
Chomsky, N. and Lasnik, H. (1977). &amp;quot;Filters and Con-
trol,&amp;quot; Linguistic Inquiry 8, 425-504.
Fahlman, S. E. (1979). NETL. A System for Represent-
ing and Using Real-World Knowledge. MIT Press, Cam-
bridge.
Fahlman, S. E., Hinton, G. E. and Sejnowski. T. J.
(1983). &amp;quot;Massively Parallel Architectures for Al: NETL,
Thistle, and Boltzmann Machines,&amp;quot; AAAI-83 Conference
Proceedings.
Feldman. J. A. and Ballard, D. H. (1982). &amp;quot;Connection-
ist Models and Their Properties,&amp;quot; Cognitive Science 6,
205-254.
Gazdar, G. (1982). &amp;quot;Phrase Structure Grammar,&amp;quot; The
Nature of Syntactic Representation, Jacobson and Pullum,
eds., Reidel. Boston, 131-186.
Jones, M. A. • (1983). &amp;quot;Activation-Based Parsing,&amp;quot; 8th
IJCAI. Karlsruhe, W. Germany, 678-682.
Jones, M. A. (forthcoming). submitted for publication.
Marcus, M. P. (1980). A Theory of Syntactic Recogni-
tion for Natural Language. MIT Press, Cambridge.
Pereira, F. (1983). &amp;quot;Logic for Natural Language
Analysis,&amp;quot; technical report 275, SRI International. Menlo
Park.
Ross, J. R. (1967). Constraints on Variables .in Syntax,
unpublished Ph.D. thesis, MIT, Cambridge.
Waltz. D. L. and Pollack. J. B. (1985). &amp;quot;Massively
Parallel Parsing: A Strongly Interactive Model of Natural
Language Interpretation.&amp;quot; Cognitive Science, 9, 51-74.
</reference>
<page confidence="0.993975">
165
</page>
<figure confidence="0.998410967213115">
50(2).
NP0(4) VP
/
CNP0(4)
.....„..09M0
Oet0 NO 30(4)
/ 1 -....
1 1 &apos;&apos;&apos;&apos;&apos;&apos;&apos;Il(4)
the() catO
Exec) -src0 EXOQ -arc&apos; 0.
/0=1,......■
Re10 NPI( vP0(9)
It • /&amp;quot;iwnich0 Ma -trace v0(9)
1 ,./ I
/ / 1
e.cg-s.c2(9)N■ , E.09-src2 /g ren0(9)
N%Nie&apos;
1
1Rel-srct Exog -src3(9)
50(2)
NP0(4) *vP
/
..............p__________.cNPo(4) 30(4)
Oat° NO
/ 1
the(),0 -3&amp;quot; — -........s
1 catO
I Como0(9.20 I
1 ,■ ..... .43 &apos; .. .■
exag-s0c0 Extwarct
Re10(9) I
.h4cm0(9) I NP-tas CNP
N.
&apos;Rai-arc&apos;
(a) trace structure after the cat which
(b) trace structure after the cat which ... ran
■■ -■
NPO(9) vP
CNP0(9)
50(9)
NPO(9) VP
C4( 9)
Oet0 NO
/ 1
tne0 catO
1
EA0g-1 src0 E,og-srcl
To (o) Dot° NO
1
thal0 catO e
S1(9) I I Como° .SI(9)
AP-......... Exog-src0 Exog-src1 X;r............
NPI(9) vP0 Re10 R.00(9) NP(9) VP0
/ 11 I / 1
NP-traceO(9) VO wnicrt0 0nicy)00(9 NP-trace0(9) vo
I I
ran0 Exog-src2 ran°
i 1
Exag-src3 Exog-src3
•
(c) trace structure just after the cat which t ran (d) final trace structure
</figure>
<figureCaption confidence="0.996931">
Figure 6. Parsing Relative Clauses
</figureCaption>
<page confidence="0.996114">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928198">
<title confidence="0.999507">MOVEMENT IN ACTIVE PRODUCTION NETWORKS</title>
<author confidence="0.999915">Mark A Jones Alan S Driscoll</author>
<affiliation confidence="0.999969">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.999819">Murray Hill, New Jersey 07974</address>
<abstract confidence="0.9927544">We describe how movement is handled in a class of devices called production networks APN model is a parallel, activation-based framework that has been applied to other aspects of natural language processing. The model is briefly defined, the notation and mechanism for movement is explained, and then several examples are given which illustrate how various conditions on movement can naturally be explained in terms of limitations of the APN device.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Anderson</author>
</authors>
<title>The Architecture of Cognition,</title>
<date>1983</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1956" citStr="Anderson (1983)" startWordPosition="294" endWordPosition="295">ter point in the processing. This paper builds on the research reported in Jones (1983) and Jones (forthcoming). 2. ACTIVE PRODUCTION NETWORKS 2.1 The Device Our contention is that only a class of parallel devices will prove to be powerful enough to allow broad contextual priming, to pursue alternative hypotheses, and to explain the paradox that the performance of a sequential system often degrades with new knowledge, whereas human performance usually improves with learning and experience.&apos; There are a number of new parallel processing (connectionist) models which are sympathetic to this view—Anderson (1983). Feldman and Ballard (1982), Waltz and Pollack (1985), McClelland and Rumelhart (1981, 1982), and Fahlman, Hinton and Sejnowski (1983). Many of the connectionist models use iterative relaxation techniques with networks containing excitatory and inhibitory links. They have primarily been used as best-fit categorizers in large recognition spaces, and it is not yet clear how they will implement the rule-governed behavior of parsers or problem solvers. Rule-based systems need a strong notion of an operating state, and they depend heavily on appropriate variable binding schemes for operations such</context>
</contexts>
<marker>Anderson, 1983</marker>
<rawString>Anderson, J. R. (1983). The Architecture of Cognition, Harvard University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding, Foris Publications,</booktitle>
<location>Dordrecht.</location>
<contexts>
<context position="19911" citStr="Chomsky, 1981" startWordPosition="3223" endWordPosition="3224">chniques with the rule-based framework of APNs. This is particularly necessary in helping the network to identify expectation coalition.s. In Figure 5(a), for example. there should be virtually no expectations for Rd-sic, since it cannot satisfy any of the dominating synchronous conjunctions. Some type of non-activating feedback from the sources seems to be necessary. 5. SUMMARY Recent linguistic theories have attempted to induce general principles (e.g., CN PC, Subjacency, and the Structure Preserving Hypothesis) from the detailed structural descriptions of earlier transformational theories (Chomsky, 1981). Our research can be viewed as an attempt to induce the machine that embodies these principles. In this paper, we have described a class of candidate machines, called active production networks, and outlined how they handle movement as a natural way in which machine and grammar interact. The APN framework was initially developed as a plausible cognitive model for language processing, which would have real-time processing behavior, and extensive L Due to recency consideraticris such relate to expectant= streagth. traces are bound IN • way that reserves meting. 164 SOC 4) -— ••••••• , NPO(9) VP</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, N. (1981). Lectures on Government and Binding, Foris Publications, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
<author>H Lasnik</author>
</authors>
<title>Filters and Control,&amp;quot;</title>
<date>1977</date>
<journal>Linguistic Inquiry</journal>
<volume>8</volume>
<pages>425--504</pages>
<contexts>
<context position="13174" citStr="Chomsky and Lasnik (1977)" startWordPosition="2142" endWordPosition="2145">). When an instance, XO, first activates this rule, vi is bound to XO: the second occurrence X in the rule is constrained to match instances of X0, and expectations for X0, X-src and Y are created. No new exogenous source can satisfy the synchronous conjunction; only an endogenous X-src can. The rule is similar to the notion of an X followed by a Y with an X hole in it (cf. Gazdar, 1982). Figure 4. A Grammar for Relative Clauses Figure 4 defines a grammar with an NP hole in a relative clause; other types of left-extraposition are handled analogously. Our treatment of relatives is adapted from Chomsky and Lasnik (1977). The movement rule for f is: (seq (S vi (and Comp Rel (or Exog-src PRO-ire)) IS v2 (and Re( Re! -arc S))). The rule restricts the first instance of Re to arise either from an exogenous relative pronoun such as which or from an endogenously generated (phonologically null) pronoun PRO. The second variable is satisfied when Re! -arc simultaneously reactivates a trace of the Rel instance and inserts an NP-trace into an S. It is instructive to consider how phonologically null pronouns are inserted before we discuss how movement occurs by trace insertion. The phrase, [NP the mouse ig PRO; that ...I</context>
</contexts>
<marker>Chomsky, Lasnik, 1977</marker>
<rawString>Chomsky, N. and Lasnik, H. (1977). &amp;quot;Filters and Control,&amp;quot; Linguistic Inquiry 8, 425-504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Fahlman</author>
</authors>
<title>NETL. A System for Representing and Using Real-World Knowledge.</title>
<date>1979</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Fahlman, 1979</marker>
<rawString>Fahlman, S. E. (1979). NETL. A System for Representing and Using Real-World Knowledge. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J</author>
</authors>
<title>Massively Parallel Architectures for Al: NETL, Thistle, and Boltzmann Machines,&amp;quot;</title>
<date>1983</date>
<booktitle>AAAI-83 Conference Proceedings.</booktitle>
<marker>J, 1983</marker>
<rawString>Fahlman, S. E., Hinton, G. E. and Sejnowski. T. J. (1983). &amp;quot;Massively Parallel Architectures for Al: NETL, Thistle, and Boltzmann Machines,&amp;quot; AAAI-83 Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A</author>
<author>D H Ballard</author>
</authors>
<title>Connectionist Models and Their Properties,&amp;quot;</title>
<date>1982</date>
<journal>Cognitive Science</journal>
<volume>6</volume>
<pages>205--254</pages>
<marker>A, Ballard, 1982</marker>
<rawString>Feldman. J. A. and Ballard, D. H. (1982). &amp;quot;Connectionist Models and Their Properties,&amp;quot; Cognitive Science 6, 205-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Phrase Structure Grammar,&amp;quot; The Nature of Syntactic Representation,</title>
<date>1982</date>
<pages>131--186</pages>
<editor>Jacobson and Pullum, eds., Reidel.</editor>
<location>Boston,</location>
<contexts>
<context position="754" citStr="Gazdar, 1982" startWordPosition="111" endWordPosition="112">be how movement is handled in a class of computational devices called active production networks (APNs). The APN model is a parallel, activation-based framework that has been applied to other aspects of natural language processing. The model is briefly defined, the notation and mechanism for movement is explained, and then several examples are given which illustrate how various conditions on movement can naturally be explained in terms of limitations of the APN device. 1. INTRODUCTION Movement is an important phenomenon in natural languages. Recently, proposals such as Gazdar&apos;s derived rules (Gazdar, 1982) and Pereira&apos;s extraposition grammars (Pereira, 1983) have attempted to find minimal extensions to the context-free framework that would allow the description of movement. In this paper, we describe a class of computational devices for natural language processing, called active production networks (APNs), and explore how certain kinds of movement are handled. In particular, we are concerned with left extraposition, such as Subject. auxiliary Inversion, Wh-movement, and NP holes in relative clauses. In these cases, the extraposed constituent leaves a trace which is inserted at a later point in </context>
<context position="12939" citStr="Gazdar, 1982" startWordPosition="2105" endWordPosition="2106">activation of a trace that was created earlier in the process. To capture the trace so that expectations for its reactivation can be posted, we use the following type of rule: (seq (S vi X... ) (S v2 ... (and X X-src Y) ...). When an instance, XO, first activates this rule, vi is bound to XO: the second occurrence X in the rule is constrained to match instances of X0, and expectations for X0, X-src and Y are created. No new exogenous source can satisfy the synchronous conjunction; only an endogenous X-src can. The rule is similar to the notion of an X followed by a Y with an X hole in it (cf. Gazdar, 1982). Figure 4. A Grammar for Relative Clauses Figure 4 defines a grammar with an NP hole in a relative clause; other types of left-extraposition are handled analogously. Our treatment of relatives is adapted from Chomsky and Lasnik (1977). The movement rule for f is: (seq (S vi (and Comp Rel (or Exog-src PRO-ire)) IS v2 (and Re( Re! -arc S))). The rule restricts the first instance of Re to arise either from an exogenous relative pronoun such as which or from an endogenously generated (phonologically null) pronoun PRO. The second variable is satisfied when Re! -arc simultaneously reactivates a tra</context>
</contexts>
<marker>Gazdar, 1982</marker>
<rawString>Gazdar, G. (1982). &amp;quot;Phrase Structure Grammar,&amp;quot; The Nature of Syntactic Representation, Jacobson and Pullum, eds., Reidel. Boston, 131-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Jones</author>
</authors>
<title>Activation-Based Parsing,&amp;quot; 8th IJCAI.</title>
<date>1983</date>
<pages>678--682</pages>
<location>Karlsruhe, W.</location>
<contexts>
<context position="1428" citStr="Jones (1983)" startWordPosition="214" endWordPosition="215">empted to find minimal extensions to the context-free framework that would allow the description of movement. In this paper, we describe a class of computational devices for natural language processing, called active production networks (APNs), and explore how certain kinds of movement are handled. In particular, we are concerned with left extraposition, such as Subject. auxiliary Inversion, Wh-movement, and NP holes in relative clauses. In these cases, the extraposed constituent leaves a trace which is inserted at a later point in the processing. This paper builds on the research reported in Jones (1983) and Jones (forthcoming). 2. ACTIVE PRODUCTION NETWORKS 2.1 The Device Our contention is that only a class of parallel devices will prove to be powerful enough to allow broad contextual priming, to pursue alternative hypotheses, and to explain the paradox that the performance of a sequential system often degrades with new knowledge, whereas human performance usually improves with learning and experience.&apos; There are a number of new parallel processing (connectionist) models which are sympathetic to this view—Anderson (1983). Feldman and Ballard (1982), Waltz and Pollack (1985), McClelland and R</context>
</contexts>
<marker>Jones, 1983</marker>
<rawString>Jones, M. A. • (1983). &amp;quot;Activation-Based Parsing,&amp;quot; 8th IJCAI. Karlsruhe, W. Germany, 678-682.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M A Jones</author>
</authors>
<note>submitted for publication.</note>
<marker>Jones, </marker>
<rawString>Jones, M. A. (forthcoming). submitted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="11939" citStr="Marcus, 1980" startWordPosition="1931" endWordPosition="1932">tems of classification. The cascading of expectations from different levels can implement context-sensitive behaviors such as feature agreement and semantic sclectional restrictions. This is described in Jones (forthcoming). In the next section, we will introduce a grammar writing paradigm to represent movement, another type of non-context-free behavior. S. It is interesting to speculate on the consequences of various relaxations of this auumption. Fundamental iiMilatiON in the allocation at sources may be related to limitations in shunt term memory (or buffer space in deterministic mode* toe Marcus, 1980). Linguistic canatraints based on constituent length could be related to source decay. Some syntactic garden path behavior might be related to accelerated source decay caused by inhibitioo from • competing ltypotbesie Anything more ha.. footnote is premature at this time. 3. MOVEMENT From the APN perspective, movement (limited here to left-extraposition) necessitates the endogenous reactivation of a trace that was created earlier in the process. To capture the trace so that expectations for its reactivation can be posted, we use the following type of rule: (seq (S vi X... ) (S v2 ... (and X X-</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M. P. (1980). A Theory of Syntactic Recognition for Natural Language. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Logic for Natural Language Analysis,&amp;quot; technical report 275, SRI International. Menlo Park.</title>
<date>1983</date>
<contexts>
<context position="807" citStr="Pereira, 1983" startWordPosition="117" endWordPosition="118">al devices called active production networks (APNs). The APN model is a parallel, activation-based framework that has been applied to other aspects of natural language processing. The model is briefly defined, the notation and mechanism for movement is explained, and then several examples are given which illustrate how various conditions on movement can naturally be explained in terms of limitations of the APN device. 1. INTRODUCTION Movement is an important phenomenon in natural languages. Recently, proposals such as Gazdar&apos;s derived rules (Gazdar, 1982) and Pereira&apos;s extraposition grammars (Pereira, 1983) have attempted to find minimal extensions to the context-free framework that would allow the description of movement. In this paper, we describe a class of computational devices for natural language processing, called active production networks (APNs), and explore how certain kinds of movement are handled. In particular, we are concerned with left extraposition, such as Subject. auxiliary Inversion, Wh-movement, and NP holes in relative clauses. In these cases, the extraposed constituent leaves a trace which is inserted at a later point in the processing. This paper builds on the research rep</context>
</contexts>
<marker>Pereira, 1983</marker>
<rawString>Pereira, F. (1983). &amp;quot;Logic for Natural Language Analysis,&amp;quot; technical report 275, SRI International. Menlo Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Ross</author>
</authors>
<date>1967</date>
<booktitle>Constraints on Variables .in Syntax, unpublished Ph.D. thesis, MIT,</booktitle>
<location>Cambridge.</location>
<contexts>
<context position="16827" citStr="Ross, 1967" startWordPosition="2737" endWordPosition="2738">nditions and filters, and, to the extent that its architecture is independently motivated by processing (i.e., performance) considerations, of greater theoretical interest.&apos; As we have seen, certain interpretations can be suppressed by expectations from elsewhere in the network. Furthermore, the occurrence of traces and empty constituents is severely constrained because they must be supplied by endogenous sources, which can only support a single constituent at any given time. For NP movement. these two properties of the device, taken together. effectively enforce Ross&apos;s Complex NP Constraint (Ross, 1967), which states that, -No element contained in a 6. Anotha way of stating this 4 that the noe-synchronicity of the two variables in the pattern hu been &apos;,elated. The self-inbibitioe of • source occurs in other contexts in the API.) framework. men fat exogenous somas. Is networks that comae. left•razursive cycles or ambiguous attachments (e.g.. PP attachment), will-inhibition caw arias naturally as the exult of necessary non-determinism. RCHIC11■1160111 of • &apos;elf-inhibited somas effectively preserve the non-synchrookity of patterns. 7. The work of Marcus (19110) is in this same spirit. sentence </context>
</contexts>
<marker>Ross, 1967</marker>
<rawString>Ross, J. R. (1967). Constraints on Variables .in Syntax, unpublished Ph.D. thesis, MIT, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L</author>
<author>J B Pollack</author>
</authors>
<title>Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation.&amp;quot;</title>
<date>1985</date>
<journal>Cognitive Science,</journal>
<volume>9</volume>
<pages>51--74</pages>
<marker>L, Pollack, 1985</marker>
<rawString>Waltz. D. L. and Pollack. J. B. (1985). &amp;quot;Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation.&amp;quot; Cognitive Science, 9, 51-74.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>