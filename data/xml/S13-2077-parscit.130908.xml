<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018768">
<title confidence="0.966503">
FBK: Sentiment Analysis in Twitter with Tweetsted
</title>
<author confidence="0.956206">
Md. Faisal Mahbub Chowdhury
</author>
<affiliation confidence="0.941807">
FBK and University of Trento, Italy
</affiliation>
<email confidence="0.968186">
fmchowdhury@gmail.com
</email>
<author confidence="0.96544">
Sara Tonelli
</author>
<affiliation confidence="0.88021">
FBK, Trento, Italy
</affiliation>
<email confidence="0.99475">
satonelli@fbk.eu
</email>
<author confidence="0.789835">
Marco Guerini
</author>
<affiliation confidence="0.642433">
Trento RISE, Italy
</affiliation>
<email confidence="0.968096">
marco.guerini@trentorise.eu
</email>
<author confidence="0.985606">
Alberto Lavelli
</author>
<affiliation confidence="0.89832">
FBK, Trento, Italy
</affiliation>
<email confidence="0.996411">
lavelli@fbk.eu
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937545454545">
This paper presents the Tweetsted system im-
plemented for the SemEval 2013 task on Sen-
timent Analysis in Twitter. In particular, we
participated in Task B on Message Polar-
ity Classification in the Constrained setting.
The approach is based on the exploitation of
various resources such as SentiWordNet and
LIWC. Official results show that our approach
yields a F-score of 0.5976 for Twitter mes-
sages (11th out of 35) and a F-score of 0.5487
for SMS messages (8th out of 28 participants).
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962729166667">
Microblogging is currently a very popular commu-
nication tool where millions of users share opinions
on different aspects of life. For this reason it is a
valuable source of data for opinion mining and sen-
timent analysis.
Working with such type of texts presents chal-
lenges for NLP beyond those typically encountered
when dealing with more traditional texts, such as
newswire data. Tweets are short, the language used
is very informal, with creative spelling and punctua-
tion, misspellings, slang, new words, URLs, genre-
specific terminology and abbreviations, and #hash-
tags. These characteristics need to be handled with
specific approaches.
This paper presents the approach adopted for the
SemEval 2013 task on Sentiment Analysis in Twit-
ter, in particular Task B on Message Polarity Clas-
sification in the Constrained setting (i.e., using the
provided training data only).
The goal of Task B on Message Polarity Classi-
fication is the following: given a message, decide
whether it expresses a positive, negative, or neutral
sentiment. For messages conveying both a positive
and a negative sentiment, whichever is the stronger
sentiment should be chosen.
Two modalities are possible: (1) Constrained (us-
ing the provided training data only; other resources,
such as lexica, are allowed; however, it is not al-
lowed to use additional tweets/SMS messages or ad-
ditional sentences with sentiment annotations); and
(2) Unconstrained (using additional data for train-
ing, e.g., additional tweets/SMS messages or addi-
tional sentences annotated for sentiment). We par-
ticipated in the Constrained modality.
We adopted a supervised machine learning (ML)
approach based on various contextual and seman-
tic features. In particular, we exploited resources
such as SentiWordNet (Esuli and Sebastiani, 2006),
LIWC (Pennebaker and Francis, 2001), and the lex-
icons described in Mohammad et al. (2009).
Critical features include: whether the mes-
sage contains intensifiers, adjectives, interjections,
presence of positive or negative emoticons, pos-
sible message polarity based on SentiWordNet
scores (Esuli and Sebastiani, 2006; Gatti and
Guerini, 2012), scores based on LIWC cate-
gories (Pennebaker and Francis, 2001), negated
words, etc.
</bodyText>
<sectionHeader confidence="0.960325" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.693458333333333">
Our supervised ML-based approach relies on Sup-
port Vector Machines (SVMs). The SVM imple-
mentation used in the system is LIBSVM (Chang
</bodyText>
<page confidence="0.991771">
466
</page>
<bodyText confidence="0.989849866666667">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 466–470, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
and Lin, 2001) for training SVM models and test-
ing. Moreover, in the preprocessing phase we used
TweetNLP (Owoputi et al., 2013), a POS tagger ex-
plicitly tailored for working on tweets.
We adopted a 2 stage approach: (1) during stage
1, we performed a binary classification of messages
according to the classes neutral vs subjective; (2)
in stage 2, we performed a binary classification of
subjective messages according to the classes positive
vs negative. We performed various experiments on
the training and development sets exploring the use
of different features (see Section 2.1) to find the best
configurations for the official submission.
</bodyText>
<subsectionHeader confidence="0.971845">
2.1 Feature list
</subsectionHeader>
<bodyText confidence="0.980099633333333">
We implement several features divided into three
groups: contextual features, semantic features from
context and semantic features from external re-
sources. The complete list is reported in Table 1.
Contextual features are features computed by
considering only the tokens in the tweets/SMS and
the associated part of speech.
Semantic Features from Context are features
based on words polarity. Emoticons were recog-
nized through a list of emoticons extracted from
Wikipedia1 and then manually labeled as positive or
negative. Negated words (feature n. 18) are any to-
ken occurring between n’t, not, no and a comma, ex-
cluding those tagged as function words. Feature n.
19 captures tokens (or sequences of tokens) labeled
with a positive or negative polarity in the resource
described in Mohammad et al. (2009). The intensi-
fiers considered for Feature n. 20 have been identi-
fied by implementing a simple algorithm that detects
tokens containing anomalously repeated characters
(e.g. happyyyyy). Feature n. 21 was computed by
training the system on the training data and predict-
ing labels for the test data, and then using these la-
bels as new features to train the system again.
Semantic Features from external resources in-
clude word classes from the Linguistic Inquiry
and Word Count (LIWC), a tool that calculates
the degree to which people use different cate-
gories of words related to psycholinguistic pro-
cesses (Pennebaker and Francis, 2001). LIWC in-
</bodyText>
<footnote confidence="0.8975295">
1http://en.wikipedia.org/wiki/List_of_
emoticons
</footnote>
<bodyText confidence="0.999897375">
cludes about 2,200 words and stems grouped into 70
broad categories relevant to psychological processes
(e.g., EMOTION, COGNITION). Sample words are
shown in Table 2.
For each non-zero valued LIWC category of a cor-
responding tweet/SMS, we added a feature for that
category and used the category score as the value
of that feature. We call this LWIC string feature.
Alternatively, we also added a separate feature for
each non-zero valued LIWC category and set 1 as
the value of that feature. This feature is called LWIC
boolean.
We also used words prior polarity - i.e. if a word
out of context evokes something positive or nega-
tive. For this, we relied on SentiWordNet, a broad-
coverage resource that provides polarities for (al-
most) every word. Since words can have multi-
ple senses, we compute the prior polarity of a word
starting from the polarity of each sense and returning
its polarity strength as an index between -1 and 1.
We tested 14 formulae that combine posterior polar-
ities in different ways to obtain a word prior polarity,
as reported in (Gatti and Guerini, 2012).
For the SWNscoresMaximum feature, we select
the prior polarity of the word in a tweet/SMS hav-
ing the maximum absolute score among all words
(of that tweet/SMS). For SWNscoresPolarityCount,
we select the polarity (positive, negative or neutral)
that is assigned to the majority of the words. As
for SWNscoresSum, it corresponds to the sum of
the prior polarities associated with all words in the
tweet/SMS.
</bodyText>
<sectionHeader confidence="0.998811" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.7778714">
In order to select the best performing feature set,
we carried out several 5-fold cross validation ex-
periments on the training data. We report in Table
3 the best performing feature set. In particular, we
adopted a 2 stage approach:
</bodyText>
<listItem confidence="0.970947666666667">
1. during the first stage we performed a binary
classification of messages according to the
classes neutral vs subjective;
2. in the second stage, we performed a binary
classification of subjective messages according
to the classes positive vs negative.
</listItem>
<bodyText confidence="0.9998495">
We opted for a two stage binary classification ap-
proach, since we observed that it produces slightly
</bodyText>
<page confidence="0.993763">
467
</page>
<table confidence="0.992669965517242">
Contextual Features
1. noOfAdjectives num
2. adjective list string
3. interjection list string
4. firstInterj string
5. lastInterj string
6. bigramList string
7. beginsWithRT boolean
8. hasRTinMiddle boolean
9. endsWithLink boolean
10. endsWithHashtag boolean
11. hasQuestion boolean
Semantic Features from Context
12. noOfPositiveEmoticons num
13. noOfNegativeEmoticons num
14. beginsWithPosEmoticon boolean
15. beginsWithNegEmoticon boolean
16. endsWithPosEmoticon boolean
17. endsWithNegEmoticon boolean
18. negatedWords string
19. indexOfChunksWithPolarity string
20. containsIntensifier boolean
21. labelPredictedBySystem pos./neg./neut.
Semantic Features from External Resources
22. LIWC string string
23. LIWC boolean string
24. SWNscoresMaximum pos./neg./neut.
25. SWNscoresPolarityCount pos./neg./neut.
26. SWNscoresSum pos./neg./neut.
</table>
<tableCaption confidence="0.996566">
Table 1: Complete feature list.
</tableCaption>
<table confidence="0.9999247">
LABEL Sample words
CERTAIN all, very, fact*, exact*, certain*, completely
DISCREP but, if, expect*, should
TENTAT or, some, may, possib*, probab*
SENSES observ*, discuss*, shows, appears
SELF we, our, I, us
SOCIAL discuss*, interact*, suggest*, argu*
OPTIM best, easy*, enthus*, hope, pride
ANGER hate, kill, annoyed
INHIB block, constrain, stop
</table>
<tableCaption confidence="0.998728">
Table 2: Word categories along with sample words
</tableCaption>
<bodyText confidence="0.903670210526316">
better results than a single stage multi-class ap-
proach (i.e. neutral vs positive vs negative).2 Dif-
ferent combinations of classifiers were explored ob-
taining comparable results. Here we will report only
2The average F-scores (pos and neg) for two stage and single
stage approaches obtained using the official scorer, by training
on the training data and testing on the development data, are
0.5682 and 0.5611 respectively.
the best results.
STAGE 1. The best result for stage (1), neutral vs
subjective, obtained with 5-fold cross validation on
training set only, accounts for an accuracy of 69.6%.
Instead, the best result for stage (1), obtained with
training on training data and testing on development
data, accounts for an accuracy of 72.67%.
The list of best features is reported in Table 3.
Feature selection was performed by starting from a
small set of basic features, and then by adding the
remaining features incrementally.
</bodyText>
<table confidence="0.959909">
Contextual Features
2. adjective list string
3. interjection list string
5. lastInterj string
Semantic Features from Context
12. noOfPositiveEmoticons num
13. noOfNegativeEmoticons num
18. negatedWords string
19. indexOfChunksWithPolarity string
20. containsIntensifier boolean
Semantic Features from external resources
23. LIWC boolean string
24. SWNscoresMaximum posi./neg./neut.
</table>
<tableCaption confidence="0.99913">
Table 3: Best performing feature set.
</tableCaption>
<bodyText confidence="0.984969066666667">
STAGE 2. In stage (2), positive vs negative, we
started from the best feature set obtained from stage
(1) and added the remaining features one by one in-
crementally. In this case, we kept SWNscoresMaxi-
mum without testing again other formulae; in partic-
ular, to compute words prior polarity, we also kept
the first sense approach, that assigns to every word
the SWN score of its most frequent sense and proved
to be the most discriminative in the first stage neutral
vs. subjective. We found that none of the feature sets
produced better results than that obtained using the
best feature set selected from stage (1). So, the best
feature set for stage (2) is unchanged. We trained
the system on the training data and tested it on the
development data, achieving an accuracy of 80.67%.
</bodyText>
<sectionHeader confidence="0.997793" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99856425">
The SemEval task organizers (Wilson et al., 2013)
provided two test sets on which the systems were
to be evaluated: one included Twitter messages, i.e.
the same type of texts included in the training set,
</bodyText>
<page confidence="0.998871">
468
</page>
<bodyText confidence="0.9999438">
while the other comprised SMS messages, i.e. texts
having more or less the same length as the Twitter
data but (supposedly) a different style. We applied
the same model, trained both on the training and the
development set, on the two types of data, without
any specific adaptation.
The Twitter test set was composed of 3,813
tweets. Official results show that our approach
yields an F-score of 0.5976 for Twitter messages
(11th out of 35), while the best performing system
obtained an F-score of 0.6902. The confusion ma-
trix is reported in Table 4, while the score details
in Table 5. The latter table shows that our system
achieves the lowest results on negative tweets, both
in terms of precision and of recall.
</bodyText>
<table confidence="0.9926595">
gs/pred positive negative neutral
positive 946 101 525
negative 90 274 237
neutral 210 70 1360
</table>
<tableCaption confidence="0.997067">
Table 4: Confusion matrix for Twitter task
</tableCaption>
<table confidence="0.9575084">
class prec recall F-score
positive 0.7592 0.6018 0.6714
negative 0.6157 0.4559 0.5239
neutral 0.6409 0.8293 0.7230
average(pos and neg) 0.5976
</table>
<tableCaption confidence="0.997681">
Table 5: Detailed results for Twitter task
</tableCaption>
<bodyText confidence="0.99983205">
The SMS test set for the competition was com-
posed of 2,094 SMS. Official results provided by the
task organizers show that our approach yields an F-
score of 0.5487 for SMS messages (8th out of 28
participants), while the best performing system ob-
tained an F-score of 0.6846. The confusion matrix
is reported in Table 6, while the score details in Ta-
ble 7. Also in this case the recognition of negative
messages achieves by far the poorest performance.
A comparison of the results on the two test sets
shows that, as expected, our system performs bet-
ter on tweets than on SMS. However, precision
achieved by the system on neutral SMS is 0.12
points better on text messages than on tweets.
Interestingly, it appears from the results in Ta-
bles 5 and 7 (and from the distribution of the classes
in the data sets) that there may be a correlation be-
tween the number of tweets/SMS for a particular
class and the performance obtained for such class.
We plan to further investigate this issue.
</bodyText>
<table confidence="0.993952">
gs/pred positive negative neutral
positive 320 44 128
negative 66 171 157
neutral 208 64 936
</table>
<tableCaption confidence="0.989126">
Table 6: Confusion matrix for SMS task
</tableCaption>
<table confidence="0.9900888">
class prec recall F-score
positive 0.5387 0.6504 0.5893
negative 0.6129 0.4340 0.5082
neutral 0.7666 0.7748 0.7707
average(pos and neg) 0.5487
</table>
<tableCaption confidence="0.996687">
Table 7: Detailed results for SMS task
</tableCaption>
<sectionHeader confidence="0.99891" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999973470588235">
In this paper, we presented Tweetsted, the system de-
veloped by FBK for the SemEval 2013 task on Sen-
timent Analysis. We trained a classifier performing
a two-step binary classification, i.e. first neutral vs.
subjective data, and then positive vs. negative ones.
We implemented a set of features including contex-
tual and semantic ones. We also integrated in our
feature representation external knowledge from Sen-
tiWordNet, LIWC and the resource by Mohammad
et al. (2009). On both test sets (i.e., Twitter mes-
sages and SMS) of the constrained modality of the
challenge, we achieved a good performance, being
among the top 30% of the competing systems. In
the near future, we plan to perform an error analysis
of the wrongly classified data to investigate possible
classification issues, in particular the lower perfor-
mance on negative tweets and SMS.
</bodyText>
<sectionHeader confidence="0.998365" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996103666666667">
This work is supported by “eOnco - Pervasive knowledge
and data management in cancer care” and “Trento RISE
PerTe” projects.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8988685">
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/˜cjlin/libsvm.
</reference>
<page confidence="0.995849">
469
</page>
<reference confidence="0.995449037037037">
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation (LREC
2006), Genoa, Italy.
Lorenzo Gatti and Marco Guerini. 2012. Assessing sen-
timent strength in words prior polarities. In Proceed-
ings of COLING 2012: Posters, pages 361–370, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
Saif Mohammad, Bonnie Dorr, and Cody Dunne. 2009.
Generating High-Coverage Semantic Orientation Lex-
icons From Overtly Marked Words and a Thesaurus.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL 2013, Atlanta, Georgia, June.
J. Pennebaker and M. Francis. 2001. Linguistic inquiry
and word count: LIWC. Erlbaum Publishers.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ’13, June.
</reference>
<page confidence="0.998184">
470
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.033193">
<title confidence="0.998637">FBK: Sentiment Analysis in Twitter with Tweetsted</title>
<author confidence="0.778297">Faisal Mahbub</author>
<affiliation confidence="0.924716">FBK and University of Trento,</affiliation>
<email confidence="0.99932">fmchowdhury@gmail.com</email>
<author confidence="0.613139">Sara</author>
<address confidence="0.641211">FBK, Trento,</address>
<email confidence="0.996862">satonelli@fbk.eu</email>
<author confidence="0.8739455">Marco Trento RISE</author>
<author confidence="0.8739455">Italy</author>
<email confidence="0.986561">marco.guerini@trentorise.eu</email>
<author confidence="0.829179">Alberto</author>
<affiliation confidence="0.626397">FBK, Trento,</affiliation>
<email confidence="0.99864">lavelli@fbk.eu</email>
<abstract confidence="0.920253583333333">paper presents the implemented for the SemEval 2013 task on Sentiment Analysis in Twitter. In particular, we participated in Task B on Message Polarity Classification in the Constrained setting. The approach is based on the exploitation of various resources such as SentiWordNet and LIWC. Official results show that our approach yields a F-score of 0.5976 for Twitter messages (11th out of 35) and a F-score of 0.5487 for SMS messages (8th out of 28 participants).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.</title>
<date>2001</date>
<tech>tw/˜cjlin/libsvm.</tech>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu. tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="2591" citStr="Esuli and Sebastiani, 2006" startWordPosition="390" endWordPosition="393">dalities are possible: (1) Constrained (using the provided training data only; other resources, such as lexica, are allowed; however, it is not allowed to use additional tweets/SMS messages or additional sentences with sentiment annotations); and (2) Unconstrained (using additional data for training, e.g., additional tweets/SMS messages or additional sentences annotated for sentiment). We participated in the Constrained modality. We adopted a supervised machine learning (ML) approach based on various contextual and semantic features. In particular, we exploited resources such as SentiWordNet (Esuli and Sebastiani, 2006), LIWC (Pennebaker and Francis, 2001), and the lexicons described in Mohammad et al. (2009). Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (Esuli and Sebastiani, 2006; Gatti and Guerini, 2012), scores based on LIWC categories (Pennebaker and Francis, 2001), negated words, etc. 2 System Description Our supervised ML-based approach relies on Support Vector Machines (SVMs). The SVM implementation used in the system is LIBSVM (Chang 466 Second Joint </context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>A. Esuli and F. Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenzo Gatti</author>
<author>Marco Guerini</author>
</authors>
<title>Assessing sentiment strength in words prior polarities.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>361--370</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="2933" citStr="Gatti and Guerini, 2012" startWordPosition="438" endWordPosition="441">itional sentences annotated for sentiment). We participated in the Constrained modality. We adopted a supervised machine learning (ML) approach based on various contextual and semantic features. In particular, we exploited resources such as SentiWordNet (Esuli and Sebastiani, 2006), LIWC (Pennebaker and Francis, 2001), and the lexicons described in Mohammad et al. (2009). Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (Esuli and Sebastiani, 2006; Gatti and Guerini, 2012), scores based on LIWC categories (Pennebaker and Francis, 2001), negated words, etc. 2 System Description Our supervised ML-based approach relies on Support Vector Machines (SVMs). The SVM implementation used in the system is LIBSVM (Chang 466 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 466–470, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics and Lin, 2001) for training SVM models and testing. Moreover, in the preprocessing phase we used TweetNL</context>
<context position="6667" citStr="Gatti and Guerini, 2012" startWordPosition="1032" endWordPosition="1035">d set 1 as the value of that feature. This feature is called LWIC boolean. We also used words prior polarity - i.e. if a word out of context evokes something positive or negative. For this, we relied on SentiWordNet, a broadcoverage resource that provides polarities for (almost) every word. Since words can have multiple senses, we compute the prior polarity of a word starting from the polarity of each sense and returning its polarity strength as an index between -1 and 1. We tested 14 formulae that combine posterior polarities in different ways to obtain a word prior polarity, as reported in (Gatti and Guerini, 2012). For the SWNscoresMaximum feature, we select the prior polarity of the word in a tweet/SMS having the maximum absolute score among all words (of that tweet/SMS). For SWNscoresPolarityCount, we select the polarity (positive, negative or neutral) that is assigned to the majority of the words. As for SWNscoresSum, it corresponds to the sum of the prior polarities associated with all words in the tweet/SMS. 3 Experimental Setup In order to select the best performing feature set, we carried out several 5-fold cross validation experiments on the training data. We report in Table 3 the best performi</context>
</contexts>
<marker>Gatti, Guerini, 2012</marker>
<rawString>Lorenzo Gatti and Marco Guerini. 2012. Assessing sentiment strength in words prior polarities. In Proceedings of COLING 2012: Posters, pages 361–370, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Cody Dunne</author>
</authors>
<title>Generating High-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2682" citStr="Mohammad et al. (2009)" startWordPosition="405" endWordPosition="408">such as lexica, are allowed; however, it is not allowed to use additional tweets/SMS messages or additional sentences with sentiment annotations); and (2) Unconstrained (using additional data for training, e.g., additional tweets/SMS messages or additional sentences annotated for sentiment). We participated in the Constrained modality. We adopted a supervised machine learning (ML) approach based on various contextual and semantic features. In particular, we exploited resources such as SentiWordNet (Esuli and Sebastiani, 2006), LIWC (Pennebaker and Francis, 2001), and the lexicons described in Mohammad et al. (2009). Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (Esuli and Sebastiani, 2006; Gatti and Guerini, 2012), scores based on LIWC categories (Pennebaker and Francis, 2001), negated words, etc. 2 System Description Our supervised ML-based approach relies on Support Vector Machines (SVMs). The SVM implementation used in the system is LIBSVM (Chang 466 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International W</context>
<context position="4897" citStr="Mohammad et al. (2009)" startWordPosition="741" endWordPosition="744">ed in Table 1. Contextual features are features computed by considering only the tokens in the tweets/SMS and the associated part of speech. Semantic Features from Context are features based on words polarity. Emoticons were recognized through a list of emoticons extracted from Wikipedia1 and then manually labeled as positive or negative. Negated words (feature n. 18) are any token occurring between n’t, not, no and a comma, excluding those tagged as function words. Feature n. 19 captures tokens (or sequences of tokens) labeled with a positive or negative polarity in the resource described in Mohammad et al. (2009). The intensifiers considered for Feature n. 20 have been identified by implementing a simple algorithm that detects tokens containing anomalously repeated characters (e.g. happyyyyy). Feature n. 21 was computed by training the system on the training data and predicting labels for the test data, and then using these labels as new features to train the system again. Semantic Features from external resources include word classes from the Linguistic Inquiry and Word Count (LIWC), a tool that calculates the degree to which people use different categories of words related to psycholinguistic proces</context>
</contexts>
<marker>Mohammad, Dorr, Dunne, 2009</marker>
<rawString>Saif Mohammad, Bonnie Dorr, and Cody Dunne. 2009. Generating High-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL 2013,</booktitle>
<location>Atlanta, Georgia,</location>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL 2013, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pennebaker</author>
<author>M Francis</author>
</authors>
<title>Linguistic inquiry and word count: LIWC.</title>
<date>2001</date>
<publisher>Erlbaum Publishers.</publisher>
<contexts>
<context position="2628" citStr="Pennebaker and Francis, 2001" startWordPosition="395" endWordPosition="398">ned (using the provided training data only; other resources, such as lexica, are allowed; however, it is not allowed to use additional tweets/SMS messages or additional sentences with sentiment annotations); and (2) Unconstrained (using additional data for training, e.g., additional tweets/SMS messages or additional sentences annotated for sentiment). We participated in the Constrained modality. We adopted a supervised machine learning (ML) approach based on various contextual and semantic features. In particular, we exploited resources such as SentiWordNet (Esuli and Sebastiani, 2006), LIWC (Pennebaker and Francis, 2001), and the lexicons described in Mohammad et al. (2009). Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (Esuli and Sebastiani, 2006; Gatti and Guerini, 2012), scores based on LIWC categories (Pennebaker and Francis, 2001), negated words, etc. 2 System Description Our supervised ML-based approach relies on Support Vector Machines (SVMs). The SVM implementation used in the system is LIBSVM (Chang 466 Second Joint Conference on Lexical and Computation</context>
<context position="5531" citStr="Pennebaker and Francis, 2001" startWordPosition="843" endWordPosition="846"> intensifiers considered for Feature n. 20 have been identified by implementing a simple algorithm that detects tokens containing anomalously repeated characters (e.g. happyyyyy). Feature n. 21 was computed by training the system on the training data and predicting labels for the test data, and then using these labels as new features to train the system again. Semantic Features from external resources include word classes from the Linguistic Inquiry and Word Count (LIWC), a tool that calculates the degree to which people use different categories of words related to psycholinguistic processes (Pennebaker and Francis, 2001). LIWC in1http://en.wikipedia.org/wiki/List_of_ emoticons cludes about 2,200 words and stems grouped into 70 broad categories relevant to psychological processes (e.g., EMOTION, COGNITION). Sample words are shown in Table 2. For each non-zero valued LIWC category of a corresponding tweet/SMS, we added a feature for that category and used the category score as the value of that feature. We call this LWIC string feature. Alternatively, we also added a separate feature for each non-zero valued LIWC category and set 1 as the value of that feature. This feature is called LWIC boolean. We also used </context>
</contexts>
<marker>Pennebaker, Francis, 2001</marker>
<rawString>J. Pennebaker and M. Francis. 2001. Linguistic inquiry and word count: LIWC. Erlbaum Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<contexts>
<context position="11161" citStr="Wilson et al., 2013" startWordPosition="1699" endWordPosition="1702"> other formulae; in particular, to compute words prior polarity, we also kept the first sense approach, that assigns to every word the SWN score of its most frequent sense and proved to be the most discriminative in the first stage neutral vs. subjective. We found that none of the feature sets produced better results than that obtained using the best feature set selected from stage (1). So, the best feature set for stage (2) is unchanged. We trained the system on the training data and tested it on the development data, achieving an accuracy of 80.67%. 4 Evaluation The SemEval task organizers (Wilson et al., 2013) provided two test sets on which the systems were to be evaluated: one included Twitter messages, i.e. the same type of texts included in the training set, 468 while the other comprised SMS messages, i.e. texts having more or less the same length as the Twitter data but (supposedly) a different style. We applied the same model, trained both on the training and the development set, on the two types of data, without any specific adaptation. The Twitter test set was composed of 3,813 tweets. Official results show that our approach yields an F-score of 0.5976 for Twitter messages (11th out of 35),</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, Ritter, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013. SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>