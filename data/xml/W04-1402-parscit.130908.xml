<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.98353">
Work-in-Progress project report : CESTA - Machine Translation Evaluation
Campaign
</title>
<author confidence="0.799613">
Widad Mustafa El Hadi Marianne Dabbadie Ismaïl Timimi Martin Rajman
</author>
<table confidence="0.976409777777778">
IDIST / CERSATES IDIST / CERSATES IDIST / CERSATES LIA
Universit6 de Lille 3 Universit6 de Lille 3 Universit6 de Lille 3 Ecole
Domaine universitaire Domaine universitaire Domaine universitaire Polytechnique
du &amp;quot;Pont de Bois&amp;quot; du &amp;quot;Pont de Bois&amp;quot; du &amp;quot;Pont de Bois&amp;quot; F6d6rale de Lausanne
rue du Barreau rue du Barreau rue du Barreau Bât. INR
BP 149 BP 149 BP 149 CH-1015 Lausanne
59653 Villeneuve d&apos;Ascq 59653 Villeneuve d&apos;Ascq 59653 Villeneuve d&apos;Ascq Switzerland
Cedex - France Cedex - France Cedex - France martin.rajman@epfl.ch
mustafa@univ-lille3.fr dabbadie@univ-lille3.fr timimi@univ-lille3.fr
</table>
<figure confidence="0.115962666666667">
Philippe Langlais
RALI / DIRO -
Universit6 de Montr6al
C.P. 6128,
succursale Centre-ville
Montr6al (Qu6bec) -
Canada, H3C 3J7
felipe@IRO.UMontreal.
CA
</figure>
<author confidence="0.946229">
Antony Hartley
</author>
<affiliation confidence="0.66484825">
University of Leeds
Centre for Translation
Studies
Woodhouse Lane
</affiliation>
<address confidence="0.5028295">
LEEDS LS2 9JT
UK
</address>
<email confidence="0.892513">
a.hartley@leeds.ac.uk
</email>
<author confidence="0.588755">
Andrei Popescu Belis
</author>
<affiliation confidence="0.749291">
University of Geneva 40
bvd du Pont d&apos;Arve CH-
1211 Geneva 4
Switzerland
Andrei.Popescu-
</affiliation>
<email confidence="0.707308">
Belis@issco.unige.ch
</email>
<sectionHeader confidence="0.986356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965318181818">
CESTA, the first European Campaign
dedicated to MT Evaluation, is a project
labelled by the French Technolangue action.
CESTA provides an evaluation of six
commercial and academic MT systems using a
protocol set by an international panel of
experts. CESTA aims at producing reusable
resources and information about reliability of
the metrics. Two runs will be carried out: one
using the system’s basic dictionary, another
after terminological adaptation. Evaluation
task, test material, resources, evaluation
measures, metrics, will be detailed in the full
paper. The protocol is the combination of a
contrastive reference to: IBM “BLEU”
protocol (Papineni, K., S. Roukos, T. Ward
and Z. Wei-Jing, 2001); “BLANC” protocol
derived from (Hartley, Rajman, 2002).;
“ROUGE” protocol (Babych, Hartley, Atwell,
2003). The results of the campaign will be
published in a final report and be the object of
two intermediary and final workshops.
</bodyText>
<sectionHeader confidence="0.961206" genericHeader="introduction">
1 Introduction
</sectionHeader>
<footnote confidence="0.419222">
1.1 CESTA and the Technolangue Action in
France
</footnote>
<bodyText confidence="0.999132935483871">
This article is a collective paper written by the
CESTA scientific committee that aims at
presenting the CESTA evaluation campaign, a
project labelled in 2002 by the French Ministry of
Research and Education within the framework of
the Technolangue call for projects and integrated
to the EVALDA evaluation platform. It reports
work in progress and therefore is the description of
an on-going campaign for which system results are
not yet available.
In France, EVALDA is the new Evaluation
platform, a joint venture between the French
Ministry of Research and Technology and ELRA
(European Language Resources and Evaluation
Association, Paris, France). Within the framework
of this initiative eight evaluation projets are being
conducted: ARCADE II: campagne d’6valuation
de l’alignement de corpus multilingues; CESART:
campagne d&apos;Evaluation de Systèmes
d’Acquisition de Ressources Terminologiques;
CESTA : campagne d&apos;Evaluation de Systèmes de
Traduction automatique; Easy: Evaluation des
Analyseurs Syntaxiques du français; Campagne
EQueR, Evaluation en question-r6ponse;
Campagne ESTER, Evaluation de transcriptions
d’6missions radio; Campagne EvaSY, Evaluation
en synthèse vocale; and Campagne MEDIA,
Evaluation du dialogue hors et en contexte.
Regarding evaluation, the objectives of the
Action as Joseph Mariani pointed out in his
presentation at the LREC 2002 conference are to:
</bodyText>
<listItem confidence="0.962064181818182">
• Improve the present evaluation
methodologies
• Identify new (quantitative and qualitative)
approaches for already evaluated
technologies: socio-technical and psycho-
cognitive aspects
• Identify protocols for new technologies
and applications
• Identification of language resources
relevant for evaluation (to promote the
development of new linguistic resources
</listItem>
<bodyText confidence="0.983964153846154">
for those languages and domains where
they do not exist yet, or only exist in a
prototype stage, or exist but cannot be
made available to the interested users);
The object of the CESTA campaign is twofold.
It is on the one hand to provide an evaluation of
commercial Machine Translation Systems and on
the other hand, to work collectively on the setting
of a new reusable Machine Translation Evaluation
protocol that is both user oriented and accounts for
the necessity to use semantic metrics in order to
make available a high quality reusable machine
translation protocol to system providers.
</bodyText>
<subsectionHeader confidence="0.998006">
1.2 Object of the campaign
</subsectionHeader>
<bodyText confidence="0.9997208">
The object of the CESTA campaign is to
evaluate technologies together with metrics, i.e. to
contribute to the setting of a state of the art within
the field of Machine Translation systems
evaluation.
</bodyText>
<subsectionHeader confidence="0.90595">
1.3 CESTA user oriented protocol
</subsectionHeader>
<bodyText confidence="0.972194541666667">
The campaign will last three years, starting
from January 2003. A board of European
experts are members of CESTA Scientific
committee and have been working together in
order to determine the protocol to use for the
campaign. Six systems are being evaluated.
Five of these systems are commercial MT
systems and one is a prototype developed at
the university of Montreal by the RALI
research centre. Evaluation is carried out on
text rather than sentences. Text approximate
width will be 400 words. Two runs will be
carried out. For industrial reasons, systems
will be made anonymous.
2 State-of-the-art in the field of Machine
Translation evaluation
In 1966, the ALPAC report draws light on the
limits of Machine Translation systems. In 1979,
the Van Slype report presented a study dedicated to
Machine Translation metrics.
In 1992, the JEIDA campaign puts the user at the
center of evaluator’s preoccupation. JEIDA
proposed to draw human measures on the basis of
three questionnaires:
</bodyText>
<listItem confidence="0.9968285">
• One destined to users (containing a
hundred questions)
• Other questionnaires are destined to
system Machine translation systems
editors (three different questionnaires),
• And a set of other questionnaires reserved
to Machine Translation systems
developers.
</listItem>
<bodyText confidence="0.996210066666667">
Scores are worked out on the background of
fourteen categories of questions. From these
scores, graphs are produced according to the
answers obtained. A comparison of different
graphs for each systems is used as a basis for
systems classification.
The first DARPA Machine Translation
evaluation campaign (1992-1994) makes use of
human judgments. It is a very expensive method
but interesting however, as regards the reliability
of the evaluation thus produced. This campaign is
based on tests carried out from French, Spanish
and Japanese as source languages and English as a
target language. The measures used for each of the
following criteria are:
</bodyText>
<listItem confidence="0.857342769230769">
• Fidelity – a proximity distance is worked
out between a source sentence and a target
sentence on a 1 to 5 scale.
• Intelligibility, that corresponds to
linguistic acceptability of a translation is
measured on a 1 to 5 evaluation scale.
• Informativeness: the test is carried out on
reading of the target text alone. A
questionnaire on text informative content
is displayed allowing to work out a
measure calculated on the basis of the
percentage of good answers provided in
system translation.
</listItem>
<bodyText confidence="0.971835357142857">
In 1995, the OVUM report proposes to compare
commercial Machine Translation systems on the
basis of ten criteria.
In 1996, the EAGLES report (EAGLES, 1999)
sets new standards for Natural Language
Processing software evaluation on the background
of ISO 9126.
Initiated in 1999, and coordinated by Pr Antonio
Zampolli, the ISLE project is divided into three
working groups, one being a Machine Translation
group.
Starting from ISO 9126 standard (King, 1999b),
the aim of the project is to produce two taxonomies
(c.f. section 3 of this article) and :
</bodyText>
<listItem confidence="0.9950601875">
• One defining quality subcriteria with the
aim of refining the six criteria defined by
ISO 9126 (i.e. functionality, reliability,
user-friendliness, efficiency, maintenance
portability)
• The second one specifying use contexts
that define the type of task induced the use
of a by Machine Translation system, the
types of users and input data. This
taxonomy uses contextual parameters to
select and order the quality criteria subject
to evaluation. This taxonomy can be
viewed and downloaded on the ISSCO
website at the following address :
http://www.issco.unige.ch/projects/isle/fe
mti/
</listItem>
<bodyText confidence="0.993495">
The second DARPA campaign (Papineni, K., S.
Roukos, T. Ward and Z. Wei-Jing, 2001), making
use of the IBM BLEU metric is mentioned in the
CESTA protocol (c.f. section 8.1 of this article).
</bodyText>
<sectionHeader confidence="0.992335" genericHeader="method">
3 User-oriented evaluations
</sectionHeader>
<bodyText confidence="0.979460822222222">
An emerging evaluation methodology in NLP
technology focuses on quality requirements
analysis. The needs and consequently the
satisfaction of end-users, and this will depend on
the tasks and expected results requirement
domains, which we have identified as diagnostic
quality dimensions. One of the most suitable
methods in this type of evaluation is the adequacy
evaluation that aims at finding out whether a
system or product is adequate to someone’s needs
(see Sparck-Jones &amp; Gallier, 1996 and King, 1996
among many others for a more detailed discussion
of these issues). This approach encourages
communication between users and developers.
The definition of the CESTA evaluation
protocol took into account the Framework for
MT Evaluation in ISLE (FEMTI), available
online. FEMTI offers the possibility to define
evaluation requirements, then to select relevant
&apos;qualities&apos;, and the metrics commonly used to
score them (cf. ISO/IEC 9126, 14598). The
CESTA evaluation methodology is founded on
a black box approach.
CESTA evaluators considered a generic user,
which is interested in general-purpose, ready-
to-use translations, preferably using an off-the-
shelf system. In addition, CESTA aims at
producing reusable resources, and providing
information about the reliability of the metrics
(validation), while being cost-effective and
fast.
With these evaluation requirements in mind
(FEMTI-1), it appears that the relevant
qualities (FEMTI-2) are &apos;suitability&apos;, &apos;accuracy&apos;
and &apos;well-formedness&apos;. Automated metrics best
meet the CESTA needs for reusability, among
which BLEU, X-score and D-score (chosen for
internal reasons). Their validation requires the
comparison of their scores with recognised
human scores for the same qualities (e.g.,
human assessment of fidelity or fluency).
&apos;Efficiency&apos;, measured through post-editing
time, was also discussed. For the evaluation,
first a general-purpose dictionary could be
used, then a domain-specific one.
</bodyText>
<subsectionHeader confidence="0.990647">
3.1 An approach based on use cases
</subsectionHeader>
<bodyText confidence="0.999975">
ISO 14598 directives for evaluators put forth as
a prequisite for systems development the detailed
identification of user needs that ought to be
specified through the use case document.
Moreover, conducting a full evaluation process
involves going through the establishment of an
evaluation requirements document. ISO 14598
document specifies that quality requirements
should be identified “according to user needs,
application area and experience, software integrity
and experience, regulations, law, required
standards, etc.”.
The evaluation specification document is created
using the Software Requirement Specifications
(SRS) and the Use-Case document. The CESTA
protocol relies on a use case that refers to a
translation need grounded on basic syntactic
correctness and simple understanding of a text, as
required by information watch tasks for example,
and excludes making a direct use of the text for
post editing purposes.
</bodyText>
<sectionHeader confidence="0.98401" genericHeader="method">
4 Two campaigns
</sectionHeader>
<subsectionHeader confidence="0.997618">
4.1 Specificities of the CESTA campaign
</subsectionHeader>
<bodyText confidence="0.999828083333333">
Two campaigns are being organised :
The first campaign is organised using a system’s
default dictionary. After systems terminological
adaptation a second campaign will be organised.
Two studies previously carried out and presented
respectively at the 2001 MT Summit (Mustafa El
Hadi, Dabbadie, Timimi, 2001) and at the 2002
LREC conference (Mustafa Mustafa El Hadi,
Dabbadie, Timimi, 2002) allowed us to realise the
gap in terms in terms of quality between results
obtained on target text after terminological
enrichment.
</bodyText>
<subsectionHeader confidence="0.993068">
4.2 First campaign
</subsectionHeader>
<bodyText confidence="0.9977875">
The organisation of the campaign implies going
through several steps :
</bodyText>
<listItem confidence="0.922254857142857">
• Identification of potential participants
• Original protocol readjustement,
• The setting of a specific test tool that is
currently being be implemented in
conformity with protocol specifications
validated by CESTA scientific
committee. CESTA protocol
</listItem>
<bodyText confidence="0.997800714285714">
specifications have been
communicated to participants in
particular as regards data formatting,
test schedule, metrics and adaptation
phase. For cost requirements, CESTA
will not include a training phase. The
first run will start during autumn 2004
</bodyText>
<subsectionHeader confidence="0.9994">
4.3 Second campaign
</subsectionHeader>
<bodyText confidence="0.999981613636364">
The systems having already been tuned, an
adaptation phase will not be carried out for the
second campaign. However terminological
adaptation will be necessary at this stage. The
second series of tests being carried out on a
thematically homogeneous corpus, the thematic
domain only will be communicated to participants
for terminological adaptation. For thematic
adaptation, and in order to avoid system
optimisation after the first series of tests, a new
domain specific 200.000 word hiding corpus will
be used.
The terminological domain on which evaluation
will be carried out will then have to be defined.
This terminological domain will be communicated
to participants but not the corpus used itself. On
the other hand, participants will be asked to send
organisers a written agreement by which they will
commit themselves to provide organisers with any
relevant information regarding system tuning and
specific adaptations that have made on each of the
participating MT systems, in order to allow the
scientific committee to understand and analyse the
origin of the potential system ranking changes. The
second run will start during year 2005.
Organisers have committed themselves not to
publish the results between the two campaigns.
After the training phase, the second campaign
will take place. Participants will be given a fifteen
days delay to send the results. An additional three
months period will be necessary to carry out result
analysis and prepare data publication and
workshop organisation.
CESTA scientific committee also decided in
parallel with the two campaigns, to evaluate
systems capacity to process formatted texts
including images and HTML tags. Participants
who do not wish to participate to this additional
test have informed the scientific committee. Most
of the time the reason is that their system is only
capable of processing raw text. This is the case
mainly for academic systems involved in the
campaign, most of the commercial systems being
nowadays able to process formatted text.
</bodyText>
<sectionHeader confidence="0.992568" genericHeader="method">
5 Contrastive evaluation
</sectionHeader>
<bodyText confidence="0.999951107142857">
One of the particularities of the CESTA protocol
is to provide a Meta evaluation of the automated
metrics used for the campaign – a kind of state of
the art of evaluation metrics. The robustness of the
metrics will be tested on minor language pairs
through a contrastive evaluation against human
judgement.
The scientific committee has decided to use
Arabic4French as a minor language pair.
Evaluation on the minor language pair will be
carried directly on two of the participating systems
and using English as a pivotal language on the
other systems. Translation through a pivotal
language will then be the following :
Arabic4English4French.
Organiser are, of course, perfectly aware of the
potential loss of quality provoked by the use of a
pivotal language but recall however that, contrarily
to the major language pair, evaluation carried out
on the minor language pair through a pivotal
system will not be used to evaluate these systems
themselves, but metric robustness. Results of
metric evaluation and systems evaluation will, of
course, be obtained and disseminated separately.
During the tests of the first campaign, the
FrenchÆEnglish system obtaining the best ranking
will be selected to be used as a pivotal system for
metrics robustness Meta evaluation.
</bodyText>
<sectionHeader confidence="0.94051" genericHeader="method">
6 Test material
</sectionHeader>
<bodyText confidence="0.998984">
The required material is a set of corpora as
detailed in the following section and a test tool that
will be implemented according to metrics
requirements and under the responsibility of
CESTA organisers.
</bodyText>
<subsectionHeader confidence="0.977735">
6.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9737974">
The evaluation corpus is composed of 50 texts,
each text length is 400 words to be translated
twice, considering that a translation already exists
in the original corpus. The different corpora are
provided by ELRA. The masking corpus has
250.000 words and must be thematically
homogeneous.
For each language pair the following corpora
will be used:
Adaptation
</bodyText>
<listItem confidence="0.996593235294118">
• This 200.000 à 250.000 word corpus is a
bilingual corpus. It is used to validate
exchanges between organisers and
participants and for system tuning.
First Campaign
• One 20.000 word evaluation corpus will be
used (50 texts of 400 words each)
• One 200.000 to 250.000 word masking
corpus that hides the evaluation corpus.
Second campaign
• One new 20.000 word corpus will be used
but it will have to be thematically
homogeneous (on a specific domain that
will be communicated to participants a few
months before the run takes place)
• One masking corpus similar to the
previous one.
</listItem>
<bodyText confidence="0.963045466666667">
Additional requirement
The BLANC metric requires the use of a
bilingual aligned corpus at document scale.
Three human translations will be used for each
of the evaluation source texts. Considering that the
corpora used, already provide one official
translations, only two additional human
translations will be necessary. These translations
will be carried out under the organisers
responsibility. Within the framework of CESTA
use cases, evaluation is not made in order to obtain
a ready to publish target language translation, but
rather to provide a foreign user a simple access to
information within the limits of basic grammatical
correctness, as already mentioned in this article.
</bodyText>
<sectionHeader confidence="0.966837" genericHeader="method">
7 The BLEU, BLANC and ROUGE metrics
</sectionHeader>
<bodyText confidence="0.99368935">
Three types of metrics will be tested on the
corpus, the CESTA protocol being the combination
of a contrastive reference to three different
protocols:
7.1 The IBM “BLEU” protocol (Papineni, K.,
S. Roukos, T. Ward and Z. Wei-Jing,
2001).
The IBM BLEU metric used by the DARPA for
its 2001 evaluation campaign, uses co-occurrence
measures based on N-Grams. The translation in
English of 80 Chinese source documents by six
different commercial Machine Translation
systems, was submitted to evaluation. From a
reference corpus of translations made by experts,
this metric works out quality measures according
to a distance calculated between an automatically
produced translation and the reference translation
corpus based on shared N-grams (n=1,2,3...). The
results of this evaluation are then compared to
human judgments.
</bodyText>
<listItem confidence="0.9319766">
• NIST now offers an online evaluation of
MT systems performance, i.e.:
o A program that can be
downloaded for research aims.
The user then provides source
texts and reference translations for
a determined pair of languages.
o An e-mail evaluation service, for
more formal evaluations. Results
can be obtained in a few minutes.
</listItem>
<subsectionHeader confidence="0.94777">
7.2 The “BLANC” protocol
</subsectionHeader>
<bodyText confidence="0.9997618">
It is a metric derived from a study presented at
the LREC 2002 conference (Hartley A., Rajman
M., 2002). We only take into account a part of the
protocol described in the referred paper, i.e. the X
score, that corresponds to grammatical correctness.
We will not give an exhaustive description of
this experience and shall only detail the elements
that are relevant to the CESTA evaluation protocol.
The protocol has been tested on the following
languages.
</bodyText>
<listItem confidence="0.984320111111111">
• Source language: French
• Target language: English
• Source corpus : 100 texts – domain :
newspaper articles
Human judgements for comparison referential:
• 12 English monolingual students.
• No human translation reference corpus.
• Three criteria were tested: Fluency,
Adequacy, Informativeness
</listItem>
<bodyText confidence="0.802534333333333">
Six systems were submitted to evaluation :
Candide (CD), Globalink (GL), MetalSystem
(MS), Reverso (RV), Systran (SY), XS (XS)
</bodyText>
<listItem confidence="0.991632708333333">
• Each of the systems is due to translate a
hundred source texts ranging from 250 to
300 words each. A corpus of 600
translations is thus produced.
• For each of the source texts, a corpus of 6
translations is produced automatically.
These translations are then regrouped by
series of six texts.
• According to the protocol initiated by
(White &amp; Forner, 2001) these series are
then ranked by medium adequacy score.
• Every 5 series, a series is extracted from
the whole. Packs of twenty series of target
translations are thus obtained and
submitted to human evaluators.
7.2.1 Evaluators’ tasks
• Each evaluator reads 10 series of 6
translations i.e. 60 texts.
• Each of these series is then read by six
different evaluators
• The evaluators must observe a ten minute
compulsory break every two series.
• The evaluators do not know that the texts
have been translated automatically.
</listItem>
<bodyText confidence="0.998836307692308">
The directive given to them is the following:
« rank these six texts from best to worst. If
you cannot manage to give a different ranking
to two texts, regroup them under the same
parenthesis and give them the same score, as in
the following example : 4 [1 2] 6 [3 5].”
The aim of this instruction is to produce
rankings that are similar to the rankings attributed
automatically.
Human judgement that ranks from best to worse
corresponds in reality to a set of the fluency,
adequacy and Informativeness criteria that can be
attributed to the texts translated automatically.
</bodyText>
<listItem confidence="0.957247">
7.2.2 Automatically generated scores
• X-score : syntactic score
• D-score : semantic score
</listItem>
<bodyText confidence="0.9963096">
Within the framework of the CESTA
evaluation campaign the scientific committee
decided to make use of the X-score only, the
semantic D-score having proved to be unstable
and that it could be advantageously replaced
by the a metric based on (Bogdan, B.; Hartley,
A.; Atwell, 2003), a reformulation of the D-
score developed by (Rajman, M. and T.
Hartley, 2001), and which we refer to as the
ROUGE metric in this article.
</bodyText>
<listItem confidence="0.992886466666667">
7.2.3 X-score: definition
• This score corresponds to a grammaticality
metric
• Each of the texts is previously parsed with
XELDA Xerox parser.
• 22 types of syntactic dependencies
identified through the corpus of automatic
translations.
• The syntactic profile of each source
document is computed. This profile is then
used to derive the X-score for each
document, making use of the following
formula:
• X-score = (#RELSUBJ+#RELSUBJPASS-
#PADJ-#ADVADJ)
</listItem>
<subsectionHeader confidence="0.903509">
7.3 The “ROUGE” protocol
</subsectionHeader>
<bodyText confidence="0.998576101449275">
This protocol, developed by Anthony Hartley in
(Bogdan, B.; Hartley, A.; Atwell, 2003), is a
semantic score. It is the result of a reformulation of
the D-Score, the semantic score initiated through
previous collaboration with Martin Rajman
(Rajman, M. and T. Hartley, 2001), as explained in
the previous section.
The original idea on which this protocol is based
relies on the fact that MT evaluation metrics that
“are based on comparing the distribution of
statistically significant words in corpora of MT
output and in human reference translation
corpora”.
The method used to measure MT quality is the
following: a statistical model for MT output
corpora and for a parallel corpus of human
translations, each statistically significant word
being highlighted in the corpus. On the other hand,
a statistical significance score is given for each
highlighted word. Then statistical models for MT
target texts and human translations are compared,
special attention being paid to words that are
automatically marked as significant in MT outputs,
whereas they do not appear to be marked as
significant in human translations. These words are
considered to be “over generated”. The same
operation is then carried out on “under generated
words”. At this stage, a third operation consists in
the marking of the words equally marked as
significant by the MT systems and the human
translations. The overall difference is then
calculated for each pair of texts in the corpora.
Three measures specifying differences in statistical
models for MT and human translations are then
implemented : the first one aiming at avoiding
“over generation”, the second one aiming at
avoiding “under generation” and the last one being
a combination of these two measures. The average
scores for each of the MT systems are then
computed.
As detailed in (Bogdan, B.; Hartley, A.; Atwell,
2003):
“1. The score of statistical significance is
computed for each word (with absolute frequency
≥ 2 in the particular text) for each text in the
corpus, as follows:
where:
Sword[text] is the score of statistical significance for
a particular word in a particular text[]
Pword[text] is the relative frequency of the word in
the text;
Pword[rest-corp] is the relative frequency of the same
word in the rest of the corpus, without this text;
Nword[txt-not-found] is the proportion of texts in the
corpus, where this word is not found (number of
texts, where it is not found divided by number of
texts in the corpus)❑
Pword[all-corp] is the relative frequency of the word
in the whole corpus, including this particular text
2. In the second stage, the lists of statistically
significant words for corresponding texts together
with their Sword[text] scores are compared across
different MT systems. Comparison is done in the
following way:
For all words which are present in lists of
statistically significant words both in the human
reference translation and in the MT output, we
compute the sum of changes of their Sword[text]
scores:
</bodyText>
<equation confidence="0.916871">
Stext.diff = ∑ (Sword[text.reference] − Sword[text.MT] )
</equation>
<bodyText confidence="0.999817142857143">
The score Stext.diff is added to the scores of all
&amp;quot;over-generated&amp;quot; words (words that do not appear
in the list of statistically significant words for
human reference translation, but are present in
such list for MT output). The resulting score
becomes the general &amp;quot;over-generation&amp;quot; score for
this particular text:
</bodyText>
<equation confidence="0.900293">
S = S +
over generation text
− . text diff
.
</equation>
<bodyText confidence="0.997320384615385">
The opposite &amp;quot;under-generation&amp;quot; score for
each text in the corpus is computed by adding
Stext.dif and all Sword[text] scores of &amp;quot;under-generated&amp;quot;
words – words present in the human reference
translation, but absent from the MT output.
It is more convenient to use inverted scores,
which increases as the MT system improves. These
scores, So.text and Su.text, could be interpreted as
scores for ability to avoid &amp;quot;over-generation&amp;quot; and
&amp;quot;under-generation&amp;quot; of statistically significant
words. The combined (o&amp;u) score is computed
similarly to the F-measure, where Precision and
Recall are equally important:
</bodyText>
<table confidence="0.885994576923077">
1
= ;
Sover
−generation .text
1
= ;
−generation .text
S o text
.
S u text
.
Sunder
Sword [text] = ln
(Pword[text] − Pword[rest−corp] )
∑ Sword.over− generated[text]
words .text
Sunder−generation.text St ex t .d iff + ∑ Sword.undergenerated[text]
words .text
]
× Nword
− −
[ txts not found
Pword [ all corp
− ]
So&amp;u.text S.text + Su
o
</table>
<page confidence="0.991721">
2
</page>
<bodyText confidence="0.968186857142857">
So.text
Su
.text
.text
The number of statistically significant words
could be different in each text, so in order to make
the scores compatible across texts we compute the
average over-generation and under-generation
scores per each statistically significant word in a
given text. For the otext score we divide So.text by the
number of statistically significant words in the MT
text, for the utext score we divide Su.text by the
number of statistically significant words in the
human (reference) translation:
</bodyText>
<figure confidence="0.8534472">
S o text
.
o = ;
text
nstatSignWo
S u text
.
u = ;
text
nstatSignWordsInHT
</figure>
<bodyText confidence="0.995229">
The general performance of an MT system for IE
tasks could be characterised by the average o-
score, u-score and u&amp;o-score for all texts in the
corpus”.
</bodyText>
<sectionHeader confidence="0.691362" genericHeader="method">
8 Time Schedule and result dissemination
</sectionHeader>
<bodyText confidence="0.99994647826087">
The CESTA evaluation campaign started in
January 2003 after having been labeled by the
French Ministry of Research. During year 2003
CESTA scientific committee went through
protocol detailed redefinition and specification and
a time schedule was agreed upon.
2004 first semester is being dedicated to corpus
untagging and the programming of CESTA
evaluation tool. Reference human translations will
also have to be produced and the implemented
evaluation tool submitted to trial and validation.
After this preliminary work, the first run will
start during autumn 2004. At the end of the first
campaign, result analysis will be carried out. A
workshop will then be organized for CESTA
participants. Then the second campaign will take
place at the end of Spring 2005, the terminological
adaptation phase being scheduled on a five month
scale.
After carrying out result analysis and final report
redaction, a public workshop will be organized and
the results disseminated and subject to publication
at the end of 2005.
</bodyText>
<sectionHeader confidence="0.996934" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999981258064516">
CESTA is the first European Campaign
dedicated to MT Evaluation. The results of the
campaign will be published in a final report and be
the object of an intermediary workshop between
the two campaigns and a final workshop at the end
of the campaign.
It is a noticeable point that the CESTA campaign
aims at providing a state of the art of automated
metrics in order to ensure protocol reusability. The
originality of the CESTA protocol lies in the
combination and contrastive use of three different
types of measures carried out in parallel with a
Meta evaluation of the metrics.
It is also important to note that CESTA aims at
providing a black box evaluation of available
Machine Translation technologies, rather than a
comparison of systems and interfaces, that can be
tuned to match a particular need. If systems had to
be compared, the fact that these applications
should be compared including all software lawyers
and ergonomic properties, ought to be taken into
consideration.
Moreover apart from providing a state of the art
through a Meta evaluation of the metrics used in its
protocol, thanks to the setting of this original
protocol that relies on the contrastive use of
complementary metrics, CESTA aims at protocol
reusability. One of the outputs of the campaign
will be the creation of a Machine Translation
evaluation toolkit that will be put at users and
system developers’ disposal.Acknowledgements
</bodyText>
<sectionHeader confidence="0.992143" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.836832">
Besançon, R. and Rajman, M., (2002). Evaluation
of aVector Space similarity measure in a
multilingual framework. Procs. 3rd
International Conference on Language
Resources and Evaluation, Las
Palmas,Spain,.1252
Bogdan, B.; Hartley, A.; Atwell E.; Statistical
modelling of MT output corpora for
Information Extraction Proceedings Corpus
Linguistics 2003, Lancaster, UK, 28-31
March 2003, pp. 62-70
Chaudiron, S. Technolangue. In:
http://www.apil.asso.fr/metil.htm, mars 2001
</reference>
<figureCaption confidence="0.403090166666667">
Chaudiron, S. L’évaluation des systèmes de
traitement de l’information textuelle : vers un
changement de paradigmes, Mémoire pour
l’habilitation à diriger des recherches en sciences
de l’information, présenté devant l’Université de
Paris 10, Paris, novembre 2001
</figureCaption>
<figure confidence="0.996457875">
u
2otext
utext
&amp; o
text
otext
+ utext
rdsInMT
</figure>
<reference confidence="0.999569589285715">
Dabbadie, M, Mustafa El Hadi, W., Timimi, I.
(2001). Setting a Methodology for Machine
Translation Evaluation. In: Machine
Translation Summit VIII, ISLE/EMTA,
Santiago de Compestela, Spain, 18-23
October 2001, pp. 49-54.
Dabbadie, M., Mustafa El Hadi, W., Timimi, I.,
(2002). Terminological Enrichment for non-
Interactive MT Evaluation. In: LREC 2002
Proceedings – Las Palmas de Gran Canaria,
Spain – 29th – 31st May 2002 – vol 6 – 1878-
1884
EAGLES-Evaluation-Workgroup. (1996).
EAGLES evaluation of natural language
processing systems. Final report, Center for
Sprogteknologi, Denmark, October 1996.
EAGLES (1999). EAGLES Reports (Expert
Advisory Group on Language Engineering
Standards)http://www.issco.unige.ch/project
s/eagles/ewg99.
ISLE (2001). MT Evaluation Classification,
Expanded Classification.
http://www.isi.edu/natural-
language/mteval/2b-MT-classification.htm.
ISO/IEC-9126. 1991. ISO/IEC 9126:1991 (E) —
Information Technology — Software
Product Evaluation — Quality
Characteristics and Guidelines for Their Use.
ISO/IEC, Geneva.
ISO (1999). Standard ISO/IEC 9126-1 Information
Technology – Software Engineering –
Quality characteristics and sub-
characteristics. Software Quality
Characteristics and Metrics - Part 1
ISO (1999). Standard ISO/IEC 9126-2 Information
Technology – Software Engineering –
Software products Quality : External Metrics
- Part 2
ISO/IEC-14598. 1998-2001. ISO/IEC 14598 —
Information technology — Software product
evaluation — Part 1: General overview
(1999), Part 2: Planning and management
(2000), Part 3: Process for developers
(2000), Part 4: Process for acquirers (1999),
Part 5: Process for evaluators (1998), Part 6:
Documentation of evaluation modules
(2001). ISO/IEC, Geneva.
ISSCO (2001) Machine Translation Evaluation :
An Invitation to Get Your Hands Dirty!,
ISSCO, University of Geneva, Workshop
organised by M. King (ISSCO) &amp; F. Reed,
(Mitre Corporation), April 19-24 2001.
King (1999a) EAGLES Evaluation Working
Group, report,http://www.issco.unige.ch/
projects/eagles.
King, M. (1999b). “ISO Standards as a Point of
Departure for EAGLES Work in EELS
Conference (European Evaluation of
Language Systems), 12-13 April 1999.
Mariani, Joseph. «Language Technologies :
Technolangue Action ». Presentation. In:
LREC&apos;2002 International Strategy Panel17, Las
Palmas, May 2002.
Nomura, H. and J. Isahara. (1992). The JEIDA
report on MT. In Workshop on MT
Evaluation: Basis for Future Directions, San
Diego, CA. Association for Machine
Translation in the Americas (AMTA).
Popescu-Belis, A. S. Manzi, and M. King. (2001).
Towards a two-stage taxonomy for MT
evaluation. In Workshop on MT Evaluation
”Who did what to whom?” at Mt Summit
VIII, pages 1–8, Santiago de Compostela,
Spain.
Rajman, M. and T. Hartley, (2001). Automatically
predicting MT systems rankings compatible
with Fluency, Adequacy or Informativeness
scores. Procs. 4th ISLE Workshop on MT
Evaluation, MT Summit VIII, 29-34.
Rajman, M. and T. Hartley, (2002). Automatic
ranking of MT systems. In: LREC 2002
Proceedings – Las Palmas de Gran Canaria,
Spain – 29th – 31st May 2002 – vol 4 – 1247-
1253
Reeder, F., K. Miller, J. Doyon, and J. White, J.
(2001). The naming of things and the
confusion of tongues: an MT metric. Procs.
4th ISLE Workshop on MT Evaluation, MT
Summit VIII, 55-59.
Sparck-Jones K., Gallier, J.R. (1996). Evaluating
Natural Language Processing Systems: An
Analysis and Review, Springer, Berlin.
TREC, NIST Website, last updated, August 1st,
2000, visited by the authors, 23-03-2003
Vanni, M. and K. Miller (2001). Scaling the ISLE
framework: validating tests of machine
translation quality for multi-dimensional
measurement. Procs. 4th ISLE Workshop on
MT Evaluation, MT Summit VIII, 21-27.
VanSlype., G. (1979). Critical study of methods
for evaluating the quality of MT. Technical
Report BR 19142, European Commission /
Directorate for General Scientific and
Technical Information Management (DG
XIII).
Véronis, J., Langlais, Ph. (2000). ARCADE:
évaluation de systèmes d&apos;alignement de textes
multilingues. In Chibout, K., Mariani, J.,
Masson, N., Neel, F. éds., (2000). Ressources et
évaluation en ingénierie de la langue, Duculot,
Coll. Champs linguistiques, et Collection
Universités Francophones (AUF).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000295">
<title confidence="0.9807725">Work-in-Progress project report : CESTA - Machine Translation Evaluation Campaign</title>
<author confidence="0.999716">Widad Mustafa El_Hadi Marianne Dabbadie Ismaïl Timimi Martin Rajman</author>
<affiliation confidence="0.9003975">IDIST / CERSATES IDIST / CERSATES IDIST / CERSATES LIA Universit6 de Lille 3 Universit6 de Lille 3 Universit6 de Lille 3 Ecole</affiliation>
<abstract confidence="0.366997142857143">Domaine universitaire Domaine universitaire Domaine universitaire Polytechnique du &amp;quot;Pont de Bois&amp;quot; du &amp;quot;Pont de Bois&amp;quot; du &amp;quot;Pont de Bois&amp;quot; F6d6rale de Lausanne rue du Barreau rue du Barreau rue du Barreau Bât. INR BP 149 BP 149 BP 149 CH-1015 Lausanne 59653 Villeneuve d&apos;Ascq 59653 Villeneuve d&apos;Ascq 59653 Villeneuve d&apos;Ascq Switzerland Cedex - France Cedex - France Cedex - France martin.rajman@epfl.ch mustafa@univ-lille3.fr dabbadie@univ-lille3.fr timimi@univ-lille3.fr</abstract>
<author confidence="0.915227">Philippe Langlais</author>
<affiliation confidence="0.75847325">RALI / DIRO - Universit6 de Montr6al C.P. 6128, succursale Centre-ville</affiliation>
<address confidence="0.8523395">Montr6al (Qu6bec) - Canada, H3C 3J7</address>
<email confidence="0.949632">felipe@IRO.UMontreal.</email>
<title confidence="0.540388">CA</title>
<author confidence="0.99317">Antony Hartley</author>
<affiliation confidence="0.604397666666667">University of Leeds Centre for Translation Studies</affiliation>
<address confidence="0.448750666666667">Woodhouse LEEDS LS2 UK</address>
<email confidence="0.961967">a.hartley@leeds.ac.uk</email>
<author confidence="0.972003">Andrei Popescu Belis</author>
<affiliation confidence="0.949339">University of Geneva 40 bvd du Pont d&apos;Arve CH-</affiliation>
<address confidence="0.7332445">1211 Geneva 4 Switzerland</address>
<note confidence="0.575877">Andrei.Popescu- Belis@issco.unige.ch</note>
<abstract confidence="0.991397304347826">CESTA, the first European Campaign dedicated to MT Evaluation, is a project labelled by the French Technolangue action. CESTA provides an evaluation of six commercial and academic MT systems using a protocol set by an international panel of experts. CESTA aims at producing reusable resources and information about reliability of the metrics. Two runs will be carried out: one using the system’s basic dictionary, another after terminological adaptation. Evaluation task, test material, resources, evaluation measures, metrics, will be detailed in the full paper. The protocol is the combination of a contrastive reference to: IBM “BLEU” protocol (Papineni, K., S. Roukos, T. Ward and Z. Wei-Jing, 2001); “BLANC” protocol derived from (Hartley, Rajman, 2002).; “ROUGE” protocol (Babych, Hartley, Atwell, 2003). The results of the campaign will be published in a final report and be the object of two intermediary and final workshops.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Besançon</author>
<author>M Rajman</author>
</authors>
<title>Evaluation of aVector Space similarity measure in a multilingual framework.</title>
<date>2002</date>
<booktitle>Procs. 3rd International Conference on Language Resources and Evaluation, Las Palmas,Spain,.1252</booktitle>
<marker>Besançon, Rajman, 2002</marker>
<rawString>Besançon, R. and Rajman, M., (2002). Evaluation of aVector Space similarity measure in a multilingual framework. Procs. 3rd International Conference on Language Resources and Evaluation, Las Palmas,Spain,.1252</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bogdan</author>
<author>A Hartley</author>
<author>E Atwell</author>
</authors>
<title>Statistical modelling of MT output corpora for Information Extraction</title>
<date>2003</date>
<booktitle>Proceedings Corpus Linguistics 2003,</booktitle>
<pages>62--70</pages>
<location>Lancaster, UK,</location>
<marker>Bogdan, Hartley, Atwell, 2003</marker>
<rawString>Bogdan, B.; Hartley, A.; Atwell E.; Statistical modelling of MT output corpora for Information Extraction Proceedings Corpus Linguistics 2003, Lancaster, UK, 28-31 March 2003, pp. 62-70</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Technolangue Chaudiron</author>
</authors>
<date>2001</date>
<note>In: http://www.apil.asso.fr/metil.htm, mars</note>
<marker>Chaudiron, 2001</marker>
<rawString>Chaudiron, S. Technolangue. In: http://www.apil.asso.fr/metil.htm, mars 2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dabbadie</author>
<author>Mustafa El Hadi</author>
<author>W Timimi</author>
<author>I</author>
</authors>
<title>Setting a Methodology for Machine Translation Evaluation. In:</title>
<date>2001</date>
<booktitle>Machine Translation Summit VIII, ISLE/EMTA, Santiago de Compestela,</booktitle>
<pages>49--54</pages>
<marker>Dabbadie, El Hadi, Timimi, I, 2001</marker>
<rawString>Dabbadie, M, Mustafa El Hadi, W., Timimi, I. (2001). Setting a Methodology for Machine Translation Evaluation. In: Machine Translation Summit VIII, ISLE/EMTA, Santiago de Compestela, Spain, 18-23 October 2001, pp. 49-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dabbadie</author>
<author>Mustafa El Hadi</author>
<author>W Timimi</author>
<author>I</author>
</authors>
<title>Terminological Enrichment for nonInteractive MT Evaluation. In:</title>
<date>2002</date>
<journal></journal>
<booktitle>LREC 2002 Proceedings – Las Palmas de Gran Canaria, Spain – 29th – 31st</booktitle>
<volume>6</volume>
<pages>1878--1884</pages>
<marker>Dabbadie, El Hadi, Timimi, I, 2002</marker>
<rawString>Dabbadie, M., Mustafa El Hadi, W., Timimi, I., (2002). Terminological Enrichment for nonInteractive MT Evaluation. In: LREC 2002 Proceedings – Las Palmas de Gran Canaria, Spain – 29th – 31st May 2002 – vol 6 – 1878-1884</rawString>
</citation>
<citation valid="true">
<authors>
<author>EAGLES-Evaluation-Workgroup</author>
</authors>
<title>EAGLES evaluation of natural language processing systems.</title>
<date>1996</date>
<tech>Final report,</tech>
<institution>Center for Sprogteknologi,</institution>
<marker>EAGLES-Evaluation-Workgroup, 1996</marker>
<rawString>EAGLES-Evaluation-Workgroup. (1996). EAGLES evaluation of natural language processing systems. Final report, Center for Sprogteknologi, Denmark, October 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EAGLES</author>
</authors>
<title>EAGLES Reports (Expert Advisory Group on Language Engineering Standards)http://www.issco.unige.ch/project s/eagles/ewg99.</title>
<date>1999</date>
<contexts>
<context position="7298" citStr="EAGLES, 1999" startWordPosition="1103" endWordPosition="1104">orked out between a source sentence and a target sentence on a 1 to 5 scale. • Intelligibility, that corresponds to linguistic acceptability of a translation is measured on a 1 to 5 evaluation scale. • Informativeness: the test is carried out on reading of the target text alone. A questionnaire on text informative content is displayed allowing to work out a measure calculated on the basis of the percentage of good answers provided in system translation. In 1995, the OVUM report proposes to compare commercial Machine Translation systems on the basis of ten criteria. In 1996, the EAGLES report (EAGLES, 1999) sets new standards for Natural Language Processing software evaluation on the background of ISO 9126. Initiated in 1999, and coordinated by Pr Antonio Zampolli, the ISLE project is divided into three working groups, one being a Machine Translation group. Starting from ISO 9126 standard (King, 1999b), the aim of the project is to produce two taxonomies (c.f. section 3 of this article) and : • One defining quality subcriteria with the aim of refining the six criteria defined by ISO 9126 (i.e. functionality, reliability, user-friendliness, efficiency, maintenance portability) • The second one sp</context>
</contexts>
<marker>EAGLES, 1999</marker>
<rawString>EAGLES (1999). EAGLES Reports (Expert Advisory Group on Language Engineering Standards)http://www.issco.unige.ch/project s/eagles/ewg99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ISLE</author>
</authors>
<title>MT Evaluation Classification, Expanded Classification.</title>
<date>2001</date>
<note>http://www.isi.edu/naturallanguage/mteval/2b-MT-classification.htm.</note>
<marker>ISLE, 2001</marker>
<rawString>ISLE (2001). MT Evaluation Classification, Expanded Classification. http://www.isi.edu/naturallanguage/mteval/2b-MT-classification.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ISOIEC-9126</author>
</authors>
<date>1991</date>
<booktitle>ISO/IEC 9126:1991 (E) — Information Technology — Software Product Evaluation — Quality Characteristics and Guidelines for Their Use. ISO/IEC,</booktitle>
<location>Geneva.</location>
<marker>ISOIEC-9126, 1991</marker>
<rawString>ISO/IEC-9126. 1991. ISO/IEC 9126:1991 (E) — Information Technology — Software Product Evaluation — Quality Characteristics and Guidelines for Their Use. ISO/IEC, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ISO</author>
</authors>
<date>1999</date>
<booktitle>Standard ISO/IEC 9126-1 Information Technology – Software Engineering – Quality characteristics and subcharacteristics. Software Quality Characteristics and Metrics - Part</booktitle>
<volume>1</volume>
<marker>ISO, 1999</marker>
<rawString>ISO (1999). Standard ISO/IEC 9126-1 Information Technology – Software Engineering – Quality characteristics and subcharacteristics. Software Quality Characteristics and Metrics - Part 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>ISO</author>
</authors>
<date>1999</date>
<booktitle>Standard ISO/IEC 9126-2 Information Technology – Software Engineering – Software products Quality : External Metrics - Part</booktitle>
<volume>2</volume>
<marker>ISO, 1999</marker>
<rawString>ISO (1999). Standard ISO/IEC 9126-2 Information Technology – Software Engineering – Software products Quality : External Metrics - Part 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>ISOIEC-14598</author>
</authors>
<title>ISO/IEC 14598 — Information technology — Software product evaluation — Part 1: General overview</title>
<date>1999</date>
<publisher>ISO/IEC, Geneva.</publisher>
<marker>ISOIEC-14598, 1999</marker>
<rawString>ISO/IEC-14598. 1998-2001. ISO/IEC 14598 — Information technology — Software product evaluation — Part 1: General overview (1999), Part 2: Planning and management (2000), Part 3: Process for developers (2000), Part 4: Process for acquirers (1999), Part 5: Process for evaluators (1998), Part 6: Documentation of evaluation modules (2001). ISO/IEC, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ISSCO</author>
</authors>
<title>Machine Translation Evaluation : An Invitation to Get Your Hands Dirty!, ISSCO,</title>
<date>2001</date>
<institution>University of Geneva, Workshop</institution>
<note>organised by</note>
<marker>ISSCO, 2001</marker>
<rawString>ISSCO (2001) Machine Translation Evaluation : An Invitation to Get Your Hands Dirty!, ISSCO, University of Geneva, Workshop organised by M. King (ISSCO) &amp; F. Reed, (Mitre Corporation), April 19-24 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>King</author>
</authors>
<title>EAGLES Evaluation Working Group, report,http://www.issco.unige.ch/ projects/eagles.</title>
<date>1999</date>
<contexts>
<context position="7597" citStr="King, 1999" startWordPosition="1149" endWordPosition="1150">t informative content is displayed allowing to work out a measure calculated on the basis of the percentage of good answers provided in system translation. In 1995, the OVUM report proposes to compare commercial Machine Translation systems on the basis of ten criteria. In 1996, the EAGLES report (EAGLES, 1999) sets new standards for Natural Language Processing software evaluation on the background of ISO 9126. Initiated in 1999, and coordinated by Pr Antonio Zampolli, the ISLE project is divided into three working groups, one being a Machine Translation group. Starting from ISO 9126 standard (King, 1999b), the aim of the project is to produce two taxonomies (c.f. section 3 of this article) and : • One defining quality subcriteria with the aim of refining the six criteria defined by ISO 9126 (i.e. functionality, reliability, user-friendliness, efficiency, maintenance portability) • The second one specifying use contexts that define the type of task induced the use of a by Machine Translation system, the types of users and input data. This taxonomy uses contextual parameters to select and order the quality criteria subject to evaluation. This taxonomy can be viewed and downloaded on the ISSCO </context>
</contexts>
<marker>King, 1999</marker>
<rawString>King (1999a) EAGLES Evaluation Working Group, report,http://www.issco.unige.ch/ projects/eagles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M King</author>
</authors>
<title>ISO Standards as a Point of Departure for</title>
<date>1999</date>
<booktitle>EAGLES Work in EELS Conference (European Evaluation of Language Systems),</booktitle>
<pages>12--13</pages>
<contexts>
<context position="7597" citStr="King, 1999" startWordPosition="1149" endWordPosition="1150">t informative content is displayed allowing to work out a measure calculated on the basis of the percentage of good answers provided in system translation. In 1995, the OVUM report proposes to compare commercial Machine Translation systems on the basis of ten criteria. In 1996, the EAGLES report (EAGLES, 1999) sets new standards for Natural Language Processing software evaluation on the background of ISO 9126. Initiated in 1999, and coordinated by Pr Antonio Zampolli, the ISLE project is divided into three working groups, one being a Machine Translation group. Starting from ISO 9126 standard (King, 1999b), the aim of the project is to produce two taxonomies (c.f. section 3 of this article) and : • One defining quality subcriteria with the aim of refining the six criteria defined by ISO 9126 (i.e. functionality, reliability, user-friendliness, efficiency, maintenance portability) • The second one specifying use contexts that define the type of task induced the use of a by Machine Translation system, the types of users and input data. This taxonomy uses contextual parameters to select and order the quality criteria subject to evaluation. This taxonomy can be viewed and downloaded on the ISSCO </context>
</contexts>
<marker>King, 1999</marker>
<rawString>King, M. (1999b). “ISO Standards as a Point of Departure for EAGLES Work in EELS Conference (European Evaluation of Language Systems), 12-13 April 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Mariani</author>
</authors>
<title>Language Technologies : Technolangue Action ». Presentation. In:</title>
<date>2002</date>
<booktitle>LREC&apos;2002 International Strategy Panel17,</booktitle>
<location>Las Palmas,</location>
<marker>Mariani, 2002</marker>
<rawString>Mariani, Joseph. «Language Technologies : Technolangue Action ». Presentation. In: LREC&apos;2002 International Strategy Panel17, Las Palmas, May 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nomura</author>
<author>J Isahara</author>
</authors>
<title>The JEIDA report on MT.</title>
<date>1992</date>
<booktitle>In Workshop on MT Evaluation: Basis for Future Directions,</booktitle>
<publisher>Association for</publisher>
<location>San Diego, CA.</location>
<marker>Nomura, Isahara, 1992</marker>
<rawString>Nomura, H. and J. Isahara. (1992). The JEIDA report on MT. In Workshop on MT Evaluation: Basis for Future Directions, San Diego, CA. Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Manzi Popescu-Belis</author>
<author>M King</author>
</authors>
<title>Towards a two-stage taxonomy for MT evaluation.</title>
<date>2001</date>
<booktitle>In Workshop on MT Evaluation ”Who did what to whom?” at Mt Summit VIII,</booktitle>
<pages>1--8</pages>
<location>Santiago de Compostela,</location>
<marker>Popescu-Belis, King, 2001</marker>
<rawString>Popescu-Belis, A. S. Manzi, and M. King. (2001). Towards a two-stage taxonomy for MT evaluation. In Workshop on MT Evaluation ”Who did what to whom?” at Mt Summit VIII, pages 1–8, Santiago de Compostela, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rajman</author>
<author>T Hartley</author>
</authors>
<title>Automatically predicting MT systems rankings compatible with Fluency, Adequacy or Informativeness scores.</title>
<date>2001</date>
<booktitle>Procs. 4th ISLE Workshop on MT Evaluation, MT Summit VIII,</booktitle>
<pages>29--34</pages>
<marker>Rajman, Hartley, 2001</marker>
<rawString>Rajman, M. and T. Hartley, (2001). Automatically predicting MT systems rankings compatible with Fluency, Adequacy or Informativeness scores. Procs. 4th ISLE Workshop on MT Evaluation, MT Summit VIII, 29-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rajman</author>
<author>T Hartley</author>
</authors>
<title>Automatic ranking of MT systems. In:</title>
<date>2002</date>
<journal></journal>
<booktitle>LREC 2002 Proceedings – Las Palmas de Gran Canaria, Spain – 29th – 31st</booktitle>
<volume>4</volume>
<pages>1247--1253</pages>
<marker>Rajman, Hartley, 2002</marker>
<rawString>Rajman, M. and T. Hartley, (2002). Automatic ranking of MT systems. In: LREC 2002 Proceedings – Las Palmas de Gran Canaria, Spain – 29th – 31st May 2002 – vol 4 – 1247-1253</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Reeder</author>
<author>K Miller</author>
<author>J Doyon</author>
<author>J White</author>
<author>J</author>
</authors>
<title>The naming of things and the confusion of tongues: an</title>
<date>2001</date>
<booktitle>MT metric. Procs. 4th ISLE Workshop on MT Evaluation, MT Summit VIII,</booktitle>
<pages>55--59</pages>
<marker>Reeder, Miller, Doyon, White, J, 2001</marker>
<rawString>Reeder, F., K. Miller, J. Doyon, and J. White, J. (2001). The naming of things and the confusion of tongues: an MT metric. Procs. 4th ISLE Workshop on MT Evaluation, MT Summit VIII, 55-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck-Jones</author>
<author>J R Gallier</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review,</title>
<date>1996</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="8989" citStr="Sparck-Jones &amp; Gallier, 1996" startWordPosition="1362" endWordPosition="1365">), making use of the IBM BLEU metric is mentioned in the CESTA protocol (c.f. section 8.1 of this article). 3 User-oriented evaluations An emerging evaluation methodology in NLP technology focuses on quality requirements analysis. The needs and consequently the satisfaction of end-users, and this will depend on the tasks and expected results requirement domains, which we have identified as diagnostic quality dimensions. One of the most suitable methods in this type of evaluation is the adequacy evaluation that aims at finding out whether a system or product is adequate to someone’s needs (see Sparck-Jones &amp; Gallier, 1996 and King, 1996 among many others for a more detailed discussion of these issues). This approach encourages communication between users and developers. The definition of the CESTA evaluation protocol took into account the Framework for MT Evaluation in ISLE (FEMTI), available online. FEMTI offers the possibility to define evaluation requirements, then to select relevant &apos;qualities&apos;, and the metrics commonly used to score them (cf. ISO/IEC 9126, 14598). The CESTA evaluation methodology is founded on a black box approach. CESTA evaluators considered a generic user, which is interested in general</context>
</contexts>
<marker>Sparck-Jones, Gallier, 1996</marker>
<rawString>Sparck-Jones K., Gallier, J.R. (1996). Evaluating Natural Language Processing Systems: An Analysis and Review, Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST Website TREC</author>
<author>last updated</author>
</authors>
<title>visited by the authors,</title>
<date>2000</date>
<pages>23--03</pages>
<marker>TREC, updated, 2000</marker>
<rawString>TREC, NIST Website, last updated, August 1st, 2000, visited by the authors, 23-03-2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vanni</author>
<author>K Miller</author>
</authors>
<title>Scaling the ISLE framework: validating tests of machine translation quality for multi-dimensional measurement.</title>
<date>2001</date>
<booktitle>Procs. 4th ISLE Workshop on MT Evaluation, MT Summit VIII,</booktitle>
<pages>21--27</pages>
<marker>Vanni, Miller, 2001</marker>
<rawString>Vanni, M. and K. Miller (2001). Scaling the ISLE framework: validating tests of machine translation quality for multi-dimensional measurement. Procs. 4th ISLE Workshop on MT Evaluation, MT Summit VIII, 21-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G VanSlype</author>
</authors>
<title>Critical study of methods for evaluating the quality of MT.</title>
<date>1979</date>
<booktitle>European Commission / Directorate for General Scientific and Technical Information Management (DG XIII).</booktitle>
<tech>Technical Report BR 19142,</tech>
<marker>VanSlype, 1979</marker>
<rawString>VanSlype., G. (1979). Critical study of methods for evaluating the quality of MT. Technical Report BR 19142, European Commission / Directorate for General Scientific and Technical Information Management (DG XIII).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Véronis</author>
<author>Ph Langlais</author>
</authors>
<title>ARCADE: évaluation de systèmes d&apos;alignement de textes multilingues. In</title>
<date>2000</date>
<marker>Véronis, Langlais, 2000</marker>
<rawString>Véronis, J., Langlais, Ph. (2000). ARCADE: évaluation de systèmes d&apos;alignement de textes multilingues. In Chibout, K., Mariani, J., Masson, N., Neel, F. éds., (2000). Ressources et évaluation en ingénierie de la langue, Duculot, Coll. Champs linguistiques, et Collection Universités Francophones (AUF).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>