<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000228">
<title confidence="0.997589">
Adaptive Parameters for Entity Recognition with Perceptron HMMs
</title>
<author confidence="0.985081">
Massimiliano Ciaramita* Olivier Chapelle
</author>
<affiliation confidence="0.953219">
Google Yahoo! Research
</affiliation>
<address confidence="0.538709">
Z¨urich, Switzerland Sunnyvale, CA, USA
</address>
<email confidence="0.997448">
massi@google.com chap@yahoo-inc.com
</email>
<sectionHeader confidence="0.993861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951">
We discuss the problem of model adapta-
tion for the task of named entity recog-
nition with respect to the variation of la-
bel distributions in data from different do-
mains. We investigate an adaptive exten-
sion of the sequence perceptron, where the
adaptive component includes parameters
estimated from unlabelled data in combi-
nation with background knowledge in the
form of gazetteers. We apply this idea
empirically on adaptation experiments in-
volving two newswire datasets from dif-
ferent domains and compare with other
popular methods such as self training and
structural correspondence learning.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998287603773585">
Model adaptation is a central problem in learning-
based natural language processing. In the typical
setting a model is trained on annotated in domain,
or source, data, and is used on out of domain, or
target, data. The main difference with respect to
similar problems such as semi-supervised learning
is that source and target data are not assumed to
be drawn from the same distribution, which might
actually differ in relevant distributional properties:
topic, domain, genre, style, etc. In some formu-
lations of the problem a few target labeled data is
assumed to be available (Daum´e III, 2007). How-
ever, we are interested in the case in which no la-
beled data is available from the target domain –
except for evaluation purposes and fine tuning of
hyperparameters.
Most of the work in adaptation has focused
so far on the input side; e.g, proposing solutions
based on generating shared source-target represen-
tations (Blitzer et al., 2006). Here we focus in-
stead on the output aspect. We hypothesize that
This work was carried out while the first author was
working at Yahoo! Research Barcelona.
part of the loss incurred in using a model out of
domain is due to its built-in class priors which
do not match the class distribution in the target
data. Thus we attempt to explicitly correct the
prediction of a pre-trained model for a given la-
bel by taking into account a noisy estimate of the
label frequency in the target data. The correc-
tion is carried out by means of adaptive param-
eters, estimated from unlabelled target data and
background “world knowledge” in the form of
gazetteers, and taken in consideration in the de-
coding phase. We built a suitable dataset for exper-
imenting with different adaptation approaches for
named entity recognition (NER). The main find-
ings from our experiments are as follows. First,
the problem is challenging and only marginal im-
provements are possible under all evaluated frame-
works. Second, we found that our method com-
pares well with current state-of-the-art approaches
such as self training and structural correspondence
learning (McClosky et al., 2006; Blitzer et al.,
2006) and taps on an interesting aspect which
seems worth of further research. Although we
concentrate on a segmentation task within a spe-
cific framework, the perceptron HMM introduced
by Collins (2002), we speculate that the same in-
tuition could be straightforwardly applied in other
learning frameworks (e.g., Support Vector Ma-
chines) and different tasks (e.g., standard classi-
fication).
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999283222222222">
Recent work in domain adaptation has focused
on approaches such as self-training and struc-
tural correspondence learning (SCL). The former
approach involves adding self-labeled data from
the target domain produced by a model trained
in-domain (McClosky et al., 2006). The latter
approach focuses on ways of generating shared
source-target representations based on good cross-
domain (pivot) features (Blitzer et al., 2006) (see
</bodyText>
<page confidence="0.820732">
1
</page>
<note confidence="0.630687">
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 1–7,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999873419354839">
also (Ando, 2004)). Self training has proved ef-
fective in syntactic parsing, particularly in tan-
dem with discriminative re-ranking (Charniak and
Johnson, 2005), while the SCL has been applied
successfully to tasks such PoS tagging and opin-
ion analysis (Blitzer et al., 2006; Blitzer et al.,
2007). We address a different aspect of the adapta-
tion problem, namely the difference in label distri-
butions between source and target domains. Chan
and Ng (2006) proposed correcting the class priors
for domain adaptation purposes in a word sense
disambiguation task. They adopt a generative
framework where the base model is a naive Bayes
classifier and priors are re-estimated with EM. The
approach proposed by Chelba and Acero (2004) is
also related as they propose a MAP adaptation via
Gaussian priors of a MaxEnt model for recovering
the correct capitalization of text.
Domain adaptation naturally invokes the exis-
tence of a specific task and data. As such it is
natural to consider the modeling aspects within
the context of a specific application. Here we
focus on the problem of named entity recogni-
tion (NER). There is still little work on adapta-
tion for NER. Ando (2004) reports successful ex-
periments on adapting with an SCL-like approach,
while Ciaramita and Altun (2005) effectively used
external knowledge in the form of gazetteers in
a semi-Markov model. Mika et al. (2008) used
Wikipedia to generate additional training data for
domain adaptation purposes.
</bodyText>
<sectionHeader confidence="0.979804" genericHeader="method">
3 Problem statement
</sectionHeader>
<bodyText confidence="0.999622">
Named entity taggers detect mentions of instances
of pre-defined categories such as person (Per),
location (Loc), organization (Org) and miscel-
laneous (Misc). The problem can be naturally
framed as a segmentation and labeling task. State
of the art systems, e.g., based on sequential op-
timization, achieve excellent accuracy in domain.
However, accuracy degrades if the target data di-
verges in relevant distributional aspects from the
source. As an example, the following is the out-
put of a perceptron HMM1 trained on the CoNLL
2003 English data (news) (Sang and Muelder,
2003) when applied to a molecular biology text:2
</bodyText>
<footnote confidence="0.999474833333333">
1We used the implementation available from http:
//sourceforge.net/projects/supersensetag,
more details on this tagger can be found in (Ciaramita and
Altun, 2006).
2The same model achieves F-scores well in excess of 90%
evaluated in domain.
</footnote>
<note confidence="0.318461">
(1) Cdc2-cyclin Org B-activated Polo-like mise
</note>
<bodyText confidence="0.996439947368421">
kinase specifically phosphorylates at least three
components of APC Org .
The tagger predicts several CoNLL entities which
are unlikely to occur in that context. One source
of confusion is probably the shape of words, in-
cluding case, numbers, and non alphabetical char-
acters, which are also typical, and thus mislead-
ing, of unrelated CoNLL entities. However, we
argue that the problem is partially due to the pa-
rameters learned which reflect the distribution of
classes in the source data. The parameter, acting
as biased priors, lead the tagger to generate inap-
propriate distributions of labels. We propose that
this aspect of the problem might be alleviated by
correcting the score for each class with an estimate
of the class frequency in the target data. Thus,
with respect to the example, we would like to de-
crease the score of “Org” labels according to their
expected frequency in a molecular biology corpus.
</bodyText>
<sectionHeader confidence="0.922259" genericHeader="method">
4 A perceptron with adjustable priors
</sectionHeader>
<bodyText confidence="0.986016545454545">
As generic taggers we adopt perceptron-trained
HMMs (Collins, 2002) which have excellent ef-
ficiency/performance trade-off (Nguyen and Guo,
2007). The objective of learning is a discrimi-
nant F : X x Y —* IR, where Y denotes se-
quences of labels from a pre-defined set of cate-
gories Y . F(x, y; α) = (α, 4b(x, y)) is linear in
a feature representation 4b defined over a joint in-
put/output space,3 a global feature representation
mapping each (x, y) pair to a vector of feature
counts 4b(x, y) E IRd:
</bodyText>
<equation confidence="0.976365666666667">
|y|
[4&apos;(x, y)Ii = L Oi(yj−1,yj,x), (2)
j=1
</equation>
<bodyText confidence="0.980120142857143">
where Oi is a (binary) predicate. Given an input
sequence x, we find the optimal label sequence,
f(x; α) = arg maxyEY F(x, y; α), with Viterbi
decoding. The model α is learned with the per-
ceptron algorithm.
Each feature represents a spelling or contex-
tual property, or the previous label. The sim-
plest baseline (model B) uses the features listed
in the upper half of Table 1. In previous work
on NER adaptation, Ciaramita and Altun (2005)
found that gazetteers, in combination with semi-
Markov models, significantly improved adapta-
tion. Similarly, we define additional features using
3(u, v) denoting the inner product between u and v.
</bodyText>
<page confidence="0.989308">
2
</page>
<table confidence="0.973922428571428">
Model B features
Feature example token feature value(s) Position
Lowercase word Pierre pierre i-1, i, i+1
Part of Speech Pierre NNP i-1, i, i+1
Word Shape Pierre Xx i-1, i, i+1
Suffix2/3 Pierre fre, rre} i
Prefix2/3 Pierre fpi, pie} i
Previous label Vinken (in “Pierre Vinken”) B-PER (label on “Pierre”) i
Additional features of model BG
Feature example token feature value(s) Position
InGazetteer Islands (in “Cayman Islands”) I-Country2 (inside a 2-word country name) i-1, i, i+1
Most frequent supersense Eve B-Per1 (1 token Person label) i
2 most frequent supersenses Eve B-Per-Time1 (1 token Person/Time label) i
Number of supersenses Eve B-NSS41 i
</table>
<tableCaption confidence="0.779095166666667">
Table 1. Feature list and examples. The upper half lists the features for the baseline tagger (B), the lower half
lists the additional features extracted from the gazetteers included to the second non-adapted tagger (BG). The
last number on the feature indicates the length of the entry in the list; e.g., “Islands” in the example is the end of
a two-word item, in the country gazetteer, because of “Cayman Islands”. The remaining features capture the most
frequent Wordnet supersense of the word, the first and second most frequent supersenses, and the total number of
supersenses.
</tableCaption>
<bodyText confidence="0.999936571428571">
the gazetteers from GATE,4 (Cunningham et al.,
2002) namely, countries, person first/last names,
trigger words; and also from Wordnet: using the
lexicographers or supersense labels; and a list of
company names from Fortune 500. For this sec-
ond baseline (model BG) we also extract the fea-
tures in the bottom half of Table 1.
</bodyText>
<subsectionHeader confidence="0.999698">
4.1 Decoding with external priors
</subsectionHeader>
<bodyText confidence="0.999973333333333">
In our method training is performed on the source
data using the perceptron algorithm. Adaptation
takes place at decoding time, when the score of
the entity labels is adjusted according to a k-
dimensional parameter vector B, k = |Y |, esti-
mated by comparing the source and the unlabeled
target data. The score of a sequence y� for input x
in the target domain is computed with a variant of
the original discriminant:
</bodyText>
<equation confidence="0.677652">
�Oi(yj−1, yj, x)αi + rByj (3)
</equation>
<bodyText confidence="0.999959666666667">
where Byj is the adaptive parameter associated
with yj, and r is a scaling factor. The new predic-
tion for x is f&apos;(x; α) = arg maxyEY F&apos;(x, y; α).
</bodyText>
<sectionHeader confidence="0.96621" genericHeader="method">
5 Adaptive parameters
</sectionHeader>
<subsectionHeader confidence="0.929916">
5.1 Theta
</subsectionHeader>
<bodyText confidence="0.999923666666667">
The vector B encodes information about the ex-
pected difference in frequency of each cate-
gory between source and target. Let gQ(c) =
</bodyText>
<equation confidence="0.899063714285714">
4http://www.gate.ac.uk/.
°°unt(
� �, count(c�, cQ) be an estimate of the relative fre-
quency of class c in corpus Q. We propose to for-
mulate Bc as:
Bc = gT (c) − gS(c) (4)
gS(c)
</equation>
<bodyText confidence="0.9999915">
where T and 5 are, respectively, the source and
target data. This is the ratio of the difference be-
tween in and out domain relative frequencies for
class c, with respect to the in domain frequency.
Intuitively, gS(c) represents an estimate of the fre-
quency of c in the source 5, and Bc an estimate
of the expected decrease/increase as a fraction of
the initial guess; Bc is negative if class c is less
frequent in the target data than in the source data,
and positive otherwise. From this, it is clear that
equation (3) will offset the scores in the desired
direction.
A crucial issue is the estimation of count(c, Q),
a guess of the frequency of c in Q. A simple
solution could be to count directly the class fre-
quencies from the labeled source data, and to ob-
tain a noisy estimate on the target data by count-
ing the occurrence of entities that have known la-
bels in the source data. This approach unfortu-
nately works very badly for at least two reasons.
First, the number of entities in each class reflects
the frequency of the class in the source. There-
fore using lists of entities from the source as prox-
ies for the class in the target data can transfer the
source bias to the target. Second, entities can have
different senses in different domains; e.g., several
English city names occur in the Wall Street Jour-
nal as locations (Liverpool, Manchester, etc.) and
</bodyText>
<equation confidence="0.999592">
F&apos;(x, y; α) =
|y |d
L
j=1 i=1
</equation>
<page confidence="0.988272">
3
</page>
<table confidence="0.9999469">
Attribute CoNLL BBN-4
# tokens 300K 1.046M
Source Reuters Wall Street Journal
Domain General news Financial
Years 1992 1987
# entities 34,841 58,637
Loc 30.48% 22.51%
Per 28.58% 20.08%
Org 26.55% 46.27%
Misc 14.38% 10.41%
</table>
<tableCaption confidence="0.999778">
Table 2. BBN and CoNLL datasets.
</tableCaption>
<bodyText confidence="0.999847833333333">
in Reuters news as both locations and organiza-
tions (football clubs). We propose to use lists of
words which are strongly associated with entities
of specific classes but are extracted from an inde-
pendent third source. In this way, we hope the bias
they carry will be transferred in similar ways to
both source and target. Similarly, potential am-
biguities should be randomly distributed between
source and target. Thus, as a first approximation,
we propose that given a list of words Lc, suppos-
edly related to c and generated independently from
source and target, count(c, Q) can be defined as:
</bodyText>
<equation confidence="0.975162">
�count(c, Q) ≡ count(w, Q) (5)
wEL�
</equation>
<subsectionHeader confidence="0.984101">
5.2 Tau
</subsectionHeader>
<bodyText confidence="0.9993417">
The scalar τ needs to be large enough to revise
the decision of the base model, if necessary. How-
ever, τ should not be too large, otherwise the best
prediction of the base model would be ignored.
In order for τ to have an effective, but balanced,
magnitude we introduce a simple notion of mar-
gin. Let the score of a given label ys on token s
be: G(x, ys; α) = �d i=1 φi(ys_1, ys,x)αi, and let
ys = arg maxyEY G(x, y; α), we define the mar-
gin on s as:
</bodyText>
<equation confidence="0.8362735">
Ms ≡ min (G(x, ys; α) − G(x, ys; α)). (6)
ys=,4ˆys
</equation>
<bodyText confidence="0.999924">
The mean of M provides a rough quantification
of the necessary amount by which we need to
offset the scores G(x, ys; α) in order to change
the predictions. As a first guess, we take τ =
µ(MS) = |s |Es|S |Ms, which we interpret as an
upper bound on the desired value of τ. While
experimenting on the development data we found
that τ/2 yields good results.
</bodyText>
<sectionHeader confidence="0.99604" genericHeader="method">
6 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.974837">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.991983866666667">
We used two datasets for evaluation. The first
is the English CoNLL 2003 dataset (Sang and
Muelder, 2003), a corpus of Reuters news an-
notated with person, location, organization and
miscellaneous entity tags. The second is the
BBN corpus (BBN, 2005), which supplements the
WSJ Penn TreeBank with annotation for 105 cat-
egories: named entities, nominal entities and nu-
meric types. We made the two datasets “seman-
tically” compatible as follows. We tagged a large
collection of text from the English Wikipedia with
CoNLL and BBN taggers. We counted the fre-
quencies of BBN/CoNLL tag pairs for the same
strings, and assigned each BBN tag the most fre-
quent CoNLL tag;5 e.g.,
</bodyText>
<equation confidence="0.4199856">
BBN tag CoNLL tag
Work of art:Book → Misc
Organization:Educational → Org
Location:Continent → Loc
Person → Per
</equation>
<bodyText confidence="0.99904165">
48 BBN-to-CoNLL pairs were labelled in this
way. Remaining categories, e.g., descriptive and
numerical types, were mapped to the Outside tag
as they are not marked in CoNLL. Finally, we sub-
stituted all tags in the BBN corpus with the corre-
sponding CoNLL tag, we call this corpus BBN-
4. The data is summarized in Table 2. Notice
the different label distributions: the BBN-4 data
is characterized by a skewed distribution of labels
with organization by far the most frequent class,
while the CoNLL data has a more uniform dis-
tribution with location as the most frequent class.
The CoNLL data was randomly split in three dis-
joint sets of sentences for training (16,540 sen-
tences), development (2.068) and test (2,136). For
BBN-4 we used WSJ sections 2-21 for training
(39,823), section 22 for development (1,700) and
section 23 for test (2,416). We evaluated models in
both directions; i.e., swapping CoNLL and BBN-4
as source/target.
</bodyText>
<subsectionHeader confidence="0.999743">
6.2 Model tuning
</subsectionHeader>
<bodyText confidence="0.999909">
We regularize the perceptrons by averaging (Fre-
und and Schapire, 1999). The perceptron HMM
</bodyText>
<footnote confidence="0.96583325">
5A simpler approach might that of manually mapping the
two tagsets, however a number of cases that are not trivial to
resolve emerges in this way. For this reason we decided to
adopt the described data-driven heuristic approach.
</footnote>
<page confidence="0.997494">
4
</page>
<bodyText confidence="0.9898320625">
has only one hyper-parameter, the number of train-
ing iterations (or epochs). Models trained for ap-
plication out of domain can benefit from early
stopping which provides an additional mean of
regularization. For all models compared we used
the development sets for choosing the number of
epochs for training the perceptron on the source
data. This is an important step as different adapta-
tion approaches yield different overfitting pattern
and it is important to control for this factor for a
fair comparison. As an example, we found that
the self-training models consistently overfit after
just a few iterations after which performance has a
steep drop. The order of presentation of instances
in the training algorithm is randomized; for each
method we repeat the process 10 times and report
average F-score and standard error.
The vector 0 was estimated using one of the
same gazetteers used in the base tagger (BG), a
list of 1,438 trigger words from GATE.6 These
are words associated with certain categories; e.g.,
“abbess/Per”, “academy/Org”, “caves/Loc”, and
“manifesto/Misc”. The lists for different classes
contain varying numbers of items and might con-
tain misleading words. To obtain more reliable es-
timates of comparable magnitude between classes
we computed equation (4) several times by sam-
pling an equal number of words from each list
and taking the mean. On the development set this
proved better than computing the counts from the
entire list.
Other sources could be evaluated, for exam-
ple lists of entities of each class extracted from
Wikipedia. We used all single-word triggers: 191
for Loc, 171 for Misc, 89 for Org and 592 for Per.
With each list we estimated 0 as in Section 5.1 for
each of the four labels starting with “B”, i.e., en-
tity beginnings, 0 = 0 for the other five labels. To
find 0 we use as source 5, the in-domain data, and
as target T the out-domain data. The lists contain
different number of items and might contain mis-
leading words.
To set r we compute the mean margin (6)
on CoNLL, using the tagger trained on CoNLL
(mean(M3) Pz� 50), similarly for BBN-4
(mean(M3) Pz� 38). We used the development
set to fine tune the adaptive rate setting it equal
to T = 2mean(M3).
</bodyText>
<footnote confidence="0.993533">
6This list corresponds to the list of words L. of Sec-
tion 5.1.
</footnote>
<subsectionHeader confidence="0.995661">
6.3 Self training
</subsectionHeader>
<bodyText confidence="0.999912090909091">
To compare with self-training we trained a tagger
(BG) on the training set of CoNLL. With the tag-
ger we annotated the training set of BBN-4, and
added the self-labeled data, 39,823 BBN-4 sen-
tences, to the gold standard CoNLL training. Sim-
ilarly, in the reverse direction we trained a tagger
(BG) on the training set of BBN-4, annotated the
training set of CoNLL, and added the self-labeled
16,540 CoNLL sentences to the BBN-4 training.
We denote these models BGSELF, and the aug-
mented sources as CoNLL+ and BBN-4+.
</bodyText>
<subsectionHeader confidence="0.999021">
6.4 Structural correspondence learning
</subsectionHeader>
<bodyText confidence="0.999982270270271">
We first implemented a simple baseline following
the idea presented in (Ando, 2004). The basic idea
consists in performing an SVD decomposition of
the feature-token matrix, where the matrix con-
tains all the sentences from the source and target
domains. The goal is to capture co-occurrences of
features and derive new features which are more
stable. More specifically, we extracted the 50 prin-
cipal directions of the feature-token matrix and
projected all the data onto these directions. This
results in 50 new additional features for each to-
ken that we append to the original (sparse binary)
feature vector OZ, 1 &lt; i &lt; d. In order to give
equal importance to the original and new features,
we multiplied the new features by a constant fac-
tor such that the average Li norms of the new and
old features are the same. Note that this weight-
ing might not be optimal but should be sufficient
to detect if these new features are helpful or not.
We then implemented several versions of struc-
tural correspondence learning. First, following the
original formulation (we refer to this model as
SCL1), 100 pivot features are selected, these are
frequent features in both source and target data.
For a given pivot feature k, a vector wk E Rd
is computed by performing a regularized linear
regression between all the other features and the
given pivot feature. The matrix W whose columns
are the wk is formed and the original feature vec-
tors are projected onto the 50 top left singular vec-
tors of W, yielding 50 new features. We also tried
the following variants. In the version we refer to
as SCL2 we rescale the left singular vectors of W
by their corresponding singular values. In the last
variant (SCL3) we select the pivot features which
are frequent in the source and target domains and
which are also predictive for the task (as measured
</bodyText>
<page confidence="0.991222">
5
</page>
<table confidence="0.999920055555556">
Model Source Target Test
B BBN-4 CoNLL 60.4 f.28
BG BBN-4 CoNLL 66.1 f.32
BGSVD BBN-4 CoNLL 66.5 f.26
BGSCL1 BBN-4 CoNLL 66.8 f.18
BGSCL2 BBN-4 CoNLL 64.7 f.24
BGSCL3 BBN-4 CoNLL 66.8 f.27
BGSELF BBN-4+ CoNLL 65.5 f.26
BGe BBN-4 CoNLL 66.8 f.53
Model Source Target Test
B CoNLL BBN-4 65.0 f.77
BG CoNLL BBN-4 67.6 f.69
BGSVD CoNLL BBN-4 67.9 f.54
BGSCL1 CoNLL BBN-4 67.9 f.45
BGSCL2 CoNLL BBN-4 68.1 f.53
BGSCL3 CoNLL BBN-4 67.8 f.34
BGSELF CoNLL+ BBN-4 68.3 f.36
BGe CoNLL BBN-4 70.3 f.61
</table>
<tableCaption confidence="0.999848">
Table 3. Results of baselines and adaptive models.
</tableCaption>
<bodyText confidence="0.999908666666667">
by the mutual information between the feature and
the class label). The 50 additional features are ap-
pended to the original (sparse binary) feature vec-
tor 0Z, 1 &lt; i &lt; d, and again, they are first rescaled
in order to have the same average L1 norm as the
old features over the entire dataset.
</bodyText>
<sectionHeader confidence="0.99716" genericHeader="evaluation">
7 Results and discussion
</sectionHeader>
<bodyText confidence="0.999924641025641">
Table 3 summarizes the experimental results on
both datasets. We refer to our adaptive model as
BGB. Adapting a model from BBN-4 to CoNLL,
self training (BGSELF, 65.5%) performs slightly
worse than the base model (BG, 66.1%). The
best SCL model, the original formulation, pro-
duces a small, but likely significant, improvement
(BGSCL1, 66.8%). Our model (BGB, 66.8%),
achieves the same result but with larger variance.
The improvement of the best models over the first
baseline (B, 60.4%) is considerable, +6.4%, but
mostly due to gazetteers.
In the adaptation experiments from CoNLL to
BBN-4 both self training (BGSELF, 68.3%) and
the best SCL model (BGSCL1, 68.1%) are com-
parable to the baseline (BG, 67.6%). The adap-
tive perceptron HMM (BGB, 70.3%) improves by
2.7%, as much as model BG over B, again with a
slightly larger variance. It is not clear why other
methods do not improve as much. Speculatively,
although we implemented several variants, SCL
might benefit from further tuning as it involves
several pre-processing steps. As for self training,
the base tagger might be too inaccurate to support
this technique. It is fair to assume that the ad-
ditional hyperparameters available to our model,
e.g., r, provided some additional flexibility. We
also experimented with a few variants of estimat-
ing 0 on the development set; i.e., different splits
of the unlabeled source/target data and different
sampling modes: with and without replacement,
number of trials. All of these aspects can have
a significant impact on the quality of the model.
This point brings up a more general issue with
the type of approach explored here: while adapt-
ing the class priors seems easier than adapting the
full model it is not trivial to encode noisy world
knowledge into meaningful priors. Alternatively,
in the presence of some labeled data one could op-
timize 0 directly. This information could be also
elicited from domain experts. Another interesting
alternative is the unsupervised estimation via EM
as in (Chan and Ng, 2006).
Overall, adaptation from BBN-4 to CoNLL is
harder than from CoNLL to BBN-4. A possi-
ble explanation is that adapting from specific to
general is harder then in the opposite direction:
the specific corpus is more heavily biased towards
a domain (finance). This intuition is compatible
with the baselines performing better in the CoNLL
to BBN-4 direction. However, the opposite argu-
ment, that adapting from specific to general should
be easier, has some appeal as well; e.g., if more
general means higher entropy it seems easier to
make a distribution more uniform than finding the
right peak.
In general, all adaptive techniques we evalu-
ated provided only marginal improvements over
the baseline (BG) model. To put things in con-
text, it is useful to recall that when evaluated in
domain the CoNLL and BBN-4 taggers (model
BG) achieve, respectively, 92.7% and 91.6% aver-
age F-scores on the test data. As the results illus-
trate there is a considerable drop in out domain ac-
curacy, significantly alleviated by adding features
from gazetteers and to some extent by other meth-
ods. Following Dredze et al. (2007) we hypoth-
esize that a significant fraction of the loss is due
to labeling inconsistencies between datasets. Al-
though we did our best to optimize the benchmark
methods it is possible that even better results could
be achieved with self-training and SCL. However
we stress that different methods get at different
aspects of the problem: self-training targets data
sparseness, SCL methods aims at generating better
shared input representations, while our approach
focuses on generating output distribution more
compatible with the target data. It seems reason-
</bodyText>
<page confidence="0.996153">
6
</page>
<bodyText confidence="0.9882">
able to expect that better adaptation performance
would result from composite approaches, aiming
at both better machine learning and task-specific
aspects for the named entity recognition problem.
</bodyText>
<sectionHeader confidence="0.344811" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.816019833333333">
reranking. In Proceedings ofACL 2005, pages 173–
180. Association for Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help
a lot. In Proceedings of EMNLP, pages 285–292.
Association for Computational Linguistics.
We investigated the model adaptation problem for
named entity recognition where the base model is
a discriminatively trained HMM (Collins, 2002).
We hypothesized that part of the loss incurred in
using a pre-trained model out of domain is due
to its built-in class priors which do not match the
class distribution of the out of domain data. To
test this hypothesis, and attempt a solution, we
propose to explicitly correct the prediction of the
model for a given label by taking into account a
noisy estimate of the label frequency in the tar-
get data. We found encouraging results from pre-
liminary experiments. It might thus be worth in-
vestigating more principled formulations of this
type of method, in particular to eliminate some
heuristic aspects, improve unsupervised estima-
tions, and generalize to other classification tasks
beyond NER.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999894">
We would like to thank the anonymous reviewers
for useful comments and pointers to related work.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999862971428572">
Rie Kubota Ando. 2004. Exploiting unannotated cor-
pora for tagging and chunking. In Proceedings of
ACL 2004. Association for Computational Linguis-
tics.
BBN. 2005. Pronoun coreference and entity type
corpus. Linguistic Data Consortium (LDC) catalog
number LDC2005T33.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP 2006.
Association for Computational Linguistics.
John Blitzer, Mark Dredzde, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes, and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL 2007.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proceedings of Coling-ACL, pages
89–96. Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Advances in Structured
Learning for Text and Speech Processing (NIPS
2005).
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of EMNLP, pages 594–602. Associ-
ation for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002, pages 1–8.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: A
framework and graphical development environment
for robust NLP tools and applications. In Proceed-
ings of ACL 2002. Association for Computational
Linguistics.
Hal Daum´e III. 2007. Frustratingly easy domain
adaptation. In Proceedings of ACL. Association for
Computational Linguistics.
Mark Dredze, John Blitzer, Pratha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adaptation
for parsing. In Proceedings of CoNLL Shared Task
2007. Association for Computational Linguistics.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277–296.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of COLING-ACL 2006,
pages 337–344. Association for Computational Lin-
guistics.
Peter Mika, Massimiliano Ciaramita, Hugo Zaragoza,
and Jordi Atserias. 2008. Learning to tag and tag-
ging to learn: A case study on Wikipedia. IEEE
Intelligent Systems, 23(5):26–33.
Nam Nguyen and Yunsong Guo. 2007. Comparison
of sequence labeling algorithms and extensions. In
Proceedings of ICML 2007, pages 681–688.
Erik F. Tjong Kim Sang and Fien De Muelder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL 2003 Shared Task, pages
142–147. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.999517">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697964">
<title confidence="0.993997">Adaptive Parameters for Entity Recognition with Perceptron HMMs</title>
<author confidence="0.920828">Chapelle</author>
<affiliation confidence="0.970236">Google Yahoo! Research</affiliation>
<address confidence="0.898598">Z¨urich, Switzerland Sunnyvale, CA, USA</address>
<email confidence="0.998633">massi@google.comchap@yahoo-inc.com</email>
<abstract confidence="0.9906155625">We discuss the problem of model adaptation for the task of named entity recognition with respect to the variation of label distributions in data from different domains. We investigate an adaptive extension of the sequence perceptron, where the adaptive component includes parameters estimated from unlabelled data in combination with background knowledge in the form of gazetteers. We apply this idea empirically on adaptation experiments involving two newswire datasets from different domains and compare with other popular methods such as self training and structural correspondence learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
</authors>
<title>Exploiting unannotated corpora for tagging and chunking.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL 2004. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3972" citStr="Ando, 2004" startWordPosition="622" endWordPosition="623"> adaptation has focused on approaches such as self-training and structural correspondence learning (SCL). The former approach involves adding self-labeled data from the target domain produced by a model trained in-domain (McClosky et al., 2006). The latter approach focuses on ways of generating shared source-target representations based on good crossdomain (pivot) features (Blitzer et al., 2006) (see 1 Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 1–7, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics also (Ando, 2004)). Self training has proved effective in syntactic parsing, particularly in tandem with discriminative re-ranking (Charniak and Johnson, 2005), while the SCL has been applied successfully to tasks such PoS tagging and opinion analysis (Blitzer et al., 2006; Blitzer et al., 2007). We address a different aspect of the adaptation problem, namely the difference in label distributions between source and target domains. Chan and Ng (2006) proposed correcting the class priors for domain adaptation purposes in a word sense disambiguation task. They adopt a generative framework where the base model is </context>
<context position="19159" citStr="Ando, 2004" startWordPosition="3215" endWordPosition="3216">ing we trained a tagger (BG) on the training set of CoNLL. With the tagger we annotated the training set of BBN-4, and added the self-labeled data, 39,823 BBN-4 sentences, to the gold standard CoNLL training. Similarly, in the reverse direction we trained a tagger (BG) on the training set of BBN-4, annotated the training set of CoNLL, and added the self-labeled 16,540 CoNLL sentences to the BBN-4 training. We denote these models BGSELF, and the augmented sources as CoNLL+ and BBN-4+. 6.4 Structural correspondence learning We first implemented a simple baseline following the idea presented in (Ando, 2004). The basic idea consists in performing an SVD decomposition of the feature-token matrix, where the matrix contains all the sentences from the source and target domains. The goal is to capture co-occurrences of features and derive new features which are more stable. More specifically, we extracted the 50 principal directions of the feature-token matrix and projected all the data onto these directions. This results in 50 new additional features for each token that we append to the original (sparse binary) feature vector OZ, 1 &lt; i &lt; d. In order to give equal importance to the original and new fe</context>
</contexts>
<marker>Ando, 2004</marker>
<rawString>Rie Kubota Ando. 2004. Exploiting unannotated corpora for tagging and chunking. In Proceedings of ACL 2004. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BBN</author>
</authors>
<title>Pronoun coreference and entity type corpus. Linguistic Data Consortium (LDC) catalog number LDC2005T33.</title>
<date>2005</date>
<contexts>
<context position="14433" citStr="BBN, 2005" startWordPosition="2418" endWordPosition="2419"> rough quantification of the necessary amount by which we need to offset the scores G(x, ys; α) in order to change the predictions. As a first guess, we take τ = µ(MS) = |s |Es|S |Ms, which we interpret as an upper bound on the desired value of τ. While experimenting on the development data we found that τ/2 yields good results. 6 Experimental setup 6.1 Data We used two datasets for evaluation. The first is the English CoNLL 2003 dataset (Sang and Muelder, 2003), a corpus of Reuters news annotated with person, location, organization and miscellaneous entity tags. The second is the BBN corpus (BBN, 2005), which supplements the WSJ Penn TreeBank with annotation for 105 categories: named entities, nominal entities and numeric types. We made the two datasets “semantically” compatible as follows. We tagged a large collection of text from the English Wikipedia with CoNLL and BBN taggers. We counted the frequencies of BBN/CoNLL tag pairs for the same strings, and assigned each BBN tag the most frequent CoNLL tag;5 e.g., BBN tag CoNLL tag Work of art:Book → Misc Organization:Educational → Org Location:Continent → Loc Person → Per 48 BBN-to-CoNLL pairs were labelled in this way. Remaining categories,</context>
</contexts>
<marker>BBN, 2005</marker>
<rawString>BBN. 2005. Pronoun coreference and entity type corpus. Linguistic Data Consortium (LDC) catalog number LDC2005T33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP 2006. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1762" citStr="Blitzer et al., 2006" startWordPosition="270" endWordPosition="273"> target data are not assumed to be drawn from the same distribution, which might actually differ in relevant distributional properties: topic, domain, genre, style, etc. In some formulations of the problem a few target labeled data is assumed to be available (Daum´e III, 2007). However, we are interested in the case in which no labeled data is available from the target domain – except for evaluation purposes and fine tuning of hyperparameters. Most of the work in adaptation has focused so far on the input side; e.g, proposing solutions based on generating shared source-target representations (Blitzer et al., 2006). Here we focus instead on the output aspect. We hypothesize that This work was carried out while the first author was working at Yahoo! Research Barcelona. part of the loss incurred in using a model out of domain is due to its built-in class priors which do not match the class distribution in the target data. Thus we attempt to explicitly correct the prediction of a pre-trained model for a given label by taking into account a noisy estimate of the label frequency in the target data. The correction is carried out by means of adaptive parameters, estimated from unlabelled target data and backgr</context>
<context position="3759" citStr="Blitzer et al., 2006" startWordPosition="589" endWordPosition="592"> we speculate that the same intuition could be straightforwardly applied in other learning frameworks (e.g., Support Vector Machines) and different tasks (e.g., standard classification). 2 Related work Recent work in domain adaptation has focused on approaches such as self-training and structural correspondence learning (SCL). The former approach involves adding self-labeled data from the target domain produced by a model trained in-domain (McClosky et al., 2006). The latter approach focuses on ways of generating shared source-target representations based on good crossdomain (pivot) features (Blitzer et al., 2006) (see 1 Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 1–7, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics also (Ando, 2004)). Self training has proved effective in syntactic parsing, particularly in tandem with discriminative re-ranking (Charniak and Johnson, 2005), while the SCL has been applied successfully to tasks such PoS tagging and opinion analysis (Blitzer et al., 2006; Blitzer et al., 2007). We address a different aspect of the adaptation problem, namely the difference in label distributions betw</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of EMNLP 2006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredzde</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes, and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="4251" citStr="Blitzer et al., 2007" startWordPosition="664" endWordPosition="667">uses on ways of generating shared source-target representations based on good crossdomain (pivot) features (Blitzer et al., 2006) (see 1 Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 1–7, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics also (Ando, 2004)). Self training has proved effective in syntactic parsing, particularly in tandem with discriminative re-ranking (Charniak and Johnson, 2005), while the SCL has been applied successfully to tasks such PoS tagging and opinion analysis (Blitzer et al., 2006; Blitzer et al., 2007). We address a different aspect of the adaptation problem, namely the difference in label distributions between source and target domains. Chan and Ng (2006) proposed correcting the class priors for domain adaptation purposes in a word sense disambiguation task. They adopt a generative framework where the base model is a naive Bayes classifier and priors are re-estimated with EM. The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. Domain adaptation naturally in</context>
</contexts>
<marker>Blitzer, Dredzde, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredzde, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes, and blenders: Domain adaptation for sentiment classification. In Proceedings of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Estimating class priors in domain adaptation for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of Coling-ACL,</booktitle>
<pages>89--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4408" citStr="Chan and Ng (2006)" startWordPosition="690" endWordPosition="693"> Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 1–7, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics also (Ando, 2004)). Self training has proved effective in syntactic parsing, particularly in tandem with discriminative re-ranking (Charniak and Johnson, 2005), while the SCL has been applied successfully to tasks such PoS tagging and opinion analysis (Blitzer et al., 2006; Blitzer et al., 2007). We address a different aspect of the adaptation problem, namely the difference in label distributions between source and target domains. Chan and Ng (2006) proposed correcting the class priors for domain adaptation purposes in a word sense disambiguation task. They adopt a generative framework where the base model is a naive Bayes classifier and priors are re-estimated with EM. The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. Domain adaptation naturally invokes the existence of a specific task and data. As such it is natural to consider the modeling aspects within the context of a specific application. Here we</context>
<context position="23787" citStr="Chan and Ng, 2006" startWordPosition="4003" endWordPosition="4006">ent sampling modes: with and without replacement, number of trials. All of these aspects can have a significant impact on the quality of the model. This point brings up a more general issue with the type of approach explored here: while adapting the class priors seems easier than adapting the full model it is not trivial to encode noisy world knowledge into meaningful priors. Alternatively, in the presence of some labeled data one could optimize 0 directly. This information could be also elicited from domain experts. Another interesting alternative is the unsupervised estimation via EM as in (Chan and Ng, 2006). Overall, adaptation from BBN-4 to CoNLL is harder than from CoNLL to BBN-4. A possible explanation is that adapting from specific to general is harder then in the opposite direction: the specific corpus is more heavily biased towards a domain (finance). This intuition is compatible with the baselines performing better in the CoNLL to BBN-4 direction. However, the opposite argument, that adapting from specific to general should be easier, has some appeal as well; e.g., if more general means higher entropy it seems easier to make a distribution more uniform than finding the right peak. In gene</context>
</contexts>
<marker>Chan, Ng, 2006</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2006. Estimating class priors in domain adaptation for word sense disambiguation. In Proceedings of Coling-ACL, pages 89–96. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and MaxEnt discriminative</title>
<date>2005</date>
<contexts>
<context position="4114" citStr="Charniak and Johnson, 2005" startWordPosition="640" endWordPosition="643">involves adding self-labeled data from the target domain produced by a model trained in-domain (McClosky et al., 2006). The latter approach focuses on ways of generating shared source-target representations based on good crossdomain (pivot) features (Blitzer et al., 2006) (see 1 Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 1–7, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics also (Ando, 2004)). Self training has proved effective in syntactic parsing, particularly in tandem with discriminative re-ranking (Charniak and Johnson, 2005), while the SCL has been applied successfully to tasks such PoS tagging and opinion analysis (Blitzer et al., 2006; Blitzer et al., 2007). We address a different aspect of the adaptation problem, namely the difference in label distributions between source and target domains. Chan and Ng (2006) proposed correcting the class priors for domain adaptation purposes in a word sense disambiguation task. They adopt a generative framework where the base model is a naive Bayes classifier and priors are re-estimated with EM. The approach proposed by Chelba and Acero (2004) is also related as they propose</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and MaxEnt discriminative</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Named-entity recognition in novel domains with external lexical knowledge.</title>
<date>2005</date>
<booktitle>In Advances in Structured Learning for Text and Speech Processing (NIPS</booktitle>
<contexts>
<context position="5229" citStr="Ciaramita and Altun (2005)" startWordPosition="825" endWordPosition="828">iors are re-estimated with EM. The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. Domain adaptation naturally invokes the existence of a specific task and data. As such it is natural to consider the modeling aspects within the context of a specific application. Here we focus on the problem of named entity recognition (NER). There is still little work on adaptation for NER. Ando (2004) reports successful experiments on adapting with an SCL-like approach, while Ciaramita and Altun (2005) effectively used external knowledge in the form of gazetteers in a semi-Markov model. Mika et al. (2008) used Wikipedia to generate additional training data for domain adaptation purposes. 3 Problem statement Named entity taggers detect mentions of instances of pre-defined categories such as person (Per), location (Loc), organization (Org) and miscellaneous (Misc). The problem can be naturally framed as a segmentation and labeling task. State of the art systems, e.g., based on sequential optimization, achieve excellent accuracy in domain. However, accuracy degrades if the target data diverges</context>
<context position="8279" citStr="Ciaramita and Altun (2005)" startWordPosition="1328" endWordPosition="1331">defined over a joint input/output space,3 a global feature representation mapping each (x, y) pair to a vector of feature counts 4b(x, y) E IRd: |y| [4&apos;(x, y)Ii = L Oi(yj−1,yj,x), (2) j=1 where Oi is a (binary) predicate. Given an input sequence x, we find the optimal label sequence, f(x; α) = arg maxyEY F(x, y; α), with Viterbi decoding. The model α is learned with the perceptron algorithm. Each feature represents a spelling or contextual property, or the previous label. The simplest baseline (model B) uses the features listed in the upper half of Table 1. In previous work on NER adaptation, Ciaramita and Altun (2005) found that gazetteers, in combination with semiMarkov models, significantly improved adaptation. Similarly, we define additional features using 3(u, v) denoting the inner product between u and v. 2 Model B features Feature example token feature value(s) Position Lowercase word Pierre pierre i-1, i, i+1 Part of Speech Pierre NNP i-1, i, i+1 Word Shape Pierre Xx i-1, i, i+1 Suffix2/3 Pierre fre, rre} i Prefix2/3 Pierre fpi, pie} i Previous label Vinken (in “Pierre Vinken”) B-PER (label on “Pierre”) i Additional features of model BG Feature example token feature value(s) Position InGazetteer Isl</context>
</contexts>
<marker>Ciaramita, Altun, 2005</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2005. Named-entity recognition in novel domains with external lexical knowledge. In Advances in Structured Learning for Text and Speech Processing (NIPS 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>594--602</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6222" citStr="Ciaramita and Altun, 2006" startWordPosition="975" endWordPosition="978">. The problem can be naturally framed as a segmentation and labeling task. State of the art systems, e.g., based on sequential optimization, achieve excellent accuracy in domain. However, accuracy degrades if the target data diverges in relevant distributional aspects from the source. As an example, the following is the output of a perceptron HMM1 trained on the CoNLL 2003 English data (news) (Sang and Muelder, 2003) when applied to a molecular biology text:2 1We used the implementation available from http: //sourceforge.net/projects/supersensetag, more details on this tagger can be found in (Ciaramita and Altun, 2006). 2The same model achieves F-scores well in excess of 90% evaluated in domain. (1) Cdc2-cyclin Org B-activated Polo-like mise kinase specifically phosphorylates at least three components of APC Org . The tagger predicts several CoNLL entities which are unlikely to occur in that context. One source of confusion is probably the shape of words, including case, numbers, and non alphabetical characters, which are also typical, and thus misleading, of unrelated CoNLL entities. However, we argue that the problem is partially due to the parameters learned which reflect the distribution of classes in t</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of EMNLP, pages 594–602. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3137" citStr="Collins (2002)" startWordPosition="500" endWordPosition="501">adaptation approaches for named entity recognition (NER). The main findings from our experiments are as follows. First, the problem is challenging and only marginal improvements are possible under all evaluated frameworks. Second, we found that our method compares well with current state-of-the-art approaches such as self training and structural correspondence learning (McClosky et al., 2006; Blitzer et al., 2006) and taps on an interesting aspect which seems worth of further research. Although we concentrate on a segmentation task within a specific framework, the perceptron HMM introduced by Collins (2002), we speculate that the same intuition could be straightforwardly applied in other learning frameworks (e.g., Support Vector Machines) and different tasks (e.g., standard classification). 2 Related work Recent work in domain adaptation has focused on approaches such as self-training and structural correspondence learning (SCL). The former approach involves adding self-labeled data from the target domain produced by a model trained in-domain (McClosky et al., 2006). The latter approach focuses on ways of generating shared source-target representations based on good crossdomain (pivot) features </context>
<context position="7368" citStr="Collins, 2002" startWordPosition="1163" endWordPosition="1164">he parameters learned which reflect the distribution of classes in the source data. The parameter, acting as biased priors, lead the tagger to generate inappropriate distributions of labels. We propose that this aspect of the problem might be alleviated by correcting the score for each class with an estimate of the class frequency in the target data. Thus, with respect to the example, we would like to decrease the score of “Org” labels according to their expected frequency in a molecular biology corpus. 4 A perceptron with adjustable priors As generic taggers we adopt perceptron-trained HMMs (Collins, 2002) which have excellent efficiency/performance trade-off (Nguyen and Guo, 2007). The objective of learning is a discriminant F : X x Y —* IR, where Y denotes sequences of labels from a pre-defined set of categories Y . F(x, y; α) = (α, 4b(x, y)) is linear in a feature representation 4b defined over a joint input/output space,3 a global feature representation mapping each (x, y) pair to a vector of feature counts 4b(x, y) E IRd: |y| [4&apos;(x, y)Ii = L Oi(yj−1,yj,x), (2) j=1 where Oi is a (binary) predicate. Given an input sequence x, we find the optimal label sequence, f(x; α) = arg maxyEY F(x, y; α</context>
<context position="26094" citStr="Collins, 2002" startWordPosition="4364" endWordPosition="4365">daptation performance would result from composite approaches, aiming at both better machine learning and task-specific aspects for the named entity recognition problem. 8 Conclusion reranking. In Proceedings ofACL 2005, pages 173– 180. Association for Computational Linguistics. Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy capitalizer: Little data can help a lot. In Proceedings of EMNLP, pages 285–292. Association for Computational Linguistics. We investigated the model adaptation problem for named entity recognition where the base model is a discriminatively trained HMM (Collins, 2002). We hypothesized that part of the loss incurred in using a pre-trained model out of domain is due to its built-in class priors which do not match the class distribution of the out of domain data. To test this hypothesis, and attempt a solution, we propose to explicitly correct the prediction of the model for a given label by taking into account a noisy estimate of the label frequency in the target data. We found encouraging results from preliminary experiments. It might thus be worth investigating more principled formulations of this type of method, in particular to eliminate some heuristic a</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP 2002, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9766" citStr="Cunningham et al., 2002" startWordPosition="1565" endWordPosition="1568">ature list and examples. The upper half lists the features for the baseline tagger (B), the lower half lists the additional features extracted from the gazetteers included to the second non-adapted tagger (BG). The last number on the feature indicates the length of the entry in the list; e.g., “Islands” in the example is the end of a two-word item, in the country gazetteer, because of “Cayman Islands”. The remaining features capture the most frequent Wordnet supersense of the word, the first and second most frequent supersenses, and the total number of supersenses. the gazetteers from GATE,4 (Cunningham et al., 2002) namely, countries, person first/last names, trigger words; and also from Wordnet: using the lexicographers or supersense labels; and a list of company names from Fortune 500. For this second baseline (model BG) we also extract the features in the bottom half of Table 1. 4.1 Decoding with external priors In our method training is performed on the source data using the perceptron algorithm. Adaptation takes place at decoding time, when the score of the entity labels is adjusted according to a kdimensional parameter vector B, k = |Y |, estimated by comparing the source and the unlabeled target d</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In Proceedings of ACL 2002. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL. Association for Computational Linguistics.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
</authors>
<title>Pratha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira.</title>
<date>2007</date>
<booktitle>In Proceedings of CoNLL Shared Task</booktitle>
<marker>Dredze, Blitzer, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Pratha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira. 2007. Frustratingly hard domain adaptation for parsing. In Proceedings of CoNLL Shared Task 2007. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--277</pages>
<contexts>
<context position="15981" citStr="Freund and Schapire, 1999" startWordPosition="2673" endWordPosition="2677">d by a skewed distribution of labels with organization by far the most frequent class, while the CoNLL data has a more uniform distribution with location as the most frequent class. The CoNLL data was randomly split in three disjoint sets of sentences for training (16,540 sentences), development (2.068) and test (2,136). For BBN-4 we used WSJ sections 2-21 for training (39,823), section 22 for development (1,700) and section 23 for test (2,416). We evaluated models in both directions; i.e., swapping CoNLL and BBN-4 as source/target. 6.2 Model tuning We regularize the perceptrons by averaging (Freund and Schapire, 1999). The perceptron HMM 5A simpler approach might that of manually mapping the two tagsets, however a number of cases that are not trivial to resolve emerges in this way. For this reason we decided to adopt the described data-driven heuristic approach. 4 has only one hyper-parameter, the number of training iterations (or epochs). Models trained for application out of domain can benefit from early stopping which provides an additional mean of regularization. For all models compared we used the development sets for choosing the number of epochs for training the perceptron on the source data. This i</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R.E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37:277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL 2006,</booktitle>
<pages>337--344</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2917" citStr="McClosky et al., 2006" startWordPosition="463" endWordPosition="466">ptive parameters, estimated from unlabelled target data and background “world knowledge” in the form of gazetteers, and taken in consideration in the decoding phase. We built a suitable dataset for experimenting with different adaptation approaches for named entity recognition (NER). The main findings from our experiments are as follows. First, the problem is challenging and only marginal improvements are possible under all evaluated frameworks. Second, we found that our method compares well with current state-of-the-art approaches such as self training and structural correspondence learning (McClosky et al., 2006; Blitzer et al., 2006) and taps on an interesting aspect which seems worth of further research. Although we concentrate on a segmentation task within a specific framework, the perceptron HMM introduced by Collins (2002), we speculate that the same intuition could be straightforwardly applied in other learning frameworks (e.g., Support Vector Machines) and different tasks (e.g., standard classification). 2 Related work Recent work in domain adaptation has focused on approaches such as self-training and structural correspondence learning (SCL). The former approach involves adding self-labeled d</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In Proceedings of COLING-ACL 2006, pages 337–344. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Mika</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
<author>Jordi Atserias</author>
</authors>
<title>Learning to tag and tagging to learn: A case study on Wikipedia.</title>
<date>2008</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>23</volume>
<issue>5</issue>
<contexts>
<context position="5334" citStr="Mika et al. (2008)" startWordPosition="842" endWordPosition="845"> MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. Domain adaptation naturally invokes the existence of a specific task and data. As such it is natural to consider the modeling aspects within the context of a specific application. Here we focus on the problem of named entity recognition (NER). There is still little work on adaptation for NER. Ando (2004) reports successful experiments on adapting with an SCL-like approach, while Ciaramita and Altun (2005) effectively used external knowledge in the form of gazetteers in a semi-Markov model. Mika et al. (2008) used Wikipedia to generate additional training data for domain adaptation purposes. 3 Problem statement Named entity taggers detect mentions of instances of pre-defined categories such as person (Per), location (Loc), organization (Org) and miscellaneous (Misc). The problem can be naturally framed as a segmentation and labeling task. State of the art systems, e.g., based on sequential optimization, achieve excellent accuracy in domain. However, accuracy degrades if the target data diverges in relevant distributional aspects from the source. As an example, the following is the output of a perc</context>
</contexts>
<marker>Mika, Ciaramita, Zaragoza, Atserias, 2008</marker>
<rawString>Peter Mika, Massimiliano Ciaramita, Hugo Zaragoza, and Jordi Atserias. 2008. Learning to tag and tagging to learn: A case study on Wikipedia. IEEE Intelligent Systems, 23(5):26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nam Nguyen</author>
<author>Yunsong Guo</author>
</authors>
<title>Comparison of sequence labeling algorithms and extensions.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML</booktitle>
<pages>681--688</pages>
<contexts>
<context position="7445" citStr="Nguyen and Guo, 2007" startWordPosition="1171" endWordPosition="1174"> source data. The parameter, acting as biased priors, lead the tagger to generate inappropriate distributions of labels. We propose that this aspect of the problem might be alleviated by correcting the score for each class with an estimate of the class frequency in the target data. Thus, with respect to the example, we would like to decrease the score of “Org” labels according to their expected frequency in a molecular biology corpus. 4 A perceptron with adjustable priors As generic taggers we adopt perceptron-trained HMMs (Collins, 2002) which have excellent efficiency/performance trade-off (Nguyen and Guo, 2007). The objective of learning is a discriminant F : X x Y —* IR, where Y denotes sequences of labels from a pre-defined set of categories Y . F(x, y; α) = (α, 4b(x, y)) is linear in a feature representation 4b defined over a joint input/output space,3 a global feature representation mapping each (x, y) pair to a vector of feature counts 4b(x, y) E IRd: |y| [4&apos;(x, y)Ii = L Oi(yj−1,yj,x), (2) j=1 where Oi is a (binary) predicate. Given an input sequence x, we find the optimal label sequence, f(x; α) = arg maxyEY F(x, y; α), with Viterbi decoding. The model α is learned with the perceptron algorith</context>
</contexts>
<marker>Nguyen, Guo, 2007</marker>
<rawString>Nam Nguyen and Yunsong Guo. 2007. Comparison of sequence labeling algorithms and extensions. In Proceedings of ICML 2007, pages 681–688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Fien De Muelder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL 2003 Shared Task,</booktitle>
<pages>142--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Sang, De Muelder, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Muelder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of CoNLL 2003 Shared Task, pages 142–147. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>