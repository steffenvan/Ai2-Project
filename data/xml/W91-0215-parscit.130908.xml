<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.998381666666667">
A model for the interaction of lexical and non-lexical
knowledge
in the determination of word meaning
</title>
<author confidence="0.996226">
Peter Gerstl
</author>
<affiliation confidence="0.976907">
IBM Germany, Scientific Center
Institute for Knowledge Based Systems
</affiliation>
<address confidence="0.715475">
Schlofistr. 70
7000 Stuttgart I
</address>
<email confidence="0.930991">
e-mail: gerstl dsOlilog.bitnet
</email>
<sectionHeader confidence="0.959783" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999843272727273">
The lexicon of a natural language understanding system that is not restricted to one
single application but should be adaptable to a whole range of different tasks has to
provide a flexible mechanism for the determination of word meaning. The reason for
such a mechanism is the semantic variability of words, i.e. their potential to denote
different things in different contexts. The goal of our project is a model that makes
these phenomena explicit. We approach this goal by defining word meaning as a
complex function resulting from the interaction of processes operating on knowledge
elements. In the following we characterize the range of phenomena our model is
intended to describe and give an outline of the way in which the interpretation
process may determine the referential potential of words by the integration and
evaluation of a variety of factors.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998345">
A system with the capability of natural language understanding typically relies on knowl-
edge about a restricted domain of application. For example, as a natural language com-
ponent of an information system, it needs to be able to identify the relevant linguistic
patterns. In case of an information system for flight scheduling words such as &amp;quot;plane&amp;quot;,
&amp;quot;departure&amp;quot;, &amp;quot;late&amp;quot;, typically be more relevant than for example: &amp;quot;palm&amp;quot;, &amp;quot;this-
tle&amp;quot;, &amp;quot;pine&amp;quot; which might be appropriate for a different domain. In any event, there will be
a whole range of words that are commonly used in conversation and, thus, are indepen-
dent from the choice of a specific domain. It is therefore desirable to have a multi-level
architecture which can be adapted to different domains without being forced to redesign
the whole system. A text understanding system based on this kind of architecture would
provide the kernel functionality that allows it to couple principles and mechanisms not
immediately dependent on the domain a specific implementation of the system will be
used for. The main problem of such a modular architecture is how and where to draw
the boundary between domain-independent and domain-specific knowledge. There are at
least two more reasons which motivate a domain-oriented design strategy:
With regard to knowledge representation, the history in artificial intelligence research
has lead from early enthusiastic plans of &apos;general problem solving capabilities&apos; to more
realistic applications of expert systems. One reason was the huge amount of data that.
would have to be represented together with a large set of regularities introducing a level
of complexity which could not be handeled in a realistic manner by the systems currently
</bodyText>
<page confidence="0.997786">
165
</page>
<bodyText confidence="0.999515928571429">
available. Another problem is the inconsistency of data that would necessarily arise once
a lot of different and sometimes conflicting information had to be integrated into a sin-
gle knowledge base. Under this perspective task- and domain-orientation is a matter
of rendering the knowledge base manageable and to allow reasoning processes to draw
meaningful inferences on the basis of consistent data.
The second argument in favour of a domain-oriented design strategy comes from the
area of lexical semantics. It is a well known fact that the meaning of a word depends
on a multitude of contextual influences. In a very broad notion of context the task and
the domain of a text understanding system may be considered a part of the context that
licences an effective restriction of the &apos;semantic scope&apos; of a single word. This again is
mainly an argument of tractability which in this case helps to minimize the amount of
lexical information needed. In our example it is a natural design decision to assume that
the lexicon of a language understanding system as part of an information system about
flight schedules does not have to account for the `plant&apos;-reading of &amp;quot;plane&amp;quot;.
</bodyText>
<sectionHeader confidence="0.825375" genericHeader="introduction">
2 The variability of words
</sectionHeader>
<bodyText confidence="0.999890454545455">
The way in which a word might contribute to the determination of the meaning of linguistic
expressions is indeterminate in different ways. W. Labov calls the semantic potential
of words enabling them to constitute various links between linguistic expressions and
elements of the domain (semantic) variability. It is this potential which is responsible
for the already mentioned context dependence of word meaning. An important goal of
our model is to classify types of variability according to a set of more or less specific
properties. The questions guiding this classification are: Which kind of representation
does the variability affect; by which means are the variants related and, how can the
referential potential be restricted in order to single out the intended meaning? In the
linguistic tradition, these variability phenomena fall into one of four classes which we will
sketch out below.
</bodyText>
<subsectionHeader confidence="0.992746">
2.1 Morphological ambiguity
</subsectionHeader>
<bodyText confidence="0.9999483125">
This class represents cases of identical surface representations of words extracted from a
discourse. Depending on the kind of representation in which the natural language input
is encoded (i.e. orthographic, phonetic, ...form) cases of homography or homophony may
belong to this class or not. Homonymy as a specific instance of morphological ambiguity
results from the identity of lexical base forms. In a lexicon using orthographic representa-
tions as is normally the case in dictionaries (singular nouns, infinitive forms of verbs, ...)
there are for example two homonymous entries for &amp;quot;firm&amp;quot; (the adjective and the noun
variant). In addition, morphological ambiguity captures the more general case of an in-
flected form that is identical to a base form or to another inflected form. An example is
the occurence of &amp;quot;saw&amp;quot; which depending on the context can be understood as noun, as
base form of the verb &amp;quot;saw&amp;quot; or as inflected form derived from the base form &amp;quot;see&amp;quot;.
It is a notorious problem in lexical semantics [Kooij 71] to justify the distinction be-
tween coincidental identity of forms (morphological ambiguity) and semantic variants of
a single lexical item (chapter 2.2). In our approach it depends on the purpose of the
system and, thus, is a design decision comparable to modelling conventions for the do-
main knowledge. Identical basic word forms which are in no relevant and transparent
</bodyText>
<page confidence="0.997995">
166
</page>
<bodyText confidence="0.999954666666667">
way related to the representation of knowledge about the domain will be represented by
different lexical items. The question whether two identical forms &apos;collapse&apos; into a single
entry is then directed by the choice of the domain and the task analogous to the way in
which drawing a boundary between elements of the domain is motivated. Defining two
word forms as being homonymous yields the consequence that once the occurence in a
discourse has been morphologically identified and thus mapped onto the corresponding
lexical item it is no more possible to skip to a different homonymous variant. Words which
shall not expose this behaviour should not be modelled as homonymous but as semantic
variants of a single lexical item.
</bodyText>
<subsectionHeader confidence="0.999545">
2.2 Polysemy and polyfunctionality
</subsectionHeader>
<bodyText confidence="0.999983307692308">
In the preceding chapter we outlined the situation where two lexical items realize the same
word form. The potential of semantic variation encoded in a single lexical item is known
as polysemy. It remains in effect once the appropriate lexical item has been identified
by means of morphological processes. A special case of polysemy is what [Weber 74]
calls polyfunclionality refering to the situation where two variants of a lexical item belong
to different syntactic categories. Polyfunctionality occurs very frequently since many
lexical items allow identical realizations which belong to different syntactic categories
being related by means of conversion or other morphological processes which do not affect
the word stem. Examples are the nominal and verbal reading of &amp;quot;point&amp;quot; and the variants
of &amp;quot;clean&amp;quot; which are categorized as adjective, adverb and verb. The comparison with
variants in a dictionary is not as effective as it was in chapter 2.1 because the entries in a
dictionary tend to conflate phenomena we call polysemy and cases of variability outlined
in chapter 2.3.
</bodyText>
<subsectionHeader confidence="0.999643">
2.3 Metonymy and change of semantic type
</subsectionHeader>
<bodyText confidence="0.999922588235294">
In the previous chapter we mentioned that a polyfunctional expression can be the analyzed
as the realization of different categories in different contexts. The difference in semantic
potential that arises from this variability sometimes not only involves the transition to
a different semantic type in the sense of Montague grammar but it may be paralleled
by a more or less extensive shift in conceptual interpretation (cf. the one-place versus
two-place predicate reading of &amp;quot;drink&amp;quot;). Apart from ambiguities which are reflected by
morphosyntactic properties of a lexical item (eg. its argument structure) there are in-
stances of semantic variation allowing to change the interpretation of a class of linguistic
items in a systematic manner. The crucial question is if this class should be characterized
by lexical information or on the basis of regularities found in the domain. Metonymy is
a specific instance of this type of variability where the different readings are related by
elements of a set of fundamental relationships such as &apos;part—whole&apos;, &apos;cause—effect&apos;, etc.
[Nunberg 78] investigates more general mechanisms that licence the use of a word in place
of another in cases where both of them are related by means of a context-specific relation.
The phenomena range from cases of systematic correspondence such as in the &apos;newspaper-
example&apos;l to more ideosyncratic ones as the famous (ham-sandwhich-case&apos;2. As Nunberg
notes these are not cases of linguistic ambiguity because pointing to the sandwhich would
</bodyText>
<footnote confidence="0.656506">
1The word &amp;quot;newspaper&amp;quot; can be used to refer either to the publisher or to the publication.
2A waiter might apply the expression &amp;quot;the ham sandwhich is sitting at table 20&amp;quot; in order to identify
a unique guest who ordered a ham sandwhich.
</footnote>
<page confidence="0.997457">
167
</page>
<bodyText confidence="0.999966815789474">
serve the same purpose as the utterance of the complex phrase. Nevertheless, it is not
clear if the relations are considered as instances of lexical or encyclopaedic knowledge.
An example similar to the &apos;newspaper-case&apos; is the potential of words such as &amp;quot;school&amp;quot;,
&amp;quot;opera&amp;quot;, ...to select one of the alternative meaning variants &apos;building&apos;, &apos;process&apos;, &apos;insti-
tution&apos;, etc. The approach outlined in [Bierwisch 82] derives this potential from system-
atic relationships between concepts representing entities of the domain. The conceptual
knowledge about social instutions has to provide the background information that &amp;quot;the
parliament is at the end of the street&amp;quot; is semantically well-formed though &amp;quot;&apos;the govern-
ment is at the end of the street&amp;quot; is not. Since the lexical specifications of &amp;quot;parliament&amp;quot;
and &amp;quot;government&amp;quot; cannot account for the fact that it is naturally assumed that the former
can be associated with a specific building whereas a similar assignment is not possible for
the latter. A comparable argument may be found for the `substance&apos;-reading of words
naming trees. The ill-formedness of &amp;quot;&apos;this table is made of plane&amp;quot; in contrast to &amp;quot;this
table is made of oak&amp;quot; results from the non-verifiability in the common-sense model of
the domain. If an expert would affirm that is quite common to use the wood from palm
trees for the construction of tables we would probably change our model of the domain
and licence the acceptability of the first sentence. This example is different from the
`school&apos;/&apos; newspaper&apos;-cases since in addition to the conceptual shift it involves a modifica-
tion of semantic properties of the underlying lexical items. This in turn is an argument
in favour of a lexicalist position which would classify this type of variability as cases of
polysemy. [Pustejovsky 90] shows that a lexicalist approach to event structure allows to
systematically characterize a whole range of type-shifting phenomena together with their
consequences with respect to well-formedness conditions.
As a consequence of seeking the portability of domain specific knowledge we follow
the lines of Bierwisch in distinguishing lexical and conceptual information. Yet we do
not reject the lexicalist position since we consider semantic type-shifting effects as driven
by regularities of the conceptual structure. In the following section, we generalize this
position to a systematic distinction between linguistic and non-lingustic knowledge. This
allows us to keep variants introduced by linguistic ambiguities systematically apart from
those cases we classify as non-linguistic variations.
Following [Binnick 70] we define polysemous variants of a word as those cases of vari-
ability which are (at least in principle) distinguishable on the basis of linguistic properties
of the corresonding lexical item. Polysemous variants of a lexical item thus differ in at
least one morphosyntactic or semantic property. For non-linguistic variants introduced by
means of metonymy or type-shifting linguistic properties of a lexical item do not help to
identify the intended reading because the variants have exactly the same linguistic prop-
erties. This situation calls for the disambiguation potential of contextual information in
order to reduce the &apos;semantic scope&apos;3.
</bodyText>
<subsectionHeader confidence="0.999375">
2.4 Contextual relativity
</subsectionHeader>
<bodyText confidence="0.947376714285714">
Vagueness and indexicality also belong to the class of variability phenomena. They are
usually associated with specific groups of linguistic expressions (graduable predicates in
case of vagueness and deictic expressions in case of indexicality). In contrast to the
effects introduced so far, in these cases the class of potential referents cannot simply
3We consider the &apos;semantic scope&apos; of a word as the possible range of interpretation implied by the
literal use of a word. The more general notion of &apos;referential potential&apos; additionally accounts for cases of
conceptual shift as in the &apos;ham-sandwich&apos; example.
</bodyText>
<page confidence="0.989757">
168
</page>
<bodyText confidence="0.99999235483871">
be characterized by an enumeration of alternatives. As [Pinkal 80] points out it is an
inherent property of vague predicates to provide a &apos;grey area&apos; where the decision whether
the predicate is applicable or not depends on the discourse context. It is even impossible
to precisely delimit the area of positive or negative applicability. Following the lines of
[Bosch 83] we do not consider vagueness as an isolated semantic property of a specific class
of words but as an instance of the more general notion of contert-dependence4. Since this
is an aspect of the referential potential of words our model has to cover theses phenomena
as well.
Indexicality is the potential of deictic expressions to select their meaning by exploiting
peculiarities of the discourse situation. It is similar to vagueness since the implied referen-
tial indeterminacy cannot be resolved independently from the specific discourse context.
Yet, even deictic expressions are not immune to other types of variability. For example,
the pronoun &amp;quot;I&amp;quot; may be used to refer to an entity which is somehow related to the speaker
in a certain discourse situation. An example is the utterance of &amp;quot;I am over there&amp;quot; with
the speaker pointing to a desk. The expression &amp;quot;I&amp;quot; in this case can be used to refer to
the place, where the speaker usually works. This is an example of a systematic shift in
meaning motivated by a conceptual relation. It thus belongs to the phenomena described
in the previous chapter.
Another instance of context relativity occurs in cases of privative opposition. This
relativity results from the lack of semantic information for a specific word which could
be provided by the use of a different word. According to [Zwicky/Sadock 75] &amp;quot;dog&amp;quot; is
ambiguous between the readings &apos;male dog&apos; and &apos;female dog&apos; because it can be forced to
provide both readings in sentences like &amp;quot;that is a dog, but it isn&apos;t a dog&amp;quot;. In contrast to
&amp;quot;&apos;that is a lion, but it isn&apos;t a lion&amp;quot; it seems that a meaningful interpretation can be found
for the former (the one which forces the selection of different variants for both occurences
of &amp;quot;dog&amp;quot;) whereas the latter leads to a contradiction. The choice of a variant could be
forced by the use of &amp;quot;bitch&amp;quot; instead of dog which is not possible for &amp;quot;lion&amp;quot; since there
is not regular lexical specification for something like &amp;quot;lioness&amp;quot;. This illustrates the fact
that lack of semantic information for a lexical item can under certain circumstances yield
the same effect as a disjunction of alternative readings. This may occur whenever the
semantic &apos;gap&apos; can be filled by one of a small set of possible alternatives.
</bodyText>
<sectionHeader confidence="0.969865" genericHeader="method">
3 A classification of knowledge types
</sectionHeader>
<bodyText confidence="0.996294076923077">
In order to have a precise representational basis for our model of word meaning, this
chapter is intended to introduce the basic notions used to classify the relevant phenom-
ena. The distinction between linguistic and non-linguistic knowledge mentioned in the
preceding section constitutes the methodical basis of our model. Up to a certain degree,
this distinction allows an independent examination of properties characteristic for only
one type of knowledge. By assuming this distinction we do not claim that linguistic and
non-linguistic knowledge are in a way fundamentally different. We use this distinction as a
methodological tool that makes it possible to isolate certain aspects of word meaning not
directly involving the whole range of both types of knowledge. In the course of stepwise
extending the complexity of interrelations between linguistic and non-linguistic knowledge
we will have to carefully analyze the tenability of this distinction.
41n general the notion of context dependence applies to referential expressions such as definite nominal
phrases. We will restrict our attention to cases of context-dependence which apply to single words.
</bodyText>
<page confidence="0.993969">
169
</page>
<bodyText confidence="0.975877489795918">
We account for possible similarities between both types of knowledge by using the
same formalism for the representation of linguistic and non-linguistic knowledge. It is
a variant of order-sorted predicate logic [Nebel/Smolka 89] which combines properties
of the KL-ONE family of knowledge representation languages with properties of feature
based unification grammars such as HPSG [Pollard/Sag 87]. In order to concentrate on
the description of our model we will not go into the details of our formalism here5.
The central components of our model are the lexicon on the linguistic side and the
ontology on the non-linguistic side. The lexicon and the ontology provide the &apos;basic
building blocks&apos; of linguistic and non-linguistic knowledge respectively. The elements of
the lexicon are called categories; the elements of the ontology concepts. It is important to
note that the lexicon in our model integrates specifications of syntactic categories (i.e N,
V, ... , N&apos;, , AP, ... ) with lexical items.
The formal means for the description of categories and concepts are sorts which are
related by means of attributes and rules. Attributes may be used to express characteristic
properties or relationships motivating the choice of a specific distinction between sorts.
Rules on the other hand are not considered as tools for the description of inherent and
permanent properties but as representations of regularities which might arise under certain
circumstances. Apart from that, the collection of attributive characterizations has to be
consistent. That is not necessarily required for the system of rules as a whole. The
fundamental organizational principle of subsumption relates categories and concepts are
licencing the inheritance of attributes between sorts. The subsumption order does only
apply between elements of one and the same type of knowledge. The notation used for
the description of sorts is a feature-logic as in [Shieber 86] for categories and a simple
relational notation for concepts. We represent the fact that A subsumes B as A C B.
Rules of grammar and rules of inference are represented by using simple predicate logic
notion. Sorts, attributes and rules will be called knowledge elements. All of them put
together constitute the knowledge base of our system.
According to our argument in favour of a design strategy specifically tailored to the
domain and the task the system is intended for, the structure of the ontology must be
covered by an appropriate theory about entities of the domain, their inherent properties
and the diverse aspects in which they are related. We reiterate this methodological claim
here since a similar argument can be applied to the organization of the lexicon. The
typical task of a text understanding system is to facilitate the analysis and production of
textual input. It depends on the capabilities required whether certain aspects of this task
involve restrictions on the set of relevant linguistic phenomena.6. The choice of lexical
items depends on the domain at issue since the lexical inventory should cover at least
the range of non-linguistic phenomena represented in the non-linguistic component of the
system.
Categorial knowledge provided by the lexicon together with the rules of grammar con-
stitutes the descriptional apparatus for the classification of expressions. On the one hand
expressions serve as input for linguistic processing and on the other hand they represent
sequential patterns of written or spoken language. This intermediate status makes them
5 Most of our assumptions about the representation of linguistic and non-linguistic knowledge are based
on experiences gained from work in the LILOG-project at IBM Stuttgart. A description of formalisms and
methods applied in this project can be found in [Geurts 90].
6 One can for example reduce the computational complexity by limiting the relevant sentence-level
constructions to simple propositional clauses if the system is not meant to deal with other types of
modality. Even if this argument sounds quite trivial the determination of a set of requirements for the
linguistic component are as important as they are for the representation of domain knowledge.
</bodyText>
<page confidence="0.984282">
170
</page>
<bodyText confidence="0.999854636363636">
elements of discourse knowledge. Expressions belong to the type of knowledge which serves
as a kind of record for the registration of linguistic interactions together with their spatio-
temporal specifications. It directly corresponds to episodic knowledge on the non-linguistic
side. Episodic knowledge has the same intermeditate status as discourse knowledge since
on the one hand it serves as a record of &apos;statements&apos; and other &apos;experiences&apos; with respect
to the domain and on the other hand it is used to characterize entities from the domain
as individuals on the basis of conceptual knowledge. Conceptual knowledge combines the
structural information conveyed by the ontology with the additional information expressed
by rules of inference. Individuals which result from the processing of certain linguistic ex-
pressions are called referents since they are open to further reference by linguistic means.
The figure below shows the whole classification assumed as the basis of our model.
</bodyText>
<table confidence="0.973719">
Knowledge
Linguistic Non-linguistic
genera znd.vn,ual / \
Categorial Discourse general individual
elements rules / \
Lexical Grammatical Conceptual Episodic
Expressions elements rules
Ontological Inferential
Referents
</table>
<figureCaption confidence="0.997548">
Figure 1: The classification of knowledge types.
</figureCaption>
<sectionHeader confidence="0.96874" genericHeader="method">
4 Word meaning
</sectionHeader>
<bodyText confidence="0.999918176470588">
The task of our model is an approach to the various aspects of word meaning which
are responsible for the variability effects described in section 2. In order to reduce the
complexity of the linguistic domain we restrict the relevant linguistic expressions to those
representing simple word forms which cannot be further decomposed by mophological
processes other than inflection. As a consequence, the granularity for the representation
of linguistic knowledge treats basic morphemes as minimal elements. Another consequence
is the width of the temporal grid which specifies the minimal &apos;temporal distance&apos; between
elements of discourse knowledge. We introduce temporal indices, allowing to subdivide
the linguistic input into a chain of word-level segments. Each index uniquely identifies
a gap between two words and directly corresponds to a set of intermediate results in
the course of processing the input. These results are what we call the context. Formally
speaking, a context is a set of factors marked by a temporal index specific to a certain stage
of processing at which the system is observed. Factors are functions between knowledge
elements or between instances thereof. They are elements of specific contexts and therefore
differ from attributes because their existence is strictly tied to a certain stage of processing
associated with a temporal index. As a result of iterated forwarding a factor may remain
applicable during a sequence of processing steps. Factors may be classified according
</bodyText>
<page confidence="0.996407">
171
</page>
<bodyText confidence="0.995932">
to their origin. Factors which are directly derived from knowledge elements are called
primitive factors. Depending on the type of knowledge elements involved we distinguish
two modes of origin. Primitive factors can be ...
</bodyText>
<listItem confidence="0.9828885">
• selected as instances of attributes or
• established by the application of rules.
Complex factors are derived from primitive ones by one of the following operations:
• the restriction of the domain and/or range of a primitve factor
• the application of set-theoretical operations on primitive factors
• the functional composition of primitive factors
</listItem>
<bodyText confidence="0.999941166666667">
We present this classification because our analysis of word meaning crucially depends
on the notion of contextual factors. It is the main goal of our project to reconstruct
word meaning as the result of the interaction of processes wich cope with an effective
integration of various linguistic and non-linguistic factors primitive and complex in nature.
Since we investigate word meaning under the aspect of the potential of words to refer to
representations of entities of the domain, word meaning in our terminology is a complex
factor which links elements from discourse knowledge (expressions representing words)
to elements from episodical knowledge (referents). The temporary status of factors is
responsible for the fact that for the identification of the referential meaning of a word
the whole context has to be taken into account. The linguistic notion of &apos;word meaning&apos;
therefore derives from the analysis of subsets of factors that result from the intersection
of contexts present in a sufficiently large group of different uses of the same word.
</bodyText>
<subsectionHeader confidence="0.998168">
4.1 The constituents of reference
</subsectionHeader>
<bodyText confidence="0.998247272727273">
In order to characterize the interrelation between factors introducing variability effects
(productive factors) and those limiting the &apos;semantic search space&apos; (restrictive factors) we
need to examine the way in which word meaning can be decomposed into a small number
of factors7. A segmentation of the interpretation process according to our classification of
knowledge types leads to three components which by application of functional composition
constitute word meaning. Since components are derived by functional decomposition of
a complex factor (word meaning) they are factors as well. Components may be further
analyzed as the results of set-theoretic operations on basic factors some of which limit and
some of which extend the &apos;semantic scope&apos; of a word form8. Factors extending the scope
of interpretation are directly responsible for the variability effects described in section 2.
Factors constraining the scope of interpretation are the topic of chapter 4.2. The following
list introduces the three components of meaning together with examples of the relevant
productive factors. An interesting criterion for the classification of productive factors is
whether they are established by the application of rules or selected from attributes be-
tween knowledge elements.
7 We do not assume contexts to be finite but our approach relies on the fact that a finite subset of
the context suffices to describe word meaning precisely enough to demonstrate the requirements a system
with reasonable disambiguation capabilities has to fulfil.
8 In fact the same function may in one stage of the interpretatio process serve as productive factor and
in another as a restrictive factor. Thus, the property of productivity or restrictivity cannot definitively
assigned to specific factors. More precisely speaking, it is property of factors dependent on the current
stage of processing represented by the temporal index.
</bodyText>
<page confidence="0.9934">
172
</page>
<sectionHeader confidence="0.509703" genericHeader="method">
(1) Categorization
</sectionHeader>
<bodyText confidence="0.9956654">
Categorization as the first component of the chain maps expressions representing words
onto lexical items. Its relevant productive factor is established by the application of mor-
phological rules in some cases involving morphological ambiguity.
The following categorization of &amp;quot;saw&amp;quot; is selected from the lexicon because of the identity
of phonological forms:
</bodyText>
<table confidence="0.790166894736842">
PHON &apos;saw&apos;
MAJOR V
SYN TENSE PRESENT
[
SUBCAT &lt; ... &gt; v ... v &lt;
cati4
MAJOR N
V TENSE PRESENT
[
SUBCAT &lt;&gt;
SEM Conc54
A morphological rule establishes the categorization of &amp;quot;saw&amp;quot; as an inflected form de-
rived from the lexical base form for &amp;quot;see&amp;quot;:
PHON f3rdang(&apos;sees) &gt;]
MAJOR V
cat is SYN TENSE PAST
SUBCAT &lt; &gt; v v &lt;
SEM COnC75
(2) Lexical meaning
</table>
<bodyText confidence="0.962810222222222">
The semantic specification of a lexical item and the properties of the corresponding
concept are related by means of the SEM value. Two basic factors which contribute to the
relevant productive factor of lexical meaning originate from attributes in the knowledge
base by means of selection. The linguistic constituent of lexical meaning may involve
polysemy or polyfunctionality if it provides a range of semantic alternatives and the cor-
responding morphosyntactic properties for a single lexical item.
(2a) The linguistic constituent of lexical meaning
The subcategorization entry of the lexical item selects the following three9 polysemous
readings for cat 14:
</bodyText>
<table confidence="0.9973602">
SUBCAT SYN [ SYN NP[ACC] [ SYN conc35 NP[NOM] NP[NOM] [ SEM NP[NOM]
V SEM PP SEM concio SEM &apos; COnCg corms conc8
V&lt; WITH] D j SYN
[ SSEYMN SEM
NP[ACC]
</table>
<bodyText confidence="0.5439048">
The non-linguistic constituent of lexical meaning is responsible for variabilities origi-
nating from systematic relationships between different concepts related to a single lexical
9 As a matter of illustration the subcategorization frame does not exhaust the range of alternative
readings. It again depends on the task of the linguistic component wether the lexical item has to provide
further polysemous variants such as the intransitive reading of &amp;quot;see&amp;quot;.
</bodyText>
<page confidence="0.994246">
173
</page>
<table confidence="0.617397833333333">
item by means of the semantic specification.
(2b) The non-linguistic constituent of lexical meaning
conc75 SITUATION C . . .
time conc5 spatio-temporal properties
location : conc3
conc 23 PERCEPTION C SITUATION
</table>
<bodyText confidence="0.696457333333333">
actor conc8 The filler of the actor role visually perceives
theme conci3 the filler of the theme role
instrument : conc35 by using the filler of the instrument role
</bodyText>
<sectionHeader confidence="0.388902" genericHeader="method">
COELC34 REALIZATION C SITUATION
</sectionHeader>
<bodyText confidence="0.680419">
actor conc8 The filler of the actor role
proposition : conc 19 realizes the filler of the proposition role
conc39 VISTING_SITUATION C SITUATION
visitor conc2 The filler of the visitor role
visited COTIC4
</bodyText>
<sectionHeader confidence="0.742095" genericHeader="method">
(3) Individuation
</sectionHeader>
<bodyText confidence="0.9996478">
This last factor in the chain of meaning components becomes established by the ap-
plication of rules of inference. It maps concepts onto referents. The productive factor of
individuation is referentiality extending the range of possible referents a concept can be
individuated to. Referentiality here serves as cover term for the phenomena described in
chapter 2.4 together with cases of conceptual variation which qualify as &apos;ad-hoc-anaphora&apos;
because they succeed to identify a unique referent in a specific context but cannot be char-
acterized as instances of general principles guiding a shift in conceptual interpretation10.
Additional parameters of the discourse situation (the time and location of the utterance
as well as a proposition ri) allow to establish an individuation which maps conc34 onto a
referent r2 with the following properties:
</bodyText>
<equation confidence="0.999923">
actor(r2) = rspeaker A
proposition(r2) ri A
location(r2) = r3 A r3 C Sdiscourse A
time(r2) = r4 A r4 &lt; idi3COUrs e
</equation>
<bodyText confidence="0.999962">
The three components of word meaning can be considered intermediate steps of the
interpretation process. They may be analyzed and described in isolation since their in-
teraction results from the way in which the range of the preceding component fits to the
domain of the following. The task of the interpretation process on this background is to
find a &apos;path&apos; leading from an expression to an individual which under consideration of all
the available contextual factors qualifies as plausible candidate for the referential meaning
of the expression.
</bodyText>
<subsectionHeader confidence="0.999417">
4.2 How the semantic scope can be restricted
</subsectionHeader>
<bodyText confidence="0.9998095">
The crucial question now is how the diverse components interact in order to reduce the
range of word meaning by the exclusion of implausible variants. Here we pick out three
</bodyText>
<footnote confidence="0.861576">
10Nunberg&apos;s ham-sandwich is an example instance of this kind of context specific ad-hoc-anaphora.
</footnote>
<page confidence="0.996743">
174
</page>
<bodyText confidence="0.9539735">
example groups of factors which in a typical situation may support the reductive factors
of meaning components and such help to reduce the referential potential of a word.
</bodyText>
<listItem confidence="0.788739">
(1) Word-specific factors
</listItem>
<bodyText confidence="0.9982893125">
The first group are factors which result from structural relationships expressed by
morphosyntactic attributes and rules of grammar. As we mentioned in chapter 2.3 these
factors only affect variabilities which are introduced by the linguistic part of our knowledge
base. Factors of this group thus may help to resolve cases of morphological ambiguity, pol-
ysemy or polyfunctionality but they have no effect on variants that result from metonymy,
change of semantic type or other instances of contextual relativity.
Consider the following part of discourse:
&amp;quot;I tried to find a possibility to escape. Then I saw a hole in the fence.&amp;quot;
We&apos;ll give a sketch of an analysis of the meaning of &amp;quot;saw&amp;quot; in this example on the
basis of the knowledge elements introduced in the previous section. The rules of gram-
mar suppress the nominal reading of &amp;quot;saw&amp;quot; since the principles of X-syntax require the
constituent &amp;quot;a hole in the wall&amp;quot; to be &apos;absorbed&apos;. Morphological rules do not support
the disambiguation process. On the contrary, their productive potential causes the in-
troduction of the variant derived from the base form &amp;quot;see&amp;quot;. The variant cat56 is ruled
out because of incompatibilities between its subcategorization frame and the syntactic
environment of &amp;quot;saw&amp;quot; in our example.
</bodyText>
<table confidence="0.980094636363636">
cat56 PHON MAJOR SYN PRESENT NPENOM) &gt;
SYN TENSE SEM V COAC18 NP[NOM]
V SUBCAT &apos;saw&apos; &apos; SYN conci8
V [SYN &lt;&gt; [ SEM
SEM
NP[ACC]
conc42
[ MAJOR
TENSE PRESENT
SUBCAT
SEM conc25
</table>
<bodyText confidence="0.975080909090909">
The intransitive polysemous variant fails because of the same reasons as the nominal
homonymous variant. The transitive reading of cat56 would force an optional preposi-
tional argument to be headed by &amp;quot;into&amp;quot;11. Thus, the polysemous variant conc42 can be
singled out purely on the basis of word-specific factors if the rules of grammar do not
account for the adjunction of a locative PP with the head &amp;quot;in&amp;quot;. In case the grammar
licences the existence of a prepositional adjunct, our model of the domain would have to
contribute the restrictive factor that the concept associated with &amp;quot;the fence&amp;quot; does not fit
with conditions on `sawing&apos;-events. Since the reductive influence of word-specific factors
ends with the selection of polysemous variants we cannot expect a further restriction of
the &apos;semantic scope&apos; without additionally considering other types of factors.
&amp;quot;In order to simplify the example this alternative does not occur in the feature structure of cat56.
</bodyText>
<page confidence="0.996244">
175
</page>
<sectionHeader confidence="0.680275" genericHeader="method">
(2) Selections&apos; restrictions
</sectionHeader>
<bodyText confidence="0.941586714285714">
A different group of factors belongs to the semantic levell2 of our model. Factors in
this group neither are clear instances of linguistic regularities nor of non-linguistic ones.
They are partially linguistic and partially non-linguistic in nature and therefore considered
as complex factors derived by the integration of elements from both types of knowledge.
They may help to reduce variabilities affecting categorization or lexical meaning. Yet, like
word-secific factors they do not constrain contextual relativity.
Consider the following part of discourse:
{
a piece of wood .
&amp;quot;I saw in the bathroom.&amp;quot;
a cup of coffee
The semantic specification for the internal argument of &amp;quot;saw&amp;quot; leads to a concept conc42
representing a class of entities which qualify as fillers of the corresponding role in the
conceptual representation of the `sawing&apos;-event.
</bodyText>
<sectionHeader confidence="0.812025" genericHeader="method">
conc75 SAWING C SITUATION
</sectionHeader>
<bodyText confidence="0.990259545454546">
actor concis for example: a human beeing
object COrie42 for example: a concrete object
instrument conc29 for example: a set of tools
The compatibility between the semantic specification of the internal argument of the
two polysemous readings of &amp;quot;saw&amp;quot; and the type specification of a role belonging to the cor-
respondint sEm-value account for the existence of selectional restrictions. The conceptual
representation of &amp;quot;a piece of wood&amp;quot; must be compatible with conc42 in order to establish
the lexical meaning leading to the concept SAVIIG_EVENT. In the case of &amp;quot;this hand-saw saws
well&amp;quot; the external argument would because of requirements on fillers of conceptual roles
have to be mapped on the instruemt role of conc75. The situation is more tricky if we
compare the instances of the external argument of cat23 in the following example:
</bodyText>
<listItem confidence="0.9984025">
(1) The policeman saw an accident.
(2) *The ball saw an accident.
(3) The automatic traffic control camera saw an accident.
(4) ?The morning saw an accident.
</listItem>
<bodyText confidence="0.944952583333333">
An interesting aspect of this phenomena is that selectional restrictions may be can-
celled by contextual factors or by means of rhetoric devices. The example sentences show
how difficult it might be to identify an obligatory set of selectional restrictions. Compar-
ing (1) with (2) suggests being an instance of the concept PERSON as a reasonable choice.
Example (3) however shows that the critical property is something like &apos;having an optical
sensoring mechanism capable of detecting objects&apos;. Sentence (4) might imply a metaphor-
ical interpretation in spite of its apparent semantic illformedness. This again yields an
argument in favour of a domain-driven design strategy for the semantic level linking be-
tween categories and concepts.
12 The semantic level is the &apos;interface&apos; between linguistic and non-linguistic knowledge represented by the
SEM values in lexical items together with a set of rules which attune semantic specifications of argument
structure to attributes of the corresponding conceptual definitions.
</bodyText>
<page confidence="0.995637">
176
</page>
<bodyText confidence="0.9707938">
(3) The set of possible referents
The last group of factors exemplified here are the only means available to reduce the
semantic scope resulting from contextual relativity. As we saw in chapter 2.4 contextual
relativity is a fundamental property all referential expressions have in common. Since the
&apos;semantic scope&apos; introduced by variabilities of this type cannot be subdivided into a set
of alternative readings neither lexical nor conceptual information does help to restrict the
range of indivivation.
The only way out of this dilemma is to derive a set of possible referents from knowledge
about the domain and from information occuring in the preceding part of discourse. The
latter calls for an investigation of discourse properties on the basis of pragmatic devices
such as the Gricean conversation principles. Bridging phenomenal&apos; as generalizations
of anaphoric binding are promising candidates for an approach to the determination of
possible referents. C. Sidner emphasizes: &amp;quot;anaphor interpretation can be studied as a
computational process that uses the already existing specification of a noun phrase to
find the specification of an anaphor&amp;quot; [Sidner 83, p.269]. The actual limits of a set of
possible referents thus very much depend on the inferential capabilities of our system
to reconstruct the conceptual relationships undelying text coherence. The notion of fo-
cus presented in the work of Sidner certainly plays a crucial role in the reducion of the
computational complexity a computation of all possible bindings would involve if realistic
discourse situations were to be considered.
</bodyText>
<sectionHeader confidence="0.999062" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999921785714285">
Up to now we merely picked a collection of phenomena with an impact on the referential
potential of words ranging from lexical properties to &apos;genuine&apos; discourse phenomena. We
presented a moderately general framework designed in a way that each of these phenomena
has a place to fit in. The example analysis of &amp;quot;saw&amp;quot; illustrated the interaction of elements
of our model and thereby pointed to problematic aspects that yet have to be resoved. A
crucial problem is to distinguish between linguistic aspects of lexical meaning which the
lexicon has to account for from non-linguistic aspects which derive from relations in con-
ceptual structure. The next step in the evaluation of our model requires the determination
of criteria which help to keep linguistic and non-linguistic aspects of the semantic level
apart. The ultimate goal of this distinction is to narrow down the flow of information that
passes through the semantic &apos;interface&apos; between linguistic and non-linguistic knowledge. If
this strategy succeeds it should be possible to adapt the conceptual knowledge to different
domains of application not affecting linguistic knowledge as the basis for capabilities of
discourse understanding.
</bodyText>
<sectionHeader confidence="0.998121" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.859164">
[Bierwisch 82] M. Bierwisch, &amp;quot;Semantische und konzeptuelle Reprisentation
lexikalischer Einheiten&amp;quot;, R. Rtilioka / W. Motsch (eds.), Unter-
suchungen zur Semantik, Studio Grammatica XXII , Berlin, 1982.
131n the terminology of [Clarkillaviland 771
</reference>
<page confidence="0.981701">
177
</page>
<reference confidence="0.998449974358974">
[Binnick 70] R. I. Binnick, &amp;quot;Ambiguity and vagueness&amp;quot;, Papers from the Sixth
Regional Meeting of the Chicago Linguistic Society, Chicago, 1970,
pp. 147-153.
[Bosch 83] P. Bosch, &amp;quot;Vagueness is Context-Dependance: A solution to the
sorites paradox&amp;quot;, T. T. Ballmer / M. Pinkal (eds.), Approaching
Vagueness, Elsevier Science Publishers B.v. (North-Holland), 1983,
pp. 189-210.
[Clark/Haviland 77] H. H. Clark and S. E. Haviland, &amp;quot;Comprehension and the given-new
contract&amp;quot;, R. 0. Freedle (ed.), Discourse Production and Compre-
hension, Norwood, New Jersey, 1977.
[Geurts 90] Bart Geurts (ed.), &amp;quot;Natural Language Understanding in LILOG: An
Intermediate Overview&amp;quot;, IWBS Report 137, IBM Germany GmbH,
Scientific Center, 1990.
[Kooij 71] J. G. Kooij. &amp;quot;Ambiguity in Natural Language. An investigation of
certain problems in its linguistic description&amp;quot;, Amsterdam, London,
1971.
[Nebel/Smolka 89] B. Nebel / G. Smolka, &amp;quot;Representation and Reasoning with Attribu-
tive Descriptions&amp;quot;, I WBS Report 81, IBM Germany GmbH, Scientific
Center, 1989.
[Nunberg 78] G. D. Nunberg. &amp;quot;The Pragmatics of Reference&amp;quot;, PhD Thesis, Repro-
duced by the Indiana University Linguistics Club, 1978.
[Pinkal 80] M. Pinkal, &amp;quot;Semantische Vagheit: Phanomen und Theorien: Teil I&amp;quot;
Linguistische Berichte, 1980.
[Pollard/Sag 87] C. Pollard / E. A. Sag, &amp;quot;Information-based Syntax and Semantics:
Volume I&amp;quot;, CSLI Lecture Notes Number 13 Stanford, 1987.
[Pustejovsky 90] J. Pustejovsky, &amp;quot;Semantic Function and Lexical Decomposition&amp;quot;,
Schmitz / Schuetz / Kunz (eds.), Linguistic Approaches to Artifi-
cial Intelligence, Lang, 1990, pp. 243-303.
[Shieber 86] S. M. Shieber. &amp;quot;An Introduction to Unification-Based Approaches to
Grammar&amp;quot;, CLSI Lecture Notes Number 4, Stanford, 1986.
[Sidner 83] C. L. Sidner. &amp;quot;Focusing in the comprehension of definite anaphora&amp;quot;,
M. Brady / Robert C. Berwick, (eds.), Computational Models of Dis-
ourse, Cambridge, Mass. and London, 1983.
[Weber 74] H. J. Weber, &amp;quot;Mehrdeutige Wortformen im heutigen Deutsch: Stu-
dien zu ihrer grammatischen Beschreibung und lexikographischen
Erfassung&amp;quot;, Tubingen, 1974.
[Zwicky/Sadock 75] A. M. Zwicky and J. M. Sadock, &amp;quot;Ambiguity tests and how to fail
them&amp;quot;, J. P. Kimball (ed.) ,Syntax and Semantics, pp. 1-36, San
Diego, 1975.
</reference>
<page confidence="0.997175">
178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.551158">
<title confidence="0.973929">A model for the interaction of lexical and non-lexical knowledge in the determination of word meaning</title>
<author confidence="0.999717">Peter Gerstl</author>
<affiliation confidence="0.985454">IBM Germany, Scientific Institute for Knowledge Based</affiliation>
<address confidence="0.8023055">Schlofistr. 7000 Stuttgart</address>
<email confidence="0.997832">e-mail:gerstldsOlilog.bitnet</email>
<abstract confidence="0.99888775">The lexicon of a natural language understanding system that is not restricted to one single application but should be adaptable to a whole range of different tasks has to provide a flexible mechanism for the determination of word meaning. The reason for a mechanism is the semantic words, i.e. their potential to denote different things in different contexts. The goal of our project is a model that makes phenomena explicit. We approach this goal by defining meaning as function resulting from the interaction of processes operating on the following we characterize the range of phenomena our model is intended to describe and give an outline of the way in which the interpretation process may determine the referential potential of words by the integration and evaluation of a variety of factors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>M Bierwisch</author>
</authors>
<title>Semantische und konzeptuelle Reprisentation lexikalischer Einheiten&amp;quot;,</title>
<booktitle>Untersuchungen zur Semantik, Studio Grammatica XXII , Berlin, 1982. 131n the terminology of [Clarkillaviland 771</booktitle>
<editor>R. Rtilioka / W. Motsch (eds.),</editor>
<marker>[Bierwisch 82]</marker>
<rawString>M. Bierwisch, &amp;quot;Semantische und konzeptuelle Reprisentation lexikalischer Einheiten&amp;quot;, R. Rtilioka / W. Motsch (eds.), Untersuchungen zur Semantik, Studio Grammatica XXII , Berlin, 1982. 131n the terminology of [Clarkillaviland 771</rawString>
</citation>
<citation valid="true">
<authors>
<author>R I Binnick</author>
</authors>
<title>Ambiguity and vagueness&amp;quot;, Papers from the Sixth Regional Meeting of the Chicago Linguistic Society,</title>
<date>1970</date>
<pages>147--153</pages>
<location>Chicago,</location>
<marker>[Binnick 70]</marker>
<rawString>R. I. Binnick, &amp;quot;Ambiguity and vagueness&amp;quot;, Papers from the Sixth Regional Meeting of the Chicago Linguistic Society, Chicago, 1970, pp. 147-153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bosch</author>
</authors>
<title>Vagueness is Context-Dependance: A solution to the sorites paradox&amp;quot;,</title>
<date>1983</date>
<booktitle>Approaching Vagueness, Elsevier Science Publishers B.v.</booktitle>
<pages>189--210</pages>
<editor>T. T. Ballmer / M. Pinkal (eds.),</editor>
<location>(North-Holland),</location>
<marker>[Bosch 83]</marker>
<rawString>P. Bosch, &amp;quot;Vagueness is Context-Dependance: A solution to the sorites paradox&amp;quot;, T. T. Ballmer / M. Pinkal (eds.), Approaching Vagueness, Elsevier Science Publishers B.v. (North-Holland), 1983, pp. 189-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>S E Haviland</author>
</authors>
<title>Comprehension and the given-new contract&amp;quot;,</title>
<date>1977</date>
<booktitle>Discourse Production and Comprehension,</booktitle>
<editor>R. 0. Freedle (ed.),</editor>
<location>Norwood, New Jersey,</location>
<marker>[Clark/Haviland 77]</marker>
<rawString>H. H. Clark and S. E. Haviland, &amp;quot;Comprehension and the given-new contract&amp;quot;, R. 0. Freedle (ed.), Discourse Production and Comprehension, Norwood, New Jersey, 1977.</rawString>
</citation>
<citation valid="true">
<title>Natural Language Understanding in LILOG: An Intermediate Overview&amp;quot;,</title>
<date>1990</date>
<booktitle>IWBS Report 137, IBM Germany GmbH, Scientific Center,</booktitle>
<editor>Bart Geurts (ed.),</editor>
<marker>[Geurts 90]</marker>
<rawString>Bart Geurts (ed.), &amp;quot;Natural Language Understanding in LILOG: An Intermediate Overview&amp;quot;, IWBS Report 137, IBM Germany GmbH, Scientific Center, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Kooij</author>
</authors>
<title>Ambiguity in Natural Language. An investigation of certain problems in its linguistic description&amp;quot;,</title>
<date>1971</date>
<location>Amsterdam, London,</location>
<marker>[Kooij 71]</marker>
<rawString>J. G. Kooij. &amp;quot;Ambiguity in Natural Language. An investigation of certain problems in its linguistic description&amp;quot;, Amsterdam, London, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Nebel G Smolka</author>
</authors>
<title>Representation and Reasoning with Attributive Descriptions&amp;quot;,</title>
<date>1989</date>
<journal>I WBS Report</journal>
<volume>81</volume>
<institution>IBM Germany GmbH, Scientific Center,</institution>
<marker>[Nebel/Smolka 89]</marker>
<rawString>B. Nebel / G. Smolka, &amp;quot;Representation and Reasoning with Attributive Descriptions&amp;quot;, I WBS Report 81, IBM Germany GmbH, Scientific Center, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Nunberg</author>
</authors>
<title>The Pragmatics of Reference&amp;quot;,</title>
<date>1978</date>
<tech>PhD Thesis,</tech>
<institution>Reproduced by the Indiana University Linguistics Club,</institution>
<marker>[Nunberg 78]</marker>
<rawString>G. D. Nunberg. &amp;quot;The Pragmatics of Reference&amp;quot;, PhD Thesis, Reproduced by the Indiana University Linguistics Club, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pinkal</author>
</authors>
<title>Semantische Vagheit: Phanomen und Theorien: Teil I&amp;quot; Linguistische Berichte,</title>
<date>1980</date>
<marker>[Pinkal 80]</marker>
<rawString>M. Pinkal, &amp;quot;Semantische Vagheit: Phanomen und Theorien: Teil I&amp;quot; Linguistische Berichte, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard E A Sag</author>
</authors>
<title>Information-based Syntax and Semantics: Volume I&amp;quot;,</title>
<date>1987</date>
<journal>CSLI Lecture Notes Number</journal>
<volume>13</volume>
<location>Stanford,</location>
<marker>[Pollard/Sag 87]</marker>
<rawString>C. Pollard / E. A. Sag, &amp;quot;Information-based Syntax and Semantics: Volume I&amp;quot;, CSLI Lecture Notes Number 13 Stanford, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>Semantic Function and Lexical Decomposition&amp;quot;,</title>
<date>1990</date>
<booktitle>Linguistic Approaches to Artificial Intelligence,</booktitle>
<pages>243--303</pages>
<editor>Schmitz / Schuetz / Kunz (eds.),</editor>
<location>Lang,</location>
<marker>[Pustejovsky 90]</marker>
<rawString>J. Pustejovsky, &amp;quot;Semantic Function and Lexical Decomposition&amp;quot;, Schmitz / Schuetz / Kunz (eds.), Linguistic Approaches to Artificial Intelligence, Lang, 1990, pp. 243-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>An Introduction to Unification-Based Approaches to Grammar&amp;quot;,</title>
<date>1986</date>
<journal>CLSI Lecture Notes Number</journal>
<volume>4</volume>
<location>Stanford,</location>
<marker>[Shieber 86]</marker>
<rawString>S. M. Shieber. &amp;quot;An Introduction to Unification-Based Approaches to Grammar&amp;quot;, CLSI Lecture Notes Number 4, Stanford, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
</authors>
<title>Focusing in the comprehension of definite anaphora&amp;quot;,</title>
<date>1983</date>
<booktitle>Computational Models of Disourse,</booktitle>
<editor>M. Brady / Robert C. Berwick, (eds.),</editor>
<location>Cambridge, Mass. and London,</location>
<marker>[Sidner 83]</marker>
<rawString>C. L. Sidner. &amp;quot;Focusing in the comprehension of definite anaphora&amp;quot;, M. Brady / Robert C. Berwick, (eds.), Computational Models of Disourse, Cambridge, Mass. and London, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Weber</author>
</authors>
<title>Mehrdeutige Wortformen im heutigen Deutsch: Studien zu ihrer grammatischen Beschreibung und lexikographischen Erfassung&amp;quot;,</title>
<date>1974</date>
<location>Tubingen,</location>
<marker>[Weber 74]</marker>
<rawString>H. J. Weber, &amp;quot;Mehrdeutige Wortformen im heutigen Deutsch: Studien zu ihrer grammatischen Beschreibung und lexikographischen Erfassung&amp;quot;, Tubingen, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Zwicky</author>
<author>J M Sadock</author>
</authors>
<title>Ambiguity tests and how to fail them&amp;quot;,</title>
<date>1975</date>
<booktitle>Syntax and Semantics,</booktitle>
<pages>1--36</pages>
<editor>J. P. Kimball (ed.)</editor>
<location>San Diego,</location>
<marker>[Zwicky/Sadock 75]</marker>
<rawString>A. M. Zwicky and J. M. Sadock, &amp;quot;Ambiguity tests and how to fail them&amp;quot;, J. P. Kimball (ed.) ,Syntax and Semantics, pp. 1-36, San Diego, 1975.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>