<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.8064185">
Branch and Bound Algorithm for Dependency Parsing
with Non-local Features
</title>
<author confidence="0.993969">
Xian Qian and Yang Liu
</author>
<affiliation confidence="0.9988425">
Computer Science Department
The University of Texas at Dallas
</affiliation>
<email confidence="0.998846">
{qx,yangl}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.99666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999857157894737">
Graph based dependency parsing is inefficient
when handling non-local features due to high
computational complexity of inference. In
this paper, we proposed an exact and effi-
cient decoding algorithm based on the Branch
and Bound (B&amp;B) framework where non-
local features are bounded by a linear combi-
nation of local features. Dynamic program-
ming is used to search the upper bound. Ex-
periments are conducted on English PTB and
Chinese CTB datasets. We achieved competi-
tive Unlabeled Attachment Score (UAS) when
no additional resources are available: 93.17%
for English and 87.25% for Chinese. Parsing
speed is 177 words per second for English and
97 words per second for Chinese. Our algo-
rithm is general and can be adapted to non-
projective dependency parsing or other graph-
ical models.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99972797826087">
For graph based projective dependency parsing, dy-
namic programming (DP) is popular for decoding
due to its efficiency when handling local features.
It performs cubic time parsing for arc-factored mod-
els (Eisner, 1996; McDonald et al., 2005a) and bi-
quadratic time for higher order models with richer
sibling and grandchild features (Carreras, 2007; Koo
and Collins, 2010). However, for models with gen-
eral non-local features, DP is inefficient.
There have been numerous studies on global in-
ference algorithms for general higher order parsing.
One popular approach is reranking (Collins, 2000;
Charniak and Johnson, 2005; Hall, 2007). It typi-
cally has two steps: the low level classifier gener-
ates the top k hypotheses using local features, then
the high level classifier reranks these candidates us-
ing global features. Since the reranking quality is
bounded by the oracle performance of candidates,
some work has combined candidate generation and
reranking steps using cube pruning (Huang, 2008;
Zhang and McDonald, 2012) to achieve higher or-
acle performance. They parse a sentence in bottom
up order and keep the top k derivations for each s-
pan using k best parsing (Huang and Chiang, 2005).
After merging the two spans, non-local features are
used to rerank top k combinations. This approach
is very efficient and flexible to handle various non-
local features. The disadvantage is that it tends to
compute non-local features as early as possible so
that the decoder can utilize that information at inter-
nal spans, hence it may miss long historical features
such as long dependency chains.
Smith and Eisner modeled dependency parsing
using Markov Random Fields (MRFs) with glob-
al constraints and applied loopy belief propaga-
tion (LBP) for approximate learning and inference
(Smith and Eisner, 2008). Similar work was done
for Combinatorial Categorial Grammar (CCG) pars-
ing (Auli and Lopez, 2011). They used posterior
marginal beliefs for inference to satisfy the tree con-
straint: for each factor, only legal messages (satisfy-
ing global constraints) are considered in the partition
function.
A similar line of research investigated the use
of integer linear programming (ILP) based parsing
(Riedel and Clarke, 2006; Martins et al., 2009). This
</bodyText>
<page confidence="0.997044">
37
</page>
<note confidence="0.278328">
Transactions of the Association for Computational Linguistics, 1 (2013) 37–48. Action Editor: Ryan McDonald.
Submitted 11/2012; Revised 2/2013; Published 3/2013. c�2013 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99845152173913">
method is very expressive. It can handle arbitrary
non-local features determined or bounded by linear
inequalities of local features. For local models, LP is
less efficient than DP. The reason is that, DP works
on a small number of dimensions in each recursion,
while for LP, the popular revised simplex method
needs to solve a m dimensional linear system in
each iteration (Nocedal and Wright, 2006), where
m is the number of constraints, which is quadratic
in sentence length for projective dependency pars-
ing (Martins et al., 2009).
Dual Decomposition (DD) (Rush et al., 2010;
Koo et al., 2010) is a special case of Lagrangian re-
laxation. It relies on standard decoding algorithms
as oracle solvers for sub-problems, together with a
simple method for forcing agreement between the
different oracles. This method does not need to con-
sider the tree constraint explicitly, as it resorts to dy-
namic programming which guarantees its satisfac-
tion. It works well if the sub-problems can be well
defined, especially for joint learning tasks. Howev-
er, for the task of dependency parsing, using various
non-local features may result in many overlapped
sub-problems, hence it may take a long time to reach
a consensus (Martins et al., 2011).
In this paper, we propose a novel Branch and
Bound (B&amp;B) algorithm for efficient parsing with
various non-local features. B&amp;B (Land and Doig,
1960) is generally used for combinatorial optimiza-
tion problems such as ILP. The difference between
our method and ILP is that the sub-problem in ILP
is a relaxed LP, which requires a numerical solution,
while ours bounds the non-local features by a lin-
ear combination of local features and uses DP for
decoding as well as calculating the upper bound of
the objective function. An exact solution is achieved
if the bound is tight. Though in the worst case,
time complexity is exponential in sentence length,
it is practically efficient especially when adopting a
pruning strategy.
Experiments are conducted on English PennTree
Bank and Chinese Tree Bank 5 (CTB5) with stan-
dard train/develop/test split. We achieved 93.17%
Unlabeled Attachment Score (UAS) for English at a
speed of 177 words per second and 87.25% for Chi-
nese at a speed of 97 words per second.
</bodyText>
<sectionHeader confidence="0.916484" genericHeader="method">
2 Graph Based Parsing
</sectionHeader>
<subsectionHeader confidence="0.911802">
2.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.998952916666667">
Given a sentence x = x1, x2, ... , xn where xi is
the ith word of the sentence, dependency parsing as-
signs exactly one head word to each word, so that
dependencies from head words to modifiers form a
tree. The root of the tree is a special symbol de-
noted by x0 which has exactly one modifier. In this
paper, we focus on unlabeled projective dependency
parsing but our algorithm can be adapted for labeled
or non-projective dependency parsing (McDonald et
al., 2005b).
The inference problem is to search the optimal
parse tree y∗
</bodyText>
<equation confidence="0.99734">
* O( y)
y = arg maxyEy(�) x,
</equation>
<bodyText confidence="0.882399333333333">
where Y(x) is the set of all candidate parse trees of
sentence x. O(x, y) is a given score function which
is usually decomposed into small parts
</bodyText>
<equation confidence="0.9773615">
O(x, y) = ∑ Oc(x) (1)
c⊆y
</equation>
<bodyText confidence="0.9996108">
where c is a subset of edges, and is called a factor.
For example, in the all grandchild model (Koo and
Collins, 2010), the score function can be represented
as
where the first term is the sum of scores of all edges
xh -+ xm, and the second term is the sum of the
scores of all edge chains xg -+ xh -+ xm.
In discriminative models, the score of a parse tree
y is the weighted sum of the fired feature functions,
which can be represented by the sum of the factors
</bodyText>
<equation confidence="0.979372">
O(x, y) = wTf(x, y) = ∑ ∑wT f(x, c) = Oc(x)
c⊆y c⊆y
</equation>
<bodyText confidence="0.931220333333333">
where f(x, c) is the feature vector that depends on
c. For example, we could define a feature for grand-
child c = {egh, ehm}
</bodyText>
<equation confidence="0.9082926">
1 if xg =would n xh = be
nxm = happy n c is selected
0 otherwise
∑ ∑ Oegh,ehm(x)
O(x, y) = Oehm(x) +
ehm∈y egh,ehm∈y



f(x, c) =
</equation>
<page confidence="0.993981">
38
</page>
<subsectionHeader confidence="0.991764">
2.2 Dynamic Programming for Local Models
</subsectionHeader>
<bodyText confidence="0.999317">
In first order models, all factors c in Eq(1) contain a
single edge. The optimal parse tree can be derived
by DP with running time O(n3) (Eisner, 1996). The
algorithm has two types of structures: complete s-
pan, which consists of a headword and its descen-
dants on one side, and incomplete span, which con-
sists of a dependency and the region between the
head and modifier. It starts at single word spans, and
merges the spans in bottom up order.
For second order models, the score function
ϕ(x, y) adds the scores of siblings (adjacent edges
with a common head) and grandchildren
</bodyText>
<equation confidence="0.990508571428571">
ϕ(x, y) = � ϕehm(x)
ehmEy �
+
egh,ehmEy
�
+
ehm,ehsEy
</equation>
<bodyText confidence="0.999950285714286">
There are two versions of second order models,
used respectively by Carreras (2007) and Koo et al.
(2010). The difference is that Carreras’ only con-
siders the outermost grandchildren, while Koo and
Collin’s allows all grandchild features. Both models
permit O(n4) running time.
Third-order models score edge triples such as
three adjacent sibling modifiers, or grand-siblings
that score a word, its modifier and its adjacent grand-
children, and the inference complexity is O(n4)
(Koo and Collins, 2010).
In this paper, for all the factors/features that can
be handled by DP, we call them the local fac-
tors/features.
</bodyText>
<sectionHeader confidence="0.97924" genericHeader="method">
3 The Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.998995">
3.1 Basic Idea
</subsectionHeader>
<bodyText confidence="0.999784346153846">
For general high order models with non-local fea-
tures, we propose to use Branch and Bound (B&amp;B)
algorithm to search the optimal parse tree. A B&amp;B
algorithm has two steps: branching and bounding.
The branching step recursively splits the search s-
pace Y(x) into two disjoint subspaces Y(x) =
Y1 U Y2 by fixing assignment of one edge. For each
subspace Yi, the bounding step calculates the upper
bound of the optimal parse tree score in the sub-
space: UBYi ? maxyEYi ϕ(x, y). If this bound is
no more than any obtained parse tree score UBYi G
ϕ(x, y′), then all parse trees in subspace Yi are no
more optimal than y′, and Yi could be pruned safely.
The efficiency of B&amp;B depends on the branching
strategy and upper bound computation. For exam-
ple, Sun et al. (2012) used B&amp;B for MRFs, where
they proposed two branching strategies and a novel
data structure for efficient upper bound computation.
Klenner and Ailloud (2009) proposed a variation of
Balas algorithm (Balas, 1965) for coreference reso-
lution, where candidate branching variables are sort-
ed by their weights.
Our bounding strategy is to find an upper bound
for the score of each non-local factor c containing
multiple edges. The bound is the sum of new scores
of edges in the factor plus a constant
</bodyText>
<equation confidence="0.9879775">
ϕc(x) G � ψe(x) + αc
eEc
</equation>
<bodyText confidence="0.9978795">
Based on the new scores {ψe(x)} and constants
{αc}, we define the new score of parse tree y
</bodyText>
<equation confidence="0.8825412">
(x, y) = Y:(eEc
I: ψe(x) + αc
cCy
Then we have
ψ(x, y) ? ϕ(x, y), by E Y(x)
</equation>
<bodyText confidence="0.9808448">
The advantage of such a bound is that, it is the
sum of new edge scores. Hence, its optimum tree
maxyEY(x) ψ(x, y) can be found by DP, which is
the upper bound of maxyEY(x) ϕ(x, y), as for any
y E Y(x), ψ(x, y) ? ϕ(x, y).
</bodyText>
<subsectionHeader confidence="0.999921">
3.2 The Upper Bound Function
</subsectionHeader>
<bodyText confidence="0.9998795">
In this section, we derive the upper bound function
ψ(x, y) described above. To simplify notation, we
drop x throughout the rest of the paper. Let zc be
a binary variable indicating whether factor c is se-
lected in the parse tree. We reformulate the score
function in Eq(1) as
</bodyText>
<equation confidence="0.9913">
�
ϕ(y) ≡ ϕ(z) = ϕczc (2)
c
ϕehm,egh(x)
ϕehm,ehs(x)
</equation>
<page confidence="0.992901">
39
</page>
<bodyText confidence="0.999114">
Correspondingly, the tree constraint is replaced by
z ∈ Z. Then the parsing task is
According to the last inequality, we have the upper
bound for negative scored factors
</bodyText>
<equation confidence="0.9660454">
z∗ = arg maxz∈Zϕczc (3)
Notice that, for any zc, we have
(∑ )
ϕczc ≤ ϕc ze − (rc − 1) (6)
e∈c
</equation>
<bodyText confidence="0.956611090909091">
zc = min ze where rc is the number of edges
e∈c c. For simpli
in
city,
we use the notation
which means that factor c appears in parse tree if and
only if all its edges {e|e ∈ c} are selected in the tree.
Here ze is short for z{e} for simplicity.
Our bounding method is based on the following
fact: for a set {a1, a2,... ar} (aj denotes the jth el-
ement) , its minimum
</bodyText>
<equation confidence="0.960249">
∑min{aj} = min
p∈∆
j
where ∆ is probability simplex
∑ pj = 1} =min
∆ = {p|pj ≥ 0, ϕczc
j {σc(z),0}
pjaj (4)
</equation>
<bodyText confidence="0.9598">
We discuss the bound for ϕczc in two cases: ϕc ≥
</bodyText>
<figure confidence="0.7740821875">
0 and ϕc &lt; 0.
If ϕc ≥ 0, we have
ϕczc = ϕc min
e∈c
with domhc = {pc
ze
hc(pc, z) =p1 cσc(z)+p2 c · 0
∈∆;
∈{0,1}, ∀e ∈ c}.
According to Eq(4), we have
ze
∑=ϕc min e =min hc(pc, z) (8)
pc∈∆ e∈c pcze ϕczc
pc
∑=min
pc∈∆ e∈c
</figure>
<bodyText confidence="0.8071065">
The second equation comes from Eq(4). For sim-
plicity, let
</bodyText>
<equation confidence="0.7373335">
gc(pc, z) = ∑ ϕcpecze
e∈c
ϕcpecze
Let
ψ(p, z) = ∑ gc(pc, z) + ∑ hc(pc, z)
c,ϕc≥0 c,ϕc&lt;0
</equation>
<bodyText confidence="0.7593442">
ψ with respect to p, we have
min
p
with domain domgc = {pc ∈ ∆; ze ∈ {0, 1}, ∀e ∈
c}. Then we have
</bodyText>
<equation confidence="0.978136677419355">
ϕczc = min gc(pc,z) (5)
pc
If ϕc &lt; 0, we have two upper bounds. One is
commonly used in ILP when all the variables are bi-
nary
a∗ = min{aj}r
j=1
j
⇔
a∗ ≤ aj
∑a∗ ≥ aj − (r − 1)
j
hc(pc,z)

∑∑
gc(pc,z)
c,ϕc≥0c,ϕc&lt;0
z) +
∑
gc(pc,
c,ϕc&lt;0
∑= ∑ϕczc
+
c,ϕc&lt;0
c,ϕc≥0
(∑ )
σc(z) = ϕc ze − (rc − 1)
e∈c
The other upper bound when
&lt; 0 is simple
0 (7)
</equation>
<bodyText confidence="0.973591736842105">
Notice that, for an
ϕc
ϕczc≤
y parse tree, one of the upper
bounds must be tight. Eq(6) is tight if c appears
in the parse tree: zc = 1, otherwise Eq(7) is tight.
Therefore
Let
Minimize
ψ(p, z)
hc(pc, z)
The second equation holds since, for any two fac-
tors, c and
(or
and
(or
are separable.
The third equation comes from Eq(5) and Eq(8).
Based on this, we have the foll
</bodyText>
<equation confidence="0.605517866666667">
= ϕ(z)
c′,gc
hc)
gc′
hc′)
owing proposition:
= min
p
∑=
c,ϕc≥0
min
pc
min
pc
ϕczc
</equation>
<page confidence="0.936179">
40
</page>
<construct confidence="0.865853">
Proposition 1. For any p, pc E ∆, and z E Z,
ψ(p, z) ? ϕ(z).
</construct>
<bodyText confidence="0.9980978">
Therefore, ψ(p, z) is an upper bound function of
ϕ(z). Furthermore, fixing p, ψ(p, z) is a linear func-
tion of ze , see Eq(5) and Eq(8), variables zc for large
factors are eliminated. Hence z′ = arg maxzψ(p, z)
can be solved efficiently by DP.
</bodyText>
<figure confidence="0.96134804">
Because
AL
min max ψ(p, z)
p z∈Z
ze1=0
ze2=1
ψ=9
φ=4
=8
ψ=7
ψ
φ
=5
φ=4
ψ=7
φ=4
=7
ψ=4
ψ=6
ψ&lt;LB
ψ
φ
=5
φ=2
φ=3
</figure>
<equation confidence="0.997719">
z e1= 0 1
z e2= 0 1 0 1
ψ(p,z′) ? ψ(p,z∗) ? ϕ(z∗) ? ϕ(z′)
</equation>
<bodyText confidence="0.99757125">
after obtaining z′ , we get the upper bound and lower
bound of ϕ(z∗): ψ(p, z′) and ϕ(z′).
The upper bound is expected to be as tight as pos-
sible. Using min-max inequality, we get
</bodyText>
<equation confidence="0.9937552">
max ϕ(z) = max min ψ(p, z)
z∈Z z∈Z p
Z
z∈
max ψ (p, z)
</equation>
<bodyText confidence="0.9995515">
which provides the tightest upper bound of ϕ(z∗).
Since ψ is not differentiable w.r.t p, projected
sub-gradient (Calamai and Mor´e, 1987; Rush et al.,
2010) is used to search the saddle point. More
specifically, in each iteration, we first fix p and
search z using DP, then we fix z and update p by
</bodyText>
<equation confidence="0.9962075">
pnew = P∆ �
p + ∂ ∂pψ α
</equation>
<bodyText confidence="0.9990285">
where α &gt; 0 is the step size in line search, function
P∆(q) denotes the projection of q onto the proba-
bility simplex ∆. In this paper, we use Euclidean
projection, that is
</bodyText>
<equation confidence="0.79262975">
P∆(q) = min 11p − q112
p∈∆
which can be solved efficiently by sorting (Duchi et
al., 2008).
</equation>
<subsectionHeader confidence="0.99763">
3.3 Branch and Bound Based Parsing
</subsectionHeader>
<bodyText confidence="0.9983105">
As discussed in Section 3.1, the B&amp;B recursive pro-
cedure yields a binary tree structure called Branch
and Bound tree. Each node of the B&amp;B tree has
some fixed ze, specifying some must-select edges
and must-remove edges. The root of the B&amp;B tree
has no constraints, so it can produce all possible
parse trees including z∗. Each node has two chil-
dren. One adds a constraint ze = 1 for a free edge
</bodyText>
<figureCaption confidence="0.918151">
Figure 1: A part of B&amp;B tree. ϕ, ψ are short for
</figureCaption>
<bodyText confidence="0.947629038461539">
ϕ(z′) and ψ(p′, z′) respectively. For each node,
some edges of the parse tree are fixed. All parse
trees that satisfy the fixed edges compose the subset
of S C_ Z. A min-max problem is solved to get the
upper bound and lower bound of the optimal parse
tree over S. Once the upper bound ψ is less than
LB, the node is removed safely.
e and the other fixes ze = 0. We can explore the
search space {z|ze E {0,1}} by traversing the B&amp;B
tree in breadth first order.
Let S C_ Z be subspace of parse trees satisfying
the constraint, i.e., in the branch of the node. For
each node in B&amp;B tree, we solve
to get the upper bound and lower bound of the best
parse tree in S. A global lower bound LB is main-
tained which is the maximum of all obtained lower
bounds. If the upper bound of the current node is
lower than the global lower bound, the node can be
pruned from the B&amp;B tree safely. An example is
shown in Figure 1.
When the upper bound is not tight: ψ &gt; LB, we
need to choose a good branching variable to gener-
ate the child nodes. Let G(z′) = ψ(p′, z′) − ϕ(z′)
denote the gap between the upper bound and lower
bound. This gap is actually the accumulated gaps of
all factors c. Let Gc be the gap of c
</bodyText>
<equation confidence="0.993120875">
Gc =
rgc(p′c, z′) − ϕczc′ if ϕc ? 0
&lt; min
p
z∈maSx ψ (p, z)
p′, z′ = arg min
p
hc(p′c, z′) − ϕcz′c if ϕc &lt; 0
</equation>
<page confidence="0.988512">
41
</page>
<bodyText confidence="0.999361666666667">
We choose the branching variable heuristically:
for each edge e, we define its gap as the sum of the
gaps of factors that contain it
</bodyText>
<equation confidence="0.884314">
∑Ge = Gc
c,eEc
</equation>
<bodyText confidence="0.999625285714286">
The edge with the maximum gap is selected as the
branching variable.
Suppose there are N nodes on a level of B&amp;B
tree, and correspondingly, we get N branching vari-
ables, among which, we choose the one with the
highest lower bound as it likely reaches the optimal
value faster.
</bodyText>
<subsectionHeader confidence="0.955259">
3.4 Lower Bound Initialization
</subsectionHeader>
<bodyText confidence="0.9455894">
A large lower bound is critical for efficient pruning.
In this section, we discuss an alternative way to ini-
tialize the lower bound LB. We apply the similar
trick to get the lower bound function of ϕ(z).
Similar to Eq(8), for ϕc ≥ 0, we have
</bodyText>
<equation confidence="0.981209733333333">
(∑ )
ϕczc = max{ϕc ze − (rc − 1) , 0}
eEc
= max{σc(z), 0}
Using the fact that
∑max{aj} = max
pEO
j
we have
For ϕc &lt; 0, we have
ϕczc = max{ϕcze}
eEc ∑= max
pCEO eEc
= max gc(pc, z)
pC
</equation>
<bodyText confidence="0.8100485">
Put the two cases together, we get the lower bound
function
</bodyText>
<equation confidence="0.918806142857143">
π(p, z) = ∑ hc(pc, z) + ∑ gc(pc, z)
c,OC&gt;_0 c,OC&lt;0
Algorithm 1 Branch and Bound based parsing
Require: {ϕc}
Ensure: Optimal parse tree z*
Solve p*, z* = arg maxp,zπ(p, z)
Initialize S = {i}, LB = π(p*, z*)
while S ≠ 0 do
Set S′ = 0{nodes that survive from pruning}
foreach S E S
Solve minp maxz ψ(p, z) to get LBS, UBS
LB = max{LB, LBSES}, update z*
foreach S E S, add S to S′, if UBS &gt; LB
Select a branching variable ze.
Clear S = 0
foreach S E S′
Add S1 = {z|z E S,ze = 1} to S
Add S2 = {z|z E S, ze = 0} to S.
end while
For any p, pc E ∆, z E i
π(p, z) ≤ ϕ(z)
</equation>
<bodyText confidence="0.867315">
π(p, z) is not concave, however, we could alterna-
tively optimize z and p to get a good approximation,
which provides a lower bound for ϕ(z*).
</bodyText>
<subsectionHeader confidence="0.74454">
3.5 Summary
</subsectionHeader>
<bodyText confidence="0.999993583333333">
We summarize our B&amp;B algorithm in Algorithm 1.
It is worth pointing out that so far in the above
description, we have used the assumption that the
backbone DP uses first order models, however, the
backbone DP can be the second or third order ver-
sion. The difference is that, for higher order DP,
higher order factors such as adjacent siblings, grand-
children are directly handled as local factors.
In the worst case, all the edges are selected for
branching, and the complexity grows exponentially
in sentence length. However, in practice, it is quite
efficient, as we will show in the next section.
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997189">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.998404714285714">
The datasets we used are the English Penn Tree
Bank (PTB) and Chinese Tree Bank 5.0 (CTB5). We
use the standard train/develop/test split as described
in Table 1.
We extracted dependencies using Joakim Nivre’s
Penn2Malt tool with standard head rules: Yamada
and Matsumoto’s (Yamada and Matsumoto, 2003)
</bodyText>
<equation confidence="0.9176645">
pjaj
ϕczc = max p1cσc(z) + p&apos; · 0
pCEO
= max hc(pc, z)
pC
pecϕcze
</equation>
<page confidence="0.995223">
42
</page>
<table confidence="0.99941475">
Train Develop Test
PTB sec. 2-21 sec. 22 sec. 23
CTB5 sec. 001-815 sec. 886-931 sec. 816-885
1001-1136 1148-1151 1137-1147
</table>
<tableCaption confidence="0.999869">
Table 1: Data split in our experiment
</tableCaption>
<bodyText confidence="0.996777">
for English, and Zhang and Clark’s (Zhang and
Clark, 2008) for Chinese. Unlabeled attachment s-
core (UAS) is used to evaluate parsing quality1. The
B&amp;B parser is implemented with C++. All the ex-
periments are conducted on the platform Intel Core
i5-2500 CPU 3.30GHz.
</bodyText>
<subsectionHeader confidence="0.972304">
4.2 Baseline: DP Based Second Order Parser
</subsectionHeader>
<bodyText confidence="0.9999659">
We use the dynamic programming based second or-
der parser (Carreras, 2007) as the baseline. Aver-
aged structured perceptron (Collins, 2002) is used
for parameter estimation. We determine the number
of iterations on the validation set, which is 6 for both
corpora.
For English, we train the POS tagger using linear
chain perceptron on training set, and predict POS
tags for the development and test data. The parser is
trained using the automatic POS tags generated by
10 fold cross validation. For Chinese, we use the
gold standard POS tags.
We use five types of features: unigram features,
bigram features, in-between features, adjacent sib-
ling features and outermost grand-child features.
The first three types of features are firstly introduced
by McDonald et al. (2005a) and the last two type-
s of features are used by Carreras (2007). All the
features are the concatenation of surrounding words,
lower cased words (English only), word length (Chi-
nese only), prefixes and suffixes of words (Chinese
only), POS tags, coarse POS tags which are derived
from POS tags using a simple mapping table, dis-
tance between head and modifier, direction of edges.
For English, we used 674 feature templates to gener-
ate large amounts of features, and finally got 86.7M
non-zero weighted features after training. The base-
line parser got 92.81% UAS on the testing set. For
Chinese, we used 858 feature templates, and finally
got 71.5M non-zero weighted features after train-
</bodyText>
<footnote confidence="0.903296">
1For English, we follow Koo and Collins (2010) and ignore
any word whose gold-standard POS tag is one of { “ ” : , .�. For
Chinese, we ignore any word whose POS tag is PU.
</footnote>
<bodyText confidence="0.9922485">
ing. The baseline parser got 86.89% UAS on the
testing set.
</bodyText>
<subsectionHeader confidence="0.997145">
4.3 B&amp;B Based Parser with Non-local Features
</subsectionHeader>
<bodyText confidence="0.999392">
We use the baseline parser as the backbone of our
B&amp;B parser. We tried different types of non-local
features as listed below:
</bodyText>
<listItem confidence="0.908114769230769">
• All grand-child features. Notice that this fea-
ture can be handled by Koo’s second order
model (Koo and Collins, 2010) directly.
• All great grand-child features.
• All sibling features: all the pairs of edges with
common head. An example is shown in Fig-
ure 2.
• All tri-sibling features: all the 3-tuples of edges
with common head.
• Comb features: for any word with more than 3
consecutive modifiers, the set of all the edges
from the word to the modifiers form a comb.2
• Hand crafted features: We perform cross val-
</listItem>
<bodyText confidence="0.978411875">
idation on the training data using the baseline
parser, and designed features that may correc-
t the most common errors. We designed 13
hand-craft features for English in total. One ex-
ample is shown in Figure 3. For Chinese, we
did not add any hand-craft features, as the er-
rors in the cross validation result vary a lot, and
we did not find general patterns to fix them.
</bodyText>
<subsectionHeader confidence="0.997231">
4.4 Implementation Details
</subsectionHeader>
<bodyText confidence="0.984046714285714">
To speed up the solution of the min-max subprob-
lem, for each node in the B&amp;B tree, we initialize p
with the optimal solution of its parent node, since
the child node fixes only one additional edge, its op-
timal point is likely to be closed to its parent’s. For
the root node of B&amp;B tree, we initialize p&apos; = for
factors with non-negative weights and pl = 0 for
2In fact, our algorithm can deal with non-consecutive mod-
ifiers; however, in such cases, factor detection (detect regular
expressions like x1. * x2. * ... ) requires the longest com-
mon subsequence algorithm (LCS), which is time-consuming
if many comb features are generated. Similar problems arise
for sub-tree features, which may contain many non-consecutive
words.
</bodyText>
<page confidence="0.999859">
43
</page>
<figureCaption confidence="0.918452888888889">
Figure 2: An example of all sibling features. Top:
a sub-tree; Bottom: extracted sibling features. Ex-
isting higher order DP systems can not handle the
siblings on both sides of head.
regulation occurs through inaction , rather than through ...
Figure 3: An example of hand-craft feature: for the
word sequence A ... rather than A, where A is a
preposition, the first A is the head of than, than is
the head of rather and the second A.
</figureCaption>
<table confidence="0.999898545454546">
System PTB CTB
Our baseline 92.81 86.89
B&amp;B +all grand-child 92.97 87.02
+all great grand-child 92.78 86.77
+all sibling 93.00 87.05
+all tri-sibling 92.79 86.81
+comb 92.86 86.91
+hand craft 92.89 N/A
+all grand-child + all sibling + com- 93.17 87.25
b + hand craft
3rd order re-impl. 93.03 87.07
TurboParser (reported) 92.62 N/A
TurboParser (our run) 92.82 86.05
Koo and Collins (2010) 93.04 N/A
Zhang and McDonald (2012) 93.06 86.87
Zhang and Nivre (2011) 92.90 86.00
System integration
Bohnet and Kuhn (2012) 93.39 87.5
Systems using additional resources
Suzuki et al. (2009) 93.79 N/A
Koo et al. (2008) 93.5 N/A
Chen et al. (2012) 92.76 N/A
</table>
<tableCaption confidence="0.972148">
Table 2: Comparison between our system and the-
state-of-art systems.
</tableCaption>
<figure confidence="0.661570666666667">
eco re
c c c
c
</figure>
<bodyText confidence="0.983341">
negative weighted factors. Step size α is initialized
with mazc,ϕ6̸=0{ |. }, as the vector p is bounded in
ated using the same strategy as
a unit box. α is ui�
Rush et al. (2010). Two stopping criteria are used.
One is 0 ≤ ψold − ψnew ≤ c, where c &gt; 0 is a given
precision3. The other checks if the bound is tight:
UB = LB. Because all features are boolean (note
that they can be integer), their weights are integer
during each perceptron update, hence the scores of
parse trees are discrete. The minimal gap between
different scores is 1
N×T after averaging, where N is
the number of training samples, and T is the itera-
tion number for perceptron training. Therefore the
upper bound can be tightened as UB = ⌊NT ψ⌋
NT.
During testing, we use the pre-pruning method as
used in Martins et al. (2009) for both datasets to bal-
ance parsing quality and speed. This method uses a
simple classifier to select the top k candidate head-
s for each word and exclude the other heads from
search space. In our experiment, we set k = 10.
3we use e = 10−8 in our implementation
</bodyText>
<subsectionHeader confidence="0.997447">
4.5 Main Result
</subsectionHeader>
<bodyText confidence="0.999947666666667">
Experimental results are listed in Table 2. For com-
parison, we also include results of representative
state-of-the-art systems. For the third order pars-
er, we re-implemented Model 1 (Koo and Collins,
2010), and removed the longest sentence in the CTB
dataset, which contains 240 words, due to the O(n4)
space complexity 4. For ILP based parsing, we used
TurboParser5, a speed-optimized parser toolkit. We
trained full models (which use all grandchild fea-
tures, all sibling features and head bigram features
(Martins et al., 2011)) for both datasets using its de-
fault settings. We also list the performance in its
documentation on English corpus.
The observation is that, the all-sibling features are
most helpful for our parser, as some good sibling
features can not be encoded in DP based parser. For
example, a matched pair of parentheses are always
siblings, but their head may lie between them. An-
</bodyText>
<footnote confidence="0.99773175">
4In fact, Koo’s algorithm requires only O(n3) space. Our
implementation is O(n4) because we store the feature vectors
for fast training.
5http://www.ark.cs.cmu.edu/TurboParser/
</footnote>
<page confidence="0.999275">
44
</page>
<bodyText confidence="0.999932809523809">
other observation is that all great grandchild features
and all tri-sibling features slightly hurt the perfor-
mance and we excluded them from the final system.
When no additional resource is available, our
parser achieved competitive performance: 93.17%
Unlabeled Attachment Score (UAS) for English at
a speed of 177 words per second and 87.25% for
Chinese at a speed of 97 words per second. High-
er UAS is reported by joint tagging and parsing
(Bohnet and Nivre, 2012) or system integration
(Bohnet and Kuhn, 2012) which benefits from both
transition based parsing and graph based parsing.
Previous work shows that combination of the two
parsing techniques can learn to overcome the short-
comings of each non-integrated system (Nivre and
McDonald, 2008; Zhang and Clark, 2008). Sys-
tem combination will be an interesting topic for our
future research. The highest reported performance
on English corpus is 93.79%, obtained by semi-
supervised learning with a large amount of unla-
beled data (Suzuki et al., 2009).
</bodyText>
<subsectionHeader confidence="0.990554">
4.6 Tradeoff Between Accuracy and Speed
</subsectionHeader>
<bodyText confidence="0.99989716">
In this section, we study the trade off between ac-
curacy and speed using different pre-pruning setups.
In Table 3, we show the parsing accuracy and in-
ference time in testing stage with different numbers
of candidate heads k in pruning step. We can see
that, on English dataset, when k ≥ 10, our pars-
er could gain 2 − 3 times speedup without losing
much parsing accuracy. There is a further increase
of the speed with smaller k, at the cost of some ac-
curacy. Compared with TurboParser, our parser is
less efficient but more accurate. Zhang and McDon-
ald (2012) is a state-of-the-art system which adopts
cube pruning for efficient parsing. Notice that, they
did not use pruning which seems to increase parsing
speed with little hit in accuracy. Moreover, they did
labeled parsing, which also makes their speed not
directly comparable.
For each node of B&amp;B tree, our parsing algorithm
uses projected sub-gradient method to find the sad-
dle point, which requires a number of calls to a DP,
hence the efficiency of Algorithm 1 is mainly deter-
mined by the number of DP calls. Figure 4 and Fig-
ure 5 show the averaged parsing time and number of
calls to DP relative to the sentence length with differ-
ent pruning settings. Parsing time grows smoothly
</bodyText>
<table confidence="0.999599666666667">
System PTB CTB
UAS w/s UAS w/s
Ours (no prune) 93.18 52 87.28 73
Ours (k = 20) 93.17 105 87.28 76
Ours (k = 10) 93.17 177 87.25 97
Ours (k = 5) 93.10 264 86.94 108
Ours (k = 3) 92.68 493 85.76 128
TurboParser(full) 92.82 402 86.05 192
TurboParser(standard) 92.68 638 85.80 283
TurboParser(basic) 90.97 4670 82.28 2736
Zhang and McDon- 93.06 220 86.87 N/A
ald (2012)†
</table>
<tableCaption confidence="0.99703">
Table 3: Trade off between parsing accuracy (UAS)
</tableCaption>
<bodyText confidence="0.9922508">
and speed (words per second) with different pre-
pruning settings. k denotes the number of candi-
date heads of each word preserved for B&amp;B parsing.
†Their speed is not directly comparable as they per-
forms labeled parsing without pruning.
when sentence length G 40. There is some fluctua-
tion for the long sentences. This is because there are
very few sentences for a specific long length (usual-
ly 1 or 2 sentences), and the statistics are not stable
or meaningful for the small samples.
Without pruning, there are in total 132,161 calls
to parse 2, 416 English sentences, that is, each sen-
tence requires 54.7 calls on average. For Chinese,
there are 84, 645 calls for 1, 910 sentences, i.e., 44.3
calls for each sentence on average.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="method">
5 Discussion
</sectionHeader>
<subsectionHeader confidence="0.996791">
5.1 Polynomial Non-local Factors
</subsectionHeader>
<bodyText confidence="0.99995">
Our bounding strategy can handle a family of non-
local factors that can be expressed as a polynomial
function of local factors. To see this, suppose
</bodyText>
<equation confidence="0.8739678">
�zc = �αi ze
i eEEi
For each i, we introduce new variable zEi =
mineEEi ze. Because ze is binary, zEi = Ha
eEEi ze.
</equation>
<bodyText confidence="0.999388">
In this way, we replace zc by several zEi that can be
handled by our bounding strategy.
We give two examples of these polynomial non-
local factors. First is the OR of local factors: zc =
max{ze, z′e}, which can be expressed by zc = ze +
z′e−zez′e. The second is the factor of valency feature
</bodyText>
<page confidence="0.992972">
45
</page>
<figure confidence="0.998648541666667">
parsing time (sec.)
parsing time (sec.)
10
5
00 10 20 30 40 50 60
sentence length
(a) PTB corpus
k=3
k=5
k=10
k=20
no prune
20
00 20 40 60 80 100 120 140
sentence length
(b) CTB corpus
80
60
40
k=3
k=5
k=10
k=20
no prune
</figure>
<figureCaption confidence="0.9993925">
Figure 4 Averaged parsing time (seconds) relative to sentence length with different pruning settings, k
denotes the number of candidate heads of each word in pruning step.
</figureCaption>
<figure confidence="0.9998257">
1000
Calls to DP
500
k=3
k=5
k=10
k=20
no prune
00 20 40 60 80 100 120 140
200
100
00 10 20 30 40 50 60
Calls to DP
k=3
k=5
k=10
k=20
no prune
sentence length sentence length
(a) PTB corpus (b) CTB corpus
</figure>
<figureCaption confidence="0.9976295">
Figure 5 Averaged number of Calls to DP relative to sentence length with different pruning settings, k
denotes the number of candidate heads of each word in pruning step.
</figureCaption>
<bodyText confidence="0.937508863636364">
(Martins et al., 2009). Let binary variable vik indi-
cate whether word i has k modifiers. Given {ze} for
the edges with head i, then {vik|k = 1, ... , n − 1}
can be solved by
The left side of the equation is the li
near function of
vik. The right side of the equation is a polynomial
function of ze. Hence, vik could be expressed as a
polynomial function of ze.
mation from multiple candidates to optimize task-
specific performance. We have not conducted any
experiment for k best parsing, hence we only dis-
cuss the algorithm.
According to proposition 1, we have
Proposition 2. Given p and subset S C_ i, let zk
denote the kth best solution of
z). If a
parse tree
E S satisfies
Proof. Since zk is the kth best solution of
z),
for
</bodyText>
<figure confidence="0.867114322580645">
j &gt; k, we have
zk) &gt;_
zj)
Since the size of the set
&gt; k} is
k, hence there are at least
k parse trees
whose scores
are less than
zk). Because
&gt;_
ψ(p,
zj,
ψ(p,
ψ(p,
&gt;_ϕ(zj).
{zj|j
|S|−
|S|−
ϕ(zj)
ψ(p,
ϕ(z′)
ψ(p, zk), hence z′ is at least the kth best
parse tree in subset S.
(�
kjvik = ze
e
�
k
)j
0 G j G n − 1
</figure>
<bodyText confidence="0.996764961538462">
d especially utilize infor- 5.2 k Best Parsing
Though our B&amp;B algorithm is able to capture a va-
riety of non-local features, it is still difficult to han-
dle many kinds of features, such as the depth of the
parse tree. Hence, a reranking approach may be use-
ful in order to incorporate such information, where
k parse trees can be generated first and then a second
pass model is used to rerank these candidates based
on more global or non-local features. In addition,
k-best parsing may be needed in many applications
to use parse information an
46
maxz∈S ψ(p,
z′
ϕ(z′)&gt;_ψ(p, zk), then z′
is one of the k best parse trees in subset S.
Therefore, we can search the k best parse trees
in this way: for each sub-problem, we use DP to
derive the k best parse trees. For each parse tree
if
&gt;_
z,
ϕ(z)
ψ(p, zk), then z is selected into the k
best set. Algorithm terminates until the kth bound is
tight.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999725">
In this paper we proposed a new parsing algorithm
based on a Branch and Bound framework. The mo-
tivation is to use dynamic programming to search
for the bound. Experimental results on PTB and
CTB5 datasets show that our method is competitive
in terms of both performance and efficiency. Our
method can be adapted to non-projective dependen-
cy parsing, as well as the k best MST algorithm
(Hall, 2007) to find the k best candidates.
</bodyText>
<sectionHeader confidence="0.998168" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999757">
We’d like to thank Hao Zhang, Andre Martins and
Zhenghua Li for their helpful discussions. We al-
so thank Ryan McDonald and three anonymous re-
viewers for their valuable comments. This work
is partly supported by DARPA under Contract No.
HR0011-12-C-0016 and FA8750-13-2-0041. Any
opinions expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA.
</bodyText>
<sectionHeader confidence="0.998594" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999534102564102">
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated CCG supertagging and parsing. In Proc. of
ACL-HLT.
Egon Balas. 1965. An additive algorithm for solving
linear programs with zero-one variables. Operations
Research, 39(4).
Bernd Bohnet and Jonas Kuhn. 2012. The best of
bothworlds – a graph-based completion model for
transition-based parsers. In Proc. of EACL.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proc. of
EMNLP-CoNLL.
Paul Calamai and Jorge Mor´e. 1987. Projected gradien-
t methods for linearly constrained problems. Mathe-
matical Programming, 39(1).
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of EMNLP-
CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012. U-
tilizing dependency language models for graph-based
dependency parsing models. In Proc. of ACL.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
l1-ball for learning in high dimensions. In Proc. of
ICML.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: an exploration. In Proc. of
COLING.
Keith Hall. 2007. K-best spanning tree parsing. In Proc.
of ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ofACL-HLT.
Manfred Klenner and ´Etienne Ailloud. 2009. Opti-
mization in coreference resolution is not needed: A
nearly-optimal algorithm with intensional constraints.
In Proc. of EACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL-HLT.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proc. of EMNLP.
Ailsa H. Land and Alison G. Doig. 1960. An automat-
ic method of solving discrete programming problems.
Econometrica, 28(3):497–520.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proc. of ACL.
Andre Martins, Noah Smith, Mario Figueiredo, and Pe-
dro Aguiar. 2011. Dual decomposition with many
overlapping components. In Proc. of EMNLP.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proc. of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proc. of ACL-HLT.
Jorge Nocedal and Stephen J. Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
</reference>
<page confidence="0.986694">
47
</page>
<reference confidence="0.999768793103448">
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proc. of EMNLP.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proc. of EMNLP.
David Smith and Jason Eisner. 2008. Dependency pars-
ing by belief propagation. In Proc. of EMNLP.
Min Sun, Murali Telaprolu, Honglak Lee, and Silvio
Savarese. 2012. Efficient and exact MAP-MRF in-
ference using branch and bound. In Proc. of AISTATS.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proc. of EMNLP.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proc. of IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of t-
wo parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Proc. of
EMNLP.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proc. of EMNLP.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proc. of ACL-HLT.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839989">
<title confidence="0.9995825">Branch and Bound Algorithm for Dependency with Non-local Features</title>
<author confidence="0.990729">Qian</author>
<affiliation confidence="0.980747">Computer Science The University of Texas at</affiliation>
<abstract confidence="0.9935892">Graph based dependency parsing is inefficient when handling non-local features due to high computational complexity of inference. In this paper, we proposed an exact and efficient decoding algorithm based on the Branch and Bound (B&amp;B) framework where nonlocal features are bounded by a linear combination of local features. Dynamic programming is used to search the upper bound. Experiments are conducted on English PTB and Chinese CTB datasets. We achieved competitive Unlabeled Attachment Score (UAS) when additional resources are available: English and Chinese. Parsing is per second for English and per second for Chinese. Our algorithm is general and can be adapted to nonprojective dependency parsing or other graphical models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="2889" citStr="Auli and Lopez, 2011" startWordPosition="452" endWordPosition="455">ions. This approach is very efficient and flexible to handle various nonlocal features. The disadvantage is that it tends to compute non-local features as early as possible so that the decoder can utilize that information at internal spans, hence it may miss long historical features such as long dependency chains. Smith and Eisner modeled dependency parsing using Markov Random Fields (MRFs) with global constraints and applied loopy belief propagation (LBP) for approximate learning and inference (Smith and Eisner, 2008). Similar work was done for Combinatorial Categorial Grammar (CCG) parsing (Auli and Lopez, 2011). They used posterior marginal beliefs for inference to satisfy the tree constraint: for each factor, only legal messages (satisfying global constraints) are considered in the partition function. A similar line of research investigated the use of integer linear programming (ILP) based parsing (Riedel and Clarke, 2006; Martins et al., 2009). This 37 Transactions of the Association for Computational Linguistics, 1 (2013) 37–48. Action Editor: Ryan McDonald. Submitted 11/2012; Revised 2/2013; Published 3/2013. c�2013 Association for Computational Linguistics. method is very expressive. It can han</context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011. A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing. In Proc. of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egon Balas</author>
</authors>
<title>An additive algorithm for solving linear programs with zero-one variables.</title>
<date>1965</date>
<journal>Operations Research,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="9535" citStr="Balas, 1965" startWordPosition="1609" endWordPosition="1610">ounding step calculates the upper bound of the optimal parse tree score in the subspace: UBYi ? maxyEYi ϕ(x, y). If this bound is no more than any obtained parse tree score UBYi G ϕ(x, y′), then all parse trees in subspace Yi are no more optimal than y′, and Yi could be pruned safely. The efficiency of B&amp;B depends on the branching strategy and upper bound computation. For example, Sun et al. (2012) used B&amp;B for MRFs, where they proposed two branching strategies and a novel data structure for efficient upper bound computation. Klenner and Ailloud (2009) proposed a variation of Balas algorithm (Balas, 1965) for coreference resolution, where candidate branching variables are sorted by their weights. Our bounding strategy is to find an upper bound for the score of each non-local factor c containing multiple edges. The bound is the sum of new scores of edges in the factor plus a constant ϕc(x) G � ψe(x) + αc eEc Based on the new scores {ψe(x)} and constants {αc}, we define the new score of parse tree y (x, y) = Y:(eEc I: ψe(x) + αc cCy Then we have ψ(x, y) ? ϕ(x, y), by E Y(x) The advantage of such a bound is that, it is the sum of new edge scores. Hence, its optimum tree maxyEY(x) ψ(x, y) can be f</context>
</contexts>
<marker>Balas, 1965</marker>
<rawString>Egon Balas. 1965. An additive algorithm for solving linear programs with zero-one variables. Operations Research, 39(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Jonas Kuhn</author>
</authors>
<title>The best of bothworlds – a graph-based completion model for transition-based parsers.</title>
<date>2012</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="23314" citStr="Bohnet and Kuhn (2012)" startWordPosition="4268" endWordPosition="4271">an A, where A is a preposition, the first A is the head of than, than is the head of rather and the second A. System PTB CTB Our baseline 92.81 86.89 B&amp;B +all grand-child 92.97 87.02 +all great grand-child 92.78 86.77 +all sibling 93.00 87.05 +all tri-sibling 92.79 86.81 +comb 92.86 86.91 +hand craft 92.89 N/A +all grand-child + all sibling + com- 93.17 87.25 b + hand craft 3rd order re-impl. 93.03 87.07 TurboParser (reported) 92.62 N/A TurboParser (our run) 92.82 86.05 Koo and Collins (2010) 93.04 N/A Zhang and McDonald (2012) 93.06 86.87 Zhang and Nivre (2011) 92.90 86.00 System integration Bohnet and Kuhn (2012) 93.39 87.5 Systems using additional resources Suzuki et al. (2009) 93.79 N/A Koo et al. (2008) 93.5 N/A Chen et al. (2012) 92.76 N/A Table 2: Comparison between our system and thestate-of-art systems. eco re c c c c negative weighted factors. Step size α is initialized with mazc,ϕ6̸=0{ |. }, as the vector p is bounded in ated using the same strategy as a unit box. α is ui� Rush et al. (2010). Two stopping criteria are used. One is 0 ≤ ψold − ψnew ≤ c, where c &gt; 0 is a given precision3. The other checks if the bound is tight: UB = LB. Because all features are boolean (note that they can be int</context>
<context position="26203" citStr="Bohnet and Kuhn, 2012" startWordPosition="4767" endWordPosition="4770"> is O(n4) because we store the feature vectors for fast training. 5http://www.ark.cs.cmu.edu/TurboParser/ 44 other observation is that all great grandchild features and all tri-sibling features slightly hurt the performance and we excluded them from the final system. When no additional resource is available, our parser achieved competitive performance: 93.17% Unlabeled Attachment Score (UAS) for English at a speed of 177 words per second and 87.25% for Chinese at a speed of 97 words per second. Higher UAS is reported by joint tagging and parsing (Bohnet and Nivre, 2012) or system integration (Bohnet and Kuhn, 2012) which benefits from both transition based parsing and graph based parsing. Previous work shows that combination of the two parsing techniques can learn to overcome the shortcomings of each non-integrated system (Nivre and McDonald, 2008; Zhang and Clark, 2008). System combination will be an interesting topic for our future research. The highest reported performance on English corpus is 93.79%, obtained by semisupervised learning with a large amount of unlabeled data (Suzuki et al., 2009). 4.6 Tradeoff Between Accuracy and Speed In this section, we study the trade off between accuracy and spee</context>
</contexts>
<marker>Bohnet, Kuhn, 2012</marker>
<rawString>Bernd Bohnet and Jonas Kuhn. 2012. The best of bothworlds – a graph-based completion model for transition-based parsers. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="26157" citStr="Bohnet and Nivre, 2012" startWordPosition="4760" endWordPosition="4763">m requires only O(n3) space. Our implementation is O(n4) because we store the feature vectors for fast training. 5http://www.ark.cs.cmu.edu/TurboParser/ 44 other observation is that all great grandchild features and all tri-sibling features slightly hurt the performance and we excluded them from the final system. When no additional resource is available, our parser achieved competitive performance: 93.17% Unlabeled Attachment Score (UAS) for English at a speed of 177 words per second and 87.25% for Chinese at a speed of 97 words per second. Higher UAS is reported by joint tagging and parsing (Bohnet and Nivre, 2012) or system integration (Bohnet and Kuhn, 2012) which benefits from both transition based parsing and graph based parsing. Previous work shows that combination of the two parsing techniques can learn to overcome the shortcomings of each non-integrated system (Nivre and McDonald, 2008; Zhang and Clark, 2008). System combination will be an interesting topic for our future research. The highest reported performance on English corpus is 93.79%, obtained by semisupervised learning with a large amount of unlabeled data (Suzuki et al., 2009). 4.6 Tradeoff Between Accuracy and Speed In this section, we</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Calamai</author>
<author>Jorge Mor´e</author>
</authors>
<title>Projected gradient methods for linearly constrained problems.</title>
<date>1987</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>39</volume>
<issue>1</issue>
<marker>Calamai, Mor´e, 1987</marker>
<rawString>Paul Calamai and Jorge Mor´e. 1987. Projected gradient methods for linearly constrained problems. Mathematical Programming, 39(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL.</booktitle>
<contexts>
<context position="1341" citStr="Carreras, 2007" startWordPosition="207" endWordPosition="208"> are available: 93.17% for English and 87.25% for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to nonprojective dependency parsing or other graphical models. 1 Introduction For graph based projective dependency parsing, dynamic programming (DP) is popular for decoding due to its efficiency when handling local features. It performs cubic time parsing for arc-factored models (Eisner, 1996; McDonald et al., 2005a) and biquadratic time for higher order models with richer sibling and grandchild features (Carreras, 2007; Koo and Collins, 2010). However, for models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features. Since the reranking quality is bounded by the oracle performance of candidates, some work has combined candidate generation and </context>
<context position="7995" citStr="Carreras (2007)" startWordPosition="1347" endWordPosition="1348">erived by DP with running time O(n3) (Eisner, 1996). The algorithm has two types of structures: complete span, which consists of a headword and its descendants on one side, and incomplete span, which consists of a dependency and the region between the head and modifier. It starts at single word spans, and merges the spans in bottom up order. For second order models, the score function ϕ(x, y) adds the scores of siblings (adjacent edges with a common head) and grandchildren ϕ(x, y) = � ϕehm(x) ehmEy � + egh,ehmEy � + ehm,ehsEy There are two versions of second order models, used respectively by Carreras (2007) and Koo et al. (2010). The difference is that Carreras’ only considers the outermost grandchildren, while Koo and Collin’s allows all grandchild features. Both models permit O(n4) running time. Third-order models score edge triples such as three adjacent sibling modifiers, or grand-siblings that score a word, its modifier and its adjacent grandchildren, and the inference complexity is O(n4) (Koo and Collins, 2010). In this paper, for all the factors/features that can be handled by DP, we call them the local factors/features. 3 The Proposed Method 3.1 Basic Idea For general high order models w</context>
<context position="18935" citStr="Carreras, 2007" startWordPosition="3515" endWordPosition="3516">03) pjaj ϕczc = max p1cσc(z) + p&apos; · 0 pCEO = max hc(pc, z) pC pecϕcze 42 Train Develop Test PTB sec. 2-21 sec. 22 sec. 23 CTB5 sec. 001-815 sec. 886-931 sec. 816-885 1001-1136 1148-1151 1137-1147 Table 1: Data split in our experiment for English, and Zhang and Clark’s (Zhang and Clark, 2008) for Chinese. Unlabeled attachment score (UAS) is used to evaluate parsing quality1. The B&amp;B parser is implemented with C++. All the experiments are conducted on the platform Intel Core i5-2500 CPU 3.30GHz. 4.2 Baseline: DP Based Second Order Parser We use the dynamic programming based second order parser (Carreras, 2007) as the baseline. Averaged structured perceptron (Collins, 2002) is used for parameter estimation. We determine the number of iterations on the validation set, which is 6 for both corpora. For English, we train the POS tagger using linear chain perceptron on training set, and predict POS tags for the development and test data. The parser is trained using the automatic POS tags generated by 10 fold cross validation. For Chinese, we use the gold standard POS tags. We use five types of features: unigram features, bigram features, in-between features, adjacent sibling features and outermost grand-</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proc. of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1613" citStr="Charniak and Johnson, 2005" startWordPosition="246" endWordPosition="249">duction For graph based projective dependency parsing, dynamic programming (DP) is popular for decoding due to its efficiency when handling local features. It performs cubic time parsing for arc-factored models (Eisner, 1996; McDonald et al., 2005a) and biquadratic time for higher order models with richer sibling and grandchild features (Carreras, 2007; Koo and Collins, 2010). However, for models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features. Since the reranking quality is bounded by the oracle performance of candidates, some work has combined candidate generation and reranking steps using cube pruning (Huang, 2008; Zhang and McDonald, 2012) to achieve higher oracle performance. They parse a sentence in bottom up order and keep the top k derivations for each span using k best parsing (Huang and Chiang, 2005). After merging the two span</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Utilizing dependency language models for graph-based dependency parsing models.</title>
<date>2012</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="23437" citStr="Chen et al. (2012)" startWordPosition="4290" endWordPosition="4293">r baseline 92.81 86.89 B&amp;B +all grand-child 92.97 87.02 +all great grand-child 92.78 86.77 +all sibling 93.00 87.05 +all tri-sibling 92.79 86.81 +comb 92.86 86.91 +hand craft 92.89 N/A +all grand-child + all sibling + com- 93.17 87.25 b + hand craft 3rd order re-impl. 93.03 87.07 TurboParser (reported) 92.62 N/A TurboParser (our run) 92.82 86.05 Koo and Collins (2010) 93.04 N/A Zhang and McDonald (2012) 93.06 86.87 Zhang and Nivre (2011) 92.90 86.00 System integration Bohnet and Kuhn (2012) 93.39 87.5 Systems using additional resources Suzuki et al. (2009) 93.79 N/A Koo et al. (2008) 93.5 N/A Chen et al. (2012) 92.76 N/A Table 2: Comparison between our system and thestate-of-art systems. eco re c c c c negative weighted factors. Step size α is initialized with mazc,ϕ6̸=0{ |. }, as the vector p is bounded in ated using the same strategy as a unit box. α is ui� Rush et al. (2010). Two stopping criteria are used. One is 0 ≤ ψold − ψnew ≤ c, where c &gt; 0 is a given precision3. The other checks if the bound is tight: UB = LB. Because all features are boolean (note that they can be integer), their weights are integer during each perceptron update, hence the scores of parse trees are discrete. The minimal g</context>
</contexts>
<marker>Chen, Zhang, Li, 2012</marker>
<rawString>Wenliang Chen, Min Zhang, and Haizhou Li. 2012. Utilizing dependency language models for graph-based dependency parsing models. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="1585" citStr="Collins, 2000" startWordPosition="244" endWordPosition="245">models. 1 Introduction For graph based projective dependency parsing, dynamic programming (DP) is popular for decoding due to its efficiency when handling local features. It performs cubic time parsing for arc-factored models (Eisner, 1996; McDonald et al., 2005a) and biquadratic time for higher order models with richer sibling and grandchild features (Carreras, 2007; Koo and Collins, 2010). However, for models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features. Since the reranking quality is bounded by the oracle performance of candidates, some work has combined candidate generation and reranking steps using cube pruning (Huang, 2008; Zhang and McDonald, 2012) to achieve higher oracle performance. They parse a sentence in bottom up order and keep the top k derivations for each span using k best parsing (Huang and Chiang, 2005)</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="18999" citStr="Collins, 2002" startWordPosition="3524" endWordPosition="3525">ϕcze 42 Train Develop Test PTB sec. 2-21 sec. 22 sec. 23 CTB5 sec. 001-815 sec. 886-931 sec. 816-885 1001-1136 1148-1151 1137-1147 Table 1: Data split in our experiment for English, and Zhang and Clark’s (Zhang and Clark, 2008) for Chinese. Unlabeled attachment score (UAS) is used to evaluate parsing quality1. The B&amp;B parser is implemented with C++. All the experiments are conducted on the platform Intel Core i5-2500 CPU 3.30GHz. 4.2 Baseline: DP Based Second Order Parser We use the dynamic programming based second order parser (Carreras, 2007) as the baseline. Averaged structured perceptron (Collins, 2002) is used for parameter estimation. We determine the number of iterations on the validation set, which is 6 for both corpora. For English, we train the POS tagger using linear chain perceptron on training set, and predict POS tags for the development and test data. The parser is trained using the automatic POS tags generated by 10 fold cross validation. For Chinese, we use the gold standard POS tags. We use five types of features: unigram features, bigram features, in-between features, adjacent sibling features and outermost grand-child features. The first three types of features are firstly in</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Tushar Chandra</author>
</authors>
<title>Efficient projections onto the l1-ball for learning in high dimensions.</title>
<date>2008</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="13935" citStr="Duchi et al., 2008" startWordPosition="2539" endWordPosition="2542">p, z) z∈Z z∈Z p Z z∈ max ψ (p, z) which provides the tightest upper bound of ϕ(z∗). Since ψ is not differentiable w.r.t p, projected sub-gradient (Calamai and Mor´e, 1987; Rush et al., 2010) is used to search the saddle point. More specifically, in each iteration, we first fix p and search z using DP, then we fix z and update p by pnew = P∆ � p + ∂ ∂pψ α where α &gt; 0 is the step size in line search, function P∆(q) denotes the projection of q onto the probability simplex ∆. In this paper, we use Euclidean projection, that is P∆(q) = min 11p − q112 p∈∆ which can be solved efficiently by sorting (Duchi et al., 2008). 3.3 Branch and Bound Based Parsing As discussed in Section 3.1, the B&amp;B recursive procedure yields a binary tree structure called Branch and Bound tree. Each node of the B&amp;B tree has some fixed ze, specifying some must-select edges and must-remove edges. The root of the B&amp;B tree has no constraints, so it can produce all possible parse trees including z∗. Each node has two children. One adds a constraint ze = 1 for a free edge Figure 1: A part of B&amp;B tree. ϕ, ψ are short for ϕ(z′) and ψ(p′, z′) respectively. For each node, some edges of the parse tree are fixed. All parse trees that satisfy t</context>
</contexts>
<marker>Duchi, Shalev-Shwartz, Singer, Chandra, 2008</marker>
<rawString>John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. 2008. Efficient projections onto the l1-ball for learning in high dimensions. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: an exploration.</title>
<date>1996</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="1211" citStr="Eisner, 1996" startWordPosition="187" endWordPosition="188">d on English PTB and Chinese CTB datasets. We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17% for English and 87.25% for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to nonprojective dependency parsing or other graphical models. 1 Introduction For graph based projective dependency parsing, dynamic programming (DP) is popular for decoding due to its efficiency when handling local features. It performs cubic time parsing for arc-factored models (Eisner, 1996; McDonald et al., 2005a) and biquadratic time for higher order models with richer sibling and grandchild features (Carreras, 2007; Koo and Collins, 2010). However, for models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features</context>
<context position="7431" citStr="Eisner, 1996" startWordPosition="1247" endWordPosition="1248">e y is the weighted sum of the fired feature functions, which can be represented by the sum of the factors O(x, y) = wTf(x, y) = ∑ ∑wT f(x, c) = Oc(x) c⊆y c⊆y where f(x, c) is the feature vector that depends on c. For example, we could define a feature for grandchild c = {egh, ehm} 1 if xg =would n xh = be nxm = happy n c is selected 0 otherwise ∑ ∑ Oegh,ehm(x) O(x, y) = Oehm(x) + ehm∈y egh,ehm∈y    f(x, c) = 38 2.2 Dynamic Programming for Local Models In first order models, all factors c in Eq(1) contain a single edge. The optimal parse tree can be derived by DP with running time O(n3) (Eisner, 1996). The algorithm has two types of structures: complete span, which consists of a headword and its descendants on one side, and incomplete span, which consists of a dependency and the region between the head and modifier. It starts at single word spans, and merges the spans in bottom up order. For second order models, the score function ϕ(x, y) adds the scores of siblings (adjacent edges with a common head) and grandchildren ϕ(x, y) = � ϕehm(x) ehmEy � + egh,ehmEy � + ehm,ehsEy There are two versions of second order models, used respectively by Carreras (2007) and Koo et al. (2010). The differen</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: an exploration. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
</authors>
<title>K-best spanning tree parsing.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1626" citStr="Hall, 2007" startWordPosition="250" endWordPosition="251">ective dependency parsing, dynamic programming (DP) is popular for decoding due to its efficiency when handling local features. It performs cubic time parsing for arc-factored models (Eisner, 1996; McDonald et al., 2005a) and biquadratic time for higher order models with richer sibling and grandchild features (Carreras, 2007; Koo and Collins, 2010). However, for models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features. Since the reranking quality is bounded by the oracle performance of candidates, some work has combined candidate generation and reranking steps using cube pruning (Huang, 2008; Zhang and McDonald, 2012) to achieve higher oracle performance. They parse a sentence in bottom up order and keep the top k derivations for each span using k best parsing (Huang and Chiang, 2005). After merging the two spans, non-local </context>
</contexts>
<marker>Hall, 2007</marker>
<rawString>Keith Hall. 2007. K-best spanning tree parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="2185" citStr="Huang and Chiang, 2005" startWordPosition="341" endWordPosition="344">eranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features. Since the reranking quality is bounded by the oracle performance of candidates, some work has combined candidate generation and reranking steps using cube pruning (Huang, 2008; Zhang and McDonald, 2012) to achieve higher oracle performance. They parse a sentence in bottom up order and keep the top k derivations for each span using k best parsing (Huang and Chiang, 2005). After merging the two spans, non-local features are used to rerank top k combinations. This approach is very efficient and flexible to handle various nonlocal features. The disadvantage is that it tends to compute non-local features as early as possible so that the decoder can utilize that information at internal spans, hence it may miss long historical features such as long dependency chains. Smith and Eisner modeled dependency parsing using Markov Random Fields (MRFs) with global constraints and applied loopy belief propagation (LBP) for approximate learning and inference (Smith and Eisner</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. ofACL-HLT.</booktitle>
<contexts>
<context position="1988" citStr="Huang, 2008" startWordPosition="307" endWordPosition="308">or models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features. Since the reranking quality is bounded by the oracle performance of candidates, some work has combined candidate generation and reranking steps using cube pruning (Huang, 2008; Zhang and McDonald, 2012) to achieve higher oracle performance. They parse a sentence in bottom up order and keep the top k derivations for each span using k best parsing (Huang and Chiang, 2005). After merging the two spans, non-local features are used to rerank top k combinations. This approach is very efficient and flexible to handle various nonlocal features. The disadvantage is that it tends to compute non-local features as early as possible so that the decoder can utilize that information at internal spans, hence it may miss long historical features such as long dependency chains. Smit</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. ofACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Klenner</author>
<author>´Etienne Ailloud</author>
</authors>
<title>Optimization in coreference resolution is not needed: A nearly-optimal algorithm with intensional constraints.</title>
<date>2009</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="9481" citStr="Klenner and Ailloud (2009)" startWordPosition="1599" endWordPosition="1602">1 U Y2 by fixing assignment of one edge. For each subspace Yi, the bounding step calculates the upper bound of the optimal parse tree score in the subspace: UBYi ? maxyEYi ϕ(x, y). If this bound is no more than any obtained parse tree score UBYi G ϕ(x, y′), then all parse trees in subspace Yi are no more optimal than y′, and Yi could be pruned safely. The efficiency of B&amp;B depends on the branching strategy and upper bound computation. For example, Sun et al. (2012) used B&amp;B for MRFs, where they proposed two branching strategies and a novel data structure for efficient upper bound computation. Klenner and Ailloud (2009) proposed a variation of Balas algorithm (Balas, 1965) for coreference resolution, where candidate branching variables are sorted by their weights. Our bounding strategy is to find an upper bound for the score of each non-local factor c containing multiple edges. The bound is the sum of new scores of edges in the factor plus a constant ϕc(x) G � ψe(x) + αc eEc Based on the new scores {ψe(x)} and constants {αc}, we define the new score of parse tree y (x, y) = Y:(eEc I: ψe(x) + αc cCy Then we have ψ(x, y) ? ϕ(x, y), by E Y(x) The advantage of such a bound is that, it is the sum of new edge scor</context>
</contexts>
<marker>Klenner, Ailloud, 2009</marker>
<rawString>Manfred Klenner and ´Etienne Ailloud. 2009. Optimization in coreference resolution is not needed: A nearly-optimal algorithm with intensional constraints. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1365" citStr="Koo and Collins, 2010" startWordPosition="209" endWordPosition="212">93.17% for English and 87.25% for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to nonprojective dependency parsing or other graphical models. 1 Introduction For graph based projective dependency parsing, dynamic programming (DP) is popular for decoding due to its efficiency when handling local features. It performs cubic time parsing for arc-factored models (Eisner, 1996; McDonald et al., 2005a) and biquadratic time for higher order models with richer sibling and grandchild features (Carreras, 2007; Koo and Collins, 2010). However, for models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features. Since the reranking quality is bounded by the oracle performance of candidates, some work has combined candidate generation and reranking steps using cu</context>
<context position="6580" citStr="Koo and Collins, 2010" startWordPosition="1068" endWordPosition="1071">e is a special symbol denoted by x0 which has exactly one modifier. In this paper, we focus on unlabeled projective dependency parsing but our algorithm can be adapted for labeled or non-projective dependency parsing (McDonald et al., 2005b). The inference problem is to search the optimal parse tree y∗ * O( y) y = arg maxyEy(�) x, where Y(x) is the set of all candidate parse trees of sentence x. O(x, y) is a given score function which is usually decomposed into small parts O(x, y) = ∑ Oc(x) (1) c⊆y where c is a subset of edges, and is called a factor. For example, in the all grandchild model (Koo and Collins, 2010), the score function can be represented as where the first term is the sum of scores of all edges xh -+ xm, and the second term is the sum of the scores of all edge chains xg -+ xh -+ xm. In discriminative models, the score of a parse tree y is the weighted sum of the fired feature functions, which can be represented by the sum of the factors O(x, y) = wTf(x, y) = ∑ ∑wT f(x, c) = Oc(x) c⊆y c⊆y where f(x, c) is the feature vector that depends on c. For example, we could define a feature for grandchild c = {egh, ehm} 1 if xg =would n xh = be nxm = happy n c is selected 0 otherwise ∑ ∑ Oegh,ehm(x</context>
<context position="8413" citStr="Koo and Collins, 2010" startWordPosition="1409" endWordPosition="1412"> of siblings (adjacent edges with a common head) and grandchildren ϕ(x, y) = � ϕehm(x) ehmEy � + egh,ehmEy � + ehm,ehsEy There are two versions of second order models, used respectively by Carreras (2007) and Koo et al. (2010). The difference is that Carreras’ only considers the outermost grandchildren, while Koo and Collin’s allows all grandchild features. Both models permit O(n4) running time. Third-order models score edge triples such as three adjacent sibling modifiers, or grand-siblings that score a word, its modifier and its adjacent grandchildren, and the inference complexity is O(n4) (Koo and Collins, 2010). In this paper, for all the factors/features that can be handled by DP, we call them the local factors/features. 3 The Proposed Method 3.1 Basic Idea For general high order models with non-local features, we propose to use Branch and Bound (B&amp;B) algorithm to search the optimal parse tree. A B&amp;B algorithm has two steps: branching and bounding. The branching step recursively splits the search space Y(x) into two disjoint subspaces Y(x) = Y1 U Y2 by fixing assignment of one edge. For each subspace Yi, the bounding step calculates the upper bound of the optimal parse tree score in the subspace: U</context>
<context position="20365" citStr="Koo and Collins (2010)" startWordPosition="3747" endWordPosition="3750">rrounding words, lower cased words (English only), word length (Chinese only), prefixes and suffixes of words (Chinese only), POS tags, coarse POS tags which are derived from POS tags using a simple mapping table, distance between head and modifier, direction of edges. For English, we used 674 feature templates to generate large amounts of features, and finally got 86.7M non-zero weighted features after training. The baseline parser got 92.81% UAS on the testing set. For Chinese, we used 858 feature templates, and finally got 71.5M non-zero weighted features after train1For English, we follow Koo and Collins (2010) and ignore any word whose gold-standard POS tag is one of { “ ” : , .�. For Chinese, we ignore any word whose POS tag is PU. ing. The baseline parser got 86.89% UAS on the testing set. 4.3 B&amp;B Based Parser with Non-local Features We use the baseline parser as the backbone of our B&amp;B parser. We tried different types of non-local features as listed below: • All grand-child features. Notice that this feature can be handled by Koo’s second order model (Koo and Collins, 2010) directly. • All great grand-child features. • All sibling features: all the pairs of edges with common head. An example is </context>
<context position="23189" citStr="Koo and Collins (2010)" startWordPosition="4248" endWordPosition="4251"> through inaction , rather than through ... Figure 3: An example of hand-craft feature: for the word sequence A ... rather than A, where A is a preposition, the first A is the head of than, than is the head of rather and the second A. System PTB CTB Our baseline 92.81 86.89 B&amp;B +all grand-child 92.97 87.02 +all great grand-child 92.78 86.77 +all sibling 93.00 87.05 +all tri-sibling 92.79 86.81 +comb 92.86 86.91 +hand craft 92.89 N/A +all grand-child + all sibling + com- 93.17 87.25 b + hand craft 3rd order re-impl. 93.03 87.07 TurboParser (reported) 92.62 N/A TurboParser (our run) 92.82 86.05 Koo and Collins (2010) 93.04 N/A Zhang and McDonald (2012) 93.06 86.87 Zhang and Nivre (2011) 92.90 86.00 System integration Bohnet and Kuhn (2012) 93.39 87.5 Systems using additional resources Suzuki et al. (2009) 93.79 N/A Koo et al. (2008) 93.5 N/A Chen et al. (2012) 92.76 N/A Table 2: Comparison between our system and thestate-of-art systems. eco re c c c c negative weighted factors. Step size α is initialized with mazc,ϕ6̸=0{ |. }, as the vector p is bounded in ated using the same strategy as a unit box. α is ui� Rush et al. (2010). Two stopping criteria are used. One is 0 ≤ ψold − ψnew ≤ c, where c &gt; 0 is a g</context>
<context position="24814" citStr="Koo and Collins, 2010" startWordPosition="4545" endWordPosition="4548">refore the upper bound can be tightened as UB = ⌊NT ψ⌋ NT. During testing, we use the pre-pruning method as used in Martins et al. (2009) for both datasets to balance parsing quality and speed. This method uses a simple classifier to select the top k candidate heads for each word and exclude the other heads from search space. In our experiment, we set k = 10. 3we use e = 10−8 in our implementation 4.5 Main Result Experimental results are listed in Table 2. For comparison, we also include results of representative state-of-the-art systems. For the third order parser, we re-implemented Model 1 (Koo and Collins, 2010), and removed the longest sentence in the CTB dataset, which contains 240 words, due to the O(n4) space complexity 4. For ILP based parsing, we used TurboParser5, a speed-optimized parser toolkit. We trained full models (which use all grandchild features, all sibling features and head bigram features (Martins et al., 2011)) for both datasets using its default settings. We also list the performance in its documentation on English corpus. The observation is that, the all-sibling features are most helpful for our parser, as some good sibling features can not be encoded in DP based parser. For exa</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="23409" citStr="Koo et al. (2008)" startWordPosition="4284" endWordPosition="4287">second A. System PTB CTB Our baseline 92.81 86.89 B&amp;B +all grand-child 92.97 87.02 +all great grand-child 92.78 86.77 +all sibling 93.00 87.05 +all tri-sibling 92.79 86.81 +comb 92.86 86.91 +hand craft 92.89 N/A +all grand-child + all sibling + com- 93.17 87.25 b + hand craft 3rd order re-impl. 93.03 87.07 TurboParser (reported) 92.62 N/A TurboParser (our run) 92.82 86.05 Koo and Collins (2010) 93.04 N/A Zhang and McDonald (2012) 93.06 86.87 Zhang and Nivre (2011) 92.90 86.00 System integration Bohnet and Kuhn (2012) 93.39 87.5 Systems using additional resources Suzuki et al. (2009) 93.79 N/A Koo et al. (2008) 93.5 N/A Chen et al. (2012) 92.76 N/A Table 2: Comparison between our system and thestate-of-art systems. eco re c c c c negative weighted factors. Step size α is initialized with mazc,ϕ6̸=0{ |. }, as the vector p is bounded in ated using the same strategy as a unit box. α is ui� Rush et al. (2010). Two stopping criteria are used. One is 0 ≤ ψold − ψnew ≤ c, where c &gt; 0 is a given precision3. The other checks if the bound is tight: UB = LB. Because all features are boolean (note that they can be integer), their weights are integer during each perceptron update, hence the scores of parse trees</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proc. of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="4049" citStr="Koo et al., 2010" startWordPosition="630" endWordPosition="633">tional Linguistics. method is very expressive. It can handle arbitrary non-local features determined or bounded by linear inequalities of local features. For local models, LP is less efficient than DP. The reason is that, DP works on a small number of dimensions in each recursion, while for LP, the popular revised simplex method needs to solve a m dimensional linear system in each iteration (Nocedal and Wright, 2006), where m is the number of constraints, which is quadratic in sentence length for projective dependency parsing (Martins et al., 2009). Dual Decomposition (DD) (Rush et al., 2010; Koo et al., 2010) is a special case of Lagrangian relaxation. It relies on standard decoding algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. This method does not need to consider the tree constraint explicitly, as it resorts to dynamic programming which guarantees its satisfaction. It works well if the sub-problems can be well defined, especially for joint learning tasks. However, for the task of dependency parsing, using various non-local features may result in many overlapped sub-problems, hence it may take a long time to reach</context>
<context position="8017" citStr="Koo et al. (2010)" startWordPosition="1350" endWordPosition="1353">nning time O(n3) (Eisner, 1996). The algorithm has two types of structures: complete span, which consists of a headword and its descendants on one side, and incomplete span, which consists of a dependency and the region between the head and modifier. It starts at single word spans, and merges the spans in bottom up order. For second order models, the score function ϕ(x, y) adds the scores of siblings (adjacent edges with a common head) and grandchildren ϕ(x, y) = � ϕehm(x) ehmEy � + egh,ehmEy � + ehm,ehsEy There are two versions of second order models, used respectively by Carreras (2007) and Koo et al. (2010). The difference is that Carreras’ only considers the outermost grandchildren, while Koo and Collin’s allows all grandchild features. Both models permit O(n4) running time. Third-order models score edge triples such as three adjacent sibling modifiers, or grand-siblings that score a word, its modifier and its adjacent grandchildren, and the inference complexity is O(n4) (Koo and Collins, 2010). In this paper, for all the factors/features that can be handled by DP, we call them the local factors/features. 3 The Proposed Method 3.1 Basic Idea For general high order models with non-local features</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ailsa H Land</author>
<author>Alison G Doig</author>
</authors>
<title>An automatic method of solving discrete programming problems.</title>
<date>1960</date>
<journal>Econometrica,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="4833" citStr="Land and Doig, 1960" startWordPosition="757" endWordPosition="760"> agreement between the different oracles. This method does not need to consider the tree constraint explicitly, as it resorts to dynamic programming which guarantees its satisfaction. It works well if the sub-problems can be well defined, especially for joint learning tasks. However, for the task of dependency parsing, using various non-local features may result in many overlapped sub-problems, hence it may take a long time to reach a consensus (Martins et al., 2011). In this paper, we propose a novel Branch and Bound (B&amp;B) algorithm for efficient parsing with various non-local features. B&amp;B (Land and Doig, 1960) is generally used for combinatorial optimization problems such as ILP. The difference between our method and ILP is that the sub-problem in ILP is a relaxed LP, which requires a numerical solution, while ours bounds the non-local features by a linear combination of local features and uses DP for decoding as well as calculating the upper bound of the objective function. An exact solution is achieved if the bound is tight. Though in the worst case, time complexity is exponential in sentence length, it is practically efficient especially when adopting a pruning strategy. Experiments are conducte</context>
</contexts>
<marker>Land, Doig, 1960</marker>
<rawString>Ailsa H. Land and Alison G. Doig. 1960. An automatic method of solving discrete programming problems. Econometrica, 28(3):497–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3230" citStr="Martins et al., 2009" startWordPosition="504" endWordPosition="507">dependency parsing using Markov Random Fields (MRFs) with global constraints and applied loopy belief propagation (LBP) for approximate learning and inference (Smith and Eisner, 2008). Similar work was done for Combinatorial Categorial Grammar (CCG) parsing (Auli and Lopez, 2011). They used posterior marginal beliefs for inference to satisfy the tree constraint: for each factor, only legal messages (satisfying global constraints) are considered in the partition function. A similar line of research investigated the use of integer linear programming (ILP) based parsing (Riedel and Clarke, 2006; Martins et al., 2009). This 37 Transactions of the Association for Computational Linguistics, 1 (2013) 37–48. Action Editor: Ryan McDonald. Submitted 11/2012; Revised 2/2013; Published 3/2013. c�2013 Association for Computational Linguistics. method is very expressive. It can handle arbitrary non-local features determined or bounded by linear inequalities of local features. For local models, LP is less efficient than DP. The reason is that, DP works on a small number of dimensions in each recursion, while for LP, the popular revised simplex method needs to solve a m dimensional linear system in each iteration (Noc</context>
<context position="24329" citStr="Martins et al. (2009)" startWordPosition="4459" endWordPosition="4462">). Two stopping criteria are used. One is 0 ≤ ψold − ψnew ≤ c, where c &gt; 0 is a given precision3. The other checks if the bound is tight: UB = LB. Because all features are boolean (note that they can be integer), their weights are integer during each perceptron update, hence the scores of parse trees are discrete. The minimal gap between different scores is 1 N×T after averaging, where N is the number of training samples, and T is the iteration number for perceptron training. Therefore the upper bound can be tightened as UB = ⌊NT ψ⌋ NT. During testing, we use the pre-pruning method as used in Martins et al. (2009) for both datasets to balance parsing quality and speed. This method uses a simple classifier to select the top k candidate heads for each word and exclude the other heads from search space. In our experiment, we set k = 10. 3we use e = 10−8 in our implementation 4.5 Main Result Experimental results are listed in Table 2. For comparison, we also include results of representative state-of-the-art systems. For the third order parser, we re-implemented Model 1 (Koo and Collins, 2010), and removed the longest sentence in the CTB dataset, which contains 240 words, due to the O(n4) space complexity </context>
<context position="30519" citStr="Martins et al., 2009" startWordPosition="5546" endWordPosition="5549">(b) CTB corpus 80 60 40 k=3 k=5 k=10 k=20 no prune Figure 4 Averaged parsing time (seconds) relative to sentence length with different pruning settings, k denotes the number of candidate heads of each word in pruning step. 1000 Calls to DP 500 k=3 k=5 k=10 k=20 no prune 00 20 40 60 80 100 120 140 200 100 00 10 20 30 40 50 60 Calls to DP k=3 k=5 k=10 k=20 no prune sentence length sentence length (a) PTB corpus (b) CTB corpus Figure 5 Averaged number of Calls to DP relative to sentence length with different pruning settings, k denotes the number of candidate heads of each word in pruning step. (Martins et al., 2009). Let binary variable vik indicate whether word i has k modifiers. Given {ze} for the edges with head i, then {vik|k = 1, ... , n − 1} can be solved by The left side of the equation is the li near function of vik. The right side of the equation is a polynomial function of ze. Hence, vik could be expressed as a polynomial function of ze. mation from multiple candidates to optimize taskspecific performance. We have not conducted any experiment for k best parsing, hence we only discuss the algorithm. According to proposition 1, we have Proposition 2. Given p and subset S C_ i, let zk denote the k</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andre Martins, Noah Smith, and Eric Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Mario Figueiredo</author>
<author>Pedro Aguiar</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="4684" citStr="Martins et al., 2011" startWordPosition="733" endWordPosition="736">case of Lagrangian relaxation. It relies on standard decoding algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. This method does not need to consider the tree constraint explicitly, as it resorts to dynamic programming which guarantees its satisfaction. It works well if the sub-problems can be well defined, especially for joint learning tasks. However, for the task of dependency parsing, using various non-local features may result in many overlapped sub-problems, hence it may take a long time to reach a consensus (Martins et al., 2011). In this paper, we propose a novel Branch and Bound (B&amp;B) algorithm for efficient parsing with various non-local features. B&amp;B (Land and Doig, 1960) is generally used for combinatorial optimization problems such as ILP. The difference between our method and ILP is that the sub-problem in ILP is a relaxed LP, which requires a numerical solution, while ours bounds the non-local features by a linear combination of local features and uses DP for decoding as well as calculating the upper bound of the objective function. An exact solution is achieved if the bound is tight. Though in the worst case,</context>
<context position="25138" citStr="Martins et al., 2011" startWordPosition="4597" endWordPosition="4600">e. In our experiment, we set k = 10. 3we use e = 10−8 in our implementation 4.5 Main Result Experimental results are listed in Table 2. For comparison, we also include results of representative state-of-the-art systems. For the third order parser, we re-implemented Model 1 (Koo and Collins, 2010), and removed the longest sentence in the CTB dataset, which contains 240 words, due to the O(n4) space complexity 4. For ILP based parsing, we used TurboParser5, a speed-optimized parser toolkit. We trained full models (which use all grandchild features, all sibling features and head bigram features (Martins et al., 2011)) for both datasets using its default settings. We also list the performance in its documentation on English corpus. The observation is that, the all-sibling features are most helpful for our parser, as some good sibling features can not be encoded in DP based parser. For example, a matched pair of parentheses are always siblings, but their head may lie between them. An4In fact, Koo’s algorithm requires only O(n3) space. Our implementation is O(n4) because we store the feature vectors for fast training. 5http://www.ark.cs.cmu.edu/TurboParser/ 44 other observation is that all great grandchild f</context>
</contexts>
<marker>Martins, Smith, Figueiredo, Aguiar, 2011</marker>
<rawString>Andre Martins, Noah Smith, Mario Figueiredo, and Pedro Aguiar. 2011. Dual decomposition with many overlapping components. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1234" citStr="McDonald et al., 2005" startWordPosition="189" endWordPosition="192">TB and Chinese CTB datasets. We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17% for English and 87.25% for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to nonprojective dependency parsing or other graphical models. 1 Introduction For graph based projective dependency parsing, dynamic programming (DP) is popular for decoding due to its efficiency when handling local features. It performs cubic time parsing for arc-factored models (Eisner, 1996; McDonald et al., 2005a) and biquadratic time for higher order models with richer sibling and grandchild features (Carreras, 2007; Koo and Collins, 2010). However, for models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features. Since the reranking q</context>
<context position="6197" citStr="McDonald et al., 2005" startWordPosition="992" endWordPosition="995"> (UAS) for English at a speed of 177 words per second and 87.25% for Chinese at a speed of 97 words per second. 2 Graph Based Parsing 2.1 Problem Definition Given a sentence x = x1, x2, ... , xn where xi is the ith word of the sentence, dependency parsing assigns exactly one head word to each word, so that dependencies from head words to modifiers form a tree. The root of the tree is a special symbol denoted by x0 which has exactly one modifier. In this paper, we focus on unlabeled projective dependency parsing but our algorithm can be adapted for labeled or non-projective dependency parsing (McDonald et al., 2005b). The inference problem is to search the optimal parse tree y∗ * O( y) y = arg maxyEy(�) x, where Y(x) is the set of all candidate parse trees of sentence x. O(x, y) is a given score function which is usually decomposed into small parts O(x, y) = ∑ Oc(x) (1) c⊆y where c is a subset of edges, and is called a factor. For example, in the all grandchild model (Koo and Collins, 2010), the score function can be represented as where the first term is the sum of scores of all edges xh -+ xm, and the second term is the sum of the scores of all edge chains xg -+ xh -+ xm. In discriminative models, the</context>
<context position="19632" citStr="McDonald et al. (2005" startWordPosition="3625" endWordPosition="3628"> parameter estimation. We determine the number of iterations on the validation set, which is 6 for both corpora. For English, we train the POS tagger using linear chain perceptron on training set, and predict POS tags for the development and test data. The parser is trained using the automatic POS tags generated by 10 fold cross validation. For Chinese, we use the gold standard POS tags. We use five types of features: unigram features, bigram features, in-between features, adjacent sibling features and outermost grand-child features. The first three types of features are firstly introduced by McDonald et al. (2005a) and the last two types of features are used by Carreras (2007). All the features are the concatenation of surrounding words, lower cased words (English only), word length (Chinese only), prefixes and suffixes of words (Chinese only), POS tags, coarse POS tags which are derived from POS tags using a simple mapping table, distance between head and modifier, direction of edges. For English, we used 674 feature templates to generate large amounts of features, and finally got 86.7M non-zero weighted features after training. The baseline parser got 92.81% UAS on the testing set. For Chinese, we u</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of HLTEMNLP.</booktitle>
<contexts>
<context position="1234" citStr="McDonald et al., 2005" startWordPosition="189" endWordPosition="192">TB and Chinese CTB datasets. We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17% for English and 87.25% for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to nonprojective dependency parsing or other graphical models. 1 Introduction For graph based projective dependency parsing, dynamic programming (DP) is popular for decoding due to its efficiency when handling local features. It performs cubic time parsing for arc-factored models (Eisner, 1996; McDonald et al., 2005a) and biquadratic time for higher order models with richer sibling and grandchild features (Carreras, 2007; Koo and Collins, 2010). However, for models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features. Since the reranking q</context>
<context position="6197" citStr="McDonald et al., 2005" startWordPosition="992" endWordPosition="995"> (UAS) for English at a speed of 177 words per second and 87.25% for Chinese at a speed of 97 words per second. 2 Graph Based Parsing 2.1 Problem Definition Given a sentence x = x1, x2, ... , xn where xi is the ith word of the sentence, dependency parsing assigns exactly one head word to each word, so that dependencies from head words to modifiers form a tree. The root of the tree is a special symbol denoted by x0 which has exactly one modifier. In this paper, we focus on unlabeled projective dependency parsing but our algorithm can be adapted for labeled or non-projective dependency parsing (McDonald et al., 2005b). The inference problem is to search the optimal parse tree y∗ * O( y) y = arg maxyEy(�) x, where Y(x) is the set of all candidate parse trees of sentence x. O(x, y) is a given score function which is usually decomposed into small parts O(x, y) = ∑ Oc(x) (1) c⊆y where c is a subset of edges, and is called a factor. For example, in the all grandchild model (Koo and Collins, 2010), the score function can be represented as where the first term is the sum of scores of all edges xh -+ xm, and the second term is the sum of the scores of all edge chains xg -+ xh -+ xm. In discriminative models, the</context>
<context position="19632" citStr="McDonald et al. (2005" startWordPosition="3625" endWordPosition="3628"> parameter estimation. We determine the number of iterations on the validation set, which is 6 for both corpora. For English, we train the POS tagger using linear chain perceptron on training set, and predict POS tags for the development and test data. The parser is trained using the automatic POS tags generated by 10 fold cross validation. For Chinese, we use the gold standard POS tags. We use five types of features: unigram features, bigram features, in-between features, adjacent sibling features and outermost grand-child features. The first three types of features are firstly introduced by McDonald et al. (2005a) and the last two types of features are used by Carreras (2007). All the features are the concatenation of surrounding words, lower cased words (English only), word length (Chinese only), prefixes and suffixes of words (Chinese only), POS tags, coarse POS tags which are derived from POS tags using a simple mapping table, distance between head and modifier, direction of edges. For English, we used 674 feature templates to generate large amounts of features, and finally got 86.7M non-zero weighted features after training. The baseline parser got 92.81% UAS on the testing set. For Chinese, we u</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proc. of HLTEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="26440" citStr="Nivre and McDonald, 2008" startWordPosition="4803" endWordPosition="4806">uded them from the final system. When no additional resource is available, our parser achieved competitive performance: 93.17% Unlabeled Attachment Score (UAS) for English at a speed of 177 words per second and 87.25% for Chinese at a speed of 97 words per second. Higher UAS is reported by joint tagging and parsing (Bohnet and Nivre, 2012) or system integration (Bohnet and Kuhn, 2012) which benefits from both transition based parsing and graph based parsing. Previous work shows that combination of the two parsing techniques can learn to overcome the shortcomings of each non-integrated system (Nivre and McDonald, 2008; Zhang and Clark, 2008). System combination will be an interesting topic for our future research. The highest reported performance on English corpus is 93.79%, obtained by semisupervised learning with a large amount of unlabeled data (Suzuki et al., 2009). 4.6 Tradeoff Between Accuracy and Speed In this section, we study the trade off between accuracy and speed using different pre-pruning setups. In Table 3, we show the parsing accuracy and inference time in testing stage with different numbers of candidate heads k in pruning step. We can see that, on English dataset, when k ≥ 10, our parser </context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proc. of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen J Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="3852" citStr="Nocedal and Wright, 2006" startWordPosition="597" endWordPosition="600">09). This 37 Transactions of the Association for Computational Linguistics, 1 (2013) 37–48. Action Editor: Ryan McDonald. Submitted 11/2012; Revised 2/2013; Published 3/2013. c�2013 Association for Computational Linguistics. method is very expressive. It can handle arbitrary non-local features determined or bounded by linear inequalities of local features. For local models, LP is less efficient than DP. The reason is that, DP works on a small number of dimensions in each recursion, while for LP, the popular revised simplex method needs to solve a m dimensional linear system in each iteration (Nocedal and Wright, 2006), where m is the number of constraints, which is quadratic in sentence length for projective dependency parsing (Martins et al., 2009). Dual Decomposition (DD) (Rush et al., 2010; Koo et al., 2010) is a special case of Lagrangian relaxation. It relies on standard decoding algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. This method does not need to consider the tree constraint explicitly, as it resorts to dynamic programming which guarantees its satisfaction. It works well if the sub-problems can be well defined, </context>
</contexts>
<marker>Nocedal, Wright, 2006</marker>
<rawString>Jorge Nocedal and Stephen J. Wright. 2006. Numerical Optimization. Springer, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>James Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="3207" citStr="Riedel and Clarke, 2006" startWordPosition="500" endWordPosition="503">Smith and Eisner modeled dependency parsing using Markov Random Fields (MRFs) with global constraints and applied loopy belief propagation (LBP) for approximate learning and inference (Smith and Eisner, 2008). Similar work was done for Combinatorial Categorial Grammar (CCG) parsing (Auli and Lopez, 2011). They used posterior marginal beliefs for inference to satisfy the tree constraint: for each factor, only legal messages (satisfying global constraints) are considered in the partition function. A similar line of research investigated the use of integer linear programming (ILP) based parsing (Riedel and Clarke, 2006; Martins et al., 2009). This 37 Transactions of the Association for Computational Linguistics, 1 (2013) 37–48. Action Editor: Ryan McDonald. Submitted 11/2012; Revised 2/2013; Published 3/2013. c�2013 Association for Computational Linguistics. method is very expressive. It can handle arbitrary non-local features determined or bounded by linear inequalities of local features. For local models, LP is less efficient than DP. The reason is that, DP works on a small number of dimensions in each recursion, while for LP, the popular revised simplex method needs to solve a m dimensional linear system</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>Sebastian Riedel and James Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="4030" citStr="Rush et al., 2010" startWordPosition="626" endWordPosition="629">ciation for Computational Linguistics. method is very expressive. It can handle arbitrary non-local features determined or bounded by linear inequalities of local features. For local models, LP is less efficient than DP. The reason is that, DP works on a small number of dimensions in each recursion, while for LP, the popular revised simplex method needs to solve a m dimensional linear system in each iteration (Nocedal and Wright, 2006), where m is the number of constraints, which is quadratic in sentence length for projective dependency parsing (Martins et al., 2009). Dual Decomposition (DD) (Rush et al., 2010; Koo et al., 2010) is a special case of Lagrangian relaxation. It relies on standard decoding algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. This method does not need to consider the tree constraint explicitly, as it resorts to dynamic programming which guarantees its satisfaction. It works well if the sub-problems can be well defined, especially for joint learning tasks. However, for the task of dependency parsing, using various non-local features may result in many overlapped sub-problems, hence it may take a</context>
<context position="13506" citStr="Rush et al., 2010" startWordPosition="2450" endWordPosition="2453">Hence z′ = arg maxzψ(p, z) can be solved efficiently by DP. Because AL min max ψ(p, z) p z∈Z ze1=0 ze2=1 ψ=9 φ=4 =8 ψ=7 ψ φ =5 φ=4 ψ=7 φ=4 =7 ψ=4 ψ=6 ψ&lt;LB ψ φ =5 φ=2 φ=3 z e1= 0 1 z e2= 0 1 0 1 ψ(p,z′) ? ψ(p,z∗) ? ϕ(z∗) ? ϕ(z′) after obtaining z′ , we get the upper bound and lower bound of ϕ(z∗): ψ(p, z′) and ϕ(z′). The upper bound is expected to be as tight as possible. Using min-max inequality, we get max ϕ(z) = max min ψ(p, z) z∈Z z∈Z p Z z∈ max ψ (p, z) which provides the tightest upper bound of ϕ(z∗). Since ψ is not differentiable w.r.t p, projected sub-gradient (Calamai and Mor´e, 1987; Rush et al., 2010) is used to search the saddle point. More specifically, in each iteration, we first fix p and search z using DP, then we fix z and update p by pnew = P∆ � p + ∂ ∂pψ α where α &gt; 0 is the step size in line search, function P∆(q) denotes the projection of q onto the probability simplex ∆. In this paper, we use Euclidean projection, that is P∆(q) = min 11p − q112 p∈∆ which can be solved efficiently by sorting (Duchi et al., 2008). 3.3 Branch and Bound Based Parsing As discussed in Section 3.1, the B&amp;B recursive procedure yields a binary tree structure called Branch and Bound tree. Each node of the</context>
<context position="23709" citStr="Rush et al. (2010)" startWordPosition="4343" endWordPosition="4346">03 87.07 TurboParser (reported) 92.62 N/A TurboParser (our run) 92.82 86.05 Koo and Collins (2010) 93.04 N/A Zhang and McDonald (2012) 93.06 86.87 Zhang and Nivre (2011) 92.90 86.00 System integration Bohnet and Kuhn (2012) 93.39 87.5 Systems using additional resources Suzuki et al. (2009) 93.79 N/A Koo et al. (2008) 93.5 N/A Chen et al. (2012) 92.76 N/A Table 2: Comparison between our system and thestate-of-art systems. eco re c c c c negative weighted factors. Step size α is initialized with mazc,ϕ6̸=0{ |. }, as the vector p is bounded in ated using the same strategy as a unit box. α is ui� Rush et al. (2010). Two stopping criteria are used. One is 0 ≤ ψold − ψnew ≤ c, where c &gt; 0 is a given precision3. The other checks if the bound is tight: UB = LB. Because all features are boolean (note that they can be integer), their weights are integer during each perceptron update, hence the scores of parse trees are discrete. The minimal gap between different scores is 1 N×T after averaging, where N is the number of training samples, and T is the iteration number for perceptron training. Therefore the upper bound can be tightened as UB = ⌊NT ψ⌋ NT. During testing, we use the pre-pruning method as used in M</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2792" citStr="Smith and Eisner, 2008" startWordPosition="437" endWordPosition="440">nd Chiang, 2005). After merging the two spans, non-local features are used to rerank top k combinations. This approach is very efficient and flexible to handle various nonlocal features. The disadvantage is that it tends to compute non-local features as early as possible so that the decoder can utilize that information at internal spans, hence it may miss long historical features such as long dependency chains. Smith and Eisner modeled dependency parsing using Markov Random Fields (MRFs) with global constraints and applied loopy belief propagation (LBP) for approximate learning and inference (Smith and Eisner, 2008). Similar work was done for Combinatorial Categorial Grammar (CCG) parsing (Auli and Lopez, 2011). They used posterior marginal beliefs for inference to satisfy the tree constraint: for each factor, only legal messages (satisfying global constraints) are considered in the partition function. A similar line of research investigated the use of integer linear programming (ILP) based parsing (Riedel and Clarke, 2006; Martins et al., 2009). This 37 Transactions of the Association for Computational Linguistics, 1 (2013) 37–48. Action Editor: Ryan McDonald. Submitted 11/2012; Revised 2/2013; Publishe</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Sun</author>
<author>Murali Telaprolu</author>
<author>Honglak Lee</author>
<author>Silvio Savarese</author>
</authors>
<title>Efficient and exact MAP-MRF inference using branch and bound.</title>
<date>2012</date>
<booktitle>In Proc. of AISTATS.</booktitle>
<contexts>
<context position="9324" citStr="Sun et al. (2012)" startWordPosition="1575" endWordPosition="1578">&amp;B algorithm has two steps: branching and bounding. The branching step recursively splits the search space Y(x) into two disjoint subspaces Y(x) = Y1 U Y2 by fixing assignment of one edge. For each subspace Yi, the bounding step calculates the upper bound of the optimal parse tree score in the subspace: UBYi ? maxyEYi ϕ(x, y). If this bound is no more than any obtained parse tree score UBYi G ϕ(x, y′), then all parse trees in subspace Yi are no more optimal than y′, and Yi could be pruned safely. The efficiency of B&amp;B depends on the branching strategy and upper bound computation. For example, Sun et al. (2012) used B&amp;B for MRFs, where they proposed two branching strategies and a novel data structure for efficient upper bound computation. Klenner and Ailloud (2009) proposed a variation of Balas algorithm (Balas, 1965) for coreference resolution, where candidate branching variables are sorted by their weights. Our bounding strategy is to find an upper bound for the score of each non-local factor c containing multiple edges. The bound is the sum of new scores of edges in the factor plus a constant ϕc(x) G � ψe(x) + αc eEc Based on the new scores {ψe(x)} and constants {αc}, we define the new score of p</context>
</contexts>
<marker>Sun, Telaprolu, Lee, Savarese, 2012</marker>
<rawString>Min Sun, Murali Telaprolu, Honglak Lee, and Silvio Savarese. 2012. Efficient and exact MAP-MRF inference using branch and bound. In Proc. of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="23381" citStr="Suzuki et al. (2009)" startWordPosition="4278" endWordPosition="4281"> is the head of rather and the second A. System PTB CTB Our baseline 92.81 86.89 B&amp;B +all grand-child 92.97 87.02 +all great grand-child 92.78 86.77 +all sibling 93.00 87.05 +all tri-sibling 92.79 86.81 +comb 92.86 86.91 +hand craft 92.89 N/A +all grand-child + all sibling + com- 93.17 87.25 b + hand craft 3rd order re-impl. 93.03 87.07 TurboParser (reported) 92.62 N/A TurboParser (our run) 92.82 86.05 Koo and Collins (2010) 93.04 N/A Zhang and McDonald (2012) 93.06 86.87 Zhang and Nivre (2011) 92.90 86.00 System integration Bohnet and Kuhn (2012) 93.39 87.5 Systems using additional resources Suzuki et al. (2009) 93.79 N/A Koo et al. (2008) 93.5 N/A Chen et al. (2012) 92.76 N/A Table 2: Comparison between our system and thestate-of-art systems. eco re c c c c negative weighted factors. Step size α is initialized with mazc,ϕ6̸=0{ |. }, as the vector p is bounded in ated using the same strategy as a unit box. α is ui� Rush et al. (2010). Two stopping criteria are used. One is 0 ≤ ψold − ψnew ≤ c, where c &gt; 0 is a given precision3. The other checks if the bound is tight: UB = LB. Because all features are boolean (note that they can be integer), their weights are integer during each perceptron update, hen</context>
<context position="26696" citStr="Suzuki et al., 2009" startWordPosition="4845" endWordPosition="4848">ond. Higher UAS is reported by joint tagging and parsing (Bohnet and Nivre, 2012) or system integration (Bohnet and Kuhn, 2012) which benefits from both transition based parsing and graph based parsing. Previous work shows that combination of the two parsing techniques can learn to overcome the shortcomings of each non-integrated system (Nivre and McDonald, 2008; Zhang and Clark, 2008). System combination will be an interesting topic for our future research. The highest reported performance on English corpus is 93.79%, obtained by semisupervised learning with a large amount of unlabeled data (Suzuki et al., 2009). 4.6 Tradeoff Between Accuracy and Speed In this section, we study the trade off between accuracy and speed using different pre-pruning setups. In Table 3, we show the parsing accuracy and inference time in testing stage with different numbers of candidate heads k in pruning step. We can see that, on English dataset, when k ≥ 10, our parser could gain 2 − 3 times speedup without losing much parsing accuracy. There is a further increase of the speed with smaller k, at the cost of some accuracy. Compared with TurboParser, our parser is less efficient but more accurate. Zhang and McDonald (2012)</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semi-supervised structured conditional models for dependency parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="18323" citStr="Yamada and Matsumoto, 2003" startWordPosition="3406" endWordPosition="3409">ors such as adjacent siblings, grandchildren are directly handled as local factors. In the worst case, all the edges are selected for branching, and the complexity grows exponentially in sentence length. However, in practice, it is quite efficient, as we will show in the next section. 4 Experiments 4.1 Experimental Settings The datasets we used are the English Penn Tree Bank (PTB) and Chinese Tree Bank 5.0 (CTB5). We use the standard train/develop/test split as described in Table 1. We extracted dependencies using Joakim Nivre’s Penn2Malt tool with standard head rules: Yamada and Matsumoto’s (Yamada and Matsumoto, 2003) pjaj ϕczc = max p1cσc(z) + p&apos; · 0 pCEO = max hc(pc, z) pC pecϕcze 42 Train Develop Test PTB sec. 2-21 sec. 22 sec. 23 CTB5 sec. 001-815 sec. 886-931 sec. 816-885 1001-1136 1148-1151 1137-1147 Table 1: Data split in our experiment for English, and Zhang and Clark’s (Zhang and Clark, 2008) for Chinese. Unlabeled attachment score (UAS) is used to evaluate parsing quality1. The B&amp;B parser is implemented with C++. All the experiments are conducted on the platform Intel Core i5-2500 CPU 3.30GHz. 4.2 Baseline: DP Based Second Order Parser We use the dynamic programming based second order parser (Car</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="18612" citStr="Zhang and Clark, 2008" startWordPosition="3460" endWordPosition="3463">ments 4.1 Experimental Settings The datasets we used are the English Penn Tree Bank (PTB) and Chinese Tree Bank 5.0 (CTB5). We use the standard train/develop/test split as described in Table 1. We extracted dependencies using Joakim Nivre’s Penn2Malt tool with standard head rules: Yamada and Matsumoto’s (Yamada and Matsumoto, 2003) pjaj ϕczc = max p1cσc(z) + p&apos; · 0 pCEO = max hc(pc, z) pC pecϕcze 42 Train Develop Test PTB sec. 2-21 sec. 22 sec. 23 CTB5 sec. 001-815 sec. 886-931 sec. 816-885 1001-1136 1148-1151 1137-1147 Table 1: Data split in our experiment for English, and Zhang and Clark’s (Zhang and Clark, 2008) for Chinese. Unlabeled attachment score (UAS) is used to evaluate parsing quality1. The B&amp;B parser is implemented with C++. All the experiments are conducted on the platform Intel Core i5-2500 CPU 3.30GHz. 4.2 Baseline: DP Based Second Order Parser We use the dynamic programming based second order parser (Carreras, 2007) as the baseline. Averaged structured perceptron (Collins, 2002) is used for parameter estimation. We determine the number of iterations on the validation set, which is 6 for both corpora. For English, we train the POS tagger using linear chain perceptron on training set, and </context>
<context position="26464" citStr="Zhang and Clark, 2008" startWordPosition="4807" endWordPosition="4810">ystem. When no additional resource is available, our parser achieved competitive performance: 93.17% Unlabeled Attachment Score (UAS) for English at a speed of 177 words per second and 87.25% for Chinese at a speed of 97 words per second. Higher UAS is reported by joint tagging and parsing (Bohnet and Nivre, 2012) or system integration (Bohnet and Kuhn, 2012) which benefits from both transition based parsing and graph based parsing. Previous work shows that combination of the two parsing techniques can learn to overcome the shortcomings of each non-integrated system (Nivre and McDonald, 2008; Zhang and Clark, 2008). System combination will be an interesting topic for our future research. The highest reported performance on English corpus is 93.79%, obtained by semisupervised learning with a large amount of unlabeled data (Suzuki et al., 2009). 4.6 Tradeoff Between Accuracy and Speed In this section, we study the trade off between accuracy and speed using different pre-pruning setups. In Table 3, we show the parsing accuracy and inference time in testing stage with different numbers of candidate heads k in pruning step. We can see that, on English dataset, when k ≥ 10, our parser could gain 2 − 3 times s</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Ryan McDonald</author>
</authors>
<title>Generalized higher-order dependency parsing with cube pruning.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2015" citStr="Zhang and McDonald, 2012" startWordPosition="309" endWordPosition="312">h general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Charniak and Johnson, 2005; Hall, 2007). It typically has two steps: the low level classifier generates the top k hypotheses using local features, then the high level classifier reranks these candidates using global features. Since the reranking quality is bounded by the oracle performance of candidates, some work has combined candidate generation and reranking steps using cube pruning (Huang, 2008; Zhang and McDonald, 2012) to achieve higher oracle performance. They parse a sentence in bottom up order and keep the top k derivations for each span using k best parsing (Huang and Chiang, 2005). After merging the two spans, non-local features are used to rerank top k combinations. This approach is very efficient and flexible to handle various nonlocal features. The disadvantage is that it tends to compute non-local features as early as possible so that the decoder can utilize that information at internal spans, hence it may miss long historical features such as long dependency chains. Smith and Eisner modeled depend</context>
<context position="23225" citStr="Zhang and McDonald (2012)" startWordPosition="4254" endWordPosition="4257">hrough ... Figure 3: An example of hand-craft feature: for the word sequence A ... rather than A, where A is a preposition, the first A is the head of than, than is the head of rather and the second A. System PTB CTB Our baseline 92.81 86.89 B&amp;B +all grand-child 92.97 87.02 +all great grand-child 92.78 86.77 +all sibling 93.00 87.05 +all tri-sibling 92.79 86.81 +comb 92.86 86.91 +hand craft 92.89 N/A +all grand-child + all sibling + com- 93.17 87.25 b + hand craft 3rd order re-impl. 93.03 87.07 TurboParser (reported) 92.62 N/A TurboParser (our run) 92.82 86.05 Koo and Collins (2010) 93.04 N/A Zhang and McDonald (2012) 93.06 86.87 Zhang and Nivre (2011) 92.90 86.00 System integration Bohnet and Kuhn (2012) 93.39 87.5 Systems using additional resources Suzuki et al. (2009) 93.79 N/A Koo et al. (2008) 93.5 N/A Chen et al. (2012) 92.76 N/A Table 2: Comparison between our system and thestate-of-art systems. eco re c c c c negative weighted factors. Step size α is initialized with mazc,ϕ6̸=0{ |. }, as the vector p is bounded in ated using the same strategy as a unit box. α is ui� Rush et al. (2010). Two stopping criteria are used. One is 0 ≤ ψold − ψnew ≤ c, where c &gt; 0 is a given precision3. The other checks if</context>
<context position="27296" citStr="Zhang and McDonald (2012)" startWordPosition="4952" endWordPosition="4956">ata (Suzuki et al., 2009). 4.6 Tradeoff Between Accuracy and Speed In this section, we study the trade off between accuracy and speed using different pre-pruning setups. In Table 3, we show the parsing accuracy and inference time in testing stage with different numbers of candidate heads k in pruning step. We can see that, on English dataset, when k ≥ 10, our parser could gain 2 − 3 times speedup without losing much parsing accuracy. There is a further increase of the speed with smaller k, at the cost of some accuracy. Compared with TurboParser, our parser is less efficient but more accurate. Zhang and McDonald (2012) is a state-of-the-art system which adopts cube pruning for efficient parsing. Notice that, they did not use pruning which seems to increase parsing speed with little hit in accuracy. Moreover, they did labeled parsing, which also makes their speed not directly comparable. For each node of B&amp;B tree, our parsing algorithm uses projected sub-gradient method to find the saddle point, which requires a number of calls to a DP, hence the efficiency of Algorithm 1 is mainly determined by the number of DP calls. Figure 4 and Figure 5 show the averaged parsing time and number of calls to DP relative to</context>
</contexts>
<marker>Zhang, McDonald, 2012</marker>
<rawString>Hao Zhang and Ryan McDonald. 2012. Generalized higher-order dependency parsing with cube pruning. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="23260" citStr="Zhang and Nivre (2011)" startWordPosition="4260" endWordPosition="4263">d-craft feature: for the word sequence A ... rather than A, where A is a preposition, the first A is the head of than, than is the head of rather and the second A. System PTB CTB Our baseline 92.81 86.89 B&amp;B +all grand-child 92.97 87.02 +all great grand-child 92.78 86.77 +all sibling 93.00 87.05 +all tri-sibling 92.79 86.81 +comb 92.86 86.91 +hand craft 92.89 N/A +all grand-child + all sibling + com- 93.17 87.25 b + hand craft 3rd order re-impl. 93.03 87.07 TurboParser (reported) 92.62 N/A TurboParser (our run) 92.82 86.05 Koo and Collins (2010) 93.04 N/A Zhang and McDonald (2012) 93.06 86.87 Zhang and Nivre (2011) 92.90 86.00 System integration Bohnet and Kuhn (2012) 93.39 87.5 Systems using additional resources Suzuki et al. (2009) 93.79 N/A Koo et al. (2008) 93.5 N/A Chen et al. (2012) 92.76 N/A Table 2: Comparison between our system and thestate-of-art systems. eco re c c c c negative weighted factors. Step size α is initialized with mazc,ϕ6̸=0{ |. }, as the vector p is bounded in ated using the same strategy as a unit box. α is ui� Rush et al. (2010). Two stopping criteria are used. One is 0 ≤ ψold − ψnew ≤ c, where c &gt; 0 is a given precision3. The other checks if the bound is tight: UB = LB. Becau</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proc. of ACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>