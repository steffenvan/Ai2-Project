<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001186">
<title confidence="0.9986885">
Graph-based Semi-Supervised Learning of Translation Models from
Monolingual Data
</title>
<author confidence="0.947284">
Avneesh Saluja⇤ Hany Hassan, Kristina Toutanova, Chris Quirk
</author>
<affiliation confidence="0.749675">
Carnegie Mellon University Microsoft Research
Pittsburgh, PA 15213, USA Redmond, WA 98502, USA
</affiliation>
<email confidence="0.997867">
avneesh@cs.cmu.edu hanyh,kristout,chrisq@microsoft.com
</email>
<sectionHeader confidence="0.993856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921">
Statistical phrase-based translation learns
translation rules from bilingual corpora,
and has traditionally only used monolin-
gual evidence to construct features that
rescore existing translation candidates. In
this work, we present a semi-supervised
graph-based approach for generating new
translation rules that leverages bilingual
and monolingual data. The proposed tech-
nique first constructs phrase graphs using
both source and target language mono-
lingual corpora. Next, graph propaga-
tion identifies translations of phrases that
were not observed in the bilingual cor-
pus, assuming that similar phrases have
similar translations. We report results
on a large Arabic-English system and a
medium-sized Urdu-English system. Our
proposed approach significantly improves
the performance of competitive phrase-
based systems, leading to consistent im-
provements between 1 and 4 BLEU points
on standard evaluation sets.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983005218181818">
Statistical approaches to machine translation
(SMT) use sentence-aligned, parallel corpora to
learn translation rules along with their probabil-
ities. With large amounts of data, phrase-based
translation systems (Koehn et al., 2003; Chiang,
2007) achieve state-of-the-art results in many ty-
pologically diverse language pairs (Bojar et al.,
2013). However, the limiting factor in the suc-
cess of these techniques is parallel data availabil-
ity. Even in resource-rich languages, learning re-
liable translations of multiword phrases is a chal-
lenge, and an adequate phrasal inventory is crucial
⇤This work was done while the first author was interning
at Microsoft Research
for effective translation. This problem is exacer-
bated in the many language pairs for which par-
allel resources are either limited or nonexistent.
While parallel data is generally scarce, monolin-
gual resources exist in abundance and are being
created at accelerating rates. Can we use monolin-
gual data to augment the phrasal translations ac-
quired from parallel data?
The challenge of learning translations from
monolingual data is of long standing interest,
and has been approached in several ways (Rapp,
1995; Callison-Burch et al., 2006; Haghighi et
al., 2008; Ravi and Knight, 2011). Our work in-
troduces a new take on the problem using graph-
based semi-supervised learning to acquire trans-
lation rules and probabilities by leveraging both
monolingual and parallel data resources. On the
source side, labeled phrases (those with known
translations) are extracted from bilingual corpora,
and unlabeled phrases are extracted from mono-
lingual corpora; together they are embedded as
nodes in a graph, with the monolingual data de-
termining edge strengths between nodes (§2.2).
Unlike previous work (Irvine and Callison-Burch,
2013a; Razmara et al., 2013), we use higher order
n-grams instead of restricting to unigrams, since
our approach goes beyond OOV mitigation and
can enrich the entire translation model by using
evidence from monolingual text. This enhance-
ment alone results in an improvement of almost
1.4 BLEU points. On the target side, phrases ini-
tially consisting of translations from the parallel
data are selectively expanded with generated can-
didates (§2.1), and are embedded in a target graph.
We then limit the set of translation options for
each unlabeled source phrase (§2.3), and using
a structured graph propagation algorithm, where
translation information is propagated from la-
beled to unlabeled phrases proportional to both
source and target phrase similarities, we esti-
mate probability distributions over translations for
</bodyText>
<page confidence="0.984875">
676
</page>
<note confidence="0.837724">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 676–686,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.999781489361702">
Target
canine
dog
É
Target
the cat
cat
É
Prob.
0.6
0.3
É
Prob.
0.7
0.15
É
canino
el gato
Source Target
los gatos
felino
el perro
un gato
Target
the cats
cats
É
Target
the dog
dog
É
Prob.
0.8
0.1
É
Prob.
0.9
0.05
É
the dog
dog
the cat
canine
catlike
the cats
a cat
cat
</figure>
<figureCaption confidence="0.99537225">
Figure 1: Example source and target graphs used in our approach. Labeled phrases on the source side are black (with their
corresponding translations on the target side also black); unlabeled and generated (�2.1) phrases on the source and target sides
respectively are white. Labeled phrases also have conditional probability distributions defined over target phrases, which are
extracted from the parallel corpora.
</figureCaption>
<bodyText confidence="0.999894666666667">
the unlabeled source phrases (§2.4). The addi-
tional phrases are incorporated in the SMT sys-
tem through a secondary phrase table (§2.5). We
evaluated the proposed approach on both Arabic-
English and Urdu-English under a range of sce-
narios (§3), varying the amount and type of mono-
lingual corpora used, and obtained improvements
between 1 and 4 BLEU points, even when using
very large language models.
</bodyText>
<sectionHeader confidence="0.904263" genericHeader="introduction">
2 Generation &amp; Propagation
</sectionHeader>
<bodyText confidence="0.999857538461539">
Our goal is to obtain translation distributions for
source phrases that are not present in the phrase
table extracted from the parallel corpus. Both par-
allel and monolingual corpora are used to obtain
these probability distributions over target phrases.
We assume that sufficient parallel resources ex-
ist to learn a basic translation model using stan-
dard techniques, and also assume the availability
of larger monolingual corpora in both the source
and target languages. Although our technique ap-
plies to phrases of any length, in this work we con-
centrate on unigram and bigram phrases, which
provides substantial computational cost savings.
Monolingual data is used to construct separate
similarity graphs over phrases (word sequences),
as illustrated in Fig. 1. The source similarity graph
consists of phrase nodes representing sequences of
words in the source language. If a source phrase
is found in the baseline phrase table it is called a
labeled phrase: its conditional empirical probabil-
ity distribution over target phrases (estimated from
the parallel data) is used as the label, and is sub-
sequently never changed. Otherwise it is called an
unlabeled phrase, and our algorithm finds labels
(translations) for these unlabeled phrases, with the
help of the graph-based representation. The la-
bel space is thus the phrasal translation inventory,
and like the source side it can also be represented
in terms of a graph, initially consisting of target
phrase nodes from the parallel corpus.
For the unlabeled phrases, the set of possible
target translations could be extremely large (e.g.,
all target language n-grams). Therefore, we first
generate and fix a list of possible target transla-
tions for each unlabeled source phrase. We then
propagate by deriving a probability distribution
over these target phrases using graph propagation
techniques. Next, we will describe the generation,
graph construction and propagation steps.
</bodyText>
<subsectionHeader confidence="0.957882">
2.1 Generation
</subsectionHeader>
<bodyText confidence="0.999951066666667">
The objective of the generation step is to popu-
late the target graph with additional target phrases
for all unlabeled source phrases, yielding the full
set of possible translations for the phrase. Prior to
generation, one phrase node for each target phrase
occurring in the baseline phrase table is added to
the target graph (black nodes in Fig. 1’s target
graph). We only consider target phrases whose
source phrase is a bigram, but it is worth noting
that the target phrases are of variable length.
The generation component is based on the ob-
servation that for structured label spaces, such as
translation candidates for source phrases in SMT,
even similar phrases have slightly different labels
(target translations). The exponential dependence
</bodyText>
<page confidence="0.997646">
677
</page>
<bodyText confidence="0.999981139534884">
of the sizes of these spaces on the length of in-
stances is to blame. Thus, the target phrase inven-
tory from the parallel corpus may be inadequate
for unlabeled instances. We therefore need to en-
rich the target or label space for unknown phrases.
A naive way to achieve this goal would be to ex-
tract all n-grams, from n = 1 to a maximum n-
gram order, from the monolingual data, but this
strategy would lead to a combinatorial explosion
in the number of target phrases.
Instead, by intelligently expanding the target
space using linguistic information such as mor-
phology (Toutanova et al., 2008; Chahuneau et al.,
2013), or relying on the baseline system to gener-
ate candidates similar to self-training (McClosky
et al., 2006), we can tractably propose novel trans-
lation candidates (white nodes in Fig. 1’s target
graph) whose probabilities are then estimated dur-
ing propagation. We refer to these additional can-
didates as “generated” candidates.
To generate new translation candidates using
the baseline system, we decode each unlabeled
source bigram to generate its m-best translations.
This set of candidate phrases is filtered to include
only n-grams occurring in the target monolingual
corpus, and helps to prune passed-through OOV
words and invalid translations. To generate new
translation candidates using morphological infor-
mation, we morphologically segment words into
prefixes, stem, and suffixes using linguistic re-
sources. We assume that a morphological ana-
lyzer which provides context-independent analysis
of word types exists, and implements the functions
STEM(f) and STEM(e) for source and target word
types. Based on these functions, source and target
sequences of words can be mapped to sequences
of stems. The morphological generation step adds
to the target graph all target word sequences from
the monolingual data that map to the same stem
sequence as one of the target phrases occurring in
the baseline phrase table. In other words, this step
adds phrases that are morphological variants of ex-
isting phrases, differing only in their affixes.
</bodyText>
<subsectionHeader confidence="0.99837">
2.2 Graph Construction
</subsectionHeader>
<bodyText confidence="0.995355794117647">
At this stage, there exists a list of source bigram
phrases, both labeled and unlabeled, as well as a
list of target language phrases of variable length,
originating from both the phrase table and the gen-
eration step. To determine pairwise phrase similar-
ities in order to embed these nodes in their graphs,
we utilize the monolingual corpora on both the
source and target sides to extract distributional
features based on the context surrounding each
phrase. For a phrase, we look at the p words before
and the p words after the phrase, explicitly distin-
guishing between the two sides, but not distance
(i.e., bag of words on each side). Co-occurrence
counts for each feature (context word) are accu-
mulated over the monolingual corpus, and these
counts are converted to pointwise mutual infor-
mation (PMI) values, as is standard practice when
computing distributional similarities. Cosine sim-
ilarity between two phrases’ PMI vectors is used
for similarity, and we take only the k most simi-
lar phrases for each phrase, to create a k-nearest
neighbor similarity matrix for both source and tar-
get language phrases. These graphs are distinct,
in that propagation happens within the two graphs
but not between them.
While accumulating co-occurrence counts for
each phrase, we also maintain an inverted index
data structure, which is a mapping from features
(context words) to phrases that co-occur with that
feature within a window of p.1 The inverted index
structure reduces the graph construction cost from
✓(n2), by only computing similarities for a sub-
set of all possible pairs of phrases, namely other
phrases that have at least one feature in common.
</bodyText>
<subsectionHeader confidence="0.999495">
2.3 Candidate Translation List Construction
</subsectionHeader>
<bodyText confidence="0.999748714285714">
As mentioned previously, we construct and fix
a set of translation candidates, i.e., the label set
for each unlabeled source phrase. The probabil-
ity distribution over these translations is estimated
through graph propagation, and the probabilities
of items outside the list are assumed to be zero.
We obtain these candidates from two sources:2
</bodyText>
<listItem confidence="0.988641666666667">
1. The union of each unlabeled phrase’s la-
beled neighbors’ labels, which represents the
set of target phrases that occur as transla-
tions of source phrases that are similar to
the unlabeled source phrase. For un gato in
Fig. 1, this source would yield the cat and
cat, among others, as candidates.
2. The generated candidates for the unlabeled
phrase – the ones from the baseline system’s
</listItem>
<footnote confidence="0.988171833333333">
1The q most frequent words in the monolingual corpus
were removed as keys from this mapping, as these high en-
tropy features do not provide much information.
2We also obtained the k-nearest neighbors of the transla-
tion candidates generated through these methods by utilizing
the target graph, but this had minimal impact.
</footnote>
<page confidence="0.9946">
678
</page>
<bodyText confidence="0.996827956521739">
decoder output, or from a morphological gen-
erator (e.g., a cat and catlike in Fig. 1).
The morphologically-generated candidates for a
given source unlabeled phrase are initially de-
fined as the target word sequences in the mono-
lingual data that have the same stem sequence
as one of the baseline’s target translations for a
source phrase which has the same stem sequence
as the unlabeled source phrase. These candidates
are scored using stem-level translation probabili-
ties, morpheme-level lexical weighting probabili-
ties, and a language model, and only the top 30
candidates are included.
After obtaining candidates from these two pos-
sible sources, the list is sorted by forward lexical
score, using the lexical models of the baseline sys-
tem. The top r candidates are then chosen for each
phrase’s translation candidate list.
In Figure 2 we provide example outputs of
our system for a handful of unlabeled source
phrases, and explicitly note the source of the trans-
lation candidate (‘G’ for generated, ‘N’ for labeled
neighbor’s label).
</bodyText>
<subsectionHeader confidence="0.990441">
2.4 Graph Propagation
</subsectionHeader>
<bodyText confidence="0.999990333333333">
A graph propagation algorithm transfers label in-
formation from labeled nodes to unlabeled nodes
by following the graph’s structure. In some appli-
cations, a label may consist of class membership
information, e.g., each node can belong to one of
a certain number of classes. In our problem, the
“label” for each node is actually a probability dis-
tribution over a set of translation candidates (target
phrases). For a given node f, let e refer to a can-
didate in the label set for node f; then in graph
propagation, the probability of candidate e given
source phrase f in iteration t + 1 is:
</bodyText>
<equation confidence="0.997768">
Pt+1(e|f) = X Ts(j|f)Pt(e|j) (1)
j2N(f)
</equation>
<bodyText confidence="0.999640882352941">
where the set N(f) contains the (labeled and unla-
beled) neighbors of node f, and Ts(j|f) is a term
that captures how similar nodes f and j are. This
quantity is also known as the propagation proba-
bility, and its exact form will depend on the type
of graph propagation algorithm used. For our pur-
poses, node f is a source phrasal node, the set
N(f) refers to other source phrases that are neigh-
bors of f (restricted to the k-nearest neighbors as
in §2.2), and the aim is to estimate P(e|f), the
probability of target phrase e being a phrasal trans-
lation of source phrase f.
A classic propagation algorithm that has been
suitably modified for use in bilingual lexicon in-
duction (Tamura et al., 2012; Razmara et al., 2013)
is the label propagation (LP) algorithm of Zhu et
al. (2003). In this case, Ts(f, j) is chosen to be:
</bodyText>
<equation confidence="0.999556">
Ts(j|f) = E j&apos;2N(f) wfs,j&apos;
wsf,j (2)
</equation>
<bodyText confidence="0.999976857142857">
where wsf,j is the cosine similarity (as computed
in §2.2) between phrase f and phrase j on side s
(the source side).
As evident in Eq. 2, LP only takes into account
source language similarity of phrases. To see this
observation more clearly, let us reformulate Eq. 1
more generally as:
</bodyText>
<equation confidence="0.999762">
Pt+�(ejf) = X T�(jjf) X Tt(e&apos;je)Pt(e&apos;jj) (3)
jEN(f) e&apos;EW(j)
</equation>
<bodyText confidence="0.999698571428571">
where H(j) is the translation candidate set for
source phrase j, and Tt(e0|e) is the propagation
probability between nodes or phrases e and e0
on the target side. We have simply replaced
Pt(e|j) with Ee&apos;2H(j) Tt(e0|e)Pt(e0|j), defining it
in terms of j’s translation candidate list.
Note that in the original LP formulation the tar-
get side information is disregarded, i.e., Tt(e0|e) =
1 if and only if e = e0 and 0 otherwise. As a
result, LP is suboptimal for our needs, since it is
unable to appropriately handle generated transla-
tion candidates for the unlabeled phrases. These
translation candidates are usually not present as
translations for the labeled phrases (or for the la-
beled phrases that neighbor the unlabeled one in
question). When propagating information from
the labeled phrases, such candidates will obtain
no probability mass since e =6 e0. Thus, due to
the setup of the problem, LP naturally biases away
from translation candidates produced during the
generation step (§2.1).
</bodyText>
<subsectionHeader confidence="0.898693">
2.4.1 Structured Label Propagation
</subsectionHeader>
<bodyText confidence="0.999109222222222">
The label set we are considering has a similarity
structure encoded by the target graph. How can
we exploit this structure in graph propagation on
the source graph? In Liu et al. (2012), the authors
generalize label propagation to structured label
propagation (SLP) in an effort to work more el-
egantly with structured labels. In particular, the
definition of target similarity is similar to that of
source similarity:
</bodyText>
<equation confidence="0.897679666666667">
t
Tt (e0  |e) = we,e&apos; t (4)
Ee&apos;&apos;2H(j) we e&apos;&apos;
</equation>
<page confidence="0.971884">
679
</page>
<bodyText confidence="0.979753">
Therefore, the final update equation in SLP is:
</bodyText>
<equation confidence="0.999758">
Pt+�(ejf) � X T�(jjf) X Tt(e0je)Pt(e0jj) (5)
jEN(f) e&apos;EN(j)
</equation>
<bodyText confidence="0.999850571428571">
With this formulation, even if e =6 e&apos;, the simi-
larity Tt(e&apos;|e) as determined by the target phrase
graph will dictate propagation probability. We re-
normalize the probability distributions after each
propagation step to sum to one over the fixed list
of translation candidates, and run the SLP algo-
rithm to convergence.3
</bodyText>
<subsectionHeader confidence="0.98446">
2.5 Phrase-based SMT Expansion
</subsectionHeader>
<bodyText confidence="0.99998676">
After graph propagation, each unlabeled phrase
is labeled with a categorical distribution over
the set of translation candidates defined in §2.3.
In order to utilize these newly acquired phrase
pairs, we need to compute their relevant features.
The phrase pairs have four log-probability fea-
tures with two likelihood features and two lexical
weighting features. In addition, we use a sophis-
ticated lexicalized hierarchical reordering model
(HRM) (Galley and Manning, 2008) with five fea-
tures for each phrase pair.
We utilize the graph propagation-estimated for-
ward phrasal probabilities P(e|f) as the forward
likelihood probabilities for the acquired phrases;
to obtain the backward phrasal probability for a
given phrase pair, we make use of Bayes’ Theo-
rem:
where the marginal probabilities of source and tar-
get phrases e and f are obtained from the counts
extracted from the monolingual data. The baseline
system’s lexical models are used for the forward
and backward lexical scores. The HRM probabil-
ities for the new phrase pairs are estimated from
the baseline system by backing-off to the average
values for phrases with similar length.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="background">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999615295454545">
We performed an extensive evaluation to exam-
ine various aspects of the approach along with
overall system performance. Two language pairs
were used: Arabic-English and Urdu-English. The
Arabic-English evaluation was used to validate the
decisions made during the development of our
3Empirically within a few iterations and a wall-clock time
of less than 10 minutes in total.
method and also to highlight properties of the
technique. With it, in §3.2 we first analyzed the
impact of utilizing phrases instead of words and
SLP instead of LP; the latter experiment under-
scores the importance of generated candidates. We
also look at how adding morphological knowledge
to the generation process can further enrich per-
formance. In §3.3, we then examined the effect of
using a very large 5-gram language model train-
ing on 7.5 billion English tokens to understand the
nature of the improvements in §3.2. The Urdu to
English evaluation in §3.4 focuses on how noisy
parallel data and completely monolingual (i.e., not
even comparable) text can be used for a realistic
low-resource language pair, and is evaluated with
the larger language model only. We also exam-
ine how our approach can learn from noisy parallel
data compared to the traditional SMT system.
Baseline phrasal systems are used both for com-
parison and for generating translation candidates
for unlabeled phrases as described in §2.1. The
baseline is a state-of-the-art phrase-based system;
we perform word alignment using a lexicalized
hidden Markov model, and then the phrase ta-
ble is extracted using the grow-diag-final
heuristic (Koehn et al., 2003). The 13 baseline
features (2 lexical, 2 phrasal, 5 HRM, and 1 lan-
guage model, word penalty, phrase length feature
and distortion penalty feature) were tuned using
MERT (Och, 2003), which is also used to tune
the 4 feature weights introduced by the secondary
phrase table (2 lexical and 2 phrasal, other fea-
tures being shared between the two tables). For
all systems, we use a distortion limit of 4. We use
case-insensitive BLEU (Papineni et al., 2002) to
evaluate translation quality.
</bodyText>
<subsectionHeader confidence="0.982553">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999969461538462">
Bilingual corpus statistics for both language pairs
are presented in Table 2. For Arabic-English, our
training corpus consisted of 685k sentence pairs
from standard LDC corpora4. The NIST MT06
and MT08 Arabic-English evaluation sets (com-
bining the newswire and weblog domains for both
sets), with four references each, were used as
tuning and testing sets respectively. For Urdu-
English, the training corpus was provided by the
LDC for the NIST Urdu-English MT evaluation,
and most of the data was automatically acquired
from the web, making it quite noisy. After fil-
tering, there are approximately 65k parallel sen-
</bodyText>
<equation confidence="0.95983925">
4LDC2007T08 and LDC2008T09
P(e|f)P(f)
P(f|e) =
P( e)
</equation>
<page confidence="0.98654">
680
</page>
<table confidence="0.392179">
Parameter Description Value
</table>
<tableCaption confidence="0.939795125">
m m-best candidate list size when bootstrapping candidates in generation stage. 100
p Window size on each side when extracting features for phrases. 2
q Filter the q most frequent words when storing the inverted index data structure for graph construction. 25
Both source and target sides share the same value.
k Number of neighbors stored for each phrase for both source and target graphs. This parameter controls 500
the sparsity of the graph.
r Maximum size of translation candidate list for unlabeled phrases. 20
Table 1: Parameters, explanation of their function, and value chosen.
</tableCaption>
<bodyText confidence="0.9719965">
tences; these were supplemented by an additional
100k dictionary entries. Tuning and test data con-
sisted of the MT08 and MT09 evaluation corpora,
once again a mixture of news and web text.
</bodyText>
<table confidence="0.999740857142857">
Corpus Sentences Words (Src)
Ar-En Train 685,502 17,055,168
Ar-En Tune (MT06) 1,664 33,739
Ar-En Test (MT08) 1,360 42,472
Ur-En Train 165,159 1,169,367
Ur-En Tune (MT08) 1,864 39,925
Ur-En Test (MT09) 1,792 39,922
</table>
<tableCaption confidence="0.9911445">
Table 2: Bilingual corpus statistics for the Arabic-English
and Urdu-English datasets used.
</tableCaption>
<bodyText confidence="0.9484761">
Table 3 contains statistics for the monolingual
corpora used in our experiments. From these cor-
pora, we extracted all sentences that contained at
least one source or target phrase match to com-
pute features for graph construction. For the Ara-
bic to English experiments, the monolingual cor-
pora are taken from the AFP Arabic and English
Gigaword corpora and are of a similar date range
to each other (1994-2010), rendering them compa-
rable but not sentence-aligned or parallel.
</bodyText>
<subsectionHeader confidence="0.614866">
Corpus Sentences Words
</subsectionHeader>
<table confidence="0.997454">
Ar Comparable 10.2m 290m
En I Comparable 29.8m 900m
Ur Noisy Parallel 470k 5m
En II Noisy Parallel 470k 4.7m
Ur Non-Comparable 7m 119m
En II Non-Comparable 17m 510m
</table>
<tableCaption confidence="0.75332">
Table 3: Monolingual corpus statistics for the Arabic-English
and Urdu-English evaluations. The monolingual corpora can
be sub-divided into comparable, noisy parallel, and non-
comparable components. En I refers to the English side of
the Arabic-English corpora, and En II to the English side of
the Urdu-English corpora.
</tableCaption>
<bodyText confidence="0.999978764705882">
For the Urdu-English experiments, completely
non-comparable monolingual text was used for
graph construction; we obtained the Urdu side
through a web-crawler, and a subset of the AFP
Gigaword English corpus was used for English. In
addition, we obtained a corpus from the ELRA5,
which contains a mix of parallel and monolingual
data; based on timestamps, we extracted a compa-
rable English corpus for the ELRA Urdu monolin-
gual data to form a roughly 470k-sentence “noisy
parallel” set. We used this set in two ways: ei-
ther to augment the parallel data presented in Table
2, or to augment the non-comparable monolingual
data in Table 3 for graph construction.
For the parameters introduced throughout the
text, we present in Table 1 a reminder of their in-
terpretation as well as the values used in this work.
</bodyText>
<subsectionHeader confidence="0.995704">
3.2 Experimental Variations
</subsectionHeader>
<bodyText confidence="0.99999072">
In our first set of experiments, we looked at the im-
pact of choosing bigrams over unigrams as our ba-
sic unit of representation, along with performance
of LP (Eq. 2) compared to SLP (Eq. 4). Re-
call that LP only takes into account source sim-
ilarity; since the vast majority of generated can-
didates do not occur as labeled neighbors’ labels,
restricting propagation to the source graph dras-
tically reduces the usage of generated candidates
as labels, but does not completely eliminate it. In
these experiments, we utilize a reasonably-sized
4-gram language model trained on 900m English
tokens, i.e., the English monolingual corpus.
Table 4 presents the results of these variations;
overall, by taking into account generated candi-
dates appropriately and using bigrams (“SLP 2-
gram”), we obtained a 1.13 BLEU gain on the
test set. Using unigrams (“SLP 1-gram”) actu-
ally does worse than the baseline, indicating the
importance of focusing on translations for sparser
bigrams. While LP (“LP 2-gram”) does reason-
ably well, its underperformance compared to SLP
underlines the importance of enriching the trans-
lation space with generated candidates and han-
dling these candidates appropriately.6 In “SLP-
</bodyText>
<footnote confidence="0.9632515">
5ELRA-W0038
6It is relatively straightforward to combine both unigrams
and bigrams in one source graph, but for experimental clarity
we did not mix these phrase lengths.
</footnote>
<page confidence="0.998172">
681
</page>
<bodyText confidence="0.998887222222222">
HalfMono”, we use only half of the monolingual
comparable corpora, and still obtain an improve-
ment of 0.56 BLEU points, indicating that adding
more monolingual data is likely to improve the
system further. Interestingly, biasing away from
generated candidates using all the monolingual
data (“LP 2-gram”) performs similarly to using
half the monolingual corpora and handling gener-
ated candidates properly (“SLP-HalfMono”).
</bodyText>
<table confidence="0.99801775">
BLEU
Setup Tune Test
Baseline 39.33 38.09
SLP 1-gram 39.47 37.85
LP 2-gram 40.75 38.68
SLP 2-gram 41.00 39.22
SLP-HalfMono 2-gram 40.82 38.65
SLP+Morph 2-gram 41.02 39.35
</table>
<tableCaption confidence="0.977185857142857">
Table 4: Results for the Arabic-English evaluation. The LP
vs. SLP comparison highlights the importance of target side
enrichment via translation candidate generation, 1-gram vs.
2-gram comparisons highlight the importance of emphasiz-
ing phrases, utilizing half the monolingual data shows sensi-
tivity to monolingual corpus size, and adding morphological
information results in additional improvement.
</tableCaption>
<bodyText confidence="0.998884875">
Additional morphologically generated candi-
dates were added in this experiment as detailed in
§2.3. We used a simple hand-built Arabic morpho-
logical analyzer that segments word types based
on regular expressions, and an English lexicon-
based morphological analyzer. The morphological
candidates add a small amount of improvement,
primarily by targeting genuine OOVs.
</bodyText>
<subsectionHeader confidence="0.997911">
3.3 Large Language Model Effect
</subsectionHeader>
<bodyText confidence="0.9999155">
In this set of experiments, we examined if the
improvements in §3.2 can be explained primar-
ily through the extraction of language model char-
acteristics during the semi-supervised learning
phase, or through orthogonal pieces of evidence.
Would the improvement be less substantial had we
used a very large language model?
To answer this question we trained a 5-gram
language model on 570M sentences (7.6B tokens),
with data from various sources including the Gi-
gaword corpus7, WMT and European Parliamen-
tary Proceedings8, and web-crawled data from
Wikipedia and the web. Only m-best generated
candidates from the baseline were considered dur-
ing generation, along with labeled neighbors’ la-
bels.
</bodyText>
<footnote confidence="0.961238">
7LDC2011T07
8http://www.statmt.org/wmt13/
</footnote>
<table confidence="0.86041675">
BLEU
Setup Tune Test
Baseline+LargeLM 41.48 39.86
SLP+LargeLM 42.82 41.29
</table>
<tableCaption confidence="0.9853195">
Table 5: Results with the large language model scenario. The
gains are even better than with the smaller language model.
</tableCaption>
<bodyText confidence="0.998450428571429">
Table 5 presents the results of using this lan-
guage model. We obtained a robust, 1.43-BLEU
point gain, indicating that the addition of the
newly induced phrases provided genuine transla-
tion improvements that cannot be compensated by
the language model effect. Further examination of
the differences between the two systems yielded
that most of the improvements are due to better
bigrams and trigrams, as indicated by the break-
down of the BLEU score precision per n-gram,
and primarily leverages higher quality generated
candidates from the baseline system. We analyze
the output of these systems further in the output
analysis section below (§3.5).
</bodyText>
<subsectionHeader confidence="0.997866">
3.4 Urdu-English
</subsectionHeader>
<bodyText confidence="0.999824230769231">
In order to evaluate the robustness of these results
beyond one language pair, we looked at Urdu-
English, a low resource pair likely to benefit from
this approach. In this set of experiments, we used
the large language model in §3.3, and only used
baseline-generated candidates. We experimented
with two extreme setups that differed in the data
assumed parallel, from which we built our base-
line system, and the data treated as monolingual,
from which we built our source and target graphs.
In the first setup, we use the noisy parallel
data for graph construction and augment the non-
comparable corpora with it:
</bodyText>
<listItem confidence="0.9946356">
• parallel: “Ur-En Train”
• Urdu monolingual: “Ur Noisy Parallel”+“Ur
Non-Comparable”
• English monolingual: “En II Noisy Paral-
lel”+“En II Non-Comparable”
</listItem>
<bodyText confidence="0.991818">
The results from this setup are presented as “Base-
line” and “SLP+Noisy” in Table 6. In the second
setup, we train a baseline system using the data in
Table 2, augmented with the noisy parallel text:
</bodyText>
<listItem confidence="0.9843358">
• parallel: “Ur-En Train”+“Ur Noisy Paral-
lel”+“En II Noisy Parallel”
• Urdu monolingual: “Ur Non-Comparable”
• English monolingual: “En II Non-
Comparable”
</listItem>
<page confidence="0.991187">
682
</page>
<table confidence="0.9779024">
Ex Source Reference Baseline System
1 (Ar) &amp;quot;I j� jju11 JLﺳj sending reinforcements strong reinforcements sending reinforcements (N)
2 (Ar) Jf.�3YI +J with extinction OOV with extinction (N)
3 (Ar) ������ !ﺑﺣﺗ thwarts address thwarted (N)
4 (Ar) ﻲﻟ# !ﺑﺳﻧ was quoted as saying attributed to was quoted as saying (G)
5 (Ar) oJ.�.�l !ﺑﻋ ﺢﺿ#$ abdalmahmood said he said abdul mahmood mahmood said (G)
6 (Ar) tﺑﻛﻧﻣ el#ﺗ it deems OOV it deems (G)
7 (Ur) !&amp;quot;ﻣ$ !ﭘ I am hopeful this hope I am hopeful (N)
8 (Ur) tLﻓ$ ﺎﻧﭘ$ to defend him to defend to defend himself (G)
9 (Ur) ۔ﯽﮐ !ﮕﺗﻔﮔ while speaking In the in conversation (N)
</table>
<figureCaption confidence="0.985442666666667">
Figure 2: Nine example outputs of our system vs. the baseline highlighting the properties of our approach. Each example is
labeled (Ar) for Arabic source or (Ur) for Urdu source, and system candidates are labeled with (N) if the candidate unlabeled
phrase’s labeled neighbor’s label, or (G) if the candidate was generated.
</figureCaption>
<bodyText confidence="0.999852777777778">
The results from this setup are presented as “Base-
line+Noisy” and “SLP” in Table 6. The two setups
allow us to examine how effectively our method
can learn from the noisy parallel data by treating it
as monolingual (i.e., for graph construction), com-
pared to treating this data as parallel, and also ex-
amines the realistic scenario of using completely
non-comparable monolingual text for graph con-
struction as in the second setup.
</bodyText>
<table confidence="0.997922333333333">
BLEU
Setup Tune Test
Baseline 21.87 21.17
SLP+Noisy 26.42 25.38
Baseline+Noisy 27.59 27.24
SLP 28.53 28.43
</table>
<tableCaption confidence="0.9058675">
Table 6: Results for the Urdu-English evaluation evaluated
with BLEU. All experiments were conducted with the larger
language model, and generation only considered the m-best
candidates from the baseline system.
</tableCaption>
<bodyText confidence="0.999966882352941">
In the first setup, we get a huge improvement of
4.2 BLEU points (“SLP+Noisy”) when using the
monolingual data and the noisy parallel data for
graph construction. Our method obtained much
of the gains achieved by the supervised baseline
approach that utilizes the noisy parallel data in
conjunction with the NIST-provided parallel data
(“Baseline+Noisy”), but with fewer assumptions
on the nature of the corpora (monolingual vs.
parallel). Furthermore, despite completely un-
aligned, non-comparable monolingual text on the
Urdu and English sides, and a very large language
model, we can still achieve gains in excess of
1.2 BLEU points (“SLP”) in a difficult evaluation
scenario, which shows that the technique adds a
genuine translation improvement over and above
naive memorization of n-gram sequences.
</bodyText>
<subsectionHeader confidence="0.992809">
3.5 Analysis of Output
</subsectionHeader>
<bodyText confidence="0.99719268292683">
Figure 2 looks at some of the sample hypotheses
produced by our system and the baseline, along
with reference translations. The outputs produced
by our system are additionally annotated with the
origin of the candidate, i.e., labeled neighbor’s la-
bel (N) or generated (G).
The Arabic-English examples are numbered 1
to 5. The first example shows a source bigram un-
known to the baseline system, resulting in a sub-
optimal translation, while our system proposes the
correct translation of “sending reinforcements”.
The second example shows a word that was an
OOV for the baseline system, while our system
got a perfect translation. The third and fourth ex-
amples represent bigram phrases with much bet-
ter translations compared to backing off to the
lexical translations as in the baseline. The fifth
Arabic-English example demonstrates the pitfalls
of over-reliance on the distributional hypothesis:
the source bigram corresponding to the name “abd
almahmood” is distributional similar to another
named entity “mahmood” and the English equiva-
lent is offered as a translation. The distributional
hypothesis can sometimes be misleading. The
sixth example shows how morphological informa-
tion can propose novel candidates: an OOV word
is broken down to its stem via the analyzer and
candidates are generated based on the stem.
The Urdu-English examples are numbered 7
to 9. In example 7, the bigram “par umeed”
(corresponding to “hopeful”) is never seen in the
baseline system, which has only seen “umeed”
(“hope”). By leveraging the monolingual corpus
to understand the context of this unlabeled bigram,
we can utilize the graph structure to propose a syn-
tactically correct form, also resulting in a more flu-
ent and correct sentence as determined by the lan-
guage model. Examples 8 &amp; 9 show cases where
the baseline deletes words or translates them into
more common words e.g., “conversation” to “the”,
while our system proposes reasonable candidates.
</bodyText>
<page confidence="0.999264">
683
</page>
<sectionHeader confidence="0.99973" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999994752941177">
The idea presented in this paper is similar in spirit
to bilingual lexicon induction (BLI), where a seed
lexicon in two different languages is expanded
with the help of monolingual corpora, primarily by
extracting distributional similarities from the data
using word context. This line of work, initiated
by Rapp (1995) and continued by others (Fung
and Yee, 1998; Koehn and Knight, 2002) (inter
alia) is limited from a downstream perspective, as
translations for only a small number of words are
induced and oftentimes for common or frequently
occurring ones only. Recent improvements to BLI
(Tamura et al., 2012; Irvine and Callison-Burch,
2013b) have contained a graph-based flavor by
presenting label propagation-based approaches us-
ing a seed lexicon, but evaluation is once again
done on top-1 or top-3 accuracy, and the focus is
on unigrams.
Razmara et al. (2013) and Irvine and Callison-
Burch (2013a) conduct a more extensive evalua-
tion of their graph-based BLI techniques, where
the emphasis and end-to-end BLEU evaluations
concentrated on OOVs, i.e., unigrams, and not on
enriching the entire translation model. As with
previous BLI work, these approaches only take
into account source-side similarity of words; only
moderate gains (and in the latter work, on a sub-
set of language pairs evaluated) are obtained. Ad-
ditionally, because of our structured propagation
algorithm, our approach is better at handling mul-
tiple translation candidates and does not need to
restrict itself to the top translation.
Klementiev et al. (2012) propose a method that
utilizes a pre-existing phrase table and a small
bilingual lexicon, and performs BLI using mono-
lingual corpora. The operational scope of their ap-
proach is limited in that they assume a scenario
where unknown phrase pairs are provided (thereby
sidestepping the issue of translation candidate
generation for completely unknown phrases), and
what remains is the estimation of phrasal proba-
bilities. In our case, we obtain the phrase pairs
from the graph structure (and therefore indirectly
from the monolingual data) and a separate gener-
ation step, which plays an important role in good
performance of the method. Similarly, Zhang and
Zong (2013) present a series of heuristics that are
applicable in a fairly narrow setting.
The notion of translation consensus, wherein
similar sentences on the source side are encour-
aged to have similar target language translations,
has also been explored via a graph-based approach
(Alexandrescu and Kirchhoff, 2009). Liu et al.
(2012) extend this method by proposing a novel
structured label propagation algorithm to deal with
the generalization of propagating sets of labels
instead of single labels, and also integrated in-
formation from the graph into the decoder. In
fact, we utilize this algorithm in our propagation
step (§2.4). However, the former work operates
only at the level of sentences, and while the latter
does extend the framework to sub-spans of sen-
tences, they do not discover new translation pairs
or phrasal probabilities for new pairs at all, but
instead re-estimate phrasal probabilities using the
graph structure and add this score as an additional
feature during decoding.
The goal of leveraging non-parallel data in ma-
chine translation has been explored from several
different angles. Paraphrases extracted by “pivot-
ing” via a third language (Callison-Burch et al.,
2006) can be derived solely from monolingual
corpora using distributional similarity (Marton et
al., 2009). Snover et al. (2008) use cross-lingual
information retrieval techniques to find potential
sentence-level translation candidates among com-
parable corpora. In this case, the goal is to
try and construct a corpus as close to parallel
as possible from comparable corpora, and is a
fairly different take on the problem we are look-
ing at. Decipherment-based approaches (Ravi and
Knight, 2011; Dou and Knight, 2012) have gen-
erally taken a monolingual view to the problem
and combine phrase tables through the log-linear
model during feature weight training.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999201">
In this work, we presented an approach that
can expand a translation model extracted from a
sentence-aligned, bilingual corpus using a large
amount of unstructured, monolingual data in both
source and target languages, which leads to im-
provements of 1.4 and 1.2 BLEU points over
strong baselines on evaluation sets, and in some
scenarios gains in excess of 4 BLEU points. In
the future, we plan to estimate the graph structure
through other learned, distributed representations.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999668666666667">
The authors would like to thank Chris Dyer, Arul
Menezes, and the anonymous reviewers for their
helpful comments and suggestions.
</bodyText>
<page confidence="0.998368">
684
</page>
<sectionHeader confidence="0.988002" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999542345132743">
Andrei Alexandrescu and Katrin Kirchhoff. 2009.
Graph-based learning for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, NAACL-HLT ’09, pages 119–
127. Association for Computational Linguistics,
June.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings of the
Human Language Technology Conference of the
NAACL, Main Conference, pages 17–24, New York
City, USA, June. Association for Computational
Linguistics.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proc. of
EMNLP.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228,
June.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
266–275. Association for Computational Linguis-
tics, July.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 1, ACL ’98, pages 414–
420, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. EMNLP ’08, pages 848–856, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771–779, Columbus, Ohio, June.
Association for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low re-
source machine translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, pages 262–270, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 518–523, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward sta-
tistical machine translation without parallel corpora.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 130–140, Avignon, France, April.
Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition, pages 9–16.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 48–54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Shujie Liu, Chi-Ho Li, Mu Li, and Ming Zhou. 2012.
Learning translation consensus with structured la-
bel propagation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, ACL ’12, pages
302–310, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’09, pages 381–390, Singapore, August. Association
for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152–159, New York City, USA, June. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 160–
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.984989">
685
</page>
<reference confidence="0.999734647058823">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. pages 311–318.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, ACL ’95.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 12–
21, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Majid Razmara, Maryam Siahbani, Gholamreza Haf-
fari, and Anoop Sarkar. 2013. Graph propagation
for paraphrasing out-of-vocabulary words in statis-
tical machine translation. In Proceedings of the
51st of the Association for Computational Linguis-
tics, ACL-51, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’08, pages 857–866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 24–36.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08:
HLT, pages 514–522, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1425–1434, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Laf-
ferty. 2003. Semi-supervised learning using gaus-
sian fields and harmonic functions. In Proceedings
of the Twentieth International Conference on Ma-
chine Learning, ICML ’03, pages 912–919.
</reference>
<page confidence="0.998729">
686
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.772728">
<title confidence="0.9103415">Graph-based Semi-Supervised Learning of Translation Models from Monolingual Data</title>
<author confidence="0.980069">Kristina Toutanova Hassan</author>
<author confidence="0.980069">Chris Quirk</author>
<affiliation confidence="0.999933">Carnegie Mellon University Microsoft Research</affiliation>
<address confidence="0.944595">Pittsburgh, PA 15213, USA Redmond, WA 98502,</address>
<email confidence="0.998616">avneesh@cs.cmu.eduhanyh,kristout,chrisq@microsoft.com</email>
<abstract confidence="0.999872375">Statistical phrase-based translation learns translation rules from bilingual corpora, and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates. In this work, we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data. The proposed technique first constructs phrase graphs using both source and target language monolingual corpora. Next, graph propagation identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrei Alexandrescu</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Graph-based learning for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT ’09,</booktitle>
<pages>119--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="36855" citStr="Alexandrescu and Kirchhoff, 2009" startWordPosition="5840" endWordPosition="5843">unknown phrases), and what remains is the estimation of phrasal probabilities. In our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate generation step, which plays an important role in good performance of the method. Similarly, Zhang and Zong (2013) present a series of heuristics that are applicable in a fairly narrow setting. The notion of translation consensus, wherein similar sentences on the source side are encouraged to have similar target language translations, has also been explored via a graph-based approach (Alexandrescu and Kirchhoff, 2009). Liu et al. (2012) extend this method by proposing a novel structured label propagation algorithm to deal with the generalization of propagating sets of labels instead of single labels, and also integrated information from the graph into the decoder. In fact, we utilize this algorithm in our propagation step (§2.4). However, the former work operates only at the level of sentences, and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using</context>
</contexts>
<marker>Alexandrescu, Kirchhoff, 2009</marker>
<rawString>Andrei Alexandrescu and Katrin Kirchhoff. 2009. Graph-based learning for statistical machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT ’09, pages 119– 127. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1572" citStr="Bojar et al., 2013" startWordPosition="204" endWordPosition="207"> a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabilities. With large amounts of data, phrase-based translation systems (Koehn et al., 2003; Chiang, 2007) achieve state-of-the-art results in many typologically diverse language pairs (Bojar et al., 2013). However, the limiting factor in the success of these techniques is parallel data availability. Even in resource-rich languages, learning reliable translations of multiword phrases is a challenge, and an adequate phrasal inventory is crucial ⇤This work was done while the first author was interning at Microsoft Research for effective translation. This problem is exacerbated in the many language pairs for which parallel resources are either limited or nonexistent. While parallel data is generally scarce, monolingual resources exist in abundance and are being created at accelerating rates. Can w</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="2431" citStr="Callison-Burch et al., 2006" startWordPosition="339" endWordPosition="342">is crucial ⇤This work was done while the first author was interning at Microsoft Research for effective translation. This problem is exacerbated in the many language pairs for which parallel resources are either limited or nonexistent. While parallel data is generally scarce, monolingual resources exist in abundance and are being created at accelerating rates. Can we use monolingual data to augment the phrasal translations acquired from parallel data? The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara e</context>
<context position="37736" citStr="Callison-Burch et al., 2006" startWordPosition="5979" endWordPosition="5982">n fact, we utilize this algorithm in our propagation step (§2.4). However, the former work operates only at the level of sentences, and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structure and add this score as an additional feature during decoding. The goal of leveraging non-parallel data in machine translation has been explored from several different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 17–24, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Chahuneau</author>
<author>Eva Schlinger</author>
<author>Noah A Smith</author>
<author>Chris Dyer</author>
</authors>
<title>Translating into morphologically rich languages with synthetic phrases.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="8460" citStr="Chahuneau et al., 2013" startWordPosition="1300" endWordPosition="1303">ce 677 of the sizes of these spaces on the length of instances is to blame. Thus, the target phrase inventory from the parallel corpus may be inadequate for unlabeled instances. We therefore need to enrich the target or label space for unknown phrases. A naive way to achieve this goal would be to extract all n-grams, from n = 1 to a maximum ngram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases. Instead, by intelligently expanding the target space using linguistic information such as morphology (Toutanova et al., 2008; Chahuneau et al., 2013), or relying on the baseline system to generate candidates similar to self-training (McClosky et al., 2006), we can tractably propose novel translation candidates (white nodes in Fig. 1’s target graph) whose probabilities are then estimated during propagation. We refer to these additional candidates as “generated” candidates. To generate new translation candidates using the baseline system, we decode each unlabeled source bigram to generate its m-best translations. This set of candidate phrases is filtered to include only n-grams occurring in the target monolingual corpus, and helps to prune p</context>
</contexts>
<marker>Chahuneau, Schlinger, Smith, Dyer, 2013</marker>
<rawString>Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich languages with synthetic phrases. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1473" citStr="Chiang, 2007" startWordPosition="192" endWordPosition="193">lingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabilities. With large amounts of data, phrase-based translation systems (Koehn et al., 2003; Chiang, 2007) achieve state-of-the-art results in many typologically diverse language pairs (Bojar et al., 2013). However, the limiting factor in the success of these techniques is parallel data availability. Even in resource-rich languages, learning reliable translations of multiword phrases is a challenge, and an adequate phrasal inventory is crucial ⇤This work was done while the first author was interning at Microsoft Research for effective translation. This problem is exacerbated in the many language pairs for which parallel resources are either limited or nonexistent. While parallel data is generally </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Dou</author>
<author>Kevin Knight</author>
</authors>
<title>Large scale decipherment for out-of-domain machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>266--275</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="38247" citStr="Dou and Knight, 2012" startWordPosition="6057" endWordPosition="6060">veral different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. 5 Conclusion In this work, we presented an approach that can expand a translation model extracted from a sentence-aligned, bilingual corpus using a large amount of unstructured, monolingual data in both source and target languages, which leads to improvements of 1.4 and 1.2 BLEU points over strong baselines on evaluation sets, and in some scenarios gains in excess of 4 BLEU points. In the future, we plan to estimate the graph structure through other lea</context>
</contexts>
<marker>Dou, Knight, 2012</marker>
<rawString>Qing Dou and Kevin Knight. 2012. Large scale decipherment for out-of-domain machine translation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 266–275. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An ir approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL ’98,</booktitle>
<pages>414--420</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="34710" citStr="Fung and Yee, 1998" startWordPosition="5506" endWordPosition="5509"> correct sentence as determined by the language model. Examples 8 &amp; 9 show cases where the baseline deletes words or translates them into more common words e.g., “conversation” to “the”, while our system proposes reasonable candidates. 683 4 Related Work The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a more extensive evaluation of their graph-based</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An ir approach for translating new words from nonparallel, comparable texts. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL ’98, pages 414– 420, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<journal>EMNLP</journal>
<volume>08</volume>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18001" citStr="Galley and Manning, 2008" startWordPosition="2859" endWordPosition="2862"> propagation step to sum to one over the fixed list of translation candidates, and run the SLP algorithm to convergence.3 2.5 Phrase-based SMT Expansion After graph propagation, each unlabeled phrase is labeled with a categorical distribution over the set of translation candidates defined in §2.3. In order to utilize these newly acquired phrase pairs, we need to compute their relevant features. The phrase pairs have four log-probability features with two likelihood features and two lexical weighting features. In addition, we use a sophisticated lexicalized hierarchical reordering model (HRM) (Galley and Manning, 2008) with five features for each phrase pair. We utilize the graph propagation-estimated forward phrasal probabilities P(e|f) as the forward likelihood probabilities for the acquired phrases; to obtain the backward phrasal probability for a given phrase pair, we make use of Bayes’ Theorem: where the marginal probabilities of source and target phrases e and f are obtained from the counts extracted from the monolingual data. The baseline system’s lexical models are used for the forward and backward lexical scores. The HRM probabilities for the new phrase pairs are estimated from the baseline system </context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. EMNLP ’08, pages 848–856, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>771--779</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2454" citStr="Haghighi et al., 2008" startWordPosition="343" endWordPosition="346">e while the first author was interning at Microsoft Research for effective translation. This problem is exacerbated in the many language pairs for which parallel resources are either limited or nonexistent. While parallel data is generally scarce, monolingual resources exist in abundance and are being created at accelerating rates. Can we use monolingual data to augment the phrasal translations acquired from parallel data? The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use hi</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL08: HLT, pages 771–779, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Combining bilingual and comparable corpora for low resource machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>262--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3019" citStr="Irvine and Callison-Burch, 2013" startWordPosition="428" endWordPosition="431">ys (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. We then limit the set of translation options for each unlabeled source phrase (§2.3), and using a structured graph propa</context>
<context position="34993" citStr="Irvine and Callison-Burch, 2013" startWordPosition="5550" endWordPosition="5553">ted in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a more extensive evaluation of their graph-based BLI techniques, where the emphasis and end-to-end BLEU evaluations concentrated on OOVs, i.e., unigrams, and not on enriching the entire translation model. As with previous BLI work, these approaches only take into account source-side similarity of words; only moderate gains (and i</context>
</contexts>
<marker>Irvine, Callison-Burch, 2013</marker>
<rawString>Ann Irvine and Chris Callison-Burch. 2013a. Combining bilingual and comparable corpora for low resource machine translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 262–270, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Supervised bilingual lexicon induction with multiple monolingual signals.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>518--523</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="3019" citStr="Irvine and Callison-Burch, 2013" startWordPosition="428" endWordPosition="431">ys (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. We then limit the set of translation options for each unlabeled source phrase (§2.3), and using a structured graph propa</context>
<context position="34993" citStr="Irvine and Callison-Burch, 2013" startWordPosition="5550" endWordPosition="5553">ted in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a more extensive evaluation of their graph-based BLI techniques, where the emphasis and end-to-end BLEU evaluations concentrated on OOVs, i.e., unigrams, and not on enriching the entire translation model. As with previous BLI work, these approaches only take into account source-side similarity of words; only moderate gains (and i</context>
</contexts>
<marker>Irvine, Callison-Burch, 2013</marker>
<rawString>Ann Irvine and Chris Callison-Burch. 2013b. Supervised bilingual lexicon induction with multiple monolingual signals. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 518–523, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ann Irvine</author>
<author>Chris CallisonBurch</author>
<author>David Yarowsky</author>
</authors>
<title>Toward statistical machine translation without parallel corpora.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>130--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="35882" citStr="Klementiev et al. (2012)" startWordPosition="5689" endWordPosition="5692">uct a more extensive evaluation of their graph-based BLI techniques, where the emphasis and end-to-end BLEU evaluations concentrated on OOVs, i.e., unigrams, and not on enriching the entire translation model. As with previous BLI work, these approaches only take into account source-side similarity of words; only moderate gains (and in the latter work, on a subset of language pairs evaluated) are obtained. Additionally, because of our structured propagation algorithm, our approach is better at handling multiple translation candidates and does not need to restrict itself to the top translation. Klementiev et al. (2012) propose a method that utilizes a pre-existing phrase table and a small bilingual lexicon, and performs BLI using monolingual corpora. The operational scope of their approach is limited in that they assume a scenario where unknown phrase pairs are provided (thereby sidestepping the issue of translation candidate generation for completely unknown phrases), and what remains is the estimation of phrasal probabilities. In our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate generation step, which plays an important role i</context>
</contexts>
<marker>Klementiev, Irvine, CallisonBurch, Yarowsky, 2012</marker>
<rawString>Alexandre Klementiev, Ann Irvine, Chris CallisonBurch, and David Yarowsky. 2012. Toward statistical machine translation without parallel corpora. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 130–140, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora. In</title>
<date>2002</date>
<booktitle>In Proceedings of ACL Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="34735" citStr="Koehn and Knight, 2002" startWordPosition="5510" endWordPosition="5513"> determined by the language model. Examples 8 &amp; 9 show cases where the baseline deletes words or translates them into more common words e.g., “conversation” to “the”, while our system proposes reasonable candidates. 683 4 Related Work The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a more extensive evaluation of their graph-based BLI techniques, where th</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In In Proceedings of ACL Workshop on Unsupervised Lexical Acquisition, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1458" citStr="Koehn et al., 2003" startWordPosition="188" endWordPosition="191">t observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabilities. With large amounts of data, phrase-based translation systems (Koehn et al., 2003; Chiang, 2007) achieve state-of-the-art results in many typologically diverse language pairs (Bojar et al., 2013). However, the limiting factor in the success of these techniques is parallel data availability. Even in resource-rich languages, learning reliable translations of multiword phrases is a challenge, and an adequate phrasal inventory is crucial ⇤This work was done while the first author was interning at Microsoft Research for effective translation. This problem is exacerbated in the many language pairs for which parallel resources are either limited or nonexistent. While parallel dat</context>
<context position="20291" citStr="Koehn et al., 2003" startWordPosition="3225" endWordPosition="3228">nolingual (i.e., not even comparable) text can be used for a realistic low-resource language pair, and is evaluated with the larger language model only. We also examine how our approach can learn from noisy parallel data compared to the traditional SMT system. Baseline phrasal systems are used both for comparison and for generating translation candidates for unlabeled phrases as described in §2.1. The baseline is a state-of-the-art phrase-based system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase table is extracted using the grow-diag-final heuristic (Koehn et al., 2003). The 13 baseline features (2 lexical, 2 phrasal, 5 HRM, and 1 language model, word penalty, phrase length feature and distortion penalty feature) were tuned using MERT (Och, 2003), which is also used to tune the 4 feature weights introduced by the secondary phrase table (2 lexical and 2 phrasal, other features being shared between the two tables). For all systems, we use a distortion limit of 4. We use case-insensitive BLEU (Papineni et al., 2002) to evaluate translation quality. 3.1 Datasets Bilingual corpus statistics for both language pairs are presented in Table 2. For Arabic-English, our</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Chi-Ho Li</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>Learning translation consensus with structured label propagation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>302--310</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16789" citStr="Liu et al. (2012)" startWordPosition="2669" endWordPosition="2672">ates are usually not present as translations for the labeled phrases (or for the labeled phrases that neighbor the unlabeled one in question). When propagating information from the labeled phrases, such candidates will obtain no probability mass since e =6 e0. Thus, due to the setup of the problem, LP naturally biases away from translation candidates produced during the generation step (§2.1). 2.4.1 Structured Label Propagation The label set we are considering has a similarity structure encoded by the target graph. How can we exploit this structure in graph propagation on the source graph? In Liu et al. (2012), the authors generalize label propagation to structured label propagation (SLP) in an effort to work more elegantly with structured labels. In particular, the definition of target similarity is similar to that of source similarity: t Tt (e0 |e) = we,e&apos; t (4) Ee&apos;&apos;2H(j) we e&apos;&apos; 679 Therefore, the final update equation in SLP is: Pt+�(ejf) � X T�(jjf) X Tt(e0je)Pt(e0jj) (5) jEN(f) e&apos;EN(j) With this formulation, even if e =6 e&apos;, the similarity Tt(e&apos;|e) as determined by the target phrase graph will dictate propagation probability. We renormalize the probability distributions after each propagation </context>
<context position="36874" citStr="Liu et al. (2012)" startWordPosition="5844" endWordPosition="5847">is the estimation of phrasal probabilities. In our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate generation step, which plays an important role in good performance of the method. Similarly, Zhang and Zong (2013) present a series of heuristics that are applicable in a fairly narrow setting. The notion of translation consensus, wherein similar sentences on the source side are encouraged to have similar target language translations, has also been explored via a graph-based approach (Alexandrescu and Kirchhoff, 2009). Liu et al. (2012) extend this method by proposing a novel structured label propagation algorithm to deal with the generalization of propagating sets of labels instead of single labels, and also integrated information from the graph into the decoder. In fact, we utilize this algorithm in our propagation step (§2.4). However, the former work operates only at the level of sentences, and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structur</context>
</contexts>
<marker>Liu, Li, Li, Zhou, 2012</marker>
<rawString>Shujie Liu, Chi-Ho Li, Mu Li, and Ming Zhou. 2012. Learning translation consensus with structured label propagation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 302–310, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Chris Callison-Burch</author>
<author>Philip Resnik</author>
</authors>
<title>Improved statistical machine translation using monolingually-derived paraphrases.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09,</booktitle>
<pages>381--390</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="37837" citStr="Marton et al., 2009" startWordPosition="5993" endWordPosition="5996">he level of sentences, and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structure and add this score as an additional feature during decoding. The goal of leveraging non-parallel data in machine translation has been explored from several different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. 5 Conclusion In this work, we presented an appr</context>
</contexts>
<marker>Marton, Callison-Burch, Resnik, 2009</marker>
<rawString>Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Improved statistical machine translation using monolingually-derived paraphrases. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09, pages 381–390, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="8567" citStr="McClosky et al., 2006" startWordPosition="1317" endWordPosition="1320">y from the parallel corpus may be inadequate for unlabeled instances. We therefore need to enrich the target or label space for unknown phrases. A naive way to achieve this goal would be to extract all n-grams, from n = 1 to a maximum ngram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases. Instead, by intelligently expanding the target space using linguistic information such as morphology (Toutanova et al., 2008; Chahuneau et al., 2013), or relying on the baseline system to generate candidates similar to self-training (McClosky et al., 2006), we can tractably propose novel translation candidates (white nodes in Fig. 1’s target graph) whose probabilities are then estimated during propagation. We refer to these additional candidates as “generated” candidates. To generate new translation candidates using the baseline system, we decode each unlabeled source bigram to generate its m-best translations. This set of candidate phrases is filtered to include only n-grams occurring in the target monolingual corpus, and helps to prune passed-through OOV words and invalid translations. To generate new translation candidates using morphologica</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="20471" citStr="Och, 2003" startWordPosition="3257" endWordPosition="3258"> learn from noisy parallel data compared to the traditional SMT system. Baseline phrasal systems are used both for comparison and for generating translation candidates for unlabeled phrases as described in §2.1. The baseline is a state-of-the-art phrase-based system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase table is extracted using the grow-diag-final heuristic (Koehn et al., 2003). The 13 baseline features (2 lexical, 2 phrasal, 5 HRM, and 1 language model, word penalty, phrase length feature and distortion penalty feature) were tuned using MERT (Och, 2003), which is also used to tune the 4 feature weights introduced by the secondary phrase table (2 lexical and 2 phrasal, other features being shared between the two tables). For all systems, we use a distortion limit of 4. We use case-insensitive BLEU (Papineni et al., 2002) to evaluate translation quality. 3.1 Datasets Bilingual corpus statistics for both language pairs are presented in Table 2. For Arabic-English, our training corpus consisted of 685k sentence pairs from standard LDC corpora4. The NIST MT06 and MT08 Arabic-English evaluation sets (combining the newswire and weblog domains for b</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 160– 167, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<pages>311--318</pages>
<contexts>
<context position="20743" citStr="Papineni et al., 2002" startWordPosition="3303" endWordPosition="3306">system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase table is extracted using the grow-diag-final heuristic (Koehn et al., 2003). The 13 baseline features (2 lexical, 2 phrasal, 5 HRM, and 1 language model, word penalty, phrase length feature and distortion penalty feature) were tuned using MERT (Och, 2003), which is also used to tune the 4 feature weights introduced by the secondary phrase table (2 lexical and 2 phrasal, other features being shared between the two tables). For all systems, we use a distortion limit of 4. We use case-insensitive BLEU (Papineni et al., 2002) to evaluate translation quality. 3.1 Datasets Bilingual corpus statistics for both language pairs are presented in Table 2. For Arabic-English, our training corpus consisted of 685k sentence pairs from standard LDC corpora4. The NIST MT06 and MT08 Arabic-English evaluation sets (combining the newswire and weblog domains for both sets), with four references each, were used as tuning and testing sets respectively. For UrduEnglish, the training corpus was provided by the LDC for the NIST Urdu-English MT evaluation, and most of the data was automatically acquired from the web, making it quite noi</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL ’95.</booktitle>
<contexts>
<context position="2402" citStr="Rapp, 1995" startWordPosition="337" endWordPosition="338">l inventory is crucial ⇤This work was done while the first author was interning at Microsoft Research for effective translation. This problem is exacerbated in the many language pairs for which parallel resources are either limited or nonexistent. While parallel data is generally scarce, monolingual resources exist in abundance and are being created at accelerating rates. Can we use monolingual data to augment the phrasal translations acquired from parallel data? The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Cal</context>
<context position="34666" citStr="Rapp (1995)" startWordPosition="5500" endWordPosition="5501">, also resulting in a more fluent and correct sentence as determined by the language model. Examples 8 &amp; 9 show cases where the baseline deletes words or translates them into more common words e.g., “conversation” to “the”, while our system proposes reasonable candidates. 683 4 Related Work The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a mo</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL ’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Deciphering foreign language.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>12--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2478" citStr="Ravi and Knight, 2011" startWordPosition="347" endWordPosition="350">r was interning at Microsoft Research for effective translation. This problem is exacerbated in the many language pairs for which parallel resources are either limited or nonexistent. While parallel data is generally scarce, monolingual resources exist in abundance and are being created at accelerating rates. Can we use monolingual data to augment the phrasal translations acquired from parallel data? The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams inste</context>
<context position="38224" citStr="Ravi and Knight, 2011" startWordPosition="6053" endWordPosition="6056">s been explored from several different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. 5 Conclusion In this work, we presented an approach that can expand a translation model extracted from a sentence-aligned, bilingual corpus using a large amount of unstructured, monolingual data in both source and target languages, which leads to improvements of 1.4 and 1.2 BLEU points over strong baselines on evaluation sets, and in some scenarios gains in excess of 4 BLEU points. In the future, we plan to estimate the graph stru</context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011. Deciphering foreign language. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 12– 21, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>Maryam Siahbani</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st of the Association for Computational Linguistics, ACL-51,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3043" citStr="Razmara et al., 2013" startWordPosition="432" endWordPosition="435">al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. We then limit the set of translation options for each unlabeled source phrase (§2.3), and using a structured graph propagation algorithm, where </context>
<context position="15087" citStr="Razmara et al., 2013" startWordPosition="2383" endWordPosition="2386"> that captures how similar nodes f and j are. This quantity is also known as the propagation probability, and its exact form will depend on the type of graph propagation algorithm used. For our purposes, node f is a source phrasal node, the set N(f) refers to other source phrases that are neighbors of f (restricted to the k-nearest neighbors as in §2.2), and the aim is to estimate P(e|f), the probability of target phrase e being a phrasal translation of source phrase f. A classic propagation algorithm that has been suitably modified for use in bilingual lexicon induction (Tamura et al., 2012; Razmara et al., 2013) is the label propagation (LP) algorithm of Zhu et al. (2003). In this case, Ts(f, j) is chosen to be: Ts(j|f) = E j&apos;2N(f) wfs,j&apos; wsf,j (2) where wsf,j is the cosine similarity (as computed in §2.2) between phrase f and phrase j on side s (the source side). As evident in Eq. 2, LP only takes into account source language similarity of phrases. To see this observation more clearly, let us reformulate Eq. 1 more generally as: Pt+�(ejf) = X T�(jjf) X Tt(e&apos;je)Pt(e&apos;jj) (3) jEN(f) e&apos;EW(j) where H(j) is the translation candidate set for source phrase j, and Tt(e0|e) is the propagation probability betw</context>
<context position="35216" citStr="Razmara et al. (2013)" startWordPosition="5586" endWordPosition="5589">he data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a more extensive evaluation of their graph-based BLI techniques, where the emphasis and end-to-end BLEU evaluations concentrated on OOVs, i.e., unigrams, and not on enriching the entire translation model. As with previous BLI work, these approaches only take into account source-side similarity of words; only moderate gains (and in the latter work, on a subset of language pairs evaluated) are obtained. Additionally, because of our structured propagation algorithm, our approach is better at handling multiple translation candidates and does not need t</context>
</contexts>
<marker>Razmara, Siahbani, Haffari, Sarkar, 2013</marker>
<rawString>Majid Razmara, Maryam Siahbani, Gholamreza Haffari, and Anoop Sarkar. 2013. Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation. In Proceedings of the 51st of the Association for Computational Linguistics, ACL-51, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Language and translation model adaptation using comparable corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>857--866</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="37859" citStr="Snover et al. (2008)" startWordPosition="5997" endWordPosition="6000"> and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structure and add this score as an additional feature during decoding. The goal of leveraging non-parallel data in machine translation has been explored from several different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. 5 Conclusion In this work, we presented an approach that can expand a</context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2008. Language and translation model adaptation using comparable corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 857–866, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Tamura</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora using label propagation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>24--36</pages>
<contexts>
<context position="15064" citStr="Tamura et al., 2012" startWordPosition="2379" endWordPosition="2382">and Ts(j|f) is a term that captures how similar nodes f and j are. This quantity is also known as the propagation probability, and its exact form will depend on the type of graph propagation algorithm used. For our purposes, node f is a source phrasal node, the set N(f) refers to other source phrases that are neighbors of f (restricted to the k-nearest neighbors as in §2.2), and the aim is to estimate P(e|f), the probability of target phrase e being a phrasal translation of source phrase f. A classic propagation algorithm that has been suitably modified for use in bilingual lexicon induction (Tamura et al., 2012; Razmara et al., 2013) is the label propagation (LP) algorithm of Zhu et al. (2003). In this case, Ts(f, j) is chosen to be: Ts(j|f) = E j&apos;2N(f) wfs,j&apos; wsf,j (2) where wsf,j is the cosine similarity (as computed in §2.2) between phrase f and phrase j on side s (the source side). As evident in Eq. 2, LP only takes into account source language similarity of phrases. To see this observation more clearly, let us reformulate Eq. 1 more generally as: Pt+�(ejf) = X T�(jjf) X Tt(e&apos;je)Pt(e&apos;jj) (3) jEN(f) e&apos;EW(j) where H(j) is the translation candidate set for source phrase j, and Tt(e0|e) is the propa</context>
<context position="34960" citStr="Tamura et al., 2012" startWordPosition="5546" endWordPosition="5549"> Work The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a more extensive evaluation of their graph-based BLI techniques, where the emphasis and end-to-end BLEU evaluations concentrated on OOVs, i.e., unigrams, and not on enriching the entire translation model. As with previous BLI work, these approaches only take into account source-side similarity of </context>
</contexts>
<marker>Tamura, Watanabe, Sumita, 2012</marker>
<rawString>Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2012. Bilingual lexicon extraction from comparable corpora using label propagation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 24–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
<author>Achim Ruopp</author>
</authors>
<title>Applying morphology generation models to machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>514--522</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="8435" citStr="Toutanova et al., 2008" startWordPosition="1296" endWordPosition="1299">The exponential dependence 677 of the sizes of these spaces on the length of instances is to blame. Thus, the target phrase inventory from the parallel corpus may be inadequate for unlabeled instances. We therefore need to enrich the target or label space for unknown phrases. A naive way to achieve this goal would be to extract all n-grams, from n = 1 to a maximum ngram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases. Instead, by intelligently expanding the target space using linguistic information such as morphology (Toutanova et al., 2008; Chahuneau et al., 2013), or relying on the baseline system to generate candidates similar to self-training (McClosky et al., 2006), we can tractably propose novel translation candidates (white nodes in Fig. 1’s target graph) whose probabilities are then estimated during propagation. We refer to these additional candidates as “generated” candidates. To generate new translation candidates using the baseline system, we decode each unlabeled source bigram to generate its m-best translations. This set of candidate phrases is filtered to include only n-grams occurring in the target monolingual cor</context>
</contexts>
<marker>Toutanova, Suzuki, Ruopp, 2008</marker>
<rawString>Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying morphology generation models to machine translation. In Proceedings of ACL-08: HLT, pages 514–522, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Chengqing Zong</author>
</authors>
<title>Learning a phrase-based translation model from monolingual data with application to domain adaptation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1425--1434</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="36548" citStr="Zhang and Zong (2013)" startWordPosition="5794" endWordPosition="5797">g phrase table and a small bilingual lexicon, and performs BLI using monolingual corpora. The operational scope of their approach is limited in that they assume a scenario where unknown phrase pairs are provided (thereby sidestepping the issue of translation candidate generation for completely unknown phrases), and what remains is the estimation of phrasal probabilities. In our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate generation step, which plays an important role in good performance of the method. Similarly, Zhang and Zong (2013) present a series of heuristics that are applicable in a fairly narrow setting. The notion of translation consensus, wherein similar sentences on the source side are encouraged to have similar target language translations, has also been explored via a graph-based approach (Alexandrescu and Kirchhoff, 2009). Liu et al. (2012) extend this method by proposing a novel structured label propagation algorithm to deal with the generalization of propagating sets of labels instead of single labels, and also integrated information from the graph into the decoder. In fact, we utilize this algorithm in our</context>
</contexts>
<marker>Zhang, Zong, 2013</marker>
<rawString>Jiajun Zhang and Chengqing Zong. 2013. Learning a phrase-based translation model from monolingual data with application to domain adaptation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1425–1434, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John D Lafferty</author>
</authors>
<title>Semi-supervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning, ICML ’03,</booktitle>
<pages>912--919</pages>
<contexts>
<context position="15148" citStr="Zhu et al. (2003)" startWordPosition="2394" endWordPosition="2397">so known as the propagation probability, and its exact form will depend on the type of graph propagation algorithm used. For our purposes, node f is a source phrasal node, the set N(f) refers to other source phrases that are neighbors of f (restricted to the k-nearest neighbors as in §2.2), and the aim is to estimate P(e|f), the probability of target phrase e being a phrasal translation of source phrase f. A classic propagation algorithm that has been suitably modified for use in bilingual lexicon induction (Tamura et al., 2012; Razmara et al., 2013) is the label propagation (LP) algorithm of Zhu et al. (2003). In this case, Ts(f, j) is chosen to be: Ts(j|f) = E j&apos;2N(f) wfs,j&apos; wsf,j (2) where wsf,j is the cosine similarity (as computed in §2.2) between phrase f and phrase j on side s (the source side). As evident in Eq. 2, LP only takes into account source language similarity of phrases. To see this observation more clearly, let us reformulate Eq. 1 more generally as: Pt+�(ejf) = X T�(jjf) X Tt(e&apos;je)Pt(e&apos;jj) (3) jEN(f) e&apos;EW(j) where H(j) is the translation candidate set for source phrase j, and Tt(e0|e) is the propagation probability between nodes or phrases e and e0 on the target side. We have sim</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the Twentieth International Conference on Machine Learning, ICML ’03, pages 912–919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>