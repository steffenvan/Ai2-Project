<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000051">
<title confidence="0.6256445">
NOUN CLASSIFICATION FROM PREDICATE-ARGUMENT STRUCTURES
Donald Hindle
AT&amp;T Bell Laboratories
600 Mountain Avenue
</title>
<author confidence="0.496059">
Murray Hill, NI 07974
</author>
<sectionHeader confidence="0.791334" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999953666666667">
A method of determining the similarity of nouns
on the basis of a metric derived from the distribution
of subject, verb and object in a large text corpus is
described. The resulting quasi-semantic classification
of nouns demonstrates the plausibility of the
distributional hypothesis, and has potential
application to a variety of tasks, including automatic
indexing, resolving nominal compounds, and
determining the scope of modification.
</bodyText>
<sectionHeader confidence="0.993587" genericHeader="method">
I. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999963353846154">
A variety of linguistic relations apply to sets of
semantically similar words. For example, modifiers
select semantically similar nouns, selectional
restrictions are expressed in terms of the semantic
class of objects, and semantic type restricts the
possibilities for noun compounding. Therefore, it is
useful to have a classification of words into
semantically similar sets. Standard approaches to
classifying nouns, in terms of an &amp;quot;is-a&amp;quot; hierarchy,
have proven hard to apply to unrestricted language.
Is-a hierarchies are expensive to acquire by hand for
anything but highly restricted domains, while
attempts to automatically derive these hierarchies
from existing dictionaries have been only partially
successful (Chodorow, Byrd, and Heidom 1985).
This paper describes an approach to classifying
English words according to the predicate-argument
structures they show in a corpus of text. The general
idea is straightforward: in any natural language there
are restrictions on what words can appear together in
the same construction, and in particular, on what can
be arguments of what predicates. For nouns, there is
a restricted set of verbs that it appears as subject of
or object of. For example, wine may be drunk,
produced, and sold but not pruned. Each noun may
therefore be characterized according to the verbs that
it occurs with. Nouns may then be grouped
according to the extent to which they appear in
similar environments.
This basic idea of the distributional foundation of
meaning is not new. Harris (1968) makes this
&amp;quot;distributional hypothesis&amp;quot; central to his linguistic
theory. His claim is that: &amp;quot;the meaning of entities,
and the meaning of grammatical relations among
them, is related to the restriction of combinations of
these entities relative to other entities.&amp;quot; (Harris
1968:12). Sparck Jones (1986) takes a similar view.
It is however by no means obvious that the
distribution of words will directly provide a useful
semantic classification, at least in the absence of
considerable human intervention. The work that has
been done based on Harris&apos; distributional hypothesis
(most notably, the work of the associates of the
Linguistic String Project (see for example,
Hirschman, Grishrnan, and Sager 1975))
unfortunately does not provide a direct answer, since
the corpora used have been small (tens of thousands
of words rather than millions) and the analysis has
typically involved considerable intervention by the
researchers. The stumbling block to any automatic
use of distributional patterns has been that no
sufficiently robust syntactic analyzer has been
available.
This paper reports an investigation of automatic
distributional classification of words in English,
using a parser developed for extracting grammatical
structures from unrestricted text (Hindle 1983). We
propose a particular measure of similarity that is a
function of mutual information estimated from text.
On the basis of a six million word sample of
Associated Press news stories, a classification of
nouns was developed according to the predicates
they occur with. This purely syntax-based similarity
measure shows remarkably plausible semantic
relations.
</bodyText>
<sectionHeader confidence="0.999455" genericHeader="method">
2. ANALYZING THE CORPUS
</sectionHeader>
<bodyText confidence="0.981248">
A 6 million word sample of Associated Press
news stories was analyzed, one sentence at a time,
</bodyText>
<page confidence="0.97756">
258
</page>
<figure confidence="0.991594555555556">
COM
SEAR
NP
VP
N&apos; COMP NP AUX
1 1 I
C PRO TNS VS PRO , CN Q D NPL PRO TNS V PRO TNS VS D
1 1 1 1 1 1 1 1 1 1 1 1 I
the land that t * sustains us . and many of the products we use are the result
</figure>
<figureCaption confidence="0.999956">
Figure 1. Parser output for a fragment of sentence (1),
</figureCaption>
<bodyText confidence="0.988180125">
by a deterministic parser (Fidditch) of the sort
originated by Marcus (1980). Fidditch provides
a single syntactic analysis -- a tree or sequence
of trees -- for each sentence; Figure 1 shows part
of the output for sentence (1).
(1) The clothes we wear, the food we eat, the
air we breathe, the water we drink, the land that
sustains us, and many of the products we use are
the result of agricultural research. (March 22
1987)
The parser aims to be non-committal when it is
unsure of an analysis. For example, it is
perfectly willing to parse an embedded clause
and then leave it unattached. If the object or
subject of a clause is not found, Fidditch leaves
it empty, as in the last two clauses in Figure 1.
This non-committal approach simply reduces the
effective size of the sample.
The aim of the parser is to produce an
annotated surface structure, building constituents
as large as it can, and reconstructing the
underlying clause structure when it can. In
sentence (1), six clauses are found. Their
predicate-argument information may be coded as
a table of 5-tupks, consisting of verb, surface
subject, surface object, underlying subject,
underlying object, as shown in Table 1. In the
subject-verb-object table, the root form of the
head of phrases is recorded, and the deep subject
and object are used when available. (Noun
phrases of the form a n1 of n2 are coded as n1
n2; an example is the first entry in Table 2).
</bodyText>
<tableCaption confidence="0.8321045">
Table 1. Predicate-argument relations found
in an AP news sentence (1).
</tableCaption>
<bodyText confidence="0.99981605">
The parser&apos;s analysis of sentence (1) is far
fio.n perfect: the object of wear is not found, the
object of use is not found, and the single element
land rather than the conjunction of clothes, food,
air, water, land, products is taken to be the
subject of be. Despite these errors, the analysis
is succeeds in discovering a number of the
correct predicate-argument relations. The
parsing errors that do occur seem to result, for
the current purposes, in the omission of
predicate-argument relations, rather than their
misidentification. This makes the sample less
effective than it might he, but it is not in general
misleading. (It may also skew the sample to the
extent that the parsing errors are consistent.)
The analysis of the 6 million word 1987 AP
sample yields 4789 verbs in 274613 clausal
structures, and 26742 head nouns. This table of
predicate-argument relations is the basis of our
similarity metric.
</bodyText>
<figure confidence="0.994908">
verb
wear
eat
breathe
drink
sustain
use
be
subject object
surface deep surface deep
we Otrace food
we °trace air
we °trace water
we land us
Otrace result
we
land
</figure>
<page confidence="0.997247">
269
</page>
<sectionHeader confidence="0.997373" genericHeader="method">
3. TYPICAL ARGUMENTS
</sectionHeader>
<bodyText confidence="0.999941">
For any of verb in the sample, we can ask
what nouns it has as subjects or objects. Table 2
shows the objects of the verb drink that occur
(more than once) in the sample, in effect giving
the answer to the question &amp;quot;what can you drink?&amp;quot;
</bodyText>
<tableCaption confidence="0.989918">
Table 2. Objects of the verb drink.
</tableCaption>
<table confidence="0.417258076923077">
OBJECT COUNT WEIGHT
bunch beer 2 12.34
tea 4 11.75
Pepsi 2 11.75
champagne 4 11.75
liquid 2 10.53
beer 5 10.20
wine 2 9.34
water 7 7.65
anything 3 5.15
much 3 2.54
it 3 1.25
&lt;SOME AMOUNT&gt; 2 1.22
</table>
<bodyText confidence="0.999854294117647">
This list of drinkable things is intuitively
quite good. The objects in Table 2 are ranked
not by raw frequency, but by a cooccurrence
score listed in the last column. The idea is that,
in ranking the importance of noun-verb
associations, we are interested not in the raw
frequency of cooccurrence of a predicate and
argument, but in their frequency normalized by
what we would expect. More is to be learned
from the fact that you can drink wine than from
the fact that you can drink it even though there
are more clauses in our sample with it as an
object of drink than with wine. To capture this
intuition, we turn, following Church and Hanks
(1989), to &amp;quot;mutual information&amp;quot; (see Fano 1961).
The mutual information of two events /(x y)
is defined as follows:
</bodyText>
<equation confidence="0.9985455">
P(x y)
1(x y) = log2 p(x) p(y)
</equation>
<bodyText confidence="0.999973777777778">
where P(x y) is the joint probability of events x
and y, and P(x) and P(y) are the respective
independent probabilities. When the joint
probability P(x y) is high relative to the product
of the independent probabilities, f is positive;
when the joint probability is relatively low, f is
negative. We use the observed frequencies to
derive a cooccurrence score Cab; (an estimate of
mutual information) defined as follows.
</bodyText>
<equation confidence="0.981283333333333">
f(n v)
,,,1(n v) = log2 f(n) f(v)
N N
</equation>
<bodyText confidence="0.9997434">
where fin v) is the frequency of noun n occurring
as object of verb v, fin) is the frequency of the
noun n occurring as argument of any verb, f(v) is
the frequency of the verb v, and N is the count
of clauses in the sample. (C„,hi(n v) is defined
analogously.)
Calculating the cooccurrence weight for
drink, shown in the third column of Table 2,
gives us a reasonable ranking of terms, with it
near the bottom.
</bodyText>
<subsectionHeader confidence="0.834745">
Multiple Relationships
</subsectionHeader>
<bodyText confidence="0.999805533333333">
For any two nouns in the sample, we can ask
what verb contexts they share. The distributional
hypothesis is that nouns are similar to the extent
that they share contexts. For example, Table 3
shows all the verbs which wine and beer can be
objects of, highlighting the three verbs they have
in common. The verb drink is the key common
factor. There are of course many other objects
that can be sold, but most of them are less alike
than wine or beer because they can&apos;t also be
drunk. So for example, a car is an object that
you can have and sell, like wine and beer, but
you do not — in this sample (confirming what we
know from the meanings of the words) --
typically drink a car.
</bodyText>
<sectionHeader confidence="0.992241" genericHeader="method">
4. NOUN SIMILARITY
</sectionHeader>
<bodyText confidence="0.971113823529412">
We propose the following metric of
similarity, based on the mutual information of
verbs and arguments. Each noun has a set of
verbs that it occurs with (either as subject or
object), and for each such relationship, there is a
mutual information value. For each noun and
verb pair, we get two mutual information values,
for subject and object,
n) and C j(v nj)
We define the object similarity of two nouns
with respect to a verb in terms of the minimum
shared COOCCCUrrenCe weights, as in (2).
The subject similarity of two nouns, SfM,1,
is defined analogously.
Now define the overall similarity of two
nouns as the sum across all verbs of the object
similarity and the subject similarity, as in (3).
</bodyText>
<page confidence="0.981072">
270
</page>
<table confidence="0.98234475">
(2) Object similarity.
„( v n i) , Cc,b)(vink)), if Cobi(v,ni) &gt; 0 and r
_ohj(vink) &gt; 0
= {17Lin(C if Coki(v,n)) &lt;0 and C pki(V fr) &lt;0
Slbfa,i(Vinink) abs (raux(Cobj(vinj) , Ca,j(vink))),
0, otherwise
(3) Noun similarity.
S/M(n1n2) E + Saf0b1(vrnIn2)
</table>
<bodyText confidence="0.996029863636364">
The metric of similarity in (2) and (3) is but
one of many that might be explored, but it has
some useful properties. Unlike an inner product
measure, it is guaranteed that a noun will be
most similar to itself. And unlike cosine
distance, this metric is roughly proportional to
the number of different verb contexts that are
shared by two nouns.
Using the definition of similarity in (3), we
can begin to explore nouns that show the
greatest similarity. Table 4 shows the ten nouns
most similar to boat, according to our similarity
metric. The first column lists the noun which is
similar to boat. The second column in each
table shows the number of instances that the
noun appears in a predicate-argument pair
(including verb environments not in the list in
the fifth column). Tie third column is the
number of distinct verb environments (either
subject or object) that the noun occurs in which
are shared with the target noun of the table.
Thus, boat is found in 79 verb environment. Of
these, ship shares 25 common environments
(ship also occurs in many other unshared
environments). The fourth column is the
measure of similarity of the noun with the target
noun of the table, SIM(n n 2), as defined above.
The fifth column shows the common verb
environments, ordered by cooccurrence score,
C(vi ni), as defined above. An underscore
before the verb indicates that it is a subject
environment; a following underscore indicates an
object environment. In Table 4, we see that boat
is a subject of cruise, and object of sink. In the
list for boat, in column five, cruise appears
earlier in the list than carry because cruise has a
higher cooccurrence score. A before a verb
means that the cooccurrence score is negative --
i.e. the noun is less likely to occur in that
argument context than expected.
For many nouns, encouragingly appropriate
sets of semantically similar nouns are found.
Thus, of the ten nouns most similar to boat
(Table 4), nine are words for vehicles; the most
</bodyText>
<tableCaption confidence="0.998125">
Table 3. Verbs taking wine and beer as objects.
</tableCaption>
<table confidence="0.97716852">
VERB wine beer
drug count weight count weight
2 12.26
sit around 1 10.29
smell 1 10.07
contaminate 1 9.75
rest 2 9.56
drink 2 9.34 5 10.20
rescue 1 7.07
purchase 6.79
lift 6.72
prohibit 6.69
love 1 6.33
deliver 1 5.82
buy 3 5.44
name 1 5.42
keep 2 4.86
offer 1 4.13
begin 1 4.09
allow 1 3.90
be on 1 3.79
sell 4.21 1 3.75
&apos;s 2 2.84
make 1 1.27
have 1 0.84 2 1.38
</table>
<bodyText confidence="0.997450333333333">
similar noun is the near-synonym ship. The ten
nouns most similar to treaty (agreement, plan,
constitution, contract, proposal, accord,
amendment, rule, law, legislation) seem to make
up a cluster involving the notions of agreement
and rule. Table 5 shows the ten nouns most
similar to legislator, again a fairly coherent set.
Of course, not all nouns fall into such neat
clusters: Table 6 shows a quite heterogeneous
group of nouns similar to table, though even
here the most similar word (floor) is plausible.
We need, in further work, to explore both
automatic and supervised means of
discriminating the semantically relevant
associations from the spurious.
</bodyText>
<page confidence="0.999105">
271
</page>
<tableCaption confidence="0.999841">
Table 4. Nouns similar to boat.
</tableCaption>
<table confidence="0.973904230769231">
Noun f(n) verbs SIM
boat 153 79 370.16
ship 353 25 79.02
plane 445 26 68.85
bus 104 20 64.49
jet 153 17 62.77
vessel 172 18 57.14
truck 146 21 56.71
car 414 24 52.22
helicopter 151 14 50.66
ferry 37 10 39.76
man 1396 30 38.31
Verbs
</table>
<construct confidence="0.498023390243903">
cruise, keel_, _plow, sink_, drift, step off_, step from_, dock_,
right_, submerge_, _near, hoist_, _intercept, charter_, stay on,
buzz_, stabilize_, _sit on, intercept_, hijack_, park_, be from,
rock_, get off_, board, miss_, stay with_, _catch, yield_, bring in_
seize_, pull, grab_, _hit, exclude„ weigh_, _issue, demonstrate_,
_force, cover, supply_, name, _attack, damage, launcb_,
_provide, appear, _carry, _go to, look at_, attack_, _reach, be on,
watch_, use, retum_, _ask, destroy_, fire _, be on_, describe,
charge_, include, be in_, report, identify_, expect_, cause_, &apos;s_,
_&apos;s, _take, _make, -be_, -say_, -give_, see_, be,-have_, -get_
_near, charter_, hijack_, get off_, buzz_, intercept_, board_,
damage, sink_, seize_, _carry, attack_, -have_, be on, _hit,
destroy_, watch_, _go to, -give_, _ask, The, be on_, -say_,
identify_, see_
hijack_, intercept_, charter_, board_, get off_, _near, _attack,
_carry, seize, -have_, _be on, catch, destroy_, bit, be on_,
damage_, use, -be, _go to, _reach, -say_, identify_, _provide,
expect_, cause_, see_
step off_, hijack_, park_, get off_, board, _catch, seize_, _carry,
attack_, be on, be on_, charge_, expect_, -have_, Jake, -say_,
_make, include_, be in_, -_be
charter, intercept, hijack_, park_, board_ , _hit, seize, _attack,
_force, _carry, use_, describe_, include, _ be on, be, _make,
say_
right, dock_, intercept, sink_, seize_, catch, _attack, _carry,
attack_, -have_, describe_, identify, use, report, -be_,
expect_, -give_
park_, intercept_, stay with_, _be from, _hit, seize, damage_,
_carry, _reach, use_, return_, destroy_, attack_, be, be in_, _take,
-have_, -say_, _make, include, see_
step from_, park_, board, bit, catch, pull, carry, damage„
destroy_, watch_, miss, retum_, -give„ -be, be, be in, -have_
-say_, charge_, _&apos;s, identify_, see_, _take, -get_
hijack_, park_, board_, bring in_, _catch, _attack, watch_, use,
retum_, fire_, _be on, include_, _make, -_be
dock_, sink_, board_, pull_, _carry, use_, be ort, cause, _take,
say_
hoist, bring in_, stay with_, _attack, grab, exclude_, _catch,
charge„ -have_, identify_, describe_, -give, _be from, appear_,
_go to, carry, _reach, _take, pull, bit, -get, &apos;s_, attack_, cause_,
make - be see cover, name ask
</construct>
<page confidence="0.998725">
272
</page>
<tableCaption confidence="0.99964">
Table 5. Nouns simliar to legislator.
</tableCaption>
<table confidence="0.99994875">
Noun &apos;In) verbs SIM
legislator 45 35 165.85
Senate 366 11 40.19
committee 697 20 39.97
organization 351 16 34.29
commission 389 17 34.28
legislature 86 12 34.12
delegate 132 13 33.65
lawmaker 176 14 32.78
panel 253 12 31.23
Congress 827 15 31.20
side 327 15 30.00
</table>
<tableCaption confidence="0.895402">
Table 6. Nouns similar to table.
</tableCaption>
<table confidence="0.381656">
Verbs
</table>
<tableCaption confidence="0.902019148148148">
cajole_, _thump, _grasp, convince_, inform_, address_, _vote,
_predict, _address, _withdraw, _adopt, _approve, criticize,
criticize, represent, reach, write, _reject, _accuse, supporta, go
to_, _consider, _win, pay_, allow_, tell, _hold, call_, kill, _call,
give_, _get, say_, _take, be
_vote, address_, _approve, inform_, _reject, go to_, _consider,
_adopt, tell_, be, give_
_vote, _approve, go to_, inform_, _reject, tell_, be, convince_,
_hold, address, _consider, _address, _adopt, call_,
allow_, support, _accuse, give_, _call
_adopt, inform_, address, go to_, _predict, support, _reject,
represent, _call, _approve, be, allow_, _take, say_, _hold, tell_
_reject, _vote, criticize_, convince_, inform_, allow_, _accuse,
address, _adopt, be, _hold, _approve, give_, go to_, tell_,
consider, pay_
convince_, _approve, criticize, _vote, _address, _hold, _consider,
call_, give_, say_, _take
_vote, inform_, _approve, _adopt, allow_, _reject, consider,
_reach, te11_, give, be, _call, say_
criticize, _approve, _vote, _predict, tell_, _reject, _accuse, be,
call, give_, _consider, _win, _get, _take
_vote, _approve, convince_, te11_, _reject, _adopt, _criticize,
consider, be, _hold, give_, _reach
inform_, _approve, _vote, tell_, _consider, convince_, go to_,
address, give_, criticize_, _address, _reach, _adopt, _hold
_reach, _predict, criticize_, _withdraw, _consider, go to_, _hold,
be, _accuse, support_, represent, tell, give_, allow_, _take
</tableCaption>
<table confidence="0.999830083333333">
Noun flu) verbs SIM
table 66 30 181.43
floor 94 6 30.01
farm 80 8 22.94
scene 135 10 20.85
America 156 7 19.68
experience 129 5 19.04
river 95 4 18.73
town 195 6 18.68
side 327 8 18.57
hospital 190 7 18.10
House 453 6 17.84
</table>
<subsectionHeader confidence="0.744286">
Verbs
</subsectionHeader>
<bodyText confidence="0.980148125">
hide beneath„ convolute, memorize_, sit at, sit across_, redo_,
structure_, sit around, litter_, _carry, lie on_, go from_, _hold,
wait_, come to_, return to, turn_, approach_, cover, be on_,
share_, publish_, claim_, mean_, go to_, raise, leave_, -have_,
do_, be
litter, lie on, cover_, be on_, come to_, go to_
_carry, be on_, cover, return to_, tum_, go to_, leave_, -have_
approach_, return to_, mean_, go to_, be on_, tum_, come to_,
leave, do_, be
go from_, come to, return to_, claim_, go to, -have_, do_
structure, share_, claim_, publish_, be
sit across_, mean_, be on, leave_
litter_, approach_, go to_, return to_, come to_, leave_
lie on_, be on, go to_, _hold, -have_, cover_, leave_, come to_
go from_, come to_, cover_, return to, go to_, leave_, -have_
return to, claim_, come to_, go to, cover, leave_
</bodyText>
<page confidence="0.995854">
273
</page>
<bodyText confidence="0.975740533333333">
Reciprocally most similar nouns
We can define &amp;quot;reciprocally most similar&amp;quot;
nouns or &amp;quot;reciprocal nearest neighbors&amp;quot; (RNN)
as two nouns which are each other&apos;s most
similar noun. This is a rather stringent
definition; under this definition, boat and ship do
not qualify because, while ship is the most
similar to boat, the word most similar to ship is
not boat but plane (boat is second). For a
sample of all the 319 nouns of frequency greater
than 100 and less than 200, we asked whether
each has a reciprocally most similar noun in the
sample. For this sample, 36 had a reciprocal
nearest neighbor. These are shown in Table 7
(duplicates are shown only once).
</bodyText>
<tableCaption confidence="0.9603245">
Table 7. A sample of reciprocally nearest
neighbors.
</tableCaption>
<table confidence="0.9534084">
RNN word counts
bomb device (192 101)
ruling decision (192 761)
street road (188 145)
protest strike (187 254)
list field (184 104)
debt deficit (183 351)
guerrilla rebel (180 314)
fear - concern (176 355)
higher lower (175 78)
freedom - right (164 609)
battle fight (163 131)
jet plane (153 445)
shot bullet (152 35)
truck - car (146 414)
researcher - scientist (142 112)
peace stability (133 64)
property land (132 119)
star editor (131 85)
trend pattern (126 58)
quake - earthquake (126 120)
economist analyst (120 318)
remark comment (115 385)
data information (115 505)
explosion blast (115 52)
tie relation (114 251)
protester demonstrator (11099)
college school (109 380)
radio IRNA (107 18)
2 - 3 (105 90)
</table>
<bodyText confidence="0.997197090909091">
The list in Table 7 shows quite a good set of
substitutable words, many of which are near
synonyms. Some are not synonyms but are
nevertheless closely related: economist - analyst,
2 - 3. Some we recognize as synonyms in news
reporting style: explosion - blast, bomb - device,
tie - relation. And some are hard to interpret. Is
the close relation between star and editor some
reflection of news reporters&apos; world view? Is list
most like field because neither one has much
meaning by itself?
</bodyText>
<sectionHeader confidence="0.99975" genericHeader="conclusions">
5. DISCUSSION
</sectionHeader>
<bodyText confidence="0.999754604651163">
Using a similarity metric derived from the
distribution of subjects, verbs and objects in a
corpus of English text, we have shown the
plausibility of deriving semantic relatedness from
the distribution of syntactic forms. This
demonstration has depended on: 1) the
availability of relatively large text corpora; 2) the
existence of parsing technology that, despite a
large error rate, allows us to find the relevant
syntactic relations in unrestricted text; and 3)
(most important) the fact that the lexical
relations involved in the distribution of words in
syntactic structures are an extremely strong
linguistic constraint.
A number of issues will have to be
confronted to further exploit these structurally-
mediated lexical constraints, including:
Potysemy. The analysis presented here does
not distinguish among related senses of the
(orthographically) same word. Thus, in the table
of words similar to table, we find at least two
distinct senses of table conflated; the table one
can hide beneath is not the table that can be
commuted or memorized. Means of separating
senses need to be developed.
Empty words. Not all nouns are equally
contentful. For example, section is a general
word that can refer to sections of all sorts of
things. As a result, the ten words most similar
to section (school, building, exchange, book,
house, ship, some, headquarter, industry, office)
are a semantically diverse list of words. The
reason is clear: section is semantically a rather
empty word, and the selectional restrictions on
its cooccurence depend primarily on its
complement. You might wad a section of a
book but not., typically, a section of a house. It
would be possible to predetermine a set of empty
words in advance of analysis, and thus avoid
some of the problem presented by empty words.
But it is unlikely that the class is well-defined.
Rather, we expect that nouns could be ranked, on
the basis of their distribution, according to how
</bodyText>
<page confidence="0.991439">
274
</page>
<bodyText confidence="0.999972288888889">
empty they are; this is a matter for further
exploration.
Sample size. The current sample is too
small; many words occur too infrequently to be
adequately sampled, and it is easy to think of
usages that are not represented in the sample.
For example, it is quite expected to talk about
brewing beer, but the pair of brew and beer does
not appear in this sample. Part of the reason for
missing selectional pairs is surely the restricted
nature of the AP news sublangu age.
Further analysis. The similarity metric
proposed here, based on subject-verb-object
relations, represents a considerable reduction in
the information available in the subjec-verb-
object table. This reduction is useful in that it
permits, for example, a clustering analysis of the
nouns in the sample, and for some purposes
(such as demonstrating the plausibility of the
distribution-based metric) such clustering is
useful. However, it is worth noting that the
particular information about, for example, which
nouns may be objects of a given verb, should not
be discarded, and is in itself useful for analysis
of text.
In this study, we have looked only at the
lexical relationship between a verb and the head
nouns of its subject and object. Obviously, there
are many other relationships among words -- for
example, adjectival modification or the
possibility of particular prepositional adjuncts --
that can be extracted from a corpus and that
contribute to our lexical knowledge. It will be
useful to extend the analysis presented here to
other kinds of relationships, including more
complex kinds of verb complementation, noun
complementation. and modification both
preceding and following the head noun. But in
expanding the number of different structural
relations noted, it may become less useful to
compute a single-dimensional similarity score of
the sort proposed in Section 4. Rather, the
various lexical relations revealed by parsing a
corpus, will be available to be combined in many
different ways yet to be explored.
</bodyText>
<sectionHeader confidence="0.998548" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999708540540541">
Chodorow, Martin S., Roy J. Byrd, and George
E. Heidom. 1985. Extracting semantic
hierarchies from a large on-line dictionary.
Proceedings of the 23rd Annual Meeting
of the ACL, 299-304.
Church, Kenneth. 1988. A stochastic parts
program and noun phrase parser for
unrestricted text. Proceedings of the second
ACL Conference on Applied Natural
Language Processing,
Church, Kenneth and Patrick Hanks. 1989. Word
association norms, mutual information and
lexicography. Proceedings of the 23rd
Annual Meeting of the ACL, 76-83.
Fano, R. 1961. Transmission of Information.
Cambridge, Mass:M1T Press.
Harris, Zelig S. 1968. Mathematical Structures of
Language. New York: Wiley.
Hindle, Donald. 1983. User manual for Fidditch.
Naval Research Laboratory Technical
Memorandum #7590-142.
Hirschman, Lynette. 1985. Discovering
sublanguage structures, in Grishrnan, Ralph
and Richard Kittredge, eds. Analyzing
Language in Restricted Domains, 211-234.
Lawrence Erlbaum: llillsdale, NJ.
Hirschman, Lynette, Ralph Grishman, and Naomi
Sager. 1975. Grammatically-based
automatic word class formation.
Information Processing and Management,
11, 39-57.
Marcus, Mitchell P. 1980. A Theory of Syntactic
Recognition for Natural Language. MET
Press.
Sparck Jones, Karen. 1986. Synomyny and
Semantic Classification. Edinburgh
University Press.
</reference>
<page confidence="0.998437">
275
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986731">
<title confidence="0.999003">NOUN CLASSIFICATION FROM PREDICATE-ARGUMENT STRUCTURES</title>
<author confidence="0.999495">Donald Hindle</author>
<affiliation confidence="0.999948">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.999791">600 Mountain Avenue Murray Hill, NI 07974</address>
<abstract confidence="0.998825">A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Martin S Chodorow</author>
<author>Roy J Byrd</author>
<author>George E Heidom</author>
</authors>
<title>Extracting semantic hierarchies from a large on-line dictionary.</title>
<date>1985</date>
<booktitle>Proceedings of the 23rd Annual Meeting of the ACL,</booktitle>
<pages>299--304</pages>
<marker>Chodorow, Byrd, Heidom, 1985</marker>
<rawString>Chodorow, Martin S., Roy J. Byrd, and George E. Heidom. 1985. Extracting semantic hierarchies from a large on-line dictionary. Proceedings of the 23rd Annual Meeting of the ACL, 299-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>Proceedings of the second ACL Conference on Applied Natural Language Processing,</booktitle>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth. 1988. A stochastic parts program and noun phrase parser for unrestricted text. Proceedings of the second ACL Conference on Applied Natural Language Processing,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1989</date>
<booktitle>Proceedings of the 23rd Annual Meeting of the ACL,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="7853" citStr="Church and Hanks (1989)" startWordPosition="1307" endWordPosition="1310">s is intuitively quite good. The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column. The idea is that, in ranking the importance of noun-verb associations, we are interested not in the raw frequency of cooccurrence of a predicate and argument, but in their frequency normalized by what we would expect. More is to be learned from the fact that you can drink wine than from the fact that you can drink it even though there are more clauses in our sample with it as an object of drink than with wine. To capture this intuition, we turn, following Church and Hanks (1989), to &amp;quot;mutual information&amp;quot; (see Fano 1961). The mutual information of two events /(x y) is defined as follows: P(x y) 1(x y) = log2 p(x) p(y) where P(x y) is the joint probability of events x and y, and P(x) and P(y) are the respective independent probabilities. When the joint probability P(x y) is high relative to the product of the independent probabilities, f is positive; when the joint probability is relatively low, f is negative. We use the observed frequencies to derive a cooccurrence score Cab; (an estimate of mutual information) defined as follows. f(n v) ,,,1(n v) = log2 f(n) f(v) N N </context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Church, Kenneth and Patrick Hanks. 1989. Word association norms, mutual information and lexicography. Proceedings of the 23rd Annual Meeting of the ACL, 76-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fano</author>
</authors>
<title>Transmission of Information.</title>
<date>1961</date>
<publisher>Press.</publisher>
<location>Cambridge, Mass:M1T</location>
<contexts>
<context position="7894" citStr="Fano 1961" startWordPosition="1315" endWordPosition="1316">e ranked not by raw frequency, but by a cooccurrence score listed in the last column. The idea is that, in ranking the importance of noun-verb associations, we are interested not in the raw frequency of cooccurrence of a predicate and argument, but in their frequency normalized by what we would expect. More is to be learned from the fact that you can drink wine than from the fact that you can drink it even though there are more clauses in our sample with it as an object of drink than with wine. To capture this intuition, we turn, following Church and Hanks (1989), to &amp;quot;mutual information&amp;quot; (see Fano 1961). The mutual information of two events /(x y) is defined as follows: P(x y) 1(x y) = log2 p(x) p(y) where P(x y) is the joint probability of events x and y, and P(x) and P(y) are the respective independent probabilities. When the joint probability P(x y) is high relative to the product of the independent probabilities, f is positive; when the joint probability is relatively low, f is negative. We use the observed frequencies to derive a cooccurrence score Cab; (an estimate of mutual information) defined as follows. f(n v) ,,,1(n v) = log2 f(n) f(v) N N where fin v) is the frequency of noun n o</context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>Fano, R. 1961. Transmission of Information. Cambridge, Mass:M1T Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig S Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="2123" citStr="Harris (1968)" startWordPosition="317" endWordPosition="318">s straightforward: in any natural language there are restrictions on what words can appear together in the same construction, and in particular, on what can be arguments of what predicates. For nouns, there is a restricted set of verbs that it appears as subject of or object of. For example, wine may be drunk, produced, and sold but not pruned. Each noun may therefore be characterized according to the verbs that it occurs with. Nouns may then be grouped according to the extent to which they appear in similar environments. This basic idea of the distributional foundation of meaning is not new. Harris (1968) makes this &amp;quot;distributional hypothesis&amp;quot; central to his linguistic theory. His claim is that: &amp;quot;the meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities.&amp;quot; (Harris 1968:12). Sparck Jones (1986) takes a similar view. It is however by no means obvious that the distribution of words will directly provide a useful semantic classification, at least in the absence of considerable human intervention. The work that has been done based on Harris&apos; distributional hypothesis (most notably, the work</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Harris, Zelig S. 1968. Mathematical Structures of Language. New York: Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>User manual for Fidditch.</title>
<date>1983</date>
<journal>Naval Research Laboratory Technical Memorandum</journal>
<pages>7590--142</pages>
<contexts>
<context position="3398" citStr="Hindle 1983" startWordPosition="504" endWordPosition="505">ample, Hirschman, Grishrnan, and Sager 1975)) unfortunately does not provide a direct answer, since the corpora used have been small (tens of thousands of words rather than millions) and the analysis has typically involved considerable intervention by the researchers. The stumbling block to any automatic use of distributional patterns has been that no sufficiently robust syntactic analyzer has been available. This paper reports an investigation of automatic distributional classification of words in English, using a parser developed for extracting grammatical structures from unrestricted text (Hindle 1983). We propose a particular measure of similarity that is a function of mutual information estimated from text. On the basis of a six million word sample of Associated Press news stories, a classification of nouns was developed according to the predicates they occur with. This purely syntax-based similarity measure shows remarkably plausible semantic relations. 2. ANALYZING THE CORPUS A 6 million word sample of Associated Press news stories was analyzed, one sentence at a time, 258 COM SEAR NP VP N&apos; COMP NP AUX 1 1 I C PRO TNS VS PRO , CN Q D NPL PRO TNS V PRO TNS VS D 1 1 1 1 1 1 1 1 1 1 1 1 I </context>
</contexts>
<marker>Hindle, 1983</marker>
<rawString>Hindle, Donald. 1983. User manual for Fidditch. Naval Research Laboratory Technical Memorandum #7590-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
</authors>
<title>Discovering sublanguage structures,</title>
<date>1985</date>
<booktitle>Analyzing Language in Restricted Domains,</booktitle>
<pages>211--234</pages>
<editor>in Grishrnan, Ralph and Richard Kittredge, eds.</editor>
<marker>Hirschman, 1985</marker>
<rawString>Hirschman, Lynette. 1985. Discovering sublanguage structures, in Grishrnan, Ralph and Richard Kittredge, eds. Analyzing Language in Restricted Domains, 211-234. Lawrence Erlbaum: llillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Ralph Grishman</author>
<author>Naomi Sager</author>
</authors>
<title>Grammatically-based automatic word class formation.</title>
<date>1975</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>11</volume>
<pages>39--57</pages>
<marker>Hirschman, Grishman, Sager, 1975</marker>
<rawString>Hirschman, Lynette, Ralph Grishman, and Naomi Sager. 1975. Grammatically-based automatic word class formation. Information Processing and Management, 11, 39-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MET Press.</publisher>
<contexts>
<context position="4209" citStr="Marcus (1980)" startWordPosition="661" endWordPosition="662">tion of nouns was developed according to the predicates they occur with. This purely syntax-based similarity measure shows remarkably plausible semantic relations. 2. ANALYZING THE CORPUS A 6 million word sample of Associated Press news stories was analyzed, one sentence at a time, 258 COM SEAR NP VP N&apos; COMP NP AUX 1 1 I C PRO TNS VS PRO , CN Q D NPL PRO TNS V PRO TNS VS D 1 1 1 1 1 1 1 1 1 1 1 1 I the land that t * sustains us . and many of the products we use are the result Figure 1. Parser output for a fragment of sentence (1), by a deterministic parser (Fidditch) of the sort originated by Marcus (1980). Fidditch provides a single syntactic analysis -- a tree or sequence of trees -- for each sentence; Figure 1 shows part of the output for sentence (1). (1) The clothes we wear, the food we eat, the air we breathe, the water we drink, the land that sustains us, and many of the products we use are the result of agricultural research. (March 22 1987) The parser aims to be non-committal when it is unsure of an analysis. For example, it is perfectly willing to parse an embedded clause and then leave it unattached. If the object or subject of a clause is not found, Fidditch leaves it empty, as in t</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, Mitchell P. 1980. A Theory of Syntactic Recognition for Natural Language. MET Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>Karen</author>
</authors>
<title>Synomyny and Semantic Classification.</title>
<date>1986</date>
<publisher>Edinburgh University Press.</publisher>
<marker>Jones, Karen, 1986</marker>
<rawString>Sparck Jones, Karen. 1986. Synomyny and Semantic Classification. Edinburgh University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>