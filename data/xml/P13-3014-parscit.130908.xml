<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001678">
<title confidence="0.997962">
Question Analysis for Polish Question Answering
</title>
<author confidence="0.981848">
Piotr Przybyła
</author>
<affiliation confidence="0.970784">
Institute of Computer Science, Polish Academy of Sciences,
</affiliation>
<note confidence="0.467536">
ul. Jana Kazimierza 5, 01-248 Warszawa, Poland,
</note>
<email confidence="0.932116">
P.Przybyla@phd.ipipan.waw.pl
</email>
<sectionHeader confidence="0.991948" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935533333333">
This study is devoted to the problem of
question analysis for a Polish question an-
swering system. The goal of the question
analysis is to determine its general struc-
ture, type of an expected answer and cre-
ate a search query for finding relevant doc-
uments in a textual knowledge base. The
paper contains an overview of available
solutions of these problems, description of
their implementation and presents an eval-
uation based on a set of 1137 questions
from a Polish quiz TV show. The results
help to understand how an environment
of a Slavonic language affects the perfor-
mance of methods created for English.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.942085619047619">
The main motivation for building Question An-
swering (QA) systems is that they relieve a user
of a need to translate his problem to a machine-
readable form. To make it possible, we need to
equip a computer system with an ability to under-
stand requests in a natural language, find answers
in a knowledge base and formulate them in the nat-
ural language. The aim of this paper is to deal with
the first of these steps, i.e. question analysis mod-
ule. It accepts the question as an input and returns
a data structure containing relevant information,
herein called question model. It consists of two
elements: a question type and a search query.
The question type classifies a question to one
of the categories based on its structure. A gen-
eral question type takes one of the following val-
ues: verification (Czy Lee Oswald zabił Johna
Kennedy’ego?, Eng. Did Lee Oswald kill John
Kennedy?), option choosing (Który z nich zabił
Johna Kennedy’ego: Lance Oswald czy Lee Os-
wald?, Eng. Which one killed John Kennedy:
Lance Oswald or Lee Oswald?), named entity
(Kto zabił Johna Kennedy’ego?, Eng. Who killed
John Kennedy?), unnamed entity (Czego u˙zył
Lee Oswald, ˙zeby zabi´c Johna Kennedy’ego?,
Eng. What did Lee Oswald use to kill John
Kennedy?), other name for a given named en-
tity (Jakiego pseudonimu u˙zywał John Kennedy w
trakcie słu˙zby wojskowej?, Eng. What nickname
did John Kennedy use during his military service?)
and multiple entities (Którzy prezydenci Stanów
Zjednoczonych zostali zabici w trakcie kadencji?,
Eng. Which U.S. presidents were assassinated in
office?). There are many others possible, such as
definition or explanation questions, but they re-
quire specific techniques for answer finding and
remain beyond the scope of this work. For exam-
ple, the Question Answering for Machine Read-
ing Evaluation (QA4MRE) competition (Peñas et
al., 2012) included these complex questions (e.g.
What caused X?, How did X happen?, Why did X
happen?). In case of named entity questions, it
is also useful to find its named entity type, cor-
responding to a type of an entity which could be
provided as an answer. A list of possible options,
suited to questions about general knowledge, is
given in Table 1. As some of the categories in-
clude others (e.g. CITY is a PLACE), the goal of
a classifier is to find the narrowest available.
The need for a search query is motivated by
performance reasons. A linguistic analysis ap-
plied to a source text to find the expected answer
is usually resource-consuming, so it cannot be per-
formed on the whole corpus (in case of this exper-
iment 839,269 articles). To avoid it, we transform
the question into the search query, which is sub-
sequently used in a search engine, incorporating a
full-text index of the corpus. As a result we get a
list of documents, possibly related to the question.
Although the query generation plays an auxiliary
role, failure at this stage may lead both to too long
processing times (in case of excessive number of
returned documents) and lack of a final answer (in
</bodyText>
<page confidence="0.940045">
96
</page>
<note confidence="0.596476">
Proceedings of the ACL Student Research Workshop, pages 96–102,
</note>
<address confidence="0.291714">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</address>
<table confidence="0.998984210526316">
Question type Occurrences
NAMED_ENTITY 657
OPTION 28
VERIFICATION 25
MULTIPLE 28
UNNAMED_ENTITY 377
OTHER_NAME 22
PLACE 33
CONTINENT 4
RIVER 11
LAKE 9
MOUNTAIN 4
RANGE 2
ISLAND 5
ARCHIPELAGO 2
SEA 2
CELESTIAL_BODY 8
COUNTRY 52
STATE 7
CITY 52
NATIONALITY 12
PERSON 260
NAME 11
SURNAME 10
BAND 6
DYNASTY 6
ORGANISATION 20
COMPANY 2
EVENT 7
TIME 2
CENTURY 9
YEAR 34
PERIOD 1
COUNT 31
QUANTITY 6
VEHICLE 10
ANIMAL 1
TITLE 38
</table>
<tableCaption confidence="0.913195666666667">
Table 1: The 6 general question types and the 31
named entity types and numbers of their occur-
rences in the test set.
</tableCaption>
<bodyText confidence="0.915783">
case of not returning a relevant document).
</bodyText>
<sectionHeader confidence="0.997264" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999966787878788">
The problem of determination of the general ques-
tion type is not frequent in existing QA solutions,
as most of the public evaluation tasks, such as
the TREC question answering track (Dang et al.,
2007) either provide it explicitly or focus on one
selected type. However, when it comes to named
entity type determination, a proper classification
is indispensable for finding an answer of a desired
type. Some of the interrogative pronouns, such as
gdzie (Eng. where) or kiedy (Eng. when) uniquely
define this type, so the most obvious approach uses
a list of manually defined patterns. For example,
Lee et al. (2005) base solely on such rules, but
need to have 1273 of them. Unfortunately, some
pronouns (i.e. jaki, Eng. what, and który, Eng.
which) may refer to different types of entities. In
questions created with them, such as Który znany
malarz twierdził, ie obci ˛ał sobie ucho? (Eng.
Which famous painter claimed to have cut his
ear?) the question focus (znany malarz, Eng. fa-
mous painter), following the pronoun, should be
analysed, as its type corresponds to a named en-
tity type (a PERSON in this case). Such approach
is applied in a paper by Harabagiu et al. (2001),
where the Princeton WordNet (Fellbaum, 1998)
serves as an ontology to determine foci types. Fi-
nally, one could use a machine learning (ML) ap-
proach, treating the task as a classification prob-
lem. To do that, a set of features (such as occur-
rences of words, beginning pronouns, etc.) should
be defined and extracted from every question. Li
and Roth (2002) implemented this solution, using
as much as 200,000 features, and also evaluated
an influence of taking into account hierarchy of
class labels. ˇCeh and Ojsteršek (2009) used this
approach in a Slovene QA system for closed do-
main (students’ faculty-related questions) with a
SVM (support vector machines) classifier.
The presented problem of question classifica-
tion for Polish question answering is studied in a
paper by Przybyła (2013). The type determination
part presented here bases on that solution, but in-
cludes several improvements.
To find relevant documents, existing QA solu-
tions usually employ one of the widely available
general-purpose search engines, such as Lucene.
Words of the question are interpreted as keywords
and form a boolean query, where all the con-
stituents are considered required. This procedure
suffices only in case of a web-based QA, where
we can rely on a high redundancy of the WWW,
which makes finding a similar expression proba-
ble enough. Such an approach, using the Google
search engine is presented by Brill et al. (2002).
When working with smaller corpora, one needs
to take into account different formulations of the
desired information. Therefore, an initial query
is subject to some modifications. First, some of
the keywords may be dropped from the query;
Moldovan et al. (2000) present 8 different heuris-
tics of selecting them, based on quotation marks,
parts of speech, detected named entities and other
features, whereas Katz et al. (2003) drop terms in
order of increasing IDF. ˇCeh and Ojsteršek (2009)
start term removal from the end of the sentence.
Apart from simplifying the query, its expansion is
</bodyText>
<page confidence="0.997586">
97
</page>
<bodyText confidence="0.99995675">
also possible. For example, Hovy et al. (2000) add
synonyms for each keyword, extracted from Word-
Net while Katz et al. (2003) introduce their inflec-
tional and derivational morphological forms.
</bodyText>
<sectionHeader confidence="0.648608" genericHeader="method">
3 Question analysis
</sectionHeader>
<bodyText confidence="0.999943857142857">
For the purpose of building an open-domain
corpus-based Polish question answering system, a
question analysis module, based on some of the
solutions presented above, has been implemented.
The module accepts a single question in Polish
and outputs a data structure, called a question
model. It includes a general question type, a set
of named entity types (if the general type equals
NAMED_ENTITY) and a Lucene search query. A
set of named entity types, instead of a single one,
is possible as some of the question constructions
are ambiguous, e.g. a Kto? (Eng. Who?) ques-
tion may be answered by a PERSON, COUNTRY,
BAND, etc.
</bodyText>
<subsectionHeader confidence="0.999819">
3.1 Question type classification
</subsectionHeader>
<bodyText confidence="0.991994428571428">
For the question type classification all the tech-
niques presented above are implemented. Pat-
tern matching stage bases on a list of 176 regu-
lar expressions and sets of corresponding question
types. If any of the expressions matches the ques-
tion, its corresponding set of types may be imme-
diately returned at this stage. These expressions
cover only the most obvious cases and have been
created using general linguistic knowledge. The
length of the list arises from some of the features
of Polish, typical for Slavonic languages, i.e. rel-
atively free word order and rich nominal inflec-
tion (Przepiórkowski, 2007). For example one En-
glish pattern Whose ... ? corresponds to 11 Polish
</bodyText>
<construct confidence="0.884798666666667">
patterns (Czyj ... ?, Czyjego ... ?, Czyjemu ... ?,
Czyim ... ?, Czyja ... ?,Czyjej ... ?, Czyja˛ ... ?,
Czyje ... ?, Czyi ... ?, Czyich ... ?, Czyimi ... ?).
</construct>
<bodyText confidence="0.999993259259259">
However, in case of ambiguous interrogative
pronouns, such as jaki (Eng. what) or który
(Eng. which), a further analysis gets necessary
to determine a question focus type. The ques-
tion is annotated using the morphological anal-
yser Morfeusz (Woli´nski, 2006), the tagger PAN-
TERA (Aceda´nski, 2010) and the shallow parser
Spejd (Przepiórkowski, 2008). The first nomi-
nal group after the pronoun is assumed to be a
question focus. The Polish WordNet database
plWordNet (Maziarz et al., 2012) is used to find
its corresponding lexeme. If nothing is found,
the procedure repeats with the current group’s
semantic head until a single segment remains.
Failure at that stage results in returning an UN-
NAMED_ENTITY label, whereas success leads
us to a synset in WordNet. Then, we check
whether its direct and indirect parents (i.e. synsets
connected via hypernymy relations) include one
of the predefined synsets, corresponding to the
available named entity types. The whole proce-
dure is outlined in Figure 1. The error analysis
of this procedure performed in (Przybyła, 2013)
shows a high number of errors caused by a lack
of a word sense disambiguation. A lexeme may
be connected to many synsets, each correspond-
ing to a specific word sense and having a differ-
ent parent list. Among the possible ways to com-
bine them are: intersection (corresponding to us-
ing only the parents common for all word senses),
union (the parents of any word sense), voting (the
parents common for the majority of word senses)
and selecting only the first word sense (which usu-
ally is the most common in the language). The
experiments have shown a better precision of clas-
sification using the first word sense (84.35%) than
other techniques (intersection - 72.00%, union -
80.95%, voting - 79.07%). Experimental details
are provided in the next section.
As an alternative, a machine learning approach
has been implemented. After annotation using the
same tools, we extract the features as a set of root
forms appearing in the question. Only the lem-
mas appearing in at least 3 sentences are used for
further processing. In this way, each sentence is
described with a set of boolean features (420 for
the evaluation set described in next section), de-
noting the appearance of a particular root form.
Additionally, morphological interpretations of the
first five words in the question are also extracted
as features. Two classifiers, implemented in the R
statistical environment, were used: a decision tree
(for human-readable results) and a random forest
(for high accuracy).
</bodyText>
<subsectionHeader confidence="0.996374">
3.2 Query formation
</subsectionHeader>
<bodyText confidence="0.999475428571429">
The basic procedure for creating a query treats
each segment from the question (apart from the
words included in a matched regular expression)
as a keyword of an OR boolean query. No term
weighting or stop-words removal is implemented
as Lucene uses TF/IDF statistic, which penalizes
omnipresent tokens. However, several other im-
</bodyText>
<page confidence="0.989258">
98
</page>
<figure confidence="0.997303043478261">
Which russian submarine sank in 2000 with its whole crew?
Która rosyjska #ódź podwodna zatonę#ą w 2000 roku wraz z ca#ą za#ogą?
Interrogative
pronoun
Question
focus
WordNet search
first nominal group
(rosyjska (#ódź podwodna))
semantic head
No synset
(#ódź podwodna) WordNet search {#ódź podwodna 1}
hypernym
{statek podwodny 1}
submersible ship
hypernym
{statek 1}
ship
hypernym
NAMED_ENTITY
VEHICLE
{środek lokomocji 1, środek transportu 1}
vehicle
</figure>
<figureCaption confidence="0.987463">
Figure 1: Outline of the disambiguation procedure, used to determine named entity type in case of
ambiguous interrogative pronouns (see explanation in text).
</figureCaption>
<bodyText confidence="0.999966111111111">
provements are used. First, we start with a restric-
tive AND query and fall back into OR only in case
it provides no results. A question focus removal
(applied by Moldovan et al. (2000)) requires spe-
cial attention. For example, let us consider again
the question Który znany malarz twierdził, ie ob-
ci ˛ał sobie ucho?. The words of the question fo-
cus znany malarz are not absolutely necessary in
a source document, but their appearance may be
a helpful clue. The query could also be expanded
by replacing each keyword by a nested OR query,
containing synonyms of the keyword, extracted
from plWordNet. Both the focus removal and syn-
onym expansion have been implemented as op-
tions of the presented query formation mechanism.
Finally, one needs to remember about an
important feature of Polish, typical for a
Slavonic language, namely rich nominal inflection
(Przepiórkowski, 2007). It means that the ortho-
graphic forms of nouns change as they appear in
different roles in a sentence. We could either ig-
nore this fact and look for exact matches between
words in the question and a document or allow
some modifications. These could be done by stem-
ming (available for Polish in Lucene, see the de-
scription in (Galambos, 2001)), fuzzy queries (al-
lowing a difference between the keyword and a
document word restricted by a specified Leven-
shtein distance) or a full morphological analysis
and tagging of the source corpus and the query. All
the enumerated possibilities are evaluated in this
study, apart from the last one, requiring a sizeable
amount of computing resources. This problem is
less acute in case of English; most authors (e.g.
Hovy et al. (2000)) use simple (such as Porter’s)
stemmers or do not address the problem at all.
</bodyText>
<sectionHeader confidence="0.998855" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999686923076923">
For the purpose of evaluation, a set of 1137 ques-
tions from a Polish quiz TV show &amp;quot;Jeden z dziesi˛e-
ciu&amp;quot;, published in (Karzewski, 1997), has been
manually reviewed and updated. A general ques-
tion type and a named entity type has been as-
signed to each of the questions. Table 1 presents
the number of question types occurrences in the
test set. As a source corpus, a textual version of the
Polish Wikipedia has been used. To evaluate query
generation an article name has been assigned to
those questions (1057), for which a single article
in Wikipedia containing an answer exists.
Outputs of type classifiers have been gathered
</bodyText>
<page confidence="0.997829">
99
</page>
<table confidence="0.9991238">
Classifier Classified Precision Overall
pattern matching 36.15% 95.37% 34.48%
WordNet-aided 98.33% 84.35% 82.94%
decision tree 100% 67.02% 67.02%
random forest 100% 72.91% 72.91%
</table>
<tableCaption confidence="0.996993">
Table 2: Accuracy of the four question type classi-
</tableCaption>
<bodyText confidence="0.9716432">
fiers: numbers of questions classified, percentages
of correct answers and products of these two.
and compared to the expected ones. The machine
learning classifiers have been evaluated using 100-
fold cross-validation1.
Four of the presented improvements of query
generation tested here include: basic OR query,
AND query with fallback to OR, focus segments
removal and expansion with synonyms. For each
of those, three types of segment matching strate-
gies have been applied: exact, stemming-based
and fuzzy. The recorded results include recall
(percentage of result lists including the desired ar-
ticle among the first 100) and average position of
the article in the list.
</bodyText>
<sectionHeader confidence="0.999813" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.998699458333333">
The result of evaluation of classifiers is presented
in Table 2. The pattern matching stage behaves
as expected: accepts only a small part of ques-
tions, but yields a high precision. The WordNet-
aided focus analysis is able to handle almost all
questions with an acceptable precision. Unfortu-
nately, the accuracy of ML classifiers is not sat-
isfactory, which could be easily explained using
Table 1: there are many categories represented by
very few cases. An expansion of training set or
dropping the least frequent categories (depending
on a particular application) is necessary for better
classification.
Results of considered query generation tech-
niques are shown in Table 3. It turns out that the
basic technique generally yields the best result.
Starting with an AND query and using OR only
in case of a failure leads to an improvement of the
expected article ranking position but the recall ra-
tio drops significantly, which means that quite of-
ten the results of a restrictive query do not include
the relevant article. The removal of the question
focus from the list of keywords also has a nega-
tive impact on performance. The most surprising
</bodyText>
<footnote confidence="0.977518">
1I.e. the whole test set has been divided into 100 nearly
equal subsets and each of them has been classified using the
classifier trained on the remaining 99 subsets.
</footnote>
<table confidence="0.9996953">
Match Stemming Fuzzy
Query❳❳❳Exact
basic 69.97% 80.08% 82.19%
OR query 14.32 12.90 12.36
priority for 57.94% 57.07% 34.84%
AND query 11.36 8.80 7.07
with focus 62.75% 71.99% 73.34%
segments removed 14.65 14.00 12.84
with synonyms 47.06% 65.64% 58.71%
21.42 15.47 16.00
</table>
<tableCaption confidence="0.999034">
Table 3: Results of the four considered query gen-
</tableCaption>
<bodyText confidence="0.994897214285714">
eration techniques, each with the three types of
matching strategy. For each combination a recall
(measured by the presence of a given source docu-
ment in the first 100 returned) and an average po-
sition on the ranked list is given.
results are those of expanding a query with syn-
onyms - the number of matching articles grows
abruptly and Lucene ranking mechanism does not
lead to satisfying selection of the best 100. One
needs to remember that only one article has been
selected for each test question, whereas probably
there are many relevant Wikipedia entries in most
cases. Unfortunately, finding all of them manually
would require a massive amount of time.
</bodyText>
<figure confidence="0.936127785714286">
85
83
81
79
Recall (%) 77
75
73
71
69
67
65
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Query fuzziness
Relative Absolute Fixed prefix
</figure>
<figureCaption confidence="0.998136">
Figure 2: Impact of the fuzziness of queries on
</figureCaption>
<bodyText confidence="0.979309375">
the recall using three types of fuzzy queries. To
show the relative and absolute fuzziness on one
plot, a word-length of 10 letters is assumed. See a
description in text.
We can also notice a questionable impact of the
stemming. As expected, taking into account in-
flection is necessary (cf. results of exact match-
ing), but fuzzy queries provide more accurate re-
</bodyText>
<page confidence="0.974195">
100
</page>
<bodyText confidence="0.999827333333334">
sults, although they use no linguistic knowledge.
As the fuzzy queries yield the best results, an
additional experiment becomes necessary to find
an optimal fuzziness, i.e. a maximal Levenshtein
distance between the matched words. This param-
eter needs tuning for particular language of im-
plementation (in this case Polish) as it reflects a
mutability of its words, caused by inflection and
derivation. Three strategies for specifying the dis-
tance have been used: relative (with distance be-
ing a fraction of a keyword’s length), absolute (the
same distance for all keywords) and with prefix
(same as absolute, but with changes limited to the
end of a keyword; with fixed prefix). In Figure
2 the results are shown - it seems that allowing 3
changes at the end of the keyword is enough. This
option reflects the Polish inflection schemes and is
also very fast thanks to the fixedness of the prefix.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999945807692308">
In this paper a set of techniques used to build a
question model has been presented. They have
been implemented as a question analysis module
for the Polish question answering task. Several ex-
periments using Polish questions and knowledge
base have been performed to evaluate their per-
formance in the environment of the Slavonic lan-
guage. They have led to the following conclu-
sions: firstly, the best technique to find a correct
question type is to combine pattern matching with
the WordNet-aided focus analysis. Secondly, it
does not suffice to process the first 100 article, re-
turned by the search engine using the default rank-
ing procedure, as they may not contain desired
information. Thirdly, the stemmer of Polish pro-
vided by the Lucene is not reliable enough - prob-
ably it would be best to include a full morpholog-
ical analysis and tagging process in the document
indexing process.
This study is part of an effort to build an open-
domain corpus-based question answering system
for Polish. The obvious next step is to create a sen-
tence similarity measure to select the best answer
in the source document. There exist a variety of
techniques for that purpose, but their performance
in case of Polish needs to be carefully examined.
</bodyText>
<sectionHeader confidence="0.996515" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999806333333333">
Critical reading of the manuscript by Agnieszka
Mykowiecka is gratefully acknowledged. Study
was supported by research fellowship within &amp;quot;In-
formation technologies: research and their in-
terdisciplinary applications&amp;quot; agreement number
POKL.04.01.01-00-051/10-00.
</bodyText>
<sectionHeader confidence="0.989822" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998902708333333">
Szymon Aceda´nski. 2010. A morphosyntactic Brill
Tagger for inflectional languages. In Proceedings
of the 7th international conference on Advances in
Natural Language Processing (IceTAL’10 ), pages
3–14.
Eric Brill, Susan Dumais, and Michele Banko. 2002.
An analysis of the AskMSR question-answering sys-
tem. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing
- EMNLP ’02, volume 10, pages 257–264, Morris-
town, NJ, USA, July. Association for Computational
Linguistics.
Hoa Trang Dang, Diane Kelly, and Jimmy Lin. 2007.
Overview of the TREC 2007 Question Answering
track. In Proceedings of The Sixteenth Text RE-
trieval Conference, TREC 2007.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Leo Galambos. 2001. Lemmatizer for Document
Information Retrieval Systems in JAVA. In Pro-
ceedings of the 28th Conference on Current Trends
in Theory and Practice of Informatics (SOFSEM
2001), pages 243–252.
Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada
Mihalcea, Mihai Surdeanu, Razvan Bunescu, Rox-
ana Gîrju, Vasile Rus, and Paul Morarescu. 2001.
The role of lexico-semantic feedback in open-
domain textual question-answering. In Proceedings
of the 39th Annual Meeting on Association for Com-
putational Linguistics - ACL ’01, pages 282–289.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2000. Question Answer-
ing in Webclopedia. In Proceedings of The Ninth
Text REtrieval Conference (TREC 2000).
Marek Karzewski. 1997. Jeden z dziesi˛eciu - pytania i
odpowiedzi. Muza SA.
Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-
des, Gregory Marton, and Federico Mora. 2003.
Integrating Web-based and corpus-based techniques
for question answering. In Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
Changki Lee, Ji-Hyun Wang, Hyeon-Jin Kim, and
Myung-Gil Jang. 2005. Extracting Template for
Knowledge-based Question-Answering Using Con-
ditional Random Fields. In Proceedings of the
28th Annual International ACM SIGIR Workshop on
MFIR, pages 428–434.
</reference>
<page confidence="0.976426">
101
</page>
<reference confidence="0.999033093023256">
Xin Li and Dan Roth. 2002. Learning Question Classi-
fiers. In Proceedings of the 19th International Con-
ference on Computational Linguistics (COLING-
2002), volume 1 of COLING ’02.
Marek Maziarz, Maciej Piasecki, and Stanisław Sz-
pakowicz. 2012. Approaching plWordNet 2.0. In
Proceedings of the 6th Global Wordnet Conference.
Dan Moldovan, Sanda Harabagiu, Marius Pa¸sca, Rada
Mihalcea, Roxana Gîrju, Richard Goodrum, and
Vasile Rus. 2000. The structure and performance
of an open-domain question answering system. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics - ACL ’00, pages
563–570, Morristown, NJ, USA, October. Associa-
tion for Computational Linguistics.
Anselmo Peñas, Eduard H. Hovy, Pamela Forner, Ál-
varo Rodrigo, Richard F. E. Sutcliffe, Caroline
Sporleder, Corina Forascu, Yassine Benajiba, and
Petya Osenova. 2012. QA4MRE: Question An-
swering for Machine Reading Evaluation at CLEF
2012. In CLEF 2012 Evaluation Labs and Work-
shop Online Working Notes.
Adam Przepiórkowski. 2007. Slavonic information
extraction and partial parsing. In Proceedings of the
Workshop on Balto-Slavonic Natural Language Pro-
cessing Information Extraction and Enabling Tech-
nologies - ACL ’07.
Adam Przepiórkowski. 2008. Powierzchniowe
przetwarzanie j˛ezyka polskiego. Akademicka Ofi-
cyna Wydawnicza EXIT, Warszawa.
Piotr Przybyła. 2013. Question classification for Pol-
ish question answering. In Proceedings of the 20th
International Conference of Language Processing
and Intelligent Information Systems (LP&amp;IIS 2013).
Ines ˇCeh and Milan Ojsteršek. 2009. Developing a
question answering system for the slovene language.
WSEAS Transactions on Information Science and
Applications, 6(9):1533–1543.
Marcin Woli´nski. 2006. Morfeusz — a Practical
Tool for the Morphological Analysis of Polish. In
Mieczysław Kłopotek, Sławomir Wierzcho´n, and
Krzysztof Trojanowski, editors, Intelligent Informa-
tion Processing and Web Mining, pages 511–520.
</reference>
<page confidence="0.998623">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.478607">
<title confidence="0.999101">Question Analysis for Polish Question Answering</title>
<author confidence="0.92305">Piotr</author>
<affiliation confidence="0.925234">Institute of Computer Science, Polish Academy of</affiliation>
<address confidence="0.527283">ul. Jana Kazimierza 5, 01-248 Warszawa,</address>
<email confidence="0.992282">P.Przybyla@phd.ipipan.waw.pl</email>
<abstract confidence="0.9968658125">This study is devoted to the problem of question analysis for a Polish question answering system. The goal of the question analysis is to determine its general structure, type of an expected answer and create a search query for finding relevant documents in a textual knowledge base. The paper contains an overview of available solutions of these problems, description of their implementation and presents an evaluation based on a set of 1137 questions from a Polish quiz TV show. The results help to understand how an environment of a Slavonic language affects the performance of methods created for English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Szymon Aceda´nski</author>
</authors>
<title>A morphosyntactic Brill Tagger for inflectional languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th international conference on Advances in Natural Language Processing (IceTAL’10 ),</booktitle>
<pages>3--14</pages>
<marker>Aceda´nski, 2010</marker>
<rawString>Szymon Aceda´nski. 2010. A morphosyntactic Brill Tagger for inflectional languages. In Proceedings of the 7th international conference on Advances in Natural Language Processing (IceTAL’10 ), pages 3–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Susan Dumais</author>
<author>Michele Banko</author>
</authors>
<title>An analysis of the AskMSR question-answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - EMNLP ’02,</booktitle>
<volume>10</volume>
<pages>257--264</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="7194" citStr="Brill et al. (2002)" startWordPosition="1202" endWordPosition="1205">he type determination part presented here bases on that solution, but includes several improvements. To find relevant documents, existing QA solutions usually employ one of the widely available general-purpose search engines, such as Lucene. Words of the question are interpreted as keywords and form a boolean query, where all the constituents are considered required. This procedure suffices only in case of a web-based QA, where we can rely on a high redundancy of the WWW, which makes finding a similar expression probable enough. Such an approach, using the Google search engine is presented by Brill et al. (2002). When working with smaller corpora, one needs to take into account different formulations of the desired information. Therefore, an initial query is subject to some modifications. First, some of the keywords may be dropped from the query; Moldovan et al. (2000) present 8 different heuristics of selecting them, based on quotation marks, parts of speech, detected named entities and other features, whereas Katz et al. (2003) drop terms in order of increasing IDF. ˇCeh and Ojsteršek (2009) start term removal from the end of the sentence. Apart from simplifying the query, its expansion is 97 also </context>
</contexts>
<marker>Brill, Dumais, Banko, 2002</marker>
<rawString>Eric Brill, Susan Dumais, and Michele Banko. 2002. An analysis of the AskMSR question-answering system. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - EMNLP ’02, volume 10, pages 257–264, Morristown, NJ, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Diane Kelly</author>
<author>Jimmy Lin</author>
</authors>
<title>Overview of the TREC</title>
<date>2007</date>
<booktitle>In Proceedings of The Sixteenth Text REtrieval Conference, TREC</booktitle>
<contexts>
<context position="4805" citStr="Dang et al., 2007" startWordPosition="806" endWordPosition="809">HIPELAGO 2 SEA 2 CELESTIAL_BODY 8 COUNTRY 52 STATE 7 CITY 52 NATIONALITY 12 PERSON 260 NAME 11 SURNAME 10 BAND 6 DYNASTY 6 ORGANISATION 20 COMPANY 2 EVENT 7 TIME 2 CENTURY 9 YEAR 34 PERIOD 1 COUNT 31 QUANTITY 6 VEHICLE 10 ANIMAL 1 TITLE 38 Table 1: The 6 general question types and the 31 named entity types and numbers of their occurrences in the test set. case of not returning a relevant document). 2 Related work The problem of determination of the general question type is not frequent in existing QA solutions, as most of the public evaluation tasks, such as the TREC question answering track (Dang et al., 2007) either provide it explicitly or focus on one selected type. However, when it comes to named entity type determination, a proper classification is indispensable for finding an answer of a desired type. Some of the interrogative pronouns, such as gdzie (Eng. where) or kiedy (Eng. when) uniquely define this type, so the most obvious approach uses a list of manually defined patterns. For example, Lee et al. (2005) base solely on such rules, but need to have 1273 of them. Unfortunately, some pronouns (i.e. jaki, Eng. what, and który, Eng. which) may refer to different types of entities. In questio</context>
</contexts>
<marker>Dang, Kelly, Lin, 2007</marker>
<rawString>Hoa Trang Dang, Diane Kelly, and Jimmy Lin. 2007. Overview of the TREC 2007 Question Answering track. In Proceedings of The Sixteenth Text REtrieval Conference, TREC 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5825" citStr="Fellbaum, 1998" startWordPosition="980" endWordPosition="981">l. (2005) base solely on such rules, but need to have 1273 of them. Unfortunately, some pronouns (i.e. jaki, Eng. what, and który, Eng. which) may refer to different types of entities. In questions created with them, such as Który znany malarz twierdził, ie obci ˛ał sobie ucho? (Eng. Which famous painter claimed to have cut his ear?) the question focus (znany malarz, Eng. famous painter), following the pronoun, should be analysed, as its type corresponds to a named entity type (a PERSON in this case). Such approach is applied in a paper by Harabagiu et al. (2001), where the Princeton WordNet (Fellbaum, 1998) serves as an ontology to determine foci types. Finally, one could use a machine learning (ML) approach, treating the task as a classification problem. To do that, a set of features (such as occurrences of words, beginning pronouns, etc.) should be defined and extracted from every question. Li and Roth (2002) implemented this solution, using as much as 200,000 features, and also evaluated an influence of taking into account hierarchy of class labels. ˇCeh and Ojsteršek (2009) used this approach in a Slovene QA system for closed domain (students’ faculty-related questions) with a SVM (support v</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Galambos</author>
</authors>
<title>Lemmatizer for Document Information Retrieval Systems in JAVA.</title>
<date>2001</date>
<booktitle>In Proceedings of the 28th Conference on Current Trends in Theory and Practice of Informatics (SOFSEM</booktitle>
<pages>243--252</pages>
<contexts>
<context position="14222" citStr="Galambos, 2001" startWordPosition="2353" endWordPosition="2354"> the focus removal and synonym expansion have been implemented as options of the presented query formation mechanism. Finally, one needs to remember about an important feature of Polish, typical for a Slavonic language, namely rich nominal inflection (Przepiórkowski, 2007). It means that the orthographic forms of nouns change as they appear in different roles in a sentence. We could either ignore this fact and look for exact matches between words in the question and a document or allow some modifications. These could be done by stemming (available for Polish in Lucene, see the description in (Galambos, 2001)), fuzzy queries (allowing a difference between the keyword and a document word restricted by a specified Levenshtein distance) or a full morphological analysis and tagging of the source corpus and the query. All the enumerated possibilities are evaluated in this study, apart from the last one, requiring a sizeable amount of computing resources. This problem is less acute in case of English; most authors (e.g. Hovy et al. (2000)) use simple (such as Porter’s) stemmers or do not address the problem at all. 4 Evaluation For the purpose of evaluation, a set of 1137 questions from a Polish quiz TV</context>
</contexts>
<marker>Galambos, 2001</marker>
<rawString>Leo Galambos. 2001. Lemmatizer for Document Information Retrieval Systems in JAVA. In Proceedings of the 28th Conference on Current Trends in Theory and Practice of Informatics (SOFSEM 2001), pages 243–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Mihai Surdeanu</author>
<author>Razvan Bunescu</author>
<author>Roxana Gîrju</author>
<author>Vasile Rus</author>
<author>Paul Morarescu</author>
</authors>
<title>The role of lexico-semantic feedback in opendomain textual question-answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics - ACL ’01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="5779" citStr="Harabagiu et al. (2001)" startWordPosition="972" endWordPosition="975">st of manually defined patterns. For example, Lee et al. (2005) base solely on such rules, but need to have 1273 of them. Unfortunately, some pronouns (i.e. jaki, Eng. what, and który, Eng. which) may refer to different types of entities. In questions created with them, such as Który znany malarz twierdził, ie obci ˛ał sobie ucho? (Eng. Which famous painter claimed to have cut his ear?) the question focus (znany malarz, Eng. famous painter), following the pronoun, should be analysed, as its type corresponds to a named entity type (a PERSON in this case). Such approach is applied in a paper by Harabagiu et al. (2001), where the Princeton WordNet (Fellbaum, 1998) serves as an ontology to determine foci types. Finally, one could use a machine learning (ML) approach, treating the task as a classification problem. To do that, a set of features (such as occurrences of words, beginning pronouns, etc.) should be defined and extracted from every question. Li and Roth (2002) implemented this solution, using as much as 200,000 features, and also evaluated an influence of taking into account hierarchy of class labels. ˇCeh and Ojsteršek (2009) used this approach in a Slovene QA system for closed domain (students’ fa</context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunescu, Gîrju, Rus, Morarescu, 2001</marker>
<rawString>Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana Gîrju, Vasile Rus, and Paul Morarescu. 2001. The role of lexico-semantic feedback in opendomain textual question-answering. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics - ACL ’01, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Laurie Gerber</author>
<author>Ulf Hermjakob</author>
<author>Michael Junk</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Question Answering in Webclopedia.</title>
<date>2000</date>
<booktitle>In Proceedings of The Ninth Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="7835" citStr="Hovy et al. (2000)" startWordPosition="1306" endWordPosition="1309">ller corpora, one needs to take into account different formulations of the desired information. Therefore, an initial query is subject to some modifications. First, some of the keywords may be dropped from the query; Moldovan et al. (2000) present 8 different heuristics of selecting them, based on quotation marks, parts of speech, detected named entities and other features, whereas Katz et al. (2003) drop terms in order of increasing IDF. ˇCeh and Ojsteršek (2009) start term removal from the end of the sentence. Apart from simplifying the query, its expansion is 97 also possible. For example, Hovy et al. (2000) add synonyms for each keyword, extracted from WordNet while Katz et al. (2003) introduce their inflectional and derivational morphological forms. 3 Question analysis For the purpose of building an open-domain corpus-based Polish question answering system, a question analysis module, based on some of the solutions presented above, has been implemented. The module accepts a single question in Polish and outputs a data structure, called a question model. It includes a general question type, a set of named entity types (if the general type equals NAMED_ENTITY) and a Lucene search query. A set of </context>
<context position="14654" citStr="Hovy et al. (2000)" startWordPosition="2422" endWordPosition="2425">matches between words in the question and a document or allow some modifications. These could be done by stemming (available for Polish in Lucene, see the description in (Galambos, 2001)), fuzzy queries (allowing a difference between the keyword and a document word restricted by a specified Levenshtein distance) or a full morphological analysis and tagging of the source corpus and the query. All the enumerated possibilities are evaluated in this study, apart from the last one, requiring a sizeable amount of computing resources. This problem is less acute in case of English; most authors (e.g. Hovy et al. (2000)) use simple (such as Porter’s) stemmers or do not address the problem at all. 4 Evaluation For the purpose of evaluation, a set of 1137 questions from a Polish quiz TV show &amp;quot;Jeden z dziesi˛eciu&amp;quot;, published in (Karzewski, 1997), has been manually reviewed and updated. A general question type and a named entity type has been assigned to each of the questions. Table 1 presents the number of question types occurrences in the test set. As a source corpus, a textual version of the Polish Wikipedia has been used. To evaluate query generation an article name has been assigned to those questions (1057</context>
</contexts>
<marker>Hovy, Gerber, Hermjakob, Junk, Lin, 2000</marker>
<rawString>Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael Junk, and Chin-Yew Lin. 2000. Question Answering in Webclopedia. In Proceedings of The Ninth Text REtrieval Conference (TREC 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marek Karzewski</author>
</authors>
<title>Jeden z dziesi˛eciu - pytania i odpowiedzi. Muza SA.</title>
<date>1997</date>
<contexts>
<context position="14881" citStr="Karzewski, 1997" startWordPosition="2465" endWordPosition="2466">ween the keyword and a document word restricted by a specified Levenshtein distance) or a full morphological analysis and tagging of the source corpus and the query. All the enumerated possibilities are evaluated in this study, apart from the last one, requiring a sizeable amount of computing resources. This problem is less acute in case of English; most authors (e.g. Hovy et al. (2000)) use simple (such as Porter’s) stemmers or do not address the problem at all. 4 Evaluation For the purpose of evaluation, a set of 1137 questions from a Polish quiz TV show &amp;quot;Jeden z dziesi˛eciu&amp;quot;, published in (Karzewski, 1997), has been manually reviewed and updated. A general question type and a named entity type has been assigned to each of the questions. Table 1 presents the number of question types occurrences in the test set. As a source corpus, a textual version of the Polish Wikipedia has been used. To evaluate query generation an article name has been assigned to those questions (1057), for which a single article in Wikipedia containing an answer exists. Outputs of type classifiers have been gathered 99 Classifier Classified Precision Overall pattern matching 36.15% 95.37% 34.48% WordNet-aided 98.33% 84.35%</context>
</contexts>
<marker>Karzewski, 1997</marker>
<rawString>Marek Karzewski. 1997. Jeden z dziesi˛eciu - pytania i odpowiedzi. Muza SA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
<author>Daniel Loreto</author>
<author>Wesley Hildebrandt</author>
<author>Matthew Bilotti</author>
<author>Sue Felshin</author>
<author>Aaron Fernandes</author>
<author>Gregory Marton</author>
<author>Federico Mora</author>
</authors>
<title>Integrating Web-based and corpus-based techniques for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twelfth Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="7620" citStr="Katz et al. (2003)" startWordPosition="1269" endWordPosition="1272">here we can rely on a high redundancy of the WWW, which makes finding a similar expression probable enough. Such an approach, using the Google search engine is presented by Brill et al. (2002). When working with smaller corpora, one needs to take into account different formulations of the desired information. Therefore, an initial query is subject to some modifications. First, some of the keywords may be dropped from the query; Moldovan et al. (2000) present 8 different heuristics of selecting them, based on quotation marks, parts of speech, detected named entities and other features, whereas Katz et al. (2003) drop terms in order of increasing IDF. ˇCeh and Ojsteršek (2009) start term removal from the end of the sentence. Apart from simplifying the query, its expansion is 97 also possible. For example, Hovy et al. (2000) add synonyms for each keyword, extracted from WordNet while Katz et al. (2003) introduce their inflectional and derivational morphological forms. 3 Question analysis For the purpose of building an open-domain corpus-based Polish question answering system, a question analysis module, based on some of the solutions presented above, has been implemented. The module accepts a single qu</context>
</contexts>
<marker>Katz, Lin, Loreto, Hildebrandt, Bilotti, Felshin, Fernandes, Marton, Mora, 2003</marker>
<rawString>Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hildebrandt, Matthew Bilotti, Sue Felshin, Aaron Fernandes, Gregory Marton, and Federico Mora. 2003. Integrating Web-based and corpus-based techniques for question answering. In Proceedings of the Twelfth Text REtrieval Conference (TREC 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changki Lee</author>
<author>Ji-Hyun Wang</author>
<author>Hyeon-Jin Kim</author>
<author>Myung-Gil Jang</author>
</authors>
<title>Extracting Template for Knowledge-based Question-Answering Using Conditional Random Fields.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th Annual International ACM SIGIR Workshop on MFIR,</booktitle>
<pages>428--434</pages>
<contexts>
<context position="5219" citStr="Lee et al. (2005)" startWordPosition="874" endWordPosition="877">k The problem of determination of the general question type is not frequent in existing QA solutions, as most of the public evaluation tasks, such as the TREC question answering track (Dang et al., 2007) either provide it explicitly or focus on one selected type. However, when it comes to named entity type determination, a proper classification is indispensable for finding an answer of a desired type. Some of the interrogative pronouns, such as gdzie (Eng. where) or kiedy (Eng. when) uniquely define this type, so the most obvious approach uses a list of manually defined patterns. For example, Lee et al. (2005) base solely on such rules, but need to have 1273 of them. Unfortunately, some pronouns (i.e. jaki, Eng. what, and który, Eng. which) may refer to different types of entities. In questions created with them, such as Który znany malarz twierdził, ie obci ˛ał sobie ucho? (Eng. Which famous painter claimed to have cut his ear?) the question focus (znany malarz, Eng. famous painter), following the pronoun, should be analysed, as its type corresponds to a named entity type (a PERSON in this case). Such approach is applied in a paper by Harabagiu et al. (2001), where the Princeton WordNet (Fellbaum,</context>
</contexts>
<marker>Lee, Wang, Kim, Jang, 2005</marker>
<rawString>Changki Lee, Ji-Hyun Wang, Hyeon-Jin Kim, and Myung-Gil Jang. 2005. Extracting Template for Knowledge-based Question-Answering Using Conditional Random Fields. In Proceedings of the 28th Annual International ACM SIGIR Workshop on MFIR, pages 428–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning Question Classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING2002),</booktitle>
<volume>1</volume>
<contexts>
<context position="6135" citStr="Li and Roth (2002)" startWordPosition="1033" endWordPosition="1036">laimed to have cut his ear?) the question focus (znany malarz, Eng. famous painter), following the pronoun, should be analysed, as its type corresponds to a named entity type (a PERSON in this case). Such approach is applied in a paper by Harabagiu et al. (2001), where the Princeton WordNet (Fellbaum, 1998) serves as an ontology to determine foci types. Finally, one could use a machine learning (ML) approach, treating the task as a classification problem. To do that, a set of features (such as occurrences of words, beginning pronouns, etc.) should be defined and extracted from every question. Li and Roth (2002) implemented this solution, using as much as 200,000 features, and also evaluated an influence of taking into account hierarchy of class labels. ˇCeh and Ojsteršek (2009) used this approach in a Slovene QA system for closed domain (students’ faculty-related questions) with a SVM (support vector machines) classifier. The presented problem of question classification for Polish question answering is studied in a paper by Przybyła (2013). The type determination part presented here bases on that solution, but includes several improvements. To find relevant documents, existing QA solutions usually e</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning Question Classifiers. In Proceedings of the 19th International Conference on Computational Linguistics (COLING2002), volume 1 of COLING ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marek Maziarz</author>
<author>Maciej Piasecki</author>
<author>Stanisław Szpakowicz</author>
</authors>
<title>Approaching plWordNet 2.0.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th Global Wordnet Conference.</booktitle>
<contexts>
<context position="9987" citStr="Maziarz et al., 2012" startWordPosition="1662" endWordPosition="1665">jego ... ?, Czyjemu ... ?, Czyim ... ?, Czyja ... ?,Czyjej ... ?, Czyja˛ ... ?, Czyje ... ?, Czyi ... ?, Czyich ... ?, Czyimi ... ?). However, in case of ambiguous interrogative pronouns, such as jaki (Eng. what) or który (Eng. which), a further analysis gets necessary to determine a question focus type. The question is annotated using the morphological analyser Morfeusz (Woli´nski, 2006), the tagger PANTERA (Aceda´nski, 2010) and the shallow parser Spejd (Przepiórkowski, 2008). The first nominal group after the pronoun is assumed to be a question focus. The Polish WordNet database plWordNet (Maziarz et al., 2012) is used to find its corresponding lexeme. If nothing is found, the procedure repeats with the current group’s semantic head until a single segment remains. Failure at that stage results in returning an UNNAMED_ENTITY label, whereas success leads us to a synset in WordNet. Then, we check whether its direct and indirect parents (i.e. synsets connected via hypernymy relations) include one of the predefined synsets, corresponding to the available named entity types. The whole procedure is outlined in Figure 1. The error analysis of this procedure performed in (Przybyła, 2013) shows a high number </context>
</contexts>
<marker>Maziarz, Piasecki, Szpakowicz, 2012</marker>
<rawString>Marek Maziarz, Maciej Piasecki, and Stanisław Szpakowicz. 2012. Approaching plWordNet 2.0. In Proceedings of the 6th Global Wordnet Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
<author>Marius Pa¸sca</author>
<author>Rada Mihalcea</author>
<author>Roxana Gîrju</author>
<author>Richard Goodrum</author>
<author>Vasile Rus</author>
</authors>
<title>The structure and performance of an open-domain question answering system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics - ACL ’00,</booktitle>
<pages>563--570</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA,</location>
<marker>Moldovan, Harabagiu, Pa¸sca, Mihalcea, Gîrju, Goodrum, Rus, 2000</marker>
<rawString>Dan Moldovan, Sanda Harabagiu, Marius Pa¸sca, Rada Mihalcea, Roxana Gîrju, Richard Goodrum, and Vasile Rus. 2000. The structure and performance of an open-domain question answering system. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics - ACL ’00, pages 563–570, Morristown, NJ, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselmo Peñas</author>
<author>Eduard H Hovy</author>
<author>Pamela Forner</author>
<author>Álvaro Rodrigo</author>
<author>Richard F E Sutcliffe</author>
<author>Caroline Sporleder</author>
</authors>
<title>Corina Forascu, Yassine Benajiba, and Petya Osenova.</title>
<date>2012</date>
<booktitle>In CLEF 2012 Evaluation Labs and Workshop Online Working Notes.</booktitle>
<contexts>
<context position="2684" citStr="Peñas et al., 2012" startWordPosition="432" endWordPosition="435">), other name for a given named entity (Jakiego pseudonimu u˙zywał John Kennedy w trakcie słu˙zby wojskowej?, Eng. What nickname did John Kennedy use during his military service?) and multiple entities (Którzy prezydenci Stanów Zjednoczonych zostali zabici w trakcie kadencji?, Eng. Which U.S. presidents were assassinated in office?). There are many others possible, such as definition or explanation questions, but they require specific techniques for answer finding and remain beyond the scope of this work. For example, the Question Answering for Machine Reading Evaluation (QA4MRE) competition (Peñas et al., 2012) included these complex questions (e.g. What caused X?, How did X happen?, Why did X happen?). In case of named entity questions, it is also useful to find its named entity type, corresponding to a type of an entity which could be provided as an answer. A list of possible options, suited to questions about general knowledge, is given in Table 1. As some of the categories include others (e.g. CITY is a PLACE), the goal of a classifier is to find the narrowest available. The need for a search query is motivated by performance reasons. A linguistic analysis applied to a source text to find the ex</context>
</contexts>
<marker>Peñas, Hovy, Forner, Rodrigo, Sutcliffe, Sporleder, 2012</marker>
<rawString>Anselmo Peñas, Eduard H. Hovy, Pamela Forner, Álvaro Rodrigo, Richard F. E. Sutcliffe, Caroline Sporleder, Corina Forascu, Yassine Benajiba, and Petya Osenova. 2012. QA4MRE: Question Answering for Machine Reading Evaluation at CLEF 2012. In CLEF 2012 Evaluation Labs and Workshop Online Working Notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Przepiórkowski</author>
</authors>
<title>Slavonic information extraction and partial parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing Information Extraction and Enabling Technologies - ACL ’07.</booktitle>
<contexts>
<context position="9270" citStr="Przepiórkowski, 2007" startWordPosition="1541" endWordPosition="1542">sification For the question type classification all the techniques presented above are implemented. Pattern matching stage bases on a list of 176 regular expressions and sets of corresponding question types. If any of the expressions matches the question, its corresponding set of types may be immediately returned at this stage. These expressions cover only the most obvious cases and have been created using general linguistic knowledge. The length of the list arises from some of the features of Polish, typical for Slavonic languages, i.e. relatively free word order and rich nominal inflection (Przepiórkowski, 2007). For example one English pattern Whose ... ? corresponds to 11 Polish patterns (Czyj ... ?, Czyjego ... ?, Czyjemu ... ?, Czyim ... ?, Czyja ... ?,Czyjej ... ?, Czyja˛ ... ?, Czyje ... ?, Czyi ... ?, Czyich ... ?, Czyimi ... ?). However, in case of ambiguous interrogative pronouns, such as jaki (Eng. what) or który (Eng. which), a further analysis gets necessary to determine a question focus type. The question is annotated using the morphological analyser Morfeusz (Woli´nski, 2006), the tagger PANTERA (Aceda´nski, 2010) and the shallow parser Spejd (Przepiórkowski, 2008). The first nominal gr</context>
<context position="13880" citStr="Przepiórkowski, 2007" startWordPosition="2291" endWordPosition="2292">stion Który znany malarz twierdził, ie obci ˛ał sobie ucho?. The words of the question focus znany malarz are not absolutely necessary in a source document, but their appearance may be a helpful clue. The query could also be expanded by replacing each keyword by a nested OR query, containing synonyms of the keyword, extracted from plWordNet. Both the focus removal and synonym expansion have been implemented as options of the presented query formation mechanism. Finally, one needs to remember about an important feature of Polish, typical for a Slavonic language, namely rich nominal inflection (Przepiórkowski, 2007). It means that the orthographic forms of nouns change as they appear in different roles in a sentence. We could either ignore this fact and look for exact matches between words in the question and a document or allow some modifications. These could be done by stemming (available for Polish in Lucene, see the description in (Galambos, 2001)), fuzzy queries (allowing a difference between the keyword and a document word restricted by a specified Levenshtein distance) or a full morphological analysis and tagging of the source corpus and the query. All the enumerated possibilities are evaluated in</context>
</contexts>
<marker>Przepiórkowski, 2007</marker>
<rawString>Adam Przepiórkowski. 2007. Slavonic information extraction and partial parsing. In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing Information Extraction and Enabling Technologies - ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Przepiórkowski</author>
</authors>
<title>Powierzchniowe przetwarzanie j˛ezyka polskiego. Akademicka Oficyna Wydawnicza EXIT,</title>
<date>2008</date>
<location>Warszawa.</location>
<contexts>
<context position="9848" citStr="Przepiórkowski, 2008" startWordPosition="1640" endWordPosition="1641">h nominal inflection (Przepiórkowski, 2007). For example one English pattern Whose ... ? corresponds to 11 Polish patterns (Czyj ... ?, Czyjego ... ?, Czyjemu ... ?, Czyim ... ?, Czyja ... ?,Czyjej ... ?, Czyja˛ ... ?, Czyje ... ?, Czyi ... ?, Czyich ... ?, Czyimi ... ?). However, in case of ambiguous interrogative pronouns, such as jaki (Eng. what) or który (Eng. which), a further analysis gets necessary to determine a question focus type. The question is annotated using the morphological analyser Morfeusz (Woli´nski, 2006), the tagger PANTERA (Aceda´nski, 2010) and the shallow parser Spejd (Przepiórkowski, 2008). The first nominal group after the pronoun is assumed to be a question focus. The Polish WordNet database plWordNet (Maziarz et al., 2012) is used to find its corresponding lexeme. If nothing is found, the procedure repeats with the current group’s semantic head until a single segment remains. Failure at that stage results in returning an UNNAMED_ENTITY label, whereas success leads us to a synset in WordNet. Then, we check whether its direct and indirect parents (i.e. synsets connected via hypernymy relations) include one of the predefined synsets, corresponding to the available named entity </context>
</contexts>
<marker>Przepiórkowski, 2008</marker>
<rawString>Adam Przepiórkowski. 2008. Powierzchniowe przetwarzanie j˛ezyka polskiego. Akademicka Oficyna Wydawnicza EXIT, Warszawa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Przybyła</author>
</authors>
<title>Question classification for Polish question answering.</title>
<date>2013</date>
<booktitle>In Proceedings of the 20th International Conference of Language Processing and Intelligent Information Systems (LP&amp;IIS</booktitle>
<contexts>
<context position="6572" citStr="Przybyła (2013)" startWordPosition="1102" endWordPosition="1103">sification problem. To do that, a set of features (such as occurrences of words, beginning pronouns, etc.) should be defined and extracted from every question. Li and Roth (2002) implemented this solution, using as much as 200,000 features, and also evaluated an influence of taking into account hierarchy of class labels. ˇCeh and Ojsteršek (2009) used this approach in a Slovene QA system for closed domain (students’ faculty-related questions) with a SVM (support vector machines) classifier. The presented problem of question classification for Polish question answering is studied in a paper by Przybyła (2013). The type determination part presented here bases on that solution, but includes several improvements. To find relevant documents, existing QA solutions usually employ one of the widely available general-purpose search engines, such as Lucene. Words of the question are interpreted as keywords and form a boolean query, where all the constituents are considered required. This procedure suffices only in case of a web-based QA, where we can rely on a high redundancy of the WWW, which makes finding a similar expression probable enough. Such an approach, using the Google search engine is presented </context>
<context position="10566" citStr="Przybyła, 2013" startWordPosition="1756" endWordPosition="1757">ase plWordNet (Maziarz et al., 2012) is used to find its corresponding lexeme. If nothing is found, the procedure repeats with the current group’s semantic head until a single segment remains. Failure at that stage results in returning an UNNAMED_ENTITY label, whereas success leads us to a synset in WordNet. Then, we check whether its direct and indirect parents (i.e. synsets connected via hypernymy relations) include one of the predefined synsets, corresponding to the available named entity types. The whole procedure is outlined in Figure 1. The error analysis of this procedure performed in (Przybyła, 2013) shows a high number of errors caused by a lack of a word sense disambiguation. A lexeme may be connected to many synsets, each corresponding to a specific word sense and having a different parent list. Among the possible ways to combine them are: intersection (corresponding to using only the parents common for all word senses), union (the parents of any word sense), voting (the parents common for the majority of word senses) and selecting only the first word sense (which usually is the most common in the language). The experiments have shown a better precision of classification using the firs</context>
</contexts>
<marker>Przybyła, 2013</marker>
<rawString>Piotr Przybyła. 2013. Question classification for Polish question answering. In Proceedings of the 20th International Conference of Language Processing and Intelligent Information Systems (LP&amp;IIS 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines ˇCeh</author>
<author>Milan Ojsteršek</author>
</authors>
<title>Developing a question answering system for the slovene language.</title>
<date>2009</date>
<journal>WSEAS Transactions on Information Science and Applications,</journal>
<volume>6</volume>
<issue>9</issue>
<marker>ˇCeh, Ojsteršek, 2009</marker>
<rawString>Ines ˇCeh and Milan Ojsteršek. 2009. Developing a question answering system for the slovene language. WSEAS Transactions on Information Science and Applications, 6(9):1533–1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcin Woli´nski</author>
</authors>
<title>Morfeusz — a Practical Tool for the Morphological Analysis of Polish.</title>
<date>2006</date>
<booktitle>In Mieczysław Kłopotek, Sławomir Wierzcho´n, and Krzysztof Trojanowski, editors, Intelligent Information Processing and Web Mining,</booktitle>
<pages>511--520</pages>
<marker>Woli´nski, 2006</marker>
<rawString>Marcin Woli´nski. 2006. Morfeusz — a Practical Tool for the Morphological Analysis of Polish. In Mieczysław Kłopotek, Sławomir Wierzcho´n, and Krzysztof Trojanowski, editors, Intelligent Information Processing and Web Mining, pages 511–520.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>