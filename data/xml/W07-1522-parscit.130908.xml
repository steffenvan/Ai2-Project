<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9976115">
Annotating a Japanese Text Corpus with
Predicate-Argument and Coreference Relations
</title>
<author confidence="0.995904">
Ryu Iida, Mamoru Komachi, Kentaro Inui and Yuji Matsumoto
</author>
<affiliation confidence="0.998821">
Graduate School of Information Science,
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.716927">
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
</address>
<email confidence="0.999062">
fryu-i,mamoru-k,inui,matsul@is.naist.jp
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923954545455">
In this paper, we discuss how to anno-
tate coreference and predicate-argument re-
lations in Japanese written text. There
have been research activities for building
Japanese text corpora annotated with coref-
erence and predicate-argument relations as
are done in the Kyoto Text Corpus version
4.0 (Kawahara et al., 2002) and the GDA-
Tagged Corpus (Hasida, 2005). However,
there is still much room for refining their
specifications. For this reason, we discuss
issues in annotating these two types of re-
lations, and propose a new specification for
each. In accordance with the specification,
we built a large-scaled annotated corpus, and
examined its reliability. As a result of our
current work, we have released an anno-
tated corpus named the NAIST Text Corpus1,
which is used as the evaluation data set in
the coreference and zero-anaphora resolu-
tion tasks in Iida et al. (2005) and Iida et al.
(2006).
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999731666666667">
Coreference resolution and predicate-argument
structure analysis has recently been a growing field
of research due to the demands from NLP appli-
cation such as information extraction and machine
translation. With the research focus placed on these
tasks, the specification of annotating corpora and the
</bodyText>
<footnote confidence="0.959987">
1The NAIST Text Corpus is downloadable from
http://cl.naist.jp/nldata/corpus/, and it has already been
downloaded by 102 unique users.
</footnote>
<bodyText confidence="0.99825353125">
data sets used in supervised techniques (Soon et al.,
2001; Ng and Cardie, 2002, etc.) have also grown in
sophistication.
For English, several annotation schemes have al-
ready been proposed for both coreference relation
and argument structure, and annotated corpora have
been developed accordingly (Hirschman, 1997; Poe-
sio et al., 2004; Doddington et al., 2004). For in-
stance, in the Coreference task on Message Under-
standing Conference (MUC) and the Entity Detec-
tion and Tracking (EDT) task in the Automatic Con-
tent Extraction (ACE) program, which is the suc-
cessor of MUC, the details of specification of anno-
tating coreference relation have been discussed for
several years. On the other hand, the specification
of predicate-argument structure analysis has mainly
been discussed in the context of the CoNLL shared
task2 on the basis of the PropBank (Palmer et al.,
2005).
In parallel with these efforts, there have also been
research activities for building Japanese text corpora
annotated with coreference and predicate-argument
relations such as the Kyoto Text Corpus version 4.0
(Kawahara et al., 2002) and the GDA3-Tagged Cor-
pus (Hasida, 2005). However, as we discuss in this
paper, there is still much room for arguing and re-
fining the specification of such sorts of semantic an-
notation. In fact, for neither of the above two cor-
pora, the adequacy and reliability of the annotation
scheme has been deeply examined.
In this paper, we discuss how to annotate coref-
erence and predicate-argument relations in Japanese
</bodyText>
<footnote confidence="0.999128">
2http://www.lsi.upc.edu/˜srlconll/
3The Global Document Annotation
</footnote>
<page confidence="0.910316">
132
</page>
<note confidence="0.7160655">
Proceedings of the Linguistic Annotation Workshop, pages 132–139,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99911425">
text. In Section 2 to Section 4, we examine the an-
notation issues of coreference, predicate-argument
relations, and event-nouns and their argument rela-
tions respectively, and define adequate specification
of each annotation task. Then, we report the results
of actual annotation taking the Kyoto Corpus 3.0 as a
starting point. Section 6 discusses the open issues of
each annotation task and we conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.983942" genericHeader="introduction">
2 Annotating coreference relations
</sectionHeader>
<subsectionHeader confidence="0.999071">
2.1 Approaches to coreference annotation
</subsectionHeader>
<bodyText confidence="0.997078090909091">
Coreference annotation in English has been evolving
mainly in the context of information extraction. For
instance, in the 6th and 7th Message Understand-
ing Conferences (MUC), coreference resolution is
treated as a subtask of information extraction4. The
annotated corpora built in the MUC contain coref-
erence relations between NPs, which are used as a
gold standard data set for machine learning-based
approaches to coreference resolution by researchers
such as Soon et al. (2001) and Ng and Cardie (2002).
However, van Deemter and Kibble (1999) claim
that the specification of the MUC coreference task
guides us to annotate expressions that are not nor-
mally considered coreferential, such as appositive
relations (e.g. Julius Caesari, a well-known em-
perori, ...).
In the task of Entity Detection and Tracking
(EDT) in the Automatic Content Extraction (ACE)
program (Doddington et al., 2004), the successor
of MUC, the coreference relations are redefined in
terms of two concepts, mentions and entities, in or-
der to avoid inappropriate co-indexing. In the speci-
fication of EDT, mentions are defined as the expres-
sions appearing in the texts, and entities mean the
collective set of specific entities referred to by the
mentions in the texts. Entities are limited to named
entities such as PERSON and ORGANIZATION for
adequacy and reliability of annotation. Therefore,
the ACE data set has the drawback that not all coref-
erence relations in the text are exhaustively anno-
tated. It is insufficient to resolve only the annotated
coreference relations in order to properly analyze a
text.
</bodyText>
<footnote confidence="0.997441">
4http://www-nlpir.nist.gov/related projects/muc/
proceedings/co task.html
</footnote>
<subsectionHeader confidence="0.999014">
2.2 Coreference annotated corpora of Japanese
</subsectionHeader>
<bodyText confidence="0.999184363636364">
In parallel with these efforts, Japanese corpora have
been developed that are annotated with coreference
relations, such as the Kyoto Text Corpus version
4.0 (Kawahara et al., 2002) and GDA-Tagged Cor-
pus (Hasida, 2005). Before reviewing these works,
we explain the relationship between anaphora and
coreference in Japanese, referring to the following
examples. In example (1), the pronoun sorei (it)
points back to iPodi, and these two mentions refer
to the same entity in the world and thus are consid-
ered both anaphoric and coreferential.
</bodyText>
<equation confidence="0.9753148">
(1) Tom-wa iPodi-o ka-tta .
Tom-TOP iPodi-ACC buy-PAST PUNC
Tom bought an iPod.
kare-wa sorei-de ongaku-o ki-ita .
he-TOP iti-INS music-ACC listen to-PAST PUNC
</equation>
<bodyText confidence="0.98916">
He listened to music on it.
On the other hand, in example (2), we still see an
anaphoric relation between iPodi (iPodi) and sorej
(itj) and sorej points back to iPodi. However, these
two mentions are not coreferential since they refer
to different entities in the world.
</bodyText>
<equation confidence="0.818826833333333">
(2) Tom-wa iPodi-o ka-tta .
Tom-TOP iPodi-ACC buy-PAST PUNC
Tom bought an iPod.
Mary-mo sorej-o ka-tta .
Mary-TOP onej-ACC buy-PAST PUNC
Mary also bought one.
</equation>
<bodyText confidence="0.999900375">
As in the above examples, an anaphoric relation
can be either coreferential or not. The former case is
called an identity-of-reference anaphora (IRA) and
the latter an identity-of-sense anaphora (ISA) (see
Mitkov (2002)). In English the difference between
IRA and ISA is clearly expressed by the anaphoric
relations formed with ‘it’ and ‘one’ respectively.
This makes it possible to treat these classes sepa-
rately. However, in Japanese, no such clear lexical
distinction can be drawn. In both the Kyoto Cor-
pus and GDA-Tagged Corpus, there is no discussion
in regards to distinction between ISA and IRA, thus
it is unclear what types of coreference relations the
annotators annotated. To make matters worse, their
approaches do not consider whether or not a mention
refers to a specific entity like in the EDT task.
</bodyText>
<subsectionHeader confidence="0.996747">
2.3 Annotating IRA relations in Japanese
</subsectionHeader>
<bodyText confidence="0.998241">
As described in the previous section, conventional
specifications in Japanese are not based on a pre-
</bodyText>
<page confidence="0.998514">
133
</page>
<bodyText confidence="0.99631">
cise definition of coreference relations, resulting in
inappropriate annotation. On the other hand, in our
specification, we consider two or more mentions as
coreferential in case they satisfy the following two
conditions:
</bodyText>
<listItem confidence="0.998379">
• The mentions refer to not a generic entity but
to a specific entity.
• The relation between the mentions is consid-
ered as an IRA relation.
</listItem>
<sectionHeader confidence="0.640288" genericHeader="method">
3 Annotating predicate-argument relations
</sectionHeader>
<subsectionHeader confidence="0.999937">
3.1 Labels of predicate-argument relations
</subsectionHeader>
<bodyText confidence="0.999906454545454">
One debatable issue in the annotation of predicate-
argument relations is what level of abstraction we
should label those relations at.
The GDA-Tagged Corpus, for example, adopts a
fixed set of somewhat “traditional” semantic roles
such as Agent, Theme, and Goal that are defined
across verbs. The PropBank (Palmer et al., 2005),
on the other hand, defines a set of semantic roles (la-
beled ARG0, ARG1, and AM-ADV, etc.) for each
verb and annotates each sentence in the corpus with
those labels as in (3).
</bodyText>
<listItem confidence="0.994577">
(3) [ARGM−TMP A year earlier], [ARG0 the refiner] [rel
</listItem>
<bodyText confidence="0.987403733333333">
earned] [ARG1 $66 million, or $1.19 a share].
In the FrameNet (Fillmore and Baker, 2000), a spe-
cific set of semantic roles is defined for each set of
semantically-related verbs called a FrameNet frame.
However, there is still only limited consensus on
how many kinds of semantic roles should be iden-
tified and which linguistic theory we should adopt
to define them at least for the Japanese language.
An alternative way of labeling predicate-
argument relations is to use syntactic cases as
labels. In Japanese, arguments of a verb are marked
by a postposition, which functions as a case marker.
In sentence (4), for example, the verb tabe has
two arguments, each of which is marked by a
postposition, ga or o.
</bodyText>
<listItem confidence="0.546132">
(4) Tom-ga ringo-o tabe-ru
</listItem>
<equation confidence="0.7692935">
Tom-NOM apple-ACC eat-PRES
(Tom eats an apple.)
</equation>
<bodyText confidence="0.998853333333333">
Labeling predicate-argument relations in terms of
syntactic cases has a few more advantages over se-
mantic roles as far as Japanese is concerned:
</bodyText>
<listItem confidence="0.915300684210526">
• Manual annotation of syntactic cases is likely
to be more cost-efficient than semantic roles
because they are often explicitly marked by
case markers. This fact also allows us to avoid
the difficulties in defining a label set.
• In Japanese, the mapping from syntactic cases
to semantic roles tends to be reasonably
straightforward if a semantically rich lexicon of
verbs like the VerbNet (Kipper et al., 2000) is
available.
• Furthermore, we have not yet found many NLP
applications for which the utility of seman-
tic roles is actually demonstrated. One may
think of using semantic roles in textual infer-
ence as exemplified by, for example, Tatu and
Moldovan (2006). However, similar sort of
inference may well be realized with syntactic
cases as demonstrated in the information ex-
traction and question answering literature.
</listItem>
<bodyText confidence="0.99993925">
Taking these respects into account, we choose to
label predicate-argument relations in terms of syn-
tactic cases, which follows the annotation scheme
adopted in the Kyoto Corpus.
</bodyText>
<subsectionHeader confidence="0.999616">
3.2 Syntactic case alternation
</subsectionHeader>
<bodyText confidence="0.999995833333333">
Once the level of syntactic cases is chosen for our
annotation, another issue immediately arises, alter-
ation of syntactic cases by syntactic transformations
such as passivization and causativization. For exam-
ple, sentence (5) is an example of causativization,
where Mary causes Tom’s eating action.
</bodyText>
<listItem confidence="0.688045">
(5) Mary-ga Tom-ni ringo-o tabe-saseru
</listItem>
<subsectionHeader confidence="0.388196">
Mary-NOM Tom-DAT apple-ACC eat-CAUSATIVIZED
</subsectionHeader>
<bodyText confidence="0.971645166666667">
(Mary helps Tom eat an apple.)
One way of annotating these arguments is some-
thing like (6), where the relations between the
causativized predicate tabe-saseru (to make some-
one eat) and its arguments are indicated in terms of
surface syntactic cases.
</bodyText>
<listItem confidence="0.7834545">
(6) [REL=tabe-saseru (eat-CAUSATIVE),
GA=Mary, NI=Tom, O=ringo (apple)]
</listItem>
<bodyText confidence="0.999667222222222">
In fact, the Kyoto Corpus adopts this way of label-
ing.
An alternative way of treating such case alterna-
tions is to identify logical (or deep) case relations,
i.e. the relations between the base form of each pred-
icate and its arguments. (7) illustrates how the ar-
guments in sentence (5) are annotated with logical
case relations: Tom is labeled as the ga-case (Nom-
inative) filler of the verb tabe (to eat) and Mary is
</bodyText>
<page confidence="0.993396">
134
</page>
<bodyText confidence="0.994510125">
labeled as the Extra-Nominative (EX-GA) which we
newly invent to indicate the Causer of a syntactically
causativized clause.
(7) [REL=tabe-(ru) (eat), GA=Tom, O=ringo (ap-
ple), EX-GA=Mary]
In the NAIST Text Corpus, we choose to this lat-
ter way of annotation motivated by such considera-
tions as follows:
</bodyText>
<listItem confidence="0.995489222222222">
• Knowing that, for example, Tom is the filler of
the ga-case (Nominative) of the verb tabe (to
eat) in (5) is more useful than knowing that Tom
is the ni-case (Dative) of the causativized verb
tabe-saseru (to make someone eat) for such ap-
plications as information extraction.
• The mapping from syntactic cases to semantic
roles should be described in terms of logical
case relations associated with bare verbs.
</listItem>
<subsectionHeader confidence="0.997655">
3.3 Zero-anaphora
</subsectionHeader>
<bodyText confidence="0.999942666666667">
In the PropBank the search space for a given pred-
icate’s arguments is limited to the sentence that
predicate appears in, because, syntactically, English
obligatory arguments are overtly expressed except
pro-form (e.g. John hopes [PRO to leave.]).
In contrast, Japanese is characterized by extensive
use of nominal ellipses, called zero-pronouns, which
behave like pronouns in English texts. Thus, if an
argument is omitted, and an expression correspond-
ing to that argument does not appear in the same
sentence, annotators should search for its antecedent
outside of the sentence. Furthermore, if an argument
is not explicitly mentioned in the text, they need to
annotate that relation as “exophoric.” In the second
sentence of example (8), for instance, the ga (Nomi-
native) argument of the predicate kaeru (go back) is
omitted and refers to Tom in the first sentence. The
kara (Ablative) argument of that predicate is also
omitted, however the corresponding argument does
not explicitly appear in the text. In such cases, omit-
ted arguments should be considered as “exophoric.”
</bodyText>
<figure confidence="0.542485">
(8) Tomi-wa kyo gakko-ni it-ta .
Tomi-TOP today school-LOC go-PAST PUNC
Tom went to school today.
</figure>
<equation confidence="0.6702705">
(oi-ga) (oexophoric-kara) kae-tte suguni
φi-NOM φexophoric-ABL go back immediately
(oi-ga) kouen-ni dekake-ta .
φi-NOM park-LOC go out-PAST PUNC
</equation>
<bodyText confidence="0.7268055">
He went to the park as soon as he came back
from school.
</bodyText>
<tableCaption confidence="0.9887215">
Table 1: Comparison of annotating predicate-
argument relations
</tableCaption>
<table confidence="0.805986444444444">
corpus label search space
PropBank semantic role intra
GDA Corpus semantic role inter, exo
Kyoto Corpus surface case intra, inter,
(voice alternation involved) exo
NAIST Corpus logical (deep) case intra, inter,
(our corpus) (relation with bare verb) exo
intra: intra-sentential relations, inter: inter-sentential relations,
exo: exophoric relations
</table>
<bodyText confidence="0.999393666666667">
To the best of our knowledge, the GDA-Tagged Cor-
pus does not contain intra-sentential zero-anaphoric
relations as predicate-argument relations, so it has a
serious drawback when used as training data in ma-
chine learning approaches.
Unlike coreference between two explicit nouns
where only an IRA is possible, the relation between
a zero-pronoun and its antecedent can be either IRA
or ISA. For example, in example (8), φi is annotated
as having an IRA relation with its antecedent Tomi.
In contrast, example (9) exhibits an ISA relation be-
tween iPodi and φi.
</bodyText>
<equation confidence="0.884184714285714">
(9) Tom-wa iPodi-o kaa-tta .
Tom-TOP iPodi-ACC buya-PAST PUNC
Tom bought an iPod.
Mary-mo (oi-o) kab-tta .
Mary-TOP φi-ACC buyb-PAST PUNC
Mary also bought one.
[REL=ka-(u) (buy), GA=Mary, O=iPodi]
</equation>
<bodyText confidence="0.999991363636363">
The above examples indicate that predicate-
argument annotation in Japanese can potentially be
annotated as either an IRA or ISA relation. Note that
in Japanese these two relations cannot be explicitly
separated by syntactic clues. Thus, in our corpus
we annotate them without explicit distinction. It is
arguable that separate treatment of IRA and ISA in
predicate-argument annotation could be preferable.
We consider this issue as a task of future work.
A comparison of the specification is summarized
in Table 1.
</bodyText>
<sectionHeader confidence="0.9961965" genericHeader="method">
4 Annotating event-noun-argument
relations
</sectionHeader>
<bodyText confidence="0.995181">
Meyers et al. (2004) propose to annotate seman-
tic relations between nouns referring to an event
in the context, which we call event-nouns in this
</bodyText>
<page confidence="0.995272">
135
</page>
<bodyText confidence="0.999287285714286">
paper. They release the NomBank corpus, in
which PropBank-style semantic relations are anno-
tated for event-nouns. In (10), for example, the
noun “growth” refers to an event and “dividends”
and “next year” are annotated as ARG1 (roughly
corresponding to the theme role) and ARGM-TMP
(temporal adjunct).
</bodyText>
<listItem confidence="0.747287">
(10) 12% growth in dividends next year [REL=growth,
ARG1=in dividends, ARGM-TMP=next year]
</listItem>
<bodyText confidence="0.998708166666667">
Following the PropBank-style annotation, the Nom-
Bank also restricts the search space for the argu-
ments of a given event-noun to the sentence in which
the event-noun appears. In Japanese, on the other
hand, since predicate-argument relations are often
zero-anaphoric, this restriction should be relaxed.
</bodyText>
<subsectionHeader confidence="0.999775">
4.1 Labels of event-noun-relations
</subsectionHeader>
<bodyText confidence="0.9921535">
Regarding the choice between semantic roles and
syntactic cases, we take the same approach as
that for predicate-argument relations, which is also
adopted in the Kyoto Corpus. For example, in (11),
akajii (deficit) is identified as the ga argument of the
event-noun eikyo (influence).
</bodyText>
<equation confidence="0.8923395">
(11) kono boueki akajii-wa waga kuni-no
this trade deficit-TOP our country-OF
kyosoryokuj-ni eikyo-o oyobosu
competitiveness-DAT influence-ACC affect
[REL=eikyo (influence), GA=akajii (deficit),
O=kyosoryokuj (competitiveness)]
</equation>
<bodyText confidence="0.9996657">
The trade deficit affects our competitiveness.
Note that unlike verbal predicates, event-nouns can
never be a subject of voice alternation. An event-
noun-argument relation is, therefore, necessarily an-
notated in terms of the relation between the bare
verb corresponding to the event-noun and its argu-
ment. This is another reason why we consider it
reasonable to annotate the logical case relations be-
tween bare verbs and their arguments for predicate-
argument relations.
</bodyText>
<subsectionHeader confidence="0.99521">
4.2 Event-hood
</subsectionHeader>
<bodyText confidence="0.999556333333333">
Another issue to be addressed is on the determina-
tion of the “event-hood” of noun phrases, i.e. the
task of determining whether a given noun refers to
an event or not. In Japanese, since neither singular-
plural nor definite-indefinite distinction is explic-
itly marked, event-hood determination tends to be
highly context-dependent. In sentence (12), for ex-
ample, the first occurrence of denwa (phone-call),
subscripted with i, should be interpreted as Tom’s
calling event, whereas the second occurrence of the
same noun denwa should be interpreted as a physical
telephone (cellphone).
</bodyText>
<equation confidence="0.811976">
(12) karea-karano denwai-niyoruto watashib-wa
hea-ABL phone-calli according to Ib-NOM
kare-no ie-ni denwaj-o wasure-tarasii
his-OF home-LOC phonej-ACC leave-PAST
</equation>
<bodyText confidence="0.991009">
According to his phone call, I might have left
my cell phone at his home.
To control the quality of event-hood determina-
tion, we constrain the range of potential event-nouns
from two different points of view, neither of which
is explicitly discussed in designing the specifications
of the Kyoto Corpus.
First, we impose a POS-based constraint. In our
corpus annotation, we consider only verbal nouns
(sahen-verbs; e.g. denwa (phone) ) and deverbal
nouns (the nominalized forms of verbs; e.g. furumai
(behavior)) as potential event-nouns. This means
that event-nouns that are not associated with a verb,
such as jiko (accident), are out of scope of our anno-
tation.
Second, the determination of the event-hood of
a noun tends to be obscure when the noun consti-
tutes a compound. In (13), for example, the ver-
bal noun kensetsu (construction) constituting a com-
pound douro-kensetsu (road construction) can be in-
terpreted as a constructing event. We annotate it as
an event and douro (road) as the o argument.
</bodyText>
<equation confidence="0.764397">
(13) (φ-ga) douro-kensetsu-o tsuzukeru
φ-NOM road construction-ACC continue
</equation>
<bodyText confidence="0.926645571428572">
Someone continues road construction.
In (14), on the other hand, since the compound
furansu kakumei (French Revolution) is a named-
entity and is not semantically decomposable, it is
not reasonable to consider any sort of predicate-
argument-like relations between its constituents fu-
ransu (France) and kakumei (revolution).
</bodyText>
<listItem confidence="0.699347">
(14) furansu-kakumei-ga okoru
</listItem>
<subsectionHeader confidence="0.602872">
French Revolution-NOM take place
</subsectionHeader>
<bodyText confidence="0.8586075">
The French Revolution took place.
We therefore do not consider constituents of such se-
mantically non-decomposable compounds as a tar-
get of annotation.
</bodyText>
<sectionHeader confidence="0.718555" genericHeader="method">
5 Statistics of the new corpus
</sectionHeader>
<bodyText confidence="0.987605">
Two annotators annotated predicate-argument and
coreference relations according to the specifications,
</bodyText>
<page confidence="0.997746">
136
</page>
<bodyText confidence="0.999909333333334">
using all the documents in Kyoto Text Corpus ver-
sion 3.0 (containing 38,384 sentences in 2,929 texts)
as a target corpus. We have so far annotated
predicate-argument relations with only three major
cases: ga (Nominative), o (Accusative) and ni (Da-
tive). We decided not to annotate other case relations
like kara-case (Ablative) because the annotation of
those cases was considered even further unreliable at
the point where we did not have enough experiences
in this annotation task. Annotating other cases is one
of our future directions.
The numbers of the annotated predicate-argument
relations are shown in Table 2. These relations are
categorized into five cases: (a) a predicate and its
argument appear in the same phrase, (b) the argu-
ment syntactically depends on its predicate or vice
versa, (c) the predicate and its argument have an
intra-sentential zero-anaphora relation, (d) the pred-
icate and its argument have an inter-sentential zero-
anaphora relation and (e) the argument does not ex-
plicitly appear in the text (i.e. exophoric). Table 2
shows that in annotation for predicates over 80%
of both o- and ni-arguments were found in depen-
dency relations, while around 60% of ga-arguments
were in zero-anaphoric relations. In comparison, in
the case of event-nouns, o- and ni-arguments are
likely to appear in the same phrase of given event-
nouns, and about 80% of ga-arguments have zero-
anaphoric relations with event-nouns. With respect
to the corpus size, we created a large-scaled anno-
tated corpus with predicate-argument and corefer-
ence relations. The data size of our corpus along
with other corpora is shown in Table 3.
Next, to evaluate the agreement between the two
human annotators, 287 randomly selected articles
were annotated by both of them. The results are
evaluated by calculating recall and precision in
which one annotation result is regarded as correct
and the other’s as the output of system. Note that
only the predicates annotated by both annotators are
used in calculating recall and precision. For eval-
uation of coreference relations, we calculated re-
call and precision based on the MUC score (Vilain
et al., 1995). The results are shown in Table 4,
where we can see that most annotating work was
done with high quality except for the ni-argument of
event-nouns. The most common source of error was
caused by verb alternation, and we will discuss this
</bodyText>
<tableCaption confidence="0.999757">
Table 3: Data size of each corpus
Table 4: Agreement of annotating each relation
</tableCaption>
<table confidence="0.9986544">
recall precision
predicate 0.947 (6512/6880) 0.941 (6512/6920)
ga (NOM) 0.861 (5638/6549) 0.856 (5638/6567)
o (ACC) 0.943 (2447/2595) 0.919 (2447/2664)
ni (DAT) 0.892 (1060/1189) 0.817 (1060/1298)
event-noun 0.905 (1281/1415) 0.810 (1281/1582)
ga (NOM) 0.798 (1038/1300) 0.804 (1038/1291)
o (ACC) 0.893 (469/525) 0.765 (469/613)
ni (DAT) 0.717 (66/92) 0.606 (66/109)
coreference 0.893 (1802/2019) 0.831 (1802/2168)
</table>
<bodyText confidence="0.999247714285714">
issue in detail in Section 6. Such investigation of the
reliability of annotation has not been reported for ei-
ther the Kyoto Corpus or the GDA-Tagged Corpus.
However, our results also show that each annotating
task still leaves room for improvement. We summa-
rize open issues and discuss the future directions in
the next section.
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="method">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.980425">
6.1 Identification of predicates and
event-nouns
</subsectionHeader>
<bodyText confidence="0.999820277777778">
Identification of predicates is sometimes unreliable
due to the ambiguity between a literal usage and a
compound functional usage. For instance, the ex-
pression “to-shi-te”, which includes the verb shi (to
do), is ambiguous: either the verb shi functions as a
content word, i.e. an event-denoting word, or it con-
stitutes a multi-word expression together with to and
te. In the latter case, it does not make sense to inter-
pret the verb shi to denote an event. However, this
judgment is highly context-dependent and we have
not been able to devise a reliable criterion for it.
Tsuchiya et al. (2006) have built a functional
expression-tagged corpus for automatically classify-
ing these usages. They reported that the agreement
ratio of functional expressions is higher than ours.
We believe their findings to also become helpful in-
formation for annotating predicates in our corpus.
With regards to event-nouns, a similar problem
</bodyText>
<figure confidence="0.974890071428572">
corpus
size
PropBank I
NomBank 0.8
ACE (2005 English)
GDA Corpus
Kyoto Corpus
NAIST Corpus (ours)
7,891 sentences
24,311 sentences
269 articles
2,177 articles
555 articles (5,127 sentences)
2,929 articles (38,384 sentences)
</figure>
<page confidence="0.984851">
137
</page>
<tableCaption confidence="0.994318">
Table 2: Statistics: annotating predicate-arguments relations
</tableCaption>
<table confidence="0.998700153846154">
ga (Nominative) o (Accusative) ni (Dative)
predicates (a) in same phrase 177 (0.002) 60 (0.001) 591 (0.027)
106,628 (b) dependency relations 44,402 (0.419) 35,882 (0.835) 18,912 (0.879)
(c) zero-anaphoric (intra-sentential) 32,270 (0.305) 5,625 (0.131) 1,417 (0.066)
(d) zero-anaphoric (inter-sentential) 13,181 (0.124) 1,307 (0.030) 542 (0.025)
(e) exophoric 15,885 (0.150) 96 (0.002) 45 (0.002)
total 105,915 (1.000) 42,970 (1.000) 21,507 (1.000)
event-nouns (a) in same phrase 2,195 (0.077) 5,574 (0.506) 846 (0.436)
28,569 (b) dependency relations 4,332 (0.152) 2,890 (0.263) 298 (0.154)
(c) zero-anaphoric (intra-sentential) 9,222 (0.324) 1,645 (0.149) 586 (0.302)
(d) zero-anaphoric (inter-sentential) 5,190 (0.183) 854 (0.078) 201 (0.104)
(e) exophoric 7,525 (0.264) 42 (0.004) 10 (0.005)
total 28,464 (1.000) 11,005 (1.000) 1,941 (1.000)
</table>
<bodyText confidence="0.997561538461538">
also arises. If, for example, a compound noun con-
tains a verbal noun, we have to judge whether the
verbal noun can be interpreted as an event-noun or
not. Currently, we ask annotators to check if the
meaning of a given compound noun can be compo-
sitionally decomposed into those of its constituents.
However, the judgement of compositionality tends
to be highly subjective, causing the degradation of
the agreement ratio of event-nouns as shown in
Table 4. We are planning to investigate this problem
more closely and refine the current compositionality
criterion. One option is to build lexical resources of
multi-word expressions and compounds.
</bodyText>
<subsectionHeader confidence="0.999634">
6.2 Identification of arguments
</subsectionHeader>
<bodyText confidence="0.999719142857143">
As we mentioned in 3.1, we use (deep) cases instead
of semantic roles as labels of predicate-argument re-
lations. While it has several advantages as discussed
in 3.1, this choice has also a drawback that should
be removed. The problem arises from lexical verb
alternation. It can sometimes be hard for annota-
tors to determine a case frame of a given predicate
when verb alternation takes place. For example, sen-
tence (15) can be analyzed simply as in (16a). How-
ever, since the verb shibaru (bind) has also another
alternative case frame as in (16b), the labeling of the
case of the argument kisoku (rule), i.e. either GA
(NODI) or DE (INST) may be undecidable if the argu-
ment is omitted.
</bodyText>
<reference confidence="0.290361142857143">
(15) kisoku-ga hitobito-o shibaru
rule-NOM people-ACC bind
The rule binds people.
(16) a. [REL = shibaru (bind), GA = kisoku (rule), O = hitobito
(people)]
b. [REL = shibaru (bind), GA = φ (exophoric), O = hito-
bito (people), DE (Instrumental) = kisoku (rule)]
</reference>
<bodyText confidence="0.999772538461538">
Similar problems occur for event-nouns as well.
For example, the event-noun hassei (realization) has
both transitive and intransitive readings, which may
produce awkward ambiguities.
To avoid this problem, we have two options; one
is to predefine the preference in case frames as a
convention for annotation and the other is to deal
with such alternations based on generic resources of
lexical semantics such as Lexical Conceptual Struc-
ture (LCS) (Jackendoff, 1990). Creating a Japanese
LCS dictionary is another on-going project, so we
can collaborate with them in developing the valuable
resources.
</bodyText>
<subsectionHeader confidence="0.955327">
6.3 Event-hood determination
</subsectionHeader>
<bodyText confidence="0.999963818181818">
Event-nouns of some semantic types such as keiyaku
(contract), kisei (regulation) and toushi (investment)
are interpreted as either an event or an entity result-
ing from an event depending on are context. How-
ever, it is sometimes difficult to judge whether such
an event-noun should be interpreted as an event or a
resultant entity even by considering the whole con-
text, which degrades the stability of annotation. This
phenomena is also discussed in the NomBank, and
we will share their insights and refine our annotation
manual in the next step.
</bodyText>
<subsectionHeader confidence="0.998684">
6.4 Identification of coreference relation
</subsectionHeader>
<bodyText confidence="0.999587">
Even though coreference relation is defined as IRA
relations, the lack of agreement on the granularity of
noun classes makes the agreement ratio worse. In
other words, it is crucial to decide how to annotate
abstract nouns in order to improve the annotation.
</bodyText>
<page confidence="0.995363">
138
</page>
<bodyText confidence="0.99987819047619">
Annotators judge coreference relations as whether
or not abstract nouns refer to the same entity in the
world. However, the equivalence of the referents of
abstract nouns cannot be reconciled based on real-
world existence since by definition abstract nouns
have no physical entities in the real world.
As far as predicate-argument relation is con-
cerned, there might be a need for treating generic
entities in addition to specific entities as coreferen-
tial in some application. For example, one may want
to relate kids to children in sentence (17).
(17) We all want children to be fit and healthy.
However, the current invasion of fast food is
creating overweight and unhealthy kids.
The coreference relation between generic nouns are
missed in the current specification since we annotate
only IRA relations between specific nouns. Even
though there are various discussions in the area of
semantics, the issue of how to deal with generic
nouns as either coreferential or not in real texts is
still left open.
</bodyText>
<sectionHeader confidence="0.999412" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.98703275">
In this paper, we reported on the current specifica-
tion of our annotated corpus for coreference reso-
lution and predicate-argument analysis. Taking the
previous work of corpus annotation into account, we
decided to annotate predicate-argument relations by
ISA and IRA relations, and coreference relations ac-
cording to IRA relations. With the Kyoto Text Cor-
pus version 3.0 as a starting point, we built a large
annotated corpus. We also discussed the revelations
made from annotating our corpus, and discussed fu-
ture directions for refining our specifications of the
NAIST Text Corpus.
</bodyText>
<sectionHeader confidence="0.959264" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.979836">
This work is partially supported by the Grant-in-Aid
for Scientific Research in Priority Areas JAPANESE
CORPUS (http://tokuteicorpus.jp).
</bodyText>
<sectionHeader confidence="0.999491" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941768115942">
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. Automatic content
extraction (ace) program - task definitions and performance
measures. In Proceedings of the 4rd International Confer-
ence on Language Resources and Evaluation (LREC-2004),
pages 837–840.
Charles J. Fillmore and Collin F. Baker. 2000. Framenet:
Frame semantics meets the corpus. In Proceedings of the
74th Annual Meeting of the Linguistic Society of America.
K. Hasida. 2005. Global document annotation (gda) annotation
manual. http://i-content.org/tagman.html.
L. Hirschman. 1997. MUC-7 coreference task definition. ver-
sion 3.0.
R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora reso-
lution by antecedent identification followed by anaphoricity
determination. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 4:417–434.
R. Iida, K. Inui, and Y. Matsumoto. 2006. Exploiting syntac-
tic patterns as clues in zero-anaphora resolution. In Proced-
dings of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 625–632.
R. Jackendoff. 1990. Semantic Structures. Current Studies in
Linguistics 18. The MIT Press.
D. Kawahara, T. Kurohashi, and K. Hasida. 2002. Construc-
tion of a japanese relevance-tagged corpus (in japanese). In
Proceedings of the 8th Annual Meeting of the Association for
Natural Language Processing, pages 495–498.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-based con-
struction of a verb lexicon. In Proceedings of the 17th Na-
tional Conference on Artificial Intelligence and 12th Confer-
ence on on Innovative Applications of Artificial Intelligence,
pages 691–696.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska,
B. Young, and R. Grishman. 2004. The nombank project:
An interimreport. In Proceedings of the HLT-NAACL Work-
shop on Frontiers in Corpus Annotation.
Ruslan Mitkov. 2002. Anaphora Resolution. Studies in Lan-
guage and Linguistics. Pearson Education.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th ACL, pages 104–111.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposi-
tion bank: An annotated corpus of semantic roles. Compu-
tational Linguistics, 31(1):71–106.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proceddings of
the 42nd Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 144–151.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.
M. Tatu and D. Moldovan. 2006. A logic-based semantic ap-
proach to recognizing textual entailment. In Proceddings
of the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 819–826.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2006. Development and analysis of an exam-
ple database of japanese compound functional expressions.
IPSJ Journal, 47(6):1728–1741.
K. van Deemter and R. Kibble. 1999. What is coreference, and
what should coreference annotation be? In Proceedings of
the ACL ’99 Workshop on Coreference and its applications,
pages 90–96.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scoring
scheme. In Proceedings of the 6th Message Understanding
Conference (MUC-6), pages 45–52.
</reference>
<page confidence="0.998866">
139
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.619877">
<title confidence="0.998823">Annotating a Japanese Text Corpus Predicate-Argument and Coreference Relations</title>
<author confidence="0.972236">Ryu Iida</author>
<author confidence="0.972236">Mamoru Komachi</author>
<author confidence="0.972236">Kentaro Inui</author>
<author confidence="0.972236">Yuji</author>
<affiliation confidence="0.999335">Graduate School of Information Nara Institute of Science and</affiliation>
<address confidence="0.948905">8916-5 Takayama, Ikoma, Nara, 630-0192,</address>
<abstract confidence="0.999433363636364">In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA- Tagged Corpus (Hasida, 2005). However, there is still much room for refining their specifications. For this reason, we discuss issues in annotating these two types of relations, and propose a new specification for each. In accordance with the specification, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annocorpus named the Text which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al.</abstract>
<intro confidence="0.675564">(2006).</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>kisoku-ga hitobito-o shibaru rule-NOM people-ACC bind The rule binds people.</title>
<date></date>
<marker></marker>
<rawString>(15) kisoku-ga hitobito-o shibaru rule-NOM people-ACC bind The rule binds people.</rawString>
</citation>
<citation valid="false">
<authors>
<author>a</author>
</authors>
<title>REL = shibaru (bind),</title>
<journal>GA = kisoku (rule), O =</journal>
<note>hitobito (people</note>
<marker>a, </marker>
<rawString>(16) a. [REL = shibaru (bind), GA = kisoku (rule), O = hitobito (people)]</rawString>
</citation>
<citation valid="false">
<authors>
<author>b</author>
</authors>
<title>shibaru (bind),</title>
<journal>GA = φ (exophoric), O =</journal>
<note>hitobito (people), DE (Instrumental) = kisoku (rule</note>
<marker>b, </marker>
<rawString>b. [REL = shibaru (bind), GA = φ (exophoric), O = hitobito (people), DE (Instrumental) = kisoku (rule)]</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
<author>A Mitchell</author>
<author>M Przybocki</author>
<author>L Ramshaw</author>
<author>S Strassel</author>
<author>R Weischedel</author>
</authors>
<title>Automatic content extraction (ace) program - task definitions and performance measures.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4rd International Conference on Language Resources and Evaluation (LREC-2004),</booktitle>
<pages>837--840</pages>
<contexts>
<context position="2029" citStr="Doddington et al., 2004" startWordPosition="300" endWordPosition="303">traction and machine translation. With the research focus placed on these tasks, the specification of annotating corpora and the 1The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of specification of annotating coreference relation have been discussed for several years. On the other hand, the specification of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research activities for building J</context>
<context position="4779" citStr="Doddington et al., 2004" startWordPosition="722" endWordPosition="725">pora built in the MUC contain coreference relations between NPs, which are used as a gold standard data set for machine learning-based approaches to coreference resolution by researchers such as Soon et al. (2001) and Ng and Cardie (2002). However, van Deemter and Kibble (1999) claim that the specification of the MUC coreference task guides us to annotate expressions that are not normally considered coreferential, such as appositive relations (e.g. Julius Caesari, a well-known emperori, ...). In the task of Entity Detection and Tracking (EDT) in the Automatic Content Extraction (ACE) program (Doddington et al., 2004), the successor of MUC, the coreference relations are redefined in terms of two concepts, mentions and entities, in order to avoid inappropriate co-indexing. In the specification of EDT, mentions are defined as the expressions appearing in the texts, and entities mean the collective set of specific entities referred to by the mentions in the texts. Entities are limited to named entities such as PERSON and ORGANIZATION for adequacy and reliability of annotation. Therefore, the ACE data set has the drawback that not all coreference relations in the text are exhaustively annotated. It is insuffic</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. 2004. Automatic content extraction (ace) program - task definitions and performance measures. In Proceedings of the 4rd International Conference on Language Resources and Evaluation (LREC-2004), pages 837–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Collin F Baker</author>
</authors>
<title>Framenet: Frame semantics meets the corpus.</title>
<date>2000</date>
<booktitle>In Proceedings of the 74th Annual Meeting of the Linguistic</booktitle>
<publisher>Society of America.</publisher>
<contexts>
<context position="8779" citStr="Fillmore and Baker, 2000" startWordPosition="1358" endWordPosition="1361">n the annotation of predicateargument relations is what level of abstraction we should label those relations at. The GDA-Tagged Corpus, for example, adopts a fixed set of somewhat “traditional” semantic roles such as Agent, Theme, and Goal that are defined across verbs. The PropBank (Palmer et al., 2005), on the other hand, defines a set of semantic roles (labeled ARG0, ARG1, and AM-ADV, etc.) for each verb and annotates each sentence in the corpus with those labels as in (3). (3) [ARGM−TMP A year earlier], [ARG0 the refiner] [rel earned] [ARG1 $66 million, or $1.19 a share]. In the FrameNet (Fillmore and Baker, 2000), a specific set of semantic roles is defined for each set of semantically-related verbs called a FrameNet frame. However, there is still only limited consensus on how many kinds of semantic roles should be identified and which linguistic theory we should adopt to define them at least for the Japanese language. An alternative way of labeling predicateargument relations is to use syntactic cases as labels. In Japanese, arguments of a verb are marked by a postposition, which functions as a case marker. In sentence (4), for example, the verb tabe has two arguments, each of which is marked by a po</context>
</contexts>
<marker>Fillmore, Baker, 2000</marker>
<rawString>Charles J. Fillmore and Collin F. Baker. 2000. Framenet: Frame semantics meets the corpus. In Proceedings of the 74th Annual Meeting of the Linguistic Society of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hasida</author>
</authors>
<title>Global document annotation (gda) annotation</title>
<date>2005</date>
<note>manual. http://i-content.org/tagman.html.</note>
<contexts>
<context position="674" citStr="Hasida, 2005" startWordPosition="92" endWordPosition="93"> Coreference Relations Ryu Iida, Mamoru Komachi, Kentaro Inui and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan fryu-i,mamoru-k,inui,matsul@is.naist.jp Abstract In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for refining their specifications. For this reason, we discuss issues in annotating these two types of relations, and propose a new specification for each. In accordance with the specification, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1, which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argum</context>
<context position="2817" citStr="Hasida, 2005" startWordPosition="427" endWordPosition="428">ogram, which is the successor of MUC, the details of specification of annotating coreference relation have been discussed for several years. On the other hand, the specification of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations such as the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA3-Tagged Corpus (Hasida, 2005). However, as we discuss in this paper, there is still much room for arguing and refining the specification of such sorts of semantic annotation. In fact, for neither of the above two corpora, the adequacy and reliability of the annotation scheme has been deeply examined. In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese 2http://www.lsi.upc.edu/˜srlconll/ 3The Global Document Annotation 132 Proceedings of the Linguistic Annotation Workshop, pages 132–139, Prague, June 2007. c�2007 Association for Computational Linguistics text. In Section 2 to S</context>
<context position="5811" citStr="Hasida, 2005" startWordPosition="881" endWordPosition="882">ON for adequacy and reliability of annotation. Therefore, the ACE data set has the drawback that not all coreference relations in the text are exhaustively annotated. It is insufficient to resolve only the annotated coreference relations in order to properly analyze a text. 4http://www-nlpir.nist.gov/related projects/muc/ proceedings/co task.html 2.2 Coreference annotated corpora of Japanese In parallel with these efforts, Japanese corpora have been developed that are annotated with coreference relations, such as the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and GDA-Tagged Corpus (Hasida, 2005). Before reviewing these works, we explain the relationship between anaphora and coreference in Japanese, referring to the following examples. In example (1), the pronoun sorei (it) points back to iPodi, and these two mentions refer to the same entity in the world and thus are considered both anaphoric and coreferential. (1) Tom-wa iPodi-o ka-tta . Tom-TOP iPodi-ACC buy-PAST PUNC Tom bought an iPod. kare-wa sorei-de ongaku-o ki-ita . he-TOP iti-INS music-ACC listen to-PAST PUNC He listened to music on it. On the other hand, in example (2), we still see an anaphoric relation between iPodi (iPod</context>
</contexts>
<marker>Hasida, 2005</marker>
<rawString>K. Hasida. 2005. Global document annotation (gda) annotation manual. http://i-content.org/tagman.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
</authors>
<title>MUC-7 coreference task definition. version 3.0.</title>
<date>1997</date>
<contexts>
<context position="1982" citStr="Hirschman, 1997" startWordPosition="293" endWordPosition="294">NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the specification of annotating corpora and the 1The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of specification of annotating coreference relation have been discussed for several years. On the other hand, the specification of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there ha</context>
</contexts>
<marker>Hirschman, 1997</marker>
<rawString>L. Hirschman. 1997. MUC-7 coreference task definition. version 3.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Anaphora resolution by antecedent identification followed by anaphoricity determination.</title>
<date>2005</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<pages>4--417</pages>
<contexts>
<context position="1192" citStr="Iida et al. (2005)" startWordPosition="176" endWordPosition="179">in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for refining their specifications. For this reason, we discuss issues in annotating these two types of relations, and propose a new specification for each. In accordance with the specification, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1, which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing field of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the specification of annotating corpora and the 1The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. F</context>
</contexts>
<marker>Iida, Inui, Matsumoto, 2005</marker>
<rawString>R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora resolution by antecedent identification followed by anaphoricity determination. ACM Transactions on Asian Language Information Processing (TALIP), 4:417–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Exploiting syntactic patterns as clues in zero-anaphora resolution.</title>
<date>2006</date>
<booktitle>In Proceddings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL),</booktitle>
<pages>625--632</pages>
<contexts>
<context position="1215" citStr="Iida et al. (2006)" startWordPosition="181" endWordPosition="184">s version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for refining their specifications. For this reason, we discuss issues in annotating these two types of relations, and propose a new specification for each. In accordance with the specification, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1, which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing field of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the specification of annotating corpora and the 1The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several ann</context>
</contexts>
<marker>Iida, Inui, Matsumoto, 2006</marker>
<rawString>R. Iida, K. Inui, and Y. Matsumoto. 2006. Exploiting syntactic patterns as clues in zero-anaphora resolution. In Proceddings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Semantic Structures. Current Studies in Linguistics 18.</title>
<date>1990</date>
<publisher>The MIT Press.</publisher>
<marker>Jackendoff, 1990</marker>
<rawString>R. Jackendoff. 1990. Semantic Structures. Current Studies in Linguistics 18. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kawahara</author>
<author>T Kurohashi</author>
<author>K Hasida</author>
</authors>
<title>Construction of a japanese relevance-tagged corpus (in japanese).</title>
<date>2002</date>
<booktitle>In Proceedings of the 8th Annual Meeting of the Association for Natural Language Processing,</booktitle>
<pages>495--498</pages>
<contexts>
<context position="634" citStr="Kawahara et al., 2002" startWordPosition="83" endWordPosition="86"> Japanese Text Corpus with Predicate-Argument and Coreference Relations Ryu Iida, Mamoru Komachi, Kentaro Inui and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan fryu-i,mamoru-k,inui,matsul@is.naist.jp Abstract In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for refining their specifications. For this reason, we discuss issues in annotating these two types of relations, and propose a new specification for each. In accordance with the specification, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1, which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Co</context>
<context position="2775" citStr="Kawahara et al., 2002" startWordPosition="418" endWordPosition="421">) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of specification of annotating coreference relation have been discussed for several years. On the other hand, the specification of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations such as the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA3-Tagged Corpus (Hasida, 2005). However, as we discuss in this paper, there is still much room for arguing and refining the specification of such sorts of semantic annotation. In fact, for neither of the above two corpora, the adequacy and reliability of the annotation scheme has been deeply examined. In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese 2http://www.lsi.upc.edu/˜srlconll/ 3The Global Document Annotation 132 Proceedings of the Linguistic Annotation Workshop, pages 132–139, Prague, June 2007. c�2007 Association for Computa</context>
<context position="5774" citStr="Kawahara et al., 2002" startWordPosition="873" endWordPosition="876">o named entities such as PERSON and ORGANIZATION for adequacy and reliability of annotation. Therefore, the ACE data set has the drawback that not all coreference relations in the text are exhaustively annotated. It is insufficient to resolve only the annotated coreference relations in order to properly analyze a text. 4http://www-nlpir.nist.gov/related projects/muc/ proceedings/co task.html 2.2 Coreference annotated corpora of Japanese In parallel with these efforts, Japanese corpora have been developed that are annotated with coreference relations, such as the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and GDA-Tagged Corpus (Hasida, 2005). Before reviewing these works, we explain the relationship between anaphora and coreference in Japanese, referring to the following examples. In example (1), the pronoun sorei (it) points back to iPodi, and these two mentions refer to the same entity in the world and thus are considered both anaphoric and coreferential. (1) Tom-wa iPodi-o ka-tta . Tom-TOP iPodi-ACC buy-PAST PUNC Tom bought an iPod. kare-wa sorei-de ongaku-o ki-ita . he-TOP iti-INS music-ACC listen to-PAST PUNC He listened to music on it. On the other hand, in example (2), we still see an a</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>D. Kawahara, T. Kurohashi, and K. Hasida. 2002. Construction of a japanese relevance-tagged corpus (in japanese). In Proceedings of the 8th Annual Meeting of the Association for Natural Language Processing, pages 495–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>H T Dang</author>
<author>M Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th National Conference on Artificial Intelligence and 12th Conference on on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>691--696</pages>
<contexts>
<context position="10032" citStr="Kipper et al., 2000" startWordPosition="1567" endWordPosition="1570">ingo-o tabe-ru Tom-NOM apple-ACC eat-PRES (Tom eats an apple.) Labeling predicate-argument relations in terms of syntactic cases has a few more advantages over semantic roles as far as Japanese is concerned: • Manual annotation of syntactic cases is likely to be more cost-efficient than semantic roles because they are often explicitly marked by case markers. This fact also allows us to avoid the difficulties in defining a label set. • In Japanese, the mapping from syntactic cases to semantic roles tends to be reasonably straightforward if a semantically rich lexicon of verbs like the VerbNet (Kipper et al., 2000) is available. • Furthermore, we have not yet found many NLP applications for which the utility of semantic roles is actually demonstrated. One may think of using semantic roles in textual inference as exemplified by, for example, Tatu and Moldovan (2006). However, similar sort of inference may well be realized with syntactic cases as demonstrated in the information extraction and question answering literature. Taking these respects into account, we choose to label predicate-argument relations in terms of syntactic cases, which follows the annotation scheme adopted in the Kyoto Corpus. 3.2 Syn</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-based construction of a verb lexicon. In Proceedings of the 17th National Conference on Artificial Intelligence and 12th Conference on on Innovative Applications of Artificial Intelligence, pages 691–696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The nombank project: An interimreport.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on Frontiers in Corpus Annotation.</booktitle>
<contexts>
<context position="15635" citStr="Meyers et al. (2004)" startWordPosition="2440" endWordPosition="2443"> one. [REL=ka-(u) (buy), GA=Mary, O=iPodi] The above examples indicate that predicateargument annotation in Japanese can potentially be annotated as either an IRA or ISA relation. Note that in Japanese these two relations cannot be explicitly separated by syntactic clues. Thus, in our corpus we annotate them without explicit distinction. It is arguable that separate treatment of IRA and ISA in predicate-argument annotation could be preferable. We consider this issue as a task of future work. A comparison of the specification is summarized in Table 1. 4 Annotating event-noun-argument relations Meyers et al. (2004) propose to annotate semantic relations between nouns referring to an event in the context, which we call event-nouns in this 135 paper. They release the NomBank corpus, in which PropBank-style semantic relations are annotated for event-nouns. In (10), for example, the noun “growth” refers to an event and “dividends” and “next year” are annotated as ARG1 (roughly corresponding to the theme role) and ARGM-TMP (temporal adjunct). (10) 12% growth in dividends next year [REL=growth, ARG1=in dividends, ARGM-TMP=next year] Following the PropBank-style annotation, the NomBank also restricts the searc</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The nombank project: An interimreport. In Proceedings of the HLT-NAACL Workshop on Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Anaphora Resolution. Studies in Language and Linguistics.</title>
<date>2002</date>
<publisher>Pearson Education.</publisher>
<contexts>
<context position="6943" citStr="Mitkov (2002)" startWordPosition="1061" endWordPosition="1062">ther hand, in example (2), we still see an anaphoric relation between iPodi (iPodi) and sorej (itj) and sorej points back to iPodi. However, these two mentions are not coreferential since they refer to different entities in the world. (2) Tom-wa iPodi-o ka-tta . Tom-TOP iPodi-ACC buy-PAST PUNC Tom bought an iPod. Mary-mo sorej-o ka-tta . Mary-TOP onej-ACC buy-PAST PUNC Mary also bought one. As in the above examples, an anaphoric relation can be either coreferential or not. The former case is called an identity-of-reference anaphora (IRA) and the latter an identity-of-sense anaphora (ISA) (see Mitkov (2002)). In English the difference between IRA and ISA is clearly expressed by the anaphoric relations formed with ‘it’ and ‘one’ respectively. This makes it possible to treat these classes separately. However, in Japanese, no such clear lexical distinction can be drawn. In both the Kyoto Corpus and GDA-Tagged Corpus, there is no discussion in regards to distinction between ISA and IRA, thus it is unclear what types of coreference relations the annotators annotated. To make matters worse, their approaches do not consider whether or not a mention refers to a specific entity like in the EDT task. 2.3 </context>
</contexts>
<marker>Mitkov, 2002</marker>
<rawString>Ruslan Mitkov. 2002. Anaphora Resolution. Studies in Language and Linguistics. Pearson Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1748" citStr="Ng and Cardie, 2002" startWordPosition="259" endWordPosition="262">erence and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing field of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the specification of annotating corpora and the 1The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of specification of annotating coreference relation have been discussed for several years. On </context>
<context position="4393" citStr="Ng and Cardie (2002)" startWordPosition="663" endWordPosition="666">ion task and we conclude in Section 7. 2 Annotating coreference relations 2.1 Approaches to coreference annotation Coreference annotation in English has been evolving mainly in the context of information extraction. For instance, in the 6th and 7th Message Understanding Conferences (MUC), coreference resolution is treated as a subtask of information extraction4. The annotated corpora built in the MUC contain coreference relations between NPs, which are used as a gold standard data set for machine learning-based approaches to coreference resolution by researchers such as Soon et al. (2001) and Ng and Cardie (2002). However, van Deemter and Kibble (1999) claim that the specification of the MUC coreference task guides us to annotate expressions that are not normally considered coreferential, such as appositive relations (e.g. Julius Caesari, a well-known emperori, ...). In the task of Entity Detection and Tracking (EDT) in the Automatic Content Extraction (ACE) program (Doddington et al., 2004), the successor of MUC, the coreference relations are redefined in terms of two concepts, mentions and entities, in order to avoid inappropriate co-indexing. In the specification of EDT, mentions are defined as the</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th ACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="2540" citStr="Palmer et al., 2005" startWordPosition="384" endWordPosition="387">tated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of specification of annotating coreference relation have been discussed for several years. On the other hand, the specification of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations such as the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA3-Tagged Corpus (Hasida, 2005). However, as we discuss in this paper, there is still much room for arguing and refining the specification of such sorts of semantic annotation. In fact, for neither of the above two corpora, the adequacy and reliability of the annotation scheme has been deeply examined. In this paper, we discuss how to annotate corefere</context>
<context position="8459" citStr="Palmer et al., 2005" startWordPosition="1301" endWordPosition="1304">as coreferential in case they satisfy the following two conditions: • The mentions refer to not a generic entity but to a specific entity. • The relation between the mentions is considered as an IRA relation. 3 Annotating predicate-argument relations 3.1 Labels of predicate-argument relations One debatable issue in the annotation of predicateargument relations is what level of abstraction we should label those relations at. The GDA-Tagged Corpus, for example, adopts a fixed set of somewhat “traditional” semantic roles such as Agent, Theme, and Goal that are defined across verbs. The PropBank (Palmer et al., 2005), on the other hand, defines a set of semantic roles (labeled ARG0, ARG1, and AM-ADV, etc.) for each verb and annotates each sentence in the corpus with those labels as in (3). (3) [ARGM−TMP A year earlier], [ARG0 the refiner] [rel earned] [ARG1 $66 million, or $1.19 a share]. In the FrameNet (Fillmore and Baker, 2000), a specific set of semantic roles is defined for each set of semantically-related verbs called a FrameNet frame. However, there is still only limited consensus on how many kinds of semantic roles should be identified and which linguistic theory we should adopt to define them at </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>R Mehta</author>
<author>A Maroudas</author>
<author>J Hitzeman</author>
</authors>
<title>Learning to resolve bridging references.</title>
<date>2004</date>
<booktitle>In Proceddings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>144--151</pages>
<contexts>
<context position="2003" citStr="Poesio et al., 2004" startWordPosition="295" endWordPosition="299">uch as information extraction and machine translation. With the research focus placed on these tasks, the specification of annotating corpora and the 1The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of specification of annotating coreference relation have been discussed for several years. On the other hand, the specification of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research</context>
</contexts>
<marker>Poesio, Mehta, Maroudas, Hitzeman, 2004</marker>
<rawString>M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004. Learning to resolve bridging references. In Proceddings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1727" citStr="Soon et al., 2001" startWordPosition="255" endWordPosition="258">ta set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing field of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the specification of annotating corpora and the 1The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of specification of annotating coreference relation have been discussed f</context>
<context position="4368" citStr="Soon et al. (2001)" startWordPosition="658" endWordPosition="661"> issues of each annotation task and we conclude in Section 7. 2 Annotating coreference relations 2.1 Approaches to coreference annotation Coreference annotation in English has been evolving mainly in the context of information extraction. For instance, in the 6th and 7th Message Understanding Conferences (MUC), coreference resolution is treated as a subtask of information extraction4. The annotated corpora built in the MUC contain coreference relations between NPs, which are used as a gold standard data set for machine learning-based approaches to coreference resolution by researchers such as Soon et al. (2001) and Ng and Cardie (2002). However, van Deemter and Kibble (1999) claim that the specification of the MUC coreference task guides us to annotate expressions that are not normally considered coreferential, such as appositive relations (e.g. Julius Caesari, a well-known emperori, ...). In the task of Entity Detection and Tracking (EDT) in the Automatic Content Extraction (ACE) program (Doddington et al., 2004), the successor of MUC, the coreference relations are redefined in terms of two concepts, mentions and entities, in order to avoid inappropriate co-indexing. In the specification of EDT, me</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tatu</author>
<author>D Moldovan</author>
</authors>
<title>A logic-based semantic approach to recognizing textual entailment.</title>
<date>2006</date>
<booktitle>In Proceddings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL),</booktitle>
<pages>819--826</pages>
<contexts>
<context position="10287" citStr="Tatu and Moldovan (2006)" startWordPosition="1610" endWordPosition="1613">ikely to be more cost-efficient than semantic roles because they are often explicitly marked by case markers. This fact also allows us to avoid the difficulties in defining a label set. • In Japanese, the mapping from syntactic cases to semantic roles tends to be reasonably straightforward if a semantically rich lexicon of verbs like the VerbNet (Kipper et al., 2000) is available. • Furthermore, we have not yet found many NLP applications for which the utility of semantic roles is actually demonstrated. One may think of using semantic roles in textual inference as exemplified by, for example, Tatu and Moldovan (2006). However, similar sort of inference may well be realized with syntactic cases as demonstrated in the information extraction and question answering literature. Taking these respects into account, we choose to label predicate-argument relations in terms of syntactic cases, which follows the annotation scheme adopted in the Kyoto Corpus. 3.2 Syntactic case alternation Once the level of syntactic cases is chosen for our annotation, another issue immediately arises, alteration of syntactic cases by syntactic transformations such as passivization and causativization. For example, sentence (5) is an</context>
</contexts>
<marker>Tatu, Moldovan, 2006</marker>
<rawString>M. Tatu and D. Moldovan. 2006. A logic-based semantic approach to recognizing textual entailment. In Proceddings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), pages 819–826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tsuchiya</author>
<author>T Utsuro</author>
<author>S Matsuyoshi</author>
<author>S Sato</author>
<author>S Nakagawa</author>
</authors>
<title>Development and analysis of an example database of japanese compound functional expressions.</title>
<date>2006</date>
<journal>IPSJ Journal,</journal>
<volume>47</volume>
<issue>6</issue>
<contexts>
<context position="23859" citStr="Tsuchiya et al. (2006)" startWordPosition="3704" endWordPosition="3707">redicates and event-nouns Identification of predicates is sometimes unreliable due to the ambiguity between a literal usage and a compound functional usage. For instance, the expression “to-shi-te”, which includes the verb shi (to do), is ambiguous: either the verb shi functions as a content word, i.e. an event-denoting word, or it constitutes a multi-word expression together with to and te. In the latter case, it does not make sense to interpret the verb shi to denote an event. However, this judgment is highly context-dependent and we have not been able to devise a reliable criterion for it. Tsuchiya et al. (2006) have built a functional expression-tagged corpus for automatically classifying these usages. They reported that the agreement ratio of functional expressions is higher than ours. We believe their findings to also become helpful information for annotating predicates in our corpus. With regards to event-nouns, a similar problem corpus size PropBank I NomBank 0.8 ACE (2005 English) GDA Corpus Kyoto Corpus NAIST Corpus (ours) 7,891 sentences 24,311 sentences 269 articles 2,177 articles 555 articles (5,127 sentences) 2,929 articles (38,384 sentences) 137 Table 2: Statistics: annotating predicate-a</context>
</contexts>
<marker>Tsuchiya, Utsuro, Matsuyoshi, Sato, Nakagawa, 2006</marker>
<rawString>M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nakagawa. 2006. Development and analysis of an example database of japanese compound functional expressions. IPSJ Journal, 47(6):1728–1741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
<author>R Kibble</author>
</authors>
<title>What is coreference, and what should coreference annotation be?</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL ’99 Workshop on Coreference and its applications,</booktitle>
<pages>90--96</pages>
<marker>van Deemter, Kibble, 1999</marker>
<rawString>K. van Deemter and R. Kibble. 1999. What is coreference, and what should coreference annotation be? In Proceedings of the ACL ’99 Workshop on Coreference and its applications, pages 90–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<contexts>
<context position="22137" citStr="Vilain et al., 1995" startWordPosition="3431" endWordPosition="3434">rgument and coreference relations. The data size of our corpus along with other corpora is shown in Table 3. Next, to evaluate the agreement between the two human annotators, 287 randomly selected articles were annotated by both of them. The results are evaluated by calculating recall and precision in which one annotation result is regarded as correct and the other’s as the output of system. Note that only the predicates annotated by both annotators are used in calculating recall and precision. For evaluation of coreference relations, we calculated recall and precision based on the MUC score (Vilain et al., 1995). The results are shown in Table 4, where we can see that most annotating work was done with high quality except for the ni-argument of event-nouns. The most common source of error was caused by verb alternation, and we will discuss this Table 3: Data size of each corpus Table 4: Agreement of annotating each relation recall precision predicate 0.947 (6512/6880) 0.941 (6512/6920) ga (NOM) 0.861 (5638/6549) 0.856 (5638/6567) o (ACC) 0.943 (2447/2595) 0.919 (2447/2664) ni (DAT) 0.892 (1060/1189) 0.817 (1060/1298) event-noun 0.905 (1281/1415) 0.810 (1281/1582) ga (NOM) 0.798 (1038/1300) 0.804 (103</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference (MUC-6), pages 45–52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>