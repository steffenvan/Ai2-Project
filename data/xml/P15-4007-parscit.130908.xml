<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001196">
<title confidence="0.985505">
Visual Error Analysis for Entity Linking
</title>
<author confidence="0.831069">
Michael Strube
</author>
<affiliation confidence="0.75036">
Heidelberg Institute for
</affiliation>
<address confidence="0.666380666666667">
Theoretical Studies gGmbH
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
</address>
<email confidence="0.57252">
michael.strube@h-its.org
</email>
<author confidence="0.798693">
Benjamin Heinzerling
</author>
<affiliation confidence="0.473113">
Research Training Group AIPHES
Heidelberg Institute for
Theoretical Studies gGmbH
</affiliation>
<address confidence="0.802137">
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
</address>
<email confidence="0.848445">
benjamin.heinzerling@h-its.org
</email>
<sectionHeader confidence="0.993681" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968555555556">
We present the Visual Entity Explorer
(VEX), an interactive tool for visually ex-
ploring and analyzing the output of en-
tity linking systems. VEX is designed to
aid developers in improving their systems
by visualizing system results, gold anno-
tations, and various mention detection and
entity linking error types in a clear, con-
cise, and customizable manner.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99973635">
Entity linking (EL) is the task of automatically
linking mentions of entities (e.g. persons, loca-
tions, organizations) in a text to their correspond-
ing entry in a given knowledge base (KB), such as
Wikipedia or Freebase. Depending on the setting,
the task may also require detection of entity men-
tions1, as well as identifying and clustering Not-
In-Lexicon (NIL) entities.
In recent years, the increasing interest in EL, re-
flected in the emergence of shared tasks such as
the TAC Entity Linking track (Ji et al., 2014), ERD
2014 (Carmel et al., 2014), and NEEL (Cano et
al., 2014), has fostered research on evaluation met-
rics for EL systems, leading to the development of
a dedicated scorer that covers different aspects of
EL system results using multiple metrics (Hachey
et al., 2014).
Based on the observation that representations in
entity linking (mentions linked to the same KB
entry) are very similar to those encountered in
</bodyText>
<footnote confidence="0.9908355">
1This setting is called Entity Discovery and Linking
(EDL) in the TAC 2014/15 entity linking tracks, and En-
tity Recognition and Disambiguation (ERD) in the Microsoft
ERD 2014 challenge.
</footnote>
<bodyText confidence="0.999671225806452">
coreference resolution (mentions linked by coref-
erence relations to the same entity), these metrics
include ones originally proposed for evaluation of
coreference resolutions systems, such as the MUC
score (Vilain et al., 1995), B3 (Bagga and Bald-
win, 1998), and CEAF (Luo, 2005) and variants
thereof (Cai and Strube, 2010).
While such metrics, which express system per-
formance in numeric terms of precision, recall,
and F1 scores, are well-suited for comparing sys-
tems, they are of limited use to EL system devel-
opers trying to identify problem areas and compo-
nents whose improvement will likely result in the
largest performance increase.
To address this problem, we present the Visual
Entity Explorer (VEX), an interactive tool for vi-
sually exploring the results produced by an EL
system. To our knowledge, there exist no other
dedicated tools for visualizing the output of EL
systems or similar representations.
VEX is available as free, open-source soft-
ware for download at http://github.com/
noutenki/vex and as a web service at http:
//cosyne.h-its.org/vex.
In the remainder of this paper, we first give an
overview of VEX (Section 2), proceed to present
several usage examples and discuss some of the in-
sights gained from performing a visual error anal-
ysis (Section 3), then describe its implementation
(Section 4), before concluding and discussing fu-
ture work (Section 5).
</bodyText>
<sectionHeader confidence="0.965841" genericHeader="method">
2 The Visual Entity Explorer
</sectionHeader>
<bodyText confidence="0.999670333333333">
After loading system results and gold standard an-
notations in TAC 2014 or JSON format, as well
as the original document text files, VEX displays
</bodyText>
<page confidence="0.996054">
37
</page>
<note confidence="0.8393135">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 37–42,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<figureCaption confidence="0.9069335">
Figure 1: Screenshot of VEX’s main display, consisting of document list (left), entity selectors (bottom
right), and the annotated document text (top right).
</figureCaption>
<bodyText confidence="0.999405625">
gold annotations, correct results, and errors as
shown in Figure 1. The document to be analyzed
can be selected via the clickable list of document
IDs on the left. Located bottom right, the entity
selectors for gold, true positive, and false positive
entities (defined below) can be used to toggle the
display of individual entities2. The selected enti-
ties are visualized in the top-right main area.
Similarly to the usage in coreference resolution,
where a cluster of mentions linked by coreference
relations is referred to as an entity, we define en-
tity to mean a cluster of mentions clustered either
implicitly by being linked to the same KB entry
(in case of non-NIL mentions) or clustered explic-
itly by performing NIL clustering (in case of NIL
mentions).
</bodyText>
<footnote confidence="0.9544515">
2For space reasons, the entity selectors are shown only
partially.
</footnote>
<subsectionHeader confidence="0.998805">
2.1 Visualizing Entity Linking Errors
</subsectionHeader>
<bodyText confidence="0.9997988">
Errors committed by an EL system can be broadly
categorized into mention detection errors and link-
ing/clustering errors. Mention detection errors, in
turn, can be divided into partial errors and full er-
rors.
</bodyText>
<subsectionHeader confidence="0.544406">
2.1.1 Partial Mention Detection Errors
</subsectionHeader>
<bodyText confidence="0.999849714285714">
A partial mention detection error is a system men-
tion span that overlaps but is not identical to any
gold mention span. In VEX, partial mention detec-
tion errors are displayed using red square brackets,
either inside or outside the gold mention spans sig-
nified by golden-bordered rectangles (cf. the first
and last mention in Figure 2).
</bodyText>
<subsubsectionHeader confidence="0.654894">
2.1.2 Full Mention Detection Errors
</subsubsectionHeader>
<bodyText confidence="0.99991325">
A full mention detection error is either (a) a sys-
tem mention span that has no overlapping gold
mention span at all, corresponding to a false pos-
itive (FP) detection, i.e. a precision error, or (b) a
</bodyText>
<page confidence="0.998372">
38
</page>
<figureCaption confidence="0.9996825">
Figure 2: Visualization of various mention detec-
tion and entity linking error types (see Section 2
for a detailed description).
Figure 3: Visualization showing a mention detec-
</figureCaption>
<bodyText confidence="0.9831195">
tion error and an annotation error (see Section 3
for a description).
gold mention span that has no overlap with any
system mention span, corresponding to a false
negative (FN) detection, i.e. a recall error. In VEX,
FP mention detections are marked by a dashed red
border and struck-out red text (cf. the second men-
tion in Figure 2), and FN mention detections by a
dashed gold-colored border and black text (cf. the
third mention in Figure 2). For further emphasis,
both gold and system mentions are displayed in
bold font.
</bodyText>
<subsectionHeader confidence="0.95051">
2.1.3 Linking/Clustering Errors
</subsectionHeader>
<bodyText confidence="0.999942666666667">
Entities identified by the system are categorized
– and possibly split up – into True Positive (TP)
and False Positive (FP) entities. The mentions of
system entities are connected using dashed green
lines for TP entities and dashed red lines for FP en-
tities, while gold entity mentions are connected by
solid gold-colored lines. This choice of line styles
prevents loss of information through occlusion in
case of two lines connecting the same pair of men-
tions, as is the case with the first and last mention
in Figure 2.
Additionally, the text of system mentions linked
to the correct KB entry or identified correctly as
NIL is colored green and any text associated with
erroneous system entity links red.
</bodyText>
<sectionHeader confidence="0.92518" genericHeader="method">
3 Usage examples
</sectionHeader>
<bodyText confidence="0.9998635">
In this section we show how VEX can be used to
perform a visual error analysis, gaining insights
that arguably cannot be attained by relying only
on evaluation metrics.
</bodyText>
<subsectionHeader confidence="0.99584">
3.1 Example 1
</subsectionHeader>
<bodyText confidence="0.9965315">
Figure 2 shows mentions of VULCAN INC.3 as
identified by an EL system (marked red and green)
</bodyText>
<subsectionHeader confidence="0.61002">
3In this paper, SMALL CAPS denote KB entries.
</subsectionHeader>
<bodyText confidence="0.999887952380952">
and the corresponding gold annotation4 (marked
in gold color). Of the three gold mentions, two
were detected and linked correctly by the system
and are thus colored green and connected with
a green dashed line. One gold mention is sur-
rounded with a gold-colored dashed box to indi-
cate a FN mention not detected by the system at
all. The dashed red box signifies a FP entity, re-
sulting from the system having detected a mention
that is not listed in the gold standard. However,
rather than a system error, this is arguably an an-
notation mistake.
Inspection of other entities and other documents
reveals that spurious FPs caused by gold anno-
tation errors appear to be a common occurrence
(see Figure 3 for another example). Since the su-
pervised machine learning algorithms commonly
used for named entity recognition, such as Con-
ditional Random Fields (Sutton and McCallum,
2007), require consistent training data, such incon-
sistencies hamper performance.
</bodyText>
<subsectionHeader confidence="0.979078">
3.2 Example 2
</subsectionHeader>
<bodyText confidence="0.999866125">
From Figure 2 we can also tell that two men-
tion detection errors are caused by the inclusion
of sentence-final punctuation that doubles as ab-
breviation marker. The occurrence of similar cases
in other documents, e.g. inconsistent annotation of
“U.S.” and “U.S” as mentions of UNITED STATES,
shows the need for consistently applied annotation
guidelines.
</bodyText>
<subsectionHeader confidence="0.985367">
3.3 Example 3
</subsectionHeader>
<bodyText confidence="0.999521666666667">
Another type of mention detection error is shown
in Figure 3: Here the system fails to detect “wash-
ington” as a mention of WASHINGTON, D.C.,
</bodyText>
<footnote confidence="0.99571">
4The gold annotations are taken from the TAC 2014 EDL
Evaluation Queries and Links (V1.1).
</footnote>
<page confidence="0.998847">
39
</page>
<bodyText confidence="0.919058">
likely due to the non-standard lower-case spelling.
</bodyText>
<subsectionHeader confidence="0.819693">
3.4 Example 4
</subsectionHeader>
<bodyText confidence="0.999994153846154">
The visualization of the gold mentions of PAUL
ALLEN in Figure 1 shows that the EL system sim-
plistically partitioned and linked the mentions ac-
cording to string match, resulting in three system
entities, of which only the first, consisting of the
two “Paul Allen” mentions, is a TP. Even though
the four “Allen” mentions in Figure 1 align cor-
rectly with gold mentions, they are categorized as
a FP entity, since the system erroneously linked
them to the KB entry for the city of Allen, Texas,
resulting in a system entity that does not intersect
with any gold entity. The system commits a simi-
lar mistake for the mention “Paul”.
</bodyText>
<sectionHeader confidence="0.780334" genericHeader="method">
3.5 Insights
</sectionHeader>
<bodyText confidence="0.9920265">
This analysis of only a few examples has already
revealed several categories of errors, either com-
mitted by the EL system or resulting from gold
annotation mistakes:
</bodyText>
<listItem confidence="0.98069975">
• mention detection errors due to non-standard
letter case, which suggest incorporating true-
casing (Lita et al., 2003) and/or a caseless
named entity recognition model (Manning et
al., 2014) into the mention detection process
could improve performance;
• mention detection errors due to off-by-one
errors involving punctuation, which sug-
gest the need for clear and consistently ap-
plied annotation guidelines, enabling devel-
opers to add hard-coded, task-specific post-
processing rules for dealing with such cases;
• mention detection errors due to missing gold
standard annotations, which suggest per-
forming a simple string match against already
annotated mentions to find cases of unanno-
tated mentions could significantly improve
the gold standard at little cost;
• linking/clustering errors, likely due to the
overly strong influence of features based on
</listItem>
<bodyText confidence="0.856142571428571">
string match with Wikipedia article titles,
which in some cases appears to outweigh
features designed to encourage clustering of
mentions if there exists a substring match be-
tween them, hence leading to an erroneous
partitioning of the gold entity by its various
surface forms.
</bodyText>
<sectionHeader confidence="0.995733" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.999991444444445">
In this section we describe VEX’s implementation
and some of the design decisions made to achieve
an entity visualization suited for convenient error
analysis.
VEX consists of three main components. The
input component, implemented in Java 8, reads
gold and system annotations files, as well as the
original documents. Currently, the annotation for-
mat read by the official TAC 2014 scorer5, as well
as a simple JSON input format are supported. All
system and gold character offset ranges contained
in the input files are converted into HTML spans
and inserted into the document text. Since HTML
elements are required to conform to a tree struc-
ture, any overlap or nesting of spans is handled by
breaking up such spans into non-overlapping sub-
spans.
At this point, gold NIL clusters and system
NIL clusters are aligned by employing the Kuhn-
Munkres algorithm6 (Kuhn, 1955; Munkres,
1957), as is done in calculation of the CEAF met-
ric (Luo, 2005). The input component then
stores all inserted, non-overlapping spans in an in-
memory database.
The processing component queries gold
and system entity data for each document and
inventorizes all errors of interest. All data col-
lected by this component is added to the respec-
tive HTML spans in the form of CSS classes, en-
abling simple customization of the visualization
via a plain-text stylesheet.
The output component employs a tem-
plate engine7 to convert the data collected by
the processing component into HTML and
JavaScript for handling display and user interac-
tion in the web browser.
</bodyText>
<subsectionHeader confidence="0.976599">
4.1 Design Decisions
</subsectionHeader>
<bodyText confidence="0.999192">
One of VEX’s main design goals is enabling the
user to quickly identify entity linking and clus-
tering errors. Because a naive approach to entity
visualization by drawing edges between all possi-
ble pairings of mention spans quickly leads to a
cluttered graph (Figure 4a), we instead visualize
entities using Euclidean minimum spanning trees,
inspired by Martschat and Strube’s (2014) use of
</bodyText>
<footnote confidence="0.99912825">
5http://github.com/wikilinks/neleval
6Also known as Hungarian algorithm.
7https://github.com/jknack/handlebars.
java
</footnote>
<page confidence="0.995938">
40
</page>
<figure confidence="0.999466">
(a) (b)
</figure>
<figureCaption confidence="0.987457333333333">
Figure 4: Cluttered visualization of an entity via its complete graph, drawing all pairwise connections
between mentions (a), and a more concise visualization of the same entity using an Euclidean minimum
spanning tree, connecting all mentions while minimizing total edge length (b).
</figureCaption>
<bodyText confidence="0.997858153846154">
spanning trees in error analysis for coreference
resolution.
An Euclidean minimum spanning tree is a min-
imum spanning tree (MST) of a graph whose ver-
tices represent points in a metric space and whose
edge weights are the spatial distances between
points8, i.e., it spans all graph vertices while min-
imizing total edge length. This allows for a much
more concise visualization (Figure 4b).
Since the actual positions of mention span el-
ements on the user’s screen depend on various
user environment factors such as font size and
browser window dimensions, the MSTs of dis-
played entities are computed using a client-side
JavaScript library9 and are automatically redrawn
if the browser window is resized. Drawing of
edges is performed via jsPlumb10, a highly cus-
tomizable library for line drawing in HTML doc-
uments.
In order not to overemphasize mention detec-
tion errors when displaying entities, VEX assumes
a system mention span to be correct if it has a non-
zero overlap with a gold mention span. For exam-
ple, consider the first gold mention “Vulcan Inc”
in Figure 2, which has not been detected correctly
by the system; it detected “Vulcan Inc.” instead.
</bodyText>
<footnote confidence="0.984305375">
8In our case, the metric space is the DOM document being
rendered by the web browser, a point is the top-left corner of a
text span element, and the distance metric is the pixel distance
between the top-left corners of text span elements.
9https://github.com/abetusk/
euclideanmst.js. This library employs Kruskal’s
algorithm (Kruskal, 1956) for finding MSTs.
10http://www.jsplumb.org
</footnote>
<bodyText confidence="0.999826428571429">
While a strict evaluation requiring perfect men-
tion spans will give no credit at all for this par-
tially correct result, seeing that this mention de-
tection error is already visually signified (by the
red square bracket), VEX treats the mention as de-
tected correctly for the purpose of visualizing the
entity graph, and counts it as a true positive in-
stance if it has been linked correctly.
While VEX provides sane defaults, the visual-
ization style can be easily customized via CSS,
e.g., in order to achieve a finer-grained catego-
rization of error types such as off-by-one mention
detection errors, or classification of non-NILs as
NILs and vice-versa.
</bodyText>
<sectionHeader confidence="0.998176" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999353125">
We presented the Visual Entity Explorer (VEX),
a tool for visual error analysis of entity linking
(EL) systems. We have shown how VEX can be
used for quickly identifying the components of an
EL system that appear to have a high potential for
improvement, as well as for finding errors in the
gold standard annotations. Since visual error anal-
ysis of our own EL system revealed several issues
and possible improvements, we believe perform-
ing such an analysis will prove useful for other de-
velopers of EL systems, as well.
In future work, we plan to extend VEX with
functionality for visualizing additional error types,
and for exploring entities not only in a single doc-
ument, but across documents. Given the structural
similarities entities in coreference resolution and
</bodyText>
<page confidence="0.998059">
41
</page>
<bodyText confidence="0.983153333333333">
entities in entity linking share, we also will add
methods for visualizing entities found by corefer-
ence resolution systems.
</bodyText>
<sectionHeader confidence="0.990265" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999868111111111">
This work has been supported by the German Re-
search Foundation as part of the Research Training
Group “Adaptive Preparation of Information from
Heterogeneous Sources” (AIPHES) under grant
No. GRK 1994/1, and partially funded by the
Klaus Tschira Foundation, Heidelberg, Germany.
We would like to thank our colleague Sebastian
Martschat who commented on earlier drafts of this
paper.
</bodyText>
<sectionHeader confidence="0.997738" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9916788375">
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings
of the 1st International Conference on Language
Resources and Evaluation, Granada, Spain, 28–30
May 1998, pages 563–566.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In
Proceedings of the SIGdial 2010 Conference: The
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, Tokyo, Japan, 24–25
September 2010, pages 28–36.
Amparo E. Cano, Giuseppe Rizzo, Andrea Varga,
Matthew Rowe, Milan Stankovic, and Aba-Sah
Dadzie. 2014. Making sense of microposts named
entity extraction &amp; linking challenge. In Proceed-
ings of the 4th Workshop on Making Sense of Micro-
posts, Seoul, Korea, 7 April 2014, pages 54–60.
David Carmel, Ming-Wei Chang, Evgeniy Gabrilovich,
Bo-June Paul Hsu, and Kuansan Wang. 2014.
ERD’14: Entity recognition and disambiguation
challenge. In ACM SIGIR Forum, volume 48, pages
63–77. ACM.
Ben Hachey, Joel Nothman, and Will Radford. 2014.
Cheap and easy entity evaluation. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Pa-
pers), Baltimore, Md., 22–27 June 2014, pages 464–
469.
Heng Ji, Joel Nothman, and Ben Hachey. 2014.
Overview of TAC-KBP2014 entity discovery and
linking tasks. In Proceedings of the Text Analy-
sis Conference, National Institute of Standards and
Technology, Gaithersburg, Maryland, USA, 17–18
November 2014.
Joseph B. Kruskal. 1956. On the shortest spanning
subtree of a graph and the traveling salesman prob-
lem. Proceedings of the American Mathematical so-
ciety, 7(1):48–50.
Harold W. Kuhn. 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2(1-2):83–97.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and
Nanda Kambhatla. 2003. Truecasing. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, Sapporo, Japan, 7–
12 July 2003, pages 152–159. Association for Com-
putational Linguistics.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of the Hu-
man Language Technology Conference and the 2005
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 6–8
October 2005, pages 25–32.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics: System Demonstrations, Balti-
more, Md., 22–27 June 2014, pages 55–60. Associ-
ation for Computational Linguistics.
Sebastian Martschat and Michael Strube. 2014. Recall
error analysis for coreference resolution. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing, Doha, Qatar,
25–29 October 2014, pages 2070–2081.
James Munkres. 1957. Algorithms for the assignment
and transportation problems. Journal of the Society
for Industrial &amp; Applied Mathematics, 5(1):32–38.
Charles Sutton and Andrew McCallum. 2007. An in-
troduction to conditional random fields for relational
learning. In L. Getoor and B. Taskar, editors, In-
troduction to Statistical Relational Learning, pages
93–128. MIT Press, Cambridge, Mass.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th Message Understanding Conference
(MUC-6), pages 45–52, San Mateo, Cal. Morgan
Kaufmann.
</reference>
<page confidence="0.999285">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.343580">
<title confidence="0.99932">Visual Error Analysis for Entity Linking</title>
<author confidence="0.99987">Michael Strube</author>
<affiliation confidence="0.9432605">Heidelberg Institute for Theoretical Studies gGmbH</affiliation>
<address confidence="0.9521905">Schloss-Wolfsbrunnenweg 35 69118 Heidelberg, Germany</address>
<email confidence="0.996955">michael.strube@h-its.org</email>
<author confidence="0.87922">Benjamin</author>
<affiliation confidence="0.86458425">Research Training Group Heidelberg Institute Theoretical Studies Schloss-Wolfsbrunnenweg</affiliation>
<address confidence="0.999646">69118 Heidelberg, Germany</address>
<email confidence="0.999315">benjamin.heinzerling@h-its.org</email>
<abstract confidence="0.9880961">We present the Visual Entity Explorer (VEX), an interactive tool for visually exploring and analyzing the output of entity linking systems. VEX is designed to aid developers in improving their systems by visualizing system results, gold annotations, and various mention detection and entity linking error types in a clear, concise, and customizable manner.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation,</booktitle>
<pages>563--566</pages>
<location>Granada,</location>
<contexts>
<context position="2127" citStr="Bagga and Baldwin, 1998" startWordPosition="316" endWordPosition="320">tiple metrics (Hachey et al., 2014). Based on the observation that representations in entity linking (mentions linked to the same KB entry) are very similar to those encountered in 1This setting is called Entity Discovery and Linking (EDL) in the TAC 2014/15 entity linking tracks, and Entity Recognition and Disambiguation (ERD) in the Microsoft ERD 2014 challenge. coreference resolution (mentions linked by coreference relations to the same entity), these metrics include ones originally proposed for evaluation of coreference resolutions systems, such as the MUC score (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF (Luo, 2005) and variants thereof (Cai and Strube, 2010). While such metrics, which express system performance in numeric terms of precision, recall, and F1 scores, are well-suited for comparing systems, they are of limited use to EL system developers trying to identify problem areas and components whose improvement will likely result in the largest performance increase. To address this problem, we present the Visual Entity Explorer (VEX), an interactive tool for visually exploring the results produced by an EL system. To our knowledge, there exist no other dedicated tools for visual</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the 1st International Conference on Language Resources and Evaluation, Granada, Spain, 28–30 May 1998, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Cai</author>
<author>Michael Strube</author>
</authors>
<title>Evaluation metrics for end-to-end coreference resolution systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the SIGdial 2010 Conference: The 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>28--36</pages>
<location>Tokyo, Japan, 24–25</location>
<contexts>
<context position="2193" citStr="Cai and Strube, 2010" startWordPosition="328" endWordPosition="331">presentations in entity linking (mentions linked to the same KB entry) are very similar to those encountered in 1This setting is called Entity Discovery and Linking (EDL) in the TAC 2014/15 entity linking tracks, and Entity Recognition and Disambiguation (ERD) in the Microsoft ERD 2014 challenge. coreference resolution (mentions linked by coreference relations to the same entity), these metrics include ones originally proposed for evaluation of coreference resolutions systems, such as the MUC score (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF (Luo, 2005) and variants thereof (Cai and Strube, 2010). While such metrics, which express system performance in numeric terms of precision, recall, and F1 scores, are well-suited for comparing systems, they are of limited use to EL system developers trying to identify problem areas and components whose improvement will likely result in the largest performance increase. To address this problem, we present the Visual Entity Explorer (VEX), an interactive tool for visually exploring the results produced by an EL system. To our knowledge, there exist no other dedicated tools for visualizing the output of EL systems or similar representations. VEX is </context>
</contexts>
<marker>Cai, Strube, 2010</marker>
<rawString>Jie Cai and Michael Strube. 2010. Evaluation metrics for end-to-end coreference resolution systems. In Proceedings of the SIGdial 2010 Conference: The 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Tokyo, Japan, 24–25 September 2010, pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amparo E Cano</author>
<author>Giuseppe Rizzo</author>
<author>Andrea Varga</author>
<author>Matthew Rowe</author>
<author>Milan Stankovic</author>
<author>Aba-Sah Dadzie</author>
</authors>
<title>Making sense of microposts named entity extraction &amp; linking challenge.</title>
<date>2014</date>
<booktitle>In Proceedings of the 4th Workshop on Making Sense of Microposts, Seoul,</booktitle>
<pages>54--60</pages>
<contexts>
<context position="1332" citStr="Cano et al., 2014" startWordPosition="193" endWordPosition="196">nd customizable manner. 1 Introduction Entity linking (EL) is the task of automatically linking mentions of entities (e.g. persons, locations, organizations) in a text to their corresponding entry in a given knowledge base (KB), such as Wikipedia or Freebase. Depending on the setting, the task may also require detection of entity mentions1, as well as identifying and clustering NotIn-Lexicon (NIL) entities. In recent years, the increasing interest in EL, reflected in the emergence of shared tasks such as the TAC Entity Linking track (Ji et al., 2014), ERD 2014 (Carmel et al., 2014), and NEEL (Cano et al., 2014), has fostered research on evaluation metrics for EL systems, leading to the development of a dedicated scorer that covers different aspects of EL system results using multiple metrics (Hachey et al., 2014). Based on the observation that representations in entity linking (mentions linked to the same KB entry) are very similar to those encountered in 1This setting is called Entity Discovery and Linking (EDL) in the TAC 2014/15 entity linking tracks, and Entity Recognition and Disambiguation (ERD) in the Microsoft ERD 2014 challenge. coreference resolution (mentions linked by coreference relatio</context>
</contexts>
<marker>Cano, Rizzo, Varga, Rowe, Stankovic, Dadzie, 2014</marker>
<rawString>Amparo E. Cano, Giuseppe Rizzo, Andrea Varga, Matthew Rowe, Milan Stankovic, and Aba-Sah Dadzie. 2014. Making sense of microposts named entity extraction &amp; linking challenge. In Proceedings of the 4th Workshop on Making Sense of Microposts, Seoul, Korea, 7 April 2014, pages 54–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Carmel</author>
<author>Ming-Wei Chang</author>
<author>Evgeniy Gabrilovich</author>
<author>Bo-June Paul Hsu</author>
<author>Kuansan Wang</author>
</authors>
<title>ERD’14: Entity recognition and disambiguation challenge.</title>
<date>2014</date>
<journal>In ACM SIGIR Forum,</journal>
<volume>48</volume>
<pages>63--77</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1302" citStr="Carmel et al., 2014" startWordPosition="187" endWordPosition="190">ror types in a clear, concise, and customizable manner. 1 Introduction Entity linking (EL) is the task of automatically linking mentions of entities (e.g. persons, locations, organizations) in a text to their corresponding entry in a given knowledge base (KB), such as Wikipedia or Freebase. Depending on the setting, the task may also require detection of entity mentions1, as well as identifying and clustering NotIn-Lexicon (NIL) entities. In recent years, the increasing interest in EL, reflected in the emergence of shared tasks such as the TAC Entity Linking track (Ji et al., 2014), ERD 2014 (Carmel et al., 2014), and NEEL (Cano et al., 2014), has fostered research on evaluation metrics for EL systems, leading to the development of a dedicated scorer that covers different aspects of EL system results using multiple metrics (Hachey et al., 2014). Based on the observation that representations in entity linking (mentions linked to the same KB entry) are very similar to those encountered in 1This setting is called Entity Discovery and Linking (EDL) in the TAC 2014/15 entity linking tracks, and Entity Recognition and Disambiguation (ERD) in the Microsoft ERD 2014 challenge. coreference resolution (mentions</context>
</contexts>
<marker>Carmel, Chang, Gabrilovich, Hsu, Wang, 2014</marker>
<rawString>David Carmel, Ming-Wei Chang, Evgeniy Gabrilovich, Bo-June Paul Hsu, and Kuansan Wang. 2014. ERD’14: Entity recognition and disambiguation challenge. In ACM SIGIR Forum, volume 48, pages 63–77. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Joel Nothman</author>
<author>Will Radford</author>
</authors>
<title>Cheap and easy entity evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>464--469</pages>
<location>Baltimore, Md.,</location>
<contexts>
<context position="1538" citStr="Hachey et al., 2014" startWordPosition="226" endWordPosition="229">ven knowledge base (KB), such as Wikipedia or Freebase. Depending on the setting, the task may also require detection of entity mentions1, as well as identifying and clustering NotIn-Lexicon (NIL) entities. In recent years, the increasing interest in EL, reflected in the emergence of shared tasks such as the TAC Entity Linking track (Ji et al., 2014), ERD 2014 (Carmel et al., 2014), and NEEL (Cano et al., 2014), has fostered research on evaluation metrics for EL systems, leading to the development of a dedicated scorer that covers different aspects of EL system results using multiple metrics (Hachey et al., 2014). Based on the observation that representations in entity linking (mentions linked to the same KB entry) are very similar to those encountered in 1This setting is called Entity Discovery and Linking (EDL) in the TAC 2014/15 entity linking tracks, and Entity Recognition and Disambiguation (ERD) in the Microsoft ERD 2014 challenge. coreference resolution (mentions linked by coreference relations to the same entity), these metrics include ones originally proposed for evaluation of coreference resolutions systems, such as the MUC score (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF </context>
</contexts>
<marker>Hachey, Nothman, Radford, 2014</marker>
<rawString>Ben Hachey, Joel Nothman, and Will Radford. 2014. Cheap and easy entity evaluation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Baltimore, Md., 22–27 June 2014, pages 464– 469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Joel Nothman</author>
<author>Ben Hachey</author>
</authors>
<title>Overview of TAC-KBP2014 entity discovery and linking tasks.</title>
<date>2014</date>
<booktitle>In Proceedings of the Text Analysis Conference, National Institute of Standards and Technology,</booktitle>
<location>Gaithersburg, Maryland, USA,</location>
<contexts>
<context position="1270" citStr="Ji et al., 2014" startWordPosition="181" endWordPosition="184">ection and entity linking error types in a clear, concise, and customizable manner. 1 Introduction Entity linking (EL) is the task of automatically linking mentions of entities (e.g. persons, locations, organizations) in a text to their corresponding entry in a given knowledge base (KB), such as Wikipedia or Freebase. Depending on the setting, the task may also require detection of entity mentions1, as well as identifying and clustering NotIn-Lexicon (NIL) entities. In recent years, the increasing interest in EL, reflected in the emergence of shared tasks such as the TAC Entity Linking track (Ji et al., 2014), ERD 2014 (Carmel et al., 2014), and NEEL (Cano et al., 2014), has fostered research on evaluation metrics for EL systems, leading to the development of a dedicated scorer that covers different aspects of EL system results using multiple metrics (Hachey et al., 2014). Based on the observation that representations in entity linking (mentions linked to the same KB entry) are very similar to those encountered in 1This setting is called Entity Discovery and Linking (EDL) in the TAC 2014/15 entity linking tracks, and Entity Recognition and Disambiguation (ERD) in the Microsoft ERD 2014 challenge. </context>
</contexts>
<marker>Ji, Nothman, Hachey, 2014</marker>
<rawString>Heng Ji, Joel Nothman, and Ben Hachey. 2014. Overview of TAC-KBP2014 entity discovery and linking tasks. In Proceedings of the Text Analysis Conference, National Institute of Standards and Technology, Gaithersburg, Maryland, USA, 17–18 November 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph B Kruskal</author>
</authors>
<title>On the shortest spanning subtree of a graph and the traveling salesman problem.</title>
<date>1956</date>
<booktitle>Proceedings of the American Mathematical society,</booktitle>
<pages>7--1</pages>
<contexts>
<context position="14622" citStr="Kruskal, 1956" startWordPosition="2350" endWordPosition="2351">g entities, VEX assumes a system mention span to be correct if it has a nonzero overlap with a gold mention span. For example, consider the first gold mention “Vulcan Inc” in Figure 2, which has not been detected correctly by the system; it detected “Vulcan Inc.” instead. 8In our case, the metric space is the DOM document being rendered by the web browser, a point is the top-left corner of a text span element, and the distance metric is the pixel distance between the top-left corners of text span elements. 9https://github.com/abetusk/ euclideanmst.js. This library employs Kruskal’s algorithm (Kruskal, 1956) for finding MSTs. 10http://www.jsplumb.org While a strict evaluation requiring perfect mention spans will give no credit at all for this partially correct result, seeing that this mention detection error is already visually signified (by the red square bracket), VEX treats the mention as detected correctly for the purpose of visualizing the entity graph, and counts it as a true positive instance if it has been linked correctly. While VEX provides sane defaults, the visualization style can be easily customized via CSS, e.g., in order to achieve a finer-grained categorization of error types suc</context>
</contexts>
<marker>Kruskal, 1956</marker>
<rawString>Joseph B. Kruskal. 1956. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical society, 7(1):48–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<pages>2--1</pages>
<contexts>
<context position="11638" citStr="Kuhn, 1955" startWordPosition="1877" endWordPosition="1878">ystem annotations files, as well as the original documents. Currently, the annotation format read by the official TAC 2014 scorer5, as well as a simple JSON input format are supported. All system and gold character offset ranges contained in the input files are converted into HTML spans and inserted into the document text. Since HTML elements are required to conform to a tree structure, any overlap or nesting of spans is handled by breaking up such spans into non-overlapping subspans. At this point, gold NIL clusters and system NIL clusters are aligned by employing the KuhnMunkres algorithm6 (Kuhn, 1955; Munkres, 1957), as is done in calculation of the CEAF metric (Luo, 2005). The input component then stores all inserted, non-overlapping spans in an inmemory database. The processing component queries gold and system entity data for each document and inventorizes all errors of interest. All data collected by this component is added to the respective HTML spans in the form of CSS classes, enabling simple customization of the visualization via a plain-text stylesheet. The output component employs a template engine7 to convert the data collected by the processing component into HTML and JavaScri</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(1-2):83–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucian Vlad Lita</author>
<author>Abe Ittycheriah</author>
<author>Salim Roukos</author>
<author>Nanda Kambhatla</author>
</authors>
<date>2003</date>
<booktitle>Truecasing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>7</volume>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo,</location>
<contexts>
<context position="9737" citStr="Lita et al., 2003" startWordPosition="1577" endWordPosition="1580">en” mentions in Figure 1 align correctly with gold mentions, they are categorized as a FP entity, since the system erroneously linked them to the KB entry for the city of Allen, Texas, resulting in a system entity that does not intersect with any gold entity. The system commits a similar mistake for the mention “Paul”. 3.5 Insights This analysis of only a few examples has already revealed several categories of errors, either committed by the EL system or resulting from gold annotation mistakes: • mention detection errors due to non-standard letter case, which suggest incorporating truecasing (Lita et al., 2003) and/or a caseless named entity recognition model (Manning et al., 2014) into the mention detection process could improve performance; • mention detection errors due to off-by-one errors involving punctuation, which suggest the need for clear and consistently applied annotation guidelines, enabling developers to add hard-coded, task-specific postprocessing rules for dealing with such cases; • mention detection errors due to missing gold standard annotations, which suggest performing a simple string match against already annotated mentions to find cases of unannotated mentions could significant</context>
</contexts>
<marker>Lita, Ittycheriah, Roukos, Kambhatla, 2003</marker>
<rawString>Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and Nanda Kambhatla. 2003. Truecasing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan, 7– 12 July 2003, pages 152–159. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>25--32</pages>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="2149" citStr="Luo, 2005" startWordPosition="323" endWordPosition="324"> Based on the observation that representations in entity linking (mentions linked to the same KB entry) are very similar to those encountered in 1This setting is called Entity Discovery and Linking (EDL) in the TAC 2014/15 entity linking tracks, and Entity Recognition and Disambiguation (ERD) in the Microsoft ERD 2014 challenge. coreference resolution (mentions linked by coreference relations to the same entity), these metrics include ones originally proposed for evaluation of coreference resolutions systems, such as the MUC score (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF (Luo, 2005) and variants thereof (Cai and Strube, 2010). While such metrics, which express system performance in numeric terms of precision, recall, and F1 scores, are well-suited for comparing systems, they are of limited use to EL system developers trying to identify problem areas and components whose improvement will likely result in the largest performance increase. To address this problem, we present the Visual Entity Explorer (VEX), an interactive tool for visually exploring the results produced by an EL system. To our knowledge, there exist no other dedicated tools for visualizing the output of EL</context>
<context position="11712" citStr="Luo, 2005" startWordPosition="1891" endWordPosition="1892"> annotation format read by the official TAC 2014 scorer5, as well as a simple JSON input format are supported. All system and gold character offset ranges contained in the input files are converted into HTML spans and inserted into the document text. Since HTML elements are required to conform to a tree structure, any overlap or nesting of spans is handled by breaking up such spans into non-overlapping subspans. At this point, gold NIL clusters and system NIL clusters are aligned by employing the KuhnMunkres algorithm6 (Kuhn, 1955; Munkres, 1957), as is done in calculation of the CEAF metric (Luo, 2005). The input component then stores all inserted, non-overlapping spans in an inmemory database. The processing component queries gold and system entity data for each document and inventorizes all errors of interest. All data collected by this component is added to the respective HTML spans in the form of CSS classes, enabling simple customization of the visualization via a plain-text stylesheet. The output component employs a template engine7 to convert the data collected by the processing component into HTML and JavaScript for handling display and user interaction in the web browser. 4.1 Desig</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada, 6–8 October 2005, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Md.,</location>
<contexts>
<context position="9809" citStr="Manning et al., 2014" startWordPosition="1588" endWordPosition="1591"> categorized as a FP entity, since the system erroneously linked them to the KB entry for the city of Allen, Texas, resulting in a system entity that does not intersect with any gold entity. The system commits a similar mistake for the mention “Paul”. 3.5 Insights This analysis of only a few examples has already revealed several categories of errors, either committed by the EL system or resulting from gold annotation mistakes: • mention detection errors due to non-standard letter case, which suggest incorporating truecasing (Lita et al., 2003) and/or a caseless named entity recognition model (Manning et al., 2014) into the mention detection process could improve performance; • mention detection errors due to off-by-one errors involving punctuation, which suggest the need for clear and consistently applied annotation guidelines, enabling developers to add hard-coded, task-specific postprocessing rules for dealing with such cases; • mention detection errors due to missing gold standard annotations, which suggest performing a simple string match against already annotated mentions to find cases of unannotated mentions could significantly improve the gold standard at little cost; • linking/clustering errors</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Baltimore, Md., 22–27 June 2014, pages 55–60. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Martschat</author>
<author>Michael Strube</author>
</authors>
<title>Recall error analysis for coreference resolution.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>2070--2081</pages>
<location>Doha,</location>
<marker>Martschat, Strube, 2014</marker>
<rawString>Sebastian Martschat and Michael Strube. 2014. Recall error analysis for coreference resolution. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, 25–29 October 2014, pages 2070–2081.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Munkres</author>
</authors>
<title>Algorithms for the assignment and transportation problems.</title>
<date>1957</date>
<journal>Journal of the Society for Industrial &amp; Applied Mathematics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="11654" citStr="Munkres, 1957" startWordPosition="1879" endWordPosition="1880">tions files, as well as the original documents. Currently, the annotation format read by the official TAC 2014 scorer5, as well as a simple JSON input format are supported. All system and gold character offset ranges contained in the input files are converted into HTML spans and inserted into the document text. Since HTML elements are required to conform to a tree structure, any overlap or nesting of spans is handled by breaking up such spans into non-overlapping subspans. At this point, gold NIL clusters and system NIL clusters are aligned by employing the KuhnMunkres algorithm6 (Kuhn, 1955; Munkres, 1957), as is done in calculation of the CEAF metric (Luo, 2005). The input component then stores all inserted, non-overlapping spans in an inmemory database. The processing component queries gold and system entity data for each document and inventorizes all errors of interest. All data collected by this component is added to the respective HTML spans in the form of CSS classes, enabling simple customization of the visualization via a plain-text stylesheet. The output component employs a template engine7 to convert the data collected by the processing component into HTML and JavaScript for handling </context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>James Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of the Society for Industrial &amp; Applied Mathematics, 5(1):32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>An introduction to conditional random fields for relational learning.</title>
<date>2007</date>
<booktitle>Introduction to Statistical Relational Learning,</booktitle>
<pages>93--128</pages>
<editor>In L. Getoor and B. Taskar, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="8051" citStr="Sutton and McCallum, 2007" startWordPosition="1298" endWordPosition="1301">old-colored dashed box to indicate a FN mention not detected by the system at all. The dashed red box signifies a FP entity, resulting from the system having detected a mention that is not listed in the gold standard. However, rather than a system error, this is arguably an annotation mistake. Inspection of other entities and other documents reveals that spurious FPs caused by gold annotation errors appear to be a common occurrence (see Figure 3 for another example). Since the supervised machine learning algorithms commonly used for named entity recognition, such as Conditional Random Fields (Sutton and McCallum, 2007), require consistent training data, such inconsistencies hamper performance. 3.2 Example 2 From Figure 2 we can also tell that two mention detection errors are caused by the inclusion of sentence-final punctuation that doubles as abbreviation marker. The occurrence of similar cases in other documents, e.g. inconsistent annotation of “U.S.” and “U.S” as mentions of UNITED STATES, shows the need for consistently applied annotation guidelines. 3.3 Example 3 Another type of mention detection error is shown in Figure 3: Here the system fails to detect “washington” as a mention of WASHINGTON, D.C., </context>
</contexts>
<marker>Sutton, McCallum, 2007</marker>
<rawString>Charles Sutton and Andrew McCallum. 2007. An introduction to conditional random fields for relational learning. In L. Getoor and B. Taskar, editors, Introduction to Statistical Relational Learning, pages 93–128. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, Cal.</location>
<contexts>
<context position="2097" citStr="Vilain et al., 1995" startWordPosition="311" endWordPosition="314">L system results using multiple metrics (Hachey et al., 2014). Based on the observation that representations in entity linking (mentions linked to the same KB entry) are very similar to those encountered in 1This setting is called Entity Discovery and Linking (EDL) in the TAC 2014/15 entity linking tracks, and Entity Recognition and Disambiguation (ERD) in the Microsoft ERD 2014 challenge. coreference resolution (mentions linked by coreference relations to the same entity), these metrics include ones originally proposed for evaluation of coreference resolutions systems, such as the MUC score (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF (Luo, 2005) and variants thereof (Cai and Strube, 2010). While such metrics, which express system performance in numeric terms of precision, recall, and F1 scores, are well-suited for comparing systems, they are of limited use to EL system developers trying to identify problem areas and components whose improvement will likely result in the largest performance increase. To address this problem, we present the Visual Entity Explorer (VEX), an interactive tool for visually exploring the results produced by an EL system. To our knowledge, there exist no ot</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference (MUC-6), pages 45–52, San Mateo, Cal. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>