<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001789">
<title confidence="0.984947">
Distributed Parse Mining
</title>
<author confidence="0.971043">
Scott A. Waterman, PhD
</author>
<affiliation confidence="0.945023">
Microsoft Live Search/Powerset
</affiliation>
<address confidence="0.939183">
475 Brannan St.
San Francisco, USA
</address>
<email confidence="0.99896">
waterman@acm.org
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999661833333333">
We describe the design and implementation of
a system for data exploration over dependency
parses and derived semantic representations
in a large-scale NLP-based search system at
powerset.com. Because of the distributed
nature of the document repository and the pro-
cessing infrastructure, and also the complex
representations of the corpus data, standard
text analysis tools such as grep or awk or
language modeling toolkits are not applicable.
This paper explores the challenges of extract-
ing statistical information and of building lan-
guage models in such a distributed NLP envi-
ronment, and introduces a corpus analysis sys-
tem, Oceanography, that simplifies the writ-
ing of analysis code and transparently takes
advantage of existing distributed processing
infrastructure.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999899">
In computational linguistics we deal with large cor-
pora and vast amounts of data from which we would
like to extract useful information. The size of the
text resources, derived linguistic analyses, and the
complexity of their representations is often a stum-
bling block on the way to understanding the statisti-
cal and linguistic behavior within the corpus. Sim-
ple software tools suffice for small or simple anal-
ysis problems, or for building models of easily rep-
resented relations. However, as the size of data, the
intricacy of relations to be analyzed, and the com-
plexity of the representation grow, so too does the
technical difficulty of conducting the analysis.
</bodyText>
<page confidence="0.951269">
56
</page>
<bodyText confidence="0.999970583333333">
Software is our given means of escape from this
escalation of complexity. However, as “computa-
tional linguists,” we often find ourselves spending
more time and attention building software to per-
form the required computations than we do on un-
derstanding the linguistics.
Even once a suitable set of NLP tools (e.g. tag-
gers, chunkers, parsers, etc.) has been chosen, anal-
ysis software, in the CL world, often consists of
“throw away” scripts. Small, ad hoc programs are
often the norm, often with no assurance (via strict
design or testing) of correctness or completeness.
</bodyText>
<subsectionHeader confidence="0.96715">
1.1 Oceanography
</subsectionHeader>
<bodyText confidence="0.999977263157895">
Our goal is to ensure that analysis is not so prob-
lematic. Powerset is a group within the Microsoft
Live Search team focused on using semantic NLP
to improve web search. We face many problems
with the scale and integration of our NLP compo-
nents, and are approaching solving them by applying
sound software design and abstraction principles to
corpus processing. By generalizing tools to fit the
processing environment, and the nature of the prob-
lems at hand, we enable flexible processing which
scales with the size of the platform and the data.
The Oceanography software environment is de-
signed to address two important needs in large cor-
pus analysis. The first is to simplify the actual pro-
gramming of analysis code to reduce development
time and increase reliability, and the second is to use
the available distributed computing resources to re-
duce running time and provide for rapid testing and
experimental turnaraound.
</bodyText>
<note confidence="0.995758">
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 56–64,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.738475">
1.2 Linguistic and Diagnostic data analysis
</subsectionHeader>
<bodyText confidence="0.99998752">
There are two separate kinds of analysis we want
to support over this processed corpus. The first is
linguistic modeling. In order to achieve the best se-
mantic interpretation of each source document, we
seek to understand the linguistic behavior within
the corpus. Probabilistic parsing, entity extraction,
sense disambiguation, and argument role assign-
ment are all informed by structured, statistical mod-
els of word behavior within the corpus. Some mod-
els can be built from simple tokenized text, while
other models need to incorporate parse dependen-
cies or real-word knowledge of entities. Some of
these tasks are exploratory and underspecified (e.g.
selectional restrictions), while others, such as name
tagging, have a well-developed literature and a num-
ber of almost standard methodologies.
The second kind of analysis is aimed at character-
izing and improving system behavior. For example,
distributions of POS-tags or preposition attachments
can serve as regression indicators for parser perfor-
mance. In order to perform error analysis, we need
to selectively sample various types of label assign-
ments or parse structures. So summarization and
sampling from the various intermediate NL analyses
are very important processes to support.
</bodyText>
<sectionHeader confidence="0.912562" genericHeader="method">
2 Generalizing Text Mining
</sectionHeader>
<bodyText confidence="0.999946825">
We have found that most of these analysis and data
modeling tasks share certain higher order steps that
allow us to generalize them all into a single pro-
gramming framework. All involve identifying some
phenomena in one of the NLP outputs, represent-
ing it in some characteristic form, and then sum-
ming or comparing distributions. These general
steps apply to many corpus tasks, including building
n-gram data, learning sentence breaks, identifying
selectional preferences, or building role mappings
for verb nominalizations.
The Oceanography system generalizes these steps
into a declarative language for stating the selection
of data, and the form of output, in a way that avoids
repetitive and error prone boilerplate code for file
traversal, regular expression matching, and statistics
programming. By matching a declarative syntax to
the general analysis steps, these common functions
can be relegated to library code, or wrapped into the
executable in the compilation step. The less time
spent in describing a task, or in coding and debug-
ging the implementation, the more time and atten-
tion can be spent in understanding the results and
modeling the linguistic processes that underly the
data.
This sort of abstraction away from the details
of file representation, storage architecture, and pro-
cessing model fits a general trend toward data min-
ing, or text mining (Feldman and Dagan, 1995). In
data mining or KDD systems (Fayyad et al., 1996),
the goal is to separate the tasks of creative anal-
ysis and theorizing from the mundane aspects of
traversing the data collection and computing statis-
tics. These are much the same goals emphasized
by Tukey (1977) – exploration of the data and in-
teractions in order to understand which hypotheses,
and which models of interaction, would be fruitful
to explore. For our needs in analyzing collections of
text, parses, and semantic representations, we have
achieved a very practical step toward these goals.
</bodyText>
<subsectionHeader confidence="0.994473">
2.1 Matching process to conception
</subsectionHeader>
<bodyText confidence="0.9991712">
We have found four steps that map very closely to
our conception of the data analysis problem, which
at the same time are easily translated to implemen-
tations that can be run on both small local data sets
and on very large distributed corpora.
</bodyText>
<listItem confidence="0.979616083333333">
1. Pattern matching – find the interesting phe-
nomena among the mass of data, by declaring a
set of desired properties to be met. In Oceanog-
raphy, these are matched per-sentence.
2. Transformation – rewrite the raw occurrence
data to identify the interesting part, and isolate
it from the background
3. Aggregation – group together instances of the
same kind
4. Statistics – compute statistics for counts, relative
frequency, conditional distributions, distribu-
tional comparisons, etc.
</listItem>
<bodyText confidence="0.984453">
In the following sections we describe the nature of
each step in more detail, map these steps to a declar-
ative data analysis language, give some motivating
examples, and describe how these steps are typically
</bodyText>
<page confidence="0.997325">
57
</page>
<bodyText confidence="0.99976975">
accomplished in an exploratory setting for NLP in-
vestigations.
Later, in section 4, we describe how the steps
are mapped to processing operations within the NLP
pipeline architecture. Following that, we give exam-
ples of how this framework maps to specific prob-
lems, of both the exploratory and the diagnostic
type.
</bodyText>
<subsectionHeader confidence="0.999053">
2.2 Pattern Matching
</subsectionHeader>
<bodyText confidence="0.999984338983051">
The first step is to identify the specific phenomena of
interest within the source data. If the data is a com-
plex structure, it is helpful to express the patterns in
a logical representation of the structure, rather than
matching the representation directly.
Pattern matching in Oceanography for depen-
dency parse structures is handled using a domain
specific language (DSL) built explicitly for pattern-
based manipulation of parse trees and semantic rep-
resentations generated by the XLE linguistic compo-
nents (Crouch et al., 2006). This Transfer language
(Crouch, 2006) is normally used in the regular lin-
guistic processing pipeline to incrementally rewrite
LFG dependency parses into a role-labeled seman-
tic representations (semreps) of entities, events, and
relations in the text. Transfer matches pattern rules
to a current set of parse dependencies or semantic
facts, and writes alternate expressions derived from
the matched components. Variables in these expres-
sions are bound via Prolog-style unification (Huet,
1975).
For example, in figure 1, the first expression
word(· · ·) will match word forms in a parse that
are ‘verb’s, and bind %VerbSk variable to a
unique occurrence id and %VerbWord to the verb
lemma. The second pattern finds the node in the
dependency graph that fills the ob (object) role for
that verb, and extracts its lemmas. (The %%’s are
placeholder variables in the pattern, needed to match
the arity of the expression.) Below, in the same
figure, is a representation of the verb and object
from a parse of the phrase “determined the struc-
ture”. On matching these facts, the VerbWord and
ObjLemma variables would be bound to the strings
determine and structure.
In a simpler environment, with more basic textual
representations, this pattern matching step would be
written with regular expressions, for example using
the familiar grep command. The balance provided
by grep between the simplicity of its operational
model (a transform from stdin to stdout) and the ex-
pressiveness of the regular expressions allows grep
to be a workhorse for data analysis over text.
However, except for simple cases such as word
cooccurrence models, the typical need in deep lin-
guistic analysis is not well served by regular expres-
sions over strings. Anyone in the NLP field who
has written regular expressions to match, say, part-
of-speech labeled text knows the difficulties of hav-
ing a pattern language which differs from the logical
structures being matched. Another typical solution
is to write a short program in a scripting language
(e.g. perl, python, SNOBOL) which combines regu-
lar expressions to provide a simple structure parser.
Tgrep (Pito, 1993) is a one such program which ex-
tends this regular expression notion to patterns over
trees, and can output subtrees matching those ex-
pressions, but only provided they are represented as
text in the LDC TreeBank format.
</bodyText>
<subsectionHeader confidence="0.996874">
2.3 Transformation
</subsectionHeader>
<bodyText confidence="0.999969714285714">
Once the items of the pattern have been identified in
their original context, it is often necessary to isolate
them from that context, and remove the extraneous,
irrelevant information. For instance, if one is do-
ing a simple word count, the tokenized words of text
must be separated from any annotation and counted
independently. For more complicated tasks, such as
finding a verb’s distribution of occurrence with di-
rect objects, the verb and object need to be isolated
from the remainder of the parse tree, perhaps as the
simple tuple (verb, object), or in a more complex
structure, with additional dependent information.
In our case, we express the transformed output of
each pattern match with an expression built from the
unification variables bound to the match. In figure 2,
we construct a vo pair of (verb, object). This new
construct is simply added to the collection of facts
and representations already present. All other pre-
existing facts in the NL analysis of the sentence also
remain in context, potentially available for aggrega-
tion and counting.
</bodyText>
<figureCaption confidence="0.627202">
==&gt; vo_pair(%VerbWord, %ObjLemma).
Figure 2: Transforming the matched pattern
</figureCaption>
<page confidence="0.984013">
58
</page>
<construct confidence="0.44736">
word(%VerbSk, %VerbWord, verb, verb, %%, %%, %%, %% ),
in_context(%%, role(hier(ob, %%), %VerbSk, %ObjLemma:%%))
word(determine:n(41,3),determine,verb,verb, ....)
in_context(t,role(hier(ob,[[ob,root],..]),
determine:n(82,3),structure:n(91,3))))
</construct>
<figureCaption confidence="0.998761">
Figure 1: Pattern matching using Transfer
</figureCaption>
<bodyText confidence="0.999983666666667">
In shallower text mining, this might be accom-
plished using regex matching in a perl program. An-
other common approach is to use command-line text
tools such as awk or sed. Awk (Aho et al., 1977)
is designed especially for text mining, but is limited
to plain text files, on single machines, and doesn’t
extend easily to structured graph representations or
distributed processing. (But see, e.g. Sawzall (Pike
et al., 2005) for a scalable awk-like language.)
</bodyText>
<subsectionHeader confidence="0.993016">
2.4 Aggregation
</subsectionHeader>
<bodyText confidence="0.999961481481482">
The aggregation step collects the extracted instances
and groups them by type and by key. Rather
than have the matched, transformed results simply
dumped out in some enormous file or database in
their order of occurrence in the data set (as one
would get e.g. from grep), it is quite useful even in
the simplest of cases to aggregate all similar output
items. This condenses the mass of data selected, and
allows one to see the extent and diversity of the items
that are found by the patterns. This simple counting
is often enough for diagnostic tasks, and sometimes
for exploratory tasks when a statistical judgement is
not yet desired. The aggregation key might be, for
various kinds of extraction: the head noun of an NN-
compound, or the error type for parse errors, or the
controlling verb of a relative clause.
In Oceanography, we require a declaration of the
data that will be aggregated, in order to separate it
from the remainder which will be discarded. These
declarations take the form of familiar static type dec-
larations, in the style of C++ or Java. Figure 3
shows the simple declaration for our vo pair type,
where both fields are declared as strings. These
named fields also provide a handle to refer to struc-
ture members in later statements.
In the command line text world, aggregation
might be accomplished by using the unix pipeline
</bodyText>
<equation confidence="0.8422345">
vo_pair :: {
verb::String, object::String }
</equation>
<figureCaption confidence="0.996743">
Figure 3: Declaring aggregation types
</figureCaption>
<bodyText confidence="0.997807">
command sort I uniq -c , to organize the out-
put by the appropriate key. If using a small program
to do this kind of analysis, one would use a dictio-
nary or hash-table and sorting routines to organize
the data before output.
</bodyText>
<subsectionHeader confidence="0.971328">
2.5 Statistics
</subsectionHeader>
<bodyText confidence="0.999868923076923">
With the matched and extracted data, one can build
up a statistical picture of the data and its interrela-
tions. In our practice, and in the computational NLP
literature, we have found a few fundamental statisti-
cal operations that are frequently used to make sense
of the corpus data. Primary among these are sim-
ple class counts: the number of occurrences of a
given phenomena. For instance, the count of part-
of-speech tags, or of head nouns with adjective mod-
ifiers, or the counts of (verb,object) pairs. These
counts can be computed easily by summing the oc-
currences in the aggregated groups.
Other statistics are more complicated, requiring
combinations of the simple counts and sums —
normalizing distributions by the total occurrence
counts, for instance, as in the conditional occurrence
of a part-of-speech label relative to the frequency
of the token. Estimation of log-likelihood ratios
or Pearson’s Chi-square test for pairwise correlation
also falls in this category. These kinds of computa-
tions are used heavily for building classifiers and for
diagnostic purposes.
Higher order functions of the counts are also in-
teresting, in which various distributions compared.
These include computing KL distance between con-
ditional distributions for similarity measurements,
</bodyText>
<page confidence="0.994774">
59
</page>
<bodyText confidence="0.9885645">
clustering over similarity, and building predictive or
classification models for model corpus behavior.
</bodyText>
<sectionHeader confidence="0.949892" genericHeader="method">
3 Data Parallel Document Processing at
Powerset
</sectionHeader>
<bodyText confidence="0.999976372881356">
To simplify the processing of large web document
collections, and flexibly include new processing
modules, we have built a single consistent pro-
cessing architecture for the natural language doc-
ument pipeline, which allows us to process mil-
lions of documents and handle terabytes of analy-
sis data effectively. Coral is the name of the dis-
tributed, document-parallel NLP pipeline at Power-
set. Coral provides both a process and a data man-
agement framework in order to smoothly execute the
multi-step linguistic analysis of all content indexed
for Powerset’s search.
Coral controls a multi-step pipeline for deep lin-
guistic processing of documents indexed for search.
A partial list of the steps every web document un-
dergoes includes: HTML destructuring, sentence
breaking, name tagging, parsing, semantic inter-
pretation, anaphora resolution, and indexing. The
pipeline is similar to the UIMA (Ferrucci and Lally,
2004) architecture in that each step adds interme-
diate data — tagged spans, dependency trees, co-
referent expressions — that can be used in subse-
quent steps. Each step adds a different kind of data
to the set, with its own labels and meanings. The
output of all these steps is a daunting amount of in-
formation, all of which is valuable for understanding
the linguistic relations within the text, and also the
behavior and effectiveness of the NLP pipeline.
Documents are processed in a data-parallel fash-
ion. Multiple documents are processed indepen-
dently, across multiple processes on multiple com-
pute nodes within a clustered environment. The doc-
ument processing model is sequential, with multi-
ple steps run in a fixed sequence for each document
in the index. All processing for a single document
is typically performed on a single compute node.
The steps of the pipeline communicate through in-
termediate data writen to the local filesystem in be-
tween steps, where each step is free to consume data
produced earlier. Output from the stages is check-
pointed to backing storage at various points along
the way, and the final index fragments are merged at
the end.
This kind of data-parallel process lends itself well
to a map/reduce programming infrastructure (Dean
and Ghemawat, 2004). Map/reduce divides process-
ing into two classes: data-parallel ‘map’ operations,
and commutative ‘reduce’ operations, in which all
map output aggregated under a particular key is pro-
cessed together. In map/reduce terms, the entire
linguistic processing runs as a sequence of ‘map’
steps (there is no inter-document communication),
with a final ‘reduce’ step to collect index fragments
and construct a unified search index. Coral uses the
open-source hadoop implementation of map/reduce
(Cutting, ) as the central task control and distribu-
tion mechanism for assigning NLP pipeline jobs to
documents in the input data, and it has full control
of the map/reduce processing layer.
</bodyText>
<subsectionHeader confidence="0.954706">
3.1 Difficulties for data mining in Coral
</subsectionHeader>
<bodyText confidence="0.9999955">
All of the intermediate processing output of the
pipeline, the name tags, parses, semantic representa-
tions, etc., are are retained by this complex process.
Unfortunately, they are retained in an unfriendly
format: small document-addressed chunks scattered
across a large distributed filesystem, on hundreds of
machines. There is no operational way to collect
these chunks in any single file, or to traverse them
efficiently from any single point. Traditional script-
ing techniques, even if scalable to the terabytes of
data, are not applicable to the distributed organiza-
tion of the underlying data.
</bodyText>
<subsectionHeader confidence="0.9455635">
3.2 Re-using processing infrastructure for
mining
</subsectionHeader>
<bodyText confidence="0.999971928571429">
However, we can re-use the same Coral process and
data management for the problems of data analy-
sis. The breakdown of parse-mining steps presented
earlier, in addition to providing a coherent model
for data analysis, also maps very cleanly to the
distributed map/reduce computational model. By
translating the four steps of any analysis into corre-
sponding map/reduce operations across the linguis-
tic pipeline data, we can efficiently translate the cor-
pus analytics to an arbitrarily large data setting. Fur-
ther, because we can rely on the Coral process and
data management infrastructure to handle the data
movement and traversal, we allow the researcher or
language engineer to concentrate on specifying the
</bodyText>
<page confidence="0.996494">
60
</page>
<bodyText confidence="0.979546820512821">
patterns and relations to be investigated, rather than
burdening them with complex yet necessary details.
4 Oceanography - a compiled data mining
language
Oceanography has a compiler that transforms short
analysis programs into multiple map/reduce steps
that will operate over the corpus of text and deep lin-
guistic analyses. These multiple sub-operations are
then farmed out through the distributed cluster envi-
ronment, managed by the Coral job controller. The
data flow and dependencies between these jobs are
compiled to a Coral-specific job control language.
An oceanography program (cf. figure 4) is a
single-file description of the data analysis task. It
contains specifications for each of the four oper-
ations: pattern matching, transformation, aggrega-
tion, and statistics. The program style is declarative
– there are no instructions for iterating over files,
summing distributions, or parsing the dependency
graph representations.
We find that this matches our intuitions and con-
ception of the parse mining task. A statement of
the end-product of the analysis is natural: e.g. find
the conditional distribution of object head nouns for
verbs, or symbolically p(obj verb). The style of the
oceanography program matches this well, where the
statistics statement such as
dist triple.object cond on triple.ve
states the desired output, and the preceding pat-
tern match and type declarations serve as definitions
to specify precisely what is meant by the variable
names.
In the following sections, we will follow the steps
of the Oceanography program in the listing in fig-
ure 4. The example analysis presented is a simple
one – to find all verbs with both subject and object
roles, i.e. triples of (subject, object, verb), and re-
port some counts and relative frequencies of verbs,
subjects, and objects.
</bodyText>
<subsectionHeader confidence="0.999102">
4.1 Step 1: Pattern Matching
</subsectionHeader>
<bodyText confidence="0.997516777777778">
The pattern matching rules are similar to those
presented above in sec. 2.2. The first line
matches a verb term, and the next two lines
require the presence of terms in both the sub-
ject (role(hier(sb, %%))) and object
role (hier (ob, %%) ) roles. Following the
explicit pattern expression, we add negative checks
to ensure that neither the subject or object are PRO
elements, which have no surface form.
</bodyText>
<subsectionHeader confidence="0.992306">
4.2 Transformation
</subsectionHeader>
<bodyText confidence="0.9770165">
The transformation expressed in figure 4 is almost
trivial. We capture the verb-subject-object triple in a
simple three place predicate. Recall that the values
of the triple:
( %VerbWord, %SubjLemma, %ObjLemma)
are bound by unification to the terms matched in the
pattern, above.
Although we have only one pattern and one
matching transformation in this example, we are not
in general limited in the number of patterns or out-
put expressions we might use. Multiple transforms,
from multiple patterns, can be used.
During compilation, these Transfer rules are com-
piled into a binary object module, then distributed
at runtime to the compute nodes where they will be
executed in the proper sequence by the Coral job
controller. Output from the transformation step, and
between all the steps, is encoded as a set of hierar-
chically structured objects using JSON (Crockford,
2006). Because JSON provides a simple structural
encoding with named fields, and many programming
environments can handle the JSON format, it pro-
vides a flexible and self-describing interchange for-
mat between steps in the Oceanography runtime.
</bodyText>
<subsectionHeader confidence="0.999667">
4.3 Aggregation
</subsectionHeader>
<bodyText confidence="0.999994666666667">
The third section of the Oceanography program de-
clares the types of objects to be aggregated follow-
ing the transform step. The type declarations in
this section serve two purposes. First, they spec-
ify exactly what types of data from the match-
ing/transformation phase should be carried forward.
Recall that all of the source data is available for pro-
cessing, but we are likely only interested in a small
portion of it. Secondly, the declarations serve as type
hints to the compiler so that operations and data stor-
age are performed correctly in the later phases (e.g.
adding strings vs. integers).
</bodyText>
<subsectionHeader confidence="0.996212">
4.4 Statistics
</subsectionHeader>
<bodyText confidence="0.9996115">
The simplest statistic we can compute is the count
of a type that has been aggregated. For example,
</bodyText>
<page confidence="0.99848">
61
</page>
<table confidence="0.923038047619048">
## Step 1: pattern matching
rules {
word(%VerbSk, %VerbWord, verb, verb, %%Pos, %%SentNum, %%Context, %%LexicalInfo ),
in_context(%%, role(hier(sb, %%), %VerbSk, %SubjLemma:%%)),
in_context(%%, role(hier(ob, %%), %VerbSk, %ObjLemma:%%)),
{ \+memberchk( %SubjLemma, [group_object, null_pro, agent_pro]),
\+memberchk( %ObjLemma, [group_object, null_pro, agent_pro]) }
## Step 2: Transformation
==&gt; triple(%VerbWord, %SubjLemma, %ObjLemma).
}
## Step 3: Aggregation
triple :: {
verb :: String,
subject :: String,
object :: String
}
## Step 4: Statistics
count triple
count triple.verb
count triple.verb, triple.subject
dist triple.object cond on triple.verb
</table>
<figureCaption confidence="0.997356">
Figure 4: A complete Oceanography program
</figureCaption>
<bodyText confidence="0.984391041666667">
count triple.verb
will result in occurrence counts of each verb seen
in the parses. We can combine primitive types into
tuples, in order to count n-grams (which are not nec-
essarily adjacent), e.g.
count triple.verb, triple.subject
to give occurrence counts for all (verb,subject) pairs.
The dist X cond on Y statement is used to
produce the conditional distribution p(x y). The
map/reduce framework collates all occurrences with
a given value yz to a single reduce function, which
sums the conditional counts of x, and normalizes by
the total.
Other statistics require multiple map/reduce op-
erations. Computing the probability for the verb
unigrams requires knowing the total number of oc-
currences, which, in this kind of data-parallel pro-
cessing architecture, is not available until the out-
put of all occurrence counts is known. So, a prob
triple.verb statistic must implicitly compute
count triple.verb, sum all occurrences, and
normalize across the set. For a good type-driven
analysis of information flow during various stages
of a map/reduce computations, see L¨ammel (2008).
</bodyText>
<subsectionHeader confidence="0.85253">
4.5 Output
</subsectionHeader>
<bodyText confidence="0.999986142857143">
Output is given two forms. For ease of interpreta-
tions, human-readable tab delimited files are writ-
ten, in which each record is preceded by the type,
as given in the argument to the statistics declaration.
To simplify later offline computation, the record can
also be written out in a JSON encoded structure with
named fields corresponding to the type.
</bodyText>
<sectionHeader confidence="0.9933905" genericHeader="method">
5 Development and testing in
Oceanography
</sectionHeader>
<bodyText confidence="0.999885642857143">
Rapid turnaround and testing in exploratory corpus
analytics is essential to understanding the nature of
the data, and the performance and behavior of one’s
program. Because the tools on which Oceanogra-
phy is built are modular, we can compile an anal-
ysis program for a local, single machine target as
easily as we can for a cluster of arbitrarily many
compute nodes. The resulting compiled programs
differ somewhat in the ways they traverse the data,
and in the control structures for the Coral processing
steps. However, it was an important design require-
ment that we could compile and test using small data
on a single machine as easily as on a muti-terabyte
corpus on a distributed cluster.
</bodyText>
<page confidence="0.997426">
62
</page>
<bodyText confidence="0.9999955">
The same source program is compiled for either
single machine or cluster execution. The user must
specify a different type of store location for input
and output data, depending on environment. Compi-
lation is done using a command line program, which
takes as input the Oceanography program, and pro-
duces a set of executable outputs, corresponding to
the tasks in the map/reduce process. These can also
be run immediately in the single machine setting,
with results going to stdout.
</bodyText>
<subsectionHeader confidence="0.989818">
5.1 Some sample tasks
</subsectionHeader>
<bodyText confidence="0.9999902">
Although these tools have been available at Power-
set only a few months, we have already used them
to great advantage in diagnostic and linguistic anal-
ysis tasks. Diagnostically, it is important to un-
derstand the failure modes of the various linguistic
pipeline components. For instance, the morpholog-
ical analysis component of the XLE parser will on
occasion encounter tokens it cannot analyze. Hand-
examining a few hundred parses (which starts to ex-
ceed the mental fatigue threshold), one can find nu-
merous examples. But one has no idea of the rel-
ative frequency of any given type of error, or their
combined effect on the parse output. Oceanography
enables a very simple single pattern match rule to be
used to find the frequency distribution of unknown
tokens over 100M sentences as easily as 100, and the
grammar engineers can use this information to pri-
oritize their effort. Other diagnostics on the parse,
such as the frequency of certain rare grammatical
constructs (e.g. reduced relatives), or the prevalence
of unparseable fragments, or relative frequencies of
transitive v. intransitive use, are immensely impor-
tant for understanding the nature of the corpus and
the behavior of the parser.
The S-V-O triples used as an example also have
practical import. By identifying the most common
verb expressions, we can, just as in a keyword stop
list, eliminate or downweight some of the less mean-
ingful relations in our semantic index. For example,
in the Wikipedia corpus, one of the most common S-
V-O triples comes from the phrase “this article needs
references.”
We are also beginning a series of lexical seman-
tic studies, looking at selectional preferences and
their dependence on surface form. Correspondence
between prepositional adjunct roles and other sur-
face realizations is also an active area. Additionally,
Oceanography is being used to analyze feature data
from the parses in order to experiment with an unsu-
pervised word sense disambiguation project.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999982888888889">
We have presented a methodology for understanding
a certain class of linguistic data analysis problems,
which identifies the steps of pattern matching, data
transformation, aggregation, and statistics. We have
also presented a programming system, Oceanogra-
phy, which by following this breakdown simplifies
the programming of these tasks while at the same
time enabling us to take advantage of existing large
scale distributed processing infrastructure.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999176">
I would like to thank Jim Firby, creator of the Coral
document processing pipeline at Powerset, and Dick
Crouch, creator of the XLE Transfer system, for
their foundational work which makes these present
developments possible.
</bodyText>
<page confidence="0.999259">
63
</page>
<sectionHeader confidence="0.995887" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905128205128">
Alfred V. Aho, Peter J. Weinberger, and Brian W.
Kernighan. 1977. awk.
D. Crockford. 2006. The application/json Media Type
for JavaScript Object Notation (JSON). RFC 4627 (In-
formational), July.
Richard S. Crouch, Mary Dalrymple, Ronald M. Kaplan,
Tracy Holloway King, John Maxwell, and P. Newman.
2006. XLE documentation.
Richard S. Crouch. 2006. Packed rewriting for mapping
text to semantics and KR.
Doug Cutting. Apache Hadoop Project.
http://hadoop.apache.org/.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapRe-
duce: simplified data processing on large clusters. In
OSDI’04: Proceedings of the 6th conference on Sym-
posium on Opearting Systems Design &amp; Implementa-
tion, Berkeley, CA, USA. USENIX Association.
Usama M. Fayyad, David Haussler, and Paul E. Stolorz.
1996. KDD for Science Data Analysis: Issues and
Examples. In KDD, pages 50–56.
Ronen Feldman and Ido Dagan. 1995. Knowledge Dis-
covery in Textual Databases (KDT). In KDD, pages
112–117.
David Ferrucci and Adam Lally. 2004. UIMA: an archi-
tectural approach to unstructured information process-
ing in the corporate research environment. Nat. Lang.
Eng., 10(3-4):327–348.
Grard P. Huet. 1975. A unification algorithm for typed
lambda-calculus. Theor. Comput. Sci, 1:27.
Ralf L¨ammel. 2008. Google’s MapReduce programming
model - Revisited. Sci. Comput. Program., 70(1):1–
30.
Rob Pike, Sean Dorward, Robert Griesemer, and Sean
Quinlan. 2005. Interpreting the data: Parallel analy-
sis with Sawzall. Scientific Programming, 13(4):277–
298.
Richard Pito. 1993. Tgrep.
John Wilder Tukey. 1977. Exploratory Data Analysis.
Addison-Wesley, New York.
</reference>
<page confidence="0.999418">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.502679">
<title confidence="0.999552">Distributed Parse Mining</title>
<author confidence="0.999977">Scott A Waterman</author>
<affiliation confidence="0.984025">Microsoft Live</affiliation>
<address confidence="0.966442">475 Brannan</address>
<author confidence="0.555342">San Francisco</author>
<email confidence="0.998265">waterman@acm.org</email>
<abstract confidence="0.997068894736842">We describe the design and implementation of a system for data exploration over dependency parses and derived semantic representations in a large-scale NLP-based search system at Because of the distributed nature of the document repository and the processing infrastructure, and also the complex representations of the corpus data, standard analysis tools such as language modeling toolkits are not applicable. This paper explores the challenges of extracting statistical information and of building language models in such a distributed NLP environment, and introduces a corpus analysis systhat simplifies the writing of analysis code and transparently takes advantage of existing distributed processing infrastructure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Peter J Weinberger</author>
<author>Brian W Kernighan</author>
</authors>
<date>1977</date>
<note>awk.</note>
<contexts>
<context position="12435" citStr="Aho et al., 1977" startWordPosition="1951" endWordPosition="1954">xt, potentially available for aggregation and counting. ==&gt; vo_pair(%VerbWord, %ObjLemma). Figure 2: Transforming the matched pattern 58 word(%VerbSk, %VerbWord, verb, verb, %%, %%, %%, %% ), in_context(%%, role(hier(ob, %%), %VerbSk, %ObjLemma:%%)) word(determine:n(41,3),determine,verb,verb, ....) in_context(t,role(hier(ob,[[ob,root],..]), determine:n(82,3),structure:n(91,3)))) Figure 1: Pattern matching using Transfer In shallower text mining, this might be accomplished using regex matching in a perl program. Another common approach is to use command-line text tools such as awk or sed. Awk (Aho et al., 1977) is designed especially for text mining, but is limited to plain text files, on single machines, and doesn’t extend easily to structured graph representations or distributed processing. (But see, e.g. Sawzall (Pike et al., 2005) for a scalable awk-like language.) 2.4 Aggregation The aggregation step collects the extracted instances and groups them by type and by key. Rather than have the matched, transformed results simply dumped out in some enormous file or database in their order of occurrence in the data set (as one would get e.g. from grep), it is quite useful even in the simplest of cases</context>
</contexts>
<marker>Aho, Weinberger, Kernighan, 1977</marker>
<rawString>Alfred V. Aho, Peter J. Weinberger, and Brian W. Kernighan. 1977. awk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Crockford</author>
</authors>
<title>The application/json Media Type for JavaScript Object Notation (JSON).</title>
<date>2006</date>
<tech>RFC 4627 (Informational),</tech>
<contexts>
<context position="23224" citStr="Crockford, 2006" startWordPosition="3677" endWordPosition="3678">n the pattern, above. Although we have only one pattern and one matching transformation in this example, we are not in general limited in the number of patterns or output expressions we might use. Multiple transforms, from multiple patterns, can be used. During compilation, these Transfer rules are compiled into a binary object module, then distributed at runtime to the compute nodes where they will be executed in the proper sequence by the Coral job controller. Output from the transformation step, and between all the steps, is encoded as a set of hierarchically structured objects using JSON (Crockford, 2006). Because JSON provides a simple structural encoding with named fields, and many programming environments can handle the JSON format, it provides a flexible and self-describing interchange format between steps in the Oceanography runtime. 4.3 Aggregation The third section of the Oceanography program declares the types of objects to be aggregated following the transform step. The type declarations in this section serve two purposes. First, they specify exactly what types of data from the matching/transformation phase should be carried forward. Recall that all of the source data is available for</context>
</contexts>
<marker>Crockford, 2006</marker>
<rawString>D. Crockford. 2006. The application/json Media Type for JavaScript Object Notation (JSON). RFC 4627 (Informational), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Crouch</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
<author>Tracy Holloway King</author>
<author>John Maxwell</author>
<author>P Newman</author>
</authors>
<date>2006</date>
<note>XLE documentation.</note>
<contexts>
<context position="8436" citStr="Crouch et al., 2006" startWordPosition="1324" endWordPosition="1327">o specific problems, of both the exploratory and the diagnostic type. 2.2 Pattern Matching The first step is to identify the specific phenomena of interest within the source data. If the data is a complex structure, it is helpful to express the patterns in a logical representation of the structure, rather than matching the representation directly. Pattern matching in Oceanography for dependency parse structures is handled using a domain specific language (DSL) built explicitly for patternbased manipulation of parse trees and semantic representations generated by the XLE linguistic components (Crouch et al., 2006). This Transfer language (Crouch, 2006) is normally used in the regular linguistic processing pipeline to incrementally rewrite LFG dependency parses into a role-labeled semantic representations (semreps) of entities, events, and relations in the text. Transfer matches pattern rules to a current set of parse dependencies or semantic facts, and writes alternate expressions derived from the matched components. Variables in these expressions are bound via Prolog-style unification (Huet, 1975). For example, in figure 1, the first expression word(· · ·) will match word forms in a parse that are ‘ve</context>
</contexts>
<marker>Crouch, Dalrymple, Kaplan, King, Maxwell, Newman, 2006</marker>
<rawString>Richard S. Crouch, Mary Dalrymple, Ronald M. Kaplan, Tracy Holloway King, John Maxwell, and P. Newman. 2006. XLE documentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Crouch</author>
</authors>
<title>Packed rewriting for mapping text to semantics and KR.</title>
<date>2006</date>
<contexts>
<context position="8475" citStr="Crouch, 2006" startWordPosition="1331" endWordPosition="1332">nd the diagnostic type. 2.2 Pattern Matching The first step is to identify the specific phenomena of interest within the source data. If the data is a complex structure, it is helpful to express the patterns in a logical representation of the structure, rather than matching the representation directly. Pattern matching in Oceanography for dependency parse structures is handled using a domain specific language (DSL) built explicitly for patternbased manipulation of parse trees and semantic representations generated by the XLE linguistic components (Crouch et al., 2006). This Transfer language (Crouch, 2006) is normally used in the regular linguistic processing pipeline to incrementally rewrite LFG dependency parses into a role-labeled semantic representations (semreps) of entities, events, and relations in the text. Transfer matches pattern rules to a current set of parse dependencies or semantic facts, and writes alternate expressions derived from the matched components. Variables in these expressions are bound via Prolog-style unification (Huet, 1975). For example, in figure 1, the first expression word(· · ·) will match word forms in a parse that are ‘verb’s, and bind %VerbSk variable to a un</context>
</contexts>
<marker>Crouch, 2006</marker>
<rawString>Richard S. Crouch. 2006. Packed rewriting for mapping text to semantics and KR.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Doug Cutting</author>
</authors>
<title>Apache Hadoop Project.</title>
<note>http://hadoop.apache.org/.</note>
<marker>Cutting, </marker>
<rawString>Doug Cutting. Apache Hadoop Project. http://hadoop.apache.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>MapReduce: simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In OSDI’04: Proceedings of the 6th conference on Symposium on Opearting Systems Design &amp; Implementation,</booktitle>
<publisher>USENIX Association.</publisher>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="18010" citStr="Dean and Ghemawat, 2004" startWordPosition="2857" endWordPosition="2860">sing model is sequential, with multiple steps run in a fixed sequence for each document in the index. All processing for a single document is typically performed on a single compute node. The steps of the pipeline communicate through intermediate data writen to the local filesystem in between steps, where each step is free to consume data produced earlier. Output from the stages is checkpointed to backing storage at various points along the way, and the final index fragments are merged at the end. This kind of data-parallel process lends itself well to a map/reduce programming infrastructure (Dean and Ghemawat, 2004). Map/reduce divides processing into two classes: data-parallel ‘map’ operations, and commutative ‘reduce’ operations, in which all map output aggregated under a particular key is processed together. In map/reduce terms, the entire linguistic processing runs as a sequence of ‘map’ steps (there is no inter-document communication), with a final ‘reduce’ step to collect index fragments and construct a unified search index. Coral uses the open-source hadoop implementation of map/reduce (Cutting, ) as the central task control and distribution mechanism for assigning NLP pipeline jobs to documents i</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: simplified data processing on large clusters. In OSDI’04: Proceedings of the 6th conference on Symposium on Opearting Systems Design &amp; Implementation, Berkeley, CA, USA. USENIX Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Usama M Fayyad</author>
<author>David Haussler</author>
<author>Paul E Stolorz</author>
</authors>
<title>KDD for Science Data Analysis: Issues and Examples. In</title>
<date>1996</date>
<booktitle>KDD,</booktitle>
<pages>50--56</pages>
<contexts>
<context position="6089" citStr="Fayyad et al., 1996" startWordPosition="944" endWordPosition="947"> syntax to the general analysis steps, these common functions can be relegated to library code, or wrapped into the executable in the compilation step. The less time spent in describing a task, or in coding and debugging the implementation, the more time and attention can be spent in understanding the results and modeling the linguistic processes that underly the data. This sort of abstraction away from the details of file representation, storage architecture, and processing model fits a general trend toward data mining, or text mining (Feldman and Dagan, 1995). In data mining or KDD systems (Fayyad et al., 1996), the goal is to separate the tasks of creative analysis and theorizing from the mundane aspects of traversing the data collection and computing statistics. These are much the same goals emphasized by Tukey (1977) – exploration of the data and interactions in order to understand which hypotheses, and which models of interaction, would be fruitful to explore. For our needs in analyzing collections of text, parses, and semantic representations, we have achieved a very practical step toward these goals. 2.1 Matching process to conception We have found four steps that map very closely to our conce</context>
</contexts>
<marker>Fayyad, Haussler, Stolorz, 1996</marker>
<rawString>Usama M. Fayyad, David Haussler, and Paul E. Stolorz. 1996. KDD for Science Data Analysis: Issues and Examples. In KDD, pages 50–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronen Feldman</author>
<author>Ido Dagan</author>
</authors>
<date>1995</date>
<booktitle>Knowledge Discovery in Textual Databases (KDT). In KDD,</booktitle>
<pages>112--117</pages>
<contexts>
<context position="6036" citStr="Feldman and Dagan, 1995" startWordPosition="934" endWordPosition="937">ng, and statistics programming. By matching a declarative syntax to the general analysis steps, these common functions can be relegated to library code, or wrapped into the executable in the compilation step. The less time spent in describing a task, or in coding and debugging the implementation, the more time and attention can be spent in understanding the results and modeling the linguistic processes that underly the data. This sort of abstraction away from the details of file representation, storage architecture, and processing model fits a general trend toward data mining, or text mining (Feldman and Dagan, 1995). In data mining or KDD systems (Fayyad et al., 1996), the goal is to separate the tasks of creative analysis and theorizing from the mundane aspects of traversing the data collection and computing statistics. These are much the same goals emphasized by Tukey (1977) – exploration of the data and interactions in order to understand which hypotheses, and which models of interaction, would be fruitful to explore. For our needs in analyzing collections of text, parses, and semantic representations, we have achieved a very practical step toward these goals. 2.1 Matching process to conception We hav</context>
</contexts>
<marker>Feldman, Dagan, 1995</marker>
<rawString>Ronen Feldman and Ido Dagan. 1995. Knowledge Discovery in Textual Databases (KDT). In KDD, pages 112–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>UIMA: an architectural approach to unstructured information processing in the corporate research environment.</title>
<date>2004</date>
<journal>Nat. Lang. Eng.,</journal>
<pages>10--3</pages>
<contexts>
<context position="16732" citStr="Ferrucci and Lally, 2004" startWordPosition="2644" endWordPosition="2647"> data effectively. Coral is the name of the distributed, document-parallel NLP pipeline at Powerset. Coral provides both a process and a data management framework in order to smoothly execute the multi-step linguistic analysis of all content indexed for Powerset’s search. Coral controls a multi-step pipeline for deep linguistic processing of documents indexed for search. A partial list of the steps every web document undergoes includes: HTML destructuring, sentence breaking, name tagging, parsing, semantic interpretation, anaphora resolution, and indexing. The pipeline is similar to the UIMA (Ferrucci and Lally, 2004) architecture in that each step adds intermediate data — tagged spans, dependency trees, coreferent expressions — that can be used in subsequent steps. Each step adds a different kind of data to the set, with its own labels and meanings. The output of all these steps is a daunting amount of information, all of which is valuable for understanding the linguistic relations within the text, and also the behavior and effectiveness of the NLP pipeline. Documents are processed in a data-parallel fashion. Multiple documents are processed independently, across multiple processes on multiple compute nod</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. UIMA: an architectural approach to unstructured information processing in the corporate research environment. Nat. Lang. Eng., 10(3-4):327–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grard P Huet</author>
</authors>
<title>A unification algorithm for typed lambda-calculus.</title>
<date>1975</date>
<journal>Theor. Comput. Sci,</journal>
<volume>1</volume>
<contexts>
<context position="8930" citStr="Huet, 1975" startWordPosition="1397" endWordPosition="1398">pulation of parse trees and semantic representations generated by the XLE linguistic components (Crouch et al., 2006). This Transfer language (Crouch, 2006) is normally used in the regular linguistic processing pipeline to incrementally rewrite LFG dependency parses into a role-labeled semantic representations (semreps) of entities, events, and relations in the text. Transfer matches pattern rules to a current set of parse dependencies or semantic facts, and writes alternate expressions derived from the matched components. Variables in these expressions are bound via Prolog-style unification (Huet, 1975). For example, in figure 1, the first expression word(· · ·) will match word forms in a parse that are ‘verb’s, and bind %VerbSk variable to a unique occurrence id and %VerbWord to the verb lemma. The second pattern finds the node in the dependency graph that fills the ob (object) role for that verb, and extracts its lemmas. (The %%’s are placeholder variables in the pattern, needed to match the arity of the expression.) Below, in the same figure, is a representation of the verb and object from a parse of the phrase “determined the structure”. On matching these facts, the VerbWord and ObjLemma</context>
</contexts>
<marker>Huet, 1975</marker>
<rawString>Grard P. Huet. 1975. A unification algorithm for typed lambda-calculus. Theor. Comput. Sci, 1:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf L¨ammel</author>
</authors>
<title>Google’s MapReduce programming model -</title>
<date>2008</date>
<journal>Revisited. Sci. Comput. Program.,</journal>
<volume>70</volume>
<issue>1</issue>
<pages>30</pages>
<marker>L¨ammel, 2008</marker>
<rawString>Ralf L¨ammel. 2008. Google’s MapReduce programming model - Revisited. Sci. Comput. Program., 70(1):1– 30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Pike</author>
<author>Sean Dorward</author>
<author>Robert Griesemer</author>
<author>Sean Quinlan</author>
</authors>
<title>Interpreting the data: Parallel analysis with Sawzall. Scientific Programming,</title>
<date>2005</date>
<volume>13</volume>
<issue>4</issue>
<pages>298</pages>
<contexts>
<context position="12663" citStr="Pike et al., 2005" startWordPosition="1986" endWordPosition="1989">VerbSk, %ObjLemma:%%)) word(determine:n(41,3),determine,verb,verb, ....) in_context(t,role(hier(ob,[[ob,root],..]), determine:n(82,3),structure:n(91,3)))) Figure 1: Pattern matching using Transfer In shallower text mining, this might be accomplished using regex matching in a perl program. Another common approach is to use command-line text tools such as awk or sed. Awk (Aho et al., 1977) is designed especially for text mining, but is limited to plain text files, on single machines, and doesn’t extend easily to structured graph representations or distributed processing. (But see, e.g. Sawzall (Pike et al., 2005) for a scalable awk-like language.) 2.4 Aggregation The aggregation step collects the extracted instances and groups them by type and by key. Rather than have the matched, transformed results simply dumped out in some enormous file or database in their order of occurrence in the data set (as one would get e.g. from grep), it is quite useful even in the simplest of cases to aggregate all similar output items. This condenses the mass of data selected, and allows one to see the extent and diversity of the items that are found by the patterns. This simple counting is often enough for diagnostic ta</context>
</contexts>
<marker>Pike, Dorward, Griesemer, Quinlan, 2005</marker>
<rawString>Rob Pike, Sean Dorward, Robert Griesemer, and Sean Quinlan. 2005. Interpreting the data: Parallel analysis with Sawzall. Scientific Programming, 13(4):277– 298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Pito</author>
</authors>
<date>1993</date>
<publisher>Tgrep.</publisher>
<contexts>
<context position="10574" citStr="Pito, 1993" startWordPosition="1669" endWordPosition="1670">khorse for data analysis over text. However, except for simple cases such as word cooccurrence models, the typical need in deep linguistic analysis is not well served by regular expressions over strings. Anyone in the NLP field who has written regular expressions to match, say, partof-speech labeled text knows the difficulties of having a pattern language which differs from the logical structures being matched. Another typical solution is to write a short program in a scripting language (e.g. perl, python, SNOBOL) which combines regular expressions to provide a simple structure parser. Tgrep (Pito, 1993) is a one such program which extends this regular expression notion to patterns over trees, and can output subtrees matching those expressions, but only provided they are represented as text in the LDC TreeBank format. 2.3 Transformation Once the items of the pattern have been identified in their original context, it is often necessary to isolate them from that context, and remove the extraneous, irrelevant information. For instance, if one is doing a simple word count, the tokenized words of text must be separated from any annotation and counted independently. For more complicated tasks, such</context>
</contexts>
<marker>Pito, 1993</marker>
<rawString>Richard Pito. 1993. Tgrep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Wilder Tukey</author>
</authors>
<title>Exploratory Data Analysis.</title>
<date>1977</date>
<publisher>Addison-Wesley,</publisher>
<location>New York.</location>
<contexts>
<context position="6302" citStr="Tukey (1977)" startWordPosition="982" endWordPosition="983">the implementation, the more time and attention can be spent in understanding the results and modeling the linguistic processes that underly the data. This sort of abstraction away from the details of file representation, storage architecture, and processing model fits a general trend toward data mining, or text mining (Feldman and Dagan, 1995). In data mining or KDD systems (Fayyad et al., 1996), the goal is to separate the tasks of creative analysis and theorizing from the mundane aspects of traversing the data collection and computing statistics. These are much the same goals emphasized by Tukey (1977) – exploration of the data and interactions in order to understand which hypotheses, and which models of interaction, would be fruitful to explore. For our needs in analyzing collections of text, parses, and semantic representations, we have achieved a very practical step toward these goals. 2.1 Matching process to conception We have found four steps that map very closely to our conception of the data analysis problem, which at the same time are easily translated to implementations that can be run on both small local data sets and on very large distributed corpora. 1. Pattern matching – find t</context>
</contexts>
<marker>Tukey, 1977</marker>
<rawString>John Wilder Tukey. 1977. Exploratory Data Analysis. Addison-Wesley, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>