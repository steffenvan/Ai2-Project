<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000107">
<title confidence="0.996602">
NRC-Canada: Building the State-of-the-Art in
Sentiment Analysis of Tweets
</title>
<author confidence="0.998178">
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu
</author>
<affiliation confidence="0.815704">
National Research Council Canada
Ottawa, Ontario, Canada K1A 0R6
</affiliation>
<email confidence="0.928593">
{saif.mohammad,svetlana.kiritchenko,xiaodan.zhu}@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.998125" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999720631578947">
In this paper, we describe how we created two
state-of-the-art SVM classifiers, one to de-
tect the sentiment of messages such as tweets
and SMS (message-level task) and one to de-
tect the sentiment of a term within a message
(term-level task). Among submissions from
44 teams in a competition, our submissions
stood first in both tasks on tweets, obtaining
an F-score of 69.02 in the message-level task
and 88.93 in the term-level task. We imple-
mented a variety of surface-form, semantic,
and sentiment features. We also generated
two large word–sentiment association lexi-
cons, one from tweets with sentiment-word
hashtags, and one from tweets with emoticons.
In the message-level task, the lexicon-based
features provided a gain of 5 F-score points
over all others. Both of our systems can be
replicated using freely available resources.1
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983423630434783">
Hundreds of millions of people around the world ac-
tively use microblogging websites such as Twitter.
Thus there is tremendous interest in sentiment anal-
ysis of tweets across a variety of domains such as
commerce (Jansen et al., 2009), health (Chew and
Eysenbach, 2010; Salath´e and Khandelwal, 2011),
and disaster management (Verma et al., 2011; Man-
del et al., 2012).
1The three authors contributed equally to this paper. Svet-
lana Kiritchenko developed the system for the message-level
task, Xiaodan Zhu developed the system for the term-level task,
and Saif Mohammad led the overall effort, co-ordinated both
tasks, and contributed to feature development.
In this paper, we describe how we created two
state-of-the-art SVM classifiers, one to detect the
sentiment of messages such as tweets and SMS
(message-level task) and one to detect the sentiment
of a term within a message (term-level task). The
sentiment can be one out of three possibilities: posi-
tive, negative, or neutral. We developed these classi-
fiers to participate in an international competition or-
ganized by the Conference on Semantic Evaluation
Exercises (SemEval-2013) (Wilson et al., 2013).2
The organizers created and shared sentiment-labeled
tweets for training, development, and testing. The
distributions of the labels in the different datasets is
shown in Table 1. The competition, officially re-
ferred to as Task 2: Sentiment Analysis in Twitter,
had 44 teams (34 for the message-level task and 23
for the term-level task). Our submissions stood first
in both tasks, obtaining a macro-averaged F-score
of 69.02 in the message-level task and 88.93 in the
term-level task.
The task organizers also provided a second test
dataset, composed of Short Message Service (SMS)
messages (no training data of SMS messages was
provided). We applied our classifiers on the SMS
test set without any further tuning. Nonetheless, the
classifiers still obtained the first position in identify-
ing sentiment of SMS messages (F-score of 68.46)
and second position in detecting the sentiment of
terms within SMS messages (F-score of 88.00, only
0.39 points behind the first ranked system).
We implemented a number of surface-form, se-
mantic, and sentiment features. We also gener-
ated two large word–sentiment association lexicons,
</bodyText>
<footnote confidence="0.793957">
2http://www.cs.york.ac.uk/semeval-2013/task2
</footnote>
<page confidence="0.95101">
321
</page>
<note confidence="0.8465005">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 321–327, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</note>
<tableCaption confidence="0.784041333333333">
Table 1: Class distributions in the training set (Train), de-
velopment set (Dev) and testing set (Test). The Train set
was accessed through tweet ids and a download script.
However, not all tweets were accessible. Below is the
number of Train examples we were able to download.
The Dev and Test sets were provided by FTP.
</tableCaption>
<table confidence="0.998980266666667">
Dataset Positive Negative Neutral Total
Tweets
Message-level task:
Train 3,045 (37%) 1,209 (15%) 4,004 (48%) 8,258
Dev 575 (35%) 340 (20%) 739 (45%) 1,654
Test 1,572 (41%) 601 (16%) 1,640 (43%) 3,813
Term-level task:
Train 4,831 (62%) 2,540 (33%) 385 (5%) 7,756
Dev 648 (57%) 430 (38%) 57 (5%) 1,135
Test 2,734 (62%) 1,541 (35%) 160 (3%) 4,435
SMS
Message-level task:
Test 492 (23%) 394 (19%) 1,208 (58%) 2,094
Term-level task:
Test 1,071 (46%) 1,104 (47%) 159 (7%) 2,334
</table>
<bodyText confidence="0.999366857142857">
one from tweets with sentiment-word hashtags, and
one from tweets with emoticons. The automatically
generated lexicons were particularly useful. In the
message-level task for tweets, they alone provided a
gain of more than 5 F-score points over and above
that obtained using all other features. The lexicons
are made freely available.3
</bodyText>
<sectionHeader confidence="0.930018" genericHeader="method">
2 Sentiment Lexicons
</sectionHeader>
<bodyText confidence="0.993321">
Sentiment lexicons are lists of words with associa-
tions to positive and negative sentiments.
</bodyText>
<subsectionHeader confidence="0.9620335">
2.1 Existing, Automatically Created Sentiment
Lexicons
</subsectionHeader>
<bodyText confidence="0.999845833333334">
The manually created lexicons we used include the
NRC Emotion Lexicon (Mohammad and Turney,
2010; Mohammad and Yang, 2011) (about 14,000
words), the MPQA Lexicon (Wilson et al., 2005)
(about 8,000 words), and the Bing Liu Lexicon (Hu
and Liu, 2004) (about 6,800 words).
</bodyText>
<subsectionHeader confidence="0.946055333333333">
2.2 New, Tweet-Specific, Automatically
Generated Sentiment Lexicons
2.2.1 NRC Hashtag Sentiment Lexicon
</subsectionHeader>
<bodyText confidence="0.9841155">
Certain words in tweets are specially marked with
a hashtag (#) to indicate the topic or sentiment. Mo-
</bodyText>
<footnote confidence="0.517853">
3www.purl.com/net/sentimentoftweets
</footnote>
<bodyText confidence="0.999702666666667">
hammad (2012) showed that hashtagged emotion
words such as joy, sadness, angry, and surprised are
good indicators that the tweet as a whole (even with-
out the hashtagged emotion word) is expressing the
same emotion. We adapted that idea to create a large
corpus of positive and negative tweets.
We polled the Twitter API every four hours from
April to December 2012 in search of tweets with ei-
ther a positive word hashtag or a negative word hash-
tag. A collection of 78 seed words closely related
to positive and negative such as #good, #excellent,
#bad, and #terrible were used (32 positive and 36
negative). These terms were chosen from entries for
positive and negative in the Roget’s Thesaurus.
A set of 775,000 tweets were used to generate a
large word–sentiment association lexicon. A tweet
was considered positive if it had one of the 32 pos-
itive hashtagged seed words, and negative if it had
one of the 36 negative hashtagged seed words. The
association score for a term w was calculated from
these pseudo-labeled tweets as shown below:
</bodyText>
<equation confidence="0.9991765">
score(w) = PMI(w, positive) − PMI(w, negative)
(1)
</equation>
<bodyText confidence="0.9994165">
where PMI stands for pointwise mutual informa-
tion. A positive score indicates association with pos-
itive sentiment, whereas a negative score indicates
association with negative sentiment. The magni-
tude is indicative of the degree of association. The
final lexicon, which we will refer to as the NRC
Hashtag Sentiment Lexicon has entries for 54,129
unigrams and 316,531 bigrams. Entries were also
generated for unigram–unigram, unigram–bigram,
and bigram–bigram pairs that were not necessarily
contiguous in the tweets corpus. Pairs with cer-
tain punctuations, ‘@’ symbols, and some function
words were removed. The lexicon has entries for
308,808 non-contiguous pairs.
</bodyText>
<subsubsectionHeader confidence="0.692915">
2.2.2 Sentiment140 Lexicon
</subsubsectionHeader>
<bodyText confidence="0.999747111111111">
The sentiment140 corpus (Go et al., 2009) is a
collection of 1.6 million tweets that contain pos-
itive and negative emoticons. The tweets are la-
beled positive or negative according to the emoti-
con. We generated a sentiment lexicon from this
corpus in the same manner as described above (Sec-
tion 2.2.1). This lexicon has entries for 62,468
unigrams, 677,698 bigrams, and 480,010 non-
contiguous pairs.
</bodyText>
<page confidence="0.996495">
322
</page>
<sectionHeader confidence="0.971627" genericHeader="method">
3 Task: Automatically Detecting the
</sectionHeader>
<subsectionHeader confidence="0.995716">
Sentiment of a Message
</subsectionHeader>
<bodyText confidence="0.999986">
The objective of this task is to determine whether a
given message is positive, negative, or neutral.
</bodyText>
<subsectionHeader confidence="0.998957">
3.1 Classifier and features
</subsectionHeader>
<bodyText confidence="0.999210153846154">
We trained a Support Vector Machine (SVM) (Fan
et al., 2008) on the training data provided. SVM
is a state-of-the-art learning algorithm proved to be
effective on text categorization tasks and robust on
large feature spaces. The linear kernel and the value
for the parameter C=0.005 were chosen by cross-
validation on the training data.
We normalized all URLs to http://someurl and all
userids to @someuser. We tokenized and part-of-
speech tagged the tweets with the Carnegie Mellon
University (CMU) Twitter NLP tool (Gimpel et al.,
2011). Each tweet was represented as a feature vec-
tor made up of the following groups of features:
</bodyText>
<listItem confidence="0.989204848484848">
• word ngrams: presence or absence of contigu-
ous sequences of 1, 2, 3, and 4 tokens; non-
contiguous ngrams (ngrams with one token re-
placed by *);
• character ngrams: presence or absence of con-
tiguous sequences of 3, 4, and 5 characters;
• all-caps: the number of words with all charac-
ters in upper case;
• POS: the number of occurrences of each part-
of-speech tag;
• hashtags: the number of hashtags;
• lexicons: the following sets of features were
generated for each of the three manually con-
structed sentiment lexicons (NRC Emotion
Lexicon, MPQA, Bing Liu Lexicon) and for
each of the two automatically constructed lex-
icons (Hashtag Sentiment Lexicon and Senti-
ment140 Lexicon). Separate feature sets were
produced for unigrams, bigrams, and non-
contiguous pairs. The lexicon features were
created for all tokens in the tweet, for each part-
of-speech tag, for hashtags, and for all-caps to-
kens. For each token w and emotion or po-
larity p, we used the sentiment/emotion score
score(w, p) to determine:
– total count of tokens in the tweet with
score(w,p) &gt; 0;
– total score= EwEtweet score(w, p);
– the maximal score =
maxwEtweetscore(w, p);
– the score of the last token in the tweet with
score(w,p) &gt; 0;
• punctuation:
</listItem>
<bodyText confidence="0.9316218">
– the number of contiguous sequences of
exclamation marks, question marks, and
both exclamation and question marks;
– whether the last token contains an excla-
mation or question mark;
</bodyText>
<listItem confidence="0.884009166666667">
• emoticons: The polarity of an emoticon was
determined with a regular expression adopted
from Christopher Potts’ tokenizing script:4
– presence or absence of positive and nega-
tive emoticons at any position in the tweet;
– whether the last token is a positive or neg-
ative emoticon;
• elongated words: the number of words with one
character repeated more than two times, for ex-
ample, ‘soooo’;
• clusters: The CMU pos-tagging tool provides
the token clusters produced with the Brown
clustering algorithm on 56 million English-
language tweets. These 1,000 clusters serve as
alternative representation of tweet content, re-
ducing the sparcity of the token space.
– the presence or absence of tokens from
each of the 1000 clusters;
• negation: the number of negated contexts. Fol-
lowing (Pang et al., 2002), we defined a negated
context as a segment of a tweet that starts
with a negation word (e.g., no, shouldn’t) and
ends with one of the punctuation marks: ‘,’,
‘.’, ‘:’, ‘;’, ‘!’, ‘?’.A negated context af-
</listItem>
<bodyText confidence="0.85738475">
fects the ngram and lexicon features: we add
‘ NEG’ suffix to each word following the nega-
tion word (‘perfect’ becomes ‘perfect NEG’).
The ‘ NEG’ suffix is also added to polarity and
emotion features (‘POLARITY positive’ be-
comes ‘POLARITY positive NEG’). The list
of negation words was adopted from Christo-
pher Potts’ sentiment tutorial.5
</bodyText>
<footnote confidence="0.9999405">
4http://sentiment.christopherpotts.net/tokenizing.html
5http://sentiment.christopherpotts.net/lingstruc.html
</footnote>
<page confidence="0.996166">
323
</page>
<subsectionHeader confidence="0.956717">
3.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999965173913043">
We trained the SVM classifier on the set of 9,912
annotated tweets (8,258 in the training set and 1,654
in the development set). We applied the model to the
test set of 3,813 unseen tweets. The same model was
applied unchanged to the other test set of 2,094 SMS
messages as well. The bottom-line score used by the
task organizers was the macro-averaged F-score of
the positive and negative classes. The results ob-
tained by our system on the training set (ten-fold
cross-validation), development set (when trained on
the training set), and test sets (when trained on the
combined set of tweets in the training and devel-
opment sets) are shown in Table 2. The table also
shows baseline results obtained by a majority clas-
sifier that always predicts the most frequent class as
output. Since the bottom-line F-score is based only
on the F-scores of positive and negative classes (and
not on neutral), the majority baseline chose the most
frequent class among positive and negative, which
in this case was the positive class. We also show
baseline results obtained using an SVM and unigram
features alone. Our system (SVM and all features)
obtained a macro-averaged F-score of 69.02 on the
tweet set and 68.46 on the SMS set. In the SemEval-
2013 competition, our submission ranked first on
both datasets. There were 48 submissions from 34
teams for this task.
Table 3 shows the results of the ablation experi-
ments where we repeat the same classification pro-
cess but remove one feature group at a time. The
most influential features for both datasets turned out
to be the sentiment lexicon features: they provided
gains of more than 8.5%. It is interesting to note
that tweets benefited mostly from the automatic sen-
timent lexicons (NRC Hashtag Lexicon and the Sen-
timent140 Lexicon) whereas the SMS set benefited
more from the manual lexicons (MPQA, NRC Emo-
tion Lexicon, Bing Liu Lexicon). Among the au-
tomatic lexicons, both the Hashtag Sentiment Lex-
icon and the Sentiment140 Lexicon contributed to
roughly the same amount of improvement in perfor-
mance on the tweet set.
The second most important feature group for
the message-level task was that of ngrams (word
and character ngrams). Expectedly, the impact of
ngrams on the SMS dataset was less extensive since
</bodyText>
<tableCaption confidence="0.9477595">
Table 2: Message-level Task: The macro-averaged F-
scores on different datasets.
</tableCaption>
<table confidence="0.99982325">
Classifier Tweets SMS
Training set: Majority 26.94 -
SVM-all 67.20 -
Development set: Majority 26.85 -
SVM-all 68.72 -
Test set: Majority 29.19 19.03
SVM-unigrams 39.61 39.29
SVM-all 69.02 68.46
</table>
<tableCaption confidence="0.955194">
Table 3: Message-level Task: The macro-averaged F-
</tableCaption>
<figureCaption confidence="0.946897823529412">
scores obtained on the test sets with one of the feature
groups removed. The number in the brackets is the dif-
ference with the all features score. The biggest drops are
shown in bold.
Experiment Tweets SMS
all features 69.02 68.46
all - lexicons 60.42 (-8.60) 59.73 (-8.73)
all - manual lex. 67.45 (-1.57) 65.64 (-2.82)
all - auto. lex. 63.78 (-5.24) 67.12 (-1.34)
all - Senti140 lex. 65.25 (-3.77) 67.33 (-1.13)
all - Hashtag lex. 65.22 (-3.80) 70.28 (1.82)
all - ngrams 61.77 (-7.25) 67.27 (-1.19)
all - word ngrams 64.64 (-4.38) 66.56 (-1.9)
all - char. ngrams 67.10 (-1.92) 68.94 (0.48)
all - negation 67.20 (-1.82) 66.22 (-2.24)
all - POS 68.38 (-0.64) 67.07 (-1.39)
all - clusters 69.01 (-0.01) 68.10 (-0.36)
</figureCaption>
<bodyText confidence="0.9134005">
all - encodings (elongated, emoticons, punctuations,
all-caps, hashtags) 69.16 (0.14) 68.28 (-0.18)
the classifier model was trained only on tweets.
Attention to negations improved performance on
both datasets. Removing the sentiment encoding
features like hashtags, emoticons, and elongated
words, had almost no impact on performance, but
this is probably because the discriminating informa-
tion in them was also captured by some other fea-
tures such as character and word ngrams.
</bodyText>
<sectionHeader confidence="0.98969" genericHeader="method">
4 Task: Automatically Detecting the
</sectionHeader>
<subsectionHeader confidence="0.97659">
Sentiment of a Term in a Message
</subsectionHeader>
<bodyText confidence="0.9999142">
The objective of this task is to detect whether a term
(a word or phrase) within a message conveys a pos-
itive, negative, or neutral sentiment. Note that the
same term may express different sentiments in dif-
ferent contexts.
</bodyText>
<page confidence="0.997264">
324
</page>
<subsectionHeader confidence="0.982414">
4.1 Classifier and features
</subsectionHeader>
<bodyText confidence="0.984645142857143">
We trained an SVM using the LibSVM package
(Chang and Lin, 2011) and a linear kernel. In ten-
fold cross-validation over the training data, the lin-
ear kernel outperformed other kernels implemented
in LibSVM as well as a maximum-entropy classi-
fier. Our model leverages a variety of features, as
described below:
</bodyText>
<listItem confidence="0.986542395348837">
• word ngrams:
– presence or absence of unigrams, bigrams,
and the full word string of a target term;
– leading and ending unigrams and bigrams;
• character ngrams: presence or absence of two-
and three-character prefixes and suffixes of all
the words in a target term (note that the target
term may be a multi-word sequence);
• elongated words: presence or absence of elon-
gated words (e.g., ’sooo’);
• emoticons: the numbers and categories of
emoticons that a term contains6;
• punctuation: presence or absence of punctua-
tion sequences such as ‘?!’ and ‘!!!’;
• uppercase:
– whether all the words in the target start
with an upper case letter followed by
lower case letters;
– whether the target words are all in upper-
case (to capture a potential named entity);
• stopwords: whether a term contains only stop-
words. If so, separate features indicate whether
there are 1, 2, 3, or more stop-words;
• lengths:
– the length of a target term (number of
words);
– the average length of words (number of
characters) in a term;
– a binary feature indicating whether a term
contains long words;
6http://en.wikipedia.org/wiki/List of emoticons
• negation: similar to those described for the
message-level task. Whenever a negation word
was found immediately before the target or
within the target, the polarities of all tokens af-
ter the negation term were flipped;
• position: whether a term is at the beginning,
end, or another position;
• sentiment lexicons: we used automatically cre-
ated lexicons (NRC Hashtag Sentiment Lexi-
con, Sentiment140 Lexicon) as well as manu-
ally created lexicons (NRC Emotion Lexicon,
MPQA, Bing Liu Lexicon).
</listItem>
<bodyText confidence="0.57659475">
– total count of tokens in the target term
with sentiment score greater than 0;
– the sum of the sentiment scores for all to-
kens in the target;
</bodyText>
<listItem confidence="0.995553454545455">
– the maximal sentiment score;
– the non-zero sentiment score of the last to-
ken in the target;
• term splitting: when a term contains a hash-
tag made of multiple words (e.g., #biggest-
daythisyear), we split the hashtag into compo-
nent words;
• others:
– whether a term contains a Twitter user
name;
– whether a term contains a URL.
</listItem>
<bodyText confidence="0.999934666666667">
The above features were extracted from target
terms as well as from the rest of the message (the
context). For unigrams and bigrams, we used four
words on either side of the target as the context. The
window size was chosen through experiments on the
development set.
</bodyText>
<subsectionHeader confidence="0.962721">
4.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999219333333333">
We trained an SVM classifier on the 8,891 annotated
terms in tweets (7,756 terms in the training set and
1,135 terms in the development set). We applied the
model to 4,435 terms in the tweets test set. The same
model was applied unchanged to the other test set of
2,334 terms in unseen SMS messages as well. The
bottom-line score used by the task organizers was
the macro-averaged F-score of the positive and neg-
ative classes.
</bodyText>
<page confidence="0.997875">
325
</page>
<bodyText confidence="0.999985229166667">
The results on the training set (ten-fold cross-
validation), the development set (trained on the
training set), and the test sets (trained on the com-
bined set of tweets in the training and development
sets) are shown in Table 4. The table also shows
baseline results obtained by a majority classifier that
always predicts the most frequent class as output,
and an additional baseline result obtained using an
SVM and unigram features alone. Our submission
obtained a macro-averaged F-score of 88.93 on the
tweet set and was ranked first among 29 submissions
from 23 participating teams. Even with no tuning
specific to SMS data, our SMS submission still ob-
tained second rank with an F-score of 88.00. The
score of the first ranking system on the SMS set was
88.39. A post-competition bug-fix in the bigram fea-
tures resulted in a small improvement: F-score of
89.10 on the tweets set and 88.34 on the SMS set.
Note that the performance is significantly higher
in the term-level task than in the message-level task.
This is largely because of the ngram features (see
unigram baselines in Tables 2 and 4). We analyzed
the labeled data provided to determine why ngrams
performed so strongly in this task. We found that the
percentage of test tokens already seen within train-
ing data targets was 85.1%. Further, the average ra-
tio of instances pertaining to the most dominant po-
larity of a target term to the total number of instances
of that target term was 0.808.
Table 5 presents the ablation F-scores. Observe
that the ngram features were the most useful. Note
also that removing just the word ngram features or
just the character ngram features results in only a
small drop in performance. This indicates that the
two feature groups capture similar information.
The sentiment lexicon features are the next most
useful group—removing them leads to a drop in F-
score of 3.95 points for the tweets set and 4.64 for
the SMS set. Modeling negation improves the F-
score by 0.72 points on the tweets set and 1.57 points
on the SMS set.
The last two rows in Table 5 show the results ob-
tained when the features are extracted only from the
target (and not from its context) and when they are
extracted only from the context of the target (and
not from the target itself). Observe that even though
the context may influence the polarity of the tar-
get, using target features alone is substantially more
</bodyText>
<tableCaption confidence="0.878689">
Table 4: Term-level Task: The macro-averaged F-scores
on the datasets. The official scores of our submission are
shown in bold. SVM-all* shows results after a bug fix.
</tableCaption>
<table confidence="0.999824222222222">
Classifier Tweets SMS
Training set: Majority 38.38 -
SVM-all 86.80 -
Development set: Majority 36.34 -
SVM-all 86.49 -
Test set: Majority 38.13 32.11
SVM-unigrams 80.28 78.71
official SVM-all 88.93 88.00
SVM-all* 89.10 88.34
</table>
<tableCaption confidence="0.909065">
Table 5: Term-level Task: The F-scores obtained on the
test sets with one of the feature groups removed. The
number in brackets is the difference with the all features
score. The biggest drops are shown in bold.
</tableCaption>
<figure confidence="0.876698">
Experiment
all features
</figure>
<figureCaption confidence="0.827481111111111">
all - ngrams
all - word ngrams
all - char. ngrams
all - lexicons
all - manual lex.
all - auto lex.
all - negation
all - stopwords
all - encodings (elongated words, emoticons, punctns.,
</figureCaption>
<bodyText confidence="0.939099">
uppercase) 89.16 (0.06) 88.39 (0.05)
all - target 72.97 (-16.13) 68.96 (-19.38)
all - context 85.02 (-4.08) 85.93 (-2.41)
useful than using context features alone. Nonethe-
less, adding context features improves the F-scores
by roughly 2 to 4 points.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999985909090909">
We created two state-of-the-art SVM classifiers, one
to detect the sentiment of messages and one to de-
tect the sentiment of a term within a message. Our
submissions on tweet data stood first in both these
subtasks of the SemEval-2013 competition ‘Detect-
ing Sentiment in Twitter’. We implemented a variety
of features based on surface form and lexical cate-
gories. The sentiment lexicon features (both manu-
ally created and automatically generated) along with
ngram features (both word and character ngrams)
led to the most gain in performance.
</bodyText>
<table confidence="0.9983795">
Tweets SMS
89.10 88.34
83.86 (-5.24) 80.49 (-7.85)
88.38 (-0.72) 87.37 (-0.97)
89.01 (-0.09) 87.31 (-1.03)
85.15 (-3.95) 83.70 (-4.64)
87.69 (-1.41) 86.84 (-1.5)
88.24 (-0.86) 86.65 (-1.69)
88.38 (-0.72) 86.77 (-1.57)
89.17 (0.07) 88.30 (-0.04)
</table>
<page confidence="0.997087">
326
</page>
<sectionHeader confidence="0.99921" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9985485">
We thank Colin Cherry for providing his SVM code
and for helpful discussions.
</bodyText>
<sectionHeader confidence="0.995622" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999177976470589">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1–27:27.
Cynthia Chew and Gunther Eysenbach. 2010. Pan-
demics in the Age of Twitter: Content Analysis of
Tweets during the 2009 H1N1 Outbreak. PLoS ONE,
5(11):e14118+, November.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
Lin C.-J. 2008. LIBLINEAR: A Library for Large
Linear Classification. Journal of Machine Learning
Research, 9:1871–1874.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-Speech Tagging for
Twitter: Annotation, Features, and Experiments. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter
Sentiment Classification using Distant Supervision. In
Final Projects from CS224N for Spring 2008/2009 at
The Stanford Natural Language Processing Group.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ’04, pages 168–
177, New York, NY, USA. ACM.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as electronic
word of mouth. Journal of the American Society for
Information Science and Technology, 60(11):2169–
2188.
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during hurricane irene. In Proceedings of the Second
Workshop on Language in Social Media, LSM ’12,
pages 27–36, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions Evoked by Common Words and Phrases: Using
Mechanical Turk to Create an Emotion Lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California.
Saif Mohammad and Tony Yang. 2011. Tracking Sen-
timent in Mail: How Genders Differ on Emotional
Axes. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis (WASSA 2.011), pages 70–79, Portland, Ore-
gon. Association for Computational Linguistics.
Saif Mohammad. 2012. #Emotional Tweets. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics (*SEM), pages 246–
255, Montr´eal, Canada. Association for Computa-
tional Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment Classification Using
Machine Learning Techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79–86, Philadelphia, PA.
Marcel Salath´e and Shashank Khandelwal. 2011. As-
sessing vaccination sentiments with online social me-
dia: Implications for infectious disease dynamics and
control. PLoS Computational Biology, 7(10).
Sudha Verma, Sarah Vieweg, William Corvey, Leysia
Palen, James Martin, Martha Palmer, Aaron Schram,
and Kenneth Anderson. 2011. Natural language pro-
cessing to the rescue? extracting ”situational aware-
ness” tweets during mass emergency. In International
AAAI Conference on Weblogs and Social Media.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ’05,
pages 347–354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter. In
Proceedings of the International Workshop on Seman-
tic Evaluation, SemEval ’13, Atlanta, Georgia, USA,
June.
</reference>
<page confidence="0.9984">
327
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.713804">
<title confidence="0.998303">NRC-Canada: Building the State-of-the-Art Sentiment Analysis of Tweets</title>
<author confidence="0.989751">Saif M Mohammad</author>
<author confidence="0.989751">Svetlana Kiritchenko</author>
<author confidence="0.989751">Xiaodan</author>
<affiliation confidence="0.965309">National Research Council</affiliation>
<address confidence="0.921434">Ottawa, Ontario, Canada K1A</address>
<abstract confidence="0.98898525">In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a message (term-level task). Among submissions from 44 teams in a competition, our submissions stood first in both tasks on tweets, obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task. We implemented a variety of surface-form, semantic, and sentiment features. We also generated two large word–sentiment association lexicons, one from tweets with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be using freely available</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="15554" citStr="Chang and Lin, 2011" startWordPosition="2482" endWordPosition="2485">res like hashtags, emoticons, and elongated words, had almost no impact on performance, but this is probably because the discriminating information in them was also captured by some other features such as character and word ngrams. 4 Task: Automatically Detecting the Sentiment of a Term in a Message The objective of this task is to detect whether a term (a word or phrase) within a message conveys a positive, negative, or neutral sentiment. Note that the same term may express different sentiments in different contexts. 324 4.1 Classifier and features We trained an SVM using the LibSVM package (Chang and Lin, 2011) and a linear kernel. In tenfold cross-validation over the training data, the linear kernel outperformed other kernels implemented in LibSVM as well as a maximum-entropy classifier. Our model leverages a variety of features, as described below: • word ngrams: – presence or absence of unigrams, bigrams, and the full word string of a target term; – leading and ending unigrams and bigrams; • character ngrams: presence or absence of twoand three-character prefixes and suffixes of all the words in a target term (note that the target term may be a multi-word sequence); • elongated words: presence or</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Chew</author>
<author>Gunther Eysenbach</author>
</authors>
<title>Pandemics in the Age of Twitter: Content Analysis of Tweets during the 2009 H1N1 Outbreak.</title>
<date>2010</date>
<journal>PLoS ONE,</journal>
<volume>5</volume>
<issue>11</issue>
<contexts>
<context position="1388" citStr="Chew and Eysenbach, 2010" startWordPosition="204" endWordPosition="207">ment features. We also generated two large word–sentiment association lexicons, one from tweets with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated using freely available resources.1 1 Introduction Hundreds of millions of people around the world actively use microblogging websites such as Twitter. Thus there is tremendous interest in sentiment analysis of tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salath´e and Khandelwal, 2011), and disaster management (Verma et al., 2011; Mandel et al., 2012). 1The three authors contributed equally to this paper. Svetlana Kiritchenko developed the system for the message-level task, Xiaodan Zhu developed the system for the term-level task, and Saif Mohammad led the overall effort, co-ordinated both tasks, and contributed to feature development. In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term with</context>
</contexts>
<marker>Chew, Eysenbach, 2010</marker>
<rawString>Cynthia Chew and Gunther Eysenbach. 2010. Pandemics in the Age of Twitter: Content Analysis of Tweets during the 2009 H1N1 Outbreak. PLoS ONE, 5(11):e14118+, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="7954" citStr="Fan et al., 2008" startWordPosition="1228" endWordPosition="1231">, 2009) is a collection of 1.6 million tweets that contain positive and negative emoticons. The tweets are labeled positive or negative according to the emoticon. We generated a sentiment lexicon from this corpus in the same manner as described above (Section 2.2.1). This lexicon has entries for 62,468 unigrams, 677,698 bigrams, and 480,010 noncontiguous pairs. 322 3 Task: Automatically Detecting the Sentiment of a Message The objective of this task is to determine whether a given message is positive, negative, or neutral. 3.1 Classifier and features We trained a Support Vector Machine (SVM) (Fan et al., 2008) on the training data provided. SVM is a state-of-the-art learning algorithm proved to be effective on text categorization tasks and robust on large feature spaces. The linear kernel and the value for the parameter C=0.005 were chosen by crossvalidation on the training data. We normalized all URLs to http://someurl and all userids to @someuser. We tokenized and part-ofspeech tagged the tweets with the Carnegie Mellon University (CMU) Twitter NLP tool (Gimpel et al., 2011). Each tweet was represented as a feature vector made up of the following groups of features: • word ngrams: presence or abs</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and Lin C.-J. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter Sentiment Classification using Distant Supervision. In Final Projects from CS224N for Spring 2008/2009 at The Stanford Natural Language Processing Group.</title>
<date>2009</date>
<contexts>
<context position="7344" citStr="Go et al., 2009" startWordPosition="1127" endWordPosition="1130">eas a negative score indicates association with negative sentiment. The magnitude is indicative of the degree of association. The final lexicon, which we will refer to as the NRC Hashtag Sentiment Lexicon has entries for 54,129 unigrams and 316,531 bigrams. Entries were also generated for unigram–unigram, unigram–bigram, and bigram–bigram pairs that were not necessarily contiguous in the tweets corpus. Pairs with certain punctuations, ‘@’ symbols, and some function words were removed. The lexicon has entries for 308,808 non-contiguous pairs. 2.2.2 Sentiment140 Lexicon The sentiment140 corpus (Go et al., 2009) is a collection of 1.6 million tweets that contain positive and negative emoticons. The tweets are labeled positive or negative according to the emoticon. We generated a sentiment lexicon from this corpus in the same manner as described above (Section 2.2.1). This lexicon has entries for 62,468 unigrams, 677,698 bigrams, and 480,010 noncontiguous pairs. 322 3 Task: Automatically Detecting the Sentiment of a Message The objective of this task is to determine whether a given message is positive, negative, or neutral. 3.1 Classifier and features We trained a Support Vector Machine (SVM) (Fan et </context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter Sentiment Classification using Distant Supervision. In Final Projects from CS224N for Spring 2008/2009 at The Stanford Natural Language Processing Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5251" citStr="Hu and Liu, 2004" startWordPosition="798" endWordPosition="801"> useful. In the message-level task for tweets, they alone provided a gain of more than 5 F-score points over and above that obtained using all other features. The lexicons are made freely available.3 2 Sentiment Lexicons Sentiment lexicons are lists of words with associations to positive and negative sentiments. 2.1 Existing, Automatically Created Sentiment Lexicons The manually created lexicons we used include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011) (about 14,000 words), the MPQA Lexicon (Wilson et al., 2005) (about 8,000 words), and the Bing Liu Lexicon (Hu and Liu, 2004) (about 6,800 words). 2.2 New, Tweet-Specific, Automatically Generated Sentiment Lexicons 2.2.1 NRC Hashtag Sentiment Lexicon Certain words in tweets are specially marked with a hashtag (#) to indicate the topic or sentiment. Mo3www.purl.com/net/sentimentoftweets hammad (2012) showed that hashtagged emotion words such as joy, sadness, angry, and surprised are good indicators that the tweet as a whole (even without the hashtagged emotion word) is expressing the same emotion. We adapted that idea to create a large corpus of positive and negative tweets. We polled the Twitter API every four hours</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04, pages 168– 177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<title>Twitter power: Tweets as electronic word of mouth.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>11</issue>
<pages>2188</pages>
<contexts>
<context position="1354" citStr="Jansen et al., 2009" startWordPosition="199" endWordPosition="202">face-form, semantic, and sentiment features. We also generated two large word–sentiment association lexicons, one from tweets with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated using freely available resources.1 1 Introduction Hundreds of millions of people around the world actively use microblogging websites such as Twitter. Thus there is tremendous interest in sentiment analysis of tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salath´e and Khandelwal, 2011), and disaster management (Verma et al., 2011; Mandel et al., 2012). 1The three authors contributed equally to this paper. Svetlana Kiritchenko developed the system for the message-level task, Xiaodan Zhu developed the system for the term-level task, and Saif Mohammad led the overall effort, co-ordinated both tasks, and contributed to feature development. In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to d</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter power: Tweets as electronic word of mouth. Journal of the American Society for Information Science and Technology, 60(11):2169– 2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Mandel</author>
<author>Aron Culotta</author>
<author>John Boulahanis</author>
<author>Danielle Stark</author>
<author>Bonnie Lewis</author>
<author>Jeremy Rodrigue</author>
</authors>
<title>A demographic analysis of online sentiment during hurricane irene.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media, LSM ’12,</booktitle>
<pages>27--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1487" citStr="Mandel et al., 2012" startWordPosition="219" endWordPosition="223">ntiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated using freely available resources.1 1 Introduction Hundreds of millions of people around the world actively use microblogging websites such as Twitter. Thus there is tremendous interest in sentiment analysis of tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salath´e and Khandelwal, 2011), and disaster management (Verma et al., 2011; Mandel et al., 2012). 1The three authors contributed equally to this paper. Svetlana Kiritchenko developed the system for the message-level task, Xiaodan Zhu developed the system for the term-level task, and Saif Mohammad led the overall effort, co-ordinated both tasks, and contributed to feature development. In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a message (term-level task). The sentiment can be one out of three possibilities: positive, nega</context>
</contexts>
<marker>Mandel, Culotta, Boulahanis, Stark, Lewis, Rodrigue, 2012</marker>
<rawString>Benjamin Mandel, Aron Culotta, John Boulahanis, Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue. 2012. A demographic analysis of online sentiment during hurricane irene. In Proceedings of the Second Workshop on Language in Social Media, LSM ’12, pages 27–36, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<location>LA, California.</location>
<contexts>
<context position="5099" citStr="Mohammad and Turney, 2010" startWordPosition="772" endWordPosition="775">04 (47%) 159 (7%) 2,334 one from tweets with sentiment-word hashtags, and one from tweets with emoticons. The automatically generated lexicons were particularly useful. In the message-level task for tweets, they alone provided a gain of more than 5 F-score points over and above that obtained using all other features. The lexicons are made freely available.3 2 Sentiment Lexicons Sentiment lexicons are lists of words with associations to positive and negative sentiments. 2.1 Existing, Automatically Created Sentiment Lexicons The manually created lexicons we used include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011) (about 14,000 words), the MPQA Lexicon (Wilson et al., 2005) (about 8,000 words), and the Bing Liu Lexicon (Hu and Liu, 2004) (about 6,800 words). 2.2 New, Tweet-Specific, Automatically Generated Sentiment Lexicons 2.2.1 NRC Hashtag Sentiment Lexicon Certain words in tweets are specially marked with a hashtag (#) to indicate the topic or sentiment. Mo3www.purl.com/net/sentimentoftweets hammad (2012) showed that hashtagged emotion words such as joy, sadness, angry, and surprised are good indicators that the tweet as a whole (even without the hashtagged emotion word) i</context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2010. Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon. In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, LA, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Tony Yang</author>
</authors>
<title>Tracking Sentiment in Mail: How Genders Differ on Emotional Axes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011),</booktitle>
<pages>70--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<contexts>
<context position="5125" citStr="Mohammad and Yang, 2011" startWordPosition="776" endWordPosition="779"> from tweets with sentiment-word hashtags, and one from tweets with emoticons. The automatically generated lexicons were particularly useful. In the message-level task for tweets, they alone provided a gain of more than 5 F-score points over and above that obtained using all other features. The lexicons are made freely available.3 2 Sentiment Lexicons Sentiment lexicons are lists of words with associations to positive and negative sentiments. 2.1 Existing, Automatically Created Sentiment Lexicons The manually created lexicons we used include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011) (about 14,000 words), the MPQA Lexicon (Wilson et al., 2005) (about 8,000 words), and the Bing Liu Lexicon (Hu and Liu, 2004) (about 6,800 words). 2.2 New, Tweet-Specific, Automatically Generated Sentiment Lexicons 2.2.1 NRC Hashtag Sentiment Lexicon Certain words in tweets are specially marked with a hashtag (#) to indicate the topic or sentiment. Mo3www.purl.com/net/sentimentoftweets hammad (2012) showed that hashtagged emotion words such as joy, sadness, angry, and surprised are good indicators that the tweet as a whole (even without the hashtagged emotion word) is expressing the same emot</context>
</contexts>
<marker>Mohammad, Yang, 2011</marker>
<rawString>Saif Mohammad and Tony Yang. 2011. Tracking Sentiment in Mail: How Genders Differ on Emotional Axes. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011), pages 70–79, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
</authors>
<title>Emotional Tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>246--255</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<marker>Mohammad, 2012</marker>
<rawString>Saif Mohammad. 2012. #Emotional Tweets. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), pages 246– 255, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment Classification Using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="10722" citStr="Pang et al., 2002" startWordPosition="1694" endWordPosition="1697"> emoticons at any position in the tweet; – whether the last token is a positive or negative emoticon; • elongated words: the number of words with one character repeated more than two times, for example, ‘soooo’; • clusters: The CMU pos-tagging tool provides the token clusters produced with the Brown clustering algorithm on 56 million Englishlanguage tweets. These 1,000 clusters serve as alternative representation of tweet content, reducing the sparcity of the token space. – the presence or absence of tokens from each of the 1000 clusters; • negation: the number of negated contexts. Following (Pang et al., 2002), we defined a negated context as a segment of a tweet that starts with a negation word (e.g., no, shouldn’t) and ends with one of the punctuation marks: ‘,’, ‘.’, ‘:’, ‘;’, ‘!’, ‘?’.A negated context affects the ngram and lexicon features: we add ‘ NEG’ suffix to each word following the negation word (‘perfect’ becomes ‘perfect NEG’). The ‘ NEG’ suffix is also added to polarity and emotion features (‘POLARITY positive’ becomes ‘POLARITY positive NEG’). The list of negation words was adopted from Christopher Potts’ sentiment tutorial.5 4http://sentiment.christopherpotts.net/tokenizing.html 5ht</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment Classification Using Machine Learning Techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 79–86, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Salath´e</author>
<author>Shashank Khandelwal</author>
</authors>
<title>Assessing vaccination sentiments with online social media: Implications for infectious disease dynamics and control.</title>
<date>2011</date>
<journal>PLoS Computational Biology,</journal>
<volume>7</volume>
<issue>10</issue>
<marker>Salath´e, Khandelwal, 2011</marker>
<rawString>Marcel Salath´e and Shashank Khandelwal. 2011. Assessing vaccination sentiments with online social media: Implications for infectious disease dynamics and control. PLoS Computational Biology, 7(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudha Verma</author>
<author>Sarah Vieweg</author>
<author>William Corvey</author>
<author>Leysia Palen</author>
<author>James Martin</author>
<author>Martha Palmer</author>
<author>Aaron Schram</author>
<author>Kenneth Anderson</author>
</authors>
<title>Natural language processing to the rescue? extracting ”situational awareness” tweets during mass emergency.</title>
<date>2011</date>
<booktitle>In International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="1465" citStr="Verma et al., 2011" startWordPosition="215" endWordPosition="218"> from tweets with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated using freely available resources.1 1 Introduction Hundreds of millions of people around the world actively use microblogging websites such as Twitter. Thus there is tremendous interest in sentiment analysis of tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salath´e and Khandelwal, 2011), and disaster management (Verma et al., 2011; Mandel et al., 2012). 1The three authors contributed equally to this paper. Svetlana Kiritchenko developed the system for the message-level task, Xiaodan Zhu developed the system for the term-level task, and Saif Mohammad led the overall effort, co-ordinated both tasks, and contributed to feature development. In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a message (term-level task). The sentiment can be one out of three possibi</context>
</contexts>
<marker>Verma, Vieweg, Corvey, Palen, Martin, Palmer, Schram, Anderson, 2011</marker>
<rawString>Sudha Verma, Sarah Vieweg, William Corvey, Leysia Palen, James Martin, Martha Palmer, Aaron Schram, and Kenneth Anderson. 2011. Natural language processing to the rescue? extracting ”situational awareness” tweets during mass emergency. In International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5186" citStr="Wilson et al., 2005" startWordPosition="786" endWordPosition="789">th emoticons. The automatically generated lexicons were particularly useful. In the message-level task for tweets, they alone provided a gain of more than 5 F-score points over and above that obtained using all other features. The lexicons are made freely available.3 2 Sentiment Lexicons Sentiment lexicons are lists of words with associations to positive and negative sentiments. 2.1 Existing, Automatically Created Sentiment Lexicons The manually created lexicons we used include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011) (about 14,000 words), the MPQA Lexicon (Wilson et al., 2005) (about 8,000 words), and the Bing Liu Lexicon (Hu and Liu, 2004) (about 6,800 words). 2.2 New, Tweet-Specific, Automatically Generated Sentiment Lexicons 2.2.1 NRC Hashtag Sentiment Lexicon Certain words in tweets are specially marked with a hashtag (#) to indicate the topic or sentiment. Mo3www.purl.com/net/sentimentoftweets hammad (2012) showed that hashtagged emotion words such as joy, sadness, angry, and surprised are good indicators that the tweet as a whole (even without the hashtagged emotion word) is expressing the same emotion. We adapted that idea to create a large corpus of positiv</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2280" citStr="Wilson et al., 2013" startWordPosition="341" endWordPosition="344">evel task, and Saif Mohammad led the overall effort, co-ordinated both tasks, and contributed to feature development. In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a message (term-level task). The sentiment can be one out of three possibilities: positive, negative, or neutral. We developed these classifiers to participate in an international competition organized by the Conference on Semantic Evaluation Exercises (SemEval-2013) (Wilson et al., 2013).2 The organizers created and shared sentiment-labeled tweets for training, development, and testing. The distributions of the labels in the different datasets is shown in Table 1. The competition, officially referred to as Task 2: Sentiment Analysis in Twitter, had 44 teams (34 for the message-level task and 23 for the term-level task). Our submissions stood first in both tasks, obtaining a macro-averaged F-score of 69.02 in the message-level task and 88.93 in the term-level task. The task organizers also provided a second test dataset, composed of Short Message Service (SMS) messages (no tra</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, Ritter, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013. SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, Atlanta, Georgia, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>