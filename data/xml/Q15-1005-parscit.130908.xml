<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9996915">
A Sense-Topic Model for Word Sense Induction
with Unsupervised Data Enrichment
</title>
<author confidence="0.998971">
Jing Wang* Mohit Bansal† Kevin Gimpel† Brian D. Ziebart* Clement T. Yu*
</author>
<affiliation confidence="0.9984">
*University of Illinois at Chicago, Chicago, IL, 60607, USA
</affiliation>
<email confidence="0.988335">
{jwang69,bziebart,cyu}@uic.edu
</email>
<affiliation confidence="0.757718">
†Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA
</affiliation>
<email confidence="0.997106">
{mbansal,kgimpel}@ttic.edu
</email>
<sectionHeader confidence="0.997349" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997519352941176">
Word sense induction (WSI) seeks to automat-
ically discover the senses of a word in a cor-
pus via unsupervised methods. We propose
a sense-topic model for WSI, which treats
sense and topic as two separate latent vari-
ables to be inferred jointly. Topics are in-
formed by the entire document, while senses
are informed by the local context surrounding
the ambiguous word. We also discuss unsu-
pervised ways of enriching the original cor-
pus in order to improve model performance,
including using neural word embeddings and
external corpora to expand the context of each
data instance. We demonstrate significant im-
provements over the previous state-of-the-art,
achieving the best results reported to date on
the SemEval-2013 WSI task.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999802411764706">
Word sense induction (WSI) is the task of automat-
ically discovering all senses of an ambiguous word
in a corpus. The inputs to WSI are instances of the
ambiguous word with its surrounding context. The
output is a grouping of these instances into clusters
corresponding to the induced senses. WSI is gen-
erally conducted as an unsupervised learning task,
relying on the assumption that the surrounding con-
text of a word indicates its meaning. Most previous
work assumed that each instance is best labeled with
a single sense, and therefore, that each instance be-
longs to exactly one sense cluster. However, recent
work (Erk and McCarthy, 2009; Jurgens, 2013) has
shown that more than one sense can be used to inter-
pret certain instances, due to context ambiguity and
sense relatedness.
To handle these characteristics of WSI (unsuper-
vised, senses represented by token clusters, multiple
senses per instance), we consider approaches based
on topic models. A topic model is an unsupervised
method that discovers the semantic topics underly-
ing a collection of documents. The most popular is
latent Dirichlet allocation (LDA; Blei et al., 2003),
in which each topic is represented as a multinomial
distribution over words, and each document is rep-
resented as a multinomial distribution over topics.
One approach would be to run LDA on the in-
stances for an ambiguous word, then simply inter-
pret topics as induced senses (Brody and Lapata,
2009). However, while sense and topic are related,
they are distinct linguistic phenomena. Topics are
assigned to entire documents and are expressed by
all word tokens, while senses relate to a single am-
biguous word and are expressed through the local
context of that word. One possible approach would
be to only keep the local context of each ambigu-
ous word, discarding the global context. However,
the topical information contained in the broader con-
text, though it may not determine the sense directly,
might still be useful for narrowing down the likely
senses of the ambiguous word.
Consider the ambiguous word cold. In the sen-
tence “His reaction to the experiments was cold”,
the possible senses for cold include cold tempera-
ture, a cold sensation, common cold, or a negative
emotional reaction. However, if we know that the
topic of the document concerns the effects of low
temperatures on physical health, then the negative
emotional reaction sense should become less likely.
Therefore, in this case, knowing the topic helps nar-
row down the set of plausible senses.
</bodyText>
<page confidence="0.994824">
59
</page>
<bodyText confidence="0.956337625">
Transactions of the Association for Computational Linguistics, vol. 3, pp. 59–71, 2015. Action Editor: Hwee Tou Ng.
Submission batch: 10/2014; Revision batch 12/2014; Revision batch 1/2015; Published 1/2015. c�2015 Association for Computational Linguistics.
At the same time, knowing the sense can also help
determine possible topics. Consider a set of texts
that all include the word cold. Without further in-
formation, the texts might discuss any of a number
of possible topics. However, if the sense of cold is
that of cold ischemia, then the most probable topics
would be those related to organ transplantation.
In this paper, we propose a sense-topic model for
WSI, which treats sense and topic as two separate
latent variables to be inferred jointly (§4). When re-
lating the sense and topic variables, a bidirectional
edge is drawn between them to represent their cyclic
dependence (Heckerman et al., 2001). We perform
inference using collapsed Gibbs sampling (§4.2),
then estimate the sense distribution for each instance
as the solution to the WSI task. We conduct exper-
iments on the SemEval-2013 Task 13 WSI dataset,
showing improvements over several strong baselines
and task systems (§5).
We also present unsupervised ways of enriching
our dataset, including using neural word embed-
dings (Mikolov et al., 2013) and external Web-scale
corpora to enrich the context of each data instance
or to add more instances (§6). Each data enrich-
ment method gives further gains, resulting in sig-
nificant improvements over existing state-of-the-art
WSI systems. Overall, we find gains of up to 22%
relative improvement in fuzzy B-cubed and 50% rel-
ative improvement in fuzzy normalized mutual in-
formation (Jurgens and Klapaftis, 2013).
</bodyText>
<sectionHeader confidence="0.963598" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999741741935484">
We discuss the WSI task, then discuss several areas
of research that are related to our approach, includ-
ing applications of topic modeling to WSI as well
as other approaches that use word embeddings and
clustering algorithms.
WSD and WSI: WSI is related to but distinct
from word sense disambiguation (WSD). WSD
seeks to assign a particular sense label to each
target word instance, where the sense labels are
known and usually drawn from an existing sense
inventory like WordNet (Miller et al., 1990). Al-
though extensive research has been devoted to WSD,
WSI may be more useful for downstream tasks.
WSD relies on sense inventories whose construc-
tion is time-intensive, expensive, and subject to poor
inter-annotator agreement (Passonneau et al., 2010).
Sense inventories also impose a fixed sense gran-
ularity for each ambiguous word, which may not
match the ideal granularity for the task of interest.
Finally, they may lack domain-specific senses and
are difficult to adapt to low-resource domains or lan-
guages. In contrast, senses induced by WSI are more
likely to represent the task and domain of interest.
Researchers in machine translation and information
retrieval have found that predefined senses are of-
ten not well-suited for these tasks (Voorhees, 1993;
Carpuat and Wu, 2005), while induced senses can
lead to improved performance (V´eronis, 2004; Vick-
rey et al., 2005; Carpuat and Wu, 2007).
Topic Modeling for WSI: Brody and Lapata
(2009) proposed a topic model that uses a weighted
combination of separate LDA models based on dif-
ferent feature sets (e.g. word tokens, parts of speech,
and dependency relations). They only used smaller
units of text surrounding the ambiguous word, dis-
carding the global context of each instance. Yao and
Van Durme (2011) proposed a model based on a hi-
erarchical Dirichlet process (HDP; Teh et al., 2006),
which has the advantage that it can automatically
discover the number of senses. Lau et al. (2012) de-
scribed a model based on an HDP with positional
word features; it formed the basis for their submis-
sion (unimelb, Lau et al., 2013) to the SemEval-
2013 WSI task (Jurgens and Klapaftis, 2013).
Our sense-topic model is distinct from this prior
work in that we model sense and topic as two sepa-
rate latent variables and learn them jointly. We com-
pare to the performance of unimelb in §5.
For word sense disambiguation, there also exist
several approaches that use topic models (Cai et al.,
2007; Boyd-Graber and Blei, 2007; Boyd-Graber et
al., 2007; Li et al., 2010); space does not permit a
full discussion.
Word Representations for WSI: Another ap-
proach to solving WSI is to use word representations
built by distributional semantic models (DSMs;
Sahlgren, 2006) or neural net language models
(NNLMs; Bengio et al., 2003; Mnih and Hinton,
2007). Their assumption is that words with similar
distributions have similar meanings. Akkaya et al.
(2012) use word representations learned from DSMs
directly for WSI. Each word is represented by a co-
</bodyText>
<page confidence="0.996954">
60
</page>
<bodyText confidence="0.998938621621621">
occurrence vector, and the meaning of an ambigu-
ous word in a specific context is computed through
element-wise multiplication applied to the vector of
the target word and its surrounding words in the con-
text. Then instances are clustered by hierarchical
clustering based on their representations.
Word representations trained by NNLMs, often
called word embeddings, capture information via
training criteria based on predicting nearby words.
They have been useful as features in many NLP
tasks (Turian et al., 2010; Collobert et al., 2011;
Dhillon et al., 2012; Hisamoto et al., 2013; Bansal
et al., 2014). The similarity between two words can
be computed using cosine similarity of their embed-
ding vectors. Word embeddings are often also used
to build representations for larger units of text, such
as sentences, through vector operations (e.g., sum-
mation) applied to the vector of each token in the
sentence. In our work, we use word embeddings
to compute word similarities (for better modeling of
our data distribution), to represent sentences (to find
similar sentences in external corpora for data enrich-
ment), and in a product-of-embeddings baseline.
Baskaya et al. (2013) represent the context of each
ambiguous word by using the most likely substitutes
according to a 4-gram LM. They pair the ambigu-
ous word with likely substitutes, project the pairs
onto a sphere (Maron et al., 2010), and obtain final
senses via k-means clustering. We compare to their
SemEval-2013 system AI-KU (§5).
Other Approaches to WSI: Other approaches in-
clude clustering algorithms to partition instances
of an ambiguous word into sense-based clus-
ters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare
and Pedersen, 2004), or graph-based methods to in-
duce senses (Dorow and Widdows, 2003; V´eronis,
2004; Agirre and Soroa, 2007).
</bodyText>
<sectionHeader confidence="0.971841" genericHeader="method">
3 Problem Setting
</sectionHeader>
<bodyText confidence="0.9997954">
In this paper, we induce senses for a set of word
types, which we refer to as target words. For each
target word, we have a set of instances. Each in-
stance provides context for a single occurrence of
the target word.1 For our experiments, we use the
</bodyText>
<footnote confidence="0.986269333333333">
1The target word token may occur multiple times in an in-
stance, but only one occurrence is chosen as the target word
occurrence.
</footnote>
<figureCaption confidence="0.994298666666667">
Figure 1: Proposed sense-topic model in plate notation.
There are MD instances for the given target word. In
an instance, there are N9 global context words (w9) and
Nj local context words (wj), all of which are observed.
There is one latent variable (“topic” t9) for the w9 and
two latent variables (“topic” tj and “sense” sj) for the
wj. Each instance has topic mixing proportions θt and
sense mixing proportions θ3. For clarity, not all variables
are shown. The complete figure with all variables is given
in Appendix A. This is a dependency network, not a di-
rected graphical model, as shown by the directed arrows
between tj and sj; see text for details.
</figureCaption>
<bodyText confidence="0.999970153846154">
dataset released for SemEval-2013 Task 13 (Jur-
gens and Klapaftis, 2013), collected from the Open
American National Corpus (OANC; Ide and Suder-
man, 2004).2 It includes 50 target words: 20 verbs,
20 nouns, and 10 adjectives. There are a total of
4,664 instances across all target words. Each in-
stance contains only one sentence, with a minimum
length of 22 and a maximum length of 100. The gold
standard for the dataset was prepared by multiple
annotators, where each annotator labeled instances
based on the sense inventories in WordNet 3.1. For
each instance, they rated all senses of a target word
on a Likert scale from one to five.
</bodyText>
<sectionHeader confidence="0.996323" genericHeader="method">
4 A Sense-Topic Model for WSI
</sectionHeader>
<bodyText confidence="0.999835666666666">
We now present our sense-topic model, shown in
plate notation in Figure 1. It generates the words in
the set of instances for a single target word; we run
the model separately for each target word, sharing
no parameters across target words. We treat sense
and topic as two separate latent variables to be in-
ferred jointly. To differentiate sense and topic, we
use a window around the target word in each in-
stance. Word tokens inside the window are local
</bodyText>
<footnote confidence="0.984212666666667">
2“Word Sense Induction for Graded and Non-
Graded Senses,” http://www.cs.york.ac.uk/
semeval-2013/task13
</footnote>
<page confidence="0.999409">
61
</page>
<bodyText confidence="0.968345909090909">
context words (w`), while tokens outside the win-
dow are global context words (wg). The number of
words in the window is fixed to 21 in all experiments
(10 words before the target word and 10 after).
Generating global context words: As shown in
the left part of Figure 1, each global context word
wg is generated from a latent topic variable tg for the
instance, which follows the same generative story
as LDA. The corresponding probability of the ith
global context wordw(i) gwithin instance d is:3
where T is the number of topics, Pψtj (w(i)
</bodyText>
<equation confidence="0.853228090909091">
g |t(i)
g = j)
is the multinomial distribution over words for topic
j (parameterized by ψtj) and Pθt(t(i)
g = j|d) is the
multinomial distribution over topics for instance d
(parameterized by θt).
Generating local context words: A local context
word w` is generated from a topic variable t` and a
sense variable s`:
Pr(w`|d, θt, ψt, θs, ψs, θs|t, θt|s, θst) =
</equation>
<bodyText confidence="0.9997882">
where S is the number of senses, Pr(w`|t` = j, s` =
k) is the probability of generating word w` given
topic j and sense k, and Pr(t` = j, s` = k|d) is
the joint probability over topics and senses for d.4
Unlike in Eq. (1), we do not use multinomial
parameterizations for the distributions in Eq. (2).
When parameterizing them, we make several de-
partures from purely-generative modeling. All our
choices result in distributions over smaller event
spaces and/or those that condition on fewer vari-
ables. This helps to mitigate data sparsity is-
sues arising from attempting to estimate high-
dimensional distributions from small datasets. A
secondary benefit is that we can avoid biases caused
by particular choices of generative directionality in
</bodyText>
<footnote confidence="0.98917875">
3We use Pr() for generic probability distributions without
further qualifiers and PB() for distributions parameterized by θ.
4For clarity, we drop the (i) superscripts in these and the
following equations.
</footnote>
<bodyText confidence="0.992154857142857">
the model. We later include an empirical compari-
son to justify some of our modeling choices (§5).
First, when relating the sense and topic variables,
we avoid making a single decision about generative
dependence. Taking inspiration from dependency
networks (Heckerman et al., 2001), we use the fol-
lowing factorization:
</bodyText>
<equation confidence="0.999932">
Pr(t` = j, s` = k|d) =
Pr(s` = k|d,t` = j)Pr(t` = j|d,s` = k) (3)
</equation>
<bodyText confidence="0.999181">
where Zd is a normalization constant.
We factorize further by using redundant proba-
bilistic events, then ignore the normalization con-
stants during learning, a concept commonly called
deficiency (Brown et al., 1993). Deficient modeling
has been found to be useful for a wide range of NLP
tasks (Klein and Manning, 2002; May and Knight,
2007; Toutanova and Johnson, 2007). In particular,
we factor the conditional probabilities in Eq. (3) into
products of multinomial probabilities:
</bodyText>
<equation confidence="0.9940716">
Pr(s` = k|d,t` = j) =
Pθs(s` =k|d)Pθs|tj(s` =k|t` =j)Pθst(t` =j, s` =k)
Zd,tj
Pθt(t` =j|d)Pθt|sk(t` =j|s` =k)
Zd,sk
</equation>
<bodyText confidence="0.9965092">
where Zd,tj and Zd,sk are normalization factors and
we have introduced new multinomial parameters
θs, θs|tj, θst, and θt|sk.
We use the same idea to factor the word genera-
tion distribution:
</bodyText>
<equation confidence="0.96234">
Pψtj(w`|t`=j)Pψsk(w`|s`=k)
Ztj,sk
</equation>
<bodyText confidence="0.999892166666667">
where Ztj,sk is a normalization factor, and we have
new multinomial parameters ψsk for the sense-word
distributions. One advantage of this parameteriza-
tion is that we naturally tie the topic-word distribu-
tions across the global and local context words by
using the same parameters ψtj.
</bodyText>
<subsectionHeader confidence="0.983854">
4.1 Generative Story
</subsectionHeader>
<bodyText confidence="0.99995825">
We now give the full generative story of our model.
We describe it for generating a set of instances of
size MD, where all instances contain the same tar-
get word. We use symmetric Dirichlet priors for
</bodyText>
<equation confidence="0.998755238095238">
Pr(w`|t` =j, s` =k) Pr(t` =j, s` =k|d) (2)
S
E
k=1
T
E
j=1
Pr(w(i)
g |d,θt, ψt)=
T
E
j=1
Pψtj(w(i)
g |t(i)
g =j)Pθt(t(i)
g =j|d)
(1)
1
Zd
Pr(t`=j|d,s`=k)=
Pr(w`|t` =j, s` =k)=
</equation>
<page confidence="0.973435">
62
</page>
<bodyText confidence="0.974542933333333">
all multinomial distributions mentioned above, us-
ing the same fixed hyperparameter value (α) for
all. We use ψ to denote parameters of multinomial
distributions over words, and θ to denote parame-
ters of multinomial distributions over topics and/or
senses. We leave unspecified the distributions over
N` (number of local words in an instance) and Ng
(number of global words in an instance), as we only
use our model to perform inference given fixed in-
stances, not to generate new instances.
The generative story first follows the steps de-
scribed in Algo. 1 to generate parameters that are
shared across all instances; then for each instance d,
it follows Algo. 2 to generate global and local words.
Algorithm 1 Generative story for instance set
</bodyText>
<listItem confidence="0.995716653846154">
1: for each topic j +- 1 to T do
2: Choose topic-word params. ψtj — Dir(α)
3: Choose topic-sense params. θs|tj Dir(α)
4: for each sense k +- 1 to S do
5: Choose sense-word params. ψsk — Dir(α)
6: Choose sense-topic params. θt|sk Dir(α)
7: Choose topic/sense params. θst — Dir(α)
Algorithm 2 Generative story for instance d
1: Choose topic proportions θt Dir(α)
2: Choose sense proportions θs Dir(α)
3: Choose Ng and N` from unspecified distributions
4: fori+- 1 to Ng do
5: Choose a topic j — Mult(θt)
6: Choose a word wg — Mult(ψtj)
7: fori+- 1 to N` do
8: repeat
9: Choose a topic j Mult(θt)
10: Choose a sense k Mult(θs)
11: Choose a topic j0 Mult(θt|sk)
12: Choose a sense k0 Mult(θs|tj)
13: Choose topic/sense (j00, k00) — Mult(θst)
14: until j = j0 = j00 and k = k0 = k00
15: repeat
16: Choose a word w` — Mult(ψtj)
17: Choose a word w0` — Mult(ψsk)
18: until w` = w0`
</listItem>
<subsectionHeader confidence="0.736063">
4.2 Inference
</subsectionHeader>
<bodyText confidence="0.989132102040817">
We use collapsed Gibbs sampling (Geman and Ge-
man, 1984) to obtain samples from the posterior dis-
tribution over latent variables, with all multinomial
parameters analytically integrated out before sam-
pling. Then we estimate the sense distribution θs for
each instance using maximum likelihood estimation
on the samples. These sense distributions are the
output of our WSI system.
We note that deficient modeling does not ordinar-
ily affect Gibbs sampling when used for computing
posteriors over latent variables, as long as parame-
ters (the θ and ψ) are kept fixed. This is the case
during the E step of an EM algorithm, which is the
usual setting in which deficiency is used. Only the
M step is affected; it becomes an approximate M
step by assuming the normalization constants equal
1 (Brown et al., 1993).
However, here we use collapsed Gibbs sampling
for posterior inference, and the analytic integration
is disrupted by the presence of the normalization
constants. To bypass this, we employ the standard
approximation of deficient models that all normal-
ization constants are 1, permitting us to use stan-
dard formulas for analytic integration of multino-
mial parameters with Dirichlet priors. Empirically,
we found this “collapsed deficient Gibbs sampler”
to slightly outperform a more principled approach
based on EM, presumably due to the ability of col-
lapsing to accelerate mixing.
During the sampling process, each sampler is run
on the full set of instances for a target word, iterating
through all word tokens in each instance. If the cur-
rent word token is a global context word, we sample
a new topic for it conditioned on all other latent vari-
ables across instances. If the current word is a local
context word, we sample a new topic/sense pair for it
again conditioned on all other latent variable values.
We write the conditional posterior distribution
over topics for global context word token i in in-
stance d as Pr(t(i)
g = j|d, t−i, s, ·), where t(i) g= j
is the topic assignment of token i, d is the current
instance, t−i is the set of topic assignments of all
word tokens aside from i for instance d, s is the
set of sense assignments for all local word tokens
in instance d, and “·” stands for all other observed or
known information, including all words, all Dirich-
let hyperparameters, and all latent variable assign-
ments in other instances. The conditional posterior
</bodyText>
<page confidence="0.971857">
63
</page>
<equation confidence="0.932394666666667">
can be computed by:
Pr(t(i) g= j|d, t−i, s, ·)
CDT
dj + α
∝
PTk=1 CDT
dk + Tα
 |{z }
Pr(t=j|d,t−i,8,·)
</equation>
<bodyText confidence="0.977894421052632">
where we use the superscript DT as a mnemonic for
“instance/topic” when counting topic assignments in
an instance and WT for “word/topic” when count-
ing topic assignments for a word. CDT
dj contains the
number of times topic j is assigned to some word
token in instance d, excluding the current word to-
ken w(i)
g ; CWT ijis the number of times word w(i) gis
assigned to topic j, across all instances, excluding
the current word token. Wt is the number of dis-
tinct word types in the full set of instances. We show
the corresponding conditional posterior probabilities
underneath each term; the count ratios are obtained
using standard Dirichlet-multinomial collapsing.
The conditional posterior distribution over
topic/sense pairs for a local context word tokenw(i)
`
can be computed by:
</bodyText>
<equation confidence="0.998084">
Pr(t(i)
` = j, s(i)
` = k|d, t−i, s−i, ·) ∝
CWT ij+ α
</equation>
<bodyText confidence="0.9996084">
Decoding After the sampling process, we obtain a
fixed-point estimate of the sense distribution (θs) for
each instance d using the counts from our samples.
Where we use θks to denote the probability of sense
k for the instance, this amounts to:
</bodyText>
<table confidence="0.904337">
θk CDS
s dk
=S DS (6)
Pk0=1 Cdk0
</table>
<bodyText confidence="0.997293428571429">
This distribution is considered the final sense assign-
ment distribution for the target word in instance d for
the WSI task; the full distribution is fed to the eval-
uation metrics defined in the next section.
To inspect what the model learned, we similarly
obtain the sense-word distribution (ψs) from the
counts as follows, where ψiis the probability of
</bodyText>
<equation confidence="0.935751666666667">
sk
word type i given sense k:
ψi sk = CWS
PWs ik (7)
i0=1 CW S
i0k
</equation>
<sectionHeader confidence="0.998518" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999588333333333">
In this section, we evaluate our sense-topic model
and compare it to several strong baselines and state-
of-the-art systems.
</bodyText>
<equation confidence="0.999505605263158">
CWT ij+ α
PWt
k0=1 CWT
k0j + Wtα
 |{z }
Pr(w(i)
g |t=j,t−i,8,·)
(4)
CDT
dj + α
PT PWt
k0=1 CDT
dk0 + Tα k0=1 CW T
k0j + Wtα
 |{z }
Pr(w(i)
` |t=j,t−i,8,·)
CW S
ik + α
PWs
k0=1 CW S
k0k + Wsα
 |{z }
Pr(w(i)
` |s=k,s−i,·)
CST
kj + α
PSk0=1 CST
k0j + Sα
 |{z }
Pr(s=k|t=j,t−i,s−i,·)
CST
kj + α
PS PT j0=1 CST
k0j0 + STα
k0=1
 |{z }
Pr(s=k,t=j|t−i,s−i,·)
</equation>
<bodyText confidence="0.996316580645161">
where CDS
dk contains the number of times sense k is
assigned to some local word token in instance d, ex-
cluding the current word token; CW S
ik contains the
number of time word w(i)
` is assigned to sense k, ex-
cluding the current time; CST
kj contains the number
of times sense k and topic j are assigned to some lo-
cal word tokens. Ws is the number of distinct local
context word types across the collection.
Evaluation Metrics To evaluate WSI systems, Ju-
rgens and Klapaftis (2013) propose two metrics:
fuzzy B-cubed and fuzzy normalized mutual infor-
mation (NMI). They are each computed separately
for each target word, then averaged across target
words. Fuzzy B-cubed prefers labeling all instances
with the same sense, while fuzzy NMI prefers the
opposite extreme of labeling all instances with dis-
tinct senses. Hence, we report both fuzzy B-cubed
(%) and fuzzy NMI (%) in our evaluation. For ease
of comparison, we also report the geometric mean
of the 2 metrics, which we denote by AVG.5
SemEval-2013 Task 13 also provided a trial
dataset (TRIAL) that consists of eight target ambigu-
ous words, each with 50 instances (Erk et al., 2009).
We use it for preliminary experiments of our model
and for tuning certain hyperparameters, and evalu-
ate final performance on the SemEval-2013 dataset
(TEST) with 50 target words.
</bodyText>
<footnote confidence="0.805907">
5We do not use an arithmetic mean because the effective
range of the two metrics is substantially different.
</footnote>
<equation confidence="0.993740722222222">
 |{z }
Pr(t=j|d,t−i,8,·)
CDS
dk + α
PS k0=1 CDS
dk0 + Sα
 |}
{z
Pr(s=k|d,s−i,·)
CST
kj + α
(5)
PT k0=1 CST
kk0 + Tα
|
{z
}
Pr(t=j|s=k,t−i
</equation>
<page confidence="0.996547">
64
</page>
<table confidence="0.999202333333333">
S B-cubed(%) NMI(%) AVG
2 42.9 4.18 13.39
3 31.9 6.50 14.40
5 22.3 8.60 13.85
7 15.4 8.72 11.61
10 12.5 10.91 11.67
</table>
<tableCaption confidence="0.892758333333333">
Table 1: Performance on TRIAL for the sense-topic
model with different numbers of senses (S). Best score
in each column is bold.
</tableCaption>
<bodyText confidence="0.999809564102565">
Hyperparameter Tuning We use TRIAL to ana-
lyze performance of our sense-topic model under
different settings for the numbers of senses (S) and
topics (T); see Table 1. We always set T = 2S
for simplicity. We find that small S values work
best, which is unsurprising considering the relatively
small number of instances and small size of each in-
stance. When evaluating on TEST, we use S = 3
(which gives the best AVG results on TRIAL). Later,
when we add larger context or more instances (see
§6), tuning on TRIAL chooses a larger S value.
During inference, the Gibbs sampler was run for
4,000 iterations for each target word, setting the first
500 iterations as the burn-in period. In order to get a
representative set of samples, every 13th sample (af-
ter burn-in) is saved to prevent correlations among
samples. Due to the randomized nature of the in-
ference procedure, all reported results are average
scores over 5 runs. The hyperparameters (α) for all
Dirichlet priors in our model are set to the (untuned)
value of 0.01, following prior work on topic model-
ing (Griffiths and Steyvers, 2004; Heinrich, 2005).
Baselines We include two naive baselines corre-
sponding to the two extremes (biases) preferred by
fuzzy B-cubed and NMI, respectively: 1 sense (la-
bel each instance with the same single sense) and all
distinct (label each instance with its own sense).
We also consider two baselines based on LDA.
We run LDA for each target word in TEST, using the
set of instances as the set of documents. We treat the
learned topics as induced senses. When setting the
number of topics (senses), we use the gold-standard
number of senses for each target word, making this
baseline unreasonably strong. We run LDA both
with full context (FULL) and local context (LOCAL),
using the same window size as above (10 words be-
fore and after the target word).
We also present results for the two best systems
in the SemEval-2013 task (according to fuzzy B-
cubed and fuzzy NMI, respectively): unimelb and
AI-KU. As described in Section 2, unimelb uses
hierarchical Dirichlet processes (HDPs). It extracts
50,000 extra instances for each target word as train-
ing data from the ukWac corpus−a web corpus of
approximately 2 billion tokens.6 Among all systems
in the task, it performs best according to fuzzy B-
cubed. AI-KU is based on a lexical substitution
method; a language model is built to identify lexical
substitutes for target words from the dataset and the
ukWac corpus. It performed best among all systems
according to fuzzy NMI.
Results In Table 2, we present results for these
systems and compare them to our basic (i.e., without
any data enrichment) sense-topic model with S = 3
(row 9). According to both fuzzy B-cubed and fuzzy
NMI, our model outperforms the other WSI systems
(LDA, AI-KU, and unimelb). Hence, we are able
to achieve state-of-the-art results on the SemEval-
2013 task even when only using the single sentence
of context given in each instance (while AI-KU and
unimelb use large training sets from ukWac). We
found similar performance improvements when only
tested on instances labeled with a single sense.
Bidirectionalilty Analysis To measure the impact
of the bidirectional dependency between the topic
and sense variables in our model, we also evalu-
ate the performance of our sense-topic model when
dropping one of the directions. In Table 3, we
compare their performance with our full sense-topic
model on TEST. Both unidirectional models perform
worse than the full model, and dropping t → s hurts
more. This result verifies our intuition that topics
would help narrow down the set of likely senses, and
suggests that bidirectional modeling between topic
and sense is desirable for WSI.
In subsequent sections, we investigate several
ways of exploiting additional data to build better-
performing sense-topic models.
</bodyText>
<sectionHeader confidence="0.998443" genericHeader="method">
6 Unsupervised Data Enrichment
</sectionHeader>
<bodyText confidence="0.99897">
The primary signal used by our model is word co-
occurrence information across instances. If we en-
</bodyText>
<footnote confidence="0.9761515">
6http://wacky.sslmit.unibo.it/doku.php?
id=corpora
</footnote>
<page confidence="0.997741">
65
</page>
<table confidence="0.998346333333333">
Model Data Enrichment Fuzzy B-cubed % Fuzzy NMI % AVG
1 1 sense – 62.3 0 –
2 all distinct – 0 7.09 –
3 unimelb add 50k instances 48.3 6.0 17.02
4 AI-KU add 20k instances 39.0 6.5 15.92
5 LDA (LOCAL) none 47.1 5.93 16.71
6 LDA (FULL) none 47.3 5.79 16.55
7 LDA (FULL) add actual context (§6.1) 43.5 6.41 16.70
8 word embedding product (§6.3) none 33.3 7.24 15.53
THIS PAPER
9 none 53.5 6.96 19.30
10 add ukWac context (§6.1) 54.5 9.74 23.04
11 Sense-Topic Model add actual context (§6.1) 59.1 9.39 23.56
12 add instances (§6.2) 58.9 6.01 18.81
13 weight by sim. (§6.3) 55.4 7.14 19.89
</table>
<tableCaption confidence="0.992318">
Table 2: Performance on TEST for baselines and our sense-topic model. Best score in each column is bold.
</tableCaption>
<table confidence="0.99972675">
Model B-cubed(%) NMI(%) AVG
Drop s t 52.1 6.84 18.88
Drop t s 51.1 6.78 18.61
Full 53.5 6.96 19.30
</table>
<tableCaption confidence="0.995429">
Table 3: Performance on TEST for the sense-topic model
with ablation of links between sense and topic variables.
</tableCaption>
<bodyText confidence="0.999556136363636">
rich the instances, we can have more robust co-
occurrence statistics. The SemEval-2013 dataset
may be too small to induce meaningful senses, since
there are only about 100 instances for each target
word, and each instance only contains one sentence.
This is why most shared task systems added in-
stances from external corpora.
In this section, we consider three unsupervised
ways of enriching data and measure their impact on
performance. In §6.1 we augment the context of
each instance in our original dataset while keeping
the number of instances fixed. In §6.2 we collect
more instances of each target word from ukWac,
similar to the AI-KU and unimelb systems. In
§6.3, we change the distribution of words in each in-
stance based on their similarity to the target word.
Throughout, we make use of word embeddings
(see §2). We trained 100-dimensional skip-gram
vectors (Mikolov et al., 2013) on English Wikipedia
(tokenized/lowercased, resulting in 1.8B tokens of
text) using window size 10, hierarchical softmax,
and no downsampling.7
</bodyText>
<footnote confidence="0.913006">
7We used a minimum count cutoff of 20 during training,
</footnote>
<subsectionHeader confidence="0.99661">
6.1 Adding Context
</subsectionHeader>
<bodyText confidence="0.999381310344828">
The first way we explore of enriching data is to add
a broader context for each instance while keeping
the number of instances unchanged. This will intro-
duce more word tokens into the set of global con-
text words, while keeping the set of local context
words mostly unchanged, as the window size we use
is typically smaller than the length of the original in-
stance. With more global context words, the model
has more evidence to learn coherent topics, which
could also improve the induced senses via the con-
nection between sense and topic.
The ideal way of enriching context for an instance
is to add its actual context from the corpus from
which it was extracted. To do this for the SemEval-
2013 task, we find each instance in the OANC and
retrieve three sentences before the instance and three
sentences after. While not provided for the SemEval
task, it is reasonable to assume this larger context in
many real-world applications, such as information
retrieval and machine translation of documents.
However, in other settings, the corpus may only
have a single sentence containing the target word
(e.g., search queries or machine translation of sen-
tences). To address this, we find a semantically-
similar sentence from the English ukWac corpus and
append it to the instance as additional context. For
each instance in the original dataset, we extract its
then only retained vectors for the most frequent 100,000 word
types, averaging the rest to get a vector for unknown words.
</bodyText>
<page confidence="0.978089">
66
</page>
<bodyText confidence="0.991821638297872">
most similar sentence that contains the same target
word and add it to increase its set of global context
words. To compute similarity, we first represent in-
stances and ukWac sentences by summing the word
embeddings across their word tokens, then compute
cosine similarity. The ukWac sentence (s*) with the
highest cosine similarity to each original instance
(d) is appended to that instance:
s* = arg max,∈ukWac sim(d, s)
Results Since the vocabulary has increased, we
expect we may need larger values for S and T. On
TRIAL, we find best performance for S = 10, so
we run on TEST with this value. Performance is
shown in Table 2 (rows 10 and 11). These two meth-
ods have higher AVG scores than all others. Both
their fuzzy B-cubed and NMI improvements over
the baselines and previous WSI systems are statisti-
cally significant, as measured by a paired bootstrap
test (p &lt; 0.01; Efron and Tibshirani, 1994).
It is unsurprising that we find best performance
with actual context. Interestingly, however, we
can achieve almost the same gains when automati-
cally finding relevant context from a different cor-
pus. Thus, even in real-world settings where we only
have a single sentence of context, we can induce
substantially better senses by automatically broad-
ening the global context in an unsupervised manner.
As a comparative experiment, we also evaluate
the performance of LDA when adding actual con-
text (Table 2, row 7). Compared with LDA with
full context (FULL) in row 6, performance is slightly
improved, perhaps due to the fact that longer con-
texts induce more accurate topics. However, those
topics are not necessarily related to senses, which
is why LDA with only local context actually per-
forms best among all three LDA models. Thus we
see that merely adding context does not necessarily
help topic models for WSI. Importantly, since our
model includes both sense and topic, we are able to
leverage the additional context to learn better top-
ics while also improving the quality of the induced
senses, leading to our strongest results.
Examples We present examples to illustrate our
sense-topic model’s advantage over LDA and the
further improvement when adding actual context.
Consider instances (1) and (2) below, with target
word occurrences in bold:
</bodyText>
<listItem confidence="0.95417925">
(1) Nigeria then sent troops to challenge the coup, evi-
dently to restore the president and repair Nigeria’s
corrupt image abroad. (image%1:07:01::/4)8
(2) When asked about the Bible’s literal account of
</listItem>
<bodyText confidence="0.868578055555555">
creation, as opposed to the attractive concept of
divine creation, every major Republican presiden-
tial candidate—even Bauer—has squirmed, ducked,
and tried to steer the discussion back to “faith,”
“morals,” and the general idea that humans “were
created in the image of God.” (image%1:06:00::/2
image%1:09:02::/4)
Both instances share the common word stem pres-
ident. LDA uses this to put these two instances
into the same topic (i.e., sense). In our sense-topic
model, president is a local context word in instance
(1) but a global context word in instance (2). So
the effect of sharing words is decreased, and these
two instances are assigned to different senses by our
model. According to the gold standard, the two in-
stances are annotated with different senses, so our
sense-topic model provides the correct prediction.
Next, consider instances (3), (4), and (5):
</bodyText>
<listItem confidence="0.966700352941177">
(3) I have recently deliberately begun to use variations
of “kick ass” and “bites X in the ass” because they
are colorful, evocative phrases; because, thanks
to South Park, ass references are newly familiar
and hilarious and because they don’t evoke partic-
ularly vivid mental image of asses any longer. (im-
age%1:09:00::/4)
(4) Also, playing video games that require rapid mental
rotation of visual image enhances the spatial test
scores of boys and girls alike. (image%1:06:00::/4)
(5) Practicing and solidifying modes of representa-
tion, Piaget emphasized, make it possible for the
child to free thought from the here and now; cre-
ate larger images of reality that take into account
past, present, and future; and transform those im-
age mentally in the service of logical thinking. (im-
age%1:09:00::/4)
</listItem>
<bodyText confidence="0.999851">
In the gold standard, instances (3) and (4) have dif-
ferent senses while (3) and (5) have the same sense.
However, sharing the local context word “mental”
</bodyText>
<footnote confidence="0.999417333333333">
8This is the gold standard sense label, where im-
age%1:07:01:: indexes the wordnet senses, and 4 is the score
assigned by the annotators.The possible range of a score is [1,5].
</footnote>
<page confidence="0.999441">
67
</page>
<bodyText confidence="0.998736368421053">
triggers both LDA and our sense-topic model to as-
sign them to the same sense label with high proba-
bility. When augmenting the instances by their real
contexts, we have a better understanding about the
topics. Instance (3) is about phrase variations, in-
stance (4) is about enhancing boys’ spatial skills,
while instance (5) discusses the effect of make-
believe play for children’s development.
When LDA is run with the actual context, it leaves
(4) and (5) in the same topic (i.e., sense), while as-
signing (3) into another topic with high probability.
This could be because (4) and (5) both relate to child
development, and therefore LDA considers them as
sharing the same topic. However, topic is not the
same as sense, especially when larger contexts are
available. Our sense-topic model built on the ac-
tual context makes correct predictions, leaving (3)
and (5) into the same sense cluster while labeling
(4) with a different sense.
</bodyText>
<subsectionHeader confidence="0.999889">
6.2 Adding Instances
</subsectionHeader>
<bodyText confidence="0.999979580645161">
We also consider a way to augment our dataset with
additional instances from an external corpus. We
have no gold standard senses for these instances, so
we will not evaluate our model on them; they are
merely used to provide richer co-occurrence statis-
tics about the target word so that we can perform
better on the instances on which we evaluate.
If we added randomly-chosen instances (contain-
ing the target word), we would be concerned that the
learned topics and senses may not reflect the distri-
butions of the original instance set. So we only add
instances that are semantically similar to instances in
our original set (Moore and Lewis, 2010; Chambers
and Jurafsky, 2011). Also, to avoid changing the
original sense distribution by adding too many in-
stances, we only add a single instance for each orig-
inal instance. As in §6.1, for each instance in the
original dataset, we find the most similar sentence
in ukWac for each instance using word embeddings
and add it into the dataset. Therefore, the number of
instances is doubled, and we use the enriched dataset
for our sense-topic model.
Results Similarly to §6.1, on TRIAL, we find best
performance for S = 10, so we run on TEST with
this value. As shown in Table 2 (row 12), this
improves fuzzy B-cubed by 5.4%, but fuzzy NMI
is lower, making the AVG worse than the original
model. A possible reason for this is that the sense
distribution in the added instances disturbs that in
the original set of instances, even though we picked
the most semantically similar ones to add.
</bodyText>
<subsectionHeader confidence="0.998653">
6.3 Weighting by Word Similarity
</subsectionHeader>
<bodyText confidence="0.999974588235294">
Another approach is inspired by the observation that
each local context token is treated equally in terms
of its contribution to the sense. However, our intu-
ition is that certain tokens are more indicative than
others. Consider the target word window. Since
glass evokes a particular sense of window, we would
like to weight it more highly than, say, day.
To measure word relatedness, we use cosine sim-
ilarity of word embeddings. We (softly) replicate
each local context word according to its exponenti-
ated cosine similarity to the target word.9 The result
is that the local context in each instance has been
modified to contain fewer occurrences of unrelated
words and more occurrences of related words. If
each cosine similarity is 0, we obtain our original
sense-topic model. During inference, the posterior
sense distribution for instance d is now given by:
</bodyText>
<equation confidence="0.97208775">
Pr(s = k|d, ·) =
E
wCd` exp(sim(w, w*))✶sw=k + α
E
</equation>
<bodyText confidence="0.945142285714286">
w&apos;Cd` exp(sim(w&apos;, w*)) + Sα
where d` is the set of local context tokens in d,
sim(w, w*) is the cosine similarity between w and
target word w*, and ✶s.=k is an indicator returning
1 when w is assigned to sense k and 0 otherwise.
The posterior distribution of sampling a token of
word wi from sense k becomes:
</bodyText>
<equation confidence="0.6594365">
CW S
ik exp(sim(wi, w*)) + α
</equation>
<bodyText confidence="0.982295777777778">
EiW=1 Ci ks exp(sim(wi&apos;, w*)) + Wsα (9)
where CW S
ik counts the number of times wi is as-
signed to sense k.
Results We again use TRIAL to tune S (and still
use T = 2S). We find best TRIAL performance at
S = 3; this is unsurprising since this approach does
not change the vocabulary. In Table 2, we present re-
sults on TEST with S = 3 (row 13). We also report
</bodyText>
<footnote confidence="0.9786455">
9Cosine similarities range from -1 to 1, so we use exponen-
tiation to ensure we always use positive counts.
</footnote>
<figure confidence="0.361317">
(8)
</figure>
<page confidence="0.988742">
68
</page>
<table confidence="0.998317555555556">
Sense Top-5 terms per sense
Sense-Topic Model
1 include, depict, party, paint, visual
2 zero, manage, company, culture, figure
3 create, clinton, people, american, popular
+weight by similarity (§6.3)
1 depict, create, culture, mental, include
2 picture, visual, pictorial, matrix, movie
3 public, means, view, american, story
</table>
<tableCaption confidence="0.905826333333333">
Table 4: Top 5 terms for each sense induced for the noun
image by the sense-topic model and when weighting local
context words by similarity. S = 3 for both.
</tableCaption>
<bodyText confidence="0.99996375">
an additional baseline: “word embedding product”
(row 8), where we represent each instance by mul-
tiplying (element-wise) the word vectors of all lo-
cal context words, and then feed the instance vectors
into the fuzzy c-means clustering algorithm (Pal and
Bezdek, 1995), c = 3. Compared to this baseline,
our approach improves 4.36% on average; compared
with results for the original sense-topic model (row
9), this approach improves 0.69% on average.
In Table 4 we show the top-5 terms for each sense
induced for image, both for the original sense-topic
model and when additionally weighting by similar-
ity. We find that the original model provides less
distinguishable senses, as it is difficult to derive sep-
arate senses from these top terms. In contrast, senses
learned from the model with weighted similarities
are more distinct. Sense 1 relates to mental repre-
sentation; sense 2 is about visual representation pro-
duced on a surface; and sense 3 is about the general
impression that something presents to the public.
</bodyText>
<sectionHeader confidence="0.998007" genericHeader="evaluation">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999987709677419">
We presented a novel sense-topic model for the
problem of word sense induction. We considered
sense and topic as distinct latent variables, defining
a model that generates global context words using
topic variables and local context words using both
topic and sense variables. Sense and topic are re-
lated using a bidirectional dependency with a robust
parameterization based on deficient modeling.
We explored ways of enriching data using word
embeddings from neural language models and exter-
nal corpora. We found enriching context to be most
effective, even when the original context of the in-
stance is not available. Evaluating on the SemEval-
2013 WSI dataset, we demonstrate that our model
yields significant improvements over current state-
of-the-art systems, giving 59.1% fuzzy B-cubed and
9.39% fuzzy NMI in our best setting. Moreover, we
find that modeling both sense and topic is critical
to enable us to effectively exploit broader context,
showing that LDA does not improve when each in-
stance is enriched by actual context.
In future work, we plan to further explore the
space of sense-topic models, including non-deficient
models. One possibility is to use “switching vari-
ables” (Paul and Girju, 2009) to choose whether to
generate each word from a topic or sense, with a
stronger preference to generate from senses closer to
the target word. Another possibility is to use locally-
normalized log-linear distributions and include fea-
tures pairing words with particular senses and topics,
rather than redundant generative steps.
</bodyText>
<sectionHeader confidence="0.98057" genericHeader="conclusions">
Appendix A
</sectionHeader>
<bodyText confidence="0.9890655">
The plate diagram for the complete sense-topic
model is shown in Figure 2.
</bodyText>
<figureCaption confidence="0.9284895">
Figure 2: Plate notation for the proposed sense-topic
model with all variables (except α, the fixed Dirichlet
hyperparameter used as prior for all multinomial distri-
butions). Each instance has topic mixing proportions θt
and sense mixing proportions θs. The instance set shares
sense/topic parameter θst, topic-sense distribution θs|t,
sense-topic distribution θt|s, topic-word distribution ψt,
and sense-word distribution ψs.
</figureCaption>
<sectionHeader confidence="0.99849" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999779166666667">
We thank the editor and the anonymous reviewers
for their helpful comments. This research was par-
tially supported by NIH LM010817. The opinions
expressed in this work are those of the authors and
do not necessarily reflect the views of the funding
agency.
</bodyText>
<page confidence="0.999072">
69
</page>
<sectionHeader confidence="0.996399" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999955644230769">
E. Agirre and A. Soroa. 2007. SemEval-2007 Task 02:
Evaluating word sense induction and discrimination
systems. In Proc. of SemEval, pages 7–12.
C. Akkaya, J. Wiebe, and R. Mihalcea. 2012. Utilizing
semantic composition in distributional semantic mod-
els for word sense discrimination and word sense dis-
ambiguation. In Proc. of ICSC, pages 45–51.
M. Bansal, K. Gimpel, and K. Livescu. 2014. Tailoring
continuous word representations for dependency pars-
ing. In Proc. of ACL, pages 809–815.
O. Baskaya, E. Sert, V. Cirik, and D. Yuret. 2013. AI-
KU: Using substitute vectors and co-occurrence mod-
eling for word sense induction and disambiguation. In
Proc. of SemEval, pages 300–306.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. J.
Mach. Learn. Res., 3:1137–1155.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. La-
tent Dirichlet allocation. J. Mach. Learn. Res., 3:993–
1022.
J. Boyd-Graber and D. M. Blei. 2007. PUTOP: Turning
predominant senses into a topic model for word sense
disambiguation. In Proc. of SemEval, pages 277–281.
J. Boyd-Graber, D. M. Blei, and X. Zhu. 2007. A topic
model for word sense disambiguation. In Proc. of
EMNLP-CoNLL, pages 1024–1033.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. In Proc. of EACL, pages 103–111.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
J. F. Cai, W. S. Lee, and Y. W. Teh. 2007. Improving
word sense disambiguation using topic features. In
Proc. of EMNLP-CoNLL, pages 1015–1023.
M. Carpuat and D. Wu. 2005. Word sense disambigua-
tion vs. statistical machine translation. In Proc. of
ACL, pages 387–394.
M. Carpuat and D. Wu. 2007. Improving statistical ma-
chine translation using word sense disambiguation. In
Proc. of EMNLP-CoNLL, pages 61–72.
N. Chambers and D. Jurafsky. 2011. Template-based
information extraction without the templates. In Proc.
of ACL, pages 976–986.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. J. Mach.
Learn. Res., 12:2493–2537.
P. Dhillon, J. Rodu, D. Foster, and L. Ungar. 2012. Two
Step CCA: A new spectral method for estimating vec-
tor models of words. In ICML, pages 1551–1558.
B. Dorow and D. Widdows. 2003. Discovering corpus-
specific word senses. In Proc. of EACL, pages 79–82.
B. Efron and R. J. Tibshirani. 1994. An introduction to
the bootstrap, volume 57. CRC press.
K. Erk and D. McCarthy. 2009. Graded word sense as-
signment. In Proc. of EMNLP, pages 440–449.
K. Erk, D. McCarthy, and N. Gaylord. 2009. Investi-
gations on word senses and word usages. In Proc. of
ACL, pages 10–18.
S. Geman and D. Geman. 1984. Stochastic relax-
ation, Gibbs distributions, and the Bayesian restoration
of images. IEEE Trans. Pattern Anal. Mach. Intell.,
6(6):721–741.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proc. of the National Academy of Sciences
of the United States of America, 101(Suppl 1):5228–
5235.
D. Heckerman, D. M. Chickering, C. Meek, R. Roun-
thwaite, and C. Kadie. 2001. Dependency networks
for inference, collaborative filtering, and data visual-
ization. J. Mach. Learn. Res., 1:49–75.
G. Heinrich. 2005. Parameter estimation for text analy-
sis. Technical report.
S. Hisamoto, K. Duh, and Y. Matsumoto. 2013. An em-
pirical investigation of word representations for pars-
ing the web. In ANLP.
N. Ide and K. Suderman. 2004. The American National
Corpus first release. In Proc. of LREC, pages 1681–
1684.
D. Jurgens and I. Klapaftis. 2013. SemEval-2013 Task
13: Word sense induction for graded and non-graded
senses. In Proc. of SemEval, pages 290–299.
D. Jurgens. 2013. Embracing ambiguity: A comparison
of annotation methodologies for crowdsourcing word
sense labels. In Proc. of NAACL, pages 556–562.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proc. of ACL, pages 128–135.
J. H. Lau, P. Cook, D. McCarthy, D. Newman, and
T. Baldwin. 2012. Word sense induction for novel
sense detection. In Proc. of EACL, pages 591–601.
J. H. Lau, P. Cook, and T. Baldwin. 2013. unimelb:
Topic modelling-based word sense induction. In Proc.
of SemEval, pages 307–311.
L. Li, B. Roth, and C. Sporleder. 2010. Topic models
for word sense disambiguation and token-based idiom
detection. In Proc. of ACL, pages 1138–1147.
Y. Maron, E. Bienenstock, and M. James. 2010. Sphere
embedding: An application to part-of-speech induc-
tion. In Advances in NIPS 23.
J. May and K. Knight. 2007. Syntactic re-alignment
models for machine translation. In Proc. of EMNLP-
CoNLL, pages 360–368.
</reference>
<page confidence="0.966871">
70
</page>
<reference confidence="0.999455333333333">
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013.
Efficient estimation of word representations in vector
space. In Proc. of ICLR.
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. J. Miller. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography, 3(4).
A. Mnih and G. Hinton. 2007. Three new graphical
models for statistical language modelling. In Proc. of
ICML, pages 641–648.
R. C. Moore and W. Lewis. 2010. Intelligent selection of
language model training data. In Proc. of ACL, pages
220–224.
N. R. Pal and J. C. Bezdek. 1995. On cluster validity for
the fuzzy c-means model. Trans. Fuz Sys., 3:370–379.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD, pages 613–619.
R. J. Passonneau, A. Salleb-Aoussi, V. Bhardwaj, and
N. Ide. 2010. Word sense annotation of polysemous
words by multiple annotators. In Proc. of LREC.
M. Paul and R. Girju. 2009. Cross-cultural analysis of
blogs and forums with mixed-collection topic models.
In Proc. of EMNLP, pages 1408–1417.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Proc. of CoNLL, pages 41–48.
M. Sahlgren. 2006. The word-space model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. dissertation, Stock-
holm University.
H. Sch¨utze. 1998. Automatic word sense discrimination.
Comput. Linguist., 24(1):97–123.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566–1581.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Advances in NIPS 20.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In Proc. of ACL, pages 384–394.
J. V´eronis. 2004. Hyperlex: lexical cartography for in-
formation retrieval. Computer Speech &amp; Language,
18(3):223–252.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-sense disambiguation for machine translation.
In Proc. of HLT-EMNLP, pages 771–778.
E. M. Voorhees. 1993. Using WordNet to disambiguate
word senses for text retrieval. In Proc. of SIGIR, pages
171–180.
X. Yao and B. Van Durme. 2011. Nonparamet-
ric Bayesian word sense induction. In Proc. of
TextGraphs-6: Graph-based Methods for Natural Lan-
guage Processing, pages 10–14.
</reference>
<page confidence="0.9994">
71
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.560675">
<title confidence="0.9953935">A Sense-Topic Model for Word Sense with Unsupervised Data Enrichment</title>
<author confidence="0.960709">Mohit D Clement T</author>
<address confidence="0.768426">of Illinois at Chicago, Chicago, IL, 60607, Technological Institute at Chicago, Chicago, IL, 60637,</address>
<abstract confidence="0.9947265">Word sense induction (WSI) seeks to automatically discover the senses of a word in a corpus via unsupervised methods. We propose a sense-topic model for WSI, which treats sense and topic as two separate latent variables to be inferred jointly. Topics are informed by the entire document, while senses are informed by the local context surrounding the ambiguous word. We also discuss unsupervised ways of enriching the original corpus in order to improve model performance, including using neural word embeddings and external corpora to expand the context of each data instance. We demonstrate significant improvements over the previous state-of-the-art, achieving the best results reported to date on the SemEval-2013 WSI task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>SemEval-2007 Task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proc. of SemEval,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="10166" citStr="Agirre and Soroa, 2007" startWordPosition="1628" endWordPosition="1631">the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provides context for a single occurrence of the target word.1 For our experiments, we use the 1The target word token may occur multiple times in an instance, but only one occurrence is chosen as the target word occurrence. Figure 1: Proposed sense-topic model in plate notation. There are MD instances for the given target word. In an instance, there are N9 global context words (w9) and Nj local context words (wj), all of </context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>E. Agirre and A. Soroa. 2007. SemEval-2007 Task 02: Evaluating word sense induction and discrimination systems. In Proc. of SemEval, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Akkaya</author>
<author>J Wiebe</author>
<author>R Mihalcea</author>
</authors>
<title>Utilizing semantic composition in distributional semantic models for word sense discrimination and word sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proc. of ICSC,</booktitle>
<pages>45--51</pages>
<contexts>
<context position="8257" citStr="Akkaya et al. (2012)" startWordPosition="1326" endWordPosition="1329">m jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training criteria based on predicting nearby words. They have been useful as features in many NLP tasks (Turian e</context>
</contexts>
<marker>Akkaya, Wiebe, Mihalcea, 2012</marker>
<rawString>C. Akkaya, J. Wiebe, and R. Mihalcea. 2012. Utilizing semantic composition in distributional semantic models for word sense discrimination and word sense disambiguation. In Proc. of ICSC, pages 45–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bansal</author>
<author>K Gimpel</author>
<author>K Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>809--815</pages>
<contexts>
<context position="8959" citStr="Bansal et al., 2014" startWordPosition="1438" endWordPosition="1441">nted by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training criteria based on predicting nearby words. They have been useful as features in many NLP tasks (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of e</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>M. Bansal, K. Gimpel, and K. Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proc. of ACL, pages 809–815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Baskaya</author>
<author>E Sert</author>
<author>V Cirik</author>
<author>D Yuret</author>
</authors>
<title>AIKU: Using substitute vectors and co-occurrence modeling for word sense induction and disambiguation.</title>
<date>2013</date>
<booktitle>In Proc. of SemEval,</booktitle>
<pages>300--306</pages>
<contexts>
<context position="9532" citStr="Baskaya et al. (2013)" startWordPosition="1528" endWordPosition="1531">012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´ero</context>
</contexts>
<marker>Baskaya, Sert, Cirik, Yuret, 2013</marker>
<rawString>O. Baskaya, E. Sert, V. Cirik, and D. Yuret. 2013. AIKU: Using substitute vectors and co-occurrence modeling for word sense induction and disambiguation. In Proc. of SemEval, pages 300–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1137</pages>
<contexts>
<context position="8130" citStr="Bengio et al., 2003" startWordPosition="1307" endWordPosition="1310">e-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture in</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>3</volume>
<pages>1022</pages>
<contexts>
<context position="2227" citStr="Blei et al., 2003" startWordPosition="345" endWordPosition="348">se, and therefore, that each instance belongs to exactly one sense cluster. However, recent work (Erk and McCarthy, 2009; Jurgens, 2013) has shown that more than one sense can be used to interpret certain instances, due to context ambiguity and sense relatedness. To handle these characteristics of WSI (unsupervised, senses represented by token clusters, multiple senses per instance), we consider approaches based on topic models. A topic model is an unsupervised method that discovers the semantic topics underlying a collection of documents. The most popular is latent Dirichlet allocation (LDA; Blei et al., 2003), in which each topic is represented as a multinomial distribution over words, and each document is represented as a multinomial distribution over topics. One approach would be to run LDA on the instances for an ambiguous word, then simply interpret topics as induced senses (Brody and Lapata, 2009). However, while sense and topic are related, they are distinct linguistic phenomena. Topics are assigned to entire documents and are expressed by all word tokens, while senses relate to a single ambiguous word and are expressed through the local context of that word. One possible approach would be t</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:993– 1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D M Blei</author>
</authors>
<title>PUTOP: Turning predominant senses into a topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of SemEval,</booktitle>
<pages>277--281</pages>
<contexts>
<context position="7830" citStr="Boyd-Graber and Blei, 2007" startWordPosition="1259" endWordPosition="1262">2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific contex</context>
</contexts>
<marker>Boyd-Graber, Blei, 2007</marker>
<rawString>J. Boyd-Graber and D. M. Blei. 2007. PUTOP: Turning predominant senses into a topic model for word sense disambiguation. In Proc. of SemEval, pages 277–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D M Blei</author>
<author>X Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>1024--1033</pages>
<contexts>
<context position="7856" citStr="Boyd-Graber et al., 2007" startWordPosition="1263" endWordPosition="1266">ge that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through elem</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>J. Boyd-Graber, D. M. Blei, and X. Zhu. 2007. A topic model for word sense disambiguation. In Proc. of EMNLP-CoNLL, pages 1024–1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>M Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>103--111</pages>
<contexts>
<context position="2526" citStr="Brody and Lapata, 2009" startWordPosition="396" endWordPosition="399">s of WSI (unsupervised, senses represented by token clusters, multiple senses per instance), we consider approaches based on topic models. A topic model is an unsupervised method that discovers the semantic topics underlying a collection of documents. The most popular is latent Dirichlet allocation (LDA; Blei et al., 2003), in which each topic is represented as a multinomial distribution over words, and each document is represented as a multinomial distribution over topics. One approach would be to run LDA on the instances for an ambiguous word, then simply interpret topics as induced senses (Brody and Lapata, 2009). However, while sense and topic are related, they are distinct linguistic phenomena. Topics are assigned to entire documents and are expressed by all word tokens, while senses relate to a single ambiguous word and are expressed through the local context of that word. One possible approach would be to only keep the local context of each ambiguous word, discarding the global context. However, the topical information contained in the broader context, though it may not determine the sense directly, might still be useful for narrowing down the likely senses of the ambiguous word. Consider the ambi</context>
<context position="6810" citStr="Brody and Lapata (2009)" startWordPosition="1083" endWordPosition="1086"> word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submi</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>S. Brody and M. Lapata. 2009. Bayesian word sense induction. In Proc. of EACL, pages 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="14897" citStr="Brown et al., 1993" startWordPosition="2437" endWordPosition="2440">ollowing equations. the model. We later include an empirical comparison to justify some of our modeling choices (§5). First, when relating the sense and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = Pr(s` = k|d,t` = j)Pr(t` = j|d,s` = k) (3) where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d,t` = j) = Pθs(s` =k|d)Pθs|tj(s` =k|t` =j)Pθst(t` =j, s` =k) Zd,tj Pθt(t` =j|d)Pθt|sk(t` =j|s` =k) Zd,sk where Zd,tj and Zd,sk are normalization factors and we have introduced new multinomial parameters θs, θs|tj, θst, and θt|sk. We use the same idea to factor the word generation distribution: Pψtj(w`|t`=j)Pψsk(w`|s`=k) </context>
<context position="18628" citStr="Brown et al., 1993" startWordPosition="3087" endWordPosition="3090">out before sampling. Then we estimate the sense distribution θs for each instance using maximum likelihood estimation on the samples. These sense distributions are the output of our WSI system. We note that deficient modeling does not ordinarily affect Gibbs sampling when used for computing posteriors over latent variables, as long as parameters (the θ and ψ) are kept fixed. This is the case during the E step of an EM algorithm, which is the usual setting in which deficiency is used. Only the M step is affected; it becomes an approximate M step by assuming the normalization constants equal 1 (Brown et al., 1993). However, here we use collapsed Gibbs sampling for posterior inference, and the analytic integration is disrupted by the presence of the normalization constants. To bypass this, we employ the standard approximation of deficient models that all normalization constants are 1, permitting us to use standard formulas for analytic integration of multinomial parameters with Dirichlet priors. Empirically, we found this “collapsed deficient Gibbs sampler” to slightly outperform a more principled approach based on EM, presumably due to the ability of collapsing to accelerate mixing. During the sampling</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Cai</author>
<author>W S Lee</author>
<author>Y W Teh</author>
</authors>
<title>Improving word sense disambiguation using topic features.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>1015--1023</pages>
<contexts>
<context position="7802" citStr="Cai et al., 2007" startWordPosition="1255" endWordPosition="1258">(HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguo</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>J. F. Cai, W. S. Lee, and Y. W. Teh. 2007. Improving word sense disambiguation using topic features. In Proc. of EMNLP-CoNLL, pages 1015–1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Word sense disambiguation vs. statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>387--394</pages>
<contexts>
<context position="6645" citStr="Carpuat and Wu, 2005" startWordPosition="1056" endWordPosition="1059">ive, expensive, and subject to poor inter-annotator agreement (Passonneau et al., 2010). Sense inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it ca</context>
</contexts>
<marker>Carpuat, Wu, 2005</marker>
<rawString>M. Carpuat and D. Wu. 2005. Word sense disambiguation vs. statistical machine translation. In Proc. of ACL, pages 387–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="6761" citStr="Carpuat and Wu, 2007" startWordPosition="1075" endWordPosition="1078">se a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional w</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proc. of EMNLP-CoNLL, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chambers</author>
<author>D Jurafsky</author>
</authors>
<title>Template-based information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>976--986</pages>
<contexts>
<context position="37751" citStr="Chambers and Jurafsky, 2011" startWordPosition="6347" endWordPosition="6350">h additional instances from an external corpus. We have no gold standard senses for these instances, so we will not evaluate our model on them; they are merely used to provide richer co-occurrence statistics about the target word so that we can perform better on the instances on which we evaluate. If we added randomly-chosen instances (containing the target word), we would be concerned that the learned topics and senses may not reflect the distributions of the original instance set. So we only add instances that are semantically similar to instances in our original set (Moore and Lewis, 2010; Chambers and Jurafsky, 2011). Also, to avoid changing the original sense distribution by adding too many instances, we only add a single instance for each original instance. As in §6.1, for each instance in the original dataset, we find the most similar sentence in ukWac for each instance using word embeddings and add it into the dataset. Therefore, the number of instances is doubled, and we use the enriched dataset for our sense-topic model. Results Similarly to §6.1, on TRIAL, we find best performance for S = 10, so we run on TEST with this value. As shown in Table 2 (row 12), this improves fuzzy B-cubed by 5.4%, but f</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>N. Chambers and D. Jurafsky. 2011. Template-based information extraction without the templates. In Proc. of ACL, pages 976–986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2493</pages>
<contexts>
<context position="8892" citStr="Collobert et al., 2011" startWordPosition="1426" endWordPosition="1429">resentations learned from DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training criteria based on predicting nearby words. They have been useful as features in many NLP tasks (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment), and in a product-of-em</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dhillon</author>
<author>J Rodu</author>
<author>D Foster</author>
<author>L Ungar</author>
</authors>
<title>Two Step CCA: A new spectral method for estimating vector models of words.</title>
<date>2012</date>
<booktitle>In ICML,</booktitle>
<pages>1551--1558</pages>
<contexts>
<context position="8914" citStr="Dhillon et al., 2012" startWordPosition="1430" endWordPosition="1433">m DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training criteria based on predicting nearby words. They have been useful as features in many NLP tasks (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment), and in a product-of-embeddings baseline. Bas</context>
</contexts>
<marker>Dhillon, Rodu, Foster, Ungar, 2012</marker>
<rawString>P. Dhillon, J. Rodu, D. Foster, and L. Ungar. 2012. Two Step CCA: A new spectral method for estimating vector models of words. In ICML, pages 1551–1558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorow</author>
<author>D Widdows</author>
</authors>
<title>Discovering corpusspecific word senses.</title>
<date>2003</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>79--82</pages>
<contexts>
<context position="10125" citStr="Dorow and Widdows, 2003" startWordPosition="1622" endWordPosition="1625">aseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provides context for a single occurrence of the target word.1 For our experiments, we use the 1The target word token may occur multiple times in an instance, but only one occurrence is chosen as the target word occurrence. Figure 1: Proposed sense-topic model in plate notation. There are MD instances for the given target word. In an instance, there are N9 global context words (w9)</context>
</contexts>
<marker>Dorow, Widdows, 2003</marker>
<rawString>B. Dorow and D. Widdows. 2003. Discovering corpusspecific word senses. In Proc. of EACL, pages 79–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R J Tibshirani</author>
</authors>
<title>An introduction to the bootstrap, volume 57.</title>
<date>1994</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>440--449</pages>
<publisher>CRC</publisher>
<contexts>
<context position="32562" citStr="Efron and Tibshirani, 1994" startWordPosition="5500" endWordPosition="5503"> ukWac sentence (s*) with the highest cosine similarity to each original instance (d) is appended to that instance: s* = arg max,∈ukWac sim(d, s) Results Since the vocabulary has increased, we expect we may need larger values for S and T. On TRIAL, we find best performance for S = 10, so we run on TEST with this value. Performance is shown in Table 2 (rows 10 and 11). These two methods have higher AVG scores than all others. Both their fuzzy B-cubed and NMI improvements over the baselines and previous WSI systems are statistically significant, as measured by a paired bootstrap test (p &lt; 0.01; Efron and Tibshirani, 1994). It is unsurprising that we find best performance with actual context. Interestingly, however, we can achieve almost the same gains when automatically finding relevant context from a different corpus. Thus, even in real-world settings where we only have a single sentence of context, we can induce substantially better senses by automatically broadening the global context in an unsupervised manner. As a comparative experiment, we also evaluate the performance of LDA when adding actual context (Table 2, row 7). Compared with LDA with full context (FULL) in row 6, performance is slightly improved</context>
</contexts>
<marker>Efron, Tibshirani, 1994</marker>
<rawString>B. Efron and R. J. Tibshirani. 1994. An introduction to the bootstrap, volume 57. CRC press. K. Erk and D. McCarthy. 2009. Graded word sense assignment. In Proc. of EMNLP, pages 440–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>D McCarthy</author>
<author>N Gaylord</author>
</authors>
<title>Investigations on word senses and word usages.</title>
<date>2009</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="23484" citStr="Erk et al., 2009" startWordPosition="3955" endWordPosition="3958">uzzy normalized mutual information (NMI). They are each computed separately for each target word, then averaged across target words. Fuzzy B-cubed prefers labeling all instances with the same sense, while fuzzy NMI prefers the opposite extreme of labeling all instances with distinct senses. Hence, we report both fuzzy B-cubed (%) and fuzzy NMI (%) in our evaluation. For ease of comparison, we also report the geometric mean of the 2 metrics, which we denote by AVG.5 SemEval-2013 Task 13 also provided a trial dataset (TRIAL) that consists of eight target ambiguous words, each with 50 instances (Erk et al., 2009). We use it for preliminary experiments of our model and for tuning certain hyperparameters, and evaluate final performance on the SemEval-2013 dataset (TEST) with 50 target words. 5We do not use an arithmetic mean because the effective range of the two metrics is substantially different. |{z } Pr(t=j|d,t−i,8,·) CDS dk + α PS k0=1 CDS dk0 + Sα |} {z Pr(s=k|d,s−i,·) CST kj + α (5) PT k0=1 CST kk0 + Tα | {z } Pr(t=j|s=k,t−i 64 S B-cubed(%) NMI(%) AVG 2 42.9 4.18 13.39 3 31.9 6.50 14.40 5 22.3 8.60 13.85 7 15.4 8.72 11.61 10 12.5 10.91 11.67 Table 1: Performance on TRIAL for the sense-topic model</context>
</contexts>
<marker>Erk, McCarthy, Gaylord, 2009</marker>
<rawString>K. Erk, D. McCarthy, and N. Gaylord. 2009. Investigations on word senses and word usages. In Proc. of ACL, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="17879" citStr="Geman and Geman, 1984" startWordPosition="2961" endWordPosition="2965"> 1: Choose topic proportions θt Dir(α) 2: Choose sense proportions θs Dir(α) 3: Choose Ng and N` from unspecified distributions 4: fori+- 1 to Ng do 5: Choose a topic j — Mult(θt) 6: Choose a word wg — Mult(ψtj) 7: fori+- 1 to N` do 8: repeat 9: Choose a topic j Mult(θt) 10: Choose a sense k Mult(θs) 11: Choose a topic j0 Mult(θt|sk) 12: Choose a sense k0 Mult(θs|tj) 13: Choose topic/sense (j00, k00) — Mult(θst) 14: until j = j0 = j00 and k = k0 = k00 15: repeat 16: Choose a word w` — Mult(ψtj) 17: Choose a word w0` — Mult(ψsk) 18: until w` = w0` 4.2 Inference We use collapsed Gibbs sampling (Geman and Geman, 1984) to obtain samples from the posterior distribution over latent variables, with all multinomial parameters analytically integrated out before sampling. Then we estimate the sense distribution θs for each instance using maximum likelihood estimation on the samples. These sense distributions are the output of our WSI system. We note that deficient modeling does not ordinarily affect Gibbs sampling when used for computing posteriors over latent variables, as long as parameters (the θ and ψ) are kept fixed. This is the case during the E step of an EM algorithm, which is the usual setting in which d</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Trans. Pattern Anal. Mach. Intell., 6(6):721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proc. of the National Academy of Sciences of the United States of America, 101(Suppl</booktitle>
<volume>1</volume>
<pages>5235</pages>
<contexts>
<context position="25250" citStr="Griffiths and Steyvers, 2004" startWordPosition="4269" endWordPosition="4272">ext or more instances (see §6), tuning on TRIAL chooses a larger S value. During inference, the Gibbs sampler was run for 4,000 iterations for each target word, setting the first 500 iterations as the burn-in period. In order to get a representative set of samples, every 13th sample (after burn-in) is saved to prevent correlations among samples. Due to the randomized nature of the inference procedure, all reported results are average scores over 5 runs. The hyperparameters (α) for all Dirichlet priors in our model are set to the (untuned) value of 0.01, following prior work on topic modeling (Griffiths and Steyvers, 2004; Heinrich, 2005). Baselines We include two naive baselines corresponding to the two extremes (biases) preferred by fuzzy B-cubed and NMI, respectively: 1 sense (label each instance with the same single sense) and all distinct (label each instance with its own sense). We also consider two baselines based on LDA. We run LDA for each target word in TEST, using the set of instances as the set of documents. We treat the learned topics as induced senses. When setting the number of topics (senses), we use the gold-standard number of senses for each target word, making this baseline unreasonably stro</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proc. of the National Academy of Sciences of the United States of America, 101(Suppl 1):5228– 5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Heckerman</author>
<author>D M Chickering</author>
<author>C Meek</author>
<author>R Rounthwaite</author>
<author>C Kadie</author>
</authors>
<title>Dependency networks for inference, collaborative filtering, and data visualization.</title>
<date>2001</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>1--49</pages>
<contexts>
<context position="4508" citStr="Heckerman et al., 2001" startWordPosition="717" endWordPosition="720"> sense can also help determine possible topics. Consider a set of texts that all include the word cold. Without further information, the texts might discuss any of a number of possible topics. However, if the sense of cold is that of cold ischemia, then the most probable topics would be those related to organ transplantation. In this paper, we propose a sense-topic model for WSI, which treats sense and topic as two separate latent variables to be inferred jointly (§4). When relating the sense and topic variables, a bidirectional edge is drawn between them to represent their cyclic dependence (Heckerman et al., 2001). We perform inference using collapsed Gibbs sampling (§4.2), then estimate the sense distribution for each instance as the solution to the WSI task. We conduct experiments on the SemEval-2013 Task 13 WSI dataset, showing improvements over several strong baselines and task systems (§5). We also present unsupervised ways of enriching our dataset, including using neural word embeddings (Mikolov et al., 2013) and external Web-scale corpora to enrich the context of each data instance or to add more instances (§6). Each data enrichment method gives further gains, resulting in significant improvemen</context>
<context position="14579" citStr="Heckerman et al., 2001" startWordPosition="2382" endWordPosition="2385">tions from small datasets. A secondary benefit is that we can avoid biases caused by particular choices of generative directionality in 3We use Pr() for generic probability distributions without further qualifiers and PB() for distributions parameterized by θ. 4For clarity, we drop the (i) superscripts in these and the following equations. the model. We later include an empirical comparison to justify some of our modeling choices (§5). First, when relating the sense and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = Pr(s` = k|d,t` = j)Pr(t` = j|d,s` = k) (3) where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d,t` </context>
</contexts>
<marker>Heckerman, Chickering, Meek, Rounthwaite, Kadie, 2001</marker>
<rawString>D. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. 2001. Dependency networks for inference, collaborative filtering, and data visualization. J. Mach. Learn. Res., 1:49–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Heinrich</author>
</authors>
<title>Parameter estimation for text analysis.</title>
<date>2005</date>
<tech>Technical report.</tech>
<contexts>
<context position="25267" citStr="Heinrich, 2005" startWordPosition="4273" endWordPosition="4274">, tuning on TRIAL chooses a larger S value. During inference, the Gibbs sampler was run for 4,000 iterations for each target word, setting the first 500 iterations as the burn-in period. In order to get a representative set of samples, every 13th sample (after burn-in) is saved to prevent correlations among samples. Due to the randomized nature of the inference procedure, all reported results are average scores over 5 runs. The hyperparameters (α) for all Dirichlet priors in our model are set to the (untuned) value of 0.01, following prior work on topic modeling (Griffiths and Steyvers, 2004; Heinrich, 2005). Baselines We include two naive baselines corresponding to the two extremes (biases) preferred by fuzzy B-cubed and NMI, respectively: 1 sense (label each instance with the same single sense) and all distinct (label each instance with its own sense). We also consider two baselines based on LDA. We run LDA for each target word in TEST, using the set of instances as the set of documents. We treat the learned topics as induced senses. When setting the number of topics (senses), we use the gold-standard number of senses for each target word, making this baseline unreasonably strong. We run LDA bo</context>
</contexts>
<marker>Heinrich, 2005</marker>
<rawString>G. Heinrich. 2005. Parameter estimation for text analysis. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hisamoto</author>
<author>K Duh</author>
<author>Y Matsumoto</author>
</authors>
<title>An empirical investigation of word representations for parsing the web. In ANLP.</title>
<date>2013</date>
<contexts>
<context position="8937" citStr="Hisamoto et al., 2013" startWordPosition="1434" endWordPosition="1437">I. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training criteria based on predicting nearby words. They have been useful as features in many NLP tasks (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) repr</context>
</contexts>
<marker>Hisamoto, Duh, Matsumoto, 2013</marker>
<rawString>S. Hisamoto, K. Duh, and Y. Matsumoto. 2013. An empirical investigation of word representations for parsing the web. In ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>K Suderman</author>
</authors>
<title>The American National Corpus first release.</title>
<date>2004</date>
<booktitle>In Proc. of LREC,</booktitle>
<pages>1681--1684</pages>
<contexts>
<context position="11375" citStr="Ide and Suderman, 2004" startWordPosition="1840" endWordPosition="1844"> (wj), all of which are observed. There is one latent variable (“topic” t9) for the w9 and two latent variables (“topic” tj and “sense” sj) for the wj. Each instance has topic mixing proportions θt and sense mixing proportions θ3. For clarity, not all variables are shown. The complete figure with all variables is given in Appendix A. This is a dependency network, not a directed graphical model, as shown by the directed arrows between tj and sj; see text for details. dataset released for SemEval-2013 Task 13 (Jurgens and Klapaftis, 2013), collected from the Open American National Corpus (OANC; Ide and Suderman, 2004).2 It includes 50 target words: 20 verbs, 20 nouns, and 10 adjectives. There are a total of 4,664 instances across all target words. Each instance contains only one sentence, with a minimum length of 22 and a maximum length of 100. The gold standard for the dataset was prepared by multiple annotators, where each annotator labeled instances based on the sense inventories in WordNet 3.1. For each instance, they rated all senses of a target word on a Likert scale from one to five. 4 A Sense-Topic Model for WSI We now present our sense-topic model, shown in plate notation in Figure 1. It generates</context>
</contexts>
<marker>Ide, Suderman, 2004</marker>
<rawString>N. Ide and K. Suderman. 2004. The American National Corpus first release. In Proc. of LREC, pages 1681– 1684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurgens</author>
<author>I Klapaftis</author>
</authors>
<title>SemEval-2013 Task 13: Word sense induction for graded and non-graded senses.</title>
<date>2013</date>
<booktitle>In Proc. of SemEval,</booktitle>
<pages>290--299</pages>
<contexts>
<context position="5326" citStr="Jurgens and Klapaftis, 2013" startWordPosition="844" endWordPosition="847">13 Task 13 WSI dataset, showing improvements over several strong baselines and task systems (§5). We also present unsupervised ways of enriching our dataset, including using neural word embeddings (Mikolov et al., 2013) and external Web-scale corpora to enrich the context of each data instance or to add more instances (§6). Each data enrichment method gives further gains, resulting in significant improvements over existing state-of-the-art WSI systems. Overall, we find gains of up to 22% relative improvement in fuzzy B-cubed and 50% relative improvement in fuzzy normalized mutual information (Jurgens and Klapaftis, 2013). 2 Background and Related Work We discuss the WSI task, then discuss several areas of research that are related to our approach, including applications of topic modeling to WSI as well as other approaches that use word embeddings and clustering algorithms. WSD and WSI: WSI is related to but distinct from word sense disambiguation (WSD). WSD seeks to assign a particular sense label to each target word instance, where the sense labels are known and usually drawn from an existing sense inventory like WordNet (Miller et al., 1990). Although extensive research has been devoted to WSD, WSI may be m</context>
<context position="7501" citStr="Jurgens and Klapaftis, 2013" startWordPosition="1202" endWordPosition="1205">arate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language model</context>
<context position="11294" citStr="Jurgens and Klapaftis, 2013" startWordPosition="1827" endWordPosition="1831">ord. In an instance, there are N9 global context words (w9) and Nj local context words (wj), all of which are observed. There is one latent variable (“topic” t9) for the w9 and two latent variables (“topic” tj and “sense” sj) for the wj. Each instance has topic mixing proportions θt and sense mixing proportions θ3. For clarity, not all variables are shown. The complete figure with all variables is given in Appendix A. This is a dependency network, not a directed graphical model, as shown by the directed arrows between tj and sj; see text for details. dataset released for SemEval-2013 Task 13 (Jurgens and Klapaftis, 2013), collected from the Open American National Corpus (OANC; Ide and Suderman, 2004).2 It includes 50 target words: 20 verbs, 20 nouns, and 10 adjectives. There are a total of 4,664 instances across all target words. Each instance contains only one sentence, with a minimum length of 22 and a maximum length of 100. The gold standard for the dataset was prepared by multiple annotators, where each annotator labeled instances based on the sense inventories in WordNet 3.1. For each instance, they rated all senses of a target word on a Likert scale from one to five. 4 A Sense-Topic Model for WSI We now</context>
<context position="22826" citStr="Jurgens and Klapaftis (2013)" startWordPosition="3845" endWordPosition="3849">(i) ` |s=k,s−i,·) CST kj + α PSk0=1 CST k0j + Sα |{z } Pr(s=k|t=j,t−i,s−i,·) CST kj + α PS PT j0=1 CST k0j0 + STα k0=1 |{z } Pr(s=k,t=j|t−i,s−i,·) where CDS dk contains the number of times sense k is assigned to some local word token in instance d, excluding the current word token; CW S ik contains the number of time word w(i) ` is assigned to sense k, excluding the current time; CST kj contains the number of times sense k and topic j are assigned to some local word tokens. Ws is the number of distinct local context word types across the collection. Evaluation Metrics To evaluate WSI systems, Jurgens and Klapaftis (2013) propose two metrics: fuzzy B-cubed and fuzzy normalized mutual information (NMI). They are each computed separately for each target word, then averaged across target words. Fuzzy B-cubed prefers labeling all instances with the same sense, while fuzzy NMI prefers the opposite extreme of labeling all instances with distinct senses. Hence, we report both fuzzy B-cubed (%) and fuzzy NMI (%) in our evaluation. For ease of comparison, we also report the geometric mean of the 2 metrics, which we denote by AVG.5 SemEval-2013 Task 13 also provided a trial dataset (TRIAL) that consists of eight target </context>
</contexts>
<marker>Jurgens, Klapaftis, 2013</marker>
<rawString>D. Jurgens and I. Klapaftis. 2013. SemEval-2013 Task 13: Word sense induction for graded and non-graded senses. In Proc. of SemEval, pages 290–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurgens</author>
</authors>
<title>Embracing ambiguity: A comparison of annotation methodologies for crowdsourcing word sense labels.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="1745" citStr="Jurgens, 2013" startWordPosition="271" endWordPosition="272">automatically discovering all senses of an ambiguous word in a corpus. The inputs to WSI are instances of the ambiguous word with its surrounding context. The output is a grouping of these instances into clusters corresponding to the induced senses. WSI is generally conducted as an unsupervised learning task, relying on the assumption that the surrounding context of a word indicates its meaning. Most previous work assumed that each instance is best labeled with a single sense, and therefore, that each instance belongs to exactly one sense cluster. However, recent work (Erk and McCarthy, 2009; Jurgens, 2013) has shown that more than one sense can be used to interpret certain instances, due to context ambiguity and sense relatedness. To handle these characteristics of WSI (unsupervised, senses represented by token clusters, multiple senses per instance), we consider approaches based on topic models. A topic model is an unsupervised method that discovers the semantic topics underlying a collection of documents. The most popular is latent Dirichlet allocation (LDA; Blei et al., 2003), in which each topic is represented as a multinomial distribution over words, and each document is represented as a m</context>
</contexts>
<marker>Jurgens, 2013</marker>
<rawString>D. Jurgens. 2013. Embracing ambiguity: A comparison of annotation methodologies for crowdsourcing word sense labels. In Proc. of NAACL, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="15000" citStr="Klein and Manning, 2002" startWordPosition="2456" endWordPosition="2459">ling choices (§5). First, when relating the sense and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = Pr(s` = k|d,t` = j)Pr(t` = j|d,s` = k) (3) where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d,t` = j) = Pθs(s` =k|d)Pθs|tj(s` =k|t` =j)Pθst(t` =j, s` =k) Zd,tj Pθt(t` =j|d)Pθt|sk(t` =j|s` =k) Zd,sk where Zd,tj and Zd,sk are normalization factors and we have introduced new multinomial parameters θs, θs|tj, θst, and θt|sk. We use the same idea to factor the word generation distribution: Pψtj(w`|t`=j)Pψsk(w`|s`=k) Ztj,sk where Ztj,sk is a normalization factor, and we have new multinomial parameters ψsk for the sense</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proc. of ACL, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lau</author>
<author>P Cook</author>
<author>D McCarthy</author>
<author>D Newman</author>
<author>T Baldwin</author>
</authors>
<title>Word sense induction for novel sense detection.</title>
<date>2012</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>591--601</pages>
<contexts>
<context position="7309" citStr="Lau et al. (2012)" startWordPosition="1167" endWordPosition="1170">formance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full disc</context>
</contexts>
<marker>Lau, Cook, McCarthy, Newman, Baldwin, 2012</marker>
<rawString>J. H. Lau, P. Cook, D. McCarthy, D. Newman, and T. Baldwin. 2012. Word sense induction for novel sense detection. In Proc. of EACL, pages 591–601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lau</author>
<author>P Cook</author>
<author>T Baldwin</author>
</authors>
<title>unimelb: Topic modelling-based word sense induction.</title>
<date>2013</date>
<booktitle>In Proc. of SemEval,</booktitle>
<pages>307--311</pages>
<contexts>
<context position="7443" citStr="Lau et al., 2013" startWordPosition="1192" endWordPosition="1195">c model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic </context>
</contexts>
<marker>Lau, Cook, Baldwin, 2013</marker>
<rawString>J. H. Lau, P. Cook, and T. Baldwin. 2013. unimelb: Topic modelling-based word sense induction. In Proc. of SemEval, pages 307–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>B Roth</author>
<author>C Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1138--1147</pages>
<contexts>
<context position="7874" citStr="Li et al., 2010" startWordPosition="1267" endWordPosition="1270">ly discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplic</context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>L. Li, B. Roth, and C. Sporleder. 2010. Topic models for word sense disambiguation and token-based idiom detection. In Proc. of ACL, pages 1138–1147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Maron</author>
<author>E Bienenstock</author>
<author>M James</author>
</authors>
<title>Sphere embedding: An application to part-of-speech induction.</title>
<date>2010</date>
<booktitle>In Advances in NIPS 23.</booktitle>
<contexts>
<context position="9747" citStr="Maron et al., 2010" startWordPosition="1565" endWordPosition="1568">arger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provi</context>
</contexts>
<marker>Maron, Bienenstock, James, 2010</marker>
<rawString>Y. Maron, E. Bienenstock, and M. James. 2010. Sphere embedding: An application to part-of-speech induction. In Advances in NIPS 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J May</author>
<author>K Knight</author>
</authors>
<title>Syntactic re-alignment models for machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL,</booktitle>
<pages>360--368</pages>
<contexts>
<context position="15022" citStr="May and Knight, 2007" startWordPosition="2460" endWordPosition="2463"> when relating the sense and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = Pr(s` = k|d,t` = j)Pr(t` = j|d,s` = k) (3) where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d,t` = j) = Pθs(s` =k|d)Pθs|tj(s` =k|t` =j)Pθst(t` =j, s` =k) Zd,tj Pθt(t` =j|d)Pθt|sk(t` =j|s` =k) Zd,sk where Zd,tj and Zd,sk are normalization factors and we have introduced new multinomial parameters θs, θs|tj, θst, and θt|sk. We use the same idea to factor the word generation distribution: Pψtj(w`|t`=j)Pψsk(w`|s`=k) Ztj,sk where Ztj,sk is a normalization factor, and we have new multinomial parameters ψsk for the sense-word distributions. O</context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>J. May and K. Knight. 2007. Syntactic re-alignment models for machine translation. In Proc. of EMNLPCoNLL, pages 360–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>K Chen</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proc. of ICLR.</booktitle>
<contexts>
<context position="4917" citStr="Mikolov et al., 2013" startWordPosition="780" endWordPosition="783"> as two separate latent variables to be inferred jointly (§4). When relating the sense and topic variables, a bidirectional edge is drawn between them to represent their cyclic dependence (Heckerman et al., 2001). We perform inference using collapsed Gibbs sampling (§4.2), then estimate the sense distribution for each instance as the solution to the WSI task. We conduct experiments on the SemEval-2013 Task 13 WSI dataset, showing improvements over several strong baselines and task systems (§5). We also present unsupervised ways of enriching our dataset, including using neural word embeddings (Mikolov et al., 2013) and external Web-scale corpora to enrich the context of each data instance or to add more instances (§6). Each data enrichment method gives further gains, resulting in significant improvements over existing state-of-the-art WSI systems. Overall, we find gains of up to 22% relative improvement in fuzzy B-cubed and 50% relative improvement in fuzzy normalized mutual information (Jurgens and Klapaftis, 2013). 2 Background and Related Work We discuss the WSI task, then discuss several areas of research that are related to our approach, including applications of topic modeling to WSI as well as ot</context>
<context position="29958" citStr="Mikolov et al., 2013" startWordPosition="5059" endWordPosition="5062">red task systems added instances from external corpora. In this section, we consider three unsupervised ways of enriching data and measure their impact on performance. In §6.1 we augment the context of each instance in our original dataset while keeping the number of instances fixed. In §6.2 we collect more instances of each target word from ukWac, similar to the AI-KU and unimelb systems. In §6.3, we change the distribution of words in each instance based on their similarity to the target word. Throughout, we make use of word embeddings (see §2). We trained 100-dimensional skip-gram vectors (Mikolov et al., 2013) on English Wikipedia (tokenized/lowercased, resulting in 1.8B tokens of text) using window size 10, hierarchical softmax, and no downsampling.7 7We used a minimum count cutoff of 20 during training, 6.1 Adding Context The first way we explore of enriching data is to add a broader context for each instance while keeping the number of instances unchanged. This will introduce more word tokens into the set of global context words, while keeping the set of local context words mostly unchanged, as the window size we use is typically smaller than the length of the original instance. With more global</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013. Efficient estimation of word representations in vector space. In Proc. of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="5859" citStr="Miller et al., 1990" startWordPosition="933" endWordPosition="936">lative improvement in fuzzy normalized mutual information (Jurgens and Klapaftis, 2013). 2 Background and Related Work We discuss the WSI task, then discuss several areas of research that are related to our approach, including applications of topic modeling to WSI as well as other approaches that use word embeddings and clustering algorithms. WSD and WSI: WSI is related to but distinct from word sense disambiguation (WSD). WSD seeks to assign a particular sense label to each target word instance, where the sense labels are known and usually drawn from an existing sense inventory like WordNet (Miller et al., 1990). Although extensive research has been devoted to WSD, WSI may be more useful for downstream tasks. WSD relies on sense inventories whose construction is time-intensive, expensive, and subject to poor inter-annotator agreement (Passonneau et al., 2010). Sense inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>641--648</pages>
<contexts>
<context position="8154" citStr="Mnih and Hinton, 2007" startWordPosition="1311" endWordPosition="1314">inct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training c</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>A. Mnih and G. Hinton. 2007. Three new graphical models for statistical language modelling. In Proc. of ICML, pages 641–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>W Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>220--224</pages>
<contexts>
<context position="37721" citStr="Moore and Lewis, 2010" startWordPosition="6343" endWordPosition="6346">augment our dataset with additional instances from an external corpus. We have no gold standard senses for these instances, so we will not evaluate our model on them; they are merely used to provide richer co-occurrence statistics about the target word so that we can perform better on the instances on which we evaluate. If we added randomly-chosen instances (containing the target word), we would be concerned that the learned topics and senses may not reflect the distributions of the original instance set. So we only add instances that are semantically similar to instances in our original set (Moore and Lewis, 2010; Chambers and Jurafsky, 2011). Also, to avoid changing the original sense distribution by adding too many instances, we only add a single instance for each original instance. As in §6.1, for each instance in the original dataset, we find the most similar sentence in ukWac for each instance using word embeddings and add it into the dataset. Therefore, the number of instances is doubled, and we use the enriched dataset for our sense-topic model. Results Similarly to §6.1, on TRIAL, we find best performance for S = 10, so we run on TEST with this value. As shown in Table 2 (row 12), this improve</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>R. C. Moore and W. Lewis. 2010. Intelligent selection of language model training data. In Proc. of ACL, pages 220–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N R Pal</author>
<author>J C Bezdek</author>
</authors>
<title>On cluster validity for the fuzzy c-means model.</title>
<date>1995</date>
<journal>Trans. Fuz Sys.,</journal>
<pages>3--370</pages>
<contexts>
<context position="41118" citStr="Pal and Bezdek, 1995" startWordPosition="6940" endWordPosition="6943">eate, clinton, people, american, popular +weight by similarity (§6.3) 1 depict, create, culture, mental, include 2 picture, visual, pictorial, matrix, movie 3 public, means, view, american, story Table 4: Top 5 terms for each sense induced for the noun image by the sense-topic model and when weighting local context words by similarity. S = 3 for both. an additional baseline: “word embedding product” (row 8), where we represent each instance by multiplying (element-wise) the word vectors of all local context words, and then feed the instance vectors into the fuzzy c-means clustering algorithm (Pal and Bezdek, 1995), c = 3. Compared to this baseline, our approach improves 4.36% on average; compared with results for the original sense-topic model (row 9), this approach improves 0.69% on average. In Table 4 we show the top-5 terms for each sense induced for image, both for the original sense-topic model and when additionally weighting by similarity. We find that the original model provides less distinguishable senses, as it is difficult to derive separate senses from these top terms. In contrast, senses learned from the model with weighted similarities are more distinct. Sense 1 relates to mental represent</context>
</contexts>
<marker>Pal, Bezdek, 1995</marker>
<rawString>N. R. Pal and J. C. Bezdek. 1995. On cluster validity for the fuzzy c-means model. Trans. Fuz Sys., 3:370–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proc. of KDD,</booktitle>
<pages>613--619</pages>
<contexts>
<context position="10028" citStr="Pantel and Lin, 2002" startWordPosition="1607" endWordPosition="1610">d similar sentences in external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provides context for a single occurrence of the target word.1 For our experiments, we use the 1The target word token may occur multiple times in an instance, but only one occurrence is chosen as the target word occurrence. Figure 1: Proposed sense-topic model in plate notation. There a</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D. Lin. 2002. Discovering word senses from text. In Proc. of KDD, pages 613–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Passonneau</author>
<author>A Salleb-Aoussi</author>
<author>V Bhardwaj</author>
<author>N Ide</author>
</authors>
<title>Word sense annotation of polysemous words by multiple annotators.</title>
<date>2010</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="6111" citStr="Passonneau et al., 2010" startWordPosition="971" endWordPosition="974"> modeling to WSI as well as other approaches that use word embeddings and clustering algorithms. WSD and WSI: WSI is related to but distinct from word sense disambiguation (WSD). WSD seeks to assign a particular sense label to each target word instance, where the sense labels are known and usually drawn from an existing sense inventory like WordNet (Miller et al., 1990). Although extensive research has been devoted to WSD, WSI may be more useful for downstream tasks. WSD relies on sense inventories whose construction is time-intensive, expensive, and subject to poor inter-annotator agreement (Passonneau et al., 2010). Sense inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis,</context>
</contexts>
<marker>Passonneau, Salleb-Aoussi, Bhardwaj, Ide, 2010</marker>
<rawString>R. J. Passonneau, A. Salleb-Aoussi, V. Bhardwaj, and N. Ide. 2010. Word sense annotation of polysemous words by multiple annotators. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
<author>R Girju</author>
</authors>
<title>Cross-cultural analysis of blogs and forums with mixed-collection topic models.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1408--1417</pages>
<contexts>
<context position="43110" citStr="Paul and Girju, 2009" startWordPosition="7261" endWordPosition="7264">he instance is not available. Evaluating on the SemEval2013 WSI dataset, we demonstrate that our model yields significant improvements over current stateof-the-art systems, giving 59.1% fuzzy B-cubed and 9.39% fuzzy NMI in our best setting. Moreover, we find that modeling both sense and topic is critical to enable us to effectively exploit broader context, showing that LDA does not improve when each instance is enriched by actual context. In future work, we plan to further explore the space of sense-topic models, including non-deficient models. One possibility is to use “switching variables” (Paul and Girju, 2009) to choose whether to generate each word from a topic or sense, with a stronger preference to generate from senses closer to the target word. Another possibility is to use locallynormalized log-linear distributions and include features pairing words with particular senses and topics, rather than redundant generative steps. Appendix A The plate diagram for the complete sense-topic model is shown in Figure 2. Figure 2: Plate notation for the proposed sense-topic model with all variables (except α, the fixed Dirichlet hyperparameter used as prior for all multinomial distributions). Each instance </context>
</contexts>
<marker>Paul, Girju, 2009</marker>
<rawString>M. Paul and R. Girju. 2009. Cross-cultural analysis of blogs and forums with mixed-collection topic models. In Proc. of EMNLP, pages 1408–1417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Purandare</author>
<author>T Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="10059" citStr="Purandare and Pedersen, 2004" startWordPosition="1611" endWordPosition="1614"> external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provides context for a single occurrence of the target word.1 For our experiments, we use the 1The target word token may occur multiple times in an instance, but only one occurrence is chosen as the target word occurrence. Figure 1: Proposed sense-topic model in plate notation. There are MD instances for the given t</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>A. Purandare and T. Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proc. of CoNLL, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces.</title>
<date>2006</date>
<institution>Stockholm University.</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="8072" citStr="Sahlgren, 2006" startWordPosition="1299" endWordPosition="1300">l2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations </context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>M. Sahlgren. 2006. The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. Ph.D. dissertation, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Comput. Linguist.,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>H. Sch¨utze. 1998. Automatic word sense discrimination. Comput. Linguist., 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--1566</pages>
<contexts>
<context position="7209" citStr="Teh et al., 2006" startWordPosition="1150" endWordPosition="1153">or these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>M Johnson</author>
</authors>
<title>A Bayesian LDAbased model for semi-supervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Advances in NIPS 20.</booktitle>
<contexts>
<context position="15052" citStr="Toutanova and Johnson, 2007" startWordPosition="2464" endWordPosition="2467">se and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = Pr(s` = k|d,t` = j)Pr(t` = j|d,s` = k) (3) where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d,t` = j) = Pθs(s` =k|d)Pθs|tj(s` =k|t` =j)Pθst(t` =j, s` =k) Zd,tj Pθt(t` =j|d)Pθt|sk(t` =j|s` =k) Zd,sk where Zd,tj and Zd,sk are normalization factors and we have introduced new multinomial parameters θs, θs|tj, θst, and θt|sk. We use the same idea to factor the word generation distribution: Pψtj(w`|t`=j)Pψsk(w`|s`=k) Ztj,sk where Ztj,sk is a normalization factor, and we have new multinomial parameters ψsk for the sense-word distributions. One advantage of this parameter</context>
</contexts>
<marker>Toutanova, Johnson, 2007</marker>
<rawString>K. Toutanova and M. Johnson. 2007. A Bayesian LDAbased model for semi-supervised part-of-speech tagging. In Advances in NIPS 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: A simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="8868" citStr="Turian et al., 2010" startWordPosition="1422" endWordPosition="1425">. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a co60 occurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training criteria based on predicting nearby words. They have been useful as features in many NLP tasks (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment)</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semisupervised learning. In Proc. of ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J V´eronis</author>
</authors>
<title>Hyperlex: lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>V´eronis, 2004</marker>
<rawString>J. V´eronis. 2004. Hyperlex: lexical cartography for information retrieval. Computer Speech &amp; Language, 18(3):223–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vickrey</author>
<author>L Biewald</author>
<author>M Teyssier</author>
<author>D Koller</author>
</authors>
<title>Word-sense disambiguation for machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP,</booktitle>
<pages>771--778</pages>
<contexts>
<context position="6738" citStr="Vickrey et al., 2005" startWordPosition="1070" endWordPosition="1074"> inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on a</context>
</contexts>
<marker>Vickrey, Biewald, Teyssier, Koller, 2005</marker>
<rawString>D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005. Word-sense disambiguation for machine translation. In Proc. of HLT-EMNLP, pages 771–778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Using WordNet to disambiguate word senses for text retrieval.</title>
<date>1993</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>171--180</pages>
<contexts>
<context position="6622" citStr="Voorhees, 1993" startWordPosition="1054" endWordPosition="1055">n is time-intensive, expensive, and subject to poor inter-annotator agreement (Passonneau et al., 2010). Sense inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has t</context>
</contexts>
<marker>Voorhees, 1993</marker>
<rawString>E. M. Voorhees. 1993. Using WordNet to disambiguate word senses for text retrieval. In Proc. of SIGIR, pages 171–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yao</author>
<author>B Van Durme</author>
</authors>
<title>Nonparametric Bayesian word sense induction.</title>
<date>2011</date>
<booktitle>In Proc. of TextGraphs-6: Graph-based Methods for Natural Language Processing,</booktitle>
<pages>10--14</pages>
<marker>Yao, Van Durme, 2011</marker>
<rawString>X. Yao and B. Van Durme. 2011. Nonparametric Bayesian word sense induction. In Proc. of TextGraphs-6: Graph-based Methods for Natural Language Processing, pages 10–14.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>