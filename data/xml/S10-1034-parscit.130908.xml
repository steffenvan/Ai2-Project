<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056230">
<title confidence="0.989306">
Likey: Unsupervised Language-independent Keyphrase Extraction
</title>
<author confidence="0.97531">
Mari-Sanna Paukkeri and Tinto Honkela
</author>
<affiliation confidence="0.991166">
Adaptive Informatics Research Centre
Aalto University School of Science and Technology
</affiliation>
<address confidence="0.862772">
P.O. Box 15400, FI-00076 AALTO, Finland
</address>
<email confidence="0.99626">
mari-sanna.paukkeri@tkk.fi
</email>
<sectionHeader confidence="0.993781" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999859214285714">
Likey is an unsupervised statistical ap-
proach for keyphrase extraction. The
method is language-independent and the
only language-dependent component is
the reference corpus with which the doc-
uments to be analyzed are compared.
In this study, we have also used an-
other language-dependent component: an
English-specific Porter stemmer as a pre-
processing step. In our experiments
of keyphrase extraction from scientific
articles, the Likey method outperforms
both supervised and unsupervised baseline
methods.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999824190476191">
Keyphrase extraction is a natural language pro-
cessing task for collecting the main topics of a
document into a list of phrases. Keyphrases are
supposed to be available in the processed docu-
ments themselves, and the aim is to extract these
most meaningful words and phrases from the doc-
uments. Keyphrase extraction summarises the
content of a document as few phrases and thus
provides a quick way to find out what the docu-
ment is about. Keyphrase extraction is a basic text
mining procedure that can be used as a ground
for other, more sophisticated text analysis meth-
ods. Automatically extracted keyphrases may be
used to improve the performance of information
retrieval, automatic user model generation, docu-
ment collection clustering and visualisation, sum-
marisation and question-answering, among others.
This article describes the participation of the
Likey method in the Task 5 of the SemEval 2010
challenge, automatic keyphrase extraction from
scientific articles (Kim et al., 2010).
</bodyText>
<sectionHeader confidence="0.872732" genericHeader="introduction">
1.1 Related work
</sectionHeader>
<bodyText confidence="0.99992925">
In statistical keyphrase extraction, many variations
for term frequency counts have been proposed in
the literature including relative frequencies (Dam-
erau, 1993), collection frequency (Hulth, 2003),
term frequency–inverse document frequency (tf-
idf) (Salton and Buckley, 1988), among others.
Additional features to frequency that have been
experimented are e.g., relative position of the first
occurrence of the term (Frank et al., 1999), im-
portance of the sentence in which the term oc-
curs (HaCohen-Kerner, 2003), and widely stud-
ied part-of-speech tag patterns, e.g. Hulth (2003).
Matsuo and Ishizuka (2004) present keyword ex-
traction method using word co-occurrence statis-
tics. An unsupervised keyphrase extraction
method by Liu et al. (2009) uses clustering to find
exemplar terms that are then used for keyphrase
extraction. Most of the presented methods require
a reference corpus or a training corpus to produce
keyphrases. Statistical keyphrase extraction meth-
ods without reference corpora have also been pro-
posed, e.g. (Matsuo and Ishizuka, 2004; Bracewell
et al., 2005). The later study is carried out for
bilingual corpus.
</bodyText>
<sectionHeader confidence="0.988859" genericHeader="method">
2 Data
</sectionHeader>
<bodyText confidence="0.999778461538462">
The data used in this work are from the SemEval
2010 challenge Task 5, automatic keyphrase ex-
traction from scientific articles. The data consist
of train, trial, and test data sets. The number of
scientific articles and the total number of word to-
kens in each of the original data sets (before pre-
processing) are given in Table 1.
Three sets of “correct” keyphrases are pro-
vided for each article in each data set: reader-
assigned keyphrases, author-provided keyphrases,
and a combination of them. All reader-assigned
keyphrases have been extracted manually from
the papers whereas some of author-provided
</bodyText>
<page confidence="0.972336">
162
</page>
<note confidence="0.648406">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 162–165,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.99952725">
Data set Articles Word tokens
train 144 1 159 015
trial 40 334 379
test 100 798 049
</table>
<tableCaption confidence="0.747208">
Table 1: Number of scientific articles and total
number of word tokens in the data sets.
keyphrases may not occur in the content. The
numbers of correct keyphrases in each data set are
shown in Table 2.
</tableCaption>
<table confidence="0.955841">
Data set Reader Author Combined
train 1 824 559 2 223
trial 526 149 621
test 1 204 387 1 466
</table>
<tableCaption confidence="0.916636">
Table 2: Number of correct answers in reader, au-
thor, and combined answer sets for each data set.
</tableCaption>
<bodyText confidence="0.923777">
More detailed information on the data set can
be found in (Kim et al., 2010).
</bodyText>
<sectionHeader confidence="0.997693" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999955">
Likey keyphrase extraction approach comes
from the tradition of statistical machine learn-
ing (Paukkeri et al., 2008). The method has
been developed to be as language-independent as
possible. The only language-specific component
needed is a corpus in each language. This kind
of data is readily available online or from other
sources.
Likey selects the words and phrases that best
crystallize the meaning of the documents by com-
paring ranks of frequencies in the documents to
those in the reference corpus. The Likey ra-
tio (Paukkeri et al., 2008) for each phrase is de-
fined as
</bodyText>
<equation confidence="0.976989666666667">
rankd(p)
L(p, d) =
rankr(p), (1)
</equation>
<bodyText confidence="0.999991478260869">
where rankd(p) is the rank value of phrase p in
document d and rankr(p) is the rank value of
phrase p in the reference corpus. The rank val-
ues are calculated according to the frequencies of
phrases of the same length n. If the phrase p does
not exist in the reference corpus, the value of the
maximum rank for phrases of length n is used:
rankr(p) = max rankr(n) + 1. The Likey ra-
tio orders the phrases in a document in such a way
that the phrases that have the smallest ratio are the
best candidates for being a keyphrase.
As a post-processing step, the phrases of length
n &gt; 1 face an extra removal process: if one of
the words composing the phrase has a rank of less
than a threshold ξ in the reference corpus, the
phrase is removed from the keyphrase list. This
procedure excludes phrases that contain function
words such as “of” or “the”. As another post-
processing step, phrases that are subphrases of
those that have occurred earlier on the keyphrase
list are removed, excluding e.g. “language model”
if “unigram language model” has been already ac-
cepted as a keyphrase.
</bodyText>
<subsectionHeader confidence="0.995591">
3.1 Reference corpus
</subsectionHeader>
<bodyText confidence="0.999975666666667">
Likey needs a reference corpus that is seen as a
sample of the general language. In the present
study, we use a combination of the English part of
Europarl, European Parliament plenary speeches
(Koehn, 2005) and the preprocessed training set as
the reference corpus. All XML tags of meta infor-
mation are excluded from the Europarl data. The
size of the Europarl corpus is 35 800 000 words
after removal of XML tags.
</bodyText>
<subsectionHeader confidence="0.998267">
3.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999903692307692">
The scientific articles are preprocessed by remov-
ing all headers including the names and addresses
of the authors. Also the reference section is re-
moved from the articles, as well as all tables, fig-
ures, equations and citations. Both scientific arti-
cles and the Europarl data is lowercased, punctua-
tion is removed (the hyphens surrounded by word
characters and apostrophes are kept) and the num-
bers are changed to &lt;NUM&gt; tag.
The data is stemmed with English Porter stem-
mer implementation provided by the challenge or-
ganizers, which differs from our earlier experi-
ments.
</bodyText>
<subsectionHeader confidence="0.996816">
3.3 Baselines
</subsectionHeader>
<bodyText confidence="0.999664">
We use three baseline methods for keyphrase ex-
traction. The baselines use uni-, bi-, and trigrams
as candidates of keyphrases with tf-idf weight-
ing scheme. One of the baselines is unsuper-
vised and the other two are supervised approaches.
The unsupervised method is to rank the candidates
according to their tf-idf scores. The supervised
methods are Naive Bayes (NB) and Maximum En-
tropy (ME) implementations from WEKA pack-
age1.
</bodyText>
<footnote confidence="0.988697">
1http://www.cs.waikato.ac.nz/˜ml/weka/
</footnote>
<page confidence="0.999109">
163
</page>
<sectionHeader confidence="0.999196" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999892888888889">
We participated the challenge with Likey results of
three different parameter settings. The settings are
given in Table 3. Likey-1 has phrases up to 3 words
and Likey-2 and Likey-3 up to 4 words. The thresh-
old value for postprocessing was selected against
the trial set, with ξ = 100 performing best. It
is used for Likey-1 and Likey-2. Also a bit larger
threshold ξ = 130 was tried for Likey-3 to exclude
more function words.
</bodyText>
<table confidence="0.99333625">
Repr. n ξ
Likey-1 1–3 100
Likey-2 1–4 100
Likey-3 1–4 130
</table>
<tableCaption confidence="0.827034">
Table 3: Different parametrizations for Likey: n-
gram length and threshold value ξ.
</tableCaption>
<bodyText confidence="0.99653605">
An example of the resulting keyphrases ex-
tracted by Likey-1 from the first scientific arti-
cle in the test set (article C-1) is given in Ta-
ble 4. Also the corresponding “correct” answers in
reader-assigned and author-provided answer sets
are shown. The keyphrases are given in stemmed
versions. Likey keyphrases that can be found in the
reader or author answer sets are emphasized.
Likey-1 uddi registri, proxi registri, servic
discoveri, grid servic discoveri, uddi kei, uniqu
uddi kei, servic discoveri mechan, distribut
hash tabl, web servic, dht, servic name, web
servic discoveri, local proxi registri, local uddi
registri, queri multipl registri
Reader grid servic discoveri, uddi, distribut
web-servic discoveri architectur, dht base uddi
registri hierarchi, deploy issu, bamboo dht
code, case-insensit search, queri, longest avail
prefix, qo-base servic discoveri, autonom
control, uddi registri, scalabl issu, soft state
Author uddi, dht, web servic, grid comput,
md, discoveri
Table 4: Extracted keyphrases by Likey-1 from ar-
ticle C-1 and the corresponding correct answers in
reader and author answer sets.
The example shows clearly that many of the ex-
tracted keyphrases contain the same words that
can be found in the correct answer sets but the
length of the phrases vary and thus they cannot be
counted as successfully extracted keyphrases.
The results for the three different Likey
parametrizations and the three baselines are given
in Table 5 for reader-assigned keyphrases and Ta-
ble 6 for the combined set of reader and author-
assigned keyphrases. The evaluation is conducted
by calculating precision (P), recall (R) and F-
measure (F) for top 5, 10, and 15 keyphrase candi-
dates for each method, using the reader-assigned
and author-provided lists as correct answers. The
baseline methods are unsupervised tf-idf and su-
pervised Naive Bayes (NB) and Maximum Entropy
(ME).
Likey-1 performed best in the competition and
is thus selected as the official result of Likey in the
task. Anyway, all Likey parametrizations outper-
form the baselines, Likey-1 having the best pre-
cision 24.60% for top-5 candidates in the reader
data set and 29.20% for top-5 candidates in the
combined data set. The best F-measure is obtained
with Likey-1 for top-10 candidates for both reader
and combined data set: 16.24% and 17.11%,
respectively. Likey seems to produce the best
keyphrases in the beginning of the keyphrase list:
for reader-assigned keyphrases the top 5 keyphrase
precision for Likey-1 is 6.8 points better than
the best-performing baseline tf-idf and the cor-
responding F-measure is 4.0 points better. For
the combined set, the numbers are 7.2 and 3.7
points, respectively. The difference decreases for
the larger keyphrase sets.
</bodyText>
<sectionHeader confidence="0.979214" genericHeader="conclusions">
5 Conclusions and discussion
</sectionHeader>
<bodyText confidence="0.999916894736842">
This article describes our submission to SemEval
2010 Task 5, keyphrase extraction from scien-
tific articles. Our unsupervised and language-
independent method Likey uses reference corpus
and is able to outperform both the unsupervised
and supervised baseline methods. The best results
are obtained with the top-5 keyphrases: precision
of 24.60% with reader-assigned keyphrases and
29.20% with the combination of reader-assigned
and author-provided keyphrases.
There are some keyphrases in the answer sets
that our method does not find: due to the com-
paratively large threshold value ξ many phrases
that contain function words, e.g. “of”, cannot be
found. We also extract keyphrases of maximum
length of three or four words and thus cannot find
keyphrases longer than that. The next step of this
research would be to take these problems into ac-
count.
</bodyText>
<page confidence="0.993398">
164
</page>
<table confidence="0.9998845">
Method Top 5 candidates Top 10 candidates Top 15 candidates
P % R % F % P % R % F % P % R % F %
Likey-1 24.60 10.22 14.44 17.90 14.87 16.24 13.80 17.19 15.31
Likey-2 23.80 9.88 13.96 16.90 14.04 15.34 13.40 16.69 14.87
Likey-3 23.40 9.72 13.73 16.80 13.95 15.24 13.73 17.11 15.23
tf-idf 17.80 7.39 10.44 13.90 11.54 12.61 11.60 14.45 12.87
NB 16.80 6.98 9.86 13.30 11.05 12.07 11.40 14.20 12.65
ME 16.80 6.98 9.86 13.30 11.05 12.07 11.40 14.20 12.65
</table>
<tableCaption confidence="0.8488745">
Table 5: Results for Likey and the baselines for the reader data set. The best precision (P), recall (R) and
F-measure (F) are highlighted.
</tableCaption>
<table confidence="0.99995475">
Method Top 5 candidates Top 10 candidates Top 15 candidates
P % R % F % P % R % F % P % R % F %
Likey-1 29.20 9.96 14.85 21.10 14.39 17.11 16.33 16.71 16.52
Likey-2 28.40 9.69 14.45 19.90 13.57 16.14 15.73 16.10 15.91
Likey-3 28.00 9.55 14.24 19.60 13.37 15.90 16.07 16.44 16.25
tf-idf 22.00 7.50 11.19 17.70 12.07 14.35 14.93 15.28 15.10
NB 21.40 7.30 10.89 17.30 11.80 14.03 14.53 14.87 14.70
ME 21.40 7.30 10.89 17.30 11.80 14.03 14.53 14.87 14.70
</table>
<tableCaption confidence="0.859328">
Table 6: Results for Likey and the baselines for the combined (reader+author) data set. The best precision
(P), recall (R) and F-measure (F) are highlighted.
</tableCaption>
<sectionHeader confidence="0.995169" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.991738">
This work was supported by the Finnish Graduate
School in Language Studies (Langnet) funded by
Ministry of Education of Finland.
</bodyText>
<sectionHeader confidence="0.998546" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998013125">
David B. Bracewell, Fuji Ren, and Shingo Kuriowa.
2005. Multilingual single document keyword ex-
traction for information retrieval. In Proceedings of
NLP-KE’05.
Fred Damerau. 1993. Generating and evaluating
domain-oriented multi-word terms from text. In-
formation Processing and Management, 29(4):433–
447.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings ofIJCAI’99, pages 668–673.
Yaakov HaCohen-Kerner. 2003. Automatic extrac-
tion of keywords from abstracts. In V. Palade, R.J.
Howlett, and L.C. Jain, editors, KES 2003, LNAI
2773, pages 843–849. Springer-Verlag.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 216–223.
Su Nam Kim, Alyona Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. SemEval-2010 Task 5:
Automatic Keyphrase Extraction from Scientific Ar-
ticles. In Proceedings of the ACL 2010 Workshop on
Evaluation Exercises on Semantic Evaluation (Se-
mEval 2010). to appear.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit 2005.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 257–266, Singapore, Au-
gust. Association for Computational Linguistics.
Yutaka Matsuo and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13(1):157–
169.
Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti P¨oll¨a,
and Timo Honkela. 2008. A language-independent
approach to keyphrase extraction and evaluation. In
Coling 2008: Companion volume: Posters, pages
83–86, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
Gerard Salton and Chris Buckley. 1988. Term weight-
ing approaches in automatic text retrieval. Informa-
tion Processing and Management, 24(5):513–523.
</reference>
<page confidence="0.998742">
165
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.910203">
<title confidence="0.999866">Likey: Unsupervised Language-independent Keyphrase Extraction</title>
<author confidence="0.997698">Mari-Sanna Paukkeri</author>
<author confidence="0.997698">Tinto Honkela</author>
<affiliation confidence="0.981185">Adaptive Informatics Research Centre Aalto University School of Science and Technology</affiliation>
<address confidence="0.999391">P.O. Box 15400, FI-00076 AALTO, Finland</address>
<email confidence="0.987858">mari-sanna.paukkeri@tkk.fi</email>
<abstract confidence="0.997302133333333">an unsupervised statistical approach for keyphrase extraction. The method is language-independent and the only language-dependent component is the reference corpus with which the documents to be analyzed are compared. In this study, we have also used another language-dependent component: an Porter stemmer as a preprocessing step. In our experiments of keyphrase extraction from scientific the outperforms both supervised and unsupervised baseline methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David B Bracewell</author>
<author>Fuji Ren</author>
<author>Shingo Kuriowa</author>
</authors>
<title>Multilingual single document keyword extraction for information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of NLP-KE’05.</booktitle>
<contexts>
<context position="2869" citStr="Bracewell et al., 2005" startWordPosition="417" endWordPosition="420">e sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). Matsuo and Ishizuka (2004) present keyword extraction method using word co-occurrence statistics. An unsupervised keyphrase extraction method by Liu et al. (2009) uses clustering to find exemplar terms that are then used for keyphrase extraction. Most of the presented methods require a reference corpus or a training corpus to produce keyphrases. Statistical keyphrase extraction methods without reference corpora have also been proposed, e.g. (Matsuo and Ishizuka, 2004; Bracewell et al., 2005). The later study is carried out for bilingual corpus. 2 Data The data used in this work are from the SemEval 2010 challenge Task 5, automatic keyphrase extraction from scientific articles. The data consist of train, trial, and test data sets. The number of scientific articles and the total number of word tokens in each of the original data sets (before preprocessing) are given in Table 1. Three sets of “correct” keyphrases are provided for each article in each data set: readerassigned keyphrases, author-provided keyphrases, and a combination of them. All reader-assigned keyphrases have been e</context>
</contexts>
<marker>Bracewell, Ren, Kuriowa, 2005</marker>
<rawString>David B. Bracewell, Fuji Ren, and Shingo Kuriowa. 2005. Multilingual single document keyword extraction for information retrieval. In Proceedings of NLP-KE’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Damerau</author>
</authors>
<title>Generating and evaluating domain-oriented multi-word terms from text.</title>
<date>1993</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>29</volume>
<issue>4</issue>
<pages>447</pages>
<contexts>
<context position="1953" citStr="Damerau, 1993" startWordPosition="281" endWordPosition="283">analysis methods. Automatically extracted keyphrases may be used to improve the performance of information retrieval, automatic user model generation, document collection clustering and visualisation, summarisation and question-answering, among others. This article describes the participation of the Likey method in the Task 5 of the SemEval 2010 challenge, automatic keyphrase extraction from scientific articles (Kim et al., 2010). 1.1 Related work In statistical keyphrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency–inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al., 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). Matsuo and Ishizuka (2004) present keyword extraction method using word co-occurrence statistics. An unsupervised keyphrase extraction method by Liu et al. (2009) uses clustering t</context>
</contexts>
<marker>Damerau, 1993</marker>
<rawString>Fred Damerau. 1993. Generating and evaluating domain-oriented multi-word terms from text. Information Processing and Management, 29(4):433– 447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings ofIJCAI’99,</booktitle>
<pages>668--673</pages>
<contexts>
<context position="2228" citStr="Frank et al., 1999" startWordPosition="319" endWordPosition="322">ribes the participation of the Likey method in the Task 5 of the SemEval 2010 challenge, automatic keyphrase extraction from scientific articles (Kim et al., 2010). 1.1 Related work In statistical keyphrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency–inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al., 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). Matsuo and Ishizuka (2004) present keyword extraction method using word co-occurrence statistics. An unsupervised keyphrase extraction method by Liu et al. (2009) uses clustering to find exemplar terms that are then used for keyphrase extraction. Most of the presented methods require a reference corpus or a training corpus to produce keyphrases. Statistical keyphrase extraction methods without reference corpora have also been proposed, e.g. (Matsuo an</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proceedings ofIJCAI’99, pages 668–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaakov HaCohen-Kerner</author>
</authors>
<title>Automatic extraction of keywords from abstracts.</title>
<date>2003</date>
<booktitle>KES 2003, LNAI 2773,</booktitle>
<pages>843--849</pages>
<editor>In V. Palade, R.J. Howlett, and L.C. Jain, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="2304" citStr="HaCohen-Kerner, 2003" startWordPosition="334" endWordPosition="335">010 challenge, automatic keyphrase extraction from scientific articles (Kim et al., 2010). 1.1 Related work In statistical keyphrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency–inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al., 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). Matsuo and Ishizuka (2004) present keyword extraction method using word co-occurrence statistics. An unsupervised keyphrase extraction method by Liu et al. (2009) uses clustering to find exemplar terms that are then used for keyphrase extraction. Most of the presented methods require a reference corpus or a training corpus to produce keyphrases. Statistical keyphrase extraction methods without reference corpora have also been proposed, e.g. (Matsuo and Ishizuka, 2004; Bracewell et al., 2005). The later study is carried out fo</context>
</contexts>
<marker>HaCohen-Kerner, 2003</marker>
<rawString>Yaakov HaCohen-Kerner. 2003. Automatic extraction of keywords from abstracts. In V. Palade, R.J. Howlett, and L.C. Jain, editors, KES 2003, LNAI 2773, pages 843–849. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="1989" citStr="Hulth, 2003" startWordPosition="286" endWordPosition="287">ted keyphrases may be used to improve the performance of information retrieval, automatic user model generation, document collection clustering and visualisation, summarisation and question-answering, among others. This article describes the participation of the Likey method in the Task 5 of the SemEval 2010 challenge, automatic keyphrase extraction from scientific articles (Kim et al., 2010). 1.1 Related work In statistical keyphrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency–inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al., 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). Matsuo and Ishizuka (2004) present keyword extraction method using word co-occurrence statistics. An unsupervised keyphrase extraction method by Liu et al. (2009) uses clustering to find exemplar terms that are then </context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>Anette Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Alyona Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>SemEval-2010 Task 5: Automatic Keyphrase Extraction from Scientific Articles.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Workshop on Evaluation Exercises on Semantic Evaluation (SemEval</booktitle>
<note>to appear.</note>
<contexts>
<context position="1772" citStr="Kim et al., 2010" startWordPosition="255" endWordPosition="258">thus provides a quick way to find out what the document is about. Keyphrase extraction is a basic text mining procedure that can be used as a ground for other, more sophisticated text analysis methods. Automatically extracted keyphrases may be used to improve the performance of information retrieval, automatic user model generation, document collection clustering and visualisation, summarisation and question-answering, among others. This article describes the participation of the Likey method in the Task 5 of the SemEval 2010 challenge, automatic keyphrase extraction from scientific articles (Kim et al., 2010). 1.1 Related work In statistical keyphrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency–inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al., 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003).</context>
<context position="4271" citStr="Kim et al., 2010" startWordPosition="659" endWordPosition="662">16 July 2010. c�2010 Association for Computational Linguistics Data set Articles Word tokens train 144 1 159 015 trial 40 334 379 test 100 798 049 Table 1: Number of scientific articles and total number of word tokens in the data sets. keyphrases may not occur in the content. The numbers of correct keyphrases in each data set are shown in Table 2. Data set Reader Author Combined train 1 824 559 2 223 trial 526 149 621 test 1 204 387 1 466 Table 2: Number of correct answers in reader, author, and combined answer sets for each data set. More detailed information on the data set can be found in (Kim et al., 2010). 3 Methods Likey keyphrase extraction approach comes from the tradition of statistical machine learning (Paukkeri et al., 2008). The method has been developed to be as language-independent as possible. The only language-specific component needed is a corpus in each language. This kind of data is readily available online or from other sources. Likey selects the words and phrases that best crystallize the meaning of the documents by comparing ranks of frequencies in the documents to those in the reference corpus. The Likey ratio (Paukkeri et al., 2008) for each phrase is defined as rankd(p) L(p</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Alyona Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010. SemEval-2010 Task 5: Automatic Keyphrase Extraction from Scientific Articles. In Proceedings of the ACL 2010 Workshop on Evaluation Exercises on Semantic Evaluation (SemEval 2010). to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="6197" citStr="Koehn, 2005" startWordPosition="999" endWordPosition="1000">he reference corpus, the phrase is removed from the keyphrase list. This procedure excludes phrases that contain function words such as “of” or “the”. As another postprocessing step, phrases that are subphrases of those that have occurred earlier on the keyphrase list are removed, excluding e.g. “language model” if “unigram language model” has been already accepted as a keyphrase. 3.1 Reference corpus Likey needs a reference corpus that is seen as a sample of the general language. In the present study, we use a combination of the English part of Europarl, European Parliament plenary speeches (Koehn, 2005) and the preprocessed training set as the reference corpus. All XML tags of meta information are excluded from the Europarl data. The size of the Europarl corpus is 35 800 000 words after removal of XML tags. 3.2 Preprocessing The scientific articles are preprocessed by removing all headers including the names and addresses of the authors. Also the reference section is removed from the articles, as well as all tables, figures, equations and citations. Both scientific articles and the Europarl data is lowercased, punctuation is removed (the hyphens surrounded by word characters and apostrophes </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Peng Li</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Clustering to find exemplar terms for keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>257--266</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2535" citStr="Liu et al. (2009)" startWordPosition="366" endWordPosition="369">elative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency–inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al., 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). Matsuo and Ishizuka (2004) present keyword extraction method using word co-occurrence statistics. An unsupervised keyphrase extraction method by Liu et al. (2009) uses clustering to find exemplar terms that are then used for keyphrase extraction. Most of the presented methods require a reference corpus or a training corpus to produce keyphrases. Statistical keyphrase extraction methods without reference corpora have also been proposed, e.g. (Matsuo and Ishizuka, 2004; Bracewell et al., 2005). The later study is carried out for bilingual corpus. 2 Data The data used in this work are from the SemEval 2010 challenge Task 5, automatic keyphrase extraction from scientific articles. The data consist of train, trial, and test data sets. The number of scientif</context>
</contexts>
<marker>Liu, Li, Zheng, Sun, 2009</marker>
<rawString>Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun. 2009. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Keyword extraction from a single document using word co-occurrence statistical information.</title>
<date>2004</date>
<journal>International Journal on Artificial Intelligence Tools,</journal>
<volume>13</volume>
<issue>1</issue>
<pages>169</pages>
<contexts>
<context position="2399" citStr="Matsuo and Ishizuka (2004)" startWordPosition="346" endWordPosition="349"> 1.1 Related work In statistical keyphrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency–inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al., 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). Matsuo and Ishizuka (2004) present keyword extraction method using word co-occurrence statistics. An unsupervised keyphrase extraction method by Liu et al. (2009) uses clustering to find exemplar terms that are then used for keyphrase extraction. Most of the presented methods require a reference corpus or a training corpus to produce keyphrases. Statistical keyphrase extraction methods without reference corpora have also been proposed, e.g. (Matsuo and Ishizuka, 2004; Bracewell et al., 2005). The later study is carried out for bilingual corpus. 2 Data The data used in this work are from the SemEval 2010 challenge Task </context>
</contexts>
<marker>Matsuo, Ishizuka, 2004</marker>
<rawString>Yutaka Matsuo and Mitsuru Ishizuka. 2004. Keyword extraction from a single document using word co-occurrence statistical information. International Journal on Artificial Intelligence Tools, 13(1):157– 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari-Sanna Paukkeri</author>
<author>Ilari T Nieminen</author>
<author>Matti P¨oll¨a</author>
<author>Timo Honkela</author>
</authors>
<title>A language-independent approach to keyphrase extraction and evaluation.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2008: Companion volume: Posters,</booktitle>
<pages>83--86</pages>
<location>Manchester, UK,</location>
<marker>Paukkeri, Nieminen, P¨oll¨a, Honkela, 2008</marker>
<rawString>Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti P¨oll¨a, and Timo Honkela. 2008. A language-independent approach to keyphrase extraction and evaluation. In Coling 2008: Companion volume: Posters, pages 83–86, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Chris Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="2067" citStr="Salton and Buckley, 1988" startWordPosition="294" endWordPosition="297">ion retrieval, automatic user model generation, document collection clustering and visualisation, summarisation and question-answering, among others. This article describes the participation of the Likey method in the Task 5 of the SemEval 2010 challenge, automatic keyphrase extraction from scientific articles (Kim et al., 2010). 1.1 Related work In statistical keyphrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency–inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al., 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). Matsuo and Ishizuka (2004) present keyword extraction method using word co-occurrence statistics. An unsupervised keyphrase extraction method by Liu et al. (2009) uses clustering to find exemplar terms that are then used for keyphrase extraction. Most of the presented methods require a referen</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Chris Buckley. 1988. Term weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513–523.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>