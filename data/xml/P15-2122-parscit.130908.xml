<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019560">
<title confidence="0.99294">
Automatic Identification of Rhetorical Questions
</title>
<author confidence="0.990522">
Shohini Bhattasali
</author>
<affiliation confidence="0.888544333333333">
Dept. of Linguistics
Cornell University
Ithaca, NY, USA
</affiliation>
<author confidence="0.984444">
Jeremy Cytryn
</author>
<affiliation confidence="0.893748333333333">
Dept. of Computer Science
Cornell University
Ithaca, NY, USA
</affiliation>
<author confidence="0.974378">
Elana Feldman
</author>
<affiliation confidence="0.88477">
Dept. of Linguistics
Cornell University
Ithaca, NY, USA
</affiliation>
<author confidence="0.977665">
Joonsuk Park
</author>
<affiliation confidence="0.9967755">
Dept. of Computer Science
Cornell University
</affiliation>
<address confidence="0.504628">
Ithaca, NY, USA
</address>
<email confidence="0.9807295">
jpark@cs.cornell.edu
{sb2295, jmc677, eaf82}@cornell.edu
</email>
<sectionHeader confidence="0.993342" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956789473684">
A question may be asked not only to elicit
information, but also to make a state-
ment. Questions serving the latter pur-
pose, called rhetorical questions, are often
lexically and syntactically indistinguish-
able from other types of questions. Still,
it is desirable to be able to identify rhetor-
ical questions, as it is relevant for many
NLP tasks, including information extrac-
tion and text summarization. In this paper,
we explore the largely understudied prob-
lem of rhetorical question identification.
Specifically, we present a simple n-gram
based language model to classify rhetori-
cal questions in the Switchboard Dialogue
Act Corpus. We find that a special treat-
ment of rhetorical questions which incor-
porates contextual information achieves
the highest performance.
</bodyText>
<sectionHeader confidence="0.998972" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999084333333333">
Rhetorical questions frequently appear in every-
day conversations. A rhetorical question is func-
tionally different from other types of questions in
that it is expressing a statement, rather than seek-
ing information. Thus, rhetorical questions must
be identified to fully capture the meaning of an
utterance. This is not an easy task; despite their
drastic functional differences, rhetorical questions
are formulated like regular questions.
Bhatt (1998) states that in principle, a given
question can be interpreted as either an informa-
tion seeking question or as a rhetorical question
and that intonation can be used to identify the in-
terpretation intended by the speaker. For instance,
consider the following example:
</bodyText>
<listItem confidence="0.870858">
(1) Did I tell you that writing a dissertation
was easy?
</listItem>
<bodyText confidence="0.999818384615385">
Just from reading the text, it is difficult to tell
whether the speaker is asking an informational
question or whether they are implying that they
did not say that writing a dissertation was easy.
However, according to our observation, which
forms the basis of this work, there are two cases in
which rhetorical questions can be identified solely
based on the text. Firstly, certain linguistic cues
make a question obviously rhetorical, which can
be seen in examples (2) and (3)1. Secondly, the
context, or neighboring utterances, often reveal
the rhetorical nature of the question, as we can see
in example (4).
</bodyText>
<listItem confidence="0.990864666666667">
(2) Who ever lifted a finger to help George?
(3) After all, who has any time during the
exam period?
(4) Who likes winter? It is always cold and
windy and gray and everyone feels miser-
able all the time.
</listItem>
<bodyText confidence="0.999774333333333">
There has been substantial work in the area
of classifying dialog acts, within which rhetor-
ical questions fall. To our knowledge, prior
work on dialog act tagging has largely ignored
rhetorical questions, and there has not been any
previous work specifically addressing rhetorical
question identification. Nevertheless, classifica-
tion of rhetorical questions is crucial and has nu-
merous potential applications, including question-
answering, document summarization, author iden-
tification, and opinion extraction.
We provide an overview of related work in Sec-
tion 2, discuss linguistic characteristics of rhetor-
ical questions in Section 3, describe the experi-
mental setup in Section 4, and present and analyze
the experiment results in Section 5. We find that,
while the majority of the classification relies on
features extracted from the question itself, adding
</bodyText>
<footnote confidence="0.930672">
1See Section 3 for more details.
</footnote>
<page confidence="0.812424">
743
</page>
<bodyText confidence="0.828192888888889">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 743–749,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
in n-gram features from the context improves the
performance. An Fl-score of 53.71% is achieved
by adding features extracted from the preceding
and subsequent utterances, which is about a 10%
improvement from a baseline classifier using only
the features from the question itself.
</bodyText>
<sectionHeader confidence="0.998696" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999969702702703">
Jurafsky et al. (1997a) and Reithinger and Kle-
sen (1997) used n-gram language modeling on the
Switchboard and Verbmobil corpora respectively
to classify dialog acts. Grau et al. (2004) uses
a Bayesian approach with n-grams to categorize
dialog acts. We also employ a similar language
model to achieve our results.
Samuel et al. (1999) used transformation-based
learning on the Verbmobil corpus over a num-
ber of utterance features such as utterance length,
speaker turn, and the dialog act tags of adja-
cent utterances. Stolcke et al. (2000) utilized
Hidden Markov Models on the Switchboard cor-
pus and used word order within utterances and
the order of dialog acts over utterances. Zech-
ner (2002) worked on automatic summarization
of open-domain spoken dialogues i.e., important
pieces of information are found in the back and
forth of a dialogue that is absent in a written piece.
Webb et al. (2005) used intra-utterance features
in the Switchboard corpus and calculated n-grams
for each utterance of all dialogue acts. For each n-
gram, they computed the maximal predictivity i.e.,
its highest predictivity value within any dialogue
act category. We utilized a similar metric for n-
gram selection.
Verbree et al. (2006) constructed their baseline
for three different corpora using the performance
of the LIT set, as proposed by Samuel (2000).
In this approach, they also chose to use a com-
pressed feature set for n-grams and POS n-grams.
We chose similar feature sets to classify rhetorical
questions.
Our work extends these approaches to dialog
act classification by exploring additional features
which are specific to rhetorical question identifi-
cation, such as context n-grams.
</bodyText>
<sectionHeader confidence="0.860899" genericHeader="method">
3 Features for Identifying Rhetorical
Questions
</sectionHeader>
<bodyText confidence="0.999937727272727">
In order to correctly classify rhetorical ques-
tions, we theorize that the choice of words in the
question itself may be an important indicator of
speaker intent. To capture intent in the words
themselves, it makes sense to consider a common
unigram, while a bigram model will likely capture
short phrasal cues. For instance, we might expect
the existence of n-grams such as well or you know
to be highly predictive features of the rhetorical
nature of the question.
Additionally, some linguistic cues are helpful
in identifying rhetorical questions. Strong nega-
tive polarity items (NPIs), also referred to as em-
phatic or even-NPIs in the literature, are consid-
ered definitive markers. Some examples are budge
an inch, in years, give a damn, bat an eye, and
lift a finger (Giannakidou 1999, van Rooy 2003).
Gresillon (1980) notes that a question containing a
modal auxiliary, such as could or would, together
with negation tends to be rhetorical. Certain ex-
pressions such as yet and after all can only ap-
pear in rhetorical questions (Sadock 1971, Sadock
1974). Again, using common n-grams as features
should partially capture the above cues because n-
gram segments of strong NPIs should occur more
frequently.
We also wanted to incorporate common gram-
matical sequences found in rhetorical questions.
To that end, we can consider part of speech (POS)
n-grams to capture common grammatical relations
which are predictive.
Similarly, for rhetorical questions, we expect
context to be highly predictive for correct classi-
fication. For instance, the existence of a question
mark in the subsequent utterance when spoken by
the questioner, will likely be a weak positive cue,
since the speaker may not have been expecting a
response. However, the existence of a question
mark by a different speaker may not be indicative.
This suggests a need to decompose the context-
based feature space by speaker. Similarly, phrases
uttered prior to the question will likely give rise to
a different set of predictive n-grams.
Using these observations, we decided to im-
plement a simple n-gram model incorporating
contextual cues to identify rhetorical questions.
Specifically, we used unigrams, bigrams, POS bi-
grams, and POS trigrams of a question and its im-
mediately preceding and following context as fea-
ture sets. Based on preliminary results, we did not
use trigrams or POS unigrams. POS tags did not
capture sufficient contextual information and tri-
grams were not implemented since the utterances
in our dataset were too small to fully utilize them.
Also, to capture the contextual information, we
</bodyText>
<page confidence="0.989262">
744
</page>
<bodyText confidence="0.999949181818182">
distinguish three distinct categories - questions,
utterances immediately preceding questions, and
utterances immediately following questions. In
order to capture the effect of a feature if it is used
by the same speaker versus a different speaker,
we divided the feature space contextual utter-
ances into four disjoint groups: precedent-same-
speaker, precedent-different-speaker, subsequent-
same-speaker, and subsequent-different-speaker.
Features in each group are all considered indepen-
dently.
</bodyText>
<sectionHeader confidence="0.995093" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.984684">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9953053">
For the experiments, we used the Switchboard
Dialog Act Corpus (Godfrey et al. 1992; Juraf-
sky et al. 1997b), which contains labeled utter-
ances from phone conversations between differ-
ent pairs of people. We preprocessed the data to
contain only the utterances marked as questions
(rhetorical or otherwise), as well as the utterances
immediately preceding and following the ques-
tions. Additionally, connectives like and and but
were marked as t con, the end of conversation was
marked as t empty, and laughter was marked as
t laugh.
After filtering down to questions, we split the
data into 5960 questions in the training set and
2555 questions in the test set. We find the dataset
to be highly skewed with only 128or 5% of the
2555
test instances labeled as rhetorical. Because of
this, a classifier that naively labels all questions as
non-rhetorical would achieve a 94.99% accuracy.
Thus, we chose precision, recall and F1-measure
as more appropriate metrics of our classifier per-
formance. We should note also that our results as-
sume a high level of consistency of the hand anno-
tations from the original taggging of the Switch-
board Corpus. However, based on our observation
and the strict guidelines followed by annotators as
mentioned in Jurafsky et al. (1997a), we are rea-
sonably confident in the reliability of the rhetori-
cal labels.
</bodyText>
<subsectionHeader confidence="0.995693">
4.2 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999983380952381">
We experimented with both Naive Bayes and a
Support Vector Machine (SVM) classifiers. Our
Naive Bayes classifier was smoothed with an add-
alpha Laplacian kernel, where alpha was selected
via cross-validation. For our SVM, to account for
the highly skewed nature of our dataset, we set the
cost-factor based on the ratio of positive (rhetori-
cal) to negative (non-rhetorical) questions in our
training set as in Morik et al. (1999). We tuned
the trade-off between margin and training error via
cross validation over the training set.
In early experiments, Naive Bayes performed
comparably to or outperformed SVM because the
dimensionality of the feature space was relatively
low. However, we found that SVM performed
more robustly over the large range and dimension-
ality of features we employed in the later experi-
ments. Thus, we conducted the main experiments
using SVMLite (Joachims 1999).
As the number of parameters is linear in the
number of feature sets, an exhaustive search
through the space would be intractable. So as to
make this feasible, we employ a greedy approach
to model selection. We make a naive assumption
that parameters of feature sets are independent or
codependent on up to one other feature set in the
same group. Each pair of codependent feature sets
is considered alone while holding other feature
sets fixed. Classifier parameters are also assumed
to be independent for tuning purposes.
In order to optimize search time without sam-
pling the parameter space too coarsely, we em-
ployed an adaptive refinement variant to a tradi-
tional grid search. First, we discretely sampled the
Cartesian product of dependent parameters sam-
pled at regular geometric or arithmetic intervals
between a user-specified minimum and maximum.
We then updated minimum and maximum values
to center around the highest scoring sample and
recursed on the search with the newly downsized
span for a fixed recursion depth d. In practice, we
choose k = 4 and d = 3.
</bodyText>
<subsectionHeader confidence="0.965314">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.999960857142857">
Unigrams, bigrams, POS bigrams, and POS tri-
grams were extracted from the questions and
neighboring utterances as features, based on the
analysis in Section 3. Then, feature selection was
performed as follows.
For all features sets, we considered both uni-
gram and bigram features. All unigrams and bi-
grams in the training data are considered as po-
tential candidates for features. For each feature set
above, we estimated the maximal predictivity over
both rhetorical and non-rhetorical classes, corre-
sponding to using the MLE of P(cln), where n
denotes the n-gram and c is the class. We used
these estimates as a score and select the j n-grams
</bodyText>
<page confidence="0.992738">
745
</page>
<bodyText confidence="0.999788222222222">
with the highest score for each n over each group,
regardless of class, where j was selected via 4-fold
cross validation.
Each feature was then encoded as a simple oc-
currence count within its respective group for a
given exchange. The highest scoring unigrams
and bigrams are as follows: “you”, “do”, “what”,
“to”, “t con”, “do you”, “you know”, “going to”,
“you have”, and “well ,”.
POS features were computed by running a POS
tagger on all exchanges and and then picking the
j-best n-grams as described above. For our exper-
iments, we used the maximum entropy treebank
POS tagger from the NLTK package (Bird et al.
2009) to compute POS bigrams and trigrams.
Lastly, in order to assess the relative value of
question-based and context-based features, we de-
signed the following seven feature sets:
</bodyText>
<listItem confidence="0.999967714285714">
• Question (baseline)
• Precedent
• Subsequent
• Question + Precedent
• Question + Subsequent
• Precedent + Subsequent
• Question + Precedent + Subsequent
</listItem>
<bodyText confidence="0.9973206">
The question-only feature set serves as our
baseline without considering context, whereas the
other feature sets serve to test the power of the
preceding and following context alone and when
paired with features from the question itself.
</bodyText>
<table confidence="0.998441538461539">
Feature set Acc Pre Rec F1 Error 95%
Question 92.41 35.00 60.16 44.25 7.59 ±1.02
Precedent 85.64 12.30 30.47 17.53 14.36 ±1.36
Subsequent 78.98 13.68 60.16 22.29 21.02 ±1.58
Question + 93.82 41.94 60.94 49.68 6.18 ±0.93
Precedent
Question + 93.27 39.52 64.84 49.11 6.73 ±0.97
Subsequent
Precedent+ 84.93 19.62 64.84 30.14 15.07 ±1.38
Subsequent
Question + 94.87 49.03 59.38 53.71 5.13± 0.86
Precedent +
Subsequent
</table>
<tableCaption confidence="0.995648">
Table 1: Experimental results (%)
</tableCaption>
<table confidence="0.9997966">
AC PC Utterance
+ + X: ‘i mean, why not.’
- X: ‘what are you telling that student?’
- + X: ‘t laugh why don’t we do that?’
- X: ‘who, was in that.’
</table>
<tableCaption confidence="0.987104">
Table 2: Classification without Context Features (AC: Actual
Class, P: Predicted Class. X denotes the speaker)
</tableCaption>
<table confidence="0.999848875">
AC PC Utterances
+ + X: ‘t con you give them an f on something that
doesn’t seem that bad to me.’
X: ‘what are you telling that student?’
X: ‘you’re telling them that, hey, you might as well
forget it, you know.’
- X: ‘get homework done,’
X: ‘t con you know, where do you find the time’.
Y:‘well, in the first place it’s not your homework,’
- + X: ‘ha, ha, lots of luck.’
X: ‘is she spayed.’
Y: ‘yeah’.
- Y: ‘t con it says when the conversation is over just
say your good-byes and hang up.’
X: ‘t laugh why don’t we do that?
Y: ‘i, guess so.’
</table>
<tableCaption confidence="0.992184">
Table 3: Classification with Context Features (AC: Actual
Class, PC: Predicted Class. X and Y denote the speakers)
</tableCaption>
<sectionHeader confidence="0.99802" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999944153846154">
Table 1 shows the performance of the feature sets
cross-valided and trained on 5960 questions (with
context) in the Switchboard corpus and tested on
the 2555 remaining questions.
Our results largely reflect our intuition on the
expected utility of our various feature sets. Fea-
tures in the question group prove by far the most
useful single source, while features within the
subsequent prove to be more useful than features
in the precedent. Somewhat surprisingly however,
an Fl-score of 30.14% is achieved by training on
contextual features alone while ignoring any cues
from the question itself, suggesting the power of
context in identifying a question as rhetorical. Ad-
ditionally, one of the highest scoring bigrams is
you know, matching our earlier intuitions.
Some examples of the success and failings of
our system can be found in Table 2 and 3. For
instance, in our question-only feature space, the
phrase what are you telling that student? was in-
correctly classified as non-rhetorical. When the
contextual features were added in, the classifier
correctly identified it as rhetorical as we might ex-
pect. Failure cases of our simple language model
based system can be seen for instance in the false
positive question is she spayed which is inter-
</bodyText>
<page confidence="0.996013">
746
</page>
<bodyText confidence="0.999956333333333">
preted as rhetorical, likely due to the unigram yeah
in the response.
Overall, we achieve our best results when in-
cluding both precedent and subsequent context
along with the question in our feature space. Thus,
our results suggest that incorporating contextual
cues from both directly before and after the ques-
tion itself outperforms classifiers trained on a
naive question-only feature space.
</bodyText>
<subsectionHeader confidence="0.996896">
5.1 Feature Dimensionality
</subsectionHeader>
<bodyText confidence="0.9999938">
After model selection via cross validation, our to-
tal feature space dimensionality varies between
2914 for the precedent only feature set and 16615
for the question + subsequent feature set. Distinct
n-gram and POS n-gram features are considered
for each of same speaker and different speaker for
precedents and subsequents so as to capture the
distinction between the two. Examining the rel-
ative number of features selected for these sub-
feature sets also gives a rough idea of the strength
of the various cues. For instance, same speaker
feature dimensionality tended to be much lower
than different speaker feature dimensionality, sug-
gesting that considering context uttered by the re-
spondent is a better cue as to whether the question
is rhetorical. Additionally, unigrams and bigrams
tend to be more useful features than POS n-grams
for the task of rhetorical question identification, or
at least considering the less common POS n-grams
is not as predictive.
</bodyText>
<subsectionHeader confidence="0.998665">
5.2 Evenly Split Distribution
</subsectionHeader>
<bodyText confidence="0.999505">
As the highly skewed nature of our data does not
allow us to get a good estimate of error rate, we
also tested our feature sets on a subsection of the
dataset with a 50-50 split between rhetorical and
non-rhetorical questions to get a better sense of
the accuracy of our classifier. The results can be
seen in Table 4. Our classifier achieves an accu-
racy of 81% when trained on the questions alone
and 84% when integrating precedent and subse-
quent context. Due to the reduced size of the
evenly split dataset, performing a McNemar’s test
with Edwards’ correction (Edwards 1948) does
not allow us to reject the null hypothesis that the
two experiments do not derive from the same dis-
tribution with 95% confidence (x2 = 1.49 giv-
ing a 2-tailed p value of 0.22). However, over the
whole skewed dataset, we find x2 = 30.74 giv-
ing a 2-tailed p &lt; 0.00001 so we have reason to
believe that with a larger evenly-split dataset inte-
grating context-based features provides a quantifi-
able advantage.
</bodyText>
<table confidence="0.9984196">
Feature set Acc Pre Rec F1 Error 95%
Question 81.25 82.71 78.01 80.29 0.19 ±0.05
Question + 84.38 88.71 78.01 83.02 0.16 ±0.04
Precedent +
Subsequent
</table>
<tableCaption confidence="0.994167">
Table 4: Experimental results (%) on evenly distributed data
(training set size: 670 &amp; test set size: 288)
</tableCaption>
<sectionHeader confidence="0.998401" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999688">
In this paper, we tackle the largely understud-
ied problem of rhetorical question identification.
While the majority of the classification relies on
features extracted from the question itself, adding
in n-gram features from the context improves the
performance. We achieve a 53.71% F1-score by
adding features extracted from the preceding and
the subsequent utterances, which is about a 10%
improvement from a baseline classifier using only
the features from the question itself.
For future work, we would like to employ more
complicated features like the sentiment of the con-
text, and dictionary features based on an NPI lex-
icon. Also, if available, prosodic information like
focus, pauses, and intonation may be useful.
</bodyText>
<sectionHeader confidence="0.998358" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99994925">
We thank Mary Moroney and Andrea Hummel
for helping us identify linguistic characteristics of
rhetorical questions and the anonymous reviewers
for their thoughtful feedback.
</bodyText>
<sectionHeader confidence="0.998245" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99152">
Jeremy Ang, Yang Liu, and Elizabeth Shriberg.
2005. Automatic dialog act segmentation
and classification in multiparty meetings. In
ICASSP (1), pages 1061–1064.
Rajesh Bhatt. 1998. Argument-adjunct asymme-
tries in rhetorical questions. In NELS 29.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural language processing with
Python. O’Reilly Media, Inc.
Allen L. Edwards. 1948. Note on the “correction
for continuity” in testing the significance of the
difference between correlated proportions. In
Psychometrika, 13(3):185–187.
Anastasia Giannakidou. 1999. Affective depen-
dencies In Linguistics and Philosophy, 22(4):
367–421. Springer
</reference>
<page confidence="0.987994">
747
</page>
<reference confidence="0.994758907216494">
John J. Godfrey, Edward C. Holliman, and Jane
McDaniel. 1992. Switchboard: telephone
speech corpus for research and development.
In Acoustics, Speech, and Signal Processing,
1992. ICASSP-92., 1992 IEEE International
Conference on, volume 1, pages 517–520 vol.1.
Sergio Grau, Emilio Sanchis, Maria Jos´e Castro,
David Vilar. 2004. Dialogue act classification
using a Bayesian approach In 9th Conference
Speech and Computer.
Almuth Gresillon. 1980. Zum linguistischen Sta-
tus rhetorischer Fragen InZeitschrift f¨ur ger-
manistische Linguistik, 8(3): 273–289.
Chung-Hye Han. 1998. Deriving the interpreta-
tion of rhetorical questions. In Proceedings of
West Coast Conference in Formal Linguistics,
volume 16, pages 237–253. Citeseer.
T. Joachims. 1999. Making large-scale svm
learning practical. Advances in kernel methods-
support vector learning.
Dan Jurafsky, Rebecca Bates, Noah Coccaro,
Rachel Martin, Marie Meteer, Klaus Ries, Eliz-
abeth Shriberg, Andreas Stolcke, Paul Tay-
lor, Carol V. Ess-Dykema, et al. 1997a. Au-
tomatic detection of discourse structure for
speech recognition and understanding. In Auto-
matic Speech Recognition and Understanding,
1997. Proceedings., 1997 IEEE Workshop on,
pages 88–95. IEEE.
Dan Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997b. Switchboard SWBD-DAMSL
shallow-discourse-function annotation coders
manual. Technical Report Draft 13, University
of Colorado, Institute of Cognitive Science.
Simon Keizer, Anton Nijholt, et al. 2002. Dia-
logue act recognition with bayesian networks
for dutch dialogues. In Proceedings of the 3rd
SIGdial workshop on Discourse and dialogue-
Volume 2, pages 88–94. Association for Com-
putational Linguistics.
Katharina Morik, Peter Brockhausen, and T.
Joachims. 1999. Combining statistical learning
with a knowledge-based approach: a case study
in intensive care monitoring. Technical report,
Technical Report, SFB 475: Komplexit¨atsre-
duktion in Multivariaten Datenstrukturen, Uni-
versit¨at Dortmund.
Norbert Reithinger and Martin Klesen. 1997. Dia-
logue act classification using language models.
In EuroSpeech. Citeseer.
Jerrold M. Saddock. 1971. Queclaratives In Sev-
enth Regional Meeting of the Chicago Linguis-
tic Society, 7: 223–232.
Jerrold M. Saddock. 1974. Toward a linguistic
theory of speech acts Academic Press New
York
Ken Samuel, Sandra Carberry, and K. Vijay-
Shanker. 1999. Automatically selecting useful
phrases for dialogue act tagging. arXiv preprint
cs/9906016.
Ken B. Samuel. 2000. Discourse learning: an
investigation of dialogue act tagging using
transformation-based learning. University of
Delaware.
Elizabeth Shriberg, Andreas Stolcke, Dan Juraf-
sky, Noah Coccaro, Marie Meteer, Rebecca
Bates, Paul Taylor, Klaus Ries, Rachel Martin,
and Carol Van Ess-Dykema. 1998. Can prosody
aid the automatic classification of dialog acts in
conversational speech? Language and speech,
41(3-4):443–492.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-
abeth Shriberg, Rebecca Bates, Dan Jurafsky,
Paul Taylor, Rachel Martin, Carol Van Ess-
Dykema, and Marie Meteer. 2000. Dialogue
act modeling for automatic tagging and recog-
nition of conversational speech. Computational
linguistics, 26(3):339–373.
Robert van Rooy. 2003. Negative polarity items in
questions: Strength as relevance In Journal of
Semantics, 20(3): 239–273. Oxford University
Press.
Anand Venkataraman, Andreas Stolcke, and Eliz-
abeth Shriberg. 2002. Automatic dialog act la-
beling with minimal supervision. In 9th Aus-
tralian International Conference on Speech Sci-
ence and Technology, SST 2002.
Daan Verbree, Rutger Rienks, and Dirk Heylen.
2006. Dialogue-act tagging using smart fea-
ture selection; results on multiple corpora. In
Spoken Language Technology Workshop, 2006.
IEEE, pages 70–73. IEEE.
Volker Warnke, Ralf Kompe, Heinrich Nie-
mann, and Elmar N¨oth. 1997. Integrated di-
alog act segmentation and classification using
prosodic features and language models. In EU-
ROSPEECH.
</reference>
<page confidence="0.974305">
748
</page>
<reference confidence="0.9992962">
Nick Webb, Mark Hepple, and Yorik Wilks.
2005. Dialogue act classification based on
intra-utterance features. In Proceedings of the
AAAI Workshop on Spoken Language Under-
standing. Citeseer.
Klaus Zechner. 2002. Automatic summarization
of open-domain multiparty dialogues in diverse
genres. Computational Linguistics, 28(4):447–
485.
Matthias Zimmerman, Yang Liu, Elizabeth
Shriberg, and Andreas Stolcke. 2005. A*
based joint segentation and classification of di-
alog acts in multiparty meetings. In Automatic
Speech Recognition and Understanding, 2005
IEEE Workshop on, pages 215–219. IEEE.
</reference>
<page confidence="0.998734">
749
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.100793">
<title confidence="0.99984">Automatic Identification of Rhetorical Questions</title>
<author confidence="0.981179">Shohini</author>
<affiliation confidence="0.978214">Dept. of Cornell</affiliation>
<address confidence="0.998245">Ithaca, NY, USA</address>
<author confidence="0.88352">Jeremy</author>
<affiliation confidence="0.959433">Dept. of Computer Cornell</affiliation>
<address confidence="0.997925">Ithaca, NY, USA</address>
<email confidence="0.498422">Elana</email>
<affiliation confidence="0.966172">Dept. of Cornell</affiliation>
<address confidence="0.998782">Ithaca, NY, USA</address>
<email confidence="0.75629">Joonsuk</email>
<affiliation confidence="0.96511">Dept. of Computer Cornell</affiliation>
<address confidence="0.76186">Ithaca, NY,</address>
<email confidence="0.771455">jpark@cs.cornell.edujmc677,</email>
<abstract confidence="0.99921765">A question may be asked not only to elicit information, but also to make a statement. Questions serving the latter purpose, called rhetorical questions, are often lexically and syntactically indistinguishable from other types of questions. Still, it is desirable to be able to identify rhetorical questions, as it is relevant for many NLP tasks, including information extraction and text summarization. In this paper, we explore the largely understudied problem of rhetorical question identification. Specifically, we present a simple n-gram based language model to classify rhetorical questions in the Switchboard Dialogue Act Corpus. We find that a special treatment of rhetorical questions which incorporates contextual information achieves the highest performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jeremy Ang</author>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Automatic dialog act segmentation and classification in multiparty meetings.</title>
<date>2005</date>
<booktitle>In ICASSP (1),</booktitle>
<pages>1061--1064</pages>
<marker>Ang, Liu, Shriberg, 2005</marker>
<rawString>Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005. Automatic dialog act segmentation and classification in multiparty meetings. In ICASSP (1), pages 1061–1064.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajesh Bhatt</author>
</authors>
<title>Argument-adjunct asymmetries in rhetorical questions.</title>
<date>1998</date>
<booktitle>In NELS 29.</booktitle>
<contexts>
<context position="1644" citStr="Bhatt (1998)" startWordPosition="236" endWordPosition="237"> Act Corpus. We find that a special treatment of rhetorical questions which incorporates contextual information achieves the highest performance. 1 Introduction Rhetorical questions frequently appear in everyday conversations. A rhetorical question is functionally different from other types of questions in that it is expressing a statement, rather than seeking information. Thus, rhetorical questions must be identified to fully capture the meaning of an utterance. This is not an easy task; despite their drastic functional differences, rhetorical questions are formulated like regular questions. Bhatt (1998) states that in principle, a given question can be interpreted as either an information seeking question or as a rhetorical question and that intonation can be used to identify the interpretation intended by the speaker. For instance, consider the following example: (1) Did I tell you that writing a dissertation was easy? Just from reading the text, it is difficult to tell whether the speaker is asking an informational question or whether they are implying that they did not say that writing a dissertation was easy. However, according to our observation, which forms the basis of this work, ther</context>
</contexts>
<marker>Bhatt, 1998</marker>
<rawString>Rajesh Bhatt. 1998. Argument-adjunct asymmetries in rhetorical questions. In NELS 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural language processing with Python.</title>
<date>2009</date>
<publisher>O’Reilly Media, Inc.</publisher>
<contexts>
<context position="13687" citStr="Bird et al. 2009" startWordPosition="2169" endWordPosition="2172"> the highest score for each n over each group, regardless of class, where j was selected via 4-fold cross validation. Each feature was then encoded as a simple occurrence count within its respective group for a given exchange. The highest scoring unigrams and bigrams are as follows: “you”, “do”, “what”, “to”, “t con”, “do you”, “you know”, “going to”, “you have”, and “well ,”. POS features were computed by running a POS tagger on all exchanges and and then picking the j-best n-grams as described above. For our experiments, we used the maximum entropy treebank POS tagger from the NLTK package (Bird et al. 2009) to compute POS bigrams and trigrams. Lastly, in order to assess the relative value of question-based and context-based features, we designed the following seven feature sets: • Question (baseline) • Precedent • Subsequent • Question + Precedent • Question + Subsequent • Precedent + Subsequent • Question + Precedent + Subsequent The question-only feature set serves as our baseline without considering context, whereas the other feature sets serve to test the power of the preceding and following context alone and when paired with features from the question itself. Feature set Acc Pre Rec F1 Erro</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python. O’Reilly Media, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allen L Edwards</author>
</authors>
<title>Note on the “correction for continuity” in testing the significance of the difference between correlated proportions.</title>
<date>1948</date>
<journal>In Psychometrika,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="18892" citStr="Edwards 1948" startWordPosition="3044" endWordPosition="3045">tive. 5.2 Evenly Split Distribution As the highly skewed nature of our data does not allow us to get a good estimate of error rate, we also tested our feature sets on a subsection of the dataset with a 50-50 split between rhetorical and non-rhetorical questions to get a better sense of the accuracy of our classifier. The results can be seen in Table 4. Our classifier achieves an accuracy of 81% when trained on the questions alone and 84% when integrating precedent and subsequent context. Due to the reduced size of the evenly split dataset, performing a McNemar’s test with Edwards’ correction (Edwards 1948) does not allow us to reject the null hypothesis that the two experiments do not derive from the same distribution with 95% confidence (x2 = 1.49 giving a 2-tailed p value of 0.22). However, over the whole skewed dataset, we find x2 = 30.74 giving a 2-tailed p &lt; 0.00001 so we have reason to believe that with a larger evenly-split dataset integrating context-based features provides a quantifiable advantage. Feature set Acc Pre Rec F1 Error 95% Question 81.25 82.71 78.01 80.29 0.19 ±0.05 Question + 84.38 88.71 78.01 83.02 0.16 ±0.04 Precedent + Subsequent Table 4: Experimental results (%) on eve</context>
</contexts>
<marker>Edwards, 1948</marker>
<rawString>Allen L. Edwards. 1948. Note on the “correction for continuity” in testing the significance of the difference between correlated proportions. In Psychometrika, 13(3):185–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anastasia Giannakidou</author>
</authors>
<date>1999</date>
<booktitle>Affective dependencies In Linguistics and Philosophy,</booktitle>
<volume>22</volume>
<issue>4</issue>
<pages>367--421</pages>
<publisher>Springer</publisher>
<contexts>
<context position="6768" citStr="Giannakidou 1999" startWordPosition="1051" endWordPosition="1052"> intent in the words themselves, it makes sense to consider a common unigram, while a bigram model will likely capture short phrasal cues. For instance, we might expect the existence of n-grams such as well or you know to be highly predictive features of the rhetorical nature of the question. Additionally, some linguistic cues are helpful in identifying rhetorical questions. Strong negative polarity items (NPIs), also referred to as emphatic or even-NPIs in the literature, are considered definitive markers. Some examples are budge an inch, in years, give a damn, bat an eye, and lift a finger (Giannakidou 1999, van Rooy 2003). Gresillon (1980) notes that a question containing a modal auxiliary, such as could or would, together with negation tends to be rhetorical. Certain expressions such as yet and after all can only appear in rhetorical questions (Sadock 1971, Sadock 1974). Again, using common n-grams as features should partially capture the above cues because ngram segments of strong NPIs should occur more frequently. We also wanted to incorporate common grammatical sequences found in rhetorical questions. To that end, we can consider part of speech (POS) n-grams to capture common grammatical re</context>
</contexts>
<marker>Giannakidou, 1999</marker>
<rawString>Anastasia Giannakidou. 1999. Affective dependencies In Linguistics and Philosophy, 22(4): 367–421. Springer</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>517--520</pages>
<contexts>
<context position="9170" citStr="Godfrey et al. 1992" startWordPosition="1419" endWordPosition="1422">ion, we 744 distinguish three distinct categories - questions, utterances immediately preceding questions, and utterances immediately following questions. In order to capture the effect of a feature if it is used by the same speaker versus a different speaker, we divided the feature space contextual utterances into four disjoint groups: precedent-samespeaker, precedent-different-speaker, subsequentsame-speaker, and subsequent-different-speaker. Features in each group are all considered independently. 4 Experimental Setup 4.1 Data For the experiments, we used the Switchboard Dialog Act Corpus (Godfrey et al. 1992; Jurafsky et al. 1997b), which contains labeled utterances from phone conversations between different pairs of people. We preprocessed the data to contain only the utterances marked as questions (rhetorical or otherwise), as well as the utterances immediately preceding and following the questions. Additionally, connectives like and and but were marked as t con, the end of conversation was marked as t empty, and laughter was marked as t laugh. After filtering down to questions, we split the data into 5960 questions in the training set and 2555 questions in the test set. We find the dataset to </context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. Switchboard: telephone speech corpus for research and development. In Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on, volume 1, pages 517–520 vol.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Grau</author>
<author>Emilio Sanchis</author>
<author>Maria Jos´e Castro</author>
<author>David Vilar</author>
</authors>
<title>Dialogue act classification using a Bayesian approach</title>
<date>2004</date>
<booktitle>In 9th Conference Speech and Computer.</booktitle>
<contexts>
<context position="4438" citStr="Grau et al. (2004)" startWordPosition="674" endWordPosition="677">ral Language Processing (Short Papers), pages 743–749, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics in n-gram features from the context improves the performance. An Fl-score of 53.71% is achieved by adding features extracted from the preceding and subsequent utterances, which is about a 10% improvement from a baseline classifier using only the features from the question itself. 2 Related work Jurafsky et al. (1997a) and Reithinger and Klesen (1997) used n-gram language modeling on the Switchboard and Verbmobil corpora respectively to classify dialog acts. Grau et al. (2004) uses a Bayesian approach with n-grams to categorize dialog acts. We also employ a similar language model to achieve our results. Samuel et al. (1999) used transformation-based learning on the Verbmobil corpus over a number of utterance features such as utterance length, speaker turn, and the dialog act tags of adjacent utterances. Stolcke et al. (2000) utilized Hidden Markov Models on the Switchboard corpus and used word order within utterances and the order of dialog acts over utterances. Zechner (2002) worked on automatic summarization of open-domain spoken dialogues i.e., important pieces </context>
</contexts>
<marker>Grau, Sanchis, Castro, Vilar, 2004</marker>
<rawString>Sergio Grau, Emilio Sanchis, Maria Jos´e Castro, David Vilar. 2004. Dialogue act classification using a Bayesian approach In 9th Conference Speech and Computer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almuth Gresillon</author>
</authors>
<date>1980</date>
<booktitle>Zum linguistischen Status rhetorischer Fragen InZeitschrift f¨ur germanistische Linguistik,</booktitle>
<volume>8</volume>
<issue>3</issue>
<pages>273--289</pages>
<contexts>
<context position="6802" citStr="Gresillon (1980)" startWordPosition="1056" endWordPosition="1057">t makes sense to consider a common unigram, while a bigram model will likely capture short phrasal cues. For instance, we might expect the existence of n-grams such as well or you know to be highly predictive features of the rhetorical nature of the question. Additionally, some linguistic cues are helpful in identifying rhetorical questions. Strong negative polarity items (NPIs), also referred to as emphatic or even-NPIs in the literature, are considered definitive markers. Some examples are budge an inch, in years, give a damn, bat an eye, and lift a finger (Giannakidou 1999, van Rooy 2003). Gresillon (1980) notes that a question containing a modal auxiliary, such as could or would, together with negation tends to be rhetorical. Certain expressions such as yet and after all can only appear in rhetorical questions (Sadock 1971, Sadock 1974). Again, using common n-grams as features should partially capture the above cues because ngram segments of strong NPIs should occur more frequently. We also wanted to incorporate common grammatical sequences found in rhetorical questions. To that end, we can consider part of speech (POS) n-grams to capture common grammatical relations which are predictive. Simi</context>
</contexts>
<marker>Gresillon, 1980</marker>
<rawString>Almuth Gresillon. 1980. Zum linguistischen Status rhetorischer Fragen InZeitschrift f¨ur germanistische Linguistik, 8(3): 273–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-Hye Han</author>
</authors>
<title>Deriving the interpretation of rhetorical questions.</title>
<date>1998</date>
<booktitle>In Proceedings of West Coast Conference in Formal Linguistics,</booktitle>
<volume>16</volume>
<pages>237--253</pages>
<publisher>Citeseer.</publisher>
<marker>Han, 1998</marker>
<rawString>Chung-Hye Han. 1998. Deriving the interpretation of rhetorical questions. In Proceedings of West Coast Conference in Formal Linguistics, volume 16, pages 237–253. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale svm learning practical. Advances in kernel methodssupport vector learning.</title>
<date>1999</date>
<contexts>
<context position="11335" citStr="Joachims 1999" startWordPosition="1774" endWordPosition="1775">et, we set the cost-factor based on the ratio of positive (rhetorical) to negative (non-rhetorical) questions in our training set as in Morik et al. (1999). We tuned the trade-off between margin and training error via cross validation over the training set. In early experiments, Naive Bayes performed comparably to or outperformed SVM because the dimensionality of the feature space was relatively low. However, we found that SVM performed more robustly over the large range and dimensionality of features we employed in the later experiments. Thus, we conducted the main experiments using SVMLite (Joachims 1999). As the number of parameters is linear in the number of feature sets, an exhaustive search through the space would be intractable. So as to make this feasible, we employ a greedy approach to model selection. We make a naive assumption that parameters of feature sets are independent or codependent on up to one other feature set in the same group. Each pair of codependent feature sets is considered alone while holding other feature sets fixed. Classifier parameters are also assumed to be independent for tuning purposes. In order to optimize search time without sampling the parameter space too c</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale svm learning practical. Advances in kernel methodssupport vector learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Rebecca Bates</author>
<author>Noah Coccaro</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
<author>Klaus Ries</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Paul Taylor</author>
<author>Carol V Ess-Dykema</author>
</authors>
<title>Automatic detection of discourse structure for speech recognition and understanding.</title>
<date>1997</date>
<booktitle>In Automatic Speech Recognition and Understanding,</booktitle>
<pages>88--95</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4275" citStr="Jurafsky et al. (1997" startWordPosition="649" endWordPosition="652">ion 3 for more details. 743 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 743–749, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics in n-gram features from the context improves the performance. An Fl-score of 53.71% is achieved by adding features extracted from the preceding and subsequent utterances, which is about a 10% improvement from a baseline classifier using only the features from the question itself. 2 Related work Jurafsky et al. (1997a) and Reithinger and Klesen (1997) used n-gram language modeling on the Switchboard and Verbmobil corpora respectively to classify dialog acts. Grau et al. (2004) uses a Bayesian approach with n-grams to categorize dialog acts. We also employ a similar language model to achieve our results. Samuel et al. (1999) used transformation-based learning on the Verbmobil corpus over a number of utterance features such as utterance length, speaker turn, and the dialog act tags of adjacent utterances. Stolcke et al. (2000) utilized Hidden Markov Models on the Switchboard corpus and used word order withi</context>
<context position="9192" citStr="Jurafsky et al. 1997" startWordPosition="1423" endWordPosition="1427">sh three distinct categories - questions, utterances immediately preceding questions, and utterances immediately following questions. In order to capture the effect of a feature if it is used by the same speaker versus a different speaker, we divided the feature space contextual utterances into four disjoint groups: precedent-samespeaker, precedent-different-speaker, subsequentsame-speaker, and subsequent-different-speaker. Features in each group are all considered independently. 4 Experimental Setup 4.1 Data For the experiments, we used the Switchboard Dialog Act Corpus (Godfrey et al. 1992; Jurafsky et al. 1997b), which contains labeled utterances from phone conversations between different pairs of people. We preprocessed the data to contain only the utterances marked as questions (rhetorical or otherwise), as well as the utterances immediately preceding and following the questions. Additionally, connectives like and and but were marked as t con, the end of conversation was marked as t empty, and laughter was marked as t laugh. After filtering down to questions, we split the data into 5960 questions in the training set and 2555 questions in the test set. We find the dataset to be highly skewed with </context>
</contexts>
<marker>Jurafsky, Bates, Coccaro, Martin, Meteer, Ries, Shriberg, Stolcke, Taylor, Ess-Dykema, 1997</marker>
<rawString>Dan Jurafsky, Rebecca Bates, Noah Coccaro, Rachel Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg, Andreas Stolcke, Paul Taylor, Carol V. Ess-Dykema, et al. 1997a. Automatic detection of discourse structure for speech recognition and understanding. In Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on, pages 88–95. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Debra Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual.</title>
<date>1997</date>
<tech>Technical Report Draft 13,</tech>
<institution>University of Colorado, Institute of Cognitive Science.</institution>
<contexts>
<context position="4275" citStr="Jurafsky et al. (1997" startWordPosition="649" endWordPosition="652">ion 3 for more details. 743 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 743–749, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics in n-gram features from the context improves the performance. An Fl-score of 53.71% is achieved by adding features extracted from the preceding and subsequent utterances, which is about a 10% improvement from a baseline classifier using only the features from the question itself. 2 Related work Jurafsky et al. (1997a) and Reithinger and Klesen (1997) used n-gram language modeling on the Switchboard and Verbmobil corpora respectively to classify dialog acts. Grau et al. (2004) uses a Bayesian approach with n-grams to categorize dialog acts. We also employ a similar language model to achieve our results. Samuel et al. (1999) used transformation-based learning on the Verbmobil corpus over a number of utterance features such as utterance length, speaker turn, and the dialog act tags of adjacent utterances. Stolcke et al. (2000) utilized Hidden Markov Models on the Switchboard corpus and used word order withi</context>
<context position="9192" citStr="Jurafsky et al. 1997" startWordPosition="1423" endWordPosition="1427">sh three distinct categories - questions, utterances immediately preceding questions, and utterances immediately following questions. In order to capture the effect of a feature if it is used by the same speaker versus a different speaker, we divided the feature space contextual utterances into four disjoint groups: precedent-samespeaker, precedent-different-speaker, subsequentsame-speaker, and subsequent-different-speaker. Features in each group are all considered independently. 4 Experimental Setup 4.1 Data For the experiments, we used the Switchboard Dialog Act Corpus (Godfrey et al. 1992; Jurafsky et al. 1997b), which contains labeled utterances from phone conversations between different pairs of people. We preprocessed the data to contain only the utterances marked as questions (rhetorical or otherwise), as well as the utterances immediately preceding and following the questions. Additionally, connectives like and and but were marked as t con, the end of conversation was marked as t empty, and laughter was marked as t laugh. After filtering down to questions, we split the data into 5960 questions in the training set and 2555 questions in the test set. We find the dataset to be highly skewed with </context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca. 1997b. Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual. Technical Report Draft 13, University of Colorado, Institute of Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Keizer</author>
<author>Anton Nijholt</author>
</authors>
<title>Dialogue act recognition with bayesian networks for dutch dialogues.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd SIGdial workshop on Discourse and dialogueVolume 2,</booktitle>
<pages>88--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Keizer, Nijholt, 2002</marker>
<rawString>Simon Keizer, Anton Nijholt, et al. 2002. Dialogue act recognition with bayesian networks for dutch dialogues. In Proceedings of the 3rd SIGdial workshop on Discourse and dialogueVolume 2, pages 88–94. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Morik</author>
<author>Peter Brockhausen</author>
<author>T Joachims</author>
</authors>
<title>Combining statistical learning with a knowledge-based approach: a case study in intensive care monitoring.</title>
<date>1999</date>
<tech>Technical report, Technical Report, SFB 475:</tech>
<institution>Komplexit¨atsreduktion in Multivariaten Datenstrukturen, Universit¨at Dortmund.</institution>
<contexts>
<context position="10876" citStr="Morik et al. (1999)" startWordPosition="1701" endWordPosition="1704">tion and the strict guidelines followed by annotators as mentioned in Jurafsky et al. (1997a), we are reasonably confident in the reliability of the rhetorical labels. 4.2 Learning Algorithm We experimented with both Naive Bayes and a Support Vector Machine (SVM) classifiers. Our Naive Bayes classifier was smoothed with an addalpha Laplacian kernel, where alpha was selected via cross-validation. For our SVM, to account for the highly skewed nature of our dataset, we set the cost-factor based on the ratio of positive (rhetorical) to negative (non-rhetorical) questions in our training set as in Morik et al. (1999). We tuned the trade-off between margin and training error via cross validation over the training set. In early experiments, Naive Bayes performed comparably to or outperformed SVM because the dimensionality of the feature space was relatively low. However, we found that SVM performed more robustly over the large range and dimensionality of features we employed in the later experiments. Thus, we conducted the main experiments using SVMLite (Joachims 1999). As the number of parameters is linear in the number of feature sets, an exhaustive search through the space would be intractable. So as to </context>
</contexts>
<marker>Morik, Brockhausen, Joachims, 1999</marker>
<rawString>Katharina Morik, Peter Brockhausen, and T. Joachims. 1999. Combining statistical learning with a knowledge-based approach: a case study in intensive care monitoring. Technical report, Technical Report, SFB 475: Komplexit¨atsreduktion in Multivariaten Datenstrukturen, Universit¨at Dortmund.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Reithinger</author>
<author>Martin Klesen</author>
</authors>
<title>Dialogue act classification using language models.</title>
<date>1997</date>
<booktitle>In EuroSpeech. Citeseer.</booktitle>
<contexts>
<context position="4310" citStr="Reithinger and Klesen (1997)" startWordPosition="654" endWordPosition="658">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 743–749, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics in n-gram features from the context improves the performance. An Fl-score of 53.71% is achieved by adding features extracted from the preceding and subsequent utterances, which is about a 10% improvement from a baseline classifier using only the features from the question itself. 2 Related work Jurafsky et al. (1997a) and Reithinger and Klesen (1997) used n-gram language modeling on the Switchboard and Verbmobil corpora respectively to classify dialog acts. Grau et al. (2004) uses a Bayesian approach with n-grams to categorize dialog acts. We also employ a similar language model to achieve our results. Samuel et al. (1999) used transformation-based learning on the Verbmobil corpus over a number of utterance features such as utterance length, speaker turn, and the dialog act tags of adjacent utterances. Stolcke et al. (2000) utilized Hidden Markov Models on the Switchboard corpus and used word order within utterances and the order of dialo</context>
</contexts>
<marker>Reithinger, Klesen, 1997</marker>
<rawString>Norbert Reithinger and Martin Klesen. 1997. Dialogue act classification using language models. In EuroSpeech. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold M Saddock</author>
</authors>
<date>1971</date>
<booktitle>Queclaratives In Seventh Regional Meeting of the Chicago Linguistic Society,</booktitle>
<volume>7</volume>
<pages>223--232</pages>
<marker>Saddock, 1971</marker>
<rawString>Jerrold M. Saddock. 1971. Queclaratives In Seventh Regional Meeting of the Chicago Linguistic Society, 7: 223–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold M Saddock</author>
</authors>
<title>Toward a linguistic theory of speech acts</title>
<date>1974</date>
<publisher>Academic Press</publisher>
<location>New York</location>
<marker>Saddock, 1974</marker>
<rawString>Jerrold M. Saddock. 1974. Toward a linguistic theory of speech acts Academic Press New York</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Samuel</author>
<author>Sandra Carberry</author>
<author>K VijayShanker</author>
</authors>
<title>Automatically selecting useful phrases for dialogue act tagging. arXiv preprint cs/9906016.</title>
<date>1999</date>
<contexts>
<context position="4588" citStr="Samuel et al. (1999)" startWordPosition="699" endWordPosition="702">features from the context improves the performance. An Fl-score of 53.71% is achieved by adding features extracted from the preceding and subsequent utterances, which is about a 10% improvement from a baseline classifier using only the features from the question itself. 2 Related work Jurafsky et al. (1997a) and Reithinger and Klesen (1997) used n-gram language modeling on the Switchboard and Verbmobil corpora respectively to classify dialog acts. Grau et al. (2004) uses a Bayesian approach with n-grams to categorize dialog acts. We also employ a similar language model to achieve our results. Samuel et al. (1999) used transformation-based learning on the Verbmobil corpus over a number of utterance features such as utterance length, speaker turn, and the dialog act tags of adjacent utterances. Stolcke et al. (2000) utilized Hidden Markov Models on the Switchboard corpus and used word order within utterances and the order of dialog acts over utterances. Zechner (2002) worked on automatic summarization of open-domain spoken dialogues i.e., important pieces of information are found in the back and forth of a dialogue that is absent in a written piece. Webb et al. (2005) used intra-utterance features in th</context>
</contexts>
<marker>Samuel, Carberry, VijayShanker, 1999</marker>
<rawString>Ken Samuel, Sandra Carberry, and K. VijayShanker. 1999. Automatically selecting useful phrases for dialogue act tagging. arXiv preprint cs/9906016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken B Samuel</author>
</authors>
<title>Discourse learning: an investigation of dialogue act tagging using transformation-based learning.</title>
<date>2000</date>
<institution>University of Delaware.</institution>
<contexts>
<context position="5592" citStr="Samuel (2000)" startWordPosition="862" endWordPosition="863">ion of open-domain spoken dialogues i.e., important pieces of information are found in the back and forth of a dialogue that is absent in a written piece. Webb et al. (2005) used intra-utterance features in the Switchboard corpus and calculated n-grams for each utterance of all dialogue acts. For each ngram, they computed the maximal predictivity i.e., its highest predictivity value within any dialogue act category. We utilized a similar metric for ngram selection. Verbree et al. (2006) constructed their baseline for three different corpora using the performance of the LIT set, as proposed by Samuel (2000). In this approach, they also chose to use a compressed feature set for n-grams and POS n-grams. We chose similar feature sets to classify rhetorical questions. Our work extends these approaches to dialog act classification by exploring additional features which are specific to rhetorical question identification, such as context n-grams. 3 Features for Identifying Rhetorical Questions In order to correctly classify rhetorical questions, we theorize that the choice of words in the question itself may be an important indicator of speaker intent. To capture intent in the words themselves, it make</context>
</contexts>
<marker>Samuel, 2000</marker>
<rawString>Ken B. Samuel. 2000. Discourse learning: an investigation of dialogue act tagging using transformation-based learning. University of Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Dan Jurafsky</author>
<author>Noah Coccaro</author>
<author>Marie Meteer</author>
<author>Rebecca Bates</author>
<author>Paul Taylor</author>
<author>Klaus Ries</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
</authors>
<title>Can prosody aid the automatic classification of dialog acts in conversational speech? Language and speech,</title>
<date>1998</date>
<pages>41--3</pages>
<marker>Shriberg, Stolcke, Jurafsky, Coccaro, Meteer, Bates, Taylor, Ries, Martin, Van Ess-Dykema, 1998</marker>
<rawString>Elizabeth Shriberg, Andreas Stolcke, Dan Jurafsky, Noah Coccaro, Marie Meteer, Rebecca Bates, Paul Taylor, Klaus Ries, Rachel Martin, and Carol Van Ess-Dykema. 1998. Can prosody aid the automatic classification of dialog acts in conversational speech? Language and speech, 41(3-4):443–492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Dan Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van EssDykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational linguistics,</journal>
<pages>26--3</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van EssDykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Dan Jurafsky, Paul Taylor, Rachel Martin, Carol Van EssDykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert van Rooy</author>
</authors>
<title>Negative polarity items in questions: Strength as relevance In</title>
<date>2003</date>
<journal>Journal of Semantics,</journal>
<volume>20</volume>
<issue>3</issue>
<pages>239--273</pages>
<publisher>Oxford University Press.</publisher>
<marker>van Rooy, 2003</marker>
<rawString>Robert van Rooy. 2003. Negative polarity items in questions: Strength as relevance In Journal of Semantics, 20(3): 239–273. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Venkataraman</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Automatic dialog act labeling with minimal supervision.</title>
<date>2002</date>
<booktitle>In 9th Australian International Conference on Speech Science and Technology, SST</booktitle>
<marker>Venkataraman, Stolcke, Shriberg, 2002</marker>
<rawString>Anand Venkataraman, Andreas Stolcke, and Elizabeth Shriberg. 2002. Automatic dialog act labeling with minimal supervision. In 9th Australian International Conference on Speech Science and Technology, SST 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daan Verbree</author>
<author>Rutger Rienks</author>
<author>Dirk Heylen</author>
</authors>
<title>Dialogue-act tagging using smart feature selection; results on multiple corpora.</title>
<date>2006</date>
<booktitle>In Spoken Language Technology Workshop,</booktitle>
<pages>70--73</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="5470" citStr="Verbree et al. (2006)" startWordPosition="841" endWordPosition="844"> and used word order within utterances and the order of dialog acts over utterances. Zechner (2002) worked on automatic summarization of open-domain spoken dialogues i.e., important pieces of information are found in the back and forth of a dialogue that is absent in a written piece. Webb et al. (2005) used intra-utterance features in the Switchboard corpus and calculated n-grams for each utterance of all dialogue acts. For each ngram, they computed the maximal predictivity i.e., its highest predictivity value within any dialogue act category. We utilized a similar metric for ngram selection. Verbree et al. (2006) constructed their baseline for three different corpora using the performance of the LIT set, as proposed by Samuel (2000). In this approach, they also chose to use a compressed feature set for n-grams and POS n-grams. We chose similar feature sets to classify rhetorical questions. Our work extends these approaches to dialog act classification by exploring additional features which are specific to rhetorical question identification, such as context n-grams. 3 Features for Identifying Rhetorical Questions In order to correctly classify rhetorical questions, we theorize that the choice of words </context>
</contexts>
<marker>Verbree, Rienks, Heylen, 2006</marker>
<rawString>Daan Verbree, Rutger Rienks, and Dirk Heylen. 2006. Dialogue-act tagging using smart feature selection; results on multiple corpora. In Spoken Language Technology Workshop, 2006. IEEE, pages 70–73. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volker Warnke</author>
<author>Ralf Kompe</author>
<author>Heinrich Niemann</author>
<author>Elmar N¨oth</author>
</authors>
<title>Integrated dialog act segmentation and classification using prosodic features and language models.</title>
<date>1997</date>
<booktitle>In EUROSPEECH.</booktitle>
<marker>Warnke, Kompe, Niemann, N¨oth, 1997</marker>
<rawString>Volker Warnke, Ralf Kompe, Heinrich Niemann, and Elmar N¨oth. 1997. Integrated dialog act segmentation and classification using prosodic features and language models. In EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Webb</author>
<author>Mark Hepple</author>
<author>Yorik Wilks</author>
</authors>
<title>Dialogue act classification based on intra-utterance features.</title>
<date>2005</date>
<booktitle>In Proceedings of the AAAI Workshop on Spoken Language Understanding. Citeseer.</booktitle>
<contexts>
<context position="5152" citStr="Webb et al. (2005)" startWordPosition="792" endWordPosition="795">age model to achieve our results. Samuel et al. (1999) used transformation-based learning on the Verbmobil corpus over a number of utterance features such as utterance length, speaker turn, and the dialog act tags of adjacent utterances. Stolcke et al. (2000) utilized Hidden Markov Models on the Switchboard corpus and used word order within utterances and the order of dialog acts over utterances. Zechner (2002) worked on automatic summarization of open-domain spoken dialogues i.e., important pieces of information are found in the back and forth of a dialogue that is absent in a written piece. Webb et al. (2005) used intra-utterance features in the Switchboard corpus and calculated n-grams for each utterance of all dialogue acts. For each ngram, they computed the maximal predictivity i.e., its highest predictivity value within any dialogue act category. We utilized a similar metric for ngram selection. Verbree et al. (2006) constructed their baseline for three different corpora using the performance of the LIT set, as proposed by Samuel (2000). In this approach, they also chose to use a compressed feature set for n-grams and POS n-grams. We chose similar feature sets to classify rhetorical questions.</context>
</contexts>
<marker>Webb, Hepple, Wilks, 2005</marker>
<rawString>Nick Webb, Mark Hepple, and Yorik Wilks. 2005. Dialogue act classification based on intra-utterance features. In Proceedings of the AAAI Workshop on Spoken Language Understanding. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
</authors>
<title>Automatic summarization of open-domain multiparty dialogues in diverse genres.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>485</pages>
<contexts>
<context position="4948" citStr="Zechner (2002)" startWordPosition="759" endWordPosition="761">deling on the Switchboard and Verbmobil corpora respectively to classify dialog acts. Grau et al. (2004) uses a Bayesian approach with n-grams to categorize dialog acts. We also employ a similar language model to achieve our results. Samuel et al. (1999) used transformation-based learning on the Verbmobil corpus over a number of utterance features such as utterance length, speaker turn, and the dialog act tags of adjacent utterances. Stolcke et al. (2000) utilized Hidden Markov Models on the Switchboard corpus and used word order within utterances and the order of dialog acts over utterances. Zechner (2002) worked on automatic summarization of open-domain spoken dialogues i.e., important pieces of information are found in the back and forth of a dialogue that is absent in a written piece. Webb et al. (2005) used intra-utterance features in the Switchboard corpus and calculated n-grams for each utterance of all dialogue acts. For each ngram, they computed the maximal predictivity i.e., its highest predictivity value within any dialogue act category. We utilized a similar metric for ngram selection. Verbree et al. (2006) constructed their baseline for three different corpora using the performance </context>
</contexts>
<marker>Zechner, 2002</marker>
<rawString>Klaus Zechner. 2002. Automatic summarization of open-domain multiparty dialogues in diverse genres. Computational Linguistics, 28(4):447– 485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Zimmerman</author>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
</authors>
<title>A* based joint segentation and classification of dialog acts in multiparty meetings. In Automatic Speech Recognition and Understanding,</title>
<date>2005</date>
<booktitle>IEEE Workshop on,</booktitle>
<pages>215--219</pages>
<publisher>IEEE.</publisher>
<marker>Zimmerman, Liu, Shriberg, Stolcke, 2005</marker>
<rawString>Matthias Zimmerman, Yang Liu, Elizabeth Shriberg, and Andreas Stolcke. 2005. A* based joint segentation and classification of dialog acts in multiparty meetings. In Automatic Speech Recognition and Understanding, 2005 IEEE Workshop on, pages 215–219. IEEE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>