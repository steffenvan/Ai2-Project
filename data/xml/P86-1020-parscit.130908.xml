<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.452787428571429" genericHeader="method">
BULK PROCESSING OF TEXT ON A MASSIVELY
PARALLEL COMPUTER
Gary W. Sabot
Thinking Machines Corporation
245 First St.
Cambridge, MA 02142
Abstract
</sectionHeader>
<bodyText confidence="0.99824225">
Dictionary lookup is a computational activity
that can be greatly accelerated when performed
on large amounts of text by a parallel computer
such as the Connection Machine&apos;m Computer
(CM). Several algorithms for parallel dictionary
lookup are discussed, including one that allows
the CM to lookup words at a rate 450 times that
of lookup on a Symbolics 3600 Lisp Machine.
</bodyText>
<sectionHeader confidence="0.904327" genericHeader="method">
1 An Overview of the Dictionary
Problem
</sectionHeader>
<bodyText confidence="0.999919772727273">
This paper will discuss one of the text processing prob-
lems that was encountered during the implementation of
the CM-Indexer, a natural language processing program
that runs on the Connection Machine (CM). The prob-
lem is that of parallel dictionary lookup: given both
a dictionary and a text consisting of many thousands
of words, how can the appropriate definitions be dis-
tributed to the words in the text as rapidly as possible?
A parallel dictionary lookup algorithm that makes ef-
ficient use of the CM hardware was discovered and is
described in this paper.
It is clear that there are many natural language pro-
cessing applications in which such a dictionary algo-
rithm is necessary. Indexing and searching of databases
consisting of unformatted natural language text is one
such application. The proliferation of personal comput-
ers, the widespread use of electronic memos and elec-
tronic mail in large corporations, and the CD-ROM are
all contributing to an explosion in the amount of useful
unformatted text in computer readable form. Parallel
computers and algorithms provide one way of dealing
with this explosion.
</bodyText>
<sectionHeader confidence="0.972016" genericHeader="method">
2 The CM: Machine Description
</sectionHeader>
<bodyText confidence="0.999798181818182">
The CM consists of a large number number of proces-
sor/memory cells. These cells are used to store data
structures. In accordance with a stream of instructions
that are broadcast from a single conventional host com-
puter, the many processors can manipulate the data in
the nodes of the data structure in parallel.
Each processor in the CM can have its own local
variables. These variables are called parallel variables,
or parallel fields. When a host computer program per-
forms a serial operation on a parallel variable, that op-
eration is performed separately in each processor in the
CM. For example, a program might compare two paral-
lel string variables. Each CM processor would execute
the comparison on its own local data and produce its
own local result. Thus, a single command can result in
tens of thousands of simultaneous CM comparisons.
In addition to their computation ability, CM pro-
cessors can communicate with each other via a special
hardware communication network. In effect, commu-
nication is the parallel analog of the pointer-following
executed by a serial computer as it traverses the links
of a data structure or graph.
</bodyText>
<sectionHeader confidence="0.998731" genericHeader="method">
3 Dictionary Access
</sectionHeader>
<bodyText confidence="0.998681666666667">
A dictionary may be defined as a mapping that takes a
particular word and returns a group of status bits. Sta-
tus bits indicate which sets or groups of words a partic-
ular word belongs to. Some of the sets that are useful in
natural language processing include syntactic categories
such as nouns, verbs, and prepositions. Programs also
can use semantic characterization information. For ex-
ample, knowing whether a word is name of a famous
person (i.e. Lincoln, Churchill), a place, an interjection,
or a time or calendar term will often be useful to a text
processing program.
The task of looking up the definition of a word con-
sists of returning a binary number that contains l&apos;s only.
in bit positions that correspond with the groups to which
that word belongs. Thus, the definition of &amp;quot;Lincoln&amp;quot;
contains a zero in the bit that indicates a word can serve
as a verb, but it contains a 1 in the famous person&apos;s name
bit.
While all of the examples in this paper involve only
a few words, it should be understood that the CM is
efficient and cost effective only when large amounts of
</bodyText>
<page confidence="0.996474">
128
</page>
<figureCaption confidence="0.988897">
Figure 1. Simple Broadcasting Mtiontuy Algorithm marking famous names
</figureCaption>
<note confidence="0.58045975">
Format of Processor Diagram: string - Prootme‘**
Famous-Oft: F
Proper-Noun-bit P
Mack a Selected: I
</note>
<tableCaption confidence="0.416724">
a Select processors containing &amp;quot;Lincoln&amp;quot;:
</tableCaption>
<table confidence="0.91025964">
The-I book-2 15-3 0001.4 Mchasitingeo-5 00.7 Italian-8 painter-9
F: 0 F: 0 F: 0 F: 0 F: o F: 0 F: 0 F: 0 F: 0
In: 0 r P: 0 P: 0 P: 0 r P: 0 P: 0 P: 0 P: 0 Ft: 0 r
b. Mark selected processors as famous names:
The-1 Itock-2 104 about-4 tolichaelangeb-5 an-7 Italian-8 painter-9
F: 0 F: 0 F: o F: 0 F: 0 F: 0 F: 0 F: 0 F: 0
P: 5r, Ft 0 r P: 0 Ft 0 r In: 0 P: 0 P: 0 Ft 0 In: 0 r
---C.
c. Select processors containing &amp;quot;Michaelangelo&amp;quot;:
The-1 book-2 8-3 about-4 Mchae00geb-5 ..s art -7 18030-8 painter-i
F: 0 F: 0 F: 0 F: 0 F: 0 F: 0 F: 0 F: 0 F: 0
P: 0 r P: 0 P: 0 P: 0 P: 0 Ft 0 In: 0 In: 0 P: 0 r
--C I r
d. Mark selected processors as famous names:
Figura 2. Syntactic Proper Ngua Label.
b. Subselect for processors not at start of sentence:
Theg book-2 about-4 kittnesiangeb-5&amp;quot; an-7 tallan-8 painter-9
F: 0 F: 0 F: 0 F: 1 F: 0 F: 0 F: 0 -F: 0
P: 0 In: F1 5 0 P: 0 P: 0 P: 0 In: 0 P: 0 r
c. Mark selected processors as proper nouns:
The-I 0008-2 15.3 abtalt-4 k30iealangeb-9 ,-6 an -7 italian-8 pamter.9
F: 0 F: 0 F: 0 F: 0 F: I F: 0 F: 0 F; 0 F: 0
P: F.: 0 P: 0 Ft: 0 P: 1 In: 0 P: 0 F: 1 o
r
I- C. —C 111
</table>
<figure confidence="0.984743656716418">
Proper Noun Proper Noun
Marked Marked
F: 1
P: 0
F: 0
P: 0
Mcnaehrgeo-5
about-a
F: 0
0
an-7
110080-8
F: 0
P: 0
—C
F: 0
P: 0
booe-2
F: 0
P: 0
C.
painter-9
F: 0
Ft 0 r
—5: 0
P: 0
1.
Note: famous name is marked
The-1
F: 0
P: 0 r
an-7
F: 1
P: 0
Mghaelangeb-5
F: 0
F:0
—C
F: o
Ft 0
—C
Format of Processor Diagram:
a Select processors with an upper case, alphabetic first character
The-I
F: 0
P. 0
book-2
F: 0
P: 0
about-4
F: 0
P: 0
Painter-9
F: 0
P: o r
is-3
F: 0
P: 0
r1
rP:an-8
o
F: 0
•
Ssino - Processor *
Famous-bit F
Proper-Noun-bit: P
Black if Selected: •
</figure>
<bodyText confidence="0.9966582">
text are to be processed. One would use the dictionary
algorithms described in this paper to look up all of the
words in an entire novel; one would not use them to
look up the ten words in a user&apos;s query to a question
answering system.
</bodyText>
<sectionHeader confidence="0.9679765" genericHeader="method">
4 A Simple Broadcasting Dictio-
nary Algorithm
</sectionHeader>
<bodyText confidence="0.999953170731707">
One way to implement a parallel dictionary is to seri-
ally broadcast all of the words in a given set. Processors
that contain a broadcast word check off the appropriate
status bits. When all of the words in one set have been
broadcast, the next set is then broadcast. For exam-
ple, suppose that the dictionary lookup program begins
by attempting to mark the words that are also famous
last names. Figure 1 illustrates the progress of the algo-
rithm as the words &amp;quot;Lincoln&amp;quot; and then &amp;quot;Michaelangelo&amp;quot;
are broadcast. In the first step, all occurrences of &amp;quot;Lin-
coln&amp;quot; are marked as famous names. Since that word
does not occur in the sample sentence, no marking ac-
tion takes place. In the second step, all occurrences of
&amp;quot;Michaelangelo&amp;quot; are marked, including the one in the
sample sentence.
In step d, where all processors containing &amp;quot;Michae-
langelo&amp;quot; are marked as containing famous names, the
program could simultaneously mark the selected pro-
cessors as containing proper nouns. Such shortcuts will
not be examined at this time.
After all of the words in the set of famous names
have been broadcast, the algorithm would then begin to
broadcast the next set, perhaps the set containing the
names of the days of the week.
In addition to using this broadcast algorithm, the
CM-Indexer uses syntactic definitions of some of the dic-
tionary sets. For example, it defines a proper noun as a
capitalized word that does not begin a sentence. (Proper
nouns that begin a sentence are not found by this cap-
italization based rule; this can be corrected by a more
sophisticated rule. The more sophisticated rule would
mark the first word in a sentence as a proper noun if it
could find another capitalized occurrence of the word in
a nearby sentence.) Figure 2 illustrates the progress of
this simple syntactic algorithm as it executes.
The implementation of both the broadcast algorithm
and the syntactic proper noun rule takes a total of less
than 30 lines of code in the *Lisp (pronounced &amp;quot;star-
lisp&amp;quot;) programming language. The entire syntactic rule
that finds all proper nouns executes in less than 5 mil-
liseconds. However, the algorithm that transmits word
</bodyText>
<page confidence="0.996125">
129
</page>
<tableCaption confidence="0.702534333333333">
a Select all processors where d?-0 (not yet defined). If no processors
are selected, then algorithm terminates. Otherwise, find the
minimum of the selected processors addresses.
</tableCaption>
<table confidence="0.5535228">
the-1 bey-.2 ate-3 tne-4 pissa-r
B: 0 s: 0 13: 0 B: 0 8: 0
DT 0 DT 0 D? 0 D? 0 DT 0
11 111 111
I\Host Machine quickly determines that the minimum address is 1
</table>
<bodyText confidence="0.852677333333333">
b. Host machine pulls out word in that minimum procesor
and looks up its definition in its own serial dictionary/hash table.
In this case, the definition of the is determined to be the bit sequence 001.
(The bits are the status bits discussed in the text.)
Next, the host machine selects all processors containing the word whose
definition was just looked up:
</bodyText>
<figure confidence="0.352246923076923">
the-1 boy-2 ate-3 the-4 pizZa-5
B: 0 8: 0 8: 0 B: 0 V 0
13? 0 DT 0 DT 0 0? 0 DT 0 r
V
c. The entire looked up definition is assigned to all selected processors
and all selected processors are marked as defined.
the-1 boy-2 010-3 the-4 pizza-5
B: 001
D? 1
11
B: 001 B: 0 8: 0 B: 0
D? 1 D? 0 D? 0 r. D? 0
d. goto a
</figure>
<bodyText confidence="0.999929666666667">
lists takes an average of more than 5 milliseconds per
word to broadcast a list of words from the host to the
CM. Thus, since it takes time proportional to the num-
ber of words in a given set, the algorithm becomes a
bottleneck for sets of more than a few thousand words.
This means that the larger sets listed above (all nouns,
all verbs, etc.) cannot be transmitted. The reason that
this slow algorithm was used in the CM-Indexer was the
ease with which it could be implemented and tested.
</bodyText>
<sectionHeader confidence="0.966228" genericHeader="method">
5 An Improved Broadcasting Dic-
tionary Algorithm
</sectionHeader>
<bodyText confidence="0.999993166666667">
One improvement to the simple broadcasting algorithm
would be to broadcast entire definitions (i.e. several
bits), rather than a single bit indicating membership in
a set. This would mean that each word in the dictio-
nary would only be broadcast once (i.e. &amp;quot;fly&amp;quot; is both
a noun and a verb). A second improvement would be
to broadcast only the words that are actually contained
in the text being looked up. Thus, words that rarely
occur in English, which make up a large percentage of
the dictionary, would rarely be broadcast.
In summary, this improved dictionary broadcasting
algorithm will loop for the unique words that are con-
tained in the text to be indexed, look up the definition
of each such word in a serial dictionary on the host ma-
chine, and broadcast the looked-up definition to the en-
tire CM. Figure 3 illustrates how this algorithm would
assign the definition of all occurrences of the word &amp;quot;the&amp;quot;
in a sample text. (Again, in practice the algorithm oper-
ates on many thousands of words, not on one sentence.)
In order to select a currently undefined word to look
up, the host machine executing this algorithm must de-
termine the address of a selected processor. The figure
indicates that one way to do this is to take the min-
imum address of the processors that are currently se-
lected. This can be done in constant time on the CM.
This improved dictionary lookup method is useful
when the dictionary is much larger than the number of
unique words contained in the text to be indexed. How-
ever, since the same basic operation is used to broadcast
definitions as in the first algorithm, it is clear that this
second implementation of a dictionary will not be fea-
sible when a text contains more than a few thousand
unique words.
By analyzing a number of online texts ranging in
size from 2,000 words to almost 60,000 words, it was
found that as the size of the text approaches many tens
of thousands of words, the number of unique words in-
creased into the thousands. Therefore, it can be con-
cluded that the second implementation of the broad-
casting dictionary algorithm is not feasible when there
are more than a few tens of thousands of words in the
text file to be indexed.
</bodyText>
<sectionHeader confidence="0.894383" genericHeader="method">
6 Making Efficient Use of Paral-
lel Hardware
</sectionHeader>
<bodyText confidence="0.999926571428571">
In both of the above algorithms, the &amp;quot;heart&amp;quot; of the dic-
tionary resided in the serial host. In the first case, the
heart was the lists that represented sets of words; in the
second case, the heart was the call to a serial dictionary
lookup procedure. Perhaps if the heart of the dictionary
could be stored in the CM, alongside the words from the
text, the lookup process could be accelerated.
</bodyText>
<sectionHeader confidence="0.8139165" genericHeader="method">
7 Implementation of Dictionary
Lookup by Parallel Hashing
</sectionHeader>
<bodyText confidence="0.999966">
One possible approach to dictionary lookup would be to
create a hash code for each word in each CM processor in
parallel. The hash code represents the address of a dif-
ferent processor. Each processor can then send a lookup
request to the processor at the hash-code address, where
</bodyText>
<figure confidence="0.8641365">
Rgure 3. Unique Wont Diceonaty Implementation
Format of Processor Diagram:
Sleep • Pea- sqnr
Defintbon Bits: BBBB
Defined-yet? D
Bladte Selected
-
130
a Select all processors, set original address field to be
the processor number:
the-I boy-2 ate-3 the-4 putss-5
B: bits B: bits B: bits B: bits B: bits
N: 1 N: 2 N: 3 N: 4 N: 5
111 111
b. Call sort with string as the key, and string and N as
the fields to copy. The final result is:
ate-h boy-2 putsa-3 the-4 the-5
B: bit B: bits B: bits B: bits B: bits
N: 3 N: N: 5 N: 1 N: 4
•
</figure>
<subsectionHeader confidence="0.988155">
8.1 Parallel Sorting
</subsectionHeader>
<bodyText confidence="0.995166866666667">
A parallel sort is similar in function to a serial sort. It
accepts as arguments a parallel data field and a par-
allel comparison predicate, and it sorts among the se-
lected processors so that the data in each successive (by
address) processor increases monotonically. There are
parallel sorting algorithms that execute in time propor-
tional to the square of the logarithm of the number of
items to be sorted. One easily implemented sort, the
enumerate-and-pack sort, takes about 1.5 milliseconds
per bit to sort 64,000 numbers on the CM. Thus, it
takes 48 milliseconds to sort 64,000 32-bit numbers.
Figure 4 illustrates the effect a parallel sort has on a
single sentence. Notice that pointers back to the original
location of each word can be attached to words before
the textual order of the words is scrambled by the sort.
</bodyText>
<subsectionHeader confidence="0.744236">
8.2 Scan: Spreading Information in Log-
arithmic Time
</subsectionHeader>
<figureCaption confidence="0.64551525">
Figure 4. Illustration ot Sort _SrAgn - Pk, ksnr
Fortnat ot Processor Diagram: Definition Bits: BBBB
OriginakAddress: N
Black d Selected:
</figureCaption>
<bodyText confidence="0.999901714285714">
the definition of the word that hashes to that address
has been stored in advance. The processors that receive
requests would then respond by sending back the pre-
stored definition of their word to the address contained
in the request packet.
One problem with this approach is that all of the
processors containing a given word will send a request
for a definition to the same hashed address. To some ex-
tent, this problem can be ameliorated by broadcasting
a list of the n (i.e. 200) most common words in English,
before attempting any dictionary lookup cycles. An-
other problem with this approach is that the hash code
itself will cause collisions between different text words
that hash to the same value.
</bodyText>
<sectionHeader confidence="0.986233" genericHeader="method">
8 An Efficient Dictionary Algo-
rithm
</sectionHeader>
<bodyText confidence="0.999721307692308">
There is a faster and more elegant approach to building
a dictionary than the hashing scheme. This other ap-
proach has the additional advantage that it can be built
from two generally useful submodules each of which has
a regular, easily debugged structure.
The first submodule is the sort function, the second
is the scan function. After describing the two submod-
ules, a simple version of the fast dictionary algorithm
will be presented, along with suggestions for dealing
with memory and processor limitations.
A scan algorithm takes an associative function of two
arguments, call it F, and quickly applies it to data field
values in successive processors of:
</bodyText>
<listItem confidence="0.9999304">
• a
• b
• c
• d
• e
</listItem>
<bodyText confidence="0.9985645">
The scan algorithm produces output fields in the
same processors with the values:
</bodyText>
<listItem confidence="0.999963">
• a
• F(a, b)
• F(F(a, b), c)
• F(F(F(a, b), c), d)
• etc.
</listItem>
<bodyText confidence="0.999952181818182">
The key point is that a scan algorithm can take ad-
vantage of the associative law and perform this task in
logarithmic time. Thus, 16 applications of F are suf-
ficient to scan F across 64,000 processors. Figure 5
shows one possible scheme for implementing scan. While
the scheme in the diagram is based on a simple linked
list structure, scan may also be implemented on binary
trees, hypercubes, and other graph data structures. The
nature of the routing system of a particular parallel com-
puter will select which data structures can be scanned
most rapidly and efficiently.
</bodyText>
<page confidence="0.997896">
131
</page>
<figureCaption confidence="0.9928155">
Figure 5. Illustration of Scan
Format of Processor Diagram.
</figureCaption>
<bodyText confidence="0.8038672">
Backward pointer can be calculated
in constant time: all processors
send their own addresses to the
processors pointed to by P.
f is any associative function of two arguments
</bodyText>
<figure confidence="0.987295">
a. Select all processors, initialize function value to string, forward pointer
to self address + 1.
a-I b-2 c-3 d-4 0-5 I-6 9-7 h-8
F: a F: C F: d F: e F: b Fi C F:
F.: 2 P: 3 P: 4 P: 5 P: 6 P: 7 F.: 8 Pi
• • • • • • •
b. Get back pointer, get function value from processor at back pointer,
call this value BF. Replace the current function value, F, with f(BF,F):
a-1 b-2 C-3 d-4 0-5 f -6 g-7 5-8
a (0.6) I(0.C) f(c.a) tide) flea) 1046 4IM
P: 3 P: 4 P: 5 P: 6 P: 7 P: 8 P:
Pi 26 • • •
c. Calculate a forward pointer that goes twice as far as the current forward pointer.
This can be done as follows: Get the value of P at the processor pointed to
by your own P, and replace your own P with that new value:
a-I b-2 C-3 0-4 0-5 1-6 9:7 1,8
a f(a.b) 1(04) f(c.• f(em 111.8) 4IM
P: 36 P: 4 F.: 5 P: 6 P: 7 P: 88 P: P:
• • a
d. If any processor has a valid forward pointer goto b
</figure>
<figureCaption confidence="0.683941">
(the next execution of b has the following effect on the first 4 processors:
</figureCaption>
<bodyText confidence="0.991164608695652">
Note that since f &apos;s associative,
(a, f(b, c)) is always equal to f( f (a,b), c),
and f( f(a,b), f(c,d)) - f( f( f(a, b), c), d)
When combined with an appropriate F, scan has ap-
plications in a variety of contexts. For example, scan is
useful in the parallel enumeration of objects and for re-
gion labeling. Just as the FFT can be used to efficiently
solve many problems involving polynomials, scan can be
used to create efficient programs that operate on graphs,
and in particular on linked lists that contain natural,lan-
guage text.
First, an alphabetic sort is applied in parallel to all
processors, with the word stored in each processor serv-
ing as the primary key, and the DEFINED? bit acting
as a secondary key. The result will be that all copies of
a given word are grouped together into sequential (by
processor address) lists, with the single dictionary copy
of each word immediately preceding any and all text
copies of the same word.
The definitions that are contained in the dictionary
processors can then be distributed to all of the text
words in logarithmic time by scanning the processors
with the following associative function f:
</bodyText>
<tableCaption confidence="0.354698714285714">
x and y are processors that have the following
;; fields or parallel variables:
STRING (a word)
DEFINED? (1 if word contains a correct definition)
ORIGINAL-ADDRESS (where word resided before sort)
DEFINITION (initially correct only in dictionary
words)
</tableCaption>
<bodyText confidence="0.975767666666667">
function f returns a variable containing the same
four fields. This is a pseudo language; the actual
program was written in *Lisp.
</bodyText>
<equation confidence="0.942869125">
function f(x,y):
f.STRING = y.STRING
f.ORIGINAL-ADDRESS = y.ORIGINAL-ADDRESS
if y.DEFINED? = 1 then
if y is defined, just return y
f.DEFINED? = 1
f.DEFINITION = y.DEFINITION
}
</equation>
<construct confidence="0.524126666666667">
else
if x.STRING = y.STRING then
if words are-the same, take
</construct>
<figure confidence="0.866708307692308">
any definition that x may have
f.DEFINED? = x.DEFINED?
f.DEFINITION = x.DEFINITION
6-1 D-2 c-3 5-4
4.1.1)) I( a, 40,C)) I) 1(cd))
P: 3 P: 4 P: 5 P. 6
1•1.
String-Processor*
Function value: F
Forward pointer: P
(Po an cm addresss)
Black N Selected:
else
</figure>
<subsectionHeader confidence="0.882636">
8.3 Application of Scan and Sort to Dic-
tionary Lookup
</subsectionHeader>
<bodyText confidence="0.99894268">
To combine these two modules into a dictionary, we need
to allocate a bit, DEFINED?, that is 1 only in processors
that contain a valid definition of their word. Initially, it
is 1 in the processors that contain words from the dictio-
nary, and 0 in processors that contain words that come
from the text to be looked up. The DEFINED? bit will
be used by the algorithm as it assigns definitions to text
words. As soon as a word receives its definition, it will
have its DEFINED? bit turned on. The word can then
begin to serve as an additional copy of the dictionary
entry for the remainder of the lookup cycle. (This is the
&amp;quot;trick&amp;quot; that allows scan to execute in logarithmic time.)
no definition yet
f.DEFINED? = 0
note that text words that are not found in the
dictionary correctly end up with DEFINED? = 0
This function F will spread dictionary definitions
from a definition to all of the words following it (in
processor address order), up until the next dictionary
word. Therefore, each word will have its own copy of
the dictionary definition of that word. All that remains
is to have a single routing cycle that sends each def-
inition back to the original location of its text word.
Figure 6 illustrates the execution of the entire sort-scan
algorithm on a sample sentence.
</bodyText>
<page confidence="0.994989">
132
</page>
<bodyText confidence="0.841429">
Next, the following function creates the field that
will be scanned:
</bodyText>
<note confidence="0.475838">
Figures. Illustration of Sort-Scan Algorithm
</note>
<table confidence="0.97180012">
Forma( of Processor Diagram: ,...al.finnirom7P4tnre
Defined?
Definition Bits: BBBB
Original-Address: N
8100sf Selected:
a. Both the dictionary words and the text words are stored in the CM
the-1 bov-2 ate-3 0 Me-4 0 Ri7211.5 ate-6 boy-7 clog-8 the-9
D?: 0 D?: 0 D?: - D?: 4 D?: 0 D?: 1 D?: 1 D?: 1 D?: 1
W 1 N: 2 W 3 B: j - B: 100 B: 010 B: 011 B: 001
I N; W N: 5U N: N: • •
•
c. Scan 1
b. Peform Me2 an alphabetic sort: doo-5 dictionary
al0-1 Text (Merge Dict onary
bov-3 boy-4 in o text)
the-7
D?: 1 07: 0 D?: 1 D?: 0 j i?: mal! the-8 the-9
B: 100 W 3 B: 010.1 B: D?: 1 0 IP: 1 137: 0 D?: 0
11 N: 2 B: 011 B B: 001 B: - IV: 4
N: - f: 5 N: N: 1
111
•
us rig the F described in the text:
ate-1 ate-2 bofp3 boy-4 Oo0-5 pizza-6 Me-7 the-8 he-9
D?: Di: 1 D?: 1 CO: 1 D?: 1 D?: 0 D?: 1 D?: 1 D?: 1
B: 10 B: 100 010.1 W : 01011 B: 011 - 0: 001 B: 011 : 011
N. N: 3 N: 2 N: - W 5W N: • 14:m : 4
U •
1\ 1\
Definition Definition
not used not In dictionary
d. Send definition back to original address
the-1 bon-2 110-3 the-4 pral-5 ate-6 boy-7 cloo-8 the-9
D?: D?: 1 D?: 1 D?: 1 D?: 0 DT: 1 D?: 1 D?: 1 D?: 1
B: 001 B: 001 B: 100 B: 001j B: B: 100 B: 010 B: 011 B: 001
N: 1 N: 2 N: N: 4 N: 5 N. N: - N:
V U • • U
;; header-p is a parallel boolean variable that is
true in headers, false otherwise
function create-field-for-scan(header-p):
define a type for a large bit field
var FIELD : record
most significant bits contain
processor address
ADDRESS
least significant bits will
contain the definition
DEFINITION
end
;;initialize to address 0, no definition
</table>
<equation confidence="0.909605">
FIELD.ADDRESS = 0
FIELD.DEFINITION = 0
</equation>
<bodyText confidence="0.9548625">
next, the headers that are dictionary words store
their definitions in the correct part of FIELD
Non-dictionary headers (text words not found
in dictionary) are given null definitions.
</bodyText>
<figure confidence="0.7538184">
if header
FIELD.DEFINITION = definition
Text Dict onary
8.4 Improvements to the Sart-Scam Dic-
tionary Algorithm
</figure>
<bodyText confidence="0.977495545454546">
Since the CM is a bit serial Machine, string operations
are relatively expensive operation. The dictionary func-
tion F described above performs a string comparison
and a string copy operation each time it is invoked. On
a full size CM, the function is invoked 16 times (log
64K words). A simple optimization can be made to the
sort-scan algorithm that allows the string comparison
to be performed only once. This allows a faster dictio-
nary function that performs no string comparisons to be
used.
The optimization consists of two parts. First, a new
stage is inserted after the sorting step, before the scan-
ning step. In this new step, each word is compared to
the word to its left, and if it is different, it is marked as
a &amp;quot;header.&amp;quot; Such words begin a new segment of iden-
tical words. All dictionary words are headers, because
the sort places them before all occurrences of identical
words. In addition, the first word of each group of words
that does not occur in the dictionary is also marked as
a header.
self-address contains each processorVs
own unique address
</bodyText>
<equation confidence="0.364697">
FIELD.ADDRESS = self-address
}
return (FIELD)
</equation>
<bodyText confidence="0.999602111111111">
Finally, instead of scanning the dictionary function
across this field, the maximum function (which returns
the maximum of two input numbers) is scanned across
it. Definitions will propagate from a header to all of
the words within its segment, but they will not cross
past the next header. This is because the next header
has a greater self-address in the most significant bits
of the field being scanned, and the maximum function
selects it rather than the earlier header g smaller field
value. If a header had no definition, because a word was
not found in the dictionary, the null definition would be
propagated to all copies of that word.
The process of scanning the maximum function across
a field was determined to be generally useful. As a re-
sult, the max-scan function was implemented in an effi-
cient pipelined, bit-serial manner by Guy Blelloch, and
was incorporated into the general library of CM func-
tions.
</bodyText>
<page confidence="0.99829">
133
</page>
<tableCaption confidence="0.3549595">
n. In headers only, set the A to the self address and the D to the
definition, if there is one.
</tableCaption>
<table confidence="0.99750365">
ate-1 ate-2 boy-3 boy-4 ore-5
137: 1 0?: 0 D?: 1 D?: 0 07: 1
EL 100 B: 13: 010 B: 011
H?: H?: 0 H?: 1 H?: 0 H?: 1
AM 1:100 AS 00 AD 3010 Ail 0:0 A:0 5011
• •
C. Scan the Maximum function across the A:D
ate-1 ate-2 boy-3 boy-4 000-5
1)?: D?: 0 0?: 0?: 0 13?: 1
13: 100 B. - B: 010 B: 13: 011
j
H?: 1 H?: 0 H?: H?: 0 HT: 1
0:0 1:100 AM 11 AS 3010 AS 3:010 A:0 5011
• •
d. Copy definition bits from D to B. and set D? if defined.
ate-1 ate-2 boy-3 boy-4 003-5
DI: 1 D?: 1 D?: D?. 0?: 1
B: 100 13: 100 B: 010 B: 010 13: 011
H?: 1 H7: 0 H?: 1 A:D 0 H?: 1
AS 1:100 0:1) 1:100 AD 3010 3010 A:0 5:011r
</table>
<figureCaption confidence="0.953121">
Figure 7 illustrates the creation of this field, and the
</figureCaption>
<bodyText confidence="0.984980222222222">
scanning of the maximum function across it. Note that
the size of the field being scanned is the size of the def-
inition (8 bits for the timings below) plus the size of a
processor address (16 bits). In comparison, the earlier
dictionary function had to be scanned across the def-
inition and the original address, along with the entire
string. Scanning this much larger field, even if the dic-
tionary function was as fast as the maximum function,
would necessarily result in slower execution times.
</bodyText>
<subsectionHeader confidence="0.961647">
8.5 Evaluation of the Sort-Scan Dictio-
nary Algorithm
</subsectionHeader>
<bodyText confidence="0.999831063829787">
The improved sort-scan dictionary algorithm is much
more efficient than the broadcasting algorithms described
earlier. The algorithm was implemented and timed on
a Connection Machine.
In a bit-serial computer like the CM, the time needed
to process a string grows linearly with the number of bits
used to store the string. A string length of 8 characters
is adequete for the CM-Indexer. Words longer than 8
characters are represented by the simple concatenation
of their first 4 and last 4 characters. ASCII characters
therefore require 64 bits per word in the CM; 4 more
bits are used for a length count.
Because dictionary lookup is only performed on al-
phabetic characters, the 64 bits of ASCII data described
above can be compacted without collision. Each of the
twenty-six letters of the alphabet can be represented
using 5 bits, instead of 8, thereby reducing the length
of the character field to 40 bits; 4 bits are still needed
for the length count. Additional compression could be
achieved, perhaps by hashing, although that would in-
troduce the possibility of collisions. No additional com-
pression is performed in the prototype implementation.
The timings given below assume that each processor
stores an 8 character word using 44 bits.
First of all, to sort a bit field in the CM currently
takes about 1.5 milliseconds per bit. Second, the func-
tion that finds the header words was timed and took
less than 4 milliseconds to execute. The scan of the
max function across all of the processors completed in
under in 2 milliseconds. The routing cycle to return the
definitions to the original processors of the text took
approximately one millisecond to complete.
As a result, with the improved sort-scan algorithm,
an entire machine full of 64,000 words can be looked
up in about 73 milliseconds. In comparison to this, the
original sort-scan implementation requires an additional
32 milliseconds (2 milliseconds per invocation of the slow
dictionary function), along with a few more milliseconds
for the inefficient communications pattern it requires.
This lookup rate is approximately equivalent to a
serial dictionary lookup of .9 words per microsecond.
In comparison, a Symbolics Lisp Machine can look up
words at a rate of 1/500 words per microsecond. (The
timing was made for a lookup of a single bit of infor-
mation about a word in a hash table containing 1500
words). Thus, the CM can perform dictionary lookup
about 450 times faster than the Lisp Machine.
</bodyText>
<subsectionHeader confidence="0.985817">
8.6 Coping with Limited Processor Re-
sources
</subsectionHeader>
<bodyText confidence="0.999942133333333">
Since there are obviously more than 64,000 words in the
English language, a dictionary containing many words
will have to be handled in sections. Each dictionary pro-
cessor will have to hold several dictionary words, and
the look-up cycle will have to be repeated several times.
These adjustments will slow the CM down by a multi-
plicative factor, but Lisp Machines also slow down when
large hash tables (often paged out to disk) are queried.
There is an alternative way to view the above algo-
rithm modifications: since they are motivated by limited
processor resources, they should be handled by some
sort of run time package, just as virtual memory is used
to handle the problem of limited physical memory re-
sources on serial machines. In fact, a virtual processor
facility is currently being used on the CM.
</bodyText>
<figureCaption confidence="0.999285">
Figure 7. Illustration of Improvements to Sod-Scan Algorithm
</figureCaption>
<figure confidence="0.997903285714286">
Format of Processor Diagram:
a. Atter sort, detect the headers (words different from lett neighbor)
ate-1 ate-2 boy-3 boy-4 obg-5
D?: 1 DT: 0 DT. 1 D?: 0 D?:
B: 100 B: B: 010 01;
11?: 1 H?: 0 H?: 0:0 H?: 00 H?: 0:0
AD 00 A:0 00 AD A:13 kD
• •
geom., • Pmr44sor
Defined?
Deflation, Bast B
Header?
Addr-Deftnition: AD
Black If Selected:
</figure>
<page confidence="0.987523">
134
</page>
<sectionHeader confidence="0.718254" genericHeader="method">
9 Further Applications of Scan
to Bulk Processing of Text
</sectionHeader>
<bodyText confidence="0.999997108108108">
The scan algorithm has many other applications in text
processing. For example, it can be used to lexically
parse text in the form of 1 character per processor into
the form of 1 word per processor. Syntactic rules could
rapidly determine which characters begin and end words.
Scan could then be used to enumerate how many words
there are, and what position each character occupies
within its word. The processors could then use this in-
formation to send their characters to the word-processor
at which they belong. Each word-processor would re-
ceive the characters making up its word and would as-
semble them into a string.
Another application of scan, suggested by Guy L.
Steele, Jr., would be as a regular expression parser, or
lexer. Each word in the CM is viewed as a transition
matrix from one set of finite automata states to another
set. Scan is used, along with an F which would have
the effect of composing transition matrices, to apply a
finite automata to many sentences in parallel. After this
application of scan, the last word in each sentence con-
tains the state that a finite automata parsing the string
would reach. The lexer&apos;s state transition function F
would be associative, since string concatenation is asso-
ciative, and the purpose of a lexer is to discover which
particular strings/tokens were concatenated to create a
given string/file.
The experience of actually implementing parallel nat-
ural language programs on real hardware has clarified
which operations and programming techniques are the
most efficient and useful. Programs that build upon
general algorithms such as sort and scan are far, easier
to debug than programs that attempt a direct assault on
a problem (i.e. the hashing scheme discussed earlier; or
a slow, hand-coded regular expression parser that I im-
plemented). Despite their ease of implementation, pro-
grams based upon generally useful submodules often are
more efficient than specialized, hand-coded programs.
</bodyText>
<sectionHeader confidence="0.991606" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999912">
I would like to thank Dr. David Waltz for his help in this
research and in reviewing a draft of this paper. I would
also like to thank Dr. Stephen Omohundro, Cliff Lasser,
and Guy Blelloch for their suggestions concerning the
implementation of the dictionary algorithm.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999657416666667">
Akl, Selim G. Parallel Sorting Algorithms, 1985, Aca-
demic Press, Inc.
Feynman, Carl Richard, and Guy L. Steele Jr. Connec-
tion Machine Macroinstruction Set, REL 2.3., Thinking
Machines Corporation. (to appear)
Hillis, W. Daniel. The Connection Machine, 1985, The
MIT Press, Cambridge, MA.
Lasser, Clifford A., and Stephen M. Omohundro. The
Essential *Lisp Manual, Thinking Machines Corpora-
tion. (to appear)
Leiserson, Charles, and Bruce Maggs. &amp;quot;Communication-
Efficient Parallel Graph Algorithms,&amp;quot; Laboratory for
Computer Science, Massachusetts Institute of Technol-
ogy. (to appear) (Note: scan is a special case of the
treefix algorithm described in this paper.)
Omohundro, Steven M. &amp;quot;A Connection Machine Algo-
rithms Primer,&amp;quot; Thinking Machines Corporation. (to
appear)
Resnikoff, Howard. The Illusion of Reality, 1985, in
preparation.
Waltz, David L. and Jordan B. Pollack. &amp;quot;Massively Par-
allel Parsing: A Strongly Interactive Model of Natural
Language Interpretation,&amp;quot; Cognitive Science, Volume 9,
Number 1, pp. 51-74, January-March, 1985.
</reference>
<page confidence="0.998783">
135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9927895">BULK PROCESSING OF TEXT ON A MASSIVELY PARALLEL COMPUTER</title>
<author confidence="0.999984">Gary W Sabot</author>
<affiliation confidence="0.999476">Thinking Machines Corporation</affiliation>
<address confidence="0.9994945">245 First St. Cambridge, MA 02142</address>
<abstract confidence="0.9940203625">Dictionary lookup is a computational activity that can be greatly accelerated when performed on large amounts of text by a parallel computer such as the Connection Machine&apos;m Computer (CM). Several algorithms for parallel dictionary lookup are discussed, including one that allows the CM to lookup words at a rate 450 times that of lookup on a Symbolics 3600 Lisp Machine. 1 An Overview of the Dictionary Problem This paper will discuss one of the text processing probthat was encountered during the implementation the CM-Indexer, a natural language processing program that runs on the Connection Machine (CM). The problem is that of parallel dictionary lookup: given both a dictionary and a text consisting of many thousands of words, how can the appropriate definitions be distributed to the words in the text as rapidly as possible? A parallel dictionary lookup algorithm that makes efficient use of the CM hardware was discovered and is described in this paper. It is clear that there are many natural language processing applications in which such a dictionary algorithm is necessary. Indexing and searching of databases consisting of unformatted natural language text is one such application. The proliferation of personal computers, the widespread use of electronic memos and electronic mail in large corporations, and the CD-ROM are all contributing to an explosion in the amount of useful unformatted text in computer readable form. Parallel computers and algorithms provide one way of dealing with this explosion. 2 The CM: Machine Description The CM consists of a large number number of processor/memory cells. These cells are used to store data structures. In accordance with a stream of instructions are broadcast from a single conventional computer, the many processors can manipulate the data in the nodes of the data structure in parallel. Each processor in the CM can have its own local These variables are called variables, parallel a host computer program performs a serial operation on a parallel variable, that operation is performed separately in each processor in the CM. For example, a program might compare two parallel string variables. Each CM processor would execute the comparison on its own local data and produce its own local result. Thus, a single command can result in tens of thousands of simultaneous CM comparisons. In addition to their computation ability, CM processors can communicate with each other via a special hardware communication network. In effect, communication is the parallel analog of the pointer-following executed by a serial computer as it traverses the links of a data structure or graph. 3 Dictionary Access A dictionary may be defined as a mapping that takes a word and returns a group of bits. Status bits indicate which sets or groups of words a particular word belongs to. Some of the sets that are useful in natural language processing include syntactic categories such as nouns, verbs, and prepositions. Programs also can use semantic characterization information. For example, knowing whether a word is name of a famous person (i.e. Lincoln, Churchill), a place, an interjection, or a time or calendar term will often be useful to a text processing program. The task of looking up the definition of a word consists of returning a binary number that contains l&apos;s only. in bit positions that correspond with the groups to which that word belongs. Thus, the definition of &amp;quot;Lincoln&amp;quot; contains a zero in the bit that indicates a word can serve a verb, but it contains a 1 in the person&apos;s bit. While all of the examples in this paper involve only a few words, it should be understood that the CM is efficient and cost effective only when large amounts of 128 1. Simple Broadcasting marking famous names</abstract>
<title confidence="0.8230398">Format of Processor Diagram: Famous-Oft: F Proper-Noun-bit P Mack a Selected: I a Select processors containing &amp;quot;Lincoln&amp;quot;:</title>
<note confidence="0.911003734939759">The-I book-2 0001.4 Mchasitingeo-5 00.7 Italian-8 painter-9 F: 0 F: 0 F: 0 F: o F: 0 F: 0 F: 0 0 r P: 0 P: 0 0 r P: 0 P: 0 P: 0 P: 0 0 r b. Mark selected processors as famous names: The-1 Itock-2 104 about-4 tolichaelangeb-5 an-7 Italian-8 painter-9 F: 0 F: 0 F: 0 F: 0 F: 0 F: 0 0 P: 0 0 r In: 0 P: 0 P: 0 Ft 0 0 r ---C. c. Select processors containing &amp;quot;Michaelangelo&amp;quot;: The-1 book-2 about-4 Mchae00geb-5 ..s art -7 18030-8 painter-i F: 0 F: 0 F: 0 F: 0 F: 0 F: 0 F: 0 F: 0 F: 0 0 r P: 0 P: 0 P: 0 P: 0 Ft 0 --C In: 0 In: 0 P: r I r d. Mark selected processors as famous names: 2. Syntactic Proper Label. b. Subselect for processors not at start of sentence: Theg book-2 about-4 kittnesiangeb-5&amp;quot; an-7 tallan-8 painter-9 F: 0 F: 0 F: 0 F: 0 0 P: 0 In: 0 P: 0 P: 0 P: 0 In: 0 r c. Mark selected processors as proper nouns: The-I 0008-2 15.3 abtalt-4 k30iealangeb-9 ,-6 an -7 italian-8 pamter.9 F: 0 F: 0 F: 0 F: 0 F: I F: 0 F: 0 F; 0 F: 0 P: F.: 0 P: 0 Ft: 0 P: 1 In: 0 P: 0 F: 1 o r C. —C 111 Proper Noun Proper Noun Marked Marked F: 1 P: 0 P: 0 Mcnaehrgeo-5 about-a F: 0 0 an-7 110080-8 0 P: 0 —C F: 0 P: 0 booe-2 F: 0 P: 0 painter-9 F: 0 0 r 0 P: 0 Note: famous name is marked The-1 F: 0 r an-7 F: 1 P: 0 Mghaelangeb-5 F: 0 F:0 —C o Ft 0 —C Format of Processor Diagram: Select processors with case, alphabetic first character The-I F: 0 P. 0 book-2 F: 0 P: 0 about-4 F: 0 P: 0 Painter-9 F: 0 r is-3 F: 0 P: 0 r1 o F: 0</note>
<title confidence="0.9116665">Ssino - Processor * Famous-bit F Proper-Noun-bit: P</title>
<author confidence="0.60655">Black if Selected</author>
<abstract confidence="0.985149034722222">text are to be processed. One would use the dictionary algorithms described in this paper to look up all of the words in an entire novel; one would not use them to look up the ten words in a user&apos;s query to a question answering system. 4 A Simple Broadcasting Dictionary Algorithm One way to implement a parallel dictionary is to serially broadcast all of the words in a given set. Processors that contain a broadcast word check off the appropriate status bits. When all of the words in one set have been broadcast, the next set is then broadcast. For example, suppose that the dictionary lookup program begins by attempting to mark the words that are also famous last names. Figure 1 illustrates the progress of the algorithm as the words &amp;quot;Lincoln&amp;quot; and then &amp;quot;Michaelangelo&amp;quot; are broadcast. In the first step, all occurrences of &amp;quot;Lincoln&amp;quot; are marked as famous names. Since that word does not occur in the sample sentence, no marking action takes place. In the second step, all occurrences of &amp;quot;Michaelangelo&amp;quot; are marked, including the one in the sample sentence. In step d, where all processors containing &amp;quot;Michaelangelo&amp;quot; are marked as containing famous names, the program could simultaneously mark the selected processors as containing proper nouns. Such shortcuts will not be examined at this time. all of the words in the set of have been broadcast, the algorithm would then begin to broadcast the next set, perhaps the set containing the names of the days of the week. In addition to using this broadcast algorithm, the CM-Indexer uses syntactic definitions of some of the dictionary sets. For example, it defines a proper noun as a capitalized word that does not begin a sentence. (Proper nouns that begin a sentence are not found by this capitalization based rule; this can be corrected by a more sophisticated rule. The more sophisticated rule would mark the first word in a sentence as a proper noun if it could find another capitalized occurrence of the word in a nearby sentence.) Figure 2 illustrates the progress of this simple syntactic algorithm as it executes. The implementation of both the broadcast algorithm and the syntactic proper noun rule takes a total of less than 30 lines of code in the *Lisp (pronounced &amp;quot;starlisp&amp;quot;) programming language. The entire syntactic rule that finds all proper nouns executes in less than 5 milliseconds. However, the algorithm that transmits word 129 a Select all processors where d?-0 (not yet defined). If no processors are selected, then algorithm terminates. Otherwise, find the minimum of the selected processors addresses. the-1 bey-.2 tne-4 B: 0 13: 0 B: 0 8: 0 DT 0 DT 0 D? 0 D? 0 DT 0 11 111 111 Machine quickly determines that the minimum address is 1 b. Host machine pulls out word in that minimum procesor and looks up its definition in its own serial dictionary/hash table. In this case, the definition of the is determined to be the bit sequence 001. (The bits are the status bits discussed in the text.) Next, the host machine selects all processors containing the word whose definition was just looked up: the-1 boy-2 ate-3 the-4 pizZa-5 B: 0 8: 0 8: 0 B: 0 V 0 13? 0 DT 0 DT 0 0? 0 0 r V c. The entire looked up definition is assigned to all selected processors and all selected processors are marked as defined. the-1 boy-2 010-3 the-4 pizza-5 B: 001 D? 1 11 B: 001 D? 1 B: 0 8: 0 B: 0 D? 0 0 r. D? 0 d. goto a lists takes an average of more than 5 milliseconds per word to broadcast a list of words from the host to the CM. Thus, since it takes time proportional to the number of words in a given set, the algorithm becomes a bottleneck for sets of more than a few thousand words. This means that the larger sets listed above (all nouns, all verbs, etc.) cannot be transmitted. The reason that this slow algorithm was used in the CM-Indexer was the ease with which it could be implemented and tested. 5 An Improved Broadcasting Dictionary Algorithm One improvement to the simple broadcasting algorithm would be to broadcast entire definitions (i.e. several bits), rather than a single bit indicating membership in a set. This would mean that each word in the dictionary would only be broadcast once (i.e. &amp;quot;fly&amp;quot; is both a noun and a verb). A second improvement would be to broadcast only the words that are actually contained in the text being looked up. Thus, words that rarely occur in English, which make up a large percentage of the dictionary, would rarely be broadcast. In summary, this improved dictionary broadcasting algorithm will loop for the unique words that are contained in the text to be indexed, look up the definition of each such word in a serial dictionary on the host machine, and broadcast the looked-up definition to the entire CM. Figure 3 illustrates how this algorithm would assign the definition of all occurrences of the word &amp;quot;the&amp;quot; in a sample text. (Again, in practice the algorithm operates on many thousands of words, not on one sentence.) In order to select a currently undefined word to look up, the host machine executing this algorithm must determine the address of a selected processor. The figure indicates that one way to do this is to take the minimum address of the processors that are currently selected. This can be done in constant time on the CM. This improved dictionary lookup method is useful when the dictionary is much larger than the number of unique words contained in the text to be indexed. However, since the same basic operation is used to broadcast definitions as in the first algorithm, it is clear that this second implementation of a dictionary will not be feasible when a text contains more than a few thousand unique words. By analyzing a number of online texts ranging in size from 2,000 words to almost 60,000 words, it was found that as the size of the text approaches many tens of thousands of words, the number of unique words increased into the thousands. Therefore, it can be concluded that the second implementation of the broadcasting dictionary algorithm is not feasible when there are more than a few tens of thousands of words in the text file to be indexed. 6 Making Efficient Use of Parallel Hardware In both of the above algorithms, the &amp;quot;heart&amp;quot; of the dictionary resided in the serial host. In the first case, the heart was the lists that represented sets of words; in the second case, the heart was the call to a serial dictionary lookup procedure. Perhaps if the heart of the dictionary could be stored in the CM, alongside the words from the text, the lookup process could be accelerated. 7 Implementation of Dictionary Lookup by Parallel Hashing One possible approach to dictionary lookup would be to create a hash code for each word in each CM processor in parallel. The hash code represents the address of a different processor. Each processor can then send a lookup request to the processor at the hash-code address, where</abstract>
<title confidence="0.722475666666667">Rgure 3. Unique Wont Diceonaty Implementation Format of Processor Diagram: Sleep • Peasqnr Defintbon Bits: BBBB Defined-yet? D Bladte Selected</title>
<note confidence="0.820634571428571">130 a Select all processors, set original address field to be the processor number: the-I boy-2 ate-3 the-4 putss-5 B: bits B: bits B: bits B: bits B: bits N: 1 N: 2 N: 3 N: 5</note>
<phone confidence="0.661648">111 111</phone>
<abstract confidence="0.979670113402062">b. Call sort with string as the key, and string and N as the fields to copy. The final result is: ate-h boy-2 putsa-3 the-4 the-5 B: bit B: bits B: bits B: bits B: bits N: 3 N: N: 5 N: 1 N: 4 • 8.1 Parallel Sorting A parallel sort is similar in function to a serial sort. It accepts as arguments a parallel data field and a parallel comparison predicate, and it sorts among the selected processors so that the data in each successive (by address) processor increases monotonically. There are parallel sorting algorithms that execute in time proportional to the square of the logarithm of the number of items to be sorted. One easily implemented sort, the enumerate-and-pack sort, takes about 1.5 milliseconds per bit to sort 64,000 numbers on the CM. Thus, it takes 48 milliseconds to sort 64,000 32-bit numbers. Figure 4 illustrates the effect a parallel sort has on a single sentence. Notice that pointers back to the original location of each word can be attached to words before the textual order of the words is scrambled by the sort. 8.2 Scan: Spreading Information in Logarithmic Time Figure 4. Illustration ot Sort - Pk, Fortnat ot Processor Diagram: Definition Bits: BBBB OriginakAddress: N Black d Selected: the definition of the word that hashes to that address has been stored in advance. The processors that receive requests would then respond by sending back the prestored definition of their word to the address contained in the request packet. problem with this approach is that the processors containing a given word will send a request for a definition to the same hashed address. To some extent, this problem can be ameliorated by broadcasting a list of the n (i.e. 200) most common words in English, before attempting any dictionary lookup cycles. Another problem with this approach is that the hash code itself will cause collisions between different text words that hash to the same value. 8 An Efficient Dictionary Algorithm There is a faster and more elegant approach to building a dictionary than the hashing scheme. This other approach has the additional advantage that it can be built from two generally useful submodules each of which has a regular, easily debugged structure. first submodule is the the second is the scan function. After describing the two submodules, a simple version of the fast dictionary algorithm will be presented, along with suggestions for dealing with memory and processor limitations. A scan algorithm takes an associative function of two arguments, call it F, and quickly applies it to data field values in successive processors of: • a • b • c • d • e The scan algorithm produces output fields in the same processors with the values: • a • F(a, b) • F(F(a, b), c) • F(F(F(a, b), c), d) • etc. The key point is that a scan algorithm can take advantage of the associative law and perform this task in logarithmic time. Thus, 16 applications of F are sufficient to scan F across 64,000 processors. Figure 5 shows one possible scheme for implementing scan. While the scheme in the diagram is based on a simple linked list structure, scan may also be implemented on binary trees, hypercubes, and other graph data structures. The nature of the routing system of a particular parallel computer will select which data structures can be scanned most rapidly and efficiently. 131 Figure 5. Illustration of Scan of Processor Backward pointer can be calculated in constant time: all processors send their own addresses to the processors pointed to by P. f is any associative function of two arguments a. Select all processors, initialize function value to string, forward pointer to self address + 1. a-I b-2 c-3 d-4 0-5 I-6 h-8 F: a F: C F: d F: e F: b Fi C F: F.: 2 P: 3 P: 4 P: 5 P: 6 P: 7 F.: 8 Pi • • • • • • • b. Get back pointer, get function value from processor at back pointer, call this value BF. Replace the current function value, F, with f(BF,F): a-1 b-2 C-3 d-4 0-5 f -6 g-7 5-8 a (0.6) I(0.C) f(c.a) tide) flea) 4IM P: 3 P: 4 P: 5 P: 6 P: 7 P: 8 P: • • • c. Calculate a forward pointer that goes twice as far as the current forward pointer. This can be done as follows: Get the value of P at the processor pointed to by your own P, and replace your own P with that new value: a-I b-2 C-3 0-4 0-5 1-6 1,8 a f(a.b) 1(04) f(c.• f(em 111.8) 4IM P: 4 F.: 5 P: 6 P: 7 P: P: • • a d. If any processor has a valid forward pointer goto b next execution of b has the following effect on the first Note that since f &apos;s associative, (a, f(b, c)) is always equal to f( f (a,b), c), and f( f(a,b), f(c,d)) f( f( f(a, b), c), d) When combined with an appropriate F, scan has applications in a variety of contexts. For example, scan is useful in the parallel enumeration of objects and for region labeling. Just as the FFT can be used to efficiently solve many problems involving polynomials, scan can be used to create efficient programs that operate on graphs, and in particular on linked lists that contain natural,language text. First, an alphabetic sort is applied in parallel to all processors, with the word stored in each processor serving as the primary key, and the DEFINED? bit acting as a secondary key. The result will be that all copies of a given word are grouped together into sequential (by processor address) lists, with the single dictionary copy of each word immediately preceding any and all text copies of the same word. The definitions that are contained in the dictionary processors can then be distributed to all of the text words in logarithmic time by scanning the processors with the following associative function f: x and y are processors that have the following ;; fields or parallel variables: STRING (a word) DEFINED? (1 if word contains a correct definition) ORIGINAL-ADDRESS (where word resided before sort) DEFINITION (initially correct only in dictionary words) function f returns a variable containing the same four fields. This is a pseudo language; the actual program was written in *Lisp. function f(x,y): f.STRING = y.STRING f.ORIGINAL-ADDRESS = y.ORIGINAL-ADDRESS if y.DEFINED? = 1 then if y is defined, just return y f.DEFINED? = 1 f.DEFINITION = y.DEFINITION } else if x.STRING = y.STRING then words same, take any definition that x may have f.DEFINED? = x.DEFINED? f.DEFINITION = x.DEFINITION 6-1 D-2 c-3 5-4 4.1.1)) 40,C)) I) 1(cd)) P: 3 P: 5 6 1•1. Function value: F Forward pointer: P (Po an cm addresss) else 8.3 Application of Scan and Sort to Dictionary Lookup To combine these two modules into a dictionary, we need to allocate a bit, DEFINED?, that is 1 only in processors that contain a valid definition of their word. Initially, it is 1 in the processors that contain words from the dictionary, and 0 in processors that contain words that come from the text to be looked up. The DEFINED? bit will be used by the algorithm as it assigns definitions to text words. As soon as a word receives its definition, it will have its DEFINED? bit turned on. The word can then begin to serve as an additional copy of the dictionary entry for the remainder of the lookup cycle. (This is the &amp;quot;trick&amp;quot; that allows scan to execute in logarithmic time.) no definition yet f.DEFINED? = 0 note that text words that are not found in the dictionary correctly end up with DEFINED? = 0 This function F will spread dictionary definitions from a definition to all of the words following it (in processor address order), up until the next dictionary word. Therefore, each word will have its own copy of the dictionary definition of that word. All that remains is to have a single routing cycle that sends each definition back to the original location of its text word. Figure 6 illustrates the execution of the entire sort-scan algorithm on a sample sentence. 132 Next, the following function creates the field that will be scanned: Figures. Illustration of Sort-Scan Algorithm</abstract>
<note confidence="0.581907444444445">of Processor Diagram: Defined? Original-Address: N 8100sf Selected: a. Both the dictionary words and the text words are stored in the CM ate-3D?: W N; 0 Me-4D?: B: W 0 D?: 0 D?: 0 D?: 0 - 4 - D?: 1 D?: 1 D?: 1 D?: 1 W 1 N: 2 3 j B: 100 B: 010 B: 011 B: 001 I N: N: • • • c. Scan 1 b. Peform Me2 an alphabetic sort: doo-5 dictionary Text (Merge Dict onary in o text) bov-3 boy-4 D?: 1 07: 0 W 3 D?: 1 D?: 0 B: j 0 mal! D?: 0 B: 100 N: 2 D?: 1 B IP: 1 137: 0 IV: 4 11 B: 011 5 B: 001 N: B: - N: 1</note>
<date confidence="0.286826">111</date>
<abstract confidence="0.928448110497237">us rig the F described in the text: ate-2 bofp3 boy-4 Oo0-5 pizza-6 the-8 he-9 D?: Di: 1 D?: 1 CO: 1 D?: 1 D?: 0 D?: 1 D?: 1 D?: 1 B: 10 N. B: 100 010.1W B: 011 - 0: 001 B: 011 : 011 N: 3 N: 2 N: - : 4 U • Definition Definition not used not In dictionary d. Send definition back to original address the-1 bon-2 110-3 the-4 pral-5 ate-6 cloo-8 the-9 D?: D?: 1 D?: 1 D?: 1 D?: 0 DT: 1 D?: 1 D?: 1 D?: 1 B: 001 B: 001 B: 100 B: B: 100 B: 011 B: 001 N: 1 N: 2 N: N: 4 N: 5 N. N: - N: V U • • U ;; header-p is a parallel boolean variable that is true in headers, false otherwise function create-field-for-scan(header-p): define a type for a large bit field var FIELD : record most significant bits contain processor address ADDRESS least significant bits will contain the definition DEFINITION end ;;initialize to address 0, no definition FIELD.ADDRESS = 0 FIELD.DEFINITION = 0 next, the headers that are dictionary words store their definitions in the correct part of FIELD Non-dictionary headers (text words not found in dictionary) are given null definitions. if header FIELD.DEFINITION = definition Text Dict onary Sart-Scam Dictionary Algorithm Since the CM is a bit serial Machine, string operations are relatively expensive operation. The dictionary function F described above performs a string comparison and a string copy operation each time it is invoked. On a full size CM, the function is invoked 16 times (log 64K words). A simple optimization can be made to the sort-scan algorithm that allows the string comparison to be performed only once. This allows a faster dictionary function that performs no string comparisons to be used. The optimization consists of two parts. First, a new stage is inserted after the sorting step, before the scanning step. In this new step, each word is compared to the word to its left, and if it is different, it is marked as a &amp;quot;header.&amp;quot; Such words begin a new segment of identical words. All dictionary words are headers, because the sort places them before all occurrences of identical words. In addition, the first word of each group of words that does not occur in the dictionary is also marked as a header. contains each address FIELD.ADDRESS = self-address } return (FIELD) Finally, instead of scanning the dictionary function across this field, the maximum function (which returns the maximum of two input numbers) is scanned across it. Definitions will propagate from a header to all of the words within its segment, but they will not cross past the next header. This is because the next header has a greater self-address in the most significant bits of the field being scanned, and the maximum function selects it rather than the earlier header g smaller field value. If a header had no definition, because a word was not found in the dictionary, the null definition would be propagated to all copies of that word. The process of scanning the maximum function across a field was determined to be generally useful. As a result, the max-scan function was implemented in an efficient pipelined, bit-serial manner by Guy Blelloch, and was incorporated into the general library of CM functions. 133 n. In headers only, set the A to the self address and the D to the definition, if there is one. ate-1 ate-2 boy-3 137: 1 0?: 0 D?: 1 D?: 0 07: 1 EL 100 B: 13: 010 B: 011 H?: H?: 0 H?: 1 H?: 0 H?: 1 AM 1:100 AS 00 AD 3010 Ail 0:0 A:0 5011 • • C. Scan the Maximum function across the A:D ate-2 boy-3 boy-4 1)?: D?: 0 0?: 0?: 0 13?: 1 13: 100 B. - B: 010 B: 13: 011 j H?: 1 H?: 0 H?: H?: 0 HT: 1 0:0 1:100 AM 11 AS 3010 AS 3:010 A:0 5011 • • d. Copy definition bits from D to B. and set D? if defined. ate-1 ate-2 boy-3 003-5 DI: 1 D?: 1 D?: D?. 0?: 1 B: 100 13: 100 B: 010 B: 010 13: 011 H?: AS 1 H7: 0 H?: 1 A:D 0 H?: 1 1:100 0:1) 1:100 AD 3010 3010 A:0 Figure 7 illustrates the creation of this field, and the scanning of the maximum function across it. Note that the size of the field being scanned is the size of the definition (8 bits for the timings below) plus the size of a processor address (16 bits). In comparison, the earlier dictionary function had to be scanned across the definition and the original address, along with the entire string. Scanning this much larger field, even if the dictionary function was as fast as the maximum function, would necessarily result in slower execution times. 8.5 Evaluation of the Sort-Scan Dictionary Algorithm The improved sort-scan dictionary algorithm is much more efficient than the broadcasting algorithms described earlier. The algorithm was implemented and timed on a Connection Machine. In a bit-serial computer like the CM, the time needed to process a string grows linearly with the number of bits used to store the string. A string length of 8 characters is adequete for the CM-Indexer. Words longer than 8 characters are represented by the simple concatenation of their first 4 and last 4 characters. ASCII characters therefore require 64 bits per word in the CM; 4 more bits are used for a length count. Because dictionary lookup is only performed on alphabetic characters, the 64 bits of ASCII data described above can be compacted without collision. Each of the twenty-six letters of the alphabet can be represented using 5 bits, instead of 8, thereby reducing the length of the character field to 40 bits; 4 bits are still needed for the length count. Additional compression could be achieved, perhaps by hashing, although that would inthe possibility of collisions. No additional compression is performed in the prototype implementation. The timings given below assume that each processor stores an 8 character word using 44 bits. First of all, to sort a bit field in the CM currently about 1.5 milliseconds per bit. Second, the function that finds the header words was timed and took less than 4 milliseconds to execute. The scan of the max function across all of the processors completed in under in 2 milliseconds. The routing cycle to return the definitions to the original processors of the text took approximately one millisecond to complete. As a result, with the improved sort-scan algorithm, an entire machine full of 64,000 words can be looked up in about 73 milliseconds. In comparison to this, the original sort-scan implementation requires an additional 32 milliseconds (2 milliseconds per invocation of the slow dictionary function), along with a few more milliseconds for the inefficient communications pattern it requires. This lookup rate is approximately equivalent to a serial dictionary lookup of .9 words per microsecond. In comparison, a Symbolics Lisp Machine can look up words at a rate of 1/500 words per microsecond. (The timing was made for a lookup of a single bit of information about a word in a hash table containing 1500 words). Thus, the CM can perform dictionary lookup about 450 times faster than the Lisp Machine. 8.6 Coping with Limited Processor Resources Since there are obviously more than 64,000 words in the English language, a dictionary containing many words will have to be handled in sections. Each dictionary processor will have to hold several dictionary words, and the look-up cycle will have to be repeated several times. These adjustments will slow the CM down by a multiplicative factor, but Lisp Machines also slow down when large hash tables (often paged out to disk) are queried. There is an alternative way to view the above algorithm modifications: since they are motivated by limited processor resources, they should be handled by some sort of run time package, just as virtual memory is used to handle the problem of limited physical memory resources on serial machines. In fact, a virtual processor facility is currently being used on the CM.</abstract>
<note confidence="0.8630194">Figure 7. Illustration of Improvements to Sod-Scan Algorithm Format of Processor Diagram: a. Atter sort, detect the headers (words different from lett neighbor) ate-1 ate-2 boy-3 boy-4 obg-5 D?: 1 DT: 0 DT. 1 D?: 0 D?: B: 100 B: B: 010 01; 11?: AD 1 H?: 0 H?: 0:0 H?: 00 H?: 0:0 00 A:0 00 AD A:13 kD • • geom., • Pmr44sor</note>
<title confidence="0.67118425">Defined? B Header? Black If Selected:</title>
<pubnum confidence="0.327089">134</pubnum>
<abstract confidence="0.985311644444445">Applications of Scan to Bulk Processing of Text The scan algorithm has many other applications in text processing. For example, it can be used to lexically parse text in the form of 1 character per processor into the form of 1 word per processor. Syntactic rules could rapidly determine which characters begin and end words. Scan could then be used to enumerate how many words there are, and what position each character occupies within its word. The processors could then use this information to send their characters to the word-processor at which they belong. Each word-processor would receive the characters making up its word and would assemble them into a string. Another application of scan, suggested by Guy L. Steele, Jr., would be as a regular expression parser, or lexer. Each word in the CM is viewed as a transition matrix from one set of finite automata states to another set. Scan is used, along with an F which would have the effect of composing transition matrices, to apply a finite automata to many sentences in parallel. After this application of scan, the last word in each sentence contains the state that a finite automata parsing the string would reach. The lexer&apos;s state transition function F would be associative, since string concatenation is associative, and the purpose of a lexer is to discover which particular strings/tokens were concatenated to create a given string/file. The experience of actually implementing parallel natural language programs on real hardware has clarified which operations and programming techniques are the most efficient and useful. Programs that build upon algorithms such as sort and scan are easier to debug than programs that attempt a direct assault on a problem (i.e. the hashing scheme discussed earlier; or a slow, hand-coded regular expression parser that I implemented). Despite their ease of implementation, programs based upon generally useful submodules often are more efficient than specialized, hand-coded programs. Acknowledgements I would like to thank Dr. David Waltz for his help in this research and in reviewing a draft of this paper. I would also like to thank Dr. Stephen Omohundro, Cliff Lasser, and Guy Blelloch for their suggestions concerning the implementation of the dictionary algorithm.</abstract>
<note confidence="0.666240653846154">References Selim G. Sorting Algorithms, Academic Press, Inc. Carl Richard, and Guy L. Steele Jr. Connec- Machine Macroinstruction Set, REL 2.3., Machines Corporation. (to appear) W. Daniel. Connection Machine, The MIT Press, Cambridge, MA. Clifford A., and Stephen M. Omohundro. *Lisp Thinking Machines Corporation. (to appear) Leiserson, Charles, and Bruce Maggs. &amp;quot;Communication- Efficient Parallel Graph Algorithms,&amp;quot; Laboratory for Computer Science, Massachusetts Institute of Technology. (to appear) (Note: scan is a special case of the described in this paper.) Omohundro, Steven M. &amp;quot;A Connection Machine Algorithms Primer,&amp;quot; Thinking Machines Corporation. (to appear) Howard. Illusion of Reality, in preparation. David L. and Jordan &amp;quot;Massively Parallel Parsing: A Strongly Interactive Model of Natural Interpretation,&amp;quot; Science, 9, Number 1, pp. 51-74, January-March, 1985. 135</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Selim G Akl</author>
</authors>
<title>Parallel Sorting Algorithms,</title>
<date>1985</date>
<publisher>Academic Press, Inc.</publisher>
<marker>Akl, 1985</marker>
<rawString>Akl, Selim G. Parallel Sorting Algorithms, 1985, Academic Press, Inc.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Carl Richard Feynman</author>
<author>Guy L Steele Jr</author>
</authors>
<booktitle>Connection Machine Macroinstruction Set, REL 2.3., Thinking Machines Corporation.</booktitle>
<note>(to appear)</note>
<marker>Feynman, Jr, </marker>
<rawString>Feynman, Carl Richard, and Guy L. Steele Jr. Connection Machine Macroinstruction Set, REL 2.3., Thinking Machines Corporation. (to appear)</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daniel Hillis</author>
</authors>
<title>The Connection Machine,</title>
<date>1985</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Hillis, 1985</marker>
<rawString>Hillis, W. Daniel. The Connection Machine, 1985, The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Clifford A Lasser</author>
<author>Stephen M Omohundro</author>
</authors>
<title>The Essential *Lisp Manual, Thinking Machines Corporation.</title>
<note>(to appear)</note>
<marker>Lasser, Omohundro, </marker>
<rawString>Lasser, Clifford A., and Stephen M. Omohundro. The Essential *Lisp Manual, Thinking Machines Corporation. (to appear)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Charles Leiserson</author>
<author>Bruce Maggs</author>
</authors>
<title>CommunicationEfficient Parallel Graph Algorithms,&amp;quot;</title>
<institution>Laboratory for Computer Science, Massachusetts Institute of Technology.</institution>
<note>(to appear)</note>
<marker>Leiserson, Maggs, </marker>
<rawString>Leiserson, Charles, and Bruce Maggs. &amp;quot;CommunicationEfficient Parallel Graph Algorithms,&amp;quot; Laboratory for Computer Science, Massachusetts Institute of Technology. (to appear) (Note: scan is a special case of the treefix algorithm described in this paper.)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Steven M Omohundro</author>
</authors>
<title>A Connection Machine Algorithms Primer,&amp;quot; Thinking Machines Corporation.</title>
<note>(to appear)</note>
<marker>Omohundro, </marker>
<rawString>Omohundro, Steven M. &amp;quot;A Connection Machine Algorithms Primer,&amp;quot; Thinking Machines Corporation. (to appear)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Resnikoff</author>
</authors>
<title>The Illusion of Reality,</title>
<date>1985</date>
<note>in preparation.</note>
<marker>Resnikoff, 1985</marker>
<rawString>Resnikoff, Howard. The Illusion of Reality, 1985, in preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Waltz</author>
<author>Jordan B Pollack</author>
</authors>
<title>Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation,&amp;quot;</title>
<date>1985</date>
<journal>Cognitive Science, Volume 9, Number</journal>
<volume>1</volume>
<pages>51--74</pages>
<location>January-March,</location>
<marker>Waltz, Pollack, 1985</marker>
<rawString>Waltz, David L. and Jordan B. Pollack. &amp;quot;Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation,&amp;quot; Cognitive Science, Volume 9, Number 1, pp. 51-74, January-March, 1985.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>