<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023623">
<note confidence="0.538997666666667">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.997088">
Optimizing Feature Set for Chinese Word Sense Disambiguation
</title>
<author confidence="0.998958">
Zheng-Yu Niu, Dong-Hong Ji
</author>
<affiliation confidence="0.994362">
Institute for Infocomm Research
</affiliation>
<address confidence="0.964139">
21 Heng Mui Keng Terrace
119613 Singapore
</address>
<email confidence="0.996487">
{zniu, dhji}@i2r.a-star.edu.sg
</email>
<author confidence="0.981713">
Chew-Lim Tan
</author>
<affiliation confidence="0.892391333333333">
Department of Computer Science
National University of Singapore
3 Science Drive 2
</affiliation>
<address confidence="0.711303">
117543 Singapore
</address>
<email confidence="0.997071">
tancl@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.995608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999658">
This article describes the implementation of I2R
word sense disambiguation system (I2R − WSD)
that participated in one senseval3 task: Chinese lex-
ical sample task. Our core algorithm is a supervised
Naive Bayes classifier. This classifier utilizes an op-
timal feature set, which is determined by maximiz-
ing the cross validated accuracy of NB classifier on
training data. The optimal feature set includes part-
of-speech with position information in local con-
text, and bag of words in topical context.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999751793103448">
Word sense disambiguation (WSD) is to assign ap-
propriate meaning to a given ambiguous word in
a text. Corpus based method is one of the suc-
cessful lines of research on WSD. Many supervised
learning algorithms have been applied for WSD,
ex. Bayesian learning (Leacock et al., 1998), ex-
emplar based learning (Ng and Lee, 1996), decision
list (Yarowsky, 2000), neural network (Towel and
Voorheest, 1998), maximum entropy method (Dang
et al., 2002), etc.. In this paper, we employ Naive
Bayes classifier to perform WSD.
Resolving the ambiguity of words usually relies
on the contexts of their occurrences. The feature
set used for context representation consists of lo-
cal and topical features. Local features include part
of speech tags of words within local context, mor-
phological information of target word, local collo-
cations, and syntactic relations between contextual
words and target word, etc.. Topical features are
bag of words occurred within topical context. Con-
textual features play an important role in providing
discrimination information for classifiers in WSD.
In other words, an informative feature set will help
classifiers to accurately disambiguate word senses,
but an uninformative feature set will deteriorate the
performance of classifiers. In this paper, we opti-
mize feature set by maximizing the cross validated
accuracy of Naive Bayes classifier on sense tagged
training data.
</bodyText>
<sectionHeader confidence="0.97907" genericHeader="method">
2 Naive Bayes Classifier
</sectionHeader>
<bodyText confidence="0.988062777777778">
Let C = {c1, c2, ..., cL} represent class labels,
F = {f1, f2, ..., fM} be a set of features. The
value of fj, 1 ≤ j ≤ M, is 1 if fj is present in
the context of target word, otherwise 0. In classi-
fication process, the Naive Bayes classifier tries to
find the class that maximizes P(ci|F), the proba-
bility of class ci given feature set F, 1 ≤ i ≤ L.
Assuming the independence between features, the
classification procedure can be formulated as:
</bodyText>
<equation confidence="0.999193">
p(ci)�Mj=1 p(fj|ci) (1)
rlMj= 1 p(fj )
</equation>
<bodyText confidence="0.999964">
where p(ci), p(fj|ci) and p(fj) are estimated using
maximum likelihood method. To avoid the effects
of zero counts when estimating p(fj|ci), the zero
counts of p(fj|ci) are replaced with p(ci)/N, where
N is the number of training examples.
</bodyText>
<sectionHeader confidence="0.9896" genericHeader="method">
3 Feature Set
</sectionHeader>
<bodyText confidence="0.999839636363636">
For Chinese WSD, there are two strategies to extract
contextual information. One is based on Chinese
characters, the other is to utilize Chinese words and
related morphological or syntactic information. In
our system, context representation is based on Chi-
nese words, since words are less ambiguous than
characters.
We use two types of features for Chinese WSD:
local features and topical features. All of these fea-
tures are acquired from data at senseval3 without
utilization of any other knowledge resource.
</bodyText>
<subsectionHeader confidence="0.998779">
3.1 Local features
</subsectionHeader>
<bodyText confidence="0.996220555555556">
Two sets of local features are investigated, which
are represented by LocalA and LocalB. Let nl de-
note the local context window size.
LocalA contains only part of speech tags
with position information: POS−nl, ...,
POS−1, POS0, POS+1, ..., POS+nl, where
POS−i (POS+i) is the part of speech (POS) of the
i-th words to the left (right) of target word w, and
POS0 is the POS of w.
</bodyText>
<equation confidence="0.986502">
ˆi = arg max
1&lt;i&lt;L
</equation>
<bodyText confidence="0.999138285714286">
LocalB enriches the local context by including
the following features: local words with position in-
formation (W_nl, ..., W_1, W+1, ..., W+nl), bigram
templates ((W_nl, W_(nl_1)), ..., (W_1, W+1),
..., (W+(nl_1), W+nl)), local words with POS tags
(W POS) (position information is not considered),
and part of speech tags with position information.
All of these POS tags, words, and bigrams are
gathered and each of them contributed as one fea-
ture. For a training or test example, the value of
some feature is 1 if it occurred in local context, oth-
erwise it is 0. In this paper, we investigate two val-
ues of nl for LocalA and LocalB, 1 and 2, which
results in four feature sets.
</bodyText>
<subsectionHeader confidence="0.99871">
3.2 Topical features
</subsectionHeader>
<bodyText confidence="0.999978227272727">
We consider all Chinese words within a context
window size nt as topical features. For each training
or test example, senseval3 data provides one sen-
tence as the context of ambiguous word. In sense-
val3 Chinese training data, all contextual sentences
are segmented into words and tagged with part of
speech.
Words which contain non-Chinese character are
removed, and remaining words occurred within
context window size nt are gathered. Each remain-
ing word is considered as one feature. The value of
topical feature is 1 if it occurred within window size
nt, otherwise it is 0.
In later experiment, we set different values for nt,
ex. 1, 2, 3, 4, 5, 10, 20, 30, 40, 50. Our experimen-
tal result indicated that the accuracy of sense dis-
ambiguation is related to the value of nt. For differ-
ent ambiguous words, the value of nt which yields
best disambiguation accuracy is different. It is de-
sirable to determine an optimal value, ˆnt, for each
ambiguous word by maximizing the cross validated
accuracy.
</bodyText>
<sectionHeader confidence="0.991666" genericHeader="method">
4 Data Set
</sectionHeader>
<bodyText confidence="0.9999822">
In Chinese lexical sample task, training data con-
sists of 793 sense-tagged examples for 20 ambigu-
ous Chinese words. Test data consists of 380 un-
tagged examples for the same 20 target words. Ta-
ble 1 shows the details of training data and test data.
</bodyText>
<sectionHeader confidence="0.998371" genericHeader="method">
5 Criterion for Evaluation of Feature Sets
</sectionHeader>
<bodyText confidence="0.999990090909091">
In this paper, five fold cross validation method was
employed to estimate the accuracy of our classi-
fier, which was the criterion for evaluation of fea-
ture sets. All of the sense tagged examples of some
target word in senseval3 training data were shuf-
fled and divided into five equal folds. We used four
folds as training set and the remaining fold as test
set. This procedure was repeated five times under
different division between training set and test set.
The average accuracy over five runs is defined as the
accuracy of our classifier.
</bodyText>
<sectionHeader confidence="0.977011" genericHeader="method">
6 Evaluation of Feature Sets
</sectionHeader>
<bodyText confidence="0.999694292682927">
Four feature sets were investigated:
FEATUREA1: LocalA with nl = 1, and topical
feature within optimal context window size ˆnt;
FEATUREA2: LocalA with nl = 2, and topical
feature within optimal context window size ˆnt;
FEATUREB1: LocalB with nl = 1, and topical
feature within optimal context window size ˆnt;
FEATUREB2: LocalB with nl = 2, and topical
feature within optimal context window size ˆnt.
We performed training and test procedure using
exactly same training and test set for each feature
set. For each word, the optimal value of topical con-
text window size ˆnt was determined by selecting a
minimal value of nt which maximized the cross val-
idated accuracy.
Table 2 summarizes the results of Naive Bayes
classifier using four feature sets evaluated on sen-
seval3 Chinese training data. Figure 1 shows the
accuracy of Naive Bayes classifier as a function of
topical context window size on four nouns and three
verbs. Several results should be noted specifically:
If overall accuracy over 20 Chinese charac-
ters is used as evaluation criterion for feature
set, the four feature sets can be sorted as fol-
lows: FEAT UREA1 &gt; FEAT UREA2 ≈
FEATUREB1 &gt; FEATUREB2. This indi-
cated that simply increasing local window size or
enriching feature set by incorporating bigram tem-
plates, local word with position information, and lo-
cal words with POS tags did not improve the perfor-
mance of sense disambiguation.
In table 2, it showed that with FEATUREA1, the
optimal topical context window size was less than
10 words for 13 out of 20 target words. Figure
1 showed that for most of nouns and verbs, Naive
Bayes classifier achieved best disambiguation accu-
racy with small topical context window size (&lt;10
words). This gives the evidence that for most of
Chinese words, including nouns and verbs, the near
distance context is more important than the long dis-
tance context for sense disambiguation.
</bodyText>
<sectionHeader confidence="0.99312" genericHeader="method">
7 Experimental Result
</sectionHeader>
<bodyText confidence="0.9993764">
The empirical study in section 6 showed that FEA-
TUREA1 performed best among all the feature sets.
A Naive Bayes classifier with FEATUREA1 as fea-
ture set was learned from all the senseval3 Chinese
training data for each target word. Then we used
</bodyText>
<tableCaption confidence="0.998212">
Table 1: Details of training data and test data in Chinese lexical sample task.
</tableCaption>
<table confidence="0.998695681818182">
Ambiguous word POS occurred # training examples # senses occurred # test examples
in training data in training data
ba3wo4 n v vn 31 4 15
bao1 n nr q v 76 8 36
cai2liao4 n 20 2 10
chong1ji1 v vn 28 3 13
chuan1 v 28 3 14
di4fang1 b n 36 4 17
fen1zi3 n 36 2 16
huo2dong4 a v vn 36 5 16
lao3 Ng a an d j 57 6 26
lu4 n nr q 57 6 28
mei2you3 d v 30 3 15
qi3lai2 v 40 4 20
qian2 n nr 40 4 20
ri4zi5 n 48 3 21
shao3 Ng a ad j v 42 5 20
tu1chu1 a ad v 30 3 15
yan2jiu1 n v vn 30 3 15
yun4dong4 n nz v vn 54 3 27
zou3 v vn 49 5 24
zuo4 v 25 3 12
</table>
<bodyText confidence="0.9982936">
this classifier to determine the senses of occurrences
of target words in test data. The official result of
I2R − W 5D system in Chinese lexical sample task
is listed below:
Precision: 60.40% (229.00 correct of 379.00 at-
tempted).
Recall: 60.40% (229.00 correct of 379.00 in to-
tal).
Attempted: 100.00% (379.00 attempted of
379.00 in total).
</bodyText>
<sectionHeader confidence="0.987068" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999855545454546">
In this paper, we described the implementation of
I2R − W5D system that participated in one sen-
seval3 task: Chinese lexical sample task. An op-
timal feature set was selected by maximizing the
cross validated accuracy of supervised Naive Bayes
classifier on sense-tagged data. The senses of occur-
rences of target words in test data were determined
using Naive Bayes classifier with optimal feature
set learned from training data. Our system achieved
60.40% precision and recall in Chinese lexical sam-
ple task.
</bodyText>
<sectionHeader confidence="0.998291" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.659527">
Dang, H. T., Chia, C. Y., Palmer M., &amp; Chiou, F.D.
(2002) Simple Features for Chinese Word Sense
Disambiguation. In Proc. of COLING.
Leacock, C., Chodorow, M., &amp; Miller G. A. (1998)
Using Corpus Statistics and WordNet Relations
for Sense Identification. Computational Linguis-
tics, 24:1, 147–165.
Mooney, R. J. (1996) Comparative Experiments on
Disambiguating Word Senses: An Illustration of
the Role of Bias in Machine Learning. In Proc.
ofEMNLP, pp. 82-91, Philadelphia, PA.
Ng, H. T., &amp; Lee H. B. (1996) Integrating Multi-
ple Knowledge Sources to Disambiguate Word
Sense: An Exemplar-Based Approach. In Proc.
ofACL, pp. 40-47.
Pedersen, T. (2001) A Decision Tree of Bigrams is
an Accurate Predictor of Word Sense. In Proc. of
NAACL.
Towel, G., &amp; Voorheest, E. M. (1998) Disambiguat-
ing Highly Ambiguous Words. Computational
Linguistics, 24:1, 125–146.
Yarowsky, D. (2000) Hierarchical Decision Lists
for Word Sense Disambiguation. Computers and
the Humanities, 34(1-2), 179–186.
</reference>
<tableCaption confidence="0.993441">
Table 2: Accuracy of Naive Bayes classifier with different feature sets on Senseval3 Chinese training data.
</tableCaption>
<table confidence="0.988625565217391">
FEATUREA1 FEATUREA2 FEATUREB1 FEATUREB2
Ambiguous word ˆnt Accuracy ˆnt Accuracy ˆnt Accuracy ˆnt Accuracy
ba3wo4 5 30.0 4 23.3 4 30.0 3 30.0
bao1 2 30.7 20 34.0 2 33.3 20 32.0
cai2liao4 2 85.0 2 80.0 2 75.0 2 60.0
chong1ji1 20 40.0 3 40.0 30 36.0 1 28.0
chuan1 3 72.0 5 68.0 3 56.0 5 64.0
di4fang1 2 74.3 1 62.9 1 71.4 1 65.7
fen1zi3 20 91.4 50 91.4 20 88.6 20 85.7
huo2dong4 5 40.0 20 51.4 10 42.9 4 40.0
lao3 3 49.1 4 47.3 3 52.7 20 52.7
lu4 1 83.6 2 78.2 2 81.8 1 76.4
mei2you3 20 50.0 20 47.9 4 43.3 3 50.0
qi3lai2 4 75.0 1 75.0 1 80.0 1 77.5
qian2 3 57.5 4 57.5 3 60.0 5 57.5
ri4zi5 4 62.2 4 57.8 10 55.6 4 55.6
shao3 4 45.0 3 50.0 10 42.5 20 50.0
tu1chu1 10 83.3 10 80.0 10 80.0 10 76.7
yan2jiu1 20 43.3 20 46.7 10 50.0 20 36.7
yun4dong4 10 64.0 10 66.0 10 62.0 10 58.0
zou3 5 44.4 5 44.4 4 51.1 4 51.1
zuo4 20 64.0 30 60.0 20 64.0 20 64.0
Overall 57.7 56.9 57.0 55.1
</table>
<figure confidence="0.963169416666667">
nt
nt
0 1 2 3 4 5 10 20 30 40 50
0.9
0.8
0.7
0.6
0.5
0.4
1
cai2liao4
fen1zi3
qian2
ri4zi5
0 1 2 3 4 5 10 20 30 40 50
0.8
0.7
0.6
0.5
0.4
0.3
chuan1
qi3lai2
zuo4
</figure>
<figureCaption confidence="0.9935035">
Figure 1: Accuracy of Naive Bayes classifier with the optimal feature set FEATUREA1 on four nouns (top
figure) and three verbs (bottom figure). The horizontal axis represents the topical context window size.
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.150374">
<note confidence="0.676483666666667">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics</note>
<title confidence="0.999538">Optimizing Feature Set for Chinese Word Sense Disambiguation</title>
<author confidence="0.992892">Zheng-Yu Niu</author>
<author confidence="0.992892">Dong-Hong</author>
<affiliation confidence="0.998491">Institute for Infocomm</affiliation>
<address confidence="0.953498">21 Heng Mui Keng 119613</address>
<email confidence="0.853466">Chew-Lim</email>
<affiliation confidence="0.998004333333333">Department of Computer National University of 3 Science Drive</affiliation>
<address confidence="0.962903">117543</address>
<email confidence="0.983194">tancl@comp.nus.edu.sg</email>
<abstract confidence="0.955116363636364">article describes the implementation of sense disambiguation system that participated in one senseval3 task: Chinese lexical sample task. Our core algorithm is a supervised Naive Bayes classifier. This classifier utilizes an optimal feature set, which is determined by maximizing the cross validated accuracy of NB classifier on training data. The optimal feature set includes partof-speech with position information in local context, and bag of words in topical context.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H T Dang</author>
<author>C Y Chia</author>
<author>M Palmer</author>
<author>F D Chiou</author>
</authors>
<title>Simple Features for Chinese Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="1468" citStr="Dang et al., 2002" startWordPosition="216" endWordPosition="219">B classifier on training data. The optimal feature set includes partof-speech with position information in local context, and bag of words in topical context. 1 Introduction Word sense disambiguation (WSD) is to assign appropriate meaning to a given ambiguous word in a text. Corpus based method is one of the successful lines of research on WSD. Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc.. In this paper, we employ Naive Bayes classifier to perform WSD. Resolving the ambiguity of words usually relies on the contexts of their occurrences. The feature set used for context representation consists of local and topical features. Local features include part of speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and target word, etc.. Topical features are bag of words occurred within topical context. Contextual features play an important role in providing discrimination informat</context>
</contexts>
<marker>Dang, Chia, Palmer, Chiou, 2002</marker>
<rawString>Dang, H. T., Chia, C. Y., Palmer M., &amp; Chiou, F.D. (2002) Simple Features for Chinese Word Sense Disambiguation. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>G A Miller</author>
</authors>
<title>Using Corpus Statistics and WordNet Relations for Sense Identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>147--165</pages>
<contexts>
<context position="1304" citStr="Leacock et al., 1998" startWordPosition="191" endWordPosition="194"> algorithm is a supervised Naive Bayes classifier. This classifier utilizes an optimal feature set, which is determined by maximizing the cross validated accuracy of NB classifier on training data. The optimal feature set includes partof-speech with position information in local context, and bag of words in topical context. 1 Introduction Word sense disambiguation (WSD) is to assign appropriate meaning to a given ambiguous word in a text. Corpus based method is one of the successful lines of research on WSD. Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc.. In this paper, we employ Naive Bayes classifier to perform WSD. Resolving the ambiguity of words usually relies on the contexts of their occurrences. The feature set used for context representation consists of local and topical features. Local features include part of speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and t</context>
</contexts>
<marker>Leacock, Chodorow, Miller, 1998</marker>
<rawString>Leacock, C., Chodorow, M., &amp; Miller G. A. (1998) Using Corpus Statistics and WordNet Relations for Sense Identification. Computational Linguistics, 24:1, 147–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Mooney</author>
</authors>
<title>Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning.</title>
<date>1996</date>
<booktitle>In Proc. ofEMNLP,</booktitle>
<pages>82--91</pages>
<location>Philadelphia, PA.</location>
<marker>Mooney, 1996</marker>
<rawString>Mooney, R. J. (1996) Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning. In Proc. ofEMNLP, pp. 82-91, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>H B Lee</author>
</authors>
<title>Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach.</title>
<date>1996</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="1348" citStr="Ng and Lee, 1996" startWordPosition="199" endWordPosition="202">r. This classifier utilizes an optimal feature set, which is determined by maximizing the cross validated accuracy of NB classifier on training data. The optimal feature set includes partof-speech with position information in local context, and bag of words in topical context. 1 Introduction Word sense disambiguation (WSD) is to assign appropriate meaning to a given ambiguous word in a text. Corpus based method is one of the successful lines of research on WSD. Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc.. In this paper, we employ Naive Bayes classifier to perform WSD. Resolving the ambiguity of words usually relies on the contexts of their occurrences. The feature set used for context representation consists of local and topical features. Local features include part of speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and target word, etc.. Topical features are bag o</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Ng, H. T., &amp; Lee H. B. (1996) Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach. In Proc. ofACL, pp. 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>A Decision Tree of Bigrams is an Accurate Predictor of Word Sense.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Pedersen, 2001</marker>
<rawString>Pedersen, T. (2001) A Decision Tree of Bigrams is an Accurate Predictor of Word Sense. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Towel</author>
<author>E M Voorheest</author>
</authors>
<title>Disambiguating Highly Ambiguous Words.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>125--146</pages>
<contexts>
<context position="1424" citStr="Towel and Voorheest, 1998" startWordPosition="209" endWordPosition="212">ined by maximizing the cross validated accuracy of NB classifier on training data. The optimal feature set includes partof-speech with position information in local context, and bag of words in topical context. 1 Introduction Word sense disambiguation (WSD) is to assign appropriate meaning to a given ambiguous word in a text. Corpus based method is one of the successful lines of research on WSD. Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc.. In this paper, we employ Naive Bayes classifier to perform WSD. Resolving the ambiguity of words usually relies on the contexts of their occurrences. The feature set used for context representation consists of local and topical features. Local features include part of speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and target word, etc.. Topical features are bag of words occurred within topical context. Contextual features play an importa</context>
</contexts>
<marker>Towel, Voorheest, 1998</marker>
<rawString>Towel, G., &amp; Voorheest, E. M. (1998) Disambiguating Highly Ambiguous Words. Computational Linguistics, 24:1, 125–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Hierarchical Decision Lists for Word Sense Disambiguation. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="1380" citStr="Yarowsky, 2000" startWordPosition="205" endWordPosition="206">imal feature set, which is determined by maximizing the cross validated accuracy of NB classifier on training data. The optimal feature set includes partof-speech with position information in local context, and bag of words in topical context. 1 Introduction Word sense disambiguation (WSD) is to assign appropriate meaning to a given ambiguous word in a text. Corpus based method is one of the successful lines of research on WSD. Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc.. In this paper, we employ Naive Bayes classifier to perform WSD. Resolving the ambiguity of words usually relies on the contexts of their occurrences. The feature set used for context representation consists of local and topical features. Local features include part of speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and target word, etc.. Topical features are bag of words occurred within topical </context>
</contexts>
<marker>Yarowsky, 2000</marker>
<rawString>Yarowsky, D. (2000) Hierarchical Decision Lists for Word Sense Disambiguation. Computers and the Humanities, 34(1-2), 179–186.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>