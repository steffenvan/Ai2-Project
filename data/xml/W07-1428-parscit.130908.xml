<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.9878295">
A Discourse Commitment-Based Framework for Recognizing Textual
Entailment
</title>
<author confidence="0.36552">
Andrew Hickl and Jeremy Bensley
</author>
<affiliation confidence="0.22982">
Language Computer Corporation
</affiliation>
<address confidence="0.5954765">
1701 North Collins Boulevard
Richardson, Texas 75080 USA
</address>
<email confidence="0.99854">
{andy,jeremy}@languagecomputer.com
</email>
<sectionHeader confidence="0.99666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835375">
In this paper, we introduce a new framework
for recognizing textual entailment which de-
pends on extraction of the set of publicly-
held beliefs – known as discourse commit-
ments – that can be ascribed to the author of
a text or a hypothesis. Once a set of commit-
ments have been extracted from a t-h pair,
the task of recognizing textual entailment is
reduced to the identification of the commit-
ments from a t which support the inference
of the h. Promising results were achieved:
our system correctly identified more than
80% of examples from the RTE-3 Test Set
correctly, without the need for additional
sources of training data or other web-based
resources.
</bodyText>
<sectionHeader confidence="0.998863" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9988234375">
Systems participating in the previous two PAS-
CAL Recognizing Textual Entailment (RTE) Chal-
lenges (Bar-Haim et al., 2006) have successfully em-
ployed a variety of “shallow” techniques in order to
recognize instances of textual entailment, including
methods based on: (1) sets of heuristics (Vander-
wende et al., 2006), (2) measures of term overlap
(Jijkoun and de Rijke, 2005), (3) the alignment of
graphs created from syntactic or semantic dependen-
cies (Haghighi et al., 2005), or (4) statistical classi-
fiers which leverage a wide range of features, includ-
ing the output of paraphrase generation (Hickl et al.,
2006) or model building systems (Bos and Markert,
2006).
While relatively “shallow” approaches have
shown much promise in RTE for entailment pairs
where the text and hypothesis remain short, we ex-
pect that performance of these types of systems will
ultimately degrade as longer and more syntactically
complex entailment pairs are considered. In order
to remain effective as texts get longer, we believe
that RTE systems will need to employ techniques
that will enable them to enumerate the set of propo-
sitions which are inferable – whether asserted, pre-
supposed, or conventionally or conversationally im-
plicated – from a text-hypothesis pair.
In this paper, we introduce a new framework for
recognizing textual entailment which depends on ex-
traction of the set of publicly-held beliefs – or dis-
course commitments – that can be ascribed to the
author of a text or a hypothesis. We show that once
a set of discourse commitments have been extracted
from a text-hypothesis pair, the task of recognizing
textual entailment can be reduced to the identifica-
tion of the one (or more) commitments from the
text which are most likely to support the inference
of each commitment extracted from the hypothesis.
More formally, we assume that given a commitment
set {ct} consisting of the set of discourse commit-
ments inferable from a text t and a hypothesis h, we
define the task of RTE as a search for the commit-
ment c E {ct} which maximizes the likelihood that
c textually entails h.
The rest of this paper is organized in the fol-
lowing way. Section 2 provides a sketch of the
system we used in the PASCAL RTE-3 Challenge.
Sections 3, 4, and 5 describe details of our sys-
tems for Commitment Extraction, Commitment Se-
</bodyText>
<page confidence="0.971713">
171
</page>
<note confidence="0.6929565">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 171–176,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<figure confidence="0.999475821428572">
Preprocessing
Text
Hyp
Extracted
Knowledge
Entailed
Knowledge
− Contradiction
+ Contradiction
NO
YES
Text Commitments
� �
�
Hyp Commitments
Contradiction
Detection
Extracted Commitments from Text and Hypothesis
Commitment
Extraction
Commitment
Selection
Lexical
Alignment
YES
NO
Entailment
Classification
</figure>
<figureCaption confidence="0.999987">
Figure 1: System Architecture.
</figureCaption>
<bodyText confidence="0.971445333333333">
lection, and Entailment Classification, respectively.
Finally, Section 6 discusses results from this year’s
evaluation, and Section 7 provides our conclusions.
</bodyText>
<sectionHeader confidence="0.9769" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.997453375">
The architecture of our system for recognizing tex-
tual entailment (RTE) is presented in Figure 1.
In our system, text-hypothesis (t-h) pairs are ini-
tially submitted to a Preprocessing module which
(1) syntactic parses each passage (using an imple-
mentation of the (Collins, 1999) parser), (2) iden-
tifies semantic dependencies (using a semantic de-
pendency parser trained on PropBank (Palmer et
al., 2005) and NomBank (Meyers et al., 2004)),
(3) annotates named entities (using LCC’s Cicero-
Lite named entity recognition system), (4) resolves
instances of pronominal and nominal coreference
(using a system based on (Luo et al., 2004)), and
(5) normalizes temporal and spatial expressions to
fully-resolved instances (using a technique first in-
troduced in (Aarseth et al., 2006)).
Annotated passages are then sent to a Commit-
ment Extraction module, which uses a series of ex-
traction heuristics in order to enumerate a subset of
the discourse commitments that are inferable from
either the text or hypothesis. Following (Gunlog-
son, 2001; Stalnaker, 1979), we assume that a dis-
course commitment (c) represents the any of the set
of propositions that can necessarily be inferred to be
true, given a conventional reading of a text passage.
The complete list of commitments that our system
is able to extract from from the t used in examples
34 and 36 from the RTE-3 Test Set is presented in
Figure 2. (Details of our commitment extraction ap-
proach are presented in Section 3.)
Commitments are then sent to a Commitment Se-
lection module, which uses a weighted bipartite
matching algorithm first described in (Taskar et al.,
2005b) in order to identify the commitment from the
t which features the best alignment for each commit-
ment extracted from the h. The commitment pairs
identified for the hypotheses from 34 and 36 are
highlighted in Figure 2. (Details of our method for
selecting and aligning commitments are provided in
Section 4.)
Each pair of commitments are then considered in
turn by an Entailment Classification module, which
follows (Bos and Markert, 2006; Hickl et al., 2006)
in using a decision tree classifier in order to compute
the likelihood that a commitment extracted from a t
textually entails a commitment extracted from an h.
If a commitment pair is judged to be a pos-
itive instance of TE, it is sent to an Entailment
Validation module, which uses a system for rec-
ognizing instances of textual contradiction (RTC)
based on (Harabagiu et al., 2006) in order to de-
termine whether the (presumably) entailed hypothe-
sis is contradicted by any of other commitments ex-
tracted from the t during commitment extraction. If
no text commitment can be identified which contra-
dicts the hypothesis, it is presumed to be textually
entailed, and a judgment of YES is returned. Alter-
natively, if the entailed h is textually contradicted by
one (or more) of the commitments extracted from
the t, the h is considered to be contradicted by the
t, the entailment pair is classified as a negative in-
stance of TE, and a judgment of NO is returned.
In contrast, when commitment pairs are judged to
be negative instances of TE by the Entailment Clas-
sifier, the current pair is removed from further con-
sideration by the system, and the next most likely
commitment pair is considered. Commitment pairs
are considered in decreasing order of the probability
output by the Commitment Selection module until a
positive instance of TE is identified – or until there
are no more commitment pairs with a selection prob-
ability greater than a pre-defined threshold.
</bodyText>
<page confidence="0.987482">
172
</page>
<figure confidence="0.414446">
Text: A Revenue Cutter, the ship was named for Harriet Lane, niece of President James Buchanan,
who served as Buchanan’s White House hostess.
T1. A Revenue Cutter is a ship.
T2. The ship was named for Harriet Lane.
T3. Harriet Lane was the niece of President James Buchanan.
T16. Harriet Lane was related to President James Buchanan.
T17. Harriet Lane was the niece of a President.
T18. Harriet Lane was related to a President.
T4. The niece of Buchanan served as Buchanan’s White House hostess. T19. Harriet Lane was related to James Buchanan.
</figure>
<figureCaption confidence="0.939005">
Figure 2: Text Commitments Extracted from Examples 34 and 36.
</figureCaption>
<figure confidence="0.875950148148148">
T20. James Buchanan had title of President.
T21. James Buchanan had a White House hostess.
T22. James Buchanan had a hostess.
T23. James Buchanan was associated with the White House.
Negative Instance of Textual Entailment Positive Instance of Textual Entailment
T5. A Revenue Cutter was named for Harriet Lane.
T6. A Revenue Cutter was named for the niece of President James Buchanan.
T7. A Revenue Cutter was named for Buchanan’s White House hostess.
T8. A Revenue Cutter was named for a White House hostess.
T9. A Revenue Cutter was named for a hostess.
T10. The niece of a President served as Buchanan’s White House hostess.
T11. The niece of a President served as Buchanan’s hostess.
T12. The niece of a President served as a White House hostess.
T13. The niece of a President served at the White House.
T14. The niece of a President had occupation hostess.
T15. The niece of a President served as a hostess.
Hyp(34): Harriet Lane owned a Revenue Cutter.
T24. James Buchanan had a niece.
T25. Harriet Lane served as Buchanan’s White House hostess.
T26. Harriet Lane served as Buchanan’s hostess.
T27. Harriet Lane served as a White House hostess.
T28. Harriet Lane served at the White House.
T29. Harriet Lane had occupation hostess.
T30. Harriet Lane served as a hostess..
Selected Commitment
Hyp(36): Harriet Lane worked at the White House.
Selected Commitment
</figure>
<sectionHeader confidence="0.952712" genericHeader="method">
3 Extracting Discourse Commitments
</sectionHeader>
<bodyText confidence="0.999247404255319">
Following Preprocessing, our system for RTE lever-
ages a series of heuristics in order to extract a subset
of the discourse commitments available from a text-
hypothesis pair. In this section, we outline the five
classes of heuristics we used to extract commitments
for the RTE-3 Challenge.
Sentence Segmentation: We use a sentence seg-
menter to break text passages into sets of individ-
ual sentences; commitments are then extracted from
each sentence independently.
Syntactic Decomposition: We use heuristics to
syntactically decompose sentences featuring coordi-
nation and lists into well-formed sentences that only
include a single conjunct or list element.
Supplemental Expressions: Recent work
by (Potts, 2005; Huddleston and Pullum, 2002)
has demonstrated that the class of supplemental
expressions – including appositives, as-clauses,
parentheticals, parenthetical adverbs, non-restrictive
relative clauses, and epithets – trigger conventional
implicatures (CI) whose truth is necessarily pre-
supposed, even if the truth conditions of a sentence
are not satisfied. In our current system, heuristics
were used to extract supplemental expressions from
each sentence under consideration and to create new
sentences which specify the CI conveyed by the
expression.
Relation Extraction: We used an in-house rela-
tion extraction system to recognize six types of se-
mantic relations between named entities, including:
(1) artifact (e.g. OWNER-OF), (2) general affilia-
tion (e.g. LOCATION-OF), (3) organization affilia-
tion (e.g. EMPLOYEE-OF), (4) part-whole, (5) social
affiliation (e.g. RELATED-TO), and (6) physical lo-
cation (e.g. LOCATED-NEAR) relations. Again, as
with supplemental expressions, heuristics were used
to generate new commitments which expressed the
semantics conveyed by these nominal relations.
Coreference Resolution: We used systems for re-
solving pronominal and nominal coreference in or-
der to expand the number of commitments avail-
able to the system. After a set of co-referential
entity mentions were detected (e.g. Harriet Lane,
the niece, Buchanan’s White House hostess), new
commitments were generated from the existing
set of commitments which incorporated each co-
referential mention.
</bodyText>
<sectionHeader confidence="0.994067" genericHeader="method">
4 Commitment Selection
</sectionHeader>
<bodyText confidence="0.984781461538461">
Following Commitment Extraction, we used an
word alignment technique first introduced in (Taskar
et al., 2005b) in order to select the commitment
extracted from t (henceforth, ct) which represents
the best alignment for each of the commitments ex-
tracted from h (henceforth, ch).
We assume that the alignment of two discourse
commitments can be cast as a maximum weighted
matching problem in which each pair of words
(ti,hj) in an commitment pair (ct,ch) is assigned a
score sij(t, h) corresponding to the likelihood that
ti is aligned to hj.1 As with (Taskar et al., 2005b),
we use the large-margin structured prediction model
</bodyText>
<footnote confidence="0.411262666666667">
1In order to ensure that content from the h is reflected in the
t, we assume that each word from the h is aligned to exactly one
or zero words from the t.
</footnote>
<page confidence="0.996838">
173
</page>
<bodyText confidence="0.9998284">
introduced in (Taskar et al., 2005a) in order to com-
pute a set of parameters w (computed with respect to
a set of features f) which maximize the number of
correct alignment predictions (yi) made given a set
of training examples (xi), as in Equation (1).
</bodyText>
<equation confidence="0.993869">
yi = arg max w⊤f(xi, yi),bi (1)
Yi∈Y
</equation>
<bodyText confidence="0.999970727272727">
We used three sets of features in our model: (1)
string features (including Levenshtein edit distance,
string equality, and stemmed string equality), (2)
lexico-semantic features (including WordNet Simi-
larity (Pedersen et al., 2004) and named entity sim-
ilarity equality), and (3) word association features
(computed using the Dice coefficient (Dice, 1945)2).
In order to provide a training set which most closely
resembled the RTE-3 Test Set, we hand-annotated
token alignments for each of the 800 entailment
pairs included in the Development Set.
Following alignment, we used the sum of the edge
scores (EZj_1 sij(ti, hj)) computed for each of the
possible (ct, ch) pairs in order to search for the ct
which represented the reciprocal best hit (Mushe-
gian and Koonin, 2005) of each ch extracted from
the hypothesis. This was performed by selecting
a commitment pair (ct, ch) where ct was the top-
scoring alignment candidate for ch and ch was the
top-scoring alignment candidate for ct. If no recip-
rocal best-hit could be found for any of the commit-
ments extracted from the h, the system automatically
returned a TE judgment of NO.
We compared the performance of our word align-
ment and commitment selection algorithms against
an implementation of the lexical alignment classi-
fier described in (Hickl et al., 2006) on commitments
extracted from the entailment pairs from the RTE-2
Test Set. Table 1 presents results from evaluations of
these two models on the token alignment and com-
mitment selection tasks. (Gold standard annotations
for each task were created by hand by a team of 3
annotators following the RTE-3 evaluations.)
</bodyText>
<footnote confidence="0.708860666666667">
2The Dice coefficient was computed as Dice(i) _
2Cth M where Cth is equal to the number of times a word
CtMChM,
</footnote>
<tableCaption confidence="0.3302772">
i was found in both the t and an h of a single entailment pair,
while Ct and Ch were equal to the number of times a word
was found in any t or h, respectively. A hand-crafted corpus
of 100,000 entailment pairs was used to compute values for
Ct, Ch, and Cth.
</tableCaption>
<table confidence="0.9993884">
Task Measurement Current Work Hickl et al.
Token Alignment Precision 94.55% 92.22%
Token Alignment MRR 0.9219 0.8797
Commitment Selection Precision 89.50% 72.50%
Commitment Selection MRR 0.8853 0.7410
</table>
<tableCaption confidence="0.999674">
Table 1: Alignment and Selection Performance
</tableCaption>
<sectionHeader confidence="0.989424" genericHeader="method">
5 Entailment Classification
</sectionHeader>
<bodyText confidence="0.998896411764706">
Following work done by (Bos and Markert, 2006;
Hickl et al., 2006) for the RTE-2 Challenge, we used
a decision tree (C5.0 (Quinlan, 1998)) to estimate
the likelihood that a commitment pair represented
a valid instance of textual entailment.3 Confidence
values associated with each leaf node (i.e. YES or
NO) were normalized and used to rank examples for
the official submission.
In a departure from previous work (such as (Hickl
et al., 2006)) which leveraged large corpora of en-
tailment pairs to train an entailment classifier, our
model was only trained on the 800 text-hypothesis
pairs found in the RTE-3 Development Set (DevSet).
Features were selected manually by performing ten-
fold cross validation on the DevSet. Maximum per-
formance of the entailment classifier on the DevSet
is provided in Table 2.
</bodyText>
<table confidence="0.992768333333333">
IE IR QA SUM Total
Accuracy 0.8450 0.8750 0.8850 0.8600 0.8663
Average Precision 0.8522 0.8953 0.9005 0.8959 0.8860
</table>
<tableCaption confidence="0.998062">
Table 2: Entailment Classifier Performance.
</tableCaption>
<bodyText confidence="0.997559666666667">
A partial list of the features used in the Entailment
Classifier used in our official submission is provided
in Figure 3.
</bodyText>
<sectionHeader confidence="0.998673" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.969711666666667">
We submitted one ranked run in our official submis-
sion for this year’s evaluation. Official results from
the RTE-3 Test Set are presented in Table 3.
</bodyText>
<table confidence="0.993937">
IE IR QA SUM Total
Accuracy 0.6750 0.8000 0.9000 0.8400 0.8038
Average Precision 0.7760 0.8133 0.9308 0.8974 0.8815
</table>
<tableCaption confidence="0.999679">
Table 3: Official RTE-3 Results.
</tableCaption>
<bodyText confidence="0.995766571428572">
Accuracy and average precision varied signifi-
cantly (p &lt; 0.05) across each of the four tasks. Per-
formance (in terms of accuracy and average preci-
sion) was highest on the QA set (90.0% precision)
and lowest on the IE set (67.5%).
The length of the text (either short or long) did not
significantly impact performance, however; in fact,
</bodyText>
<footnote confidence="0.94363">
3We used a pruning confidence of 20% in our model.
</footnote>
<page confidence="0.974648">
174
</page>
<note confidence="0.955195444444444">
ALIGNMENT FEATURES: Derived from the results of the alignment
of each pair of commitments performed during Commitment Selec-
tion.
⋄1⋄ LONGEST COMMON STRING: This feature represents the longest
contiguous string common to both texts.
⋄2⋄ UNALIGNED CHUNK: This feature represents the number of
chunks in one text that are not aligned with a chunk from the other
⋄3⋄ LEXICAL ENTAILMENT PROBABILITY: Defined as in (Glickman
and Dagan, 2005).
</note>
<tableCaption confidence="0.960617827586207">
DEPENDENCY FEATURES: Computed from the semantic depen-
dencies identified by the PropBank- and NomBank-based semantic
parsers.
⋄1⋄ ENTITY-ARG MATCH: This is a boolean feature which fires when
aligned entities were assigned the same argument role label.
⋄2⋄ ENTITY-NEAR-ARG MATCH: This feature is collapsing the ar-
guments Arg, and Arg2 (as well as the ArgM subtypes) into single
categories for the purpose of counting matches.
⋄3⋄ PREDICATE-ARG MATCH: This boolean feature is flagged when
at least two aligned arguments have the same role.
⋄4⋄ PREDICATE-NEAR-ARG MATCH: This feature is collapsing the ar-
guments Arg, and Arg2 (as well as the ArgM subtypes) into single
categories for the purpose of counting matches.
SEMANTIC/PRAGMATIC FEATURES: Extracted during prepro-
cessing.
⋄1⋄ NAMED ENTITY CLASS: This feature has a different value for
each of the 150 named entity classes.
⋄2⋄ TEMPORAL NORMALIZATION: This boolean feature is flagged
when the temporal expressions are normalized to the same ISO
9000 equivalents.
⋄3⋄ MODALITY MARKER: This boolean feature is flagged when the
two texts use the same modal verbs.
⋄4⋄ SPEECH-ACT: This boolean feature is flagged when the lexicons
indicate the same speech act in both texts.
⋄5⋄ FACTIVITY MARKER: This boolean feature is flagged when the
factivity markers indicate either TRUE or FALSE in both texts simul-
taneously.
⋄6⋄ BELIEF MARKER: This boolean feature is set when the belief
markers indicate either TRUE or FALSE in both texts simultaneously.
</tableCaption>
<figureCaption confidence="0.943909">
Figure 3: Features used in the Entailment Classifier
</figureCaption>
<bodyText confidence="0.886311">
as can be seen in Table 4, total accuracy was nearly
the same for examples featuring short or long texts.
</bodyText>
<table confidence="0.997379571428571">
Short Long
n Accuracy n Accuracy
IE 181 0.6685 19 0.7368
IR 146 0.8082 54 0.7778
QA 165 0.8909 35 0.9429
SUM 191 0.8482 9 0.6667
Total 683 0.8023 117 0.8120
</table>
<tableCaption confidence="0.999884">
Table 4: Short vs. Long Pairs.
</tableCaption>
<bodyText confidence="0.999176230769231">
In experiments conducted following the RTE-3
submission deadline, we found that using a sys-
tem for recognizing textual contradiction to vali-
date judgments output by the entailment classifier
had only a slight positive impact on the overall per-
formance of our system. Table 5 compares per-
formance of our RTE system when four different
configurations of our system for recognizing textual
contradiction was used.
When used with its default threshold (A = 0.85),
we discovered that using textual contradiction en-
abled us to identify 17 additional examples (2.13%
overall) that were not available when using our sys-
</bodyText>
<table confidence="0.9975426">
Validation? a IE IR QA SUM Total
Yes (RTE-3) 0.85 0.6750 0.8000 0.9000 0.8400 0.8038
Yes 0.75 0.6900 0.8100 0.8850 0.8650 0.8125
Yes 0.65 0.6550 0.8000 0.8850 0.8250 0.7913
No – 0.6550 0.8000 0.8650 0.8250 0.7865
</table>
<tableCaption confidence="0.999076">
Table 5: Impact of Validation.
</tableCaption>
<bodyText confidence="0.999957793103448">
tem for RTE alone.4 When we hand-tuned A to max-
imize performance on the RTE-3 Test Set, we found
that accuracy could be increased by 3.0% over the
baseline (to 81.25% overall). Despite its limited ef-
fectiveness on this year’s Test Set, we believe that
net positive effect of using textual contradiction to
validate textual entailment judgments suggests that
this technique has merit and should be explored in
future evaluations.
In a second post hoc experiment, we sought to
quantify the impact that additional sources of train-
ing data could have on the performance of our RTE
system. Although our official submission was only
trained on the 800 t-h pairs found in the RTE-3 De-
velopment Set, we followed (Hickl et al., 2006) in
using a large, hand-crafted training set of 100,000
text-hypothesis pairs in order to train our entailment
classifier. Even though previous work has shown
that RTE accuracy increased with the size of the
training set, our experiments showed no correlation
between the size of the training corpus and the over-
all accuracy of the system. Table 6 summarizes the
performance of our RTE system when trained on in-
creasing amounts of training data. While increasing
the training data to approximately 10,000 training
examples did positively impact performance, we dis-
covered that using a training corpus of a size equal
to (Hickl et al., 2006)’s had nearly no measurable
impact on the observed performance of our system.
</bodyText>
<table confidence="0.998762833333333">
Training Corpus Accuracy Average Precision
800 pairs (RTE-3 Dev) 0.8038 0.8815
10,000 pairs 0.8150 0.8939
25,000 pairs 0.8225 0.8834
50,000 pairs 0.8125 0.8355
100,000 pairs 0.8050 0.8003
</table>
<tableCaption confidence="0.999239">
Table 6: Impact of Training Corpus Size.
</tableCaption>
<bodyText confidence="0.999265166666667">
While large training corpora (like (Hickl et al.,
2006)’s or the one compiled for this work) may pro-
vide an important source of lexico-semantic infor-
mation that can be leveraged in performing an entail-
ment classification, these results suggest that our ap-
proach based on commitment extraction may nullify
</bodyText>
<footnote confidence="0.735195">
4We learned the default threshold by training on the textual
contradiction corpus compiled by (Harabagiu et al., 2006).
</footnote>
<page confidence="0.989918">
175
</page>
<note confidence="0.91735125">
the gains in performance seen by these approaches.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu. 2006.
Negation, Contrast, and Contradiction in Text Processing. In
Proceedings of AAAI, Boston, MA.
</note>
<sectionHeader confidence="0.961035" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999782571428572">
This paper introduced a new framework for recog-
nizing textual entailment which depends on the ex-
traction of the discourse commitments that can be
inferred from a conventional interpretation of a text
passage. By explicitly enumerating the set of infer-
ences that can be drawn from a t or h, our approach
is able to reduce the task of RTE to the identification
of the set of commitments that support the inference
of each corresponding commitment extracted from a
hypothesis. In our current work, we show that this
approach can be used to correctly classify more than
80% of examples from the RTE-3 Test Set, without
the need for additional sources of training data or
web-based resources.
</bodyText>
<sectionHeader confidence="0.999379" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99978353164557">
Paul Aarseth, John Lehmann, Murat Deligonul, and Luke
Nezda. 2006. TASER: A Temporal and Spatial Expression
Recognition and Normalization System. In Proceedings of
the Automatic Content Extraction (ACE) Conference.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-
ampiccolo, Bernardo Magnini, and Idan Szpektor. 2006.
The Second PASCAL Recognising Textual Entailment Chal-
lenge. In Proceedings of the Second PASCAL Challenges
Workshop.
Johan Bos and Katya Markert. 2006. When logical infer-
ence helps in determining textual entailment (and when it
doesn’t). In Proceedings of the Second PASCAL Recogniz-
ing Textual Entailment Conference, Venice, Italy.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Univ. of Pennsyl-
vania.
L.R. Dice. 1945. Measures of the Amount of Ecologic Asso-
ciation Between Speices. In Journal of Ecology, volume 26,
pages 297–302.
Oren Glickman and Ido Dagan. 2005. A Probabilistic Setting
and Lexical Co-occurrence Model for Textual Entailment. In
Proceedings of the ACL Workshop on Empirical Modeling of
Semantic Equivalence and Entailment, Ann Arbor, USA.
Christine Gunlogson. 2001. True to Form: Rising and Falling
Declaratives as Questions in English. Ph.D. thesis, Univer-
sity of California, Santa Cruz.
Aria Haghighi, Andrew Ng, and Christopher Manning. 2005.
Robust textual inference via graph matching. In Proceed-
ings of Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language Process-
ing, pages 387–394.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing Textual En-
tailment with LCC’s Groundhog System. In Proceedings of
the Second PASCAL Challenges Workshop.
Rodney Huddleston and Geoffrey Pullum, editors, 2002. The
Cambridge Grammar of the English Language. Cambridge-
University Press.
V. Jijkoun and M. de Rijke. 2005. Recognizing Textual Entail-
ment Using Lexical Similarity. In Proceedings of the First
PASCAL Challenges Workshop.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.
2004. A mention-synchronous coreference resolution algo-
rithm based on the Bell Tree. In Proceedings of the ACL-
2004, Barcelona, Spain.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska,
B. Young, and R. Grishman. 2004. The nombank project:
An interim report. In A. Meyers, editor, HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, pages 24–31,
Boston, Massachusetts, USA, May 2 - May 7. Association
for Computational Linguistics.
Arcady Mushegian and Eugene Koonin. 2005. A minimal gene
set for cellular life derived by compraison of complete bac-
terial genomes. In Proceedings of the National Academies
of Science, volume 93, pages 10268–10273.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic Roles.
Computational Linguistics, 31(1):71–106.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. Word-
Net::Similarity - Measuring the Relatedness of Concepts. In
Proceedings of the Nineteenth National Conference on Arti-
ficial Intelligence (AAAI-04), San Jose, CA.
Christopher Potts, editor, 2005. The Logic of Conventional Im-
plicatures. Oxford University Press.
R. Quinlan. 1998. C5.0: An Informal Tutorial. RuleQuest.
Robert Stalnaker, 1979. Assertion, volume 9, pages 315–332.
Ben Taskar, Simone Lacoste-Julien, and Michael Jordan.
2005a. Structured prediction via the extragradient method.
In Proceedings of Neural Information Processing Systems,
Vancouver, Canada.
Ben Taskar, Simone Lacoste-Julien, and Dan Klein. 2005b. A
discriminative matching approach to word alignment. In
Proceedings of Human Language Technology Conference
and Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005), Vancouver, Canada.
Lucy Vanderwende, Arul Menezes, and Rion Snow. 2006. Mi-
crosoft Research at RTE-2: Syntactic Contributions in the
Entailment Task: an implementation. In Proceedings of the
Second PASCAL Challenges Workshop.
</reference>
<page confidence="0.998741">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.909153">
<title confidence="0.9863535">A Discourse Commitment-Based Framework for Recognizing Textual Entailment</title>
<author confidence="0.990146">Andrew Hickl</author>
<author confidence="0.990146">Jeremy</author>
<affiliation confidence="0.98968">Language Computer</affiliation>
<address confidence="0.9961395">1701 North Collins Richardson, Texas 75080</address>
<abstract confidence="0.997268941176471">In this paper, we introduce a new framework for recognizing textual entailment which depends on extraction of the set of publiclybeliefs – known as committhat can be ascribed to the author of a text or a hypothesis. Once a set of commithave been extracted from a the task of recognizing textual entailment is reduced to the identification of the commitfrom a support the inference the Promising results were achieved: our system correctly identified more than 80% of examples from the RTE-3 Test Set correctly, without the need for additional sources of training data or other web-based resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul Aarseth</author>
<author>John Lehmann</author>
<author>Murat Deligonul</author>
<author>Luke Nezda</author>
</authors>
<title>TASER: A Temporal and Spatial Expression Recognition and Normalization System.</title>
<date>2006</date>
<booktitle>In Proceedings of the Automatic Content Extraction (ACE) Conference.</booktitle>
<contexts>
<context position="4666" citStr="Aarseth et al., 2006" startWordPosition="729" endWordPosition="732">y submitted to a Preprocessing module which (1) syntactic parses each passage (using an implementation of the (Collins, 1999) parser), (2) identifies semantic dependencies (using a semantic dependency parser trained on PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004)), (3) annotates named entities (using LCC’s CiceroLite named entity recognition system), (4) resolves instances of pronominal and nominal coreference (using a system based on (Luo et al., 2004)), and (5) normalizes temporal and spatial expressions to fully-resolved instances (using a technique first introduced in (Aarseth et al., 2006)). Annotated passages are then sent to a Commitment Extraction module, which uses a series of extraction heuristics in order to enumerate a subset of the discourse commitments that are inferable from either the text or hypothesis. Following (Gunlogson, 2001; Stalnaker, 1979), we assume that a discourse commitment (c) represents the any of the set of propositions that can necessarily be inferred to be true, given a conventional reading of a text passage. The complete list of commitments that our system is able to extract from from the t used in examples 34 and 36 from the RTE-3 Test Set is pres</context>
</contexts>
<marker>Aarseth, Lehmann, Deligonul, Nezda, 2006</marker>
<rawString>Paul Aarseth, John Lehmann, Murat Deligonul, and Luke Nezda. 2006. TASER: A Temporal and Spatial Expression Recognition and Normalization System. In Proceedings of the Automatic Content Extraction (ACE) Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The Second PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="1029" citStr="Bar-Haim et al., 2006" startWordPosition="156" endWordPosition="159">ents – that can be ascribed to the author of a text or a hypothesis. Once a set of commitments have been extracted from a t-h pair, the task of recognizing textual entailment is reduced to the identification of the commitments from a t which support the inference of the h. Promising results were achieved: our system correctly identified more than 80% of examples from the RTE-3 Test Set correctly, without the need for additional sources of training data or other web-based resources. 1 Introduction Systems participating in the previous two PASCAL Recognizing Textual Entailment (RTE) Challenges (Bar-Haim et al., 2006) have successfully employed a variety of “shallow” techniques in order to recognize instances of textual entailment, including methods based on: (1) sets of heuristics (Vanderwende et al., 2006), (2) measures of term overlap (Jijkoun and de Rijke, 2005), (3) the alignment of graphs created from syntactic or semantic dependencies (Haghighi et al., 2005), or (4) statistical classifiers which leverage a wide range of features, including the output of paraphrase generation (Hickl et al., 2006) or model building systems (Bos and Markert, 2006). While relatively “shallow” approaches have shown much </context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The Second PASCAL Recognising Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katya Markert</author>
</authors>
<title>When logical inference helps in determining textual entailment (and when it doesn’t).</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Recognizing Textual Entailment Conference,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="1573" citStr="Bos and Markert, 2006" startWordPosition="243" endWordPosition="246">PASCAL Recognizing Textual Entailment (RTE) Challenges (Bar-Haim et al., 2006) have successfully employed a variety of “shallow” techniques in order to recognize instances of textual entailment, including methods based on: (1) sets of heuristics (Vanderwende et al., 2006), (2) measures of term overlap (Jijkoun and de Rijke, 2005), (3) the alignment of graphs created from syntactic or semantic dependencies (Haghighi et al., 2005), or (4) statistical classifiers which leverage a wide range of features, including the output of paraphrase generation (Hickl et al., 2006) or model building systems (Bos and Markert, 2006). While relatively “shallow” approaches have shown much promise in RTE for entailment pairs where the text and hypothesis remain short, we expect that performance of these types of systems will ultimately degrade as longer and more syntactically complex entailment pairs are considered. In order to remain effective as texts get longer, we believe that RTE systems will need to employ techniques that will enable them to enumerate the set of propositions which are inferable – whether asserted, presupposed, or conventionally or conversationally implicated – from a text-hypothesis pair. In this pape</context>
<context position="5951" citStr="Bos and Markert, 2006" startWordPosition="945" endWordPosition="948">h are presented in Section 3.) Commitments are then sent to a Commitment Selection module, which uses a weighted bipartite matching algorithm first described in (Taskar et al., 2005b) in order to identify the commitment from the t which features the best alignment for each commitment extracted from the h. The commitment pairs identified for the hypotheses from 34 and 36 are highlighted in Figure 2. (Details of our method for selecting and aligning commitments are provided in Section 4.) Each pair of commitments are then considered in turn by an Entailment Classification module, which follows (Bos and Markert, 2006; Hickl et al., 2006) in using a decision tree classifier in order to compute the likelihood that a commitment extracted from a t textually entails a commitment extracted from an h. If a commitment pair is judged to be a positive instance of TE, it is sent to an Entailment Validation module, which uses a system for recognizing instances of textual contradiction (RTC) based on (Harabagiu et al., 2006) in order to determine whether the (presumably) entailed hypothesis is contradicted by any of other commitments extracted from the t during commitment extraction. If no text commitment can be ident</context>
<context position="15068" citStr="Bos and Markert, 2006" startWordPosition="2426" endWordPosition="2429">equal to the number of times a word CtMChM, i was found in both the t and an h of a single entailment pair, while Ct and Ch were equal to the number of times a word was found in any t or h, respectively. A hand-crafted corpus of 100,000 entailment pairs was used to compute values for Ct, Ch, and Cth. Task Measurement Current Work Hickl et al. Token Alignment Precision 94.55% 92.22% Token Alignment MRR 0.9219 0.8797 Commitment Selection Precision 89.50% 72.50% Commitment Selection MRR 0.8853 0.7410 Table 1: Alignment and Selection Performance 5 Entailment Classification Following work done by (Bos and Markert, 2006; Hickl et al., 2006) for the RTE-2 Challenge, we used a decision tree (C5.0 (Quinlan, 1998)) to estimate the likelihood that a commitment pair represented a valid instance of textual entailment.3 Confidence values associated with each leaf node (i.e. YES or NO) were normalized and used to rank examples for the official submission. In a departure from previous work (such as (Hickl et al., 2006)) which leveraged large corpora of entailment pairs to train an entailment classifier, our model was only trained on the 800 text-hypothesis pairs found in the RTE-3 Development Set (DevSet). Features we</context>
</contexts>
<marker>Bos, Markert, 2006</marker>
<rawString>Johan Bos and Katya Markert. 2006. When logical inference helps in determining textual entailment (and when it doesn’t). In Proceedings of the Second PASCAL Recognizing Textual Entailment Conference, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Pennsylvania.</institution>
<contexts>
<context position="4170" citStr="Collins, 1999" startWordPosition="656" endWordPosition="657">itments from Text and Hypothesis Commitment Extraction Commitment Selection Lexical Alignment YES NO Entailment Classification Figure 1: System Architecture. lection, and Entailment Classification, respectively. Finally, Section 6 discusses results from this year’s evaluation, and Section 7 provides our conclusions. 2 System Overview The architecture of our system for recognizing textual entailment (RTE) is presented in Figure 1. In our system, text-hypothesis (t-h) pairs are initially submitted to a Preprocessing module which (1) syntactic parses each passage (using an implementation of the (Collins, 1999) parser), (2) identifies semantic dependencies (using a semantic dependency parser trained on PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004)), (3) annotates named entities (using LCC’s CiceroLite named entity recognition system), (4) resolves instances of pronominal and nominal coreference (using a system based on (Luo et al., 2004)), and (5) normalizes temporal and spatial expressions to fully-resolved instances (using a technique first introduced in (Aarseth et al., 2006)). Annotated passages are then sent to a Commitment Extraction module, which uses a series of extraction</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, Univ. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Dice</author>
</authors>
<title>Measures of the Amount of Ecologic Association Between Speices.</title>
<date>1945</date>
<journal>In Journal of Ecology,</journal>
<volume>26</volume>
<pages>297--302</pages>
<contexts>
<context position="13107" citStr="Dice, 1945" startWordPosition="2096" endWordPosition="2097">) in order to compute a set of parameters w (computed with respect to a set of features f) which maximize the number of correct alignment predictions (yi) made given a set of training examples (xi), as in Equation (1). yi = arg max w⊤f(xi, yi),bi (1) Yi∈Y We used three sets of features in our model: (1) string features (including Levenshtein edit distance, string equality, and stemmed string equality), (2) lexico-semantic features (including WordNet Similarity (Pedersen et al., 2004) and named entity similarity equality), and (3) word association features (computed using the Dice coefficient (Dice, 1945)2). In order to provide a training set which most closely resembled the RTE-3 Test Set, we hand-annotated token alignments for each of the 800 entailment pairs included in the Development Set. Following alignment, we used the sum of the edge scores (EZj_1 sij(ti, hj)) computed for each of the possible (ct, ch) pairs in order to search for the ct which represented the reciprocal best hit (Mushegian and Koonin, 2005) of each ch extracted from the hypothesis. This was performed by selecting a commitment pair (ct, ch) where ct was the topscoring alignment candidate for ch and ch was the top-scorin</context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>L.R. Dice. 1945. Measures of the Amount of Ecologic Association Between Speices. In Journal of Ecology, volume 26, pages 297–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
</authors>
<title>A Probabilistic Setting and Lexical Co-occurrence Model for Textual Entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="17262" citStr="Glickman and Dagan, 2005" startWordPosition="2783" endWordPosition="2786">owest on the IE set (67.5%). The length of the text (either short or long) did not significantly impact performance, however; in fact, 3We used a pruning confidence of 20% in our model. 174 ALIGNMENT FEATURES: Derived from the results of the alignment of each pair of commitments performed during Commitment Selection. ⋄1⋄ LONGEST COMMON STRING: This feature represents the longest contiguous string common to both texts. ⋄2⋄ UNALIGNED CHUNK: This feature represents the number of chunks in one text that are not aligned with a chunk from the other ⋄3⋄ LEXICAL ENTAILMENT PROBABILITY: Defined as in (Glickman and Dagan, 2005). DEPENDENCY FEATURES: Computed from the semantic dependencies identified by the PropBank- and NomBank-based semantic parsers. ⋄1⋄ ENTITY-ARG MATCH: This is a boolean feature which fires when aligned entities were assigned the same argument role label. ⋄2⋄ ENTITY-NEAR-ARG MATCH: This feature is collapsing the arguments Arg, and Arg2 (as well as the ArgM subtypes) into single categories for the purpose of counting matches. ⋄3⋄ PREDICATE-ARG MATCH: This boolean feature is flagged when at least two aligned arguments have the same role. ⋄4⋄ PREDICATE-NEAR-ARG MATCH: This feature is collapsing the </context>
</contexts>
<marker>Glickman, Dagan, 2005</marker>
<rawString>Oren Glickman and Ido Dagan. 2005. A Probabilistic Setting and Lexical Co-occurrence Model for Textual Entailment. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Gunlogson</author>
</authors>
<title>True to Form: Rising and Falling Declaratives as Questions in English.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Santa Cruz.</institution>
<contexts>
<context position="4923" citStr="Gunlogson, 2001" startWordPosition="772" endWordPosition="774">Meyers et al., 2004)), (3) annotates named entities (using LCC’s CiceroLite named entity recognition system), (4) resolves instances of pronominal and nominal coreference (using a system based on (Luo et al., 2004)), and (5) normalizes temporal and spatial expressions to fully-resolved instances (using a technique first introduced in (Aarseth et al., 2006)). Annotated passages are then sent to a Commitment Extraction module, which uses a series of extraction heuristics in order to enumerate a subset of the discourse commitments that are inferable from either the text or hypothesis. Following (Gunlogson, 2001; Stalnaker, 1979), we assume that a discourse commitment (c) represents the any of the set of propositions that can necessarily be inferred to be true, given a conventional reading of a text passage. The complete list of commitments that our system is able to extract from from the t used in examples 34 and 36 from the RTE-3 Test Set is presented in Figure 2. (Details of our commitment extraction approach are presented in Section 3.) Commitments are then sent to a Commitment Selection module, which uses a weighted bipartite matching algorithm first described in (Taskar et al., 2005b) in order </context>
</contexts>
<marker>Gunlogson, 2001</marker>
<rawString>Christine Gunlogson. 2001. True to Form: Rising and Falling Declaratives as Questions in English. Ph.D. thesis, University of California, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>387--394</pages>
<contexts>
<context position="1383" citStr="Haghighi et al., 2005" startWordPosition="212" endWordPosition="215">f examples from the RTE-3 Test Set correctly, without the need for additional sources of training data or other web-based resources. 1 Introduction Systems participating in the previous two PASCAL Recognizing Textual Entailment (RTE) Challenges (Bar-Haim et al., 2006) have successfully employed a variety of “shallow” techniques in order to recognize instances of textual entailment, including methods based on: (1) sets of heuristics (Vanderwende et al., 2006), (2) measures of term overlap (Jijkoun and de Rijke, 2005), (3) the alignment of graphs created from syntactic or semantic dependencies (Haghighi et al., 2005), or (4) statistical classifiers which leverage a wide range of features, including the output of paraphrase generation (Hickl et al., 2006) or model building systems (Bos and Markert, 2006). While relatively “shallow” approaches have shown much promise in RTE for entailment pairs where the text and hypothesis remain short, we expect that performance of these types of systems will ultimately degrade as longer and more syntactically complex entailment pairs are considered. In order to remain effective as texts get longer, we believe that RTE systems will need to employ techniques that will enab</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Aria Haghighi, Andrew Ng, and Christopher Manning. 2005. Robust textual inference via graph matching. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 387–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing Textual Entailment with LCC’s Groundhog System.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="1523" citStr="Hickl et al., 2006" startWordPosition="235" endWordPosition="238">tion Systems participating in the previous two PASCAL Recognizing Textual Entailment (RTE) Challenges (Bar-Haim et al., 2006) have successfully employed a variety of “shallow” techniques in order to recognize instances of textual entailment, including methods based on: (1) sets of heuristics (Vanderwende et al., 2006), (2) measures of term overlap (Jijkoun and de Rijke, 2005), (3) the alignment of graphs created from syntactic or semantic dependencies (Haghighi et al., 2005), or (4) statistical classifiers which leverage a wide range of features, including the output of paraphrase generation (Hickl et al., 2006) or model building systems (Bos and Markert, 2006). While relatively “shallow” approaches have shown much promise in RTE for entailment pairs where the text and hypothesis remain short, we expect that performance of these types of systems will ultimately degrade as longer and more syntactically complex entailment pairs are considered. In order to remain effective as texts get longer, we believe that RTE systems will need to employ techniques that will enable them to enumerate the set of propositions which are inferable – whether asserted, presupposed, or conventionally or conversationally impl</context>
<context position="5972" citStr="Hickl et al., 2006" startWordPosition="949" endWordPosition="952">ion 3.) Commitments are then sent to a Commitment Selection module, which uses a weighted bipartite matching algorithm first described in (Taskar et al., 2005b) in order to identify the commitment from the t which features the best alignment for each commitment extracted from the h. The commitment pairs identified for the hypotheses from 34 and 36 are highlighted in Figure 2. (Details of our method for selecting and aligning commitments are provided in Section 4.) Each pair of commitments are then considered in turn by an Entailment Classification module, which follows (Bos and Markert, 2006; Hickl et al., 2006) in using a decision tree classifier in order to compute the likelihood that a commitment extracted from a t textually entails a commitment extracted from an h. If a commitment pair is judged to be a positive instance of TE, it is sent to an Entailment Validation module, which uses a system for recognizing instances of textual contradiction (RTC) based on (Harabagiu et al., 2006) in order to determine whether the (presumably) entailed hypothesis is contradicted by any of other commitments extracted from the t during commitment extraction. If no text commitment can be identified which contradic</context>
<context position="14063" citStr="Hickl et al., 2006" startWordPosition="2256" endWordPosition="2259">for the ct which represented the reciprocal best hit (Mushegian and Koonin, 2005) of each ch extracted from the hypothesis. This was performed by selecting a commitment pair (ct, ch) where ct was the topscoring alignment candidate for ch and ch was the top-scoring alignment candidate for ct. If no reciprocal best-hit could be found for any of the commitments extracted from the h, the system automatically returned a TE judgment of NO. We compared the performance of our word alignment and commitment selection algorithms against an implementation of the lexical alignment classifier described in (Hickl et al., 2006) on commitments extracted from the entailment pairs from the RTE-2 Test Set. Table 1 presents results from evaluations of these two models on the token alignment and commitment selection tasks. (Gold standard annotations for each task were created by hand by a team of 3 annotators following the RTE-3 evaluations.) 2The Dice coefficient was computed as Dice(i) _ 2Cth M where Cth is equal to the number of times a word CtMChM, i was found in both the t and an h of a single entailment pair, while Ct and Ch were equal to the number of times a word was found in any t or h, respectively. A hand-craft</context>
<context position="15465" citStr="Hickl et al., 2006" startWordPosition="2491" endWordPosition="2494">t MRR 0.9219 0.8797 Commitment Selection Precision 89.50% 72.50% Commitment Selection MRR 0.8853 0.7410 Table 1: Alignment and Selection Performance 5 Entailment Classification Following work done by (Bos and Markert, 2006; Hickl et al., 2006) for the RTE-2 Challenge, we used a decision tree (C5.0 (Quinlan, 1998)) to estimate the likelihood that a commitment pair represented a valid instance of textual entailment.3 Confidence values associated with each leaf node (i.e. YES or NO) were normalized and used to rank examples for the official submission. In a departure from previous work (such as (Hickl et al., 2006)) which leveraged large corpora of entailment pairs to train an entailment classifier, our model was only trained on the 800 text-hypothesis pairs found in the RTE-3 Development Set (DevSet). Features were selected manually by performing tenfold cross validation on the DevSet. Maximum performance of the entailment classifier on the DevSet is provided in Table 2. IE IR QA SUM Total Accuracy 0.8450 0.8750 0.8850 0.8600 0.8663 Average Precision 0.8522 0.8953 0.9005 0.8959 0.8860 Table 2: Entailment Classifier Performance. A partial list of the features used in the Entailment Classifier used in ou</context>
<context position="20684" citStr="Hickl et al., 2006" startWordPosition="3341" endWordPosition="3344">uracy could be increased by 3.0% over the baseline (to 81.25% overall). Despite its limited effectiveness on this year’s Test Set, we believe that net positive effect of using textual contradiction to validate textual entailment judgments suggests that this technique has merit and should be explored in future evaluations. In a second post hoc experiment, we sought to quantify the impact that additional sources of training data could have on the performance of our RTE system. Although our official submission was only trained on the 800 t-h pairs found in the RTE-3 Development Set, we followed (Hickl et al., 2006) in using a large, hand-crafted training set of 100,000 text-hypothesis pairs in order to train our entailment classifier. Even though previous work has shown that RTE accuracy increased with the size of the training set, our experiments showed no correlation between the size of the training corpus and the overall accuracy of the system. Table 6 summarizes the performance of our RTE system when trained on increasing amounts of training data. While increasing the training data to approximately 10,000 training examples did positively impact performance, we discovered that using a training corpus</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing Textual Entailment with LCC’s Groundhog System. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<title>The Cambridge Grammar of the English Language.</title>
<date>2002</date>
<editor>Rodney Huddleston and Geoffrey Pullum, editors,</editor>
<publisher>CambridgeUniversity Press.</publisher>
<marker>2002</marker>
<rawString>Rodney Huddleston and Geoffrey Pullum, editors, 2002. The Cambridge Grammar of the English Language. CambridgeUniversity Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M de Rijke</author>
</authors>
<title>Recognizing Textual Entailment Using Lexical Similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of the First PASCAL Challenges Workshop.</booktitle>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>V. Jijkoun and M. de Rijke. 2005. Recognizing Textual Entailment Using Lexical Similarity. In Proceedings of the First PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the Bell Tree.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="4522" citStr="Luo et al., 2004" startWordPosition="708" endWordPosition="711">ure of our system for recognizing textual entailment (RTE) is presented in Figure 1. In our system, text-hypothesis (t-h) pairs are initially submitted to a Preprocessing module which (1) syntactic parses each passage (using an implementation of the (Collins, 1999) parser), (2) identifies semantic dependencies (using a semantic dependency parser trained on PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004)), (3) annotates named entities (using LCC’s CiceroLite named entity recognition system), (4) resolves instances of pronominal and nominal coreference (using a system based on (Luo et al., 2004)), and (5) normalizes temporal and spatial expressions to fully-resolved instances (using a technique first introduced in (Aarseth et al., 2006)). Annotated passages are then sent to a Commitment Extraction module, which uses a series of extraction heuristics in order to enumerate a subset of the discourse commitments that are inferable from either the text or hypothesis. Following (Gunlogson, 2001; Stalnaker, 1979), we assume that a discourse commitment (c) represents the any of the set of propositions that can necessarily be inferred to be true, given a conventional reading of a text passage</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the Bell Tree. In Proceedings of the ACL2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<volume>2</volume>
<pages>24--31</pages>
<editor>In A. Meyers, editor,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="4328" citStr="Meyers et al., 2004" startWordPosition="679" endWordPosition="682">ture. lection, and Entailment Classification, respectively. Finally, Section 6 discusses results from this year’s evaluation, and Section 7 provides our conclusions. 2 System Overview The architecture of our system for recognizing textual entailment (RTE) is presented in Figure 1. In our system, text-hypothesis (t-h) pairs are initially submitted to a Preprocessing module which (1) syntactic parses each passage (using an implementation of the (Collins, 1999) parser), (2) identifies semantic dependencies (using a semantic dependency parser trained on PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004)), (3) annotates named entities (using LCC’s CiceroLite named entity recognition system), (4) resolves instances of pronominal and nominal coreference (using a system based on (Luo et al., 2004)), and (5) normalizes temporal and spatial expressions to fully-resolved instances (using a technique first introduced in (Aarseth et al., 2006)). Annotated passages are then sent to a Commitment Extraction module, which uses a series of extraction heuristics in order to enumerate a subset of the discourse commitments that are inferable from either the text or hypothesis. Following (Gunlogson, 2001; Sta</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The nombank project: An interim report. In A. Meyers, editor, HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 24–31, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arcady Mushegian</author>
<author>Eugene Koonin</author>
</authors>
<title>A minimal gene set for cellular life derived by compraison of complete bacterial genomes.</title>
<date>2005</date>
<booktitle>In Proceedings of the National Academies of Science,</booktitle>
<volume>93</volume>
<pages>10268--10273</pages>
<contexts>
<context position="13525" citStr="Mushegian and Koonin, 2005" startWordPosition="2164" endWordPosition="2168">y), (2) lexico-semantic features (including WordNet Similarity (Pedersen et al., 2004) and named entity similarity equality), and (3) word association features (computed using the Dice coefficient (Dice, 1945)2). In order to provide a training set which most closely resembled the RTE-3 Test Set, we hand-annotated token alignments for each of the 800 entailment pairs included in the Development Set. Following alignment, we used the sum of the edge scores (EZj_1 sij(ti, hj)) computed for each of the possible (ct, ch) pairs in order to search for the ct which represented the reciprocal best hit (Mushegian and Koonin, 2005) of each ch extracted from the hypothesis. This was performed by selecting a commitment pair (ct, ch) where ct was the topscoring alignment candidate for ch and ch was the top-scoring alignment candidate for ct. If no reciprocal best-hit could be found for any of the commitments extracted from the h, the system automatically returned a TE judgment of NO. We compared the performance of our word alignment and commitment selection algorithms against an implementation of the lexical alignment classifier described in (Hickl et al., 2006) on commitments extracted from the entailment pairs from the R</context>
</contexts>
<marker>Mushegian, Koonin, 2005</marker>
<rawString>Arcady Mushegian and Eugene Koonin. 2005. A minimal gene set for cellular life derived by compraison of complete bacterial genomes. In Proceedings of the National Academies of Science, volume 93, pages 10268–10273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="4294" citStr="Palmer et al., 2005" startWordPosition="673" endWordPosition="676">fication Figure 1: System Architecture. lection, and Entailment Classification, respectively. Finally, Section 6 discusses results from this year’s evaluation, and Section 7 provides our conclusions. 2 System Overview The architecture of our system for recognizing textual entailment (RTE) is presented in Figure 1. In our system, text-hypothesis (t-h) pairs are initially submitted to a Preprocessing module which (1) syntactic parses each passage (using an implementation of the (Collins, 1999) parser), (2) identifies semantic dependencies (using a semantic dependency parser trained on PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004)), (3) annotates named entities (using LCC’s CiceroLite named entity recognition system), (4) resolves instances of pronominal and nominal coreference (using a system based on (Luo et al., 2004)), and (5) normalizes temporal and spatial expressions to fully-resolved instances (using a technique first introduced in (Aarseth et al., 2006)). Annotated passages are then sent to a Commitment Extraction module, which uses a series of extraction heuristics in order to enumerate a subset of the discourse commitments that are inferable from either the text or hypothesi</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04),</booktitle>
<location>San Jose, CA.</location>
<contexts>
<context position="12984" citStr="Pedersen et al., 2004" startWordPosition="2076" endWordPosition="2079">the t, we assume that each word from the h is aligned to exactly one or zero words from the t. 173 introduced in (Taskar et al., 2005a) in order to compute a set of parameters w (computed with respect to a set of features f) which maximize the number of correct alignment predictions (yi) made given a set of training examples (xi), as in Equation (1). yi = arg max w⊤f(xi, yi),bi (1) Yi∈Y We used three sets of features in our model: (1) string features (including Levenshtein edit distance, string equality, and stemmed string equality), (2) lexico-semantic features (including WordNet Similarity (Pedersen et al., 2004) and named entity similarity equality), and (3) word association features (computed using the Dice coefficient (Dice, 1945)2). In order to provide a training set which most closely resembled the RTE-3 Test Set, we hand-annotated token alignments for each of the 800 entailment pairs included in the Development Set. Following alignment, we used the sum of the edge scores (EZj_1 sij(ti, hj)) computed for each of the possible (ct, ch) pairs in order to search for the ct which represented the reciprocal best hit (Mushegian and Koonin, 2005) of each ch extracted from the hypothesis. This was perform</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04), San Jose, CA.</rawString>
</citation>
<citation valid="true">
<title>The Logic of Conventional Implicatures.</title>
<date>2005</date>
<editor>Christopher Potts, editor,</editor>
<publisher>Oxford University Press.</publisher>
<marker>2005</marker>
<rawString>Christopher Potts, editor, 2005. The Logic of Conventional Implicatures. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C5.0: An Informal Tutorial. RuleQuest. Robert Stalnaker,</title>
<date>1998</date>
<journal>Assertion,</journal>
<volume>9</volume>
<pages>315--332</pages>
<contexts>
<context position="15160" citStr="Quinlan, 1998" startWordPosition="2444" endWordPosition="2445">nt pair, while Ct and Ch were equal to the number of times a word was found in any t or h, respectively. A hand-crafted corpus of 100,000 entailment pairs was used to compute values for Ct, Ch, and Cth. Task Measurement Current Work Hickl et al. Token Alignment Precision 94.55% 92.22% Token Alignment MRR 0.9219 0.8797 Commitment Selection Precision 89.50% 72.50% Commitment Selection MRR 0.8853 0.7410 Table 1: Alignment and Selection Performance 5 Entailment Classification Following work done by (Bos and Markert, 2006; Hickl et al., 2006) for the RTE-2 Challenge, we used a decision tree (C5.0 (Quinlan, 1998)) to estimate the likelihood that a commitment pair represented a valid instance of textual entailment.3 Confidence values associated with each leaf node (i.e. YES or NO) were normalized and used to rank examples for the official submission. In a departure from previous work (such as (Hickl et al., 2006)) which leveraged large corpora of entailment pairs to train an entailment classifier, our model was only trained on the 800 text-hypothesis pairs found in the RTE-3 Development Set (DevSet). Features were selected manually by performing tenfold cross validation on the DevSet. Maximum performan</context>
</contexts>
<marker>Quinlan, 1998</marker>
<rawString>R. Quinlan. 1998. C5.0: An Informal Tutorial. RuleQuest. Robert Stalnaker, 1979. Assertion, volume 9, pages 315–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simone Lacoste-Julien</author>
<author>Michael Jordan</author>
</authors>
<title>Structured prediction via the extragradient method.</title>
<date>2005</date>
<booktitle>In Proceedings of Neural Information Processing Systems,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5511" citStr="Taskar et al., 2005" startWordPosition="873" endWordPosition="876">sis. Following (Gunlogson, 2001; Stalnaker, 1979), we assume that a discourse commitment (c) represents the any of the set of propositions that can necessarily be inferred to be true, given a conventional reading of a text passage. The complete list of commitments that our system is able to extract from from the t used in examples 34 and 36 from the RTE-3 Test Set is presented in Figure 2. (Details of our commitment extraction approach are presented in Section 3.) Commitments are then sent to a Commitment Selection module, which uses a weighted bipartite matching algorithm first described in (Taskar et al., 2005b) in order to identify the commitment from the t which features the best alignment for each commitment extracted from the h. The commitment pairs identified for the hypotheses from 34 and 36 are highlighted in Figure 2. (Details of our method for selecting and aligning commitments are provided in Section 4.) Each pair of commitments are then considered in turn by an Entailment Classification module, which follows (Bos and Markert, 2006; Hickl et al., 2006) in using a decision tree classifier in order to compute the likelihood that a commitment extracted from a t textually entails a commitment</context>
<context position="11784" citStr="Taskar et al., 2005" startWordPosition="1869" endWordPosition="1872">erate new commitments which expressed the semantics conveyed by these nominal relations. Coreference Resolution: We used systems for resolving pronominal and nominal coreference in order to expand the number of commitments available to the system. After a set of co-referential entity mentions were detected (e.g. Harriet Lane, the niece, Buchanan’s White House hostess), new commitments were generated from the existing set of commitments which incorporated each coreferential mention. 4 Commitment Selection Following Commitment Extraction, we used an word alignment technique first introduced in (Taskar et al., 2005b) in order to select the commitment extracted from t (henceforth, ct) which represents the best alignment for each of the commitments extracted from h (henceforth, ch). We assume that the alignment of two discourse commitments can be cast as a maximum weighted matching problem in which each pair of words (ti,hj) in an commitment pair (ct,ch) is assigned a score sij(t, h) corresponding to the likelihood that ti is aligned to hj.1 As with (Taskar et al., 2005b), we use the large-margin structured prediction model 1In order to ensure that content from the h is reflected in the t, we assume that </context>
</contexts>
<marker>Taskar, Lacoste-Julien, Jordan, 2005</marker>
<rawString>Ben Taskar, Simone Lacoste-Julien, and Michael Jordan. 2005a. Structured prediction via the extragradient method. In Proceedings of Neural Information Processing Systems, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simone Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5511" citStr="Taskar et al., 2005" startWordPosition="873" endWordPosition="876">sis. Following (Gunlogson, 2001; Stalnaker, 1979), we assume that a discourse commitment (c) represents the any of the set of propositions that can necessarily be inferred to be true, given a conventional reading of a text passage. The complete list of commitments that our system is able to extract from from the t used in examples 34 and 36 from the RTE-3 Test Set is presented in Figure 2. (Details of our commitment extraction approach are presented in Section 3.) Commitments are then sent to a Commitment Selection module, which uses a weighted bipartite matching algorithm first described in (Taskar et al., 2005b) in order to identify the commitment from the t which features the best alignment for each commitment extracted from the h. The commitment pairs identified for the hypotheses from 34 and 36 are highlighted in Figure 2. (Details of our method for selecting and aligning commitments are provided in Section 4.) Each pair of commitments are then considered in turn by an Entailment Classification module, which follows (Bos and Markert, 2006; Hickl et al., 2006) in using a decision tree classifier in order to compute the likelihood that a commitment extracted from a t textually entails a commitment</context>
<context position="11784" citStr="Taskar et al., 2005" startWordPosition="1869" endWordPosition="1872">erate new commitments which expressed the semantics conveyed by these nominal relations. Coreference Resolution: We used systems for resolving pronominal and nominal coreference in order to expand the number of commitments available to the system. After a set of co-referential entity mentions were detected (e.g. Harriet Lane, the niece, Buchanan’s White House hostess), new commitments were generated from the existing set of commitments which incorporated each coreferential mention. 4 Commitment Selection Following Commitment Extraction, we used an word alignment technique first introduced in (Taskar et al., 2005b) in order to select the commitment extracted from t (henceforth, ct) which represents the best alignment for each of the commitments extracted from h (henceforth, ch). We assume that the alignment of two discourse commitments can be cast as a maximum weighted matching problem in which each pair of words (ti,hj) in an commitment pair (ct,ch) is assigned a score sij(t, h) corresponding to the likelihood that ti is aligned to hj.1 As with (Taskar et al., 2005b), we use the large-margin structured prediction model 1In order to ensure that content from the h is reflected in the t, we assume that </context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simone Lacoste-Julien, and Dan Klein. 2005b. A discriminative matching approach to word alignment. In Proceedings of Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
<author>Arul Menezes</author>
<author>Rion Snow</author>
</authors>
<title>Microsoft Research at RTE-2: Syntactic Contributions in the Entailment Task: an implementation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="1223" citStr="Vanderwende et al., 2006" startWordPosition="185" endWordPosition="189">e identification of the commitments from a t which support the inference of the h. Promising results were achieved: our system correctly identified more than 80% of examples from the RTE-3 Test Set correctly, without the need for additional sources of training data or other web-based resources. 1 Introduction Systems participating in the previous two PASCAL Recognizing Textual Entailment (RTE) Challenges (Bar-Haim et al., 2006) have successfully employed a variety of “shallow” techniques in order to recognize instances of textual entailment, including methods based on: (1) sets of heuristics (Vanderwende et al., 2006), (2) measures of term overlap (Jijkoun and de Rijke, 2005), (3) the alignment of graphs created from syntactic or semantic dependencies (Haghighi et al., 2005), or (4) statistical classifiers which leverage a wide range of features, including the output of paraphrase generation (Hickl et al., 2006) or model building systems (Bos and Markert, 2006). While relatively “shallow” approaches have shown much promise in RTE for entailment pairs where the text and hypothesis remain short, we expect that performance of these types of systems will ultimately degrade as longer and more syntactically comp</context>
</contexts>
<marker>Vanderwende, Menezes, Snow, 2006</marker>
<rawString>Lucy Vanderwende, Arul Menezes, and Rion Snow. 2006. Microsoft Research at RTE-2: Syntactic Contributions in the Entailment Task: an implementation. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>