<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.8367495">
How Should a Large Corpus Be Built?—A Comparative Study of
Closure in Annotated Newspaper Corpora from Two Chinese
Sources, Towards Building A Larger Representative Corpus
Merged from Representative Sublanguage Collections
</note>
<author confidence="0.844727">
John Kovarik (kovariks@worldnet.attnet)
</author>
<affiliation confidence="0.808521">
U.S. Department of Defense
</affiliation>
<sectionHeader confidence="0.975234" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999711636363636">
This study measures comparative lexical and
syntactic closure rates in annotated Chinese
newspaper corpora from the Academica Sinica
Balanced Corpus and the University of Penn-
sylvania&apos;s Chinese Treebank. It then draws in-
ferences as to how large such corpora need be
to be representative models of subject-matter-
constrained language domains within the same
genre. Future large corpora should be built in-
crementally only by combining smaller repre-
sentative sublanguage collections.
</bodyText>
<sectionHeader confidence="0.981938" genericHeader="keywords">
1 Prior Work
</sectionHeader>
<bodyText confidence="0.999903621212122">
Practically speaking, earlier attempts at build-
ing corpora, such as the IBM/Lancaster ap-
proach, have taken an all-inclusive perspective
toward text selection proposing that (Garside
and McEnery, 1993) raw texts for parsed cor-
pora should come from a variety of sources.
The IBM/Lancaster group used the Canadian
Hansards collection of parallel parsed English
and French sentences as a base of English parsed
sentences and then focused on the Computer
Manuals domain, in which they attempted to
randomly select texts with some additional non-
Computer Manual material selected as &amp;quot;a mea-
sure of &apos;light relief&apos;&amp;quot; supposedly for the benefit
of the annotators. A broad approach was also
used in the Hong Kong element of the Interna-
tional Corpus of English (ICE) project (Green-
baum, 1992) which sought to assemble a range
of both spoken and written texts along with
a range of both formal and informal situations
to provide a reasonably large, well-documented
and detailed snapshot of the use of educated
English. (Bolt, 1994) Both the IBM/Lancaster
approach and the ICE project build millions of
tokens worth of corpora.
But from a more principled perspective, Dou-
glas Biber, in speaking on representativeness
in corpus design, pointed out that the linguis-
tic characterization of a corpus should include
both its central tendency and its range of varia-
tion. (Biber, 1993) Similarly Geoffrey Leech has
stated that a corpus, in order to be representa-
tive, must somehow capture the magnitude of
languages not only in their lexis but also in their
syntax. (Leech, 1991) This suggests we should
build corpora focusing on how well they can ap-
proach lexical and syntactic closure, rather than
by merely fixating on ever larger amounts of
text. To build representative corpora why not
first select representative texts constrained by
genre of writing?
In general one possible technique for methodi-
cal corpus selection would be to build large cor-
pora out of representative sub-collections con-
strained by genre and subject matter. Zelig
Harris said, &amp;quot;Certain proper subsets of the sen-
tences of a language may be closed under some
or all of the operations defined in the language,
and thus constitute a sublanguage of it.&amp;quot; (Har-
ris, 1968) Calling this an inductive definition
of sublanguage Satoshi Sekine has embarked on
studies involving new trends in the analysis of
sublanguages (Sekine, 1994). Both Harris and
Sekine recognized that sublanguages are an effi-
cient way to observe and measure the properties
of natural language in smaller, representative
blocks.
Getting down to specifics, McEnery and Wil-
son (McEnery and Wilson, 1996) have hypoth-
esized that genres of writing, such as the style
used in newspapers and similar printed publi-
cations to report news stories, represent a con-
strained subset of a natural language. Thus
newspaper texts constitute a sublanguage — a
version of a natural language which does not
display all of the creativity of that natural lan-
</bodyText>
<page confidence="0.998357">
116
</page>
<bodyText confidence="0.9999594375">
guage. The newspaper sublanguage can be fur-
ther constrained by subject matter to divide it
into smaller, more manageable subsets.
A key mathematical feature of a sublanguage
is that it will show a high degree of closure
at various levels of description, setting it apart
from unconstrained natural language. This clo-
sure property of a sublanguage is analogous to
the mathematical property of transitive closure.
McEnery and Wilson used the closure property
to measure and compare rates of lexical and syn-
tactic closure in three corpora: the IBM com-
puter manual corpus, the Canadian Hansards,
and the American Printing House for the Blind
corpus. To date, however, there has been little
work in a similar vein in other languages.
</bodyText>
<sectionHeader confidence="0.994657" genericHeader="introduction">
2 Overview
</sectionHeader>
<bodyText confidence="0.983942615384615">
This work applies the methodology of McEnery
and Wilson to examine closure rates in a com-
parative study of all available tagged Chinese
newspaper corpora. First I define lexical and
syntactic closure for this study in section 3.
Then, section 4 begins this study with an exam-
ination of all the newspaper texts of the Aca-
demica Sinica Balanced Corpus (ASBC). Sec-
tion 5 extends this study to an examination of
the newspaper texts of the UPenn Chinese Tree-
bank (CTB). Section 6 presents my findings and
section 7 discusses some implications for future
corpus building.
</bodyText>
<sectionHeader confidence="0.952219" genericHeader="method">
3 Lexical and Syntactic Closure
</sectionHeader>
<subsectionHeader confidence="0.999182">
3.1 Tokenization in Chinese
</subsectionHeader>
<bodyText confidence="0.999973454545454">
It should be pointed out that Chinese is an ag-
glutinative, not an inflected language. More-
over, while Chinese tokens can concatenate,
Chinese has no extensive morphology like many
Indo-European languages. Chinese, of course,
has no white space separating lexemes, as a re-
sult, all Chinese text must first be segmented
into word lengths. However, once a text has
been segmented, no stemming is needed so each
segmented Chinese word can be counted as it
occurs without the need of finding its lemma.
</bodyText>
<subsectionHeader confidence="0.999867">
3.2 • Lexical Closure
</subsectionHeader>
<bodyText confidence="0.9999428">
Lexical closure is that property of a collection of
text whereby given a representative sample, the
number of new lexical forms seen among every
additional 1,000 new tokens begins to level off
at a rate below 10 per cent.
</bodyText>
<subsectionHeader confidence="0.999709">
3.3 Syntactic Closure
</subsectionHeader>
<bodyText confidence="0.999409090909091">
Syntactic closure is that property of a collection
of text whereby given a representative sample of
a type of text, then the number of new syntac-
tic forms seen among every additional 1,000 new
tokens begins to level off. A syntactic form is
the combination of token plus type. Thus syn-
tactic closure approaches as the number of new
grammatical uses for a previously observed to-
ken plus the number of new tokens, regardless
of syntactic use, level off to a growth rate below
10 per cent.
</bodyText>
<sectionHeader confidence="0.924256" genericHeader="method">
4 Academica Sinica Balanced
Corpus (ASBC)
</sectionHeader>
<bodyText confidence="0.999944692307692">
While it is common practice to attempt to build
huge annotated corpora, it is of course very te-
dious, very expensive, and especially challeng-
ing for annotators to maintain consistency over
such a huge task. Consequently one must hope
that once an annotated corpus of newspaper
texts is created, it can be statistically measured
and confirmed to be a representative sample.
I first measured lexical and syntactic closure
rates in all ASBC newspaper texts but found
that when viewed as a whole this newspaper
sub-collection of the ASBC does not approach
closure (see graphs below).
</bodyText>
<subsectionHeader confidence="0.85254">
ROCLing News Lexical Closure
</subsectionHeader>
<bodyText confidence="0.769279">
UJIWIUJIII
</bodyText>
<page confidence="0.97695">
117
</page>
<subsectionHeader confidence="0.863031">
ROCLing News Syntactic Closure
</subsectionHeader>
<bodyText confidence="0.999443580645161">
This raises the question how can we hope
for NLP applications to learn on large cor-
pora if they themselves never approach statis-
tical closure, never approach being statistically
confirmed as a representative model of the lan-
guage?
I then focused downward on subsections of
the newspaper corpus-grouping them by similar
filename. I searched the ASBC corpus looking
for files of annotated newspaper text and found
a total of 57 files (18.7 Mb); my findings are
summarized in the following table.
1991 through 1995 and dealt with many differ-
ent subjects, the most frequent of which were
sports, news, and domestic politics, but even
each of these most frequent subjects only repre-
sented 9 per cent of the whole.
Let us consider the three ASBC newspaper
sub-collections (&amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, and &amp;quot;T&amp;quot; filenames)
to be potentially representative sublanguages.
If we can observe relatively high degrees of clo-
sure at various levels of description, we can pro-
pose that such sub-collections are representa-
tive sublanguages within the newspaper genre
of Chinese natural language. Conversely, those
which do not have a high degrees of closure are
definitely not sublanguage corpora and not of
further interest for this study. The following
graphs depict the observed lexical and syntac-
tic closure rates of the three ASBC newspaper
sub-collections under study.
</bodyText>
<figure confidence="0.880063">
ROCUng A Collection Lexical Closure
30000
I 20000
1
1 §111111111§11/1
=E1:1 XAV ; g S
Talons
Academica Sinica Balanced Corpus News
</figure>
<table confidence="0.8558048">
Files Size Filenames Subject Matter
05 01.9Mb A.... &apos;94 Academics
16 01.5Mb C.... &apos;93 Various News
01 00.2Mb SSSLA &apos;91 Politics etc.
36 15.1Mb T.... &apos;91-&apos;95 Sports etc.
</table>
<bodyText confidence="0.999843166666667">
The large single file, named &amp;quot;SSLA&amp;quot;, dealt
with a wide assortment of subject matter and
thus was significantly different from the other 3
newspaper collections. Not only was its individ-
ual file size rather large; it was not even close
to the size and homogeneity of the other three
newspaper multi-file collections. I rejected it
from further study.
The other sub-collections were more similar.
Topically speaking, the ASBC &amp;quot;A&amp;quot; newspaper
collection was focused primarily on news (77 per
cent) while at the same time focusing narrowly
on academic events in 1994. The ASBC &amp;quot;C&amp;quot;
newspaper collection was less narrowly focused
on news (73.5 per cent) but expanded its fo-
cus to other than academia while limiting it-
self to events of 1993. The ASBC &amp;quot;T&amp;quot; news-
paper collection, however, spanned the period
</bodyText>
<figure confidence="0.9477581875">
aCco
7000 t
COCC
•
5000
I4000
2000
1000
0
Ifni till WI
MSC *CI` Lexical Closure
0- 11
Tokens
118
ASBC &amp;quot;T* Loxical CLOSUM
D11111111
</figure>
<subsectionHeader confidence="0.680707">
Tokens
</subsectionHeader>
<bodyText confidence="0.931633111111111">
It appears that the ASBC &amp;quot;A&amp;quot; newspa-
per sub-collections does approach lexical clo-
sure; while the &amp;quot;C&amp;quot; and &amp;quot;T&amp;quot; newspaper sub-
collections definitely do not.
ROCting T Collection Synte clic Comm
It appears that the ASBC &amp;quot;A&amp;quot; newspaper
sub-collection also approaches syntactic clo-
sure; while the &amp;quot;C&amp;quot; and &amp;quot;T&amp;quot; newspaper sub-
collections do not.
</bodyText>
<sectionHeader confidence="0.499018" genericHeader="method">
5 UPenn Chinese Treebank
</sectionHeader>
<table confidence="0.48380625">
ROCLIng A Collection Syntactic Closure
Tokens
ROCUng C Collection Syntactic Closure
UnIgoe Word#1,08 Types
</table>
<bodyText confidence="0.979776857142857">
I next applied the same measures on the UPenn
Chinese Treebank corpus. I wanted to com-
pare the rate at which the UPenn collection ap-
proaches lexical and syntactic closure with that
of the ASBC &amp;quot;A&amp;quot; and &amp;quot;T&amp;quot; sub-collections. The
329 Xinhua newswire documents in the UPenn
Chinese Treebank annotated corpus came from
two sub-collections and total 3,289 sentences
averaging 27 words or 47 characters per sen-
tence excluding newspaper headlines which are
characteristically highly abbreviated clauses.
The Pen l script written to do this analysis
is freely available at http://home.attnet/— ko-
variks/closure.htm.
</bodyText>
<table confidence="0.805269125">
UPenn Chinese Treebank Newspaper Texts
Files Size Sub-Collection Subject
121 .322Mb One &apos;94 Economics
040 .114Mb Two &apos;96 Economics
076 .406Mb Two &apos;97 Economics
054 .181Mb Two &apos;98 Economics
038 .060Mb One General
Unique Word/POS Pairs
</table>
<page confidence="0.581692">
119
</page>
<figure confidence="0.5484685">
UPenn CTB LOXies61 POSure
Tokens
Wenn CTBSyrdactic Clown
Tokens
</figure>
<sectionHeader confidence="0.914674" genericHeader="method">
6 Findings
</sectionHeader>
<bodyText confidence="0.998137769230769">
The UPenn data initially approaches lexical and
syntactic closure at a rate which can be fa-
vorably compared with the ASBC &amp;quot;A&amp;quot; Chinese
newspaper sub-corpus. By the time 59,000 to-
kens of the UPenn corpus were tagged, only 56
new token+tag combinations were observed in
the preceding 1,000 tokens and 27 of those were
new proper nouns. In comparison, at this point
the dosure rate in the ASBC &amp;quot;A&amp;quot; corpus was
not quite as good (see table below). But contin-
uing to the 69,000 token mark, the ASBC &amp;quot;A&amp;quot;
closure rate had overtaken that of the UPenn
CTB.
</bodyText>
<table confidence="0.8347542">
Tokens Corpus New token-Ftags
59,000 UPenn CTB 56 (27 NR)
ASBC A 77 (34 N...)
69,000 UPenn CTB 61 (36 NR)
ASBC A 67 (49 N...)
</table>
<bodyText confidence="0.9879735">
Interestingly the graphs for both the ASBC
&amp;quot;A&amp;quot; and the UPenn CTB data reveal two-
humped curves. At approximately the 60,000
token mark on both UPenn CTB graphs the
curve was starting to flatten only to suddenly
shift into a sharper climb.
</bodyText>
<subsectionHeader confidence="0.672257">
ROCUng A Collecdan Lextcal Ckaure
</subsectionHeader>
<bodyText confidence="0.4620885">
TOkilri
!Wenn CT8 Lexical Downs
</bodyText>
<subsectionHeader confidence="0.876051">
Tokens
</subsectionHeader>
<bodyText confidence="0.9225734">
An investigation of the UPenn CTB data
revealed that the vast majority of the docu-
ments at this point dealt with international as-
pects of the Chinese economy 1, whereas pre-
viously the vast majority of documents had fo-
cused on Chinese domestic economic growth 2.
And similarly on the ASBC &amp;quot;A&amp;quot; collection clo-
sure graphs around the 30,000 token mark, both
&apos;Headlines of articles in UPenn
CTB from 66,800-68,300 tokens- 66809:
</bodyText>
<equation confidence="0.9534315">
; 66976:
E3113f3&apos;M)rEEN.TtAlEittAt
67114: itaitmxttematai
; 67282: 6, iltiVARK;WMT-2311 ;
67439: EI*14-*KI AL5E ; 67920:
litRitAMI-4 &amp;quot;VT&amp;quot; 14-viate, ; 68298:
</equation>
<footnote confidence="0.638747">
2 Topics of articles in UPenn CTB from
28,000-32,000 tokens- 28071: gi3lit97M:‘
</footnote>
<construct confidence="0.318122">
28345: 1:11E18M4110/Z:
28745: r*f11-10cWEitl: ; 29197:
</construct>
<page confidence="0.994116">
120
</page>
<bodyText confidence="0.9998019">
curves start to flatten only to suddenly around
the 40,000 token mark shift back into a climb
until about 70,000 tokens. An investigation of
the ASBC data also showed a subtle shift in
subject matter, most of the documents from
29,000 to 39,000 tokens were short notes and
simple bulletins on, shifts in academic positions,
whereas the later data had more long stories on
subjects of greater complexity 3, whereas the
later data had more long stories on subjects of
greater complexity 4.
Thus the ASBC &amp;quot;A&amp;quot; collection, more so than
the UPenn CTB corpus, did eventually seem to
approach closure. By the time we reach 80,000
tokens, the ASBC &amp;quot;A&amp;quot; collection only saw 14
new tokend-tag combinations in the last thou-
sand tokens of new text(see table below). Six of
those 14 new combinations were nouns, seven
were verbs, and one was a preposition 6, hav-
ing nearly reached lexical closure on newspa-
</bodyText>
<equation confidence="0.61093175">
X3:FAE3.EWV11111Pr-.1k • 29444:
1:3111-2=-4-AV • 29948: at&apos;ffiettigi3
31003: &amp;ini4rdinpkoi,31499:
; 31890: IRM4IA3&apos;11,1 ;
</equation>
<table confidence="0.916847692307692">
32246: trATaa .
3Topics of articles in ASBC &amp;quot;A&amp;quot; from 29,300-32,000
tokens- 29306: 29337: 29382: AA:
29537: 29581: AA: 4NE/ 29636: gig:
gt*SC • 30087: AA: ; 30153: AA
rtglig14i• 30342: 30402:
30449: 30478: 30539: A Atit
A: 44- ; 30590: 30677:
30772: 30822: 30944: AA 4144 ; 30976: 31093:
AA: **TOFFI-1•&amp;quot; .
4Topics of articles in ASBC &amp;quot;A&amp;quot; from 56,000-57,000
tokens- 56183: A*: at*IiiMifigiatilbff •
56232: AI: *f•tta2311; 56274: AA: EfEWIi.
; 56323: AE: 11443E4tffVf.e..1 Pk** •
56475: AA: 1*EfSb---1;5VJ ffiti
- 56635: Att:-70/StFie • 56798: A*:
*R-KIX1f igi*V 44 ; 56852:
AA OV;RVAitf:14±AEt ; 56930: AA:
tEliAtigi■ FIEIAREJ it 57019: AA:
FZ-haieXcel*inki
3ASBC Subcollection A* at 80,000 tokens, 14 New
token+types observed: Na 0, Na Ifflf*,
Na Na MCA, Na Ettg,
Nc Hat P VA it, VC A, VC
VC OM, VF VF 411, VJ #t; Total tags:
7, Total new items: 14.
</table>
<bodyText confidence="0.9979633">
per articles regarding academics. In contrast
the CTB collection instead logged 146 new to-
ken-Ftag combinations at the 80,000 token mark
and its new vocabulary ranged widely across 12
different parts of speech 6. While the majority
of this new vocabulary were nouns, this contin-
uing influx of new words was due primarily to
the late indusion of international news in the
CTB&apos;s collection of newspaper articles regard-
ing Chinese economics.
</bodyText>
<table confidence="0.837583225806452">
6UPenn CTB at 80,000 tokens, 146 New token+types
observed: ADA ADM, ADA*, ADfi*,
ADM&amp; ADE, ADifig, CD 1 2 N,
cD1 7 1 644 cD2 0 0, cD2 0 0*,
CD2 10 0 CD2 8 0 0 N, CD4 0 0 ff,
CD 5 5, na&amp;Mtg, JJ*I, JJV!4,
JJ,JJ,X, LA#4, LC*, LC,
LCM, M4, Mit.T., NNIWA NNI4W,
NAM, NN#M- MAIM, NNEm,
NN*Nt,NN5ttit, NN*,
NNAK NNaolik, NNW , NNW40.7.,
NNAMV, NNittA, NN*, NNrIA, NN
NN, NAM,NN3, NNEA,
NNEI-15E, NN*gea&amp;quot;, NNW, NN
NNIFR,NNittA, NN43k2, NIT% t,
NOM, NNE, NNIA, NNgt, NN*,
41, NN4‘4, NNIMR, NNEPt, NNXt,
NNXINIA, NN, NNatak, NN4A,
NN&amp;RE, NN &amp;VW, NN a,
NN &apos;tot, NN-t, NN&apos;, NA*, NNtM,
NR,
Nonmq, NR.tin4NzRe*m0*,, -RNN5FRR mv• gwat,
amt lg**-ONrA, NR
NR/NACS,
NR-T, NR1-IT • gitTri**,
NOM, NRS5r€, NR* NRIN,
NOM, NRI, NRKM15, NRifittg,
NRSZ*, NREP, NT 11 M, NT 1 8 3 3*,
NT 1 8 1km, NT 1 9 3 9 &amp;r, NT 1 9 8 2 *,
NT 1 9 8 6*, NT 1 9 8 9 *, NT*,
PUM,PtIN, Pftfa, VAP, vvittit,
</table>
<construct confidence="0.962534625">
vvItt v04114, VV1, vv3-1-E, \MAW
vv* vvitliN, vvitAN, vv*#, vvai,
vv10-, vv-Mk, VOW*, vva*,
vvEgE, vv**RI, vv1t- a, vv*Yg,
vv*IAYgeg, vAR, \TNTZt,
vvAtt, vviE1ñS9I, vv
vv*Rlagi, vvItl; Total tags: 12, Total
new items: 146.
</construct>
<page confidence="0.982775">
121
</page>
<table confidence="0.953930333333333">
Tokens Corpus New token+tags
80,000 UPenn CTB —174-6724 NR)
ASBC A 14 (6 N...)
</table>
<sectionHeader confidence="0.904354" genericHeader="method">
7 Corpus Building Implications
</sectionHeader>
<bodyText confidence="0.999828833333333">
The fact that the UPenn Chinese Treebank
data approaches lexical and syntactic closure at
rates comparable to the ASBC &amp;quot;A&amp;quot; file newspa-
per collection suggests that if the UPenn data
had been selected more narrowly, it might have
reached closure for the economics domain in the
newspaper genre even sooner. Some day corpus
linguistics may only need much smaller collec-
tions of annotated corpora than is the practice
today, relying on new directions in sublanguage
research. In the case of the ASBC &amp;quot;A&amp;quot; col-
lection, for example, a robust learning struc-
ture should be able to build a useable model
on 100,000 words worth of data like this which
exhibits strong tendencies to lexical and syn-
tactic closure in the Chinese newspaper genre
constrained to a given domain.
If the UPenn CTB were enlarged by the in-
fusion of more news stories on international
aspects of Chinese economic development, the
CTB might better reach lexical and syntactic
closure. The following graph shows that the
blind addition of 20K additional Chinese eco-
nomic news stories does not aid closure much.
This additional data spanned many topics not
seen in the original 100K collection. If it had
been selected precisely to aid closure by mea-
suring its potential contribution before exten-
sive hand annotation, the result could have been
better.
</bodyText>
<subsectionHeader confidence="0.406733">
20K Augrnented C7B Laical Omura
</subsectionHeader>
<bodyText confidence="0.451269">
Toicem
</bodyText>
<sectionHeader confidence="0.409518" genericHeader="method">
20K Augmented CTB Syntactic Closure
</sectionHeader>
<subsectionHeader confidence="0.799279">
Tokens
</subsectionHeader>
<bodyText confidence="0.884632">
Nevertheless, this expanded CTB collection
is sufficiently improved that the rate of closure
toward of the expanded collection is better (see
table below).
</bodyText>
<table confidence="0.998749142857143">
Tokens New Tokens New Token+Tags
121,000 55 64 (12 NR)
122,000 61 68 (17 NR)
123,000 61 76 (23 NR)
124,000 78 82 (18 NR)
125,000 48 63 (11 NR)
126,000 52 .61 (16 NR)
</table>
<bodyText confidence="0.999029909090909">
Consequently, this expanded CTB collection
is sufficiently improved that merging it with the
ASBC &amp;quot;A&amp;quot; collection results in a far more mea-
surably representative larger corpus as shown in
the final two graphs below. The creation of such
measurably representative large corpora out of
such smaller, better focused sublanguage build-
ing blocks would be cheaper and faster with-
out the resulting tools developed against such
corpora suffering much degradation in speed or
accuracy.
</bodyText>
<subsectionHeader confidence="0.743254">
Combined ASBC &apos;A&amp;quot; and UPenn CM Lexical Closure
</subsectionHeader>
<figure confidence="0.9521586">
Tokens
20000
18000 II.
16000
14000 . .
moo
I00
6000
• 4000
2000
</figure>
<page confidence="0.962083">
122
</page>
<note confidence="0.753604">
Combined ASBC -A: and Wenn CTB Syntactic Closure
</note>
<sectionHeader confidence="0.995546" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999920548387097">
Since only two Chinese tagged corpora are avail-
able at the present time, only about 200,000
words of Chinese corpora have been so stud-
ied. But to see more work in this vein, one
need only consult McEnery and Wilson&apos;s study
of three English corpora: the IBM computer
manual corpus, the Canadian Hansards, and the
American Printing House for the Blind corpus
(McEnery and Wilson, 1996). Their study (ex-
haustively detailed in Chapter 6 of their book)
spans more than 2.2 million words of tagged
English text in three different domains. Con-
sequently the total of 2 3 million words from
McEnery and Wilson&apos;s results in tagged En-
glish texts when combined with these results in
tagged Chinese newspaper texts should satisfy
any who might argue that there is insufficient
data upon which to draw some general conclu-
sions.
This paper does not argue that the two clo-
sure measures used are the only measures pos-
sible. The argument here is simply that these
two closure measures are used to spot when a
sublanguage corpus approaches closure-that is,
when the curve of new types and new combina-
tions of type with token begins to flatten at a
rate below ten percent. One can readily point
out that no natural language corpus can ever
guarantee closure. The best anyone can aspire
to do today, given the current state of our art,
is to only approach closure.
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="acknowledgments">
9 References
References
</sectionHeader>
<reference confidence="0.999036636363636">
D. Biber. 1993. Representativeness in corpus
design. volume 8(4), pages 243-257.
P. Bolt. 1994. The international corpus of
english project-the hong kong experience.
pages 15-24.
R. Garside and A. McEnery. 1993. Treebank-
ing: the compilation of a corpus of skeleton
parsed sentences. pages 17-35.
S. Greenbaum. 1992. A new corpus of english:
Ice. pages 171-179.
Z. Harris. 1968. Mathematical Structures of
Language. New York: John Wiley and Sons.
G. Leech. 1991. The state of the art in corpus
linguistics. pages 8-29.
T. McEnery and A. Wilson. 1996. Corpus Lin-
guistics. Edinburgh: Edinburgh University
Press.
S. Sekine. 1994. A new direction for sublan-
guage nip. In Proceedings of the Interna-
tional Conference on New Methods in Lan-
guage Processing, CCL, UMIST, pages 123-
129.
</reference>
<page confidence="0.998906">
123
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9959175">How Should a Large Corpus Be Built?—A Comparative Study Closure in Annotated Newspaper Corpora from Two Sources, Towards Building A Larger Representative Merged from Representative Sublanguage Collections</title>
<author confidence="0.999924">John Kovarik</author>
<affiliation confidence="0.87247">of Defense</affiliation>
<abstract confidence="0.994490612565445">This study measures comparative lexical and syntactic closure rates in annotated Chinese newspaper corpora from the Academica Sinica Balanced Corpus and the University of Pennsylvania&apos;s Chinese Treebank. It then draws inferences as to how large such corpora need be to be representative models of subject-matterconstrained language domains within the same genre. Future large corpora should be built incrementally only by combining smaller representative sublanguage collections. 1 Prior Work Practically speaking, earlier attempts at building corpora, such as the IBM/Lancaster approach, have taken an all-inclusive perspective toward text selection proposing that (Garside and McEnery, 1993) raw texts for parsed corpora should come from a variety of sources. The IBM/Lancaster group used the Canadian Hansards collection of parallel parsed English and French sentences as a base of English parsed sentences and then focused on the Computer Manuals domain, in which they attempted to randomly select texts with some additional non- Computer Manual material selected as &amp;quot;a measure of &apos;light relief&apos;&amp;quot; supposedly for the benefit of the annotators. A broad approach was also used in the Hong Kong element of the International Corpus of English (ICE) project (Greenbaum, 1992) which sought to assemble a range of both spoken and written texts along with a range of both formal and informal situations to provide a reasonably large, well-documented and detailed snapshot of the use of educated English. (Bolt, 1994) Both the IBM/Lancaster approach and the ICE project build millions of tokens worth of corpora. But from a more principled perspective, Douglas Biber, in speaking on representativeness in corpus design, pointed out that the linguistic characterization of a corpus should include both its central tendency and its range of variation. (Biber, 1993) Similarly Geoffrey Leech has stated that a corpus, in order to be representative, must somehow capture the magnitude of languages not only in their lexis but also in their (Leech, 1991) suggests we should build corpora focusing on how well they can approach lexical and syntactic closure, rather than merely fixating on amounts of text. To build representative corpora why not first select representative texts constrained by genre of writing? In general one possible technique for methodical corpus selection would be to build large corpora out of representative sub-collections constrained by genre and subject matter. Zelig Harris said, &amp;quot;Certain proper subsets of the sentences of a language may be closed under some or all of the operations defined in the language, and thus constitute a sublanguage of it.&amp;quot; (Harris, 1968) Calling this an inductive definition of sublanguage Satoshi Sekine has embarked on studies involving new trends in the analysis of sublanguages (Sekine, 1994). Both Harris and Sekine recognized that sublanguages are an efficient way to observe and measure the properties of natural language in smaller, representative blocks. Getting down to specifics, McEnery and Wilson (McEnery and Wilson, 1996) have hypothesized that genres of writing, such as the style used in newspapers and similar printed publications to report news stories, represent a constrained subset of a natural language. Thus newspaper texts constitute a sublanguage — a version of a natural language which does not all of the creativity of that natural lan- 116 guage. The newspaper sublanguage can be further constrained by subject matter to divide it into smaller, more manageable subsets. A key mathematical feature of a sublanguage is that it will show a high degree of closure at various levels of description, setting it apart from unconstrained natural language. This closure property of a sublanguage is analogous to the mathematical property of transitive closure. McEnery and Wilson used the closure property to measure and compare rates of lexical and synclosure in three corpora: the computer manual corpus, the Canadian Hansards, and the American Printing House for the Blind corpus. To date, however, there has been little work in a similar vein in other languages. 2 Overview This work applies the methodology of McEnery and Wilson to examine closure rates in a comparative study of all available tagged Chinese corpora. First lexical and syntactic closure for this study in section 3. Then, section 4 begins this study with an examination of all the newspaper texts of the Academica Sinica Balanced Corpus (ASBC). Section 5 extends this study to an examination of the newspaper texts of the UPenn Chinese Treebank (CTB). Section 6 presents my findings and section 7 discusses some implications for future corpus building. 3 Lexical and Syntactic Closure 3.1 Tokenization in Chinese It should be pointed out that Chinese is an agglutinative, not an inflected language. Moreover, while Chinese tokens can concatenate, Chinese has no extensive morphology like many Indo-European languages. Chinese, of course, has no white space separating lexemes, as a result, all Chinese text must first be segmented into word lengths. However, once a text has been segmented, no stemming is needed so each segmented Chinese word can be counted as it occurs without the need of finding its lemma. 3.2 • Lexical Closure Lexical closure is that property of a collection of text whereby given a representative sample, the number of new lexical forms seen among every additional 1,000 new tokens begins to level off at a rate below 10 per cent. 3.3 Syntactic Closure Syntactic closure is that property of a collection of text whereby given a representative sample of a type of text, then the number of new syntactic forms seen among every additional 1,000 new tokens begins to level off. A syntactic form is the combination of token plus type. Thus syntactic closure approaches as the number of new grammatical uses for a previously observed token plus the number of new tokens, regardless of syntactic use, level off to a growth rate below 10 per cent. 4 Academica Sinica Balanced Corpus (ASBC) While it is common practice to attempt to build huge annotated corpora, it is of course very tedious, very expensive, and especially challenging for annotators to maintain consistency over such a huge task. Consequently one must hope that once an annotated corpus of newspaper texts is created, it can be statistically measured and confirmed to be a representative sample. I first measured lexical and syntactic closure rates in all ASBC newspaper texts but found that when viewed as a whole this newspaper sub-collection of the ASBC does not approach closure (see graphs below). ROCLing News Lexical Closure 117 ROCLing News Syntactic Closure raises the question can we hope for NLP applications to learn on large corif they themselves statisclosure, being statistically confirmed as a representative model of the language? I then focused downward on subsections of the newspaper corpus-grouping them by similar the ASBC corpus looking for files of annotated newspaper text and found a total of 57 files (18.7 Mb); my findings are summarized in the following table. 1991 through 1995 and dealt with many different subjects, the most frequent of which were sports, news, and domestic politics, but even each of these most frequent subjects only represented 9 per cent of the whole. Let us consider the three ASBC newspaper sub-collections (&amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, and &amp;quot;T&amp;quot; filenames) to be potentially representative sublanguages. If we can observe relatively high degrees of closure at various levels of description, we can propose that such sub-collections are representative sublanguages within the newspaper genre of Chinese natural language. Conversely, those which do not have a high degrees of closure are definitely not sublanguage corpora and not of further interest for this study. The following graphs depict the observed lexical and syntactic closure rates of the three ASBC newspaper sub-collections under study. ROCUng A Collection Lexical Closure</abstract>
<address confidence="0.622348">30000</address>
<note confidence="0.7750045">I20000 1</note>
<title confidence="0.816677333333333">E1:1 XAV ; g S Talons Academica Sinica Balanced Corpus News</title>
<author confidence="0.384959">Files Size Filenames Subject Matter</author>
<abstract confidence="0.926547730769231">05 01.9Mb A.... &apos;94 Academics 16 01.5Mb C.... &apos;93 Various News 01 00.2Mb SSSLA &apos;91 Politics etc. 36 15.1Mb T.... &apos;91-&apos;95 Sports etc. The large single file, named &amp;quot;SSLA&amp;quot;, dealt with a wide assortment of subject matter and thus was significantly different from the other 3 newspaper collections. Not only was its individual file size rather large; it was not even close to the size and homogeneity of the other three newspaper multi-file collections. I rejected it from further study. The other sub-collections were more similar. Topically speaking, the ASBC &amp;quot;A&amp;quot; newspaper collection was focused primarily on news (77 per cent) while at the same time focusing narrowly on academic events in 1994. The ASBC &amp;quot;C&amp;quot; newspaper collection was less narrowly focused on news (73.5 per cent) but expanded its focus to other than academia while limiting itself to events of 1993. The ASBC &amp;quot;T&amp;quot; newspaper collection, however, spanned the period aCco 7000 t COCC •</abstract>
<address confidence="0.692461">5000 2000 1000</address>
<abstract confidence="0.843361540540541">0 till Closure 0- 11 Tokens 118 &amp;quot;T* Loxical D11111111 Tokens It appears that the ASBC &amp;quot;A&amp;quot; newspaper sub-collections does approach lexical closure; while the &amp;quot;C&amp;quot; and &amp;quot;T&amp;quot; newspaper subcollections definitely do not. ROCting T Collection Synte clic Comm It appears that the ASBC &amp;quot;A&amp;quot; newspaper sub-collection also approaches syntactic closure; while the &amp;quot;C&amp;quot; and &amp;quot;T&amp;quot; newspaper subcollections do not. 5 UPenn Chinese Treebank ROCLIng A Collection Syntactic Closure Tokens ROCUng C Collection Syntactic Closure Types applied the same measures on the UPenn Chinese Treebank corpus. I wanted to compare the rate at which the UPenn collection approaches lexical and syntactic closure with that of the ASBC &amp;quot;A&amp;quot; and &amp;quot;T&amp;quot; sub-collections. The 329 Xinhua newswire documents in the UPenn Chinese Treebank annotated corpus came from two sub-collections and total 3,289 sentences averaging 27 words or 47 characters per sentence excluding newspaper headlines which are characteristically highly abbreviated clauses. The Pen l script written to do this analysis freely available at kovariks/closure.htm.</abstract>
<affiliation confidence="0.445706">UPenn Chinese Treebank Newspaper Texts Files Size Sub-Collection Subject</affiliation>
<address confidence="0.8891874">121 .322Mb One &apos;94 Economics 040 .114Mb Two &apos;96 Economics 076 .406Mb Two &apos;97 Economics 054 .181Mb Two &apos;98 Economics 038 .060Mb One General</address>
<affiliation confidence="0.704091">Unique Word/POS Pairs</affiliation>
<address confidence="0.500421">119</address>
<abstract confidence="0.951024438356164">CTB POSure Wenn CTBSyrdactic Clown Tokens 6 Findings The UPenn data initially approaches lexical and syntactic closure at a rate which can be favorably compared with the ASBC &amp;quot;A&amp;quot; Chinese newspaper sub-corpus. By the time 59,000 tokens of the UPenn corpus were tagged, only 56 new token+tag combinations were observed in the preceding 1,000 tokens and 27 of those were new proper nouns. In comparison, at this point the dosure rate in the ASBC &amp;quot;A&amp;quot; corpus was not quite as good (see table below). But continuing to the 69,000 token mark, the ASBC &amp;quot;A&amp;quot; closure rate had overtaken that of the UPenn CTB. Tokens Corpus New token-Ftags 59,000 UPenn CTB 56 (27 NR) ASBC A 77 (34 N...) 69,000 UPenn CTB 61 (36 NR) ASBC A 67 (49 N...) Interestingly the graphs for both the ASBC &amp;quot;A&amp;quot; and the UPenn CTB data reveal twohumped curves. At approximately the 60,000 token mark on both UPenn CTB graphs the curve was starting to flatten only to suddenly shift into a sharper climb. ROCUng A Collecdan Lextcal Ckaure TOkilri !Wenn CT8 Lexical Downs Tokens An investigation of the UPenn CTB data revealed that the vast majority of the documents at this point dealt with international asof the Chinese economy whereas previously the vast majority of documents had foon Chinese domestic economic growth And similarly on the ASBC &amp;quot;A&amp;quot; collection closure graphs around the 30,000 token mark, both &apos;Headlines of articles in UPenn CTB from 66,800-68,300 tokens- 66809: E3113f3&apos;M)rEEN.TtAlEittAt 67282: ; AL5E ; 2Topics of articles in UPenn CTB from tokens- 28071: 28345: ; 120 curves start to flatten only to suddenly around the 40,000 token mark shift back into a climb until about 70,000 tokens. An investigation of the ASBC data also showed a subtle shift in subject matter, most of the documents from 29,000 to 39,000 tokens were short notes and simple bulletins on, shifts in academic positions, whereas the later data had more long stories on of greater complexity whereas the later data had more long stories on subjects of complexity Thus the ASBC &amp;quot;A&amp;quot; collection, more so than the UPenn CTB corpus, did eventually seem to approach closure. By the time we reach 80,000 tokens, the ASBC &amp;quot;A&amp;quot; collection only saw 14 new tokend-tag combinations in the last thousand tokens of new text(see table below). Six of those 14 new combinations were nouns, seven verbs, and one was a preposition havnearly reached lexical closure on newspa- • 31890: ; .</abstract>
<note confidence="0.592949863636363">of articles in ASBC &amp;quot;A&amp;quot; from 29,300-32,000 29306: 29337: 29382: 29581: 4NE/ gig: • 30087: ; 30402: 30478: 30539: Atit A: ; 30677: 30822: 30944: 4144 ; . of articles in ASBC &amp;quot;A&amp;quot; from 56,000-57,000 56183: A*: • AI: 56274: EfEWIi. ; 56323: AE: 11443E4tffVf.e..1 Pk** • 56635: • 56798: 44 ; ; FIEIAREJ it Subcollection A* at 80,000 tokens, 14 New token+types observed: Na 0, Na Ifflf*, Na Ettg, VA VC OM, VF VF 411, VJ #t; Total tags:</note>
<abstract confidence="0.944498909090909">7, Total new items: 14. per articles regarding academics. In contrast the CTB collection instead logged 146 new token-Ftag combinations at the 80,000 token mark and its new vocabulary ranged widely across 12 parts of speech While the majority of this new vocabulary were nouns, this continuing influx of new words was due primarily to the late indusion of international news in the CTB&apos;s collection of newspaper articles regarding Chinese economics.</abstract>
<note confidence="0.867106">CTB at 80,000 tokens, 146 New token+types ADA ADM, ADA*, ADE, CD 2 N, cD1 7 1 644 cD2 0 0, cD2 0 0*, 10 0 CD28 0 0 N, CD4 0 0 5, na&amp;Mtg, JJ*I,</note>
<keyword confidence="0.7210856">LA#4, LC*, LC, LCM, M4, Mit.T., NNIWA NNI4W, MAIM, NNEm, NN*Nt,NN5ttit, NN*, NNaolik, NNW40.7., NNrIA, NN NN, NAM,NN3, NNEA, NIT% t, NOM, NNE, NNIA, NNgt, NN*, 41, NN4‘4, NNIMR, NNEPt, NNXt, NNXINIA, NN, NNatak, NN4A, NN a, NA*, NNtM, NR, amt NR/NACS, • gitTri**, NR* NRI, NRifittg, NREP, NT M, NT 1 8 3 3*,</keyword>
<address confidence="0.78528325">8 9 3 9 &amp;r, 9 8 2 *, 9 8 6*, 9 8 9 *, NT*, Pftfa, VAP, vvittit, v04114, \MAW</address>
<email confidence="0.3424065">vvitAN,vv*#,vvai,vv-Mk,</email>
<note confidence="0.730551181818182">vvEgE, vv**RI, vv1ta, vv*Yg, vv*IAYgeg, vAR, \TNTZt, vviE1ñS9I, Total tags: 12, Total new items: 146. 121 Tokens Corpus New token+tags 80,000 UPenn CTB NR) ASBC A 14 (6 N...) 7 Corpus Building Implications The fact that the UPenn Chinese Treebank</note>
<abstract confidence="0.975922131578947">data approaches lexical and syntactic closure at rates comparable to the ASBC &amp;quot;A&amp;quot; file newspaper collection suggests that if the UPenn data had been selected more narrowly, it might have reached closure for the economics domain in the newspaper genre even sooner. Some day corpus linguistics may only need much smaller collections of annotated corpora than is the practice today, relying on new directions in sublanguage research. In the case of the ASBC &amp;quot;A&amp;quot; collection, for example, a robust learning structure should be able to build a useable model on 100,000 words worth of data like this which exhibits strong tendencies to lexical and syntactic closure in the Chinese newspaper genre constrained to a given domain. If the UPenn CTB were enlarged by the infusion of more news stories on international aspects of Chinese economic development, the CTB might better reach lexical and syntactic closure. The following graph shows that the blind addition of 20K additional Chinese economic news stories does not aid closure much. This additional data spanned many topics not seen in the original 100K collection. If it had been selected precisely to aid closure by measuring its potential contribution before extensive hand annotation, the result could have been better. 20K Augrnented C7B Laical Omura Toicem 20K Augmented CTB Syntactic Closure Tokens Nevertheless, this expanded CTB collection is sufficiently improved that the rate of closure toward of the expanded collection is better (see table below). Tokens New Tokens New Token+Tags</abstract>
<address confidence="0.8760276">121,000 55 64 (12 NR) 122,000 61 68 (17 NR) 123,000 61 76 (23 NR) 124,000 78 82 (18 NR) 125,000 48 63 (11 NR)</address>
<phone confidence="0.39717">126,000 52 .61 (16 NR)</phone>
<abstract confidence="0.984045">Consequently, this expanded CTB collection is sufficiently improved that merging it with the ASBC &amp;quot;A&amp;quot; collection results in a far more measurably representative larger corpus as shown in the final two graphs below. The creation of such measurably representative large corpora out of such smaller, better focused sublanguage building blocks would be cheaper and faster without the resulting tools developed against such corpora suffering much degradation in speed or accuracy.</abstract>
<title confidence="0.452363">Combined ASBC &apos;A&amp;quot; and UPenn CM Lexical Closure Tokens</title>
<address confidence="0.922873333333333">20000 16000 14000 . .</address>
<email confidence="0.794089">moo</email>
<address confidence="0.5307314">I00 6000 • 4000 2000 122</address>
<abstract confidence="0.939170380952381">ASBC and Wenn CTB Syntactic Closure 8 Discussion Since only two Chinese tagged corpora are available at the present time, only about 200,000 words of Chinese corpora have been so studied. But to see more work in this vein, one need only consult McEnery and Wilson&apos;s study of three English corpora: the IBM computer manual corpus, the Canadian Hansards, and the American Printing House for the Blind corpus (McEnery and Wilson, 1996). Their study (exhaustively detailed in Chapter 6 of their book) spans more than 2.2 million words of tagged English text in three different domains. Conthe total of 2 3 millionwords from McEnery and Wilson&apos;s results in tagged English texts when combined with these results in tagged Chinese newspaper texts should satisfy any who might argue that there is insufficient data upon which to draw some general conclusions. This paper does not argue that the two closure measures used are the only measures possible. The argument here is simply that these two closure measures are used to spot when a sublanguage corpus approaches closure-that is, when the curve of new types and new combinations of type with token begins to flatten at a rate below ten percent. One can readily point out that no natural language corpus can ever guarantee closure. The best anyone can aspire to do today, given the current state of our art, is to only approach closure. 9 References References D. Biber. 1993. Representativeness in corpus design. volume 8(4), pages 243-257. P. Bolt. 1994. The international corpus of english project-the hong kong experience. pages 15-24. R. Garside and A. McEnery. 1993. Treebanking: the compilation of a corpus of skeleton</abstract>
<note confidence="0.966508375">parsed sentences. pages 17-35. S. Greenbaum. 1992. A new corpus of english: Ice. pages 171-179. Harris. 1968. York: John Wiley and Sons. G. Leech. 1991. The state of the art in corpus linguistics. pages 8-29. McEnery and A. Wilson. 1996. Lin-</note>
<affiliation confidence="0.835882">Edinburgh University</affiliation>
<note confidence="0.877193571428571">Press. S. Sekine. 1994. A new direction for sublannip. In of the International Conference on New Methods in Lan- Processing, CCL, UMIST, 123- 129. 123</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Biber</author>
</authors>
<title>Representativeness in corpus design.</title>
<date>1993</date>
<volume>8</volume>
<issue>4</issue>
<pages>243--257</pages>
<contexts>
<context position="2154" citStr="Biber, 1993" startWordPosition="327" endWordPosition="328">E) project (Greenbaum, 1992) which sought to assemble a range of both spoken and written texts along with a range of both formal and informal situations to provide a reasonably large, well-documented and detailed snapshot of the use of educated English. (Bolt, 1994) Both the IBM/Lancaster approach and the ICE project build millions of tokens worth of corpora. But from a more principled perspective, Douglas Biber, in speaking on representativeness in corpus design, pointed out that the linguistic characterization of a corpus should include both its central tendency and its range of variation. (Biber, 1993) Similarly Geoffrey Leech has stated that a corpus, in order to be representative, must somehow capture the magnitude of languages not only in their lexis but also in their syntax. (Leech, 1991) This suggests we should build corpora focusing on how well they can approach lexical and syntactic closure, rather than by merely fixating on ever larger amounts of text. To build representative corpora why not first select representative texts constrained by genre of writing? In general one possible technique for methodical corpus selection would be to build large corpora out of representative sub-col</context>
</contexts>
<marker>Biber, 1993</marker>
<rawString>D. Biber. 1993. Representativeness in corpus design. volume 8(4), pages 243-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bolt</author>
</authors>
<title>The international corpus of english project-the hong kong experience.</title>
<date>1994</date>
<pages>15--24</pages>
<contexts>
<context position="1808" citStr="Bolt, 1994" startWordPosition="272" endWordPosition="273">d sentences and then focused on the Computer Manuals domain, in which they attempted to randomly select texts with some additional nonComputer Manual material selected as &amp;quot;a measure of &apos;light relief&apos;&amp;quot; supposedly for the benefit of the annotators. A broad approach was also used in the Hong Kong element of the International Corpus of English (ICE) project (Greenbaum, 1992) which sought to assemble a range of both spoken and written texts along with a range of both formal and informal situations to provide a reasonably large, well-documented and detailed snapshot of the use of educated English. (Bolt, 1994) Both the IBM/Lancaster approach and the ICE project build millions of tokens worth of corpora. But from a more principled perspective, Douglas Biber, in speaking on representativeness in corpus design, pointed out that the linguistic characterization of a corpus should include both its central tendency and its range of variation. (Biber, 1993) Similarly Geoffrey Leech has stated that a corpus, in order to be representative, must somehow capture the magnitude of languages not only in their lexis but also in their syntax. (Leech, 1991) This suggests we should build corpora focusing on how well </context>
</contexts>
<marker>Bolt, 1994</marker>
<rawString>P. Bolt. 1994. The international corpus of english project-the hong kong experience. pages 15-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>A McEnery</author>
</authors>
<title>Treebanking: the compilation of a corpus of skeleton parsed sentences.</title>
<date>1993</date>
<pages>17--35</pages>
<contexts>
<context position="992" citStr="Garside and McEnery, 1993" startWordPosition="135" endWordPosition="138"> in annotated Chinese newspaper corpora from the Academica Sinica Balanced Corpus and the University of Pennsylvania&apos;s Chinese Treebank. It then draws inferences as to how large such corpora need be to be representative models of subject-matterconstrained language domains within the same genre. Future large corpora should be built incrementally only by combining smaller representative sublanguage collections. 1 Prior Work Practically speaking, earlier attempts at building corpora, such as the IBM/Lancaster approach, have taken an all-inclusive perspective toward text selection proposing that (Garside and McEnery, 1993) raw texts for parsed corpora should come from a variety of sources. The IBM/Lancaster group used the Canadian Hansards collection of parallel parsed English and French sentences as a base of English parsed sentences and then focused on the Computer Manuals domain, in which they attempted to randomly select texts with some additional nonComputer Manual material selected as &amp;quot;a measure of &apos;light relief&apos;&amp;quot; supposedly for the benefit of the annotators. A broad approach was also used in the Hong Kong element of the International Corpus of English (ICE) project (Greenbaum, 1992) which sought to assem</context>
</contexts>
<marker>Garside, McEnery, 1993</marker>
<rawString>R. Garside and A. McEnery. 1993. Treebanking: the compilation of a corpus of skeleton parsed sentences. pages 17-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Greenbaum</author>
</authors>
<title>A new corpus of english: Ice.</title>
<date>1992</date>
<pages>171--179</pages>
<contexts>
<context position="1570" citStr="Greenbaum, 1992" startWordPosition="232" endWordPosition="234">osing that (Garside and McEnery, 1993) raw texts for parsed corpora should come from a variety of sources. The IBM/Lancaster group used the Canadian Hansards collection of parallel parsed English and French sentences as a base of English parsed sentences and then focused on the Computer Manuals domain, in which they attempted to randomly select texts with some additional nonComputer Manual material selected as &amp;quot;a measure of &apos;light relief&apos;&amp;quot; supposedly for the benefit of the annotators. A broad approach was also used in the Hong Kong element of the International Corpus of English (ICE) project (Greenbaum, 1992) which sought to assemble a range of both spoken and written texts along with a range of both formal and informal situations to provide a reasonably large, well-documented and detailed snapshot of the use of educated English. (Bolt, 1994) Both the IBM/Lancaster approach and the ICE project build millions of tokens worth of corpora. But from a more principled perspective, Douglas Biber, in speaking on representativeness in corpus design, pointed out that the linguistic characterization of a corpus should include both its central tendency and its range of variation. (Biber, 1993) Similarly Geoff</context>
</contexts>
<marker>Greenbaum, 1992</marker>
<rawString>S. Greenbaum. 1992. A new corpus of english: Ice. pages 171-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Mathematical Structures of Language.</title>
<date>1968</date>
<publisher>John Wiley and Sons.</publisher>
<location>New York:</location>
<contexts>
<context position="3009" citStr="Harris, 1968" startWordPosition="468" endWordPosition="470">on how well they can approach lexical and syntactic closure, rather than by merely fixating on ever larger amounts of text. To build representative corpora why not first select representative texts constrained by genre of writing? In general one possible technique for methodical corpus selection would be to build large corpora out of representative sub-collections constrained by genre and subject matter. Zelig Harris said, &amp;quot;Certain proper subsets of the sentences of a language may be closed under some or all of the operations defined in the language, and thus constitute a sublanguage of it.&amp;quot; (Harris, 1968) Calling this an inductive definition of sublanguage Satoshi Sekine has embarked on studies involving new trends in the analysis of sublanguages (Sekine, 1994). Both Harris and Sekine recognized that sublanguages are an efficient way to observe and measure the properties of natural language in smaller, representative blocks. Getting down to specifics, McEnery and Wilson (McEnery and Wilson, 1996) have hypothesized that genres of writing, such as the style used in newspapers and similar printed publications to report news stories, represent a constrained subset of a natural language. Thus newsp</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Z. Harris. 1968. Mathematical Structures of Language. New York: John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
</authors>
<title>The state of the art in corpus linguistics.</title>
<date>1991</date>
<pages>8--29</pages>
<contexts>
<context position="2348" citStr="Leech, 1991" startWordPosition="360" endWordPosition="361">ented and detailed snapshot of the use of educated English. (Bolt, 1994) Both the IBM/Lancaster approach and the ICE project build millions of tokens worth of corpora. But from a more principled perspective, Douglas Biber, in speaking on representativeness in corpus design, pointed out that the linguistic characterization of a corpus should include both its central tendency and its range of variation. (Biber, 1993) Similarly Geoffrey Leech has stated that a corpus, in order to be representative, must somehow capture the magnitude of languages not only in their lexis but also in their syntax. (Leech, 1991) This suggests we should build corpora focusing on how well they can approach lexical and syntactic closure, rather than by merely fixating on ever larger amounts of text. To build representative corpora why not first select representative texts constrained by genre of writing? In general one possible technique for methodical corpus selection would be to build large corpora out of representative sub-collections constrained by genre and subject matter. Zelig Harris said, &amp;quot;Certain proper subsets of the sentences of a language may be closed under some or all of the operations defined in the langu</context>
</contexts>
<marker>Leech, 1991</marker>
<rawString>G. Leech. 1991. The state of the art in corpus linguistics. pages 8-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T McEnery</author>
<author>A Wilson</author>
</authors>
<title>Corpus Linguistics. Edinburgh:</title>
<date>1996</date>
<publisher>Edinburgh University Press.</publisher>
<contexts>
<context position="3408" citStr="McEnery and Wilson, 1996" startWordPosition="527" endWordPosition="530"> and subject matter. Zelig Harris said, &amp;quot;Certain proper subsets of the sentences of a language may be closed under some or all of the operations defined in the language, and thus constitute a sublanguage of it.&amp;quot; (Harris, 1968) Calling this an inductive definition of sublanguage Satoshi Sekine has embarked on studies involving new trends in the analysis of sublanguages (Sekine, 1994). Both Harris and Sekine recognized that sublanguages are an efficient way to observe and measure the properties of natural language in smaller, representative blocks. Getting down to specifics, McEnery and Wilson (McEnery and Wilson, 1996) have hypothesized that genres of writing, such as the style used in newspapers and similar printed publications to report news stories, represent a constrained subset of a natural language. Thus newspaper texts constitute a sublanguage — a version of a natural language which does not display all of the creativity of that natural lan116 guage. The newspaper sublanguage can be further constrained by subject matter to divide it into smaller, more manageable subsets. A key mathematical feature of a sublanguage is that it will show a high degree of closure at various levels of description, setting</context>
<context position="18972" citStr="McEnery and Wilson, 1996" startWordPosition="3132" endWordPosition="3135">st such corpora suffering much degradation in speed or accuracy. Combined ASBC &apos;A&amp;quot; and UPenn CM Lexical Closure Tokens 20000 18000 II. 16000 14000 . . moo I00 6000 • 4000 2000 122 Combined ASBC -A: and Wenn CTB Syntactic Closure 8 Discussion Since only two Chinese tagged corpora are available at the present time, only about 200,000 words of Chinese corpora have been so studied. But to see more work in this vein, one need only consult McEnery and Wilson&apos;s study of three English corpora: the IBM computer manual corpus, the Canadian Hansards, and the American Printing House for the Blind corpus (McEnery and Wilson, 1996). Their study (exhaustively detailed in Chapter 6 of their book) spans more than 2.2 million words of tagged English text in three different domains. Consequently the total of 2 3 million words from McEnery and Wilson&apos;s results in tagged English texts when combined with these results in tagged Chinese newspaper texts should satisfy any who might argue that there is insufficient data upon which to draw some general conclusions. This paper does not argue that the two closure measures used are the only measures possible. The argument here is simply that these two closure measures are used to spot</context>
</contexts>
<marker>McEnery, Wilson, 1996</marker>
<rawString>T. McEnery and A. Wilson. 1996. Corpus Linguistics. Edinburgh: Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
</authors>
<title>A new direction for sublanguage nip.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing, CCL, UMIST,</booktitle>
<pages>123--129</pages>
<contexts>
<context position="3168" citStr="Sekine, 1994" startWordPosition="492" endWordPosition="493"> not first select representative texts constrained by genre of writing? In general one possible technique for methodical corpus selection would be to build large corpora out of representative sub-collections constrained by genre and subject matter. Zelig Harris said, &amp;quot;Certain proper subsets of the sentences of a language may be closed under some or all of the operations defined in the language, and thus constitute a sublanguage of it.&amp;quot; (Harris, 1968) Calling this an inductive definition of sublanguage Satoshi Sekine has embarked on studies involving new trends in the analysis of sublanguages (Sekine, 1994). Both Harris and Sekine recognized that sublanguages are an efficient way to observe and measure the properties of natural language in smaller, representative blocks. Getting down to specifics, McEnery and Wilson (McEnery and Wilson, 1996) have hypothesized that genres of writing, such as the style used in newspapers and similar printed publications to report news stories, represent a constrained subset of a natural language. Thus newspaper texts constitute a sublanguage — a version of a natural language which does not display all of the creativity of that natural lan116 guage. The newspaper </context>
</contexts>
<marker>Sekine, 1994</marker>
<rawString>S. Sekine. 1994. A new direction for sublanguage nip. In Proceedings of the International Conference on New Methods in Language Processing, CCL, UMIST, pages 123-129.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>