<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000180">
<title confidence="0.978953">
Generating Natural Language Descriptions of Z Test Cases
</title>
<author confidence="0.741628">
Maximiliano Cristi´a
</author>
<affiliation confidence="0.663115">
Flowgate Consulting and CIFASIS
</affiliation>
<address confidence="0.526875">
Rosario, Argentina
</address>
<email confidence="0.993398">
mcristia@flowgate.net
</email>
<author confidence="0.9874">
Brian Pl¨uss
</author>
<affiliation confidence="0.9712045">
Centre for Research in Computing
The Open University
</affiliation>
<address confidence="0.697155">
Milton Keynes, UK
</address>
<email confidence="0.998229">
b.pluss@open.ac.uk
</email>
<sectionHeader confidence="0.993884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999829578947368">
Critical software most often requires an
independent validation and verification
(IVV). IVV is usually performed by do-
main experts, who are not familiar with
specific, many times formal, development
technologies. In addition, model-based
testing (MBT) is a promising testing tech-
nique for the verification of critical soft-
ware. Test cases generated by MBT tools
are logical descriptions. The problem is,
then, to provide natural language (NL) de-
scriptions of these test cases, making them
accessible to domain experts. In this pa-
per, we present ongoing research aimed at
finding a suitable method for generating
NL descriptions from test cases in a for-
mal specification language. A first proto-
type has been developed and applied to a
real-world project in the aerospace sector.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998025">
Model-based testing (MBT) is an active research
area and a promising theory of software and hard-
ware testing (Utting and Legeard, 2006; Hierons
et al., 2009). MBT approaches start with a formal
model or specification of the software, from which
test cases are generated. These techniques have
been developed and applied to models written in
different formal notations, such as Z (Stocks and
Carrington, 1996), finite state machines and their
extensions (Grieskamp et al., 2002), B (Legeard et
al., 2002), algebraic specifications (Bernot et al.,
1991), and so on.
The fundamental hypothesis behind MBT is
that, as a program is correct if it verifies its specifi-
cation, then the specification is an excellent source
of test cases. Once test cases are derived from the
model, they are refined to the level of the imple-
mentation language and executed. The resulting
output is then abstracted to the level of the speci-
fication language, and the model is used again to
verify if the test case has detected an error.
The Test Template Framework (TTF) described
by Stocks and Carrington (1996) is a particular
MBT theory specially well suited for unit testing.
The TTF uses Z specifications (Spivey, 1989) as
the entry models and prescribes how to generate
test cases for each operation included in the model.
Fastest (Cristi´a and Rodr´ıguez Monetti, 2009) im-
plements the TTF allowing users to automatically
produce test cases for a given Z specification. Re-
cently, we used Fastest to test an on-board satellite
software for a major aerospace company in South
America. Since Fastest uses models written in the
Z specification language, test cases generated by
this tool are paragraphs of formal text (see Section
2). This description is suitable for the automatic
tasks involved in testing (e.g., automatic execu-
tion, hyperlinking, traceability), but humans need
to be able to read Z specifications in order to un-
derstand what is being tested. In projects where
independent verification and validation (IVV) is
required this might be a problem, as most stake-
holders will not necessarily be fluent in Z.
This is precisely the case in the project men-
tioned above, where the aerospace company re-
quested not only the test cases in Z, but also in
English. As it can be expected, in a project with
hundreds of test cases, manual translation would
increase the overall cost of testing and, most criti-
cally, reduce its quality due to the introduction of
human errors. Interestingly, this problem is op-
posite to those in mainstream industrial practice,
where test cases are described in natural language
and must be formalised, in order to augment the
quality and, hopefully, reduce the costs of testing.
Given the formal, structured nature of the
source text, natural language generation (NLG)
techniques seem to be an appropriate approach
to solving this problem. In the rest of the pa-
per, we give an example of a test case from the
project mentioned above (Section 2), describe a
template-based method for generating NL descrip-
tions (Section 3), and propose further work to-
wards a more general NLG solution (Section 4).
</bodyText>
<sectionHeader confidence="0.9673575" genericHeader="method">
2 An Example from the Aerospace
Industry
</sectionHeader>
<bodyText confidence="0.9998385">
The problem of generating NL descriptions of
specifications in Z arises in the following scenario:
a company developing the software for a satellite
needs to verify that the implementation conforms
to a certain aerospace standard (ECSS, 2003) de-
scribing the basic functionality of any satellite
software. We therefore started by modelling in Z
the services described by the standard and used the
Fastest tool to generate test cases.
The model is a “standard” Z specification: it
has a schema box that defines the state space of
the system and operations defining the transition
relation between states1. Each operation formal-
izes one of the services described by the standard
(e.g., memory dump, telecommand verification,
enabling or disabling on-board monitoring, etc.).
Figure 1 shows one of the test cases generated
for the operation DumpMemoryAbsAdd, that mod-
els a remote request for the on-board software to
dump some portion of its memory. In TTF and
Fastest, a Z test case is essentially a set of bind-
ings between variables and values, and test cases
are grouped according to the operation they test.
Identifiers appearing in a test case are the input
and state variables from the definition of the oper-
ation. These are bound to certain values defining
the state in which the system must be tested and
the input given to the operation in each unique test
case. In the example, input variables are those dec-
orated with a question mark, while state variables
are plain identifiers. All these variables are de-
clared somewhere else in the specification, by us-
ing a special schema box called valid input space,
associated with each operation.
For example, the Z schema in Figure 1 indicates
that the implementation of the dump memory ser-
vice must be tested in a state where the system is
processing a telecommand (processingTC = yes),
the telecommand is a request for a memory dump
(srv = DMMA), the system has one memory block
</bodyText>
<footnote confidence="0.940359">
1The Z specification language is essentially typed first or-
der logic, with syntactic sugar in the form of operators, that
serve as shortcuts for frequently used complex expressions.
</footnote>
<equation confidence="0.985558">
DumpMemoryAbsAdd SP 7 TCASE
mid = mid0 n srv = DMAA n lengths = 0
processingTC = yes n adds = 0
blocks = {mid0 &gt; {1 &gt; byte0, 2 &gt; byte1,
3 &gt; byte2, 4 &gt; byte3}}
m? = mid0 n sa? = (1) n len? = (2)
</equation>
<figureCaption confidence="0.995624">
Figure 1: A test case described in Z
</figureCaption>
<bodyText confidence="0.980742066666667">
which is four bytes long (blocks = I...1), there
are no other pending requests (adds = lengths =
∅); and the request is for a memory dump of
length two (len? = (2)) starting at the first ad-
dress (sa? = (1)) of the available memory block
(m? = mid0 = mid).
Fastest generated almost 200 test cases like the
one depicted in Figure 1 from a model describ-
ing a simplified version of five services listed in
the standard. The customer requested to deliver a
natural language description of each one of them
and a model describing all the services would have
thousands of test cases. Clearly, trying to make
the translation by hand would have been not only
a source of errors, but also a technical retreat.
</bodyText>
<sectionHeader confidence="0.974188" genericHeader="method">
3 A Template-Based NLG Solution
</sectionHeader>
<bodyText confidence="0.979249115384615">
As a first approach, we used a template-based
method. We started by defining a grammar to
express what we called NL test case templates
(NLTCT). It appears in Figure 22. Each NLTCT
specifies how an NL description is generated for
the Z test cases of a given operation. It starts with
the name of the operation. Next follows a text sec-
tion, intended as a parametrized NL description of
the test case, where calls to translation rules can
be inserted as needed. Finally, all necessary trans-
lation rules are defined, by indicating what is writ-
ten in replacement for a call when a certain vari-
able in the formal description of a test case appears
bound to a specific value. In this way, a different
text is generated for each test case, according to
the binding between values and variables that de-
fines the case. The Appendix shows the NLTCT
for the operation DumpMemoryAbsAdd.
We implemented a parser in awk that takes an
NLTCT and a Z test case, and generates the NL
description of each test case in the input. Figure 3
shows the result for the test case in Figure 1.
This first prototype showed that NLTCTs tend
2Fastest saves formal test cases in text files written in ZLa-
TeX, an extension of the LATEX markup language, what ex-
plains the use of this format in the NLTCT grammar.
</bodyText>
<equation confidence="0.982542611111111">
NLTCT ::= (Operation) eol
(NLTCD) eol
(TCRule){, (TCRule)}
Operation ::= operation =(identifier)
NLTCD ::= \begin{tcase} eol
(LaTeXText) eol
\end{tcase}
LaTeXText ::= LaTeX  |(TCRuleCall)  |(LaTeXText)
TCRuleCall ::= &amp; rule (identifier) &amp;
TCRule ::= \begin{trule}{(identifier)} eol
case (identifier)[, (identifier)] eol
(RuleDef) eol {, (RuleDef) eol}
endcase eol
\end{trule}
RuleDef ::= $(ZLaTeX)[“  |“ (ZLaTeX)  |&amp; (ZLaTeX)]
: (LaTeX) eol
LaTeX ::= free LATEX text
ZLaTeX ::= free Z LATEX text
</equation>
<figureCaption confidence="0.984007">
Figure 2: Grammar for NLTC templates
</figureCaption>
<bodyText confidence="0.999973958333333">
to be relatively small and simple, in spite of the
large number of test cases. This is due to test cases
combining a small set of values in many differ-
ent ways. However, NLTCTs for large operations
tend to become increasingly more complex, for
the number of combinations grows exponentially.
As a consequence, these operations require a large
number of cases within translation rules and some-
times even more translation rules3.
A thorough evaluation of this method is due. Its
suitability must be measured from the perspective
of two kinds of users: (a) the engineers who write
the formal models, generate the formal test cases
and write the NLTCTs; and (b) other stakeholders
(e.g., the customer, auditors, domain experts), who
read the descriptions of the test cases in natural
language. For the engineers, applying the method
should be more efficient, in terms of time and ef-
fort, than writing the descriptions by hand. For the
readers, success will be determined by the read-
ability of the output and, more critically, by its
precision with respect to the specification. At the
moment of writing, we are designing two empiri-
cal studies aimed at obtaining these measures.
</bodyText>
<sectionHeader confidence="0.991695" genericHeader="evaluation">
4 Future and Related Work
</sectionHeader>
<bodyText confidence="0.999081">
The solution presented above was successful in
generating adequate NL descriptions of the test
</bodyText>
<footnote confidence="0.997412">
3This is because templates are written in terms of the val-
ues bound to variables, and not in terms of the predicates sat-
isfied by those values, which are nonetheless available as part
of the MBT approach.
</footnote>
<note confidence="0.941926">
Test case: DumpMemoryAbsAdd SP 7 TCASE
Service (6,5) will be tested in a situation that verifies that:
</note>
<listItem confidence="0.9406405">
• the state is such that:
– the on-board system is currently processing a
telecommand and has not answered it yet.
– the service type of the telecommand is
DMAA.
– the set of sequences of available memory
cells contains only one sequence, associated
to a memory ID, which has four different
bytes.
– the set of starting addresses of the chunks
of memory that have been requested by the
ground is empty.
• the input memory ID to be dumped is the avail-
able memory ID, the input set of start addresses
of the memory regions to be dumped is the uni-
tary sequence composed of 1, the set of numbers of
memory cells to be dumped is the unitary sequence
composed of 2.
</listItem>
<figureCaption confidence="0.997977">
Figure 3: NL description of the test in Figure 1
</figureCaption>
<bodyText confidence="0.969014136363636">
cases in one particular project. However, the lim-
itations mentioned in the previous section show
that this solution would not generalise well to
specifications in other domains. Moreover, it re-
quires defining a new template for each operation;
a task of still considerable size for large systems.
At the same time, Z specifications contain all
the information necessary to produce the tem-
plates for the operations in the system, regardless
of its domain of application. This information is
structured according to the syntax of the formal
language. Additionally, when formally specifying
a system, it is common practice to include associ-
ations between the identifiers in the specification
(new types, operations, state schemata, variables,
constants, etc.) and the elements they refer to in
the application domain (i.e., aerospace software).
These associations are called designations (Jack-
son, 1995), some of which, relevant to the test case
in Figure 1, are shown in Figure 4.
These considerations lead us to believe in the
srv Pz� Service type of the telecommand
</bodyText>
<figureCaption confidence="0.864739666666667">
DMAA Pz� Dump memory using absolute addresses
processingTC Pz� The on-board system is currently processing
a telecommand and has not answered it yet
m? R� Memory ID to be dumped
sa? Pz� Start addresses of the memory regions to be
dumped
len? R� The number of memory cells to be dumped
for each start address
Figure 4: Designations for the test in Figure 1
</figureCaption>
<bodyText confidence="0.999940869565217">
possibility of generating NL descriptions of Z test
cases automatically by using their definitions, the
system specification and the designations of iden-
tifiers. Such a solution would be independent of
the application domain and, more importantly, of
the number of operations in the model.
The linguistic properties of the target document
are relevant in devising an adequate treatment for
the input, but the overall structure of the output re-
mains rigid and its content is determined by the
definition of each test case. The approach would
still be template-based, but in terms of the NLG
architecture of Reiter and Dale (2000), templates
would be defined at the level of the document
structure4, with minimal microplanning and sur-
face strings generated according to the part of the
test case being processed and the designations of
the identifiers5. The next stages of our project will
point in this direction, using techniques from NLG
for automating the definition of the templates pre-
sented in the previous section.
There have been efforts for producing nat-
ural language versions of formal specifications
in the past. Punshon et al. (1997) use a case
study to present the REVIEW system (Salek et
al., 1994)6. REVIEW automatically paraphrases
specifications developed with Metaview (Soren-
son et al., 1988), an academic research metasys-
tem that facilitates the construction of CASE envi-
ronments to support software specification tasks.
Coscoy (1997) describes a mechanism based on
program extraction, for generating explanations of
formal proofs in the Calculus of Inductive Con-
structions, implemented in the Coq Proof Assis-
tant (Bertot and Cast´eran, 2004). Lavoie et al.
(1997) present MODEX, a tool that generates cus-
tomizable descriptions of the relations between
classes in object-oriented models specified in the
ODL standard (Cattell and Barry, 1997). Bertani
et al. (1999) describe a controlled natural language
approach to translating formal specifications writ-
ten in an extension of TRIO (Ghezzi et al., 1990)
by transforming syntactic trees in TRIO into syn-
tactic trees of the controlled language.
The solutions presented in the related work
above are highly dependant on particular aspects
</bodyText>
<footnote confidence="0.993684142857143">
4Somewhat along the lines of what Wilcock (2005) de-
scribes for XML-based NLG.
5This approach is similar to the method proposed by Kit-
tredge and Lavoie (1998) for generating weather forecasts.
6Salek et al. (1994) also give a comprehensive survey of
related work for generating NL explanations for particular
specification languages (most of which are now obsolete).
</footnote>
<bodyText confidence="0.9998448">
of the source language and do not apply directly
to specifications written in Z. To our knowledge,
no work has been done towards producing NL de-
scriptions of Z specifications. The same holds for
test cases generated using the MBT approach.
</bodyText>
<sectionHeader confidence="0.989611" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998237">
In this paper we presented a concrete NLG prob-
lem in the area of software development involv-
ing formal methods. We focused the description
on the generation of NL descriptions of test cases,
but nothing prevents us from extending the idea to
entire system specifications.
The development of a general technique for ver-
balising formal specification would fill the com-
munication gap between system designers and
other stakeholders in the development process,
while preserving the advantages associated to the
use of formal methods: precision, lack of ambigu-
ity, formal proof of system properties, etc.
Finally, we hope this paper draws attention from
NLG experts to an area which would benefit sub-
stantially from their expertise.
</bodyText>
<sectionHeader confidence="0.694733" genericHeader="acknowledgments">
Acnowledgements
</sectionHeader>
<bodyText confidence="0.999755714285714">
A substantial part of this research was funded by
Flowgate Consulting. We would also like to thank
Richard Power from the NLG group at The Open
University for help in finding financial support,
Eva Banik for helpful comments on earlier ver-
sions of this paper, and three anonymous reviewers
for useful feedback and suggestions.
</bodyText>
<sectionHeader confidence="0.861829" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.699588857142857">
G. Bernot, M.C. Gaudel, and B. Marre. 1991. Soft-
ware testing based on formal specifications: a the-
ory and a tool. Software Engineering Journal (SEJ),
6(6):387–405.
A. Bertani, W. Castelnovo, E. Ciapessoni, and G.
Mauri. 1999. Natural language translations of for-
mal specifications for complex industrial systems.
</bodyText>
<reference confidence="0.952111175824176">
In AI*IA 1992: Proceedings of the 6th Congress
of the Italian Association for Artificial Intelligence,
pages 185–194, Bologna, Italy.
Y. Bertot and P. Cast´eran. 2004. Interactive Theorem
Proving and Program Development. Coq’Art: The
Calculus of Inductive Constructions. Texts in Theo-
retical Computer Science. Springer-Verlag.
R.G.G. Cattell and D.K. Barry, editors. 1997. The ob-
ject database standard: ODMG 2.0. Morgan Kauf-
mann Publishers Inc., San Francisco, CA.
Y. Coscoy. 1997. A natural language explanation for
formal proofs. In LACL ’96: Selected papers from
the First International Conference on Logical As-
pects of Computational Linguistics, pages 149–167,
London, UK. Springer-Verlag.
M. Cristi´a and P. Rodr´ıguez Monetti. 2009. Imple-
menting and applying the Stocks-Carrington frame-
work for model-based testing. In Karin Breitman
and Ana Cavalcanti, editors, ICFEM, volume 5885
of Lecture Notes in Computer Science, pages 167–
185. Springer-Verlag.
ECSS. 2003. Space Engineering – Ground Sys-
tems and Operations: Telemetry and Telecommand
Packet Utilization. Technical Report ECSS-E-70-
41A, European Space Agency.
C. Ghezzi, D. Mandrioli, and A. Morzenti. 1990.
TRIO: A logic language for executable specifica-
tions of real-time systems. Journal of Systems and
Software, 12(2):107–123.
W. Grieskamp, Y. Gurevich, W. Schulte, and M.
Veanes. 2002. Generating finite state machines
from abstract state machines. In ISSTA ’02: Pro-
ceedings of the 2002 ACM SIGSOFT International
Symposium on Software Testing and Analysis, pages
112–122, Rome, Italy.
R.M. Hierons, K. Bogdanov, J.P. Bowen, R. Cleave-
land, J. Derrick, et al. 2009. Using formal specifi-
cations to support testing. ACM Computing Surveys
(CSUR), 41(2):9.
M. Jackson. 1995. Software requirements &amp; specifi-
cations: a lexicon ofpractice, principles, and preju-
dices. Addison-Wesley.
R. Kittredge and B. Lavoie. 1998. Meteocogent: A
knowledge-based tool for generating weather fore-
cast texts. In Proceedings of American Meteorolog-
ical Society AI Conference (AMS-98), Phoenix, AZ.
B. Lavoie, O. Rambow, and E. Reiter. 1997. Cus-
tomizable descriptions of object-oriented models. In
Proceedings of the Conference on Applied Natural
Language Processing (ANLP’97, pages 253–256,
Washington, DC.
B. Legeard, F. Peureux, and M. Utting. 2002. A Com-
parison of the BTT and TTF Test-Generation Meth-
ods. In ZB ’02: Proceedings of the 2nd Interna-
tional Conference of B and Z Users on Formal Spec-
ification and Development in Z and B, pages 309–
329, London, UK. Springer-Verlag.
J.M. Punshon, J.P Tremblay, P.G. Sorenson, and P.S.
Findeisen. 1997. From formal specifications to nat-
ural language: A case study. In 12th IEEE Interna-
tional Conference Automated Software Engineering,
pages 309–310.
E. Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press, Cambridge, UK.
A. Salek, P.G. Sorenson, J.P. Tremblay, and J.M. Pun-
shon. 1994. The REVIEW system: From formal
specifications to natural language. In Proceedings of
the First International Conference on Requirements
Engineering, pages 220–229.
P.G. Sorenson, J.P. Tremblay, and A.J. McAllister.
1988. The Metaview system for many specification
environments. IEEE Software, 5(2):30–38.
J.M. Spivey. 1989. The Z Notation: A Reference Man-
ual. Prentice-Hall, Inc.
P. Stocks and D. Carrington. 1996. A Framework for
Specification-Based Testing. IEEE Transactions on
Software Engineering, 22(11):777–793.
M. Utting and B. Legeard. 2006. Practical Model-
Based Testing: A Tools Approach. Morgan Kauf-
mann Publishers Inc., San Francisco, CA.
G. Wilcock. 2005. An Overview of Shallow XML-
based Natural Language Generation. In Proceed-
ings of the 2nd Baltic Conference on Human Lan-
guage Technolgies, pages 67–78, Tallinn, Estonia.
Appendix A. NLTCT for the Example
NLTCT for DumpMemoryAbsAdd (some parts
were replaced by [...] due to space restrictions):
operation = DumpMemoryAbsAdd
\begin{tcase}
\centerline{{\bf Test case: \ltcaseid}}
</reference>
<bodyText confidence="0.842932416666667">
The service (6,5) will be tested in the
situation that verifies that:
\begin{itemize}
\item the state is such that:
\begin{itemize}
\item the on-board system is &amp;trule PTCr&amp;.
\item the service type of the telecommand
is &amp;trule SRVr&amp;.
[...]
\item the set of starting addresses of the
chunks of memory that have been
requested by the ground is &amp;trule
</bodyText>
<figure confidence="0.942932">
ADSr&amp;.
\end{itemize}
[...]
\end{itemize}
\end{tcase}
\begin{trule}{PTCr}
case processingTC
$yes :currently processing a telecommand and
has not answered it yet
$no :not currently processing a telecommand
endcase
\end{trule}
\begin{trule}{SRVr}
case srv
$* :*
endcase
</figure>
<reference confidence="0.820683222222222">
\end{trule}
\begin{trule}{ADSr}
case adds
$\emptyset :empty
$\langle 0 \rangle :the unitary sequence
composed of 0
endcase
\end{trule}
[...]
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.232142">
<title confidence="0.999782">Generating Natural Language Descriptions of Z Test Cases</title>
<author confidence="0.661555">Maximiliano</author>
<affiliation confidence="0.818347">Flowgate Consulting and</affiliation>
<address confidence="0.48618">Rosario,</address>
<email confidence="0.977371">mcristia@flowgate.net</email>
<author confidence="0.948428">Brian</author>
<affiliation confidence="0.933334">Centre for Research in</affiliation>
<title confidence="0.705885">The Open</title>
<author confidence="0.984609">Milton Keynes</author>
<email confidence="0.998954">b.pluss@open.ac.uk</email>
<abstract confidence="0.99971715">Critical software most often requires an independent validation and verification (IVV). IVV is usually performed by domain experts, who are not familiar with specific, many times formal, development technologies. In addition, model-based testing (MBT) is a promising testing technique for the verification of critical software. Test cases generated by MBT tools are logical descriptions. The problem is, then, to provide natural language (NL) descriptions of these test cases, making them accessible to domain experts. In this paper, we present ongoing research aimed at finding a suitable method for generating NL descriptions from test cases in a formal specification language. A first prototype has been developed and applied to a real-world project in the aerospace sector.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>In AI*IA 1992: Proceedings of the 6th Congress of the Italian Association for Artificial Intelligence,</booktitle>
<pages>185--194</pages>
<location>Bologna, Italy.</location>
<marker></marker>
<rawString>In AI*IA 1992: Proceedings of the 6th Congress of the Italian Association for Artificial Intelligence, pages 185–194, Bologna, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bertot</author>
<author>P Cast´eran</author>
</authors>
<title>Interactive Theorem Proving and Program Development. Coq’Art: The Calculus of Inductive Constructions. Texts in Theoretical Computer Science.</title>
<date>2004</date>
<publisher>Springer-Verlag.</publisher>
<marker>Bertot, Cast´eran, 2004</marker>
<rawString>Y. Bertot and P. Cast´eran. 2004. Interactive Theorem Proving and Program Development. Coq’Art: The Calculus of Inductive Constructions. Texts in Theoretical Computer Science. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R G G Cattell</author>
<author>D K Barry</author>
<author>editors</author>
</authors>
<title>The object database standard:</title>
<date>1997</date>
<journal>ODMG</journal>
<volume>2</volume>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA.</location>
<marker>Cattell, Barry, editors, 1997</marker>
<rawString>R.G.G. Cattell and D.K. Barry, editors. 1997. The object database standard: ODMG 2.0. Morgan Kaufmann Publishers Inc., San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Coscoy</author>
</authors>
<title>A natural language explanation for formal proofs.</title>
<date>1997</date>
<booktitle>In LACL ’96: Selected papers from the First International Conference on Logical Aspects of Computational Linguistics,</booktitle>
<pages>149--167</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="14275" citStr="Coscoy (1997)" startWordPosition="2383" endWordPosition="2384">dentifiers5. The next stages of our project will point in this direction, using techniques from NLG for automating the definition of the templates presented in the previous section. There have been efforts for producing natural language versions of formal specifications in the past. Punshon et al. (1997) use a case study to present the REVIEW system (Salek et al., 1994)6. REVIEW automatically paraphrases specifications developed with Metaview (Sorenson et al., 1988), an academic research metasystem that facilitates the construction of CASE environments to support software specification tasks. Coscoy (1997) describes a mechanism based on program extraction, for generating explanations of formal proofs in the Calculus of Inductive Constructions, implemented in the Coq Proof Assistant (Bertot and Cast´eran, 2004). Lavoie et al. (1997) present MODEX, a tool that generates customizable descriptions of the relations between classes in object-oriented models specified in the ODL standard (Cattell and Barry, 1997). Bertani et al. (1999) describe a controlled natural language approach to translating formal specifications written in an extension of TRIO (Ghezzi et al., 1990) by transforming syntactic tre</context>
</contexts>
<marker>Coscoy, 1997</marker>
<rawString>Y. Coscoy. 1997. A natural language explanation for formal proofs. In LACL ’96: Selected papers from the First International Conference on Logical Aspects of Computational Linguistics, pages 149–167, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cristi´a</author>
<author>P Rodr´ıguez Monetti</author>
</authors>
<title>Implementing and applying the Stocks-Carrington framework for model-based testing.</title>
<date>2009</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>5885</volume>
<pages>167--185</pages>
<editor>In Karin Breitman and Ana Cavalcanti, editors, ICFEM,</editor>
<publisher>Springer-Verlag.</publisher>
<marker>Cristi´a, Monetti, 2009</marker>
<rawString>M. Cristi´a and P. Rodr´ıguez Monetti. 2009. Implementing and applying the Stocks-Carrington framework for model-based testing. In Karin Breitman and Ana Cavalcanti, editors, ICFEM, volume 5885 of Lecture Notes in Computer Science, pages 167– 185. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ECSS</author>
</authors>
<title>Space Engineering – Ground Systems and Operations: Telemetry and Telecommand Packet Utilization.</title>
<date>2003</date>
<tech>Technical Report ECSS-E-70-41A,</tech>
<institution>European Space Agency.</institution>
<contexts>
<context position="4447" citStr="ECSS, 2003" startWordPosition="711" endWordPosition="712">techniques seem to be an appropriate approach to solving this problem. In the rest of the paper, we give an example of a test case from the project mentioned above (Section 2), describe a template-based method for generating NL descriptions (Section 3), and propose further work towards a more general NLG solution (Section 4). 2 An Example from the Aerospace Industry The problem of generating NL descriptions of specifications in Z arises in the following scenario: a company developing the software for a satellite needs to verify that the implementation conforms to a certain aerospace standard (ECSS, 2003) describing the basic functionality of any satellite software. We therefore started by modelling in Z the services described by the standard and used the Fastest tool to generate test cases. The model is a “standard” Z specification: it has a schema box that defines the state space of the system and operations defining the transition relation between states1. Each operation formalizes one of the services described by the standard (e.g., memory dump, telecommand verification, enabling or disabling on-board monitoring, etc.). Figure 1 shows one of the test cases generated for the operation DumpM</context>
</contexts>
<marker>ECSS, 2003</marker>
<rawString>ECSS. 2003. Space Engineering – Ground Systems and Operations: Telemetry and Telecommand Packet Utilization. Technical Report ECSS-E-70-41A, European Space Agency.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ghezzi</author>
<author>D Mandrioli</author>
<author>A Morzenti</author>
</authors>
<title>TRIO: A logic language for executable specifications of real-time systems.</title>
<date>1990</date>
<journal>Journal of Systems and Software,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="14845" citStr="Ghezzi et al., 1990" startWordPosition="2467" endWordPosition="2470">upport software specification tasks. Coscoy (1997) describes a mechanism based on program extraction, for generating explanations of formal proofs in the Calculus of Inductive Constructions, implemented in the Coq Proof Assistant (Bertot and Cast´eran, 2004). Lavoie et al. (1997) present MODEX, a tool that generates customizable descriptions of the relations between classes in object-oriented models specified in the ODL standard (Cattell and Barry, 1997). Bertani et al. (1999) describe a controlled natural language approach to translating formal specifications written in an extension of TRIO (Ghezzi et al., 1990) by transforming syntactic trees in TRIO into syntactic trees of the controlled language. The solutions presented in the related work above are highly dependant on particular aspects 4Somewhat along the lines of what Wilcock (2005) describes for XML-based NLG. 5This approach is similar to the method proposed by Kittredge and Lavoie (1998) for generating weather forecasts. 6Salek et al. (1994) also give a comprehensive survey of related work for generating NL explanations for particular specification languages (most of which are now obsolete). of the source language and do not apply directly to</context>
</contexts>
<marker>Ghezzi, Mandrioli, Morzenti, 1990</marker>
<rawString>C. Ghezzi, D. Mandrioli, and A. Morzenti. 1990. TRIO: A logic language for executable specifications of real-time systems. Journal of Systems and Software, 12(2):107–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Grieskamp</author>
<author>Y Gurevich</author>
<author>W Schulte</author>
<author>M Veanes</author>
</authors>
<title>Generating finite state machines from abstract state machines.</title>
<date>2002</date>
<booktitle>In ISSTA ’02: Proceedings of the 2002 ACM SIGSOFT International Symposium on Software Testing and Analysis,</booktitle>
<pages>112--122</pages>
<location>Rome, Italy.</location>
<contexts>
<context position="1533" citStr="Grieskamp et al., 2002" startWordPosition="227" endWordPosition="230">a formal specification language. A first prototype has been developed and applied to a real-world project in the aerospace sector. 1 Introduction Model-based testing (MBT) is an active research area and a promising theory of software and hardware testing (Utting and Legeard, 2006; Hierons et al., 2009). MBT approaches start with a formal model or specification of the software, from which test cases are generated. These techniques have been developed and applied to models written in different formal notations, such as Z (Stocks and Carrington, 1996), finite state machines and their extensions (Grieskamp et al., 2002), B (Legeard et al., 2002), algebraic specifications (Bernot et al., 1991), and so on. The fundamental hypothesis behind MBT is that, as a program is correct if it verifies its specification, then the specification is an excellent source of test cases. Once test cases are derived from the model, they are refined to the level of the implementation language and executed. The resulting output is then abstracted to the level of the specification language, and the model is used again to verify if the test case has detected an error. The Test Template Framework (TTF) described by Stocks and Carringt</context>
</contexts>
<marker>Grieskamp, Gurevich, Schulte, Veanes, 2002</marker>
<rawString>W. Grieskamp, Y. Gurevich, W. Schulte, and M. Veanes. 2002. Generating finite state machines from abstract state machines. In ISSTA ’02: Proceedings of the 2002 ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 112–122, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Hierons</author>
<author>K Bogdanov</author>
<author>J P Bowen</author>
<author>R Cleaveland</author>
<author>J Derrick</author>
</authors>
<title>Using formal specifications to support testing.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1213" citStr="Hierons et al., 2009" startWordPosition="178" endWordPosition="181">est cases generated by MBT tools are logical descriptions. The problem is, then, to provide natural language (NL) descriptions of these test cases, making them accessible to domain experts. In this paper, we present ongoing research aimed at finding a suitable method for generating NL descriptions from test cases in a formal specification language. A first prototype has been developed and applied to a real-world project in the aerospace sector. 1 Introduction Model-based testing (MBT) is an active research area and a promising theory of software and hardware testing (Utting and Legeard, 2006; Hierons et al., 2009). MBT approaches start with a formal model or specification of the software, from which test cases are generated. These techniques have been developed and applied to models written in different formal notations, such as Z (Stocks and Carrington, 1996), finite state machines and their extensions (Grieskamp et al., 2002), B (Legeard et al., 2002), algebraic specifications (Bernot et al., 1991), and so on. The fundamental hypothesis behind MBT is that, as a program is correct if it verifies its specification, then the specification is an excellent source of test cases. Once test cases are derived</context>
</contexts>
<marker>Hierons, Bogdanov, Bowen, Cleaveland, Derrick, 2009</marker>
<rawString>R.M. Hierons, K. Bogdanov, J.P. Bowen, R. Cleaveland, J. Derrick, et al. 2009. Using formal specifications to support testing. ACM Computing Surveys (CSUR), 41(2):9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jackson</author>
</authors>
<title>Software requirements &amp; specifications: a lexicon ofpractice, principles, and prejudices.</title>
<date>1995</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="12301" citStr="Jackson, 1995" startWordPosition="2058" endWordPosition="2060">e systems. At the same time, Z specifications contain all the information necessary to produce the templates for the operations in the system, regardless of its domain of application. This information is structured according to the syntax of the formal language. Additionally, when formally specifying a system, it is common practice to include associations between the identifiers in the specification (new types, operations, state schemata, variables, constants, etc.) and the elements they refer to in the application domain (i.e., aerospace software). These associations are called designations (Jackson, 1995), some of which, relevant to the test case in Figure 1, are shown in Figure 4. These considerations lead us to believe in the srv Pz� Service type of the telecommand DMAA Pz� Dump memory using absolute addresses processingTC Pz� The on-board system is currently processing a telecommand and has not answered it yet m? R� Memory ID to be dumped sa? Pz� Start addresses of the memory regions to be dumped len? R� The number of memory cells to be dumped for each start address Figure 4: Designations for the test in Figure 1 possibility of generating NL descriptions of Z test cases automatically by usi</context>
</contexts>
<marker>Jackson, 1995</marker>
<rawString>M. Jackson. 1995. Software requirements &amp; specifications: a lexicon ofpractice, principles, and prejudices. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kittredge</author>
<author>B Lavoie</author>
</authors>
<title>Meteocogent: A knowledge-based tool for generating weather forecast texts.</title>
<date>1998</date>
<booktitle>In Proceedings of American Meteorological Society AI Conference (AMS-98),</booktitle>
<location>Phoenix, AZ.</location>
<contexts>
<context position="15185" citStr="Kittredge and Lavoie (1998)" startWordPosition="2521" endWordPosition="2525">scriptions of the relations between classes in object-oriented models specified in the ODL standard (Cattell and Barry, 1997). Bertani et al. (1999) describe a controlled natural language approach to translating formal specifications written in an extension of TRIO (Ghezzi et al., 1990) by transforming syntactic trees in TRIO into syntactic trees of the controlled language. The solutions presented in the related work above are highly dependant on particular aspects 4Somewhat along the lines of what Wilcock (2005) describes for XML-based NLG. 5This approach is similar to the method proposed by Kittredge and Lavoie (1998) for generating weather forecasts. 6Salek et al. (1994) also give a comprehensive survey of related work for generating NL explanations for particular specification languages (most of which are now obsolete). of the source language and do not apply directly to specifications written in Z. To our knowledge, no work has been done towards producing NL descriptions of Z specifications. The same holds for test cases generated using the MBT approach. 5 Conclusion In this paper we presented a concrete NLG problem in the area of software development involving formal methods. We focused the description</context>
</contexts>
<marker>Kittredge, Lavoie, 1998</marker>
<rawString>R. Kittredge and B. Lavoie. 1998. Meteocogent: A knowledge-based tool for generating weather forecast texts. In Proceedings of American Meteorological Society AI Conference (AMS-98), Phoenix, AZ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lavoie</author>
<author>O Rambow</author>
<author>E Reiter</author>
</authors>
<title>Customizable descriptions of object-oriented models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Applied Natural Language Processing (ANLP’97,</booktitle>
<pages>253--256</pages>
<location>Washington, DC.</location>
<contexts>
<context position="14505" citStr="Lavoie et al. (1997)" startWordPosition="2416" endWordPosition="2419">ural language versions of formal specifications in the past. Punshon et al. (1997) use a case study to present the REVIEW system (Salek et al., 1994)6. REVIEW automatically paraphrases specifications developed with Metaview (Sorenson et al., 1988), an academic research metasystem that facilitates the construction of CASE environments to support software specification tasks. Coscoy (1997) describes a mechanism based on program extraction, for generating explanations of formal proofs in the Calculus of Inductive Constructions, implemented in the Coq Proof Assistant (Bertot and Cast´eran, 2004). Lavoie et al. (1997) present MODEX, a tool that generates customizable descriptions of the relations between classes in object-oriented models specified in the ODL standard (Cattell and Barry, 1997). Bertani et al. (1999) describe a controlled natural language approach to translating formal specifications written in an extension of TRIO (Ghezzi et al., 1990) by transforming syntactic trees in TRIO into syntactic trees of the controlled language. The solutions presented in the related work above are highly dependant on particular aspects 4Somewhat along the lines of what Wilcock (2005) describes for XML-based NLG.</context>
</contexts>
<marker>Lavoie, Rambow, Reiter, 1997</marker>
<rawString>B. Lavoie, O. Rambow, and E. Reiter. 1997. Customizable descriptions of object-oriented models. In Proceedings of the Conference on Applied Natural Language Processing (ANLP’97, pages 253–256, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Legeard</author>
<author>F Peureux</author>
<author>M Utting</author>
</authors>
<title>A Comparison of the BTT and TTF Test-Generation Methods.</title>
<date>2002</date>
<booktitle>In ZB ’02: Proceedings of the 2nd International Conference of B and Z Users on Formal Specification and Development in Z and B,</booktitle>
<pages>309--329</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="1559" citStr="Legeard et al., 2002" startWordPosition="232" endWordPosition="235">age. A first prototype has been developed and applied to a real-world project in the aerospace sector. 1 Introduction Model-based testing (MBT) is an active research area and a promising theory of software and hardware testing (Utting and Legeard, 2006; Hierons et al., 2009). MBT approaches start with a formal model or specification of the software, from which test cases are generated. These techniques have been developed and applied to models written in different formal notations, such as Z (Stocks and Carrington, 1996), finite state machines and their extensions (Grieskamp et al., 2002), B (Legeard et al., 2002), algebraic specifications (Bernot et al., 1991), and so on. The fundamental hypothesis behind MBT is that, as a program is correct if it verifies its specification, then the specification is an excellent source of test cases. Once test cases are derived from the model, they are refined to the level of the implementation language and executed. The resulting output is then abstracted to the level of the specification language, and the model is used again to verify if the test case has detected an error. The Test Template Framework (TTF) described by Stocks and Carrington (1996) is a particular </context>
</contexts>
<marker>Legeard, Peureux, Utting, 2002</marker>
<rawString>B. Legeard, F. Peureux, and M. Utting. 2002. A Comparison of the BTT and TTF Test-Generation Methods. In ZB ’02: Proceedings of the 2nd International Conference of B and Z Users on Formal Specification and Development in Z and B, pages 309– 329, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Punshon</author>
<author>J P Tremblay</author>
<author>P G Sorenson</author>
<author>P S Findeisen</author>
</authors>
<title>From formal specifications to natural language: A case study.</title>
<date>1997</date>
<booktitle>In 12th IEEE International Conference Automated Software Engineering,</booktitle>
<pages>309--310</pages>
<contexts>
<context position="13967" citStr="Punshon et al. (1997)" startWordPosition="2336" endWordPosition="2339">he approach would still be template-based, but in terms of the NLG architecture of Reiter and Dale (2000), templates would be defined at the level of the document structure4, with minimal microplanning and surface strings generated according to the part of the test case being processed and the designations of the identifiers5. The next stages of our project will point in this direction, using techniques from NLG for automating the definition of the templates presented in the previous section. There have been efforts for producing natural language versions of formal specifications in the past. Punshon et al. (1997) use a case study to present the REVIEW system (Salek et al., 1994)6. REVIEW automatically paraphrases specifications developed with Metaview (Sorenson et al., 1988), an academic research metasystem that facilitates the construction of CASE environments to support software specification tasks. Coscoy (1997) describes a mechanism based on program extraction, for generating explanations of formal proofs in the Calculus of Inductive Constructions, implemented in the Coq Proof Assistant (Bertot and Cast´eran, 2004). Lavoie et al. (1997) present MODEX, a tool that generates customizable description</context>
</contexts>
<marker>Punshon, Tremblay, Sorenson, Findeisen, 1997</marker>
<rawString>J.M. Punshon, J.P Tremblay, P.G. Sorenson, and P.S. Findeisen. 1997. From formal specifications to natural language: A case study. In 12th IEEE International Conference Automated Software Engineering, pages 309–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="13451" citStr="Reiter and Dale (2000)" startWordPosition="2253" endWordPosition="2256">ility of generating NL descriptions of Z test cases automatically by using their definitions, the system specification and the designations of identifiers. Such a solution would be independent of the application domain and, more importantly, of the number of operations in the model. The linguistic properties of the target document are relevant in devising an adequate treatment for the input, but the overall structure of the output remains rigid and its content is determined by the definition of each test case. The approach would still be template-based, but in terms of the NLG architecture of Reiter and Dale (2000), templates would be defined at the level of the document structure4, with minimal microplanning and surface strings generated according to the part of the test case being processed and the designations of the identifiers5. The next stages of our project will point in this direction, using techniques from NLG for automating the definition of the templates presented in the previous section. There have been efforts for producing natural language versions of formal specifications in the past. Punshon et al. (1997) use a case study to present the REVIEW system (Salek et al., 1994)6. REVIEW automat</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>E. Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Salek</author>
<author>P G Sorenson</author>
<author>J P Tremblay</author>
<author>J M Punshon</author>
</authors>
<title>The REVIEW system: From formal specifications to natural language.</title>
<date>1994</date>
<booktitle>In Proceedings of the First International Conference on Requirements Engineering,</booktitle>
<pages>220--229</pages>
<contexts>
<context position="14034" citStr="Salek et al., 1994" startWordPosition="2349" endWordPosition="2352">chitecture of Reiter and Dale (2000), templates would be defined at the level of the document structure4, with minimal microplanning and surface strings generated according to the part of the test case being processed and the designations of the identifiers5. The next stages of our project will point in this direction, using techniques from NLG for automating the definition of the templates presented in the previous section. There have been efforts for producing natural language versions of formal specifications in the past. Punshon et al. (1997) use a case study to present the REVIEW system (Salek et al., 1994)6. REVIEW automatically paraphrases specifications developed with Metaview (Sorenson et al., 1988), an academic research metasystem that facilitates the construction of CASE environments to support software specification tasks. Coscoy (1997) describes a mechanism based on program extraction, for generating explanations of formal proofs in the Calculus of Inductive Constructions, implemented in the Coq Proof Assistant (Bertot and Cast´eran, 2004). Lavoie et al. (1997) present MODEX, a tool that generates customizable descriptions of the relations between classes in object-oriented models specif</context>
</contexts>
<marker>Salek, Sorenson, Tremblay, Punshon, 1994</marker>
<rawString>A. Salek, P.G. Sorenson, J.P. Tremblay, and J.M. Punshon. 1994. The REVIEW system: From formal specifications to natural language. In Proceedings of the First International Conference on Requirements Engineering, pages 220–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P G Sorenson</author>
<author>J P Tremblay</author>
<author>A J McAllister</author>
</authors>
<title>The Metaview system for many specification environments.</title>
<date>1988</date>
<journal>IEEE Software,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="14132" citStr="Sorenson et al., 1988" startWordPosition="2360" endWordPosition="2364">tructure4, with minimal microplanning and surface strings generated according to the part of the test case being processed and the designations of the identifiers5. The next stages of our project will point in this direction, using techniques from NLG for automating the definition of the templates presented in the previous section. There have been efforts for producing natural language versions of formal specifications in the past. Punshon et al. (1997) use a case study to present the REVIEW system (Salek et al., 1994)6. REVIEW automatically paraphrases specifications developed with Metaview (Sorenson et al., 1988), an academic research metasystem that facilitates the construction of CASE environments to support software specification tasks. Coscoy (1997) describes a mechanism based on program extraction, for generating explanations of formal proofs in the Calculus of Inductive Constructions, implemented in the Coq Proof Assistant (Bertot and Cast´eran, 2004). Lavoie et al. (1997) present MODEX, a tool that generates customizable descriptions of the relations between classes in object-oriented models specified in the ODL standard (Cattell and Barry, 1997). Bertani et al. (1999) describe a controlled nat</context>
</contexts>
<marker>Sorenson, Tremblay, McAllister, 1988</marker>
<rawString>P.G. Sorenson, J.P. Tremblay, and A.J. McAllister. 1988. The Metaview system for many specification environments. IEEE Software, 5(2):30–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Spivey</author>
</authors>
<title>The Z Notation: A Reference Manual.</title>
<date>1989</date>
<publisher>Prentice-Hall, Inc.</publisher>
<contexts>
<context position="2254" citStr="Spivey, 1989" startWordPosition="351" endWordPosition="352">hesis behind MBT is that, as a program is correct if it verifies its specification, then the specification is an excellent source of test cases. Once test cases are derived from the model, they are refined to the level of the implementation language and executed. The resulting output is then abstracted to the level of the specification language, and the model is used again to verify if the test case has detected an error. The Test Template Framework (TTF) described by Stocks and Carrington (1996) is a particular MBT theory specially well suited for unit testing. The TTF uses Z specifications (Spivey, 1989) as the entry models and prescribes how to generate test cases for each operation included in the model. Fastest (Cristi´a and Rodr´ıguez Monetti, 2009) implements the TTF allowing users to automatically produce test cases for a given Z specification. Recently, we used Fastest to test an on-board satellite software for a major aerospace company in South America. Since Fastest uses models written in the Z specification language, test cases generated by this tool are paragraphs of formal text (see Section 2). This description is suitable for the automatic tasks involved in testing (e.g., automat</context>
</contexts>
<marker>Spivey, 1989</marker>
<rawString>J.M. Spivey. 1989. The Z Notation: A Reference Manual. Prentice-Hall, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Stocks</author>
<author>D Carrington</author>
</authors>
<title>A Framework for Specification-Based Testing.</title>
<date>1996</date>
<journal>IEEE Transactions on Software Engineering,</journal>
<volume>22</volume>
<issue>11</issue>
<contexts>
<context position="1464" citStr="Stocks and Carrington, 1996" startWordPosition="217" endWordPosition="220">nding a suitable method for generating NL descriptions from test cases in a formal specification language. A first prototype has been developed and applied to a real-world project in the aerospace sector. 1 Introduction Model-based testing (MBT) is an active research area and a promising theory of software and hardware testing (Utting and Legeard, 2006; Hierons et al., 2009). MBT approaches start with a formal model or specification of the software, from which test cases are generated. These techniques have been developed and applied to models written in different formal notations, such as Z (Stocks and Carrington, 1996), finite state machines and their extensions (Grieskamp et al., 2002), B (Legeard et al., 2002), algebraic specifications (Bernot et al., 1991), and so on. The fundamental hypothesis behind MBT is that, as a program is correct if it verifies its specification, then the specification is an excellent source of test cases. Once test cases are derived from the model, they are refined to the level of the implementation language and executed. The resulting output is then abstracted to the level of the specification language, and the model is used again to verify if the test case has detected an erro</context>
</contexts>
<marker>Stocks, Carrington, 1996</marker>
<rawString>P. Stocks and D. Carrington. 1996. A Framework for Specification-Based Testing. IEEE Transactions on Software Engineering, 22(11):777–793.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Utting</author>
<author>B Legeard</author>
</authors>
<title>Practical ModelBased Testing: A Tools Approach.</title>
<date>2006</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="1190" citStr="Utting and Legeard, 2006" startWordPosition="174" endWordPosition="177">on of critical software. Test cases generated by MBT tools are logical descriptions. The problem is, then, to provide natural language (NL) descriptions of these test cases, making them accessible to domain experts. In this paper, we present ongoing research aimed at finding a suitable method for generating NL descriptions from test cases in a formal specification language. A first prototype has been developed and applied to a real-world project in the aerospace sector. 1 Introduction Model-based testing (MBT) is an active research area and a promising theory of software and hardware testing (Utting and Legeard, 2006; Hierons et al., 2009). MBT approaches start with a formal model or specification of the software, from which test cases are generated. These techniques have been developed and applied to models written in different formal notations, such as Z (Stocks and Carrington, 1996), finite state machines and their extensions (Grieskamp et al., 2002), B (Legeard et al., 2002), algebraic specifications (Bernot et al., 1991), and so on. The fundamental hypothesis behind MBT is that, as a program is correct if it verifies its specification, then the specification is an excellent source of test cases. Once</context>
</contexts>
<marker>Utting, Legeard, 2006</marker>
<rawString>M. Utting and B. Legeard. 2006. Practical ModelBased Testing: A Tools Approach. Morgan Kaufmann Publishers Inc., San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Wilcock</author>
</authors>
<title>An Overview of Shallow XMLbased Natural Language Generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Baltic Conference on Human Language Technolgies,</booktitle>
<pages>67--78</pages>
<location>Tallinn, Estonia.</location>
<contexts>
<context position="15076" citStr="Wilcock (2005)" startWordPosition="2505" endWordPosition="2506"> and Cast´eran, 2004). Lavoie et al. (1997) present MODEX, a tool that generates customizable descriptions of the relations between classes in object-oriented models specified in the ODL standard (Cattell and Barry, 1997). Bertani et al. (1999) describe a controlled natural language approach to translating formal specifications written in an extension of TRIO (Ghezzi et al., 1990) by transforming syntactic trees in TRIO into syntactic trees of the controlled language. The solutions presented in the related work above are highly dependant on particular aspects 4Somewhat along the lines of what Wilcock (2005) describes for XML-based NLG. 5This approach is similar to the method proposed by Kittredge and Lavoie (1998) for generating weather forecasts. 6Salek et al. (1994) also give a comprehensive survey of related work for generating NL explanations for particular specification languages (most of which are now obsolete). of the source language and do not apply directly to specifications written in Z. To our knowledge, no work has been done towards producing NL descriptions of Z specifications. The same holds for test cases generated using the MBT approach. 5 Conclusion In this paper we presented a </context>
</contexts>
<marker>Wilcock, 2005</marker>
<rawString>G. Wilcock. 2005. An Overview of Shallow XMLbased Natural Language Generation. In Proceedings of the 2nd Baltic Conference on Human Language Technolgies, pages 67–78, Tallinn, Estonia.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Appendix</author>
</authors>
<title>NLTCT for the Example NLTCT for DumpMemoryAbsAdd (some parts were replaced by [...] due to space restrictions): operation =</title>
<note>DumpMemoryAbsAdd \begin{tcase}</note>
<marker>Appendix, </marker>
<rawString>Appendix A. NLTCT for the Example NLTCT for DumpMemoryAbsAdd (some parts were replaced by [...] due to space restrictions): operation = DumpMemoryAbsAdd \begin{tcase}</rawString>
</citation>
<citation valid="false">
<authors>
<author>\centerline\bf Test</author>
</authors>
<title>case: \ltcaseid}} \end{trule} \begin{trule}{ADSr} case adds $\emptyset :empty $\langle 0 \rangle :the unitary sequence composed of 0</title>
<marker>Test, </marker>
<rawString>\centerline{{\bf Test case: \ltcaseid}} \end{trule} \begin{trule}{ADSr} case adds $\emptyset :empty $\langle 0 \rangle :the unitary sequence composed of 0</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>