<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.912278">
The Third PASCAL Recognizing Textual Entailment Challenge
</title>
<author confidence="0.725487">
Danilo Giampiccolo
</author>
<affiliation confidence="0.434854">
CELCT
</affiliation>
<address confidence="0.6404625">
Via alla Cascata 56/c
38100 POVO TN
</address>
<email confidence="0.996191">
giampiccolo@celct.it
</email>
<author confidence="0.995678">
Ido Dagan
</author>
<affiliation confidence="0.995922">
Computer Science Department
Bar-Ilan University
</affiliation>
<address confidence="0.752933">
Ramat Gan 52900, Israel
</address>
<email confidence="0.996601">
dagan@macs.biu.ac.il
</email>
<sectionHeader confidence="0.993033" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970631578947">
This paper presents the Third PASCAL
Recognising Textual Entailment Chal-
lenge (RTE-3), providing an overview of
the dataset creating methodology and the
submitted systems. In creating this
year’s dataset, a number of longer texts
were introduced to make the challenge
more oriented to realistic scenarios. Ad-
ditionally, a pool of resources was of-
fered so that the participants could share
common tools. A pilot task was also set
up, aimed at differentiating unknown en-
tailments from identified contradictions
and providing justifications for overall
system decisions. 26 participants submit-
ted 44 runs, using different approaches
and generally presenting new entailment
models and achieving higher scores than
in the previous challenges.
</bodyText>
<subsectionHeader confidence="0.993661">
1.1 The RTE challenges
</subsectionHeader>
<bodyText confidence="0.999759">
The goal of the RTE challenges has been to cre-
ate a benchmark task dedicated to textual en-
tailment – recognizing that the meaning of one
</bodyText>
<note confidence="0.878297">
Bernardo Magnini
FBK-ITC
Via Sommarive 18,
</note>
<address confidence="0.422445">
38100 Povo TN
</address>
<email confidence="0.984725">
magnini@itc.it
</email>
<author confidence="0.955875">
Bill Dolan
</author>
<affiliation confidence="0.791256">
Microsoft Research
</affiliation>
<address confidence="0.919084">
Redmond, WA, 98052, USA
</address>
<email confidence="0.990762">
billdol@microsoft.com
</email>
<bodyText confidence="0.999981464285714">
text is entailed, i.e. can be inferred, by another1.
In the recent years, this task has raised great in-
terest since applied semantic inference concerns
many practical Natural Language Processing
(NLP) applications, such as Question Answering
(QA), Information Extraction (IE), Summariza-
tion, Machine Translation and Paraphrasing, and
certain types of queries in Information Retrieval
(IR). More specifically, the RTE challenges
have aimed to focus research and evaluation on
this common underlying semantic inference task
and separate it from other problems that differ-
ent NLP applications need to handle. For exam-
ple, in addition to textual entailment, QA sys-
tems need to handle issues such as answer re-
trieval and question type recognition.
By separating out the general problem of tex-
tual entailment from these task-specific prob-
lems, progress on semantic inference for many
application areas can be promoted. Hopefully,
research on textual entailment will finally lead to
the development of entailment “engines”, which
can be used as a standard module in many appli-
cations (similar to the role of part-of-speech tag-
gers and syntactic parsers in current NLP appli-
cations).
In the following sections, a detailed descrip-
tion of RTE-3 is presented. After a quick review
</bodyText>
<footnote confidence="0.7004785">
1 The task was first defined by Dagan and Glickman
(2004).
</footnote>
<page confidence="0.808816">
1
</page>
<note confidence="0.80076">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 1–9,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9991088">
of the previous challenges (1.2), section 2 de-
scribes the preparation of the dataset. In section
3 the evaluation process and the results are pre-
sented, together with an analysis of the perform-
ance of the participating systems.
</bodyText>
<subsectionHeader confidence="0.969036">
1.2 The First and Second RTE Challenges
</subsectionHeader>
<bodyText confidence="0.99999264">
The first RTE challenge2 aimed to provide the
NLP community with a new benchmark to test
progress in recognizing textual entailment, and
to compare the achievements of different groups.
This goal proved to be of great interest, and the
community&apos;s response encouraged the gradual
expansion of the scope of the original task.
The Second RTE challenge3 built on the suc-
cess of the first, with 23 groups from around the
world (as compared to 17 for the first challenge)
submitting the results of their systems. Repre-
sentatives of participating groups presented their
work at the PASCAL Challenges Workshop in
April 2006 in Venice, Italy. The event was suc-
cessful and the number of participants and their
contributions to the discussion demonstrated that
Textual Entailment is a quickly growing field of
NLP research. In addition, the workshops
spawned an impressive number of publications
in major conferences, with more work in pro-
gress. Another encouraging sign of the growing
interest in the RTE challenge was represented by
the increase in the number of downloads of the
challenge datasets, with about 150 registered
downloads for the RTE-2 development set.
</bodyText>
<subsectionHeader confidence="0.994845">
1.3 The Third Challenge
</subsectionHeader>
<bodyText confidence="0.9999585">
RTE-3 followed the same basic structure of the
previous campaigns, in order to facilitate the
participation of newcomers and to allow &amp;quot;veter-
ans&amp;quot; to assess the improvements of their systems
in a comparable test exercise. Nevertheless,
some innovations were introduced, on the one
hand to make the challenge more stimulating
and, on the other, to encourage collaboration
between system developers. In particular, a lim-
ited number of longer texts, i.e. up to a para-
graph in length, were incorporated in order to
move toward more comprehensive scenarios,
</bodyText>
<footnote confidence="0.9911755">
2 http://www.pascal-network.org/Challenges/RTE/.
3 http://www.pascal-network.org/Challenges/RTE2./
</footnote>
<bodyText confidence="0.999874244444445">
which incorporate the need for discourse analy-
sis. However, the majority of examples re-
mained similar to those in the previous chal-
lenges, providing pairs with relatively short
texts.
Another innovation was represented by a re-
source pool4, where contributors had the possi-
bility to share the resources they used. In fact,
one of the key conclusions at the second RTE
Challenge Workshop was that entailment model-
ing requires vast knowledge resources that cor-
respond to different types of entailment reason-
ing. Moreover, entailment systems also utilize
general NLP tools such as POS taggers, parsers
and named-entity recognizers, sometimes posing
specialized requirements to such tools. In re-
sponse to these demands, the RTE Resource
Pool was built, which may serve as a portal and
forum for publicizing and tracking resources,
and reporting on their use.
In addition, an optional pilot task, called &amp;quot;Ex-
tending the Evaluation of Inferences from Texts&amp;quot;
was set up by the US National Institute of Stan-
dards and Technology (NIST), in order to ex-
plore two other sub-tasks closely related to tex-
tual entailment: differentiating unknown entail-
ments from identified contradictions and provid-
ing justifications for system decisions. In the
first sub-task, the idea was to drive systems to
make more precise informational distinctions,
taking a three-way decision between &amp;quot;YES&amp;quot;,
&amp;quot;NO&amp;quot; and &amp;quot;UNKNOWN”, so that a hypothesis
being unknown on the basis of a text would be
distinguished from a hypothesis being shown
false/contradicted by a text. As for the other sub-
task, the goal for providing justifications for de-
cisions was to explore how eventual users of
tools incorporating entailment can be made to
understand how decisions were reached by a
system, as users are unlikely to trust a system
that gives no explanation for its decisions. The
pilot task exploited the existing RTE-3 Chal-
lenge infrastructure and evaluation process by
using the same test set, while utilizing human
assessments for the new sub-tasks.
</bodyText>
<footnote confidence="0.9876805">
4 http://aclweb.org/aclwiki/index.php?title=Textual_Entail
ment_Resource_Pool.
</footnote>
<page confidence="0.990006">
2
</page>
<table confidence="0.742666909090909">
TASK TEXT HYPOTHESIS ENTAILMENT
IE At the same time the Italian digital rights group, Elec- Italy&apos;s govern- NO
tronic Frontiers Italy, has asked the nation&apos;s government ment investigates
to investigate Sony over its use of anti-piracy software. Sony.
IE Parviz Davudi was representing Iran at a meeting of the China is a mem- YES
Shanghai Co-operation Organisation (SCO), the fledg- ber of SCO.
ling association that binds Russia, China and four for-
mer Soviet republics of central Asia together to fight
terrorism
IR Between March and June, scientific observers say, up to Hunting endan- YES
300,000 seals are killed. In Canada, seal-hunting means gers seal species.
</table>
<bodyText confidence="0.792332277777778">
jobs, but opponents say it is vicious and endangers the
species, also threatened by global warming
IR The Italian parliament may approve a draft law allow- Italian royal fam- NO
ing descendants of the exiled royal family to return ily returns home.
home. The family was banished after the Second World
War because of the King&apos;s collusion with the fascist
regime, but moves were introduced this year to allow
their return.
QA Aeschylus is often called the father of Greek tragedy; &amp;quot;The Persians&amp;quot; YES
he wrote the earliest complete plays which survive from was written by
ancient Greece. He is known to have written more than Aeschylus.
90 plays, though only seven survive. The most famous
of these are the trilogy known as Orestia. Also well-
known are The Persians and Prometheus Bound.
SUM A Pentagon committee and the congressionally char- Bush will meet NO
tered Iraq Study Group have been preparing reports for the presidents of
Bush, and Iran has asked the presidents of Iraq and Iraq and Syria in
Syria to meet in Tehran. Tehran.
</bodyText>
<tableCaption confidence="0.990872">
Table 1: Some examples taken from the Development Set.
</tableCaption>
<sectionHeader confidence="0.788937" genericHeader="method">
2 The RTE-3 Dataset
</sectionHeader>
<subsectionHeader confidence="0.916931">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.999982225806452">
The textual entailment recognition task required the
participating systems to decide, given two text
snippets t and h, whether t entails h. Textual en-
tailment is defined as a directional relation between
two text fragments, called text (t, the entailing
text), and hypothesis (h, the entailed text), so that a
human being, with common understanding of lan-
guage and common background knowledge, can
infer that h is most likely true on the basis of the
content of t.
As in the previous challenges, the RTE-3 dataset
consisted of 1600 text-hypothesis pairs, equally
divided into a development set and a test set. While
the length of the hypotheses (h) was the same as in
the past datasets, a certain number of texts (t) were
longer than in previous datasets, up to a paragraph.
The longer texts were marked as L, after being se-
lected automatically when exceeding 270 bytes. In
the test set they were about 17% of the total.
As in RTE-2, four applications – namely IE, IR,
QA and SUM – were considered as settings or con-
texts for the pairs generation (see 2.2 for a detailed
description). 200 pairs were selected for each ap-
plication in each dataset. Although the datasets
were supposed to be perfectly balanced, the num-
ber of negative examples were slightly higher in
both development and test sets (51.50% and
51.25% respectively; this was unintentional). Posi-
tive entailment examples, where t entailed h, were
annotated YES; the negative ones, where entailment
did not hold, NO. Each pair was annotated with its
</bodyText>
<page confidence="0.994961">
3
</page>
<bodyText confidence="0.995514178571428">
related task (IE/IR/QA/SUM) and entailment
judgment (YES/NO, obviously released only in the
development set). Table 1 shows some examples
taken from the development set.
The examples in the dataset were based mostly
on outputs (both correct and incorrect) of Web-
based systems. In order to avoid copyright prob-
lems, input data was limited to either what had al-
ready been publicly released by official competi-
tions or else was drawn from freely available
sources such as WikiNews and Wikipedia.
In choosing the pairs, the following judgment
criteria and guidelines were considered:
§ As entailment is a directional relation, the
hypothesis must be entailed by the given
text, but the text need not be entailed by
the hypothesis.
§ The hypothesis must be fully entailed by
the text. Judgment must be NO if the hy-
pothesis includes parts that cannot be in-
ferred from the text.
§ Cases in which inference is very probable
(but not completely certain) were judged as
YES.
§ Common world knowledge was assumed,
e.g. the capital of a country is situated in
that country, the prime minister of a state is
also a citizen of that state, and so on.
</bodyText>
<subsectionHeader confidence="0.996553">
2.2 Pair Collection
</subsectionHeader>
<bodyText confidence="0.999828714285714">
As in RTE-2, human annotators generated t-h pairs
within 4 application settings.
The IE task was inspired by the Information Ex-
traction (and Relation Extraction) application,
where texts and structured templates were replaced
by t-h pairs. As in the 2006 campaign, the pairs
were generated using four different approaches:
</bodyText>
<listItem confidence="0.940259833333333">
1) Hypotheses were taken from the relations
tested in the ACE-2004 RDR task, while
texts were extracted from the outputs of ac-
tual IE systems, which were provided with
relevant news articles. Correctly extracted
instances were used to generate positive
examples and incorrect instances to gener-
ate negative examples.
2) The same procedure was followed using
output of IE systems on the dataset of the
MUC-4 TST3 task, in which the events are
acts of terrorism.
3) The annotated MUC-4 dataset and the
news articles were also used to manually
generate entailment pairs based on ACE re-
lations.
4) Hypotheses corresponding to relations not
found in the ACE and MUC datasets were
used both to be given to IE systems and to
manually generate t-h pairs from collected
news articles. Examples of these relations,
taken from various semantic fields, were
“X beat Y”, “X invented Y”, “X steal Y”
etc.
</listItem>
<bodyText confidence="0.994498818181818">
The common aim of all these processes was to
simulate the need of IE systems to recognize that
the given text indeed entails the semantic relation
that is expected to hold between the candidate tem-
plate slot fillers.
In the IR (Information Retrieval) application set-
ting, the hypotheses were propositional IR queries,
which specify some statement, e.g. “robots are
used to find avalanche victims”. The hypotheses
were adapted and simplified from standard IR
evaluation datasets (TREC and CLEF). Texts (t)
that did or did not entail the hypotheses were se-
lected from documents retrieved by different search
engines (e.g. Google, Yahoo and MSN) for each
hypothesis. In this application setting it was as-
sumed that relevant documents (from an IR per-
spective) should entail the given propositional hy-
pothesis.
For the QA (Question Answering) task, annotators
used questions taken from the datasets of official
QA competitions, such as TREC QA and
QA@CLEF datasets, and the corresponding an-
swers extracted from the Web by actual QA sys-
tems. Then they transformed the question-answer
pairs into t-h pairs as follows:
§ An answer term of the expected answer
type was picked from the answer passage -
either a correct or an incorrect one.
§ The question was turned into an affirma-
tive sentence plugging in the answer term.
§ t-h pairs were generate, using the affirma-
tive sentences as hypotheses (h’s) and the
original answer passages as texts (t’s).
</bodyText>
<page confidence="0.987946">
4
</page>
<bodyText confidence="0.999984345454545">
For example, given the question “How high is
Mount Everest?” and a text (t) “The above men-
tioned expedition team comprising of 10 members
was permitted to climb 8848m. high Mt. Everest
from Normal Route for the period of 75 days from
15 April, 2007 under the leadership of Mr. Wolf
Herbert of Austria”, the annotator, extracting the
piece of information “8848m.” from the text,
would turn the question into an the affirmative sen-
tence “Mount Everest is 8848m high”, generating a
positive entailment pair. This process simulated the
need of a QA system to verify that the retrieved
passage text actually entailed the provided answer.
In the SUM (Summarization) setting, the
entailment pairs were generated using two proce-
dures.
In the first one, t’s and h’s were sentences taken
from a news document cluster, a collection of news
articles that describe the same news item. Annota-
tors were given the output of multi-document
summarization systems -including the document
clusters and the summary generated for each clus-
ter. Then they picked sentence pairs with high lexi-
cal overlap, preferably where at least one of the
sentences was taken from the summary (this sen-
tence usually played the role of t). For positive ex-
amples, the hypothesis was simplified by removing
sentence parts, until it was fully entailed by t.
Negative examples were simplified in a similar
manner. In alternative, “pyramids” produced for
the experimental evaluation mehod in DUC 2005
(Passonneau et al. 2005) were exploited. In this
new evaluation method, humans select sub-
sentential content units (SCUs) in several manually
produced summaries on a subject, and collocate
them in a “pyramid”, which has at the top the
SCUs with the higher frequency, i.e. those which
are present in most summaries. Each SCU is identi-
fied by a label, a sentence in natural language
which expresses the content. Afterwards, the anno-
tators individuate the SCUs present in summaries
generated automatically (called peers), and link
them to the ones present in the pyramid, in order to
assign each peer a weight. In this way, the SCUs in
the automatic summaries linked to the SCUs in the
higher tiers of the pyramid are assigned a heavier
weight than those at the bottom. For the SUM set-
ting, the RTE-3 annotators selected relevant pas-
sages from the peers and used them as T’s, mean-
while the labels of the corresponding SCUs were
used as H’s. Small adjustments were allowed,
whenever the texts were not grammatically accept-
able. This process simulated the need of a summa-
rization system to identify information redundancy,
which should be avoided in the summary.
</bodyText>
<subsectionHeader confidence="0.995887">
2.3 Final dataset
</subsectionHeader>
<bodyText confidence="0.999826833333333">
Each pair of the dataset was judged by three anno-
tators. As in previous challenges, pairs on which
the annotators disagreed were filtered-out.
On the test set, the average agreement between
each pair of annotators who shared at least 100 ex-
amples was 87.8%, with an average Kappa level of
0.75, regarded as substantial agreement according
to Landis and Koch (1997).
19.2 % of the pairs in the dataset were removed
from the test set due to disagreement. The dis-
agreement was generally due to the fact that the h
was more specific than the t, for example because
it contained more information, or made an absolute
assertion where t proposed only a personal opinion.
In addition, 9.4 % of the remaining pairs were dis-
carded, as they seemed controversial, too difficult,
or too similar when compared to other pairs.
As far as the texts extracted from the web are
concerned, spelling and punctuation errors were
sometimes fixed by the annotators, but no major
change was allowed, so that the language could be
grammatically and stylistically imperfect. The hy-
potheses were finally double-checked by a native
English speaker.
</bodyText>
<sectionHeader confidence="0.970239" genericHeader="method">
3 The RTE-3 Challenge
</sectionHeader>
<subsectionHeader confidence="0.992366">
3.1 Evaluation measures
</subsectionHeader>
<bodyText confidence="0.999912785714286">
The evaluation of all runs submitted in RTE-3 was
automatic. The judgments (classifications) returned
by the system were compared to the Gold Standard
compiled by the human assessors. The main
evaluation measure was accuracy, i.e. the percent-
age of matching judgments.
For systems that provided a confidence-ranked
list of the pairs, in addition to the YES/NO judg-
ment, an Average Precision measure was also
computed. This measure evaluates the ability of
systems to rank all the T-H pairs in the test set ac-
cording to their entailment confidence (in decreas-
ing order from the most certain entailment to the
least certain). Average precision is computed as the
</bodyText>
<page confidence="0.984998">
5
</page>
<bodyText confidence="0.999836">
average of the system&apos;s precision values at all
points in the ranked list in which recall increases,
that is at all points in the ranked list for which the
gold standard annotation is YES, or, more for-
mally:
where n is the number of the pairs in the test set, R
is the total number of positive pairs in the test set,
E(i) is 1 if the i-th pair is positive and 0 otherwise,
and i ranges over the pairs, ordered by their rank-
ing.
In other words, the more the system was confi-
dent that t entails h, the higher was the ranking of
the pair. A perfect ranking would have placed all
the positive pairs (for which the entailment holds)
before all the negative ones, yielding an average
precision value of 1.
</bodyText>
<subsectionHeader confidence="0.999264">
3.2 Submitted systems
</subsectionHeader>
<bodyText confidence="0.999944904761905">
Twenty-six teams participated in the third chal-
lenge, three more than in previous year. Table 2
presents the list of the results of each submitted
runs and the components used by the systems.
Overall, we noticed a move toward deep ap-
proaches, with a general consolidation of ap-
proaches based on the syntactic structure of Text
and Hypothesis. There is an evident increase of
systems using some form of logical inferences (at
least seven systems). However, these approaches,
with few notably exceptions, do not seem to be
consolidated enough, as several systems show re-
sults not still at the state of art (e.g. Natural Logic
introduced by Chambers et al.). For many systems
an open issue is the availability and integration of
different and complex semantic resources-
A more extensive and fine grained use of spe-
cific semantic phenomena is also emerging. As an
example, Tatu and Moldovan carry on a sophisti-
cated analysis of named entities, in particular Per-
son names, distinguishing first names from last
names. Some form of relation extraction, either
through manually built patterns (Chambers et al.)
or through the use of an information extraction sys-
tem (Hickl and Bensley) have been introduced this
year, even if still on a small scale (i.e. few rela-
tions).
On the other hand, RTE-3 confirmed that both
machine learning using lexical-syntactic features
and transformation-based approaches on depend-
ency representations are well consolidated tech-
niques to address textual entailment. The extension
of transformation-based approaches toward prob-
abilistic settings is an interesting direction investi-
gated by some systems (e.g. Harmeling). On the
side of “light” approaches to textual entailment,
Malakasiotis and Androutpoulos provide a useful
baseline for the task (0.61%) using only POS tag-
ging and then applying string-based measures to
estimate the similarity between Text and Hypothe-
sis.
As far as resources are concerned, lexical data-
bases (mostly WordNet and DIRT) are still widely
used. Extended WordNet is also a common re-
source (for instance in Iftene and Balahur-
Dobrescu) and the Extended Wordnet Knowledge
Base has been successfully used in (Tatu and
Moldovan). Verb-oriented resources are also
largely present in several systems, including Fra-
menet (e.g. Burchardt et al.), Verbnet (Bobrow et
al.) and Propbank (e.g. Adams et al.). It seems that
the use of the Web as a resource is more limited
when compared to the previous RTE workshop.
However, as in RTE-2, the use of large semantic
resources is still a crucial factor affecting the per-
formance of systems (see, for instance, the use of a
large corpus of entailment examples in Hickl and
Bensley).
Finally, an interesting aspect is that, stimulated
by the percentage of longer texts included this year,
a number of participating systems addressed anaph-
ora resolution (e.g. Delmonte, Bar-Haim et al.,
Iftene and Balahur-Dobrescu).
</bodyText>
<subsectionHeader confidence="0.930273">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999836222222222">
The accuracy achieved by the participating sys-
tems ranges from 49% to 80% (considering the best
run of each group), while most of the systems ob-
tained a score in between 59% and 66%. One sub-
mission, Hickl and Bensley achieved 80% accu-
racy, scoring 8% higher than the second system
(Tatu and Moldovan, 72%), and obtaining the best
absolute result achieved in the three RTE chal-
lenges.
</bodyText>
<equation confidence="0.993378333333333">
1nE(i)×#EntailmentUpToPair(i) ∑(1)
R i
i =1
</equation>
<page confidence="0.996657">
6
</page>
<table confidence="0.99993218">
First Author Accuracy Average System Components
precision
Lexical Relation, n-gram\word similarity Syntactic Match- Semantic Role Labeling\ Logical Inference Corpus/ Web-based ML Classification Anaphora resolution Entailment Corpora –
WordNet ing\Aligning Framenet\Probank, Statistics, LSA DIRT Background
Verbnet Knowledge
Adams 0.6700 X X X X
Bar-Haim 0.6112 0.6118 X X X X X
0.5837 0.6093 X X X X
Baral 0.4963 0.5364 X X X
Blake 0.6050 0.5897 X X X
0.6587 0.6096 X X X
Bobrow 0.5112 0.5720 X X X
0.5150 0.5807 X X X
Burchardt 0.6250 X X X
0.6262
Burek 0.5500 X X
0.5500 0.5514
Chambers 0.6050 0.6341 X X X X X
0.6362 0.6527 X X X X X
Clark 0.5088 0.4961 X X X
0.4725 0.4961 X X X
Delmonte 0.5875 0.5830 X X X X X
Ferrandez 0.6563 X X X
0.6375
Ferrés 0.6062 X X X
0.6150 X X X
Harmling 0.5600 0.5813 X X X
0.5775 0.5952 X X X
Hickl 0.8000 0.8815 X X X X X X
Iftene 0.6913 X X X
0.6913 X X X
Li 0.6400 X X X
0.6488
Litkowski 0.6125
Malakasiotis 0.6175 0.6808 X X
Marsi 0.5913 X X
Montejo-Ràez 0.5888 X X X X
0.6038 X X X X
Rodrigo 0.6238 X X X X
0.6312 X X X X
Roth 0.6262 X X X
0.5975 X X
Settembre 0.6100 0.6195 X X X
0.6262 0.6274 X X X
Tatu 0.7225 0.6942 X X X X
0.7175 0.6797 X X X
Wang 0.6650 X X
0.6687
Zanzotto 0.6675 0.6674 X X X
0.6575 0.6732 X X X
</table>
<tableCaption confidence="0.866234">
Table 2: Submission results and components of the systems.
.
</tableCaption>
<page confidence="0.998743">
7
</page>
<bodyText confidence="0.99997665">
As far as the per-task results are concerned, the
trend registered in RTE-2 was confirmed, in that
there was a marked difference in the performances
obtained in different task settings.
In fact, the average accuracy achieved in the QA
setting (0.71) was 20 points higher than that
achieved in the IE setting (0.52); the average accu-
racy in the IR and Sum settings was 0.66 and 0.58
respectively. In RTE-2 the best results were
achieved in SUM, while the lower score was al-
ways recorded in IE. As already pointed out by
Bar-Haim (2006), these differences should be fur-
ther investigated, as they could lead to a sensible
improvement of the performance.
As for the LONG pairs, which represented a
new element of this year’s challenge, no substan-
tial difference was noted in the systems’ perform-
ances: the average accuracy over the long pairs
was 58.72%, compared to 61.93% over the short
ones.
</bodyText>
<sectionHeader confidence="0.996237" genericHeader="conclusions">
4 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999975">
At its third round, the Recognizing Textual En-
tailment task has reached a noticeable level of ma-
turity, as the very high interest in the NLP commu-
nity and the continuously increasing number of
participants in the challenges demonstrate. The
relevance of Textual Entailment Recognition to
different applications, such as the AVE5 track at
QA at CLEF6, has also been acknowledged. Fur-
thermore, the debates and the numerous publica-
tions about the Textual Entailment have contrib-
uted to the better understanding the task and its
nature.
To keep a good balance between the consoli-
dated main task and the need for moving forward,
longer texts were introduced in the dataset, in order
to make the task more challenging, and a pilot task
was proposed. The Third RTE Challenge have also
confirmed that the methodology for the creation of
the datasets, developed in the first two campaigns,
is robust. Overall, the transition of the challenge
coordination from Bar-Ilan –which organized the
first two challenges- to CELCT was successful,
though some problems were encountered, espe-
cially in the preparation of the data set. The sys-
</bodyText>
<footnote confidence="0.953877">
5 http://nlp.uned.es/QA/ave/.
6 http://clef-qa.itc.it/.
</footnote>
<bodyText confidence="0.999983862068965">
tems which took part in RTE-3 showed that the
technology applied to Entailment Recognition has
made significant progress, confirmed by the results,
which were generally better than last year. In par-
ticular, visible progress in defining several new
principled scenarios for RTE was represented, such
as Hickl’s commitment-based approach, Bar
Haim’s proof system, Harmeling’s probabilistic
model, and Standford’s use of Natural Logic.
If, on the one hand, the success that RTE has
had so far is very encouraging, on the other, it in-
cites to overcome certain current limitations, and to
set realistic and, at the same time, stimulating goals
for the future. First at all, theoretical refinements
both of the task and the models applied to it need
to be developed. In particular, more efforts are re-
quired to improve knowledge acquisition, as little
progress has been made on this front so far. Also
the data set generation and the evaluation method-
ology need to be refined and extended. A major
problem in the current setting of the data collection
is that the distribution of the examples is arbitrary
to a large extent, being determined by manual se-
lection. Therefore new evaluation methodologies,
which can reflect realistic distributions should be
investigated, as well as the possibility of evaluating
Textual Entailment Recognition within additional
concrete application scenarios, following the spirit
of the QA Answer Validation Exercise.
</bodyText>
<sectionHeader confidence="0.997646" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9964605">
The following sources were used in the preparation
of the data:
</bodyText>
<listItem confidence="0.964734">
• PowerAnswer question answering system, from
</listItem>
<reference confidence="0.6389195">
Language Computer Corporation, provided by Dan
Moldovan and Marta Tatu.
http://www.languagecomputer.com/solutions/question answer-
ing/power answer/
• Cicero Custom and Cicero Relation information
extraction systems, from Language Computer Cor-
poration, provided by Sanda M. Harabagiu, An-
drew Hickl, John Lehmann and and Paul Aarseth.
http://www.languagecomputer.com/solutions/information_ext
action/cicero/index.html
• Columbia NewsBlaster multi-document summa-
rization system, from the Natural Language Proc-
</reference>
<page confidence="0.986027">
8
</page>
<figure confidence="0.845313">
essing group at Columbia University’s Departmen-
tof Computer Science.
http://newsblaster.cs.columbia.edu/
from the Butler Hill Group, which was funded by
Microsoft Research.
• NewsInEssence multi-document summarization
</figure>
<reference confidence="0.77701748">
system provided by Dragomir R. Radev and Jahna
Otterbacher from the Computational Linguistics
and Information Retrieval research group, Univer-
sity of Michigan.
http://www.newsinessence.com
• New York University’s information extraction
system, provided by Ralph Grishman, Department
of Computer Science, Courant Institute of Mathe-
matical Sciences, New York University.
• MUC-4 information extraction dataset, from the
National Institute of Standards and Technology
(NIST).
http://www.itl.nist.gov/iaui/894.02/related projects/muc/
• ACE 2004 information extraction templates,
from the National Institute of Standards and Tech-
nology (NIST).
http://www.nist.gov/speech/tests/ace/
• TREC IR queries and TREC-QA question collec-
tions, from the National Institute of Standards and
Technology (NIST).
http://trec.nist.gov/
• CLEF IR queries and CLEF-QA question collec-
tions, from DELOS Network of Excellence
for Digital Libraries.
http://www.clef-campaign.org/, http://clef-qa.itc.it/
</reference>
<listItem confidence="0.8391865">
• DUC 2005 annotated peers, from Columbia Uni-
versity, NY, provided by Ani Nenkova.
</listItem>
<bodyText confidence="0.970536476190476">
http://www1.cs.columbia.edu/~ani/DUC2005/
We would like to thank the people and organiza-
tions that made these sources available for the
challenge. In addition, we thank Idan Szpektor and
Roy Bar Haim from Bar-Ilan University for their
assistance and advice, and Valentina Bruseghini
from CELCT for managing the RTE-3 website.
We would also like to acknowledge the people
and organizations involved in creating and annotat-
ing the data: Pamela Forner, Errol Hayman, Cam-
eron Fordyce from CELCT and Courtenay
Hendricks, Adam Savel and Annika Hamalainen
This work was supported in part by the IST Pro-
gramme of the European Community, under the
PASCAL Network of Excellence, IST-2002-
506778. We wish to thank the managers of the
PASCAL challenges program, Michele Sebag and
Florence d’Alche-Buc, for their efforts and sup-
port, which made this challenge possible. We also
thank David Askey, who helped manage the RTE 3
website.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999582478260869">
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini and Idan
Szpektor. 2006. The Second PASCAL Recognizing
Textual Entailment Challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recog-
nizing Textual Entailment, Venice, Italy.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognizing Textual Entailment
Challenge. In Quiñonero-Candela et al., editors,
MLCW 2005, LNAI Volume 3944, pages 177-190.
Springer-Verlag.
J. R. Landis and G. G. Koch. 1997. The measurements
of observer agreement for categorical data. Biomet-
rics, 33:159–174.
Rebecca Passonneau, Ani Nenkova., Kathleen McKe-
own, and Sergey Sigleman. 2005. Applying the
pyramid method in DUC 2005. In Proceedings of the
Document Understanding Conference (DUC 05),
Vancouver, B.C., Canada.
Ellen M. Voorhees and Donna Harman. 1999. Overview
of the seventh text retrieval conference. In Proceed-
ings of the Seventh Text Retrieval Conference
(TREC-7). NIST Special Publication.
</reference>
<page confidence="0.997108">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998146">The Third PASCAL Recognizing Textual Entailment Challenge</title>
<author confidence="0.607487">Danilo</author>
<affiliation confidence="0.588937">Via alla Cascata</affiliation>
<address confidence="0.988415">38100 POVO TN</address>
<email confidence="0.992836">giampiccolo@celct.it</email>
<affiliation confidence="0.763994666666667">Ido Computer Science Bar-Ilan</affiliation>
<address confidence="0.982651">Ramat Gan 52900, Israel</address>
<email confidence="0.999297">dagan@macs.biu.ac.il</email>
<abstract confidence="0.988701541666667">This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year’s dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges. 1.1 The RTE challenges The goal of the RTE challenges has been to create a benchmark task dedicated to textual entailment – recognizing that the meaning of one</abstract>
<author confidence="0.7636975">Bernardo Via Sommarive</author>
<address confidence="0.86384">38100 Povo TN</address>
<email confidence="0.967249">magnini@itc.it</email>
<author confidence="0.999858">Bill Dolan</author>
<affiliation confidence="0.985307">Microsoft</affiliation>
<address confidence="0.999842">Redmond, WA, 98052, USA</address>
<email confidence="0.992816">billdol@microsoft.com</email>
<abstract confidence="0.996615528634362">is entailed, i.e. can be inferred, by In the recent years, this task has raised great interest since applied semantic inference concerns many practical Natural Language Processing (NLP) applications, such as Question Answering (QA), Information Extraction (IE), Summarization, Machine Translation and Paraphrasing, and certain types of queries in Information Retrieval (IR). More specifically, the RTE challenges have aimed to focus research and evaluation on this common underlying semantic inference task and separate it from other problems that different NLP applications need to handle. For example, in addition to textual entailment, QA systems need to handle issues such as answer retrieval and question type recognition. By separating out the general problem of textual entailment from these task-specific problems, progress on semantic inference for many application areas can be promoted. Hopefully, research on textual entailment will finally lead to the development of entailment “engines”, which can be used as a standard module in many applications (similar to the role of part-of-speech taggers and syntactic parsers in current NLP applications). In the following sections, a detailed description of RTE-3 is presented. After a quick review 1The task was first defined by Dagan and Glickman (2004). 1 of the Workshop on Textual Entailment and pages June 2007. Association for Computational Linguistics of the previous challenges (1.2), section 2 describes the preparation of the dataset. In section 3 the evaluation process and the results are presented, together with an analysis of the performance of the participating systems. 1.2 The First and Second RTE Challenges first RTE aimed to provide the NLP community with a new benchmark to test progress in recognizing textual entailment, and to compare the achievements of different groups. This goal proved to be of great interest, and the community&apos;s response encouraged the gradual expansion of the scope of the original task. Second RTE built on the success of the first, with 23 groups from around the world (as compared to 17 for the first challenge) submitting the results of their systems. Representatives of participating groups presented their work at the PASCAL Challenges Workshop in April 2006 in Venice, Italy. The event was successful and the number of participants and their contributions to the discussion demonstrated that Textual Entailment is a quickly growing field of NLP research. In addition, the workshops spawned an impressive number of publications in major conferences, with more work in progress. Another encouraging sign of the growing interest in the RTE challenge was represented by the increase in the number of downloads of the challenge datasets, with about 150 registered downloads for the RTE-2 development set. 1.3 The Third Challenge RTE-3 followed the same basic structure of the previous campaigns, in order to facilitate the participation of newcomers and to allow &amp;quot;veterans&amp;quot; to assess the improvements of their systems in a comparable test exercise. Nevertheless, some innovations were introduced, on the one hand to make the challenge more stimulating and, on the other, to encourage collaboration between system developers. In particular, a limited number of longer texts, i.e. up to a paragraph in length, were incorporated in order to move toward more comprehensive scenarios, 2http://www.pascal-network.org/Challenges/RTE/. 3http://www.pascal-network.org/Challenges/RTE2./ which incorporate the need for discourse analysis. However, the majority of examples remained similar to those in the previous challenges, providing pairs with relatively short texts. Another innovation was represented by a rewhere contributors had the possibility to share the resources they used. In fact, one of the key conclusions at the second RTE Challenge Workshop was that entailment modeling requires vast knowledge resources that correspond to different types of entailment reasoning. Moreover, entailment systems also utilize general NLP tools such as POS taggers, parsers and named-entity recognizers, sometimes posing specialized requirements to such tools. In response to these demands, the RTE Resource Pool was built, which may serve as a portal and forum for publicizing and tracking resources, and reporting on their use. addition, an optional pilot task, called the Evaluation of Inferences from was set up by the US National Institute of Standards and Technology (NIST), in order to explore two other sub-tasks closely related to textual entailment: differentiating unknown entailments from identified contradictions and providing justifications for system decisions. In the first sub-task, the idea was to drive systems to make more precise informational distinctions, taking a three-way decision between &amp;quot;YES&amp;quot;, &amp;quot;NO&amp;quot; and &amp;quot;UNKNOWN”, so that a hypothesis being unknown on the basis of a text would be distinguished from a hypothesis being shown false/contradicted by a text. As for the other subtask, the goal for providing justifications for decisions was to explore how eventual users of tools incorporating entailment can be made to understand how decisions were reached by a system, as users are unlikely to trust a system that gives no explanation for its decisions. The pilot task exploited the existing RTE-3 Challenge infrastructure and evaluation process by using the same test set, while utilizing human for the new 4 http://aclweb.org/aclwiki/index.php?title=Textual_Entail ment_Resource_Pool. 2 TASK TEXT HYPOTHESIS ENTAILMENT IE At the same time the Italian digital rights group, Electronic Frontiers Italy, has asked the nation&apos;s government to investigate Sony over its use of anti-piracy software. Italy&apos;s government investigates Sony. NO IE Parviz Davudi was representing Iran at a meeting of the Shanghai Co-operation Organisation (SCO), the fledgling association that binds Russia, China and four for-mer Soviet republics of central Asia together to fight terrorism China is a member of SCO. YES IR Between March and June, scientific observers say, up to 300,000 seals are killed. In Canada, seal-hunting means jobs, but opponents say it is vicious and endangers the species, also threatened by global warming Hunting endan- YES gers seal species. IR The Italian parliament may approve a draft law allowing descendants of the exiled royal family to return home. The family was banished after the Second World War because of the King&apos;s collusion with the fascist regime, but moves were introduced this year to allow their return. Italian royal family returns home. NO QA Aeschylus is often called the father of Greek tragedy; he wrote the earliest complete plays which survive from ancient Greece. He is known to have written more than 90 plays, though only seven survive. The most famous of these are the trilogy known as Orestia. Also well-known are The Persians and Prometheus Bound. &amp;quot;The Persians&amp;quot; YES was written Aeschylus. SUM A Pentagon committee and the congressionally chartered Iraq Study Group have been preparing reports for Bush, and Iran has asked the presidents of Iraq and Syria to meet in Tehran. Bush will meet NO the presidents of Iraq and Syria in Tehran. Table 1: Some examples taken from the Development Set. 2 The RTE-3 Dataset 2.1 Overview The textual entailment recognition task required the participating systems to decide, given two text whether Textual entailment is defined as a directional relation between text the entailing and hypothesis entailed text), so that a human being, with common understanding of language and common background knowledge, can that most likely true on the basis of the of As in the previous challenges, the RTE-3 dataset consisted of 1600 text-hypothesis pairs, equally divided into a development set and a test set. While length of the hypotheses was the same as in past datasets, a certain number of texts were longer than in previous datasets, up to a paragraph. The longer texts were marked as L, after being selected automatically when exceeding 270 bytes. In the test set they were about 17% of the total. As in RTE-2, four applications – namely IE, IR, QA and SUM – were considered as settings or contexts for the pairs generation (see 2.2 for a detailed description). 200 pairs were selected for each application in each dataset. Although the datasets were supposed to be perfectly balanced, the number of negative examples were slightly higher in both development and test sets (51.50% and 51.25% respectively; this was unintentional). Posientailment examples, where were the negative ones, where entailment not hold, pair was annotated with its 3 related task (IE/IR/QA/SUM) and entailment judgment (YES/NO, obviously released only in the development set). Table 1 shows some examples taken from the development set. The examples in the dataset were based mostly on outputs (both correct and incorrect) of Webbased systems. In order to avoid copyright problems, input data was limited to either what had already been publicly released by official competitions or else was drawn from freely available sources such as WikiNews and Wikipedia. In choosing the pairs, the following judgment criteria and guidelines were considered: entailment is a directional relation, the hypothesis must be entailed by the given text, but the text need not be entailed by the hypothesis. hypothesis must be fully entailed by the text. Judgment must be NO if the hypothesis includes parts that cannot be inferred from the text. in which inference is very probable (but not completely certain) were judged as YES. world knowledge was assumed, e.g. the capital of a country is situated in that country, the prime minister of a state is also a citizen of that state, and so on. 2.2 Pair Collection in RTE-2, human annotators generated within 4 application settings. The IE task was inspired by the Information Extraction (and Relation Extraction) application, where texts and structured templates were replaced As in the 2006 campaign, the pairs were generated using four different approaches: 1) Hypotheses were taken from the relations tested in the ACE-2004 RDR task, while texts were extracted from the outputs of actual IE systems, which were provided with relevant news articles. Correctly extracted instances were used to generate positive examples and incorrect instances to generate negative examples. 2) The same procedure was followed using output of IE systems on the dataset of the MUC-4 TST3 task, in which the events are acts of terrorism. 3) The annotated MUC-4 dataset and the news articles were also used to manually generate entailment pairs based on ACE relations. 4) Hypotheses corresponding to relations not found in the ACE and MUC datasets were used both to be given to IE systems and to generate from collected news articles. Examples of these relations, taken from various semantic fields, were “X beat Y”, “X invented Y”, “X steal Y” etc. The common aim of all these processes was to simulate the need of IE systems to recognize that the given text indeed entails the semantic relation that is expected to hold between the candidate template slot fillers. In the IR (Information Retrieval) application setting, the hypotheses were propositional IR queries, specify some statement, e.g. are to find avalanche hypotheses were adapted and simplified from standard IR datasets (TREC and CLEF). Texts that did or did not entail the hypotheses were selected from documents retrieved by different search engines (e.g. Google, Yahoo and MSN) for each hypothesis. In this application setting it was assumed that relevant documents (from an IR perspective) should entail the given propositional hypothesis. For the QA (Question Answering) task, annotators used questions taken from the datasets of official QA competitions, such as TREC QA and QA@CLEF datasets, and the corresponding answers extracted from the Web by actual QA systems. Then they transformed the question-answer into as follows: answer term of the expected answer type was picked from the answer passage either a correct or an incorrect one. question was turned into an affirmative sentence plugging in the answer term. were generate, using the affirmasentences as hypotheses the answer passages as texts 4 example, given the question high is Everest?” a text above mentioned expedition team comprising of 10 members was permitted to climb 8848m. high Mt. Everest from Normal Route for the period of 75 days from 15 April, 2007 under the leadership of Mr. Wolf of annotator, extracting the piece of information “8848m.” from the text, would turn the question into an the affirmative sen- Everest is 8848m generating a positive entailment pair. This process simulated the need of a QA system to verify that the retrieved passage text actually entailed the provided answer. In the SUM (Summarization) setting, the entailment pairs were generated using two procedures. the first one, sentences taken from a news document cluster, a collection of news articles that describe the same news item. Annotators were given the output of multi-document summarization systems -including the document clusters and the summary generated for each cluster. Then they picked sentence pairs with high lexical overlap, preferably where at least one of the sentences was taken from the summary (this senusually played the role of For positive examples, the hypothesis was simplified by removing parts, until it was fully entailed by Negative examples were simplified in a similar In alternative, produced for the experimental evaluation mehod in DUC 2005 (Passonneau et al. 2005) were exploited. In this new evaluation method, humans select subsentential content units (SCUs) in several manually produced summaries on a subject, and collocate them in a “pyramid”, which has at the top the SCUs with the higher frequency, i.e. those which are present in most summaries. Each SCU is identified by a label, a sentence in natural language which expresses the content. Afterwards, the annotators individuate the SCUs present in summaries automatically (called and link them to the ones present in the pyramid, in order to assign each peer a weight. In this way, the SCUs in the automatic summaries linked to the SCUs in the higher tiers of the pyramid are assigned a heavier weight than those at the bottom. For the SUM setting, the RTE-3 annotators selected relevant passages from the peers and used them as T’s, meanwhile the labels of the corresponding SCUs were used as H’s. Small adjustments were allowed, whenever the texts were not grammatically acceptable. This process simulated the need of a summarization system to identify information redundancy, which should be avoided in the summary. 2.3 Final dataset Each pair of the dataset was judged by three annotators. As in previous challenges, pairs on which the annotators disagreed were filtered-out. On the test set, the average agreement between pair of annotators who shared at least 100 examples was 87.8%, with an average Kappa level of 0.75, regarded as substantial agreement according to Landis and Koch (1997). 19.2 % of the pairs in the dataset were removed the test set due to disagreement. The diswas generally due to the fact that the more specific than the for example because it contained more information, or made an absolute where only a personal opinion. In addition, 9.4 % of the remaining pairs were discarded, as they seemed controversial, too difficult, or too similar when compared to other pairs. far as the from the web are concerned, spelling and punctuation errors were sometimes fixed by the annotators, but no major change was allowed, so that the language could be grammatically and stylistically imperfect. The hypotheses were finally double-checked by a native English speaker. 3 The RTE-3 Challenge 3.1 Evaluation measures The evaluation of all runs submitted in RTE-3 was automatic. The judgments (classifications) returned by the system were compared to the Gold Standard compiled by the human assessors. The main measure was the percentage of matching judgments. For systems that provided a confidence-ranked list of the pairs, in addition to the YES/NO judgment, an Average Precision measure was also computed. This measure evaluates the ability of systems to rank all the T-H pairs in the test set according to their entailment confidence (in decreasing order from the most certain entailment to the least certain). Average precision is computed as the 5 average of the system&apos;s precision values at all points in the ranked list in which recall increases, that is at all points in the ranked list for which the gold standard annotation is YES, or, more formally: the number of the pairs in the test set, is the total number of positive pairs in the test set, 1 if the pair is positive and 0 otherwise, over the pairs, ordered by their ranking. In other words, the more the system was confithat the higher was the ranking of the pair. A perfect ranking would have placed all the positive pairs (for which the entailment holds) before all the negative ones, yielding an average precision value of 1. 3.2 Submitted systems Twenty-six teams participated in the third challenge, three more than in previous year. Table 2 presents the list of the results of each submitted runs and the components used by the systems. Overall, we noticed a move toward deep approaches, with a general consolidation of approaches based on the syntactic structure of Text and Hypothesis. There is an evident increase of systems using some form of logical inferences (at least seven systems). However, these approaches, with few notably exceptions, do not seem to be consolidated enough, as several systems show results not still at the state of art (e.g. Natural Logic introduced by Chambers et al.). For many systems an open issue is the availability and integration of different and complex semantic resources- A more extensive and fine grained use of specific semantic phenomena is also emerging. As an example, Tatu and Moldovan carry on a sophisticated analysis of named entities, in particular Person names, distinguishing first names from last names. Some form of relation extraction, either through manually built patterns (Chambers et al.) or through the use of an information extraction system (Hickl and Bensley) have been introduced this year, even if still on a small scale (i.e. few relations). On the other hand, RTE-3 confirmed that both machine learning using lexical-syntactic features and transformation-based approaches on dependency representations are well consolidated techniques to address textual entailment. The extension of transformation-based approaches toward probabilistic settings is an interesting direction investigated by some systems (e.g. Harmeling). On the side of “light” approaches to textual entailment, Malakasiotis and Androutpoulos provide a useful baseline for the task (0.61%) using only POS tagging and then applying string-based measures to estimate the similarity between Text and Hypothesis. As far as resources are concerned, lexical databases (mostly WordNet and DIRT) are still widely used. Extended WordNet is also a common resource (for instance in Iftene and Balahur- Dobrescu) and the Extended Wordnet Knowledge Base has been successfully used in (Tatu and Moldovan). Verb-oriented resources are also largely present in several systems, including Framenet (e.g. Burchardt et al.), Verbnet (Bobrow et al.) and Propbank (e.g. Adams et al.). It seems that the use of the Web as a resource is more limited when compared to the previous RTE workshop. However, as in RTE-2, the use of large semantic resources is still a crucial factor affecting the performance of systems (see, for instance, the use of a large corpus of entailment examples in Hickl and Bensley). Finally, an interesting aspect is that, stimulated by the percentage of longer texts included this year, a number of participating systems addressed anaphora resolution (e.g. Delmonte, Bar-Haim et al., Iftene and Balahur-Dobrescu). 3.3 Results The accuracy achieved by the participating systems ranges from 49% to 80% (considering the best run of each group), while most of the systems obtained a score in between 59% and 66%. One submission, Hickl and Bensley achieved 80% accuracy, scoring 8% higher than the second system (Tatu and Moldovan, 72%), and obtaining the best absolute result achieved in the three RTE challenges.</abstract>
<note confidence="0.67889375">R i 6 First Author Accuracy Average precision System Components Lexical Relation, WordNet n-gram\word similarity Match- Semantic Role Labeling\ Framenet\Probank, Verbnet Logical Inference Corpus/ Web-based Statistics, LSA ML Classification Anaphora resolution Entailment Corpora –</note>
<title confidence="0.7504415">ing\Aligning DIRT Knowledge</title>
<address confidence="0.662348044444445">Adams 0.6700 X X X X Bar-Haim 0.6112 0.6118 X X X X X 0.5837 0.6093 X X X X Baral 0.4963 0.5364 X X X Blake 0.6050 0.5897 X X X 0.6587 0.6096 X X X Bobrow 0.5112 0.5720 X X X 0.5150 0.5807 X X X Burchardt 0.6250 X X X 0.6262 Burek 0.5500 X X 0.5500 0.5514 Chambers 0.6050 0.6341 X X X X X 0.6362 0.6527 X X X X X Clark 0.5088 0.4961 X X X 0.4725 0.4961 X X X Delmonte 0.5875 0.5830 X X X X X Ferrandez 0.6563 X X X 0.6375 Ferrés 0.6062 X X X 0.6150 X X X Harmling 0.5600 0.5813 X X X 0.5775 0.5952 X X X Hickl 0.8000 0.8815 X X X X X X Iftene 0.6913 X X X 0.6913 X X X Li 0.6400 X X X 0.6488 Litkowski 0.6125 Malakasiotis 0.6175 0.6808 X X Marsi 0.5913 X X Montejo-Ràez 0.5888 X X X X 0.6038 X X X X Rodrigo 0.6238 X X X X 0.6312 X X X X Roth 0.6262 X X X 0.5975 X X Settembre 0.6100 0.6195 X X X 0.6262 0.6274 X X X Tatu 0.7225 0.6942 X X X X 0.7175 0.6797 X X X Wang 0.6650 X X 0.6687 Zanzotto 0.6675 0.6674 X X X 0.6575 0.6732 X X X</address>
<abstract confidence="0.977875242105263">Table 2: Submission results and components of the systems. . 7 As far as the per-task results are concerned, the trend registered in RTE-2 was confirmed, in that there was a marked difference in the performances obtained in different task settings. In fact, the average accuracy achieved in the QA setting (0.71) was 20 points higher than that achieved in the IE setting (0.52); the average accuracy in the IR and Sum settings was 0.66 and 0.58 respectively. In RTE-2 the best results were achieved in SUM, while the lower score was always recorded in IE. As already pointed out by Bar-Haim (2006), these differences should be further investigated, as they could lead to a sensible improvement of the performance. As for the LONG pairs, which represented a new element of this year’s challenge, no substantial difference was noted in the systems’ performances: the average accuracy over the long pairs was 58.72%, compared to 61.93% over the short ones. 4 Conclusions and future work At its third round, the Recognizing Textual Entailment task has reached a noticeable level of maturity, as the very high interest in the NLP community and the continuously increasing number of participants in the challenges demonstrate. The relevance of Textual Entailment Recognition to applications, such as the track at at has also been acknowledged. Furthermore, the debates and the numerous publications about the Textual Entailment have contributed to the better understanding the task and its nature. To keep a good balance between the consolidated main task and the need for moving forward, longer texts were introduced in the dataset, in order to make the task more challenging, and a pilot task was proposed. The Third RTE Challenge have also confirmed that the methodology for the creation of the datasets, developed in the first two campaigns, is robust. Overall, the transition of the challenge coordination from Bar-Ilan –which organized the first two challengesto CELCT was successful, though some problems were encountered, espein the preparation of the data set. The systems which took part in RTE-3 showed that the technology applied to Entailment Recognition has made significant progress, confirmed by the results, which were generally better than last year. In particular, visible progress in defining several new principled scenarios for RTE was represented, such as Hickl’s commitment-based approach, Bar Haim’s proof system, Harmeling’s probabilistic model, and Standford’s use of Natural Logic. If, on the one hand, the success that RTE has had so far is very encouraging, on the other, it incites to overcome certain current limitations, and to set realistic and, at the same time, stimulating goals for the future. First at all, theoretical refinements both of the task and the models applied to it need to be developed. In particular, more efforts are required to improve knowledge acquisition, as little progress has been made on this front so far. Also the data set generation and the evaluation methodology need to be refined and extended. A major problem in the current setting of the data collection is that the distribution of the examples is arbitrary to a large extent, being determined by manual selection. Therefore new evaluation methodologies, which can reflect realistic distributions should be investigated, as well as the possibility of evaluating Textual Entailment Recognition within additional concrete application scenarios, following the spirit of the QA Answer Validation Exercise. Acknowledgments The following sources were used in the preparation of the data: • PowerAnswer question answering system, from Language Computer Corporation, provided by Dan Moldovan and Marta Tatu. http://www.languagecomputer.com/solutions/question answering/power answer/ • Cicero Custom and Cicero Relation information extraction systems, from Language Computer Corporation, provided by Sanda M. Harabagiu, Andrew Hickl, John Lehmann and and Paul Aarseth. http://www.languagecomputer.com/solutions/information_ext action/cicero/index.html • Columbia NewsBlaster multi-document summasystem, from the Natural Language Proc- 8 essing group at Columbia University’s Departmen-</abstract>
<affiliation confidence="0.781465">tof Computer Science.</affiliation>
<web confidence="0.994166">http://newsblaster.cs.columbia.edu/</web>
<note confidence="0.624635">from the Butler Hill Group, which was funded by Microsoft Research. • NewsInEssence multi-document summarization system provided by Dragomir R. Radev and Jahna Otterbacher from the Computational Linguistics</note>
<affiliation confidence="0.8432925">and Information Retrieval research group, University of Michigan.</affiliation>
<email confidence="0.574838">http://www.newsinessence.com</email>
<abstract confidence="0.448864">New York University’s information extraction system, provided by Ralph Grishman, Department</abstract>
<affiliation confidence="0.9563645">of Computer Science, Courant Institute of Mathematical Sciences, New York University.</affiliation>
<note confidence="0.490971571428571">MUC-4 information extraction dataset, from the National Institute of Standards and Technology (NIST). http://www.itl.nist.gov/iaui/894.02/related projects/muc/ • ACE 2004 information extraction templates, from the National Institute of Standards and Technology (NIST).</note>
<web confidence="0.832758">http://www.nist.gov/speech/tests/ace/</web>
<abstract confidence="0.617249666666667">TREC IR queries and TREC-QA question collections, from the National Institute of Standards and Technology (NIST).</abstract>
<web confidence="0.751881">http://trec.nist.gov/</web>
<abstract confidence="0.770011666666667">CLEF IR queries and CLEF-QA question collections, from DELOS Network of Excellence for Digital Libraries.</abstract>
<web confidence="0.992641">http://www.clef-campaign.org/, http://clef-qa.itc.it/</web>
<note confidence="0.5514685">DUC 2005 annotated peers, from Columbia University, NY, provided by Ani Nenkova.</note>
<web confidence="0.985559">http://www1.cs.columbia.edu/~ani/DUC2005/</web>
<abstract confidence="0.89186025">We would like to thank the people and organizations that made these sources available for the challenge. In addition, we thank Idan Szpektor and Roy Bar Haim from Bar-Ilan University for their assistance and advice, and Valentina Bruseghini CELCT for managing the RTE-3 We would also like to acknowledge the people and organizations involved in creating and annotating the data: Pamela Forner, Errol Hayman, Cameron Fordyce from CELCT and Courtenay Hendricks, Adam Savel and Annika Hamalainen This work was supported in part by the IST Programme of the European Community, under the Network of IST-2002- 506778. We wish to thank the managers of the PASCAL challenges program, Michele Sebag and Florence d’Alche-Buc, for their efforts and support, which made this challenge possible. We also thank David Askey, who helped manage the RTE 3 website.</abstract>
<title confidence="0.940661">References</title>
<author confidence="0.979304">Roy Bar-Haim</author>
<author confidence="0.979304">Ido Dagan</author>
<author confidence="0.979304">Bill Dolan</author>
<author confidence="0.979304">Lisa Ferro</author>
<note confidence="0.888778086956522">Danilo Giampiccolo, Bernardo Magnini and Idan Szpektor. 2006. The Second PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment, Venice, Italy. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognizing Textual Entailment Challenge. In Quiñonero-Candela et al., editors, MLCW 2005, LNAI Volume 3944, pages 177-190. Springer-Verlag. J. R. Landis and G. G. Koch. 1997. The measurements observer agreement for categorical data. Biomet- 33:159–174. Rebecca Passonneau, Ani Nenkova., Kathleen McKeown, and Sergey Sigleman. 2005. Applying the pyramid method in DUC 2005. In Proceedings of the Document Understanding Conference (DUC 05), Vancouver, B.C., Canada. Ellen M. Voorhees and Donna Harman. 1999. Overview of the seventh text retrieval conference. In Proceedings of the Seventh Text Retrieval Conference (TREC-7). NIST Special Publication. 9</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Sanda M Harabagiu</author>
<author>Andrew Hickl</author>
<author>John Lehmann</author>
<author>Paul Aarseth</author>
</authors>
<title>Corporation, provided by Dan Moldovan and Marta Tatu. http://www.languagecomputer.com/solutions/question answering/power answer/ • Cicero Custom and Cicero Relation information extraction systems, from Language Computer Corporation, provided by</title>
<institution>Language Computer</institution>
<marker>Harabagiu, Hickl, Lehmann, Aarseth, </marker>
<rawString>Language Computer Corporation, provided by Dan Moldovan and Marta Tatu. http://www.languagecomputer.com/solutions/question answering/power answer/ • Cicero Custom and Cicero Relation information extraction systems, from Language Computer Corporation, provided by Sanda M. Harabagiu, Andrew Hickl, John Lehmann and and Paul Aarseth.</rawString>
</citation>
<citation valid="false">
<note>http://www.languagecomputer.com/solutions/information_ext action/cicero/index.html</note>
<marker></marker>
<rawString>http://www.languagecomputer.com/solutions/information_ext action/cicero/index.html</rawString>
</citation>
<citation valid="false">
<authors>
<author>Columbia</author>
</authors>
<title>NewsBlaster multi-document summarization system, from the Natural Language Procsystem provided by Dragomir R.</title>
<booktitle>Radev and Jahna Otterbacher from the Computational Linguistics and Information Retrieval research</booktitle>
<institution>group, University of Michigan.</institution>
<note>http://www.newsinessence.com</note>
<marker>Columbia, </marker>
<rawString>• Columbia NewsBlaster multi-document summarization system, from the Natural Language Procsystem provided by Dragomir R. Radev and Jahna Otterbacher from the Computational Linguistics and Information Retrieval research group, University of Michigan. http://www.newsinessence.com</rawString>
</citation>
<citation valid="false">
<authors>
<author>New York</author>
</authors>
<title>University’s information extraction system, provided by Ralph Grishman,</title>
<booktitle>MUC-4 information extraction dataset, from the National Institute of Standards and Technology (NIST).</booktitle>
<institution>Department of Computer Science, Courant Institute of Mathematical Sciences, New York University.</institution>
<marker>York, </marker>
<rawString>• New York University’s information extraction system, provided by Ralph Grishman, Department of Computer Science, Courant Institute of Mathematical Sciences, New York University. • MUC-4 information extraction dataset, from the National Institute of Standards and Technology (NIST).</rawString>
</citation>
<citation valid="false">
<booktitle>http://www.itl.nist.gov/iaui/894.02/related projects/muc/ • ACE 2004 information extraction templates, from the National Institute of Standards and Technology (NIST).</booktitle>
<marker></marker>
<rawString>http://www.itl.nist.gov/iaui/894.02/related projects/muc/ • ACE 2004 information extraction templates, from the National Institute of Standards and Technology (NIST).</rawString>
</citation>
<citation valid="false">
<booktitle>http://www.nist.gov/speech/tests/ace/ • TREC IR queries and TREC-QA question collections, from the National Institute of Standards and Technology (NIST).</booktitle>
<marker></marker>
<rawString>http://www.nist.gov/speech/tests/ace/ • TREC IR queries and TREC-QA question collections, from the National Institute of Standards and Technology (NIST).</rawString>
</citation>
<citation valid="false">
<authors>
<author>http trec nist gov</author>
</authors>
<title>CLEF IR queries and CLEF-QA question collections, from DELOS Network of Excellence for Digital Libraries.</title>
<note>http://www.clef-campaign.org/, http://clef-qa.itc.it/</note>
<marker>gov, </marker>
<rawString>http://trec.nist.gov/ • CLEF IR queries and CLEF-QA question collections, from DELOS Network of Excellence for Digital Libraries. http://www.clef-campaign.org/, http://clef-qa.itc.it/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The Second PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini and Idan Szpektor. 2006. The Second PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>MLCW 2005, LNAI Volume 3944,</booktitle>
<pages>177--190</pages>
<editor>In Quiñonero-Candela et al., editors,</editor>
<publisher>Springer-Verlag.</publisher>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognizing Textual Entailment Challenge. In Quiñonero-Candela et al., editors, MLCW 2005, LNAI Volume 3944, pages 177-190. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurements of observer agreement for categorical data.</title>
<date>1997</date>
<journal>Biometrics,</journal>
<pages>33--159</pages>
<contexts>
<context position="17074" citStr="Landis and Koch (1997)" startWordPosition="2749" endWordPosition="2752">s were used as H’s. Small adjustments were allowed, whenever the texts were not grammatically acceptable. This process simulated the need of a summarization system to identify information redundancy, which should be avoided in the summary. 2.3 Final dataset Each pair of the dataset was judged by three annotators. As in previous challenges, pairs on which the annotators disagreed were filtered-out. On the test set, the average agreement between each pair of annotators who shared at least 100 examples was 87.8%, with an average Kappa level of 0.75, regarded as substantial agreement according to Landis and Koch (1997). 19.2 % of the pairs in the dataset were removed from the test set due to disagreement. The disagreement was generally due to the fact that the h was more specific than the t, for example because it contained more information, or made an absolute assertion where t proposed only a personal opinion. In addition, 9.4 % of the remaining pairs were discarded, as they seemed controversial, too difficult, or too similar when compared to other pairs. As far as the texts extracted from the web are concerned, spelling and punctuation errors were sometimes fixed by the annotators, but no major change wa</context>
</contexts>
<marker>Landis, Koch, 1997</marker>
<rawString>J. R. Landis and G. G. Koch. 1997. The measurements of observer agreement for categorical data. Biometrics, 33:159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
<author>Sergey Sigleman</author>
</authors>
<title>Applying the pyramid method in DUC</title>
<date>2005</date>
<booktitle>In Proceedings of the Document Understanding Conference (DUC 05),</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="15559" citStr="Passonneau et al. 2005" startWordPosition="2494" endWordPosition="2497">me news item. Annotators were given the output of multi-document summarization systems -including the document clusters and the summary generated for each cluster. Then they picked sentence pairs with high lexical overlap, preferably where at least one of the sentences was taken from the summary (this sentence usually played the role of t). For positive examples, the hypothesis was simplified by removing sentence parts, until it was fully entailed by t. Negative examples were simplified in a similar manner. In alternative, “pyramids” produced for the experimental evaluation mehod in DUC 2005 (Passonneau et al. 2005) were exploited. In this new evaluation method, humans select subsentential content units (SCUs) in several manually produced summaries on a subject, and collocate them in a “pyramid”, which has at the top the SCUs with the higher frequency, i.e. those which are present in most summaries. Each SCU is identified by a label, a sentence in natural language which expresses the content. Afterwards, the annotators individuate the SCUs present in summaries generated automatically (called peers), and link them to the ones present in the pyramid, in order to assign each peer a weight. In this way, the </context>
</contexts>
<marker>Passonneau, Nenkova, McKeown, Sigleman, 2005</marker>
<rawString>Rebecca Passonneau, Ani Nenkova., Kathleen McKeown, and Sergey Sigleman. 2005. Applying the pyramid method in DUC 2005. In Proceedings of the Document Understanding Conference (DUC 05), Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Donna Harman</author>
</authors>
<title>Overview of the seventh text retrieval conference.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh Text Retrieval Conference (TREC-7). NIST Special Publication.</booktitle>
<marker>Voorhees, Harman, 1999</marker>
<rawString>Ellen M. Voorhees and Donna Harman. 1999. Overview of the seventh text retrieval conference. In Proceedings of the Seventh Text Retrieval Conference (TREC-7). NIST Special Publication.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>