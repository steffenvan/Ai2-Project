<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000802">
<title confidence="0.976175">
To Annotate More Accurately or to Annotate More
</title>
<author confidence="0.996315">
Dmitriy Dligach
</author>
<affiliation confidence="0.9989585">
Department of Computer Science
University of Colorado at Boulder
</affiliation>
<email confidence="0.981572">
Dmitriy.Dligach@colorado.edu
</email>
<author confidence="0.996161">
Rodney D. Nielsen
</author>
<affiliation confidence="0.99534">
The Center for Computational Language
and Education Research
University of Colorado at Boulder
</affiliation>
<email confidence="0.993587">
Rodney.Nielsen@colorado.edu
</email>
<author confidence="0.993216">
Martha Palmer
</author>
<affiliation confidence="0.999221666666667">
Department of Linguistics
Department of Computer Science
University of Colorado at Boulder
</affiliation>
<email confidence="0.997372">
Martha.Palmer@colorado.edu
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956777777778">
The common accepted wisdom is that
blind double annotation followed by adju-
dication of disagreements is necessary to
create training and test corpora that result
in the best possible performance. We pro-
vide evidence that this is unlikely to be the
case. Rather, the greatest value for your
annotation dollar lies in single annotating
more data.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973102040816">
In recent years, supervised learning has become
the dominant paradigm in Natural Language Pro-
cessing (NLP), thus making the creation of hand-
annotated corpora a critically important task. A
corpus where each instance is annotated by a sin-
gle tagger unavoidably contains errors. To im-
prove the quality of the data, an annotation project
may choose to annotate each instance twice and
adjudicate the disagreements, thus producing the
(largely) error-free gold standard. For example,
OntoNotes (Hovy et al., 2006), a large-scale an-
notation project, chose this option.
However, given a virtually unlimited supply of
unlabeled data and limited funding – a typical set
of constraints in NLP – an annotation project must
always face the realization that for the cost of dou-
ble annotation, more than twice as much data can
be single annotated. The philosophy behind this
alternative says that modern machine learning al-
gorithms can still generalize well in the presence
of noise, especially when given larger amounts of
training data.
Currently, the commonly accepted wisdom
sides with the view that says that blind double
annotation followed by adjudication of disagree-
ments is necessary to create annotated corpora that
leads to the best possible performance. We pro-
vide empirical evidence that this is unlikely to be
the case. Rather, the greatest value for your an-
notation dollar lies in single annotating more data.
There may, however, be other considerations that
still argue in favor of double annotation.
In this paper, we also consider the arguments of
Beigman and Klebanov (2009), who suggest that
data should be multiply annotated and then filtered
to discard all of the examples where the annota-
tors do not have perfect agreement. We provide
evidence that single annotating more data for the
same cost is likely to result in better system per-
formance.
This paper proceeds as follows: first, we out-
line our evaluation framework in Section 2. Next,
we compare the single annotation and adjudica-
tion scenarios in Section 3. Then, we compare
the annotation scenario of Beigman and Klebanov
(2009) with the single annotation scenario in Sec-
tion 4. After that, we discuss the results and future
work in section 5. Finally, we draw the conclusion
in Section 6.
</bodyText>
<sectionHeader confidence="0.99827" genericHeader="introduction">
2 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.755131">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.9999586">
For evaluation we utilize the word sense data an-
notated by the OntoNotes project. The OntoNotes
data was chosen because it utilizes full double-
blind annotation by human annotators and the dis-
agreements are adjudicated by a third (more expe-
</bodyText>
<page confidence="0.992902">
64
</page>
<note confidence="0.7540945">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 64–72,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<listItem confidence="0.951486272727273">
rienced) annotator. This allows us to
• Evaluate single annotation results by using
the labels assigned by the first tagger
• Evaluate double annotation results by using
the labels assigned by the second tagger
• Evaluate adjudication results by using the la-
bels assigned by the the adjudicator to the in-
stances where the two annotators disagreed
• Measure the performance under various sce-
narios against the double annotated and adju-
dicated gold standard data
</listItem>
<bodyText confidence="0.987317076923077">
We selected the 215 most frequent verbs in the
OntoNotes data. To make the size of the dataset
more manageable, we randomly selected 500 ex-
amples of each of the 15 most frequent verbs. For
the remaining 200 verbs, we utilized all the an-
notated examples. The resulting dataset contained
66,228 instances of the 215 most frequent verbs.
Table 1 shows various important characteristics of
this dataset averaged across the 215 verbs.
Inter-tagger agreement 86%
Annotator1-gold standard agreement 93%
Share of the most frequent sense 70%
Number of classes (senses) per verb 4.74
</bodyText>
<tableCaption confidence="0.998245">
Table 1: Data used in evaluation at a glance
</tableCaption>
<subsectionHeader confidence="0.999683">
2.2 Cost of Annotation
</subsectionHeader>
<bodyText confidence="0.999983075">
Because for this set of experiments we care pri-
marily about the cost effectiveness of the annota-
tion dollars, we need to know how much it costs
to blind annotate instances and how much it costs
to adjudicate disagreements in instances. There is
an upfront cost associated with any annotation ef-
fort to organize the project, design an annotation
scheme, set up the environment, create annotation
guidelines, hire and train the annotators, etc. We
will assume, for the sake of this paper, that this
cost is fixed and is the same regardless of whether
the data is single annotated or the data is double
annotated and disagreements adjudicated.
In this paper, we focus on a scenario where there
is essentially no difference in cost to collect ad-
ditional data to be annotated, as is often the case
(e.g., there is virtually no additional cost to down-
load 2.5 versus 1.0 million words of text from the
web). However, this is not always the case (e.g.,
collecting speech can be costly).
To calculate a cost per annotated instance for
blind annotation, we take the total expenses asso-
ciated with the annotators in this group less train-
ing costs and any costs not directly associated with
annotation and divide by the total number of blind
instance annotations. This value, $0.0833, is the
per instance cost used for single annotation. We
calculated the cost for adjudicating instances sim-
ilarly, based on the expenses associated with the
adjudication group. The adjudication cost is an ad-
ditional $0.1000 per instance adjudicated. The per
instance cost for double blind, adjudicated data is
then computed as double the cost for single an-
notation plus the per instance cost of adjudication
multiplied by the percent of disagreement, 14%,
which is $0.1805.
We leave an analysis of the extent to which the
up front costs are truly fixed and whether they can
be altered to result in more value for the dollar to
future work.
</bodyText>
<subsectionHeader confidence="0.999117">
2.3 Automatic Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.983237407407408">
For the experiments we conduct in this study, we
needed a word sense disambiguation (WSD) sys-
tem. Our WSD system is modeled after the state-
of-the-art verb WSD system described in (Dligach
and Palmer, 2008). We will briefly outline it here.
We view WSD as a supervised learning prob-
lem. Each instance of the target verb is represented
as a vector of binary features that indicate the pres-
ence (or absence) of the corresponding features in
the neighborhood of the target verb. We utilize
all of the linguistic features that were shown to be
useful for disambiguating verb senses in (Chen et
al., 2007).
To extract the lexical features we POS-tag
the sentence containing the target verb and the
two surrounding sentences using MXPost soft-
ware (Ratnaparkhi, 1998). All open class words
(nouns, verbs, adjectives, and adverbs) in these
sentences are included in our feature set. In addi-
tion to that, we use as features two words on each
side of the target verb as well as their POS tags.
To extract the syntactic features we parse the
sentence containing the target verb with Bikel’s
constituency parser and utilize a set of rules to
identify the features in Table 2.
Our semantic features represent the semantic
classes of the target verb’s syntactic arguments
</bodyText>
<page confidence="0.997875">
65
</page>
<bodyText confidence="0.90179075862069">
Feature Explanation
Subject and object - Presence of subject and
object
- Head word of subject
and object NPs
- POS tag of the head
word of subject and
object NPs
Voice - Passive or Active
PP adjunct - Presence of PP adjunct
- Preposition word
- Head word of the
preposition’s NP
argument
Subordinate clause - Presence of subordinate
clause
Path - Parse tree path from
target verb to neighboring
words
- Parse tree path from
target verb to subject and
object
- Parse tree path from
target verb to subordinate
clause
Subcat frame - Phrase structure rule
expanding the target
verb’s parent node in
parse tree
</bodyText>
<tableCaption confidence="0.735847">
Table 2: Syntactic features
</tableCaption>
<bodyText confidence="0.995695">
such as subject and object. The semantic classes
are approximated as
</bodyText>
<listItem confidence="0.989692285714286">
• WordNet (Fellbaum, 1998) hypernyms
• NE tags derived from the output of Identi-
Finder (Bikel et al., 1999)
• Dynamic dependency neighbors (Dligach
and Palmer, 2008), which are extracted in an
unsupervised way from a dependency-parsed
corpus
</listItem>
<bodyText confidence="0.998420833333333">
Our WSD system uses the Libsvm software
package (Chang and Lin, 2001) for classification.
We accepted the default options (C = 1 and lin-
ear kernel) when training our classifiers. As is the
case with most WSD systems, we train a separate
model per verb.
</bodyText>
<sectionHeader confidence="0.983453" genericHeader="method">
3 Experiment One
</sectionHeader>
<bodyText confidence="0.9999862">
The results of experiment one show that in these
circumstances, better performance is achieved by
single annotating more data than by deploing re-
sources towards ensuring that the data is annotated
more accurately through an adjudication process.
</bodyText>
<subsectionHeader confidence="0.996441">
3.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999992966666667">
We conduct a number of experiments to compare
the effect of single annotated versus adjudicated
data on the accuracy of a state of the art WSD sys-
tem. Since OntoNotes does not have a specified
test set, for each word, we used repeated random
partitioning of the data with 10 trials and 10% into
the test set and the remaining 90% comprising the
training set.
We then train an SVM classifier on varying frac-
tions of the data, based on the number of examples
that could be annotated per dollar. Specifically,
in increments of $1.00, we calculate the number
of examples that can be single annotated and the
number that can be double blind annotated and ad-
judicated with that amount of money.
The number of examples computed for single
annotation is selected at random from the train-
ing data. Then the adjudicated examples are se-
lected at random from this subset. Selecting from
the same subset of data approaches pair statisti-
cal testing and results in a more accurate statistical
comparison of the models produced.
Classifiers are trained on this data using the la-
bels from the first round of annotation as the single
annotation labels and the final adjudicated labels
for the smaller subset. This procedure is repeated
ten times and the average results are reported.
For a given verb, each classifier created
throughout this process is tested on the same dou-
ble annotated and adjudicated held-out test set.
</bodyText>
<subsectionHeader confidence="0.860686">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.967727636363636">
Figure 1 shows a plot of the accuracy of the clas-
sifiers relative to the annotation investment for a
typical verb, to call. As can be seen, the accu-
racy is always higher when training on the larger
amount of single annotated data than when train-
ing on the amount of adjudicated data that had the
equivalent cost of annotation.
Figures 2 and 3 present results averaged over
all 215 verbs in the dataset. First, figure 2 shows
the average accuracy over all verbs by amount in-
vested. These accuracy curves are not smooth be-
</bodyText>
<page confidence="0.855637">
66
</page>
<figureCaption confidence="0.9486865">
Figure 1: Performance of single annotated vs. ad-
judicated data by amount invested for to call
</figureCaption>
<bodyText confidence="0.950357181818182">
cause the verbs all have a different number of total
instances. At various annotation cost values, all of
the instances of one or more verbs will have been
annotated. Hence, the accuracy values might jump
or drop by a larger amount than seen elsewhere in
the graph.
Toward the higher dollar amounts the curve is
dominated by fewer and fewer verbs. We only
display the dollar investments of up to $60 due to
the fact that only five verbs have more than $60’s
worth of instances in the training set.
</bodyText>
<figureCaption confidence="0.992783">
Figure 2: Average performance of single anno-
tated vs. adjudicated data by amount invested
</figureCaption>
<bodyText confidence="0.969829571428571">
The average difference in accuracy for Figure 2
across all amounts of investment is 1.64%.
Figure 3 presents the average accuracy relative
to the percent of the total cost to single annotate
all of the instances for a verb. The accuracy at a
given percent of total investment was interpolated
for each verb using linear interpolation and then
</bodyText>
<note confidence="0.477559">
averaged over all of the verbs.
</note>
<figureCaption confidence="0.905694636363636">
Figure 3: Average performance of single anno-
tated vs. adjudicated data by fraction of total in-
vestment
The average difference in accuracy for Figure 3
across each percent of investment is 2.10%.
Figure 4 presents essentially the same informa-
tion as Figure 2, but as a reduction in error rate for
single annotation relative to full adjudication.
Figure 4: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 2
</figureCaption>
<bodyText confidence="0.984064625">
The relative reduction in error rate averaged
over all investment amounts in Figure 2 is 7.77%.
Figure 5 presents the information in Figure 3
as a reduction in error rate for single annotation
relative to full adjudication.
The average relative reduction in error rate over
the fractions of total investment in Figure 5 is
9.32%.
</bodyText>
<page confidence="0.99824">
67
</page>
<figureCaption confidence="0.991690333333333">
Figure 5: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 3
</figureCaption>
<subsectionHeader confidence="0.68192">
3.3 Discussion
</subsectionHeader>
<figureCaption confidence="0.7559194">
First, it is worth noting that, when the amount of
annotated data is the same for both scenarios, ad-
judicated data leads to slightly better performance
than single annotated data. For example, consider
Figure 3. The accuracy at 100% of the total invest-
</figureCaption>
<bodyText confidence="0.978413852941176">
ment for the double annotation and adjudication
scenario is 81.13%. The same number of exam-
ples can be single annotated for 0.0833 / 0.1805 =
0.4615 of this dollar investment (using the costs
from Section 2.2). The system trained on that
amount of single annotated data shows a lower ac-
curacy, 80.21%. Thus, in this case, the adjudica-
tion scenario brings about a performance improve-
ment of about 1%.
However, the main thesis of this paper is that in-
stead of double annotating and adjudicating, it is
often better to single annotate more data because
it is a more cost-effective way to achieve a higher
performance. The results of our experiments sup-
port this thesis. At every dollar amount invested,
our supervised WSD system performs better when
trained on single annotated data comparing to dou-
ble annotated and adjudicated data.
The maximum annotation investment amount
for each verb is the cost of single annotating all
of its instances. When the system is trained on
the amount of double annotated data possible at
this investment, its accuracy is 81.13% (Figure 3).
When trained on single annotated data, the system
attains the same accuracy much earlier, at approxi-
mately 60% of the total investment. When trained
on the entire available single annotated data, the
system reaches an accuracy of 82.99%, nearly a
10% relative reduction in error rate over the same
system trained on the adjudicated data obtained for
the same cost.
Averaged over the 215 verbs, the single anno-
tation scenario outperformed adjudication at every
dollar amount investigated.
</bodyText>
<sectionHeader confidence="0.992412" genericHeader="method">
4 Experiment Two
</sectionHeader>
<bodyText confidence="0.997481419354839">
In this experiment, we consider the arguments of
Beigman and Klebanov (2009). They suggest that
data should be at least double annotated and then
filtered to discard all of the examples where there
were any annotator disagreements.
The main points of their argument are as fol-
lows. They first consider the data to be dividable
into two types, easy (to annotate) cases and hard
cases. Then they correctly note that some anno-
tators could have a systematic bias (i.e., could fa-
vor one label over others in certain types of hard
cases), which would in turn bias the learning of
the classifier. They show that it is theoretically
possible that a band of misclassified hard cases
running parallel to the true separating hyperplane
could mistakenly shift the decision boundary past
�
up to N easy cases.
We suggest that it is extremely unlikely that a
consequential number of easy cases would exist
nearer to the class boundary than the hard cases.
The hard cases are in fact generally considered to
define the separating hyperplane.
In this experiment, our goal is to determine how
the accuracy of classifiers trained on data labeled
according to Beigman and Klebanov’s discard dis-
agreements strategy compares empirically to the
accuracy resulting from single annotated data. As
in the previous experiment, this analysis is per-
formed relative to the investment in the annotation
effort.
</bodyText>
<subsectionHeader confidence="0.975328">
4.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.9994011">
We follow essentially the same experimental de-
sign described in section 3.1, using the same state
of the art verb WSD system. We conduct a num-
ber of experiments to compare the effect of single
annotated versus double annotated data. We uti-
lized the same training and test sets as the previous
experiment and similarly trained an SVM on frac-
tions of the data representing increments of $1.00
investments.
As before, the number of examples designated
</bodyText>
<page confidence="0.998691">
68
</page>
<bodyText confidence="0.999982611111111">
for single annotation is selected at random from
the training data and half of that subset is selected
as the training set for the double annotated data.
Again, selecting from the same subset of data re-
sults in a more accurate statistical comparison of
the models produced.
Classifiers for each annotation scenario are
trained on the labels from the first round of an-
notation, but examples where the second annota-
tor disagreed are thrown out of the double anno-
tated data. This results in slightly less than half as
much data in the double annotation scenario based
on the disagreement rate. Again, the procedure is
repeated ten times and the average results are re-
ported.
For a given verb, each classifier created
throughout this process is tested on the same dou-
ble annotated and adjudicated held-out test set.
</bodyText>
<subsectionHeader confidence="0.564439">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.99204425">
Figure 6 shows a plot of the accuracy of the classi-
fiers relative to the annotation investment for a typ-
ical verb, to call. As can be seen, the accuracy for
a specific investment performing single annotation
is always higher than it is for the same investment
in double annotated data.
points where a given verb is no longer included in
the average.
Toward the higher dollar amounts the curve is
dominated by fewer and fewer verbs. As before,
we only display the results for investments of up
to $60.
The average difference in accuracy for Figure 7
across all amounts of investment is 2.32%.
Figure 8 presents the average accuracy relative
to the percent of the total cost to single annotate
all of the instances for a verb. The accuracy at
a given percent of total investment was interpo-
lated for each verb and then averaged over all of
the verbs.
</bodyText>
<figureCaption confidence="0.9630536875">
Figure 6: Performance of single annotated vs.
double annotated data with disagreements dis-
carded by amount invested for to call
Figure 7: Average performance of single anno-
tated vs. double annotated data with disagree-
ments discarded by amount invested
Figures 7 and 8 present results averaged over
all 215 verbs in the dataset. First, figure 7 shows
the average accuracy over all verbs by amount
invested. Again, these accuracy curves are not
smooth because the verbs all have a different num-
ber of total instances. Hence, the accuracy val-
ues might jump or drop by a larger amount at the
Figure 8: Average performance of single anno-
tated vs. adjudicated data by fraction of total in-
vestment
</figureCaption>
<bodyText confidence="0.737614">
The average difference in accuracy for Figure 8
across all amounts of investment is 2.51%.
</bodyText>
<page confidence="0.997479">
69
</page>
<figureCaption confidence="0.818198272727273">
Figures 9 and 10 present this information as a
reduction in error rate for single annotation rela-
tive to full adjudication.
Figure 9: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 7
The relative reduction in error rate averaged
over all investment amounts in Figure 9 is 10.88%.
Figure 10: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 8
</figureCaption>
<bodyText confidence="0.999677666666667">
The average relative reduction in error rate over
the fractions of total investment in Figure 10 is
10.97%.
</bodyText>
<subsectionHeader confidence="0.994386">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99994512195122">
At every amount of investment, our supervised
WSD system performs better when trained on sin-
gle annotated data comparing to double annotated
data with discarded cases of disagreements.
The maximum annotation investment amount
for each verb is the cost of single annotating all
of its instances. When the system is trained on
the amount of double annotated data possible at
this investment, its accuracy is 80.78% (Figure 8).
When trained on single annotated data, the system
reaches the same accuracy much earlier, at approx-
imately 52% of the total investment. When trained
on the entire available single annotated data, the
system attains an accuracy of 82.99%, an 11.5%
relative reduction in error rate compared to the
same system trained on the double annotated data
obtained for the same cost.
The average accuracy of the single annotation
scenario outperforms the double annotated with
disagreements discarded scenario at every dollar
amount investigated.
While this empirical investigation only looked
at verb WSD, it was performed using 215 distinct
verb type datasets. These verbs each have con-
textual features that are essentially unique to that
verb type and consequently, 215 distinct classi-
fiers, one per verb type, are trained. Hence, these
could loosely be considered 215 distinct annota-
tion and classification tasks.
The fact that for the 215 classification tasks the
single annotation scenario on average performed
better than the discard disagreements scenario of
Beigman and Klebanov (2009) strongly suggests
that, while it is theoretically possible for annota-
tion bias to, in turn, bias a classifier’s learning, it
is more likely that you will achieve better results
by training on the single annotated data.
It is still an open issue whether it is generally
best to adjudicate disagreements in the test set or
to throw them out as suggested by (Beigman Kle-
banov and Beigman, 2009).
</bodyText>
<sectionHeader confidence="0.99786" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999563714285714">
We investigated 215 WSD classification tasks,
comparing performance under three annotation
scenarios each with the equivalent annotation cost,
single annotation, double annotation with dis-
agreements adjudicated, and double annotation
with disagreements discarded. Averaging over the
215 classification tasks, the system trained on sin-
gle annotated data achieved 10.0% and 11.5% rel-
ative reduction in error rates compared to training
on the equivalent investment in adjudicated and
disagreements discarded data, respectively. While
we believe these results will generalize to other an-
notation tasks, this is still an open question to be
determined by future work.
</bodyText>
<page confidence="0.992814">
70
</page>
<bodyText confidence="0.999987630434783">
There are probably similar issues in what were
considered fixed costs for the purposes of this pa-
per. For example, it may be possible to train fewer
annotators, and invest the savings into annotating
more data. Perhaps more appropriately, it may be
feasible to simply cut back on the amount of train-
ing provided per annotator and instead annotate
more data.
On the other hand, when the unlabeled data
is not freely obtainable, double annotation may
be more suitable as a route to improving system
performance. There may also be factors other
than cost-effectiveness which make double anno-
tation desirable. Many projects point to their ITA
rates and corresponding kappa values as a mea-
sure of annotation quality, and of the reliability of
the annotators (Artstein and Poesio, 2008). The
OntoNotes project used ITA rates as a way of eval-
uating the clarity of the sense inventory that was
being developed in parallel with the annotation.
Lexical entries that resulted in low ITA rates were
revised, usually improving the ITA rate. Calculat-
ing these rates requires double-blind annotation.
Annotators who consistently produced ITA rates
lower than average were also removed from the
project. Therefore, caution is advised in determin-
ing when to dispense with double annotation in fa-
vor of more cost effective single annotation.
Double annotation can also be used to shed light
on other research questions that, for example, re-
quire knowing which instances are ”hard.” That
knowledge may help with designing additional,
richer annotation layers or with cognitive science
investigations into human representations of lan-
guage.
Our results suggest that systems would likely
benefit more from the larger training datasets that
single annotation makes possible than from the
less noisy datasets resulting from adjudication.
Regardless of whether single or double annota-
tion with adjudication is used, there will always be
noise. Hence, we see the further investigation of
algorithms that generalize despite the presence of
noise to be critical to the future of computational
linguistics. Humans are able to learn in the pres-
ence of noise, and our systems must follow suit.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999920823529412">
Double annotated data contains less noise than
single annotated data and thus improves the per-
formance of supervised machine learning systems
that are trained on a specific amount of data. How-
ever, double annotation is expensive and the alter-
native of single annotating more data instead is on
the table for many annotation projects.
In this paper we compared the performance of
a supervised machine learning system trained on
double annotated data versus single annotated data
obtainable for the same cost. Our results clearly
demonstrate that single annotating more data can
be a more cost-effective way to improve the sys-
tem performance in the many cases where the un-
labeled data is freely available and there are no
other considerations that necessitate double anno-
tation.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998542090909091">
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the National Science Founda-
tion.
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997660285714286">
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555–596.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with annotation noise. In ACL-IJCNLP
’09: Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1, pages 280–287, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Com-
put. Linguist., 35(4):495–503.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what‘s
in a name. Mach. Learn., 34(1-3):211–231.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/˜cjlin/libsvm.
</reference>
<page confidence="0.980017">
71
</page>
<reference confidence="0.999347433333333">
J. Chen and M. Palmer. 2005. Towards robust high
performance word sense disambiguation of english
verbs using rich linguistic features. pages 933–944.
Springer.
Jinying Chen, Dmitriy Dligach, and Martha Palmer.
2007. Towards large-scale high-performance en-
glish verb sense disambiguation by using linguisti-
cally motivated features. In ICSC ’07: Proceed-
ings of the International Conference on Semantic
Computing, pages 378–388, Washington, DC, USA.
IEEE Computer Society.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
HLT ’08: Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies, pages 29–32, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Christiane Fellbaum. 1998. WordNet: An electronic
lexical database. MIT press Cambridge, MA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In NAACL ’06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57–60, Morristown, NJ, USA. Association for
Computational Linguistics.
A. Ratnaparkhi. 1998. Maximum entropy models for
natural language ambiguity resolution. Ph.D. the-
sis, University of Pennsylvania.
</reference>
<page confidence="0.998728">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.093849">
<title confidence="0.382416">To Annotate More Accurately or to Annotate More</title>
<author confidence="0.521705">Dmitriy</author>
<affiliation confidence="0.998143">Department of Computer University of Colorado at</affiliation>
<email confidence="0.82753">Dmitriy.Dligach@colorado.edu</email>
<author confidence="0.997689">D Rodney</author>
<affiliation confidence="0.987592666666667">The Center for Computational and Education University of Colorado at</affiliation>
<email confidence="0.349473">Rodney.Nielsen@colorado.edu</email>
<author confidence="0.984236">Martha</author>
<affiliation confidence="0.996632666666667">Department of Department of Computer University of Colorado at</affiliation>
<email confidence="0.985472">Martha.Palmer@colorado.edu</email>
<abstract confidence="0.9993243">The common accepted wisdom is that blind double annotation followed by adjudication of disagreements is necessary to create training and test corpora that result in the best possible performance. We provide evidence that this is unlikely to be the case. Rather, the greatest value for your annotation dollar lies in single annotating more data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="23288" citStr="Artstein and Poesio, 2008" startWordPosition="3853" endWordPosition="3856">ators, and invest the savings into annotating more data. Perhaps more appropriately, it may be feasible to simply cut back on the amount of training provided per annotator and instead annotate more data. On the other hand, when the unlabeled data is not freely obtainable, double annotation may be more suitable as a route to improving system performance. There may also be factors other than cost-effectiveness which make double annotation desirable. Many projects point to their ITA rates and corresponding kappa values as a measure of annotation quality, and of the reliability of the annotators (Artstein and Poesio, 2008). The OntoNotes project used ITA rates as a way of evaluating the clarity of the sense inventory that was being developed in parallel with the annotation. Lexical entries that resulted in low ITA rates were revised, usually improving the ITA rate. Calculating these rates requires double-blind annotation. Annotators who consistently produced ITA rates lower than average were also removed from the project. Therefore, caution is advised in determining when to dispense with double annotation in favor of more cost effective single annotation. Double annotation can also be used to shed light on othe</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist., 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Beigman</author>
<author>Beata Beigman Klebanov</author>
</authors>
<title>Learning with annotation noise.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>280--287</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2380" citStr="Beigman and Klebanov (2009)" startWordPosition="359" endWordPosition="362">resence of noise, especially when given larger amounts of training data. Currently, the commonly accepted wisdom sides with the view that says that blind double annotation followed by adjudication of disagreements is necessary to create annotated corpora that leads to the best possible performance. We provide empirical evidence that this is unlikely to be the case. Rather, the greatest value for your annotation dollar lies in single annotating more data. There may, however, be other considerations that still argue in favor of double annotation. In this paper, we also consider the arguments of Beigman and Klebanov (2009), who suggest that data should be multiply annotated and then filtered to discard all of the examples where the annotators do not have perfect agreement. We provide evidence that single annotating more data for the same cost is likely to result in better system performance. This paper proceeds as follows: first, we outline our evaluation framework in Section 2. Next, we compare the single annotation and adjudication scenarios in Section 3. Then, we compare the annotation scenario of Beigman and Klebanov (2009) with the single annotation scenario in Section 4. After that, we discuss the results</context>
<context position="15105" citStr="Beigman and Klebanov (2009)" startWordPosition="2506" endWordPosition="2509">nvestment, its accuracy is 81.13% (Figure 3). When trained on single annotated data, the system attains the same accuracy much earlier, at approximately 60% of the total investment. When trained on the entire available single annotated data, the system reaches an accuracy of 82.99%, nearly a 10% relative reduction in error rate over the same system trained on the adjudicated data obtained for the same cost. Averaged over the 215 verbs, the single annotation scenario outperformed adjudication at every dollar amount investigated. 4 Experiment Two In this experiment, we consider the arguments of Beigman and Klebanov (2009). They suggest that data should be at least double annotated and then filtered to discard all of the examples where there were any annotator disagreements. The main points of their argument are as follows. They first consider the data to be dividable into two types, easy (to annotate) cases and hard cases. Then they correctly note that some annotators could have a systematic bias (i.e., could favor one label over others in certain types of hard cases), which would in turn bias the learning of the classifier. They show that it is theoretically possible that a band of misclassified hard cases ru</context>
<context position="21419" citStr="Beigman and Klebanov (2009)" startWordPosition="3556" endWordPosition="3559">ated with disagreements discarded scenario at every dollar amount investigated. While this empirical investigation only looked at verb WSD, it was performed using 215 distinct verb type datasets. These verbs each have contextual features that are essentially unique to that verb type and consequently, 215 distinct classifiers, one per verb type, are trained. Hence, these could loosely be considered 215 distinct annotation and classification tasks. The fact that for the 215 classification tasks the single annotation scenario on average performed better than the discard disagreements scenario of Beigman and Klebanov (2009) strongly suggests that, while it is theoretically possible for annotation bias to, in turn, bias a classifier’s learning, it is more likely that you will achieve better results by training on the single annotated data. It is still an open issue whether it is generally best to adjudicate disagreements in the test set or to throw them out as suggested by (Beigman Klebanov and Beigman, 2009). 5 Discussion and Future Work We investigated 215 WSD classification tasks, comparing performance under three annotation scenarios each with the equivalent annotation cost, single annotation, double annotati</context>
</contexts>
<marker>Beigman, Klebanov, 2009</marker>
<rawString>Eyal Beigman and Beata Beigman Klebanov. 2009. Learning with annotation noise. In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1, pages 280–287, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
</authors>
<title>From annotator agreement to noise models.</title>
<date>2009</date>
<journal>Comput. Linguist.,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="21811" citStr="Klebanov and Beigman, 2009" startWordPosition="3623" endWordPosition="3627">nsidered 215 distinct annotation and classification tasks. The fact that for the 215 classification tasks the single annotation scenario on average performed better than the discard disagreements scenario of Beigman and Klebanov (2009) strongly suggests that, while it is theoretically possible for annotation bias to, in turn, bias a classifier’s learning, it is more likely that you will achieve better results by training on the single annotated data. It is still an open issue whether it is generally best to adjudicate disagreements in the test set or to throw them out as suggested by (Beigman Klebanov and Beigman, 2009). 5 Discussion and Future Work We investigated 215 WSD classification tasks, comparing performance under three annotation scenarios each with the equivalent annotation cost, single annotation, double annotation with disagreements adjudicated, and double annotation with disagreements discarded. Averaging over the 215 classification tasks, the system trained on single annotated data achieved 10.0% and 11.5% relative reduction in error rates compared to training on the equivalent investment in adjudicated and disagreements discarded data, respectively. While we believe these results will generali</context>
</contexts>
<marker>Klebanov, Beigman, 2009</marker>
<rawString>Beata Beigman Klebanov and Eyal Beigman. 2009. From annotator agreement to noise models. Comput. Linguist., 35(4):495–503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An algorithm that learns what‘s in a name.</title>
<date>1999</date>
<pages>34--1</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="8612" citStr="Bikel et al., 1999" startWordPosition="1408" endWordPosition="1411">adjunct - Presence of PP adjunct - Preposition word - Head word of the preposition’s NP argument Subordinate clause - Presence of subordinate clause Path - Parse tree path from target verb to neighboring words - Parse tree path from target verb to subject and object - Parse tree path from target verb to subordinate clause Subcat frame - Phrase structure rule expanding the target verb’s parent node in parse tree Table 2: Syntactic features such as subject and object. The semantic classes are approximated as • WordNet (Fellbaum, 1998) hypernyms • NE tags derived from the output of IdentiFinder (Bikel et al., 1999) • Dynamic dependency neighbors (Dligach and Palmer, 2008), which are extracted in an unsupervised way from a dependency-parsed corpus Our WSD system uses the Libsvm software package (Chang and Lin, 2001) for classification. We accepted the default options (C = 1 and linear kernel) when training our classifiers. As is the case with most WSD systems, we train a separate model per verb. 3 Experiment One The results of experiment one show that in these circumstances, better performance is achieved by single annotating more data than by deploing resources towards ensuring that the data is annotate</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. 1999. An algorithm that learns what‘s in a name. Mach. Learn., 34(1-3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.</title>
<date>2001</date>
<note>edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="8816" citStr="Chang and Lin, 2001" startWordPosition="1439" endWordPosition="1442"> words - Parse tree path from target verb to subject and object - Parse tree path from target verb to subordinate clause Subcat frame - Phrase structure rule expanding the target verb’s parent node in parse tree Table 2: Syntactic features such as subject and object. The semantic classes are approximated as • WordNet (Fellbaum, 1998) hypernyms • NE tags derived from the output of IdentiFinder (Bikel et al., 1999) • Dynamic dependency neighbors (Dligach and Palmer, 2008), which are extracted in an unsupervised way from a dependency-parsed corpus Our WSD system uses the Libsvm software package (Chang and Lin, 2001) for classification. We accepted the default options (C = 1 and linear kernel) when training our classifiers. As is the case with most WSD systems, we train a separate model per verb. 3 Experiment One The results of experiment one show that in these circumstances, better performance is achieved by single annotating more data than by deploing resources towards ensuring that the data is annotated more accurately through an adjudication process. 3.1 Experimental Design We conduct a number of experiments to compare the effect of single annotated versus adjudicated data on the accuracy of a state o</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu. edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>M Palmer</author>
</authors>
<title>Towards robust high performance word sense disambiguation of english verbs using rich linguistic features.</title>
<date>2005</date>
<pages>933--944</pages>
<publisher>Springer.</publisher>
<marker>Chen, Palmer, 2005</marker>
<rawString>J. Chen and M. Palmer. 2005. Towards robust high performance word sense disambiguation of english verbs using rich linguistic features. pages 933–944. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinying Chen</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Towards large-scale high-performance english verb sense disambiguation by using linguistically motivated features.</title>
<date>2007</date>
<booktitle>In ICSC ’07: Proceedings of the International Conference on Semantic Computing,</booktitle>
<pages>378--388</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="7140" citStr="Chen et al., 2007" startWordPosition="1156" endWordPosition="1159">nse Disambiguation For the experiments we conduct in this study, we needed a word sense disambiguation (WSD) system. Our WSD system is modeled after the stateof-the-art verb WSD system described in (Dligach and Palmer, 2008). We will briefly outline it here. We view WSD as a supervised learning problem. Each instance of the target verb is represented as a vector of binary features that indicate the presence (or absence) of the corresponding features in the neighborhood of the target verb. We utilize all of the linguistic features that were shown to be useful for disambiguating verb senses in (Chen et al., 2007). To extract the lexical features we POS-tag the sentence containing the target verb and the two surrounding sentences using MXPost software (Ratnaparkhi, 1998). All open class words (nouns, verbs, adjectives, and adverbs) in these sentences are included in our feature set. In addition to that, we use as features two words on each side of the target verb as well as their POS tags. To extract the syntactic features we parse the sentence containing the target verb with Bikel’s constituency parser and utilize a set of rules to identify the features in Table 2. Our semantic features represent the </context>
</contexts>
<marker>Chen, Dligach, Palmer, 2007</marker>
<rawString>Jinying Chen, Dmitriy Dligach, and Martha Palmer. 2007. Towards large-scale high-performance english verb sense disambiguation by using linguistically motivated features. In ICSC ’07: Proceedings of the International Conference on Semantic Computing, pages 378–388, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Novel semantic features for verb sense disambiguation.</title>
<date>2008</date>
<booktitle>In HLT ’08: Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies,</booktitle>
<pages>29--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6746" citStr="Dligach and Palmer, 2008" startWordPosition="1087" endWordPosition="1090">stance cost for double blind, adjudicated data is then computed as double the cost for single annotation plus the per instance cost of adjudication multiplied by the percent of disagreement, 14%, which is $0.1805. We leave an analysis of the extent to which the up front costs are truly fixed and whether they can be altered to result in more value for the dollar to future work. 2.3 Automatic Word Sense Disambiguation For the experiments we conduct in this study, we needed a word sense disambiguation (WSD) system. Our WSD system is modeled after the stateof-the-art verb WSD system described in (Dligach and Palmer, 2008). We will briefly outline it here. We view WSD as a supervised learning problem. Each instance of the target verb is represented as a vector of binary features that indicate the presence (or absence) of the corresponding features in the neighborhood of the target verb. We utilize all of the linguistic features that were shown to be useful for disambiguating verb senses in (Chen et al., 2007). To extract the lexical features we POS-tag the sentence containing the target verb and the two surrounding sentences using MXPost software (Ratnaparkhi, 1998). All open class words (nouns, verbs, adjectiv</context>
<context position="8670" citStr="Dligach and Palmer, 2008" startWordPosition="1416" endWordPosition="1419">- Head word of the preposition’s NP argument Subordinate clause - Presence of subordinate clause Path - Parse tree path from target verb to neighboring words - Parse tree path from target verb to subject and object - Parse tree path from target verb to subordinate clause Subcat frame - Phrase structure rule expanding the target verb’s parent node in parse tree Table 2: Syntactic features such as subject and object. The semantic classes are approximated as • WordNet (Fellbaum, 1998) hypernyms • NE tags derived from the output of IdentiFinder (Bikel et al., 1999) • Dynamic dependency neighbors (Dligach and Palmer, 2008), which are extracted in an unsupervised way from a dependency-parsed corpus Our WSD system uses the Libsvm software package (Chang and Lin, 2001) for classification. We accepted the default options (C = 1 and linear kernel) when training our classifiers. As is the case with most WSD systems, we train a separate model per verb. 3 Experiment One The results of experiment one show that in these circumstances, better performance is achieved by single annotating more data than by deploing resources towards ensuring that the data is annotated more accurately through an adjudication process. 3.1 Exp</context>
</contexts>
<marker>Dligach, Palmer, 2008</marker>
<rawString>Dmitriy Dligach and Martha Palmer. 2008. Novel semantic features for verb sense disambiguation. In HLT ’08: Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies, pages 29–32, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>MIT press</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8531" citStr="Fellbaum, 1998" startWordPosition="1395" endWordPosition="1396"> tag of the head word of subject and object NPs Voice - Passive or Active PP adjunct - Presence of PP adjunct - Preposition word - Head word of the preposition’s NP argument Subordinate clause - Presence of subordinate clause Path - Parse tree path from target verb to neighboring words - Parse tree path from target verb to subject and object - Parse tree path from target verb to subordinate clause Subcat frame - Phrase structure rule expanding the target verb’s parent node in parse tree Table 2: Syntactic features such as subject and object. The semantic classes are approximated as • WordNet (Fellbaum, 1998) hypernyms • NE tags derived from the output of IdentiFinder (Bikel et al., 1999) • Dynamic dependency neighbors (Dligach and Palmer, 2008), which are extracted in an unsupervised way from a dependency-parsed corpus Our WSD system uses the Libsvm software package (Chang and Lin, 2001) for classification. We accepted the default options (C = 1 and linear kernel) when training our classifiers. As is the case with most WSD systems, we train a separate model per verb. 3 Experiment One The results of experiment one show that in these circumstances, better performance is achieved by single annotatin</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An electronic lexical database. MIT press Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In NAACL ’06: Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1309" citStr="Hovy et al., 2006" startWordPosition="186" endWordPosition="189">e. Rather, the greatest value for your annotation dollar lies in single annotating more data. 1 Introduction In recent years, supervised learning has become the dominant paradigm in Natural Language Processing (NLP), thus making the creation of handannotated corpora a critically important task. A corpus where each instance is annotated by a single tagger unavoidably contains errors. To improve the quality of the data, an annotation project may choose to annotate each instance twice and adjudicate the disagreements, thus producing the (largely) error-free gold standard. For example, OntoNotes (Hovy et al., 2006), a large-scale annotation project, chose this option. However, given a virtually unlimited supply of unlabeled data and limited funding – a typical set of constraints in NLP – an annotation project must always face the realization that for the cost of double annotation, more than twice as much data can be single annotated. The philosophy behind this alternative says that modern machine learning algorithms can still generalize well in the presence of noise, especially when given larger amounts of training data. Currently, the commonly accepted wisdom sides with the view that says that blind do</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% solution. In NAACL ’06: Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX, pages 57–60, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum entropy models for natural language ambiguity resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="7300" citStr="Ratnaparkhi, 1998" startWordPosition="1182" endWordPosition="1183">-the-art verb WSD system described in (Dligach and Palmer, 2008). We will briefly outline it here. We view WSD as a supervised learning problem. Each instance of the target verb is represented as a vector of binary features that indicate the presence (or absence) of the corresponding features in the neighborhood of the target verb. We utilize all of the linguistic features that were shown to be useful for disambiguating verb senses in (Chen et al., 2007). To extract the lexical features we POS-tag the sentence containing the target verb and the two surrounding sentences using MXPost software (Ratnaparkhi, 1998). All open class words (nouns, verbs, adjectives, and adverbs) in these sentences are included in our feature set. In addition to that, we use as features two words on each side of the target verb as well as their POS tags. To extract the syntactic features we parse the sentence containing the target verb with Bikel’s constituency parser and utilize a set of rules to identify the features in Table 2. Our semantic features represent the semantic classes of the target verb’s syntactic arguments 65 Feature Explanation Subject and object - Presence of subject and object - Head word of subject and </context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>A. Ratnaparkhi. 1998. Maximum entropy models for natural language ambiguity resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>