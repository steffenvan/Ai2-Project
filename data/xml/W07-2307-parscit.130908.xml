<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012866">
<title confidence="0.9992375">
Evaluating algorithms for the Generation of Referring Expressions
using a balanced corpus
</title>
<author confidence="0.993034">
Albert Gatt and Ielka van der Sluis and Kees van Deemter
</author>
<affiliation confidence="0.9979745">
Department of Computing Science
University of Aberdeen
</affiliation>
<email confidence="0.997362">
{agatt,ivdsluis,kvdeemte}@csd.abdn.ac.uk
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931">
Despite being the focus of intensive research, eval-
uation of algorithms that generate referring expres-
sions is still in its infancy. We describe a corpus-
based evaluation methodology, applied to a number
of classic algorithms in this area. The methodology
focuses on balance and semantic transparency to
enable comparison of human and algorithmic out-
put. Although the Incremental Algorithm emerges
as the best match, we found that its dependency on
manually-set parameters makes its performance dif-
ficult to predict.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963695652174">
The current state of the art in the Generation of
Referring Expressions (GRE) is dominated by ver-
sions of the Incremental Algorithm (IA) of Dale
and Reiter (1995). Focusing on the generation of
“first-mention” definite descriptions, Dale and Re-
iter compared the IA to a number of its predecessors,
including a Full Brevity (FB) algorithm, which gen-
erates descriptions of minimal length, and a Greedy
algorithm (GR), which approximates Full Brevity
(Dale, 1989). In doing so, the authors focused on
Content Determination (CD, which is the purely se-
mantic part of GRE), and on a description’s ability to
identify a referent for a hearer. Under this problem
definition, GRE algorithms take as input a Knowl-
edge Base (KB), which lists domain entities and
their properties (often represented as attribute-value
pairs), together with a set of intended referents, R.
The output of CD is a distinguishing description of
R, that is, a logical form which distinguishes this set
from its distractors.
Dale and Reiter argued that the IA was a supe-
rior model, and predicted that it would be the bet-
ter match to human referential behaviour.1 This
was due in part to the way the IA searches for a
distinguishing description by performing gradient
descent along a predetermined list of domain at-
tributes, called the preference order, whose ranking
reflects general or domain-specific preferences (see
§4.1).
The Incremental Algorithm has served as a start-
ing point for later models (Horacek, 1997; Kelleher
and Kruijff, 2006), and has also served as a yard-
stick against which to compare other approaches
(Gardent, 2002; Jordan and Walker, 2005). De-
spite its influence, few empirical evaluations have
focused on the IA. Evaluation is even more desir-
able given the dependency of the algorithm on a
preference order, which can radically change its be-
haviour, so that in a domain with n attributes, there
are in principle n! different algorithms.
This paper is concerned with applying a corpus-
based methodology to evaluate content determina-
tion for GRE, comparing the three classic algorithms
that formed the basis for Dale and Reiter’s (1995)
contribution, adapted to also deal with pluralities
and gradable properties.
</bodyText>
<subsectionHeader confidence="0.801955">
1.1 Requirements for GRE evaluation
</subsectionHeader>
<bodyText confidence="0.977929272727273">
One of the problems with evaluating GRE is that
it interfaces with several other sub-tasks of NLG
including, among others, realisation and discourse
coherence (especially where anaphoric reference is
concerned). On the other hand, a large amount of
work in the area has focused on the semantic heart
of the problem. Given identification as the over-
1Dale and Reiter also observed that IA is computationally
more efficient than its competitors, although GR has only poly-
nomial complexity. Consistent with subsequent research, we
shall be de-emphasising complexity issues here.
</bodyText>
<page confidence="0.998981">
49
</page>
<bodyText confidence="0.99987425">
arching goal of such algorithms, a crucial ques-
tion concerns the extent to which their choice of
content from the available attributes for a referent
matches that produced by a speaker in a compa-
rable situation. This is the main focus of this pa-
per, whose evaluation methodology therefore tar-
gets content determination, abstracting away from
issues of lexical choice and realisation. A corpus-
based evaluation of a content determination GRE al-
gorithm requires a resource that satisfies the follow-
ing desiderata.
Semantic transparency: The human ‘gold stan-
dard’ descriptions in the corpus need to be paired
with a domain representation so that, as far as pos-
sible, an algorithm is exposed to the same domain
as an author. To evaluate content determination,
descriptions need to be semantically annotated, ab-
stacting away from variations in syntax and lexical-
isation. For example, the right-facing sofa and the
settee which is oriented towards the right are, from
the point of view of a content determination proce-
dure, semantically equivalent.
Pragmatic transparency: Ideally, the communica-
tive intention underlying corpus descriptions should
match those for which an algorithm was designed.
If an algorithm is primarily aimed at identification,
then human gold-standards should, as far as possi-
ble, be restricted to this intention.
Balance: To assess the extent to which an al-
gorithm matches human performance, the corpus
should contain an equal number of instances where
each attribute is required. Only in this way would
the claim that algorithm X matches humans on con-
tent y% of the time be reliable2.
These desiderata suggest that the way forward in
evaluation in this area is to design controlled studies
for corpus construction. The rest of this paper de-
scribes the construction of such a corpus, and the re-
sults of an evaluation that addressed the differences
between IA and its predecessors against human de-
scriptions in domains of varying complexity, con-
taining both singular and plural descriptions. The
study also aimed to contribute to a growing debate
in the NLG community, on the evaluation of NLG
</bodyText>
<footnote confidence="0.999469285714286">
2For example, the IA overspecifies descriptions by select-
ing attributes not strictly required for identification, because of
its preference order. A claim that this feature improves perfor-
mance implies that the relative priority of attributes is impor-
tant. To be reliable, such a claim would have to be made against
a corpus in which ‘preferred’ and ‘dispreferred’ attributes were
required the same number of times.
</footnote>
<bodyText confidence="0.989813333333333">
systems, arguing in favour of the careful construc-
tion of balanced and transparent corpora to serve as
resources for NLG.
</bodyText>
<sectionHeader confidence="0.999697" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999960783783784">
We are aware of three studies on GRE evaluation, all
of which compare the IA to some alternative models.
Two of these (Jordan and Walker, 2005; Gupta and
Stent, 2005) used the COCONUT dialogue corpus.
The third (Viethen and Dale, 2006) used a small cor-
pus collected in a monologue setting. These studies
meet the transparency requirements to different de-
grees. Though COCONUT dialogues were elicited
against a well-defined domain, Jordan (2000) has
emphasised that reference, in COCONUT, was often
intended to satisfy intentions over and above iden-
tification. Gupta and Stent used an evaluation met-
ric that included aspects of the syntactic structure of
descriptions (specifically, modifier placement), thus
arguably obscuring the role of content determina-
tion (CD).
Our approach is closest in spirit to that of Viethen
and Dale, who elicited descriptions from people in
a setting where identification was the sole commu-
nicative aim. However, in the case of the IA, the au-
thors averaged over 24 different preference orders,
potentially averaging over 24 very different incar-
nations of the algorithm and masking the impact of
any one order. Similarly, neither Jordan/Walker nor
Gupta/Stent are explicit about the determination of
the preference order for the IA in their studies. No
obvious attempts were made to make sure that the
corpora in question were semantically balanced.
One question that these studies raise relates to
how human-authored and automatically generated
descriptions should be compared. For instance, both
Jordan/Walker and Viethen/Dale use a measure of
recall. This indicates the coverage of an algorithm
in relation to a corpus, but does not measure the de-
gree of similarity between a description generated
by an algorithm and a description in the corpus,
punishing all mismatches with equal severity.
</bodyText>
<sectionHeader confidence="0.989887" genericHeader="method">
3 The TUNA corpus
</sectionHeader>
<bodyText confidence="0.999871714285714">
We built a corpus consisting of ca. 1800 descrip-
tions, collected through a controlled experiment run
over the web for three months. Half of this corpus
contains descriptions of real photographs of people;
the other half contains descriptions of artificially
constructed pictures of household items. In this pa-
per, the main focus is on the ‘furniture’ subcorpus,
</bodyText>
<page confidence="0.992054">
50
</page>
<table confidence="0.9990256">
TYPE COLOUR ORIENTATION SIZE
chair blue forward large
sofa red backward small
desk green leftward
fan grey rightward
</table>
<tableCaption confidence="0.999903">
Table 1: Non-numeric attributes in the domains
</tableCaption>
<bodyText confidence="0.9975245">
which represents the simpler of the two domains,
consisting of digitally constructed pictures of ob-
jects with well-defined properties. Therefore, it pro-
vides a good test case for the algorithms evaluated,
since it allows us to probe into a number of issues
that arise even with straightforwardly describable
objects. The ‘people’ sub-corpus is more complex,
since the objects are real photographs and afford an
author with many descriptive alternatives. We ex-
plicitly compare the results of the present evaluation
with a similar study on the ‘people’ sub-corpus, in
§5.
</bodyText>
<subsectionHeader confidence="0.99835">
3.1 Materials, design and procedure
</subsectionHeader>
<bodyText confidence="0.999264267605634">
The furniture sub-corpus consists of 900 descrip-
tions from 45 native or fluent speakers of English.
Participants described objects in 20 trials, each cor-
responding to a domain where there were one or
two clearly marked target referents (the target set)
and six distractor objects, placed in a 3 (row) x
5 (column) grid. Pictures of the objects represented
combinations of values of the four attributes shown
in the top panel of Table 1. In a pilot study involv-
ing 19 participants, we found that instances in which
descriptions used semantic content beyond that in-
dicated in the Table were extremely rare with these
simple objects. In each trial, the horizontal and ver-
tical position of the objects is represented using two
numeric-valued attributes, X-DIM (row) and Y-DIM
(column). Their value was randomly determined
with every fresh trial. Approximately half the cor-
pus descriptions include locative expressions3. We
will refer to this as the +LOC dataset, containing
412 descriptions from 26 authors. The other half,
the −LOC dataset (444 descriptions; 27 authors),
consists of descriptions using only COLOUR, SIZE
and ORIENTATION, apart from TYPE.
Participants were exposed to the 20 trials in ran-
domised order; in each case, they typed a descrip-
tion for the target set. They were told that they
3This was manipulated as a second, between-subjects fac-
tor. Participants were randomly placed in groups which varied
in whether they could use location or not, and in whether the
communicative situation was fault-critical or not. For more de-
tails, we refer to van Deemter et al. (2006).
would be interacting with a language-understanding
program which would remove the referents from
the domain, based on their description. Identifica-
tion was emphasised as the primary goal of descrip-
tions. Each time a participant submitted a descrip-
tion, one or two objects were automatically removed
from the domain by a function which had been pre-
set to remove the wrong objects on approximately
one-fourth of the trials. This was intended to make
the interaction seem more natural. We discuss an
evaluation of this methodology in §3.3.
The trials in the experiment were balanced. For
each possible combination of the attributes in Ta-
ble 1, there was an equal number of domains in
which an identifying description of the target(s) re-
quired the use of those attributes. We refer to this
as the minimal description (MD) of the target set.
For example, there was a domain in which a target
could be minimally distinguished by using COLOUR
and SIZE. TYPE was never included in the mini-
mal description, leaving 7 possible attribute combi-
nations. The experiment manipulated one within-
subjects variable, Cardinality/Similarity (3 levels):
Singular (SG): 7 domains contained a single refer-
ent
Plural/Similar (PS): 6 domains had two referents,
which had identical values on the MD attributes. For
example, both targets might be blue in a domain
where the minimally distinguishing description con-
sisted of COLOUR.
Plural/Dissimilar (PD): In the remaining 7 Plural
trials, the targets had different values of the mini-
mally distinguishing attributes.
Plural referents were taken into account because
plurality is pervasive in NL discourse. The litera-
ture (e.g. Gardent (2002)) suggests that they can be
treated adequately by minor variations of the classic
GRE algorithms (as long as the descriptions in ques-
tion refer distributively, cf. Stone (2000)), which is
something we considered worth testing.
</bodyText>
<subsectionHeader confidence="0.999806">
3.2 Corpus annotation
</subsectionHeader>
<bodyText confidence="0.999956333333333">
The XML annotation scheme (van der Sluis et al.,
2006) pairs each corpus description with a repre-
sentation of the domain in which it was produced.
The domain representation, exemplified in Figure
1(a)), indicates which entities are target referents or
distractors, and what combination of the attributes
and values in Table 1 they have, as well as their nu-
meric X-DIM and Y-DIM values (row and column
numbers).
</bodyText>
<page confidence="0.986839">
51
</page>
<figure confidence="0.99957162962963">
&lt;DESCRIPTION num=‘plural’&gt;
&lt;DESCRIPTION num=‘singular’&gt;
&lt;ATTRIBUTE name=‘size’ value=‘large’&gt;large&lt;/ATTRIBUTE&gt;
&lt;ATTRIBUTE name=‘type’ value=‘sofa’&gt;settee&lt;/ATTRIBUTE&gt;
&lt;ATTRIBUTE name=‘orientation’ value=‘right’&gt;
at oblique angle&lt;/ATTRIBUTE&gt;
&lt;/DESCRIPTION&gt;
and
&lt;DESCRIPTION num=‘singular’&gt;
&lt;ATTRIBUTE name=‘size’ value=‘small’&gt;small&lt;/ATTRIBUTE&gt;
&lt;ATTRIBUTE name=‘type’ value=‘desk’&gt;desk&lt;/ATTRIBUTE&gt;
&lt;/DESCRIPTION&gt;
&lt;/DESCRIPTION&gt;
(b) ‘large settee at oblique angle and small desk’
&lt;ENTITY type=target’&gt;
&lt;ATTRIBUTE name=‘orientation’ value=‘right’ /&gt;
&lt;ATTRIBUTE name=‘type’ value=‘sofa’ /&gt;
&lt;ATTRIBUTE name=‘size’ value=‘large’ /&gt;
...
&lt;/ENTITY&gt;
&lt;ENTITY type=‘target’&gt;
&lt;ATTRIBUTE name=‘colour’ value=‘red’ /&gt;
&lt;ATTRIBUTE name=‘type’ value=‘desk’ /&gt;
&lt;ATTRIBUTE name=‘size’ value=‘small’ /&gt;
...
&lt;/ENTITY&gt;
(a) Fragment of a domain
</figure>
<figureCaption confidence="0.999982">
Figure 1: Corpus annotation examples
</figureCaption>
<bodyText confidence="0.9944555">
Figure 1(b) shows the annotation of a plural de-
scription. ATTRIBUTE tags enclose segments of a
description corresponding to properties, with name
and value attributes which constitute a seman-
tic representation compatible with the domain, ab-
stracting away from lexical variation. For example,
in Figure 1(b), the expression at an oblique angle
is tagged as ORIENTATION, with the value right-
ward. If a part of a description could not be re-
solved against the domain representation, it was en-
closed in an ATTRIBUTE tag with the value other
for name. Consistent with our pilot study, this was
only necessary in 39 descriptions (3.2%).
The DESCRIPTION tag in Figure 1(b) indicates
the logical form of a description. Thus, Figure 1(b)
is a plural description enclosing two singular
ones. Correspondingly, the logical form of each
embedded description is a conjunction of attributes,
while the two sibling descriptions are disjoined:
(large n sofa n right) V (small n desk) .
</bodyText>
<subsectionHeader confidence="0.923505">
3.3 Annotator reliability and experimental
validity
</subsectionHeader>
<bodyText confidence="0.999976578947368">
The reliability of the annotation scheme was eval-
uated in a study involving two independent annota-
tors (hereafter A and B), both postgraduate students
with an interest in NLG, who used the same anno-
tation manual (van der Sluis et al., 2006). They
were given a stratified random sample of 270 de-
scriptions, 2 from each Cardinality/Similarity con-
dition, from each author in the corpus. To estimate
inter-annotator agreement, we compared their anno-
tations against the consensus labelling made by the
present authors, using a version of the Dice coef-
ficient. Let D1 and D2 be two descriptions, and
att(D) be the attributes in any description D. The
coefficient, which ranges between 0 (no agreement)
and 1 (perfect agreement) is calculated as in (1).
Because descriptions could contain more than one
instance of an attribute (e.g. Figure 1(b) contains
two instances of SIZE), the sets of attributes for this
comparison were represented as multisets.
</bodyText>
<equation confidence="0.9788875">
dice(D1,D2) = 2|att(D( )l
)l +latt(D2)l (1�
</equation>
<bodyText confidence="0.999951363636363">
In the present context, Dice is more appropriate
than agreement measures (such as the n statistic)
which rely on predefined categories in which dis-
crete events can be classified. The ‘events’ in the
corpus are NL expressions, each of which is ‘clas-
sified’ in several ways (depending on how many at-
tributes a description expresses), and it was up to
an annotator’s judgment, given the instructions, to
select those segments and mark them up.
Both annotators showed a high mean agreement
with the authors, as indicated by their mean and
modal (most frequent) scores (A:: mean = 0.93,
mode = 1 (74.4%); B: mean = 0.92; mode
= 1 (73%)). They also evinced substantial agree-
ment among themselves (mean = 0.89, mode =
1 (71.1%)). These results suggest that the anno-
tation scheme used is replicable to a high degree,
and that independent annotators are likely to pro-
duce very similar semantic markup.
In the evaluation study reported below, we use the
same measure to compare algorithm and human out-
put, because an optimally informative comparison
</bodyText>
<page confidence="0.992573">
52
</page>
<bodyText confidence="0.999984615384615">
should take into account the number of attributes
that an algorithm omits in relation to the human
gold standard, and the number of attributes that it in-
cludes. We also evaluated the validity of the experi-
mental setup. Since communicating with a machine
may have biased participants, they were asked, dur-
ing a debriefing phase, to assess the performance
of their virtual interlocutor, by indicating agreement
to the statement The system performed well on this
task. Of the 5 response categories, ranging from 1
strongly disagree to 5 (strongly agree), 34 individ-
uals selected agree or strongly agree while no one
selected strongly disagree. The mean score was 3.9.
</bodyText>
<sectionHeader confidence="0.997979" genericHeader="method">
4 Evaluating the algorithms
</sectionHeader>
<bodyText confidence="0.9991174">
The three algorithms mentioned in §1 can be charac-
terised as search problems (Bohnet and Dale, 2005)
which differ primarily in the way they structure a
search space populated by KB properties:
Full Brevity (FB): Finds the smallest distinguishing
combination of properties.
Greedy (GR): Adds properties to a description, al-
ways selecting the property with the greatest dis-
criminatory power.
Incremental (IA): Performs gradient descent along
a predefined list of properties. Like GR, IA incre-
mentally adds properties to a description until it is
distinguishing.
The evaluation was carried out separately for the
−LOC and +LOC datasets introduced in §3.1. Algo-
rithms were compared to a random baseline (RAND)
which selected a property randomly, and added it
to a description if it removed distractors and was
true of the referents. In the −LOC dataset, only
GR and IA were compared, because GR and FB give
identical output4. By contrast, the +LOC dataset,
where there are 5 attributes including X-DIM and Y-
DIM, and the values of the locative attributes were
randomly determined in all domains, there is much
greater scope for variation.
All four algorithms also included TYPE by de-
fault. Adding TYPE, despite its lack of contrastive
value, was the norm in the corpus descriptions
(93.5%). While the IA always adds TYPE, as pro-
posed by Dale and Reiter (1995), we applied the
</bodyText>
<footnote confidence="0.904237333333333">
4This is because objects were distinguishable on the basis
of three attributes. When only 1 or 2 attributes suffice to distin-
guish an object, GR and FB always return identical output. In
the case of 3 attributes, GR and FB are identical in the present
corpus because the minimal description consists of all the prop-
erties that have some discriminatory value.
</footnote>
<bodyText confidence="0.9998621875">
same trick to FB and GR to avoid penalising their
performance unnecessarily. In addition, we ex-
tended each algorithm in two ways:
Plurality: To cover the plural descriptions in the
corpus, we used the algorithm of (van Deemter,
2002), which is an extension to the IA. The algo-
rithm first searches through the KB to find a distin-
guishing conjunction of properties, failing which, it
searches through disjunctions of increasing length
until a distinguishing description is found. FB and
GR can easily be extended in the same way.
Gradable properties: Locative expressions in the
corpus are essentially gradable. For instance, the
table on the left could be used even if the table was
located in the right half of the grid, as long as it
was the leftmost table. van Deemter (2006) pro-
posed an algorithm to deal with such gradable prop-
erties, which can use any of the GRE algorithms (FB,
GR, IA). Gradable properties are represented in the
form (A = n), for example (X-DIM = 3) (i.e., the
property of being located in the middle column of
the grid). This equality is converted into a num-
ber of inequalities of the forms (X-DIM &gt; m) and
(X-DIM &lt; m’). For example, in a domain with 2
objects, in column 2 and 3, this results in the in-
equalities (X-DIM &gt; 2) and (X-DIM &lt; 4). Inequal-
ities are used by a GRE algorithm in the same way as
other properties. A postprocessing phase transforms
them into superlative form. For example, if a refer-
ent is identified by (TYPE : sofa) n (X-DIM &gt; 2),
this yields a combination expressible as “the right-
most sofa”, or “the sofa on the right”.
</bodyText>
<subsectionHeader confidence="0.993455">
4.1 Preference orders for the IA
</subsectionHeader>
<bodyText confidence="0.9999438">
In assessing the impact of preference orders on
the IA, we compare some psycholinguistically-
motivated versions to a baseline version which re-
verses the hypothesised trends. In what follows, we
denote a preference order using the first letter of the
attributes shown in Table 1.
Psycholinguists have shown that attributes such
as COLOUR are included in descriptions of objects
even when they are not required (Pechmann, 1989;
Eikmeyer and Ahls`en, 1996). Attributes such as
SIZE, which require comparison to other objects, are
more likely to be omitted (Belke and Meyer, 2002).
Based on this research, we hypothesise a ‘best’ pref-
erence order for the IA (IA-BESTi) in the −LOC
dataset, and a reverse baseline order (IA-BASET):
</bodyText>
<footnote confidence="0.362801">
IA-BEST1: C &gt;&gt; O &gt;&gt; S
</footnote>
<page confidence="0.996649">
53
</page>
<table confidence="0.999538571428571">
−LOC +LOC
IA-BEST1 IA-BASE1 GR/FB IA-BEST2 IA-BESTg IA-BEST4 IA-BASE2 FB GR
Mean .83 .75 .79 .64 .61 .63 .54 .57 .58
Mode 1 .67 .8 .67 .67 .67 .67 .67 .67
PRP 24.1 7.4 18.7 10 4.6 3.9 1.7 6.6 5.8
tS 7.002* −5.850* 3.333* 3.934* 2.313 3.406 .705 .242 .544
ti 4.632* −1.797 1.169 4.574* 3.352* 4.313* 1.776 1.286 1.900
</table>
<tableCaption confidence="0.693948">
Table 2: Comparison to the Random Baseline (∗p &lt; .05)
IA-BASE1: S &gt;&gt; O &gt;&gt; C
</tableCaption>
<bodyText confidence="0.9996105625">
In the more complex +LOC dataset, the inclusion
of the numeric-valued X-DIM and Y-DIM increases
the number of attributes to 5. Arts (2004) found
that locatives in the vertical dimension were much
more frequent than those in the horizontal (see also
Kelleher and Kruijff (2006)). Two different de-
scriptive patterns dominate her data: Either Y-DIM
and COLOUR are strongly preferred and X-DIM is
strongly dispreferred, or Y-DIM and X-DIM are both
highly preferred. This leaves us with three groups
of preference orders, namely CY &gt;&gt; {O,S} &gt;&gt;
X, YXC &gt;&gt; {O,S}, and Y,C &gt;&gt; {O,S} &gt;&gt;
X. Assuming that ORIENTATION preceeds SIZE
(which involves comparisons), three promising or-
ders emerge, with a baseline, IA-BASE2, which is
predicted to perform much worse.
</bodyText>
<listItem confidence="0.59562125">
IA-BEST2: C &gt;&gt; Y &gt;&gt; O &gt;&gt; S &gt;&gt; X
IA-BESTg: Y &gt;&gt; X &gt;&gt; C &gt;&gt; O &gt;&gt; S
IA-BEST4: Y &gt;&gt; C &gt;&gt; O &gt;&gt; S &gt;&gt; X
IA-BASE2: X &gt;&gt; O &gt;&gt; S &gt;&gt; Y &gt;&gt; C
</listItem>
<subsectionHeader confidence="0.991859">
4.2 Differences between algorithms
</subsectionHeader>
<bodyText confidence="0.999922245283019">
Table 2 displays mean and modal (most frequent)
scores of each algorithm, as well as the perfect re-
call percentage (PRP: the proportion of Dice scores
of 1). Pairwise t-tests comparing each algorithm
to RAND are reported using subjects (tS) and items
(tI) as sources of variance. These figures average
over all three Cardinality/Similarity conditions; we
return to the differences between these below.
With the exception of IA-BASE, the different ver-
sions of IA performed best on both datasets. In
the simpler −LOC dataset, IA-BEST1 achieved a
modal score of 1 24% of the time. Both the modal
score and the PRP of GR/FB were lower. Only IA-
BEST1 was significantly better than RAND both by
subjects and items. This suggests that while IA-
BEST1 reflects overall preferences, and increases the
likelihood with which a preferred attribute is in-
cluded in a description, a consideration of the rela-
tive discriminatory power of a property, or the over-
all brevity of a description, does not reflect human
tendencies.
A comparison of IA-BEST1 to FB/GR on this
dataset showed that the IA was significantly better,
though this only approached significance by items.
(tS = 2.972, p = .006; tI(19) = 2.117, p = .08).
Though this ostensibly supports the claim of Dale
and Reiter (Dale and Reiter, 1995), it should be dis-
cussed in the light of the performance of IA-BASE1,
which performed significantly worse than RAND by
subjects, as shown in Table 2, indicating a very sub-
stantial impact of the attribute order.
In the +LOC dataset, there is an overall decline in
performance. The much poorer performance of FB
and GR on this dataset (neither is better than RAND)
is due to their not selecting preferred attributes with
the same frequency as the better-performing orders
of the IA, since the chances of selecting them are
contingent on their discriminatory power.
A comparison of GR to FB revealed that the small
difference in their mean scores was not significant
(t1(24) = .773, ns; t2(19) = 1.455, ns). Pair-
wise contrasts involving IA-BEST2 showed that it
performed significantly better than both FB (tS =
4.235, p &lt; .05; tI = −2.539, ns) and GR (tS =
4.092, p &lt; .05; tI = 2.091, ns), though only
by subjects. This was also the case for IA-BEST4
against FB (tS = 3.845, p = .01; tI = 2.248, ns),
though not against GR (tS = 3.072, ns; tI = 1.723,
ns). None of the comparisons involving IA-BEST3
reached significance. Once again, the performance
of the IA on the more complex dataset displays a
strong dependency on the predetermined attribute
order.
</bodyText>
<subsectionHeader confidence="0.99582">
4.3 Plurals and similarity
</subsectionHeader>
<bodyText confidence="0.988644666666667">
The final part of the analysis considers the relative
performance of the algorithms on singular and plu-
ral data, focusing on the best IA in each dataset,
</bodyText>
<page confidence="0.997233">
54
</page>
<table confidence="0.999361571428572">
−LOC +LOC
IA-BEST1 GR IA-BEST2 GR
SG .92 .8 .71 .59
PS .80 .74 .59 .56
PD .79 .79 .59 .59
FS 50.367* 22.1* 11.098* 1.893
FI 40.025* 2.171 13.210 * * .611
</table>
<tableCaption confidence="0.999796">
Table 3: Effect of Cardinality/Similarity *p &lt; .001
</tableCaption>
<bodyText confidence="0.9996974">
IA-BEST1 and IA-BEST2, and on GR (which did not
differ from FB in +LOC). As Table 3 shows, the
algorithms’ performance declined dramatically on
the plural data; the difference between the Singu-
lar (SG), Plural Similar (PS) and Plural Dissimilar
(PD) domains is confirmed by a one-way ANOVA
with Cardinality/Similarity as independent variable,
though this is not significant for GR in +LOC.
With PS domains (where the minimally required
description is always a conjunction), van Deemter’s
algorithm will succeed at first pass, without need-
ing to search through combinations, except that a
disjunction is required for TYPE values (e.g. 2a,
below). People tend to be more redundant, be-
cause they partition a set if its elements have differ-
ent values of TYPE, describing each element sepa-
rately (2b). In the PD condition, the main problem is
that the notion of ‘preference’ becomes problematic
once the search space is populated by combinations
of attributes, rather than literals.
</bodyText>
<listItem confidence="0.743267">
(2) (a) (desk V fan) n red n large n forward
</listItem>
<bodyText confidence="0.688006">
(b) the large red desk facing forward and the
large red fan facing forward
</bodyText>
<sectionHeader confidence="0.997423" genericHeader="method">
5 The People Domain
</sectionHeader>
<bodyText confidence="0.999967490196078">
Like most work in GRE, the preceding results fo-
cus on very simple objects, with attributes such as
colour. With complex objects, the relevant proper-
ties are not always easy to ascertain. Similarly, we
expect less agreement between corpus annotators
and we expect GRE algorithms to perform worse on
complex domains, compared to those where objects
are simple and stylised. A separate study on the
‘people’ sub-corpus described in §3 was conducted,
using the same overall setup as the present study. In
this section, we briefly discuss our main findings in
this sub-corpus. A more detailed comparison of the
evaluation results on the furniture domain with par-
allel results on the people domain is reported else-
where.
The targets in the people corpus differ from their
distractors only in whether they had a beard (HAS-
BEARD), wore glasses (HASGLASSES) and/or were
young or old (AGE). But as expected, speakers used
other attributes than the ones that are necessary to
identify the photographed people. As a result de-
scriptions include, for instance, whether a referent
wears a tie, or has a certain hairstyle. To be able to
match the descriptions with the domain representa-
tion a total of 9 attributes were defined per photo-
graph. The first indication that complexity results
in much higher variation comes from results on an-
notator agreement on this data, with the same anno-
tators discussed in §3.3. Though again suggesting
a high degree of replicability, the figures indicate
greater difficulty in annotating complex data (A:
mean = .84, mode = 1 (41.1%); B: mean = .78;
mode = 1 (36.3%)).
Another problem is that complex objects, with
several attributes, give rise to several possible or-
ders, making it difficult to determine preference or-
ders for the IA a priori, particularly since, unlike
attributes such as COLOUR and SIZE, there is lit-
tle psycholinguistic data on reference with attributes
such as HASHAIR. Although in the ‘people’ domain
there exists a particular IA algorithm that performs
better than the GR algorithm, only a few of the pos-
sible preference orders yield significantly better re-
sults than GR. When comparing the mean scores of
the best IAs from both domains, the best IA in the
furniture domain performed much better than the
best IA in the people domain. Their mean scores dif-
fer substantially: while IA-BEST1 obtained a mean
of .83 on furniture descriptions, the best-performing
order on the ‘people’ corpus had a mean of .69, with
a lower recall percentage score of 21.3%.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999957533333333">
In recent years, GRE has extended considerably be-
yond what was seen as its remit a decade ago,
for example by taking linguistic context into ac-
count (Krahmer and Theune, 2002; Siddharthan and
Copestake, 2004). We have been conservative by
focusing on three classic algorithms discussed in
Dale and Reiter (1995), with straightforward exten-
sions to plurals and gradables.
We tested the Incremental Algorithm’s match
against speaker behaviour compared to other mod-
els using a a balanced, semantically and pragmat-
ically transparent corpus. It turns out that perfor-
mance depends on the preference order of the at-
tributes that are used by the IA. Preliminary indi-
cations from a study on a more complex sub-corpus
</bodyText>
<page confidence="0.995094">
55
</page>
<bodyText confidence="0.99994604">
support this view. This evaluation took a speaker-
oriented perspective. A reader-oriented perspective
might yield different results. This is our main target
for future follow-ups of this work.
One lesson to be drawn from this study is of a
practical nature. Suppose a GRE algorithm were re-
quired for an NLG system, to be deployed in a novel
domain. If the IA is the prime candidate, which pref-
erence order should be chosen? Psycholinguistic
principles can be good predictors, but an application
may involve attributes whose degree of preference
is unknown. Investigating how the subjects/authors
of interest behave requires time and resources, in
the absence of which, an algorithm like GR (suit-
ably adapted to make sure that the TYPE attribute is
represented) may be a better bet.
Finding correct preference orders is comparable
to a situation wherein a doctor has a choice of two
medicines with which to fight the flu. One of these
(nicknamed GR) produces reasonable results against
all variants of the flu; the success of the other (called
IA) depends crucially on a balancing of ingredients
that differs from case to case. Finding the right bal-
ance is an art rather than a science. – This, we feel,
is the situation in GRE today.
</bodyText>
<sectionHeader confidence="0.999293" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.992557">
Thanks to Richard Power, Ehud Reiter, Ross Turner,
Imtiaz Khan and Emiel Krahmer for helpful com-
ments. This work forms part of the TUNA project
(www.csd.abdn.ac.uk/research/tuna/),
supported by EPSRC grant GR/S13330/01.
</bodyText>
<sectionHeader confidence="0.999362" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944462686567">
A. Arts. 2004. Overspecification in Instructive
Texts. Ph.D. thesis, University of Tilburg.
E. Belke and A. Meyer. 2002. Tracking the time
course of multidimensional stimulus discrimina-
tion. European Journal of Cognitive Psychology,
14(2):237–266.
B. Bohnet and R. Dale. 2005. Viewing referring
expression generation as search. In Proc. IJCAI-
05.
R. Dale and E. Reiter. 1995. Computational inter-
pretation of the Gricean maxims in the genera-
tion of referring expressions. Cognitive Science,
19(8):233–263.
Robert Dale. 1989. Cooking up referring expres-
sions. In Proc. ACL-89.
H. J. Eikmeyer and E. Ahls`en. 1996. The cognitive
process of referring to an object. In Proc. 16th
Scandinavian Conference on Linguistics.
C. Gardent. 2002. Generating minimal definite de-
scriptions. In Proc. ACL-02.
S. Gupta and A. J. Stent. 2005. Automatic evalua-
tion of referring expression generation using cor-
pora. In Proc. 1st Workshop on Using Corpora in
NLG.
H. Horacek. 1997. An algorithm for generating ref-
erential descriptions with flexible interfaces. In
Proc. ACL-97.
P. W. Jordan and M. Walker. 2005. Learning con-
tent selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelli-
gence Research, 24:157–194.
P. W. Jordan. 2000. Influences on attribute selec-
tion in redescriptions: A corpus study. In Proc.
CogSci-00.
J. D. Kelleher and G-J Kruijff. 2006. Incremental
generation of spatial referring expressions in sit-
uated dialog. In Proc. ACL-COLING-06.
E. Krahmer and M. Theune. 2002. Efficient
context-sensitive generation of referring expres-
sions. In Kees van Deemter and Rodger Kibble,
editors, Information Sharing. Stanford: CSLI.
T. Pechmann. 1989. Incremental speech produc-
tion and referential overspecification. Linguis-
tics, 27:89–110.
A. Siddharthan and A. Copestake. 2004. Gener-
ating referring expressions in open domains. In
Proc. ACL-04.
M. Stone. 2000. On identifying sets. In Proc.
INLG-00.
K. van Deemter, I. van der Sluis, and A. Gatt. 2006.
Building a semantically transparent corpus for
the generation of referring expressions. In Proc.
INLG-06.
K. van Deemter. 2002. Generating referring ex-
pressions: Boolean extensions of the incremental
algorithm. Computational Linguistics, 28(1):37–
52.
K. van Deemter. 2006. Generating referring ex-
pressions that contain gradable properties. Com-
putational Linguistics. to appear.
I. van der Sluis, A. Gatt, and K. van Deemter.
2006. Manual for the TUNA corpus: Referring
expressions in two domains. Technical Report
AUCS/TR0705, University of Aberdeen.
J. Viethen and R. Dale. 2006. Algorithms for gen-
erating referring expressions: Do they do what
people do? In Proc. INLG-06.
</reference>
<page confidence="0.998424">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.659853">
<title confidence="0.99701">Evaluating algorithms for the Generation of Referring using a balanced corpus</title>
<author confidence="0.99906">Gatt van_der_Sluis van</author>
<affiliation confidence="0.996905">Department of Computing University of</affiliation>
<abstract confidence="0.971721833333333">being the focus of intensive research, evalalgorithms that generate referring expressions is still in its infancy. We describe a corpusbased evaluation methodology, applied to a number of classic algorithms in this area. The methodology on transparency enable comparison of human and algorithmic output. Although the Incremental Algorithm emerges as the best match, we found that its dependency on manually-set parameters makes its performance difficult to predict.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Arts</author>
</authors>
<title>Overspecification in Instructive Texts.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Tilburg.</institution>
<contexts>
<context position="22373" citStr="Arts (2004)" startWordPosition="3576" endWordPosition="3577">−LOC dataset, and a reverse baseline order (IA-BASET): IA-BEST1: C &gt;&gt; O &gt;&gt; S 53 −LOC +LOC IA-BEST1 IA-BASE1 GR/FB IA-BEST2 IA-BESTg IA-BEST4 IA-BASE2 FB GR Mean .83 .75 .79 .64 .61 .63 .54 .57 .58 Mode 1 .67 .8 .67 .67 .67 .67 .67 .67 PRP 24.1 7.4 18.7 10 4.6 3.9 1.7 6.6 5.8 tS 7.002* −5.850* 3.333* 3.934* 2.313 3.406 .705 .242 .544 ti 4.632* −1.797 1.169 4.574* 3.352* 4.313* 1.776 1.286 1.900 Table 2: Comparison to the Random Baseline (∗p &lt; .05) IA-BASE1: S &gt;&gt; O &gt;&gt; C In the more complex +LOC dataset, the inclusion of the numeric-valued X-DIM and Y-DIM increases the number of attributes to 5. Arts (2004) found that locatives in the vertical dimension were much more frequent than those in the horizontal (see also Kelleher and Kruijff (2006)). Two different descriptive patterns dominate her data: Either Y-DIM and COLOUR are strongly preferred and X-DIM is strongly dispreferred, or Y-DIM and X-DIM are both highly preferred. This leaves us with three groups of preference orders, namely CY &gt;&gt; {O,S} &gt;&gt; X, YXC &gt;&gt; {O,S}, and Y,C &gt;&gt; {O,S} &gt;&gt; X. Assuming that ORIENTATION preceeds SIZE (which involves comparisons), three promising orders emerge, with a baseline, IA-BASE2, which is predicted to perform m</context>
</contexts>
<marker>Arts, 2004</marker>
<rawString>A. Arts. 2004. Overspecification in Instructive Texts. Ph.D. thesis, University of Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Belke</author>
<author>A Meyer</author>
</authors>
<title>Tracking the time course of multidimensional stimulus discrimination.</title>
<date>2002</date>
<journal>European Journal of Cognitive Psychology,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="21666" citStr="Belke and Meyer, 2002" startWordPosition="3442" endWordPosition="3445">n the right”. 4.1 Preference orders for the IA In assessing the impact of preference orders on the IA, we compare some psycholinguisticallymotivated versions to a baseline version which reverses the hypothesised trends. In what follows, we denote a preference order using the first letter of the attributes shown in Table 1. Psycholinguists have shown that attributes such as COLOUR are included in descriptions of objects even when they are not required (Pechmann, 1989; Eikmeyer and Ahls`en, 1996). Attributes such as SIZE, which require comparison to other objects, are more likely to be omitted (Belke and Meyer, 2002). Based on this research, we hypothesise a ‘best’ preference order for the IA (IA-BESTi) in the −LOC dataset, and a reverse baseline order (IA-BASET): IA-BEST1: C &gt;&gt; O &gt;&gt; S 53 −LOC +LOC IA-BEST1 IA-BASE1 GR/FB IA-BEST2 IA-BESTg IA-BEST4 IA-BASE2 FB GR Mean .83 .75 .79 .64 .61 .63 .54 .57 .58 Mode 1 .67 .8 .67 .67 .67 .67 .67 .67 PRP 24.1 7.4 18.7 10 4.6 3.9 1.7 6.6 5.8 tS 7.002* −5.850* 3.333* 3.934* 2.313 3.406 .705 .242 .544 ti 4.632* −1.797 1.169 4.574* 3.352* 4.313* 1.776 1.286 1.900 Table 2: Comparison to the Random Baseline (∗p &lt; .05) IA-BASE1: S &gt;&gt; O &gt;&gt; C In the more complex +LOC datase</context>
</contexts>
<marker>Belke, Meyer, 2002</marker>
<rawString>E. Belke and A. Meyer. 2002. Tracking the time course of multidimensional stimulus discrimination. European Journal of Cognitive Psychology, 14(2):237–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
<author>R Dale</author>
</authors>
<title>Viewing referring expression generation as search.</title>
<date>2005</date>
<booktitle>In Proc. IJCAI05.</booktitle>
<contexts>
<context position="17856" citStr="Bohnet and Dale, 2005" startWordPosition="2794" endWordPosition="2797">aluated the validity of the experimental setup. Since communicating with a machine may have biased participants, they were asked, during a debriefing phase, to assess the performance of their virtual interlocutor, by indicating agreement to the statement The system performed well on this task. Of the 5 response categories, ranging from 1 strongly disagree to 5 (strongly agree), 34 individuals selected agree or strongly agree while no one selected strongly disagree. The mean score was 3.9. 4 Evaluating the algorithms The three algorithms mentioned in §1 can be characterised as search problems (Bohnet and Dale, 2005) which differ primarily in the way they structure a search space populated by KB properties: Full Brevity (FB): Finds the smallest distinguishing combination of properties. Greedy (GR): Adds properties to a description, always selecting the property with the greatest discriminatory power. Incremental (IA): Performs gradient descent along a predefined list of properties. Like GR, IA incrementally adds properties to a description until it is distinguishing. The evaluation was carried out separately for the −LOC and +LOC datasets introduced in §3.1. Algorithms were compared to a random baseline (</context>
</contexts>
<marker>Bohnet, Dale, 2005</marker>
<rawString>B. Bohnet and R. Dale. 2005. Viewing referring expression generation as search. In Proc. IJCAI05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<title>Computational interpretation of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>8</issue>
<contexts>
<context position="945" citStr="Dale and Reiter (1995)" startWordPosition="137" endWordPosition="140"> that generate referring expressions is still in its infancy. We describe a corpusbased evaluation methodology, applied to a number of classic algorithms in this area. The methodology focuses on balance and semantic transparency to enable comparison of human and algorithmic output. Although the Incremental Algorithm emerges as the best match, we found that its dependency on manually-set parameters makes its performance difficult to predict. 1 Introduction The current state of the art in the Generation of Referring Expressions (GRE) is dominated by versions of the Incremental Algorithm (IA) of Dale and Reiter (1995). Focusing on the generation of “first-mention” definite descriptions, Dale and Reiter compared the IA to a number of its predecessors, including a Full Brevity (FB) algorithm, which generates descriptions of minimal length, and a Greedy algorithm (GR), which approximates Full Brevity (Dale, 1989). In doing so, the authors focused on Content Determination (CD, which is the purely semantic part of GRE), and on a description’s ability to identify a referent for a hearer. Under this problem definition, GRE algorithms take as input a Knowledge Base (KB), which lists domain entities and their prope</context>
<context position="19110" citStr="Dale and Reiter (1995)" startWordPosition="2998" endWordPosition="3001">randomly, and added it to a description if it removed distractors and was true of the referents. In the −LOC dataset, only GR and IA were compared, because GR and FB give identical output4. By contrast, the +LOC dataset, where there are 5 attributes including X-DIM and YDIM, and the values of the locative attributes were randomly determined in all domains, there is much greater scope for variation. All four algorithms also included TYPE by default. Adding TYPE, despite its lack of contrastive value, was the norm in the corpus descriptions (93.5%). While the IA always adds TYPE, as proposed by Dale and Reiter (1995), we applied the 4This is because objects were distinguishable on the basis of three attributes. When only 1 or 2 attributes suffice to distinguish an object, GR and FB always return identical output. In the case of 3 attributes, GR and FB are identical in the present corpus because the minimal description consists of all the properties that have some discriminatory value. same trick to FB and GR to avoid penalising their performance unnecessarily. In addition, we extended each algorithm in two ways: Plurality: To cover the plural descriptions in the corpus, we used the algorithm of (van Deemt</context>
<context position="24433" citStr="Dale and Reiter, 1995" startWordPosition="3932" endWordPosition="3935">etter than RAND both by subjects and items. This suggests that while IABEST1 reflects overall preferences, and increases the likelihood with which a preferred attribute is included in a description, a consideration of the relative discriminatory power of a property, or the overall brevity of a description, does not reflect human tendencies. A comparison of IA-BEST1 to FB/GR on this dataset showed that the IA was significantly better, though this only approached significance by items. (tS = 2.972, p = .006; tI(19) = 2.117, p = .08). Though this ostensibly supports the claim of Dale and Reiter (Dale and Reiter, 1995), it should be discussed in the light of the performance of IA-BASE1, which performed significantly worse than RAND by subjects, as shown in Table 2, indicating a very substantial impact of the attribute order. In the +LOC dataset, there is an overall decline in performance. The much poorer performance of FB and GR on this dataset (neither is better than RAND) is due to their not selecting preferred attributes with the same frequency as the better-performing orders of the IA, since the chances of selecting them are contingent on their discriminatory power. A comparison of GR to FB revealed tha</context>
<context position="30017" citStr="Dale and Reiter (1995)" startWordPosition="4879" endWordPosition="4882">urniture domain performed much better than the best IA in the people domain. Their mean scores differ substantially: while IA-BEST1 obtained a mean of .83 on furniture descriptions, the best-performing order on the ‘people’ corpus had a mean of .69, with a lower recall percentage score of 21.3%. 6 Conclusions In recent years, GRE has extended considerably beyond what was seen as its remit a decade ago, for example by taking linguistic context into account (Krahmer and Theune, 2002; Siddharthan and Copestake, 2004). We have been conservative by focusing on three classic algorithms discussed in Dale and Reiter (1995), with straightforward extensions to plurals and gradables. We tested the Incremental Algorithm’s match against speaker behaviour compared to other models using a a balanced, semantically and pragmatically transparent corpus. It turns out that performance depends on the preference order of the attributes that are used by the IA. Preliminary indications from a study on a more complex sub-corpus 55 support this view. This evaluation took a speakeroriented perspective. A reader-oriented perspective might yield different results. This is our main target for future follow-ups of this work. One less</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>R. Dale and E. Reiter. 1995. Computational interpretation of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19(8):233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Cooking up referring expressions.</title>
<date>1989</date>
<booktitle>In Proc. ACL-89.</booktitle>
<contexts>
<context position="1243" citStr="Dale, 1989" startWordPosition="184" endWordPosition="185">lgorithm emerges as the best match, we found that its dependency on manually-set parameters makes its performance difficult to predict. 1 Introduction The current state of the art in the Generation of Referring Expressions (GRE) is dominated by versions of the Incremental Algorithm (IA) of Dale and Reiter (1995). Focusing on the generation of “first-mention” definite descriptions, Dale and Reiter compared the IA to a number of its predecessors, including a Full Brevity (FB) algorithm, which generates descriptions of minimal length, and a Greedy algorithm (GR), which approximates Full Brevity (Dale, 1989). In doing so, the authors focused on Content Determination (CD, which is the purely semantic part of GRE), and on a description’s ability to identify a referent for a hearer. Under this problem definition, GRE algorithms take as input a Knowledge Base (KB), which lists domain entities and their properties (often represented as attribute-value pairs), together with a set of intended referents, R. The output of CD is a distinguishing description of R, that is, a logical form which distinguishes this set from its distractors. Dale and Reiter argued that the IA was a superior model, and predicted</context>
</contexts>
<marker>Dale, 1989</marker>
<rawString>Robert Dale. 1989. Cooking up referring expressions. In Proc. ACL-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Eikmeyer</author>
<author>E Ahls`en</author>
</authors>
<title>The cognitive process of referring to an object.</title>
<date>1996</date>
<booktitle>In Proc. 16th Scandinavian Conference on Linguistics.</booktitle>
<marker>Eikmeyer, Ahls`en, 1996</marker>
<rawString>H. J. Eikmeyer and E. Ahls`en. 1996. The cognitive process of referring to an object. In Proc. 16th Scandinavian Conference on Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gardent</author>
</authors>
<title>Generating minimal definite descriptions.</title>
<date>2002</date>
<booktitle>In Proc. ACL-02.</booktitle>
<contexts>
<context position="2385" citStr="Gardent, 2002" startWordPosition="372" endWordPosition="373"> Dale and Reiter argued that the IA was a superior model, and predicted that it would be the better match to human referential behaviour.1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see §4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that formed the basis for Dale and Reiter’s (1995) contribution, adapted to also deal with pluralities and gradable properties. 1</context>
<context position="12490" citStr="Gardent (2002)" startWordPosition="1979" endWordPosition="1980">riment manipulated one withinsubjects variable, Cardinality/Similarity (3 levels): Singular (SG): 7 domains contained a single referent Plural/Similar (PS): 6 domains had two referents, which had identical values on the MD attributes. For example, both targets might be blue in a domain where the minimally distinguishing description consisted of COLOUR. Plural/Dissimilar (PD): In the remaining 7 Plural trials, the targets had different values of the minimally distinguishing attributes. Plural referents were taken into account because plurality is pervasive in NL discourse. The literature (e.g. Gardent (2002)) suggests that they can be treated adequately by minor variations of the classic GRE algorithms (as long as the descriptions in question refer distributively, cf. Stone (2000)), which is something we considered worth testing. 3.2 Corpus annotation The XML annotation scheme (van der Sluis et al., 2006) pairs each corpus description with a representation of the domain in which it was produced. The domain representation, exemplified in Figure 1(a)), indicates which entities are target referents or distractors, and what combination of the attributes and values in Table 1 they have, as well as the</context>
</contexts>
<marker>Gardent, 2002</marker>
<rawString>C. Gardent. 2002. Generating minimal definite descriptions. In Proc. ACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gupta</author>
<author>A J Stent</author>
</authors>
<title>Automatic evaluation of referring expression generation using corpora.</title>
<date>2005</date>
<booktitle>In Proc. 1st Workshop on Using Corpora in NLG.</booktitle>
<contexts>
<context position="6428" citStr="Gupta and Stent, 2005" startWordPosition="1017" endWordPosition="1020"> identification, because of its preference order. A claim that this feature improves performance implies that the relative priority of attributes is important. To be reliable, such a claim would have to be made against a corpus in which ‘preferred’ and ‘dispreferred’ attributes were required the same number of times. systems, arguing in favour of the careful construction of balanced and transparent corpora to serve as resources for NLG. 2 Related work We are aware of three studies on GRE evaluation, all of which compare the IA to some alternative models. Two of these (Jordan and Walker, 2005; Gupta and Stent, 2005) used the COCONUT dialogue corpus. The third (Viethen and Dale, 2006) used a small corpus collected in a monologue setting. These studies meet the transparency requirements to different degrees. Though COCONUT dialogues were elicited against a well-defined domain, Jordan (2000) has emphasised that reference, in COCONUT, was often intended to satisfy intentions over and above identification. Gupta and Stent used an evaluation metric that included aspects of the syntactic structure of descriptions (specifically, modifier placement), thus arguably obscuring the role of content determination (CD).</context>
</contexts>
<marker>Gupta, Stent, 2005</marker>
<rawString>S. Gupta and A. J. Stent. 2005. Automatic evaluation of referring expression generation using corpora. In Proc. 1st Workshop on Using Corpora in NLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Horacek</author>
</authors>
<title>An algorithm for generating referential descriptions with flexible interfaces.</title>
<date>1997</date>
<booktitle>In Proc. ACL-97.</booktitle>
<contexts>
<context position="2263" citStr="Horacek, 1997" startWordPosition="352" endWordPosition="353">put of CD is a distinguishing description of R, that is, a logical form which distinguishes this set from its distractors. Dale and Reiter argued that the IA was a superior model, and predicted that it would be the better match to human referential behaviour.1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see §4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that fo</context>
</contexts>
<marker>Horacek, 1997</marker>
<rawString>H. Horacek. 1997. An algorithm for generating referential descriptions with flexible interfaces. In Proc. ACL-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jordan</author>
<author>M Walker</author>
</authors>
<title>Learning content selection rules for generating object descriptions in dialogue.</title>
<date>2005</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>24--157</pages>
<contexts>
<context position="2411" citStr="Jordan and Walker, 2005" startWordPosition="374" endWordPosition="377">r argued that the IA was a superior model, and predicted that it would be the better match to human referential behaviour.1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see §4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that formed the basis for Dale and Reiter’s (1995) contribution, adapted to also deal with pluralities and gradable properties. 1.1 Requirements for GRE ev</context>
<context position="6404" citStr="Jordan and Walker, 2005" startWordPosition="1013" endWordPosition="1016">not strictly required for identification, because of its preference order. A claim that this feature improves performance implies that the relative priority of attributes is important. To be reliable, such a claim would have to be made against a corpus in which ‘preferred’ and ‘dispreferred’ attributes were required the same number of times. systems, arguing in favour of the careful construction of balanced and transparent corpora to serve as resources for NLG. 2 Related work We are aware of three studies on GRE evaluation, all of which compare the IA to some alternative models. Two of these (Jordan and Walker, 2005; Gupta and Stent, 2005) used the COCONUT dialogue corpus. The third (Viethen and Dale, 2006) used a small corpus collected in a monologue setting. These studies meet the transparency requirements to different degrees. Though COCONUT dialogues were elicited against a well-defined domain, Jordan (2000) has emphasised that reference, in COCONUT, was often intended to satisfy intentions over and above identification. Gupta and Stent used an evaluation metric that included aspects of the syntactic structure of descriptions (specifically, modifier placement), thus arguably obscuring the role of con</context>
</contexts>
<marker>Jordan, Walker, 2005</marker>
<rawString>P. W. Jordan and M. Walker. 2005. Learning content selection rules for generating object descriptions in dialogue. Journal of Artificial Intelligence Research, 24:157–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jordan</author>
</authors>
<title>Influences on attribute selection in redescriptions: A corpus study.</title>
<date>2000</date>
<booktitle>In Proc. CogSci-00.</booktitle>
<contexts>
<context position="6706" citStr="Jordan (2000)" startWordPosition="1061" endWordPosition="1062">uired the same number of times. systems, arguing in favour of the careful construction of balanced and transparent corpora to serve as resources for NLG. 2 Related work We are aware of three studies on GRE evaluation, all of which compare the IA to some alternative models. Two of these (Jordan and Walker, 2005; Gupta and Stent, 2005) used the COCONUT dialogue corpus. The third (Viethen and Dale, 2006) used a small corpus collected in a monologue setting. These studies meet the transparency requirements to different degrees. Though COCONUT dialogues were elicited against a well-defined domain, Jordan (2000) has emphasised that reference, in COCONUT, was often intended to satisfy intentions over and above identification. Gupta and Stent used an evaluation metric that included aspects of the syntactic structure of descriptions (specifically, modifier placement), thus arguably obscuring the role of content determination (CD). Our approach is closest in spirit to that of Viethen and Dale, who elicited descriptions from people in a setting where identification was the sole communicative aim. However, in the case of the IA, the authors averaged over 24 different preference orders, potentially averagin</context>
</contexts>
<marker>Jordan, 2000</marker>
<rawString>P. W. Jordan. 2000. Influences on attribute selection in redescriptions: A corpus study. In Proc. CogSci-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Kelleher</author>
<author>G-J Kruijff</author>
</authors>
<title>Incremental generation of spatial referring expressions in situated dialog.</title>
<date>2006</date>
<booktitle>In Proc. ACL-COLING-06.</booktitle>
<contexts>
<context position="2292" citStr="Kelleher and Kruijff, 2006" startWordPosition="354" endWordPosition="357">distinguishing description of R, that is, a logical form which distinguishes this set from its distractors. Dale and Reiter argued that the IA was a superior model, and predicted that it would be the better match to human referential behaviour.1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see §4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that formed the basis for Dale and R</context>
<context position="22511" citStr="Kelleher and Kruijff (2006)" startWordPosition="3596" endWordPosition="3599"> IA-BESTg IA-BEST4 IA-BASE2 FB GR Mean .83 .75 .79 .64 .61 .63 .54 .57 .58 Mode 1 .67 .8 .67 .67 .67 .67 .67 .67 PRP 24.1 7.4 18.7 10 4.6 3.9 1.7 6.6 5.8 tS 7.002* −5.850* 3.333* 3.934* 2.313 3.406 .705 .242 .544 ti 4.632* −1.797 1.169 4.574* 3.352* 4.313* 1.776 1.286 1.900 Table 2: Comparison to the Random Baseline (∗p &lt; .05) IA-BASE1: S &gt;&gt; O &gt;&gt; C In the more complex +LOC dataset, the inclusion of the numeric-valued X-DIM and Y-DIM increases the number of attributes to 5. Arts (2004) found that locatives in the vertical dimension were much more frequent than those in the horizontal (see also Kelleher and Kruijff (2006)). Two different descriptive patterns dominate her data: Either Y-DIM and COLOUR are strongly preferred and X-DIM is strongly dispreferred, or Y-DIM and X-DIM are both highly preferred. This leaves us with three groups of preference orders, namely CY &gt;&gt; {O,S} &gt;&gt; X, YXC &gt;&gt; {O,S}, and Y,C &gt;&gt; {O,S} &gt;&gt; X. Assuming that ORIENTATION preceeds SIZE (which involves comparisons), three promising orders emerge, with a baseline, IA-BASE2, which is predicted to perform much worse. IA-BEST2: C &gt;&gt; Y &gt;&gt; O &gt;&gt; S &gt;&gt; X IA-BESTg: Y &gt;&gt; X &gt;&gt; C &gt;&gt; O &gt;&gt; S IA-BEST4: Y &gt;&gt; C &gt;&gt; O &gt;&gt; S &gt;&gt; X IA-BASE2: X &gt;&gt; O &gt;&gt; S &gt;&gt; Y &gt;&gt; C</context>
</contexts>
<marker>Kelleher, Kruijff, 2006</marker>
<rawString>J. D. Kelleher and G-J Kruijff. 2006. Incremental generation of spatial referring expressions in situated dialog. In Proc. ACL-COLING-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>M Theune</author>
</authors>
<title>Efficient context-sensitive generation of referring expressions.</title>
<date>2002</date>
<booktitle>In Kees van Deemter</booktitle>
<editor>and Rodger Kibble, editors,</editor>
<publisher>CSLI.</publisher>
<location>Stanford:</location>
<contexts>
<context position="29880" citStr="Krahmer and Theune, 2002" startWordPosition="4859" endWordPosition="4862">e orders yield significantly better results than GR. When comparing the mean scores of the best IAs from both domains, the best IA in the furniture domain performed much better than the best IA in the people domain. Their mean scores differ substantially: while IA-BEST1 obtained a mean of .83 on furniture descriptions, the best-performing order on the ‘people’ corpus had a mean of .69, with a lower recall percentage score of 21.3%. 6 Conclusions In recent years, GRE has extended considerably beyond what was seen as its remit a decade ago, for example by taking linguistic context into account (Krahmer and Theune, 2002; Siddharthan and Copestake, 2004). We have been conservative by focusing on three classic algorithms discussed in Dale and Reiter (1995), with straightforward extensions to plurals and gradables. We tested the Incremental Algorithm’s match against speaker behaviour compared to other models using a a balanced, semantically and pragmatically transparent corpus. It turns out that performance depends on the preference order of the attributes that are used by the IA. Preliminary indications from a study on a more complex sub-corpus 55 support this view. This evaluation took a speakeroriented persp</context>
</contexts>
<marker>Krahmer, Theune, 2002</marker>
<rawString>E. Krahmer and M. Theune. 2002. Efficient context-sensitive generation of referring expressions. In Kees van Deemter and Rodger Kibble, editors, Information Sharing. Stanford: CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pechmann</author>
</authors>
<title>Incremental speech production and referential overspecification.</title>
<date>1989</date>
<journal>Linguistics,</journal>
<pages>27--89</pages>
<contexts>
<context position="21514" citStr="Pechmann, 1989" startWordPosition="3420" endWordPosition="3421">ample, if a referent is identified by (TYPE : sofa) n (X-DIM &gt; 2), this yields a combination expressible as “the rightmost sofa”, or “the sofa on the right”. 4.1 Preference orders for the IA In assessing the impact of preference orders on the IA, we compare some psycholinguisticallymotivated versions to a baseline version which reverses the hypothesised trends. In what follows, we denote a preference order using the first letter of the attributes shown in Table 1. Psycholinguists have shown that attributes such as COLOUR are included in descriptions of objects even when they are not required (Pechmann, 1989; Eikmeyer and Ahls`en, 1996). Attributes such as SIZE, which require comparison to other objects, are more likely to be omitted (Belke and Meyer, 2002). Based on this research, we hypothesise a ‘best’ preference order for the IA (IA-BESTi) in the −LOC dataset, and a reverse baseline order (IA-BASET): IA-BEST1: C &gt;&gt; O &gt;&gt; S 53 −LOC +LOC IA-BEST1 IA-BASE1 GR/FB IA-BEST2 IA-BESTg IA-BEST4 IA-BASE2 FB GR Mean .83 .75 .79 .64 .61 .63 .54 .57 .58 Mode 1 .67 .8 .67 .67 .67 .67 .67 .67 PRP 24.1 7.4 18.7 10 4.6 3.9 1.7 6.6 5.8 tS 7.002* −5.850* 3.333* 3.934* 2.313 3.406 .705 .242 .544 ti 4.632* −1.797 </context>
</contexts>
<marker>Pechmann, 1989</marker>
<rawString>T. Pechmann. 1989. Incremental speech production and referential overspecification. Linguistics, 27:89–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
<author>A Copestake</author>
</authors>
<title>Generating referring expressions in open domains. In</title>
<date>2004</date>
<booktitle>Proc. ACL-04.</booktitle>
<contexts>
<context position="29914" citStr="Siddharthan and Copestake, 2004" startWordPosition="4863" endWordPosition="4866">ly better results than GR. When comparing the mean scores of the best IAs from both domains, the best IA in the furniture domain performed much better than the best IA in the people domain. Their mean scores differ substantially: while IA-BEST1 obtained a mean of .83 on furniture descriptions, the best-performing order on the ‘people’ corpus had a mean of .69, with a lower recall percentage score of 21.3%. 6 Conclusions In recent years, GRE has extended considerably beyond what was seen as its remit a decade ago, for example by taking linguistic context into account (Krahmer and Theune, 2002; Siddharthan and Copestake, 2004). We have been conservative by focusing on three classic algorithms discussed in Dale and Reiter (1995), with straightforward extensions to plurals and gradables. We tested the Incremental Algorithm’s match against speaker behaviour compared to other models using a a balanced, semantically and pragmatically transparent corpus. It turns out that performance depends on the preference order of the attributes that are used by the IA. Preliminary indications from a study on a more complex sub-corpus 55 support this view. This evaluation took a speakeroriented perspective. A reader-oriented perspect</context>
</contexts>
<marker>Siddharthan, Copestake, 2004</marker>
<rawString>A. Siddharthan and A. Copestake. 2004. Generating referring expressions in open domains. In Proc. ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
</authors>
<title>On identifying sets.</title>
<date>2000</date>
<booktitle>In Proc. INLG-00.</booktitle>
<contexts>
<context position="12666" citStr="Stone (2000)" startWordPosition="2007" endWordPosition="2008">erents, which had identical values on the MD attributes. For example, both targets might be blue in a domain where the minimally distinguishing description consisted of COLOUR. Plural/Dissimilar (PD): In the remaining 7 Plural trials, the targets had different values of the minimally distinguishing attributes. Plural referents were taken into account because plurality is pervasive in NL discourse. The literature (e.g. Gardent (2002)) suggests that they can be treated adequately by minor variations of the classic GRE algorithms (as long as the descriptions in question refer distributively, cf. Stone (2000)), which is something we considered worth testing. 3.2 Corpus annotation The XML annotation scheme (van der Sluis et al., 2006) pairs each corpus description with a representation of the domain in which it was produced. The domain representation, exemplified in Figure 1(a)), indicates which entities are target referents or distractors, and what combination of the attributes and values in Table 1 they have, as well as their numeric X-DIM and Y-DIM values (row and column numbers). 51 &lt;DESCRIPTION num=‘plural’&gt; &lt;DESCRIPTION num=‘singular’&gt; &lt;ATTRIBUTE name=‘size’ value=‘large’&gt;large&lt;/ATTRIBUTE&gt; &lt;A</context>
</contexts>
<marker>Stone, 2000</marker>
<rawString>M. Stone. 2000. On identifying sets. In Proc. INLG-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
<author>I van der Sluis</author>
<author>A Gatt</author>
</authors>
<title>Building a semantically transparent corpus for the generation of referring expressions.</title>
<date>2006</date>
<booktitle>In Proc. INLG-06.</booktitle>
<marker>van Deemter, van der Sluis, Gatt, 2006</marker>
<rawString>K. van Deemter, I. van der Sluis, and A. Gatt. 2006. Building a semantically transparent corpus for the generation of referring expressions. In Proc. INLG-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
</authors>
<title>Generating referring expressions: Boolean extensions of the incremental algorithm.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>52</pages>
<marker>van Deemter, 2002</marker>
<rawString>K. van Deemter. 2002. Generating referring expressions: Boolean extensions of the incremental algorithm. Computational Linguistics, 28(1):37– 52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
</authors>
<title>Generating referring expressions that contain gradable properties. Computational Linguistics.</title>
<date>2006</date>
<note>to appear.</note>
<marker>van Deemter, 2006</marker>
<rawString>K. van Deemter. 2006. Generating referring expressions that contain gradable properties. Computational Linguistics. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I van der Sluis</author>
<author>A Gatt</author>
<author>K van Deemter</author>
</authors>
<title>Manual for the TUNA corpus: Referring expressions in two domains.</title>
<date>2006</date>
<tech>Technical Report AUCS/TR0705,</tech>
<institution>University of Aberdeen.</institution>
<marker>van der Sluis, Gatt, van Deemter, 2006</marker>
<rawString>I. van der Sluis, A. Gatt, and K. van Deemter. 2006. Manual for the TUNA corpus: Referring expressions in two domains. Technical Report AUCS/TR0705, University of Aberdeen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Viethen</author>
<author>R Dale</author>
</authors>
<title>Algorithms for generating referring expressions: Do they do what people do? In</title>
<date>2006</date>
<booktitle>Proc. INLG-06.</booktitle>
<contexts>
<context position="6497" citStr="Viethen and Dale, 2006" startWordPosition="1028" endWordPosition="1031">feature improves performance implies that the relative priority of attributes is important. To be reliable, such a claim would have to be made against a corpus in which ‘preferred’ and ‘dispreferred’ attributes were required the same number of times. systems, arguing in favour of the careful construction of balanced and transparent corpora to serve as resources for NLG. 2 Related work We are aware of three studies on GRE evaluation, all of which compare the IA to some alternative models. Two of these (Jordan and Walker, 2005; Gupta and Stent, 2005) used the COCONUT dialogue corpus. The third (Viethen and Dale, 2006) used a small corpus collected in a monologue setting. These studies meet the transparency requirements to different degrees. Though COCONUT dialogues were elicited against a well-defined domain, Jordan (2000) has emphasised that reference, in COCONUT, was often intended to satisfy intentions over and above identification. Gupta and Stent used an evaluation metric that included aspects of the syntactic structure of descriptions (specifically, modifier placement), thus arguably obscuring the role of content determination (CD). Our approach is closest in spirit to that of Viethen and Dale, who e</context>
</contexts>
<marker>Viethen, Dale, 2006</marker>
<rawString>J. Viethen and R. Dale. 2006. Algorithms for generating referring expressions: Do they do what people do? In Proc. INLG-06.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>