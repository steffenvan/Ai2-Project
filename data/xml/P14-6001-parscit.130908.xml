<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.059960">
<title confidence="0.981917">
Gaussian Processes for Natural Language Processing
</title>
<note confidence="0.678425333333333">
Trevor Cohn
Computing and Information Systems
The University of Melbourne
</note>
<email confidence="0.594681">
trevor.cohn@gmail.com
</email>
<author confidence="0.98209">
Daniel Preot¸iuc-Pietro and Neil Lawrence
</author>
<affiliation confidence="0.8826335">
Department of Computer Science
The University of Sheffield
</affiliation>
<email confidence="0.990265">
{daniel,n.lawrence}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.999482" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999674621621622">
Gaussian Processes (GPs) are a powerful mod-
elling framework incorporating kernels and
Bayesian inference, and are recognised as state-
of-the-art for many machine learning tasks.
Despite this, GPs have seen few applications in
natural language processing (notwithstanding
several recent papers by the authors). We argue
that the GP framework offers many benefits over
commonly used machine learning frameworks,
such as linear models (logistic regression, least
squares regression) and support vector machines.
Moreover, GPs are extremely flexible and can
be incorporated into larger graphical models,
forming an important additional tool for proba-
bilistic inference. Notably, GPs are one of the
few models which support analytic Bayesian in-
ference, avoiding the many approximation errors
that plague approximate inference techniques in
common use for Bayesian models (e.g. MCMC,
variational Bayes).1 GPs accurately model not
just the underlying task, but also the uncertainty
in the predictions, such that uncertainty can be
propagated through pipelines of probabilistic
components. Overall, GPs provide an elegant,
flexible and simple means of probabilistic infer-
ence and are well overdue for consideration of the
NLP community.
This tutorial will focus primarily on regression
and classification, both fundamental techniques of
wide-spread use in the NLP community. Within
NLP, linear models are near ubiquitous, because
they provide good results for many tasks, support
efficient inference (including dynamic program-
ming in structured prediction) and support simple
parameter interpretation. However, linear mod-
els are inherently limited in the types of relation-
ships between variables they can model. Often
</bodyText>
<footnote confidence="0.9940555">
1This holds for GP regression, but note that approximate1
inference is needed for non-Gaussian likelihoods.
</footnote>
<bodyText confidence="0.99992390625">
non-linear methods are required for better under-
standing and improved performance. Currently,
kernel methods such as Support Vector Machines
(SVM) represent a popular choice for non-linear
modelling. These suffer from lack of interoper-
ability with down-stream processing as part of a
larger model, and inflexibility in terms of parame-
terisation and associated high cost of hyperparam-
eter optimisation. GPs appear similar to SVMs, in
that they incorporate kernels, however their prob-
abilistic formulation allows for much wider appli-
cability in larger graphical models. Moreover, sev-
eral properties of Gaussian distributions (closure
under integration and Gaussian-Gaussian conju-
gacy) means that GP (regression) supports analytic
formulations for the posterior and predictive infer-
ence.
This tutorial will cover the basic motivation,
ideas and theory of Gaussian Processes and several
applications to natural language processing tasks.
GPs have been actively researched since the early
2000s, and are now reaching maturity: the fun-
damental theory and practice is well understood,
and now research is focused into their applica-
tions, and improve inference algorithms, e.g., for
scaling inference to large and high-dimensional
datasets. Several open-source packages (e.g. GPy
and GPML) have been developed which allow for
GPs to be easily used for many applications. This
tutorial aims to promote GPs, emphasising their
potential for widespread application across many
NLP tasks.
</bodyText>
<sectionHeader confidence="0.991001" genericHeader="keywords">
2 Overview
</sectionHeader>
<bodyText confidence="0.999823">
Our goal is to present the main ideas and theory
behind Gaussian Processes in order to increase
awareness within the NLP community. The first
part of the tutorial will focus on the basics of Gaus-
sian Processes in the context of regression. The
Gaussian Process defines a prior over functions
which applied at each input point gives a response
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 1–3,
Baltimore, Maryland, USA, 22 June 2014. c�2014 Association for Computational Linguistics
value. Given data, we can analytically infer the
posterior distribution of these functions assuming
Gaussian noise.
This tutorial will contrast two main applications
settings for regression: interpolation and extrapo-
lation. Interpolation suits the use of simple radial
basis function kernels which bias towards smooth
latent functions. For extrapolation, however, the
choice of the kernel is paramount, encoding our
prior belief about the type of function wish to
learn. We present several different kernels, includ-
ing non-stationary and kernels for structured data
(string and tree kernels). One of the main issues
for kernel methods is setting the hyperparameters,
which is often done in the support vector literature
using grid search on held-out validation data. In
the GP framework, we can compute the probabil-
ity of the data given the model which involves the
integral over the parameter space. This marginal
likelihood or Bayesian evidence can be used for
model selection using only training data, where by
model selection we refer either to choosing from a
set of given covariance kernels or choosing from
different model hyperparameters (kernel parame-
ters). We will present the key algorithms for type-
II maximum likelihood estimation with respect to
the hyper-parameters, using gradient ascent on the
marginal likelihood.
Many problems in NLP involve learning from
a range of different tasks. We present multi-task
learning models by representing intra-task transfer
simply and explicitly as a part of a parameterised
kernel function. GPs are an extremely flexible
probabilistic framework and have been success-
fully adapted for multi-task learning, by modelling
multiple correlated output variables (Alvarez et
al., 2011). This literature develops early work
from geostatistics (kriging and co-kriging), on
learning latent continuous spatio-temporal models
from sparse point measurements, a problem set-
ting that has clear parallels to transfer learning (in-
cluding domain adaptation).
In the application section, we start by present-
ing an open-source software package for GP mod-
elling in Python: GPy.2 The first application we
approach the regression task of predicting user in-
fluence on Twitter based on a range or profile and
word features (Lampos et al., 2014). We exem-
plify how to identifying which features are best
for predicting user impact by optimising the hy-
</bodyText>
<footnote confidence="0.925691">
2http://github.com/SheffieldML/GPy 2
</footnote>
<bodyText confidence="0.998679096774194">
perparameters (e.g. RBF kernel length-scales) us-
ing Automatic Relevance Determination (ARD).
This basically gives a ranking in importance of
the features, allowing interpretability of the mod-
els. Switching to a multi-task regression setting,
we present an application to Machine Translation
Quality Estimation. Our method shows large im-
provements over previous state-of-the-art (Cohn
and Specia, 2013). Concepts in automatic kernel
selection are exemplified in an extrapolation re-
gression setting, where we model word time series
in Social Media using different kernels (Preot¸iuc-
Pietro and Cohn, 2013). The Bayesian evidence
helps to select the most suitable kernel, thus giv-
ing an implicit classification of time series.
In the final section of the tutorial we give a
brief overview of advanced topics in the field of
GPs. First, we look at non-conjugate likelihoods
for modelling classification, count and rank data.
This is harder than regression, as Bayesian pos-
terior inference can no longer be solved analyti-
cally. We will outline strategies for non-conjugate
inference, such as expectation propagation and the
Laplace approximation. Second, we will outline
recent work on scaling GPs to big data using vari-
ational inference to induce sparse kernel matrices
(Hensman et al., 2013). Finally – time permitting
– we will finish with unsupervised learning in GPs
using the latent variable model (Lawrence, 2004),
a non-linear Bayesian analogue of principle com-
ponent analysis.
</bodyText>
<sectionHeader confidence="0.991901" genericHeader="introduction">
3 Outline
</sectionHeader>
<listItem confidence="0.995339058823529">
1. GP Regression (60 mins)
(a) Weight space view
(b) Function space view
(c) Kernels
2. NLP Applications (60 mins)
(a) Sparse GPs: Predicting user impact
(b) Multi-output GPs: Modelling multi-
annotator data
(c) Model selection: Identifying temporal
patterns in word frequencies
3. Further topics (45 mins)
(a) Non-congjugate likelihoods: classifica-
tion, counts and ranking
(b) Scaling GPs to big data: Sparse GPs and
stochastic variational inference
(c) Unsupervised inference with the GP-
LVM
</listItem>
<sectionHeader confidence="0.985221" genericHeader="acknowledgments">
4 Instructors
</sectionHeader>
<bodyText confidence="0.999457642857143">
Trevor Cohn3 is a Senior Lecturer and ARC Fu-
ture Fellow at the University of Melbourne. His
research deals with probabilistic machine learn-
ing models, particularly structured prediction and
non-parametric Bayesian models. He has recently
published several seminal papers on Gaussian Pro-
cess models for NLP with applications ranging
from translation evaluation to temporal dynamics
in social media.
Daniel Preot¸iuc-Pietro4 is a final year PhD stu-
dent in Natural Language Processing at the Uni-
versity of Sheffield. His research deals with ap-
plying Machine Learning models to model large
volumes of data, mostly coming from Social Me-
dia. Applications include forecasting future be-
haviours of text, users or real world quantities (e.g.
political voting intention), user geo-location and
impact.
Neil Lawrence5 is a Professor at the University
of Sheffield. He is one of the foremost experts on
Gaussian Processes and non-parametric Bayesian
inference, with a long history of publications and
innovations in the field, including their application
to multi-output scenarios, unsupervised learning,
deep networks and scaling to big data. He has been
programme chair for top machine learning confer-
ences (NIPS, AISTATS), and has run several past
tutorials on Gaussian Processes.
</bodyText>
<sectionHeader confidence="0.997919" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991395">
Mauricio A. Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2011. Kernels for vector-valued func-
tions: A review. Foundations and Trends in Machine
Learning, 4(3):195–266.
Trevor Cohn and Lucia Specia. 2013. Modelling anno-
tator bias with multi-task Gaussian processes: an ap-
plication to machine translation quality estimation.
In Proceedings of the 51st annual meeting of the As-
sociation for Computational Linguistics, ACL.
James Hensman, Nicolo Fusi, and Neil D. Lawrence.
2013. Gaussian processes for big data. In Proceed-
ings of the 29th Conference on Uncertainty in Artifi-
cial Intelligence, UAI.
Vasileios Lampos, Nikolaos Aletras, Daniel Preot¸iuc-
Pietro, and Trevor Cohn. 2014. Predicting and char-
acterising user impact on Twitter. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, EACL.
Neil D. Lawrence. 2004. Gaussian process latent vari-
able models for visualisation of high dimensional
data. NIPS, 16(329-336):3.
Daniel Preot¸iuc-Pietro and Trevor Cohn. 2013. A tem-
poral model of text periodicities using Gaussian Pro-
cesses. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An investigation on the effectiveness of features for
translation quality estimation. In Proceedings of the
Machine Translation Summit.
</reference>
<footnote confidence="0.99842825">
3http://staffwww.dcs.shef.ac.uk/
people/T.Cohn
4http://www.preotiuc.ro
5http://staffwww.dcs.shef.ac.uk/
</footnote>
<page confidence="0.750761">
people/N.Lawrence
3
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.727641">
<title confidence="0.999919">Gaussian Processes for Natural Language Processing</title>
<author confidence="0.911555">Trevor</author>
<affiliation confidence="0.938575">Computing and Information The University of</affiliation>
<email confidence="0.996187">trevor.cohn@gmail.com</email>
<author confidence="0.818867">Lawrence</author>
<affiliation confidence="0.996596">Department of Computer Science The University of Sheffield</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mauricio A Alvarez</author>
<author>Lorenzo Rosasco</author>
<author>Neil D Lawrence</author>
</authors>
<title>Kernels for vector-valued functions: A review. Foundations and Trends</title>
<date>2011</date>
<booktitle>in Machine Learning,</booktitle>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="5875" citStr="Alvarez et al., 2011" startWordPosition="849" endWordPosition="852">from different model hyperparameters (kernel parameters). We will present the key algorithms for typeII maximum likelihood estimation with respect to the hyper-parameters, using gradient ascent on the marginal likelihood. Many problems in NLP involve learning from a range of different tasks. We present multi-task learning models by representing intra-task transfer simply and explicitly as a part of a parameterised kernel function. GPs are an extremely flexible probabilistic framework and have been successfully adapted for multi-task learning, by modelling multiple correlated output variables (Alvarez et al., 2011). This literature develops early work from geostatistics (kriging and co-kriging), on learning latent continuous spatio-temporal models from sparse point measurements, a problem setting that has clear parallels to transfer learning (including domain adaptation). In the application section, we start by presenting an open-source software package for GP modelling in Python: GPy.2 The first application we approach the regression task of predicting user influence on Twitter based on a range or profile and word features (Lampos et al., 2014). We exemplify how to identifying which features are best f</context>
</contexts>
<marker>Alvarez, Rosasco, Lawrence, 2011</marker>
<rawString>Mauricio A. Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. 2011. Kernels for vector-valued functions: A review. Foundations and Trends in Machine Learning, 4(3):195–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling annotator bias with multi-task Gaussian processes: an application to machine translation quality estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st annual meeting of the Association for Computational Linguistics, ACL.</booktitle>
<contexts>
<context position="6959" citStr="Cohn and Specia, 2013" startWordPosition="1007" endWordPosition="1010"> on Twitter based on a range or profile and word features (Lampos et al., 2014). We exemplify how to identifying which features are best for predicting user impact by optimising the hy2http://github.com/SheffieldML/GPy 2 perparameters (e.g. RBF kernel length-scales) using Automatic Relevance Determination (ARD). This basically gives a ranking in importance of the features, allowing interpretability of the models. Switching to a multi-task regression setting, we present an application to Machine Translation Quality Estimation. Our method shows large improvements over previous state-of-the-art (Cohn and Specia, 2013). Concepts in automatic kernel selection are exemplified in an extrapolation regression setting, where we model word time series in Social Media using different kernels (Preot¸iucPietro and Cohn, 2013). The Bayesian evidence helps to select the most suitable kernel, thus giving an implicit classification of time series. In the final section of the tutorial we give a brief overview of advanced topics in the field of GPs. First, we look at non-conjugate likelihoods for modelling classification, count and rank data. This is harder than regression, as Bayesian posterior inference can no longer be </context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling annotator bias with multi-task Gaussian processes: an application to machine translation quality estimation. In Proceedings of the 51st annual meeting of the Association for Computational Linguistics, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Hensman</author>
<author>Nicolo Fusi</author>
<author>Neil D Lawrence</author>
</authors>
<title>Gaussian processes for big data.</title>
<date>2013</date>
<booktitle>In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence, UAI.</booktitle>
<contexts>
<context position="7845" citStr="Hensman et al., 2013" startWordPosition="1146" endWordPosition="1149">, thus giving an implicit classification of time series. In the final section of the tutorial we give a brief overview of advanced topics in the field of GPs. First, we look at non-conjugate likelihoods for modelling classification, count and rank data. This is harder than regression, as Bayesian posterior inference can no longer be solved analytically. We will outline strategies for non-conjugate inference, such as expectation propagation and the Laplace approximation. Second, we will outline recent work on scaling GPs to big data using variational inference to induce sparse kernel matrices (Hensman et al., 2013). Finally – time permitting – we will finish with unsupervised learning in GPs using the latent variable model (Lawrence, 2004), a non-linear Bayesian analogue of principle component analysis. 3 Outline 1. GP Regression (60 mins) (a) Weight space view (b) Function space view (c) Kernels 2. NLP Applications (60 mins) (a) Sparse GPs: Predicting user impact (b) Multi-output GPs: Modelling multiannotator data (c) Model selection: Identifying temporal patterns in word frequencies 3. Further topics (45 mins) (a) Non-congjugate likelihoods: classification, counts and ranking (b) Scaling GPs to big da</context>
</contexts>
<marker>Hensman, Fusi, Lawrence, 2013</marker>
<rawString>James Hensman, Nicolo Fusi, and Neil D. Lawrence. 2013. Gaussian processes for big data. In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence, UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Nikolaos Aletras</author>
<author>Daniel Preot¸iucPietro</author>
<author>Trevor Cohn</author>
</authors>
<title>Predicting and characterising user impact on Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL.</booktitle>
<marker>Lampos, Aletras, Preot¸iucPietro, Cohn, 2014</marker>
<rawString>Vasileios Lampos, Nikolaos Aletras, Daniel Preot¸iucPietro, and Trevor Cohn. 2014. Predicting and characterising user impact on Twitter. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil D Lawrence</author>
</authors>
<title>Gaussian process latent variable models for visualisation of high dimensional data.</title>
<date>2004</date>
<journal>NIPS,</journal>
<pages>16--329</pages>
<contexts>
<context position="7972" citStr="Lawrence, 2004" startWordPosition="1168" endWordPosition="1169">pics in the field of GPs. First, we look at non-conjugate likelihoods for modelling classification, count and rank data. This is harder than regression, as Bayesian posterior inference can no longer be solved analytically. We will outline strategies for non-conjugate inference, such as expectation propagation and the Laplace approximation. Second, we will outline recent work on scaling GPs to big data using variational inference to induce sparse kernel matrices (Hensman et al., 2013). Finally – time permitting – we will finish with unsupervised learning in GPs using the latent variable model (Lawrence, 2004), a non-linear Bayesian analogue of principle component analysis. 3 Outline 1. GP Regression (60 mins) (a) Weight space view (b) Function space view (c) Kernels 2. NLP Applications (60 mins) (a) Sparse GPs: Predicting user impact (b) Multi-output GPs: Modelling multiannotator data (c) Model selection: Identifying temporal patterns in word frequencies 3. Further topics (45 mins) (a) Non-congjugate likelihoods: classification, counts and ranking (b) Scaling GPs to big data: Sparse GPs and stochastic variational inference (c) Unsupervised inference with the GPLVM 4 Instructors Trevor Cohn3 is a S</context>
</contexts>
<marker>Lawrence, 2004</marker>
<rawString>Neil D. Lawrence. 2004. Gaussian process latent variable models for visualisation of high dimensional data. NIPS, 16(329-336):3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Preot¸iuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>A temporal model of text periodicities using Gaussian Processes.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP.</booktitle>
<marker>Preot¸iuc-Pietro, Cohn, 2013</marker>
<rawString>Daniel Preot¸iuc-Pietro and Trevor Cohn. 2013. A temporal model of text periodicities using Gaussian Processes. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>An investigation on the effectiveness of features for translation quality estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Machine Translation Summit.</booktitle>
<marker>Shah, Cohn, Specia, 2013</marker>
<rawString>Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. An investigation on the effectiveness of features for translation quality estimation. In Proceedings of the Machine Translation Summit.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>