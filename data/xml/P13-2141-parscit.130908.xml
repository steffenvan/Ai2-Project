<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002629">
<title confidence="0.98901">
Towards Accurate Distant Supervision for Relational Facts Extraction
</title>
<author confidence="0.998086">
Xingxing Zhang1 Jianwen Zhang&apos;∗ Junyu Zeng3 Jun Yan&apos; Zheng Chen&apos; Zhifang Sui1
</author>
<affiliation confidence="0.971835">
1Key Laboratory of Computational Linguistics (Peking University), Ministry of Education,China
2Microsoft Research Asia
3Beijing University of Posts and Telecommunications
</affiliation>
<address confidence="0.453723">
1{ zhangxingxing,szf}@pku.edu.cn
</address>
<email confidence="0.9537985">
2{jiazhan,junyan,zhengc}@microsoft.com
3junyu.zeng@gmail.com
</email>
<sectionHeader confidence="0.997061" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999709375">
Distant supervision (DS) is an appealing
learning method which learns from exist-
ing relational facts to extract more from
a text corpus. However, the accuracy is
still not satisfying. In this paper, we point
out and analyze some critical factors in
DS which have great impact on accuracy,
including valid entity type detection,
negative training examples construction
and ensembles. We propose an approach
to handle these factors. By experimenting
on Wikipedia articles to extract the facts in
Freebase (the top 92 relations), we show
the impact of these three factors on the
accuracy of DS and the remarkable im-
provement led by the proposed approach.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994128483870968">
Recently there are great efforts on building large
structural knowledge bases (KB) such as Free-
base, Yago, etc. They are composed of relational
facts often represented in the form of a triplet,
(SrcEntity, Relation, DstEntity),
such as “(Bill Gates, BornIn, Seattle)”. An impor-
tant task is to enrich such KBs by extracting more
facts from text. Specifically, this paper focuses on
extracting facts for existing relations. This is dif-
ferent from OpenIE (Banko et al., 2007; Carlson et
al., 2010) which needs to discover new relations.
Given large amounts of labeled sentences,
supervised methods are able to achieve good
performance (Zhao and Grishman, 2005; Bunescu
and Mooney, 2005). However, it is difficult to
handle large scale corpus due to the high cost
of labeling. Recently an approach called distant
supervision (DS) (Mintz et al., 2009) was pro-
posed, which does not require any labels on the
text. It treats the extraction problem as classifying
* The contact author.
a candidate entity pair to a relation. Then an
existing fact in a KB can be used as a labeled
example whose label is the relation name. Then
the features of all the sentences (from a given text
corpus) containing the entity pair are merged as
the feature of the example. Finally a multi-class
classifier is trained.
However, the accuracy of DS is not satisfying.
Some variants have been proposed to improve
the performance (Riedel et al., 2010; Hoffmann
et al., 2011; Takamatsu et al., 2012). They ar-
gue that DS introduces a lot of noise into the
training data by merging the features of all the
sentences containing the same entity pair, because
a sentence containing the entity pair of a relation
may not talk about the relation. Riedel et al.
(2010) and Hoffmann et al. (2011) introduce
hidden variables to indicate whether a sentence
is noise and try to infer them from the data.
Takamatsu et al. (2012) design a generative model
to identify noise patterns. However, as shown in
the experiments (Section 4), the above variants do
not lead to much improvement in accuracy.
In this paper, we point out and analyze some
critical factors in DS which have great impact on
the accuracy but has not been touched or well han-
dled before. First, each relation has its own schema
definition, i.e., the source entity and the destina-
tion entity should be of valid types, which is over-
looked in DS. Therefore, we propose a component
of entity type detection to check it. Second, DS
introduces many false negative examples into the
training set and we propose a new method to con-
struct negative training examples. Third, we find it
is difficult for a single classifier to achieve high ac-
curacy and hence we train multiple classifiers and
ensemble them.
We also notice that Nguyen and Moschitti
(2011a) and Nguyen and Moschitti (2011b) utilize
external information such as more facts from Yago
and labeled sentences from ACE to improve the
</bodyText>
<page confidence="0.946472">
810
</page>
<bodyText confidence="0.65155425">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 810–815,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
performance. These methods can also be equipped
with the approach proposed in this paper.
</bodyText>
<sectionHeader confidence="0.935975" genericHeader="method">
2 Critical Factors Affecting the Accuracy
</sectionHeader>
<bodyText confidence="0.999914704918033">
DS has four steps: (1) Detect candidate entity
pairs in the corpus. (2) Label the candidate pairs
using the KB. (3) Extract features for the pair
from sentences containing the pair. (4) Train a
multi-class classifier. Among these steps, we find
the following three critical factors have great
impact on the accuracy (see Section 4 for the
experimental results).
Valid entity type detection. In DS, a sentence
with a candidate entity pair a sentence with two
candidate entities is noisy. First, the schema of
each relation in the KB requires that the source
and destination entities should be of valid types,
e.g., the source and destination entity of the
relation “DirectorOfFilm” should be of the types
“Director” and “Film” respectively. If the two
entities in a sentence are not of the valid types, the
sentence is noisy. Second, the sentence may not
talk about the relation even when the two entities
are of the valid types. The previous works (Riedel
et al., 2010; Hoffmann et al., 2011; Takamatsu et
al., 2012) do not distinguish the two types of noise
but directly infer the overall noise from the data.
We argue that the first type of noise is very difficult
to be inferred just from the noisy relational labels.
Instead, we decouple the two types of noise, and
utilize external labeled data, i.e., the Wikipedia
anchor links, to train an entity type detection mod-
ule to handle the first type of noise. We notice that
when Ling and Weld (2012) studied a fine-grained
NER method, they applied the method to relation
extraction by adding the recognized entity tags to
the features. We worry that the contribution of the
entity type features may be drowned when many
other features are used. Their method works well
on relatively small relations, but not that well on
big ones (Section 4.2).
Negative examples construction. DS treats the
relation extraction as a multi-class classification
task. For a relation, it implies that the facts of all
the other relations together with the “Other” class
are negative examples. This introduces many false
negative examples into the training data. First,
many relations are not exclusive with each other,
e.g., “PlaceOfBorn” and “PlaceOfDeath”, the
born place of a person can be also the death place.
Second, in DS, the “Other” class is composed
of all the candidate entity pairs not existed in
the KB, which actually contains many positive
facts of non-Other relations because the KB is
not complete. Therefore we use a different way to
construct negative training examples.
Feature space partition and ensemble. The
features used in DS are very sparse and many
examples do not contain any features. Thus we
employ more features. However we find it is
difficult for a single classifier on all the features
to achieve high accuracy and hence we divide
the features into different categories and train
a separate classifier for each category and then
ensemble them finally.
</bodyText>
<sectionHeader confidence="0.99351" genericHeader="method">
3 Accurate Distant Supervision (ADS)
</sectionHeader>
<bodyText confidence="0.999826">
Different from DS, we treat the extraction
problem as N binary classification problems,
one for each relation. We modify the four steps
of DS (Section 2). In step (1), when detecting
candidate entity pairs in sentences, we use our
entity type detection module (Section 3.1) to filter
out the sentences where the entity pair is of invalid
entity types. In step (2), we use our new method
to construct negative examples (Section 3.2). In
step (3), we employ more features and design an
ensemble classifier (Section 3.3). In step (4), we
train N binary classifiers separately.
</bodyText>
<subsectionHeader confidence="0.997761">
3.1 Entity Type Detection
</subsectionHeader>
<bodyText confidence="0.999742263157895">
We divide the entity type detection into two steps.
The first step, called boundary detection, is to
detect phrases as candidate entities. The second
step, called named entity disambiguation, maps
a detected candidate entity to some entity types,
e.g., “FilmDirector”. Note that an entity might be
mapped to multiple types. For instance, ”Ventura
Pons” is a “FilmDirector” and a “Person”.
Boundary Detection Two ways are used for
boundary detection. First, for each relation, from
the training set of facts, we get two dictionaries
(one for source entities and one for destination en-
tities). The two dictionaries are used to detect the
source and destination entities. Second, an exist-
ing NER tool (StanfordNER here) is used with the
following postprocessing to filter some unwanted
entities, because a NER tool sometimes produces
too many entities. We first find the compatible N-
ER tags for an entity type in the KB. For example,
</bodyText>
<page confidence="0.987101">
811
</page>
<bodyText confidence="0.9989032">
for the type “FilmDirector”, the compatible NER
tag of Standford NER is “Person”. To do this,
for each entity type in the KB, we match all the
entities of that type (in the training set) back to the
training corpus and get the probability Ptag(ti) of
each NER tag (including the “NULL” tag meaning
not recognized as a named entity) recognized
by the NER tool. Then we retain the top k tags
Stags = {t1, · · · , tk} with the highest probabil-
ities to account for an accumulated mass z:
</bodyText>
<equation confidence="0.992749333333333">
(( k
k = argkn∑Ptag(ti) ≥ z (1)
i=1
</equation>
<bodyText confidence="0.9984545">
In the experiments we set z = 0.9. The compati-
ble ner tags are Stags\{“NULL”}. If the retained
tags contain only “NULL”, the candidate entities
recognized by NER tool will be discarded.
Named Entity Disambiguation (NED) With
a candidate entity obtained by the boundary
detection, we need a NED component to assign
some entity types to it. To obtain such a NED, we
leverage the anchor text in Wikipedia to generate
training data and train a NED component. The
referred Freebase entity and the types of an anchor
link in Wikipedia can be obtained from Freebase.
The following features are used to train the
NED component. Mention Features: Uni-grams,
Bi-grams, POS tags, word shapes in the mention,
and the length of the mention. Context Features:
Uni-grams and Bi-grams in the windows of the
mention (window size = 5).
</bodyText>
<subsectionHeader confidence="0.994713">
3.2 Negative Examples Construction
</subsectionHeader>
<bodyText confidence="0.998053515151515">
Treating the problem as a multi-class classification
implies introducing many false negative examples
for a relation; therefore, we handle each relation
with a separate binary classifier. However, a KB
only tells us which entity pairs belong to a relation,
i.e., it only provides positive examples for each re-
lation. But we also need negative examples to train
a binary classifier. To reduce the number of false
negative examples, we propose a new method
to construct negative examples by utilizing the
1-to-1/1-to-n/n-to-1/n-to-n property of a relation.
1-to-1/n-to-1/1-to-n Relation A 1-to-1 or n-to-
1 relation is a functional relation: for a relation r,
for each valid source entity e1, there is only one
unique destination entity e2 such that (e1, e2) ∈ r.
However, in a real KB like Freebase, very few
relations meet the exact criterion. Thus we use the
following approximate criterion instead: relation
r is approximately a 1-to-1/n-to-1 relation if the
Inequalities (2,3) hold, where M is the number of
unique source entities in relation r, and S(·) is an
indicator function which returns 1 if the condition
is met and returns 0 otherwise. Inequality (2)
says the proportion of source entities which have
exactly one counterpart destination entity should
be greater than a given threshold. Inequality (3)
says the average number of destination entities of
a source entity should be less than the threshold.
To check whether r is a 1-to-n relation, we simply
swap the source and destination entities of the
relation and check whether the reversed relation
is a n-to-1 relation by the above two inequalities.
In experiments we set 0 = 0.7 and -y = 1.1.
</bodyText>
<equation confidence="0.9988175">
S (1{e′|(ei, e′) ∈ r}I = 1) ≥ 0 (2)
I{e′|(ei, e′) ∈ r} I ≤ -Y (3)
</equation>
<bodyText confidence="0.997442">
n-to-n Relation Relations other than 1-to-1/n-
to-1/1-to-n are n-to-n relations. We approximately
categorize a n-to-n relation to n-to-1 or 1-to-n by
checking which one it is closer to. This is done
by computing the following two values αsrc and
αdst. r is treated as a 1-to-n relation if αsrc &gt; αdst
and as a 1-to-n relation otherwise.
</bodyText>
<equation confidence="0.998197333333333">
I{e′|(ei,e′) ∈ r}I
(4)
I{e′|(e′,ei) ∈ r}I
</equation>
<bodyText confidence="0.999423">
Negative examples For a candidate entity pair
(e1, e2) not in the relation r of the KB, we first
determine whether it is 1-to-n or n-to-1 using the
above method. If r is 1-to-1/n-to-1 and e1 exists in
some fact of r as the source entity, then (e1, e2) is
a negative example as it violates the 1-to-1/n-to-1
constraint. If r is 1-to-n, the judgement is similar
and just simply swap the source and destination
entities of the relation.
</bodyText>
<subsectionHeader confidence="0.999033">
3.3 Feature Space Partition and Ensemble
</subsectionHeader>
<bodyText confidence="0.9999878">
The features of DS (Mintz et al., 2009) are very
sparse in the corpus. We add some features in (Yao
et al., 2011): Trigger Words (the words on the
dependency path except stop words) and Entity
String (source entity and destination entity).
</bodyText>
<equation confidence="0.990608333333333">
1
M
∑M
i=1
1
M
∑M
i=1
1
αsrc =
Msrc
1
Msrc∑
i=1
Mdst∑
i=1
Mdst
αdst =
</equation>
<page confidence="0.993713">
812
</page>
<table confidence="0.9997794">
Relation Taka Ensemble
works written 0.76 0.98
river/basin countries 0.48 1
/film/director/film 0.82 1
Average 0.79 0.89
</table>
<tableCaption confidence="0.814779">
Table 1: Manual evaluation of top-ranked 50 rela-
tion instances for the most frequent 15 relations.
</tableCaption>
<bodyText confidence="0.999155076923077">
We find that without considering the reversed
order of entity pairs in a sentence, the precision
can be higher, but the recall decreases. For exam-
ple, for the entity pair ⟨Ventura Pons, Actrius⟩, we
only consider sentences with the right order (e.g.
Ventura Pons is directed by Actrius.). For each re-
lation, we train four classifiers: C1 (without con-
sidering reversed order), C2 (considering reversed
order), C1more (without considering reversed or-
der and employ more feature) and C2more (con-
sidering reversed order and employ more feature).
We then ensemble the four classifiers by averaging
the probabilities of predictions:
</bodyText>
<equation confidence="0.999959">
P1 + P2 + P1more + P2more
P(y|x) = (5)
</equation>
<page confidence="0.525096">
4
</page>
<sectionHeader confidence="0.999786" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996868">
4.1 Dataset and Configurations
</subsectionHeader>
<bodyText confidence="0.999993588235294">
We aimed to extract facts of the 92 most frequent
relations in Freebase 2009. The facts of each
relation were equally split to two parts for training
and testing. Wikipedia 2009 was used as the target
corpus, where 800,000 articles were used for
training and 400,000 for testing. During the NED
phrase, there are 94 unique entity types (they are
also relations in Freebase) for the source and desti-
nation entities. Note that some entity types contain
too few entities and they are discarded. We used
500,000 Wikipedia articles (2,000,000 sentences)
for generating training data for the NED compo-
nent. We used Open NLP POS tagger, Standford
NER (Finkel et al., 2005) and MaltParser (Nivre
et al., 2006) to label/tag sentences. We employed
liblinear (Fan et al., 2008) as classifiers for NED
and relation extraction and the solver is L2LR.
</bodyText>
<subsectionHeader confidence="0.999609">
4.2 Performance of Relation Extraction
</subsectionHeader>
<bodyText confidence="0.9953022">
Held-out Evaluation. We evaluate the perfor-
mance on the half hold-on facts for testing. We
compared performance of the n = 50, 000 best ex-
tracted relation instances of each method and the
Precision-Recall (PR) curves are in Figure 1 and
</bodyText>
<figure confidence="0.589612">
Recall
</figure>
<figureCaption confidence="0.999734">
Figure 1: Performance of different methods.
</figureCaption>
<figure confidence="0.9736325">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall
</figure>
<figureCaption confidence="0.999911">
Figure 2: Contributions of different components.
</figureCaption>
<bodyText confidence="0.96055725">
Figure 2. For a candidate fact without any enti-
ty existing in Freebase, we are not able to judge
whether it is correct. Thus we only evaluate the
candidate facts that at least one entity occurs as
the source or destination entity in the test fact set.
In Figure 1, we compared our method with
two previous methods: MultiR (Hoffmann et al.,
2011) and Takamatsu et al. (2012) (Taka). For
MultiR, we used the author’s implementation1.
We re-implemented Takamatsu’s algorithm. As
Takamatsu’s dataset (903,000 Wikipedia articles
for training and 400,000 for testing) is very similar
to ours, we used their best reported parameters.
Our method leads to much better performance.
Manual Evaluation. Following (Takamatsu et
al., 2012), we selected the top 50 ranked (accord-
ing to their classification probabilities) relation
facts of the 15 largest relations. We compared our
results with those of Takamatsu et al. (2012) and
we achieved greater average precision (Table 1).
</bodyText>
<footnote confidence="0.8302865">
1available at http://www.cs.washington.edu/ai/raphaelh/mr
We set T = 120, which leads to the best performance.
</footnote>
<figure confidence="0.9969405">
1
OrigDS
MultiR
Taka
ADS
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0.8
0.6
0.4
0.2
0
Precision
0.25
0.8
0.6
0.4
1
OrigDS
DS_Figer
ETD
ETD+Neg
More
Ensemble(ADS)
Precision
</figure>
<page confidence="0.988934">
813
</page>
<table confidence="0.99427">
Pmicro Rmicro Pmacro Rmacro
0.950 0.845 0.947 0.626
</table>
<tableCaption confidence="0.998314">
Table 2: Performance of the NED component
</tableCaption>
<subsectionHeader confidence="0.997001">
4.3 Contribution of Each Component
</subsectionHeader>
<bodyText confidence="0.999845714285714">
In Figure 2, with the entity type detection (ETD),
the performance is better than the original DS
method (OrigDS). As for the performance of NED
in the Entity Type Detection, the Micro/Macro
Precision-Recall of our NED component are in
Table 2. ETD is also better than adding the entity
types of the pair to the feature vector (DS Figer)2
as in (Ling and Weld, 2012). If we also employ the
negative example construction strategy in Section
3.2 (ETD+Neg), the precision of the top ranked
instances is improved. By adding more features
(More) and employing the ensemble learning
(Ensemble(ADS)) to ETD+Neg, the performance
is further improved.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99982">
This paper dealt with the problem of improving the
accuracy of DS. We find some factors are crucial-
ly important, including valid entity type detection,
negative training examples construction and en-
sembles. We have proposed an approach to handle
these issues. Experiments show that the approach
is very effective.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990066986842105">
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, IJCAI’07, pages 2670–
2676, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of Human Language Technolo-
gy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 724–
731, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Bur-
r Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010), volume 2, pages 3–3.
2We use Figer (Ling and Weld, 2012) to detect entity types
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05). Association for Computational Linguis-
tics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 541–550, Portland, Oregon, USA,
June. Association for Computational Linguistics.
X. Ling and D.S. Weld. 2012. Fine-grained entity
recognition. In Proceedings of the 26th Conference
on Artificial Intelligence (AAAI).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Truc-Vien T. Nguyen and Alessandro Moschitti.
2011a. End-to-end relation extraction using distant
supervision from external semantic repositories. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
’11, pages 277–282, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Truc-Vien T Nguyen and Alessandro Moschitti. 2011b.
Joint distant and direct supervision for relation ex-
traction. In Proceeding of the International Joint
Conference on Natural Language Processing, pages
732–740.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In In Proc. of LREC-2006, pages
2216–2219.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the Sixteenth Eu-
ropean Conference on Machine Learning (ECML-
2010), pages 148–163.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In Proceedings of the 50th
</reference>
<page confidence="0.985949">
814
</page>
<reference confidence="0.998554470588235">
Annual Meeting of the Association for Computation-
al Linguistics (Volume 1: Long Papers), pages 721–
729, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation dis-
covery using generative models. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1456–1466, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistic-
s (ACL’05), pages 419–426, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.998845">
815
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916928">
<title confidence="0.999904">Towards Accurate Distant Supervision for Relational Facts Extraction</title>
<author confidence="0.995199">Jianwen Junyu Jun</author>
<affiliation confidence="0.992198333333333">Laboratory of Computational Linguistics (Peking University), Ministry of Research University of Posts and</affiliation>
<abstract confidence="0.996298705882353">Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07,</booktitle>
<pages>2670--2676</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="1559" citStr="Banko et al., 2007" startWordPosition="223" endWordPosition="226">ions), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach. 1 Introduction Recently there are great efforts on building large structural knowledge bases (KB) such as Freebase, Yago, etc. They are composed of relational facts often represented in the form of a triplet, (SrcEntity, Relation, DstEntity), such as “(Bill Gates, BornIn, Seattle)”. An important task is to enrich such KBs by extracting more facts from text. Specifically, this paper focuses on extracting facts for existing relations. This is different from OpenIE (Banko et al., 2007; Carlson et al., 2010) which needs to discover new relations. Given large amounts of labeled sentences, supervised methods are able to achieve good performance (Zhao and Grishman, 2005; Bunescu and Mooney, 2005). However, it is difficult to handle large scale corpus due to the high cost of labeling. Recently an approach called distant supervision (DS) (Mintz et al., 2009) was proposed, which does not require any labels on the text. It treats the extraction problem as classifying * The contact author. a candidate entity pair to a relation. Then an existing fact in a KB can be used as a labeled</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07, pages 2670– 2676, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>724--731</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1771" citStr="Bunescu and Mooney, 2005" startWordPosition="255" endWordPosition="258">ral knowledge bases (KB) such as Freebase, Yago, etc. They are composed of relational facts often represented in the form of a triplet, (SrcEntity, Relation, DstEntity), such as “(Bill Gates, BornIn, Seattle)”. An important task is to enrich such KBs by extracting more facts from text. Specifically, this paper focuses on extracting facts for existing relations. This is different from OpenIE (Banko et al., 2007; Carlson et al., 2010) which needs to discover new relations. Given large amounts of labeled sentences, supervised methods are able to achieve good performance (Zhao and Grishman, 2005; Bunescu and Mooney, 2005). However, it is difficult to handle large scale corpus due to the high cost of labeling. Recently an approach called distant supervision (DS) (Mintz et al., 2009) was proposed, which does not require any labels on the text. It treats the extraction problem as classifying * The contact author. a candidate entity pair to a relation. Then an existing fact in a KB can be used as a labeled example whose label is the relation name. Then the features of all the sentences (from a given text corpus) containing the entity pair are merged as the feature of the example. Finally a multi-class classifier i</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 724– 731, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI</booktitle>
<volume>2</volume>
<pages>3--3</pages>
<contexts>
<context position="1582" citStr="Carlson et al., 2010" startWordPosition="227" endWordPosition="230">mpact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach. 1 Introduction Recently there are great efforts on building large structural knowledge bases (KB) such as Freebase, Yago, etc. They are composed of relational facts often represented in the form of a triplet, (SrcEntity, Relation, DstEntity), such as “(Bill Gates, BornIn, Seattle)”. An important task is to enrich such KBs by extracting more facts from text. Specifically, this paper focuses on extracting facts for existing relations. This is different from OpenIE (Banko et al., 2007; Carlson et al., 2010) which needs to discover new relations. Given large amounts of labeled sentences, supervised methods are able to achieve good performance (Zhao and Grishman, 2005; Bunescu and Mooney, 2005). However, it is difficult to handle large scale corpus due to the high cost of labeling. Recently an approach called distant supervision (DS) (Mintz et al., 2009) was proposed, which does not require any labels on the text. It treats the extraction problem as classifying * The contact author. a candidate entity pair to a relation. Then an existing fact in a KB can be used as a labeled example whose label is</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka Jr, and Tom M Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010), volume 2, pages 3–3.</rawString>
</citation>
<citation valid="true">
<title>2We use Figer (Ling and Weld,</title>
<date>2012</date>
<note>to detect entity types</note>
<contexts>
<context position="2973" citStr="(2012)" startWordPosition="468" endWordPosition="468">er is trained. However, the accuracy of DS is not satisfying. Some variants have been proposed to improve the performance (Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). They argue that DS introduces a lot of noise into the training data by merging the features of all the sentences containing the same entity pair, because a sentence containing the entity pair of a relation may not talk about the relation. Riedel et al. (2010) and Hoffmann et al. (2011) introduce hidden variables to indicate whether a sentence is noise and try to infer them from the data. Takamatsu et al. (2012) design a generative model to identify noise patterns. However, as shown in the experiments (Section 4), the above variants do not lead to much improvement in accuracy. In this paper, we point out and analyze some critical factors in DS which have great impact on the accuracy but has not been touched or well handled before. First, each relation has its own schema definition, i.e., the source entity and the destination entity should be of valid types, which is overlooked in DS. Therefore, we propose a component of entity type detection to check it. Second, DS introduces many false negative exam</context>
<context position="5768" citStr="(2012)" startWordPosition="936" endWordPosition="936"> may not talk about the relation even when the two entities are of the valid types. The previous works (Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012) do not distinguish the two types of noise but directly infer the overall noise from the data. We argue that the first type of noise is very difficult to be inferred just from the noisy relational labels. Instead, we decouple the two types of noise, and utilize external labeled data, i.e., the Wikipedia anchor links, to train an entity type detection module to handle the first type of noise. We notice that when Ling and Weld (2012) studied a fine-grained NER method, they applied the method to relation extraction by adding the recognized entity tags to the features. We worry that the contribution of the entity type features may be drowned when many other features are used. Their method works well on relatively small relations, but not that well on big ones (Section 4.2). Negative examples construction. DS treats the relation extraction as a multi-class classification task. For a relation, it implies that the facts of all the other relations together with the “Other” class are negative examples. This introduces many false</context>
<context position="15627" citStr="(2012)" startWordPosition="2597" endWordPosition="2597">cted relation instances of each method and the Precision-Recall (PR) curves are in Figure 1 and Recall Figure 1: Performance of different methods. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall Figure 2: Contributions of different components. Figure 2. For a candidate fact without any entity existing in Freebase, we are not able to judge whether it is correct. Thus we only evaluate the candidate facts that at least one entity occurs as the source or destination entity in the test fact set. In Figure 1, we compared our method with two previous methods: MultiR (Hoffmann et al., 2011) and Takamatsu et al. (2012) (Taka). For MultiR, we used the author’s implementation1. We re-implemented Takamatsu’s algorithm. As Takamatsu’s dataset (903,000 Wikipedia articles for training and 400,000 for testing) is very similar to ours, we used their best reported parameters. Our method leads to much better performance. Manual Evaluation. Following (Takamatsu et al., 2012), we selected the top 50 ranked (according to their classification probabilities) relation facts of the 15 largest relations. We compared our results with those of Takamatsu et al. (2012) and we achieved greater average precision (Table 1). 1availa</context>
</contexts>
<marker>2012</marker>
<rawString>2We use Figer (Ling and Weld, 2012) to detect entity types</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="14769" citStr="Fan et al., 2008" startWordPosition="2446" endWordPosition="2449"> and testing. Wikipedia 2009 was used as the target corpus, where 800,000 articles were used for training and 400,000 for testing. During the NED phrase, there are 94 unique entity types (they are also relations in Freebase) for the source and destination entities. Note that some entity types contain too few entities and they are discarded. We used 500,000 Wikipedia articles (2,000,000 sentences) for generating training data for the NED component. We used Open NLP POS tagger, Standford NER (Finkel et al., 2005) and MaltParser (Nivre et al., 2006) to label/tag sentences. We employed liblinear (Fan et al., 2008) as classifiers for NED and relation extraction and the solver is L2LR. 4.2 Performance of Relation Extraction Held-out Evaluation. We evaluate the performance on the half hold-on facts for testing. We compared performance of the n = 50, 000 best extracted relation instances of each method and the Precision-Recall (PR) curves are in Figure 1 and Recall Figure 1: Performance of different methods. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall Figure 2: Contributions of different components. Figure 2. For a candidate fact without any entity existing in Freebase, we are not able to judge whether it is corr</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14668" citStr="Finkel et al., 2005" startWordPosition="2430" endWordPosition="2433">uent relations in Freebase 2009. The facts of each relation were equally split to two parts for training and testing. Wikipedia 2009 was used as the target corpus, where 800,000 articles were used for training and 400,000 for testing. During the NED phrase, there are 94 unique entity types (they are also relations in Freebase) for the source and destination entities. Note that some entity types contain too few entities and they are discarded. We used 500,000 Wikipedia articles (2,000,000 sentences) for generating training data for the NED component. We used Open NLP POS tagger, Standford NER (Finkel et al., 2005) and MaltParser (Nivre et al., 2006) to label/tag sentences. We employed liblinear (Fan et al., 2008) as classifiers for NED and relation extraction and the solver is L2LR. 4.2 Performance of Relation Extraction Held-out Evaluation. We evaluate the performance on the half hold-on facts for testing. We compared performance of the n = 50, 000 best extracted relation instances of each method and the Precision-Recall (PR) curves are in Figure 1 and Recall Figure 1: Performance of different methods. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall Figure 2: Contributions of different components. Figure 2. For </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>541--550</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2532" citStr="Hoffmann et al., 2011" startWordPosition="387" endWordPosition="390">S) (Mintz et al., 2009) was proposed, which does not require any labels on the text. It treats the extraction problem as classifying * The contact author. a candidate entity pair to a relation. Then an existing fact in a KB can be used as a labeled example whose label is the relation name. Then the features of all the sentences (from a given text corpus) containing the entity pair are merged as the feature of the example. Finally a multi-class classifier is trained. However, the accuracy of DS is not satisfying. Some variants have been proposed to improve the performance (Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). They argue that DS introduces a lot of noise into the training data by merging the features of all the sentences containing the same entity pair, because a sentence containing the entity pair of a relation may not talk about the relation. Riedel et al. (2010) and Hoffmann et al. (2011) introduce hidden variables to indicate whether a sentence is noise and try to infer them from the data. Takamatsu et al. (2012) design a generative model to identify noise patterns. However, as shown in the experiments (Section 4), the above variants do not lead to much improvement in </context>
<context position="5308" citStr="Hoffmann et al., 2011" startWordPosition="851" endWordPosition="854">tity type detection. In DS, a sentence with a candidate entity pair a sentence with two candidate entities is noisy. First, the schema of each relation in the KB requires that the source and destination entities should be of valid types, e.g., the source and destination entity of the relation “DirectorOfFilm” should be of the types “Director” and “Film” respectively. If the two entities in a sentence are not of the valid types, the sentence is noisy. Second, the sentence may not talk about the relation even when the two entities are of the valid types. The previous works (Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012) do not distinguish the two types of noise but directly infer the overall noise from the data. We argue that the first type of noise is very difficult to be inferred just from the noisy relational labels. Instead, we decouple the two types of noise, and utilize external labeled data, i.e., the Wikipedia anchor links, to train an entity type detection module to handle the first type of noise. We notice that when Ling and Weld (2012) studied a fine-grained NER method, they applied the method to relation extraction by adding the recognized entity tags to the features. We </context>
<context position="15599" citStr="Hoffmann et al., 2011" startWordPosition="2589" endWordPosition="2592">ed performance of the n = 50, 000 best extracted relation instances of each method and the Precision-Recall (PR) curves are in Figure 1 and Recall Figure 1: Performance of different methods. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall Figure 2: Contributions of different components. Figure 2. For a candidate fact without any entity existing in Freebase, we are not able to judge whether it is correct. Thus we only evaluate the candidate facts that at least one entity occurs as the source or destination entity in the test fact set. In Figure 1, we compared our method with two previous methods: MultiR (Hoffmann et al., 2011) and Takamatsu et al. (2012) (Taka). For MultiR, we used the author’s implementation1. We re-implemented Takamatsu’s algorithm. As Takamatsu’s dataset (903,000 Wikipedia articles for training and 400,000 for testing) is very similar to ours, we used their best reported parameters. Our method leads to much better performance. Manual Evaluation. Following (Takamatsu et al., 2012), we selected the top 50 ranked (according to their classification probabilities) relation facts of the 15 largest relations. We compared our results with those of Takamatsu et al. (2012) and we achieved greater average </context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 541–550, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Ling</author>
<author>D S Weld</author>
</authors>
<title>Fine-grained entity recognition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="5768" citStr="Ling and Weld (2012)" startWordPosition="933" endWordPosition="936">, the sentence may not talk about the relation even when the two entities are of the valid types. The previous works (Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012) do not distinguish the two types of noise but directly infer the overall noise from the data. We argue that the first type of noise is very difficult to be inferred just from the noisy relational labels. Instead, we decouple the two types of noise, and utilize external labeled data, i.e., the Wikipedia anchor links, to train an entity type detection module to handle the first type of noise. We notice that when Ling and Weld (2012) studied a fine-grained NER method, they applied the method to relation extraction by adding the recognized entity tags to the features. We worry that the contribution of the entity type features may be drowned when many other features are used. Their method works well on relatively small relations, but not that well on big ones (Section 4.2). Negative examples construction. DS treats the relation extraction as a multi-class classification task. For a relation, it implies that the facts of all the other relations together with the “Other” class are negative examples. This introduces many false</context>
<context position="16989" citStr="Ling and Weld, 2012" startWordPosition="2812" endWordPosition="2815"> 0.4 0.5 0.6 0.7 0.8 0.6 0.4 0.2 0 Precision 0.25 0.8 0.6 0.4 1 OrigDS DS_Figer ETD ETD+Neg More Ensemble(ADS) Precision 813 Pmicro Rmicro Pmacro Rmacro 0.950 0.845 0.947 0.626 Table 2: Performance of the NED component 4.3 Contribution of Each Component In Figure 2, with the entity type detection (ETD), the performance is better than the original DS method (OrigDS). As for the performance of NED in the Entity Type Detection, the Micro/Macro Precision-Recall of our NED component are in Table 2. ETD is also better than adding the entity types of the pair to the feature vector (DS Figer)2 as in (Ling and Weld, 2012). If we also employ the negative example construction strategy in Section 3.2 (ETD+Neg), the precision of the top ranked instances is improved. By adding more features (More) and employing the ensemble learning (Ensemble(ADS)) to ETD+Neg, the performance is further improved. 5 Conclusion This paper dealt with the problem of improving the accuracy of DS. We find some factors are crucially important, including valid entity type detection, negative training examples construction and ensembles. We have proposed an approach to handle these issues. Experiments show that the approach is very effectiv</context>
</contexts>
<marker>Ling, Weld, 2012</marker>
<rawString>X. Ling and D.S. Weld. 2012. Fine-grained entity recognition. In Proceedings of the 26th Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1934" citStr="Mintz et al., 2009" startWordPosition="282" endWordPosition="285"> such as “(Bill Gates, BornIn, Seattle)”. An important task is to enrich such KBs by extracting more facts from text. Specifically, this paper focuses on extracting facts for existing relations. This is different from OpenIE (Banko et al., 2007; Carlson et al., 2010) which needs to discover new relations. Given large amounts of labeled sentences, supervised methods are able to achieve good performance (Zhao and Grishman, 2005; Bunescu and Mooney, 2005). However, it is difficult to handle large scale corpus due to the high cost of labeling. Recently an approach called distant supervision (DS) (Mintz et al., 2009) was proposed, which does not require any labels on the text. It treats the extraction problem as classifying * The contact author. a candidate entity pair to a relation. Then an existing fact in a KB can be used as a labeled example whose label is the relation name. Then the features of all the sentences (from a given text corpus) containing the entity pair are merged as the feature of the example. Finally a multi-class classifier is trained. However, the accuracy of DS is not satisfying. Some variants have been proposed to improve the performance (Riedel et al., 2010; Hoffmann et al., 2011; </context>
<context position="12795" citStr="Mintz et al., 2009" startWordPosition="2115" endWordPosition="2118">-n relation if αsrc &gt; αdst and as a 1-to-n relation otherwise. I{e′|(ei,e′) ∈ r}I (4) I{e′|(e′,ei) ∈ r}I Negative examples For a candidate entity pair (e1, e2) not in the relation r of the KB, we first determine whether it is 1-to-n or n-to-1 using the above method. If r is 1-to-1/n-to-1 and e1 exists in some fact of r as the source entity, then (e1, e2) is a negative example as it violates the 1-to-1/n-to-1 constraint. If r is 1-to-n, the judgement is similar and just simply swap the source and destination entities of the relation. 3.3 Feature Space Partition and Ensemble The features of DS (Mintz et al., 2009) are very sparse in the corpus. We add some features in (Yao et al., 2011): Trigger Words (the words on the dependency path except stop words) and Entity String (source entity and destination entity). 1 M ∑M i=1 1 M ∑M i=1 1 αsrc = Msrc 1 Msrc∑ i=1 Mdst∑ i=1 Mdst αdst = 812 Relation Taka Ensemble works written 0.76 0.98 river/basin countries 0.48 1 /film/director/film 0.82 1 Average 0.79 0.89 Table 1: Manual evaluation of top-ranked 50 relation instances for the most frequent 15 relations. We find that without considering the reversed order of entity pairs in a sentence, the precision can be h</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003–1011, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
</authors>
<title>End-to-end relation extraction using distant supervision from external semantic repositories.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>277--282</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3854" citStr="Nguyen and Moschitti (2011" startWordPosition="617" endWordPosition="620">mpact on the accuracy but has not been touched or well handled before. First, each relation has its own schema definition, i.e., the source entity and the destination entity should be of valid types, which is overlooked in DS. Therefore, we propose a component of entity type detection to check it. Second, DS introduces many false negative examples into the training set and we propose a new method to construct negative training examples. Third, we find it is difficult for a single classifier to achieve high accuracy and hence we train multiple classifiers and ensemble them. We also notice that Nguyen and Moschitti (2011a) and Nguyen and Moschitti (2011b) utilize external information such as more facts from Yago and labeled sentences from ACE to improve the 810 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 810–815, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics performance. These methods can also be equipped with the approach proposed in this paper. 2 Critical Factors Affecting the Accuracy DS has four steps: (1) Detect candidate entity pairs in the corpus. (2) Label the candidate pairs using the KB. (3) Extract features for </context>
</contexts>
<marker>Nguyen, Moschitti, 2011</marker>
<rawString>Truc-Vien T. Nguyen and Alessandro Moschitti. 2011a. End-to-end relation extraction using distant supervision from external semantic repositories. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 277–282, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Joint distant and direct supervision for relation extraction.</title>
<date>2011</date>
<booktitle>In Proceeding of the International Joint Conference on Natural Language Processing,</booktitle>
<pages>732--740</pages>
<contexts>
<context position="3854" citStr="Nguyen and Moschitti (2011" startWordPosition="617" endWordPosition="620">mpact on the accuracy but has not been touched or well handled before. First, each relation has its own schema definition, i.e., the source entity and the destination entity should be of valid types, which is overlooked in DS. Therefore, we propose a component of entity type detection to check it. Second, DS introduces many false negative examples into the training set and we propose a new method to construct negative training examples. Third, we find it is difficult for a single classifier to achieve high accuracy and hence we train multiple classifiers and ensemble them. We also notice that Nguyen and Moschitti (2011a) and Nguyen and Moschitti (2011b) utilize external information such as more facts from Yago and labeled sentences from ACE to improve the 810 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 810–815, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics performance. These methods can also be equipped with the approach proposed in this paper. 2 Critical Factors Affecting the Accuracy DS has four steps: (1) Detect candidate entity pairs in the corpus. (2) Label the candidate pairs using the KB. (3) Extract features for </context>
</contexts>
<marker>Nguyen, Moschitti, 2011</marker>
<rawString>Truc-Vien T Nguyen and Alessandro Moschitti. 2011b. Joint distant and direct supervision for relation extraction. In Proceeding of the International Joint Conference on Natural Language Processing, pages 732–740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing. In</title>
<date>2006</date>
<booktitle>In Proc. of LREC-2006,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="14704" citStr="Nivre et al., 2006" startWordPosition="2436" endWordPosition="2439">facts of each relation were equally split to two parts for training and testing. Wikipedia 2009 was used as the target corpus, where 800,000 articles were used for training and 400,000 for testing. During the NED phrase, there are 94 unique entity types (they are also relations in Freebase) for the source and destination entities. Note that some entity types contain too few entities and they are discarded. We used 500,000 Wikipedia articles (2,000,000 sentences) for generating training data for the NED component. We used Open NLP POS tagger, Standford NER (Finkel et al., 2005) and MaltParser (Nivre et al., 2006) to label/tag sentences. We employed liblinear (Fan et al., 2008) as classifiers for NED and relation extraction and the solver is L2LR. 4.2 Performance of Relation Extraction Held-out Evaluation. We evaluate the performance on the half hold-on facts for testing. We compared performance of the n = 50, 000 best extracted relation instances of each method and the Precision-Recall (PR) curves are in Figure 1 and Recall Figure 1: Performance of different methods. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall Figure 2: Contributions of different components. Figure 2. For a candidate fact without any entity </context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In In Proc. of LREC-2006, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Sixteenth European Conference on Machine Learning (ECML2010),</booktitle>
<pages>148--163</pages>
<contexts>
<context position="2509" citStr="Riedel et al., 2010" startWordPosition="383" endWordPosition="386">istant supervision (DS) (Mintz et al., 2009) was proposed, which does not require any labels on the text. It treats the extraction problem as classifying * The contact author. a candidate entity pair to a relation. Then an existing fact in a KB can be used as a labeled example whose label is the relation name. Then the features of all the sentences (from a given text corpus) containing the entity pair are merged as the feature of the example. Finally a multi-class classifier is trained. However, the accuracy of DS is not satisfying. Some variants have been proposed to improve the performance (Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). They argue that DS introduces a lot of noise into the training data by merging the features of all the sentences containing the same entity pair, because a sentence containing the entity pair of a relation may not talk about the relation. Riedel et al. (2010) and Hoffmann et al. (2011) introduce hidden variables to indicate whether a sentence is noise and try to infer them from the data. Takamatsu et al. (2012) design a generative model to identify noise patterns. However, as shown in the experiments (Section 4), the above variants do not lead </context>
<context position="5285" citStr="Riedel et al., 2010" startWordPosition="847" endWordPosition="850">al results). Valid entity type detection. In DS, a sentence with a candidate entity pair a sentence with two candidate entities is noisy. First, the schema of each relation in the KB requires that the source and destination entities should be of valid types, e.g., the source and destination entity of the relation “DirectorOfFilm” should be of the types “Director” and “Film” respectively. If the two entities in a sentence are not of the valid types, the sentence is noisy. Second, the sentence may not talk about the relation even when the two entities are of the valid types. The previous works (Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012) do not distinguish the two types of noise but directly infer the overall noise from the data. We argue that the first type of noise is very difficult to be inferred just from the noisy relational labels. Instead, we decouple the two types of noise, and utilize external labeled data, i.e., the Wikipedia anchor links, to train an entity type detection module to handle the first type of noise. We notice that when Ling and Weld (2012) studied a fine-grained NER method, they applied the method to relation extraction by adding the recognized entity ta</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the Sixteenth European Conference on Machine Learning (ECML2010), pages 148–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shingo Takamatsu</author>
<author>Issei Sato</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Reducing wrong labels in distant supervision for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>721--729</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2557" citStr="Takamatsu et al., 2012" startWordPosition="391" endWordPosition="394"> was proposed, which does not require any labels on the text. It treats the extraction problem as classifying * The contact author. a candidate entity pair to a relation. Then an existing fact in a KB can be used as a labeled example whose label is the relation name. Then the features of all the sentences (from a given text corpus) containing the entity pair are merged as the feature of the example. Finally a multi-class classifier is trained. However, the accuracy of DS is not satisfying. Some variants have been proposed to improve the performance (Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). They argue that DS introduces a lot of noise into the training data by merging the features of all the sentences containing the same entity pair, because a sentence containing the entity pair of a relation may not talk about the relation. Riedel et al. (2010) and Hoffmann et al. (2011) introduce hidden variables to indicate whether a sentence is noise and try to infer them from the data. Takamatsu et al. (2012) design a generative model to identify noise patterns. However, as shown in the experiments (Section 4), the above variants do not lead to much improvement in accuracy. In this paper, </context>
<context position="5333" citStr="Takamatsu et al., 2012" startWordPosition="855" endWordPosition="858"> DS, a sentence with a candidate entity pair a sentence with two candidate entities is noisy. First, the schema of each relation in the KB requires that the source and destination entities should be of valid types, e.g., the source and destination entity of the relation “DirectorOfFilm” should be of the types “Director” and “Film” respectively. If the two entities in a sentence are not of the valid types, the sentence is noisy. Second, the sentence may not talk about the relation even when the two entities are of the valid types. The previous works (Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012) do not distinguish the two types of noise but directly infer the overall noise from the data. We argue that the first type of noise is very difficult to be inferred just from the noisy relational labels. Instead, we decouple the two types of noise, and utilize external labeled data, i.e., the Wikipedia anchor links, to train an entity type detection module to handle the first type of noise. We notice that when Ling and Weld (2012) studied a fine-grained NER method, they applied the method to relation extraction by adding the recognized entity tags to the features. We worry that the contributi</context>
<context position="15627" citStr="Takamatsu et al. (2012)" startWordPosition="2594" endWordPosition="2597">0, 000 best extracted relation instances of each method and the Precision-Recall (PR) curves are in Figure 1 and Recall Figure 1: Performance of different methods. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall Figure 2: Contributions of different components. Figure 2. For a candidate fact without any entity existing in Freebase, we are not able to judge whether it is correct. Thus we only evaluate the candidate facts that at least one entity occurs as the source or destination entity in the test fact set. In Figure 1, we compared our method with two previous methods: MultiR (Hoffmann et al., 2011) and Takamatsu et al. (2012) (Taka). For MultiR, we used the author’s implementation1. We re-implemented Takamatsu’s algorithm. As Takamatsu’s dataset (903,000 Wikipedia articles for training and 400,000 for testing) is very similar to ours, we used their best reported parameters. Our method leads to much better performance. Manual Evaluation. Following (Takamatsu et al., 2012), we selected the top 50 ranked (according to their classification probabilities) relation facts of the 15 largest relations. We compared our results with those of Takamatsu et al. (2012) and we achieved greater average precision (Table 1). 1availa</context>
</contexts>
<marker>Takamatsu, Sato, Nakagawa, 2012</marker>
<rawString>Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 721– 729, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Aria Haghighi</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Structured relation discovery using generative models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1456--1466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="12869" citStr="Yao et al., 2011" startWordPosition="2130" endWordPosition="2133">∈ r}I (4) I{e′|(e′,ei) ∈ r}I Negative examples For a candidate entity pair (e1, e2) not in the relation r of the KB, we first determine whether it is 1-to-n or n-to-1 using the above method. If r is 1-to-1/n-to-1 and e1 exists in some fact of r as the source entity, then (e1, e2) is a negative example as it violates the 1-to-1/n-to-1 constraint. If r is 1-to-n, the judgement is similar and just simply swap the source and destination entities of the relation. 3.3 Feature Space Partition and Ensemble The features of DS (Mintz et al., 2009) are very sparse in the corpus. We add some features in (Yao et al., 2011): Trigger Words (the words on the dependency path except stop words) and Entity String (source entity and destination entity). 1 M ∑M i=1 1 M ∑M i=1 1 αsrc = Msrc 1 Msrc∑ i=1 Mdst∑ i=1 Mdst αdst = 812 Relation Taka Ensemble works written 0.76 0.98 river/basin countries 0.48 1 /film/director/film 0.82 1 Average 0.79 0.89 Table 1: Manual evaluation of top-ranked 50 relation instances for the most frequent 15 relations. We find that without considering the reversed order of entity pairs in a sentence, the precision can be higher, but the recall decreases. For example, for the entity pair ⟨Ventura</context>
</contexts>
<marker>Yao, Haghighi, Riedel, McCallum, 2011</marker>
<rawString>Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. 2011. Structured relation discovery using generative models. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1456–1466, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>419--426</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1744" citStr="Zhao and Grishman, 2005" startWordPosition="251" endWordPosition="254">on building large structural knowledge bases (KB) such as Freebase, Yago, etc. They are composed of relational facts often represented in the form of a triplet, (SrcEntity, Relation, DstEntity), such as “(Bill Gates, BornIn, Seattle)”. An important task is to enrich such KBs by extracting more facts from text. Specifically, this paper focuses on extracting facts for existing relations. This is different from OpenIE (Banko et al., 2007; Carlson et al., 2010) which needs to discover new relations. Given large amounts of labeled sentences, supervised methods are able to achieve good performance (Zhao and Grishman, 2005; Bunescu and Mooney, 2005). However, it is difficult to handle large scale corpus due to the high cost of labeling. Recently an approach called distant supervision (DS) (Mintz et al., 2009) was proposed, which does not require any labels on the text. It treats the extraction problem as classifying * The contact author. a candidate entity pair to a relation. Then an existing fact in a KB can be used as a labeled example whose label is the relation name. Then the features of all the sentences (from a given text corpus) containing the entity pair are merged as the feature of the example. Finally</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 419–426, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>