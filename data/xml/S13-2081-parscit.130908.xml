<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000164">
<title confidence="0.978524">
REACTION: A naive machine learning approach for sentiment
classification
</title>
<author confidence="0.962684">
Silvio Moreira
</author>
<affiliation confidence="0.799422">
IST/INESC-ID
</affiliation>
<address confidence="0.867294333333333">
Rua Alves Redol, 9
1000-029 Lisboa
Portugal
</address>
<email confidence="0.814911">
samir@inesc-id.pt
</email>
<author confidence="0.333751">
Jo˜ao Filgueiras
</author>
<affiliation confidence="0.287783">
INESC-ID
</affiliation>
<address confidence="0.699205">
Rua Alves Redol, 9
1000-029 Lisboa
Portugal
</address>
<email confidence="0.79226">
jfilgueiras@inesc-id.pt
</email>
<author confidence="0.629491">
Bruno Martins
</author>
<affiliation confidence="0.477456">
IST/INESC-ID
</affiliation>
<address confidence="0.807687333333333">
Rua Alves Redol, 9
1000-029 Lisboa
Portugal
</address>
<email confidence="0.990369">
bruno.g.martins@ist.utl.pt
</email>
<author confidence="0.669761666666667">
Francisco Couto
LASIGE - FCUL
Edificio C6 Piso 3
</author>
<affiliation confidence="0.485459">
Campo Grande
</affiliation>
<address confidence="0.8714185">
1749 - 016 Lisboa
Portugal
</address>
<email confidence="0.981737">
fcouto@di.fc.ul.pt
</email>
<sectionHeader confidence="0.995221" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997735388888889">
We evaluate a naive machine learning ap-
proach to sentiment classification focused
on Twitter in the context of the sentiment
analysis task of SemEval-2013. We employ
a classifier based on the Random Forests al-
gorithm to determine whether a tweet ex-
presses overall positive, negative or neu-
tral sentiment. The classifier was trained
only with the provided dataset and uses as
main features word vectors and lexicon word
counts. Our average F-score for all three
classes on the Twitter evaluation dataset
was 51.55%. The average F-score of both
positive and negative classes was 45.01%.
For the optional SMS evaluation dataset our
overall average F-score was 58.82%. The
average between positive and negative F-
scores was 50.11%.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999621111111111">
Sentiment Analysis is a growing research field, es-
pecially on web social networks. In this setting,
users share very diverse messages such as real-
time reactions to news, events and daily experi-
ences. The ability to tap on a vast repository of
opinions, such as Twitter, where there is great di-
versity of topics, has become an important goal
for many different applications. However, due to
the nature of the text, NLP systems face additional
</bodyText>
<figure confidence="0.850542833333333">
M´ario J. Silva
IST/INESC-ID
Rua Alves Redol, 9
1000-029 Lisboa
Portugal
mjs@inesc-id.pt
</figure>
<bodyText confidence="0.999301904761905">
challenges in this context. Shared messages, such
as tweets, are very short and users tend to resort to
highly informal an noisy speech.
Following this trend, the 2013 edition of Se-
mEval1 included a sentiment analysis on Twitter
task (SemEval-2013 Task 2). Participants were
asked to implement a system capable of determin-
ing whether a given tweet expresses positive, neg-
ative or neutral sentiment. To help in the develop-
ment of the system, an annotated training corpus
was released. Systems that used only the given
corpus for training were considered constrained,
while others were considered unconstrained. The
submitted prototypes were evaluated in a dataset
consisting of around 3700 tweets of several topics.
The metric used was the average F-score between
the positive and negative classes.
Our goal with this participation was to create a
baseline system from which we can build upon and
perform experiments to compare new approaches
with the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.999798" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997213666666667">
The last decade saw a growing interest in systems
to automatically process sentiment in text. Many
approaches to detect subjectivity and determine
</bodyText>
<footnote confidence="0.9806535">
1Proceedings of the 7th International Workshop on Se-
mantic Evaluation (SemEval 2013), in conjunction with the
Second Joint Conference on Lexical and Computational Se-
mantics (*SEM 2013)
</footnote>
<page confidence="0.948586">
490
</page>
<bodyText confidence="0.9924416">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 490–494, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
polarity of opinions in news articles, weblogs and
product reviews have been proposed (Pang et al.,
2002; Pang et al., 2004; Wiebe et al., 2005; Wil-
son et al., 2005). This sub-field of NLP, known as
Sentiment Analysis is presented in great depth in
(Liu, 2012).
The emergence and proliferation of microblog
platforms created a medium where people express
and convey all kinds of information. In particu-
lar, these platforms are a rich source of subjec-
tive and opinionated text, which has motivated
the application of similar techniques to this do-
main. However, in this context, messages tend
to be very short and highly informal, full of ty-
pos, slang and unconventional spelling, posing ad-
ditional challenges to NLP systems. In fact, early
experiments in Sentiment Analysis in the context
of Twitter (Barbosa et al., 2010; Davidov et al.,
2010; Koulompis et al., 2011; Pak et al., 2010;
Bifet et al., 2010) show that the techniques that
proved effective in other domains are not sufficient
in the microblog setting. In the spirit of these ap-
proaches, we included a preprocessing step, fol-
lowed by feature extraction focusing on word,
lexical and Twitter-specific features. Finally, we
use annotated data to train an automatic classifier
based on the Random Forests (Breiman, 2001) and
BESTrees (Sun et al., 2011) learning algorithms.
</bodyText>
<sectionHeader confidence="0.999" genericHeader="method">
3 Resources
</sectionHeader>
<bodyText confidence="0.9997387">
Two annotated datasets were made available to
participants of SemEval-2013 Task 2: one for
training purposes which was to contain 8000 to
12000 tweets; and another, for development, con-
taining 2000. The combined datasets ended up
amounting to a little over 7500 tweets. The distri-
bution of positives, negatives and neutrals for the
combined datasets can be found in Table 1. Nearly
half of all tweets belonged to the neutral class, and
negatives represent just 15% of these datasets.
</bodyText>
<table confidence="0.88999">
Class Number
Positive 37%
Negative 15%
Neutral 48%
</table>
<tableCaption confidence="0.999485">
Table 1: Class distribution of annotated data.
</tableCaption>
<bodyText confidence="0.9913605">
Random examples of each class drawn from the
datasets are shown in Table 2.
</bodyText>
<subsectionHeader confidence="0.308664">
Positive:
</subsectionHeader>
<bodyText confidence="0.7245324">
1 Louis inspired outfit on Monday and Zayn
inspired outfit today..4/5 done just need Harry
2 waking up to a Niners win, makes Tuesday
get off to a great start! 21-3 over the cards
and 2 games clear in the NFC West.
</bodyText>
<sectionHeader confidence="0.409004" genericHeader="method">
Negative:
</sectionHeader>
<listItem confidence="0.594832230769231">
3 Sitting at home on a Saturday night doing
absolutely nothing... Guess I’ll just watch
Greys Anatomy all night. #lonerproblems
#greysanatomy
4 Life just isn’t the same when there is no
Pretty Little Liars on Tuesday nights.
Neutral:
5 Won the match #getin . Plus,
tomorrow is a very busy day, with
Awareness Day’s and debates. Gulp. Debates
6 @ Nenaah oh cause my friend got something
from china and they said it will take at least 6
to 8 weeks and it came in the 2nd week :P
</listItem>
<tableCaption confidence="0.983799">
Table 2: Random examples of annotated tweets.
</tableCaption>
<sectionHeader confidence="0.996102" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999994">
Given our goal of creating a baseline system, we
experimented with a common set of features used
in sentiment analysis. The messages were mod-
elled as a combination of binary (or presence) uni-
grams, lexical features and Twitter-specific fea-
tures. We decided to follow a supervised approach
by learning a Random Forests classifier from the
annotated data provided by the organisers of the
workshop (see Section 3). In summary, the devel-
opment of our system consisted of four steps: 1)
preprocessing of the data, 2) feature extraction, 3)
learning the classifier, and 4) applying the classi-
fier to the test set.
</bodyText>
<subsectionHeader confidence="0.990173">
4.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999897666666667">
The lexical variation introduced by typos, ab-
breviations, slang and unconventional spelling,
leads to very large vocabularies. The resulting
</bodyText>
<page confidence="0.995057">
491
</page>
<bodyText confidence="0.999892545454545">
sparse vector representations with few non-zero
values hamper the learning process. In order to
tackle this problem, we replaced user mentions
(@&lt;username&gt;) with a fixed tag &lt;USER&gt; and
URLs with the tag &lt;URL&gt;. Then, each sentence
was normalised by converting to lower-case and
reducing character repetitions to at most 3 charac-
ters (e.g. ”heelloooooo!” would be normalised to
”heellooo!”). Finally, we performed the lemma-
tisation of the sentence using the Morphadorner2
software.
</bodyText>
<subsectionHeader confidence="0.986641">
4.2 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999828419354839">
After the preprocessing step, we extract a vector
consisting of the top uni-grams present in the train-
ing set and represent individual messages in terms
of this vector. For each message we also compute
the frequency of smileys and words with prior sen-
timent polarity using a sentiment lexicon. Finally,
we include the harmonic mean of positive and neg-
ative words. Next we explain each feature in more
detail.
Word vector: a sparse word vector containing
the top 25.000 most frequent words that occur in
the training set. This feature aims at capturing re-
lations between certain words and overall message
polarity. The vector was extracted using the Weka
toolkit (Hall et al., 2009) with the stop word list
option.
Lexicon word count: positive and negative sen-
timent word counts. When the word is preceded by
a negation particle we invert the polarity. We used
Bing Liu’s Opinion Lexicon3 that includes 2006
positive and 4783 negative words and is especially
tailored for social media because it considers mis-
spellings, slang and other domain specific varia-
tions.
Smileys count: a count of positive and negative
smileys that appear in the tweet. We take advan-
tage of these constructs being especially indicative
of the overall expressed sentiment in a text (Davi-
dov et al., 2010). Although there are smiley lexi-
cons, such as the one used on SentiStrength4, we
used regular expressions to capture most common
</bodyText>
<footnote confidence="0.99978875">
2http://morphadorner.northwestern.edu/
3http://www.cs.uic.edu/˜liub/FBS/
sentiment-analysis.html
4http://sentistrength.wlv.ac.uk
</footnote>
<bodyText confidence="0.9990119375">
smileys in a flexible way.
Hashtag count: a count of positive and negative
hashtags. This feature also uses Bing Liu’s lexicon
to determine wether a word contained in an hash-
tag is positive or negative. The rationale behind
this feature is that positive or negative words in the
form of hashtags can have a stronger meaning than
regular words (Davidov et al., 2010).
Positive/negative harmonic mean: harmonic
mean between positive and negative token counts,
including words and hashtags.
In an attempt to further reduce the dimensional-
ity of the feature space we computed the principal
components of the word vector using the Principal
Components Analysis filter in Weka but observed
that this yielded worse results.
</bodyText>
<subsectionHeader confidence="0.999258">
4.3 Learning the classifier
</subsectionHeader>
<bodyText confidence="0.999966384615384">
To implement our classifier we used the Weka ma-
chine learning framework and experimented with
two ensemble algorithms: Random Forests and
BESTrees. We eventually dropped the use of BE-
STrees as initial results were worse.
We attempted to use most of the data while be-
ing able to effectively measure the performance of
the classifier. Therefore we used the totality of
both sets for training and evaluated using 10 fold
cross-validation.
Since we used only the annotated dataset that
was provided for this task, our approach is consid-
ered constrained.
</bodyText>
<sectionHeader confidence="0.999924" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.992563">
Our results with 10 fold cross-validation using the
submitted classifier, are presented in Table 3.
</bodyText>
<table confidence="0.9994098">
Class Precision Recall F-score
positive 61.0% 63.9% 62.4%
negative 54.1% 26.8% 35.8%
neutral 64.7% 72.4% 68.3%
average F-score (pos/neg) 49.1%
</table>
<tableCaption confidence="0.999879">
Table 3: Cross-validation results using the training set.
</tableCaption>
<bodyText confidence="0.98381775">
Task evaluation results are presented in Table 4
for tweets. Our approach ranked 44th out of 48
participants. The evaluation dataset had a sim-
ilar class distribution to the annotated datasets,
</bodyText>
<page confidence="0.996881">
492
</page>
<bodyText confidence="0.9950535">
with almost half being neutral, and just 14% neg-
ative. Preliminary results with cross-validation
were similar to those of the final evaluation for
Twitter.
</bodyText>
<table confidence="0.999493">
Class Precision Recall F-score
positive 62.52% 55.28% 58.68%
negative 55.74% 21.80% 31.34%
neutral 56.54% 75.43% 64.63%
average F-score (pos/neg) 45.01%
</table>
<tableCaption confidence="0.999829">
Table 4: Task evaluation results for Tweets.
</tableCaption>
<bodyText confidence="0.997389444444444">
Also included in SemEval-2013 Task 2 was an
evaluation using a SMS dataset to understand if a
classifier trained using tweets could be applied to
SMS messages. SMS results are shown in Table 5.
In this case our approach ranked 23th out of 42 par-
ticipants. The SMS evaluation dataset was com-
posed of more than half neutral messages (58%),
and similarly distributed positives (23%) and neg-
atives (19%).
</bodyText>
<table confidence="0.9989162">
Class Precision Recall F-score
positive 53.66% 59.50% 56.45%
negative 60.54% 34.26% 43.76%
neutral 72.91% 79.90% 76.27%
average F-score (pos/neg) 50.11%
</table>
<tableCaption confidence="0.999393">
Table 5: Task evaluation results for SMS.
</tableCaption>
<sectionHeader confidence="0.973792" genericHeader="discussions">
6 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999988897435897">
As expected, our naive approach performs poorly
in the context of Twitter messages. The obtained
results are in line with similar approaches de-
scribed in the literature and we found that Ran-
dom Forests achieve the same performance as
other learning algorithms tried for the same task
(Koulompis et al., 2011).
The uneven distribution of classes in the data
may have also contributed to the low performance
of the classifier. Although the neutral class was
not considered in the evaluation, the datasets had
a great predominance of neutral messages whereas
the negative examples only accounted for 15% of
the corpus. This suggests that it could be useful to
use a minority class over-sampling method, such
as SMOTE (Chawla, 2002), to reduce the effect
of this imbalance on the data. We used n-grams
to model the words that compose each message.
However, this approach leads to very sparse rep-
resentations, thus becoming important to consider
techniques that reduce feature space. We experi-
mented with PCA, without success, but we still be-
lieve that applying feature selection algorithms or
denser word representations (Turian et al., 2010)
could improve performance in this task.
We find that our classifier performs better on the
SMS dataset. This might be explained by the fact
that SMS messages tend to be more direct, whereas
the same tweet can express, or show signs of, con-
tradictory sentiments. In fact, our naive approach
outperforms other systems that had better results
in the Twitter dataset, but it is difficult to say why,
given that we do not have access to the SMS test
set annotations.
Despite the poor ranking results, we achieved
our goal of performing basic experiments in the
task of sentiment analysis in Twitter and developed
a baseline system that will serve as a starting point
for future research.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999955555555555">
This work was partially supported by FCT
(Portuguese research funding agency) under
project grants UTA-Est/MAI/0006/2009 (RE-
ACTION) and PTDC/CPJ-CPO/116888/2010
(POPSTAR). FCT also supported scholarship
SFRH/BD/89020/2012. This research was also
funded by the PIDDAC Program funds (INESC-
ID multi annual funding) and the LASIGE multi
annual support.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993841818181818">
Barbosa, L., and Feng, J. 2010. Robust sentiment de-
tection on twitter from biased and noisy data. Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, pp. 36-44.
Bifet, A., and Frank, E. 2010. Sentiment knowledge
discovery in twitter streaming data. Discovery Sci-
ence.
Breiman, L. 2001. Random forests. Machine learning,
45(1), 5-32.
Chawla, N. V., Bowyer, K. W., Hall, L. O., and
Kegelmeyer, W. P. 2002 SMOTE: synthetic minority
</reference>
<page confidence="0.996505">
493
</page>
<reference confidence="0.998921094339622">
over-sampling technique. Journal of Artificial Intel-
ligence Research, 16, 321-357.
Davidov, D., Tsur, O., and Rappoport, A. 2010 En-
hanced sentiment learning using twitter hashtags
and smileys. Proceedings of the 23rd International
Conference on Computational Linguistics: Posters.
Pages 241-249. Association for Computational Lin-
guistics.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reute-
mann, P., and Witten, I. H. 2009. The WEKA
Data Mining Software: An Update SIGKDD Ex-
plorations, Volume 11, Issue 1.
Kouloumpis, E., Wilson, T., and Moore, J. 2011. Twit-
ter sentiment analysis: The good the bad and the
omg. Proceedings of the Fifth International AAAI
Conference on Weblogs and Social Media, 538541.
Liu, B. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technolo-
gies, 5(1), 1167.
Pak, A., and Paroubek, P. 2010. Twitter as a corpus for
sentiment analysis and opinion mining. Proceedings
of LREC.
Pang, B., Lee, L., and Vaithyanathan, S. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. Proceedings of the ACL-02 conference
on Empirical methods in natural language process-
ing. Volume 10, pp. 79-86. Association for Compu-
tational Linguistics.
Pang, B. and Lee, L. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. Proceedings of the 42nd an-
nual meeting on Association for Computational Lin-
guistics.
Sun, Q. and Pfahringer, B. 2011. Bagging Ensemble
Selection. Proceedings of the 24th Australasian Joint
Conference on Artificial Intelligence (AI’11), Perth,
Australia, pages 251-260. Springer.
Turian, J., Ratinov, L., and Bengio, Y. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (pp. 384-394). Association for Computa-
tional Linguistics.
Wiebe, J. and Riloff, E. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
Computational Linguistics and Intelligent Text Pro-
cessing, pages 486-497, Springer.
Wilson, T., Wiebe, J., and Hoffmann, P. 2005. Rec-
ognizing contextual polarity in phrase-level senti-
ment analysis. Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pp. 347-354. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.999003">
494
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.025957">
<title confidence="0.9985185">REACTION: A naive machine learning approach for classification</title>
<author confidence="0.983288">Silvio</author>
<affiliation confidence="0.7677655">IST/INESC-ID Rua Alves Redol,</affiliation>
<address confidence="0.902875">1000-029 Lisboa Portugal</address>
<email confidence="0.979789">samir@inesc-id.pt</email>
<author confidence="0.80178">Rua Alves Redol</author>
<address confidence="0.615497">1000-029 Portugal</address>
<email confidence="0.941321">jfilgueiras@inesc-id.pt</email>
<author confidence="0.983965">Bruno</author>
<affiliation confidence="0.984274">IST/INESC-ID</affiliation>
<address confidence="0.929104666666667">Rua Alves Redol, 9 1000-029 Lisboa Portugal</address>
<email confidence="0.987281">bruno.g.martins@ist.utl.pt</email>
<author confidence="0.992688">Francisco Couto</author>
<affiliation confidence="0.642421">LASIGE - FCUL</affiliation>
<address confidence="0.640535">Edificio C6 Piso 3 Campo Grande 1749 - 016 Lisboa Portugal</address>
<email confidence="0.995457">fcouto@di.fc.ul.pt</email>
<abstract confidence="0.993863473684211">We evaluate a naive machine learning approach to sentiment classification focused on Twitter in the context of the sentiment analysis task of SemEval-2013. We employ a classifier based on the Random Forests algorithm to determine whether a tweet expresses overall positive, negative or neutral sentiment. The classifier was trained only with the provided dataset and uses as main features word vectors and lexicon word counts. Our average F-score for all three classes on the Twitter evaluation dataset was 51.55%. The average F-score of both positive and negative classes was 45.01%. For the optional SMS evaluation dataset our overall average F-score was 58.82%. The average between positive and negative Fscores was 50.11%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Barbosa</author>
<author>J Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>36--44</pages>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Barbosa, L., and Feng, J. 2010. Robust sentiment detection on twitter from biased and noisy data. Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pp. 36-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bifet</author>
<author>E Frank</author>
</authors>
<title>Sentiment knowledge discovery in twitter streaming data. Discovery Science.</title>
<date>2010</date>
<marker>Bifet, Frank, 2010</marker>
<rawString>Bifet, A., and Frank, E. 2010. Sentiment knowledge discovery in twitter streaming data. Discovery Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<journal>Machine learning,</journal>
<volume>45</volume>
<issue>1</issue>
<pages>5--32</pages>
<contexts>
<context position="4595" citStr="Breiman, 2001" startWordPosition="709" endWordPosition="710">nal spelling, posing additional challenges to NLP systems. In fact, early experiments in Sentiment Analysis in the context of Twitter (Barbosa et al., 2010; Davidov et al., 2010; Koulompis et al., 2011; Pak et al., 2010; Bifet et al., 2010) show that the techniques that proved effective in other domains are not sufficient in the microblog setting. In the spirit of these approaches, we included a preprocessing step, followed by feature extraction focusing on word, lexical and Twitter-specific features. Finally, we use annotated data to train an automatic classifier based on the Random Forests (Breiman, 2001) and BESTrees (Sun et al., 2011) learning algorithms. 3 Resources Two annotated datasets were made available to participants of SemEval-2013 Task 2: one for training purposes which was to contain 8000 to 12000 tweets; and another, for development, containing 2000. The combined datasets ended up amounting to a little over 7500 tweets. The distribution of positives, negatives and neutrals for the combined datasets can be found in Table 1. Nearly half of all tweets belonged to the neutral class, and negatives represent just 15% of these datasets. Class Number Positive 37% Negative 15% Neutral 48%</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Breiman, L. 2001. Random forests. Machine learning, 45(1), 5-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N V Chawla</author>
<author>K W Bowyer</author>
<author>L O Hall</author>
<author>W P Kegelmeyer</author>
</authors>
<title>SMOTE: synthetic minority over-sampling technique.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>16</volume>
<pages>321--357</pages>
<marker>Chawla, Bowyer, Hall, Kegelmeyer, 2002</marker>
<rawString>Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. 2002 SMOTE: synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16, 321-357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>O Tsur</author>
<author>A Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics: Posters.</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4158" citStr="Davidov et al., 2010" startWordPosition="637" endWordPosition="640">resented in great depth in (Liu, 2012). The emergence and proliferation of microblog platforms created a medium where people express and convey all kinds of information. In particular, these platforms are a rich source of subjective and opinionated text, which has motivated the application of similar techniques to this domain. However, in this context, messages tend to be very short and highly informal, full of typos, slang and unconventional spelling, posing additional challenges to NLP systems. In fact, early experiments in Sentiment Analysis in the context of Twitter (Barbosa et al., 2010; Davidov et al., 2010; Koulompis et al., 2011; Pak et al., 2010; Bifet et al., 2010) show that the techniques that proved effective in other domains are not sufficient in the microblog setting. In the spirit of these approaches, we included a preprocessing step, followed by feature extraction focusing on word, lexical and Twitter-specific features. Finally, we use annotated data to train an automatic classifier based on the Random Forests (Breiman, 2001) and BESTrees (Sun et al., 2011) learning algorithms. 3 Resources Two annotated datasets were made available to participants of SemEval-2013 Task 2: one for traini</context>
<context position="8641" citStr="Davidov et al., 2010" startWordPosition="1377" endWordPosition="1381">lkit (Hall et al., 2009) with the stop word list option. Lexicon word count: positive and negative sentiment word counts. When the word is preceded by a negation particle we invert the polarity. We used Bing Liu’s Opinion Lexicon3 that includes 2006 positive and 4783 negative words and is especially tailored for social media because it considers misspellings, slang and other domain specific variations. Smileys count: a count of positive and negative smileys that appear in the tweet. We take advantage of these constructs being especially indicative of the overall expressed sentiment in a text (Davidov et al., 2010). Although there are smiley lexicons, such as the one used on SentiStrength4, we used regular expressions to capture most common 2http://morphadorner.northwestern.edu/ 3http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html 4http://sentistrength.wlv.ac.uk smileys in a flexible way. Hashtag count: a count of positive and negative hashtags. This feature also uses Bing Liu’s lexicon to determine wether a word contained in an hashtag is positive or negative. The rationale behind this feature is that positive or negative words in the form of hashtags can have a stronger meaning than regular words</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Davidov, D., Tsur, O., and Rappoport, A. 2010 Enhanced sentiment learning using twitter hashtags and smileys. Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Pages 241-249. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<date>2009</date>
<booktitle>The WEKA Data Mining Software: An Update SIGKDD Explorations, Volume 11, Issue 1.</booktitle>
<contexts>
<context position="8044" citStr="Hall et al., 2009" startWordPosition="1279" endWordPosition="1282"> the top uni-grams present in the training set and represent individual messages in terms of this vector. For each message we also compute the frequency of smileys and words with prior sentiment polarity using a sentiment lexicon. Finally, we include the harmonic mean of positive and negative words. Next we explain each feature in more detail. Word vector: a sparse word vector containing the top 25.000 most frequent words that occur in the training set. This feature aims at capturing relations between certain words and overall message polarity. The vector was extracted using the Weka toolkit (Hall et al., 2009) with the stop word list option. Lexicon word count: positive and negative sentiment word counts. When the word is preceded by a negation particle we invert the polarity. We used Bing Liu’s Opinion Lexicon3 that includes 2006 positive and 4783 negative words and is especially tailored for social media because it considers misspellings, slang and other domain specific variations. Smileys count: a count of positive and negative smileys that appear in the tweet. We take advantage of these constructs being especially indicative of the overall expressed sentiment in a text (Davidov et al., 2010). A</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., and Witten, I. H. 2009. The WEKA Data Mining Software: An Update SIGKDD Explorations, Volume 11, Issue 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kouloumpis</author>
<author>T Wilson</author>
<author>J Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the omg.</title>
<date>2011</date>
<booktitle>Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>538541</pages>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Kouloumpis, E., Wilson, T., and Moore, J. 2011. Twitter sentiment analysis: The good the bad and the omg. Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media, 538541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies,</title>
<date>2012</date>
<volume>5</volume>
<issue>1</issue>
<pages>1167</pages>
<contexts>
<context position="3576" citStr="Liu, 2012" startWordPosition="545" endWordPosition="546">onjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013) 490 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 490–494, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics polarity of opinions in news articles, weblogs and product reviews have been proposed (Pang et al., 2002; Pang et al., 2004; Wiebe et al., 2005; Wilson et al., 2005). This sub-field of NLP, known as Sentiment Analysis is presented in great depth in (Liu, 2012). The emergence and proliferation of microblog platforms created a medium where people express and convey all kinds of information. In particular, these platforms are a rich source of subjective and opinionated text, which has motivated the application of similar techniques to this domain. However, in this context, messages tend to be very short and highly informal, full of typos, slang and unconventional spelling, posing additional challenges to NLP systems. In fact, early experiments in Sentiment Analysis in the context of Twitter (Barbosa et al., 2010; Davidov et al., 2010; Koulompis et al.</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Liu, B. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies, 5(1), 1167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pak</author>
<author>P Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of LREC.</booktitle>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Pak, A., and Paroubek, P. 2010. Twitter as a corpus for sentiment analysis and opinion mining. Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>Proceedings of the ACL-02 conference on Empirical methods in natural language processing. Volume 10,</booktitle>
<pages>79--86</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="3420" citStr="Pang et al., 2002" startWordPosition="514" endWordPosition="517"> sentiment in text. Many approaches to detect subjectivity and determine 1Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013) 490 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 490–494, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics polarity of opinions in news articles, weblogs and product reviews have been proposed (Pang et al., 2002; Pang et al., 2004; Wiebe et al., 2005; Wilson et al., 2005). This sub-field of NLP, known as Sentiment Analysis is presented in great depth in (Liu, 2012). The emergence and proliferation of microblog platforms created a medium where people express and convey all kinds of information. In particular, these platforms are a rich source of subjective and opinionated text, which has motivated the application of similar techniques to this domain. However, in this context, messages tend to be very short and highly informal, full of typos, slang and unconventional spelling, posing additional challen</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, B., Lee, L., and Vaithyanathan, S. 2002. Thumbs up?: sentiment classification using machine learning techniques. Proceedings of the ACL-02 conference on Empirical methods in natural language processing. Volume 10, pp. 79-86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd annual meeting on Association for Computational Linguistics.</booktitle>
<marker>Pang, Lee, 2004</marker>
<rawString>Pang, B. and Lee, L. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. Proceedings of the 42nd annual meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Sun</author>
<author>B Pfahringer</author>
</authors>
<title>Bagging Ensemble Selection.</title>
<date>2011</date>
<booktitle>Proceedings of the 24th Australasian Joint Conference on Artificial Intelligence (AI’11),</booktitle>
<pages>251--260</pages>
<publisher>Springer.</publisher>
<location>Perth, Australia,</location>
<marker>Sun, Pfahringer, 2011</marker>
<rawString>Sun, Q. and Pfahringer, B. 2011. Bagging Ensemble Selection. Proceedings of the 24th Australasian Joint Conference on Artificial Intelligence (AI’11), Perth, Australia, pages 251-260. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>384--394</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="12819" citStr="Turian et al., 2010" startWordPosition="2024" endWordPosition="2027">dominance of neutral messages whereas the negative examples only accounted for 15% of the corpus. This suggests that it could be useful to use a minority class over-sampling method, such as SMOTE (Chawla, 2002), to reduce the effect of this imbalance on the data. We used n-grams to model the words that compose each message. However, this approach leads to very sparse representations, thus becoming important to consider techniques that reduce feature space. We experimented with PCA, without success, but we still believe that applying feature selection algorithms or denser word representations (Turian et al., 2010) could improve performance in this task. We find that our classifier performs better on the SMS dataset. This might be explained by the fact that SMS messages tend to be more direct, whereas the same tweet can express, or show signs of, contradictory sentiments. In fact, our naive approach outperforms other systems that had better results in the Twitter dataset, but it is difficult to say why, given that we do not have access to the SMS test set annotations. Despite the poor ranking results, we achieved our goal of performing basic experiments in the task of sentiment analysis in Twitter and d</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Turian, J., Ratinov, L., and Bengio, Y. 2010. Word representations: a simple and general method for semisupervised learning. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (pp. 384-394). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>E Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>486--497</pages>
<publisher>Springer.</publisher>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Wiebe, J. and Riloff, E. 2005. Creating subjective and objective sentence classifiers from unannotated texts. Computational Linguistics and Intelligent Text Processing, pages 486-497, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="3481" citStr="Wilson et al., 2005" startWordPosition="526" endWordPosition="530"> and determine 1Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013) 490 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 490–494, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics polarity of opinions in news articles, weblogs and product reviews have been proposed (Pang et al., 2002; Pang et al., 2004; Wiebe et al., 2005; Wilson et al., 2005). This sub-field of NLP, known as Sentiment Analysis is presented in great depth in (Liu, 2012). The emergence and proliferation of microblog platforms created a medium where people express and convey all kinds of information. In particular, these platforms are a rich source of subjective and opinionated text, which has motivated the application of similar techniques to this domain. However, in this context, messages tend to be very short and highly informal, full of typos, slang and unconventional spelling, posing additional challenges to NLP systems. In fact, early experiments in Sentiment A</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Wilson, T., Wiebe, J., and Hoffmann, P. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pp. 347-354. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>