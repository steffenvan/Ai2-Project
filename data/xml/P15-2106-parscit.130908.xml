<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001546">
<title confidence="0.949727">
Towards a Contextual Pragmatic Model to Detect Irony in Tweets
</title>
<author confidence="0.687126">
Jihen Karoui Farah Benamara Zitoune V´eronique Moriceau
</author>
<affiliation confidence="0.6641185">
IRIT, MIRACL IRIT, CNRS LIMSI-CNRS
Toulouse University, Sfax University Toulouse University Univ. Paris-Sud
</affiliation>
<email confidence="0.747726">
karoui@irit.fr benamara@irit.fr moriceau@limsi.fr
</email>
<author confidence="0.878296">
Nathalie Aussenac-Gilles Lamia Hadrich Belguith
</author>
<affiliation confidence="0.8583585">
IRIT, CNRS MIRACL
Nathalie.Aussenac-Gilles@irit.fr University of Sfax
</affiliation>
<email confidence="0.989324">
l.belguith@fsegs.rnu.tn
</email>
<page confidence="0.99875">
6
</page>
<sectionHeader confidence="0.982552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957166666667">
This paper proposes an approach to capture
the pragmatic context needed to infer irony in
tweets. We aim to test the validity of two main
hypotheses: (1) the presence of negations, as
an internal propriety of an utterance, can help
to detect the disparity between the literal and
the intended meaning of an utterance, (2) a
tweet containing an asserted fact of the form
Not(Pi) is ironic if and only if one can assess
the absurdity of Pl. Our first results are en-
couraging and show that deriving a pragmatic
contextual model is feasible.
</bodyText>
<sectionHeader confidence="0.987916" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999921097222223">
Irony is a complex linguistic phenomenon widely stud-
ied in philosophy and linguistics (Grice et al., 1975;
Sperber and Wilson, 1981; Utsumi, 1996). Despite the-
ories differ on how to define irony, they all commonly
agree that it involves an incongruity between the literal
meaning of an utterance and what is expected about the
speaker and/or the environment. For many researchers,
irony overlaps with a variety of other figurative devices
such as satire, parody, and sarcasm (Clark and Gerrig,
1984; Gibbs, 2000). In this paper, we use irony as an
umbrella term that covers these devices focusing for the
first time on the automatic detection of irony in French
tweets.
According to (Grice et al., 1975; Searle, 1979; At-
tardo, 2000), the search for a non-literal meaning starts
when the hearer realizes that the speaker’s utterance
is context-inappropriate, that is an utterance fails to
make sense against the context. For example, the tweet:
“Congratulation #lesbleus for your great match!” is
ironic if the French soccer team has lost the match. An
analysis of a corpus of French tweets shows that there
are two ways to infer such a context: (a) rely exclu-
sively on the lexical clues internal to the utterance, or
(b) combine these clues with an additional pragmatic
context external to the utterance. In (a), the speaker in-
tentionally creates an explicit juxtaposition of incom-
patible actions or words that can either have opposite
polarities, or can be semantically unrelated, as in “The
Voice is more important than Fukushima tonight”. Ex-
plicit opposition can also arise from an explicit posi-
tive/negative contrast between a subjective proposition
and a situation that describes an undesirable activity or
state. For instance, in “ I love when my phone turns the
volume down automatically” the writer assumes that
every one expects its cell phone to ring loud enough
to be heard. In (b), irony is due to an implicit opposi-
tion between a lexicalized proposition P describing an
event or state and a pragmatic context external to the
utterance in which P is false or is not likely to happen.
In other words, the writer asserts or affirms P while
he intends to convey P&apos; such that P&apos; = Not(P) or
P&apos; =� P. The irony occurs because the writer believes
that his audience can detect the disparity between P
and P&apos; on the basis of contextual knowledge or com-
mon background shared with the writer. For example,
in “#Hollande is really a good diplomat #Algeria.”, the
writer critics the foreign policy of the French president
Hollande in Algeria, whereas in ”The #NSA wiretapped
a whole country. No worries for #Belgium: it is not a
whole country.“, the irony occurs because the fact in
bold font is not true.
Irony detection is quite a hot topic in the research
community also due to its importance for efficient
sentiment analysis (Ghosh et al., 2015). Several ap-
proaches have been proposed to detect irony casting
the problem into a binary classification task relying
on a variety of features. Most of them are gleaned
from the utterance internal context going from n-grams
models, stylistic (punctuation, emoticons, quotations,
etc.), to dictionary-based features (sentiment and af-
fect dictionaries, slang languages, etc.). These fea-
tures have shown to be useful to learn whether a text
span is ironic/sarcastic or not (Burfoot and Baldwin,
2009; Davidov et al., 2010; Tsur et al., 2010; Gonzalez-
Ibanez et al., 2011; Reyes et al., 2013; Barbieri and
Saggion, 2014). However, many authors pointed out
the necessity of additional pragmatic features: (Ut-
sumi, 2004) showed that opposition, rhetorical ques-
tions and the politeness level are relevant. (Burfoot
and Baldwin, 2009) focused on satire detection in
newswire articles and introduced the notion of valid-
ity which models absurdity by identifying a conjunc-
</bodyText>
<page confidence="0.992701">
44
</page>
<bodyText confidence="0.941726714285714">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 644–650,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
tion of named entities present in a given document and
queries the web for the conjunction of those entities.
(Gonzalez-Ibanez et al., 2011) exploited the common
ground between speaker and hearer by looking if a
tweet is a reply to another tweet. (Reyes et al., 2013)
employed opposition in time (adverbs of time such as
now and suddenly) and context imbalance to estimate
the semantic similarity of concepts in a text to each
other. (Barbieri and Saggion, 2014) captured the gap
between rare and common words as well as the use of
common vs. rare synonyms. Finally, (Buschmeier et
al., 2014) measured the imbalance between the overall
polarity of words in a review and the star-rating. Most
of these pragmatic features rely on linguistic aspects of
the tweet by using only the text of the tweet. We aim
here to go further by proposing a novel computational
model able to capture the “outside of the utterance”
context needed to infer irony in implicit oppositions.
</bodyText>
<sectionHeader confidence="0.996297" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.99998764">
An analysis of a corpus of French ironic tweets ran-
domly chosen from various topics shows that more
than 62.75% of tweets contain explicit negation mark-
ers such as “ne...pas” (not) or negative polarity items
like “jamais” (never) or “personne” (nobody). Nega-
tion seems thus to be an important clue in ironic state-
ments, at least in French. This rises the following hy-
potheses: (H1) the presence of negations, as an internal
propriety of an utterance, can help to detect the dis-
parity between the literal and the intended meaning of
an utterance, and (H2) a tweet containing an asserted
fact of the form Not(P) is ironic if and only if one
can prove P on the basis of some external common
knowledge to the utterance shared by the author and
the reader.
To test the validity of the above hypotheses, we pro-
pose a novel three-step model involving three succes-
sive stages: (1) detect if a tweet is ironic or not relying
exclusively on the information internal to the tweet. We
use a supervised learning method relying on both state
of the art features whose efficiency has been empiri-
cally proved and new groups of features. (2) Test this
internal context against the “outside of the utterance”
context. We design an algorithm that takes the clas-
sifier’s outputs and corrects the misclassified ironic in-
stances of the form Not(P) by looking for P in reliable
external sources of information on the Web, such as
Wikipedia or online newspapers. We experiment when
labels are given by gold standard annotations and when
they are predicted by the classifier. (3) If the literal
meaning fails to make sense, i.e. P is found, then the
tweet is likely to convey a non-literal meaning.
To this end, we collected a corpus of 6,742 French
tweets using the Tweeter API focusing on tweets rel-
ative to a set of topics discussed in the media during
Spring 2014. Our intuition behind choosing such top-
ics is that a media-friendly topic is more likely to be
found in external sources of information. We chose
184 topics split into 9 categories (politics, sport, etc.).
For each topic, we selected a set of keywords with
and without hashtag: politics (e.g. Sarkozy, Hollande,
UMP), health (e.g. cancer, flu), sport (e.g. #Zlatan,
#FIFAworldcup), social media (e.g. #Facebook, Skype,
MSN), artists (e.g. Rihanna, Beyonc´e), TV shows (e.g.
TheVoice, XFactor), countries or cities (e.g. NorthKo-
rea, Brasil), the Arab Spring (e.g. Marzouki, Ben
Ali) and some other generic topics (e.g. pollution,
racism). Then we selected ironic tweets containing the
topic keywords, the #ironie or #sarcasme hashtag and a
negation word as well as ironic tweets containing only
the topic keywords with #ironie or #sarcasme hashtag
but no negation word. Finally, we selected non ironic
tweets that contained either the topic keywords and a
negation word, or only the topic keywords. We re-
moved duplicates, retweets and tweets containing pic-
tures which would need to be interpreted to understand
the ironic content. Irony hashtags (#ironie or #sar-
casme) are removed from the tweets for the following
experiments. To guarantee that tweets with negation
words contain true negations, we automatically identi-
fied negation usage of a given word using a French syn-
tactic dependency parser1. We then designed dedicated
rules to correct the parser’s decisions if necessary. At
the end, we got a total of 4,231 tweets with negation
and 2,511 without negation, among them, 30.42% are
ironic with negation and 72.36% are non ironic with
negation. At the end, we got a total of 4,231 tweets with
negation and 2,511 without negation: among them,
30.42% are ironic with negation and 72.36% are non
ironic with negation. To capture the effect of nega-
tion on our task, we split these tweets in three cor-
pora: tweets with negation only (NegOnly), tweets with
no negation (NoNeg), and a corpus that gathers all the
tweets of the previous 2 corpora (All). Table 1 shows
the repartition of tweets in our corpora.
</bodyText>
<table confidence="0.98426925">
Corpus Ironic Non ironic TOTAL
NegOnly 470 3,761 4,231
NoNeg 1,075 1,436 2,511
All 1,545 5,197 6,742
</table>
<tableCaption confidence="0.999559">
Table 1: Tweet repartition.
</tableCaption>
<sectionHeader confidence="0.887423" genericHeader="method">
3 Binary classifier
</sectionHeader>
<bodyText confidence="0.9999759">
We experiment with SMO under the Weka toolkit with
standard parameters. We also evaluated other learning
algorithms (naive bayes, decision trees, logistic regres-
sion) but the results were not as good as those obtained
with SMO. We have built three classifiers, one for each
corpus, namely CNeg, CNoNeg, and CAll. Since the
number of ironic instances in the first corpus is rela-
tively small, we learn CNeg with 10-cross validation on
a balanced subset of 940 tweets. For the second and the
last classifiers, we used 80% of the corpus for training
</bodyText>
<page confidence="0.338446">
645 1We have used Malt as a syntactic parser.
</page>
<bodyText confidence="0.999789836363636">
and 20% for test, with an equal distribution between
the ironic (henceforth IR) and non ironic (henceforth
NIR) instances2. The results presented in this paper
have been obtained when training CNoNeg on 1,720
and testing on 430 tweets. CAll has been trained on
2,472 tweets (1432 contain negation –404 IR and 1028
NIR) and tested on 618 tweets (360 contain negation –
66 IR and 294 NIR). For each classifier, we represent
each tweet with a vector composed of six groups of fea-
tures. Most of them are state of the art features, others,
in italic font are new.
Surface features include tweet length in words
(Tsur et al., 2010), the presence or absence of punc-
tuation marks (Gonzalez-Ibanez et al., 2011), words
in capital letters (Reyes et al., 2013), interjections
(Gonzalez-Ibanez et al., 2011), emoticons (Buschmeier
et al., 2014), quotations (Tsur et al., 2010), slang words
(Burfoot and Baldwin, 2009), opposition words such as
“but” and “although” (Utsumi, 2004), a sequence of ex-
clamation or a sequence of question marks (Carvalho et
al., 2009), a combination of both exclamation and ques-
tion marks (Buschmeier et al., 2014) and finally, the
presence of discourse connectives that do not convey
opposition such as “hence, therefore, as a result” since
we assume that non ironic tweets are likely to be more
verbose. To implement these features, we rely on man-
ually built French lexicons to deal with interjections,
emoticons, slang language, and discourse connectives
(Roze et al., 2012).
Sentiment features consist of features that check for
the presence of positive/negative opinion words (Reyes
and Rosso, 2012) and the number of positive and neg-
ative opinion words (Barbieri and Saggion, 2014). We
add three new features: the presence of words that ex-
press surprise or astonishment, and the presence and
the number of neutral opinions. To get these features
we use two lexicons: CASOAR, a French opinion lexi-
con (Benamara et al., 2014) and EMOTAIX, a publicly
available French emotion and affect lexicon.
Sentiment shifter features group checks if a given
tweet contains an opinion word which is in the scope of
an intensifier adverb or a modality.
Shifter features tests if a tweet contains an intensi-
fier (Liebrecht et al., 2013), a negation word (Reyes et
al., 2013), or reporting speech verbs.
Opposition features are new and check for the pres-
ence of specific lexico-syntactic patterns that verify
whether a tweet contains a sentiment opposition or an
explicit positive/negative contrast between a subjective
proposition and an objective one. These features have
been partly inspired from (Riloff et al., 2013) who
proposed a bootstrapping algorithm to detect sarcas-
tic tweets of the form [P+].[Pobj] which corresponds
to a contrast between positive sentiment and an ob-
jective negative situation. We extended this pattern to
</bodyText>
<footnote confidence="0.950560666666667">
2For CNoNeg and CAll, we also tested 10-cross valida-
tion with a balanced distribution between the ironic and non-
ironic instances but results were not conclusive.
</footnote>
<bodyText confidence="0.997914">
capture additional types of explicit oppositions. Some
of our patterns include: [Neg(P+)].[P+], [P_].[P+],
[Neg(P+)].[Pobj], [Pobj].[P_]. We consider that an
opinion expression is under the scope of a negation if it
is separated by a maximum of two tokens.
Finally, internal contextual deals with the pres-
ence/absence of personal pronouns, topic keywords and
named entities, as predicted by the parser’s outputs.
For each classifier, we investigated how each group
of features contributes to the learning process. We
applied to each training set a feature selection algo-
rithm (Chi2 and GainRatio), then trained the classifiers
over all relevant features of each group3. In all experi-
ments, we used all surface features as baseline. Table 2
presents the result in terms of precision (P), recall (R),
macro-averaged F-score (MAF) and accuracy (A). We
can see that CAll achieves better results. An analysis
of the best features combination for each classifier sug-
gests four main conclusions: (1) surface features are
primordial for irony detection. This is more salient for
NoNeg. (2) Negation is an important feature for our
task. However, having it alone is not enough to find
ironic instances. Indeed, among the 76 misclassified in-
stances in CAll, 60% contain negation clues (37 IR and
9 NIR). (3) When negation is concerned, opposition
features are among the most productive. (4) Explicit
opinion words (i.e sentiment and sentiment shifter) are
likely to be used in tweets with no negation. More im-
portantly, these results empirically validate hypothesis
(H1), i.e. negation is a good clue to detect irony.
</bodyText>
<table confidence="0.9997448">
Ironic (IR) Not ironic (NIR)
P R F P R F
CNeg 88.9 56.0 68.7 67.9 93.3 78.5
CNoNeg 71.1 65.1 68.0 67.80 73.50 70.50
CAll 93.0 81.6 86.9 83.6 93.9 88.4
Overall Results
MAF A
CNeg 73.6 74.5
CNoNeg 69.2 69.3
CAll 87.6 87.7
</table>
<tableCaption confidence="0.99445">
Table 2: Results for the best features combination.
</tableCaption>
<bodyText confidence="0.9999446">
Error analysis shows that misclassification of ironic
instances is mainly due to four factors: presence of sim-
iles (ironic comparison)4, absence of context within the
utterance (most frequent case), humor and satire5, and
wrong #ironie or #sarcasme tags. The absence of con-
text can manifest itself in several ways: (1) there is
no pointer that helps to identify the main topic of the
tweet, as in “I’ve been missing her, damn!”. Even if the
topic is present, it is often lexicalized in several col-
lapsed words or funny hashtags (#baddays, #aprilfoll),
</bodyText>
<footnote confidence="0.996804333333333">
3Results with all features are lower.
4e.g. “Benzema in the French team is like Sunday. He is
of no use.. :D”
5e.g. “I propose that we send Hollande instead of the
space probes on the next comet, it will save time and money
;) #HUMOUR”
</footnote>
<page confidence="0.985371">
646
</page>
<bodyText confidence="0.9999636875">
which are hard to automatically analyze. (2) The irony
is about specific situations (Shelley, 2001). (3) False
assertions about hot topics, like in “Don’t worry. Sene-
gal is the world champion soccer”. (4) Oppositions that
involve a contradiction between two words that are se-
mantically unrelated, a named entity and a given event
(e.g. “Tchad and “democratic election”), etc. Case (4)
is more frequent in the NoNeg corpus.
Knowing that tweets with negation represent 62.75%
of our corpus, and given that irony can focus on the
negation of a word or a proposition (Haverkate, 1990),
we propose to improve the classification of these tweets
by identifying the absurdity of their content, follow-
ing Attardo’s relevant inappropriateness model of irony
(Attardo, 2000) in which a violation of contextual ap-
propriateness signals ironical intent.
</bodyText>
<sectionHeader confidence="0.985893" genericHeader="method">
4 Deriving the pragmatic context
</sectionHeader>
<bodyText confidence="0.999951190476191">
The proposed model included two parts: binary classi-
fiers trained with tweet features, and an algorithm that
corrects the outputs of the classifiers which are likely
to be misclassified. These two phases can be applied
successively or together. In this latter case, the algo-
rithm outputs are integrated into the classifiers and the
corrected instances are used in the training process of
the binary classifier. In this paper, we only present re-
sults of the two phases applied successively because it
achieved better results.
Our approach is to query Google via its API to check
the veracity of tweets with negation that have been
classified as non ironic by the binary classifier in or-
der to correct the misclassified tweets (if a tweet say-
ing Not(P) has been classified as non-ironic but P is
found online, then we assume that the opposite content
is checked so the tweet class is changed into ironic).
Let WordsT be the set of words excluding stop words
that belong to a tweet t, and let kw be the topic key-
word used to collect t. Let N C WordsT be the set of
negation words of t. The algorithm is as follows:
</bodyText>
<listItem confidence="0.9798884">
1. Segment t into a set of sentences S.
2. For each s E S such that ∃neg E N and neg E s:
2.1 Remove # and @ symbols, emoticons, and neg,
then extract the set of tokens P C s that are on the
scope of neg (in a distance of 2 tokens).
</listItem>
<construct confidence="0.686846222222222">
2.2 Generate a query Q1 = P U kw and submit it to
Google which will return 20 results (title+snippet) or
less.
2.3 Among the returned results, keep only the reliable
ones (Wikipedia, online newspapers, web sites that do
not contain ”blog” or ”twitter” in their URL). Then,
for each result, if the query keywords are found in the
title or in the snippet, then t is considered as ironic.
STOP.
</construct>
<listItem confidence="0.8968604">
3. Generate a second query Q2 = (WordsT−N)Ukw
and submit it again to Google and follow the procedure
in 2.3. If Q2 is found, then t is considered as ironic.
Otherwise, the class predicted by the classifier does not
change.
</listItem>
<bodyText confidence="0.95297709375">
Let us illustrate our algorithm with the topic Valls
and the tweet: #Valls has learnt that Sarkozy was
wiretapped in newspapers. Fortunately he is not
the interior minister. The first step leads to two
sentences s1 (#Valls has learnt that Sarkozy was
wiretapped in newspapers.) and s2 (Fortunately
he is not the interior minister). From s2, we re-
move the negation word “not”, isolate the negation
scope P = {interior, minister} and generate
the query Q1 = {Valls interior minister}.
The step 2.3 allows to retrieve the result:
&lt;Title&gt;Manuel Valls - Wikipedia, the free encyclope-
dia&lt;/Title&gt;
&lt;Snippet&gt;... French politician. For the Spanish com-
poser, see Manuel Valls (composer)..... Valls was ap-
pointed Minister of the Interior in the Ayrault Cabinet
in May 2012.&lt;/Snippet&gt;.
All query keywords were found in this snippet (in bold
font), we can then conclude that the tweet is ironic.
We made several experiments to evaluate how the
query-based method improves tweet classification. For
this purpose, we have applied the method on both cor-
pora All and Neg: ① A first experiment evaluates the
method on tweets with negation classified as NIR but
which are ironic according to gold annotations. This
experiment represents an ideal case which we try to
achieve or improve through other ones. ②: A sec-
ond experiment consists in applying the method on all
tweets with negation that have been classified as NIR
by the classifier, no matter if the predicted class is cor-
rect or not. Table 3 shows the results for both experi-
ments.
</bodyText>
<table confidence="0.981424857142857">
① ②
NIR tweets for which: All Neg All Neg
Query applied 37 207 327 644
Results on Google 25 102 166 331
Class changed into IR 5 35 69 178
Classifier Accuracy 87.7 74.46 87.7 74.46
Query-based Accuracy 88.51 78.19 78.15 62.98
</table>
<tableCaption confidence="0.99904">
Table 3: Results for the query-based method.
</tableCaption>
<bodyText confidence="0.9999725">
All scores for the query-based method are statis-
tically significant compared to the classifier’s scores
(p value &lt; 0, 0001 when calculated with the McNe-
mar’s test.). An error analysis shows that 65% of tweets
that are still misclassified with this method are tweets
for which finding their content online is almost impos-
sible because they are personal tweets or lack internal
context. A conclusion that can be drawn is that this
method should not be applied on this type of tweets.
For this purpose, we made the same experiments only
on tweets with different combinations of relevant fea-
tures. The best results are obtained when the method is
applied only on NIR tweets with negation selected via
the internal context features, more precisely on tweets
which do not contain a personal pronoun and which
contain named entities: these results are coherent with
</bodyText>
<page confidence="0.976281">
647
</page>
<bodyText confidence="0.999498428571429">
the fact that tweets containing personal pronouns and
no named entity are likely to relate personal content im-
possible to validate on the Web (e.g. I’ve been missing
her, damn! #ironie). Table 4 shows the results for these
experiments. All scores for the query-based method are
also statistically significant compared to the classifier’s
scores.
</bodyText>
<table confidence="0.997015142857143">
© ②
NIR tweets for which: All Neg All Neg
Query applied 0 18 40 18
Results on Google - 12 17 12
Class changed into IR - 4 7 4
Classifier Accuracy 87.7 74.46 87.7 74.46
Query-based Accuracy 87.7 74.89 86.57 74.89
</table>
<tableCaption confidence="0.901786">
Table 4: Results when applied on “non-personal”
tweets.
</tableCaption>
<bodyText confidence="0.999815166666667">
For experiment ©, on All, the method is not applied
because all misclassified tweets contain a personal pro-
noun and no named entity. The query-based method
outperforms the classifier in all cases, except on All
where results on Google were found for only 42.5%
of queries whereas more than 50% of queries found
results in all other experiments (maximum is 66.6%
in NegOnly). Tweets for which no result is found are
tweets with named entities but which do not relate an
event or a statement (e.g. AHAHAHAHAHA! NO RE-
SPECT #Legorafi, where “Legorafi” is a satirical news-
paper). To evaluate the task difficulty, two annotators
were also asked to label as ironic or not the 50 tweets
(40+18) for which the method is applied. The inter-
annotator score (Cohen’s Kappa) between both anno-
tators is only rc = 0.41. Among the 12 reclassifica-
tions into IR, both annotators disagree with each other
for 5 of them. Even if this experiment is not strong
enough to lead to a formal conclusion because of the
small number of tweets, this tends to show that human
beings would not do it better.
It is interesting to note that even if internal context
features were not relevant for automatic tweet classifi-
cation, our results show that they are useful for classifi-
cation improvement. As shown by ©, the query-based
method is more effective when applied on misclassi-
fied tweets. We can then consider that using internal
contextual features (presence of personal pronouns and
named entities) can be a way to automatically detect
tweets that are likely to be misclassified.
</bodyText>
<sectionHeader confidence="0.972332" genericHeader="conclusions">
5 Discussion and conclusions
</sectionHeader>
<bodyText confidence="0.999970467741935">
This paper proposed a model to identify irony in im-
plicit oppositions in French. As far as we know, this
is the first work on irony detection in French on Twit-
ter data. Comparing to other languages, our results
are very encouraging. For example, sarcasm detection
achieved 30% precision in Dutch tweets (Liebrecht et
al., 2013) while irony detection in English data resulted
in 79% precision (Reyes et al., 2013).
We treat French irony as an overall term that covers
other figurative language devices such as sarcasm, hu-
mor, etc. This is a first step before moving to a more
fine-grained automatic identification of figurative lan-
guage in French. For interesting discussions on the dis-
tinction/similarity between irony and sarcasm hastags,
see (Wang, 2013).
One of the main contribution of this study is that the
proposed model does not rely only on the lexical clues
of a tweet, but also on its pragmatic context. Our in-
tuition is that a tweet containing an asserted fact of the
form Not(P1) is ironic if and only if one can prove Pl
on the basis of some external information. This form of
tweets is quite frequent in French (more than 62.75% of
our data contain explicit negation words), which sug-
gests two hypotheses: (H1) negation can be a good in-
dicator to detect irony, and (H2) external context can
help to detect the absurdity of ironic content.
To validate if negation helps, we built binary clas-
sifiers using both state of the art features and new
features (explicit and implicit opposition, sentiment
shifter, discourse connectives). Overall accuracies
were good when the data contain both tweets with
negation and no negation but lower when tweets con-
tain only negation or no negation at all. Error anal-
ysis show that major errors come from the presence
of implicit oppositions, particularly in CNe9 and CAll.
These results empirically validate hypothesis (H1).
Negation has been shown to be very helpful in many
NLP tasks, such as sentiment analysis (Wiegand et al.,
2010). It has also been used as a feature to detect irony
(Reyes et al., 2013). However, no one has empirically
measured how irony classification behaves in the pres-
ence or absence of negation in the data.
To test (H2), we proposed a query-based method that
corrects the classifier’s outputs in order to retrieve false
assertions. Our experiments show that the classification
after applying Google searches in reliable web sites sig-
nificantly improves the classifier accuracy when tested
on CNe9. In addition, we show that internal context
features are useful to improve classification. These re-
sults empirically validate (H2). However, even though
the algorithm improves the classifier performance, the
number of queries is small which suggests that a much
larger dataset is needed. As for negation, querying ex-
ternal source of information has been shown to give
an improvement over the basic features for many NLP
tasks (for example, in question-answering (Moldovan
et al., 2002)). However, as far as we know, this ap-
proach has not been used for irony classification.
This study is a first step towards improving irony de-
tection relying on external context. We plan to study
other ways to retrieve such a context like the conversa-
tion thread.
</bodyText>
<sectionHeader confidence="0.997052" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.984393">
This work was funded by the French National Research
Agency (ASFALDA project ANR-12-CORD-023).
</bodyText>
<page confidence="0.526414">
648
</page>
<sectionHeader confidence="0.99015" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99966823853211">
Salvatore Attardo. 2000. Irony as relevant inappropri-
ateness. Journal ofpragmatics, 32(6):793–826.
Francesco Barbieri and Horacio Saggion. 2014. Mod-
elling Irony in Twitter: Feature Analysis and Eval-
uation. In Proceedings of Language Resources and
Evaluation Conference (LREC), pages 4258–4264.
Farah Benamara, V´eronique Moriceau, and
Yvette Yannick Mathieu. 2014. Fine-grained
semantic categorization of opinion expressions for
consensus detection (Cat´egorisation s´emantique
fine des expressions d’opinion pour la d´etection de
consensus) [in French]. In TALN-RECITAL 2014
Workshop DEFT 2014 : D ´Efi Fouille de Textes
(DEFT 2014 Workshop: Text Mining Challenge),
pages 36–44, July.
Clint Burfoot and Clint Baldwin. 2009. Automatic
satire detection: Are you having a laugh? In Pro-
ceedings of the ACL-IJCNLP 2009 conference short
papers, pages 161–164. Association for Computa-
tional Linguistics.
Konstantin Buschmeier, Philipp Cimiano, and Roman
Klinger. 2014. An Impact Analysis of Features in a
Classification Approach to Irony Detection in Prod-
uct Reviews. In Proceedings of the 5th Workshop
on Computational Approaches to Subjectivity, Senti-
ment and Social Media Analysis, pages 42–49.
Paula Carvalho, Lu´ıs Sarmento, M´ario J Silva, and
Eug´enio De Oliveira. 2009. Clues for detect-
ing irony in user-generated contents: oh...!! it’s
so easy;-). In Proceedings of the 1st international
CIKM workshop on Topic-sentiment analysis for
mass opinion, pages 53–56. ACM.
Herbert H Clark and Richard J Gerrig. 1984. On the
pretense theory of irony. Journal of Experimental
Psychology: General, 113(1):121–126.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised Recognition of Sarcastic Sentences
in Twitter and Amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ’10, pages 107–116.
Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso,
Ekaterina Shutova, John Barnden, and Antonio
Reyes. 2015. Semeval-2015 task 11: Sentiment
Analysis of Figurative Language in Twitter. In Proc.
9th Int. Workshop on Semantic Evaluation (SemEval
2015), Co-located with NAACL, page 470478. Asso-
ciation for Computational Linguistics.
Raymond W Gibbs. 2000. Irony in talk among friends.
Metaphor and symbol, 15(1-2):5–27.
Roberto Gonzalez-Ibanez, Smaranda Muresan, and
Nina Wacholde. 2011. Identifying sarcasm in Twit-
ter: a closer look. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers-Volume 2, pages 581–586. Association for
Computational Linguistics.
H Paul Grice, Peter Cole, and Jerry L Morgan. 1975.
Syntax and semantics. Logic and conversation,
3:41–58.
Henk Haverkate. 1990. A speech act analysis of irony.
Journal of Pragmatics, 14(1):77 – 109.
Christine Liebrecht, Florian Kunneman, and
Bosch Antal van den. 2013. The perfect so-
lution for detecting sarcasm in tweets# not. In
Proceedings of the 4th Workshop on Computational
Approaches to Subjectivity, Sentiment and Social
Media Analysis, pages 29–37. New Brunswick, NJ:
ACL.
Dan I Moldovan, Sanda M Harabagiu, Roxana Girju,
Paul Morarescu, V Finley Lacatusu, Adrian Novis-
chi, Adriana Badulescu, and Orest Bolohan. 2002.
LCC Tools for Question Answering. In TREC.
Antonio Reyes and Paolo Rosso. 2012. Making objec-
tive decisions from subjective data: Detecting irony
in customer reviews. Decision Support Systems,
53(4):754–760.
Antonio Reyes, Paolo Rosso, and Tony Veale. 2013.
A multidimensional approach for detecting irony
in twitter. Language resources and evaluation,
47(1):239–268.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as Contrast between a Positive Sen-
timent and Negative Situation. In EMNLP, pages
704–714.
Charlotte Roze, Laurence Danlos, and Philippe Muller.
2012. Lexconn: A French lexicon of discourse con-
nectives. Discours, Multidisciplinary Perspectives
on Signalling Text Organisation, 10:(on line).
J. Searle. 1979. Expression and meaning: Studies in
the theory of speech acts. Cambridge University.
Cameron Shelley. 2001. The bicoherence theory of
situational irony. Cognitive Science, 25(5):775–818.
Dan Sperber and Deirdre Wilson. 1981. Irony and
the use-mention distinction. Radical pragmatics,
49:295–318.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
ICWSM-A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Prod-
uct Reviews. In ICWSM.
Akira Utsumi. 1996. A unified theory of irony and
its computational formalization. In Proceedings of
the 16th conference on Computational linguistics-
Volume 2, pages 962–967. Association for Compu-
tational Linguistics.
Akira Utsumi. 2004. Stylistic and contextual effects
in irony processing. In Proceedings of the 26th An-
nual Meeting of the Cognitive Science Society, pages
1369–1374.
</reference>
<page confidence="0.594241">
649
</page>
<reference confidence="0.9995095">
Po-Ya Angela Wang. 2013. #Irony or #Sarcasm-A
Quantitative and Qualitative Study Based on Twitter.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A
Survey on the Role of Negation in Sentiment Analy-
sis. In Proceedings of the Workshop on Negation and
Speculation in Natural Language Processing, pages
60–68. Association for Computational Linguistics.
</reference>
<page confidence="0.997974">
650
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999926">Towards a Contextual Pragmatic Model to Detect Irony in Tweets</title>
<author confidence="0.996003">Jihen Karoui Farah Benamara Zitoune V´eronique Moriceau</author>
<affiliation confidence="0.972084">IRIT, MIRACL IRIT, CNRS LIMSI-CNRS Toulouse University, Sfax University Toulouse University Univ. Paris-Sud</affiliation>
<abstract confidence="0.973107461538462">karoui@irit.fr benamara@irit.fr moriceau@limsi.fr Nathalie Aussenac-Gilles Lamia Hadrich Belguith IRIT, CNRS MIRACL of Sfax l.belguith@fsegs.rnu.tn 6 Abstract This paper proposes an approach to capture the pragmatic context needed to infer irony in tweets. We aim to test the validity of two main hypotheses: (1) the presence of negations, as an internal propriety of an utterance, can help to detect the disparity between the literal and the intended meaning of an utterance, (2) a tweet containing an asserted fact of the form ironic if and only if one can assess absurdity of Our first results are encouraging and show that deriving a pragmatic contextual model is feasible. 1 Motivation Irony is a complex linguistic phenomenon widely studied in philosophy and linguistics (Grice et al., 1975; Sperber and Wilson, 1981; Utsumi, 1996). Despite theories differ on how to define irony, they all commonly agree that it involves an incongruity between the literal meaning of an utterance and what is expected about the speaker and/or the environment. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that covers these devices focusing for the first time on the automatic detection of irony in French tweets. According to (Grice et al., 1975; Searle, 1979; Attardo, 2000), the search for a non-literal meaning starts when the hearer realizes that the speaker’s utterance is context-inappropriate, that is an utterance fails to make sense against the context. For example, the tweet: ironic if the French soccer team has lost the match. An analysis of a corpus of French tweets shows that there are two ways to infer such a context: (a) rely exclusively on the lexical clues internal to the utterance, or (b) combine these clues with an additional pragmatic context external to the utterance. In (a), the speaker intentionally creates an explicit juxtaposition of incompatible actions or words that can either have opposite or can be semantically unrelated, as in is more important than Fukushima Explicit opposition can also arise from an explicit positive/negative contrast between a subjective proposition and a situation that describes an undesirable activity or For instance, in “ love when my phone turns the down the writer assumes that every one expects its cell phone to ring loud enough to be heard. In (b), irony is due to an implicit opposibetween a lexicalized proposition an event or state and a pragmatic context external to the in which false or is not likely to happen. other words, the writer asserts or affirms intends to convey that The irony occurs because the writer believes his audience can detect the disparity between the basis of contextual knowledge or common background shared with the writer. For example, is really a good diplomat the writer critics the foreign policy of the French president in Algeria, whereas in #NSA wiretapped a whole country. No worries for #Belgium: it is not a the irony occurs because the fact in bold font is not true. Irony detection is quite a hot topic in the research community also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; Gonzalez- Ibanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validwhich models absurdity by identifying a conjunc-</abstract>
<note confidence="0.80608725">44 Proceedings of the 53rd Annual Meeting of the Association for Computational the 7th International Joint Conference on Natural Language Processing (Short pages China, July 26-31, 2015. Association for Computational Linguistics</note>
<abstract confidence="0.99111425">tion of named entities present in a given document and queries the web for the conjunction of those entities. (Gonzalez-Ibanez et al., 2011) exploited the common ground between speaker and hearer by looking if a tweet is a reply to another tweet. (Reyes et al., 2013) employed opposition in time (adverbs of time such as and context imbalance to estimate the semantic similarity of concepts in a text to each other. (Barbieri and Saggion, 2014) captured the gap between rare and common words as well as the use of common vs. rare synonyms. Finally, (Buschmeier et al., 2014) measured the imbalance between the overall polarity of words in a review and the star-rating. Most of these pragmatic features rely on linguistic aspects of the tweet by using only the text of the tweet. We aim here to go further by proposing a novel computational model able to capture the “outside of the utterance” context needed to infer irony in implicit oppositions. 2 Methodology An analysis of a corpus of French ironic tweets randomly chosen from various topics shows that more than 62.75% of tweets contain explicit negation markers such as “ne...pas” (not) or negative polarity items like “jamais” (never) or “personne” (nobody). Negation seems thus to be an important clue in ironic statements, at least in French. This rises the following hypotheses: (H1) the presence of negations, as an internal propriety of an utterance, can help to detect the disparity between the literal and the intended meaning of an utterance, and (H2) a tweet containing an asserted of the form ironic if and only if one prove the basis of some external common knowledge to the utterance shared by the author and the reader. To test the validity of the above hypotheses, we propose a novel three-step model involving three successive stages: (1) detect if a tweet is ironic or not relying exclusively on the information internal to the tweet. We use a supervised learning method relying on both state of the art features whose efficiency has been empirically proved and new groups of features. (2) Test this internal context against the “outside of the utterance” context. We design an algorithm that takes the classifier’s outputs and corrects the misclassified ironic inof the form looking for reliable external sources of information on the Web, such as Wikipedia or online newspapers. We experiment when labels are given by gold standard annotations and when they are predicted by the classifier. (3) If the literal fails to make sense, i.e. found, then the tweet is likely to convey a non-literal meaning. To this end, we collected a corpus of 6,742 French tweets using the Tweeter API focusing on tweets relative to a set of topics discussed in the media during Spring 2014. Our intuition behind choosing such topics is that a media-friendly topic is more likely to be found in external sources of information. We chose 184 topics split into 9 categories (politics, sport, etc.). For each topic, we selected a set of keywords with without hashtag: politics Hollande, health flu), sport social media Skype, artists Beyonc´e), TV shows XFactor), countries or cities NorthKo- Brasil), the Arab Spring Ben and some other generic topics racism). Then we selected ironic tweets containing the keywords, the and a negation word as well as ironic tweets containing only topic keywords with but no negation word. Finally, we selected non ironic tweets that contained either the topic keywords and a negation word, or only the topic keywords. We removed duplicates, retweets and tweets containing pictures which would need to be interpreted to understand ironic content. Irony hashtags are removed from the tweets for the following experiments. To guarantee that tweets with negation words contain true negations, we automatically identified negation usage of a given word using a French syndependency We then designed dedicated rules to correct the parser’s decisions if necessary. At the end, we got a total of 4,231 tweets with negation and 2,511 without negation, among them, 30.42% are ironic with negation and 72.36% are non ironic with negation. At the end, we got a total of 4,231 tweets with negation and 2,511 without negation: among them, 30.42% are ironic with negation and 72.36% are non ironic with negation. To capture the effect of negation on our task, we split these tweets in three cortweets with negation only tweets with negation and a corpus that gathers all the of the previous 2 corpora Table 1 shows the repartition of tweets in our corpora. Corpus Ironic Non ironic TOTAL NegOnly 470 3,761 4,231 NoNeg 1,075 1,436 2,511 All 1,545 5,197 6,742 Table 1: Tweet repartition. 3 Binary classifier We experiment with SMO under the Weka toolkit with standard parameters. We also evaluated other learning algorithms (naive bayes, decision trees, logistic regression) but the results were not as good as those obtained with SMO. We have built three classifiers, one for each namely andSince the number of ironic instances in the first corpus is relasmall, we learn 10-cross validation on a balanced subset of 940 tweets. For the second and the last classifiers, we used 80% of the corpus for training have used Malt as a syntactic parser. and 20% for test, with an equal distribution between ironic (henceforth and non ironic (henceforth The results presented in this paper been obtained when training 1,720 testing on 430 tweets. been trained on tweets (1432 contain negation –404 1028 and tested on 618 tweets (360 contain negation – 294 For each classifier, we represent each tweet with a vector composed of six groups of features. Most of them are state of the art features, others, in italic font are new. features tweet length in words (Tsur et al., 2010), the presence or absence of punctuation marks (Gonzalez-Ibanez et al., 2011), words in capital letters (Reyes et al., 2013), interjections (Gonzalez-Ibanez et al., 2011), emoticons (Buschmeier et al., 2014), quotations (Tsur et al., 2010), slang words (Burfoot and Baldwin, 2009), opposition words such as “but” and “although” (Utsumi, 2004), a sequence of exclamation or a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and quesmarks (Buschmeier et al., 2014) and finally, presence of discourse connectives that do not convey as “hence, therefore, as a result” since we assume that non ironic tweets are likely to be more verbose. To implement these features, we rely on manually built French lexicons to deal with interjections, emoticons, slang language, and discourse connectives (Roze et al., 2012). features of features that check for the presence of positive/negative opinion words (Reyes and Rosso, 2012) and the number of positive and negative opinion words (Barbieri and Saggion, 2014). We three new features: presence of words that exsurprise or and presence and number of neutral To get these features we use two lexicons: CASOAR, a French opinion lexicon (Benamara et al., 2014) and EMOTAIX, a publicly available French emotion and affect lexicon. shifter features checks if a given tweet contains an opinion word which is in the scope of intensifier adverb or a features if a tweet contains an intensifier (Liebrecht et al., 2013), a negation word (Reyes et 2013), or speech features new and check for the presence of specific lexico-syntactic patterns that verify whether a tweet contains a sentiment opposition or an explicit positive/negative contrast between a subjective proposition and an objective one. These features have been partly inspired from (Riloff et al., 2013) who proposed a bootstrapping algorithm to detect sarcastweets of the form corresponds to a contrast between positive sentiment and an objective negative situation. We extended this pattern to andwe also tested 10-cross validation with a balanced distribution between the ironic and nonironic instances but results were not conclusive. capture additional types of explicit oppositions. Some our patterns include: We consider that an opinion expression is under the scope of a negation if it is separated by a maximum of two tokens. contextual with the presof pronouns, topic keywords as predicted by the parser’s outputs. For each classifier, we investigated how each group of features contributes to the learning process. We applied to each training set a feature selection algorithm (Chi2 and GainRatio), then trained the classifiers all relevant features of each In all experiments, we used all surface features as baseline. Table 2 presents the result in terms of precision (P), recall (R), macro-averaged F-score (MAF) and accuracy (A). We see that better results. An analysis of the best features combination for each classifier suggests four main conclusions: (1) surface features are primordial for irony detection. This is more salient for (2) Negation is an important feature for our task. However, having it alone is not enough to find ironic instances. Indeed, among the 76 misclassified inin 60% contain negation clues (37 (3) When negation is concerned, opposition features are among the most productive. (4) Explicit opinion words (i.e sentiment and sentiment shifter) are likely to be used in tweets with no negation. More importantly, these results empirically validate hypothesis (H1), i.e. negation is a good clue to detect irony. Ironic (IR) Not ironic (NIR)</abstract>
<affiliation confidence="0.390377">P R F P R F</affiliation>
<address confidence="0.5675875">88.9 56.0 68.7 67.9 93.3 78.5 71.1 65.1 68.0 67.80 73.50 70.50</address>
<phone confidence="0.807763">93.0 81.6 86.9 83.6 93.9 88.4</phone>
<title confidence="0.5507535">Overall Results MAF A</title>
<phone confidence="0.540772">73.6 74.5 69.2 69.3</phone>
<abstract confidence="0.962437522821577">87.6 87.7 Table 2: Results for the best features combination. Error analysis shows that misclassification of ironic instances is mainly due to four factors: presence of sim- (ironic absence of context within the (most frequent case), humor and and The absence of context can manifest itself in several ways: (1) there is no pointer that helps to identify the main topic of the as in been missing her, Even if the topic is present, it is often lexicalized in several colwords or funny hashtags with all features are lower. “Benzema in the French team is like Sunday. He is of no use.. :D” propose that we send Hollande instead of the space probes on the next comet, it will save time and money ;) #HUMOUR” 646 which are hard to automatically analyze. (2) The irony is about specific situations (Shelley, 2001). (3) False about hot topics, like in worry. Seneis the world champion (4) Oppositions that involve a contradiction between two words that are semantically unrelated, a named entity and a given event and “democratic election”), etc. Case (4) more frequent in the Knowing that tweets with negation represent 62.75% of our corpus, and given that irony can focus on the negation of a word or a proposition (Haverkate, 1990), we propose to improve the classification of these tweets by identifying the absurdity of their content, following Attardo’s relevant inappropriateness model of irony (Attardo, 2000) in which a violation of contextual appropriateness signals ironical intent. 4 Deriving the pragmatic context The proposed model included two parts: binary classifiers trained with tweet features, and an algorithm that corrects the outputs of the classifiers which are likely to be misclassified. These two phases can be applied successively or together. In this latter case, the algorithm outputs are integrated into the classifiers and the corrected instances are used in the training process of the binary classifier. In this paper, we only present results of the two phases applied successively because it achieved better results. Our approach is to query Google via its API to check the veracity of tweets with negation that have been classified as non ironic by the binary classifier in order to correct the misclassified tweets (if a tweet saybeen classified as non-ironic but found online, then we assume that the opposite content is checked so the tweet class is changed into ironic). the set of words excluding stop words belong to a tweet and let the topic keyused to collect Let C WordsT the set of words of The algorithm is as follows: Segment a set of sentences For each E S that E N E Remove # and @ symbols, emoticons, and extract the set of tokens C s are on the of a distance of 2 tokens). Generate a query U kw submit it to Google which will return 20 results (title+snippet) or less. 2.3 Among the returned results, keep only the reliable ones (Wikipedia, online newspapers, web sites that do not contain ”blog” or ”twitter” in their URL). Then, for each result, if the query keywords are found in the or in the snippet, then considered as ironic. STOP. Generate a second query and submit it again to Google and follow the procedure 2.3. If found, then considered as ironic. Otherwise, the class predicted by the classifier does not change. us illustrate our algorithm with the topic the tweet: has learnt that Sarkozy was wiretapped in newspapers. Fortunately he is not interior The first step leads to two has learnt that Sarkozy was in and is not the interior From we remove the negation word “not”, isolate the negation minister} generate query interior The step 2.3 allows to retrieve the result: Valls - Wikipedia, the free encyclope- French politician. For the Spanish composer, see Manuel Valls (composer)..... Valls was appointed Minister of the Interior in the Ayrault Cabinet May All query keywords were found in this snippet (in bold font), we can then conclude that the tweet is ironic. We made several experiments to evaluate how the query-based method improves tweet classification. For this purpose, we have applied the method on both corfirst experiment evaluates the on tweets with negation classified as which are ironic according to gold annotations. This experiment represents an ideal case which we try to or improve through other ones. A second experiment consists in applying the method on all with negation that have been classified as by the classifier, no matter if the predicted class is correct or not. Table 3 shows the results for both experiments. ① ② for which: All Neg All Neg Query applied 37 207 327 644 Results on Google 25 102 166 331 changed into 5 35 69 178 Classifier Accuracy 87.7 74.46 87.7 74.46 Query-based Accuracy 88.51 78.19 78.15 62.98 Table 3: Results for the query-based method. All scores for the query-based method are statistically significant compared to the classifier’s scores value &lt; 0001 calculated with the McNemar’s test.). An error analysis shows that 65% of tweets that are still misclassified with this method are tweets for which finding their content online is almost impossible because they are personal tweets or lack internal context. A conclusion that can be drawn is that this method should not be applied on this type of tweets. For this purpose, we made the same experiments only on tweets with different combinations of relevant features. The best results are obtained when the method is only on with negation selected via the internal context features, more precisely on tweets which do not contain a personal pronoun and which contain named entities: these results are coherent with 647 the fact that tweets containing personal pronouns and no named entity are likely to relate personal content imto validate on the Web I’ve been missing damn! Table 4 shows the results for these experiments. All scores for the query-based method are also statistically significant compared to the classifier’s scores. © ② for which: All Neg All Neg Query applied 0 18 40 18 Results on Google - 12 17 12 changed into - 4 7 4 Classifier Accuracy 87.7 74.46 87.7 74.46 Query-based Accuracy 87.7 74.89 86.57 74.89 Table 4: Results when applied on “non-personal” tweets. experiment on the method is not applied because all misclassified tweets contain a personal pronoun and no named entity. The query-based method the classifier in all cases, except on where results on Google were found for only 42.5% of queries whereas more than 50% of queries found results in all other experiments (maximum is 66.6% Tweets for which no result is found are tweets with named entities but which do not relate an or a statement AHAHAHAHAHA! NO REwhere “Legorafi” is a satirical newspaper). To evaluate the task difficulty, two annotators were also asked to label as ironic or not the 50 tweets (40+18) for which the method is applied. The interannotator score (Cohen’s Kappa) between both annois only Among the 12 reclassificainto both annotators disagree with each other for 5 of them. Even if this experiment is not strong enough to lead to a formal conclusion because of the small number of tweets, this tends to show that human beings would not do it better. It is interesting to note that even if internal context features were not relevant for automatic tweet classification, our results show that they are useful for classifiimprovement. As shown by the query-based method is more effective when applied on misclassified tweets. We can then consider that using internal contextual features (presence of personal pronouns and named entities) can be a way to automatically detect tweets that are likely to be misclassified. 5 Discussion and conclusions This paper proposed a model to identify irony in implicit oppositions in French. As far as we know, this is the first work on irony detection in French on Twitter data. Comparing to other languages, our results are very encouraging. For example, sarcasm detection achieved 30% precision in Dutch tweets (Liebrecht et al., 2013) while irony detection in English data resulted in 79% precision (Reyes et al., 2013). We treat French irony as an overall term that covers other figurative language devices such as sarcasm, humor, etc. This is a first step before moving to a more fine-grained automatic identification of figurative language in French. For interesting discussions on the distinction/similarity between irony and sarcasm hastags, see (Wang, 2013). One of the main contribution of this study is that the proposed model does not rely only on the lexical clues of a tweet, but also on its pragmatic context. Our intuition is that a tweet containing an asserted fact of the ironic if and only if one can prove on the basis of some external information. This form of tweets is quite frequent in French (more than 62.75% of our data contain explicit negation words), which suggests two hypotheses: (H1) negation can be a good indicator to detect irony, and (H2) external context can help to detect the absurdity of ironic content. To validate if negation helps, we built binary classifiers using both state of the art features and new features (explicit and implicit opposition, sentiment shifter, discourse connectives). Overall accuracies were good when the data contain both tweets with negation and no negation but lower when tweets contain only negation or no negation at all. Error analysis show that major errors come from the presence implicit oppositions, particularly in These results empirically validate hypothesis (H1). Negation has been shown to be very helpful in many NLP tasks, such as sentiment analysis (Wiegand et al., 2010). It has also been used as a feature to detect irony (Reyes et al., 2013). However, no one has empirically measured how irony classification behaves in the presence or absence of negation in the data. To test (H2), we proposed a query-based method that corrects the classifier’s outputs in order to retrieve false assertions. Our experiments show that the classification after applying Google searches in reliable web sites significantly improves the classifier accuracy when tested In addition, we show that internal context features are useful to improve classification. These results empirically validate (H2). However, even though the algorithm improves the classifier performance, the number of queries is small which suggests that a much larger dataset is needed. As for negation, querying external source of information has been shown to give an improvement over the basic features for many NLP tasks (for example, in question-answering (Moldovan et al., 2002)). However, as far as we know, this approach has not been used for irony classification. This study is a first step towards improving irony detection relying on external context. We plan to study other ways to retrieve such a context like the conversation thread.</abstract>
<note confidence="0.876622384615385">Acknowledgements This work was funded by the French National Research Agency (ASFALDA project ANR-12-CORD-023). 648 References Salvatore Attardo. 2000. Irony as relevant inappropri- 32(6):793–826. Francesco Barbieri and Horacio Saggion. 2014. Modelling Irony in Twitter: Feature Analysis and Eval- In of Language Resources and Conference pages 4258–4264. Farah Benamara, V´eronique Moriceau, and Yvette Yannick Mathieu. 2014. Fine-grained</note>
<abstract confidence="0.986251666666667">semantic categorization of opinion expressions for consensus detection (Cat´egorisation s´emantique fine des expressions d’opinion pour la d´etection de</abstract>
<note confidence="0.791157561403509">[in French]. In 2014 Workshop DEFT 2014 : D ´Efi Fouille de Textes 2014 Workshop: Text Mining pages 36–44, July. Clint Burfoot and Clint Baldwin. 2009. Automatic detection: Are you having a laugh? In Proceedings of the ACL-IJCNLP 2009 conference short pages 161–164. Association for Computational Linguistics. Konstantin Buschmeier, Philipp Cimiano, and Roman Klinger. 2014. An Impact Analysis of Features in a Classification Approach to Irony Detection in Prod- Reviews. In of the 5th Workshop on Computational Approaches to Subjectivity, Sentiand Social Media pages 42–49. Paula Carvalho, Lu´ıs Sarmento, M´ario J Silva, and Eug´enio De Oliveira. 2009. Clues for detecting irony in user-generated contents: oh...!! it’s easy;-). In of the 1st international CIKM workshop on Topic-sentiment analysis for pages 53–56. ACM. Herbert H Clark and Richard J Gerrig. 1984. On the theory of irony. of Experimental 113(1):121–126. Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised Recognition of Sarcastic Sentences Twitter and Amazon. In of the Fourteenth Conference on Computational Natural Lan- CoNLL ’10, pages 107–116. Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso, Ekaterina Shutova, John Barnden, and Antonio Reyes. 2015. Semeval-2015 task 11: Sentiment of Figurative Language in Twitter. In 9th Int. Workshop on Semantic Evaluation (SemEval Co-located with page 470478. Association for Computational Linguistics. Raymond W Gibbs. 2000. Irony in talk among friends. and 15(1-2):5–27. Roberto Gonzalez-Ibanez, Smaranda Muresan, and Nina Wacholde. 2011. Identifying sarcasm in Twita closer look. In of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short pages 581–586. Association for Computational Linguistics. H Paul Grice, Peter Cole, and Jerry L Morgan. 1975. and semantics. and 3:41–58. Henk Haverkate. 1990. A speech act analysis of irony. of 14(1):77 – 109. Christine Liebrecht, Florian Kunneman, and Bosch Antal van den. 2013. The perfect solution for detecting sarcasm in tweets# not. In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social pages 29–37. New Brunswick, NJ: ACL.</note>
<author confidence="0.865925">Dan I Moldovan</author>
<author confidence="0.865925">Sanda M Harabagiu</author>
<author confidence="0.865925">Roxana Girju</author>
<author confidence="0.865925">Paul Morarescu</author>
<author confidence="0.865925">V Finley Lacatusu</author>
<author confidence="0.865925">Adrian Novis-</author>
<note confidence="0.516923">chi, Adriana Badulescu, and Orest Bolohan. 2002. Tools for Question Answering. In Antonio Reyes and Paolo Rosso. 2012. Making objective decisions from subjective data: Detecting irony customer reviews. Support 53(4):754–760. Antonio Reyes, Paolo Rosso, and Tony Veale. 2013. A multidimensional approach for detecting irony twitter. resources and 47(1):239–268. Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang. 2013. Sarcasm as Contrast between a Positive Senand Negative Situation. In pages 704–714. Charlotte Roze, Laurence Danlos, and Philippe Muller. 2012. Lexconn: A French lexicon of discourse con-</note>
<title confidence="0.4988855">Multidisciplinary Perspectives Signalling Text 10:(on line).</title>
<author confidence="0.528082">meaning Studies in</author>
<affiliation confidence="0.785477">theory of speech Cambridge University.</affiliation>
<address confidence="0.784656">Cameron Shelley. 2001. The bicoherence theory of</address>
<abstract confidence="0.741148333333333">irony. 25(5):775–818. Dan Sperber and Deirdre Wilson. 1981. Irony and use-mention distinction.</abstract>
<note confidence="0.964977933333333">49:295–318. Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010. ICWSM-A Great Catchy Name: Semi-Supervised Recognition of Sarcastic Sentences in Online Prod- Reviews. In Akira Utsumi. 1996. A unified theory of irony and computational formalization. In of the 16th conference on Computational linguisticspages 962–967. Association for Computational Linguistics. Akira Utsumi. 2004. Stylistic and contextual effects irony processing. In of the 26th An- Meeting of the Cognitive Science pages 1369–1374. 649</note>
<title confidence="0.875244">Po-Ya Angela Wang. 2013. #Irony or #Sarcasm-A Quantitative and Qualitative Study Based on Twitter.</title>
<author confidence="0.958048">Michael Wiegand</author>
<author confidence="0.958048">Alexandra Balahur</author>
<author confidence="0.958048">Benjamin Roth</author>
<note confidence="0.273359">Dietrich Klakow, and Andr´es Montoyo. 2010. A Survey on the Role of Negation in Sentiment Analy- In of the Workshop on Negation and in Natural Language pages 60–68. Association for Computational Linguistics. 650</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Salvatore Attardo</author>
</authors>
<title>Irony as relevant inappropriateness.</title>
<date>2000</date>
<journal>Journal ofpragmatics,</journal>
<pages>32--6</pages>
<contexts>
<context position="1713" citStr="Attardo, 2000" startWordPosition="262" endWordPosition="264"> Wilson, 1981; Utsumi, 1996). Despite theories differ on how to define irony, they all commonly agree that it involves an incongruity between the literal meaning of an utterance and what is expected about the speaker and/or the environment. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that covers these devices focusing for the first time on the automatic detection of irony in French tweets. According to (Grice et al., 1975; Searle, 1979; Attardo, 2000), the search for a non-literal meaning starts when the hearer realizes that the speaker’s utterance is context-inappropriate, that is an utterance fails to make sense against the context. For example, the tweet: “Congratulation #lesbleus for your great match!” is ironic if the French soccer team has lost the match. An analysis of a corpus of French tweets shows that there are two ways to infer such a context: (a) rely exclusively on the lexical clues internal to the utterance, or (b) combine these clues with an additional pragmatic context external to the utterance. In (a), the speaker intenti</context>
<context position="17178" citStr="Attardo, 2000" startWordPosition="2812" endWordPosition="2813">Don’t worry. Senegal is the world champion soccer”. (4) Oppositions that involve a contradiction between two words that are semantically unrelated, a named entity and a given event (e.g. “Tchad and “democratic election”), etc. Case (4) is more frequent in the NoNeg corpus. Knowing that tweets with negation represent 62.75% of our corpus, and given that irony can focus on the negation of a word or a proposition (Haverkate, 1990), we propose to improve the classification of these tweets by identifying the absurdity of their content, following Attardo’s relevant inappropriateness model of irony (Attardo, 2000) in which a violation of contextual appropriateness signals ironical intent. 4 Deriving the pragmatic context The proposed model included two parts: binary classifiers trained with tweet features, and an algorithm that corrects the outputs of the classifiers which are likely to be misclassified. These two phases can be applied successively or together. In this latter case, the algorithm outputs are integrated into the classifiers and the corrected instances are used in the training process of the binary classifier. In this paper, we only present results of the two phases applied successively b</context>
</contexts>
<marker>Attardo, 2000</marker>
<rawString>Salvatore Attardo. 2000. Irony as relevant inappropriateness. Journal ofpragmatics, 32(6):793–826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Barbieri</author>
<author>Horacio Saggion</author>
</authors>
<title>Modelling Irony in Twitter: Feature Analysis and Evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of Language Resources and Evaluation Conference (LREC),</booktitle>
<pages>4258--4264</pages>
<contexts>
<context position="4445" citStr="Barbieri and Saggion, 2014" startWordPosition="717" endWordPosition="720">t al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc44 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 644–650, Beijing, China, July 26-31, 2015. c�2015 Association for Co</context>
<context position="12444" citStr="Barbieri and Saggion, 2014" startWordPosition="2038" endWordPosition="2041">clamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “hence, therefore, as a result” since we assume that non ironic tweets are likely to be more verbose. To implement these features, we rely on manually built French lexicons to deal with interjections, emoticons, slang language, and discourse connectives (Roze et al., 2012). Sentiment features consist of features that check for the presence of positive/negative opinion words (Reyes and Rosso, 2012) and the number of positive and negative opinion words (Barbieri and Saggion, 2014). We add three new features: the presence of words that express surprise or astonishment, and the presence and the number of neutral opinions. To get these features we use two lexicons: CASOAR, a French opinion lexicon (Benamara et al., 2014) and EMOTAIX, a publicly available French emotion and affect lexicon. Sentiment shifter features group checks if a given tweet contains an opinion word which is in the scope of an intensifier adverb or a modality. Shifter features tests if a tweet contains an intensifier (Liebrecht et al., 2013), a negation word (Reyes et al., 2013), or reporting speech ve</context>
</contexts>
<marker>Barbieri, Saggion, 2014</marker>
<rawString>Francesco Barbieri and Horacio Saggion. 2014. Modelling Irony in Twitter: Feature Analysis and Evaluation. In Proceedings of Language Resources and Evaluation Conference (LREC), pages 4258–4264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farah Benamara</author>
<author>V´eronique Moriceau</author>
<author>Yvette Yannick Mathieu</author>
</authors>
<title>Fine-grained semantic categorization of opinion expressions for consensus detection (Cat´egorisation s´emantique fine des expressions d’opinion pour la d´etection de consensus) [in French].</title>
<date>2014</date>
<booktitle>In TALN-RECITAL 2014 Workshop DEFT 2014 : D ´Efi Fouille de Textes (DEFT</booktitle>
<pages>36--44</pages>
<contexts>
<context position="12686" citStr="Benamara et al., 2014" startWordPosition="2080" endWordPosition="2083">o implement these features, we rely on manually built French lexicons to deal with interjections, emoticons, slang language, and discourse connectives (Roze et al., 2012). Sentiment features consist of features that check for the presence of positive/negative opinion words (Reyes and Rosso, 2012) and the number of positive and negative opinion words (Barbieri and Saggion, 2014). We add three new features: the presence of words that express surprise or astonishment, and the presence and the number of neutral opinions. To get these features we use two lexicons: CASOAR, a French opinion lexicon (Benamara et al., 2014) and EMOTAIX, a publicly available French emotion and affect lexicon. Sentiment shifter features group checks if a given tweet contains an opinion word which is in the scope of an intensifier adverb or a modality. Shifter features tests if a tweet contains an intensifier (Liebrecht et al., 2013), a negation word (Reyes et al., 2013), or reporting speech verbs. Opposition features are new and check for the presence of specific lexico-syntactic patterns that verify whether a tweet contains a sentiment opposition or an explicit positive/negative contrast between a subjective proposition and an ob</context>
</contexts>
<marker>Benamara, Moriceau, Mathieu, 2014</marker>
<rawString>Farah Benamara, V´eronique Moriceau, and Yvette Yannick Mathieu. 2014. Fine-grained semantic categorization of opinion expressions for consensus detection (Cat´egorisation s´emantique fine des expressions d’opinion pour la d´etection de consensus) [in French]. In TALN-RECITAL 2014 Workshop DEFT 2014 : D ´Efi Fouille de Textes (DEFT 2014 Workshop: Text Mining Challenge), pages 36–44, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clint Burfoot</author>
<author>Clint Baldwin</author>
</authors>
<title>Automatic satire detection: Are you having a laugh?</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP</booktitle>
<pages>161--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4326" citStr="Burfoot and Baldwin, 2009" startWordPosition="696" endWordPosition="699">n is quite a hot topic in the research community also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc44 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on</context>
<context position="11646" citStr="Burfoot and Baldwin, 2009" startWordPosition="1911" endWordPosition="1914"> contain negation –404 IR and 1028 NIR) and tested on 618 tweets (360 contain negation – 66 IR and 294 NIR). For each classifier, we represent each tweet with a vector composed of six groups of features. Most of them are state of the art features, others, in italic font are new. Surface features include tweet length in words (Tsur et al., 2010), the presence or absence of punctuation marks (Gonzalez-Ibanez et al., 2011), words in capital letters (Reyes et al., 2013), interjections (Gonzalez-Ibanez et al., 2011), emoticons (Buschmeier et al., 2014), quotations (Tsur et al., 2010), slang words (Burfoot and Baldwin, 2009), opposition words such as “but” and “although” (Utsumi, 2004), a sequence of exclamation or a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “hence, therefore, as a result” since we assume that non ironic tweets are likely to be more verbose. To implement these features, we rely on manually built French lexicons to deal with interjections, emoticons, slang language, and discourse connectives (Roze et al., 2012). Sentiment </context>
</contexts>
<marker>Burfoot, Baldwin, 2009</marker>
<rawString>Clint Burfoot and Clint Baldwin. 2009. Automatic satire detection: Are you having a laugh? In Proceedings of the ACL-IJCNLP 2009 conference short papers, pages 161–164. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantin Buschmeier</author>
<author>Philipp Cimiano</author>
<author>Roman Klinger</author>
</authors>
<title>An Impact Analysis of Features in a Classification Approach to Irony Detection in Product Reviews.</title>
<date>2014</date>
<booktitle>In Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="5661" citStr="Buschmeier et al., 2014" startWordPosition="906" endWordPosition="909">n for Computational Linguistics tion of named entities present in a given document and queries the web for the conjunction of those entities. (Gonzalez-Ibanez et al., 2011) exploited the common ground between speaker and hearer by looking if a tweet is a reply to another tweet. (Reyes et al., 2013) employed opposition in time (adverbs of time such as now and suddenly) and context imbalance to estimate the semantic similarity of concepts in a text to each other. (Barbieri and Saggion, 2014) captured the gap between rare and common words as well as the use of common vs. rare synonyms. Finally, (Buschmeier et al., 2014) measured the imbalance between the overall polarity of words in a review and the star-rating. Most of these pragmatic features rely on linguistic aspects of the tweet by using only the text of the tweet. We aim here to go further by proposing a novel computational model able to capture the “outside of the utterance” context needed to infer irony in implicit oppositions. 2 Methodology An analysis of a corpus of French ironic tweets randomly chosen from various topics shows that more than 62.75% of tweets contain explicit negation markers such as “ne...pas” (not) or negative polarity items like</context>
<context position="11573" citStr="Buschmeier et al., 2014" startWordPosition="1900" endWordPosition="1903"> and testing on 430 tweets. CAll has been trained on 2,472 tweets (1432 contain negation –404 IR and 1028 NIR) and tested on 618 tweets (360 contain negation – 66 IR and 294 NIR). For each classifier, we represent each tweet with a vector composed of six groups of features. Most of them are state of the art features, others, in italic font are new. Surface features include tweet length in words (Tsur et al., 2010), the presence or absence of punctuation marks (Gonzalez-Ibanez et al., 2011), words in capital letters (Reyes et al., 2013), interjections (Gonzalez-Ibanez et al., 2011), emoticons (Buschmeier et al., 2014), quotations (Tsur et al., 2010), slang words (Burfoot and Baldwin, 2009), opposition words such as “but” and “although” (Utsumi, 2004), a sequence of exclamation or a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “hence, therefore, as a result” since we assume that non ironic tweets are likely to be more verbose. To implement these features, we rely on manually built French lexicons to deal with interjections, emoticons, </context>
</contexts>
<marker>Buschmeier, Cimiano, Klinger, 2014</marker>
<rawString>Konstantin Buschmeier, Philipp Cimiano, and Roman Klinger. 2014. An Impact Analysis of Features in a Classification Approach to Irony Detection in Product Reviews. In Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Carvalho</author>
<author>Lu´ıs Sarmento</author>
<author>M´ario J Silva</author>
<author>Eug´enio De Oliveira</author>
</authors>
<title>Clues for detecting irony in user-generated contents: oh...!! it’s so easy;-).</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion,</booktitle>
<pages>53--56</pages>
<publisher>ACM.</publisher>
<marker>Carvalho, Sarmento, Silva, De Oliveira, 2009</marker>
<rawString>Paula Carvalho, Lu´ıs Sarmento, M´ario J Silva, and Eug´enio De Oliveira. 2009. Clues for detecting irony in user-generated contents: oh...!! it’s so easy;-). In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion, pages 53–56. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Richard J Gerrig</author>
</authors>
<title>On the pretense theory of irony.</title>
<date>1984</date>
<journal>Journal of Experimental Psychology: General,</journal>
<volume>113</volume>
<issue>1</issue>
<contexts>
<context position="1479" citStr="Clark and Gerrig, 1984" startWordPosition="220" endWordPosition="223">dity of Pl. Our first results are encouraging and show that deriving a pragmatic contextual model is feasible. 1 Motivation Irony is a complex linguistic phenomenon widely studied in philosophy and linguistics (Grice et al., 1975; Sperber and Wilson, 1981; Utsumi, 1996). Despite theories differ on how to define irony, they all commonly agree that it involves an incongruity between the literal meaning of an utterance and what is expected about the speaker and/or the environment. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that covers these devices focusing for the first time on the automatic detection of irony in French tweets. According to (Grice et al., 1975; Searle, 1979; Attardo, 2000), the search for a non-literal meaning starts when the hearer realizes that the speaker’s utterance is context-inappropriate, that is an utterance fails to make sense against the context. For example, the tweet: “Congratulation #lesbleus for your great match!” is ironic if the French soccer team has lost the match. An analysis of a corpus of French tweets shows th</context>
</contexts>
<marker>Clark, Gerrig, 1984</marker>
<rawString>Herbert H Clark and Richard J Gerrig. 1984. On the pretense theory of irony. Journal of Experimental Psychology: General, 113(1):121–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Semi-supervised Recognition of Sarcastic Sentences in Twitter and Amazon.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10,</booktitle>
<pages>107--116</pages>
<contexts>
<context position="4348" citStr="Davidov et al., 2010" startWordPosition="700" endWordPosition="703">he research community also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc44 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Proc</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised Recognition of Sarcastic Sentences in Twitter and Amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10, pages 107–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniruddha Ghosh</author>
<author>Guofu Li</author>
<author>Tony Veale</author>
<author>Paolo Rosso</author>
<author>Ekaterina Shutova</author>
<author>John Barnden</author>
<author>Antonio Reyes</author>
</authors>
<title>Semeval-2015 task 11: Sentiment Analysis of Figurative Language in Twitter.</title>
<date>2015</date>
<booktitle>In Proc. 9th Int. Workshop on Semantic Evaluation (SemEval 2015), Co-located with NAACL,</booktitle>
<pages>470478</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="3830" citStr="Ghosh et al., 2015" startWordPosition="621" endWordPosition="624">se the writer believes that his audience can detect the disparity between P and P&apos; on the basis of contextual knowledge or common background shared with the writer. For example, in “#Hollande is really a good diplomat #Algeria.”, the writer critics the foreign policy of the French president Hollande in Algeria, whereas in ”The #NSA wiretapped a whole country. No worries for #Belgium: it is not a whole country.“, the irony occurs because the fact in bold font is not true. Irony detection is quite a hot topic in the research community also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and</context>
</contexts>
<marker>Ghosh, Li, Veale, Rosso, Shutova, Barnden, Reyes, 2015</marker>
<rawString>Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso, Ekaterina Shutova, John Barnden, and Antonio Reyes. 2015. Semeval-2015 task 11: Sentiment Analysis of Figurative Language in Twitter. In Proc. 9th Int. Workshop on Semantic Evaluation (SemEval 2015), Co-located with NAACL, page 470478. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond W Gibbs</author>
</authors>
<title>Irony in talk among friends. Metaphor and symbol,</title>
<date>2000</date>
<pages>15--1</pages>
<contexts>
<context position="1493" citStr="Gibbs, 2000" startWordPosition="224" endWordPosition="225">sults are encouraging and show that deriving a pragmatic contextual model is feasible. 1 Motivation Irony is a complex linguistic phenomenon widely studied in philosophy and linguistics (Grice et al., 1975; Sperber and Wilson, 1981; Utsumi, 1996). Despite theories differ on how to define irony, they all commonly agree that it involves an incongruity between the literal meaning of an utterance and what is expected about the speaker and/or the environment. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that covers these devices focusing for the first time on the automatic detection of irony in French tweets. According to (Grice et al., 1975; Searle, 1979; Attardo, 2000), the search for a non-literal meaning starts when the hearer realizes that the speaker’s utterance is context-inappropriate, that is an utterance fails to make sense against the context. For example, the tweet: “Congratulation #lesbleus for your great match!” is ironic if the French soccer team has lost the match. An analysis of a corpus of French tweets shows that there are t</context>
</contexts>
<marker>Gibbs, 2000</marker>
<rawString>Raymond W Gibbs. 2000. Irony in talk among friends. Metaphor and symbol, 15(1-2):5–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Gonzalez-Ibanez</author>
<author>Smaranda Muresan</author>
<author>Nina Wacholde</author>
</authors>
<title>Identifying sarcasm in Twitter: a closer look.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2,</booktitle>
<pages>581--586</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5209" citStr="Gonzalez-Ibanez et al., 2011" startWordPosition="828" endWordPosition="831">uestions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc44 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 644–650, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tion of named entities present in a given document and queries the web for the conjunction of those entities. (Gonzalez-Ibanez et al., 2011) exploited the common ground between speaker and hearer by looking if a tweet is a reply to another tweet. (Reyes et al., 2013) employed opposition in time (adverbs of time such as now and suddenly) and context imbalance to estimate the semantic similarity of concepts in a text to each other. (Barbieri and Saggion, 2014) captured the gap between rare and common words as well as the use of common vs. rare synonyms. Finally, (Buschmeier et al., 2014) measured the imbalance between the overall polarity of words in a review and the star-rating. Most of these pragmatic features rely on linguistic a</context>
<context position="11443" citStr="Gonzalez-Ibanez et al., 2011" startWordPosition="1882" endWordPosition="1885">th IR) and non ironic (henceforth NIR) instances2. The results presented in this paper have been obtained when training CNoNeg on 1,720 and testing on 430 tweets. CAll has been trained on 2,472 tweets (1432 contain negation –404 IR and 1028 NIR) and tested on 618 tweets (360 contain negation – 66 IR and 294 NIR). For each classifier, we represent each tweet with a vector composed of six groups of features. Most of them are state of the art features, others, in italic font are new. Surface features include tweet length in words (Tsur et al., 2010), the presence or absence of punctuation marks (Gonzalez-Ibanez et al., 2011), words in capital letters (Reyes et al., 2013), interjections (Gonzalez-Ibanez et al., 2011), emoticons (Buschmeier et al., 2014), quotations (Tsur et al., 2010), slang words (Burfoot and Baldwin, 2009), opposition words such as “but” and “although” (Utsumi, 2004), a sequence of exclamation or a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “hence, therefore, as a result” since we assume that non ironic tweets are likely </context>
</contexts>
<marker>Gonzalez-Ibanez, Muresan, Wacholde, 2011</marker>
<rawString>Roberto Gonzalez-Ibanez, Smaranda Muresan, and Nina Wacholde. 2011. Identifying sarcasm in Twitter: a closer look. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 581–586. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
<author>Peter Cole</author>
<author>Jerry L Morgan</author>
</authors>
<title>Syntax and semantics. Logic and conversation,</title>
<date>1975</date>
<pages>3--41</pages>
<contexts>
<context position="1086" citStr="Grice et al., 1975" startWordPosition="157" endWordPosition="160">ture the pragmatic context needed to infer irony in tweets. We aim to test the validity of two main hypotheses: (1) the presence of negations, as an internal propriety of an utterance, can help to detect the disparity between the literal and the intended meaning of an utterance, (2) a tweet containing an asserted fact of the form Not(Pi) is ironic if and only if one can assess the absurdity of Pl. Our first results are encouraging and show that deriving a pragmatic contextual model is feasible. 1 Motivation Irony is a complex linguistic phenomenon widely studied in philosophy and linguistics (Grice et al., 1975; Sperber and Wilson, 1981; Utsumi, 1996). Despite theories differ on how to define irony, they all commonly agree that it involves an incongruity between the literal meaning of an utterance and what is expected about the speaker and/or the environment. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that covers these devices focusing for the first time on the automatic detection of irony in French tweets. According to (Grice et al., 1975; S</context>
</contexts>
<marker>Grice, Cole, Morgan, 1975</marker>
<rawString>H Paul Grice, Peter Cole, and Jerry L Morgan. 1975. Syntax and semantics. Logic and conversation, 3:41–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henk Haverkate</author>
</authors>
<title>A speech act analysis of irony.</title>
<date>1990</date>
<journal>Journal of Pragmatics,</journal>
<volume>14</volume>
<issue>1</issue>
<pages>109</pages>
<contexts>
<context position="16995" citStr="Haverkate, 1990" startWordPosition="2786" endWordPosition="2787">save time and money ;) #HUMOUR” 646 which are hard to automatically analyze. (2) The irony is about specific situations (Shelley, 2001). (3) False assertions about hot topics, like in “Don’t worry. Senegal is the world champion soccer”. (4) Oppositions that involve a contradiction between two words that are semantically unrelated, a named entity and a given event (e.g. “Tchad and “democratic election”), etc. Case (4) is more frequent in the NoNeg corpus. Knowing that tweets with negation represent 62.75% of our corpus, and given that irony can focus on the negation of a word or a proposition (Haverkate, 1990), we propose to improve the classification of these tweets by identifying the absurdity of their content, following Attardo’s relevant inappropriateness model of irony (Attardo, 2000) in which a violation of contextual appropriateness signals ironical intent. 4 Deriving the pragmatic context The proposed model included two parts: binary classifiers trained with tweet features, and an algorithm that corrects the outputs of the classifiers which are likely to be misclassified. These two phases can be applied successively or together. In this latter case, the algorithm outputs are integrated into</context>
</contexts>
<marker>Haverkate, 1990</marker>
<rawString>Henk Haverkate. 1990. A speech act analysis of irony. Journal of Pragmatics, 14(1):77 – 109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Liebrecht</author>
<author>Florian Kunneman</author>
<author>Bosch Antal van den</author>
</authors>
<title>The perfect solution for detecting sarcasm in tweets# not.</title>
<date>2013</date>
<booktitle>In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis,</booktitle>
<pages>29--37</pages>
<publisher>ACL.</publisher>
<location>New Brunswick, NJ:</location>
<marker>Liebrecht, Kunneman, van den, 2013</marker>
<rawString>Christine Liebrecht, Florian Kunneman, and Bosch Antal van den. 2013. The perfect solution for detecting sarcasm in tweets# not. In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 29–37. New Brunswick, NJ: ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan I Moldovan</author>
<author>Sanda M Harabagiu</author>
<author>Roxana Girju</author>
<author>Paul Morarescu</author>
<author>V Finley Lacatusu</author>
<author>Adrian Novischi</author>
<author>Adriana Badulescu</author>
<author>Orest Bolohan</author>
</authors>
<title>LCC Tools for Question Answering. In TREC.</title>
<date>2002</date>
<marker>Moldovan, Harabagiu, Girju, Morarescu, Lacatusu, Novischi, Badulescu, Bolohan, 2002</marker>
<rawString>Dan I Moldovan, Sanda M Harabagiu, Roxana Girju, Paul Morarescu, V Finley Lacatusu, Adrian Novischi, Adriana Badulescu, and Orest Bolohan. 2002. LCC Tools for Question Answering. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Reyes</author>
<author>Paolo Rosso</author>
</authors>
<title>Making objective decisions from subjective data: Detecting irony in customer reviews. Decision Support Systems,</title>
<date>2012</date>
<pages>53--4</pages>
<contexts>
<context position="12361" citStr="Reyes and Rosso, 2012" startWordPosition="2024" endWordPosition="2027">a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “hence, therefore, as a result” since we assume that non ironic tweets are likely to be more verbose. To implement these features, we rely on manually built French lexicons to deal with interjections, emoticons, slang language, and discourse connectives (Roze et al., 2012). Sentiment features consist of features that check for the presence of positive/negative opinion words (Reyes and Rosso, 2012) and the number of positive and negative opinion words (Barbieri and Saggion, 2014). We add three new features: the presence of words that express surprise or astonishment, and the presence and the number of neutral opinions. To get these features we use two lexicons: CASOAR, a French opinion lexicon (Benamara et al., 2014) and EMOTAIX, a publicly available French emotion and affect lexicon. Sentiment shifter features group checks if a given tweet contains an opinion word which is in the scope of an intensifier adverb or a modality. Shifter features tests if a tweet contains an intensifier (Li</context>
</contexts>
<marker>Reyes, Rosso, 2012</marker>
<rawString>Antonio Reyes and Paolo Rosso. 2012. Making objective decisions from subjective data: Detecting irony in customer reviews. Decision Support Systems, 53(4):754–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Reyes</author>
<author>Paolo Rosso</author>
<author>Tony Veale</author>
</authors>
<title>A multidimensional approach for detecting irony in twitter. Language resources and evaluation,</title>
<date>2013</date>
<pages>47--1</pages>
<contexts>
<context position="4416" citStr="Reyes et al., 2013" startWordPosition="713" endWordPosition="716">nt analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc44 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 644–650, Beijing, China, July 26-31, 20</context>
<context position="11490" citStr="Reyes et al., 2013" startWordPosition="1890" endWordPosition="1893">sults presented in this paper have been obtained when training CNoNeg on 1,720 and testing on 430 tweets. CAll has been trained on 2,472 tweets (1432 contain negation –404 IR and 1028 NIR) and tested on 618 tweets (360 contain negation – 66 IR and 294 NIR). For each classifier, we represent each tweet with a vector composed of six groups of features. Most of them are state of the art features, others, in italic font are new. Surface features include tweet length in words (Tsur et al., 2010), the presence or absence of punctuation marks (Gonzalez-Ibanez et al., 2011), words in capital letters (Reyes et al., 2013), interjections (Gonzalez-Ibanez et al., 2011), emoticons (Buschmeier et al., 2014), quotations (Tsur et al., 2010), slang words (Burfoot and Baldwin, 2009), opposition words such as “but” and “although” (Utsumi, 2004), a sequence of exclamation or a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “hence, therefore, as a result” since we assume that non ironic tweets are likely to be more verbose. To implement these features</context>
<context position="13020" citStr="Reyes et al., 2013" startWordPosition="2136" endWordPosition="2139"> opinion words (Barbieri and Saggion, 2014). We add three new features: the presence of words that express surprise or astonishment, and the presence and the number of neutral opinions. To get these features we use two lexicons: CASOAR, a French opinion lexicon (Benamara et al., 2014) and EMOTAIX, a publicly available French emotion and affect lexicon. Sentiment shifter features group checks if a given tweet contains an opinion word which is in the scope of an intensifier adverb or a modality. Shifter features tests if a tweet contains an intensifier (Liebrecht et al., 2013), a negation word (Reyes et al., 2013), or reporting speech verbs. Opposition features are new and check for the presence of specific lexico-syntactic patterns that verify whether a tweet contains a sentiment opposition or an explicit positive/negative contrast between a subjective proposition and an objective one. These features have been partly inspired from (Riloff et al., 2013) who proposed a bootstrapping algorithm to detect sarcastic tweets of the form [P+].[Pobj] which corresponds to a contrast between positive sentiment and an objective negative situation. We extended this pattern to 2For CNoNeg and CAll, we also tested 10</context>
<context position="24509" citStr="Reyes et al., 2013" startWordPosition="4081" endWordPosition="4084">sider that using internal contextual features (presence of personal pronouns and named entities) can be a way to automatically detect tweets that are likely to be misclassified. 5 Discussion and conclusions This paper proposed a model to identify irony in implicit oppositions in French. As far as we know, this is the first work on irony detection in French on Twitter data. Comparing to other languages, our results are very encouraging. For example, sarcasm detection achieved 30% precision in Dutch tweets (Liebrecht et al., 2013) while irony detection in English data resulted in 79% precision (Reyes et al., 2013). We treat French irony as an overall term that covers other figurative language devices such as sarcasm, humor, etc. This is a first step before moving to a more fine-grained automatic identification of figurative language in French. For interesting discussions on the distinction/similarity between irony and sarcasm hastags, see (Wang, 2013). One of the main contribution of this study is that the proposed model does not rely only on the lexical clues of a tweet, but also on its pragmatic context. Our intuition is that a tweet containing an asserted fact of the form Not(P1) is ironic if and on</context>
<context position="26155" citStr="Reyes et al., 2013" startWordPosition="4361" endWordPosition="4364">t features and new features (explicit and implicit opposition, sentiment shifter, discourse connectives). Overall accuracies were good when the data contain both tweets with negation and no negation but lower when tweets contain only negation or no negation at all. Error analysis show that major errors come from the presence of implicit oppositions, particularly in CNe9 and CAll. These results empirically validate hypothesis (H1). Negation has been shown to be very helpful in many NLP tasks, such as sentiment analysis (Wiegand et al., 2010). It has also been used as a feature to detect irony (Reyes et al., 2013). However, no one has empirically measured how irony classification behaves in the presence or absence of negation in the data. To test (H2), we proposed a query-based method that corrects the classifier’s outputs in order to retrieve false assertions. Our experiments show that the classification after applying Google searches in reliable web sites significantly improves the classifier accuracy when tested on CNe9. In addition, we show that internal context features are useful to improve classification. These results empirically validate (H2). However, even though the algorithm improves the cl</context>
</contexts>
<marker>Reyes, Rosso, Veale, 2013</marker>
<rawString>Antonio Reyes, Paolo Rosso, and Tony Veale. 2013. A multidimensional approach for detecting irony in twitter. Language resources and evaluation, 47(1):239–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Ashequl Qadir</author>
<author>Prafulla Surve</author>
<author>Lalindra De Silva</author>
<author>Nathan Gilbert</author>
<author>Ruihong Huang</author>
</authors>
<title>Sarcasm as Contrast between a Positive Sentiment and Negative Situation. In</title>
<date>2013</date>
<booktitle>EMNLP,</booktitle>
<pages>704--714</pages>
<marker>Riloff, Qadir, Surve, De Silva, Gilbert, Huang, 2013</marker>
<rawString>Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang. 2013. Sarcasm as Contrast between a Positive Sentiment and Negative Situation. In EMNLP, pages 704–714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlotte Roze</author>
<author>Laurence Danlos</author>
<author>Philippe Muller</author>
</authors>
<title>Lexconn: A French lexicon of discourse connectives. Discours, Multidisciplinary Perspectives on Signalling Text Organisation, 10:(on line).</title>
<date>2012</date>
<contexts>
<context position="12234" citStr="Roze et al., 2012" startWordPosition="2006" endWordPosition="2009">ds (Burfoot and Baldwin, 2009), opposition words such as “but” and “although” (Utsumi, 2004), a sequence of exclamation or a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “hence, therefore, as a result” since we assume that non ironic tweets are likely to be more verbose. To implement these features, we rely on manually built French lexicons to deal with interjections, emoticons, slang language, and discourse connectives (Roze et al., 2012). Sentiment features consist of features that check for the presence of positive/negative opinion words (Reyes and Rosso, 2012) and the number of positive and negative opinion words (Barbieri and Saggion, 2014). We add three new features: the presence of words that express surprise or astonishment, and the presence and the number of neutral opinions. To get these features we use two lexicons: CASOAR, a French opinion lexicon (Benamara et al., 2014) and EMOTAIX, a publicly available French emotion and affect lexicon. Sentiment shifter features group checks if a given tweet contains an opinion w</context>
</contexts>
<marker>Roze, Danlos, Muller, 2012</marker>
<rawString>Charlotte Roze, Laurence Danlos, and Philippe Muller. 2012. Lexconn: A French lexicon of discourse connectives. Discours, Multidisciplinary Perspectives on Signalling Text Organisation, 10:(on line).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Searle</author>
</authors>
<title>Expression and meaning: Studies in the theory of speech acts.</title>
<date>1979</date>
<institution>Cambridge University.</institution>
<contexts>
<context position="1697" citStr="Searle, 1979" startWordPosition="260" endWordPosition="261">5; Sperber and Wilson, 1981; Utsumi, 1996). Despite theories differ on how to define irony, they all commonly agree that it involves an incongruity between the literal meaning of an utterance and what is expected about the speaker and/or the environment. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that covers these devices focusing for the first time on the automatic detection of irony in French tweets. According to (Grice et al., 1975; Searle, 1979; Attardo, 2000), the search for a non-literal meaning starts when the hearer realizes that the speaker’s utterance is context-inappropriate, that is an utterance fails to make sense against the context. For example, the tweet: “Congratulation #lesbleus for your great match!” is ironic if the French soccer team has lost the match. An analysis of a corpus of French tweets shows that there are two ways to infer such a context: (a) rely exclusively on the lexical clues internal to the utterance, or (b) combine these clues with an additional pragmatic context external to the utterance. In (a), the</context>
</contexts>
<marker>Searle, 1979</marker>
<rawString>J. Searle. 1979. Expression and meaning: Studies in the theory of speech acts. Cambridge University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cameron Shelley</author>
</authors>
<title>The bicoherence theory of situational irony.</title>
<date>2001</date>
<journal>Cognitive Science,</journal>
<volume>25</volume>
<issue>5</issue>
<contexts>
<context position="16514" citStr="Shelley, 2001" startWordPosition="2706" endWordPosition="2707"> can manifest itself in several ways: (1) there is no pointer that helps to identify the main topic of the tweet, as in “I’ve been missing her, damn!”. Even if the topic is present, it is often lexicalized in several collapsed words or funny hashtags (#baddays, #aprilfoll), 3Results with all features are lower. 4e.g. “Benzema in the French team is like Sunday. He is of no use.. :D” 5e.g. “I propose that we send Hollande instead of the space probes on the next comet, it will save time and money ;) #HUMOUR” 646 which are hard to automatically analyze. (2) The irony is about specific situations (Shelley, 2001). (3) False assertions about hot topics, like in “Don’t worry. Senegal is the world champion soccer”. (4) Oppositions that involve a contradiction between two words that are semantically unrelated, a named entity and a given event (e.g. “Tchad and “democratic election”), etc. Case (4) is more frequent in the NoNeg corpus. Knowing that tweets with negation represent 62.75% of our corpus, and given that irony can focus on the negation of a word or a proposition (Haverkate, 1990), we propose to improve the classification of these tweets by identifying the absurdity of their content, following Att</context>
</contexts>
<marker>Shelley, 2001</marker>
<rawString>Cameron Shelley. 2001. The bicoherence theory of situational irony. Cognitive Science, 25(5):775–818.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Sperber</author>
<author>Deirdre Wilson</author>
</authors>
<title>Irony and the use-mention distinction. Radical pragmatics,</title>
<date>1981</date>
<pages>49--295</pages>
<contexts>
<context position="1112" citStr="Sperber and Wilson, 1981" startWordPosition="161" endWordPosition="164">ontext needed to infer irony in tweets. We aim to test the validity of two main hypotheses: (1) the presence of negations, as an internal propriety of an utterance, can help to detect the disparity between the literal and the intended meaning of an utterance, (2) a tweet containing an asserted fact of the form Not(Pi) is ironic if and only if one can assess the absurdity of Pl. Our first results are encouraging and show that deriving a pragmatic contextual model is feasible. 1 Motivation Irony is a complex linguistic phenomenon widely studied in philosophy and linguistics (Grice et al., 1975; Sperber and Wilson, 1981; Utsumi, 1996). Despite theories differ on how to define irony, they all commonly agree that it involves an incongruity between the literal meaning of an utterance and what is expected about the speaker and/or the environment. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that covers these devices focusing for the first time on the automatic detection of irony in French tweets. According to (Grice et al., 1975; Searle, 1979; Attardo, 2000</context>
</contexts>
<marker>Sperber, Wilson, 1981</marker>
<rawString>Dan Sperber and Deirdre Wilson. 1981. Irony and the use-mention distinction. Radical pragmatics, 49:295–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>ICWSM-A Great Catchy Name: Semi-Supervised Recognition of Sarcastic Sentences in Online Product Reviews.</title>
<date>2010</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="4367" citStr="Tsur et al., 2010" startWordPosition="704" endWordPosition="707">also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc44 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Paper</context>
<context position="11366" citStr="Tsur et al., 2010" startWordPosition="1870" endWordPosition="1873"> for test, with an equal distribution between the ironic (henceforth IR) and non ironic (henceforth NIR) instances2. The results presented in this paper have been obtained when training CNoNeg on 1,720 and testing on 430 tweets. CAll has been trained on 2,472 tweets (1432 contain negation –404 IR and 1028 NIR) and tested on 618 tweets (360 contain negation – 66 IR and 294 NIR). For each classifier, we represent each tweet with a vector composed of six groups of features. Most of them are state of the art features, others, in italic font are new. Surface features include tweet length in words (Tsur et al., 2010), the presence or absence of punctuation marks (Gonzalez-Ibanez et al., 2011), words in capital letters (Reyes et al., 2013), interjections (Gonzalez-Ibanez et al., 2011), emoticons (Buschmeier et al., 2014), quotations (Tsur et al., 2010), slang words (Burfoot and Baldwin, 2009), opposition words such as “but” and “although” (Utsumi, 2004), a sequence of exclamation or a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “henc</context>
</contexts>
<marker>Tsur, Davidov, Rappoport, 2010</marker>
<rawString>Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010. ICWSM-A Great Catchy Name: Semi-Supervised Recognition of Sarcastic Sentences in Online Product Reviews. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Utsumi</author>
</authors>
<title>A unified theory of irony and its computational formalization.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguisticsVolume 2,</booktitle>
<pages>962--967</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1127" citStr="Utsumi, 1996" startWordPosition="165" endWordPosition="166">ny in tweets. We aim to test the validity of two main hypotheses: (1) the presence of negations, as an internal propriety of an utterance, can help to detect the disparity between the literal and the intended meaning of an utterance, (2) a tweet containing an asserted fact of the form Not(Pi) is ironic if and only if one can assess the absurdity of Pl. Our first results are encouraging and show that deriving a pragmatic contextual model is feasible. 1 Motivation Irony is a complex linguistic phenomenon widely studied in philosophy and linguistics (Grice et al., 1975; Sperber and Wilson, 1981; Utsumi, 1996). Despite theories differ on how to define irony, they all commonly agree that it involves an incongruity between the literal meaning of an utterance and what is expected about the speaker and/or the environment. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that covers these devices focusing for the first time on the automatic detection of irony in French tweets. According to (Grice et al., 1975; Searle, 1979; Attardo, 2000), the search f</context>
</contexts>
<marker>Utsumi, 1996</marker>
<rawString>Akira Utsumi. 1996. A unified theory of irony and its computational formalization. In Proceedings of the 16th conference on Computational linguisticsVolume 2, pages 962–967. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Utsumi</author>
</authors>
<title>Stylistic and contextual effects in irony processing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>1369--1374</pages>
<contexts>
<context position="4543" citStr="Utsumi, 2004" startWordPosition="732" endWordPosition="734">tion task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc44 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 644–650, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tion of named entities present in a given document and queries the web for</context>
<context position="11708" citStr="Utsumi, 2004" startWordPosition="1922" endWordPosition="1923">in negation – 66 IR and 294 NIR). For each classifier, we represent each tweet with a vector composed of six groups of features. Most of them are state of the art features, others, in italic font are new. Surface features include tweet length in words (Tsur et al., 2010), the presence or absence of punctuation marks (Gonzalez-Ibanez et al., 2011), words in capital letters (Reyes et al., 2013), interjections (Gonzalez-Ibanez et al., 2011), emoticons (Buschmeier et al., 2014), quotations (Tsur et al., 2010), slang words (Burfoot and Baldwin, 2009), opposition words such as “but” and “although” (Utsumi, 2004), a sequence of exclamation or a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “hence, therefore, as a result” since we assume that non ironic tweets are likely to be more verbose. To implement these features, we rely on manually built French lexicons to deal with interjections, emoticons, slang language, and discourse connectives (Roze et al., 2012). Sentiment features consist of features that check for the presence of po</context>
</contexts>
<marker>Utsumi, 2004</marker>
<rawString>Akira Utsumi. 2004. Stylistic and contextual effects in irony processing. In Proceedings of the 26th Annual Meeting of the Cognitive Science Society, pages 1369–1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Po-Ya Angela Wang</author>
</authors>
<title>Irony or #Sarcasm-A Quantitative and Qualitative Study Based on Twitter.</title>
<date>2013</date>
<contexts>
<context position="24853" citStr="Wang, 2013" startWordPosition="4137" endWordPosition="4138">French on Twitter data. Comparing to other languages, our results are very encouraging. For example, sarcasm detection achieved 30% precision in Dutch tweets (Liebrecht et al., 2013) while irony detection in English data resulted in 79% precision (Reyes et al., 2013). We treat French irony as an overall term that covers other figurative language devices such as sarcasm, humor, etc. This is a first step before moving to a more fine-grained automatic identification of figurative language in French. For interesting discussions on the distinction/similarity between irony and sarcasm hastags, see (Wang, 2013). One of the main contribution of this study is that the proposed model does not rely only on the lexical clues of a tweet, but also on its pragmatic context. Our intuition is that a tweet containing an asserted fact of the form Not(P1) is ironic if and only if one can prove Pl on the basis of some external information. This form of tweets is quite frequent in French (more than 62.75% of our data contain explicit negation words), which suggests two hypotheses: (H1) negation can be a good indicator to detect irony, and (H2) external context can help to detect the absurdity of ironic content. To</context>
</contexts>
<marker>Wang, 2013</marker>
<rawString>Po-Ya Angela Wang. 2013. #Irony or #Sarcasm-A Quantitative and Qualitative Study Based on Twitter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Alexandra Balahur</author>
<author>Benjamin Roth</author>
<author>Dietrich Klakow</author>
<author>Andr´es Montoyo</author>
</authors>
<title>A Survey on the Role of Negation in Sentiment Analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,</booktitle>
<pages>60--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26082" citStr="Wiegand et al., 2010" startWordPosition="4346" endWordPosition="4349">e if negation helps, we built binary classifiers using both state of the art features and new features (explicit and implicit opposition, sentiment shifter, discourse connectives). Overall accuracies were good when the data contain both tweets with negation and no negation but lower when tweets contain only negation or no negation at all. Error analysis show that major errors come from the presence of implicit oppositions, particularly in CNe9 and CAll. These results empirically validate hypothesis (H1). Negation has been shown to be very helpful in many NLP tasks, such as sentiment analysis (Wiegand et al., 2010). It has also been used as a feature to detect irony (Reyes et al., 2013). However, no one has empirically measured how irony classification behaves in the presence or absence of negation in the data. To test (H2), we proposed a query-based method that corrects the classifier’s outputs in order to retrieve false assertions. Our experiments show that the classification after applying Google searches in reliable web sites significantly improves the classifier accuracy when tested on CNe9. In addition, we show that internal context features are useful to improve classification. These results empi</context>
</contexts>
<marker>Wiegand, Balahur, Roth, Klakow, Montoyo, 2010</marker>
<rawString>Michael Wiegand, Alexandra Balahur, Benjamin Roth, Dietrich Klakow, and Andr´es Montoyo. 2010. A Survey on the Role of Negation in Sentiment Analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 60–68. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>