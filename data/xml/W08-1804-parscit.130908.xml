<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023613">
<title confidence="0.997624">
Passage Retrieval for Question Answering using Sliding Windows
</title>
<author confidence="0.987479">
Mahboob Alam Khalid Suzan Verberne
</author>
<affiliation confidence="0.996913">
ISLA, University of Amsterdam Radboud University Nijmegen
</affiliation>
<email confidence="0.994068">
mahboob@science.uva.nl s.verberne@let.ru.nl
</email>
<sectionHeader confidence="0.993761" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999575">
The information retrieval (IR) commu-
nity has investigated many different tech-
niques to retrieve passages from large col-
lections of documents for question answer-
ing (QA). In this paper, we specifically ex-
amine and quantitatively compare the im-
pact of passage retrieval for QA using slid-
ing windows and disjoint windows. We
consider two different data sets, the TREC
2002–2003 QA data set, and 93 why-
questions against INEX Wikipedia. We
discovered that, compared to disjoint win-
dows, using sliding windows results in im-
proved performance of TREC-QA in terms
of TDRR, and in improved performance of
why-QA in terms of success@n and MRR.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999858533333333">
In question answering (QA), text passages are an
important intermediary between full documents
and exact answers. They form a very natural unit
of response for QA systems (Tellex et al., 2003)
and it is known from user studies that users pre-
fer answers to be embedded in paragraph-sized
chunks (Lin et al., 2003) because they can provide
the context of an answer. Therefore, almost all
state-of-the-art QA systems implement some tech-
nique for extracting paragraph-sized fragments of
text from a large corpus.
Most QA systems have a pipeline architecture
consisting of at least three components: ques-
tion analysis, document/passage retrieval, and an-
swer extraction (Hirschman and Gaizauskas, 2001;
</bodyText>
<note confidence="0.723488">
@ 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967842333333333">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999318973684211">
Voorhees, 2001). The quality of a QA sys-
tem heavily depends on the effectiveness of the
integrated retrieval system (second step of the
pipeline): if a retrieval system fails to find any rel-
evant documents for a question, further processing
steps to extract an answer will inevitably fail too
(Monz, 2003). This motivates the need to study
passage retrieval for QA.
There are two common approaches to retriev-
ing passages from a corpus: one is to index each
passage as separate document and retrieve them as
such. The other option is to first retrieve relevant
documents for a given question and then retrieve
passages from the retrieved documents. The pas-
sages themselves can vary in size and degree of
overlap. Their size can be fixed as a number of
words or characters, or varying with the semantic
content (Hearst and Plaunt, 1993) or the structure
of the text (Callan, 1994). The overlap between
two adjacent passages can be either zero, in which
case we speak of disjoint passages, or the passages
may be overlapping, which we refer to as sliding
passages.
In this paper, we compare the effectiveness of
several passage retrieval techniques with respect to
their usefulness for QA. Our main interest is the
contribution of sliding passages as apposed to dis-
joint passages, and we will experiment with a num-
ber of retrieval models. We evaluate the retrieval
approaches on two different QA tasks: (1) factoid-
QA, as defined by the test collection provided by
TREC (Voorhees, 2002; Voorhees, 2003), and (2)
a relatively new problem in the QA field: that of
answering why-questions (why-QA).
The remainder of the paper is organized as fol-
lows. In the next section, we describe related work
on passage retrieval for QA and we motivate what
the main contribution of the current paper is. In
</bodyText>
<page confidence="0.965827">
26
</page>
<note confidence="0.9945025">
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 26–33
Manchester, UK. August 2008
</note>
<bodyText confidence="0.999701666666667">
section 3 we describe our general set-up for pas-
sage retrieval in both QA tasks that we consider. In
section 4, we present the results of the experiments
on TREC-QA data, and in section 5 we present our
results on why-QA. Section 6 gives an overall con-
clusion.
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999980534883721">
The use of passage retrieval for QA has been stud-
ied before. For example, (Tellex et al., 2003)
performed a quantitative evaluation of passage re-
trieval algorithms for QA. They compared differ-
ent passage retrieval algorithms in the context of
their QA system. Their system first returns a
ranked list of 200 documents and then applies dif-
ferent passage retrieval algorithms to the retrieved
documents. They find that the performance of pas-
sage retrieval depends on the performance of the
pre-applied document retrieval step, and therefore
they suggest that document and passage retrieval
technology should be developed independently.
A similar message is conveyed by (Roberts and
Gaizauskas, 2004). They investigate different ap-
proaches to passage retrieval for QA. They iden-
tify each paragraph as a seperate passage. They
find that the optimal approach is to allow multiple
passages per document to be returned and to score
passages independently of their source document.
(Tiedemann, 2007) studies the impact of doc-
ument segmentation approaches on the retrieval
performance of IR for Dutch QA. He finds that
segmentation based on document structure such
as the use of paragraph markup (discourse-based
segmentation) works well with standard informa-
tion retrieval techniques. He tests various other
techniques for document segmentation and various
passage sizes. In his experimental setting, larger
text units (such as documents) produce better per-
formance in passage retrieval. Tiedemann com-
pares different sizes of discourse-based segmen-
tation: sentences, paragraphs and documents. He
finds that larger text units result in a large search
space for subsequent QA modules and hence re-
duce the overall performance of the QA system.
That is why we do not conduct experiments with
different passage sizes in this paper: it is difficult
to measure the outcome of such experiments in-
dependently of the specific answer extraction sys-
tem. We adopt Tiedemann’s best strategy of docu-
ment segmentation strategy, i.e., paragraph-based,
but with equally sized passages instead.
</bodyText>
<sectionHeader confidence="0.982551" genericHeader="method">
3 General experiment set-up
</sectionHeader>
<bodyText confidence="0.998159">
The main purpose of our experiments is to study
the contribution of sliding windows as apposed to
disjoint windows in the context of QA. Therefore,
in our experiment setup, we have kept fixed the
other segmentation variables, passage size and de-
gree of overlap. We set out to examine two differ-
ent strategies of document segmentation (disjoint
and sliding passages) with a number of retrieval
models for two different QA tasks: TREC factoid-
QA and why-QA.
</bodyText>
<subsectionHeader confidence="0.998125">
3.1 Retrieval models
</subsectionHeader>
<bodyText confidence="0.999972242424243">
We use the Lemur retrieval engine1 for passage re-
trieval because it provides a flexible support for
different types of retrieval models including vec-
tor space models and language models. In this
paper we have selected two vector space mod-
els: TFIDF and Okapi BM25 (Robertson and
Walker, 1999), and one language model based on
Kullback-Leibler (KL) divergence (Lafferty and
Zhai, 2001).
The TFIDF weighting scheme is often used in
information retrieval. There are several variations
of the TFIDF weighting scheme that can effect the
performance significantly. The Lemur toolkit pro-
vides a variant of the TFIDF model based on the
Okapi TF formula (Robertson et al., 1995).
Lemur also provides the implementation of the
original Okapi BM25 model, and we have used
this model with default values of 1.2 for k1, 0.75
for b and 7 for k3 as suggested by (Robertson
and Walker, 1999). The KL-divergence retrieval
model, which implements the cross entropy of the
query model with respect to the document model,
is a standard metric for comparing distributions,
which has proven to work well in IR experiments
in the past. To address the data sparseness prob-
lem during model estimation, we use the Dirichlet
smoothing method (Zhai and Lafferty, 2004) with
default parameter values provided in the Lemur
toolkit.
Currently, however, the Lemur 2 does not sup-
port direct passage retrieval. For these experi-
ments, therefore, we first need to segment docu-
ments into passages before indexing them into the
</bodyText>
<footnote confidence="0.9988798">
1Lemur toolkit: http://www.lemurproject.org
2Lemur and Indri are different search engines. Indri pro-
vides the #passage operator, but it doesn’t consider para-
graph boundaries or sentence boundaries for constructing pas-
sages.
</footnote>
<page confidence="0.999271">
27
</page>
<bodyText confidence="0.992926">
Lemur retrieval engine. Our segmenting strategy
is explained below.
</bodyText>
<subsectionHeader confidence="0.999387">
3.2 Passage identification
</subsectionHeader>
<bodyText confidence="0.999629648148148">
For our experiments, we take into account two
different corpora: AQUAINT and the Wikipedia
XML corpus as used in INEX (Denoyer and Gal-
linari, 2006). The AQUAINT corpus consists of
news articles from the Associated Press, New York
Times, and Xinhua News Agency (English ver-
sion) from 1996 to 2000. The Wikipedia XML
collection consists of 659,388 articles as they oc-
cured in the online Wikipedia in the summer of
2006. As we have discussed in Section 2, (Tiede-
mann, 2007) discovered that discourse-based seg-
mentation into paragraphs works well with stan-
dard information retrieval techniques. They also
observe that larger retrieval units produce better re-
sults for passage retrieval, since larger units have
higher chance to cover the required information.
Therefore, we decide to segment each document
into similar sized passages while taking into ac-
count complete paragraphs only.
For document segmentation, our method first
detects sentences in the text using punctuation
marks as separators, and then paragraphs using
empty lines as separators. Sentence boundaries
are necessary because we aim at retrieving pas-
sages that do not contain any broken sentences.
The required passages are identified by aligning
over paragraph boundaries (merging paragraphs
into units until they have the required length ,i.e.
500 characters). The disjoint passages do not share
any content with each other, and the sliding pas-
sages slide with the difference of one paragraph
boundary, i.e., we start forming a new passage
from beginning of each paragraph of the docu-
ment. If paragraph boundaries are not detected,
then these sliding passages are half-overlapped
with each other.
For the Wikipedia XML corpus, we have found
that documents have already been annotated with
&lt;p&gt; elements. Thus we consider these elemens
as paragraph boundaries instead of empty lines as
we did for the AQAINT corpus. We observe that
some textual parts of the documents are not cov-
ered by the XML paragraph boundaries. Therefore
we have extended the existing paragraph bound-
aries such that the missing text fragments become
part of the paragraphs.
We split both corpora into disjoint and slid-
ing windows as we have discussed above. After
splitting the 1.03M documents of the AQUAINT-
1 collection we have 14.2M sliding passages, and
4.82M disjoint passages. And similarly we got
4.1M sliding passages and 2M disjoint passages
from the Wikipedia XML collection of 659,388
documents.
</bodyText>
<subsectionHeader confidence="0.997705">
3.3 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.992465657142857">
For our experiments, we use the following metrics
for evaluation:
Mean reciprocal rank (MRR) at n is the mean
(calculated over all questions) of the recipro-
cal rank (which is 1 divided by the rank or-
dinal) of the highest ranked relevant (i.e. an-
swer bearing) passage. RR is zero for a ques-
tion if no relevant passage is returned by the
system at limit n.
Success at n for a question is 1 if the answer
to this question is found in top n passages
fetched up by our system. Success@n is av-
eraged over all questions.
Total document reciprocal rank (TDRR)
(Bilotti et al., 2004) is the sum of all recipro-
cal ranks of all answer bearing passages per
question (averaged over all questions). The
value of TDRR is maximum if all retrieved
passages are relevant. TDRR is an extension
of MRR that favors a system that ranks more
that one relevant passage higher than all
non-relevant passages. This way, TDRR
extends MRR with a notion of recall.
When we compare retrieval performance of two
retrieval settings (such as the use of disjoint versus
sliding windows), then we obtain a list of paired
scores. That’s why we use the Wilcoxon signed-
rank test to show the statistical significance of the
improvements.
In summary, we experiment with three retrieval
models in Lemur: TFIDF, Okapi, and a language
model based on the Kullback-Leibler divergence.
For each of these retrieval models, we evaluate the
use of both sliding and disjoint passages. This
makes a total of six retrieval settings.
</bodyText>
<sectionHeader confidence="0.9168005" genericHeader="method">
4 Evaluating passage retrieval for
TREC-QA
</sectionHeader>
<bodyText confidence="0.9998625">
As test collection for factoid QA, we use a standard
set of 822 question/answer pairs from the TREC
</bodyText>
<page confidence="0.993068">
28
</page>
<bodyText confidence="0.99954568">
QA tasks of 2002-2003. For evaluation of the
passage retrieval approaches that we consider, we
compute strict scores as defined by (Tellex et al.,
2003). Strict scoring means that a retrieved pas-
sage is considered relevant if the passage not only
matches one of the answer patterns provided by
NIST, but its associated document is also listed as
one of the relevant documents assessed by NIST.
(Bilotti et al., 2004) have reviewed 109 factoid
questions of the TREC-2002 task and they have
extended the existing set of relevant documents by
adding more relevant documents. We have also in-
cluded this extended list of relevant documents for
these questions in our experiment setup.
We evaluate the impact of disjoint and sliding
windows on passage retrieval for QA using three
different retrieval models, using the MRR@n, Suc-
cess@n and TDRR@n metrics as described in sec-
tion 3.3. Table 1 shows the evaluation results (best
scores for each measure in bold face). The ex-
periment results show that language model based
on Kullback-Leibler divergence shows better per-
formance than two vector space models for both
types of windows retrieval according to MRR, suc-
cess@n and TDRR evaluation metrics.
</bodyText>
<subsectionHeader confidence="0.858653">
4.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999903684210526">
In a pipeline QA system, the answer extraction
module depends on the performance of passage re-
trieval. If more answer bearing passages are pro-
vided in the stream, then there is a high chance
of selecting the correct answer from the stream
in later stages of QA. (Roberts and Gaizauskas,
2004) have also discussed the importance of this
aspect of passage retrieval for QA. They have mea-
sured the answer redundancy of a retrieval system
which measures how many answer bearing pas-
sages are returned per question at limit n. (Tiede-
mann, 2007) have also used this metric and argue
that high redundancy is desired to make it easier
for the answer extraction module to spot possible
answers. We consider TDRR as the most impor-
tant measure for the passage retrieval task since
it does not only measure the redundancy of a re-
trieval system but also measures how much im-
provement there is in returning the relevant pas-
sages at top ranks.
According to TDRR@n in table 1, retrieval of
sliding windows outperforms retrieval of disjoint
windows at all limits of n for all retrieval mod-
els. For n = 100, the improvement is significant
at p = 0.01 level. This high value of TDRR@n
suggests that segmenting the documents into slid-
ing windows is a better choice in order to return as
many relevant passages as possible at top ranks.
If we consider Success@n as evaluation mea-
sure instead of TDRR, retrieval of disjoint win-
dows outperforms retrieval of sliding windows.
We think that one of the reasons for this behaviour
is that since sliding windows overlap with their
neighbours, they are more pair-wise similar than
disjoint windows. Therefore, it is possible that for
some non-answered questions many irrelevant pas-
sages are returned at top ranks and that relevant
passages are surpressed down.
</bodyText>
<sectionHeader confidence="0.859772" genericHeader="method">
5 Evaluating passage retrieval for
why-QA
</sectionHeader>
<bodyText confidence="0.9999724">
In the previous section, we showed that for TREC
data, the choice of the retrieval model and the type
of windows to be retrieved influence on the re-
trieval performance. We found that for the TREC
data, a language modeling approach (based on
Kullback-Leibler divergence) on sliding windows
gives the best results in terms of TDRR. In this
section, we aim to find out what the optimal pas-
sage retrieval approach is for a very different type
of QA, namely why-QA.
</bodyText>
<subsectionHeader confidence="0.886184">
5.1 Background of why-QA system
development
</subsectionHeader>
<bodyText confidence="0.999983894736842">
In (Verberne et al., 2008), we present an approach
for why-QA that is based on paragraph retrieval
from the INEX Wikipedia corpus (Denoyer and
Gallinari, 2006). Our system for why-QA con-
sists of two modules: a passage retrieval mod-
ule and a re-ranking module. In earlier retrieval
experiments, we used the Wumpus retrieval sys-
tem (Buttcher, 2007), and we defined passages
simply by the XML paragraph markup &lt;p&gt;. Pas-
sage ranking in Wumpus is done by the QAP pas-
sage scoring algorithm (Buttcher et al., 2004).
The second module of our why-system is a re-
ranking step that uses syntactic features of the
question and the retrieved answers for adapting the
scores of the answers and changing the ranking or-
der. The weights of the re-ranking features have
been optimized by training on our question answer
data in five folds3 using a genetic algorithm. We
let Wumpus retrieve and rank 150 paragraphs per
</bodyText>
<footnote confidence="0.987118">
3In five turns, we tune the feature weights on four of the
five folds and evaluate them on the fifth
</footnote>
<page confidence="0.999466">
29
</page>
<tableCaption confidence="0.950266">
Table 1: Results for passage retrieval for TREC-QA using disjoint windows (DW) and sliding windows
</tableCaption>
<bodyText confidence="0.5303505">
(SW). ** indicates a significant improvements of sliding windows over disjoint windows at the p = 0.01
level.
</bodyText>
<figure confidence="0.9988536875">
MRR Success@n TDRR
DW SW
0.465 0.637
0.459 0.649
n
10
0.518 0.710
ISL
0.517 0.819**
0.535 0.835**
100
0.525 0.902**
ISL
retrieval model
TFIDF
Okapi
DW SW
0.327 0.326
0.322 0.328
DW SW
51.8% 50.1%
51.9% 51.2%
0.355 0.345
55.7% 51.3%
TFIDF
Okapi
0.336 0.386
0.333 0.339
54.1% 53.3%
77.0% 76.2%
0.363 0.353
77.1% 75.2%
</figure>
<bodyText confidence="0.999383611111111">
question. This number of 150 answers was chosen
as a trade-off between covering as many as possi-
ble of the relevant answers retrieved by Wumpus,
and the system load that was needed for automatic
syntactic analysis of all answers in the second (re-
ranking) module of the system. For evaluation of
the results, we performed manual assessment of
all answers retrieved, starting at the highest-ranked
answer and ending as soon as we encountered a
relevant answer4.
The results for our original why-system are in
Table 2. We show the results in terms of suc-
cess@n and MRR@n. As opposed to the evalua-
tion of TREC-QA, we do not consider TDRR as
evaluation measure for experiments on why-QA.
This is because in why-QA, we are only interested
in the top-ranked answer-bearing passage. For cal-
culating TDRR, assessment of all 150 retrieved an-
swers would be necessary.
Table 2 shows that success@150 for the retrieval
module (Wumpus/QAP) is 73.1%. This means that
for 26.9% of the questions, no relevant answer is
retrieved in the first module. Re-ranking the an-
swers cannot increase MRR for these questions,
since none of the 150 answers in the result list
is relevant. We consider a success@150 score of
73.1% to be quite low. We aim to improve the
performance of our system by optimizing its first
module, passage retrieval.
We experiment with a number of passage re-
trieval approaches in order to reach better retrieval
in the first module of our system. We aim to find
out which type of retrieval model and what win-
dow type (disjoint or sliding) gives optimal results
for retrieving passages relevant to why-questions.
If the retrieval performance indeed goes up, we
</bodyText>
<footnote confidence="0.912798">
4We didn’t need to assess the tail since we were only in-
terested in the highest-ranked relevant answer for calculating
MRR and success@n
</footnote>
<bodyText confidence="0.99946875">
will apply our re-ranking module to the newly
retrieved data to see what overall system perfor-
mance we can reach with the new retrieval ap-
proach.
</bodyText>
<subsectionHeader confidence="0.994318">
5.2 Data and evaluation setup
</subsectionHeader>
<bodyText confidence="0.999934636363636">
For development and testing purposes, we use the
Webclopedia question set by (Hovy et al., 2002).
This set contains questions that were asked to the
online QA system answers.com. 805 of these
questions are why-questions. We manually in-
spect a sample of 400 of the Webclopedia why-
questions. Of these, 93 have an answer in the
Wikipedia XML corpus (see section 3). Manual
extraction of one correct answer for each of these
questions results in a set of 93 why-questions and
their reference answer.
In order to be able to do fast evaluation of the
different evaluation settings, we manually create
an answer pattern for each of the questions in our
set. These answer patterns are based on a set of 93
reference answers (one answer per question) that
we have manually extracted from the Wikipedia
corpus. An answer pattern is a regular expres-
sion that defines which of the retrieved passages
are considered a relevant answer to the input ques-
tion.
As opposed to the answer patterns provided by
NIST for the evaluation of factoid QA (see sec-
tion 4), our answer patterns for why-questions are
relatively strict. A why-answer can be formulated
in many different ways with different words, which
may not all be in the answer pattern. For a factoid
question such as “When was John Lennon born?”,
the answer is only one phrase, and the answer
pattern is short and unambiguous, i.e. /1940/.
However, if we consider the why-question “Why
are some organ transplants unsuccessful?”, the
answer pattern cannot be stated in one phrase. For
</bodyText>
<page confidence="0.99867">
30
</page>
<tableCaption confidence="0.981539">
Table 2: Results for the original why-QA pipeline system
</tableCaption>
<table confidence="0.998769">
success@10 success@150 MRR@150
Wumpus/QAP Retrieval 43.0% 73.1% 0.260
+ Re-ranking module 54.8% 73.1% 0.380
</table>
<bodyText confidence="0.998326666666667">
this example, we created the following answer
pattern based on the pre-extracted reference
answer5: /.*immune system.*foreign
tissues.*destroy.*/. It is however pos-
sible that a relevant answer is formulated in a
way that does not match this regular expression.
Thus, the use of answer patterns for the evaluation
of why-QA leads to conservative results: some
relevant answers may be missed in the evaluation
procedure.
After applying the answer patterns, we count the
questions that have at least one relevant answer
in the top 10 and the top 150 of the results (suc-
cess@10, success@150). For the highest ranked
relevant answer per question, we determine the re-
ciprocal rank (RR). If there is no correct answer
retrieved by the system at n = 150, the RR is 0.
Over all questions, we calculate the MRR@150.
</bodyText>
<subsectionHeader confidence="0.998886">
5.3 Passage retrieval results
</subsectionHeader>
<bodyText confidence="0.999886">
We segment and index the Wikipedia corpus as de-
scribed in section 3 and run all six retrieval set-
tings on our set of 93 why-questions. For consis-
tent evaluation, we applied the answer patterns that
we created to the newly retrieved Lemur data as
well as to the original Wumpus output.
The retrieval results for all settings are in Table
3. We show both success@10 and success@150,
and MRR@150 for each setting. Success@150 is
important if we consider the current results as input
for the re-ranking module. As explained before,
re-ranking can only be successful if at least one rel-
evant answer is retrieved by the retrieval module.
For each measure (s@10, s@150 and MRR@150),
the score of the highest-scoring setting is printed in
bold face.
As expected, the evaluation of the Wumpus data
with the use of answer patterns gives somewhat
lower scores than evaluation based on manual as-
sessment of all answers (table 2). This confirms
our idea that the use of answer patterns for why-
QA leads to conservative results. Thus we can
</bodyText>
<footnote confidence="0.872324">
5The pre-extracted reference answer is: “This is because
a normal healthy human immune system can distinguish for-
eign tissues and attempts to destroy them, just as it attempts
to destroy infective organisms such as bacteria and viruses.”
</footnote>
<bodyText confidence="0.974307545454545">
state that the Lemur scores shown in table 3 are
not overestimated and therefore reliable.
Since we are using the output of the passage re-
trieval module as input for our re-ranking mod-
ule, we are mainly interested in the scores for
success@150. For the four retrieval models, we
see that TFIDF seems to score somewhat better
on retrieving sliding windows in terms of suc-
cess@150 than Okapi and the Kullback-Leibler
language model. On the other hand, Kullback-
Leibler and QAP seem to perform better on retriev-
ing disjoint windows. However, these differences
are not significant at the p = 0.01 level. For the
differences between disjoint and sliding windows
for all retrieval models together, we see that re-
trieval of sliding windows gives significantly bet-
ter results than disjoint windows in terms of suc-
cess@150 (p &lt; 0.001).
5.4 The influence of passage retrieval on our
pipeline system
As described in section 5.1, our system is a
pipeline: after passage retrieval, we apply a re-
ranking module that uses syntactic information for
re-scoring the results from the retrieval module. As
input for our re-ranking module we use the out-
put of the retrieval setting with the highest suc-
cess@150 score: Lemur/TFIDF on sliding win-
dows. For 81.7% of the questions in our set,
Lemur/TFIDF retrieved an answer in the top-150.
This means that the maximum success@10 score
that we can obtain by re-ranking is 81.7%.
For weighting the feature values, we re-use the
weights that we had earlier found from training on
our set of 93 questions and the 150 answers that
were retrieved by Wumpus. We again take into
account five-fold cross validation for evaluation.
For a detailed description of our re-ranking mod-
ule and the syntactic features that we exploit, we
refer to (Verberne et al., 2008).
The results from re-ranking are in Table 4.
In the table, four system versions are compared:
(1) the original Wumpus/QAP module, (2) the
original why-pipeline system: Wumpus/QAP with
re-ranking, (3) TFIDF-sliding and (4) the new
</bodyText>
<page confidence="0.999901">
31
</page>
<tableCaption confidence="0.882406">
Table 3: Results for passage retrieval on why-questions against Wikipedia using disjoint windows (DW)
and sliding windows (SW)
</tableCaption>
<table confidence="0.9992675">
Success@10 Success@150 MRR@150
Retrieval model DW SW DW SW DW SW
Baseline: Wumpus/QAP 40.9% 72.0% 0.229
Lemur/TFIDF 43.0% 45.2% 71.1% 81.7% 0.247 0.338
Lemur/Okapi 41.9% 44.1% 67.7% 79.6% 0.243 0.320
Lemur/KL 48.9% 50.0% 72.8% 77.2% 0.263 0.324
</table>
<bodyText confidence="0.998327590909091">
pipeline system: TFIDF-sliding with re-ranking.
We again show MRR, success@10 and suc-
cess@150. For each measure, the score of the
highest-scoring setting is printed in bold face.
After applying our re-ranking module (right bot-
tom setting), we find a significant improvement
over bare TFIDF (left bottom setting). In terms
of MRR, we also see an improvement over the re-
sults that we had obtained by re-ranking the Wum-
pus/QAP output (right top setting). However, suc-
cess@10 does not show significant improvement.
The improvement that the re-ranking module gives
is smaller for the TFIDF retrieval results (MRR
goes from 0.338 to 0.359) than for the QAP results
(MRR increases from 0.260 to 0.328). We suspect
that this may be due to the fact that we used feature
weights for re-ranking that we had earlier obtained
from training on the Wumpus/QAP data (see sec-
tion 5.4). It would be better to re-train our feature
weights on the Lemur data. Probably, re-ranking
can then make a bigger contribution than it does
now for the Lemur data.
</bodyText>
<sectionHeader confidence="0.996399" genericHeader="conclusions">
6 Overall conclusion
</sectionHeader>
<bodyText confidence="0.999661023255814">
In this paper we have investigated the contribu-
tion of sliding windows as apposed to disjoint win-
dows with different retrieval modules for two dif-
ferent QA tasks: the TREC-QA 2002–2003 task
and why-QA.
For the TREC factoid-QA task, we have found
that retrieval of sliding windows outperfoms re-
trieval of disjoint windows in returning as many
relevant passages as possible on top ranks (accord-
ing to the TDRR metric). The experimental results
show that a language model based on Kullback-
Leibler divergence gives better performance than
two vector space models for both types of win-
dows retrieval according to MRR, success@n and
TDRR evaluation metrics. We found that the
number of answered questions (success@n) was
slightly lower when we used sliding windows for
passage retrieval than disjoint windows, but we
think one of the reasons is that sliding windows
are more homogeneous than disjoint windows, and
therefore for some questions more irrelevant pas-
sages are returned at top ranks and relevant pas-
sages are surpressed down.
For the task of retrieving answers to why-
questions from Wikipedia data, we found that the
best retrieval model is TFIDF, and sliding win-
dows give significantly better results than disjoint
windows. We also found better performance for
our complete why-pipeline system after applying
our existing re-ranking module to the passages re-
trieved with TFIDF-sliding.
In general, we find that for QA, sliding win-
dows give better results than disjoint windows in
the passage retrieval step. The best scoring re-
trieval model depends on the task under consid-
eration, because the nature of the documents and
question sets differ. This shows that for each spe-
cific QA task, different retrieval models should be
considered.
In the future, we aim to boost passage retrieval
for QA even more by applying query expansion
techniques that are specific to the QA tasks that
we consider, i.e. TREC factoid-QA and why-QA.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999593538461539">
Bilotti, M.W., B. Katz, and J. Lin. 2004. What works
better for question answering: Stemming or morpho-
logical query expansion. In Proceedings of the SI-
GIR 2004 Workshop IR4QA: Information Retrieval
for Question Answering, July.
Buttcher, S., C.L.A. Clarke, and G.V. Cormack. 2004.
Domain-specific synonym expansion and validation
for biomedical information retrieval (multitext ex-
periments for trec 2004).
Buttcher, S. 2007. The wumpus search engine.
http://www.wumpus-search.org/.
Callan, James P. 1994. Passage-level evidence in doc-
ument retrieval. In SIGIR, pages 302–310.
</reference>
<page confidence="0.999731">
32
</page>
<tableCaption confidence="0.993025">
Table 4: Results for the why-QA pipeline system for best-scoring passage retrieval setting compared
against the Wumpus baseline, for both bare retrieval and the complete system with re-ranking
</tableCaption>
<table confidence="0.99476675">
Success@10 Success@150 MRR
Retrieval model Bare +Re-rank Bare +Re-rank Bare +Re-rank
Baseline: Wumpus/QAP-disjoint 43.0% 54.8% 73.1% 73.1% 0.260 0.328
Lemur/TFIDF-sliding 45.2% 55.9% 81.7% 81.7% 0.338 0.359
</table>
<reference confidence="0.999729678571428">
Denoyer, L. and P. Gallinari. 2006. The Wikipedia
XML corpus. ACM SIGIR Forum, 40(1):64–69.
Hearst, Marti A. and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In
ACM-SIGIR, 1993, pages 59–68.
Hirschman, L. and R. Gaizauskas. 2001. Natural lan-
guage question answering: the view from here. Nat.
Lang. Eng., pages 275–300.
Hovy, E.H., U. Hermjakob, and D. Ravichandran.
2002. A question/answer typology with surface text
patterns. In Proceedings of the Human Language
Technology conference (HLT), San Diego, CA.
Lafferty, J. and C. Zhai. 2001. Document language
models, query models, and risk minimization for in-
formation retrieval. In In Proceedings of SIGIR’01,
pages 111–119.
Lin, J., D. Quan, V. Sinha, K. Bakshi, D. Huynh,
B. Katz, and D.R. Karger. 2003. The role of con-
text in question answering systems. Conference on
Human Factors in Computing Systems, pages 1006–
1007.
Monz, Christof. 2003. Document retrieval in the con-
text of question answering. In ECIR, pages 571–579.
Roberts, I. and R. Gaizauskas. 2004. Evaluating pas-
sage retrieval approaches for question answering. In
In Proceedings of ECIR , 2004.
Robertson, Stephen E. and Steve Walker. 1999.
Okapi/keenbow at trec-8. In Text Retrieval Confer-
ence.
Robertson, Stephen E., Steve Walker, Micheline
Hancock-Beaulieu, and Gatford M. 1995. Okapi at
trec-3. In Text Retrieval Conference, pages 109–26.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative evaluation ofpassage retrieval al-
gorithms for question answering. In In SIGIR con-
ference on Research and development in informaion
retrieval, 2003, pages 41–47.
Tiedemann, J¨org. 2007. Comparing document seg-
mentation strategies for passage retrieval in question
answering. In Proceedings of RANLP 07, Borovets,
Bulgaria.
Verberne, Suzan, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2008. Using Syntactic Infor-
mation for Improving Why-Question Answering. In
Proceedings of The 22nd International Conference
on Computational Linguistics (COLING 2008).
Voorhees, Ellen. 2001. Overview of trec 2001 question
answering track. In In Proceedings of TREC.
Voorhees, Ellen. 2002. Overview of trec 2002 question
answering track. In In Proceedings of TREC.
Voorhees, Ellen. 2003. Overview of trec 2003 question
answering track. In In Proceedings of TREC.
Zhai, ChengXiang and John D. Lafferty. 2004. A study
of smoothing methods for language models applied
to information retrieval. ACM Trans. Inf. Syst., pages
179–214.
</reference>
<page confidence="0.999379">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851729">
<title confidence="0.99987">Passage Retrieval for Question Answering using Sliding Windows</title>
<author confidence="0.997108">Mahboob Alam Khalid Suzan Verberne</author>
<affiliation confidence="0.999647">ISLA, University of Amsterdam Radboud University Nijmegen</affiliation>
<email confidence="0.881094">mahboob@science.uva.nls.verberne@let.ru.nl</email>
<abstract confidence="0.998012588235294">The information retrieval (IR) community has investigated many different techniques to retrieve passages from large collections of documents for question answering (QA). In this paper, we specifically examine and quantitatively compare the impact of passage retrieval for QA using sliding windows and disjoint windows. We consider two different data sets, the TREC QA data set, and 93 questions against INEX Wikipedia. We discovered that, compared to disjoint windows, using sliding windows results in improved performance of TREC-QA in terms of TDRR, and in improved performance of in terms of success@n and MRR.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M W Bilotti</author>
<author>B Katz</author>
<author>J Lin</author>
</authors>
<title>What works better for question answering: Stemming or morphological query expansion.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGIR 2004 Workshop IR4QA: Information Retrieval for Question Answering,</booktitle>
<contexts>
<context position="11366" citStr="Bilotti et al., 2004" startWordPosition="1809" endWordPosition="1812">n of 659,388 documents. 3.3 Evaluation metrics For our experiments, we use the following metrics for evaluation: Mean reciprocal rank (MRR) at n is the mean (calculated over all questions) of the reciprocal rank (which is 1 divided by the rank ordinal) of the highest ranked relevant (i.e. answer bearing) passage. RR is zero for a question if no relevant passage is returned by the system at limit n. Success at n for a question is 1 if the answer to this question is found in top n passages fetched up by our system. Success@n is averaged over all questions. Total document reciprocal rank (TDRR) (Bilotti et al., 2004) is the sum of all reciprocal ranks of all answer bearing passages per question (averaged over all questions). The value of TDRR is maximum if all retrieved passages are relevant. TDRR is an extension of MRR that favors a system that ranks more that one relevant passage higher than all non-relevant passages. This way, TDRR extends MRR with a notion of recall. When we compare retrieval performance of two retrieval settings (such as the use of disjoint versus sliding windows), then we obtain a list of paired scores. That’s why we use the Wilcoxon signedrank test to show the statistical significa</context>
<context position="12835" citStr="Bilotti et al., 2004" startWordPosition="2055" endWordPosition="2058">nt passages. This makes a total of six retrieval settings. 4 Evaluating passage retrieval for TREC-QA As test collection for factoid QA, we use a standard set of 822 question/answer pairs from the TREC 28 QA tasks of 2002-2003. For evaluation of the passage retrieval approaches that we consider, we compute strict scores as defined by (Tellex et al., 2003). Strict scoring means that a retrieved passage is considered relevant if the passage not only matches one of the answer patterns provided by NIST, but its associated document is also listed as one of the relevant documents assessed by NIST. (Bilotti et al., 2004) have reviewed 109 factoid questions of the TREC-2002 task and they have extended the existing set of relevant documents by adding more relevant documents. We have also included this extended list of relevant documents for these questions in our experiment setup. We evaluate the impact of disjoint and sliding windows on passage retrieval for QA using three different retrieval models, using the MRR@n, Success@n and TDRR@n metrics as described in section 3.3. Table 1 shows the evaluation results (best scores for each measure in bold face). The experiment results show that language model based on</context>
</contexts>
<marker>Bilotti, Katz, Lin, 2004</marker>
<rawString>Bilotti, M.W., B. Katz, and J. Lin. 2004. What works better for question answering: Stemming or morphological query expansion. In Proceedings of the SIGIR 2004 Workshop IR4QA: Information Retrieval for Question Answering, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buttcher</author>
<author>C L A Clarke</author>
<author>G V Cormack</author>
</authors>
<title>Domain-specific synonym expansion and validation for biomedical information retrieval (multitext experiments for trec</title>
<date>2004</date>
<contexts>
<context position="16460" citStr="Buttcher et al., 2004" startWordPosition="2666" endWordPosition="2669">e retrieval approach is for a very different type of QA, namely why-QA. 5.1 Background of why-QA system development In (Verberne et al., 2008), we present an approach for why-QA that is based on paragraph retrieval from the INEX Wikipedia corpus (Denoyer and Gallinari, 2006). Our system for why-QA consists of two modules: a passage retrieval module and a re-ranking module. In earlier retrieval experiments, we used the Wumpus retrieval system (Buttcher, 2007), and we defined passages simply by the XML paragraph markup &lt;p&gt;. Passage ranking in Wumpus is done by the QAP passage scoring algorithm (Buttcher et al., 2004). The second module of our why-system is a reranking step that uses syntactic features of the question and the retrieved answers for adapting the scores of the answers and changing the ranking order. The weights of the re-ranking features have been optimized by training on our question answer data in five folds3 using a genetic algorithm. We let Wumpus retrieve and rank 150 paragraphs per 3In five turns, we tune the feature weights on four of the five folds and evaluate them on the fifth 29 Table 1: Results for passage retrieval for TREC-QA using disjoint windows (DW) and sliding windows (SW).</context>
</contexts>
<marker>Buttcher, Clarke, Cormack, 2004</marker>
<rawString>Buttcher, S., C.L.A. Clarke, and G.V. Cormack. 2004. Domain-specific synonym expansion and validation for biomedical information retrieval (multitext experiments for trec 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buttcher</author>
</authors>
<title>The wumpus search engine.</title>
<date>2007</date>
<note>http://www.wumpus-search.org/.</note>
<contexts>
<context position="16300" citStr="Buttcher, 2007" startWordPosition="2639" endWordPosition="2640">d on Kullback-Leibler divergence) on sliding windows gives the best results in terms of TDRR. In this section, we aim to find out what the optimal passage retrieval approach is for a very different type of QA, namely why-QA. 5.1 Background of why-QA system development In (Verberne et al., 2008), we present an approach for why-QA that is based on paragraph retrieval from the INEX Wikipedia corpus (Denoyer and Gallinari, 2006). Our system for why-QA consists of two modules: a passage retrieval module and a re-ranking module. In earlier retrieval experiments, we used the Wumpus retrieval system (Buttcher, 2007), and we defined passages simply by the XML paragraph markup &lt;p&gt;. Passage ranking in Wumpus is done by the QAP passage scoring algorithm (Buttcher et al., 2004). The second module of our why-system is a reranking step that uses syntactic features of the question and the retrieved answers for adapting the scores of the answers and changing the ranking order. The weights of the re-ranking features have been optimized by training on our question answer data in five folds3 using a genetic algorithm. We let Wumpus retrieve and rank 150 paragraphs per 3In five turns, we tune the feature weights on f</context>
</contexts>
<marker>Buttcher, 2007</marker>
<rawString>Buttcher, S. 2007. The wumpus search engine. http://www.wumpus-search.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James P Callan</author>
</authors>
<title>Passage-level evidence in document retrieval.</title>
<date>1994</date>
<booktitle>In SIGIR,</booktitle>
<pages>302--310</pages>
<contexts>
<context position="2611" citStr="Callan, 1994" startWordPosition="402" endWordPosition="403">er will inevitably fail too (Monz, 2003). This motivates the need to study passage retrieval for QA. There are two common approaches to retrieving passages from a corpus: one is to index each passage as separate document and retrieve them as such. The other option is to first retrieve relevant documents for a given question and then retrieve passages from the retrieved documents. The passages themselves can vary in size and degree of overlap. Their size can be fixed as a number of words or characters, or varying with the semantic content (Hearst and Plaunt, 1993) or the structure of the text (Callan, 1994). The overlap between two adjacent passages can be either zero, in which case we speak of disjoint passages, or the passages may be overlapping, which we refer to as sliding passages. In this paper, we compare the effectiveness of several passage retrieval techniques with respect to their usefulness for QA. Our main interest is the contribution of sliding passages as apposed to disjoint passages, and we will experiment with a number of retrieval models. We evaluate the retrieval approaches on two different QA tasks: (1) factoidQA, as defined by the test collection provided by TREC (Voorhees, 2</context>
</contexts>
<marker>Callan, 1994</marker>
<rawString>Callan, James P. 1994. Passage-level evidence in document retrieval. In SIGIR, pages 302–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Denoyer</author>
<author>P Gallinari</author>
</authors>
<title>The Wikipedia XML corpus.</title>
<date>2006</date>
<journal>ACM SIGIR Forum,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="8474" citStr="Denoyer and Gallinari, 2006" startWordPosition="1336" endWordPosition="1340"> 2 does not support direct passage retrieval. For these experiments, therefore, we first need to segment documents into passages before indexing them into the 1Lemur toolkit: http://www.lemurproject.org 2Lemur and Indri are different search engines. Indri provides the #passage operator, but it doesn’t consider paragraph boundaries or sentence boundaries for constructing passages. 27 Lemur retrieval engine. Our segmenting strategy is explained below. 3.2 Passage identification For our experiments, we take into account two different corpora: AQUAINT and the Wikipedia XML corpus as used in INEX (Denoyer and Gallinari, 2006). The AQUAINT corpus consists of news articles from the Associated Press, New York Times, and Xinhua News Agency (English version) from 1996 to 2000. The Wikipedia XML collection consists of 659,388 articles as they occured in the online Wikipedia in the summer of 2006. As we have discussed in Section 2, (Tiedemann, 2007) discovered that discourse-based segmentation into paragraphs works well with standard information retrieval techniques. They also observe that larger retrieval units produce better results for passage retrieval, since larger units have higher chance to cover the required info</context>
<context position="16113" citStr="Denoyer and Gallinari, 2006" startWordPosition="2606" endWordPosition="2609">hat for TREC data, the choice of the retrieval model and the type of windows to be retrieved influence on the retrieval performance. We found that for the TREC data, a language modeling approach (based on Kullback-Leibler divergence) on sliding windows gives the best results in terms of TDRR. In this section, we aim to find out what the optimal passage retrieval approach is for a very different type of QA, namely why-QA. 5.1 Background of why-QA system development In (Verberne et al., 2008), we present an approach for why-QA that is based on paragraph retrieval from the INEX Wikipedia corpus (Denoyer and Gallinari, 2006). Our system for why-QA consists of two modules: a passage retrieval module and a re-ranking module. In earlier retrieval experiments, we used the Wumpus retrieval system (Buttcher, 2007), and we defined passages simply by the XML paragraph markup &lt;p&gt;. Passage ranking in Wumpus is done by the QAP passage scoring algorithm (Buttcher et al., 2004). The second module of our why-system is a reranking step that uses syntactic features of the question and the retrieved answers for adapting the scores of the answers and changing the ranking order. The weights of the re-ranking features have been opti</context>
</contexts>
<marker>Denoyer, Gallinari, 2006</marker>
<rawString>Denoyer, L. and P. Gallinari. 2006. The Wikipedia XML corpus. ACM SIGIR Forum, 40(1):64–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
<author>Christian Plaunt</author>
</authors>
<title>Subtopic structuring for full-length document access.</title>
<date>1993</date>
<booktitle>In ACM-SIGIR,</booktitle>
<pages>59--68</pages>
<contexts>
<context position="2567" citStr="Hearst and Plaunt, 1993" startWordPosition="392" endWordPosition="395">a question, further processing steps to extract an answer will inevitably fail too (Monz, 2003). This motivates the need to study passage retrieval for QA. There are two common approaches to retrieving passages from a corpus: one is to index each passage as separate document and retrieve them as such. The other option is to first retrieve relevant documents for a given question and then retrieve passages from the retrieved documents. The passages themselves can vary in size and degree of overlap. Their size can be fixed as a number of words or characters, or varying with the semantic content (Hearst and Plaunt, 1993) or the structure of the text (Callan, 1994). The overlap between two adjacent passages can be either zero, in which case we speak of disjoint passages, or the passages may be overlapping, which we refer to as sliding passages. In this paper, we compare the effectiveness of several passage retrieval techniques with respect to their usefulness for QA. Our main interest is the contribution of sliding passages as apposed to disjoint passages, and we will experiment with a number of retrieval models. We evaluate the retrieval approaches on two different QA tasks: (1) factoidQA, as defined by the t</context>
</contexts>
<marker>Hearst, Plaunt, 1993</marker>
<rawString>Hearst, Marti A. and Christian Plaunt. 1993. Subtopic structuring for full-length document access. In ACM-SIGIR, 1993, pages 59–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>R Gaizauskas</author>
</authors>
<title>Natural language question answering: the view from here.</title>
<date>2001</date>
<journal>Nat. Lang. Eng.,</journal>
<pages>275--300</pages>
<contexts>
<context position="1553" citStr="Hirschman and Gaizauskas, 2001" startWordPosition="233" endWordPosition="236">ediary between full documents and exact answers. They form a very natural unit of response for QA systems (Tellex et al., 2003) and it is known from user studies that users prefer answers to be embedded in paragraph-sized chunks (Lin et al., 2003) because they can provide the context of an answer. Therefore, almost all state-of-the-art QA systems implement some technique for extracting paragraph-sized fragments of text from a large corpus. Most QA systems have a pipeline architecture consisting of at least three components: question analysis, document/passage retrieval, and answer extraction (Hirschman and Gaizauskas, 2001; @ 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Voorhees, 2001). The quality of a QA system heavily depends on the effectiveness of the integrated retrieval system (second step of the pipeline): if a retrieval system fails to find any relevant documents for a question, further processing steps to extract an answer will inevitably fail too (Monz, 2003). This motivates the need to study passage retrieval for QA. There are two common approaches to retrieving passages</context>
</contexts>
<marker>Hirschman, Gaizauskas, 2001</marker>
<rawString>Hirschman, L. and R. Gaizauskas. 2001. Natural language question answering: the view from here. Nat. Lang. Eng., pages 275–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
<author>U Hermjakob</author>
<author>D Ravichandran</author>
</authors>
<title>A question/answer typology with surface text patterns.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology conference (HLT),</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="19549" citStr="Hovy et al., 2002" startWordPosition="3194" endWordPosition="3197">im to find out which type of retrieval model and what window type (disjoint or sliding) gives optimal results for retrieving passages relevant to why-questions. If the retrieval performance indeed goes up, we 4We didn’t need to assess the tail since we were only interested in the highest-ranked relevant answer for calculating MRR and success@n will apply our re-ranking module to the newly retrieved data to see what overall system performance we can reach with the new retrieval approach. 5.2 Data and evaluation setup For development and testing purposes, we use the Webclopedia question set by (Hovy et al., 2002). This set contains questions that were asked to the online QA system answers.com. 805 of these questions are why-questions. We manually inspect a sample of 400 of the Webclopedia whyquestions. Of these, 93 have an answer in the Wikipedia XML corpus (see section 3). Manual extraction of one correct answer for each of these questions results in a set of 93 why-questions and their reference answer. In order to be able to do fast evaluation of the different evaluation settings, we manually create an answer pattern for each of the questions in our set. These answer patterns are based on a set of 9</context>
</contexts>
<marker>Hovy, Hermjakob, Ravichandran, 2002</marker>
<rawString>Hovy, E.H., U. Hermjakob, and D. Ravichandran. 2002. A question/answer typology with surface text patterns. In Proceedings of the Human Language Technology conference (HLT), San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>C Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval. In</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR’01,</booktitle>
<pages>111--119</pages>
<contexts>
<context position="6899" citStr="Lafferty and Zhai, 2001" startWordPosition="1087" endWordPosition="1090">ze and degree of overlap. We set out to examine two different strategies of document segmentation (disjoint and sliding passages) with a number of retrieval models for two different QA tasks: TREC factoidQA and why-QA. 3.1 Retrieval models We use the Lemur retrieval engine1 for passage retrieval because it provides a flexible support for different types of retrieval models including vector space models and language models. In this paper we have selected two vector space models: TFIDF and Okapi BM25 (Robertson and Walker, 1999), and one language model based on Kullback-Leibler (KL) divergence (Lafferty and Zhai, 2001). The TFIDF weighting scheme is often used in information retrieval. There are several variations of the TFIDF weighting scheme that can effect the performance significantly. The Lemur toolkit provides a variant of the TFIDF model based on the Okapi TF formula (Robertson et al., 1995). Lemur also provides the implementation of the original Okapi BM25 model, and we have used this model with default values of 1.2 for k1, 0.75 for b and 7 for k3 as suggested by (Robertson and Walker, 1999). The KL-divergence retrieval model, which implements the cross entropy of the query model with respect to th</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>Lafferty, J. and C. Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In In Proceedings of SIGIR’01, pages 111–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Quan</author>
<author>V Sinha</author>
<author>K Bakshi</author>
<author>D Huynh</author>
<author>B Katz</author>
<author>D R Karger</author>
</authors>
<title>The role of context in question answering systems.</title>
<date>2003</date>
<booktitle>Conference on Human Factors in Computing Systems,</booktitle>
<pages>1006--1007</pages>
<contexts>
<context position="1170" citStr="Lin et al., 2003" startWordPosition="178" endWordPosition="181"> data sets, the TREC 2002–2003 QA data set, and 93 whyquestions against INEX Wikipedia. We discovered that, compared to disjoint windows, using sliding windows results in improved performance of TREC-QA in terms of TDRR, and in improved performance of why-QA in terms of success@n and MRR. 1 Introduction In question answering (QA), text passages are an important intermediary between full documents and exact answers. They form a very natural unit of response for QA systems (Tellex et al., 2003) and it is known from user studies that users prefer answers to be embedded in paragraph-sized chunks (Lin et al., 2003) because they can provide the context of an answer. Therefore, almost all state-of-the-art QA systems implement some technique for extracting paragraph-sized fragments of text from a large corpus. Most QA systems have a pipeline architecture consisting of at least three components: question analysis, document/passage retrieval, and answer extraction (Hirschman and Gaizauskas, 2001; @ 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Voorhees, 2001). The quality of a QA </context>
</contexts>
<marker>Lin, Quan, Sinha, Bakshi, Huynh, Katz, Karger, 2003</marker>
<rawString>Lin, J., D. Quan, V. Sinha, K. Bakshi, D. Huynh, B. Katz, and D.R. Karger. 2003. The role of context in question answering systems. Conference on Human Factors in Computing Systems, pages 1006– 1007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof Monz</author>
</authors>
<title>Document retrieval in the context of question answering.</title>
<date>2003</date>
<booktitle>In ECIR,</booktitle>
<pages>571--579</pages>
<contexts>
<context position="2038" citStr="Monz, 2003" startWordPosition="303" endWordPosition="304">t least three components: question analysis, document/passage retrieval, and answer extraction (Hirschman and Gaizauskas, 2001; @ 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Voorhees, 2001). The quality of a QA system heavily depends on the effectiveness of the integrated retrieval system (second step of the pipeline): if a retrieval system fails to find any relevant documents for a question, further processing steps to extract an answer will inevitably fail too (Monz, 2003). This motivates the need to study passage retrieval for QA. There are two common approaches to retrieving passages from a corpus: one is to index each passage as separate document and retrieve them as such. The other option is to first retrieve relevant documents for a given question and then retrieve passages from the retrieved documents. The passages themselves can vary in size and degree of overlap. Their size can be fixed as a number of words or characters, or varying with the semantic content (Hearst and Plaunt, 1993) or the structure of the text (Callan, 1994). The overlap between two a</context>
</contexts>
<marker>Monz, 2003</marker>
<rawString>Monz, Christof. 2003. Document retrieval in the context of question answering. In ECIR, pages 571–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Roberts</author>
<author>R Gaizauskas</author>
</authors>
<title>Evaluating passage retrieval approaches for question answering. In</title>
<date>2004</date>
<booktitle>In Proceedings of ECIR ,</booktitle>
<contexts>
<context position="4640" citStr="Roberts and Gaizauskas, 2004" startWordPosition="733" endWordPosition="736">mple, (Tellex et al., 2003) performed a quantitative evaluation of passage retrieval algorithms for QA. They compared different passage retrieval algorithms in the context of their QA system. Their system first returns a ranked list of 200 documents and then applies different passage retrieval algorithms to the retrieved documents. They find that the performance of passage retrieval depends on the performance of the pre-applied document retrieval step, and therefore they suggest that document and passage retrieval technology should be developed independently. A similar message is conveyed by (Roberts and Gaizauskas, 2004). They investigate different approaches to passage retrieval for QA. They identify each paragraph as a seperate passage. They find that the optimal approach is to allow multiple passages per document to be returned and to score passages independently of their source document. (Tiedemann, 2007) studies the impact of document segmentation approaches on the retrieval performance of IR for Dutch QA. He finds that segmentation based on document structure such as the use of paragraph markup (discourse-based segmentation) works well with standard information retrieval techniques. He tests various oth</context>
<context position="13918" citStr="Roberts and Gaizauskas, 2004" startWordPosition="2233" endWordPosition="2236">on 3.3. Table 1 shows the evaluation results (best scores for each measure in bold face). The experiment results show that language model based on Kullback-Leibler divergence shows better performance than two vector space models for both types of windows retrieval according to MRR, success@n and TDRR evaluation metrics. 4.1 Discussion In a pipeline QA system, the answer extraction module depends on the performance of passage retrieval. If more answer bearing passages are provided in the stream, then there is a high chance of selecting the correct answer from the stream in later stages of QA. (Roberts and Gaizauskas, 2004) have also discussed the importance of this aspect of passage retrieval for QA. They have measured the answer redundancy of a retrieval system which measures how many answer bearing passages are returned per question at limit n. (Tiedemann, 2007) have also used this metric and argue that high redundancy is desired to make it easier for the answer extraction module to spot possible answers. We consider TDRR as the most important measure for the passage retrieval task since it does not only measure the redundancy of a retrieval system but also measures how much improvement there is in returning </context>
</contexts>
<marker>Roberts, Gaizauskas, 2004</marker>
<rawString>Roberts, I. and R. Gaizauskas. 2004. Evaluating passage retrieval approaches for question answering. In In Proceedings of ECIR , 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
</authors>
<title>Okapi/keenbow at trec-8.</title>
<date>1999</date>
<booktitle>In Text Retrieval Conference.</booktitle>
<contexts>
<context position="6807" citStr="Robertson and Walker, 1999" startWordPosition="1074" endWordPosition="1077">efore, in our experiment setup, we have kept fixed the other segmentation variables, passage size and degree of overlap. We set out to examine two different strategies of document segmentation (disjoint and sliding passages) with a number of retrieval models for two different QA tasks: TREC factoidQA and why-QA. 3.1 Retrieval models We use the Lemur retrieval engine1 for passage retrieval because it provides a flexible support for different types of retrieval models including vector space models and language models. In this paper we have selected two vector space models: TFIDF and Okapi BM25 (Robertson and Walker, 1999), and one language model based on Kullback-Leibler (KL) divergence (Lafferty and Zhai, 2001). The TFIDF weighting scheme is often used in information retrieval. There are several variations of the TFIDF weighting scheme that can effect the performance significantly. The Lemur toolkit provides a variant of the TFIDF model based on the Okapi TF formula (Robertson et al., 1995). Lemur also provides the implementation of the original Okapi BM25 model, and we have used this model with default values of 1.2 for k1, 0.75 for b and 7 for k3 as suggested by (Robertson and Walker, 1999). The KL-divergen</context>
</contexts>
<marker>Robertson, Walker, 1999</marker>
<rawString>Robertson, Stephen E. and Steve Walker. 1999. Okapi/keenbow at trec-8. In Text Retrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
<author>Micheline Hancock-Beaulieu</author>
<author>M Gatford</author>
</authors>
<title>Okapi at trec-3.</title>
<date>1995</date>
<booktitle>In Text Retrieval Conference,</booktitle>
<pages>109--26</pages>
<contexts>
<context position="7184" citStr="Robertson et al., 1995" startWordPosition="1133" endWordPosition="1136"> retrieval because it provides a flexible support for different types of retrieval models including vector space models and language models. In this paper we have selected two vector space models: TFIDF and Okapi BM25 (Robertson and Walker, 1999), and one language model based on Kullback-Leibler (KL) divergence (Lafferty and Zhai, 2001). The TFIDF weighting scheme is often used in information retrieval. There are several variations of the TFIDF weighting scheme that can effect the performance significantly. The Lemur toolkit provides a variant of the TFIDF model based on the Okapi TF formula (Robertson et al., 1995). Lemur also provides the implementation of the original Okapi BM25 model, and we have used this model with default values of 1.2 for k1, 0.75 for b and 7 for k3 as suggested by (Robertson and Walker, 1999). The KL-divergence retrieval model, which implements the cross entropy of the query model with respect to the document model, is a standard metric for comparing distributions, which has proven to work well in IR experiments in the past. To address the data sparseness problem during model estimation, we use the Dirichlet smoothing method (Zhai and Lafferty, 2004) with default parameter value</context>
</contexts>
<marker>Robertson, Walker, Hancock-Beaulieu, Gatford, 1995</marker>
<rawString>Robertson, Stephen E., Steve Walker, Micheline Hancock-Beaulieu, and Gatford M. 1995. Okapi at trec-3. In Text Retrieval Conference, pages 109–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>B Katz</author>
<author>J Lin</author>
<author>A Fernandes</author>
<author>G Marton</author>
</authors>
<title>Quantitative evaluation ofpassage retrieval algorithms for question answering.</title>
<date>2003</date>
<booktitle>In In SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="1050" citStr="Tellex et al., 2003" startWordPosition="156" endWordPosition="159">tively compare the impact of passage retrieval for QA using sliding windows and disjoint windows. We consider two different data sets, the TREC 2002–2003 QA data set, and 93 whyquestions against INEX Wikipedia. We discovered that, compared to disjoint windows, using sliding windows results in improved performance of TREC-QA in terms of TDRR, and in improved performance of why-QA in terms of success@n and MRR. 1 Introduction In question answering (QA), text passages are an important intermediary between full documents and exact answers. They form a very natural unit of response for QA systems (Tellex et al., 2003) and it is known from user studies that users prefer answers to be embedded in paragraph-sized chunks (Lin et al., 2003) because they can provide the context of an answer. Therefore, almost all state-of-the-art QA systems implement some technique for extracting paragraph-sized fragments of text from a large corpus. Most QA systems have a pipeline architecture consisting of at least three components: question analysis, document/passage retrieval, and answer extraction (Hirschman and Gaizauskas, 2001; @ 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported </context>
<context position="4038" citStr="Tellex et al., 2003" startWordPosition="643" endWordPosition="646">work on passage retrieval for QA and we motivate what the main contribution of the current paper is. In 26 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 26–33 Manchester, UK. August 2008 section 3 we describe our general set-up for passage retrieval in both QA tasks that we consider. In section 4, we present the results of the experiments on TREC-QA data, and in section 5 we present our results on why-QA. Section 6 gives an overall conclusion. 2 Related work The use of passage retrieval for QA has been studied before. For example, (Tellex et al., 2003) performed a quantitative evaluation of passage retrieval algorithms for QA. They compared different passage retrieval algorithms in the context of their QA system. Their system first returns a ranked list of 200 documents and then applies different passage retrieval algorithms to the retrieved documents. They find that the performance of passage retrieval depends on the performance of the pre-applied document retrieval step, and therefore they suggest that document and passage retrieval technology should be developed independently. A similar message is conveyed by (Roberts and Gaizauskas, 200</context>
<context position="12571" citStr="Tellex et al., 2003" startWordPosition="2010" endWordPosition="2013">tical significance of the improvements. In summary, we experiment with three retrieval models in Lemur: TFIDF, Okapi, and a language model based on the Kullback-Leibler divergence. For each of these retrieval models, we evaluate the use of both sliding and disjoint passages. This makes a total of six retrieval settings. 4 Evaluating passage retrieval for TREC-QA As test collection for factoid QA, we use a standard set of 822 question/answer pairs from the TREC 28 QA tasks of 2002-2003. For evaluation of the passage retrieval approaches that we consider, we compute strict scores as defined by (Tellex et al., 2003). Strict scoring means that a retrieved passage is considered relevant if the passage not only matches one of the answer patterns provided by NIST, but its associated document is also listed as one of the relevant documents assessed by NIST. (Bilotti et al., 2004) have reviewed 109 factoid questions of the TREC-2002 task and they have extended the existing set of relevant documents by adding more relevant documents. We have also included this extended list of relevant documents for these questions in our experiment setup. We evaluate the impact of disjoint and sliding windows on passage retrie</context>
</contexts>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton. 2003. Quantitative evaluation ofpassage retrieval algorithms for question answering. In In SIGIR conference on Research and development in informaion retrieval, 2003, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Comparing document segmentation strategies for passage retrieval in question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP 07,</booktitle>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="4934" citStr="Tiedemann, 2007" startWordPosition="781" endWordPosition="782">o the retrieved documents. They find that the performance of passage retrieval depends on the performance of the pre-applied document retrieval step, and therefore they suggest that document and passage retrieval technology should be developed independently. A similar message is conveyed by (Roberts and Gaizauskas, 2004). They investigate different approaches to passage retrieval for QA. They identify each paragraph as a seperate passage. They find that the optimal approach is to allow multiple passages per document to be returned and to score passages independently of their source document. (Tiedemann, 2007) studies the impact of document segmentation approaches on the retrieval performance of IR for Dutch QA. He finds that segmentation based on document structure such as the use of paragraph markup (discourse-based segmentation) works well with standard information retrieval techniques. He tests various other techniques for document segmentation and various passage sizes. In his experimental setting, larger text units (such as documents) produce better performance in passage retrieval. Tiedemann compares different sizes of discourse-based segmentation: sentences, paragraphs and documents. He fin</context>
<context position="8797" citStr="Tiedemann, 2007" startWordPosition="1394" endWordPosition="1396"> sentence boundaries for constructing passages. 27 Lemur retrieval engine. Our segmenting strategy is explained below. 3.2 Passage identification For our experiments, we take into account two different corpora: AQUAINT and the Wikipedia XML corpus as used in INEX (Denoyer and Gallinari, 2006). The AQUAINT corpus consists of news articles from the Associated Press, New York Times, and Xinhua News Agency (English version) from 1996 to 2000. The Wikipedia XML collection consists of 659,388 articles as they occured in the online Wikipedia in the summer of 2006. As we have discussed in Section 2, (Tiedemann, 2007) discovered that discourse-based segmentation into paragraphs works well with standard information retrieval techniques. They also observe that larger retrieval units produce better results for passage retrieval, since larger units have higher chance to cover the required information. Therefore, we decide to segment each document into similar sized passages while taking into account complete paragraphs only. For document segmentation, our method first detects sentences in the text using punctuation marks as separators, and then paragraphs using empty lines as separators. Sentence boundaries ar</context>
<context position="14164" citStr="Tiedemann, 2007" startWordPosition="2276" endWordPosition="2278">ieval according to MRR, success@n and TDRR evaluation metrics. 4.1 Discussion In a pipeline QA system, the answer extraction module depends on the performance of passage retrieval. If more answer bearing passages are provided in the stream, then there is a high chance of selecting the correct answer from the stream in later stages of QA. (Roberts and Gaizauskas, 2004) have also discussed the importance of this aspect of passage retrieval for QA. They have measured the answer redundancy of a retrieval system which measures how many answer bearing passages are returned per question at limit n. (Tiedemann, 2007) have also used this metric and argue that high redundancy is desired to make it easier for the answer extraction module to spot possible answers. We consider TDRR as the most important measure for the passage retrieval task since it does not only measure the redundancy of a retrieval system but also measures how much improvement there is in returning the relevant passages at top ranks. According to TDRR@n in table 1, retrieval of sliding windows outperforms retrieval of disjoint windows at all limits of n for all retrieval models. For n = 100, the improvement is significant at p = 0.01 level.</context>
</contexts>
<marker>Tiedemann, 2007</marker>
<rawString>Tiedemann, J¨org. 2007. Comparing document segmentation strategies for passage retrieval in question answering. In Proceedings of RANLP 07, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzan Verberne</author>
<author>Lou Boves</author>
<author>Nelleke Oostdijk</author>
<author>Peter-Arno Coppen</author>
</authors>
<title>Using Syntactic Information for Improving Why-Question Answering.</title>
<date>2008</date>
<booktitle>In Proceedings of The 22nd International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="15980" citStr="Verberne et al., 2008" startWordPosition="2585" endWordPosition="2588"> and that relevant passages are surpressed down. 5 Evaluating passage retrieval for why-QA In the previous section, we showed that for TREC data, the choice of the retrieval model and the type of windows to be retrieved influence on the retrieval performance. We found that for the TREC data, a language modeling approach (based on Kullback-Leibler divergence) on sliding windows gives the best results in terms of TDRR. In this section, we aim to find out what the optimal passage retrieval approach is for a very different type of QA, namely why-QA. 5.1 Background of why-QA system development In (Verberne et al., 2008), we present an approach for why-QA that is based on paragraph retrieval from the INEX Wikipedia corpus (Denoyer and Gallinari, 2006). Our system for why-QA consists of two modules: a passage retrieval module and a re-ranking module. In earlier retrieval experiments, we used the Wumpus retrieval system (Buttcher, 2007), and we defined passages simply by the XML paragraph markup &lt;p&gt;. Passage ranking in Wumpus is done by the QAP passage scoring algorithm (Buttcher et al., 2004). The second module of our why-system is a reranking step that uses syntactic features of the question and the retrieved</context>
<context position="25023" citStr="Verberne et al., 2008" startWordPosition="4112" endWordPosition="4115">ing with the highest success@150 score: Lemur/TFIDF on sliding windows. For 81.7% of the questions in our set, Lemur/TFIDF retrieved an answer in the top-150. This means that the maximum success@10 score that we can obtain by re-ranking is 81.7%. For weighting the feature values, we re-use the weights that we had earlier found from training on our set of 93 questions and the 150 answers that were retrieved by Wumpus. We again take into account five-fold cross validation for evaluation. For a detailed description of our re-ranking module and the syntactic features that we exploit, we refer to (Verberne et al., 2008). The results from re-ranking are in Table 4. In the table, four system versions are compared: (1) the original Wumpus/QAP module, (2) the original why-pipeline system: Wumpus/QAP with re-ranking, (3) TFIDF-sliding and (4) the new 31 Table 3: Results for passage retrieval on why-questions against Wikipedia using disjoint windows (DW) and sliding windows (SW) Success@10 Success@150 MRR@150 Retrieval model DW SW DW SW DW SW Baseline: Wumpus/QAP 40.9% 72.0% 0.229 Lemur/TFIDF 43.0% 45.2% 71.1% 81.7% 0.247 0.338 Lemur/Okapi 41.9% 44.1% 67.7% 79.6% 0.243 0.320 Lemur/KL 48.9% 50.0% 72.8% 77.2% 0.263 </context>
</contexts>
<marker>Verberne, Boves, Oostdijk, Coppen, 2008</marker>
<rawString>Verberne, Suzan, Lou Boves, Nelleke Oostdijk, and Peter-Arno Coppen. 2008. Using Syntactic Information for Improving Why-Question Answering. In Proceedings of The 22nd International Conference on Computational Linguistics (COLING 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<title>Overview of trec</title>
<date>2001</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="1748" citStr="Voorhees, 2001" startWordPosition="254" endWordPosition="255">graph-sized chunks (Lin et al., 2003) because they can provide the context of an answer. Therefore, almost all state-of-the-art QA systems implement some technique for extracting paragraph-sized fragments of text from a large corpus. Most QA systems have a pipeline architecture consisting of at least three components: question analysis, document/passage retrieval, and answer extraction (Hirschman and Gaizauskas, 2001; @ 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Voorhees, 2001). The quality of a QA system heavily depends on the effectiveness of the integrated retrieval system (second step of the pipeline): if a retrieval system fails to find any relevant documents for a question, further processing steps to extract an answer will inevitably fail too (Monz, 2003). This motivates the need to study passage retrieval for QA. There are two common approaches to retrieving passages from a corpus: one is to index each passage as separate document and retrieve them as such. The other option is to first retrieve relevant documents for a given question and then retrieve passag</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Voorhees, Ellen. 2001. Overview of trec 2001 question answering track. In In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<title>Overview of trec 2002 question answering track. In</title>
<date>2002</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="3214" citStr="Voorhees, 2002" startWordPosition="502" endWordPosition="503">llan, 1994). The overlap between two adjacent passages can be either zero, in which case we speak of disjoint passages, or the passages may be overlapping, which we refer to as sliding passages. In this paper, we compare the effectiveness of several passage retrieval techniques with respect to their usefulness for QA. Our main interest is the contribution of sliding passages as apposed to disjoint passages, and we will experiment with a number of retrieval models. We evaluate the retrieval approaches on two different QA tasks: (1) factoidQA, as defined by the test collection provided by TREC (Voorhees, 2002; Voorhees, 2003), and (2) a relatively new problem in the QA field: that of answering why-questions (why-QA). The remainder of the paper is organized as follows. In the next section, we describe related work on passage retrieval for QA and we motivate what the main contribution of the current paper is. In 26 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 26–33 Manchester, UK. August 2008 section 3 we describe our general set-up for passage retrieval in both QA tasks that we consider. In section 4, we present the results of the exper</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>Voorhees, Ellen. 2002. Overview of trec 2002 question answering track. In In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<title>Overview of trec 2003 question answering track. In</title>
<date>2003</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="3231" citStr="Voorhees, 2003" startWordPosition="504" endWordPosition="505"> overlap between two adjacent passages can be either zero, in which case we speak of disjoint passages, or the passages may be overlapping, which we refer to as sliding passages. In this paper, we compare the effectiveness of several passage retrieval techniques with respect to their usefulness for QA. Our main interest is the contribution of sliding passages as apposed to disjoint passages, and we will experiment with a number of retrieval models. We evaluate the retrieval approaches on two different QA tasks: (1) factoidQA, as defined by the test collection provided by TREC (Voorhees, 2002; Voorhees, 2003), and (2) a relatively new problem in the QA field: that of answering why-questions (why-QA). The remainder of the paper is organized as follows. In the next section, we describe related work on passage retrieval for QA and we motivate what the main contribution of the current paper is. In 26 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 26–33 Manchester, UK. August 2008 section 3 we describe our general set-up for passage retrieval in both QA tasks that we consider. In section 4, we present the results of the experiments on TREC-QA</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Voorhees, Ellen. 2003. Overview of trec 2003 question answering track. In In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
<author>John D Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to information retrieval.</title>
<date>2004</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<pages>179--214</pages>
<contexts>
<context position="7755" citStr="Zhai and Lafferty, 2004" startWordPosition="1229" endWordPosition="1232">ased on the Okapi TF formula (Robertson et al., 1995). Lemur also provides the implementation of the original Okapi BM25 model, and we have used this model with default values of 1.2 for k1, 0.75 for b and 7 for k3 as suggested by (Robertson and Walker, 1999). The KL-divergence retrieval model, which implements the cross entropy of the query model with respect to the document model, is a standard metric for comparing distributions, which has proven to work well in IR experiments in the past. To address the data sparseness problem during model estimation, we use the Dirichlet smoothing method (Zhai and Lafferty, 2004) with default parameter values provided in the Lemur toolkit. Currently, however, the Lemur 2 does not support direct passage retrieval. For these experiments, therefore, we first need to segment documents into passages before indexing them into the 1Lemur toolkit: http://www.lemurproject.org 2Lemur and Indri are different search engines. Indri provides the #passage operator, but it doesn’t consider paragraph boundaries or sentence boundaries for constructing passages. 27 Lemur retrieval engine. Our segmenting strategy is explained below. 3.2 Passage identification For our experiments, we take</context>
</contexts>
<marker>Zhai, Lafferty, 2004</marker>
<rawString>Zhai, ChengXiang and John D. Lafferty. 2004. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., pages 179–214.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>