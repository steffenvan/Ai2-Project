<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.851213">
A LOGICAL VERSION OF FUNCTIONAL GRAMMAR
</title>
<author confidence="0.916088">
William C. Rounds
</author>
<affiliation confidence="0.7267356">
University of Michigan
Xerox PARC
Alexis Manaster-Ramer
IBM T.J. Watson Research Center
Wayne State University
</affiliation>
<sectionHeader confidence="0.984383" genericHeader="abstract">
1 Abstract
</sectionHeader>
<bodyText confidence="0.9998689">
Kay&apos;s functional-unification grammar notation [5] is
a way of expressing grammars which relies on very few
primitive notions. The primary syntactic structure is the
feature structure, which can be visualised as a directed
graph with arcs labeled by attributes of a constituent, and
the primary structure-building operation is unification.
In this paper we propose a mathematical formulation of
FUG, using logic to give a precise account of the strings
and the structures defined by any grammar written in
this notation.
</bodyText>
<sectionHeader confidence="0.998179" genericHeader="keywords">
2 Introduction
</sectionHeader>
<bodyText confidence="0.999149428571428">
Our basic approach to the problem of syntactic de-
scription is to use logical formulas to put conditions or
constraints on ordering of constituents, ancestor and de-
scendant relations, and feature attribute information in
syntactic structures. The present version of our logic
has predicates specifically designed for these purposes.
A grammar can be considered as just a logical formula,
and the structures satisfying the formula are the syntactic
structures for the sentences of the language. This notion
goes back to DCG&apos;s [6], but our formulation is quite dif-
ferent. In particular, it builds on the logic of Kasper and
Rounds [3], a logic intended specifically to describe fea-
ture structures.
The formulation has several new aspects. First, it
introduces the oriented feature structure as the primary
syntactic structure. One can think of these structures
as parse trees superimposed on directed graphs, although
the general definition allows much more flexibility. In
fact, our notation does away with the parse tree alto-
gether.
A second aspect of the notation is its treatment of
word order. Our logic allows small grammars to define
free-word order languages over large vocabularies in a way
not possible with standard ID/LP rules. It is not clear
whether or not this treatment of word order was intended
by Kay, but the issue naturally arose during the process
of making this model precise. (Joshi [I] has adopted much
the same conventions in tree adjunct grammar.)
A third aspect of our treatment is the use of fixed-
point formulas to introduce recursion into grammars. This
idea is implicit in DCG&apos;s, and has been made explicit in
the logics CLFP and ILFP [9]. We give a simple way of
expressing the semantics of these formulas which corre-
sponds closely to the usual notion of grammatical deriva-
tions. There is an interesting use of type variables to
describe syntactic categories and/or constructions.
We illustrate the power of the notation by sketching
how the constructions of relational grammar [7] can be
formulated in the logic. To our knowledge, this is the
first attempt to interpret the relational ideas in a fully
mathematical framework. Although relational networks
themselves have been precisely specified, there does not
seem to be a precise statement of how relational deriva-
tions take place. We do not claim that our formalization
is the one intended by Postal and Perlmutter, but we
do claim that our notation shows clearly the relationship
of relational to transformational grammars on one hand,
and to lexical-functional grammars on the other.
Finally, we prove that the satisfiability problem for our
logic is undecidable. This should perhaps be an expected
result, because the proof relies on simulating Turing ma-
chine computations in a grammar, and follows the stan-
dard undecidability arguments. The satisfiability prob-
lem is not quite the same problem as the universal recog-
nition problem, however, and with mild conditions on
derivations similar to those proposed for LFG [2], the
latter problem should become decidable.
We must leave efficiency questions unexamined in this
paper. The notation has not been implemented. We view
this notation as a temporary one, and anticipate that
many revisions and extensions will be necessary if it is to
be implemented at all. Of course, FUG itself could be
considered as an implementation, but we have added the
word order relations to our logic, which are not explicit
in FUG.
In this paper, which is not full because of space limi-
tations, we will give definitions and examples in Section
3; then will sketch the relational application in Section 4,
and will conclude with the undecidability result and some
final remarks.
</bodyText>
<sectionHeader confidence="0.996216" genericHeader="introduction">
3 Definitions and examples
</sectionHeader>
<subsectionHeader confidence="0.997988">
3.1 Oriented f-structures
</subsectionHeader>
<bodyText confidence="0.999631">
In this section we will describe the syntactic structures
to which our logical formulas refer. The next subsection
</bodyText>
<page confidence="0.998186">
89
</page>
<figure confidence="0.9961995">
Oleet ch.
â€¢
</figure>
<figureCaption confidence="0.999964">
Figure 1: A typical DG.
</figureCaption>
<bodyText confidence="0.999927357142857">
will give the logic itself. Our intent is to represent not
only feature information, but also information about or-
dering of constituents in a single structure. We begin with
the unordered version, which is the simple DG (directed
graph) structure commonly used for non-disjunctive in-
formation. This is formalized as an acyclic finite automa-
ton, in the manner of Kasper-Rounds [3]. Then we add
two relations on nodes of the DG: ancestor and linear
precedence. The key insight about these relations is that
they are partial; nodes of the graph need not participate
in either of the two relations. Pure feature information
about a constituent need not participate in any ordering.
This allows us to model the &amp;quot;cset&amp;quot; and &amp;quot;pattern&amp;quot; infor-
mation of FUG, while allowing structure sharing in the
usual DG representation of features.
We are basically interested in describing structures
like that shown in Figure 1.
A formalism appropriate for specifying such DG struc-
tures is that of finite automata theory. A labeled DG can
be regarded as a transition graph for a partially speci-
fied deterministic finite automaton. We will thus use the
ordinary b notation for the transition function of the au-
tomaton. Nodes of the graph correspond to states of the
automaton, and the notation b(q, x) implies that starting
at state(node) q a transition path actually exists in the
graph labeled by the sequence x, to the state b(q, x).
Let L be a set of arc labels, and A be a set of atomic
feature values. An (A, L)- automaton is a tuple
</bodyText>
<equation confidence="0.604591">
A = (Q go, r)
</equation>
<bodyText confidence="0.999341">
where Q is a finite set of states, qo is the initial state, L is
the set of labels above, b is a partial function from Q x L to
Q, and T LS a partial function from terminating states of A
to A. (q is terminating if b(q,1) is undefined for all 1 E L.)
We require that A be connected and acyclic. The map r
specifies the atomic feature values at the final nodes of the
DG. (Some of these nodes can have unspecified values, to
be unified in later. This is why r is only partial.) Let F be
the set of terminating states of A, and let P(A) be the set
of full paths of A, namely the set {x E L : b(q0,x) E F}.
Now we add the constituent ordering information to
</bodyText>
<equation confidence="0.289307">
en/
</equation>
<figureCaption confidence="0.993065">
Figure 2: An oriented f-structure for a4b4c4.
</figureCaption>
<bodyText confidence="0.980847538461538">
the nodes of the transition graph. Let E be the termi-
nal vocabulary (the set of all possible words, morphemes,
etc.) Now r can be a partial map from Q to EU A, with
the requirement that if r(q) E A, then q E F. Next,
let a and &lt; be binary relations on Q, the ancestor and
precedence relations. We require a to be reflexive, an-
tisymmetric and transitive; and the relation &lt; must be
irreflexive and transitive. There is no requirement that
any two nodes must be related by one or the other of these
relations. There is, however, a compatibility constraint
between the two relations:
V(q, r,s,t) E Q : (q &lt; r) A (q s) A (r a t) s &lt; t.
Note: We have required that the precedence and dom-
inance relations be transitive. This is not a necessary
requirement, and is only for elegance in stating condi-
tions like the compatibility constraint. A better formula-
tion of precedence for computational purposes would be
the &amp;quot;immediate precedence&amp;quot; relation, which says that one
constituent precedes another, with no constituents inter-
vening. There is no obstacle to having such a relation in
the logic directly.
Example. Consider the structure in Figure 2. This
graph represents an oriented f-structure arising from a
LFG-style grammar for the language {an b&amp;quot; cn I n&gt; 1}.
In this example, there is an underlying CFG given by
the following productions:
</bodyText>
<equation confidence="0.981966666666667">
S TC
T aTb lab
C cC c.
</equation>
<bodyText confidence="0.999977428571429">
The arcs labeled with numbers (1,2,3) are analogous
to arcs in the derivation tree of this grammar. The root
node is of &amp;quot;category&amp;quot; S, although we have not represented
this information in the structure. The nodes at the ends
of the arcs 1,2, and 3 are ordered left to right; in our
logic this will be expressed by the formula 1 &lt; 2 &lt; 3.
The other arcs, labeled by COUNT and #, are feature
</bodyText>
<page confidence="0.988819">
90
</page>
<bodyText confidence="0.999678">
arcs used to enforce the counting information required by
the language. It is a little difficult in the graph repre-
sentation to indicate the node ordering information and
the ancestor information, so this will wait until the next
section. Incidentally, no claim is made for the linguistic
naturalness of this example!
</bodyText>
<subsectionHeader confidence="0.999658">
3.2 A presentation of the logic
</subsectionHeader>
<bodyText confidence="0.999991296296296">
We will introduce the logic by continuing the exam-
ple of the previous section. Consider Figure 2. Particu-
lar nodes of this structure will be referenced by the se-
quences of arc labels necessary to reach them from the
root node. These sequences will be called paths. Thus
the path 1 2 2 2 3 leads to an occurrence of the terminal
symbol b. Then a formula of the form, say, 1 2 COUNT =
2 2 COUNT would indicate that these paths lead to the
same node. This is also how we specify linear precedence:
the last b precedes the first c, and this could be indicated
by the formula 1 2 2 2 3 &lt; 2 2 2 2 1.
It should already be clear that our formulas will de-
scribe oriented f-structures. We have just illustrated two
kinds of atomic formula in the logic. Compound formulas
will be formed using A (and), and V (or). Additionally,
let I be an arc label. Then an f-structure will satisfy a for-
mula of the form 1: (1), if there is an 1-transition from the
root node to the root of a substructure satisfying 0. What
we have not explained yet is how the recursive informa-
tion implicit in the CFG is expressed in our logic. To do
this, we introduce type variables as elementary formulas
of the logic. In the example, these are the &amp;quot;category&amp;quot;
variables S, T, and C. The grammar is given as a system
of equations (more properly, equivalences), relating these
variables.
We can now present a logical formula which describes
the language of the previous section.
</bodyText>
<equation confidence="0.903684125">
S where 1:TA2:CA( 1 count = 2 count)
S A(1 &lt; 2) A 012
C (1 :CA2:CA( count # = 2 count) A 012)
V (1 : C A (count # = end) A 01)
T (1 :0A2:TA3: bA ( count # = 2 count)
V A (1 &lt; 2) A (2 &lt; 3) A 0123)
(1 :aA2:b
A (count # = end) A(1 &lt;2) A 012),
</equation>
<bodyText confidence="0.977436136363636">
where 4612 is the formula (c a 1) A (e a 2), in which e is
the path of length 0 referring to the initial node of the
f-structure, and where the other 0 formulas are similarly
defined. (The 0 formulas give the required dominance
information.)
In this example, the set L = {1, 2, 3, #, count}, the set
E = {a, 6,c}, and the set A = {end}. Thus the atomic
symbol &amp;quot;end&amp;quot; does not appear as part of any derived
string. It is easy to see how the structure in Figure 2
satisfies this formula. The whole structure must satisfy
the formula S, which is given recursively. Thus the sub-
structure at the end of the 1 arc from the root must satisfy
the clause for T, and so forth.
It should now be clearer why we consider our logic a
logic for functional grammar. Consider the FUG descrip-
tion in Figure 3.
According to [5, page 149], this descrilition specifies
sentences, verbs, or noun phrases. Let us call such struc-
tures &amp;quot;entities&amp;quot;, and give a partial translation of this de-
scription into our logic. Create the type variables ENT,
S, VERB, and NP. Consider the recursive formula
ENT where
</bodyText>
<equation confidence="0.9903896">
ENT S V NP V VERB
subj : NP A pred : VERB
A(subj &lt; pred)
A((scomp : none)V (scomp : S
A(pred &lt; scomp)))
</equation>
<bodyText confidence="0.999968928571428">
Notice that the category names can be represented as
type variables, and that the categories NP and VERB
are free type variables. Given an assignment of a set of
f-structures to these type variables, the type ENT will
become well-specified.
A few other points need to be made concerning this
example. First, our formula does not have any ancestor
information in it, so the dominance relations implicit in
Kay&apos;s patterns are not represented. Second, our word or-
der conventions are not the same as Kay&apos;s. For example,
in the pattern (subj pred â€¢ â€¢ .), it is required that the sub-
ject be the very first constituent in the sentence, and that
nothing intervene between the subject and predicate. To
model this we would need to add the &amp;quot;immediately left or
predicate, because our &lt; predicate is transitive, and does
not require this property. Next, Kay uses &amp;quot;CAT&amp;quot; arcs to
represent category information, and considers &amp;quot;NP&amp;quot; to be
an atomic value. It would be possible to do this in our
logic as well, and this would perhaps not allow NPs to be
unified with VERBs. However, the type variables would
still be needed, because they are essential for specifying
recursion. Finally, FUG has other devices for special pur-
poses. One is the use of nonlocal paths, which are used
at inner levels of description to refer to features of the
&amp;quot;root node&amp;quot; of a DG. Our logic will not treat these, be-
cause in combination with recursion, the description of
the semantics is quite complicated. The full version of
the paper will have the complete semantics.
</bodyText>
<page confidence="0.913475">
91
</page>
<equation confidence="0.996469555555555">
cat = S
pattern = (sub j pred â€¢ â€¢ .)
subj = [ cat = NP
pred = [ cat = VERB ]
scornp = none ]
pattern = (â€¢ â€¢ â€¢ scamp)
scomp = [ cat = S ]
cat = NP]
cat = VERB]
</equation>
<figureCaption confidence="0.997312">
Figure 3: Disjunctive specification in FUG.
</figureCaption>
<subsectionHeader confidence="0.7853325">
3.3 The formalism
3.3.1 Syntax
</subsectionHeader>
<bodyText confidence="0.998509">
We summarize the formal syntax of our logic. We
postulate a set A of atomic feature names, a set L of
attribute labels, and a set E of terminal symbols (word
entries in a lexicon.) The type variables come from a
set TVAR = {X0, Xi, ...}. The following list gives the
syntactical constructions. All but the last four items are
atomic formulas.
</bodyText>
<listItem confidence="0.999292666666667">
1. NIL
2. TOP
3. X, in which X E TVAR
4. a, in which a E A
5. o, in which a E E
6. z &lt; y, in which x and y E L.
7. x a y, in which x and y E L&apos;
8. [xi, , xn], in which each xi E LÂ°
9. 1:
</listItem>
<bodyText confidence="0.931708">
10. 0 A&apos;
11. 0 v
12. 0 where [X1 ::= 951; ... X, ::= On]
Items (1) and (2) are the identically true and false
formulas, respectively. Item (8) is the way we officially
represent path equations. We could as well have used
equations like x = y, where x and y E L&apos; , but our defi-
nition lets us assert the simultaneous equality of a finite
number of paths without writing out all the pairwise path
equations. Finally, the last item (12) is the way to express
recursion. It will be explained in the next subsection.
Notice, however, that the keyword where is part of the
syntax.
</bodyText>
<subsectionHeader confidence="0.547432">
3.3.2 Semantics
</subsectionHeader>
<bodyText confidence="0.999665">
The semantics is given with a standard Tarski defini-
tion based on the inductive structure of wffs. Formulae
are satisfied by pairs (A, p), where A is an oriented f-
structure and p is a mapping from type variables to sets
off-structures, called an environment. This is needed be-
cause free type variables can occur in formulas. Here are
the official clauses in the semantics:
</bodyText>
<listItem confidence="0.959571736842106">
1. (A, p) NIL always;
2. (A, p) TOP never;
3. (A, p) X iff A E p(X);
4. (A, p) a iff r(q0) = a, where qo is the initial state
of A;
5. (A, p) a, where a E E, if r(q0) = a;
6. (A, p) v &lt; w if 6(q0, v) &lt; 6(q0,w);
7. (A, p) v a w if b(q0,v) c b(q0,w);
8. (A, p) [xl, .. .,znj if Vi, j : 6(q0, xi) =
9. (A,p) 1 : iff (All,p) 0, where All is the
automaton A started at 5(q0,1);
10. (A. 19) A TP if (A, p) HO and (A, 1)) TP;
11. (A, p) HP./ 0 similarly;
12. (A, p) where (X1 ::= 01; ::= On) if
for some k, (A, p(k)) 0, where p(k) is defined
inductively as follows:
â€¢ p(o)(xi) = 0;
â€¢ P(k+1)(X1) = 1B I (B, p) Oil
and where p(k)(X) = p(X) if X Xi for any i.
</listItem>
<bodyText confidence="0.9995354">
We need to explain the semantics of recursion. Our
semantics has two presentations. The above definition is
shorter to state, but it is not as intuitive as a syntactic,
operational definition. In fact, our notation
where [..Y1 ::= 01, â€¢ â€¢ â€¢ , Xn ::= On]
</bodyText>
<page confidence="0.99081">
92
</page>
<bodyText confidence="0.997095384615385">
is meant to suggest that the Xs can be replaced by the Os
in 0. Of course, the Os may contain free occurrences of
certain X variables, so we need to do this same replace-
ment process in the system of Os beforehand. It turns
out that the replacement process is the same as the pro-
cess of carrying out grammatical derivations, but making
replacements of nonterminal symbols all at once.
With this idea in mind, we can turn to the definition
of replacement. Here is another advantage of our logic â€”
replacement is nothing more than substitution of formu-
las for type variables. Thus, if a formula 8 has distinct
free type variables in the set D = {Xi, , Xâ€ž}, and
On â€¢ â€¢ â€¢ On are formulas, then the notation
</bodyText>
<equation confidence="0.833382">
61[Xi : Xj E I)]
</equation>
<bodyText confidence="0.990964428571429">
denotes the simultaneous replacement of any free occur-
rences of the Xi in 0 with the formula 0j, taking care
to avoid variable clashes in the usual way (ordinarily this
will not be a problem.)
Now consider the formula
where [Xi ::= 01; . â€¢ â€¢ Xn
The semantics of this can be explained as follows. Let
</bodyText>
<equation confidence="0.9970508">
D = {X1, ,X,,}, and for each k &gt; 0 define a set of
formulas {01k) I 1 &lt; i &lt; n}. This is done inductively on
k:
d(0) = 01[X TOP : X E D];
e+1) = (ki[X ckk) : X E D].
</equation>
<bodyText confidence="0.982499625">
These formulas, which can be calculated iteratively, cor-
respond to the derivation process.
Next, we consider the formula ;b. In most grammars,
0 will just be a &amp;quot;distinguished&amp;quot; type variable, say S. If
(A, p) is a pair consisting of an automaton and an envi-
ronment, then we define
(A, p) where [X1 ::= 01; â€¢ â€¢ â€¢ Xn ::= On]
if for some k,
</bodyText>
<equation confidence="0.362161333333333">
(A, p) 7I1X; d.k) : X, E
Example. Consider the formula (derived from a reg-
ular grammar)
S where
S (1:aA2:S)V(1:bA2:T)Vc
::= (1:6A2:5&apos;)V(1:aA2:T)Vd.
</equation>
<bodyText confidence="0.9998025">
Then, using the above substitutions, and simplifying ac-
cording to the laws of Kasper-Rounds, we have
</bodyText>
<equation confidence="0.998385125">
0(s0) = C,
,(0)
OT d â€¢
41) (1:aA2:0V(1:bA2:d)Vc;
(1:bA2:0V(1:aA2:d)Vd;
(s2) 1:aA2:(1:aA2:0V(1:6A2:d)Vc)
1:bA2:((1:6.A2:c)V(1:aA2:d)Vd)
Vc.
</equation>
<bodyText confidence="0.999915333333333">
The f-structures defined by the successive formulas for S
correspond in a natural way to the derivation trees of the
grammar underlying the example.
Next, we need to relate the official semantics to the
derivational semantics just explained. This is done with
the help of the following lemmas.
</bodyText>
<construct confidence="0.80362825">
Lemma 1 (A, p) (t)k) (A, p(k))
Lemma 2 (A, p) 0[X1 : Xi E iff (A, If)
0, where p* (Xi) = {5 I (13,p) j}, if X E D, and
otherwise is p(X).
</construct>
<bodyText confidence="0.936475769230769">
The proofs are omitted.
Finally, we must explain the notion of the language
defined by 0, where 0 is a logical formula. Suppose for
simplicity that 0 has no free type variables. Then the
notion A 0 makes sense, and we say that a string
w E L(0) if for some subsumption-minimal f-structure
A, A 0, and w is compatible with A. The notion
of subsumption is explained in [8]. Briefly, we have the
following definition.
Let A and 5 be two automata. We say A C B (A
subsumes B; B extends A) if there is a homomorphism
from A to B; that is, a map h :QA QB such that (for
all existing transitions)
</bodyText>
<listItem confidence="0.898208333333333">
1. h(64q,1)) =
2. r(h(q)) = r(q) for all q such that r(q) E A;
3. h(q0,) =
</listItem>
<bodyText confidence="0.999958294117647">
It can be shown that subsumption is a partial order on
isomorphism classes of automata (without orderings), and
that for any formula 0 without recursion or ordering, that
there are a finite number of subsumption-minimal au-
tomata satisfying it. We consider as candidate structures
for the language defined by a formula, only automata
which are minimal in this sense. The reason we do this
is to exclude f-structures which contain terminal symbols
not mentioned in a formula. For example, the formula
NIL is satisfied by any &amp;structure, but only the mini-
mal one, the one-node automaton, should be the principal
structure defined by this formula.
By compatibility we mean the following. In an f-
structure A, restrict the ordering &lt; to the terminal sym-
bols of A. This ordering need not be total; it may in fact
be empty. If there is an extension of this partial order on
the terminal nodes to a total order such that the labeling
</bodyText>
<page confidence="0.997221">
93
</page>
<bodyText confidence="0.9936208">
symbols agree with the symbols labeling the positions of
w, then w is compatible with A.
This is our new way of dealing with free word order.
Suppose that no precedence relations are specified in a
formula. Then, minimal satisfying f-structures will have
an empty &lt;- relation. This implies that any permutation
of the terminal symbols in such a structure will be al-
lowed. Many other ways of defining word order can also
be expressed in this logic, which enjoys an advantage over
ID/LP rules in this respect.
</bodyText>
<sectionHeader confidence="0.891491" genericHeader="method">
4 Modeling Relational Grammar
</sectionHeader>
<bodyText confidence="0.960224703703704">
Consider the relational analyses in Figures 4 and 5.
These analyses, taken from [7], have much in common
with functional analyses and also with transsformational
ones. The present pair of networks illustrates a kind of
raising construction common in the relational literature.
In Figure 4, there are arc labels P, 1, and 2, representing
&amp;quot;predicate&amp;quot;, &amp;quot;subject&amp;quot;, and &amp;quot;object&amp;quot; relations. The &amp;quot;cl&amp;quot;
indicates that this analysis is at the first linguistic stra-
tum, roughly like a transformational cycle. In Figure 5,
we learn that at the second stratum, the predicate (&amp;quot;be-
lieved&amp;quot;) is the same as at stratum 1, as is the subject.
However, the object at level 2 is now &amp;quot;John&amp;quot;, and the
phrase &amp;quot;John killed the farmer&amp;quot; has become a &amp;quot;chomeur&amp;quot;
for level 2.
The relational network is almost itself a feature struc-
ture. To make it one, we employ the trick of introducing
an arc labeled with 1, standing for &amp;quot;previous level&amp;quot;. The
conditions relating the two levels can easily be stated as
path equations, as in Figure 6.
The dotted lines in Figure 6 indicate that the nodes
they connect are actually identical. We can now indicate
precisely other information which might be specified in
a relational grammar, such as the ordering information
1 &lt; P &lt; 2. This would apply to the &amp;quot;top level&amp;quot;, which
for Perlmutter and Postal would be the &amp;quot;final level&amp;quot;, or
surface level. A recursive specification would also become
possible: thus
</bodyText>
<equation confidence="0.940385">
SENT ::= CLAUSE A (1 &lt; P&lt; 2)
CLAUSE ::= 1 : NOM A P :V ERB
A 2: (CLAUSE V NOM)
A (RAISE V PASSIVE V ...)
A 1: CLAUSE
RAISE ::= 1: 2 : CLAUSE A (equations in (6))
</equation>
<bodyText confidence="0.99683925">
This is obviously an incomplete grammar, but we think
it possible to use this notation to give a complete specifi-
cation of an RG and, perhaps at some stage, a computa-
tional test.
</bodyText>
<sectionHeader confidence="0.995418" genericHeader="method">
5 Undecidability
</sectionHeader>
<bodyText confidence="0.999296833333333">
In this section we show that the problem of satisfia-
bility â€” given a formula, decide if there is an f-structure
satisfying it â€” is undecidable. We do this by building a for-
mula which describes the computations of a given Turing
machine. In fact, we show how to speak about the com-
putations of an automaton with one stack (a pushdown
automaton.) This is done for convenience; although the
halting problem for one-stack automata is decidable, it
will be clear from the construction that the computation
of a two-stack machine could be simulated as well. This
model is equivalent to a Turing machine â€” one stack rep-
resents the tape contents to the left of the TM head, and
the other, the tape contents to the right. We need not
simulate moves which read input, because we imagine the
TM started with blank tape. The halting problem for
such machines is still undecidable.
We make the following conventions about our PDA.
Moves are of two kinds:
</bodyText>
<listItem confidence="0.9994285">
â€¢ qi : push b; go to qi ;
â€¢ qi : pop stack; if a go to qi else go to cut.
</listItem>
<bodyText confidence="0.999727428571429">
The machine has a two-character stack alphabet {a, b}.
(In the push instruction, of course pushing &amp;quot;a&amp;quot; is allowed.)
If the machine attempts to pop an empty stack, it can-
not continue. There is one final state qt. The machine
halts sucessfully in this and only this state. We reduce
the halting problem for this machine to the satisfiability
problem for our logic.
</bodyText>
<sectionHeader confidence="0.772266" genericHeader="method">
Atoms: &amp;quot;none&amp;quot; --- bookkeeping marker
</sectionHeader>
<bodyText confidence="0.9480193">
for telling what
is in the stack
q0, ql qn --- one for
each state
Labels: a, b for describing
stack contents
s -- pointer to top of stack
next --- value of next state
--- pointer to previous
stack configuration
</bodyText>
<sectionHeader confidence="0.8063855" genericHeader="method">
Type variables:
COIF -- structure represents
a machine configuration
SNIT, FINAL --configurations
</sectionHeader>
<bodyText confidence="0.924379285714286">
at start and finish
00,...,QN: property of being
in one of these states
The simulation proceeds as in the relational grammar
example. Each configuration of the stack corresponds to
a level in an RG derivation. Initially, the stack is empty.
Thus we put
</bodyText>
<page confidence="0.993974">
94
</page>
<figure confidence="0.8987245">
Cs
ill; I lit4
</figure>
<figureCaption confidence="0.860833">
Figure 4: Network for The woman believed that John killed the farmer.
Figure 5: Network for The woman believed John to have killed the farmer.
ir
</figureCaption>
<figure confidence="0.997884166666667">
.10
MD
p =
1 =
2 =
ChoP =
Cho2 =
1 p
11
121
12P
122
</figure>
<figureCaption confidence="0.998571">
Figure 6: Representing Figure 5 as an f-structure.
</figureCaption>
<page confidence="0.950221">
95
</page>
<bodyText confidence="0.9853904">
INIT ::= s : (b : none A a : none) A next : q0. results which were more positive than a general undecid-
Then we describe standard configurations: ability theorem. Similar remarks hold for theories like
relational grammar, in which many such constraints have
been studied. We hope that logical tools will provide a
way to classify these empirically motivated conditions.
</bodyText>
<sectionHeader confidence="0.687518" genericHeader="method">
CONF ::= INIT V (p : CONF A (QO V ... V QN)).
</sectionHeader>
<bodyText confidence="0.999752666666667">
Next, we show how configurations are updated, de-
pending on the move rules. If qi is push b; go to qj , then
we write
</bodyText>
<sectionHeader confidence="0.431374" genericHeader="method">
QI ::= next:qjAp:next:qiAs:a:noneAsb=ps.
</sectionHeader>
<bodyText confidence="0.9971629">
The last clause tells us that the current stack contents,
after finding a &amp;quot;b&amp;quot; on top, is the same as the previous
contents. The &amp;quot;a: none&amp;quot; clause guarantees that only a
&amp;quot;b&amp;quot; is found on the DG representing the stack. The sec-
ond clause enforces a consistent state transition from the
previous configuration, and the first clause says what the
next state should be.
If qi is
pop stack; it a go to qj else go to qk,
then we write the following.
</bodyText>
<figure confidence="0.836139333333333">
QI ::= p : next : qi
A ((s=psaAnext:qjAp:s:b:none)
V(s=psbAnext:qkAp:s:a:none))
</figure>
<bodyText confidence="0.523352">
For the last configuration, we put
</bodyText>
<sectionHeader confidence="0.63634" genericHeader="method">
QF ::= CONF A p : next : qt.
</sectionHeader>
<bodyText confidence="0.999839">
We take QF as the &amp;quot;distinguished predicate&amp;quot; of our
scheme.
It should be clear that this formula, which is a big
where-formula, is satisfiable if the machine reaches state
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999992416666667">
It would be desirable to use the notation provided
by our logic to state substantive principles of particu-
lar linguistic theories. Consider, for example, Kashket&apos;s
parser for Warlpiri [4], which is based on GB theory. For
languages like Warlpiri, we might be able to say that
linear order is only explicitly represented at the mor-
phemic level, and not at the phrase level. This would
translate into a constraint on the kinds of logical for-
mulas we could use to describe such languages: the &lt;
relation could only be used as a relation between nodes
of the MORPHEME type. Given such a condition on
formulas, it might then be possible to prove complexity
</bodyText>
<sectionHeader confidence="0.998212" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99996096875">
[1] Joshi, A. , K. Vijay-Shanker, and D. Weir, The Con-
vergence of Mildly Context-Sensitive Grammar For-
malisms. To appear in T. Wasow and P. Sells, ed.
&amp;quot;The Processing of Linguistic Structure&amp;quot;, MIT Press.
[2] Kaplan, R. and 1. Bresnan, LFG: a Formal Sys-
tem for Grammatical Representation, in Bresnan,
ed. The Mental Representation of Grammatical Re-
lations, MIT Press, Cambridge, 1982, 173-281.
[3] Kasper, R. and W. Rounds, A Logical Semantics for
Feature Structures, Proceedings of 24th ACL Annual
Meeting, June 1986.
[4] Kashket, M. Parsing a free word order language:
Warlpiri. Proc. 24th Ann. Meeting of ACL, 1986,
60-66.
[5] Kay, M. Functional Grammar. In Proceedings of the
Fifth Annual Meeting of the Berkeley Linguistics So-
ciety, Berkeley Linguistics Society, Berkeley, Califor-
nia, February 17-19., 1979.
[6] Pereira, F.C.N., and D. Warren, Definite Clause Gram-
mars for Language Analysis: A Survey of the Formal-
ism and a Comparison with Augmented Transition
Networks, Artificial Intelligence 13, (1980), 231-278.
[7] Perlmutter, D. M., Relational Grammar, in Syntax
and Semantics, vol. 13: Current Approaches to Syn-
tax, Academic Press, 1980.
[8] Rounds, W. C. and R. Kasper. A Complete Logi-
cal Calculus for Record Structures Representing Lin-
guistic Information. IEEE Symposium on Logic in
Computer Science, June, 1986.
[9] Rounds, W., LFP: A Formalism for Linguistic De-
scriptions and an Analysis of its Complexity, Com-
putational Linguistics, to appear.
</reference>
<page confidence="0.99846">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.768362">
<title confidence="0.999661">A LOGICAL VERSION OF FUNCTIONAL GRAMMAR</title>
<author confidence="0.999964">William C Rounds</author>
<affiliation confidence="0.99973">University of Michigan</affiliation>
<title confidence="0.828964">Xerox PARC</title>
<author confidence="0.999711">Alexis Manaster-Ramer</author>
<affiliation confidence="0.998709">IBM T.J. Watson Research Center Wayne State University</affiliation>
<abstract confidence="0.993327545454546">1 Abstract Kay&apos;s functional-unification grammar notation [5] is a way of expressing grammars which relies on very few primitive notions. The primary syntactic structure is the structure, can be visualised as a directed graph with arcs labeled by attributes of a constituent, and the primary structure-building operation is unification. In this paper we propose a mathematical formulation of FUG, using logic to give a precise account of the strings and the structures defined by any grammar written in this notation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>K Vijay-Shanker</author>
<author>D Weir</author>
</authors>
<title>The Convergence of Mildly Context-Sensitive Grammar Formalisms.</title>
<editor>in T. Wasow and P. Sells, ed.</editor>
<publisher>MIT Press.</publisher>
<note>To appear</note>
<marker>[1]</marker>
<rawString>Joshi, A. , K. Vijay-Shanker, and D. Weir, The Convergence of Mildly Context-Sensitive Grammar Formalisms. To appear in T. Wasow and P. Sells, ed. &amp;quot;The Processing of Linguistic Structure&amp;quot;, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
</authors>
<title>LFG: a Formal System for Grammatical Representation,</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>173--281</pages>
<editor>in Bresnan, ed.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="3721" citStr="[2]" startWordPosition="591" endWordPosition="591"> we do claim that our notation shows clearly the relationship of relational to transformational grammars on one hand, and to lexical-functional grammars on the other. Finally, we prove that the satisfiability problem for our logic is undecidable. This should perhaps be an expected result, because the proof relies on simulating Turing machine computations in a grammar, and follows the standard undecidability arguments. The satisfiability problem is not quite the same problem as the universal recognition problem, however, and with mild conditions on derivations similar to those proposed for LFG [2], the latter problem should become decidable. We must leave efficiency questions unexamined in this paper. The notation has not been implemented. We view this notation as a temporary one, and anticipate that many revisions and extensions will be necessary if it is to be implemented at all. Of course, FUG itself could be considered as an implementation, but we have added the word order relations to our logic, which are not explicit in FUG. In this paper, which is not full because of space limitations, we will give definitions and examples in Section 3; then will sketch the relational applicatio</context>
</contexts>
<marker>[2]</marker>
<rawString>Kaplan, R. and 1. Bresnan, LFG: a Formal System for Grammatical Representation, in Bresnan, ed. The Mental Representation of Grammatical Relations, MIT Press, Cambridge, 1982, 173-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kasper</author>
<author>W Rounds</author>
</authors>
<title>A Logical Semantics for Feature Structures,</title>
<date>1986</date>
<booktitle>Proceedings of 24th ACL Annual Meeting,</booktitle>
<contexts>
<context position="1346" citStr="[3]" startWordPosition="208" endWordPosition="208">oblem of syntactic description is to use logical formulas to put conditions or constraints on ordering of constituents, ancestor and descendant relations, and feature attribute information in syntactic structures. The present version of our logic has predicates specifically designed for these purposes. A grammar can be considered as just a logical formula, and the structures satisfying the formula are the syntactic structures for the sentences of the language. This notion goes back to DCG&apos;s [6], but our formulation is quite different. In particular, it builds on the logic of Kasper and Rounds [3], a logic intended specifically to describe feature structures. The formulation has several new aspects. First, it introduces the oriented feature structure as the primary syntactic structure. One can think of these structures as parse trees superimposed on directed graphs, although the general definition allows much more flexibility. In fact, our notation does away with the parse tree altogether. A second aspect of the notation is its treatment of word order. Our logic allows small grammars to define free-word order languages over large vocabularies in a way not possible with standard ID/LP r</context>
<context position="4996" citStr="[3]" startWordPosition="801" endWordPosition="801">e final remarks. 3 Definitions and examples 3.1 Oriented f-structures In this section we will describe the syntactic structures to which our logical formulas refer. The next subsection 89 Oleet ch. â€¢ Figure 1: A typical DG. will give the logic itself. Our intent is to represent not only feature information, but also information about ordering of constituents in a single structure. We begin with the unordered version, which is the simple DG (directed graph) structure commonly used for non-disjunctive information. This is formalized as an acyclic finite automaton, in the manner of Kasper-Rounds [3]. Then we add two relations on nodes of the DG: ancestor and linear precedence. The key insight about these relations is that they are partial; nodes of the graph need not participate in either of the two relations. Pure feature information about a constituent need not participate in any ordering. This allows us to model the &amp;quot;cset&amp;quot; and &amp;quot;pattern&amp;quot; information of FUG, while allowing structure sharing in the usual DG representation of features. We are basically interested in describing structures like that shown in Figure 1. A formalism appropriate for specifying such DG structures is that of fini</context>
</contexts>
<marker>[3]</marker>
<rawString>Kasper, R. and W. Rounds, A Logical Semantics for Feature Structures, Proceedings of 24th ACL Annual Meeting, June 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kashket</author>
</authors>
<title>Parsing a free word order language: Warlpiri.</title>
<date>1986</date>
<booktitle>Proc. 24th Ann. Meeting of ACL,</booktitle>
<pages>60--66</pages>
<marker>[4]</marker>
<rawString>Kashket, M. Parsing a free word order language: Warlpiri. Proc. 24th Ann. Meeting of ACL, 1986, 60-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Functional Grammar.</title>
<date>1979</date>
<booktitle>In Proceedings of the Fifth Annual Meeting of the</booktitle>
<institution>Berkeley Linguistics Society, Berkeley Linguistics Society,</institution>
<location>Berkeley, California,</location>
<marker>[5]</marker>
<rawString>Kay, M. Functional Grammar. In Proceedings of the Fifth Annual Meeting of the Berkeley Linguistics Society, Berkeley Linguistics Society, Berkeley, California, February 17-19., 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>D Warren</author>
</authors>
<title>Definite Clause Grammars for Language Analysis: A Survey of the Formalism and a Comparison with Augmented Transition Networks,</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>13</volume>
<pages>231--278</pages>
<contexts>
<context position="1242" citStr="[6]" startWordPosition="189" endWordPosition="189"> structures defined by any grammar written in this notation. 2 Introduction Our basic approach to the problem of syntactic description is to use logical formulas to put conditions or constraints on ordering of constituents, ancestor and descendant relations, and feature attribute information in syntactic structures. The present version of our logic has predicates specifically designed for these purposes. A grammar can be considered as just a logical formula, and the structures satisfying the formula are the syntactic structures for the sentences of the language. This notion goes back to DCG&apos;s [6], but our formulation is quite different. In particular, it builds on the logic of Kasper and Rounds [3], a logic intended specifically to describe feature structures. The formulation has several new aspects. First, it introduces the oriented feature structure as the primary syntactic structure. One can think of these structures as parse trees superimposed on directed graphs, although the general definition allows much more flexibility. In fact, our notation does away with the parse tree altogether. A second aspect of the notation is its treatment of word order. Our logic allows small grammars</context>
</contexts>
<marker>[6]</marker>
<rawString>Pereira, F.C.N., and D. Warren, Definite Clause Grammars for Language Analysis: A Survey of the Formalism and a Comparison with Augmented Transition Networks, Artificial Intelligence 13, (1980), 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Perlmutter</author>
</authors>
<title>Relational Grammar,</title>
<date>1980</date>
<booktitle>in Syntax and Semantics,</booktitle>
<volume>13</volume>
<publisher>Academic Press,</publisher>
<contexts>
<context position="2725" citStr="[7]" startWordPosition="433" endWordPosition="433"> has adopted much the same conventions in tree adjunct grammar.) A third aspect of our treatment is the use of fixedpoint formulas to introduce recursion into grammars. This idea is implicit in DCG&apos;s, and has been made explicit in the logics CLFP and ILFP [9]. We give a simple way of expressing the semantics of these formulas which corresponds closely to the usual notion of grammatical derivations. There is an interesting use of type variables to describe syntactic categories and/or constructions. We illustrate the power of the notation by sketching how the constructions of relational grammar [7] can be formulated in the logic. To our knowledge, this is the first attempt to interpret the relational ideas in a fully mathematical framework. Although relational networks themselves have been precisely specified, there does not seem to be a precise statement of how relational derivations take place. We do not claim that our formalization is the one intended by Postal and Perlmutter, but we do claim that our notation shows clearly the relationship of relational to transformational grammars on one hand, and to lexical-functional grammars on the other. Finally, we prove that the satisfiabilit</context>
<context position="20538" citStr="[7]" startWordPosition="3769" endWordPosition="3769"> the symbols labeling the positions of w, then w is compatible with A. This is our new way of dealing with free word order. Suppose that no precedence relations are specified in a formula. Then, minimal satisfying f-structures will have an empty &lt;- relation. This implies that any permutation of the terminal symbols in such a structure will be allowed. Many other ways of defining word order can also be expressed in this logic, which enjoys an advantage over ID/LP rules in this respect. 4 Modeling Relational Grammar Consider the relational analyses in Figures 4 and 5. These analyses, taken from [7], have much in common with functional analyses and also with transsformational ones. The present pair of networks illustrates a kind of raising construction common in the relational literature. In Figure 4, there are arc labels P, 1, and 2, representing &amp;quot;predicate&amp;quot;, &amp;quot;subject&amp;quot;, and &amp;quot;object&amp;quot; relations. The &amp;quot;cl&amp;quot; indicates that this analysis is at the first linguistic stratum, roughly like a transformational cycle. In Figure 5, we learn that at the second stratum, the predicate (&amp;quot;believed&amp;quot;) is the same as at stratum 1, as is the subject. However, the object at level 2 is now &amp;quot;John&amp;quot;, and the phrase</context>
</contexts>
<marker>[7]</marker>
<rawString>Perlmutter, D. M., Relational Grammar, in Syntax and Semantics, vol. 13: Current Approaches to Syntax, Academic Press, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Rounds</author>
<author>R Kasper</author>
</authors>
<title>A Complete Logical Calculus for Record Structures Representing Linguistic Information.</title>
<date>1986</date>
<booktitle>IEEE Symposium on Logic in Computer Science,</booktitle>
<contexts>
<context position="18694" citStr="[8]" startWordPosition="3438" endWordPosition="3438">the derivational semantics just explained. This is done with the help of the following lemmas. Lemma 1 (A, p) (t)k) (A, p(k)) Lemma 2 (A, p) 0[X1 : Xi E iff (A, If) 0, where p* (Xi) = {5 I (13,p) j}, if X E D, and otherwise is p(X). The proofs are omitted. Finally, we must explain the notion of the language defined by 0, where 0 is a logical formula. Suppose for simplicity that 0 has no free type variables. Then the notion A 0 makes sense, and we say that a string w E L(0) if for some subsumption-minimal f-structure A, A 0, and w is compatible with A. The notion of subsumption is explained in [8]. Briefly, we have the following definition. Let A and 5 be two automata. We say A C B (A subsumes B; B extends A) if there is a homomorphism from A to B; that is, a map h :QA QB such that (for all existing transitions) 1. h(64q,1)) = 2. r(h(q)) = r(q) for all q such that r(q) E A; 3. h(q0,) = It can be shown that subsumption is a partial order on isomorphism classes of automata (without orderings), and that for any formula 0 without recursion or ordering, that there are a finite number of subsumption-minimal automata satisfying it. We consider as candidate structures for the language defined </context>
</contexts>
<marker>[8]</marker>
<rawString>Rounds, W. C. and R. Kasper. A Complete Logical Calculus for Record Structures Representing Linguistic Information. IEEE Symposium on Logic in Computer Science, June, 1986.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W Rounds</author>
</authors>
<title>LFP: A Formalism for Linguistic Descriptions and an Analysis of its Complexity,</title>
<journal>Computational Linguistics,</journal>
<note>to appear.</note>
<contexts>
<context position="2381" citStr="[9]" startWordPosition="379" endWordPosition="379"> is its treatment of word order. Our logic allows small grammars to define free-word order languages over large vocabularies in a way not possible with standard ID/LP rules. It is not clear whether or not this treatment of word order was intended by Kay, but the issue naturally arose during the process of making this model precise. (Joshi [I] has adopted much the same conventions in tree adjunct grammar.) A third aspect of our treatment is the use of fixedpoint formulas to introduce recursion into grammars. This idea is implicit in DCG&apos;s, and has been made explicit in the logics CLFP and ILFP [9]. We give a simple way of expressing the semantics of these formulas which corresponds closely to the usual notion of grammatical derivations. There is an interesting use of type variables to describe syntactic categories and/or constructions. We illustrate the power of the notation by sketching how the constructions of relational grammar [7] can be formulated in the logic. To our knowledge, this is the first attempt to interpret the relational ideas in a fully mathematical framework. Although relational networks themselves have been precisely specified, there does not seem to be a precise sta</context>
</contexts>
<marker>[9]</marker>
<rawString>Rounds, W., LFP: A Formalism for Linguistic Descriptions and an Analysis of its Complexity, Computational Linguistics, to appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>