<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000993">
<note confidence="0.830784">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 115-118, Lisbon, Portugal, 2000.
</note>
<title confidence="0.830826">
Learning IE Rules for a Set of Related Concepts
</title>
<note confidence="0.78939075">
J. Turmo and H. Rodriguez
TALP Research Center. Universitat Politecnica de Catalunya
Jordi Girona Salgado, 1-3
E-08034 Barcelona - Spain
</note>
<sectionHeader confidence="0.982636" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967885714286">
The growing availability of on-line text has led
to an increase in the use of automatic knowledge
acquisition approaches from textual data. In
fact, a number of Information Extraction (IE)
systems has emerged in the past few years in
relation to the MUC conferencesl. The aim of
an IE system consists in automatically extract-
ing pieces of information from text, being this
information relevant for a set of prescribed con-
cepts (scenario). One of the main drawbacks of
applying IE systems is the high cost involved in
manually adapting them to new domains and
text styles.
In recent years, a variety of Machine Learn-
ing (ML) techniques has been used to improve
the portability of IE systems to new domains,
as in SRV (Freitag, 1998), RAPIER (Calif
and Mooney, 1997), LIEP (Huffman, 1996),
CRYSTAL (Soderland et al., 1995) and WHISK
(Soderland, 1999) . However, some drawbacks
remain in the portability of these systems: a)
existing systems generally depend on the sup-
ported text style and learn IE-rules either for
structured texts, semi-structured texts or free
text , b) IE systems are mostly single-concept
learning systems, c) consequently, an extrac-
tor (e.g., a rule set) is learned for each con-
cept within the scenario in an independent man-
ner, d) the order of execution of the learners
is set manually, and so are the scheduling and
way of combination of the resulting extractors,
and e) focusing on the training data, the size of
available training corpora can be inadequate to
accurately learn extractors for all the concepts
within the scenario2.
</bodyText>
<footnote confidence="0.980277">
1 http: / /www.muc.saic.com/
2This is so when dealing with some combinations of
text style and domain.
</footnote>
<bodyText confidence="0.997277428571429">
This paper describes EVIUS, a multi-concept
learning system for free text that follows a
multi-strategy constructive learning approach
(MCL) (Michalshi, 1993) and supports insuffi-
cient amounts of training corpora. EVIUS is
a component of a multilingual IE system, M-
TURBIO (Turmo et al., 1999).
</bodyText>
<listItem confidence="0.4784565">
2 EVIUS. Learning rule sets for a
set of related concepts
</listItem>
<bodyText confidence="0.9983783">
The input of EVIUS is both a partially-parsed
semantically-tagged3 training corpus and a de-
scription of the desired target structure. This
description is provided as a set of concepts C
related to a set of asymmetric binary relations,
TZ.
In order to learn set S of IE rule sets for the
whole C, EVIUS uses an MCL approach inte-
grating constructive learning, closed-loop learn-
ing and deductive restructuring (Ko, 1998).
In this multi-concept situation, the system
determines which concepts to learn and, later,
incrementally updates S. This can be relatively
straightforward when using knowledge about
the target structure in a closed-loop learning
approach. Starting with C, EVIUS reduces set
U of unlearned concepts iteratively by selecting
subset P C U formed by the primitive concepts
in U and learning a rule set for each c E P 4.
For instance, the single colour scenario5 in fig-
</bodyText>
<footnote confidence="0.9760911">
3With EuroWordNet (http://www.hum.uva.nlrewn/)
synsets. No attempt has been made to disambiguate
such tags.
4No cyclic scenarios are allowed so that a topological
sort of C is possible, which starts with a set of primitive
concepts.
3Our testing domain is mycology. Texts consists of
Spanish descriptions of specimens. There is a rich variety
of colour descriptions including basic colours, intervals,
changes, etc.
</footnote>
<page confidence="0.998789">
115
</page>
<bodyText confidence="0.96790725">
ure 1 is provided to learn from instances of the
following three related concepts: colour, such
as in instance &amp;quot;azul ligeramente claro&amp;quot; (slightly
pale blue), colour_interval, as in &amp;quot;entre rosa
y rojo sangre&amp;quot; (between pink and blood red),
and to_change, as in &amp;quot;rojo vira a marron&amp;quot; (red
changes to brown).
Initially, U = C ={colour, colour_interval,
to_change}. Then, EVIUS calculates
P ={colour} and once a rule set has been
learned for colour, the new 14 .{colour_interval,
to_change} is studied identifying P = U.
</bodyText>
<figure confidence="0.675887">
to
from
</figure>
<figureCaption confidence="0.9600795">
Figure 1: A single scenario for the colour do-
main
</figureCaption>
<bodyText confidence="0.999894034482758">
In order to learn a rule set for a concept,
EVIUS uses the relational learning method ex-
plained in section 3, and defines the learn-
ing space by means of a dynamic predicate
model. As a pre-process of the system, the
training corpus is translated into predicates
using the following initial predicate model:
a) attributive meta-predicates: pos_X(A),
isa_X (A), has_hypernym_X (A), word_X (A)
and lemma_X(A), where Xis instantiated with
closed categories, b) relational meta-predicates:
distance_le_X(A,B), stating that there are X
terminal nodes, at most, between A and B, and
c) relational predicates: ancestor(A,B), where B
is the syntactic ancestor of A, and brother(A,B),
where B is the right brother node of A sharing
the syntactic ancestor.
Once a rule set for concept c is learned,
new examples are added for further learning by
means of a deductive restructuring approach:
training examples are reduced to generate a
more compact and useful knowledge of the
learned concept. This is achieved by using
the induced rule set and a syntactico-semantic
transformational grammar. Further to all this,
a new predicate isa_c is added to the model.
For instance, in figure 26, the Spanish sen-
tence &amp;quot;su color rojo vira a marron oscuro&amp;quot;
(its red colour changes to dark brown) has
</bodyText>
<footnote confidence="0.834825">
6Which is presented here as a partially-parsed tree
for simplicity.
</footnote>
<figureCaption confidence="0.998808">
Figure 2: Restructuring training examples
</figureCaption>
<bodyText confidence="0.999451888888889">
two examples of colour, n3 and n6+n7, be-
ing these &amp;quot;rojo&amp;quot; (red) and &amp;quot;marrOn&amp;quot;+&amp;quot;oscuro&amp;quot;
(dark brown). No reduction is required by the
former. However, the latter example is reduced
to node n6&apos;. As a consequence, two new at-
tributes are added to the model: isa_colour(n3)
and isa_colour(n6&apos;). This new knowledge will
be used to learn the concepts to_change and
colour_interval.
</bodyText>
<sectionHeader confidence="0.986754" genericHeader="categories and subject descriptors">
3 Rule set learning
</sectionHeader>
<bodyText confidence="0.984862">
EVIUS uses FOIL (First-order Induction Learn-
ing) (Quinlan, 1990) to build an initial rule set
Ro from a set of positive and negative examples.
Positive examples E+ can be selected using a
friendly environment either as:
</bodyText>
<listItem confidence="0.858406307692308">
• test relations: c(AI,A2) where both A1 and
A2 are terminal nodes that exactly delimit
a text value for c. For instance, both text
relations co/our(n3,n3) or colour(n6,n7) in
figure 2, or as:
• ontology relations: c(Ai,A2,... ,An) where
all Ai are terminal nodes which are in-
stances of already learned concepts related
to c in the scenario. For instance, the on-
tology relation to_change(n3,n6&apos;)7 , in the
same figure, means that the colour repre-
sented by instance n3 changes to that rep-
resented by n6&apos;.
</listItem>
<bodyText confidence="0.7521785">
Negative examples 8— are automatically se-
lected as explained in section 3.1.
</bodyText>
<footnote confidence="0.763826">
7Note that, after the deductive restructuring step,
both n3 and n6&apos; are instances of the concept colour.
</footnote>
<figure confidence="0.997358214285714">
S (n12)
(n9) sn sp (n11)
gnom
(n8)
spec n a v prep
su color rojo vira a
(n1) (n2) (n3) (n4) (n5)
gnom
(n8)
spec n a v prep
su color irojoivira a
(n1) (n2) (n3) (n4) (n5)
to
from
</figure>
<page confidence="0.992638">
116
</page>
<bodyText confidence="0.99944619047619">
If any uncovered examples set, Eu+ , remains
after FOIL&apos;s performance, this is due to the lack
of sufficient examples. Thus, the system tries
to improve recall by growing set E+ with arti-
ficial examples (pseudo-examples), as explained
in 3.2. A new execution of FOIL is done by
using the new E. . The resulting rule set 1V0
is combined with &apos;R.0 in order to create &apos;R.1 by
appending the new rules from 14 to &apos;R.0. Conse-
quently, the recall value of &apos;R.1 is forced to be at
least equal to that of &apos;R.0, although the accuracy
can decrease. A better method seems to be the
merging of rules from 7?4 and &apos;R.0 by studying
empirical subsumptions. This last combination
allows to create more compact and accurate rule
sets.
EVIUS uses an incremental learning approach
to learn rule sets for each concept. This is done
by iterating the process above while uncovered
examples remain and the F1 score increment
(AF1) is greater than pre-defined constant a:
</bodyText>
<equation confidence="0.952772363636364">
select g+ and generate E—
R.0 = FOIL(E+
= uncovered_from(R0)
= Fi(R.0)
while Ett 0 and .L.Fi &gt; a do
E+= E+ Upseudo-exam,ples(E:)
= FOIL(E±,e—)
7i+1 = combine_rules(Rig)
= uncovered_f rom(R,i+i)
= — (R.i )
endwhile
</equation>
<construct confidence="0.703112">
if AFi &gt; a then return
else return
endif
</construct>
<subsectionHeader confidence="0.993289">
3.1 Generating relevant negative
examples
</subsectionHeader>
<bodyText confidence="0.99999096875">
Negative examples can be defined as any com-
bination of terminal nodes out of E+ . However,
this approach produces an extremely large num-
ber of examples, out of which only a small sub-
set is relevant to learn the concept. Related to
this, (Freitag, 1998) uses words to learn only
slot rules (learned from text-relation examples)
, selecting as negative those non-positive word
pairs that define a string as neither longer than
the maximum length in positive examples, nor
shorter than the minimum.
A more general approach is adopted to define
the distance between possible examples in the
learning space, applying a clustering method us-
ing positive examples as medoidss . The N near-
est non-positive examples to each medoid can be
selected as negative ones. Distance, in our case,
must be defined as multidimensional due to the
typology of occurring features. It is relatively
easy to define distances between examples for
word_X and lemma_X predicates, being 1 when
X values are equal, and 0 otherwise. For isa_X
predicates, the minimum of all possible concep-
tual distances (Agirre and Rigau, 1995) between
X values in EWN has been used. Greater dif-
ficulty is encountered when defining a distance
from a morpho-syntactic point of view (e.g., a
pronoun seems to be closer to a noun than a
verb). In (Turmo et al., 1999), the concept of
5-set has been presented as a syntactic relation
generalization, and a distance measure has been
based on this concept.
</bodyText>
<subsectionHeader confidence="0.999921">
3.2 Creating pseudo-examples
</subsectionHeader>
<bodyText confidence="0.999951428571429">
A method has been used inspired by the gen-
eration of convex pseudo data (Breiman, 1998),
in which a similar process to gene-combination
in genetic algorithms is used.
For each positive example c(Ai, , An)9 of
concept c to be dealt with, an attribute vector
is defined as
</bodyText>
<equation confidence="0.5905835">
(word_XB„... ,
,sem_XBTh,context)
</equation>
<bodyText confidence="0.999759266666667">
where B1, , Bn are the unrepeated terminal
nodes from A1, , An, context is the set of all
predicates subsumed by the syntactico-semantic
structure between the nearest positive exam-
ple on the left and the nearest one on the
right, and sem_XB, is the list of isa_X and
has_hypernym_X predicates for Bi.
Then, for each example uncovered by the rule
set learned by FOIL, a set of pseudo-examples is
generated. A pseudo-example is built by com-
bining both the uncovered example vector and
a randomly selected covered one. This is done
as follows: for each dimension, one of both pos-
sible values is randomly selected as value for the
pseudo-example.
</bodyText>
<footnote confidence="0.998814333333333">
8A medoid is an actual data point representing a clus-
ter.
8As defined in section 3.
</footnote>
<page confidence="0.985248">
117
</page>
<table confidence="0.999742833333333">
T. Set* E± Recall Prec. F1
150 105 56.86 100 0.725
250 206 62.74 98.45 0.766
350 270 73.53 97.40 0.838
450 328 75.49 98.72 0.856
550 398 75.49 98.72 0.856
</table>
<tableCaption confidence="0.974082666666667">
Table 1: Results for the colour concept for dif-
ferent training set sizes (* subscript 0 means
only one FOIL iteration)
</tableCaption>
<sectionHeader confidence="0.993592" genericHeader="acknowledgments">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999951178571429">
EVIUS has been tested on the mycological do-
main. A set of 68 Spanish mycological docu-
ments (covering 9800 words corresponding to
1360 lemmas) has been used. 13 of them have
been kept for testing and the others for train-
ing. The target ontology consisted of 14 con-
cepts and 24 relations.
Several experiments have been carried out
with different training sets. Results of the initial
rule set for the colour conceptl° are presented
in table 1.
Out of 34 in the 350 initial rule set, one of the
most relevant learned rules isil:
C olour (A, B):-has_hypernym_00017586n (B) ,
has _hypernym_03464624n (A) , brother (A, B).
Table 2 shows the results of adding pseudo-
examples to the 35012 training set and using the
algorithm in section 3. This was tested with
a = 0.01 (two iterations are enough, 351 and
352) and 5 pseudo-examples for each uncovered
case. The algorithm returns the rule set pro-
duced in the first iteration due to the fact that
AFIT13&gt; 0.01 between the first and the sec-
ond iterations. Higher results can be generated
when using lower values for a.
Although no direct comparison with other
systems is possible due to the domain and lan-
guage used, our results can be considered state-
</bodyText>
<footnote confidence="0.941180375">
1°This concept appears to be the most difficult to be
learned.
11A chromatic colour (03464624n) that is the left syn-
tactic brother of an attribute (00017586n) such as lumi-
nosity or another chromatic colour.
12This size has been selected to allow a better com-
parison with the results in table 1.
131\ means the F1 value for training sets
</footnote>
<table confidence="0.906657">
T. Set e+ Fi 7, Recall Prec. Fi.
351 415 0.981 76.47 97.50 0.857
352 465 0.987 79.41 97.50 0.875
</table>
<tableCaption confidence="0.7916095">
Table 2: Results from adding pseudo-examples
to the initial training set with 35 documents.
</tableCaption>
<bodyText confidence="0.951024">
of-the-art regarding similar MUC competition
tasks.
</bodyText>
<sectionHeader confidence="0.994973" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998647195121951">
Eneko Agirre and German Rigau. 1995. A Proposal
for Word Sense Disambiguation using Concep-
tual Distance. In Proceedings of the International
Conference RANLP, Tzigov Chark, Bulgaria.
L. Breiman. 1998. Arcing Classifiers. The Annals
of Statistics, 26(3):801-849.
M.E. Calif and R. Mooney. 1997. Relational learn-
ing of pattern-match rules for information extrac-
tion. In Workshop on Natural Language Learning,
pages 9-15. ACL.
D. Freitag. 1998. Machine Learning for Informa-
tion Extraction in Informal Domains. Ph.D. the-
sis, Computer Science Department. Carnegie Mel-
lon University.
S. Huffman. 1996. Learning information extraction
patterns from examples. In S. Wermter, E. Riloff,
and G. Sheller, editors, Connectionist, statistical
and symbolic approaches to learning for natural
language processing. Springer-Verlag.
H. Ko. 1998. Empirical assembly sequence planning:
A multistrategy constructive learning approach.
In I. Bratko R. S. Michalsky and M. Kubat, ed-
itors, Machine Learning and Data Mining. John
Wiley &amp; Sons LTD.
R.S. Michalshi. 1993. Towards a unified theory of
learning: Multistrategy task-adaptive learning.
In B.G. Buchanan and D. Wilkins, editors, Read-
ings in Knowledge Acquisition and Learning. Mor-
gan Kauffman.
J.R. Quinlan. 1990. Learning logical definitions
from relations. Machine Learning, 5:239-266.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehn-
ert. 1995. Crystal: Inducing a conceptual dictio-
nary. In XIV International Joint Conference on
Artificial Intelligence, pages 1314-1321.
S. Soderland. 1999. Learning information extraction
rules for semi-structured and free text. Machine
Learning, 34:233-272.
J. Turmo, N. Catala, and H. Rodriguez. 1999. An
adaptable ie system to new domains. Applied In-
telligence, 10(2/3):225-246.
</reference>
<page confidence="0.996015">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.431863">
<note confidence="0.967913">of CoNLL-2000 and LLL-2000, 115-118, Lisbon, Portugal, 2000.</note>
<title confidence="0.989581">Learning IE Rules for a Set of Related Concepts</title>
<author confidence="0.673594">J Turmo</author>
<author confidence="0.673594">H</author>
<affiliation confidence="0.6124875">TALP Research Center. Universitat Politecnica de Jordi Girona Salgado,</affiliation>
<address confidence="0.974222">E-08034 Barcelona - Spain</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>German Rigau</author>
</authors>
<title>A Proposal for Word Sense Disambiguation using Conceptual Distance.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference RANLP, Tzigov Chark,</booktitle>
<contexts>
<context position="9376" citStr="Agirre and Rigau, 1995" startWordPosition="1548" endWordPosition="1551">rter than the minimum. A more general approach is adopted to define the distance between possible examples in the learning space, applying a clustering method using positive examples as medoidss . The N nearest non-positive examples to each medoid can be selected as negative ones. Distance, in our case, must be defined as multidimensional due to the typology of occurring features. It is relatively easy to define distances between examples for word_X and lemma_X predicates, being 1 when X values are equal, and 0 otherwise. For isa_X predicates, the minimum of all possible conceptual distances (Agirre and Rigau, 1995) between X values in EWN has been used. Greater difficulty is encountered when defining a distance from a morpho-syntactic point of view (e.g., a pronoun seems to be closer to a noun than a verb). In (Turmo et al., 1999), the concept of 5-set has been presented as a syntactic relation generalization, and a distance measure has been based on this concept. 3.2 Creating pseudo-examples A method has been used inspired by the generation of convex pseudo data (Breiman, 1998), in which a similar process to gene-combination in genetic algorithms is used. For each positive example c(Ai, , An)9 of conce</context>
</contexts>
<marker>Agirre, Rigau, 1995</marker>
<rawString>Eneko Agirre and German Rigau. 1995. A Proposal for Word Sense Disambiguation using Conceptual Distance. In Proceedings of the International Conference RANLP, Tzigov Chark, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Arcing Classifiers. The Annals of Statistics,</title>
<date>1998</date>
<pages>26--3</pages>
<contexts>
<context position="9849" citStr="Breiman, 1998" startWordPosition="1632" endWordPosition="1633">g 1 when X values are equal, and 0 otherwise. For isa_X predicates, the minimum of all possible conceptual distances (Agirre and Rigau, 1995) between X values in EWN has been used. Greater difficulty is encountered when defining a distance from a morpho-syntactic point of view (e.g., a pronoun seems to be closer to a noun than a verb). In (Turmo et al., 1999), the concept of 5-set has been presented as a syntactic relation generalization, and a distance measure has been based on this concept. 3.2 Creating pseudo-examples A method has been used inspired by the generation of convex pseudo data (Breiman, 1998), in which a similar process to gene-combination in genetic algorithms is used. For each positive example c(Ai, , An)9 of concept c to be dealt with, an attribute vector is defined as (word_XB„... , ,sem_XBTh,context) where B1, , Bn are the unrepeated terminal nodes from A1, , An, context is the set of all predicates subsumed by the syntactico-semantic structure between the nearest positive example on the left and the nearest one on the right, and sem_XB, is the list of isa_X and has_hypernym_X predicates for Bi. Then, for each example uncovered by the rule set learned by FOIL, a set of pseudo</context>
</contexts>
<marker>Breiman, 1998</marker>
<rawString>L. Breiman. 1998. Arcing Classifiers. The Annals of Statistics, 26(3):801-849.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Calif</author>
<author>R Mooney</author>
</authors>
<title>Relational learning of pattern-match rules for information extraction.</title>
<date>1997</date>
<booktitle>In Workshop on Natural Language Learning,</booktitle>
<pages>9--15</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1047" citStr="Calif and Mooney, 1997" startWordPosition="168" endWordPosition="171">n fact, a number of Information Extraction (IE) systems has emerged in the past few years in relation to the MUC conferencesl. The aim of an IE system consists in automatically extracting pieces of information from text, being this information relevant for a set of prescribed concepts (scenario). One of the main drawbacks of applying IE systems is the high cost involved in manually adapting them to new domains and text styles. In recent years, a variety of Machine Learning (ML) techniques has been used to improve the portability of IE systems to new domains, as in SRV (Freitag, 1998), RAPIER (Calif and Mooney, 1997), LIEP (Huffman, 1996), CRYSTAL (Soderland et al., 1995) and WHISK (Soderland, 1999) . However, some drawbacks remain in the portability of these systems: a) existing systems generally depend on the supported text style and learn IE-rules either for structured texts, semi-structured texts or free text , b) IE systems are mostly single-concept learning systems, c) consequently, an extractor (e.g., a rule set) is learned for each concept within the scenario in an independent manner, d) the order of execution of the learners is set manually, and so are the scheduling and way of combination of the</context>
</contexts>
<marker>Calif, Mooney, 1997</marker>
<rawString>M.E. Calif and R. Mooney. 1997. Relational learning of pattern-match rules for information extraction. In Workshop on Natural Language Learning, pages 9-15. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
</authors>
<title>Machine Learning for Information Extraction in Informal Domains.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department. Carnegie Mellon University.</institution>
<contexts>
<context position="1014" citStr="Freitag, 1998" startWordPosition="165" endWordPosition="166">hes from textual data. In fact, a number of Information Extraction (IE) systems has emerged in the past few years in relation to the MUC conferencesl. The aim of an IE system consists in automatically extracting pieces of information from text, being this information relevant for a set of prescribed concepts (scenario). One of the main drawbacks of applying IE systems is the high cost involved in manually adapting them to new domains and text styles. In recent years, a variety of Machine Learning (ML) techniques has been used to improve the portability of IE systems to new domains, as in SRV (Freitag, 1998), RAPIER (Calif and Mooney, 1997), LIEP (Huffman, 1996), CRYSTAL (Soderland et al., 1995) and WHISK (Soderland, 1999) . However, some drawbacks remain in the portability of these systems: a) existing systems generally depend on the supported text style and learn IE-rules either for structured texts, semi-structured texts or free text , b) IE systems are mostly single-concept learning systems, c) consequently, an extractor (e.g., a rule set) is learned for each concept within the scenario in an independent manner, d) the order of execution of the learners is set manually, and so are the schedul</context>
<context position="8532" citStr="Freitag, 1998" startWordPosition="1414" endWordPosition="1415">F1 score increment (AF1) is greater than pre-defined constant a: select g+ and generate E— R.0 = FOIL(E+ = uncovered_from(R0) = Fi(R.0) while Ett 0 and .L.Fi &gt; a do E+= E+ Upseudo-exam,ples(E:) = FOIL(E±,e—) 7i+1 = combine_rules(Rig) = uncovered_f rom(R,i+i) = — (R.i ) endwhile if AFi &gt; a then return else return endif 3.1 Generating relevant negative examples Negative examples can be defined as any combination of terminal nodes out of E+ . However, this approach produces an extremely large number of examples, out of which only a small subset is relevant to learn the concept. Related to this, (Freitag, 1998) uses words to learn only slot rules (learned from text-relation examples) , selecting as negative those non-positive word pairs that define a string as neither longer than the maximum length in positive examples, nor shorter than the minimum. A more general approach is adopted to define the distance between possible examples in the learning space, applying a clustering method using positive examples as medoidss . The N nearest non-positive examples to each medoid can be selected as negative ones. Distance, in our case, must be defined as multidimensional due to the typology of occurring featu</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>D. Freitag. 1998. Machine Learning for Information Extraction in Informal Domains. Ph.D. thesis, Computer Science Department. Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Huffman</author>
</authors>
<title>Learning information extraction patterns from examples.</title>
<date>1996</date>
<editor>In S. Wermter, E. Riloff, and G. Sheller, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1069" citStr="Huffman, 1996" startWordPosition="173" endWordPosition="174"> Extraction (IE) systems has emerged in the past few years in relation to the MUC conferencesl. The aim of an IE system consists in automatically extracting pieces of information from text, being this information relevant for a set of prescribed concepts (scenario). One of the main drawbacks of applying IE systems is the high cost involved in manually adapting them to new domains and text styles. In recent years, a variety of Machine Learning (ML) techniques has been used to improve the portability of IE systems to new domains, as in SRV (Freitag, 1998), RAPIER (Calif and Mooney, 1997), LIEP (Huffman, 1996), CRYSTAL (Soderland et al., 1995) and WHISK (Soderland, 1999) . However, some drawbacks remain in the portability of these systems: a) existing systems generally depend on the supported text style and learn IE-rules either for structured texts, semi-structured texts or free text , b) IE systems are mostly single-concept learning systems, c) consequently, an extractor (e.g., a rule set) is learned for each concept within the scenario in an independent manner, d) the order of execution of the learners is set manually, and so are the scheduling and way of combination of the resulting extractors,</context>
</contexts>
<marker>Huffman, 1996</marker>
<rawString>S. Huffman. 1996. Learning information extraction patterns from examples. In S. Wermter, E. Riloff, and G. Sheller, editors, Connectionist, statistical and symbolic approaches to learning for natural language processing. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ko</author>
</authors>
<title>Empirical assembly sequence planning: A multistrategy constructive learning approach.</title>
<date>1998</date>
<booktitle>Machine Learning and Data Mining.</booktitle>
<editor>In I. Bratko R. S. Michalsky and M. Kubat, editors,</editor>
<publisher>John Wiley &amp; Sons LTD.</publisher>
<contexts>
<context position="2711" citStr="Ko, 1998" startWordPosition="441" endWordPosition="442">nd supports insufficient amounts of training corpora. EVIUS is a component of a multilingual IE system, MTURBIO (Turmo et al., 1999). 2 EVIUS. Learning rule sets for a set of related concepts The input of EVIUS is both a partially-parsed semantically-tagged3 training corpus and a description of the desired target structure. This description is provided as a set of concepts C related to a set of asymmetric binary relations, TZ. In order to learn set S of IE rule sets for the whole C, EVIUS uses an MCL approach integrating constructive learning, closed-loop learning and deductive restructuring (Ko, 1998). In this multi-concept situation, the system determines which concepts to learn and, later, incrementally updates S. This can be relatively straightforward when using knowledge about the target structure in a closed-loop learning approach. Starting with C, EVIUS reduces set U of unlearned concepts iteratively by selecting subset P C U formed by the primitive concepts in U and learning a rule set for each c E P 4. For instance, the single colour scenario5 in fig3With EuroWordNet (http://www.hum.uva.nlrewn/) synsets. No attempt has been made to disambiguate such tags. 4No cyclic scenarios are a</context>
</contexts>
<marker>Ko, 1998</marker>
<rawString>H. Ko. 1998. Empirical assembly sequence planning: A multistrategy constructive learning approach. In I. Bratko R. S. Michalsky and M. Kubat, editors, Machine Learning and Data Mining. John Wiley &amp; Sons LTD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Michalshi</author>
</authors>
<title>Towards a unified theory of learning: Multistrategy task-adaptive learning.</title>
<date>1993</date>
<booktitle>Readings in Knowledge Acquisition and Learning.</booktitle>
<editor>In B.G. Buchanan and D. Wilkins, editors,</editor>
<publisher>Morgan Kauffman.</publisher>
<contexts>
<context position="2100" citStr="Michalshi, 1993" startWordPosition="337" endWordPosition="338">pt within the scenario in an independent manner, d) the order of execution of the learners is set manually, and so are the scheduling and way of combination of the resulting extractors, and e) focusing on the training data, the size of available training corpora can be inadequate to accurately learn extractors for all the concepts within the scenario2. 1 http: / /www.muc.saic.com/ 2This is so when dealing with some combinations of text style and domain. This paper describes EVIUS, a multi-concept learning system for free text that follows a multi-strategy constructive learning approach (MCL) (Michalshi, 1993) and supports insufficient amounts of training corpora. EVIUS is a component of a multilingual IE system, MTURBIO (Turmo et al., 1999). 2 EVIUS. Learning rule sets for a set of related concepts The input of EVIUS is both a partially-parsed semantically-tagged3 training corpus and a description of the desired target structure. This description is provided as a set of concepts C related to a set of asymmetric binary relations, TZ. In order to learn set S of IE rule sets for the whole C, EVIUS uses an MCL approach integrating constructive learning, closed-loop learning and deductive restructuring</context>
</contexts>
<marker>Michalshi, 1993</marker>
<rawString>R.S. Michalshi. 1993. Towards a unified theory of learning: Multistrategy task-adaptive learning. In B.G. Buchanan and D. Wilkins, editors, Readings in Knowledge Acquisition and Learning. Morgan Kauffman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Learning logical definitions from relations.</title>
<date>1990</date>
<booktitle>Machine Learning,</booktitle>
<pages>5--239</pages>
<contexts>
<context position="6013" citStr="Quinlan, 1990" startWordPosition="968" endWordPosition="969">ro&amp;quot; (its red colour changes to dark brown) has 6Which is presented here as a partially-parsed tree for simplicity. Figure 2: Restructuring training examples two examples of colour, n3 and n6+n7, being these &amp;quot;rojo&amp;quot; (red) and &amp;quot;marrOn&amp;quot;+&amp;quot;oscuro&amp;quot; (dark brown). No reduction is required by the former. However, the latter example is reduced to node n6&apos;. As a consequence, two new attributes are added to the model: isa_colour(n3) and isa_colour(n6&apos;). This new knowledge will be used to learn the concepts to_change and colour_interval. 3 Rule set learning EVIUS uses FOIL (First-order Induction Learning) (Quinlan, 1990) to build an initial rule set Ro from a set of positive and negative examples. Positive examples E+ can be selected using a friendly environment either as: • test relations: c(AI,A2) where both A1 and A2 are terminal nodes that exactly delimit a text value for c. For instance, both text relations co/our(n3,n3) or colour(n6,n7) in figure 2, or as: • ontology relations: c(Ai,A2,... ,An) where all Ai are terminal nodes which are instances of already learned concepts related to c in the scenario. For instance, the ontology relation to_change(n3,n6&apos;)7 , in the same figure, means that the colour rep</context>
</contexts>
<marker>Quinlan, 1990</marker>
<rawString>J.R. Quinlan. 1990. Learning logical definitions from relations. Machine Learning, 5:239-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
<author>D Fisher</author>
<author>J Aseltine</author>
<author>W Lehnert</author>
</authors>
<title>Crystal: Inducing a conceptual dictionary.</title>
<date>1995</date>
<booktitle>In XIV International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1314--1321</pages>
<contexts>
<context position="1103" citStr="Soderland et al., 1995" startWordPosition="176" endWordPosition="179">has emerged in the past few years in relation to the MUC conferencesl. The aim of an IE system consists in automatically extracting pieces of information from text, being this information relevant for a set of prescribed concepts (scenario). One of the main drawbacks of applying IE systems is the high cost involved in manually adapting them to new domains and text styles. In recent years, a variety of Machine Learning (ML) techniques has been used to improve the portability of IE systems to new domains, as in SRV (Freitag, 1998), RAPIER (Calif and Mooney, 1997), LIEP (Huffman, 1996), CRYSTAL (Soderland et al., 1995) and WHISK (Soderland, 1999) . However, some drawbacks remain in the portability of these systems: a) existing systems generally depend on the supported text style and learn IE-rules either for structured texts, semi-structured texts or free text , b) IE systems are mostly single-concept learning systems, c) consequently, an extractor (e.g., a rule set) is learned for each concept within the scenario in an independent manner, d) the order of execution of the learners is set manually, and so are the scheduling and way of combination of the resulting extractors, and e) focusing on the training d</context>
</contexts>
<marker>Soderland, Fisher, Aseltine, Lehnert, 1995</marker>
<rawString>S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995. Crystal: Inducing a conceptual dictionary. In XIV International Joint Conference on Artificial Intelligence, pages 1314-1321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
</authors>
<title>Learning information extraction rules for semi-structured and free text.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--233</pages>
<contexts>
<context position="1131" citStr="Soderland, 1999" startWordPosition="182" endWordPosition="183">n relation to the MUC conferencesl. The aim of an IE system consists in automatically extracting pieces of information from text, being this information relevant for a set of prescribed concepts (scenario). One of the main drawbacks of applying IE systems is the high cost involved in manually adapting them to new domains and text styles. In recent years, a variety of Machine Learning (ML) techniques has been used to improve the portability of IE systems to new domains, as in SRV (Freitag, 1998), RAPIER (Calif and Mooney, 1997), LIEP (Huffman, 1996), CRYSTAL (Soderland et al., 1995) and WHISK (Soderland, 1999) . However, some drawbacks remain in the portability of these systems: a) existing systems generally depend on the supported text style and learn IE-rules either for structured texts, semi-structured texts or free text , b) IE systems are mostly single-concept learning systems, c) consequently, an extractor (e.g., a rule set) is learned for each concept within the scenario in an independent manner, d) the order of execution of the learners is set manually, and so are the scheduling and way of combination of the resulting extractors, and e) focusing on the training data, the size of available t</context>
</contexts>
<marker>Soderland, 1999</marker>
<rawString>S. Soderland. 1999. Learning information extraction rules for semi-structured and free text. Machine Learning, 34:233-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turmo</author>
<author>N Catala</author>
<author>H Rodriguez</author>
</authors>
<title>An adaptable ie system to new domains.</title>
<date>1999</date>
<journal>Applied Intelligence,</journal>
<pages>10--2</pages>
<contexts>
<context position="2234" citStr="Turmo et al., 1999" startWordPosition="358" endWordPosition="361">ng and way of combination of the resulting extractors, and e) focusing on the training data, the size of available training corpora can be inadequate to accurately learn extractors for all the concepts within the scenario2. 1 http: / /www.muc.saic.com/ 2This is so when dealing with some combinations of text style and domain. This paper describes EVIUS, a multi-concept learning system for free text that follows a multi-strategy constructive learning approach (MCL) (Michalshi, 1993) and supports insufficient amounts of training corpora. EVIUS is a component of a multilingual IE system, MTURBIO (Turmo et al., 1999). 2 EVIUS. Learning rule sets for a set of related concepts The input of EVIUS is both a partially-parsed semantically-tagged3 training corpus and a description of the desired target structure. This description is provided as a set of concepts C related to a set of asymmetric binary relations, TZ. In order to learn set S of IE rule sets for the whole C, EVIUS uses an MCL approach integrating constructive learning, closed-loop learning and deductive restructuring (Ko, 1998). In this multi-concept situation, the system determines which concepts to learn and, later, incrementally updates S. This </context>
<context position="9596" citStr="Turmo et al., 1999" startWordPosition="1589" endWordPosition="1592">examples to each medoid can be selected as negative ones. Distance, in our case, must be defined as multidimensional due to the typology of occurring features. It is relatively easy to define distances between examples for word_X and lemma_X predicates, being 1 when X values are equal, and 0 otherwise. For isa_X predicates, the minimum of all possible conceptual distances (Agirre and Rigau, 1995) between X values in EWN has been used. Greater difficulty is encountered when defining a distance from a morpho-syntactic point of view (e.g., a pronoun seems to be closer to a noun than a verb). In (Turmo et al., 1999), the concept of 5-set has been presented as a syntactic relation generalization, and a distance measure has been based on this concept. 3.2 Creating pseudo-examples A method has been used inspired by the generation of convex pseudo data (Breiman, 1998), in which a similar process to gene-combination in genetic algorithms is used. For each positive example c(Ai, , An)9 of concept c to be dealt with, an attribute vector is defined as (word_XB„... , ,sem_XBTh,context) where B1, , Bn are the unrepeated terminal nodes from A1, , An, context is the set of all predicates subsumed by the syntactico-s</context>
</contexts>
<marker>Turmo, Catala, Rodriguez, 1999</marker>
<rawString>J. Turmo, N. Catala, and H. Rodriguez. 1999. An adaptable ie system to new domains. Applied Intelligence, 10(2/3):225-246.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>