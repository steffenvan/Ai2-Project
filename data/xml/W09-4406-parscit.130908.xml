<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.073421">
<title confidence="0.995289">
Language Independent System for Definition Extraction:
First Results Using Learning Algorithms
</title>
<author confidence="0.993456">
Rosa Del Gaudio Ant´onio Branco
</author>
<affiliation confidence="0.976662">
University of Lisbon
Faculdade de Ciˆencias,Departamento de Inform´atica
</affiliation>
<address confidence="0.831612">
NLX - Natural Language and Speech Group
Campo Grande, 1749-016 Lisbon, Portugal
</address>
<email confidence="0.997369">
rosa@di.fc.ul.pt antonio.branco@di.fc.ul.pt
</email>
<sectionHeader confidence="0.995806" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9944924">
In this paper we report on the performance of dif-
ferent learning algorithms and different sampling
technique applied to a definition extraction task,
using data sets in different language. We com-
pare our results with those obtained by hand-
crafted rules to extract definitions. When Defi-
nition Extraction is handled with machine learn-
ing algorithms, two different issues arise. On the
one hand, in most cases the data set used to ex-
tract definitions is unbalanced, and this means
that it is necessary to deal with this characteris-
tic with specific techniques. On the other hand
it is possible to use the same methods to extract
definitions from documents in different corpus,
making the classifier language independent.
</bodyText>
<sectionHeader confidence="0.991779" genericHeader="keywords">
Keywords
</sectionHeader>
<keyword confidence="0.796739">
machine learning, imbalanced data set, language independent,
definition extraction
</keyword>
<sectionHeader confidence="0.9982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999923444444445">
According to Aristotle, the formal structure of a defini-
tion should resemble an equation with the definiendum
(what is to be defined) on the left hand side and the
definiens (the part which is doing the defining) on the
right hand side. The definiens should consist of two
parts: the genus (the nearest superior concept) and
the differentiae specificae (the distinguishing charac-
teristics). In this way, definitions would adequately
capture the concept to be defined.
In Hebenstreit [9], two more types of definition are
pointed out. Firstly, the definition by enumeration of
the concept species on the same level of abstraction
(extensional definition), e.g. a chess piece is a king,
a queen, a bishop, a knight, a rook or a pawn. Sec-
ondly, the definition by enumeration of the parts of
the concept (partitive definition), e.g. the solar sys-
tem is made of the planets Mercury, Venus, Earth,
Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.
Barnbrook [2] identifies 16 different types of definitions
analysing dictionary entries. In spite of the richness of
this classification, in automatic definition extraction
application only the simplest type is taken in consid-
eration, that is a sentence composed by a subject, a
copular verb and a predicative phrase. In this paper a
definition is a sentence containing an expression (the
definiendum) and its definition (the definiens) con-
nected by the verb ”to be”.
Two different approaches are possible when dealing
with automatic definition extraction. The first one
consists in building a system of rules, based on lexical
and syntactic clues. The second one is to consider the
task as a classification problem, where for each sen-
tence in the corpus it is possible to assign the correct
class. The problem of the first approach is that it is
language dependent, and in case of a large use of lexical
clues, the performance on different corpus get worst.
In the case of classification approach one of the main
issue to be dealt with is the sparseness of definitions
in a corpus. It is a matter of fact that the number
of definition bearing sentences is much lesser than the
number of sentences that are not definitions. This con-
figuration gives rise to an imbalanced data set, which
may present different degrees of imbalance, depending
on the corpus used. For corpus composed mostly by
encyclopedic documents it is likely to get a balanced
data set. For example [8] used a balanced corpus
where the definition-bearing sentences represent 59%
of the whole corpus, while [24] using a corpus consist-
ing of encyclopedic text and web documents reports
that only 18% of the sentences were definitions.
In this work we deal with the problem of imbalanced
data sets in definition extraction tasks in a language
independent way. We show not only that sampling
techniques can improve the performance of classifiers
but also that this improvement is language indepen-
dent. Other researches using learning algorithms re-
lay strongly on lexical and syntactic components as
features to describe the data set. These kinds of fea-
tures are not only language dependent but also domain
dependent, and as we want our classifier to be as gen-
eral as possible we select the most basic features, that
is n-grams of part of speech (POS). This makes the
present approach viable for all those languages that
are not equipped with rich lexical resources as learning
data or in a situation where the domain is too specific
to benefit from such resources, and moves away from
previous works that use features such as words, word
lemmas, position of the sentence in the document he
document, etc. In this paper we apply the same tech-
niques we applied to a Portuguese Corpus in a previous
experiment to a corpus in Dutch and compare results.
Our task handles several aspects that are common to
</bodyText>
<page confidence="0.996486">
33
</page>
<subsubsectionHeader confidence="0.72789">
Workshop On Definition Extraction 2009 - Borovets, Bulgaria, pages 33–39,
</subsubsectionHeader>
<bodyText confidence="0.999708833333333">
different machine learning tasks in NLP application:
small amounts of data, inherent ambiguity (definition
detection is sometimes a matter of judgment), noisy
data (human annotators make mistakes), imbalanced
class distribution, this last aspect being the main issue
addressed in this paper.
</bodyText>
<sectionHeader confidence="0.996688" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999984373333333">
As we said in the previous section there are two main
approach to deal with automatic definition extraction,
the rule based and the classification one. Regarding
the first approach Hearst [11] proposed a method to
identify a set of lexico-syntactic patterns to extract hy-
ponym relations from large corpora and extend Word-
Net with them. This method was adopted by [19] to
cover other types of relations.
DEFINDER [13] is considered a state of the art sys-
tem. It combines simple cue-phrases and structural
indicators introducing the definitions and the defined
term. The corpus used to develop the rules consists
of well-structured medical documents, where 60% of
the definitions are introduced by a set of limited text
markers. The nature of the corpus used can explain
the high performance obtained by this system (87%
precision and 75% recall).
Malaise and colleagues [16] focused their works on
the extraction of definitory expressions containing hy-
peronym and synonym relations from French corpora.
These authors used lexical-syntactic markers and pat-
terns to detect at the same time definitions and rela-
tions. For the two different relations (hyponym and
synonym), they obtained, respectively, 4% and 36%
of recall, and 61% and 66% of precision. Turning
more specifically to the Portuguese language. Pinto
and Oliveira [20] present a study on the extraction of
definitions with a corpus from a medical domain. They
first extract the relevant terms and then extract defi-
nition for each term. An evaluation is carried out for
each term; for each term recall and precision are very
variable ranging between 0% and 100%.
In the last years machine learning techniques were
combined with pattern recognition in order to improve
the general results. In particular, [8] used a maximum
entropy classifier to extract definition in order to dis-
tinguish actual definitions from other sentences. As
attributes to classify definition sentences they used-
such as n-gram and bag-of-words, sentence position,
syntactic properties and named entity classes. The
corpus used was composed by medical pages of Dutch
Wikipedia, where they extracted sentences based on
syntactic features. The data set were composed by
2,299 senteces of which 1,366 actual definitions. This
gives an initial accuracy of 59%, that was improved
with machine learning algorithms until 92.21%
In [6], it is presented a system to extract defini-
tion from off-line documents. They experimented with
three different algorithms, namely NaiveBayse, Deci-
sion Tree and Support Vector Machine (SVM), obtain-
ing the best score with SVM with a a F-measure of 0.83
with a balanced data set.
In [26] they combine syntactic patterns with a Naive
Bayes classification algorithm with the aim of ex-
tracting glossaries from tutorial documents in Dutch.
They use several properties and several combination of
them, obtaining an improvement of precision of 51.9%
but a decline in the recall of 19.1% in comparison with
a the syntactic pattern system developed previously by
the authors using the same corpus.
Recently, some authors have started to look at this
problem of imbalanced data set in the context of def-
inition extraction. In particular, [21] down-sampled
their corpus using different ratios (1:1, 1:5, 1:10) in
order to seek for best results. The corpus they used
presented an original ratio of non-definitions to defini-
tions of about 19. Although they obtained some im-
provement in terms of F-measure, in particular with
the ratio 1 to 5, they cannot improve results obtained
with a rule based grammar previously developed us-
ing the same corpus. These authors also investigated
the use of Balanced Random Forest algorithm in order
to deal with this imbalance, succeeding in outperform
the rule based grammar previously developed of 5 per-
centage points [14].
</bodyText>
<sectionHeader confidence="0.996339" genericHeader="method">
3 Corpora
</sectionHeader>
<bodyText confidence="0.99997841025641">
All the two corpora used for experiments were col-
lected in the context of the LT4eL project 1. They
were used to develop different tools, such a key-word
extractor, a glossary candidate detector and an on-
tology, in order to support e-learning activities[1] in
a multi-language context. The corpora are encoded
with a common XML format. The DTD of this for-
mat is conforming to a DTD derived from the XCE-
SAna DTD, a standard for linguistically annotated
corpora [18]. Definition-bearing sentences were man-
ually annotated. In each sentence, the term defined,
the definition and the connection verb were annotated
using a different XML tag.
The Dutch Corpus is composed by 26 tutorials with
a size of about 350,000 tokens. The corpus was anno-
tated part-of-speech information and morphosyntactic
features with the Wotan tagger and with lemmatiza-
tion information with the CGN lemmatizer (for more
information about this corpus see [26].
The Portuguese Corpus is composed by 23 tutorials
and scientific papers in the field of Information Tech-
nology and has a size of 274,000 tokens. It was then
automatically annotated with morpho-syntactic infor-
mation using the LX-Suite [23] a set of tools for the
shallow processing of Portuguese with state of the art
performance.
In order to prepare the data set for to be used in
our experiments a simple grammar for each language
was create that extracts all the sentences where the
verb ”to be” appears as the main verb. For Dutch we
obtained a sub-corpus composed by 4,829, 120 of which
are definitions, with a ratio of 39:1. For Portuguese we
obtained a sub-corpus composed by 1,360 sentences,
121 of which are definitions, with a ratio of about 10:1.
Commonly used features are: bag-of-word, n-
grams [17] (either of part-of-speech or of base forms),
the position of the definition inside the document [12],
the presence of determiners in the definiens and in the
definiendum [8]. Other relevant properties can be the
</bodyText>
<footnote confidence="0.946274">
1 www.lt4el.eu
</footnote>
<page confidence="0.997777">
34
</page>
<bodyText confidence="0.9995965">
presence of named entities [8] or data from en external
source such as encyclopedic data, wordnet, etc. [22].
Some features work well with a corpus but not so
well in a different corpus, resulting in the impossibil-
ity to use the learner with different corpora. The use
of the position of a definition-bearing sentence in [8]
is an example of a feature that is corpus dependent.
The same issue arise when lexical information is used
as feature. In order to avoid such limitation we rep-
resented instances as n-grams of POS. From both the
corpora the 100 most frequent n-grams were extracted
and were used as features. Each sentence was rep-
resented as an array where cells record the number
of occurrences of these n-grams. In this paper, for
question of space, only results obtained with the best
representation are showed, that is with bi-grams.
</bodyText>
<sectionHeader confidence="0.99683" genericHeader="method">
4 Machine Learning Algorithms
</sectionHeader>
<bodyText confidence="0.996671875">
Five different algorithms were used: C4.5, Random
Forest, Naive Bayes, k-NN, SVM. The reason that mo-
tivated this choice is twofold: we want to cover differ-
ent class of algorithms and we want to use algorithms
representing the state of the art for definition extrac-
tion.
C4.5 and Random Forest are two decision tree algo-
rithms. The first is a relatively simple algorithm that
splits the data into smaller subsets using the informa-
tion gain in order to chose the attribute for splitting
the data. The second is a classifier consisting of a col-
lection of decision trees. For each tree, it is selected
a random sample of the data set (the remaining is
used for error estimation) and for each node of the
tree, the decision at that node is based on a restricted
number of variables. Regarding C4.5, different con-
figuration were tested: reduced-error pruning instead
of C.4.5 pruning, pruned and unpruned option, and
with or without Laplace smoothing. Regarding Ran-
dom Forest, we experimented with different numbers
of randomly chosen attributes.
Naive Bayes is a simple probabilistic classifier that
is very popular in natural language application. In
spite of its simplicity, it permit to obtain results simi-
lar to the results obtained with more complex algo-
rithms. Two different implementation were tested:
one in which the numeric estimator precision values
are chosen using a kernel estimator for numeric at-
tributes and another using a normal distribution.
The k-NN algorithm is a type of instance-based
learning, also called lazy learning because, differently
from algorithms above, the training phase of the al-
gorithm consists only in storing the feature vectors
and class labels of the training samples and all com-
putation is deferred until the classification phase. In
this phase, it computes the distance between the tar-
get sample and n samples in the data set, assining the
most frequent class. Two different K nearest neighbors
classifiers were constructed, with k equal to 1 and to
3.
SVM is a classifier that tries to find an optimal hy-
perplane that correctly classifies data points as much
as possible and separate the point of two classes as far
as possible. In this experiment four different classifiers
were implemented, using four different kernels, linear,
polynominal, radial and sigmoid.
Weka workbench [27] was used to build all the learn-
ers.
</bodyText>
<sectionHeader confidence="0.938405" genericHeader="method">
5 Sampling Techniques
</sectionHeader>
<bodyText confidence="0.990109633333333">
In many real-world classification applications, most of
the examples are from one of the classes, while the
minority class is the interesting one. As most of the
learning algorithms are designed to maximize accu-
racy, the imbalance in the class distribution leads to
a poor performance of these algorithms. The issue is
therefore how to improve the classification of the mi-
nority class examples. A common solution is to sample
the data, either randomly or intelligently, to obtain an
altered class distribution.
Random over-sampling consists of random replica-
tion of minority class examples, while in random down-
sampling majority class example are randomly dis-
carded until the desired amount is reached. These
two very simple methods are often criticized due to
their drawbacks. Several authors pointed out that the
problem with under-sampling is that this method can
discard potentially useful data that could be impor-
tant for the induction process. On the other hand,
Random over-sampling can increase the likelihood of
overfitting, since it makes exact copies of the minority
class examples.
When speaking about negative and positive example
in a dataset, it is important to have in mind that not all
the examples have the same value. There are examples
that are more prototypical than others and represent
better the class to which they belong, others are too
similar to be useful, and others are just noise.
It is possible to divide examples in four different
classes:
</bodyText>
<listItem confidence="0.989882">
• Noise examples - examples that are incorrectly
classified
• Borderline examples - dangerous since a small
amount of noise can make them fall on the wrong
side of the decision border.
• Redundant examples - too similar to other exam-
ples to be useful.
• Safe examples - examples that fit perfectly the
class to which they belong.
</listItem>
<bodyText confidence="0.9934152">
Building on these considerations, several methods
were proposed in order to retain safe examples in the
re-balanced data set. We present here two of such
methods, namely the Condensed Nearest Neighbour
Rule and Tomek Link algorithm.
Condensed Nearest Neighbor Rule [10] finds a con-
sistent subset of examples in order to eliminate the ex-
amples from the majority class that are distant from
the decision border, since these examples might be
considered less relevant for learning. A subset E&apos; C E
is consistent with E if using a 1-nearest neighbor,E&apos;
correctly classies the examples in E. First, it randomly
draw one majority class example and all examples from
the minority class and put these examples in E&apos;. Next,
it uses a 1-NN over the examples in E&apos; to classify the
</bodyText>
<page confidence="0.996568">
35
</page>
<bodyText confidence="0.999914552631579">
examples in E. Every misclassified example from E is
moved to E&apos;. It is important to note that this proce-
dure does not find the smallest consistent subset from
E. The CNN is sensitive to noise and noisy examples
are likely to be misclassified as many of them will be
added to the training set.
Tomek links [25] removes both noise and border-
line examples. Tomek links are pairs of instances of
di?erent classes that have each other as their nearest
neighbors. Given two examples x and y belonging to
different classes, and d(x, y) the distance between x
and y, a (x, y) pair is called a Tomek link if there
is not an example z such that d(x, z) &lt; d(x, y) or
d(y, z) &lt; d(x, y). If two examples form a Tomek link,
then either one of these examples is noise or both ex-
amples are border-line. As an under-sampling method,
only examples belonging to the majority class are elim-
inated. The major drawback of Tomek Link under-
sampling is that this method can discard potentially
useful data that could be important for the induction
process. This method has an higher order computa-
tional complexity and will run slower than other algo-
rithms.
While the previous methods are intelligent down
sampling techniques, SMOTE is an over-sampling
method that produces new synthetic minority class ex-
amples. SMOTE [7] forms new minority class exam-
ples by interpolating between several minority class ex-
amples that lie together in ”feature space” rather than
”data space”. For each minority class example, this al-
gorithm introduces synthetic examples along the line
segments joining any/all of the k minority class near-
est neighbors (in this work k is equal to 3). Synthetic
samples are produced taking the difference between
the feature vector (sample) under consideration and
its nearest neighbors. The difference is multiplied by
a random number between 0 and 1 and added to the
feature vector under consideration.
</bodyText>
<sectionHeader confidence="0.992819" genericHeader="method">
6 Evaluation Issues
</sectionHeader>
<bodyText confidence="0.999813">
One of the most used metric is the Error Rate, defined
as 1.0- (True Positive+True Negative)/(True Positive-
False Positive+False Negative+True Negative). How-
ever using this metric implies that the class distribu-
tion is known and fixed, an assumption that does not
hold in real world applications as the one proposed
here. Moreover, Error Rate is biased to favor the ma-
jority class, making it a bad choice when evaluating
the effects of class distribution. Other aspect against
the use of Error Rate is that it considers different clas-
sification errors as equally important, and in domains
such medical diagnosis, the error of diagnosing a sick
patience as healthy is a fatal error while the contrary
is considered a much less serious error. This means
that a metric such as Error Rate is sensitive to class
imbalance.
It is possible to derive metrics that are not sensitive
to the skew of the data. In particular, four metrics are
proposed in [4]:
</bodyText>
<listItem confidence="0.957685333333333">
• False Negative rate: F N /(T P +F N) - the per-
centage of positive examples misclassified as be-
longing to the negative class
• False Positive rate: F P /(F P +T N) - the per-
centage of negative examples misclassified as be-
longing to the positive class
• True Negative rate: T N /(F P +T N) - the per-
centage of negative examples correctly classified
as belonging to the negative class
• False Positive rate: T P /(T P +F N) - the per-
centage of positive examples correctly classified as
belonging to the positive class
</listItem>
<bodyText confidence="0.999972848484849">
A good classifier should try to minimize FN and
FP rates, and maximize TN and TP rates. Unfortu-
nately, there is a tradeoff between these two metrics,
and in order to analyze this relationship ROC graphs
are used. ROC graphs are two-dimensional graphs
where TP rate is plotted on the Y axis and FP rate
is plotted on the X axis. ROC graphs are consistent
for a given problem even if the distribution of positive
and negative instances is highly skewed.
It is important to notice that the lower left point
(0, 0) represents the strategy of never issuing a pos-
itive classification: such a classifier produces no false
positive errors but also gains no true positives. The
opposite strategy, of unconditionally issuing positive
classifications, is represented by the upper right point
(1, 1).
In order to compare classifiers, it is possible to re-
duce a ROC curve to a scalar value representing the
performance of the classifier. Area Under the ROC
(AUC) is a portion of the area of the unit square.
Its value will always be between 0 and 1. However,
because random guessing produces the diagonal line
between(0,0) and(1,1), which has an area of 0.5, no
realistic classier should have an AUC less than 0.5.
The AUC is equivalent to the Wilcoxon test of ranks
and it is also related to Gini coefficient (for an exhaus-
tive description of ROC and AUC in assessing machine
learning algorithms see [5]) . In this work, we will use
the AUC measure in order to assess the performance of
classifiers. Furthermore, for each classifier, we present
also the F-measure in order to compare our results to
previous works in this area. F-measure is a combina-
tion of Recall and Precision metrics:
</bodyText>
<equation confidence="0.9727045">
F − measure = 2*Precision*Recall
(Precision+Recall)
</equation>
<sectionHeader confidence="0.991693" genericHeader="evaluation">
7 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999982866666667">
In this section, we show the results obtained with the
different learning algorithms and with the different
sampling techniques used for both corpora. We also
present results obtained using the original data set,
which is the data set with the original imbalance. This
result represents our base line against which results
obtained with sampled data sets are to be compared
with. Values in bold represent the best score for each
classifier.
Tables 1 and 2 display the performance of the two
classifiers using k-NN algorithm. In particular Table1
reports on the results of the most basic implementation
of k-NN, that is with k equal to 1 (1-NN). In this case
a test example is simply assigned to the class of its
nearest neighbor. Table 2 displays results obtained by
</bodyText>
<page confidence="0.995927">
36
</page>
<table confidence="0.999115888888889">
1-NN
P T D U
Sampling F-m AUC F-m AUC
Original 0.19 0.56 0.06 0.55
Dowsampling 0.62 0.57 0.57 0.55
Oversampling 0.36 0.55 0.18 0.52
SMOTE 0.63 0.66 0.40 0.70
CNN 0.23 0.52 0.56 0.54
Tomek 0.57 0.59 0.35 0.56
</table>
<tableCaption confidence="0.998004">
Table 1: Results using k-NN algorithm with k=1
</tableCaption>
<table confidence="0.999779222222222">
3-NN
P T D U
Sampling F-m AUC F-m AUC
Original 0.17 0.57 0.20 0.51
Dowsampling 0.62 0.59 0.59 0.61
Oversampling 0.51 0.58 0.33 0.56
SMOTE 0.66 0.70 0.42 0.74
CNN 0.65 0.61 0.57 0.55
Tomek 0.64 0.66 0.28 0.63
</table>
<tableCaption confidence="0.99476">
Table 2: Results using k-NN algorithm with k=3
</tableCaption>
<bodyText confidence="0.998478625">
a classifier using a k-NN algorithm with k equal to 3
(3-NN).
Regarding the results obtained with the algorithm
1-NN in Table 1, it is interesting to notice that, for
the AUC metric, only the SMOTE sampling technique
is able to significantly improve the base line for both
corpora. For the Portuguese corpus there is an im-
provement of 10 points while for the Dutch corpus the
improvement is even greater, reaching 15 points. The
situation is slightly different for the F-measure. In this
case, the best result is obtained by SMOTE for the
Portuguese and by down sampling for Dutch. Results
obtained with the 3-NN algorithm are very similar to
those obtained with the 1-NN in terms of which sam-
pling technique shows the greater improvements. It
is worthwhile to notice that although the base lines
for the above classifiers are very similar, they differ
in the way they respond to the sampling techniques.
In particular the 3-NN algorithm seems to take more
advantage from the use of sampling, since it obtains
better results in all the experiments and for both lan-
guages.
The results displayed in Table 3 refer to the best set-
ting for the C4.5 classifier, where the tree was pruned
using the C4.5s standard pruning procedure and no
Laplace correction. Regarding Table 4, the classifier
was built using 10 different trees. For both corpora
SMOTE sampling method presents the best results
in terms of AUC and F-measure, but in the case of
Dutch the improvement regarding the base line was
much greater in comparison with the improvement for
Portuguese. Even if the base Iine for Dutch was worst
at the end the it outperformed results obtained with
the Portuguese corpus. The same observation holds
for results present in Table 4.
Table 5 displays results obtained with a SVM clas-
sifier using a sigmoid kernel. The AUC base line for
this classifier is very low, with a value below or equal
to 0.5. With the use of sampling techniques the per-
formance of this classifier is comparable to the 1-NN.
</bodyText>
<table confidence="0.997379666666667">
C4.5
P T D U
Sampling F-m AUC F-m AUC
Original 0.17 0.65 0.09 0.49
Dowsampling 0.58 0.59 0.66 0.67
Oversampling 0.37 0.67 0.25 0.65
SMOTE 0.77 0.87 0.81 0.91
CNN 0.62 0.61 0.55 0.56
Tomek 0.63 0.60 0.58 0.63
</table>
<tableCaption confidence="0.983766">
Table 3: Results using C4.5 algorithm
</tableCaption>
<table confidence="0.999818222222222">
Random Forest
P T D U
Sampling F-m AUC
Original 0.13 0.65 0.02 0.56
Dowsampling 0.57 0.65 0.61 0.69
Oversampling 0.21 0.64 0.02 0.64
SMOTE 0.75 0.94 0.77 0.96
CNN 0.59 0.66 0.58 0.58
Tomek 0.65 0.59 0.61 0.73
</table>
<tableCaption confidence="0.998668">
Table 4: Results using Random Forest algorithm
</tableCaption>
<bodyText confidence="0.999775076923077">
Although SVM is a complex algorithm, it achieves a
performance similar to the simplest algorithm used in
this work, namely 1-NN. Furthermore it is the only
classifier where the SMOTE does not show the best
result, considering either AUC or F-measure.
The results in Table 6 refer to a Naive Bayes classi-
fier using normal distribution. As for the previous al-
gorithm (except for SVM), the best results is obtained
with the SMOTE technique, but there is a difference
between the two corpora. For the Portuguese data set
the base line is higher than for the other classifiers
in terms of both metrics taken in consideration, but
the improvements achieved with the use of sampling
do not outperform the performance of other classifiers,
namely C4.5 and Random Forest. On the other hand,
for the Dutch data set the best results are obtained
with Naive Bayes even if the initial base line is sim-
ilar to that obtained with 3-NNm atleast regarding
F-measure.
In general for both the languages, the SMOTE sam-
pling technique shows the best results in terms of
AUC, followed by Tomek Link and Random over-
sampling. These results are comparable with those
reported in the literature on imbalanced data sets in
general. In a comprehensive study on the behavior of
several methods for balancing training data, using 11
</bodyText>
<table confidence="0.992635333333333">
SVM
P T D U
Sampling F-m AUC F-m AUC
Original 0.12 0.48 0.02 0.50
Dowsampling 0.67 0.68 0.65 0.65
Oversampling 0.61 0.59 0.60 0.64
SMOTE 0.60 0.60 0.32 0.59
CNN 0.59 0.57 0.61 0.59
Tomek 0.64 0.49 0.63 0.66
</table>
<tableCaption confidence="0.999293">
Table 5: Results using SVM algorithm
</tableCaption>
<page confidence="0.964939">
37
</page>
<table confidence="0.999797222222222">
Naive Bayes
P T D U
Sampling F-m AUC
Original 0.24 0.66 0.12 0.75
Dowsampling 0.62 0.62 0.70 0.72
Oversampling 0.67 0.68 0.68 0.75
SMOTE 0.72 0.76 0.95 0.97
CNN 0.64 0.63 0.66 0.69
Tomek 0.69 0.72 0.67 0.77
</table>
<tableCaption confidence="0.999632">
Table 6: Results using Native Bayes
</tableCaption>
<bodyText confidence="0.999924285714286">
UCI data sets 2, Batista and colleagues [4] show that
in most cases and with several data sets in different
domains SMOTE and Random over-sampling are the
most effective methods. In general, they lead to a rise
in the AUC metric of few percentage points (1 to 4),
when the base line was already high (more than 0.65),
while where the base line was under this value the im-
provement was comparable to the one obtained in our
work. In particular for the flag data set, they obtained
an improvement of 34 percentage points.
Focusing on Natural Language applications [15] ap-
ply these methods to sentence boundary detection in
speech, showing that SMOTE and down-sampling get
the best results with an AUC of 0.89 (the base line
being 0.80). However, they did not experiment intelli-
gent down-sampling methods such as CNN or Tomek
Link. Batista in [3] gets the best results in terms of
AUC with an improvement of 4 percentage points on
the original data set using a combination of SMOTE
with Tomek link, followed by simple SMOTE, in a case
study on automated annotation of keywords.
In our case the improvement regarding the origi-
nal data set is between 10 and 29 percentage points,
demonstrating how these methods can be effective in
this application.
Regarding the comparison with other work in defi-
nition extraction, the improvement obtained on the F-
measure, with the best result of 0.77 with C4.5 classi-
fier, outperforms most of the systems using learning al-
gorithms, confirming the importance of sampling tech-
niques in supporting definition extraction tasks. [26],
using the same corpus we used, reports on a F-measure
of 0.73, obtained with a combination of syntactic rules
and a Naive Bayes classifiers for Dutch while [21], with
a similar approach, but for the Polish language, obtain
a F-measure of 0.35. Furthermore in all these works
a combination of features are used in order to reach
best results, while in this paper we only use bi-grams
of POS as features. To conclude, our results are com-
parable with systems that represent the state of the
art in the area, such as DEFINDER, which shows a
F-mesure of 0.80.
</bodyText>
<sectionHeader confidence="0.991025" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.997128">
In this paper we have compared the performance of dif-
ferent learning algorithms and different sampling tech-
nique on a definition extraction task, using data sets
in different language. Results presented show that this
</bodyText>
<footnote confidence="0.630531">
2 http://archive.ics.uci.edu/ml/
</footnote>
<bodyText confidence="0.998353125">
approach can be very effective in comparison to hand-
crafted rule to extract definitions, in terms of amount
of time and performance. Furthermore techniques here
presented are language and domain independent, mak-
ing them a interesting resource in the field of Question
Answering. Next steps in our researches will be inte-
grate our classifier in a QA system in order to test this
results in a much real world context.
</bodyText>
<sectionHeader confidence="0.997772" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999969393442623">
[1] M. Avel˜as, A. Branco, R. D. Gaudio, and P. Martins. Sup-
porting e-learning with language technology for portuguese. In
Proceedings of the International Conference on the Compu-
tational Processing of Portuguese (PROPOR2008). Springer,
2008.
[2] G. Barnbrook. Defining Language: a local grammar of defi-
nition sentences. John Benjamins Publishing Company, 2002.
[3] G. E. A. P. A. Batista, A. L. C. Bazzan, and M. C. Monard.
Balancing training data for automated annotation of keywords:
a case study, 2003.
[4] G. E. A. P. A. Batista, R. C. Prati, and M. C. Monard. A
study of the behavior of several methods for balancing machine
learning training data. SIGKDD Explor. Newsl., 6(1):20–29,
2004.
[5] A. P. Bradley. The use of the area under the roc curve in the
evaluation of machine learning algorithms. Pattern Recogni-
tion, 30:1145–1159, 1997.
[6] X. Chang and Q. Zheng. Offline definition extraction using
machine learning for knowledge-oriented question answering.
In D.-S. Huang, L. Heutte, and M. Loog, editors, ICIC (3),
volume 2 of Communications in Computer and Information
Science, pages 1286–1294. Springer, 2007.
[7] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
Kegelmeyer. Smote: Synthetic minority over-sampling tech-
nique. Journal of Artificial Intelligence Research, 16:321–357,
2002.
[8] I. Fahmi and G. Bouma. Learning to identify definitions us-
ing syntactic feature. In R. Basili and A. Moschitti, editors,
Proceedings of the EACL workshop on Learning Structured
Information in Natural Language Applications, Trento, Italy,
2006.
[9] H. Gernot. Defining patterns in translation studies: Revisiting
two classics of german translation. Translationswissenschaft
in Target, 19(2):197–215, 2007.
[10] P. E. Hart. The condensed nearest neighbor rule (corresp.).
Information Theory, IEEE Transactions on, 14(3):515–516,
May 1968.
[11] M. A. Hearst. Automatic acquisition of hyponyms from large
text corpora. In Proceedings of the 14th conference on Com-
putational linguistics, pages 539–545, Morristown, NJ, USA,
1992. Association for Computational Linguistics.
[12] H. Joho and M. Sanderson. Retrieving descriptive phrases from
large amounts of free text. In Proceeding of hte 9th inter-
national conference on Information and knowledge manage-
ment, pages 180–186, 2000.
[13] J. Klavans and S. Muresan. Evaluation of the DEFINDER sys-
tem for fully automatic glossary construction. In Proceedings
of the American Medical Informatics Association Symposium
(AMIA 2001), 2001.
[14] L. Kobylinski and A. Przepiorkowski. Definition extraction
with balanced random forests. In A. Ranta, editor, Go-
TAL 2008, pages 237–247, Gothenburg, 2008. Springer-Verlag
Berlin Heidelberg.
[15] Y. Liu, N. V. Chawla, M. P. Harper, E. Shriberg, and A. Stol-
cke. A study in machine learning from imbalanced data for
sentence boundary detection in speech. Computer Speech &amp;
Language, 20(4):468–494, 2006.
[16] V. Malais, P. Zweigenbaum, and B. Bachimont. Detecting se-
mantic relations between terms in definitions. In the 3rd edi-
tion of CompuTerm Workshop (CompuTerm 2004) at Coling
2004, pages 55–62, 2004.
</reference>
<page confidence="0.985266">
38
</page>
<reference confidence="0.999923325">
[17] S. Miliaraki and I. Androutsopoulos. Learning to identify
single-snippet answer to definition questions. In Proceeding
of the 20th International Conference on Computational Lin-
guistic (COLING 2004), pages 1360–1366, Geneva, Switzer-
land, 2004.
[18] I. N. and S. K. Xml, corpus encoding standard, document xces
0.2. Technical report, Department of Computer Science, Vassar
College and Equipe Langue ed Dialogue, New York, USA and
LORIA/CNRS, Vandouvre-les-Nancy, France, 2002.
[19] J. Person. The expression of definitions in specialised text:
a corpus-based analysis. In M. Gellerstam, J. Jaborg, S. G.
Malgren, K. Noren, L. Rogstrom, and C. Papmehl, editors,
7th Internation Congress on Lexicography (EURALEX 96),
pages 817–824, Goteborg, Sweden, 1996.
[20] A. S. Pinto and D. Oliveira. Extra¸cc˜ao de defini¸c˜oes no
Corp´ografo. Technical report, Faculdade de Letras da Uni-
versidade do Porto, 2004.
[21] A. Przepiorkowski, M. Marcinczuk, and L. Degorski. Noisy
and imbalanced data: Machine learning or manual grammars?
In Text, Speech and Dialogue: 9th International Conference,
TSD 2008, Brno, Czech Republic, September 2008. Lecture
Notes in Artificial Intelligence, Berlin, Springer-Verlag.
[22] H. Saggion. Identifying definitions in text collections for ques-
tion answering. In LREC 2004, 2004.
[23] J. R. Silva. Shallow processing of Portuguese: From sentence
chunking to nominal lemmatization. Master’s thesis, Universi-
dade de Lisboa, Faculdade de Ciˆencias, 2007.
[24] E. Tjong, K. Sang, G. Bouma, and M. de Rijke. Developing
offline strategies for answering medical questions. In Proceed-
ings of the AAAI-05 workshop on Question Answering in
restricted domains, pages 41–45, 2005.
[25] I. Tomek. Two modifications of cnn. Systems, Man and Cy-
bernetics, IEEE Transactions on, 6(11):769–772, November
1976.
[26] E. Westerhout and P. Monachesi. Extraction of Dutch defin-
itory contexts for elearning purposes. In CLIN proceedings
2007, 2007.
[27] I. H. Witten and E. Frank. Data Mining: Pratical Machine
Learning Tools and Techniques (Second Edition). Morgan
Kaufmann, 2005.
</reference>
<page confidence="0.999531">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.225235">
<title confidence="0.999332">Language Independent System for Definition Extraction: First Results Using Learning Algorithms</title>
<author confidence="0.99989">Rosa Del Gaudio Ant´onio</author>
<affiliation confidence="0.998306">University of</affiliation>
<title confidence="0.734678">Faculdade de Ciˆencias,Departamento de NLX - Natural Language and Speech</title>
<address confidence="0.419413">Campo Grande, 1749-016 Lisbon,</address>
<abstract confidence="0.9999303125">In this paper we report on the performance of different learning algorithms and different sampling technique applied to a definition extraction task, using data sets in different language. We compare our results with those obtained by handcrafted rules to extract definitions. When Definition Extraction is handled with machine learning algorithms, two different issues arise. On the one hand, in most cases the data set used to extract definitions is unbalanced, and this means that it is necessary to deal with this characteristic with specific techniques. On the other hand it is possible to use the same methods to extract definitions from documents in different corpus, making the classifier language independent.</abstract>
<keyword confidence="0.957564">Keywords machine learning, imbalanced data set, language independent,</keyword>
<intro confidence="0.948209">definition extraction</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Avel˜as</author>
<author>A Branco</author>
<author>R D Gaudio</author>
<author>P Martins</author>
</authors>
<title>Supporting e-learning with language technology for portuguese.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on the Computational Processing of Portuguese (PROPOR2008).</booktitle>
<publisher>Springer,</publisher>
<contexts>
<context position="9422" citStr="[1]" startWordPosition="1512" endWordPosition="1512">the ratio 1 to 5, they cannot improve results obtained with a rule based grammar previously developed using the same corpus. These authors also investigated the use of Balanced Random Forest algorithm in order to deal with this imbalance, succeeding in outperform the rule based grammar previously developed of 5 percentage points [14]. 3 Corpora All the two corpora used for experiments were collected in the context of the LT4eL project 1. They were used to develop different tools, such a key-word extractor, a glossary candidate detector and an ontology, in order to support e-learning activities[1] in a multi-language context. The corpora are encoded with a common XML format. The DTD of this format is conforming to a DTD derived from the XCESAna DTD, a standard for linguistically annotated corpora [18]. Definition-bearing sentences were manually annotated. In each sentence, the term defined, the definition and the connection verb were annotated using a different XML tag. The Dutch Corpus is composed by 26 tutorials with a size of about 350,000 tokens. The corpus was annotated part-of-speech information and morphosyntactic features with the Wotan tagger and with lemmatization information</context>
</contexts>
<marker>[1]</marker>
<rawString>M. Avel˜as, A. Branco, R. D. Gaudio, and P. Martins. Supporting e-learning with language technology for portuguese. In Proceedings of the International Conference on the Computational Processing of Portuguese (PROPOR2008). Springer, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Barnbrook</author>
</authors>
<title>Defining Language: a local grammar of definition sentences.</title>
<date>2002</date>
<publisher>John Benjamins Publishing Company,</publisher>
<contexts>
<context position="2114" citStr="[2]" startWordPosition="324" endWordPosition="324"> (the distinguishing characteristics). In this way, definitions would adequately capture the concept to be defined. In Hebenstreit [9], two more types of definition are pointed out. Firstly, the definition by enumeration of the concept species on the same level of abstraction (extensional definition), e.g. a chess piece is a king, a queen, a bishop, a knight, a rook or a pawn. Secondly, the definition by enumeration of the parts of the concept (partitive definition), e.g. the solar system is made of the planets Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto. Barnbrook [2] identifies 16 different types of definitions analysing dictionary entries. In spite of the richness of this classification, in automatic definition extraction application only the simplest type is taken in consideration, that is a sentence composed by a subject, a copular verb and a predicative phrase. In this paper a definition is a sentence containing an expression (the definiendum) and its definition (the definiens) connected by the verb ”to be”. Two different approaches are possible when dealing with automatic definition extraction. The first one consists in building a system of rules, ba</context>
</contexts>
<marker>[2]</marker>
<rawString>G. Barnbrook. Defining Language: a local grammar of definition sentences. John Benjamins Publishing Company, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E A P A Batista</author>
<author>A L C Bazzan</author>
<author>M C Monard</author>
</authors>
<title>Balancing training data for automated annotation of keywords: a case study,</title>
<date>2003</date>
<contexts>
<context position="28515" citStr="[3]" startWordPosition="4776" endWordPosition="4776">entage points (1 to 4), when the base line was already high (more than 0.65), while where the base line was under this value the improvement was comparable to the one obtained in our work. In particular for the flag data set, they obtained an improvement of 34 percentage points. Focusing on Natural Language applications [15] apply these methods to sentence boundary detection in speech, showing that SMOTE and down-sampling get the best results with an AUC of 0.89 (the base line being 0.80). However, they did not experiment intelligent down-sampling methods such as CNN or Tomek Link. Batista in [3] gets the best results in terms of AUC with an improvement of 4 percentage points on the original data set using a combination of SMOTE with Tomek link, followed by simple SMOTE, in a case study on automated annotation of keywords. In our case the improvement regarding the original data set is between 10 and 29 percentage points, demonstrating how these methods can be effective in this application. Regarding the comparison with other work in definition extraction, the improvement obtained on the Fmeasure, with the best result of 0.77 with C4.5 classifier, outperforms most of the systems using </context>
</contexts>
<marker>[3]</marker>
<rawString>G. E. A. P. A. Batista, A. L. C. Bazzan, and M. C. Monard. Balancing training data for automated annotation of keywords: a case study, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E A P A Batista</author>
<author>R C Prati</author>
<author>M C Monard</author>
</authors>
<title>A study of the behavior of several methods for balancing machine learning training data.</title>
<date>2004</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="19832" citStr="[4]" startWordPosition="3253" endWordPosition="3253">is biased to favor the majority class, making it a bad choice when evaluating the effects of class distribution. Other aspect against the use of Error Rate is that it considers different classification errors as equally important, and in domains such medical diagnosis, the error of diagnosing a sick patience as healthy is a fatal error while the contrary is considered a much less serious error. This means that a metric such as Error Rate is sensitive to class imbalance. It is possible to derive metrics that are not sensitive to the skew of the data. In particular, four metrics are proposed in [4]: • False Negative rate: F N /(T P +F N) - the percentage of positive examples misclassified as belonging to the negative class • False Positive rate: F P /(F P +T N) - the percentage of negative examples misclassified as belonging to the positive class • True Negative rate: T N /(F P +T N) - the percentage of negative examples correctly classified as belonging to the negative class • False Positive rate: T P /(T P +F N) - the percentage of positive examples correctly classified as belonging to the positive class A good classifier should try to minimize FN and FP rates, and maximize TN and TP </context>
<context position="27715" citStr="[4]" startWordPosition="4637" endWordPosition="4637">the behavior of several methods for balancing training data, using 11 SVM P T D U Sampling F-m AUC F-m AUC Original 0.12 0.48 0.02 0.50 Dowsampling 0.67 0.68 0.65 0.65 Oversampling 0.61 0.59 0.60 0.64 SMOTE 0.60 0.60 0.32 0.59 CNN 0.59 0.57 0.61 0.59 Tomek 0.64 0.49 0.63 0.66 Table 5: Results using SVM algorithm 37 Naive Bayes P T D U Sampling F-m AUC Original 0.24 0.66 0.12 0.75 Dowsampling 0.62 0.62 0.70 0.72 Oversampling 0.67 0.68 0.68 0.75 SMOTE 0.72 0.76 0.95 0.97 CNN 0.64 0.63 0.66 0.69 Tomek 0.69 0.72 0.67 0.77 Table 6: Results using Native Bayes UCI data sets 2, Batista and colleagues [4] show that in most cases and with several data sets in different domains SMOTE and Random over-sampling are the most effective methods. In general, they lead to a rise in the AUC metric of few percentage points (1 to 4), when the base line was already high (more than 0.65), while where the base line was under this value the improvement was comparable to the one obtained in our work. In particular for the flag data set, they obtained an improvement of 34 percentage points. Focusing on Natural Language applications [15] apply these methods to sentence boundary detection in speech, showing that S</context>
</contexts>
<marker>[4]</marker>
<rawString>G. E. A. P. A. Batista, R. C. Prati, and M. C. Monard. A study of the behavior of several methods for balancing machine learning training data. SIGKDD Explor. Newsl., 6(1):20–29, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Bradley</author>
</authors>
<title>The use of the area under the roc curve in the evaluation of machine learning algorithms.</title>
<date>1997</date>
<journal>Pattern Recognition,</journal>
<volume>30</volume>
<contexts>
<context position="21742" citStr="[5]" startWordPosition="3595" endWordPosition="3595"> In order to compare classifiers, it is possible to reduce a ROC curve to a scalar value representing the performance of the classifier. Area Under the ROC (AUC) is a portion of the area of the unit square. Its value will always be between 0 and 1. However, because random guessing produces the diagonal line between(0,0) and(1,1), which has an area of 0.5, no realistic classier should have an AUC less than 0.5. The AUC is equivalent to the Wilcoxon test of ranks and it is also related to Gini coefficient (for an exhaustive description of ROC and AUC in assessing machine learning algorithms see [5]) . In this work, we will use the AUC measure in order to assess the performance of classifiers. Furthermore, for each classifier, we present also the F-measure in order to compare our results to previous works in this area. F-measure is a combination of Recall and Precision metrics: F − measure = 2*Precision*Recall (Precision+Recall) 7 Results and Discussion In this section, we show the results obtained with the different learning algorithms and with the different sampling techniques used for both corpora. We also present results obtained using the original data set, which is the data set wit</context>
</contexts>
<marker>[5]</marker>
<rawString>A. P. Bradley. The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern Recognition, 30:1145–1159, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Chang</author>
<author>Q Zheng</author>
</authors>
<title>Offline definition extraction using machine learning for knowledge-oriented question answering.</title>
<date>2007</date>
<journal>ICIC</journal>
<booktitle>of Communications in Computer and Information Science,</booktitle>
<volume>3</volume>
<pages>1286--1294</pages>
<editor>In D.-S. Huang, L. Heutte, and M. Loog, editors,</editor>
<publisher>Springer,</publisher>
<contexts>
<context position="7707" citStr="[6]" startWordPosition="1232" endWordPosition="1232">icular, [8] used a maximum entropy classifier to extract definition in order to distinguish actual definitions from other sentences. As attributes to classify definition sentences they usedsuch as n-gram and bag-of-words, sentence position, syntactic properties and named entity classes. The corpus used was composed by medical pages of Dutch Wikipedia, where they extracted sentences based on syntactic features. The data set were composed by 2,299 senteces of which 1,366 actual definitions. This gives an initial accuracy of 59%, that was improved with machine learning algorithms until 92.21% In [6], it is presented a system to extract definition from off-line documents. They experimented with three different algorithms, namely NaiveBayse, Decision Tree and Support Vector Machine (SVM), obtaining the best score with SVM with a a F-measure of 0.83 with a balanced data set. In [26] they combine syntactic patterns with a Naive Bayes classification algorithm with the aim of extracting glossaries from tutorial documents in Dutch. They use several properties and several combination of them, obtaining an improvement of precision of 51.9% but a decline in the recall of 19.1% in comparison with a</context>
</contexts>
<marker>[6]</marker>
<rawString>X. Chang and Q. Zheng. Offline definition extraction using machine learning for knowledge-oriented question answering. In D.-S. Huang, L. Heutte, and M. Loog, editors, ICIC (3), volume 2 of Communications in Computer and Information Science, pages 1286–1294. Springer, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N V Chawla</author>
<author>K W Bowyer</author>
<author>L O Hall</author>
<author>W P Kegelmeyer</author>
</authors>
<title>Smote: Synthetic minority over-sampling technique.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>16</volume>
<contexts>
<context position="18267" citStr="[7]" startWordPosition="2995" endWordPosition="2995">rm a Tomek link, then either one of these examples is noise or both examples are border-line. As an under-sampling method, only examples belonging to the majority class are eliminated. The major drawback of Tomek Link undersampling is that this method can discard potentially useful data that could be important for the induction process. This method has an higher order computational complexity and will run slower than other algorithms. While the previous methods are intelligent down sampling techniques, SMOTE is an over-sampling method that produces new synthetic minority class examples. SMOTE [7] forms new minority class examples by interpolating between several minority class examples that lie together in ”feature space” rather than ”data space”. For each minority class example, this algorithm introduces synthetic examples along the line segments joining any/all of the k minority class nearest neighbors (in this work k is equal to 3). Synthetic samples are produced taking the difference between the feature vector (sample) under consideration and its nearest neighbors. The difference is multiplied by a random number between 0 and 1 and added to the feature vector under consideration. </context>
</contexts>
<marker>[7]</marker>
<rawString>N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321–357, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Fahmi</author>
<author>G Bouma</author>
</authors>
<title>Learning to identify definitions using syntactic feature.</title>
<date>2006</date>
<booktitle>Proceedings of the EACL workshop on Learning Structured Information in Natural Language Applications,</booktitle>
<editor>In R. Basili and A. Moschitti, editors,</editor>
<location>Trento, Italy,</location>
<contexts>
<context position="3571" citStr="[8]" startWordPosition="568" endWordPosition="568">and in case of a large use of lexical clues, the performance on different corpus get worst. In the case of classification approach one of the main issue to be dealt with is the sparseness of definitions in a corpus. It is a matter of fact that the number of definition bearing sentences is much lesser than the number of sentences that are not definitions. This configuration gives rise to an imbalanced data set, which may present different degrees of imbalance, depending on the corpus used. For corpus composed mostly by encyclopedic documents it is likely to get a balanced data set. For example [8] used a balanced corpus where the definition-bearing sentences represent 59% of the whole corpus, while [24] using a corpus consisting of encyclopedic text and web documents reports that only 18% of the sentences were definitions. In this work we deal with the problem of imbalanced data sets in definition extraction tasks in a language independent way. We show not only that sampling techniques can improve the performance of classifiers but also that this improvement is language independent. Other researches using learning algorithms relay strongly on lexical and syntactic components as feature</context>
<context position="7115" citStr="[8]" startWordPosition="1143" endWordPosition="1143">onym), they obtained, respectively, 4% and 36% of recall, and 61% and 66% of precision. Turning more specifically to the Portuguese language. Pinto and Oliveira [20] present a study on the extraction of definitions with a corpus from a medical domain. They first extract the relevant terms and then extract definition for each term. An evaluation is carried out for each term; for each term recall and precision are very variable ranging between 0% and 100%. In the last years machine learning techniques were combined with pattern recognition in order to improve the general results. In particular, [8] used a maximum entropy classifier to extract definition in order to distinguish actual definitions from other sentences. As attributes to classify definition sentences they usedsuch as n-gram and bag-of-words, sentence position, syntactic properties and named entity classes. The corpus used was composed by medical pages of Dutch Wikipedia, where they extracted sentences based on syntactic features. The data set were composed by 2,299 senteces of which 1,366 actual definitions. This gives an initial accuracy of 59%, that was improved with machine learning algorithms until 92.21% In [6], it is </context>
<context position="11080" citStr="[8]" startWordPosition="1787" endWordPosition="1787"> our experiments a simple grammar for each language was create that extracts all the sentences where the verb ”to be” appears as the main verb. For Dutch we obtained a sub-corpus composed by 4,829, 120 of which are definitions, with a ratio of 39:1. For Portuguese we obtained a sub-corpus composed by 1,360 sentences, 121 of which are definitions, with a ratio of about 10:1. Commonly used features are: bag-of-word, ngrams [17] (either of part-of-speech or of base forms), the position of the definition inside the document [12], the presence of determiners in the definiens and in the definiendum [8]. Other relevant properties can be the 1 www.lt4el.eu 34 presence of named entities [8] or data from en external source such as encyclopedic data, wordnet, etc. [22]. Some features work well with a corpus but not so well in a different corpus, resulting in the impossibility to use the learner with different corpora. The use of the position of a definition-bearing sentence in [8] is an example of a feature that is corpus dependent. The same issue arise when lexical information is used as feature. In order to avoid such limitation we represented instances as n-grams of POS. From both the corpora</context>
</contexts>
<marker>[8]</marker>
<rawString>I. Fahmi and G. Bouma. Learning to identify definitions using syntactic feature. In R. Basili and A. Moschitti, editors, Proceedings of the EACL workshop on Learning Structured Information in Natural Language Applications, Trento, Italy, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gernot</author>
</authors>
<title>Defining patterns in translation studies: Revisiting two classics of german translation.</title>
<date>2007</date>
<journal>Translationswissenschaft in Target,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1645" citStr="[9]" startWordPosition="245" endWordPosition="245">ependent. Keywords machine learning, imbalanced data set, language independent, definition extraction 1 Introduction According to Aristotle, the formal structure of a definition should resemble an equation with the definiendum (what is to be defined) on the left hand side and the definiens (the part which is doing the defining) on the right hand side. The definiens should consist of two parts: the genus (the nearest superior concept) and the differentiae specificae (the distinguishing characteristics). In this way, definitions would adequately capture the concept to be defined. In Hebenstreit [9], two more types of definition are pointed out. Firstly, the definition by enumeration of the concept species on the same level of abstraction (extensional definition), e.g. a chess piece is a king, a queen, a bishop, a knight, a rook or a pawn. Secondly, the definition by enumeration of the parts of the concept (partitive definition), e.g. the solar system is made of the planets Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto. Barnbrook [2] identifies 16 different types of definitions analysing dictionary entries. In spite of the richness of this classification, in aut</context>
</contexts>
<marker>[9]</marker>
<rawString>H. Gernot. Defining patterns in translation studies: Revisiting two classics of german translation. Translationswissenschaft in Target, 19(2):197–215, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P E Hart</author>
</authors>
<title>The condensed nearest neighbor rule (corresp.). Information Theory,</title>
<date>1968</date>
<journal>IEEE Transactions on,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="16466" citStr="[10]" startWordPosition="2674" endWordPosition="2674">ses: • Noise examples - examples that are incorrectly classified • Borderline examples - dangerous since a small amount of noise can make them fall on the wrong side of the decision border. • Redundant examples - too similar to other examples to be useful. • Safe examples - examples that fit perfectly the class to which they belong. Building on these considerations, several methods were proposed in order to retain safe examples in the re-balanced data set. We present here two of such methods, namely the Condensed Nearest Neighbour Rule and Tomek Link algorithm. Condensed Nearest Neighbor Rule [10] finds a consistent subset of examples in order to eliminate the examples from the majority class that are distant from the decision border, since these examples might be considered less relevant for learning. A subset E&apos; C E is consistent with E if using a 1-nearest neighbor,E&apos; correctly classies the examples in E. First, it randomly draw one majority class example and all examples from the minority class and put these examples in E&apos;. Next, it uses a 1-NN over the examples in E&apos; to classify the 35 examples in E. Every misclassified example from E is moved to E&apos;. It is important to note that t</context>
</contexts>
<marker>[10]</marker>
<rawString>P. E. Hart. The condensed nearest neighbor rule (corresp.). Information Theory, IEEE Transactions on, 14(3):515–516, May 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics,</booktitle>
<pages>539--545</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="5542" citStr="[11]" startWordPosition="890" endWordPosition="890"> aspects that are common to 33 Workshop On Definition Extraction 2009 - Borovets, Bulgaria, pages 33–39, different machine learning tasks in NLP application: small amounts of data, inherent ambiguity (definition detection is sometimes a matter of judgment), noisy data (human annotators make mistakes), imbalanced class distribution, this last aspect being the main issue addressed in this paper. 2 Related Work As we said in the previous section there are two main approach to deal with automatic definition extraction, the rule based and the classification one. Regarding the first approach Hearst [11] proposed a method to identify a set of lexico-syntactic patterns to extract hyponym relations from large corpora and extend WordNet with them. This method was adopted by [19] to cover other types of relations. DEFINDER [13] is considered a state of the art system. It combines simple cue-phrases and structural indicators introducing the definitions and the defined term. The corpus used to develop the rules consists of well-structured medical documents, where 60% of the definitions are introduced by a set of limited text markers. The nature of the corpus used can explain the high performance ob</context>
</contexts>
<marker>[11]</marker>
<rawString>M. A. Hearst. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics, pages 539–545, Morristown, NJ, USA, 1992. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Joho</author>
<author>M Sanderson</author>
</authors>
<title>Retrieving descriptive phrases from large amounts of free text.</title>
<date>2000</date>
<booktitle>In Proceeding of hte 9th international conference on Information and knowledge management,</booktitle>
<pages>180--186</pages>
<contexts>
<context position="11007" citStr="[12]" startWordPosition="1775" endWordPosition="1775">of the art performance. In order to prepare the data set for to be used in our experiments a simple grammar for each language was create that extracts all the sentences where the verb ”to be” appears as the main verb. For Dutch we obtained a sub-corpus composed by 4,829, 120 of which are definitions, with a ratio of 39:1. For Portuguese we obtained a sub-corpus composed by 1,360 sentences, 121 of which are definitions, with a ratio of about 10:1. Commonly used features are: bag-of-word, ngrams [17] (either of part-of-speech or of base forms), the position of the definition inside the document [12], the presence of determiners in the definiens and in the definiendum [8]. Other relevant properties can be the 1 www.lt4el.eu 34 presence of named entities [8] or data from en external source such as encyclopedic data, wordnet, etc. [22]. Some features work well with a corpus but not so well in a different corpus, resulting in the impossibility to use the learner with different corpora. The use of the position of a definition-bearing sentence in [8] is an example of a feature that is corpus dependent. The same issue arise when lexical information is used as feature. In order to avoid such lim</context>
</contexts>
<marker>[12]</marker>
<rawString>H. Joho and M. Sanderson. Retrieving descriptive phrases from large amounts of free text. In Proceeding of hte 9th international conference on Information and knowledge management, pages 180–186, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>S Muresan</author>
</authors>
<title>Evaluation of the DEFINDER system for fully automatic glossary construction.</title>
<date>2001</date>
<booktitle>In Proceedings of the American Medical Informatics Association Symposium (AMIA</booktitle>
<contexts>
<context position="5766" citStr="[13]" startWordPosition="928" endWordPosition="928"> sometimes a matter of judgment), noisy data (human annotators make mistakes), imbalanced class distribution, this last aspect being the main issue addressed in this paper. 2 Related Work As we said in the previous section there are two main approach to deal with automatic definition extraction, the rule based and the classification one. Regarding the first approach Hearst [11] proposed a method to identify a set of lexico-syntactic patterns to extract hyponym relations from large corpora and extend WordNet with them. This method was adopted by [19] to cover other types of relations. DEFINDER [13] is considered a state of the art system. It combines simple cue-phrases and structural indicators introducing the definitions and the defined term. The corpus used to develop the rules consists of well-structured medical documents, where 60% of the definitions are introduced by a set of limited text markers. The nature of the corpus used can explain the high performance obtained by this system (87% precision and 75% recall). Malaise and colleagues [16] focused their works on the extraction of definitory expressions containing hyperonym and synonym relations from French corpora. These authors </context>
</contexts>
<marker>[13]</marker>
<rawString>J. Klavans and S. Muresan. Evaluation of the DEFINDER system for fully automatic glossary construction. In Proceedings of the American Medical Informatics Association Symposium (AMIA 2001), 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kobylinski</author>
<author>A Przepiorkowski</author>
</authors>
<title>Definition extraction with balanced random forests.</title>
<date>2008</date>
<pages>237--247</pages>
<editor>In A. Ranta, editor,</editor>
<publisher>Springer-Verlag</publisher>
<location>GoTAL</location>
<contexts>
<context position="9154" citStr="[14]" startWordPosition="1467" endWordPosition="1467">ed their corpus using different ratios (1:1, 1:5, 1:10) in order to seek for best results. The corpus they used presented an original ratio of non-definitions to definitions of about 19. Although they obtained some improvement in terms of F-measure, in particular with the ratio 1 to 5, they cannot improve results obtained with a rule based grammar previously developed using the same corpus. These authors also investigated the use of Balanced Random Forest algorithm in order to deal with this imbalance, succeeding in outperform the rule based grammar previously developed of 5 percentage points [14]. 3 Corpora All the two corpora used for experiments were collected in the context of the LT4eL project 1. They were used to develop different tools, such a key-word extractor, a glossary candidate detector and an ontology, in order to support e-learning activities[1] in a multi-language context. The corpora are encoded with a common XML format. The DTD of this format is conforming to a DTD derived from the XCESAna DTD, a standard for linguistically annotated corpora [18]. Definition-bearing sentences were manually annotated. In each sentence, the term defined, the definition and the connectio</context>
</contexts>
<marker>[14]</marker>
<rawString>L. Kobylinski and A. Przepiorkowski. Definition extraction with balanced random forests. In A. Ranta, editor, GoTAL 2008, pages 237–247, Gothenburg, 2008. Springer-Verlag Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>N V Chawla</author>
<author>M P Harper</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>A study in machine learning from imbalanced data for sentence boundary detection in speech.</title>
<date>2006</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="28238" citStr="[15]" startWordPosition="4729" endWordPosition="4729">7 Table 6: Results using Native Bayes UCI data sets 2, Batista and colleagues [4] show that in most cases and with several data sets in different domains SMOTE and Random over-sampling are the most effective methods. In general, they lead to a rise in the AUC metric of few percentage points (1 to 4), when the base line was already high (more than 0.65), while where the base line was under this value the improvement was comparable to the one obtained in our work. In particular for the flag data set, they obtained an improvement of 34 percentage points. Focusing on Natural Language applications [15] apply these methods to sentence boundary detection in speech, showing that SMOTE and down-sampling get the best results with an AUC of 0.89 (the base line being 0.80). However, they did not experiment intelligent down-sampling methods such as CNN or Tomek Link. Batista in [3] gets the best results in terms of AUC with an improvement of 4 percentage points on the original data set using a combination of SMOTE with Tomek link, followed by simple SMOTE, in a case study on automated annotation of keywords. In our case the improvement regarding the original data set is between 10 and 29 percentage</context>
</contexts>
<marker>[15]</marker>
<rawString>Y. Liu, N. V. Chawla, M. P. Harper, E. Shriberg, and A. Stolcke. A study in machine learning from imbalanced data for sentence boundary detection in speech. Computer Speech &amp; Language, 20(4):468–494, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Malais</author>
<author>P Zweigenbaum</author>
<author>B Bachimont</author>
</authors>
<title>Detecting semantic relations between terms in definitions.</title>
<date>2004</date>
<booktitle>In the 3rd edition of CompuTerm Workshop (CompuTerm</booktitle>
<pages>55--62</pages>
<contexts>
<context position="6223" citStr="[16]" startWordPosition="1001" endWordPosition="1001">hyponym relations from large corpora and extend WordNet with them. This method was adopted by [19] to cover other types of relations. DEFINDER [13] is considered a state of the art system. It combines simple cue-phrases and structural indicators introducing the definitions and the defined term. The corpus used to develop the rules consists of well-structured medical documents, where 60% of the definitions are introduced by a set of limited text markers. The nature of the corpus used can explain the high performance obtained by this system (87% precision and 75% recall). Malaise and colleagues [16] focused their works on the extraction of definitory expressions containing hyperonym and synonym relations from French corpora. These authors used lexical-syntactic markers and patterns to detect at the same time definitions and relations. For the two different relations (hyponym and synonym), they obtained, respectively, 4% and 36% of recall, and 61% and 66% of precision. Turning more specifically to the Portuguese language. Pinto and Oliveira [20] present a study on the extraction of definitions with a corpus from a medical domain. They first extract the relevant terms and then extract defi</context>
</contexts>
<marker>[16]</marker>
<rawString>V. Malais, P. Zweigenbaum, and B. Bachimont. Detecting semantic relations between terms in definitions. In the 3rd edition of CompuTerm Workshop (CompuTerm 2004) at Coling 2004, pages 55–62, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miliaraki</author>
<author>I Androutsopoulos</author>
</authors>
<title>Learning to identify single-snippet answer to definition questions.</title>
<date>2004</date>
<booktitle>In Proceeding of the 20th International Conference on Computational Linguistic (COLING 2004),</booktitle>
<pages>1360--1366</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="10906" citStr="[17]" startWordPosition="1759" endWordPosition="1759">formation using the LX-Suite [23] a set of tools for the shallow processing of Portuguese with state of the art performance. In order to prepare the data set for to be used in our experiments a simple grammar for each language was create that extracts all the sentences where the verb ”to be” appears as the main verb. For Dutch we obtained a sub-corpus composed by 4,829, 120 of which are definitions, with a ratio of 39:1. For Portuguese we obtained a sub-corpus composed by 1,360 sentences, 121 of which are definitions, with a ratio of about 10:1. Commonly used features are: bag-of-word, ngrams [17] (either of part-of-speech or of base forms), the position of the definition inside the document [12], the presence of determiners in the definiens and in the definiendum [8]. Other relevant properties can be the 1 www.lt4el.eu 34 presence of named entities [8] or data from en external source such as encyclopedic data, wordnet, etc. [22]. Some features work well with a corpus but not so well in a different corpus, resulting in the impossibility to use the learner with different corpora. The use of the position of a definition-bearing sentence in [8] is an example of a feature that is corpus de</context>
</contexts>
<marker>[17]</marker>
<rawString>S. Miliaraki and I. Androutsopoulos. Learning to identify single-snippet answer to definition questions. In Proceeding of the 20th International Conference on Computational Linguistic (COLING 2004), pages 1360–1366, Geneva, Switzerland, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I N</author>
<author>S K Xml</author>
</authors>
<title>corpus encoding standard, document xces 0.2.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, Vassar College and Equipe Langue ed Dialogue,</institution>
<location>New York, USA</location>
<contexts>
<context position="9630" citStr="[18]" startWordPosition="1549" endWordPosition="1549">er to deal with this imbalance, succeeding in outperform the rule based grammar previously developed of 5 percentage points [14]. 3 Corpora All the two corpora used for experiments were collected in the context of the LT4eL project 1. They were used to develop different tools, such a key-word extractor, a glossary candidate detector and an ontology, in order to support e-learning activities[1] in a multi-language context. The corpora are encoded with a common XML format. The DTD of this format is conforming to a DTD derived from the XCESAna DTD, a standard for linguistically annotated corpora [18]. Definition-bearing sentences were manually annotated. In each sentence, the term defined, the definition and the connection verb were annotated using a different XML tag. The Dutch Corpus is composed by 26 tutorials with a size of about 350,000 tokens. The corpus was annotated part-of-speech information and morphosyntactic features with the Wotan tagger and with lemmatization information with the CGN lemmatizer (for more information about this corpus see [26]. The Portuguese Corpus is composed by 23 tutorials and scientific papers in the field of Information Technology and has a size of 274,</context>
</contexts>
<marker>[18]</marker>
<rawString>I. N. and S. K. Xml, corpus encoding standard, document xces 0.2. Technical report, Department of Computer Science, Vassar College and Equipe Langue ed Dialogue, New York, USA and LORIA/CNRS, Vandouvre-les-Nancy, France, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Person</author>
</authors>
<title>The expression of definitions in specialised text: a corpus-based analysis.</title>
<date>1996</date>
<booktitle>7th Internation Congress on Lexicography (EURALEX 96),</booktitle>
<pages>817--824</pages>
<editor>In M. Gellerstam, J. Jaborg, S. G. Malgren, K. Noren, L. Rogstrom, and C. Papmehl, editors,</editor>
<location>Goteborg, Sweden,</location>
<contexts>
<context position="5717" citStr="[19]" startWordPosition="920" endWordPosition="920">data, inherent ambiguity (definition detection is sometimes a matter of judgment), noisy data (human annotators make mistakes), imbalanced class distribution, this last aspect being the main issue addressed in this paper. 2 Related Work As we said in the previous section there are two main approach to deal with automatic definition extraction, the rule based and the classification one. Regarding the first approach Hearst [11] proposed a method to identify a set of lexico-syntactic patterns to extract hyponym relations from large corpora and extend WordNet with them. This method was adopted by [19] to cover other types of relations. DEFINDER [13] is considered a state of the art system. It combines simple cue-phrases and structural indicators introducing the definitions and the defined term. The corpus used to develop the rules consists of well-structured medical documents, where 60% of the definitions are introduced by a set of limited text markers. The nature of the corpus used can explain the high performance obtained by this system (87% precision and 75% recall). Malaise and colleagues [16] focused their works on the extraction of definitory expressions containing hyperonym and syno</context>
</contexts>
<marker>[19]</marker>
<rawString>J. Person. The expression of definitions in specialised text: a corpus-based analysis. In M. Gellerstam, J. Jaborg, S. G. Malgren, K. Noren, L. Rogstrom, and C. Papmehl, editors, 7th Internation Congress on Lexicography (EURALEX 96), pages 817–824, Goteborg, Sweden, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Pinto</author>
<author>D Oliveira</author>
</authors>
<title>Extra¸cc˜ao de defini¸c˜oes no Corp´ografo.</title>
<date>2004</date>
<booktitle>Faculdade de Letras da Universidade do</booktitle>
<tech>Technical report,</tech>
<location>Porto,</location>
<contexts>
<context position="6677" citStr="[20]" startWordPosition="1070" endWordPosition="1070">rs. The nature of the corpus used can explain the high performance obtained by this system (87% precision and 75% recall). Malaise and colleagues [16] focused their works on the extraction of definitory expressions containing hyperonym and synonym relations from French corpora. These authors used lexical-syntactic markers and patterns to detect at the same time definitions and relations. For the two different relations (hyponym and synonym), they obtained, respectively, 4% and 36% of recall, and 61% and 66% of precision. Turning more specifically to the Portuguese language. Pinto and Oliveira [20] present a study on the extraction of definitions with a corpus from a medical domain. They first extract the relevant terms and then extract definition for each term. An evaluation is carried out for each term; for each term recall and precision are very variable ranging between 0% and 100%. In the last years machine learning techniques were combined with pattern recognition in order to improve the general results. In particular, [8] used a maximum entropy classifier to extract definition in order to distinguish actual definitions from other sentences. As attributes to classify definition sen</context>
</contexts>
<marker>[20]</marker>
<rawString>A. S. Pinto and D. Oliveira. Extra¸cc˜ao de defini¸c˜oes no Corp´ografo. Technical report, Faculdade de Letras da Universidade do Porto, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Przepiorkowski</author>
<author>M Marcinczuk</author>
<author>L Degorski</author>
</authors>
<title>Noisy and imbalanced data: Machine learning or manual grammars?</title>
<date>2008</date>
<booktitle>In Text, Speech and Dialogue: 9th International Conference, TSD 2008,</booktitle>
<publisher>Springer-Verlag.</publisher>
<location>Brno, Czech Republic,</location>
<contexts>
<context position="8539" citStr="[21]" startWordPosition="1367" endWordPosition="1367">th a a F-measure of 0.83 with a balanced data set. In [26] they combine syntactic patterns with a Naive Bayes classification algorithm with the aim of extracting glossaries from tutorial documents in Dutch. They use several properties and several combination of them, obtaining an improvement of precision of 51.9% but a decline in the recall of 19.1% in comparison with a the syntactic pattern system developed previously by the authors using the same corpus. Recently, some authors have started to look at this problem of imbalanced data set in the context of definition extraction. In particular, [21] down-sampled their corpus using different ratios (1:1, 1:5, 1:10) in order to seek for best results. The corpus they used presented an original ratio of non-definitions to definitions of about 19. Although they obtained some improvement in terms of F-measure, in particular with the ratio 1 to 5, they cannot improve results obtained with a rule based grammar previously developed using the same corpus. These authors also investigated the use of Balanced Random Forest algorithm in order to deal with this imbalance, succeeding in outperform the rule based grammar previously developed of 5 percent</context>
<context position="29394" citStr="[21]" startWordPosition="4922" endWordPosition="4922">riginal data set is between 10 and 29 percentage points, demonstrating how these methods can be effective in this application. Regarding the comparison with other work in definition extraction, the improvement obtained on the Fmeasure, with the best result of 0.77 with C4.5 classifier, outperforms most of the systems using learning algorithms, confirming the importance of sampling techniques in supporting definition extraction tasks. [26], using the same corpus we used, reports on a F-measure of 0.73, obtained with a combination of syntactic rules and a Naive Bayes classifiers for Dutch while [21], with a similar approach, but for the Polish language, obtain a F-measure of 0.35. Furthermore in all these works a combination of features are used in order to reach best results, while in this paper we only use bi-grams of POS as features. To conclude, our results are comparable with systems that represent the state of the art in the area, such as DEFINDER, which shows a F-mesure of 0.80. 8 Conclusions and Future Work In this paper we have compared the performance of different learning algorithms and different sampling technique on a definition extraction task, using data sets in different </context>
</contexts>
<marker>[21]</marker>
<rawString>A. Przepiorkowski, M. Marcinczuk, and L. Degorski. Noisy and imbalanced data: Machine learning or manual grammars? In Text, Speech and Dialogue: 9th International Conference, TSD 2008, Brno, Czech Republic, September 2008. Lecture Notes in Artificial Intelligence, Berlin, Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
</authors>
<title>Identifying definitions in text collections for question answering.</title>
<date>2004</date>
<booktitle>In LREC</booktitle>
<contexts>
<context position="11245" citStr="[22]" startWordPosition="1814" endWordPosition="1814">d a sub-corpus composed by 4,829, 120 of which are definitions, with a ratio of 39:1. For Portuguese we obtained a sub-corpus composed by 1,360 sentences, 121 of which are definitions, with a ratio of about 10:1. Commonly used features are: bag-of-word, ngrams [17] (either of part-of-speech or of base forms), the position of the definition inside the document [12], the presence of determiners in the definiens and in the definiendum [8]. Other relevant properties can be the 1 www.lt4el.eu 34 presence of named entities [8] or data from en external source such as encyclopedic data, wordnet, etc. [22]. Some features work well with a corpus but not so well in a different corpus, resulting in the impossibility to use the learner with different corpora. The use of the position of a definition-bearing sentence in [8] is an example of a feature that is corpus dependent. The same issue arise when lexical information is used as feature. In order to avoid such limitation we represented instances as n-grams of POS. From both the corpora the 100 most frequent n-grams were extracted and were used as features. Each sentence was represented as an array where cells record the number of occurrences of th</context>
</contexts>
<marker>[22]</marker>
<rawString>H. Saggion. Identifying definitions in text collections for question answering. In LREC 2004, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Silva</author>
</authors>
<title>Shallow processing of Portuguese: From sentence chunking to nominal lemmatization. Master’s thesis, Universidade de Lisboa, Faculdade de Ciˆencias,</title>
<date>2007</date>
<contexts>
<context position="10335" citStr="[23]" startWordPosition="1658" endWordPosition="1658">ition and the connection verb were annotated using a different XML tag. The Dutch Corpus is composed by 26 tutorials with a size of about 350,000 tokens. The corpus was annotated part-of-speech information and morphosyntactic features with the Wotan tagger and with lemmatization information with the CGN lemmatizer (for more information about this corpus see [26]. The Portuguese Corpus is composed by 23 tutorials and scientific papers in the field of Information Technology and has a size of 274,000 tokens. It was then automatically annotated with morpho-syntactic information using the LX-Suite [23] a set of tools for the shallow processing of Portuguese with state of the art performance. In order to prepare the data set for to be used in our experiments a simple grammar for each language was create that extracts all the sentences where the verb ”to be” appears as the main verb. For Dutch we obtained a sub-corpus composed by 4,829, 120 of which are definitions, with a ratio of 39:1. For Portuguese we obtained a sub-corpus composed by 1,360 sentences, 121 of which are definitions, with a ratio of about 10:1. Commonly used features are: bag-of-word, ngrams [17] (either of part-of-speech or</context>
</contexts>
<marker>[23]</marker>
<rawString>J. R. Silva. Shallow processing of Portuguese: From sentence chunking to nominal lemmatization. Master’s thesis, Universidade de Lisboa, Faculdade de Ciˆencias, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong</author>
<author>K Sang</author>
<author>G Bouma</author>
<author>M de Rijke</author>
</authors>
<title>Developing offline strategies for answering medical questions.</title>
<date>2005</date>
<booktitle>In Proceedings of the AAAI-05 workshop on Question Answering in restricted domains,</booktitle>
<pages>41--45</pages>
<contexts>
<context position="3679" citStr="[24]" startWordPosition="584" endWordPosition="584">classification approach one of the main issue to be dealt with is the sparseness of definitions in a corpus. It is a matter of fact that the number of definition bearing sentences is much lesser than the number of sentences that are not definitions. This configuration gives rise to an imbalanced data set, which may present different degrees of imbalance, depending on the corpus used. For corpus composed mostly by encyclopedic documents it is likely to get a balanced data set. For example [8] used a balanced corpus where the definition-bearing sentences represent 59% of the whole corpus, while [24] using a corpus consisting of encyclopedic text and web documents reports that only 18% of the sentences were definitions. In this work we deal with the problem of imbalanced data sets in definition extraction tasks in a language independent way. We show not only that sampling techniques can improve the performance of classifiers but also that this improvement is language independent. Other researches using learning algorithms relay strongly on lexical and syntactic components as features to describe the data set. These kinds of features are not only language dependent but also domain dependen</context>
</contexts>
<marker>[24]</marker>
<rawString>E. Tjong, K. Sang, G. Bouma, and M. de Rijke. Developing offline strategies for answering medical questions. In Proceedings of the AAAI-05 workshop on Question Answering in restricted domains, pages 41–45, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tomek</author>
</authors>
<title>Two modifications of cnn.</title>
<date>1976</date>
<journal>Systems, Man and Cybernetics, IEEE Transactions on,</journal>
<volume>6</volume>
<issue>11</issue>
<contexts>
<context position="17280" citStr="[25]" startWordPosition="2822" endWordPosition="2822">A subset E&apos; C E is consistent with E if using a 1-nearest neighbor,E&apos; correctly classies the examples in E. First, it randomly draw one majority class example and all examples from the minority class and put these examples in E&apos;. Next, it uses a 1-NN over the examples in E&apos; to classify the 35 examples in E. Every misclassified example from E is moved to E&apos;. It is important to note that this procedure does not find the smallest consistent subset from E. The CNN is sensitive to noise and noisy examples are likely to be misclassified as many of them will be added to the training set. Tomek links [25] removes both noise and borderline examples. Tomek links are pairs of instances of di?erent classes that have each other as their nearest neighbors. Given two examples x and y belonging to different classes, and d(x, y) the distance between x and y, a (x, y) pair is called a Tomek link if there is not an example z such that d(x, z) &lt; d(x, y) or d(y, z) &lt; d(x, y). If two examples form a Tomek link, then either one of these examples is noise or both examples are border-line. As an under-sampling method, only examples belonging to the majority class are eliminated. The major drawback of Tomek Lin</context>
</contexts>
<marker>[25]</marker>
<rawString>I. Tomek. Two modifications of cnn. Systems, Man and Cybernetics, IEEE Transactions on, 6(11):769–772, November 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Westerhout</author>
<author>P Monachesi</author>
</authors>
<title>Extraction of Dutch definitory contexts for elearning purposes.</title>
<date>2007</date>
<booktitle>In CLIN proceedings</booktitle>
<contexts>
<context position="7993" citStr="[26]" startWordPosition="1280" endWordPosition="1280">s. The corpus used was composed by medical pages of Dutch Wikipedia, where they extracted sentences based on syntactic features. The data set were composed by 2,299 senteces of which 1,366 actual definitions. This gives an initial accuracy of 59%, that was improved with machine learning algorithms until 92.21% In [6], it is presented a system to extract definition from off-line documents. They experimented with three different algorithms, namely NaiveBayse, Decision Tree and Support Vector Machine (SVM), obtaining the best score with SVM with a a F-measure of 0.83 with a balanced data set. In [26] they combine syntactic patterns with a Naive Bayes classification algorithm with the aim of extracting glossaries from tutorial documents in Dutch. They use several properties and several combination of them, obtaining an improvement of precision of 51.9% but a decline in the recall of 19.1% in comparison with a the syntactic pattern system developed previously by the authors using the same corpus. Recently, some authors have started to look at this problem of imbalanced data set in the context of definition extraction. In particular, [21] down-sampled their corpus using different ratios (1:1</context>
<context position="10095" citStr="[26]" startWordPosition="1620" endWordPosition="1620">ML format. The DTD of this format is conforming to a DTD derived from the XCESAna DTD, a standard for linguistically annotated corpora [18]. Definition-bearing sentences were manually annotated. In each sentence, the term defined, the definition and the connection verb were annotated using a different XML tag. The Dutch Corpus is composed by 26 tutorials with a size of about 350,000 tokens. The corpus was annotated part-of-speech information and morphosyntactic features with the Wotan tagger and with lemmatization information with the CGN lemmatizer (for more information about this corpus see [26]. The Portuguese Corpus is composed by 23 tutorials and scientific papers in the field of Information Technology and has a size of 274,000 tokens. It was then automatically annotated with morpho-syntactic information using the LX-Suite [23] a set of tools for the shallow processing of Portuguese with state of the art performance. In order to prepare the data set for to be used in our experiments a simple grammar for each language was create that extracts all the sentences where the verb ”to be” appears as the main verb. For Dutch we obtained a sub-corpus composed by 4,829, 120 of which are def</context>
<context position="29232" citStr="[26]" startWordPosition="4894" endWordPosition="4894">a combination of SMOTE with Tomek link, followed by simple SMOTE, in a case study on automated annotation of keywords. In our case the improvement regarding the original data set is between 10 and 29 percentage points, demonstrating how these methods can be effective in this application. Regarding the comparison with other work in definition extraction, the improvement obtained on the Fmeasure, with the best result of 0.77 with C4.5 classifier, outperforms most of the systems using learning algorithms, confirming the importance of sampling techniques in supporting definition extraction tasks. [26], using the same corpus we used, reports on a F-measure of 0.73, obtained with a combination of syntactic rules and a Naive Bayes classifiers for Dutch while [21], with a similar approach, but for the Polish language, obtain a F-measure of 0.35. Furthermore in all these works a combination of features are used in order to reach best results, while in this paper we only use bi-grams of POS as features. To conclude, our results are comparable with systems that represent the state of the art in the area, such as DEFINDER, which shows a F-mesure of 0.80. 8 Conclusions and Future Work In this paper</context>
</contexts>
<marker>[26]</marker>
<rawString>E. Westerhout and P. Monachesi. Extraction of Dutch definitory contexts for elearning purposes. In CLIN proceedings 2007, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Pratical Machine Learning Tools and Techniques (Second Edition).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="14335" citStr="[27]" startWordPosition="2325" endWordPosition="2325">ation is deferred until the classification phase. In this phase, it computes the distance between the target sample and n samples in the data set, assining the most frequent class. Two different K nearest neighbors classifiers were constructed, with k equal to 1 and to 3. SVM is a classifier that tries to find an optimal hyperplane that correctly classifies data points as much as possible and separate the point of two classes as far as possible. In this experiment four different classifiers were implemented, using four different kernels, linear, polynominal, radial and sigmoid. Weka workbench [27] was used to build all the learners. 5 Sampling Techniques In many real-world classification applications, most of the examples are from one of the classes, while the minority class is the interesting one. As most of the learning algorithms are designed to maximize accuracy, the imbalance in the class distribution leads to a poor performance of these algorithms. The issue is therefore how to improve the classification of the minority class examples. A common solution is to sample the data, either randomly or intelligently, to obtain an altered class distribution. Random over-sampling consists </context>
</contexts>
<marker>[27]</marker>
<rawString>I. H. Witten and E. Frank. Data Mining: Pratical Machine Learning Tools and Techniques (Second Edition). Morgan Kaufmann, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>