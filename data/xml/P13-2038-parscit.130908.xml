<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006613">
<title confidence="0.9965675">
Latent Semantic Matching: Application to Cross-language Text
Categorization without Alignment Information
</title>
<author confidence="0.927295">
Tsutomu Hirao and Tomoharu Iwata and Masaaki Nagata
</author>
<affiliation confidence="0.859153">
NTT Communication Science Laboratories, NTT Corporation
</affiliation>
<address confidence="0.918637">
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
</address>
<email confidence="0.999099">
{hirao.tsutomu,iwata.tomoharu,nagata.masaaki}@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995915">
Unsupervised object matching (UOM) is
a promising approach to cross-language
natural language processing such as bilin-
gual lexicon acquisition, parallel corpus
construction, and cross-language text cat-
egorization, because it does not require
labor-intensive linguistic resources. How-
ever, UOM only finds one-to-one corre-
spondences from data sets with the same
number of instances in source and target
domains, and this prevents us from ap-
plying UOM to real-world cross-language
natural language processing tasks. To al-
leviate these limitations, we proposes la-
tent semantic matching, which embeds
objects in both source and target lan-
guage domains into a shared latent topic
space. We demonstrate the effectiveness
of our method on cross-language text cat-
egorization. The results show that our
method outperforms conventional unsu-
pervised object matching methods.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999773321428572">
Unsupervised object matching is a method for
finding one-to-one correspondences between ob-
jects across different domains without knowledge
about the relation between the domains. Kernel-
ized sorting (Novi et al., 2010) and canonical cor-
relation analysis based methods (Haghighi et al.,
2008; Tripathi et al., 2010) are two such exam-
ples of unsupervised object matching, which have
been shown to be quite useful for cross-language
natural language processing (NLP) tasks. One of
the most important properties of the unsupervised
object matching is that it does not require any lin-
guistic resources which connects between the lan-
guages. This distinguishes it from other cross-
language NLP methods such as machine transla-
tion based and projection based approaches (Du-
mais et al., 1996; Gliozzo and Strapparava, 2005;
Platt et al., 2010), which we need bilingual dictio-
naries or parallel sentences.
When we apply unsupervised object matching
methods to cross-language NLP tasks, there are
two critical problems. The first is that they only
find one-to-one matching. The second is they re-
quire the same size of source- and target-data. For
example, the correct translation of a word is not
always unique. French words ‘maison’, ‘appart-
ment’ and ‘domicile’ can be regarded as transla-
tion of an English word ‘home’. In addition, En-
glish vocabulary size is not equal to that of French.
These discussions motivate us to introduce a
shared space in which both source and target do-
main objects will reside. If we can obtain such
a shared space, we can match objects within the
space, because we can use standard distance met-
rics on this space. This will also enable us to use
various kinds of non-strict matching. For exam-
ple, k-nearest objects in the source domain will be
retrieved for a query object in the target domain.
In this paper, we propose a simple but effective
method to find the shared space by assuming that
two languages have common latent topics, which
we call latent semantic matching. With latent se-
mantic matching, we first find latent topics in two
domains independently. Then, the topics in two
domains are aligned by kernelized sorting, and ob-
jects are embedded in a shared latent topic space.
Latent topic representations are successfully used
in a wide range of NLP tasks, such as information
retrieval and text classification, because they rep-
resent intrinsic information of documents (Deer-
wester et al., 1990). By matching latent topics,
we can find relation between source and target do-
mains, and additionally we can handle different
numbers of objects in two domains.
We compared latent semantic matching with
conventional unsupervised object matching meth-
</bodyText>
<page confidence="0.977755">
212
</page>
<bodyText confidence="0.8257625">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 212–216,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
ods on the task of cross-language text categoriza-
tion, i.e. classifying target side unlabeled docu-
ments by label information obtained from source
side documents. The results show that, with more
source side documents, our method achieved the
highest classification accuracy.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999829">
Many cross-language text processing methods
have been proposed that require correspondences
between source and target languages. For exam-
ple, (Dumais et al., 1996) proposed cross-lingual
latent semantic indexing, and (Platt et al., 2010)
employed oriented principle component analysis
and canonical correlation analysis (CCA). They
concatenate the document pairs (source document
and its translation) obtained from a document-
level parallel corpus. They then apply multi-
variate analysis to acquire the translingual projec-
tion. There are extensions of latent Dirichlet allo-
cation (LDA) (Blei et al., 2003) for cross-language
analysis, such as multilingual topic models (Boyd-
Graber and Blei, 2009), joint LDA (Jagadeesh
and Daume III, 2010) and multilingual LDA (Xi-
aochuan et al., 2011). They require a bilingual dic-
tionary or document-level parallel corpora.
Unsupervised object matching methods have
been proposed recently (Novi et al., 2010;
Haghighi et al., 2008; Yamada and Sugiyama,
2011). These methods are promising in terms of
language portability because they do not require
external language resources. (Novi et al., 2010)
proposed kernelized sorting (KS); it finds one-to-
one correspondences between objects in different
domains by permuting a set to maximize the de-
pendence between two sets. Here, the Hilbert-
Schmidt independence criterion is used for mea-
suring dependence. (Djuric et al., 2012) proposed
convex kernelized sorting as an extension of KS.
(Yamada and Sugiyama, 2011) proposed least-
squares object matching which maximizes the
squared-loss mutual information between matched
pairs. (Haghighi et al., 2008) proposed another
framework, matching CCA (MCCA), based on a
probabilistic interpretation of CCA (Bach and Jor-
dan, 2005). MCCA simultaneously finds latent
variables that represent correspondences and la-
tent features so that the latent features of corre-
sponding examples exhibit the maximum correla-
tion. However, these unsupervised object match-
ing methods have limitations. They require that
the source and target domains have the same data
size, and they find one-to-one correspondences.
There are critical weaknesses of these methods
when we attempt to apply them to real world
cross-language NLP applications.
</bodyText>
<sectionHeader confidence="0.932466" genericHeader="method">
3 Latent Semantic Matching
</sectionHeader>
<bodyText confidence="0.999990428571429">
We propose latent semantic matching to find a
shared latent space by assuming that two lan-
guages have common latent topics. Our method
consists of following four steps: (1) for both
source and target domains, we map the documents
to a K-dimensional latent topic space indepen-
dently, (2) we find the one-to-one correspondences
between topics across source and target domains
by unsupervised object matching, (3) we permute
topics of the target side according to the corre-
spondences, while fixing the topics of the source
side, and (4) finally, we map documents in the
source and target domains to a shared latent space
by using permuted and fixed topics.
</bodyText>
<subsectionHeader confidence="0.997477">
3.1 Topic Extraction as Dimension Reduction
</subsectionHeader>
<bodyText confidence="0.999436684210527">
Suppose that we have N documents in the source
domain. s,,,=(s,,,z)z1 is the nth document rep-
resented as a multi-dimensional column vector in
the domain, i.e. each document is represented as
a bag-of-words vector. Here, each element of the
vectors indicates the TF·IDF score of the corre-
sponding word in the document. I is the size of the
feature set, i.e., the vocabulary size in the source
domain. Also, we have M documents in the tar-
get domain.t�=(���)�3=1 is the mth document
represented as a multi-dimensional vector. J is
the vocabulary size in the target domain. Thus,
the data set in the source domain is represented by
an I x N matrix, S=(s1, · · · , sN), the data set
in the target is represented by a J x M matrix,
T=(t1, ··· , tm).
We factorize these matrices using nonnegative
matrix factorization (Lee and Seung, 2000) to find
topics as follows:
</bodyText>
<equation confidence="0.9985545">
S Pt� WSHS, (1)
T Pt� WTHT. (2)
</equation>
<bodyText confidence="0.9972004">
WS is an I xK matrix that represents a set of top-
ics, i.e. each column vector denotes word weights
for each topic. HS is a K x N matrix that de-
notes a set of latent semantic representations of
documents in the source domain, i.e. each row
</bodyText>
<page confidence="0.99879">
213
</page>
<figureCaption confidence="0.999908">
Figure 1: Topic alignments.
</figureCaption>
<bodyText confidence="0.9999788">
vector denotes an embedding of a document in the
K-dimensional latent space. Similarly, WT is an
I × K matrix that represents a set of topics in the
target domain, and HT is a K × M matrix that
denotes a set of latent semantic representations of
target documents. K is less than I and J.
By factorizing the original matrices, we can in-
dependently map the documents in the source and
target domains to the latent topic spaces whose di-
mensionality is K.
</bodyText>
<subsectionHeader confidence="0.99445">
3.2 Finding Optimal Topic Alignments by
Unsupervised Object Matching
</subsectionHeader>
<bodyText confidence="0.999957307692308">
To connect the different latent spaces, topics ex-
tracted from the source language must be aligned
to one from the target language. This is reasonable
because we can assume that both languages share
the same latent concept.
However, we cannot quantify the similarity be-
tween the topics because we do not have any ex-
ternal language resources such as a dictionary.
Therefore, we utilize unsupervised object match-
ing method to find one-to-one correspondences
between topics. In this paper, we employ kernel-
ized sorting (KS) (Novi et al., 2010). KS finds the
best one-to-one matching as follows:
</bodyText>
<equation confidence="0.983211333333333">
7r* = arg max tr(GS7rTGT7r),
7rErIK
s.t. 7r1K=1K and 7rT1K=1K. (3)
</equation>
<bodyText confidence="0.993128">
Here, 7r is a K ×K matrix that represents the one-
to-one correspondence between topics, i.e. 7rij=1
indicates that the ith topic in the source language
corresponds to the jth one of the target language.
</bodyText>
<table confidence="0.682930166666667">
Overall Average
0.252 ± 0.112
0.249 ± 0.033
0.278 ± 0.086
0.298 ± 0.077
0.359 ± 0.062
</table>
<tableCaption confidence="0.99787">
Table 1: Average accuracy over all language pairs
</tableCaption>
<bodyText confidence="0.999119545454545">
HK indicates the set of all possible matrices stor-
ing one-to-one correspondences. G denotes the
K × K kernel matrix obtained from topic pro-
portion, Gij=K(WTi,:, W:,j), and G is the centered
matrix of G. K(,) is a kernel function. 1K is a
K-dimensional column vector of all ones. 7r* is
obtained by iterative procedure.
According to 7r*, we obtain permuted matrices,
WT=WT7r* and HT=7r*THT, and the product
of permuted matrices is the same with that of un-
permuted matrices as follows:
</bodyText>
<equation confidence="0.988058">
T ≈ WTHT=WTHT. (4)
</equation>
<bodyText confidence="0.911967285714286">
Fig. 1 shows the topic alignment procedure.
Since documents from both domains are repre-
sented in a shared latent space, we can directly cal-
culate the similarity between the nth document in
the source domain and the mth document in the
target domain based on HT:,m (mth column vec-
tor of HT) and HS:,n (nth column vector of HS).
</bodyText>
<sectionHeader confidence="0.962269" genericHeader="method">
4 Cross-language Text Categorization
</sectionHeader>
<subsectionHeader confidence="0.518014">
via Latent Semantic Matching
</subsectionHeader>
<bodyText confidence="0.9997824375">
Cross-language text categorization is the task of
exploiting labeled documents in the source lan-
guage (e.g. English) to classify documents in
the target language (e.g. French). Suppose we
have training data set {sn, yn}Nn=1 in the source
language domain. yn ∈ Y is the class label
for the nth document. We can train a classifier
in the K-dimensional latent space with data set
{HTS: n, yn}Nn= 1. HTS: n is the projected vector of
sn. Also, the mth document in the target language
domain tm is projected into the latent space as
HTT:,m. Here, the documents in both domains are
projected into the same size latent space and the
basis vectors of the spaces are aligned. Therefore,
we can classify a document in the target domain
tm by a classifier trained with {HTS:,n, yn}Nn=1.
</bodyText>
<figure confidence="0.991275636363636">
K
WS HS
d o
I
S
≈
×
K
N
π* =
0 0 1 0
0 1 0 0 π
e
0001op
1 0 0 0
J
T
≈
×
K
WT HT
M
T
≈
×
domain (tm
KS
CKS
he urce domain as
LSOM
mply k(= 1-NN as clasi-
LSM(300)
LSM(600)
</figure>
<page confidence="0.9791">
214
</page>
<table confidence="0.909795875">
Books
English Hack, Parent, tale, subversion, Interesting, centre, Paper, T., prejudice, Murphy
German Lydia, Sebastian, Seelenbrecher, Patient, Fitzek, Patrick, Fiktion, Patientenakte, Realitt, Klinik
Electronics
English SD800, Angle, Digital, Optical, Silver, understnad, camra, 7.1MP, P3N, 10MP
German *****, 550D, 600D, Objektiv, Canon, ablichten, Body, Werkzeug, Kamera, einliet
Kitchen
English Briel, Electra-Craft, Chamonix, machine, Due, crema, supervisor, technician, espresso, tamp
German ESGE, Prierkopf, Zauberstab, Gummikupplung, Suppe/Sauce, Braun, Bolognese, prieren, Testsieger, Topf
Music
English Amy, Poison, Doherty, Schottin, Mid, Prince, Song, ausdrucksstark , Tempo, knocking
German Norah, mini, ’Little, ’Rome, ’Come, Gardot, Lana, listenings , dreamlike, digipak
Watch
English watch, indicate, timex, HRM, month, icon, Timex, datum, troubleshooting, reasonable
German Orient, Diver, Lnette, Leuchtpunkt, Zahlenringes, Handgelenksdurchmesser, Stoppsekunde, Uhrforum,
Konsumbereiche, Schwingungen/Std
</table>
<tableCaption confidence="0.998672">
Table 2: Examples of aligned latent topics
</tableCaption>
<sectionHeader confidence="0.993183" genericHeader="evaluation">
5 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.969681">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999267">
We compared our method, latent semantic match-
ing (LSM), with three unsupervised object match-
ing methods: Kernelized Sorting (KS), Convex
Kernelized Sorting (CKS), Least-Squares Object
Matching (LSOM). We set the number of the la-
tent topics K to 100 and employed the k-nearest
neighbor method (k=10) as the classifier.
For, KS, CKS and LSOM, we find the one-
to-one correspondence between documents in the
source language and documents in the target lan-
guage. Then, we assign class labels of the target
documents according to the correspondence.
In order to build a corpus with various lan-
guage pairs for evaluation, we crawled product
reviews from Amazon U.S., German, France and
Japan with five categories: ‘Books’, ‘Electronics’,
‘Music’, ‘Kitchen’, ‘Watch’. The corpus is nei-
ther sentence level parallel nor comparable. For
each category, we randomly select 60 documents
as the test data (M=300) for all methods and 60
documents as the training data (N=300) for KS,
CKS, LSOM and LSM(300). We also compared
latent semantic matching with 120 training docu-
ments for each category (N=600), and called this
method LSM(600). Note that since KS, CKS and
LSOM require that the data sizes are the same for
source and target domains, they cannot use train-
ing data more than test data. To avoid local opti-
mum solutions of NMF, we executed our methods
with 100 different initialization values and chose
the solution that achieved the best objective func-
tion of KS.
</bodyText>
<subsectionHeader confidence="0.860279">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999866285714286">
Table 1 shows average accuracies with standard
division over all language pairs. From the table,
classification accuracy of all methods significantly
outperformed random classifier (accuracy=0.2).
The results showed the effectiveness of both un-
supervised object matching and latent semantic
matching. When comparing LSM(300) with KS,
CKS and LSOM, LSM(300) obtained better re-
sults than these unsupervised object matching
methods. The result supports the effectiveness of
the latent topic matching. Moreover, LSM(600)
achieved the highest accuracy. There are large dif-
ferences between LSM(600) and the others. This
result implies not only the effectiveness of the la-
tent topic matching but also increasing the number
of source side documents (labeled training data)
contributes to improving classification accuracy.
This is natural in terms of supervised learning but
only our method can deal with source side docu-
ments that are larger in number.
Table 2 shows examples of latent topics in
English and German extracted and aligned by
LSM(600). We can see that some author names,
words related to camera, and cooking equipment
appear in ‘Books’, ‘Electronics’ and ‘Kitchen’
topics, respectively. Similarity, there are some
artists’ names in ‘Music’ and watch brands in
‘Watch’.
</bodyText>
<page confidence="0.998515">
215
</page>
<sectionHeader confidence="0.999369" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999994736842105">
As an extension of unsupervised object matching,
this paper proposed latent semantic matching that
considers the shared latent space between two lan-
guage domains. To generate such a space, top-
ics of the target space are permuted by exploit-
ing unsupervised object matching. We can mea-
sure distances between objects by standard met-
rics, which enable us retrieving k-nearest objects
in the source domain for a query object in the tar-
get domain. This is a significant advantage over
conventional unsupervised object matching meth-
ods. We used Amazon review corpus to demon-
strate the effectiveness of our method on cross-
language text categorization. The results showed
that our method outperformed conventional object
matching methods with the same number of train-
ing samples. Moreover, our method achieved even
higher performance by utilizing more documents
in the source domain.
</bodyText>
<sectionHeader confidence="0.996922" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99992425">
The authors would like to thank Nemanja Djuric
for providing code for Convex Kernelized Sorting
and the three anonymous reviewers for thoughtful
suggestions.
</bodyText>
<sectionHeader confidence="0.999125" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998861879310345">
Francis Bach and Michael Jordan. 2005. A probabilis-
tic interpretation of canonical correlation analysis.
Technical report, Department of Statistics, Univer-
sity of California, Berkeley.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. JMLR, 3(Jan.):993–
1022.
Jordan Boyd-Graber and David Blei. 2009. Multilin-
gual topic model for unaligned text. In Proc. of the
25th UAI, pages 75–82.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.
Nemanja Djuric, Mihajlo Grbovic, and Slobodan
Vucetic. 2012. Convex kernelized sorting. In Proc.
of the 26th AAAI, pages 893–899.
Susan Dumais, Lanauer Thomas, and Michael Littman.
1996. Automatic cross-linguistic information re-
trieval using latent semantic indexing. In Proc.
of the Workshop on Cross-Linguistic Information
Retieval in SIGIR, pages 16–23.
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage text categorization by acquiring multilingual
domain models from comparable corpora. In Proc.
of the ACL Workshop on Building and Using Paral-
lel Texts, pages 9–16.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. of ACL-08:
HLT, pages 771–779.
Jagarlamudi Jagadeesh and Hal Daume III. 2010. Ex-
tracting multilingual topics from unaligned corpora.
In Proc of the 32nd ECIR, pages 444–456.
Daniel Lee and Sebastian Seung. 2000. Algorithm
for non-negative matrix factorization. In Advances
in Neural Information Processing Systems 13, pages
556–562.
Quadrianto Novi, Smola Alexander, Song Le, and
Tuytelaars Tinne. 2010. Kernelized sorting. IEEE
Trans. on Pattern Analysis and Machine Intelli-
gence, 32(10):1809–1821.
Jhon Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representation from discrim-
inative projections. In Proc. of the 2010 Conference
on EMNLP, pages 251–261.
Abhishek Tripathi, Arto Klami, and Sami Virpioja.
2010. Bilingual sentence matching using kernel
CCA. In Proc. of the 2010 IEEE International
Workshop on MLSP, pages 130–135.
Ni Xiaochuan, Sun Lian-Tao, Hu Jian, and Chen
Zheng. 2011. Cross lingual text classification by
mining multilingual topics from wikipedia. In Proc.
of the 4th WSDM, pages 375–384.
Makoto Yamada and Masashi Sugiyama. 2011. Cross-
domain object matching with model selection. In
Proc. of the 14th AISTATS, pages 807–815.
</reference>
<page confidence="0.999136">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.859882">
<title confidence="0.998782">Latent Semantic Matching: Application to Cross-language Categorization without Alignment Information</title>
<author confidence="0.976421">Hirao Iwata</author>
<affiliation confidence="0.973332">NTT Communication Science Laboratories, NTT</affiliation>
<address confidence="0.936281">2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237,</address>
<abstract confidence="0.997847869565217">Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To althese limitations, we proposes lasemantic which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bach</author>
<author>Michael Jordan</author>
</authors>
<title>A probabilistic interpretation of canonical correlation analysis.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Department of Statistics, University of California, Berkeley.</institution>
<contexts>
<context position="6123" citStr="Bach and Jordan, 2005" startWordPosition="912" endWordPosition="916">2010) proposed kernelized sorting (KS); it finds one-toone correspondences between objects in different domains by permuting a set to maximize the dependence between two sets. Here, the HilbertSchmidt independence criterion is used for measuring dependence. (Djuric et al., 2012) proposed convex kernelized sorting as an extension of KS. (Yamada and Sugiyama, 2011) proposed leastsquares object matching which maximizes the squared-loss mutual information between matched pairs. (Haghighi et al., 2008) proposed another framework, matching CCA (MCCA), based on a probabilistic interpretation of CCA (Bach and Jordan, 2005). MCCA simultaneously finds latent variables that represent correspondences and latent features so that the latent features of corresponding examples exhibit the maximum correlation. However, these unsupervised object matching methods have limitations. They require that the source and target domains have the same data size, and they find one-to-one correspondences. There are critical weaknesses of these methods when we attempt to apply them to real world cross-language NLP applications. 3 Latent Semantic Matching We propose latent semantic matching to find a shared latent space by assuming tha</context>
</contexts>
<marker>Bach, Jordan, 2005</marker>
<rawString>Francis Bach and Michael Jordan. 2005. A probabilistic interpretation of canonical correlation analysis. Technical report, Department of Statistics, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<date>2003</date>
<booktitle>Latent Dirichlet allocation. JMLR, 3(Jan.):993–</booktitle>
<pages>1022</pages>
<contexts>
<context position="4979" citStr="Blei et al., 2003" startWordPosition="743" endWordPosition="746">2 Related work Many cross-language text processing methods have been proposed that require correspondences between source and target languages. For example, (Dumais et al., 1996) proposed cross-lingual latent semantic indexing, and (Platt et al., 2010) employed oriented principle component analysis and canonical correlation analysis (CCA). They concatenate the document pairs (source document and its translation) obtained from a documentlevel parallel corpus. They then apply multivariate analysis to acquire the translingual projection. There are extensions of latent Dirichlet allocation (LDA) (Blei et al., 2003) for cross-language analysis, such as multilingual topic models (BoydGraber and Blei, 2009), joint LDA (Jagadeesh and Daume III, 2010) and multilingual LDA (Xiaochuan et al., 2011). They require a bilingual dictionary or document-level parallel corpora. Unsupervised object matching methods have been proposed recently (Novi et al., 2010; Haghighi et al., 2008; Yamada and Sugiyama, 2011). These methods are promising in terms of language portability because they do not require external language resources. (Novi et al., 2010) proposed kernelized sorting (KS); it finds one-toone correspondences bet</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. JMLR, 3(Jan.):993– 1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
</authors>
<title>Multilingual topic model for unaligned text.</title>
<date>2009</date>
<booktitle>In Proc. of the 25th UAI,</booktitle>
<pages>75--82</pages>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>Jordan Boyd-Graber and David Blei. 2009. Multilingual topic model for unaligned text. In Proc. of the 25th UAI, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="3646" citStr="Deerwester et al., 1990" startWordPosition="552" endWordPosition="556">rget domain. In this paper, we propose a simple but effective method to find the shared space by assuming that two languages have common latent topics, which we call latent semantic matching. With latent semantic matching, we first find latent topics in two domains independently. Then, the topics in two domains are aligned by kernelized sorting, and objects are embedded in a shared latent topic space. Latent topic representations are successfully used in a wide range of NLP tasks, such as information retrieval and text classification, because they represent intrinsic information of documents (Deerwester et al., 1990). By matching latent topics, we can find relation between source and target domains, and additionally we can handle different numbers of objects in two domains. We compared latent semantic matching with conventional unsupervised object matching meth212 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 212–216, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ods on the task of cross-language text categorization, i.e. classifying target side unlabeled documents by label information obtained from source side documents</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nemanja Djuric</author>
<author>Mihajlo Grbovic</author>
<author>Slobodan Vucetic</author>
</authors>
<title>Convex kernelized sorting.</title>
<date>2012</date>
<booktitle>In Proc. of the 26th AAAI,</booktitle>
<pages>893--899</pages>
<contexts>
<context position="5780" citStr="Djuric et al., 2012" startWordPosition="864" endWordPosition="867"> They require a bilingual dictionary or document-level parallel corpora. Unsupervised object matching methods have been proposed recently (Novi et al., 2010; Haghighi et al., 2008; Yamada and Sugiyama, 2011). These methods are promising in terms of language portability because they do not require external language resources. (Novi et al., 2010) proposed kernelized sorting (KS); it finds one-toone correspondences between objects in different domains by permuting a set to maximize the dependence between two sets. Here, the HilbertSchmidt independence criterion is used for measuring dependence. (Djuric et al., 2012) proposed convex kernelized sorting as an extension of KS. (Yamada and Sugiyama, 2011) proposed leastsquares object matching which maximizes the squared-loss mutual information between matched pairs. (Haghighi et al., 2008) proposed another framework, matching CCA (MCCA), based on a probabilistic interpretation of CCA (Bach and Jordan, 2005). MCCA simultaneously finds latent variables that represent correspondences and latent features so that the latent features of corresponding examples exhibit the maximum correlation. However, these unsupervised object matching methods have limitations. They</context>
</contexts>
<marker>Djuric, Grbovic, Vucetic, 2012</marker>
<rawString>Nemanja Djuric, Mihajlo Grbovic, and Slobodan Vucetic. 2012. Convex kernelized sorting. In Proc. of the 26th AAAI, pages 893–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
<author>Lanauer Thomas</author>
<author>Michael Littman</author>
</authors>
<title>Automatic cross-linguistic information retrieval using latent semantic indexing.</title>
<date>1996</date>
<booktitle>In Proc. of the Workshop on Cross-Linguistic Information Retieval in SIGIR,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2002" citStr="Dumais et al., 1996" startWordPosition="277" endWordPosition="281">tween the domains. Kernelized sorting (Novi et al., 2010) and canonical correlation analysis based methods (Haghighi et al., 2008; Tripathi et al., 2010) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks. One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages. This distinguishes it from other crosslanguage NLP methods such as machine translation based and projection based approaches (Dumais et al., 1996; Gliozzo and Strapparava, 2005; Platt et al., 2010), which we need bilingual dictionaries or parallel sentences. When we apply unsupervised object matching methods to cross-language NLP tasks, there are two critical problems. The first is that they only find one-to-one matching. The second is they require the same size of source- and target-data. For example, the correct translation of a word is not always unique. French words ‘maison’, ‘appartment’ and ‘domicile’ can be regarded as translation of an English word ‘home’. In addition, English vocabulary size is not equal to that of French. The</context>
<context position="4539" citStr="Dumais et al., 1996" startWordPosition="681" endWordPosition="684">1st Annual Meeting of the Association for Computational Linguistics, pages 212–216, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ods on the task of cross-language text categorization, i.e. classifying target side unlabeled documents by label information obtained from source side documents. The results show that, with more source side documents, our method achieved the highest classification accuracy. 2 Related work Many cross-language text processing methods have been proposed that require correspondences between source and target languages. For example, (Dumais et al., 1996) proposed cross-lingual latent semantic indexing, and (Platt et al., 2010) employed oriented principle component analysis and canonical correlation analysis (CCA). They concatenate the document pairs (source document and its translation) obtained from a documentlevel parallel corpus. They then apply multivariate analysis to acquire the translingual projection. There are extensions of latent Dirichlet allocation (LDA) (Blei et al., 2003) for cross-language analysis, such as multilingual topic models (BoydGraber and Blei, 2009), joint LDA (Jagadeesh and Daume III, 2010) and multilingual LDA (Xia</context>
</contexts>
<marker>Dumais, Thomas, Littman, 1996</marker>
<rawString>Susan Dumais, Lanauer Thomas, and Michael Littman. 1996. Automatic cross-linguistic information retrieval using latent semantic indexing. In Proc. of the Workshop on Cross-Linguistic Information Retieval in SIGIR, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Cross language text categorization by acquiring multilingual domain models from comparable corpora.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2033" citStr="Gliozzo and Strapparava, 2005" startWordPosition="282" endWordPosition="285">rnelized sorting (Novi et al., 2010) and canonical correlation analysis based methods (Haghighi et al., 2008; Tripathi et al., 2010) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks. One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages. This distinguishes it from other crosslanguage NLP methods such as machine translation based and projection based approaches (Dumais et al., 1996; Gliozzo and Strapparava, 2005; Platt et al., 2010), which we need bilingual dictionaries or parallel sentences. When we apply unsupervised object matching methods to cross-language NLP tasks, there are two critical problems. The first is that they only find one-to-one matching. The second is they require the same size of source- and target-data. For example, the correct translation of a word is not always unique. French words ‘maison’, ‘appartment’ and ‘domicile’ can be regarded as translation of an English word ‘home’. In addition, English vocabulary size is not equal to that of French. These discussions motivate us to i</context>
</contexts>
<marker>Gliozzo, Strapparava, 2005</marker>
<rawString>Alfio Gliozzo and Carlo Strapparava. 2005. Cross language text categorization by acquiring multilingual domain models from comparable corpora. In Proc. of the ACL Workshop on Building and Using Parallel Texts, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="1512" citStr="Haghighi et al., 2008" startWordPosition="199" endWordPosition="202">e proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods. 1 Introduction Unsupervised object matching is a method for finding one-to-one correspondences between objects across different domains without knowledge about the relation between the domains. Kernelized sorting (Novi et al., 2010) and canonical correlation analysis based methods (Haghighi et al., 2008; Tripathi et al., 2010) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks. One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages. This distinguishes it from other crosslanguage NLP methods such as machine translation based and projection based approaches (Dumais et al., 1996; Gliozzo and Strapparava, 2005; Platt et al., 2010), which we need bilingual dictionaries or parallel sentenc</context>
<context position="5339" citStr="Haghighi et al., 2008" startWordPosition="797" endWordPosition="800">e the document pairs (source document and its translation) obtained from a documentlevel parallel corpus. They then apply multivariate analysis to acquire the translingual projection. There are extensions of latent Dirichlet allocation (LDA) (Blei et al., 2003) for cross-language analysis, such as multilingual topic models (BoydGraber and Blei, 2009), joint LDA (Jagadeesh and Daume III, 2010) and multilingual LDA (Xiaochuan et al., 2011). They require a bilingual dictionary or document-level parallel corpora. Unsupervised object matching methods have been proposed recently (Novi et al., 2010; Haghighi et al., 2008; Yamada and Sugiyama, 2011). These methods are promising in terms of language portability because they do not require external language resources. (Novi et al., 2010) proposed kernelized sorting (KS); it finds one-toone correspondences between objects in different domains by permuting a set to maximize the dependence between two sets. Here, the HilbertSchmidt independence criterion is used for measuring dependence. (Djuric et al., 2012) proposed convex kernelized sorting as an extension of KS. (Yamada and Sugiyama, 2011) proposed leastsquares object matching which maximizes the squared-loss m</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proc. of ACL-08: HLT, pages 771–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagarlamudi Jagadeesh</author>
<author>Hal Daume</author>
</authors>
<title>Extracting multilingual topics from unaligned corpora.</title>
<date>2010</date>
<booktitle>In Proc of the 32nd ECIR,</booktitle>
<pages>444--456</pages>
<marker>Jagadeesh, Daume, 2010</marker>
<rawString>Jagarlamudi Jagadeesh and Hal Daume III. 2010. Extracting multilingual topics from unaligned corpora. In Proc of the 32nd ECIR, pages 444–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Lee</author>
<author>Sebastian Seung</author>
</authors>
<title>Algorithm for non-negative matrix factorization.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 13,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="8172" citStr="Lee and Seung, 2000" startWordPosition="1251" endWordPosition="1254">. Here, each element of the vectors indicates the TF·IDF score of the corresponding word in the document. I is the size of the feature set, i.e., the vocabulary size in the source domain. Also, we have M documents in the target domain.t�=(���)�3=1 is the mth document represented as a multi-dimensional vector. J is the vocabulary size in the target domain. Thus, the data set in the source domain is represented by an I x N matrix, S=(s1, · · · , sN), the data set in the target is represented by a J x M matrix, T=(t1, ··· , tm). We factorize these matrices using nonnegative matrix factorization (Lee and Seung, 2000) to find topics as follows: S Pt� WSHS, (1) T Pt� WTHT. (2) WS is an I xK matrix that represents a set of topics, i.e. each column vector denotes word weights for each topic. HS is a K x N matrix that denotes a set of latent semantic representations of documents in the source domain, i.e. each row 213 Figure 1: Topic alignments. vector denotes an embedding of a document in the K-dimensional latent space. Similarly, WT is an I × K matrix that represents a set of topics in the target domain, and HT is a K × M matrix that denotes a set of latent semantic representations of target documents. K is </context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel Lee and Sebastian Seung. 2000. Algorithm for non-negative matrix factorization. In Advances in Neural Information Processing Systems 13, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quadrianto Novi</author>
<author>Smola Alexander</author>
<author>Song Le</author>
<author>Tuytelaars Tinne</author>
</authors>
<title>Kernelized sorting.</title>
<date>2010</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>10</issue>
<contexts>
<context position="1440" citStr="Novi et al., 2010" startWordPosition="188" endWordPosition="191"> natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods. 1 Introduction Unsupervised object matching is a method for finding one-to-one correspondences between objects across different domains without knowledge about the relation between the domains. Kernelized sorting (Novi et al., 2010) and canonical correlation analysis based methods (Haghighi et al., 2008; Tripathi et al., 2010) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks. One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages. This distinguishes it from other crosslanguage NLP methods such as machine translation based and projection based approaches (Dumais et al., 1996; Gliozzo and Strapparava, 2005; Platt</context>
<context position="5316" citStr="Novi et al., 2010" startWordPosition="793" endWordPosition="796">A). They concatenate the document pairs (source document and its translation) obtained from a documentlevel parallel corpus. They then apply multivariate analysis to acquire the translingual projection. There are extensions of latent Dirichlet allocation (LDA) (Blei et al., 2003) for cross-language analysis, such as multilingual topic models (BoydGraber and Blei, 2009), joint LDA (Jagadeesh and Daume III, 2010) and multilingual LDA (Xiaochuan et al., 2011). They require a bilingual dictionary or document-level parallel corpora. Unsupervised object matching methods have been proposed recently (Novi et al., 2010; Haghighi et al., 2008; Yamada and Sugiyama, 2011). These methods are promising in terms of language portability because they do not require external language resources. (Novi et al., 2010) proposed kernelized sorting (KS); it finds one-toone correspondences between objects in different domains by permuting a set to maximize the dependence between two sets. Here, the HilbertSchmidt independence criterion is used for measuring dependence. (Djuric et al., 2012) proposed convex kernelized sorting as an extension of KS. (Yamada and Sugiyama, 2011) proposed leastsquares object matching which maxim</context>
<context position="9563" citStr="Novi et al., 2010" startWordPosition="1498" endWordPosition="1501">ity is K. 3.2 Finding Optimal Topic Alignments by Unsupervised Object Matching To connect the different latent spaces, topics extracted from the source language must be aligned to one from the target language. This is reasonable because we can assume that both languages share the same latent concept. However, we cannot quantify the similarity between the topics because we do not have any external language resources such as a dictionary. Therefore, we utilize unsupervised object matching method to find one-to-one correspondences between topics. In this paper, we employ kernelized sorting (KS) (Novi et al., 2010). KS finds the best one-to-one matching as follows: 7r* = arg max tr(GS7rTGT7r), 7rErIK s.t. 7r1K=1K and 7rT1K=1K. (3) Here, 7r is a K ×K matrix that represents the oneto-one correspondence between topics, i.e. 7rij=1 indicates that the ith topic in the source language corresponds to the jth one of the target language. Overall Average 0.252 ± 0.112 0.249 ± 0.033 0.278 ± 0.086 0.298 ± 0.077 0.359 ± 0.062 Table 1: Average accuracy over all language pairs HK indicates the set of all possible matrices storing one-to-one correspondences. G denotes the K × K kernel matrix obtained from topic proport</context>
</contexts>
<marker>Novi, Alexander, Le, Tinne, 2010</marker>
<rawString>Quadrianto Novi, Smola Alexander, Song Le, and Tuytelaars Tinne. 2010. Kernelized sorting. IEEE Trans. on Pattern Analysis and Machine Intelligence, 32(10):1809–1821.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jhon Platt</author>
<author>Kristina Toutanova</author>
<author>Wen-tau Yih</author>
</authors>
<title>Translingual document representation from discriminative projections.</title>
<date>2010</date>
<booktitle>In Proc. of the 2010 Conference on EMNLP,</booktitle>
<pages>251--261</pages>
<contexts>
<context position="2054" citStr="Platt et al., 2010" startWordPosition="286" endWordPosition="289">2010) and canonical correlation analysis based methods (Haghighi et al., 2008; Tripathi et al., 2010) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks. One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages. This distinguishes it from other crosslanguage NLP methods such as machine translation based and projection based approaches (Dumais et al., 1996; Gliozzo and Strapparava, 2005; Platt et al., 2010), which we need bilingual dictionaries or parallel sentences. When we apply unsupervised object matching methods to cross-language NLP tasks, there are two critical problems. The first is that they only find one-to-one matching. The second is they require the same size of source- and target-data. For example, the correct translation of a word is not always unique. French words ‘maison’, ‘appartment’ and ‘domicile’ can be regarded as translation of an English word ‘home’. In addition, English vocabulary size is not equal to that of French. These discussions motivate us to introduce a shared spa</context>
<context position="4613" citStr="Platt et al., 2010" startWordPosition="691" endWordPosition="694">212–216, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ods on the task of cross-language text categorization, i.e. classifying target side unlabeled documents by label information obtained from source side documents. The results show that, with more source side documents, our method achieved the highest classification accuracy. 2 Related work Many cross-language text processing methods have been proposed that require correspondences between source and target languages. For example, (Dumais et al., 1996) proposed cross-lingual latent semantic indexing, and (Platt et al., 2010) employed oriented principle component analysis and canonical correlation analysis (CCA). They concatenate the document pairs (source document and its translation) obtained from a documentlevel parallel corpus. They then apply multivariate analysis to acquire the translingual projection. There are extensions of latent Dirichlet allocation (LDA) (Blei et al., 2003) for cross-language analysis, such as multilingual topic models (BoydGraber and Blei, 2009), joint LDA (Jagadeesh and Daume III, 2010) and multilingual LDA (Xiaochuan et al., 2011). They require a bilingual dictionary or document-leve</context>
</contexts>
<marker>Platt, Toutanova, Yih, 2010</marker>
<rawString>Jhon Platt, Kristina Toutanova, and Wen-tau Yih. 2010. Translingual document representation from discriminative projections. In Proc. of the 2010 Conference on EMNLP, pages 251–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhishek Tripathi</author>
<author>Arto Klami</author>
<author>Sami Virpioja</author>
</authors>
<title>Bilingual sentence matching using kernel CCA.</title>
<date>2010</date>
<booktitle>In Proc. of the 2010 IEEE International Workshop on MLSP,</booktitle>
<pages>130--135</pages>
<contexts>
<context position="1536" citStr="Tripathi et al., 2010" startWordPosition="203" endWordPosition="206">tic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods. 1 Introduction Unsupervised object matching is a method for finding one-to-one correspondences between objects across different domains without knowledge about the relation between the domains. Kernelized sorting (Novi et al., 2010) and canonical correlation analysis based methods (Haghighi et al., 2008; Tripathi et al., 2010) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks. One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages. This distinguishes it from other crosslanguage NLP methods such as machine translation based and projection based approaches (Dumais et al., 1996; Gliozzo and Strapparava, 2005; Platt et al., 2010), which we need bilingual dictionaries or parallel sentences. When we apply unsupe</context>
</contexts>
<marker>Tripathi, Klami, Virpioja, 2010</marker>
<rawString>Abhishek Tripathi, Arto Klami, and Sami Virpioja. 2010. Bilingual sentence matching using kernel CCA. In Proc. of the 2010 IEEE International Workshop on MLSP, pages 130–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Xiaochuan</author>
<author>Sun Lian-Tao</author>
<author>Hu Jian</author>
<author>Chen Zheng</author>
</authors>
<title>Cross lingual text classification by mining multilingual topics from wikipedia.</title>
<date>2011</date>
<booktitle>In Proc. of the 4th WSDM,</booktitle>
<pages>375--384</pages>
<contexts>
<context position="5159" citStr="Xiaochuan et al., 2011" startWordPosition="770" endWordPosition="774">96) proposed cross-lingual latent semantic indexing, and (Platt et al., 2010) employed oriented principle component analysis and canonical correlation analysis (CCA). They concatenate the document pairs (source document and its translation) obtained from a documentlevel parallel corpus. They then apply multivariate analysis to acquire the translingual projection. There are extensions of latent Dirichlet allocation (LDA) (Blei et al., 2003) for cross-language analysis, such as multilingual topic models (BoydGraber and Blei, 2009), joint LDA (Jagadeesh and Daume III, 2010) and multilingual LDA (Xiaochuan et al., 2011). They require a bilingual dictionary or document-level parallel corpora. Unsupervised object matching methods have been proposed recently (Novi et al., 2010; Haghighi et al., 2008; Yamada and Sugiyama, 2011). These methods are promising in terms of language portability because they do not require external language resources. (Novi et al., 2010) proposed kernelized sorting (KS); it finds one-toone correspondences between objects in different domains by permuting a set to maximize the dependence between two sets. Here, the HilbertSchmidt independence criterion is used for measuring dependence. </context>
</contexts>
<marker>Xiaochuan, Lian-Tao, Jian, Zheng, 2011</marker>
<rawString>Ni Xiaochuan, Sun Lian-Tao, Hu Jian, and Chen Zheng. 2011. Cross lingual text classification by mining multilingual topics from wikipedia. In Proc. of the 4th WSDM, pages 375–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Yamada</author>
<author>Masashi Sugiyama</author>
</authors>
<title>Crossdomain object matching with model selection.</title>
<date>2011</date>
<booktitle>In Proc. of the 14th AISTATS,</booktitle>
<pages>807--815</pages>
<contexts>
<context position="5367" citStr="Yamada and Sugiyama, 2011" startWordPosition="801" endWordPosition="804">ource document and its translation) obtained from a documentlevel parallel corpus. They then apply multivariate analysis to acquire the translingual projection. There are extensions of latent Dirichlet allocation (LDA) (Blei et al., 2003) for cross-language analysis, such as multilingual topic models (BoydGraber and Blei, 2009), joint LDA (Jagadeesh and Daume III, 2010) and multilingual LDA (Xiaochuan et al., 2011). They require a bilingual dictionary or document-level parallel corpora. Unsupervised object matching methods have been proposed recently (Novi et al., 2010; Haghighi et al., 2008; Yamada and Sugiyama, 2011). These methods are promising in terms of language portability because they do not require external language resources. (Novi et al., 2010) proposed kernelized sorting (KS); it finds one-toone correspondences between objects in different domains by permuting a set to maximize the dependence between two sets. Here, the HilbertSchmidt independence criterion is used for measuring dependence. (Djuric et al., 2012) proposed convex kernelized sorting as an extension of KS. (Yamada and Sugiyama, 2011) proposed leastsquares object matching which maximizes the squared-loss mutual information between ma</context>
</contexts>
<marker>Yamada, Sugiyama, 2011</marker>
<rawString>Makoto Yamada and Masashi Sugiyama. 2011. Crossdomain object matching with model selection. In Proc. of the 14th AISTATS, pages 807–815.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>