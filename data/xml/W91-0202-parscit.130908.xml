<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000946">
<title confidence="0.9914835">
Syntax-Driven and Ontology-Driven Lexical
Semantics
</title>
<author confidence="0.991555">
Sergei Nirenburg and Lori Levin
</author>
<affiliation confidence="0.996214666666667">
Center for Machine Translation
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.99355">
{sergei, 1s1}@nl.cs.cmu.edu
</email>
<sectionHeader confidence="0.976672" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999465166666667">
In this position paper we describe the scopes of two schools in lexical semantics,
which we call syntax-driven lexical semantics and ontology-driven lexical semantics,
respectively. Both approaches are used in various applications at The Center for
Machine Translation. We believe that a comparative analysis of these positions and
clarification of claims and coverage is essential for the field as a whole.
There are different traditions in the study of lexical semantics. Two of them seem to
be the most current in computational linguistics and its applications — the one based
on syntactic theory studies and the other, on the artificial intelligence approaches. The
former seeks to discover semantic properties of lexical items from which syntactic behavior
(such as subcategorization and participation in transitivity alternations) is predictable.
(See Grimshaw, 1990; B. Levin Sz Rappaport Hovav, 1990.) The latter tries to establish
the meaning of natural language texts with the help of an independently constructed
&amp;quot;world model&amp;quot; (often called &amp;quot;ontology&amp;quot;) which explicates relations among entities in the
world rather than lexical units.
It is customary to pitch the two approaches as competing. And in practice, researchers
typically develop lexical-semantic theories and computer systems based on them using
only one of the two. We would like to argue, however, that these approaches to semantics
are much closer to one another in their aims and research methodologies than to any
other schools of semantics (&amp;quot;logical,&amp;quot; &amp;quot;philosophical,&amp;quot; &amp;quot;formal,&amp;quot; etc.) From a practical
standpoint, we also have a different experience to report. We believe that neither on
theoretical grounds nor with respect to computational applications is there a necessity
to make one of the approaches prominent at the expense of the other. We have carried
out both theoretical and practical work in which the approaches seem to coexist. The
central role of the syntax-driven lexical semantics in the process of NLP is to decode the
nature of the dependency between heads of phrases and their arguments in a particular
language. This knowledge is then used in ontology-driven lexical semantics as a necessary
set of heuristics which allow us to represent the meaning of a text in terms of a language-
independent conceptual model. Thus, we believe that a comprehensive approach should
combine the benefits of both approaches and that neither will on its own be sufficient for
realistic NLP.
</bodyText>
<sectionHeader confidence="0.905183" genericHeader="method">
1 Syntax-Driven Lexical Semantics
</sectionHeader>
<bodyText confidence="0.985071">
In a knowledge-based system, the primary role of syntactic analysis is to decode knowl-
edge that is syntactically encoded. An important component of such knowledge is the
</bodyText>
<page confidence="0.995505">
9
</page>
<bodyText confidence="0.999952592592592">
relationship between predicates and their arguments. For example, in order to know that
the English sentence Max interviewed Hester for a job means that Max was considering
hiring Hester and not that Hester was in the position to hire Max, it is necessary to know
that the interviewer role is expressed as the subject of the sentence, that the interviewee
role is expressed as the object, that the subject precedes the verb and that the object
follows it. Much of the information about the syntactic encoding of arguments can be
placed in lexical entries of argument-taking predicates. Classes of predicates with simi-
lar argument encodings usually have some semantic similarity, and thus define a kind of
lexical semantics based on semantic features that are predictors of syntactic behavior.
In light of these observations, our theory of lexical semantics includes a component
of lexical knowledge that describes the syntactic encoding of arguments. This lexical
knowledge consists of Lexical Conceptual Structures (LCS) and Linking Rules. We will
assume that the LCSs of any given language are composed from a universal set of building
blocks (including, but not limited to those described by Jackendoff, 1983, 1990). However,
the presence of lexical gaps—meanings that are lexicalized in some languages but must be
expressed phrasally in another language—suggests that languages do not all share exactly
the same inventory of LCSs. The most important property of the LCSs of any individual
language is that they contain all of the information necessary for Linking Rules to calculate
the syntactic encoding of predicates and arguments. Thus they contain syntactically
relevant semantic information.
Linking Rules describe how LCSs are syntactically encoded in a language and they
partition LCSs into classes of predicates that have similar syntactic encodings. Linking
Rules and the verb classes they define are language-specific (Mitamura, 1989), but gov-
erned by language-universal principles. The particular version of linking rules and verb
classes we have implemented is based on Lexical Functional Grammar (L. Levin, 1987;
Bresnan &amp; Kanerva, 1989), and includes the treatment of the phenomena briefly discussed
in the rest of this section.
</bodyText>
<subsectionHeader confidence="0.9601175">
1.1 Which Grammatical Function Is Associated with Each Argu-
ment?
</subsectionHeader>
<bodyText confidence="0.999744666666667">
Linking Rules associate grammatical functions with arguments in a LCS. For example,
the English verb interview associates its interviewer role with the grammatical function
SUBJECT and its interviewee role with the grammatical function OBJECT. Argument-
taking predicates themselves fall into subcategoriies (or subcategorization classes) based
on the grammatical functions that they assign to their arguments. For example, those
that assign SUBJECT and OBJECT, those that assign SUBJECT and OBLIQUE, etc.
</bodyText>
<subsectionHeader confidence="0.9983015">
1.2 Which Syntactic Position, Case Marker, or Agreement Mor-
pheme Encodes Each Argument?
</subsectionHeader>
<bodyText confidence="0.999851142857143">
After linking LCS arguments to grammatical functions, the next step in the syntactic
encoding of LCS arguments is to assign a syntactic position or morphological marker to
each argument. For example, in English, the interviewer role is syntactically encoded
to the left of the verb. In another language, the verb might specify a case marking for
the interviewer role. Default information about syntactic encoding of arguments (e.g.,
SUBJECTs have nominative case) can be specified in the syntactic rules of the language,
instead of in the lexicon. However, after these defaults are specified, there remains a
</bodyText>
<page confidence="0.996971">
10
</page>
<bodyText confidence="0.9883125">
considerable amount of lexically specific information—which prepositions mark oblique
arguments, which verbs take dative objects, and so on.
</bodyText>
<subsectionHeader confidence="0.998658">
1.3 Which Predicates Participate in Alternate Assignments of
Grammatical Functions?
</subsectionHeader>
<bodyText confidence="0.999929615384615">
Many predicates allow multiple assignments of grammatical functions or case markings to
their arguments. For example, the well-known spray/load alternation involves the assign-
ment of the grammatical function OBJECT or OBLIQUE to an argument that is filled or
covered, as in spray the wall with paint or spray paint on the wall. The passive construc-
tion also results from an alternation in grammatical function assignment to arguments.
An argument that has the OBJECT function in an active sentence has the SUBJECT
function in a passive sentence.
Argument-taking predicates fall into classes according to which alternations they allow.
When a set of LCSs is identified that all allow the same alternations, it is usually possible to
identify a semantic feature that they have in common that correlates with the transitivity
alternation. Hence LCSs that undergo the same transitivity alternations appear to form
semantic classes — for example, the change of state verbs, the spray-load verbs, verbs of
removing, verbs of creation, etc. (See B. Levin (1989) for an extensive list of such classes.)
</bodyText>
<subsectionHeader confidence="0.999268">
1.4 In Which Argument-Structure-Building Operations Does the
Predicate Participate?
</subsectionHeader>
<bodyText confidence="0.99998005">
Talmy (1975, 1985), Jackendoff (1990), and B. Levin &amp; Rapoport (1988) among others
have proposed rules that conflate two lexical meanings to create a new lexical meaning.
Such argument-structure-building rules in English include incorporation of manner of mo-
tion and directed motion (the argument structure for jump and the argument structure for
go combine to form the argument structure for jump into the room); resultative secondary
predication (the argument structure for become and the argument structure for hammer
combine to form the argument structure for hammer the nail flat); and many others. For
these operations, we are assuming that two LCSs are combined to form a new LCS, and
this new argument structure undergoes assignment of grammatical functions and syntac-
tic encoding of arguments. The allowable types of LCS conflation and the classes of verbs
that undergo them vary from language to language.
All of the operations mentioned in this section identify syntactic classes of verbs—the
verbs that undergo the same transitivity alternations, the verbs that map their arguments
onto the same grammatical functions, the verbs that undergo a certain argument structure
building operation, the verbs that encode their arguments syntactically in a certian way.
It is usually possible to find semantic features of LCSs that correlate with these classes—
e.g., a change of state, an effect on a patient, etc. In this sense syntactic patterns of verb
classes define a lexical semantics. This lexical semantics is language-specific in that the
membership of verb classes and the semantic features that they are based on are different
in different languages.
</bodyText>
<page confidence="0.824924">
11
</page>
<sectionHeader confidence="0.822556" genericHeader="method">
2 Ontology-Driven Lexical Semantics
</sectionHeader>
<bodyText confidence="0.999434166666667">
While the focus of attention in syntax-driven lexical semantics is on components of lexical
meaning that determine syntactic behavior, the task of ontology-driven lexical semantics is
to determine and specify meanings of lexical units, suggest how they must be represented
(in the lexicon) and how they contribute to the overall representation of text meaning. We
view this task as a component of the more general task of representing the meaning of a
text in a computational application. This view has been developed mostly in the tradition
of natural language processing within artificial intelligence. (It is impossible to reference
all of the contributors to this, by no means monolithic, paradigm; a very incomplete list of
references might include Wilks (1975), Schank (1973), Sowa (1984), Hirst (1987), articles
in Hobbs and Moore (1985) or Lehnert and Ringle (1982)).
Some of the desiderata for an ontology-driven lexical semantics are discussed in the
remainder of this section.
</bodyText>
<subsectionHeader confidence="0.978658">
2.1 It Must Be Constructive
</subsectionHeader>
<bodyText confidence="0.9999741875">
Among our differences from a typical practice in model-theoretical semantics is our &amp;quot;con-
structive&amp;quot; attitude to the models. We believe that in order to be able to discuss model-
theoretic semantics, the models (that is, the ontologies) must first be constructed and put
in correspondence with the lexicon. This premise also sets us apart from some work in
linguistic semantics, for instance, Jackendoff&apos;s. At the same time, we do not strive to use
a very limited set of primitive concepts.
One way of constructing a model is to come up with a set of properties describing
things in the world, define value sets for these properties and then describe each concept
in the world as a set of particular property-value pairs. For reference purposes such sets
can be named. The result is a world model, or computational ontology. A number of
ontologies has been recently suggested and built, though not al of them with language
processing in mind (cf. Lenat and Guha, 1990, Dahlgren and McDowell, 1989, etc.).
If some of the properties reflect links among ontological units, then the ontology can
be viewed as a multiply interconnected network. Such a network can support &amp;quot;reasoning&amp;quot;
about the world encoded in it, if special heuristic rules for network traversal (including
inheritance) are defined (see, e.g., Woods (1975) Brachman (1983), Horty et al. (1990)).
</bodyText>
<subsectionHeader confidence="0.993614">
2.2 It Must Support Disambiguation
</subsectionHeader>
<bodyText confidence="0.99995875">
In a computational environment, syntax-driven lexical semantics helps distinguish mean-
ings but not represent them. Indeed, in some cases knowledge of predicate-argument
structure can disambiguate a lexical unit entirely. Thus, the three senses of spray in
LDOCE (illustrated by spray paint on a wall; the water sprayed out over the garden and
spray a wall with paint) can be completely disambiguated on the basis of differences in
their predicate-argument structure. However, after the choice has been made, the correct
meaning needs to actually be represented. That is, we need a representation of mean-
ing that shows how the different senses of spray are different, not simply that they are
different.
However, in some cases a lexical unit cannot be completely disambiguated after the
predicate-argument analysis is performed. Consider the English verb hack. We can dis-
tinguish two senses of the word — the cutting and the programming one. The difference
</bodyText>
<page confidence="0.993065">
12
</page>
<bodyText confidence="0.999985571428571">
between these senses cannot be expressed in terms of the predicate argument structure.
We will need to specify, for instance, that in one of the senses the instrument of the event
is a knife or an axe and in the other, a computer. This type of information is, in fact, a
familiar selectional restriction. Specifying selectional restrictions is different from specify-
ing predicate-argument relationships. The specification of selectional restriction falls into
the purview of ontology-driven lexical semantics.
In the lexis realm, ontology-driven lexical semantics provides the residue of mean-
ing beyond the predicate-argument structure. Additional disambiguating power is to be
provided on top of the verb class distinctions of the syntax-driven lexical semantics— for
instance, selectional restrictions have to be formulated. Therefore, the lexicon should con-
tain knowledge that is not required for decoding the syntactic realization of arguments.
Naturally, this calls for additional expressive power in the formalism and, crucially, for
reliance on an independently developed model of the world as the source of meaning
interpretation for lexical units (see next item).
</bodyText>
<subsectionHeader confidence="0.997182">
2.3 Metalanguage Must Be Different from Object Language
</subsectionHeader>
<bodyText confidence="0.999962322580645">
No matter how extensive a computational dictionary is prepared for an application, there
will always be cases when a given set of selectional restrictions will fail to help disambiguate
a text. This means that even more disambiguating power should be added to the arsenal of
a lexical-semantic theory. The cases where selectional restrictions are not powerful enough
are typically cases of metonymy or metaphor. Note that classification of phenomena as
treatable by selectional restrictions or only by metonymy/metaphor processors crucially
depends on the content of the dictionary — the less selectional restriction information in
the dictionary and the fewer word senses distinguished, the more work remains for the
metonymy/metaphor processor.
Since processing metaphor and metonymy is more effort-consuming than using selec-
tional restrictions, it is natural that lexical semanticists have been searching for ways of
reducing the need for the former. One way is to require very large and detailed lexicons.
In part, this is the reason why some researchers suggested that word senses from a natural
language could be used as elements of a metalanguage.
We believe in the distinction of metalanguage and object language and therefore claim
that the meanings of natural language units should not be represented using unadorned
lexical units of the same language. In the most general case, the meaning-encoding met-
alanguage should be a full-blown language-independent conceptual world model.
Constraints like selectional restrictions and distinctions among word senses have to be
expressed using symbols — either words of a natural language or elements of an artificial
&amp;quot;language of thought&amp;quot; (in the sense of Fodor, 1975). One problem with using words of a
natural language is that they are typically themselves ambiguous and therefore a computer
program which has to use them as disambiguation heuristics will run into additional
disambiguation requirements.
Another reason for not basing an ontology on word senses of a natural language is that
there are cases where different word senses, as specified in a human-oriented dictionary,
seem to be divided quite artificially. Consider, for instance, the entry for preempt in
LDOCE. It has four senses all of which conform to a single predicate-argument structure.
All of them have something to do with having authority for preferential treatment with
respect to goods or services. In fact, if one is so bent, one can think about distinguishing
more than four such senses, at about the same level of generality. We believe that such
</bodyText>
<page confidence="0.99564">
13
</page>
<bodyText confidence="0.9995185">
cases are prime candidates for merging the senses into one and treating the semantic
analysis process for them in the metonymy/metaphor mode.
The above seems to argue that if automatic seeding of a computational lexicon is
undertaken from an MRD, a nontrivial manual pass over the resulting lexicon should be
carried out to make sure that the grain size of sense distinction is commensurate with the
lexical and conceptual detail in the corresponding domain model.
</bodyText>
<subsectionHeader confidence="0.9476065">
2.4 Procedures for Assignment of Meaning Are an Integral Part
of the Theory
</subsectionHeader>
<bodyText confidence="0.99985675">
Ontology-driven lexical semantics defines the meaning of a lexical unit through a mapping
between a word sense and an ontological concept (or a part thereof). The process of text
analysis, then, consists essentially of instantiating the appropriate ontological concepts or
modifying property values in concepts that have already been instantiated.&apos;
</bodyText>
<subsectionHeader confidence="0.643084">
2.5 It Must Offer a Treatment of the Phenomena of Synonymy,
Hyponymy and Antonymy
</subsectionHeader>
<bodyText confidence="0.999955333333333">
In a theory where lexical meaning is expressed in terms of mappings into instances of
ontological concepts, these standard lexical-semantic relations are explained in terms of
relations among elements of a world model. Thus, synonymy will be defined as a relation
among the lexical units at least some of whose senses map into the same ontological
concept.2 Lexical unit A is defined to be a hyponym of lexical unit B if the concept in
which at least some of the senses of A map stands in the taxonomical is-a relation to the
concept into which lexical unit B is mapped. Antonyms are defined on lexical units which
map into regions on an attribute scale. Specifically, antonyms map into regions that are
symmetrically positioned around the center point of the scale.3
</bodyText>
<subsectionHeader confidence="0.977903">
2.6 Text Meaning Representation Language Should Not Be Too
Close to the Syntax and Lexis of Any Natural Language
</subsectionHeader>
<bodyText confidence="0.9981853125">
Since the purpose of an LCS in syntax-driven lexical semantics is to reflect the syntacti-
cally relevant semantic properties, that is those properties that determine how an LCS is
mapped into a syntactic dependency representation, the LCSs are usually closely tied to
the syntax and lexis of a specific language. They are therefore not as useful in multilin-
gual computational environments as in monolingual ones. This is because languages differ
in their lexicalization patterns and in patterns of syntactic dependency. Following are
some examples that call for departure from the lexical semantics of individual languages
in mult-lingual processing enviroments.
1Lexical-semantic information is, of course, only one of several components of a text meaning represen-
tation. In practice, additional information (e.g., pragmatic and discourse-related) is encoded in lexicon
entries and is represented as part of text meaning.
21t should be noted that in our application of lexical semantics, the ION YtilIS project, such mappings
are allowed to be constrained, not just univocal (for instance, the English verb taxi maps, in one of its
senses, to the concept move-on-surface but only if the theme of this instance is constrained to the concept
of airplane or one of its ontological descendents) — see Onyshkevich and Nirenburg (1991) for additional
detail.
</bodyText>
<footnote confidence="0.432016">
3 Note that synonymy on scalars is defined as mapping onto the same region of a scale.
</footnote>
<page confidence="0.991105">
14
</page>
<listItem confidence="0.983173">
• A head–modifier relationship can be realized in opposing ways in different languages;
thus, a morpheme carrying the aspectual meaning can be realized as the head of
a phrase (like the verb continue or finish), with a word denoting an event as its
complement; alternatively, the word denoting the event can be realized as the main
verb while the aspectual meaning can be realized through an adverb or even a affix.
• Lexical Gaps: Russian has no word that corresponds exactly to the English word
afford (as in I can&apos;t afford X or I can&apos;t afford to Y). In a multilingual process-
ing environment, there might be a concept corresponding to a sense of the English
word afford. A Russian sentence Ja ne mogu sae etogo pozvolit&apos; (I can&apos;t allow my-
self this), uttered in a context of acquisition — which could have been assigned a
straightforward lexical-semantic representation if we were building a lexical seman-
tics for Russian alone — should involve the concept that represents afford. This
means that if the units of the representation language are chosen so that they are
based on Russian lexis, the meaning of afford will be missing. But this meaning
seems sufficiently basic to be included in an ontology. As a result, if lexical patterns
of many languages are used as the clues for building ontology, the quality of the
latter should increase.
• Different languages describe the same event using different lexical semantics. For
example, the ritual act of washing one&apos;s body in Islam is expressed with a change of
state verb in Arabic (to get washed), as a verb of receiving in Indonesian (to receive
ablutions), and with an agentive verb in French (to make ablutions) (Mahmoud,
1989).
</listItem>
<bodyText confidence="0.801311">
For reasons such as the above, we argue that, at least in some applications, such as
multilingual MT, a more language-independent representation scheme is preferable.
</bodyText>
<sectionHeader confidence="0.8503415" genericHeader="method">
3 Our Position on Some of the Questions Posed in
the Call for Papers
</sectionHeader>
<bodyText confidence="0.9943765">
On the basis of the above discussion, our opinions on some of the questions posed in the
call for papers are as follows.
</bodyText>
<subsectionHeader confidence="0.993246">
3.1 What is World Knowledge and What Is Knowledge of Lan-
guage
</subsectionHeader>
<bodyText confidence="0.99992325">
Lexical knowledge of a language is relevant for mapping phrases in syntactic structures
onto argument positions of a predicate. World knowledge is relevant for representing the
distinctions among the senses of particular predicates and arguments which cannot be
distinguished by distributional characteristics.
</bodyText>
<subsectionHeader confidence="0.9996945">
3.2 Cross-Linguistic Evidence for the Specificity of Lexical Se-
mantic Representation
</subsectionHeader>
<bodyText confidence="0.999514333333333">
Our experience in medium-scale KBMT projects shows that verb classes defined by linking
rules in different languages are not identical. This means that the syntax-driven lexical
semantics component of work on a computational application will be different for every
</bodyText>
<page confidence="0.99491">
15
</page>
<bodyText confidence="0.999718888888889">
language involved. For example, in Arabic there is a transitivity alternation that is similar
to the causative/inchoative alternation (Someone broke the glass / The glass broke) in En-
glish. However, The English and Arabic transitivity alternations apply to different classes
of verbs. The Arabic rule, for instance, applies systematically to verbs of psychological
change of state (excite, please, stun, entertain, confuse, etc.), but the English rule does
not apply to these verbs. Since our goal in developing KBMT systems is to support mul-
tilinguality, we preferred to create a single, deeper, interlingual meaning representation
which will not be subject to the divergences between verb classes in individual languages
(Mitamura, 1989).
</bodyText>
<sectionHeader confidence="0.820722" genericHeader="method">
4 An Example: Two Semantic Structures
</sectionHeader>
<bodyText confidence="0.98670125">
In this section we will show the differences in analyzing a single sentence of English ac-
cording to each of the two methods of semantic analysis. This sentence has been extracted
from a journalistic text, in which it was actually a clause in a compound sentence.
Revenues and earnings will begin to post double digit growth in 1992.
zFrom the standpoint of syntax-driven lexical semantics, the desiderata of analysis
include preserving argument structure and producing a semantic analysis in such a manner
that syntactic realization of predicates and their arguments will be attainable through
linking rules. We will concentrate solely on verbs.
Following Jackendoff (1976, 1983, 1990), we will analyze begin as a change of circum-
stance. In other words, revenues and earnings enter the circumstance of posting double
digit growth. For verbs in the change-of-circumstance class, the argument that changes
circumstance links with the grammatical function SUBJECT. The circumstance can be
realized as a nonfinite complement. We will treat post as a conflation whose meaning is
similar to &amp;quot;show by posting&amp;quot; and which is a member of the same verb class as show or
display in the sense of to have an observable property.4 The linking rules for this class
link the property with the OBJECT grammatical function and link the object that has
the property with the SUBJECT grammatical function.
The LCS for the above example shows two predicates — BEGIN and SHOW-BY-
POSTING. Notice that we have avoided the habitual thematic role names for their argu-
ments and adjuncts. Linking to grammatical functions is shown by markers in parentheses.
</bodyText>
<sectionHeader confidence="0.331594" genericHeader="method">
BEGIN
</sectionHeader>
<tableCaption confidence="0.5029658">
thing-that-changes-circumstance: revenues and earnings (SUBJ)
new-circumstance: SHOW-BY-POSTING (XCONP)
thing-that-has-a-property: revenues and earnings (SUBJ)
property: double digit growth (OBJ)
time: 1992 (Adjunct)
</tableCaption>
<bodyText confidence="0.996119">
zFrom the standpoint of ontology-driven lexical semantics, the desiderata of analysis
include a) independence of the syntax of representation from the syntax of the source text,
b) explicitly representing all implied meanings which are referred to in the source text –
either lexically or through deixis or ellipsis or other means, c) keeping the set of atomic
</bodyText>
<footnote confidence="0.75679175">
4A conflation is a combination of two LCSs, A and B, into a new LCS, C. The type of conflation in this
example is that C has the meaning of &amp;quot;B by A-ing&amp;quot; and follows the linking rules and subcategorization
patterns of B. More on the phenomenon of conflation see in Jackendoff (1990), B. Levin and Rapoport
(1988), Talmy (1975, 1985).
</footnote>
<page confidence="0.996619">
16
</page>
<bodyText confidence="0.999954666666667">
meaning representation elements as low as possible (this latter goal does not imply a desire
to postulate a small fixed set of meaning primitives). An ontology-driven lexical-semantic
representation for the above example can look as follows:5
</bodyText>
<note confidence="0.750461333333333">
clause
head: increase-1
aspect:
</note>
<tableCaption confidence="0.947299083333333">
phase: begin
duration: prolonged
iteration: single
time relative: &amp;quot;after time of speech&amp;quot;
absolute: &amp;quot;during 1992&amp;quot;
increase-1
theme: &amp;quot;revenues and earnings,&apos;
rate: &amp;quot;greater than 10% and less than 100%&amp;quot;
relation-1
type: temporal-before
from: &amp;quot;time of speech&apos;,
to: 1992
</tableCaption>
<bodyText confidence="0.999978363636364">
Note that the verbal meanings are factored out into propositional mganings
clause head and the aspectual and temporal meanings. Note also that the else head is
increase which means that the meanings of begin and post got incorporaled into oWer, -
representations — the former gave rise to the value beep of the:aspectual ,ineuuin4,of
the clause. The latter was understood as a functional verb whose only purpose was&apos; to
allow the English realization of the concept of increasing in a norninalized fashion.
post is treated as a collocant of growth (see e.g., Mel&apos;Zuk, 1981,.Nirenbnrg et•a..,
Smadja and McKeown, 1989 for various treatments of collocations). Properties used to
describe events and objects (such as increase) are predefined in an ontologial doma.jn.
model. Phrases in double quotes are inserted as placeholders, since for the orpoes 9
this comparison their further analysis is immaterial.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999768">
Our approach to lexical semantics combines the benefits of both syntax-driven and ontology-
driven semantics. We have been able to use both types of knowledge in several integrated
applications. We plan to use this experience to formulate a distinct lexical-semantic the-
ory based on this work. As a preparatory step toward formulating such a theory we have
suggested the structure of ontological domain models, lexicons for particular languages
and a text meaning representation language (see Carlson and Nirenburg, 1990; Meyer et
al., 1990, Nirenburg and Defrise, 1991a, b).
</bodyText>
<sectionHeader confidence="0.998692" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9608705">
[1] Brachman, R. 1983. What IS-A and Isn&apos;t: An Analysis of Taxonomic Links in
Semantic Networks. Computer, 16(10), October, 30-36.
</reference>
<footnote confidence="0.901918">
5In this representation we didn&apos;t try to follow any particular knowledge representation formalism.
</footnote>
<page confidence="0.998336">
17
</page>
<reference confidence="0.999915183098592">
[2] Bresnan, J. and J. Kanerva. 1989. Locative Inversion in Chichewa: A Case Study
of Factorization in Grammar. Linguistic Inquiry, 1-50.
[3] Carlson, L. and S. Nirenburg. 1990. World Modeling for NLP. CMU CMT Technical
Report 90-121.
[4] Dahlgren, K. and J. McDowell. 1989. Knowledge Representation for Commonsense
Reasoning with Text. Computational Linguistics, 15, 149-170.
[5] Fodor, J. A. 1975. The Language of Thought. New York: Crowell.
[6] Grimshaw, J. 1990. Argument Structure. Cambridge, MA: MIT Press.
[7] Hirst, G. 1987. Semantic Interpretation and Resolution of Ambiguity. Cam-
bridge University Press.
[8] Hobbs, J. and R. Moore (eds.) 1985. Formal Theories of the Commonsense
World. Norwood, NJ: Ablex.
[9] Horty, J. F., Thomason, R. H., and Touretzky, D. S. (1990) A skeptical theory of
inheritance in nonmonotonic semantic nets. Artificial Intelligence 42(2-3), 311-348.
[10] Jackendoff, R. 1976. Toward an Explanatory Semantic Representation. Linguistic
Inquiry, 89-150.
[11] Jackendoff, R. 1983. Semantics and Cognition. Cambridge, MA: MIT Press.
[12] Jackendoff, R. 1990. Semantic Structures. Cambridge, MA: MIT Press.
[13] Lenat, D. and R. Guha. Building Large Knowledge-Based Systems. Reading,
MA: Addison-Wesley.
[14] Lehnert, W. and M. Ringle (eds.) 1982. Strategies for Natural Language Pro-
cessing. Hillsdale, NJ: Lawrence Erlbaum.
[15] Levin, B. 1989. English Verbal Diathesis. Lexicon Project Working Papers 32.
Center for Cognitive Science, MIT.
[16] Levin, B. and M. Rappaport Hovav. 1990. Wiping the Slate Clean: A Lexical Se-
mantic Exploration. ms. Northwestern University and Bar Ilan University.
[17] Levin, B. and T. Rapoport. 1988. Lexical Subordination. Proceedings of the 24th
Annual Meeting of the Chicago Linguistic Society.
[18] Levin, L. 1986. Operations on Lexical Forms: Unaccusative Rules in Ger-
manic Languages. Ph.D. Dissertation. MIT.
[19] Levin, L. 1987. Toward a Linking Theory of Relation Changing Rules in LFG. CSLI
Report No. CSLI-87-115, November 1987, Center for the Study of Language
and Information, Stanford, California.
[20] Mahmoud, A.T. 1989. A Comparative Study of Middle and Inchoative Alternations
in Arabic and English. Ph.D. Dissertation. University of Pittsburgh.
[21] Mel&apos;euk, I.A. 1981. Meaning-Text Models: A Recent Trend in SOviet Linguistics.
The Annual Review of Anthropology.
[22] Meyer, I., B. Onyshkevych and L. Carlson. 1990. lexicographic Principles amd
Design for Knowledge-Based Machine Translation. CMU CMT Technical Report
90-118.
[23] Mitamura, T. 1989. The Hierarchical Organization of Predicate Frames fOr
Interpretive Mapping in Natural Language Processing. Ph.D. Dissertation.
University of Pittsburgh.
[24] Nirenburg, S., E. Nyberg, R. McCardell, S. Huffmann, E. Kenschaft, and I. Niren-
burg. 1988. DIOGENES-88. Technical Report CMU-CMT-88-107. Center for Ma-
chine Translation, Carnegie Mellon University, Pittsburgh, PA.
[25] Nirenburg, S. and C. Defrise. 1991a. Application-Oriented Computational Seman-
tics. In: R. Johnson and M. Rosner (eds.) Formal Semantics and Computa-
tional Linguistics. Cambridge University Press (in press).
[26] Nirenburg, S. and C. Defrise. 1991b. Aspects of Text Meaning. In: J. Pustejovsky
(ed.). Semantics and the Lexicon. Dordrecht, Holland: Kluwer.
[27] Onyshkevych, B. and S. Nirenburg. 1991. Lexicon, Ontology and Text Meaning.
This volume.
[28] Schank, R. 1973. Identification of Conceptualizations Underlying Natural Language.
In: R. Schank and K. Colby (eds.), Computer Models of Language an.
Thought. San Francisco: Freeman.
[29] Smadja, F. and K. McKeown. 1990. Automatically Extracting and Representing
Collocations for Language Generation. Proceedings of the 28th Annual Meeting of
the ACL.
[30] Sowa, J. 1984.Conceptual Structures: Information PRocessing in Mind and
Machine. Reading, MA: Addison-Wesley.
[31] Talmy, L. 1975. Semantics and Syntax of Motion. In: J.P.Kimball, ed. Syntax and
Semantics, 4. New York: Academic Press. 181-238.
[32] Talmy, L. 1985. Lexicalization Patterns: Semantic Structure in Lexical Forms. In:
T. Shopen (ed.) Language Typology and Syntactic Description, vol. 3. Cam-
bridge University Press. 57-149.
[33] Wilks, Y. 1975. A Preferential, Pattern-Seeking Semantics for Natural Language
Inference. Artificial Intelligence, 6, 144-147.
[34] Woods, W. 1975. What&apos;s in a Link: Foundations of Semantic Networks. In:
D.Bobrow and A. Collins (eds.) Representation and Understanding: Stud-
ies in Cognitive Science. New York: Academic Press.
</reference>
<page confidence="0.999331">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.009169">
<title confidence="0.9962375">Syntax-Driven and Ontology-Driven Semantics</title>
<author confidence="0.998719">Sergei Nirenburg</author>
<author confidence="0.998719">Lori Levin</author>
<affiliation confidence="0.999049">Center for Machine Translation School of Computer Science Carnegie Mellon University</affiliation>
<email confidence="0.974084">sergei@nl.cs.cmu.edu</email>
<email confidence="0.974084">1s1@nl.cs.cmu.edu</email>
<abstract confidence="0.997866456000002">In this position paper we describe the scopes of two schools in lexical semantics, which we call syntax-driven lexical semantics and ontology-driven lexical semantics, respectively. Both approaches are used in various applications at The Center for Machine Translation. We believe that a comparative analysis of these positions and clarification of claims and coverage is essential for the field as a whole. There are different traditions in the study of lexical semantics. Two of them seem to be the most current in computational linguistics and its applications — the one based on syntactic theory studies and the other, on the artificial intelligence approaches. The former seeks to discover semantic properties of lexical items from which syntactic behavior (such as subcategorization and participation in transitivity alternations) is predictable. Grimshaw, 1990; Hovav, 1990.) The latter tries to establish the meaning of natural language texts with the help of an independently constructed &amp;quot;world model&amp;quot; (often called &amp;quot;ontology&amp;quot;) which explicates relations among entities in the world rather than lexical units. It is customary to pitch the two approaches as competing. And in practice, researchers typically develop lexical-semantic theories and computer systems based on them using only one of the two. We would like to argue, however, that these approaches to semantics are much closer to one another in their aims and research methodologies than to any other schools of semantics (&amp;quot;logical,&amp;quot; &amp;quot;philosophical,&amp;quot; &amp;quot;formal,&amp;quot; etc.) From a practical standpoint, we also have a different experience to report. We believe that neither on theoretical grounds nor with respect to computational applications is there a necessity to make one of the approaches prominent at the expense of the other. We have carried out both theoretical and practical work in which the approaches seem to coexist. The central role of the syntax-driven lexical semantics in the process of NLP is to decode the nature of the dependency between heads of phrases and their arguments in a particular language. This knowledge is then used in ontology-driven lexical semantics as a necessary set of heuristics which allow us to represent the meaning of a text in terms of a languageindependent conceptual model. Thus, we believe that a comprehensive approach should combine the benefits of both approaches and that neither will on its own be sufficient for realistic NLP. 1 Syntax-Driven Lexical Semantics In a knowledge-based system, the primary role of syntactic analysis is to decode knowledge that is syntactically encoded. An important component of such knowledge is the 9 relationship between predicates and their arguments. For example, in order to know that English sentence interviewed Hester for a job that Max was considering hiring Hester and not that Hester was in the position to hire Max, it is necessary to know that the interviewer role is expressed as the subject of the sentence, that the interviewee role is expressed as the object, that the subject precedes the verb and that the object follows it. Much of the information about the syntactic encoding of arguments can be placed in lexical entries of argument-taking predicates. Classes of predicates with similar argument encodings usually have some semantic similarity, and thus define a kind of lexical semantics based on semantic features that are predictors of syntactic behavior. In light of these observations, our theory of lexical semantics includes a component of lexical knowledge that describes the syntactic encoding of arguments. This lexical knowledge consists of Lexical Conceptual Structures (LCS) and Linking Rules. We will assume that the LCSs of any given language are composed from a universal set of building blocks (including, but not limited to those described by Jackendoff, 1983, 1990). However, the presence of lexical gaps—meanings that are lexicalized in some languages but must be expressed phrasally in another language—suggests that languages do not all share exactly the same inventory of LCSs. The most important property of the LCSs of any individual language is that they contain all of the information necessary for Linking Rules to calculate the syntactic encoding of predicates and arguments. Thus they contain syntactically relevant semantic information. Linking Rules describe how LCSs are syntactically encoded in a language and they partition LCSs into classes of predicates that have similar syntactic encodings. Linking Rules and the verb classes they define are language-specific (Mitamura, 1989), but governed by language-universal principles. The particular version of linking rules and verb classes we have implemented is based on Lexical Functional Grammar (L. Levin, 1987; Bresnan &amp; Kanerva, 1989), and includes the treatment of the phenomena briefly discussed in the rest of this section. 1.1 Which Grammatical Function Is Associated with Each Argument? Linking Rules associate grammatical functions with arguments in a LCS. For example, English verb its interviewer role with the grammatical function SUBJECT and its interviewee role with the grammatical function OBJECT. Argumenttaking predicates themselves fall into subcategoriies (or subcategorization classes) based on the grammatical functions that they assign to their arguments. For example, those that assign SUBJECT and OBJECT, those that assign SUBJECT and OBLIQUE, etc. 1.2 Which Syntactic Position, Case Marker, or Agreement Morpheme Encodes Each Argument? After linking LCS arguments to grammatical functions, the next step in the syntactic encoding of LCS arguments is to assign a syntactic position or morphological marker to each argument. For example, in English, the interviewer role is syntactically encoded to the left of the verb. In another language, the verb might specify a case marking for the interviewer role. Default information about syntactic encoding of arguments (e.g., SUBJECTs have nominative case) can be specified in the syntactic rules of the language, instead of in the lexicon. However, after these defaults are specified, there remains a 10 considerable amount of lexically specific information—which prepositions mark oblique arguments, which verbs take dative objects, and so on. 1.3 Which Predicates Participate in Alternate Assignments of Grammatical Functions? Many predicates allow multiple assignments of grammatical functions or case markings to arguments. For example, the well-known involves the assignment of the grammatical function OBJECT or OBLIQUE to an argument that is filled or as in spray wall with paint paint on the wall. passive construction also results from an alternation in grammatical function assignment to arguments. An argument that has the OBJECT function in an active sentence has the SUBJECT function in a passive sentence. Argument-taking predicates fall into classes according to which alternations they allow. When a set of LCSs is identified that all allow the same alternations, it is usually possible to identify a semantic feature that they have in common that correlates with the transitivity alternation. Hence LCSs that undergo the same transitivity alternations appear to form semantic classes — for example, the change of state verbs, the spray-load verbs, verbs of removing, verbs of creation, etc. (See B. Levin (1989) for an extensive list of such classes.) 1.4 In Which Argument-Structure-Building Operations Does the Predicate Participate? (1975, 1985), Jackendoff (1990), and B. Levin (1988) among others have proposed rules that conflate two lexical meanings to create a new lexical meaning. Such argument-structure-building rules in English include incorporation of manner of moand directed motion (the argument structure for the argument structure for to form the argument structure for into the room); secondary (the argument structure for the argument structure for to form the argument structure for the nail flat); many others. For these operations, we are assuming that two LCSs are combined to form a new LCS, and this new argument structure undergoes assignment of grammatical functions and syntactic encoding of arguments. The allowable types of LCS conflation and the classes of verbs that undergo them vary from language to language. All of the operations mentioned in this section identify syntactic classes of verbs—the verbs that undergo the same transitivity alternations, the verbs that map their arguments onto the same grammatical functions, the verbs that undergo a certain argument structure building operation, the verbs that encode their arguments syntactically in a certian way. It is usually possible to find semantic features of LCSs that correlate with these classes— e.g., a change of state, an effect on a patient, etc. In this sense syntactic patterns of verb classes define a lexical semantics. This lexical semantics is language-specific in that the membership of verb classes and the semantic features that they are based on are different in different languages. 11 2 Ontology-Driven Lexical Semantics While the focus of attention in syntax-driven lexical semantics is on components of lexical meaning that determine syntactic behavior, the task of ontology-driven lexical semantics is to determine and specify meanings of lexical units, suggest how they must be represented (in the lexicon) and how they contribute to the overall representation of text meaning. We view this task as a component of the more general task of representing the meaning of a text in a computational application. This view has been developed mostly in the tradition of natural language processing within artificial intelligence. (It is impossible to reference all of the contributors to this, by no means monolithic, paradigm; a very incomplete list of references might include Wilks (1975), Schank (1973), Sowa (1984), Hirst (1987), articles in Hobbs and Moore (1985) or Lehnert and Ringle (1982)). Some of the desiderata for an ontology-driven lexical semantics are discussed in the remainder of this section. 2.1 It Must Be Constructive Among our differences from a typical practice in model-theoretical semantics is our &amp;quot;constructive&amp;quot; attitude to the models. We believe that in order to be able to discuss modeltheoretic semantics, the models (that is, the ontologies) must first be constructed and put in correspondence with the lexicon. This premise also sets us apart from some work in linguistic semantics, for instance, Jackendoff&apos;s. At the same time, we do not strive to use a very limited set of primitive concepts. One way of constructing a model is to come up with a set of properties describing things in the world, define value sets for these properties and then describe each concept in the world as a set of particular property-value pairs. For reference purposes such sets can be named. The result is a world model, or computational ontology. A number of ontologies has been recently suggested and built, though not al of them with language processing in mind (cf. Lenat and Guha, 1990, Dahlgren and McDowell, 1989, etc.). If some of the properties reflect links among ontological units, then the ontology can be viewed as a multiply interconnected network. Such a network can support &amp;quot;reasoning&amp;quot; about the world encoded in it, if special heuristic rules for network traversal (including inheritance) are defined (see, e.g., Woods (1975) Brachman (1983), Horty et al. (1990)). 2.2 It Must Support Disambiguation a computational environment, syntax-driven lexical semantics helps meanings but not represent them. Indeed, in some cases knowledge of predicate-argument structure can disambiguate a lexical unit entirely. Thus, the three senses of spray in (illustrated by paint on a wall; the water sprayed out over the garden a wall with paint) be completely disambiguated on the basis of differences in predicate-argument structure. However, choice has been made, the correct meaning needs to actually be represented. That is, we need a representation of meanthat shows different senses of spray are different, not simply that they are different. However, in some cases a lexical unit cannot be completely disambiguated after the analysis is performed. Consider the English verb can distinguish two senses of the word — the cutting and the programming one. The difference 12 between these senses cannot be expressed in terms of the predicate argument structure. We will need to specify, for instance, that in one of the senses the instrument of the event is a knife or an axe and in the other, a computer. This type of information is, in fact, a familiar selectional restriction. Specifying selectional restrictions is different from specifying predicate-argument relationships. The specification of selectional restriction falls into the purview of ontology-driven lexical semantics. In the lexis realm, ontology-driven lexical semantics provides the residue of meaning beyond the predicate-argument structure. Additional disambiguating power is to be provided on top of the verb class distinctions of the syntax-driven lexical semantics— for instance, selectional restrictions have to be formulated. Therefore, the lexicon should contain knowledge that is not required for decoding the syntactic realization of arguments. Naturally, this calls for additional expressive power in the formalism and, crucially, for reliance on an independently developed model of the world as the source of meaning interpretation for lexical units (see next item). 2.3 Metalanguage Must Be Different from Object Language No matter how extensive a computational dictionary is prepared for an application, there will always be cases when a given set of selectional restrictions will fail to help disambiguate a text. This means that even more disambiguating power should be added to the arsenal of a lexical-semantic theory. The cases where selectional restrictions are not powerful enough are typically cases of metonymy or metaphor. Note that classification of phenomena as treatable by selectional restrictions or only by metonymy/metaphor processors crucially depends on the content of the dictionary — the less selectional restriction information in the dictionary and the fewer word senses distinguished, the more work remains for the metonymy/metaphor processor. Since processing metaphor and metonymy is more effort-consuming than using selectional restrictions, it is natural that lexical semanticists have been searching for ways of reducing the need for the former. One way is to require very large and detailed lexicons. In part, this is the reason why some researchers suggested that word senses from a natural language could be used as elements of a metalanguage. We believe in the distinction of metalanguage and object language and therefore claim that the meanings of natural language units should not be represented using unadorned lexical units of the same language. In the most general case, the meaning-encoding metalanguage should be a full-blown language-independent conceptual world model. Constraints like selectional restrictions and distinctions among word senses have to be expressed using symbols — either words of a natural language or elements of an artificial &amp;quot;language of thought&amp;quot; (in the sense of Fodor, 1975). One problem with using words of a natural language is that they are typically themselves ambiguous and therefore a computer program which has to use them as disambiguation heuristics will run into additional disambiguation requirements. Another reason for not basing an ontology on word senses of a natural language is that there are cases where different word senses, as specified in a human-oriented dictionary, to be divided quite artificially. Consider, for instance, the entry for LDOCE. It has four senses all of which conform to a single predicate-argument structure. All of them have something to do with having authority for preferential treatment with respect to goods or services. In fact, if one is so bent, one can think about distinguishing four such senses, at about the same level of generality. We believe that such 13 cases are prime candidates for merging the senses into one and treating the semantic analysis process for them in the metonymy/metaphor mode. The above seems to argue that if automatic seeding of a computational lexicon is undertaken from an MRD, a nontrivial manual pass over the resulting lexicon should be carried out to make sure that the grain size of sense distinction is commensurate with the lexical and conceptual detail in the corresponding domain model. 2.4 Procedures for Assignment of Meaning Are an Integral Part of the Theory Ontology-driven lexical semantics defines the meaning of a lexical unit through a mapping between a word sense and an ontological concept (or a part thereof). The process of text analysis, then, consists essentially of instantiating the appropriate ontological concepts or modifying property values in concepts that have already been instantiated.&apos; 2.5 It Must Offer a Treatment of the Phenomena of Synonymy, Hyponymy and Antonymy In a theory where lexical meaning is expressed in terms of mappings into instances of ontological concepts, these standard lexical-semantic relations are explained in terms of relations among elements of a world model. Thus, synonymy will be defined as a relation among the lexical units at least some of whose senses map into the same ontological Lexical unit A is defined to be a hyponym of lexical unit B if the concept in at least some of the senses of A map stands in the taxonomical to the into which lexical unit mapped. Antonyms are defined on lexical units which map into regions on an attribute scale. Specifically, antonyms map into regions that are positioned around the center point of the 2.6 Text Meaning Representation Language Should Not Be Too Close to the Syntax and Lexis of Any Natural Language Since the purpose of an LCS in syntax-driven lexical semantics is to reflect the syntactically relevant semantic properties, that is those properties that determine how an LCS is mapped into a syntactic dependency representation, the LCSs are usually closely tied to the syntax and lexis of a specific language. They are therefore not as useful in multilingual computational environments as in monolingual ones. This is because languages differ in their lexicalization patterns and in patterns of syntactic dependency. Following are some examples that call for departure from the lexical semantics of individual languages in mult-lingual processing enviroments. information is, of course, only one of several components of a text meaning representation. In practice, additional information (e.g., pragmatic and discourse-related) is encoded in lexicon entries and is represented as part of text meaning. should be noted that in our application of lexical semantics, the ION such mappings allowed to be constrained, not just univocal (for instance, the English verb in one of its to the concept only if the theme of this instance is constrained to the concept one of its ontological descendents) — see Onyshkevich and Nirenburg (1991) for additional detail. 3Note that synonymy on scalars is defined as mapping onto the same region of a scale. 14 • A head–modifier relationship can be realized in opposing ways in different languages; thus, a morpheme carrying the aspectual meaning can be realized as the head of phrase (like the verb a word denoting an event as its complement; alternatively, the word denoting the event can be realized as the main verb while the aspectual meaning can be realized through an adverb or even a affix. • Lexical Gaps: Russian has no word that corresponds exactly to the English word (as in I can&apos;t afford X I can&apos;t afford to Y). In a multilingual processing environment, there might be a concept corresponding to a sense of the English Russian sentence ne mogu sae etogo pozvolit&apos; (I can&apos;t allow mythis), in a context of acquisition — which could have been assigned a straightforward lexical-semantic representation if we were building a lexical semanfor Russian alone — should involve the concept that represents means that if the units of the representation language are chosen so that they are on Russian lexis, the meaning of be missing. But this meaning seems sufficiently basic to be included in an ontology. As a result, if lexical patterns of many languages are used as the clues for building ontology, the quality of the latter should increase. • Different languages describe the same event using different lexical semantics. For example, the ritual act of washing one&apos;s body in Islam is expressed with a change of state verb in Arabic (to get washed), as a verb of receiving in Indonesian (to receive ablutions), and with an agentive verb in French (to make ablutions) (Mahmoud, 1989). For reasons such as the above, we argue that, at least in some applications, such as multilingual MT, a more language-independent representation scheme is preferable. 3 Our Position on Some of the Questions Posed in the Call for Papers On the basis of the above discussion, our opinions on some of the questions posed in the call for papers are as follows. 3.1 What is World Knowledge and What Is Knowledge of Language Lexical knowledge of a language is relevant for mapping phrases in syntactic structures onto argument positions of a predicate. World knowledge is relevant for representing the distinctions among the senses of particular predicates and arguments which cannot be distinguished by distributional characteristics. 3.2 Cross-Linguistic Evidence for the Specificity of Lexical Semantic Representation Our experience in medium-scale KBMT projects shows that verb classes defined by linking rules in different languages are not identical. This means that the syntax-driven lexical semantics component of work on a computational application will be different for every 15 language involved. For example, in Arabic there is a transitivity alternation that is similar the causative/inchoative alternation broke the glass / The glass broke) English. However, The English and Arabic transitivity alternations apply to different classes of verbs. The Arabic rule, for instance, applies systematically to verbs of psychological of state please, stun, entertain, confuse, but the English rule does not apply to these verbs. Since our goal in developing KBMT systems is to support multilinguality, we preferred to create a single, deeper, interlingual meaning representation which will not be subject to the divergences between verb classes in individual languages (Mitamura, 1989). 4 An Example: Two Semantic Structures In this section we will show the differences in analyzing a single sentence of English according to each of the two methods of semantic analysis. This sentence has been extracted from a journalistic text, in which it was actually a clause in a compound sentence. Revenues and earnings will begin to post double digit growth in 1992. zFrom the standpoint of syntax-driven lexical semantics, the desiderata of analysis include preserving argument structure and producing a semantic analysis in such a manner that syntactic realization of predicates and their arguments will be attainable through linking rules. We will concentrate solely on verbs. Jackendoff (1976, 1983, 1990), we will analyze as change of circum- In other words, and earnings the circumstance of double growth. verbs in the change-of-circumstance class, the argument that changes circumstance links with the grammatical function SUBJECT. The circumstance can be as a nonfinite complement. We will treat a conflation whose meaning is to &amp;quot;show by posting&amp;quot; and which is a member of the same verb class as the sense of have an observable The linking rules for this class link the property with the OBJECT grammatical function and link the object that has the property with the SUBJECT grammatical function. The LCS for the above example shows two predicates — BEGIN and SHOW-BY- POSTING. Notice that we have avoided the habitual thematic role names for their arguments and adjuncts. Linking to grammatical functions is shown by markers in parentheses. BEGIN thing-that-changes-circumstance: revenues and earnings (SUBJ) new-circumstance: SHOW-BY-POSTING (XCONP) thing-that-has-a-property: revenues and earnings (SUBJ) property: double digit growth (OBJ) zFrom the standpoint of ontology-driven lexical semantics, the desiderata of analysis include a) independence of the syntax of representation from the syntax of the source text, b) explicitly representing all implied meanings which are referred to in the source text – either lexically or through deixis or ellipsis or other means, c) keeping the set of atomic conflation is a combination of two LCSs, A and B, into a new LCS, C. The type of conflation in this example is that C has the meaning of &amp;quot;B by A-ing&amp;quot; and follows the linking rules and subcategorization patterns of B. More on the phenomenon of conflation see in Jackendoff (1990), B. Levin and Rapoport (1988), Talmy (1975, 1985). 16 meaning representation elements as low as possible (this latter goal does not imply a desire to postulate a small fixed set of meaning primitives). An ontology-driven lexical-semantic for the above example can look as clause head: aspect: phase: begin duration: prolonged iteration: single time relative: &amp;quot;after time of speech&amp;quot; absolute: &amp;quot;during 1992&amp;quot; increase-1 &amp;quot;revenues and rate: &amp;quot;greater than 10% and less than 100%&amp;quot; relation-1 type: temporal-before &amp;quot;time of to: 1992 Note that the verbal meanings are factored out into propositional mganings head and the aspectual and temporal meanings. Note also that the head is means that the meanings of into - — the former gave rise to the value the:aspectual ,ineuuin4,of clause. The latter was understood as a functional verb only purpose was&apos; to allow the English realization of the concept of increasing in a norninalized fashion. treated as a collocant of e.g., Mel&apos;Zuk, 1981,.Nirenbnrg et•a.., Smadja and McKeown, 1989 for various treatments of collocations). Properties used to events and objects (such as predefined in an ontologial doma.jn. Phrases in double quotes are inserted as placeholders, for the this comparison their further analysis is immaterial. 5 Conclusion Our approach to lexical semantics combines the benefits of both syntax-driven and ontologydriven semantics. We have been able to use both types of knowledge in several integrated applications. We plan to use this experience to formulate a distinct lexical-semantic theory based on this work. As a preparatory step toward formulating such a theory we have suggested the structure of ontological domain models, lexicons for particular languages and a text meaning representation language (see Carlson and Nirenburg, 1990; Meyer et al., 1990, Nirenburg and Defrise, 1991a, b).</abstract>
<note confidence="0.917746301369863">References [1] Brachman, R. 1983. What IS-A and Isn&apos;t: An Analysis of Taxonomic Links in Networks. October, 30-36. this representation we didn&apos;t try to follow any particular knowledge representation formalism. 17 [2] Bresnan, J. and J. Kanerva. 1989. Locative Inversion in Chichewa: A Case Study Factorization in Grammar. Inquiry, 1-50. Carlson, L. S. Nirenburg. 1990. World Modeling for NLP. CMU CMT Technical Report 90-121. [4] Dahlgren, K. and J. McDowell. 1989. Knowledge Representation for Commonsense with Text. Linguistics, 149-170. [5] Fodor, J. A. 1975. The Language of Thought. New York: Crowell. Grimshaw, J. 1990. Argument Structure. Cambridge, MIT Press. Hirst, G. 1987. Interpretation and Resolution of Ambiguity. Cam- Press. Hobbs, J. and R. Moore (eds.) 1985. Theories of the Commonsense Norwood, Ablex. [9] Horty, J. F., Thomason, R. H., and Touretzky, D. S. (1990) A skeptical theory of inheritance in nonmonotonic semantic nets. Artificial Intelligence 42(2-3), 311-348. Jackendoff, R. 1976. Toward an Explanatory Semantic Representation. Jackendoff, R. 1983. and Cognition. Cambridge, MIT Press. Jackendoff, R. 1990. Structures. MA: MIT Press. Lenat, D. and R. Guha. Large Knowledge-Based Systems. Reading, MA: Addison-Wesley. Lehnert, W. and M. (eds.) 1982. for Natural Language Processing. Hillsdale, NJ: Lawrence Erlbaum. Levin, B. 1989. English Verbal Diathesis. Project Working Papers Center for Cognitive Science, MIT. [16] Levin, B. and M. Rappaport Hovav. 1990. Wiping the Slate Clean: A Lexical Semantic Exploration. ms. Northwestern University and Bar Ilan University. [17] Levin, B. and T. Rapoport. 1988. Lexical Subordination. Proceedings of the 24th Annual Meeting of the Chicago Linguistic Society. Levin, L. 1986. on Lexical Forms: Unaccusative Rules in Ger- Languages. Ph.D. MIT. Levin, 1987. Toward a Linking of Relation Changing Rules in LFG. No. CSLI-87-115, November 1987, for the Study of Language and Information, Stanford, California. [20] Mahmoud, A.T. 1989. A Comparative Study of Middle and Inchoative Alternations in Arabic and English. Ph.D. Dissertation. University of Pittsburgh. [21] Mel&apos;euk, I.A. 1981. Meaning-Text Models: A Recent Trend in SOviet Linguistics. The Annual Review of Anthropology. Meyer, B. and L. Carlson. 1990. lexicographic Principles amd Design for Knowledge-Based Machine Translation. CMU CMT Technical Report 90-118. Mitamura, T. 1989. Hierarchical Organization of Predicate Frames fOr Mapping in Natural Language Processing. Dissertation. University of Pittsburgh. [24] Nirenburg, S., E. Nyberg, R. McCardell, S. Huffmann, E. Kenschaft, and I. Nirenburg. 1988. DIOGENES-88. Technical Report CMU-CMT-88-107. Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA. [25] Nirenburg, S. and C. Defrise. 1991a. Application-Oriented Computational Seman- In: R. Johnson and M. Rosner (eds.) Semantics and Computa- Linguistics. University Press (in [26] Nirenburg, S. and C. Defrise. 1991b. Aspects of Text Meaning. In: J. Pustejovsky (ed.). Semantics and the Lexicon. Dordrecht, Holland: Kluwer. [27] Onyshkevych, B. and S. Nirenburg. 1991. Lexicon, Ontology and Text Meaning. This volume. [28] Schank, R. 1973. Identification of Conceptualizations Underlying Natural Language. R. Schank and K. Colby (eds.), Models of Language Francisco: Freeman. [29] Smadja, F. and K. McKeown. 1990. Automatically Extracting and Representing Collocations for Language Generation. Proceedings of the 28th Annual Meeting of the ACL. Sowa, J. Structures: Information PRocessing in Mind and MA: Addison-Wesley. Talmy, L. 1975. Semantics and Syntax of Motion. In: J.P.Kimball, ed. and 4. Academic Press. 181-238. [32] Talmy, L. 1985. Lexicalization Patterns: Semantic Structure in Lexical Forms. In: Shopen (ed.) Typology and Syntactic Description, 3. Cambridge University Press. 57-149. [33] Wilks, Y. 1975. A Preferential, Pattern-Seeking Semantics for Natural Language Intelligence, 144-147. [34] Woods, W. 1975. What&apos;s in a Link: Foundations of Semantic Networks. In:</note>
<affiliation confidence="0.6558935">and A. Collins (eds.) and Understanding: Studin Cognitive Science. York: Academic Press.</affiliation>
<intro confidence="0.397201">19</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Brachman</author>
</authors>
<title>What IS-A and Isn&apos;t: An Analysis of Taxonomic Links in Semantic Networks.</title>
<date>1983</date>
<journal>Computer,</journal>
<volume>16</volume>
<issue>10</issue>
<pages>30--36</pages>
<marker>[1]</marker>
<rawString>Brachman, R. 1983. What IS-A and Isn&apos;t: An Analysis of Taxonomic Links in Semantic Networks. Computer, 16(10), October, 30-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bresnan</author>
<author>J Kanerva</author>
</authors>
<title>Locative Inversion in Chichewa: A Case Study of Factorization in Grammar. Linguistic Inquiry,</title>
<date>1989</date>
<pages>1--50</pages>
<marker>[2]</marker>
<rawString>Bresnan, J. and J. Kanerva. 1989. Locative Inversion in Chichewa: A Case Study of Factorization in Grammar. Linguistic Inquiry, 1-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>S Nirenburg</author>
</authors>
<title>World Modeling for NLP.</title>
<date>1990</date>
<tech>CMU CMT Technical Report 90-121.</tech>
<marker>[3]</marker>
<rawString>Carlson, L. and S. Nirenburg. 1990. World Modeling for NLP. CMU CMT Technical Report 90-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dahlgren</author>
<author>J McDowell</author>
</authors>
<title>Knowledge Representation for Commonsense Reasoning with Text.</title>
<date>1989</date>
<journal>Computational Linguistics,</journal>
<volume>15</volume>
<pages>149--170</pages>
<marker>[4]</marker>
<rawString>Dahlgren, K. and J. McDowell. 1989. Knowledge Representation for Commonsense Reasoning with Text. Computational Linguistics, 15, 149-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Fodor</author>
</authors>
<title>The Language of Thought.</title>
<date>1975</date>
<publisher>Crowell.</publisher>
<location>New York:</location>
<marker>[5]</marker>
<rawString>Fodor, J. A. 1975. The Language of Thought. New York: Crowell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Grimshaw</author>
</authors>
<title>Argument Structure.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>[6]</marker>
<rawString>Grimshaw, J. 1990. Argument Structure. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
</authors>
<title>Semantic Interpretation and Resolution of Ambiguity.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<marker>[7]</marker>
<rawString>Hirst, G. 1987. Semantic Interpretation and Resolution of Ambiguity. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<date>1985</date>
<booktitle>Formal Theories of the Commonsense World.</booktitle>
<editor>Hobbs, J. and R. Moore (eds.)</editor>
<publisher>Ablex.</publisher>
<location>Norwood, NJ:</location>
<marker>[8]</marker>
<rawString>Hobbs, J. and R. Moore (eds.) 1985. Formal Theories of the Commonsense World. Norwood, NJ: Ablex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Horty</author>
<author>R H Thomason</author>
<author>D S Touretzky</author>
</authors>
<title>A skeptical theory of inheritance in nonmonotonic semantic nets.</title>
<date>1990</date>
<journal>Artificial Intelligence</journal>
<volume>42</volume>
<issue>2</issue>
<pages>311--348</pages>
<marker>[9]</marker>
<rawString>Horty, J. F., Thomason, R. H., and Touretzky, D. S. (1990) A skeptical theory of inheritance in nonmonotonic semantic nets. Artificial Intelligence 42(2-3), 311-348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Toward an Explanatory Semantic Representation. Linguistic Inquiry,</title>
<date>1976</date>
<pages>89--150</pages>
<marker>[10]</marker>
<rawString>Jackendoff, R. 1976. Toward an Explanatory Semantic Representation. Linguistic Inquiry, 89-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Semantics and Cognition.</title>
<date>1983</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>[11]</marker>
<rawString>Jackendoff, R. 1983. Semantics and Cognition. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>[12]</marker>
<rawString>Jackendoff, R. 1990. Semantic Structures. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Lenat</author>
<author>R Guha</author>
</authors>
<title>Building Large Knowledge-Based Systems.</title>
<publisher>Addison-Wesley.</publisher>
<location>Reading, MA:</location>
<marker>[13]</marker>
<rawString>Lenat, D. and R. Guha. Building Large Knowledge-Based Systems. Reading, MA: Addison-Wesley.</rawString>
</citation>
<citation valid="false">
<booktitle>1982. Strategies for Natural Language Processing.</booktitle>
<editor>Lehnert, W. and M. Ringle (eds.)</editor>
<location>Hillsdale, NJ: Lawrence Erlbaum.</location>
<marker>[14]</marker>
<rawString>Lehnert, W. and M. Ringle (eds.) 1982. Strategies for Natural Language Processing. Hillsdale, NJ: Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verbal Diathesis. Lexicon Project Working Papers 32.</title>
<date>1989</date>
<institution>Center for Cognitive Science, MIT.</institution>
<marker>[15]</marker>
<rawString>Levin, B. 1989. English Verbal Diathesis. Lexicon Project Working Papers 32. Center for Cognitive Science, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
<author>M Rappaport Hovav</author>
</authors>
<title>Wiping the Slate Clean: A Lexical Semantic Exploration.</title>
<date>1990</date>
<institution>ms. Northwestern University and Bar Ilan University.</institution>
<marker>[16]</marker>
<rawString>Levin, B. and M. Rappaport Hovav. 1990. Wiping the Slate Clean: A Lexical Semantic Exploration. ms. Northwestern University and Bar Ilan University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
<author>T Rapoport</author>
</authors>
<title>Lexical Subordination.</title>
<date>1988</date>
<booktitle>Proceedings of the 24th Annual Meeting of the Chicago Linguistic Society.</booktitle>
<marker>[17]</marker>
<rawString>Levin, B. and T. Rapoport. 1988. Lexical Subordination. Proceedings of the 24th Annual Meeting of the Chicago Linguistic Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Levin</author>
</authors>
<title>Operations on Lexical Forms: Unaccusative Rules in Germanic Languages.</title>
<date>1986</date>
<publisher>Ph.D. Dissertation. MIT.</publisher>
<marker>[18]</marker>
<rawString>Levin, L. 1986. Operations on Lexical Forms: Unaccusative Rules in Germanic Languages. Ph.D. Dissertation. MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Levin</author>
</authors>
<title>Toward a Linking Theory of Relation Changing Rules in LFG.</title>
<date>1987</date>
<tech>CSLI Report No. CSLI-87-115,</tech>
<location>Stanford, California.</location>
<marker>[19]</marker>
<rawString>Levin, L. 1987. Toward a Linking Theory of Relation Changing Rules in LFG. CSLI Report No. CSLI-87-115, November 1987, Center for the Study of Language and Information, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A T Mahmoud</author>
</authors>
<title>A Comparative Study of Middle and Inchoative Alternations in Arabic and English.</title>
<date>1989</date>
<tech>Ph.D.</tech>
<institution>Dissertation. University of Pittsburgh.</institution>
<marker>[20]</marker>
<rawString>Mahmoud, A.T. 1989. A Comparative Study of Middle and Inchoative Alternations in Arabic and English. Ph.D. Dissertation. University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel&apos;euk</author>
</authors>
<title>Meaning-Text Models: A Recent Trend in SOviet Linguistics. The Annual Review of Anthropology.</title>
<date>1981</date>
<marker>[21]</marker>
<rawString>Mel&apos;euk, I.A. 1981. Meaning-Text Models: A Recent Trend in SOviet Linguistics. The Annual Review of Anthropology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Meyer</author>
<author>B Onyshkevych</author>
<author>L Carlson</author>
</authors>
<title>lexicographic Principles amd Design for Knowledge-Based Machine Translation.</title>
<date>1990</date>
<tech>CMU CMT Technical Report 90-118.</tech>
<marker>[22]</marker>
<rawString>Meyer, I., B. Onyshkevych and L. Carlson. 1990. lexicographic Principles amd Design for Knowledge-Based Machine Translation. CMU CMT Technical Report 90-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitamura</author>
</authors>
<title>The Hierarchical Organization of Predicate Frames fOr Interpretive Mapping</title>
<date>1989</date>
<booktitle>in Natural Language Processing. Ph.D.</booktitle>
<institution>Dissertation. University of Pittsburgh.</institution>
<marker>[23]</marker>
<rawString>Mitamura, T. 1989. The Hierarchical Organization of Predicate Frames fOr Interpretive Mapping in Natural Language Processing. Ph.D. Dissertation. University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nirenburg</author>
<author>E Nyberg</author>
<author>R McCardell</author>
<author>S Huffmann</author>
<author>E Kenschaft</author>
<author>I Nirenburg</author>
</authors>
<date>1988</date>
<tech>DIOGENES-88. Technical Report CMU-CMT-88-107.</tech>
<institution>Center for Machine Translation, Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<marker>[24]</marker>
<rawString>Nirenburg, S., E. Nyberg, R. McCardell, S. Huffmann, E. Kenschaft, and I. Nirenburg. 1988. DIOGENES-88. Technical Report CMU-CMT-88-107. Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nirenburg</author>
<author>C Defrise</author>
</authors>
<title>Application-Oriented Computational Semantics.</title>
<date>1991</date>
<booktitle>Formal Semantics and Computational Linguistics.</booktitle>
<editor>In: R. Johnson and M. Rosner (eds.)</editor>
<publisher>Cambridge University Press</publisher>
<marker>[25]</marker>
<rawString>Nirenburg, S. and C. Defrise. 1991a. Application-Oriented Computational Semantics. In: R. Johnson and M. Rosner (eds.) Formal Semantics and Computational Linguistics. Cambridge University Press (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nirenburg</author>
<author>C Defrise</author>
</authors>
<title>Aspects of Text Meaning.</title>
<date>1991</date>
<booktitle>Semantics and the Lexicon.</booktitle>
<editor>In: J. Pustejovsky (ed.).</editor>
<publisher>Kluwer.</publisher>
<location>Dordrecht, Holland:</location>
<marker>[26]</marker>
<rawString>Nirenburg, S. and C. Defrise. 1991b. Aspects of Text Meaning. In: J. Pustejovsky (ed.). Semantics and the Lexicon. Dordrecht, Holland: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Onyshkevych</author>
<author>S Nirenburg</author>
</authors>
<title>Lexicon, Ontology and Text Meaning. This volume.</title>
<date>1991</date>
<marker>[27]</marker>
<rawString>Onyshkevych, B. and S. Nirenburg. 1991. Lexicon, Ontology and Text Meaning. This volume.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schank</author>
</authors>
<title>Identification of Conceptualizations Underlying Natural Language. In:</title>
<date>1973</date>
<booktitle>Computer Models of Language an. Thought.</booktitle>
<editor>R. Schank and K. Colby (eds.),</editor>
<publisher>Freeman.</publisher>
<location>San Francisco:</location>
<marker>[28]</marker>
<rawString>Schank, R. 1973. Identification of Conceptualizations Underlying Natural Language. In: R. Schank and K. Colby (eds.), Computer Models of Language an. Thought. San Francisco: Freeman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
<author>K McKeown</author>
</authors>
<title>Automatically Extracting and Representing Collocations for Language Generation.</title>
<date>1990</date>
<booktitle>Proceedings of the 28th Annual Meeting of the ACL.</booktitle>
<marker>[29]</marker>
<rawString>Smadja, F. and K. McKeown. 1990. Automatically Extracting and Representing Collocations for Language Generation. Proceedings of the 28th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sowa</author>
</authors>
<title>Structures: Information PRocessing in Mind and Machine.</title>
<date>1984</date>
<publisher>Addison-Wesley.</publisher>
<location>Reading, MA:</location>
<marker>[30]</marker>
<rawString>Sowa, J. 1984.Conceptual Structures: Information PRocessing in Mind and Machine. Reading, MA: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Talmy</author>
</authors>
<title>Semantics and Syntax of Motion. In:</title>
<date>1975</date>
<booktitle>Syntax and Semantics, 4.</booktitle>
<pages>181--238</pages>
<editor>J.P.Kimball, ed.</editor>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<marker>[31]</marker>
<rawString>Talmy, L. 1975. Semantics and Syntax of Motion. In: J.P.Kimball, ed. Syntax and Semantics, 4. New York: Academic Press. 181-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Talmy</author>
</authors>
<title>Lexicalization Patterns: Semantic Structure in Lexical Forms. In:</title>
<date>1985</date>
<booktitle>Language Typology and Syntactic Description,</booktitle>
<volume>3</volume>
<pages>57--149</pages>
<editor>T. Shopen (ed.)</editor>
<publisher>Cambridge University Press.</publisher>
<marker>[32]</marker>
<rawString>Talmy, L. 1985. Lexicalization Patterns: Semantic Structure in Lexical Forms. In: T. Shopen (ed.) Language Typology and Syntactic Description, vol. 3. Cambridge University Press. 57-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>A Preferential, Pattern-Seeking Semantics for Natural Language Inference.</title>
<date>1975</date>
<journal>Artificial Intelligence,</journal>
<volume>6</volume>
<pages>144--147</pages>
<marker>[33]</marker>
<rawString>Wilks, Y. 1975. A Preferential, Pattern-Seeking Semantics for Natural Language Inference. Artificial Intelligence, 6, 144-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
</authors>
<title>What&apos;s in a Link: Foundations of Semantic Networks. In:</title>
<date>1975</date>
<booktitle>Representation and Understanding: Studies in Cognitive Science.</booktitle>
<editor>D.Bobrow and A. Collins (eds.)</editor>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<marker>[34]</marker>
<rawString>Woods, W. 1975. What&apos;s in a Link: Foundations of Semantic Networks. In: D.Bobrow and A. Collins (eds.) Representation and Understanding: Studies in Cognitive Science. New York: Academic Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>