<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.852187" genericHeader="abstract">
GRAMMAR SPECIALIZATION THROUGH
ENTROPY THRESHOLDS
</sectionHeader>
<author confidence="0.665812">
Christer Samuelsson
</author>
<affiliation confidence="0.993253">
Swedish Institute of Computer Science
</affiliation>
<address confidence="0.992905">
Box 1263 S-164 28 Kista, Sweden
</address>
<email confidence="0.994787">
Internet: christer@sics.se
</email>
<sectionHeader confidence="0.996977" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999657181818182">
Explanation-based generalization is used to extract a
specialized grammar from the original one using a train-
ing corpus of parse trees. This allows very much faster
parsing and gives a lower error rate, at the price of a
small loss in coverage. Previously, it has been necessary
to specify the tree-cutting criteria (or operationality cri-
teria) manually; here they are derived automatically
from the training set and the desired coverage of the
specialized grammar. This is done by assigning an en-
tropy value to each node in the parse trees and cutting
in the nodes with sufficiently high entropy values.
</bodyText>
<sectionHeader confidence="0.968548" genericHeader="background">
BACKGROUND
</sectionHeader>
<bodyText confidence="0.999945764705883">
Previous work by Manny Rayner and the author, see
[Samuelsson &amp; Rayner 1991] attempts to tailor an ex-
isting natural-language system to a specific application
domain by extracting a specialized grammar from the
original one using a large set of training examples. The
training set is a treebank consisting of implicit parse
trees that each specify a verified analysis of an input
sentence. The parse trees are implicit in the sense that
each node in the tree is the (mnemonic) name of the
grammar rule resolved on at that point, rather than the
syntactic category of the LHS of the grammar rule as is
the case in an ordinary parse tree. Figure 1 shows five
examples of implicit parse trees. The analyses are ver-
ified in the sense that each analysis has been judged to
be the preferred one for that input sentence by a human
evaluator using a semi-automatic evaluation method.
A new grammar is created by cutting up each implicit
parse tree in the treebank at appropriate points, creat-
ing a set of new rules that consist of chunks of original
grammar rules. The LHS of each new rule will be the
LHS phrase of the original grammar rule at the root of
the tree chunk and the RHS will be the RHS phrases of
the rules in the leaves of the tree chunk. For example,
cutting up the first parse tree of Figure 1 at the NP of
the rule vp_v_np yields rules 2 and 3 of Figure 3.
The idea behind this is to create a specialized gram-
mar that retains a high coverage but allows very much
faster parsing. This has turned out to be possible —
speedups compared to using the original grammar of
in median 60 times were achieved at a cost in cover-
age of about ten percent, see [Samuelsson 199441 An-
other benefit from the method is a decreased error rate
when the system is required to select a preferred ana-
lysis. In these experiments the scheme was applied to
the grammar of a version of the SRI Core Language
Engine [Alshawi ed. 1992] adapted to the Atis domain
for a speech-translation task [Rayner et al 1993] and
large corpora of real user data collected using Wizard-
of-Oz simulation. The resulting specialized gram-
mar was compiled into LR parsing tables, and a spe-
cial LR parser exploited their special properties, see
[Samuelsson 1994b].
The technical vehicle previously used to extract the
specialized grammar is explanation-based generaliza-
tion (EBG), see e.g. [Mitchell et al 1986]. Very briefly,
this consists of redoing the derivation of each train-
ing example top-down by letting the implicit parse tree
drive a rule expansion process, and aborting the expan-
sion of the specialized rule currently being extracted if
the current node of the implicit parse tree meets a set
of tree-cutting criteria2. In this case the extraction pro-
cess is invoked recursively to extract subrules rooted in
the current node. The tree-cutting criteria can be local
(&amp;quot;The LHS of the original grammar rule is an NP,&amp;quot;) or
dependent on the rest of the parse tree (&amp;quot;that doesn&apos;t
dominate the empty string only,&amp;quot;) and previous choices
of nodes to cut at (&amp;quot;and there is no cut above the cur-
rent node that is also labelled NP.&amp;quot;).
A problem not fully explored yet is how to arrive
at an optimal choice of tree-cutting criteria. In the
previous scheme, these must be specified manually, and
the choice is left to the designer&apos;s intuitions. This article
addresses the problem of automating this process and
presents a method where the nodes to cut at are selected
automatically using the information-theoretical concept
of entropy. Entropy is well-known from physics, but the
concept of perplexity is perhaps better known in the
speech-recognition and natural-language communities.
</bodyText>
<footnote confidence="0.82440375">
10ther more easily obtainable publications about this are
in preparation.
&apos;These are usually referred to as &amp;quot;operationality criteria&amp;quot;
in the EBG literature.
</footnote>
<page confidence="0.995887">
188
</page>
<bodyText confidence="0.973454">
For this reason, we will review the concept of entropy
at this point, and discuss its relation to perplexity.
</bodyText>
<subsectionHeader confidence="0.614599">
Entropy
</subsectionHeader>
<bodyText confidence="0.99656825">
Entropy is a measure of disorder. Assume for exam-
ple that a physical system can be in any of N states,
and that it will be in state si with probability pi. The
entropy S of that system is then
</bodyText>
<equation confidence="0.998097">
S = pi • In pi
If each state has equal probability, i.e. if pi = for all
i, then
S = E . In —1 = In N
</equation>
<bodyText confidence="0.999846666666667">
In this case the entropy is simply the logarithm of the
number of states the system can be in.
To take a linguistic example, assume that we are try-
ing to predict the next word in a word string from the
previous ones. Let the next word be wk and the pre-
vious word string wi ••-, wk-i• Assume further that
we have a language model that estimates the proba-
bility of each possible next word (conditional on the
previous word string). Let these probabilities be pi
</bodyText>
<equation confidence="0.538506">
for i = 1, N for the N possible next words wsk,
i.e.p = kwikI wl, wk-i). The entropy is then a
measure of how hard this prediction problem is:
S(wi , •••, wk-i) =
— p(wik wi, wk_i) p(wik wi, wk_i)
1.1
</equation>
<bodyText confidence="0.967008333333333">
If all words have equal probability, the entropy is the
logarithm of the branching factor at this point in the
input string.
</bodyText>
<subsectionHeader confidence="0.718267">
Perplexity
</subsectionHeader>
<bodyText confidence="0.981214909090909">
Perplexity is related to entropy as follows. The observed
perplexity Po of a language model with respect to an
(imaginary) infinite test sequence w1, w2, ... is defined
through the formula (see [Jelinek 1990])
In P0 = lim --ln p(wi, .•.,
n—■co n
Here p(wi, wa) denotes the probability of the word
string wi, •••, wn•
Since we cannot experimentally measure infinite lim-
its, we terminate after a finite test string w1, •••, &apos;Wm,
arriving at the measured perplexity Pm:
</bodyText>
<equation confidence="0.992125857142857">
1
ln Pm = p(wi, •••, wm)
Rewriting p(wi, wk) as p(wk
p(wi, • ••, wk-i) gives us
1
In Pm= — E p(wk wi, wk_i)
k=1
</equation>
<bodyText confidence="0.955502">
Let us call the exponential of the expectation value of
-In p(w I String) the local perplexity Pi(String), which
can be used as a measure of the information content of
the initial String.
In Pi (wi , wk_ = E(-1n p(6, I
</bodyText>
<equation confidence="0.540925">
E—p(wik w1, wk_i) • in p(wik wi, wk_i)
Here E(n) is the expectation value of ri and the sum-
</equation>
<figureCaption confidence="0.6090641">
mation is carried out over all N possible next words wik .
Comparing this with the last equation of the previous
section, we see that this is precisely the entropy S at
point k in the input string. Thus, the entropy is the
logarithm of the local perplexity at a given point in the
word string. If all words are equally probable, then the
local perplexity is simply the branching factor at this
point. If the probabilities differ, the local perplexity
can be viewed as a generalized branching factor that
takes this into account.
</figureCaption>
<subsectionHeader confidence="0.889452">
Tree entropy
</subsectionHeader>
<bodyText confidence="0.997975285714286">
We now turn to the task of calculating the entropy of a
node in a parse tree. This can be done in many different
ways; we will only describe two different ones here.
Consider the small test and training sets of Figure 1.
Assume that we wish to calculate the entropy of the
phrases of the rule PP Prep NP, which is named
pp_prep_np. In the training set, the LHS PP is at-
tached to the RHS PP of the rule np_np_pp in two
cases and to the RHS PP of the rule vp_vp_pp in one
case, giving it the entropy --pq- - ln 0.64. The
RHS preposition Prep is always a lexical lookup, and
the entropy is thus zero3, while the RHS NP in one case
attaches to the LHS of rule np_det_np, in one case to
the LHS of rule np_num, and in one case is a lexical
lookup, and the resulting entropy is thus -14 1.10.
The complete table is given here:
Rule LHS 1st RHS 2nd RHS
s_np_vp 0.00 0.56 0.56
np_np_pp 0.00 0.00 0.00
np_det_n 1.33 0.00 0.00
np_pron 0.00 0.00
np_num 0.00 0.00
vp_vp_pp 0.00 0.00 0.00
vp_v_np 0.00 0.00 0.64
vp_v 0.00 0.00
pp_prep_np 0.64 0.00 1.10
If we want to calculate the entropy of a particular
node in a parse tree, we can either simply use the phrase
&apos;Since there is only one alternative, namely a lexical
lookup. In fact, the scheme could easily be extended to en-
compass including lexical lookups of particular words into
the specialized rules by distinguishing lexical lookups of dif-
ferent words; the entropy would then determine whether or
not to cut in a node corresponding to a lookup, just as for
any other node, as is described in the following.
</bodyText>
<equation confidence="0.892533">
I WI, wk-1) &apos;
</equation>
<page confidence="0.979017">
189
</page>
<bodyText confidence="0.926862375">
entropy of the BUIS node, or take the sum of the en-
tropies of the two phrases that are unified in this node.
Training examples:
For example, the entropy when the EMS /VP of the
rule pp_prep_np is unified with the LHS of the rule
s_np_vp
A np_det_n will in the former case be 1.10 and in the
latter case be 1.10 1- 1.33 := 2.43.
</bodyText>
<figure confidence="0.896248696969697">
np_pron vp_v_np
1 A
s_np_vp lex / \ SCHEME OVERVIEW
A I / \
np_pron vp_v_np I lex np_np_pp In the following scheme, the desired coverage of the spe-
1 A 1 A cialized grammar is prescribed, and the parse trees are
lex lex np_det_n need np_det_n pp_prep_np
cut up at appropriate places without having to specify
1 1 A A A
the tree-cutting criteria manually:
I want lex lex lex lex lex lex
1 1 1 I 1 I 1. Index the treebank in an and-or tree where the or-
a ticket a flight to Boston nodes correspond to alternative choices of grammar
rules to expand with and the and-nodes correspond
s_np_vp
A to the EMS phrases of each grammar rule. Cutting
/ \ up the parse trees will involve selecting a set of or-
s_np_vp np_det_n vp_vp_pp nodes in the and-or tree. LA us can these nodes
A A A &amp;quot;cutnodes&amp;quot;.
np_pron vp_v_np lex lex vp_v pp_prep_np
1 A 1 1 1 A 2. Calculate the entropy of each or-node. We will cut at
lex / \ The flight lex lex np_num each node whose entropy exceeds a threshold value.
1 / \ 1 1 1 The rationale for this is that we wish to cut up the
We lex np_np_pp departs at lex parse trees where we can expect a lot of variation
1 A 1 i.e. where it is difficult to predict which rule will be
have / \ ten resolved on next. This corresponds exactly to the
np_det_n pp_prep_np nodes in the and-or tree that exhibit high entropy
A A values.
lex lex lex np_det_n
1 1 1 A
a departure 1 lex lex
in 1
the morning
</figure>
<bodyText confidence="0.958031615384615">
It is interesting to note that a textbook method
for constructing decision trees for classification from
attribute-value pairs is to minimize the (weighted aver-
age of the) remaining entropy5 over all possible choices
of root attribute, see [Quinlan 1986].
&apos;This can most easily be seen as follows: Imagine two
identical, but different portions of the and-or tree. If the
roots and leaves of these portions are all selected as cut-
nodes, but the distribution of cutnodes within them differ,
then we will introduce multiple ways of deriving the portions
of the parse trees that match any of these two portions of
the and-or tree.
5Defined slightly differently, as described below.
</bodyText>
<figure confidence="0.994370333333333">
Test example:
s_np_vp
np_pron vp_v_np
1
lex / \
1 / \
He lex np_np_pp
booked / \
np_det_n pp_prep_np
lex lex / \
1 1 / \
a ticket lex np_np_pp
1
1 np_det_n pp_prep_np
for A
lex lex lex lex
1 1 1 1
a flight to Dallas
</figure>
<figureCaption confidence="0.999993">
Figure 1: A tiny training set
</figureCaption>
<bodyText confidence="0.970232">
3. The nodes of the and-or tree must be partitioned
into equivalence classes dependent on the choice of
cutnodes in order to avoid redundant derivations at
parse time.4 Thus, selecting some particular node as
a cutnode may cause other nodes to also become cut-
nodes, even though their entropies are not above the
threshold.
</bodyText>
<listItem confidence="0.845953166666667">
4. Determine a threshold entropy that yields the desired
coverage. This can be done using for example interval
bisection.
5. Cut up the training examples by matching them
against the and-or tree and cutting at the determined
cutnodes.
</listItem>
<page confidence="0.995606">
190
</page>
<sectionHeader confidence="0.980653" genericHeader="method">
DETAILED SCHEME
</sectionHeader>
<bodyText confidence="0.996758">
First, the treebank is partitioned into a training set and
a test set. The training set will be indexed in an and-
or tree and used to extract the specialized rules. The
test set will be used to check the coverage of the set of
extracted rules.
</bodyText>
<subsectionHeader confidence="0.816214">
Indexing the treebank
</subsectionHeader>
<bodyText confidence="0.999806052631579">
Then, the set of implicit parse trees is stored in an and-
or tree. The parse trees have the general form of a rule
identifier Id dominating a list of subtrees or a word of
the training sentence. From the current or-node of the
and-or tree there will be arcs labelled with rule iden-
tifiers corresponding to previously stored parse trees.
From this or-node we follow an arc labelled Id, or add
a new one if there is none. We then reach (or add)
an and-node indicating the RHS phrases of the gram-
mar rule named Id. Here we follow each arc leading
out from this and-node in turn to accommodate all the
subtrees in the list. Each such arc leads to an or-node.
We have now reached a point of recursion and can index
the corresponding subtree. The recursion terminates if
Id is the special rule identifier lex and thus dominates
a word of the training sentence, rather than a list of
subtrees.
Indexing the four training examples of Figure 1 will
result in the and-or tree of Figure 2.
</bodyText>
<subsectionHeader confidence="0.820497">
Finding the cutnodes
</subsectionHeader>
<bodyText confidence="0.970296090909091">
Next, we find the set of nodes whose entropies exceed a
threshold value. First we need to calculate the entropy
of each or-node. We will here describe three different
ways of doing this, but there are many others. Before
doing this, though, we will discuss the question of re-
dundancy in the resulting set of specialized rules.
We must equate the cutnodes that correspond to the
same type of phrase. This means that if we cut at a
node corresponding to e.g. an NP, i.e. where the arcs
incident from it are labelled with grammar rules whose
left-hand-sides are NPs, we must allow all specialized
NP rules to be potentially applicable at this point, not
just the ones that are rooted in this node. This requires
that we by transitivity equate the nodes that are dom-
inated by a cutnode in a structurally equivalent way; if
there is a path from a cutnode c1 to a node ni and a
path from a cutnode c2 to a node n2 with an identical
sequence of labels, the two nodes n1 and n2 must be
equated. Now if n1 is a cutnode, then n2 must also
be a cutnode even if it has a low entropy value. The
following iterative scheme accomplishes this:
Function N*(N°)
</bodyText>
<listItem confidence="0.949392">
1. := 0;
2. Repeat i := i 1; Ni := N(Ni-1);
3. Until Ni =
4. Return Ni;
</listItem>
<equation confidence="0.97243025">
root
I s_np_ vp
/ \
/ \
/ \
1/ \2
n1(0.89) n2(0.56)
np_pron/ \np_det_n / \
/ \ / \
11 1A2
n nn /
lex I lex I hex /
vp_v_np/ \vp_vp_pp
1/ \2 1/ \2
/ \ \
n n3(1.08) (0.00)n7 n8(0.64)
lexl vp_v I I pp_prep_np
/ \ ii 1/ \ 2
np_det_n/ \np_np_pp n n n9(1.10)
/ \ lex&apos; lexI Inp_num
I 1
1A2
n n / \ hex
lex I hex 1/ 2
(1.33)n4 n5(0.64)
np_det_nI Ipp_prep_np
1/\2 /\
n n 1/ \2
lexI Ilex / \
ii n6(1.76)
lexI
lex/ \np_det_n
/ \
1A2
fin
lex I hex
</equation>
<figureCaption confidence="0.998633">
Figure 2: The resulting and-or tree
</figureCaption>
<page confidence="0.996141">
191
</page>
<bodyText confidence="0.991575310344828">
Here N(Ni ) is the set of cutnodes Ni augmented with
those induced in one step by selecting N2 as the set of
cutnodes. In practice this was accomplished by compil-
ing an and-or graph from the and-or tree and the set
of selected cutnodes, where each set of equated nodes
constituted a vertex of the graph, and traversing it.
In the simplest scheme for calculating the entropy of
an or-node, only the RHS phrase of the parent rule,
i.e. the dominating and-node, contributes to the en-
tropy, and there is in fact no need to employ an and-or
tree at all, since the tree-cutting criterion becomes local
to the parse tree being cut up.
In a slightly more elaborate scheme, we sum over the
entropies of the nodes of the parse trees that match this
node of the and-or tree. However, instead of letting each
daughter node contribute with the full entropy of the
LHS phrase of the corresponding grammar rule, these
entropies are weighted with the relative frequency of
use of each alternative choice of grammar rule.
For example, the entropy of node n3 of the and-
or tree of Figure 2 will be calculated as follows: The
mother rule vp_v_np will contribute the entropy asso-
ciated with the RHS NP, which is, referring to the table
above, 0.64. There are 2 choices of rules to resolve on,
namely np_det_n and np_np_pp with relative frequen-
cies A and 3- respectively. Again referring to the entropy
table above, we find that the LHS phrases of these rules
have entropy 1.33 and 0.00 respectively. This results in
the following entropy for node n3:
</bodyText>
<equation confidence="0.915793">
S(n3) = 0.64 -I- —1 1.33 -I- —2 0.00 = 1.08
3 3
</equation>
<bodyText confidence="0.910207666666667">
The following function determines the set of cutnodes
N that either exceed the entropy threshold, or are in-
duced by structural equivalence:
</bodyText>
<figure confidence="0.5580405">
Function N(Smin)
1. N {n : S(n) &gt; Smin};
</figure>
<sectionHeader confidence="0.350703" genericHeader="method">
2. Return N* (N);
</sectionHeader>
<bodyText confidence="0.870195">
Here S(n) is the entropy of node n.
In a third version of the scheme, the relative frequen-
cies of the daughters of the or-nodes are used directly
to calculate the node entropy:
</bodyText>
<equation confidence="0.9625275">
S(n) E - p(ni In) ln p(ni In)
n(n,n,)EA
</equation>
<bodyText confidence="0.945005333333333">
Here A is the set of arcs, and (n, ni) is an arc from n to
ni. This is basically the entropy used in [Quinlan 1986].
Unfortunately, this tends to promote daughters of cut-
nodes to in turn become cutnodes, and also results in a
problem with instability, especially in conjunction with
the additional constraints discussed in a later section,
since the entropy of each node is now dependent on the
choice of cutnodes. We must redefine the function N (S)
accordingly:
</bodyText>
<equation confidence="0.5403195">
Function N(Smin)
1. N° := 0;
2. Repeat i i 4- 1;
N := : S(niNi-1) &gt; Smin} ; Ni :=
3. Until Ni =
4. Return Ni ;
</equation>
<bodyText confidence="0.727967666666667">
Here S(niNi ) is the. entropy of node n given that the
set of cutnodes is N. Convergence can be ensured6 by
modifying the termination criterion to be
</bodyText>
<listItem confidence="0.439612">
3. Until 3j E [0, i — 1] : p(Ni , Ni) &lt; b(Ni , Ni)
</listItem>
<bodyText confidence="0.866269">
for some appropriate set metric p(Ni, N2) (e.g. the size
of the symmetric difference) and norm-like function
6(N1, N2) (e.g. ten percent of the sum of the sizes),
but this is to little avail, since we are not interested in
solutions far away from the initial assignment of cut-
nodes.
</bodyText>
<subsectionHeader confidence="0.880502">
Finding the threshold
</subsectionHeader>
<bodyText confidence="0.959321692307692">
We will use a simple interval-bisection technique for
finding the appropriate threshold value. We operate
with a range where the lower bound gives at least the
desired coverage, but where the higher bound doesn&apos;t.
We will take the midpoint of the range, find the cut-
nodes corresponding to this value of the threshold, and
check if this gives us the desired coverage. If it does,
this becomes the new lower bound, otherwise it becomes
the new upper bound. If the lower and upper bounds
are close to each other, we stop and return the nodes
corresponding to the lower bound. This termination cri-
terion can of course be replaced with something more
elaborate. This can be implemented as follows:
</bodyText>
<figure confidence="0.887594727272727">
Function N(C0)
1. Slow := 0; Shigh := largenumber; N, := N(0);
2. If Shigh — Stow &lt;6s
then goto 6
w
else Smid := Sto+Shsgh
2
3. N := N(Smid);
4. If C(N) &lt; Co
then Shigh Smid
else Slow := Smid; N c := ;
</figure>
<sectionHeader confidence="0.555536" genericHeader="method">
5. Goto 2;
6. Return N;
</sectionHeader>
<bodyText confidence="0.9804518">
Here C(N) is the coverage on the test set of the spe-
cialized grammar determined by the set of cutnodes N.
Actually, we also need to handle the boundary case
where no assignment of cutnodes gives the required cov-
erage. Likewise, the coverages of the upper and lower
bound may be far apart even though the entropy dif-
ference is small, and vice versa. These problems can
readily be taken care of by modifying the termination
criterion, but the solutions have been omitted for the
sake of clarity.
</bodyText>
<footnote confidence="0.97283">
6albeit in exponential time
</footnote>
<page confidence="0.993384">
192
</page>
<figure confidence="0.9782915">
1) &amp;quot;S =&gt; Det N V Prep NP&amp;quot;
s_np_vp
/ \
np_det_n vp_vp_pp
lex lex vp_v pp_prep_np
lex lex NP
2) &amp;quot;S =&gt; Pron V NP&amp;quot;
s_np_vp
np_pron vp_v_np
lex lex NP
3) &amp;quot;MP =&gt; Det N&amp;quot;
np_det_n
lex lex
4) &amp;quot;NP =&gt; NP Prep NP&amp;quot;
np_np_pp
NP pp_prep_np
lex NP
5) &amp;quot;NP =&gt; Num&amp;quot;
np_num
lex
</figure>
<figureCaption confidence="0.999974">
Figure 3: The specialized rules
</figureCaption>
<bodyText confidence="0.9999712">
In the running example, using the weighted sum of
the phrase entropies as the node entropy, if any thresh-
old value less than 1.08 is chosen, this will yield any
desired coverage, since the single test example of Fig-
ure 1 is then covered.
</bodyText>
<subsectionHeader confidence="0.975701">
Retrieving the specialized rules
</subsectionHeader>
<bodyText confidence="0.999773166666667">
When retrieving the specialized rules, we will match
each training example against the and-or tree. If the
current node is a cutnode, we will cut at this point in
the training example. The resulting rules will be the
set of cut-up training examples. A threshold value of
say 1.00 in our example will yield the set of cutnodes
{ n3, n4, n6, n9} and result in the set of specialized rules
of Figure 3.
If we simply let the and-or tree determine the set
of specialized rules, instead of using it to cut up the
training examples, we will in general arrive at a larger
number of rules, since some combinations of choices in
</bodyText>
<figure confidence="0.920645555555555">
6) &amp;quot;S =&gt; Det N V NP&amp;quot;
s_np_vp
np_det_n vp_v_np
lex lex lex NP
7) &amp;quot;S =&gt; Pron V Prep NP&amp;quot;
s_np_vp
np_pron vp_vp_pp
lex vp_v pp_prep_np
lex lex NP
</figure>
<figureCaption confidence="0.999926">
Figure 4: Additional specialized rules
</figureCaption>
<bodyText confidence="0.99987075">
the and-or tree may not correspond to any training ex-
ample. If this latter strategy is used in our example,
this will give us the two extra rules of Figure 4. Note
that they not correspond to any training example.
</bodyText>
<sectionHeader confidence="0.991529" genericHeader="method">
ADDITIONAL CONSTRAINTS
</sectionHeader>
<bodyText confidence="0.99998978125">
As mentioned at the beginning, the specialized gram-
mar is compiled into LR parsing tables. Just finding
any set of cutnodes that yields the desired coverage
will not necessarily result in a grammar that is well
suited for LR parsing. In particular, LR parsers, like
any other parsers employing a bottom-up parsing strat-
egy, do not blend well with empty productions. This is
because without top-down filtering, any empty produc-
tion is applicable at any point in the input string, and a
naive bottom-up parser will loop indefinitely. The LR
parsing tables constitute a type of top-down filtering,
but this may not be sufficient to guarantee termination,
and in any case, a lot of spurious applications of empty
productions will most likely take place, degrading per-
formance. For these reasons we will not allow learned
rules whose RHSs are empty, but simply refrain from
cutting in nodes of the parse trees that do not dominate
at least one lexical lookup.
Even so, the scheme described this far is not totally
successful, the performance is not as good as using
hand-coded tree-cutting criteria. This is conjectured
to be an effect of the reduction lengths being far too
short. The first reason for this is that for any spurious
rule reduction to take place, the corresponding RHS
phrases must be on the stack. The likelihood for this to
happen by chance decreases drastically with increased
rule length. A second reason for this is that the number
of states visited will decrease with increasing reduction
length. This can most easily be seen by noting that the
number of states visited by a deterministic LR parser
equals the number of shift actions plus the number of
reductions, and equals the number of nodes in the cor-
</bodyText>
<page confidence="0.997407">
193
</page>
<bodyText confidence="0.9999823">
responding parse tree, and the longer the reductions,
the more shallow the parse tree.
The hand-coded operationality criteria result in an
average rule length of four, and a distribution of reduc-
tion lengths that is such that only 17 percent are of
length one and 11 percent are of length two. This is in
sharp contrast to what the above scheme accomplishes;
the corresponding figures are about 20 or 30 percent
each for lengths one and two.
An attempted solution to this problem is to impose
restrictions on neighbouring cutnodes. This can be
done in several ways; one that has been tested is to
select for each rule the MIS phrase with the least en-
tropy, and prescribe that if a node corresponding to the
LHS of the rule is chosen as a cutnode, then no node
corresponding to this RHS phrase may be chosen as a
cutnode, and vice versa. In case of such a conflict, the
node (class) with the lowest entropy is removed from
the set of cutnodes.
We modify the function N* to handle this:
</bodyText>
<sectionHeader confidence="0.905364" genericHeader="method">
2. Repeat i := i 1; Ni := N(Ni&amp;quot;) \ B(Ni&apos;);
</sectionHeader>
<bodyText confidence="0.9999392">
Here B(N3) is the set of nodes in Ni that should be re-
moved to avoid violating the constraints on neighbour-
ing cutnodes. It is also necessary to modify the termi-
nation criterion as was done for the function N(Smin)
above. Now we can no longer safely assume that the
coverage increases with decreased entropy, and we must
also modify the interval-bisection scheme to handle this.
It has proved reasonable to assume that the coverage
is monotone on both sides of some maximum, which
simplifies this task considerably.
</bodyText>
<sectionHeader confidence="0.981642" genericHeader="evaluation">
EXPERIMENTAL RESULTS
</sectionHeader>
<bodyText confidence="0.999479375">
A module realizing this scheme has been implemented
and applied to the very setup used for the previous ex-
periments with the hand-coded tree-cutting criteria, see
[Samuelsson 1994a]. 2100 of the verified parse trees con-
stituted the training set, while 230 of them were used
for the test set. The table below summarizes the re-
sults for some grammars of different coverage extracted
using:
</bodyText>
<listItem confidence="0.5901228">
1. Hand-coded tree-cutting criteria.
2. Induced tree-cutting criteria where the node entropy
was taken to be the phrase entropy of the RHS phrase
of the dominating grammar rule.
3. Induced tree-cutting criteria where the node entropy
</listItem>
<bodyText confidence="0.985793857142857">
was the sum of the phrase entropy of the RHS phrase
of the dominating grammar rule and the weighted
sum of the phrase entropies of the LHSs of the alter-
native choices of grammar rules to resolve on.
In the latter two cases experiments were carried out
both with and without the restrictions on neighbouring
cutnodes discussed in the previous section.
</bodyText>
<table confidence="0.998716730769231">
Hand-coded tree-cutting criteria
Coverage Reduction lengths (%) Times (ms)
1 2 3 &gt;4 Ave. Med.
90.2 % 17.3 11.3 21.6 49.8 72.6 48.0
RHS phrase entropy. Neighbour restrictions
Coverage Reduction lengths (%) Times (ms)
1 2 3 &gt;4 Ave. Med.
75.8 % 11.8 26.1 17.7 44.4 128 38.5
80.5 % 11.5 27.4 20.0 41.1 133 47.2
85.3 % 14.0 37.3 24.3 24.4 241 70.5
RHS phrase entropy. No neighbour restrictions
Coverage Reduction lengths (%) Times (ms)
1 2 3 &gt;4 Ave. Med.
75.8 % 8.3 12.4 25.6 53.7 76.7 37.0
79.7 % 9.0 16.2 26.9 47.9 99.1 49.4
85.3 % 8.4 17.3 31.1 43.2 186 74.0
90.9 % 18.2 27.5 21.7 32.6 469 126
Mixed phrase entropies. Neighbour restrictions
Coverage Reduction lengths (%) Times (ms)
1 2 3 &gt;4 Ave. Med.
75.3 % 6.1 11.7 30.8 51.4 115.4 37.5
Mixed phrase entropies. No neighbour restrictions
Coverage Reduction lengths (%) Times (ms)
1 2 3 &gt;4 Ave. Med.
75 % 16.1 13.8 19.8 50.3 700 92.0
80 % 18.3 16.3 20.1 45.3 842 108
</table>
<bodyText confidence="0.999865857142857">
With the mixed entropy scheme it seems important
to include the restrictions on neighbouring cutnodes,
while this does not seem to be the case with the RHS
phrase entropy scheme. A potential explanation for the
significantly higher average parsing times for all gram-
mars extracted using the induced tree-cutting criteria
is that these are in general recursive, while the hand-
coded criteria do not allow recursion, and thus only
produce grammars that generate finite languages.
Although the hand-coded tree-cutting criteria are
substantially better than the induced ones, we must
remember that the former produce a grammar that in
median allows 60 times faster processing than the orig-
inal grammar and parser do. This means that even if
the induced criteria produce grammars that are a fac-
tor two or three slower than this, they are still approx-
imately one and a half order of magnitude faster than
the original setup. Also, this is by no means a closed
research issue, but merely a first attempt to realize the
scheme, and there is no doubt in my mind that it can
be improved on most substantially.
</bodyText>
<sectionHeader confidence="0.97377" genericHeader="conclusions">
SUMMARY
</sectionHeader>
<bodyText confidence="0.99966975">
This article proposes a method for automatically find-
ing the appropriate tree-cutting criteria in the EBG
scheme, rather than having to hand-code them. The
EBG scheme has previously proved most successful for
</bodyText>
<page confidence="0.995699">
194
</page>
<bodyText confidence="0.999976052631579">
tuning a natural-language grammar to a specific ap-
plication domain and thereby achieve very much faster
parsing, at the cost of a small reduction in coverage.
Instruments have been developed and tested for con-
trolling the coverage and for avoiding a large number
of short reductions, which is argued to be the main
source to poor parser performance. Although these
instruments are currently slightly too blunt to enable
producing grammars with the same high performance
as the hand-coded tree-cutting criteria, they can most
probably be sharpened by future research, and in par-
ticular refined to achieve the delicate balance between
high coverage and a distribution of reduction lengths
that is sufficiently biased towards long reductions. Also,
banning recursion by category specialization, i.e. by for
example distinguishing NPs that dominate other NPs
from those that do not, will be investigated, since this is
believed to be an important ingredient in the version of
the scheme employing hand-coded tree-cutting criteria.
</bodyText>
<sectionHeader confidence="0.999186" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.999947090909091">
This research was made possible by the basic research
programme at the Swedish Institute of Computer Sci-
ence (SICS). I wish to thank Manny Rayner of SRI
International, Cambridge, for help and support in mat-
ters pertaining to the treebank, and for enlightening
discussions of the scheme as a whole. I also wish to
thank the NLP group at SICS for contributing to a
very conductive atmosphere to work in, and in particu-
lar Ivan Bretan for valuable comments on draft versions
of this article. Finally, I wish to thank the anonymous
reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999725055555556">
[Alshawi ed. 1992] Hiyan Alshawi, editor. The Core
Language Engine, MIT Press 1992.
[Jelinek 1990] Fred Jelinek. &amp;quot;Self-Organizing Language
Models for Speech Recognition&amp;quot;, in Readings in
Speech Recognition, pp. 450-506, Morgan Kauf-
mann 1990.
[Mitchell et al 1986]
Tom M. Mitchell, Richard M. Keller and Smadar
T. Kedar-Cabelli. &amp;quot;Explanation-Based Generaliza-
tion: A Unifying View&amp;quot;, in Machine Learning 1,
No. 1, pp. 47-80, 1986.
[Quinlan 1986] J. Ross Quinlan. &amp;quot;Induction of Decision
Trees&amp;quot;, in Machine Learning 1, No. 1, pp. 81-107,
1986.
[Rayner et al 1993] M. Rayner, H. Alshawi, I. Bretan,
D. Carter, V. Digalakis, B. Gamback, J. Kaja,
J. Karlgren, B. Lyberg, P. Price, S. Pulman and
C. Samuelsson. &amp;quot;A Speech to Speech Transla-
tion System Built From Standard Components&amp;quot;,
in Procs. ARPA Workshop on Human Language
Technology, Princeton, NJ 1993.
[Samuelsson 1994a] Christer Samuelsson. Fast Natural-
Language Parsing Using Explanation-Based Learn-
ing, PhD thesis, Royal Institute of Technology,
Stockholm, Sweden 1994.
[Samuelsson 1994b] Christer Samuelsson. &amp;quot;Notes on
LR Parser Design&amp;quot; to appear in Procs. 15th In-
ternational Conference on Computational Linguis-
tics, Kyoto, Japan 1994.
[Samuelsson &amp; Rayner 1991] Christer Samuelsson and
Manny Rayner. &amp;quot;Quantitative Evaluation of Ex-
planation-Based Learning as an Optimization Tool
for a Large-Scale Natural Language System&amp;quot;, in
Procs. 12th International Joint Conference on Ar-
tificial Intelligence, pp. 609-615, Sydney, Australia
1991.
</reference>
<page confidence="0.998939">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000199">
<title confidence="0.9845715">GRAMMAR SPECIALIZATION THROUGH ENTROPY THRESHOLDS</title>
<author confidence="0.950559">Christer Samuelsson</author>
<affiliation confidence="0.999142">Swedish Institute of Computer Science</affiliation>
<address confidence="0.996285">Box 1263 S-164 28 Kista, Sweden</address>
<abstract confidence="0.994698013201321">Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees. This allows very much faster parsing and gives a lower error rate, at the price of a small loss in coverage. Previously, it has been necessary to specify the tree-cutting criteria (or operationality criteria) manually; here they are derived automatically from the training set and the desired coverage of the specialized grammar. This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values. BACKGROUND Previous work by Manny Rayner and the author, see 1991] attempts to tailor an existing natural-language system to a specific application domain by extracting a specialized grammar from the original one using a large set of training examples. The training set is a treebank consisting of implicit parse trees that each specify a verified analysis of an input sentence. The parse trees are implicit in the sense that each node in the tree is the (mnemonic) name of the grammar rule resolved on at that point, rather than the syntactic category of the LHS of the grammar rule as is the case in an ordinary parse tree. Figure 1 shows five examples of implicit parse trees. The analyses are verified in the sense that each analysis has been judged to be the preferred one for that input sentence by a human evaluator using a semi-automatic evaluation method. A new grammar is created by cutting up each implicit parse tree in the treebank at appropriate points, creating a set of new rules that consist of chunks of original grammar rules. The LHS of each new rule will be the LHS phrase of the original grammar rule at the root of the tree chunk and the RHS will be the RHS phrases of the rules in the leaves of the tree chunk. For example, up the first parse tree of Figure 1 at the rule vp_v_np rules 2 and 3 of Figure 3. The idea behind this is to create a specialized gramthat retains a high coverage but allows very faster parsing. This has turned out to be possible — speedups compared to using the original grammar of in median 60 times were achieved at a cost in coverof about ten percent, see [Samuelsson Another benefit from the method is a decreased error rate when the system is required to select a preferred analysis. In these experiments the scheme was applied to the grammar of a version of the SRI Core Language [Alshawi adapted to the Atis domain a speech-translation task [Rayner al and large corpora of real user data collected using Wizardof-Oz simulation. The resulting specialized grammar was compiled into LR parsing tables, and a spe- LR parser exploited their special properties, [Samuelsson 1994b]. The technical vehicle previously used to extract the specialized grammar is explanation-based generaliza- (EBG), see e.g. [Mitchell al Very briefly, this consists of redoing the derivation of each training example top-down by letting the implicit parse tree drive a rule expansion process, and aborting the expansion of the specialized rule currently being extracted if current node of the implicit parse tree meets set tree-cutting In this case the extraction prois invoked recursively to extract subrules the current node. The tree-cutting criteria can be local LHS of the original grammar rule is an dependent on the rest of the parse tree (&amp;quot;that doesn&apos;t dominate the empty string only,&amp;quot;) and previous choices of nodes to cut at (&amp;quot;and there is no cut above the curnode that is also labelled A problem not fully explored yet is how to arrive at an optimal choice of tree-cutting criteria. In the previous scheme, these must be specified manually, and the choice is left to the designer&apos;s intuitions. This article addresses the problem of automating this process and presents a method where the nodes to cut at are selected using the concept of entropy. Entropy is well-known from physics, but the concept of perplexity is perhaps better known in the speech-recognition and natural-language communities. more easily obtainable publications about this are in preparation. &apos;These are usually referred to as &amp;quot;operationality criteria&amp;quot; in the EBG literature. 188 For this reason, we will review the concept of entropy at this point, and discuss its relation to perplexity. Entropy Entropy is a measure of disorder. Assume for examthat a physical system can be in any of and that it will be in state si with probability pi. The that system is then S = • In pi each state has equal probability, i.e. if = all S In In this case the entropy is simply the logarithm of the number of states the system can be in. To take a linguistic example, assume that we are trying to predict the next word in a word string from the previous ones. Let the next word be wk and the previous word string wi ••-, wk-i• Assume further that we have a language model that estimates the probability of each possible next word (conditional on the previous word string). Let these probabilities be pi = 1, N the next words wl, entropy is then a measure of how hard this prediction problem is: S(wi , •••, wk-i) = p(wik wi, wk_i) wk_i) If all words have equal probability, the entropy is the logarithm of the branching factor at this point in the input string. Perplexity Perplexity is related to entropy as follows. The observed of a language model with respect to an infinite test sequence ... is defined through the formula (see [Jelinek 1990]) = lim --ln .•., denotes the probability of the word string wi, •••, wn• Since we cannot experimentally measure infinite limwe terminate after a finite test string w1, •••, at the measured perplexity 1 ln = •••, wm) as • ••, wk-i) us 1 — wi, k=1 Let us call the exponential of the expectation value of p(w I local perplexity can be used as a measure of the information content of initial , = E(-1n I in wk_i) E(n) is the expectation value of the sumis carried out over all next words . Comparing this with the last equation of the previous we see that this is precisely the entropy the input string. Thus, the entropy is the logarithm of the local perplexity at a given point in the word string. If all words are equally probable, then the local perplexity is simply the branching factor at this point. If the probabilities differ, the local perplexity can be viewed as a generalized branching factor that takes this into account. Tree entropy We now turn to the task of calculating the entropy of a node in a parse tree. This can be done in many different ways; we will only describe two different ones here. Consider the small test and training sets of Figure 1. Assume that we wish to calculate the entropy of the of the rule Prep NP, is named the training set, the LHS atto the RHS the rule two and to the RHS the rule one case, giving it the entropy --pq- ln 0.64. The preposition always a lexical lookup, and entropy is thus while the RHS one case to the LHS of rule one case to the LHS of rule np_num, and in one case is a lexical lookup, and the resulting entropy is thus -14 1.10. The complete table is given here: Rule LHS 1st RHS 2nd RHS s_np_vp 0.00 0.56 0.56 np_np_pp 0.00 0.00 0.00 np_det_n 1.33 0.00 0.00 np_pron 0.00 0.00 np_num 0.00 0.00 vp_vp_pp 0.00 0.00 0.00 vp_v_np 0.00 0.00 0.64 vp_v 0.00 0.00 pp_prep_np 0.64 0.00 1.10 If we want to calculate the entropy of a particular node in a parse tree, we can either simply use the phrase &apos;Since there is only one alternative, namely a lexical lookup. In fact, the scheme could easily be extended to encompass including lexical lookups of particular words into the specialized rules by distinguishing lexical lookups of different words; the entropy would then determine whether or not to cut in a node corresponding to a lookup, just as for any other node, as is described in the following. &apos; 189 entropy of the BUIS node, or take the sum of the entropies of the two phrases that are unified in this node. Training examples: For example, the entropy when the EMS /VP of the unified with the LHS of the rule s_np_vp A in the former case be 1.10 and in the latter case be 1.10 1- 1.33 := 2.43. np_pron vp_v_np 1 A s_np_vp lex / \ OVERVIEW A I / np_pron vp_v_np np_np_pp the following scheme, the desired coverage of the spe- A 1 A cialized grammar is prescribed, and the parse trees lex lex np_det_n need np_det_n pp_prep_np at appropriate places without having to specify 1 1 A A A the tree-cutting criteria manually: lex lex lex lex 1 1 1 I 1 I 1. the treebank in an and-or tree where the orticket a flight to Boston correspond to alternative choices of grammar rules to expand with and the and-nodes correspond s_np_vp A to the EMS phrases of each grammar rule. Cutting the parse trees will involve selecting a set of ors_np_vp nodes in the and-or tree. LA us can these nodes A A A &amp;quot;cutnodes&amp;quot;. np_pron vp_v_np lex lex vp_v pp_prep_np A 1 1 1 A 2. Calculate the entropy of each or-node. We will cut / \ The flight lex lex np_num node whose entropy exceeds a threshold value. 1 / \ 1 1 1 The rationale for this is that we wish to cut up the departs at lex trees where we can expect a lot of variation 1 A 1 i.e. where it is difficult to predict which rule will be / \ ten on next. This corresponds exactly to the nodes in the and-or tree that exhibit high entropy A A values. lex lex 1 1 1 A departure lex the morning It is interesting to note that a textbook method trees for classification from attribute-value pairs is to minimize the (weighted averof the) remaining over all possible choices of root attribute, see [Quinlan 1986]. can most easily seen as follows: Imagine two identical, but different portions of the and-or tree. If the roots and leaves of these portions are all selected as cutnodes, but the distribution of cutnodes within them differ, then we will introduce multiple ways of deriving the portions of the parse trees that match any of these two portions of the and-or tree. slightly differently, as described below. Test example: s_np_vp np_pron vp_v_np 1 lex / \ 1 / \ He lex np_np_pp booked / \ np_det_n pp_prep_np lex lex / \ 1 1 / \ a ticket lex np_np_pp 1 pp_prep_np lex lex lex lex 1 1 1 1 a flight to Dallas Figure 1: A tiny training set 3. The nodes of the and-or tree must be partitioned into equivalence classes dependent on the choice of cutnodes in order to avoid redundant derivations at Thus, selecting some particular node as a cutnode may cause other nodes to also become cutnodes, even though their entropies are not above the threshold. 4. Determine a threshold entropy that yields the desired coverage. This can be done using for example interval bisection. 5. Cut up the training examples by matching them against the and-or tree and cutting at the determined cutnodes. 190 DETAILED SCHEME First, the treebank is partitioned into a training set and a test set. The training set will be indexed in an andor tree and used to extract the specialized rules. The test set will be used to check the coverage of the set of extracted rules. Indexing the treebank Then, the set of implicit parse trees is stored in an andor tree. The parse trees have the general form of a rule a list of subtrees or a word of the training sentence. From the current or-node of the and-or tree there will be arcs labelled with rule identifiers corresponding to previously stored parse trees. this or-node we follow an arc labelled add a new one if there is none. We then reach (or add) an and-node indicating the RHS phrases of the gramrule named we follow each arc leading out from this and-node in turn to accommodate all the subtrees in the list. Each such arc leads to an or-node. We have now reached a point of recursion and can index the corresponding subtree. The recursion terminates if the special rule identifier thus dominates a word of the training sentence, rather than a list of subtrees. Indexing the four training examples of Figure 1 will result in the and-or tree of Figure 2. Finding the cutnodes Next, we find the set of nodes whose entropies exceed a threshold value. First we need to calculate the entropy of each or-node. We will here describe three different ways of doing this, but there are many others. Before doing this, though, we will discuss the question of redundancy in the resulting set of specialized rules. We must equate the cutnodes that correspond to the same type of phrase. This means that if we cut at a corresponding to e.g. an where the arcs incident from it are labelled with grammar rules whose are we allow to be potentially applicable at this point, not just the ones that are rooted in this node. This requires that we by transitivity equate the nodes that are dominated by a cutnode in a structurally equivalent way; if is a path from a cutnode to a node and a from a cutnode to a node n2 with an identical of labels, the two nodes n1 and must be Now if is a cutnode, then must also be a cutnode even if it has a low entropy value. The following iterative scheme accomplishes this: := Repeat := Until = Return root I s_np_ vp / \ / \ / \ 1/ \2 n1(0.89) n2(0.56) np_pron/ \np_det_n / \ / \ / \ 11 1A2 n nn / lex I lex I hex / vp_v_np/ \vp_vp_pp 1/ \2 1/ \2 / \ \ n n3(1.08) (0.00)n7 n8(0.64) vp_v I I / \ ii 1/ \ 2 np_det_n/ \np_np_pp n n n9(1.10) / \ lex&apos; lexI Inp_num I 1 1A2 n n / \ hex I hex 1/ (1.33)n4 n5(0.64) np_det_nI Ipp_prep_np 1/\2 /\ n n 1/ \2 lexI Ilex / \ n6(1.76) lexI lex/ \np_det_n / \ 1A2 fin I Figure 2: The resulting and-or tree 191 is the set of cutnodes with induced in one step by selecting as the set of cutnodes. In practice this was accomplished by compiling an and-or graph from the and-or tree and the set of selected cutnodes, where each set of equated nodes constituted a vertex of the graph, and traversing it. In the simplest scheme for calculating the entropy of an or-node, only the RHS phrase of the parent rule, i.e. the dominating and-node, contributes to the entropy, and there is in fact no need to employ an and-or tree at all, since the tree-cutting criterion becomes local to the parse tree being cut up. In a slightly more elaborate scheme, we sum over the entropies of the nodes of the parse trees that match this node of the and-or tree. However, instead of letting each daughter node contribute with the full entropy of the LHS phrase of the corresponding grammar rule, these entropies are weighted with the relative frequency of use of each alternative choice of grammar rule. example, the entropy of node the andor tree of Figure 2 will be calculated as follows: The rule contribute the entropy assowith the RHS is, referring to the table above, 0.64. There are 2 choices of rules to resolve on, relative frequencies A and 3respectively. Again referring to the entropy table above, we find that the LHS phrases of these rules have entropy 1.33 and 0.00 respectively. This results in the following entropy for node n3: = 0.64 -I- -I- = 1.08 3 3 The following function determines the set of cutnodes either exceed the entropy threshold, or are induced by structural equivalence: {n : S(n) &gt; Smin}; Return (N); the entropy of node n. In a third version of the scheme, the relative frequencies of the daughters of the or-nodes are used directly to calculate the node entropy: - In) ln p(ni In) n(n,n,)EA Here A is the set of arcs, and (n, ni) is an arc from n to This is basically the entropy used in [Quinlan 1986]. Unfortunately, this tends to promote daughters of cutnodes to in turn become cutnodes, and also results in a problem with instability, especially in conjunction with the additional constraints discussed in a later section, since the entropy of each node is now dependent on the of cutnodes. We must redefine the function (S) accordingly: 1. := 0; Repeat i 4- 1; := : &gt; ; := Until = Return ; ) entropy of node n given that the of cutnodes is N. Convergence can be by modifying the termination criterion to be Until E [0, i : , Ni) &lt; , Ni) some appropriate set metric (e.g. the size of the symmetric difference) and norm-like function N2) ten percent of the sum of the sizes), but this is to little avail, since we are not interested in solutions far away from the initial assignment of cutnodes. Finding the threshold We will use a simple interval-bisection technique for finding the appropriate threshold value. We operate with a range where the lower bound gives at least the desired coverage, but where the higher bound doesn&apos;t. We will take the midpoint of the range, find the cutnodes corresponding to this value of the threshold, and check if this gives us the desired coverage. If it does, this becomes the new lower bound, otherwise it becomes the new upper bound. If the lower and upper bounds are close to each other, we stop and return the nodes corresponding to the lower bound. This termination criterion can of course be replaced with something more elaborate. This can be implemented as follows: 1. := := := N(0); If — Stow &lt;6s goto w := 2 N := If &lt; Smid := Smid; N c := ; Goto Return the coverage on the test set of the spegrammar determined by the set of cutnodes Actually, we also need to handle the boundary case where no assignment of cutnodes gives the required coverage. Likewise, the coverages of the upper and lower bound may be far apart even though the entropy difference is small, and vice versa. These problems can readily be taken care of by modifying the termination criterion, but the solutions have been omitted for the sake of clarity. in exponential time 192 1) &amp;quot;S =&gt; Det N V Prep NP&amp;quot; s_np_vp / \ np_det_n vp_vp_pp lex lex vp_v pp_prep_np lex lex NP 2) &amp;quot;S =&gt; Pron V NP&amp;quot; s_np_vp np_pron vp_v_np lex lex NP 3) &amp;quot;MP =&gt; Det N&amp;quot; np_det_n lex lex 4) &amp;quot;NP =&gt; NP Prep NP&amp;quot; np_np_pp NP pp_prep_np lex NP 5) &amp;quot;NP =&gt; Num&amp;quot; np_num lex Figure 3: The specialized rules In the running example, using the weighted sum of the phrase entropies as the node entropy, if any threshold value less than 1.08 is chosen, this will yield any desired coverage, since the single test example of Figure 1 is then covered. Retrieving the specialized rules When retrieving the specialized rules, we will match each training example against the and-or tree. If the current node is a cutnode, we will cut at this point in the training example. The resulting rules will be the set of cut-up training examples. A threshold value of say 1.00 in our example will yield the set of cutnodes and result in the set of specialized rules of Figure 3. If we simply let the and-or tree determine the set of specialized rules, instead of using it to cut up the training examples, we will in general arrive at a larger number of rules, since some combinations of choices in 6) &amp;quot;S =&gt; Det N V NP&amp;quot; s_np_vp np_det_n vp_v_np lex lex lex NP 7) &amp;quot;S =&gt; Pron V Prep NP&amp;quot; s_np_vp np_pron vp_vp_pp pp_prep_np lex Figure 4: Additional specialized rules and-or tree may not correspond to any training exthis latter strategy is used in our example, will give us the two extra rules of Figure 4. that they not correspond to any training example. ADDITIONAL CONSTRAINTS As mentioned at the beginning, the specialized grammar is compiled into LR parsing tables. Just finding any set of cutnodes that yields the desired coverage will not necessarily result in a grammar that is well suited for LR parsing. In particular, LR parsers, like any other parsers employing a bottom-up parsing strategy, do not blend well with empty productions. This is because without top-down filtering, any empty production is applicable at any point in the input string, and a naive bottom-up parser will loop indefinitely. The LR parsing tables constitute a type of top-down filtering, but this may not be sufficient to guarantee termination, and in any case, a lot of spurious applications of empty productions will most likely take place, degrading performance. For these reasons we will not allow learned rules whose RHSs are empty, but simply refrain from cutting in nodes of the parse trees that do not dominate at least one lexical lookup. Even so, the scheme described this far is not totally successful, the performance is not as good as using hand-coded tree-cutting criteria. This is conjectured to be an effect of the reduction lengths being far too short. The first reason for this is that for any spurious rule reduction to take place, the corresponding RHS phrases must be on the stack. The likelihood for this to happen by chance decreases drastically with increased rule length. A second reason for this is that the number of states visited will decrease with increasing reduction length. This can most easily be seen by noting that the number of states visited by a deterministic LR parser equals the number of shift actions plus the number of and equals the number of nodes in the cor- 193 responding parse tree, and the longer the reductions, the more shallow the parse tree. The hand-coded operationality criteria result in an average rule length of four, and a distribution of reduction lengths that is such that only 17 percent are of length one and 11 percent are of length two. This is in sharp contrast to what the above scheme accomplishes; the corresponding figures are about 20 or 30 percent each for lengths one and two. An attempted solution to this problem is to impose restrictions on neighbouring cutnodes. This can be done in several ways; one that has been tested is to select for each rule the MIS phrase with the least entropy, and prescribe that if a node corresponding to the LHS of the rule is chosen as a cutnode, then no node corresponding to this RHS phrase may be chosen as a cutnode, and vice versa. In case of such a conflict, the node (class) with the lowest entropy is removed from the set of cutnodes. modify the function handle this: Repeat := is the set of nodes in Ni that should be removed to avoid violating the constraints on neighbouring cutnodes. It is also necessary to modify the termicriterion as was done for the function above. Now we can no longer safely assume that the coverage increases with decreased entropy, and we must also modify the interval-bisection scheme to handle this. It has proved reasonable to assume that the coverage is monotone on both sides of some maximum, which simplifies this task considerably. EXPERIMENTAL RESULTS A module realizing this scheme has been implemented and applied to the very setup used for the previous experiments with the hand-coded tree-cutting criteria, see [Samuelsson 1994a]. 2100 of the verified parse trees constituted the training set, while 230 of them were used for the test set. The table below summarizes the results for some grammars of different coverage extracted using: 1. Hand-coded tree-cutting criteria. 2. Induced tree-cutting criteria where the node entropy was taken to be the phrase entropy of the RHS phrase of the dominating grammar rule. 3. Induced tree-cutting criteria where the node entropy was the sum of the phrase entropy of the RHS phrase of the dominating grammar rule and the weighted sum of the phrase entropies of the LHSs of the alternative choices of grammar rules to resolve on. In the latter two cases experiments were carried out both with and without the restrictions on neighbouring cutnodes discussed in the previous section. Hand-coded tree-cutting criteria Coverage Reduction lengths (%) Times (ms)</abstract>
<note confidence="0.793835739130435">1 2 3 &gt;4 Ave. Med. 90.2 % 17.3 11.3 21.6 49.8 72.6 48.0 RHS phrase entropy. Neighbour restrictions Coverage Reduction lengths (%) Times (ms) 1 2 3 &gt;4 Ave. Med. 75.8 % 11.8 26.1 17.7 44.4 128 38.5 80.5 % 11.5 27.4 20.0 41.1 133 47.2 85.3 % 14.0 37.3 24.3 24.4 241 70.5 RHS phrase entropy. No neighbour restrictions Coverage Reduction lengths (%) Times (ms) 1 2 3 &gt;4 Ave. Med. 75.8 % 8.3 12.4 25.6 53.7 76.7 37.0 79.7 % 9.0 16.2 26.9 47.9 99.1 49.4 85.3 % 8.4 17.3 31.1 43.2 186 74.0 90.9 % 18.2 27.5 21.7 32.6 469 126 Mixed phrase entropies. Neighbour restrictions Coverage Reduction lengths (%) Times (ms) 1 2 3 &gt;4 Ave. Med. 75.3 % 6.1 11.7 30.8 51.4 115.4 37.5 Mixed phrase entropies. No neighbour restrictions Coverage Reduction lengths (%) Times (ms) 1 2 3 &gt;4 Ave. Med. 75 % 16.1 13.8 19.8 50.3 700 92.0</note>
<phone confidence="0.599622">80 % 18.3 16.3 20.1 45.3 842 108</phone>
<abstract confidence="0.998852637931034">With the mixed entropy scheme it seems important to include the restrictions on neighbouring cutnodes, while this does not seem to be the case with the RHS phrase entropy scheme. A potential explanation for the significantly higher average parsing times for all grammars extracted using the induced tree-cutting criteria is that these are in general recursive, while the handcoded criteria do not allow recursion, and thus only produce grammars that generate finite languages. Although the hand-coded tree-cutting criteria are substantially better than the induced ones, we must remember that the former produce a grammar that in median allows 60 times faster processing than the original grammar and parser do. This means that even if the induced criteria produce grammars that are a factor two or three slower than this, they are still approximately one and a half order of magnitude faster than the original setup. Also, this is by no means a closed research issue, but merely a first attempt to realize the scheme, and there is no doubt in my mind that it can be improved on most substantially. SUMMARY This article proposes a method for automatically finding the appropriate tree-cutting criteria in the EBG scheme, rather than having to hand-code them. The EBG scheme has previously proved most successful for 194 tuning a natural-language grammar to a specific application domain and thereby achieve very much faster parsing, at the cost of a small reduction in coverage. Instruments have been developed and tested for controlling the coverage and for avoiding a large number of short reductions, which is argued to be the main source to poor parser performance. Although these instruments are currently slightly too blunt to enable producing grammars with the same high performance as the hand-coded tree-cutting criteria, they can most probably be sharpened by future research, and in particular refined to achieve the delicate balance between high coverage and a distribution of reduction lengths that is sufficiently biased towards long reductions. Also, banning recursion by category specialization, i.e. by for distinguishing dominate other from those that do not, will be investigated, since this is believed to be an important ingredient in the version of the scheme employing hand-coded tree-cutting criteria. ACKNOWLEDGEMENTS This research was made possible by the basic research programme at the Swedish Institute of Computer Science (SICS). I wish to thank Manny Rayner of SRI International, Cambridge, for help and support in matters pertaining to the treebank, and for enlightening discussions of the scheme as a whole. I also wish to thank the NLP group at SICS for contributing to a very conductive atmosphere to work in, and in particular Ivan Bretan for valuable comments on draft versions of this article. Finally, I wish to thank the anonymous reviewers for their comments.</abstract>
<note confidence="0.963919">References Hiyan Alshawi, editor. Core Engine, Press 1992. [Jelinek 1990] Fred Jelinek. &amp;quot;Self-Organizing Language for Speech Recognition&amp;quot;, Readings in Recognition, 450-506, Morgan Kaufmann 1990.</note>
<email confidence="0.444094">al</email>
<author confidence="0.8978605">Explanation-Based Generaliza-</author>
<note confidence="0.9674308">A Unifying View&amp;quot;, Machine Learning 1, No. 1, pp. 47-80, 1986. [Quinlan 1986] J. Ross Quinlan. &amp;quot;Induction of Decision in Learning 1, 1, pp. 81-107, 1986.</note>
<author confidence="0.79869525">A Speech to Speech Transla-</author>
<title confidence="0.3871705">tion System Built From Standard Components&amp;quot;, ARPA Workshop on Human Language</title>
<note confidence="0.896194235294118">NJ 1993. 1994a] Christer Samuelsson. Natural- Language Parsing Using Explanation-Based Learnthesis, Royal Institute of Technology, Stockholm, Sweden 1994. [Samuelsson 1994b] Christer Samuelsson. &amp;quot;Notes on Parser Design&amp;quot; to appear in 15th International Conference on Computational Linguis- Japan 1994. [Samuelsson &amp; Rayner 1991] Christer Samuelsson and Manny Rayner. &amp;quot;Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for a Large-Scale Natural Language System&amp;quot;, in Procs. 12th International Joint Conference on Ar- Intelligence, 609-615, Sydney, Australia 1991. 195</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>1992</date>
<booktitle>The Core Language Engine,</booktitle>
<editor>Hiyan Alshawi, editor.</editor>
<publisher>MIT Press</publisher>
<marker>[Alshawi ed. 1992]</marker>
<rawString>Hiyan Alshawi, editor. The Core Language Engine, MIT Press 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Jelinek</author>
</authors>
<title>Self-Organizing Language Models for Speech Recognition&amp;quot;,</title>
<date>1990</date>
<booktitle>in Readings in Speech Recognition,</booktitle>
<pages>450--506</pages>
<publisher>Morgan Kaufmann</publisher>
<marker>[Jelinek 1990]</marker>
<rawString>Fred Jelinek. &amp;quot;Self-Organizing Language Models for Speech Recognition&amp;quot;, in Readings in Speech Recognition, pp. 450-506, Morgan Kaufmann 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
<author>Richard M Keller</author>
<author>Smadar T Kedar-Cabelli</author>
</authors>
<title>Explanation-Based Generalization: A Unifying View&amp;quot;,</title>
<date>1986</date>
<journal>in Machine Learning</journal>
<volume>1</volume>
<pages>47--80</pages>
<marker>[Mitchell et al 1986]</marker>
<rawString> Tom M. Mitchell, Richard M. Keller and Smadar T. Kedar-Cabelli. &amp;quot;Explanation-Based Generalization: A Unifying View&amp;quot;, in Machine Learning 1, No. 1, pp. 47-80, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>Induction of Decision Trees&amp;quot;,</title>
<date>1986</date>
<journal>in Machine Learning</journal>
<volume>1</volume>
<pages>81--107</pages>
<marker>[Quinlan 1986]</marker>
<rawString>J. Ross Quinlan. &amp;quot;Induction of Decision Trees&amp;quot;, in Machine Learning 1, No. 1, pp. 81-107, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>H Alshawi</author>
<author>I Bretan</author>
<author>D Carter</author>
<author>V Digalakis</author>
<author>B Gamback</author>
<author>J Kaja</author>
<author>J Karlgren</author>
<author>B Lyberg</author>
<author>P Price</author>
<author>S Pulman</author>
<author>C Samuelsson</author>
</authors>
<title>A Speech to Speech Translation System Built From Standard Components&amp;quot;,</title>
<date>1993</date>
<booktitle>in Procs. ARPA Workshop on Human Language Technology,</booktitle>
<location>Princeton, NJ</location>
<marker>[Rayner et al 1993]</marker>
<rawString>M. Rayner, H. Alshawi, I. Bretan, D. Carter, V. Digalakis, B. Gamback, J. Kaja, J. Karlgren, B. Lyberg, P. Price, S. Pulman and C. Samuelsson. &amp;quot;A Speech to Speech Translation System Built From Standard Components&amp;quot;, in Procs. ARPA Workshop on Human Language Technology, Princeton, NJ 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
</authors>
<title>Fast NaturalLanguage Parsing Using Explanation-Based Learning,</title>
<date>1994</date>
<tech>PhD thesis,</tech>
<institution>Royal Institute of Technology,</institution>
<location>Stockholm,</location>
<marker>[Samuelsson 1994a]</marker>
<rawString>Christer Samuelsson. Fast NaturalLanguage Parsing Using Explanation-Based Learning, PhD thesis, Royal Institute of Technology, Stockholm, Sweden 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
</authors>
<title>Notes on LR Parser Design&amp;quot; to appear in</title>
<date>1994</date>
<booktitle>Procs. 15th International Conference on Computational Linguistics, Kyoto,</booktitle>
<marker>[Samuelsson 1994b]</marker>
<rawString>Christer Samuelsson. &amp;quot;Notes on LR Parser Design&amp;quot; to appear in Procs. 15th International Conference on Computational Linguistics, Kyoto, Japan 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
<author>Manny Rayner</author>
</authors>
<title>Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for a Large-Scale Natural Language System&amp;quot;,</title>
<date>1991</date>
<booktitle>in Procs. 12th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>609--615</pages>
<location>Sydney, Australia</location>
<marker>[Samuelsson &amp; Rayner 1991]</marker>
<rawString>Christer Samuelsson and Manny Rayner. &amp;quot;Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for a Large-Scale Natural Language System&amp;quot;, in Procs. 12th International Joint Conference on Artificial Intelligence, pp. 609-615, Sydney, Australia 1991.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>