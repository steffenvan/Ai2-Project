<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000166">
<title confidence="0.989715">
Stochastic Text Structuring using the Principle of Continuity
</title>
<author confidence="0.959104">
Iikiforos Karamanis and Hisar Maruli Manurung
</author>
<affiliation confidence="0.993270333333333">
Institute for Communicating and Collaborative Systems
Division of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.995764">
nikiforo, hisarm @cogsci.ed.ac.uk
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983">
This paper explores the feasibility of im-
plementing an evolutionary algorithm for
text structuring using the heuristic of con-
tinuity as a fitness function, chosen over
other more complicated metrics of text co-
herence. Using MCGONAGALL (Manu-
rung et al., 2000) as our experimental plat-
form, we show that by employing an elitist
strategy for stochastic search it is possi-
ble to quickly reach the global optimum
of minimal violations of continuity.
</bodyText>
<sectionHeader confidence="0.989909" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.999887846153846">
Although notions of entity-based coherence have of-
ten been employed in text structuring, the definition
of an evaluation metric for entity-based coherence is
a non-trivial problem. This section reviews some of
the suggested solutions and argues for a simpler so-
lution that uses just the principle of continuity as a
predictor of the coherence of a text. In the remainder
of the paper, we report on our attempt to implement
an evolutionary algorithm guided by the heuristic
of continuity in order to reach the global optimum
quickly and effectively. Finally, we discuss how far
this effort stands from actually generating coherent
text structures stochastically.
</bodyText>
<subsectionHeader confidence="0.9621725">
1.1 Text Generation and entity-based models
of coherence
</subsectionHeader>
<bodyText confidence="0.999970644444445">
The idea of using entity-based constraints on coher-
ence in Natural Language Generation (NLG) goes
as far back as McKeown’s TEXT generation sys-
tem (McKeown, 1985). McKeown uses predefined
schemata to describe the structure of a text and ap-
plies entity-based constraints formulated as local fo-
cus rules in order to choose between the alternatives
that may match the next predicate in the schema.
The proposition that satisfies the most preferred rule
for local focus movement is chosen over the rest of
the candidates for what to say next.
Subsequent work on NLG tried to move away
from predefined schemata by using Rhetorical
Structure Theory (RST) as a domain-independent
framework for text structuring (Mann and Thomp-
son, 1987). According to RST, a natural text can be
described as a hierarchical structure with a rhetori-
cal relation between each two consecutive spans of
the text.
More recently, Knott et al. (2001) identified a
number of problems in the RST framework con-
cerning the relation OBJECT-ATTRIBUTE ELABO-
RATION. They suggest that ELABORATION be
eliminated from the group of Rhetorical Relations
and replaced by entity-based models of text coher-
ence such as Centering Theory (Grosz et al., 1995;
Walker et al., 1998).
Although Knott et al. (2001) identified Centering
Theory (henceforth CT) as one of the possible
entity-based models that can be used in the context
of text structuring for NLG, the exact formulation of
CT in order to serve this purpose remains an open
question.
The next section presents the view of
Kibble and Power (2000) on how CT can be
translated into a part of an evaluation metric that se-
lects the best structure out of a restricted number of
candidate solutions. This approach is similar to our
view of NLG as a formal search problem, already
presented in Mellish et al. (1998). Then, we discuss
how well the metric in Kibble and Power (2000)
performs as a predictor of the coherence of texts in
a specific domain and argue that a simpler metric
based on the principle of continuity appears to yield
better results with respect to this task.
</bodyText>
<subsectionHeader confidence="0.907975">
1.2 Evaluation metrics for text structuring
</subsectionHeader>
<bodyText confidence="0.999962846153846">
Kibble and Power (2000) redefine CT in terms of
the “four underlying principles” of entity-based co-
herence, formally named as continuity, coherence,
cheapness and salience. They describe ICONO-
CLAST, an NLG system that uses these principles
alongside other constraints on text quality.
Although Kibble and Power (2000) mention that
each of these principles may be assigned a differ-
ent cost, in practice they decide that all of them be
weighted equally. As a result, their evaluation metric
for entity-based coherence is reduced to a function
that sums up the number of times that each candi-
date structure violates each of the underlying prin-
ciples of CT and then adds the four resulting sums
together.
In ICONOCLAST, this metric of entity-based co-
herence is part of a larger evaluation module that
applies a battery of tests to a restricted set of can-
didate solutions and selects the one with the low-
est total cost. Kibble and Power (2000) argue that
a candidate solution that violates (some of) the CT-
based constraints might still be selected if it respects
certain stylistic preferences that are related with the
ways of realising the underlying rhetorical structure.
Mellish et al. (1998) were the first to experiment
with a range of stochastic search methods in or-
der to select the best rhetorical tree from a num-
ber of possible solutions for text structuring. As in
Kibble and Power (2000), the evaluation metric in
Mellish et al. (1998) includes entity-based features
of coherence as well as other parameters of text
quality. However, the exact weights that are as-
signed to the various features of this evaluation met-
ric are based purely on intuition.
Barzilay et al. (2001) present an integrated strat-
egy for ordering information in multidocument sum-
marization. In order to yield a coherent summary,
the chronological order of events is combined with
a constraint that ensures that sets of sentences on
the same topic occur together. This results in a
bottom-up approach for ordering that opportunisti-
cally groups together topically related sets of sen-
tences.
In this paper, topically related structures are also
favoured but since our domain is not predominantly
event-based, temporal coherence is not included
in our evaluation metric. In the next subsection,
we argue that an evaluation function based solely
on the principle of continuity represents a simpler
and more motivated solution than the ones used by
Mellish et al. (1998) and Kibble and Power (2000),
at least as far as our genre is concerned.
</bodyText>
<subsectionHeader confidence="0.985425">
1.3 The principle of continuity
</subsectionHeader>
<bodyText confidence="0.998766096774194">
While both Mellish et al. (1998) and
Kibble and Power (2000) investigate the in-
teraction between entity-based coherence and
rhetorical relations using intuitive evaluation met-
rics, Karamanis (2001) follows Knott et al. (2001)
in claiming that, in the descriptional genre, text
structuring is predominantly entity-based and that
rhetorical relations are rare and rather localised.
Karamanis (2001) then explores the usefulness of
entity-based metrics of text structure in evaluating
the overall coherence of a text without considering
additional constraints such as rhetorical relations.
Five evaluation metrics of entity-based coherence
are defined and their usefulness as predictors of the
coherence in a small corpus of descriptive texts is
tested.
The main result is that a simple metric that is
based solely on the principle of continuity, that is,
the requirement that each utterance in the discourse
refers to at least one entity in the utterance that pre-
cedes it,1 performs better than the other four met-
rics, including the addition function as defined by
Kibble and Power (2000).
Karamanis (2001) uses an input similar to the one
we are using in our current experiments.2 Starting
from an “original” ordering of facts that approxi-
mates the structure of a descriptive text written by
a human expert, all possible orderings are gener-
ated by permuting the facts in the original ordering.
For each ordering, the total number of violations of
the principle of continuity is recorded and compared
</bodyText>
<footnote confidence="0.98948325">
1A formal definition of this principle in terms of CT is:
Cf(U ) Cf(U) .
2See section 2.1 for more details on the input and the target
structures.
</footnote>
<bodyText confidence="0.965952387096774">
with the score of the original ordering which serves
as the gold standard. Finally, a complete overview of
the number of alternative solutions that score better,
equal or worse than the gold standard is obtained.
Karamanis (2001) reports that using a metric that
is based solely on the principle of continuity is found
to classify on average more than 90% of the search
space as worse than the original structure. Only
1% of the alternative text structures are found to be
better than the original one whereas the size of the
equal solutions is restricted to less than 9% of the
search space. Replacing the metric that is based
on the principle of continuity with other metrics
of entity-based coherence, including the addition
function defined by Kibble and Power (2000) con-
sistently gives worse results across the texts in the
corpus.3 More specifically, the average percentages
for the Kibble and Power metric are 44% for better,
15% for equal and only 41% for worse.
Ignoring other text structuring factors such as
rhetorical relations in the domain of descriptive texts
does not prove to be as dangerous as it originally
appears, since a metric based on the principle of
continuity permits only a limited number of possi-
ble orderings to score better than the original struc-
ture. Crucially, this metric classifies the original text
structure as better than the vast majority of its com-
petitors.
The exhaustive search in Karamanis (2001) re-
vealed a profile of the search space where texts
which do not violate continuity are very few indeed.
For example, from one text which consists of 12
facts, out of a possible orderings, only 96 order-
ings (that is, less than 0.0001%) completely satisfied
continuity. Furthermore, the orderings that violate
continuity once and appear in the same equivalence
class as the gold standard represent only 0.0027% of
the search space. This suggests that using the prin-
3The other three metrics of entity-based coherence tested
in Karamanis (2001) are (a) a simpler addition function
that computes the sum of the violations of only conti-
nuity and coherence, (b) a reformulation of that metric
in the spirit of Optimality Theory (Prince and Smolensky,
1997) that uses the preference order continuity coherence,
and (c) a similar reformulation of the addition function
used by Kibble and Power (2000) that defines the preference
order continuity coherence cheapness salience in a way
that comes close to some of the predictions of CT. See
Karamanis (2001) for more details on the definition and the per-
formance of those metrics.
ciple of continuity to look for the class of optimal
texts is a non-trivial search problem.
Due to the factorial complexity of the exhaustive
search in Karamanis (2001), the operation becomes
impractical for an input that consists of more than
12 facts. In this paper, we extend Karamanis (2001)
by discussing a stochastic approach for large inputs
which navigates the search space more efficiently.
In the section that follows, we provide more details
on the methodology that we followed and the soft-
ware that has been used in order to implement our
experiments.
</bodyText>
<sectionHeader confidence="0.995102" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.971177">
2.1 Task description
</subsectionHeader>
<bodyText confidence="0.9999027">
The input to our system consists of an unordered set
of facts that correspond to the underlying semantics
of a possible text. This input represents the output
of the content determination phase of NLG in the
standard pipeline architecture. The goal is to find
an ordering of all the facts that maximises its entity-
based coherence when eventually realised as a text.
Although this enforces an artificially rigid distinc-
tion between content determination and text struc-
turing, it is necessary for the objective evaluation of
the various coherence metrics.4
The texts that we are using in our system are
short descriptions of archaeological artefacts that
have been written in the context of the M-PIRO
project (Androutsopoulos et al., 2001). These texts
have been analysed into clause-sized propositions so
that each clause in the text roughly corresponds to a
different proposition in the database.
As a result, a text that originally appears in the
surface structure like this:
</bodyText>
<listItem confidence="0.677208">
(1) Towards the end of the archaic period, coins were
</listItem>
<bodyText confidence="0.9319665">
used for transactions. This particular coin, which
comes from that period, is a silver stater from Croton,
a Greek Colony in South Italy. On both the obverse
and the reverse side there is a tripod (vessel standing
on three legs), Apollo’s sacred symbol. Dates from
between 530-510 BC.
is taken to correspond to the following sequence of
facts in the text structure:
</bodyText>
<subsectionHeader confidence="0.709382">
4We follow Kibble and Power (2000),
</subsectionHeader>
<bodyText confidence="0.8975395">
Cheng and Mellish (2000), and Mellish et al. (1998) in
this respect.
</bodyText>
<listItem confidence="0.9491115">
(2) 1. use-coins(archaic-period)
2. creation-period(ex5, archaic-
period)
3. madeof(ex5, silver)
4. name(ex5, stater)
5. origin(ex5, croton)
6. concept-description(croton)
7. exhibit-depicts(ex5, sides,
tripod)
8. concept-description(tripod)
9. symbol(tripod, apollo)
10. dated(ex5, 530-510bc)
</listItem>
<bodyText confidence="0.99502752173913">
An unordered set of these facts is the semantic
input to our system. The ordering of the facts as de-
fined in example (2) is the target of the text structur-
ing process and serves as the basis for the evaluation
of our system.5 Since we are not concerned with is-
sues of aggregation and realisation of referring ex-
pressions, the targeted ordering can be thought to
represent a simple surface text as follows:
EAs are a broad class of optimisation meth-
ods, to which Genetic Algorithms (GA), em-
ployed in both Cheng and Mellish (2000) and
Mellish et al. (1998), belong. They are based on a
stochastic search process which maintains a popu-
lation of candidate solutions that evolve according
to rules of selection, recombination and mutation.
Each candidate receives a measure of fitness in its
environment, and selection focuses attention on high
fitness individuals. Although simplistic from a biol-
ogist’s viewpoint, they are sufficiently complex to
provide powerful search mechanisms (Spears et al.,
1993).
We can characterise our EA with the following
algorithm:
</bodyText>
<figure confidence="0.94965175">
Initialise population with random orderings of the
given facts.
Evaluate and rank/select
while optimal solution not found or maximum iterations
do
Evolve with mutation and/or crossover operations
Evaluate and rank/select
(3) (1) Towards the end of the archaic period, coins were
</figure>
<figureCaption confidence="0.954944625">
used for transactions. (2) This coin comes from the
archaic period. (3) It is made of silver. (4) It is called
a stater. (5) It comes from Croton. (6) Croton is a
Greek Colony in South Italy. (7) On both sides of
this coin there is a tripod. (8) A tripod is a vessel
resting on three legs. (9) It is god Apollo’s sacred
symbol. (10) The coin dates from between 530-510
BC.
</figureCaption>
<subsectionHeader confidence="0.996325">
2.2 Evolutionary algorithms
</subsectionHeader>
<bodyText confidence="0.998907090909091">
The task of generation does not necessarily require a
global optimum (Cheng and Mellish, 2000; Barzilay
et al., 2001). What is needed is a text that is coherent
enough to be understood. Additionally, as stated in
Mellish et al. (1998), NLG can benefit from the ad-
vantages of an anytime algorithm, i.e., an algorithm
that can be terminated at any point in time to yield
the best result found so far. These two characteris-
tics suggest that the paradigm of evolutionary algo-
rithms (henceforth EA) is a good choice for solving
our search problem.
</bodyText>
<footnote confidence="0.8016952">
5The same approach with respect to the target text has
been followed by Cheng and Mellish (2000) in their attempt
to capture the interaction between aggregation and text struc-
turing by using an evaluation function that extends the one in
Mellish et al. (1998).
</footnote>
<subsectionHeader confidence="0.707911">
end while
</subsectionHeader>
<bodyText confidence="0.999870416666667">
Our chosen selection process is the widely-used
roulette-wheel algorithm, which selects candidate
solutions from the previous generation with proba-
bility proportional to their fitness values. We also
implement an elitist strategy, where a small percent-
age (defined by the elitist ratio parameter) of the
fittest individuals are always copied over unevolved
to the next generation. This guarantees that the best
solution found so far is always kept, which improves
the EA’s overall performance. The trade-off, how-
ever, is that it can exert pressure towards premature
convergence (Goldberg, 1989).
</bodyText>
<subsectionHeader confidence="0.961938">
Fitness Function
</subsectionHeader>
<bodyText confidence="0.954346482758621">
Because we require our fitness function to assign
a higher score to more continuous texts, we sim-
ply count the number of continuity preservations be-
tween pairs of subsequent facts. Thus, the theoreti-
cal global maximum score achievable given an input
semantics of facts is .
Note, however, that for the stater text in sec-
tion 2.1, even the optimal solutions are bound to vi-
olate continuity once, that is, orderings with zero
violations of continuity do not exist. So the actual
global maximum here is 8. This is still higher than
the score of the target structure in (2) which violates
continuity twice (facts 7 and 10), thus scoring 7.
Operators
Mutation
We experimented with three simple mutation
operators, i.e. generating a completely random
permutation, random swapping of two facts
in an ordering, random repositioning of a fact
(removing it from its position and inserting it
elsewhere, shifting the other facts accordingly).
Crossover
We experimented with the combining of subse-
quences from two orderings and by taking
a randomly chosen subsequence from , insert-
ing it at a random point in , and then removing
duplicate facts from the original . This is how
crossover was implemented for the GA experi-
ment in Mellish et al. (1998).
</bodyText>
<subsectionHeader confidence="0.504663">
Implementation details
</subsectionHeader>
<bodyText confidence="0.999974666666667">
We implemented and ran our experiments using
MCGONAGALL, a system being developed with the
goal of generating simple rhyme-and-metre poetry,
i.e. texts that are highly constrained at both the se-
mantic and surface level (Manurung et al., 2000).
The underlying principles behind this system co-
incide with the view of NLG as a formal search task,
and use EAs to optimise the search. This is moti-
vated by the problems encountered when trying to
generate texts with surface constraints by using a tra-
ditional semantic goal-driven process.
MCGONAGALL is designed and implemented to
be as general-purpose as possible, enabling it to
serve as an experimental platform for various evo-
lutionary algorithm-based natural language genera-
tion research. Hence, it is an ideal system for our
purposes, and conversely, it is hoped that this exper-
iment will test its worth as an experimental platform.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999383714285714">
Six texts were chosen from the M-PIRO domain, as
in section 2.1. Three of these texts contain less than
12 facts, thus the complete profile of their search
space is known as a result of the exhaustive search in
Karamanis (2001). All experiment results reported
in this section are the average results of running the
test in question 10 times.
</bodyText>
<table confidence="0.999232">
Text name facts Target Mean Max.
stater 10 7 6.482 8.0
tetradrachm 10 8 7.602 9.0
drachma 11 9 8.384 10.0
kouros 18 13 14.022 17.0
amphora 20 17 15.328 19.0
hydria 23 20 16.783 20.8
</table>
<tableCaption confidence="0.999942">
Table 1: Results of the main experiment
</tableCaption>
<bodyText confidence="0.999967027777778">
Before carrying out our main experiments, we
conducted a preliminary experiment to find the most
promising choice of parameters for our EA. This
was done by running the EA on various possible
combinations of choice of operators and elitist ra-
tio parameters. Figure 1 shows the main results of
this preliminary experiment.
This figure plots the mean and maximum scores
of the population throughout the EA run for 500 iter-
ations on one of the texts in our domain (amphora).
The horizontal line represents the score of the target
structure, i.e. that of the text produced by the hu-
man expert. The three columns contrast the results
obtained when the elitist ratio was varied between
0 (i.e. non-elitist strategy), 0.1, and 0.2. The eli-
tist strategy proved to be crucial in our experiments
in guiding evolution towards convergence at an op-
timal solution without causing serious problems of
premature convergence.
The two rows of Figure 1 plot the results of us-
ing Crossover and Permute, two of the operators de-
tailed in section 2.2. Here it is shown that Permute
performs considerably worse than Crossover. This is
because completely random permutation is a highly
non-local move in the search space. Swap and Rein-
sert, the other two operators we experimented with,
performed similarly to Crossover.
Table 1 summarises the mean and maximum
scores of the population at the end of our main ex-
periment. For these experiments we iterated 4000
times, with a population size of 50, an elitist ratio of
0.2, and we employed the Crossover operator only.
Finally, Figure 2 plots the mean and maximum
fitness scores of the population throughout the main
experiment for our largest text, hydria (23 facts).
Similar patterns were found for the other five texts.
</bodyText>
<figure confidence="0.815682">
Elitist ratio = 0.0 0.1 0.2
Permute
</figure>
<figureCaption confidence="0.999707">
Figure 1: The differences between Crossover and Permute and elitist ratio on amphora
</figureCaption>
<figure confidence="0.998587076923077">
18
16
14
12
10
8
6
4
2
0
18
16
14
12
10
8
6
4
2
0
Crossover
18
16
14
12
10
8
6
4
2
Mean score
Maximum score
Human expert score
Mean score
Maximum score
Human expert score
0
0 50 100 150 200 250 300 350 400 450 500
0 50 100 150 200 250 300 350 400 450 500
Iterations
Iterations
0 50 100 150 200 250 300 350 400 450 500
Iterations
18
16
14
12
10
8
6
4
2
0
Mean score
Maximum score
Human expert score
18
18
16
16
14
14
12
12
10
10
8
8
6
6
4
4
2
2
Mean score
Maximum score
Human expert score
Mean score
Maximum score
Human expert score
0
0
0 50 100 150 200 250 300 350 400 450 500
0 50 100 150 200 250 300 350 400 450 500
Iterations
Iterations
Mean score
Maximum score
Human expert score
0 50 100 150 200 250 300 350 400 450 500
Iterations
</figure>
<figureCaption confidence="0.9496605">
Figure 2: Mean and maximum population scores for
hydria
</figureCaption>
<sectionHeader confidence="0.998469" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.9997189375">
Generally speaking, the graphs show that the popu-
lation swiftly matches the score of the target struc-
ture, and gradually stabilises at a slightly higher
score. This is due to the elitist strategy enforcing a
hillclimbing-like heuristic within our stochastic pro-
cess.
Returning to the stater text in section 2.1, it
is known that the optimal text structure represents
0.05% of a search space of more than 3.6 million
alternative orderings. Our EA manages to reach the
global maximum of 8 quickly and effectively.
We do not know the percentage of optimal text
structures for hydria, since with 23 facts it is im-
practical to profile the vast search space6 exhaus-
tively. Again, the algorithm reaches a solution close
to the target quite quickly.
</bodyText>
<subsectionHeader confidence="0.999685">
4.1 Quality of the generated text structures
</subsectionHeader>
<bodyText confidence="0.999485625">
Although we used the target structure and the profile
of the search space as our main basis for evaluating
the performance of our system with respect to the
search task, we also tried to realise the 10 best struc-
tures that the EA produces for the stater example
as surface texts by hand. As the following example
shows most of these texts did not appear to be very
different from (3) in section 2.1:
</bodyText>
<figureCaption confidence="0.847062">
(4) (1) Towards the end of the archaic period, coins were
used for transactions. (2) This coin comes from the
archaic period. (4) It is called a stater. (3) It is made
of silver. (7) On both sides of this coin there is a
tripod. (9) The tripod is god Apollo’s sacred symbol.
</figureCaption>
<figure confidence="0.907153636363636">
6i.e. 2,585,201,673,888,497,664,000 possible orderings!
0 500 1000 1500 2000 2500 3000 3500 4000
Iterations
20
15
10
5
0
Mean score
Maximum score
Human expert score
</figure>
<bodyText confidence="0.994121788461539">
structuring that will supplement continuity and build
a more informed evaluation function.
For example, both (4) and (5) are about a coin
and not a tripod which might be the reason why (5)
is not so good. This example seems to point to the
need to incorporate some sort of global coherence
into account in the evaluation metric.
Additionally, some of our previous experiments
have indicated that not permuting the first utterance
in the original sequence might prevent overgenera-
tion, but the results are far from conclusive. We are
currently investigating different initialisation strate-
gies in order to prevent structures like the one in (5).
In order to generate a text like the one in (1),
we intend to implement an aggregating operator
and evaluate its results. This will bring us close
to an evaluation function like the one discussed in
Cheng and Mellish (2000). Furthermore, we believe
that a careful study on the use of the title and the
layout in our genre, as well as a better definition of
the update unit for local focus might also prevent in-
coherent structures from being selected.
We also intend to explore the performance of this
metric within an integrated architecture that exploits
the interaction between content determination and
text planning, which MCGONAGALL allows for.
Finally, we recognise that the very limited evalu-
ation set of this experiment might cast doubt on the
significance of our results. Therefore, in order to test
the generality of our approach we intend to run our
experiments on additional texts from the GNOME
corpus7 that have already been annotated semanti-
cally in terms of their entity-based coherence and
can serve as a suitable input to our experiments.
(8) A tripod is a vessel resting on three legs. (10) The
coin dates from between 530-510 BC. (5) It comes
from Croton. (6) Croton is a Greek Colony in South
Italy.
The main difference between this example and the
structure in (3), is that fact 10 in example (4) ap-
pears in a position where it satisfies continuity thus
reaching the global optimum of only one violation.
Note that the original text in (1) avoids the violation
of continuity in (3), by aggregating facts 8 and 9 in
the same sentence as fact 7.
However, we also noticed that some of the pre-
ferred text structures under our approach will ac-
tually sound quite incoherent when compared with
the original texts. For example, the following text
structure starts by focusing on the entity ‘tripod’, a
strategy which does not seem to be preferred in our
domain:
</bodyText>
<figure confidence="0.656344">
(5) (8) A tripod is a vessel resting on three legs. (9) It
</figure>
<figureCaption confidence="0.82662375">
is god Apollo’s sacred symbol (7) On both sides of
this coin there is a tripod. (10) The coin dates from
between 530-510 BC. (4) It is called a stater. (1) To-
wards the end of the archaic period, coins were used
for transactions. (2) This coin comes from the ar-
chaic period. (3) It is made of silver. (5) It comes
from Croton. (6) Croton is a Greek Colony in South
Italy.
</figureCaption>
<bodyText confidence="0.99979475">
This text also achieves the same score of only one
violation, but our metric fails to discriminate be-
tween the structure in (4) and the rather incoherent
pattern in (5).
</bodyText>
<sectionHeader confidence="0.9974975" genericHeader="method">
5 Future Work
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99988280952381">
So how far are we from actually generating a coher-
ent text structure using a stochastic approach like the
one discussed above? Examples like (5) above sug-
gest that we need to elaborate on our evaluation met-
ric to ensure that we perform stochastic text structur-
ing more effectively.
In the future, we intend to implement a surface
generation component based on canned text in order
to investigate the output of our experiment more sys-
tematically. Spotting continuous text structures that
result in incoherent surface texts like the one in (5)
allows us to investigate additional principles for text
In conclusion, instead of presenting a complete so-
lution for text structuring, the experiments discussed
in this paper explore the feasibility of using stochas-
tic search guided by an evaluation function which is
based solely on continuity. Keeping this in mind, we
have shown that:
Even though the optimal solutions are quite
rare in the search space, a stochastic approach
that uses an elitist strategy manages to reach the
</bodyText>
<footnote confidence="0.834021">
7http://www.hcrc.ed.ac.uk/˜gnome/
</footnote>
<bodyText confidence="0.9983484">
global optimum very quickly and avoids pre-
mature convergence.
In most cases, our system generates coherent
text structures stochastically by using only the
principle of continuity as a fitness function.
However, some of the resulting surface texts are
quite incoherent, and our system gives us the
opportunity to investigate the limits of an eval-
uation metric that is only based on continuity
and discuss additional amendments to it.
</bodyText>
<sectionHeader confidence="0.997601" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.993701777777778">
The authors are grateful to Jon Oberlander and Chris
Mellish for extended discussions and their general
guidance and support, and to three anonymous re-
viewers for their comments. However, we remain
responsible for all the mistakes of the paper. The
first author is supported by the Greek State Scholar-
ships Foundation (IKY). The second author is sup-
ported by the World Bank QUE Project, Faculty of
Computer Science, Universitas Indonesia.
</bodyText>
<sectionHeader confidence="0.998467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999919060606061">
Ion Androutsopoulos, Vaki Kokkinaki, Aggeliki Dimitro-
manolaki, Jonathan Calder, Jon Oberlander, and Elena
Not. 2001. Generating multilingual personalized de-
scriptions of museum exhibits: the m-piro project. In
Proceedings of the International Conference on Com-
puter Applications and Quantitative Methods in Ar-
chaeology, Gotland, Sweden.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2001. Sentence ordering in multidocument sum-
marization. In Proceedings ofHLT, San Diego.
Hua Cheng and Chris Mellish. 2000. Capturing the in-
teraction between aggregation and text planning in two
generation systems. In Proceedings ofINLG-2000, Is-
rael.
D.E. Goldberg. 1989. Genetic Algorithms in Search, Op-
timization, and Machine Learning. Addison-Wesley,
Reading, MA.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
Nikiforos Karamanis. 2001. Exploring entity-based co-
herence. In Proceedings of CLUK4, pages 18–26,
University of Sheffield.
Rodger Kibble and Richard Power. 2000. An integrated
framework for text planning and pronominalisation. In
Proceedings ofINLG 2000, pages 77–84, Israel.
Alistair Knott, Jon Oberlander, Mick O’Donnell, and
Chris Mellish. 2001. Beyond elaboration: the in-
teraction of relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and W. Spooren, editors,
Text representation: linguistic and psycholinguistic
aspects, chapter 7, pages 181–196. Benjamins, Am-
sterdam.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical structure theory: A theory of text organ-
isation. Technical Report RR-87-190, University of
Southern California/ Information Sciences Institute.
Hisar Maruli Manurung, Graeme Ritchie, and Henry
Thompson. 2000. A flexible integrated architecture
for generating poetic texts. In Proceedings of the
Fourth Symposium on Natural Language Processing,
Chiang Mai, Thailand, May.
Kathleen R. McKeown. 1985. Text Generation: Using
discourse strategies and focus constraints to generate
Natural Language Text. Studies in Natural Language
Processing. Cambridge University Press.
Chris Mellish, Alistair Knott, Jon Oberlander, and Mick
O’Donnell. 1998. Experiments using stochastic
search for text planning. In Proceedings of the
9th International Workshop on NLG, pages 98–107,
Niagara-on-the-Lake, Ontario, Canada.
Alan Prince and Paul Smolensky. 1997. Optimality:
from neural networks to universal grammar. Science,
275:1604–1610.
William M. Spears, Kenneth A. De Jong, Thomas Bck,
David B. Fogel, and Hugo de Garis. 1993. An
overview of evolutionary computation. In Machine
Learning: ECML-93, Proceedings of the European
Conference on Machine Learning, pages 442–459.
Springer.
Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince.
1998. Centering in naturally occuring discourse: An
overview. In Marilyn A. Walker, Aravind K. Joshi,
and Ellen F. Prince, editors, Centering Theory in Dis-
course, pages 1–30. Clarendon Press, Oxford.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999312">Stochastic Text Structuring using the Principle of Continuity</title>
<author confidence="0.969172">Karamanis Maruli Manurung</author>
<affiliation confidence="0.989768333333333">Institute for Communicating and Collaborative Systems Division of Informatics University of Edinburgh</affiliation>
<email confidence="0.951991">nikiforo,hisarm@cogsci.ed.ac.uk</email>
<abstract confidence="0.99611681548975">This paper explores the feasibility of implementing an evolutionary algorithm for structuring using the heuristic of cona fitness function, chosen over other more complicated metrics of text co- Using (Manurung et al., 2000) as our experimental platform, we show that by employing an elitist strategy for stochastic search it is possible to quickly reach the global optimum of minimal violations of continuity. 1 Background Although notions of entity-based coherence have often been employed in text structuring, the definition of an evaluation metric for entity-based coherence is a non-trivial problem. This section reviews some of the suggested solutions and argues for a simpler solution that uses just the principle of continuity as a predictor of the coherence of a text. In the remainder of the paper, we report on our attempt to implement an evolutionary algorithm guided by the heuristic of continuity in order to reach the global optimum quickly and effectively. Finally, we discuss how far this effort stands from actually generating coherent text structures stochastically. 1.1 Text Generation and entity-based models of coherence The idea of using entity-based constraints on coherence in Natural Language Generation (NLG) goes far back as McKeown’s system (McKeown, 1985). McKeown uses predefined schemata to describe the structure of a text and applies entity-based constraints formulated as local focus rules in order to choose between the alternatives that may match the next predicate in the schema. The proposition that satisfies the most preferred rule for local focus movement is chosen over the rest of the candidates for what to say next. Subsequent work on NLG tried to move away from predefined schemata by using Rhetorical Structure Theory (RST) as a domain-independent framework for text structuring (Mann and Thompson, 1987). According to RST, a natural text can be described as a hierarchical structure with a rhetorical relation between each two consecutive spans of the text. More recently, Knott et al. (2001) identified a number of problems in the RST framework conthe relation They suggest that eliminated from the group of Rhetorical Relations and replaced by entity-based models of text coherence such as Centering Theory (Grosz et al., 1995; Walker et al., 1998). Although Knott et al. (2001) identified Centering Theory (henceforth CT) as one of the possible entity-based models that can be used in the context of text structuring for NLG, the exact formulation of CT in order to serve this purpose remains an open question. The next section presents the view of Kibble and Power (2000) on how CT can be translated into a part of an evaluation metric that selects the best structure out of a restricted number of candidate solutions. This approach is similar to our view of NLG as a formal search problem, already presented in Mellish et al. (1998). Then, we discuss how well the metric in Kibble and Power (2000) performs as a predictor of the coherence of texts in a specific domain and argue that a simpler metric based on the principle of continuity appears to yield better results with respect to this task. 1.2 Evaluation metrics for text structuring Kibble and Power (2000) redefine CT in terms of the “four underlying principles” of entity-based coformally named as coherence, They describe an NLG system that uses these principles alongside other constraints on text quality. Although Kibble and Power (2000) mention that each of these principles may be assigned a different cost, in practice they decide that all of them be weighted equally. As a result, their evaluation metric for entity-based coherence is reduced to a function that sums up the number of times that each candidate structure violates each of the underlying principles of CT and then adds the four resulting sums together. this metric of entity-based coherence is part of a larger evaluation module that applies a battery of tests to a restricted set of candidate solutions and selects the one with the lowest total cost. Kibble and Power (2000) argue that a candidate solution that violates (some of) the CTbased constraints might still be selected if it respects certain stylistic preferences that are related with the ways of realising the underlying rhetorical structure. Mellish et al. (1998) were the first to experiment with a range of stochastic search methods in order to select the best rhetorical tree from a number of possible solutions for text structuring. As in Kibble and Power (2000), the evaluation metric in Mellish et al. (1998) includes entity-based features of coherence as well as other parameters of text quality. However, the exact weights that are assigned to the various features of this evaluation metric are based purely on intuition. Barzilay et al. (2001) present an integrated strategy for ordering information in multidocument summarization. In order to yield a coherent summary, the chronological order of events is combined with a constraint that ensures that sets of sentences on the same topic occur together. This results in a bottom-up approach for ordering that opportunistically groups together topically related sets of sentences. In this paper, topically related structures are also favoured but since our domain is not predominantly event-based, temporal coherence is not included in our evaluation metric. In the next subsection, we argue that an evaluation function based solely on the principle of continuity represents a simpler and more motivated solution than the ones used by Mellish et al. (1998) and Kibble and Power (2000), at least as far as our genre is concerned. 1.3 The principle of continuity While both Mellish et al. (1998) and Kibble and Power (2000) investigate the interaction between entity-based coherence and rhetorical relations using intuitive evaluation metrics, Karamanis (2001) follows Knott et al. (2001) in claiming that, in the descriptional genre, text structuring is predominantly entity-based and that rhetorical relations are rare and rather localised. Karamanis (2001) then explores the usefulness of entity-based metrics of text structure in evaluating overall coherence of a text additional constraints such as rhetorical relations. Five evaluation metrics of entity-based coherence are defined and their usefulness as predictors of the coherence in a small corpus of descriptive texts is tested. The main result is that a simple metric that is solely on the principle of that is, requirement that utterance in the discourse refers to at least one entity in the utterance that preperforms better than the other four metrics, including the addition function as defined by Kibble and Power (2000). Karamanis (2001) uses an input similar to the one are using in our current Starting from an “original” ordering of facts that approximates the structure of a descriptive text written by a human expert, all possible orderings are generated by permuting the facts in the original ordering. For each ordering, the total number of violations of the principle of continuity is recorded and compared formal definition of this principle in terms of CT is: Cf(U ) Cf(U) . section 2.1 for more details on the input and the target structures. with the score of the original ordering which serves as the gold standard. Finally, a complete overview of number of alternative solutions that score the gold standard is obtained. Karamanis (2001) reports that using a metric that is based solely on the principle of continuity is found to classify on average more than 90% of the search as the original structure. Only 1% of the alternative text structures are found to be the original one whereas the size of the is restricted to less than 9% of the search space. Replacing the metric that is based on the principle of continuity with other metrics of entity-based coherence, including the addition function defined by Kibble and Power (2000) consistently gives worse results across the texts in the More specifically, the average percentages the Kibble and Power metric are 44% for for only 41% for Ignoring other text structuring factors such as rhetorical relations in the domain of descriptive texts does not prove to be as dangerous as it originally appears, since a metric based on the principle of continuity permits only a limited number of possible orderings to score better than the original structure. Crucially, this metric classifies the original text structure as better than the vast majority of its competitors. The exhaustive search in Karamanis (2001) revealed a profile of the search space where texts which do not violate continuity are very few indeed. For example, from one text which consists of 12 out of a possible orderings, only 96 ings (that is, less than 0.0001%) completely satisfied continuity. Furthermore, the orderings that violate continuity once and appear in the same equivalence class as the gold standard represent only 0.0027% of search space. This suggests that using the prinother three metrics of entity-based coherence tested in Karamanis (2001) are (a) a simpler addition function that computes the sum of the violations of only continuity and coherence, (b) a reformulation of that metric in the spirit of Optimality Theory (Prince and Smolensky, 1997) that uses the preference order continuity coherence, and (c) a similar reformulation of the addition function used by Kibble and Power (2000) that defines the preference order continuity coherence cheapness salience in a way that comes close to some of the predictions of CT. See Karamanis (2001) for more details on the definition and the performance of those metrics. ciple of continuity to look for the class of optimal texts is a non-trivial search problem. Due to the factorial complexity of the exhaustive search in Karamanis (2001), the operation becomes impractical for an input that consists of more than 12 facts. In this paper, we extend Karamanis (2001) by discussing a stochastic approach for large inputs which navigates the search space more efficiently. In the section that follows, we provide more details on the methodology that we followed and the software that has been used in order to implement our experiments. 2 Methodology 2.1 Task description The input to our system consists of an unordered set of facts that correspond to the underlying semantics of a possible text. This input represents the output of the content determination phase of NLG in the standard pipeline architecture. The goal is to find an ordering of all the facts that maximises its entitybased coherence when eventually realised as a text. Although this enforces an artificially rigid distinction between content determination and text structuring, it is necessary for the objective evaluation of various coherence The texts that we are using in our system are short descriptions of archaeological artefacts that have been written in the context of the M-PIRO project (Androutsopoulos et al., 2001). These texts have been analysed into clause-sized propositions so that each clause in the text roughly corresponds to a different proposition in the database. As a result, a text that originally appears in the surface structure like this: the end of the archaic period, coins were used for transactions. This particular coin, which comes from that period, is a silver stater from Croton, a Greek Colony in South Italy. On both the obverse and the reverse side there is a tripod (vessel standing on three legs), Apollo’s sacred symbol. Dates from between 530-510 BC. is taken to correspond to the following sequence of facts in the text structure: follow Kibble and Power (2000), Cheng and Mellish (2000), and Mellish et al. (1998) this respect. 2. creation-period(ex5, archaicperiod) 3. madeof(ex5, silver) 4. name(ex5, stater) 5. origin(ex5, croton) 6. concept-description(croton) 7. exhibit-depicts(ex5, sides, tripod) 8. concept-description(tripod) 9. symbol(tripod, apollo) 10. dated(ex5, 530-510bc) An unordered set of these facts is the semantic input to our system. The ordering of the facts as defined in example (2) is the target of the text structuring process and serves as the basis for the evaluation our Since we are not concerned with issues of aggregation and realisation of referring expressions, the targeted ordering can be thought to represent a simple surface text as follows: EAs are a broad class of optimisation methods, to which Genetic Algorithms (GA), employed in both Cheng and Mellish (2000) and Mellish et al. (1998), belong. They are based on a stochastic search process which maintains a population of candidate solutions that evolve according to rules of selection, recombination and mutation. Each candidate receives a measure of fitness in its environment, and selection focuses attention on high fitness individuals. Although simplistic from a biologist’s viewpoint, they are sufficiently complex to provide powerful search mechanisms (Spears et al., 1993). We can characterise our EA with the following algorithm: Initialise population with random orderings of the given facts. Evaluate and rank/select solution not found or maximum do Evolve with mutation and/or crossover operations Evaluate and rank/select Towards the end of the archaic period, coins were for transactions. This coin comes from the period. It is made of silver. It is called stater. It comes from Croton. Croton is a Colony in South Italy. On both sides of coin there is a tripod. A tripod is a vessel on three legs. It is god Apollo’s sacred The coin dates from between 530-510 BC. 2.2 Evolutionary algorithms The task of generation does not necessarily require a global optimum (Cheng and Mellish, 2000; Barzilay al., 2001). What is needed is a text that is be understood. Additionally, as stated in Mellish et al. (1998), NLG can benefit from the adof an i.e., an algorithm that can be terminated at any point in time to yield the best result found so far. These two characterissuggest that the paradigm of algo- EA) is a good choice for solving our search problem. same approach with respect to the target text has been followed by Cheng and Mellish (2000) in their attempt to capture the interaction between aggregation and text structuring by using an evaluation function that extends the one in Mellish et al. (1998). end while Our chosen selection process is the widely-used which selects candidate solutions from the previous generation with probability proportional to their fitness values. We also an where a small percent- (defined by the ratio of the fittest individuals are always copied over unevolved to the next generation. This guarantees that the best solution found so far is always kept, which improves the EA’s overall performance. The trade-off, however, is that it can exert pressure towards premature convergence (Goldberg, 1989). Fitness Function Because we require our fitness function to assign a higher score to more continuous texts, we simply count the number of continuity preservations between pairs of subsequent facts. Thus, the theoretical global maximum score achievable given an input semantics of facts is . however, that for the in section 2.1, even the optimal solutions are bound to violate continuity once, that is, orderings with zero violations of continuity do not exist. So the actual global maximum here is 8. This is still higher than the score of the target structure in (2) which violates twice (facts thus scoring 7. Operators Mutation We experimented with three simple mutation operators, i.e. generating a completely random random two facts an ordering, random a fact (removing it from its position and inserting it elsewhere, shifting the other facts accordingly). Crossover We experimented with the combining of subsequences from two orderings and by taking a randomly chosen subsequence from , inserting it at a random point in , and then removing duplicate facts from the original . This is how crossover was implemented for the GA experiment in Mellish et al. (1998). Implementation details We implemented and ran our experiments using a system being developed with the goal of generating simple rhyme-and-metre poetry, i.e. texts that are highly constrained at both the semantic and surface level (Manurung et al., 2000). The underlying principles behind this system coincide with the view of NLG as a formal search task, and use EAs to optimise the search. This is motivated by the problems encountered when trying to generate texts with surface constraints by using a traditional semantic goal-driven process. designed and implemented to be as general-purpose as possible, enabling it to serve as an experimental platform for various evolutionary algorithm-based natural language generation research. Hence, it is an ideal system for our purposes, and conversely, it is hoped that this experiment will test its worth as an experimental platform. 3 Results Six texts were chosen from the M-PIRO domain, as in section 2.1. Three of these texts contain less than 12 facts, thus the complete profile of their search space is known as a result of the exhaustive search in Karamanis (2001). All experiment results reported in this section are the average results of running the test in question 10 times. Text name facts Target Mean Max. stater 10 7 6.482 8.0 tetradrachm 10 8 7.602 9.0 drachma 11 9 8.384 10.0 kouros 18 13 14.022 17.0 amphora 20 17 15.328 19.0 hydria 23 20 16.783 20.8 Table 1: Results of the main experiment Before carrying out our main experiments, we conducted a preliminary experiment to find the most promising choice of parameters for our EA. This was done by running the EA on various possible combinations of choice of operators and elitist ratio parameters. Figure 1 shows the main results of this preliminary experiment. This figure plots the mean and maximum scores of the population throughout the EA run for 500 iteron one of the texts in our domain The horizontal line represents the score of the target structure, i.e. that of the text produced by the human expert. The three columns contrast the results when the ratio varied between 0 (i.e. non-elitist strategy), 0.1, and 0.2. The elitist strategy proved to be crucial in our experiments in guiding evolution towards convergence at an optimal solution without causing serious problems of premature convergence. The two rows of Figure 1 plot the results of using Crossover and Permute, two of the operators detailed in section 2.2. Here it is shown that Permute performs considerably worse than Crossover. This is because completely random permutation is a highly non-local move in the search space. Swap and Reinsert, the other two operators we experimented with, performed similarly to Crossover. Table 1 summarises the mean and maximum scores of the population at the end of our main experiment. For these experiments we iterated 4000 times, with a population size of 50, an elitist ratio of 0.2, and we employed the Crossover operator only. Finally, Figure 2 plots the mean and maximum fitness scores of the population throughout the main for our largest text, facts). Similar patterns were found for the other five texts.</abstract>
<note confidence="0.911438741935484">Elitist ratio = 0.0 0.1 0.2 Permute 1: The differences between Crossover and Permute and elitist ratio on 18 16 14 12 10 8 6 4 2 0 18 16 14 12 10 8 6 4 2 0 Crossover 18 16 14 12 10 8 6</note>
<abstract confidence="0.312140555555556">4 2 Mean score Maximum score Human expert score Mean score Maximum score Human expert score 0</abstract>
<phone confidence="0.7861645">0 50 100 150 200 250 300 350 400 450 500 0 50 100 150 200 250 300 350 400 450 500</phone>
<note confidence="0.903441785714286">Iterations Iterations 0 50 100 150 200 250 300 350 400 450 500 Iterations 18 16 14 12 10 8 6 4 2 0</note>
<author confidence="0.515323333333333">Mean score Maximum score Human expert score</author>
<date confidence="0.512124">18</date>
<note confidence="0.843565294117647">18 16 16 14 14 12 12 10 10 8 8 6 6 4 4 2 2</note>
<title confidence="0.565120333333333">Mean score Maximum score Human expert score</title>
<author confidence="0.573995">Mean score Maximum score</author>
<note confidence="0.657842333333333">Human expert score 0 0</note>
<phone confidence="0.8438875">0 50 100 150 200 250 300 350 400 450 500 0 50 100 150 200 250 300 350 400 450 500</phone>
<title confidence="0.959591">Iterations Iterations</title>
<author confidence="0.88067">Mean score</author>
<abstract confidence="0.959624">Maximum score Human expert score 0 50 100 150 200 250 300 350 400 450 500 Iterations Figure 2: Mean and maximum population scores for hydria 4 Discussion Generally speaking, the graphs show that the population swiftly matches the score of the target structure, and gradually stabilises at a slightly higher score. This is due to the elitist strategy enforcing a hillclimbing-like heuristic within our stochastic process. to the in section 2.1, it is known that the optimal text structure represents 0.05% of a search space of more than 3.6 million alternative orderings. Our EA manages to reach the global maximum of 8 quickly and effectively. We do not know the percentage of optimal text for since with 23 facts it is imto profile the vast search exhaustively. Again, the algorithm reaches a solution close to the target quite quickly. 4.1 Quality of the generated text structures Although we used the target structure and the profile of the search space as our main basis for evaluating the performance of our system with respect to the search task, we also tried to realise the 10 best structhat the EA produces for the as surface texts by hand. As the following example shows most of these texts did not appear to be very different from (3) in section 2.1: Towards the end of the archaic period, coins were for transactions. This coin comes from the period. It is called a stater. It is made silver. On both sides of this coin there is a The tripod is god Apollo’s sacred symbol. 2,585,201,673,888,497,664,000 possible orderings!</abstract>
<phone confidence="0.584054">0 500 1000 1500 2000 2500 3000 3500 4000</phone>
<note confidence="0.816815166666667">Iterations 20 15 10 5 0</note>
<abstract confidence="0.982362963302752">Mean score Maximum score Human expert score structuring that will supplement continuity and build a more informed evaluation function. For example, both (4) and (5) are about a coin and not a tripod which might be the reason why (5) is not so good. This example seems to point to the need to incorporate some sort of global coherence into account in the evaluation metric. Additionally, some of our previous experiments have indicated that not permuting the first utterance in the original sequence might prevent overgeneration, but the results are far from conclusive. We are currently investigating different initialisation strategies in order to prevent structures like the one in (5). In order to generate a text like the one in (1), we intend to implement an aggregating operator and evaluate its results. This will bring us close to an evaluation function like the one discussed in Cheng and Mellish (2000). Furthermore, we believe that a careful study on the use of the title and the layout in our genre, as well as a better definition of the update unit for local focus might also prevent incoherent structures from being selected. We also intend to explore the performance of this metric within an integrated architecture that exploits the interaction between content determination and planning, which for. Finally, we recognise that the very limited evaluation set of this experiment might cast doubt on the significance of our results. Therefore, in order to test the generality of our approach we intend to run our experiments on additional texts from the GNOME that have already been annotated semantically in terms of their entity-based coherence and can serve as a suitable input to our experiments. A tripod is a vessel resting on three legs. The dates from between 530-510 BC. It comes Croton. Croton is a Greek Colony in South Italy. The main difference between this example and the in (3), is that fact example (4) appears in a position where it satisfies continuity thus reaching the global optimum of only one violation. Note that the original text in (1) avoids the violation continuity in (3), by aggregating facts same sentence as fact However, we also noticed that some of the preferred text structures under our approach will actually sound quite incoherent when compared with the original texts. For example, the following text structure starts by focusing on the entity ‘tripod’, a strategy which does not seem to be preferred in our domain: A tripod is a vessel resting on three legs. It god Apollo’s sacred symbol On both sides of coin there is a tripod. The coin dates from 530-510 BC. It is called a stater. Towards the end of the archaic period, coins were used transactions. This coin comes from the arperiod. It is made of silver. It comes Croton. Croton is a Greek Colony in South Italy. This text also achieves the same score of only one violation, but our metric fails to discriminate between the structure in (4) and the rather incoherent pattern in (5). 5 Future Work 6 Conclusion So how far are we from actually generating a coherent text structure using a stochastic approach like the one discussed above? Examples like (5) above suggest that we need to elaborate on our evaluation metric to ensure that we perform stochastic text structuring more effectively. In the future, we intend to implement a surface generation component based on canned text in order to investigate the output of our experiment more systematically. Spotting continuous text structures that result in incoherent surface texts like the one in (5) allows us to investigate additional principles for text In conclusion, instead of presenting a complete solution for text structuring, the experiments discussed in this paper explore the feasibility of using stochastic search guided by an evaluation function which is based solely on continuity. Keeping this in mind, we have shown that: Even though the optimal solutions are quite rare in the search space, a stochastic approach that uses an elitist strategy manages to reach the global optimum very quickly and avoids premature convergence. In most cases, our system generates coherent text structures stochastically by using only the principle of continuity as a fitness function. However, some of the resulting surface texts are quite incoherent, and our system gives us the opportunity to investigate the limits of an evaluation metric that is only based on continuity and discuss additional amendments to it. Acknowledgements The authors are grateful to Jon Oberlander and Chris Mellish for extended discussions and their general guidance and support, and to three anonymous reviewers for their comments. However, we remain responsible for all the mistakes of the paper. The first author is supported by the Greek State Scholarships Foundation (IKY). The second author is sup-</abstract>
<note confidence="0.782317444444445">ported by the World Bank QUE Project, Faculty of Computer Science, Universitas Indonesia. References Ion Androutsopoulos, Vaki Kokkinaki, Aggeliki Dimitromanolaki, Jonathan Calder, Jon Oberlander, and Elena Not. 2001. Generating multilingual personalized descriptions of museum exhibits: the m-piro project. In Proceedings of the International Conference on Computer Applications and Quantitative Methods in Ar-</note>
<address confidence="0.860329">Gotland, Sweden.</address>
<author confidence="0.799441">Regina Barzilay</author>
<author confidence="0.799441">Noemie Elhadad</author>
<author confidence="0.799441">Kathleen McKe-</author>
<abstract confidence="0.977615">own. 2001. Sentence ordering in multidocument sum- In San Diego. Hua Cheng and Chris Mellish. 2000. Capturing the interaction between aggregation and text planning in two systems. In Israel.</abstract>
<author confidence="0.459974">Algorithms in Search</author>
<author confidence="0.459974">Op-</author>
<affiliation confidence="0.923153">and Machine Addison-Wesley,</affiliation>
<address confidence="0.892757">Reading, MA.</address>
<note confidence="0.915847736842105">Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the locoherence of discourse. Linguis- 21(2):203–225. Nikiforos Karamanis. 2001. Exploring entity-based co- In of pages 18–26, University of Sheffield. Rodger Kibble and Richard Power. 2000. An integrated framework for text planning and pronominalisation. In ofINLG pages 77–84, Israel. Alistair Knott, Jon Oberlander, Mick O’Donnell, and Chris Mellish. 2001. Beyond elaboration: the interaction of relations and focus in coherent text. In T. Sanders, J. Schilperoord, and W. Spooren, editors, Text representation: linguistic and psycholinguistic chapter 7, pages 181–196. Benjamins, Amsterdam. William C. Mann and Sandra A. Thompson. 1987. Rhetorical structure theory: A theory of text organisation. Technical Report RR-87-190, University of Southern California/ Information Sciences Institute. Hisar Maruli Manurung, Graeme Ritchie, and Henry Thompson. 2000. A flexible integrated architecture generating poetic texts. In of the Symposium on Natural Language Chiang Mai, Thailand, May. R. McKeown. 1985. Generation: Using discourse strategies and focus constraints to generate Language Studies in Natural Language Processing. Cambridge University Press. Chris Mellish, Alistair Knott, Jon Oberlander, and Mick O’Donnell. 1998. Experiments using stochastic for text planning. In of the International Workshop on pages 98–107, Niagara-on-the-Lake, Ontario, Canada. Alan Prince and Paul Smolensky. 1997. Optimality: neural networks to universal grammar. 275:1604–1610.</note>
<author confidence="0.87466">An</author>
<note confidence="0.850351333333333">of evolutionary computation. In Learning: ECML-93, Proceedings of the European on Machine pages 442–459. Springer. Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince. 1998. Centering in naturally occuring discourse: An</note>
<author confidence="0.7707825">In Marilyn A Walker</author>
<author confidence="0.7707825">Aravind K Joshi</author>
<author confidence="0.7707825">Ellen F Prince</author>
<author confidence="0.7707825">Theory in Dis- editors</author>
<address confidence="0.735839">pages 1–30. Clarendon Press, Oxford.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Vaki Kokkinaki</author>
<author>Aggeliki Dimitromanolaki</author>
<author>Jonathan Calder</author>
<author>Jon Oberlander</author>
<author>Elena Not</author>
</authors>
<title>Generating multilingual personalized descriptions of museum exhibits: the m-piro project.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Computer Applications and Quantitative Methods in Archaeology,</booktitle>
<location>Gotland,</location>
<contexts>
<context position="11644" citStr="Androutsopoulos et al., 2001" startWordPosition="1887" endWordPosition="1890">a possible text. This input represents the output of the content determination phase of NLG in the standard pipeline architecture. The goal is to find an ordering of all the facts that maximises its entitybased coherence when eventually realised as a text. Although this enforces an artificially rigid distinction between content determination and text structuring, it is necessary for the objective evaluation of the various coherence metrics.4 The texts that we are using in our system are short descriptions of archaeological artefacts that have been written in the context of the M-PIRO project (Androutsopoulos et al., 2001). These texts have been analysed into clause-sized propositions so that each clause in the text roughly corresponds to a different proposition in the database. As a result, a text that originally appears in the surface structure like this: (1) Towards the end of the archaic period, coins were used for transactions. This particular coin, which comes from that period, is a silver stater from Croton, a Greek Colony in South Italy. On both the obverse and the reverse side there is a tripod (vessel standing on three legs), Apollo’s sacred symbol. Dates from between 530-510 BC. is taken to correspon</context>
</contexts>
<marker>Androutsopoulos, Kokkinaki, Dimitromanolaki, Calder, Oberlander, Not, 2001</marker>
<rawString>Ion Androutsopoulos, Vaki Kokkinaki, Aggeliki Dimitromanolaki, Jonathan Calder, Jon Oberlander, and Elena Not. 2001. Generating multilingual personalized descriptions of museum exhibits: the m-piro project. In Proceedings of the International Conference on Computer Applications and Quantitative Methods in Archaeology, Gotland, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen McKeown</author>
</authors>
<title>Sentence ordering in multidocument summarization.</title>
<date>2001</date>
<booktitle>In Proceedings ofHLT,</booktitle>
<location>San Diego.</location>
<contexts>
<context position="5233" citStr="Barzilay et al. (2001)" startWordPosition="847" endWordPosition="850"> certain stylistic preferences that are related with the ways of realising the underlying rhetorical structure. Mellish et al. (1998) were the first to experiment with a range of stochastic search methods in order to select the best rhetorical tree from a number of possible solutions for text structuring. As in Kibble and Power (2000), the evaluation metric in Mellish et al. (1998) includes entity-based features of coherence as well as other parameters of text quality. However, the exact weights that are assigned to the various features of this evaluation metric are based purely on intuition. Barzilay et al. (2001) present an integrated strategy for ordering information in multidocument summarization. In order to yield a coherent summary, the chronological order of events is combined with a constraint that ensures that sets of sentences on the same topic occur together. This results in a bottom-up approach for ordering that opportunistically groups together topically related sets of sentences. In this paper, topically related structures are also favoured but since our domain is not predominantly event-based, temporal coherence is not included in our evaluation metric. In the next subsection, we argue th</context>
<context position="14557" citStr="Barzilay et al., 2001" startWordPosition="2356" endWordPosition="2359">crossover operations Evaluate and rank/select (3) (1) Towards the end of the archaic period, coins were used for transactions. (2) This coin comes from the archaic period. (3) It is made of silver. (4) It is called a stater. (5) It comes from Croton. (6) Croton is a Greek Colony in South Italy. (7) On both sides of this coin there is a tripod. (8) A tripod is a vessel resting on three legs. (9) It is god Apollo’s sacred symbol. (10) The coin dates from between 530-510 BC. 2.2 Evolutionary algorithms The task of generation does not necessarily require a global optimum (Cheng and Mellish, 2000; Barzilay et al., 2001). What is needed is a text that is coherent enough to be understood. Additionally, as stated in Mellish et al. (1998), NLG can benefit from the advantages of an anytime algorithm, i.e., an algorithm that can be terminated at any point in time to yield the best result found so far. These two characteristics suggest that the paradigm of evolutionary algorithms (henceforth EA) is a good choice for solving our search problem. 5The same approach with respect to the target text has been followed by Cheng and Mellish (2000) in their attempt to capture the interaction between aggregation and text stru</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2001</marker>
<rawString>Regina Barzilay, Noemie Elhadad, and Kathleen McKeown. 2001. Sentence ordering in multidocument summarization. In Proceedings ofHLT, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Cheng</author>
<author>Chris Mellish</author>
</authors>
<title>Capturing the interaction between aggregation and text planning in two generation systems.</title>
<date>2000</date>
<booktitle>In Proceedings ofINLG-2000,</booktitle>
<contexts>
<context position="12364" citStr="Cheng and Mellish (2000)" startWordPosition="2007" endWordPosition="2010">roughly corresponds to a different proposition in the database. As a result, a text that originally appears in the surface structure like this: (1) Towards the end of the archaic period, coins were used for transactions. This particular coin, which comes from that period, is a silver stater from Croton, a Greek Colony in South Italy. On both the obverse and the reverse side there is a tripod (vessel standing on three legs), Apollo’s sacred symbol. Dates from between 530-510 BC. is taken to correspond to the following sequence of facts in the text structure: 4We follow Kibble and Power (2000), Cheng and Mellish (2000), and Mellish et al. (1998) in this respect. (2) 1. use-coins(archaic-period) 2. creation-period(ex5, archaicperiod) 3. madeof(ex5, silver) 4. name(ex5, stater) 5. origin(ex5, croton) 6. concept-description(croton) 7. exhibit-depicts(ex5, sides, tripod) 8. concept-description(tripod) 9. symbol(tripod, apollo) 10. dated(ex5, 530-510bc) An unordered set of these facts is the semantic input to our system. The ordering of the facts as defined in example (2) is the target of the text structuring process and serves as the basis for the evaluation of our system.5 Since we are not concerned with issue</context>
<context position="14533" citStr="Cheng and Mellish, 2000" startWordPosition="2352" endWordPosition="2355">lve with mutation and/or crossover operations Evaluate and rank/select (3) (1) Towards the end of the archaic period, coins were used for transactions. (2) This coin comes from the archaic period. (3) It is made of silver. (4) It is called a stater. (5) It comes from Croton. (6) Croton is a Greek Colony in South Italy. (7) On both sides of this coin there is a tripod. (8) A tripod is a vessel resting on three legs. (9) It is god Apollo’s sacred symbol. (10) The coin dates from between 530-510 BC. 2.2 Evolutionary algorithms The task of generation does not necessarily require a global optimum (Cheng and Mellish, 2000; Barzilay et al., 2001). What is needed is a text that is coherent enough to be understood. Additionally, as stated in Mellish et al. (1998), NLG can benefit from the advantages of an anytime algorithm, i.e., an algorithm that can be terminated at any point in time to yield the best result found so far. These two characteristics suggest that the paradigm of evolutionary algorithms (henceforth EA) is a good choice for solving our search problem. 5The same approach with respect to the target text has been followed by Cheng and Mellish (2000) in their attempt to capture the interaction between a</context>
<context position="23755" citStr="Cheng and Mellish (2000)" startWordPosition="3951" endWordPosition="3954"> to incorporate some sort of global coherence into account in the evaluation metric. Additionally, some of our previous experiments have indicated that not permuting the first utterance in the original sequence might prevent overgeneration, but the results are far from conclusive. We are currently investigating different initialisation strategies in order to prevent structures like the one in (5). In order to generate a text like the one in (1), we intend to implement an aggregating operator and evaluate its results. This will bring us close to an evaluation function like the one discussed in Cheng and Mellish (2000). Furthermore, we believe that a careful study on the use of the title and the layout in our genre, as well as a better definition of the update unit for local focus might also prevent incoherent structures from being selected. We also intend to explore the performance of this metric within an integrated architecture that exploits the interaction between content determination and text planning, which MCGONAGALL allows for. Finally, we recognise that the very limited evaluation set of this experiment might cast doubt on the significance of our results. Therefore, in order to test the generality</context>
</contexts>
<marker>Cheng, Mellish, 2000</marker>
<rawString>Hua Cheng and Chris Mellish. 2000. Capturing the interaction between aggregation and text planning in two generation systems. In Proceedings ofINLG-2000, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Goldberg</author>
</authors>
<date>1989</date>
<booktitle>Genetic Algorithms in Search, Optimization, and Machine Learning.</booktitle>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="15848" citStr="Goldberg, 1989" startWordPosition="2567" endWordPosition="2568">l. (1998). end while Our chosen selection process is the widely-used roulette-wheel algorithm, which selects candidate solutions from the previous generation with probability proportional to their fitness values. We also implement an elitist strategy, where a small percentage (defined by the elitist ratio parameter) of the fittest individuals are always copied over unevolved to the next generation. This guarantees that the best solution found so far is always kept, which improves the EA’s overall performance. The trade-off, however, is that it can exert pressure towards premature convergence (Goldberg, 1989). Fitness Function Because we require our fitness function to assign a higher score to more continuous texts, we simply count the number of continuity preservations between pairs of subsequent facts. Thus, the theoretical global maximum score achievable given an input semantics of facts is . Note, however, that for the stater text in section 2.1, even the optimal solutions are bound to violate continuity once, that is, orderings with zero violations of continuity do not exist. So the actual global maximum here is 8. This is still higher than the score of the target structure in (2) which viola</context>
</contexts>
<marker>Goldberg, 1989</marker>
<rawString>D.E. Goldberg. 1989. Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2636" citStr="Grosz et al., 1995" startWordPosition="407" endWordPosition="410">defined schemata by using Rhetorical Structure Theory (RST) as a domain-independent framework for text structuring (Mann and Thompson, 1987). According to RST, a natural text can be described as a hierarchical structure with a rhetorical relation between each two consecutive spans of the text. More recently, Knott et al. (2001) identified a number of problems in the RST framework concerning the relation OBJECT-ATTRIBUTE ELABORATION. They suggest that ELABORATION be eliminated from the group of Rhetorical Relations and replaced by entity-based models of text coherence such as Centering Theory (Grosz et al., 1995; Walker et al., 1998). Although Knott et al. (2001) identified Centering Theory (henceforth CT) as one of the possible entity-based models that can be used in the context of text structuring for NLG, the exact formulation of CT in order to serve this purpose remains an open question. The next section presents the view of Kibble and Power (2000) on how CT can be translated into a part of an evaluation metric that selects the best structure out of a restricted number of candidate solutions. This approach is similar to our view of NLG as a formal search problem, already presented in Mellish et a</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikiforos Karamanis</author>
</authors>
<title>Exploring entity-based coherence.</title>
<date>2001</date>
<booktitle>In Proceedings of CLUK4,</booktitle>
<pages>18--26</pages>
<institution>University of Sheffield.</institution>
<contexts>
<context position="6297" citStr="Karamanis (2001)" startWordPosition="1015" endWordPosition="1016"> but since our domain is not predominantly event-based, temporal coherence is not included in our evaluation metric. In the next subsection, we argue that an evaluation function based solely on the principle of continuity represents a simpler and more motivated solution than the ones used by Mellish et al. (1998) and Kibble and Power (2000), at least as far as our genre is concerned. 1.3 The principle of continuity While both Mellish et al. (1998) and Kibble and Power (2000) investigate the interaction between entity-based coherence and rhetorical relations using intuitive evaluation metrics, Karamanis (2001) follows Knott et al. (2001) in claiming that, in the descriptional genre, text structuring is predominantly entity-based and that rhetorical relations are rare and rather localised. Karamanis (2001) then explores the usefulness of entity-based metrics of text structure in evaluating the overall coherence of a text without considering additional constraints such as rhetorical relations. Five evaluation metrics of entity-based coherence are defined and their usefulness as predictors of the coherence in a small corpus of descriptive texts is tested. The main result is that a simple metric that i</context>
<context position="7975" citStr="Karamanis (2001)" startWordPosition="1286" endWordPosition="1287">f a descriptive text written by a human expert, all possible orderings are generated by permuting the facts in the original ordering. For each ordering, the total number of violations of the principle of continuity is recorded and compared 1A formal definition of this principle in terms of CT is: Cf(U ) Cf(U) . 2See section 2.1 for more details on the input and the target structures. with the score of the original ordering which serves as the gold standard. Finally, a complete overview of the number of alternative solutions that score better, equal or worse than the gold standard is obtained. Karamanis (2001) reports that using a metric that is based solely on the principle of continuity is found to classify on average more than 90% of the search space as worse than the original structure. Only 1% of the alternative text structures are found to be better than the original one whereas the size of the equal solutions is restricted to less than 9% of the search space. Replacing the metric that is based on the principle of continuity with other metrics of entity-based coherence, including the addition function defined by Kibble and Power (2000) consistently gives worse results across the texts in the </context>
<context position="9727" citStr="Karamanis (2001)" startWordPosition="1577" endWordPosition="1578">vast majority of its competitors. The exhaustive search in Karamanis (2001) revealed a profile of the search space where texts which do not violate continuity are very few indeed. For example, from one text which consists of 12 facts, out of a possible orderings, only 96 orderings (that is, less than 0.0001%) completely satisfied continuity. Furthermore, the orderings that violate continuity once and appear in the same equivalence class as the gold standard represent only 0.0027% of the search space. This suggests that using the prin3The other three metrics of entity-based coherence tested in Karamanis (2001) are (a) a simpler addition function that computes the sum of the violations of only continuity and coherence, (b) a reformulation of that metric in the spirit of Optimality Theory (Prince and Smolensky, 1997) that uses the preference order continuity coherence, and (c) a similar reformulation of the addition function used by Kibble and Power (2000) that defines the preference order continuity coherence cheapness salience in a way that comes close to some of the predictions of CT. See Karamanis (2001) for more details on the definition and the performance of those metrics. ciple of continuity </context>
<context position="18250" citStr="Karamanis (2001)" startWordPosition="2964" endWordPosition="2965">emantic goal-driven process. MCGONAGALL is designed and implemented to be as general-purpose as possible, enabling it to serve as an experimental platform for various evolutionary algorithm-based natural language generation research. Hence, it is an ideal system for our purposes, and conversely, it is hoped that this experiment will test its worth as an experimental platform. 3 Results Six texts were chosen from the M-PIRO domain, as in section 2.1. Three of these texts contain less than 12 facts, thus the complete profile of their search space is known as a result of the exhaustive search in Karamanis (2001). All experiment results reported in this section are the average results of running the test in question 10 times. Text name facts Target Mean Max. stater 10 7 6.482 8.0 tetradrachm 10 8 7.602 9.0 drachma 11 9 8.384 10.0 kouros 18 13 14.022 17.0 amphora 20 17 15.328 19.0 hydria 23 20 16.783 20.8 Table 1: Results of the main experiment Before carrying out our main experiments, we conducted a preliminary experiment to find the most promising choice of parameters for our EA. This was done by running the EA on various possible combinations of choice of operators and elitist ratio parameters. Figu</context>
</contexts>
<marker>Karamanis, 2001</marker>
<rawString>Nikiforos Karamanis. 2001. Exploring entity-based coherence. In Proceedings of CLUK4, pages 18–26, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodger Kibble</author>
<author>Richard Power</author>
</authors>
<title>An integrated framework for text planning and pronominalisation.</title>
<date>2000</date>
<booktitle>In Proceedings ofINLG 2000,</booktitle>
<pages>77--84</pages>
<contexts>
<context position="2983" citStr="Kibble and Power (2000)" startWordPosition="466" endWordPosition="469"> number of problems in the RST framework concerning the relation OBJECT-ATTRIBUTE ELABORATION. They suggest that ELABORATION be eliminated from the group of Rhetorical Relations and replaced by entity-based models of text coherence such as Centering Theory (Grosz et al., 1995; Walker et al., 1998). Although Knott et al. (2001) identified Centering Theory (henceforth CT) as one of the possible entity-based models that can be used in the context of text structuring for NLG, the exact formulation of CT in order to serve this purpose remains an open question. The next section presents the view of Kibble and Power (2000) on how CT can be translated into a part of an evaluation metric that selects the best structure out of a restricted number of candidate solutions. This approach is similar to our view of NLG as a formal search problem, already presented in Mellish et al. (1998). Then, we discuss how well the metric in Kibble and Power (2000) performs as a predictor of the coherence of texts in a specific domain and argue that a simpler metric based on the principle of continuity appears to yield better results with respect to this task. 1.2 Evaluation metrics for text structuring Kibble and Power (2000) redef</context>
<context position="4492" citStr="Kibble and Power (2000)" startWordPosition="724" endWordPosition="727">0) mention that each of these principles may be assigned a different cost, in practice they decide that all of them be weighted equally. As a result, their evaluation metric for entity-based coherence is reduced to a function that sums up the number of times that each candidate structure violates each of the underlying principles of CT and then adds the four resulting sums together. In ICONOCLAST, this metric of entity-based coherence is part of a larger evaluation module that applies a battery of tests to a restricted set of candidate solutions and selects the one with the lowest total cost. Kibble and Power (2000) argue that a candidate solution that violates (some of) the CTbased constraints might still be selected if it respects certain stylistic preferences that are related with the ways of realising the underlying rhetorical structure. Mellish et al. (1998) were the first to experiment with a range of stochastic search methods in order to select the best rhetorical tree from a number of possible solutions for text structuring. As in Kibble and Power (2000), the evaluation metric in Mellish et al. (1998) includes entity-based features of coherence as well as other parameters of text quality. However</context>
<context position="6023" citStr="Kibble and Power (2000)" startWordPosition="971" endWordPosition="974">bined with a constraint that ensures that sets of sentences on the same topic occur together. This results in a bottom-up approach for ordering that opportunistically groups together topically related sets of sentences. In this paper, topically related structures are also favoured but since our domain is not predominantly event-based, temporal coherence is not included in our evaluation metric. In the next subsection, we argue that an evaluation function based solely on the principle of continuity represents a simpler and more motivated solution than the ones used by Mellish et al. (1998) and Kibble and Power (2000), at least as far as our genre is concerned. 1.3 The principle of continuity While both Mellish et al. (1998) and Kibble and Power (2000) investigate the interaction between entity-based coherence and rhetorical relations using intuitive evaluation metrics, Karamanis (2001) follows Knott et al. (2001) in claiming that, in the descriptional genre, text structuring is predominantly entity-based and that rhetorical relations are rare and rather localised. Karamanis (2001) then explores the usefulness of entity-based metrics of text structure in evaluating the overall coherence of a text without c</context>
<context position="8517" citStr="Kibble and Power (2000)" startWordPosition="1377" endWordPosition="1380">score better, equal or worse than the gold standard is obtained. Karamanis (2001) reports that using a metric that is based solely on the principle of continuity is found to classify on average more than 90% of the search space as worse than the original structure. Only 1% of the alternative text structures are found to be better than the original one whereas the size of the equal solutions is restricted to less than 9% of the search space. Replacing the metric that is based on the principle of continuity with other metrics of entity-based coherence, including the addition function defined by Kibble and Power (2000) consistently gives worse results across the texts in the corpus.3 More specifically, the average percentages for the Kibble and Power metric are 44% for better, 15% for equal and only 41% for worse. Ignoring other text structuring factors such as rhetorical relations in the domain of descriptive texts does not prove to be as dangerous as it originally appears, since a metric based on the principle of continuity permits only a limited number of possible orderings to score better than the original structure. Crucially, this metric classifies the original text structure as better than the vast m</context>
<context position="10078" citStr="Kibble and Power (2000)" startWordPosition="1632" endWordPosition="1635">Furthermore, the orderings that violate continuity once and appear in the same equivalence class as the gold standard represent only 0.0027% of the search space. This suggests that using the prin3The other three metrics of entity-based coherence tested in Karamanis (2001) are (a) a simpler addition function that computes the sum of the violations of only continuity and coherence, (b) a reformulation of that metric in the spirit of Optimality Theory (Prince and Smolensky, 1997) that uses the preference order continuity coherence, and (c) a similar reformulation of the addition function used by Kibble and Power (2000) that defines the preference order continuity coherence cheapness salience in a way that comes close to some of the predictions of CT. See Karamanis (2001) for more details on the definition and the performance of those metrics. ciple of continuity to look for the class of optimal texts is a non-trivial search problem. Due to the factorial complexity of the exhaustive search in Karamanis (2001), the operation becomes impractical for an input that consists of more than 12 facts. In this paper, we extend Karamanis (2001) by discussing a stochastic approach for large inputs which navigates the se</context>
<context position="12338" citStr="Kibble and Power (2000)" startWordPosition="2003" endWordPosition="2006"> each clause in the text roughly corresponds to a different proposition in the database. As a result, a text that originally appears in the surface structure like this: (1) Towards the end of the archaic period, coins were used for transactions. This particular coin, which comes from that period, is a silver stater from Croton, a Greek Colony in South Italy. On both the obverse and the reverse side there is a tripod (vessel standing on three legs), Apollo’s sacred symbol. Dates from between 530-510 BC. is taken to correspond to the following sequence of facts in the text structure: 4We follow Kibble and Power (2000), Cheng and Mellish (2000), and Mellish et al. (1998) in this respect. (2) 1. use-coins(archaic-period) 2. creation-period(ex5, archaicperiod) 3. madeof(ex5, silver) 4. name(ex5, stater) 5. origin(ex5, croton) 6. concept-description(croton) 7. exhibit-depicts(ex5, sides, tripod) 8. concept-description(tripod) 9. symbol(tripod, apollo) 10. dated(ex5, 530-510bc) An unordered set of these facts is the semantic input to our system. The ordering of the facts as defined in example (2) is the target of the text structuring process and serves as the basis for the evaluation of our system.5 Since we ar</context>
</contexts>
<marker>Kibble, Power, 2000</marker>
<rawString>Rodger Kibble and Richard Power. 2000. An integrated framework for text planning and pronominalisation. In Proceedings ofINLG 2000, pages 77–84, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
<author>Jon Oberlander</author>
<author>Mick O’Donnell</author>
<author>Chris Mellish</author>
</authors>
<title>Beyond elaboration: the interaction of relations and focus in coherent text. In</title>
<date>2001</date>
<pages>181--196</pages>
<editor>T. Sanders, J. Schilperoord, and W. Spooren, editors,</editor>
<publisher>Benjamins,</publisher>
<location>Amsterdam.</location>
<marker>Knott, Oberlander, O’Donnell, Mellish, 2001</marker>
<rawString>Alistair Knott, Jon Oberlander, Mick O’Donnell, and Chris Mellish. 2001. Beyond elaboration: the interaction of relations and focus in coherent text. In T. Sanders, J. Schilperoord, and W. Spooren, editors, Text representation: linguistic and psycholinguistic aspects, chapter 7, pages 181–196. Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: A theory of text organisation.</title>
<date>1987</date>
<tech>Technical Report RR-87-190,</tech>
<institution>University of Southern California/ Information Sciences Institute.</institution>
<contexts>
<context position="2158" citStr="Mann and Thompson, 1987" startWordPosition="329" endWordPosition="333"> back as McKeown’s TEXT generation system (McKeown, 1985). McKeown uses predefined schemata to describe the structure of a text and applies entity-based constraints formulated as local focus rules in order to choose between the alternatives that may match the next predicate in the schema. The proposition that satisfies the most preferred rule for local focus movement is chosen over the rest of the candidates for what to say next. Subsequent work on NLG tried to move away from predefined schemata by using Rhetorical Structure Theory (RST) as a domain-independent framework for text structuring (Mann and Thompson, 1987). According to RST, a natural text can be described as a hierarchical structure with a rhetorical relation between each two consecutive spans of the text. More recently, Knott et al. (2001) identified a number of problems in the RST framework concerning the relation OBJECT-ATTRIBUTE ELABORATION. They suggest that ELABORATION be eliminated from the group of Rhetorical Relations and replaced by entity-based models of text coherence such as Centering Theory (Grosz et al., 1995; Walker et al., 1998). Although Knott et al. (2001) identified Centering Theory (henceforth CT) as one of the possible en</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1987. Rhetorical structure theory: A theory of text organisation. Technical Report RR-87-190, University of Southern California/ Information Sciences Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hisar Maruli Manurung</author>
<author>Graeme Ritchie</author>
<author>Henry Thompson</author>
</authors>
<title>A flexible integrated architecture for generating poetic texts.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Symposium on Natural Language Processing,</booktitle>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="17371" citStr="Manurung et al., 2000" startWordPosition="2815" endWordPosition="2818">elsewhere, shifting the other facts accordingly). Crossover We experimented with the combining of subsequences from two orderings and by taking a randomly chosen subsequence from , inserting it at a random point in , and then removing duplicate facts from the original . This is how crossover was implemented for the GA experiment in Mellish et al. (1998). Implementation details We implemented and ran our experiments using MCGONAGALL, a system being developed with the goal of generating simple rhyme-and-metre poetry, i.e. texts that are highly constrained at both the semantic and surface level (Manurung et al., 2000). The underlying principles behind this system coincide with the view of NLG as a formal search task, and use EAs to optimise the search. This is motivated by the problems encountered when trying to generate texts with surface constraints by using a traditional semantic goal-driven process. MCGONAGALL is designed and implemented to be as general-purpose as possible, enabling it to serve as an experimental platform for various evolutionary algorithm-based natural language generation research. Hence, it is an ideal system for our purposes, and conversely, it is hoped that this experiment will te</context>
</contexts>
<marker>Manurung, Ritchie, Thompson, 2000</marker>
<rawString>Hisar Maruli Manurung, Graeme Ritchie, and Henry Thompson. 2000. A flexible integrated architecture for generating poetic texts. In Proceedings of the Fourth Symposium on Natural Language Processing, Chiang Mai, Thailand, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Text Generation: Using discourse strategies and focus constraints to generate Natural Language Text. Studies in Natural Language Processing.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1591" citStr="McKeown, 1985" startWordPosition="240" endWordPosition="241">n that uses just the principle of continuity as a predictor of the coherence of a text. In the remainder of the paper, we report on our attempt to implement an evolutionary algorithm guided by the heuristic of continuity in order to reach the global optimum quickly and effectively. Finally, we discuss how far this effort stands from actually generating coherent text structures stochastically. 1.1 Text Generation and entity-based models of coherence The idea of using entity-based constraints on coherence in Natural Language Generation (NLG) goes as far back as McKeown’s TEXT generation system (McKeown, 1985). McKeown uses predefined schemata to describe the structure of a text and applies entity-based constraints formulated as local focus rules in order to choose between the alternatives that may match the next predicate in the schema. The proposition that satisfies the most preferred rule for local focus movement is chosen over the rest of the candidates for what to say next. Subsequent work on NLG tried to move away from predefined schemata by using Rhetorical Structure Theory (RST) as a domain-independent framework for text structuring (Mann and Thompson, 1987). According to RST, a natural tex</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>Kathleen R. McKeown. 1985. Text Generation: Using discourse strategies and focus constraints to generate Natural Language Text. Studies in Natural Language Processing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Alistair Knott</author>
<author>Jon Oberlander</author>
<author>Mick O’Donnell</author>
</authors>
<title>Experiments using stochastic search for text planning.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th International Workshop on NLG,</booktitle>
<pages>98--107</pages>
<location>Niagara-on-the-Lake, Ontario, Canada.</location>
<marker>Mellish, Knott, Oberlander, O’Donnell, 1998</marker>
<rawString>Chris Mellish, Alistair Knott, Jon Oberlander, and Mick O’Donnell. 1998. Experiments using stochastic search for text planning. In Proceedings of the 9th International Workshop on NLG, pages 98–107, Niagara-on-the-Lake, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Prince</author>
<author>Paul Smolensky</author>
</authors>
<title>Optimality: from neural networks to universal grammar.</title>
<date>1997</date>
<journal>Science,</journal>
<pages>275--1604</pages>
<contexts>
<context position="9936" citStr="Prince and Smolensky, 1997" startWordPosition="1610" endWordPosition="1613">one text which consists of 12 facts, out of a possible orderings, only 96 orderings (that is, less than 0.0001%) completely satisfied continuity. Furthermore, the orderings that violate continuity once and appear in the same equivalence class as the gold standard represent only 0.0027% of the search space. This suggests that using the prin3The other three metrics of entity-based coherence tested in Karamanis (2001) are (a) a simpler addition function that computes the sum of the violations of only continuity and coherence, (b) a reformulation of that metric in the spirit of Optimality Theory (Prince and Smolensky, 1997) that uses the preference order continuity coherence, and (c) a similar reformulation of the addition function used by Kibble and Power (2000) that defines the preference order continuity coherence cheapness salience in a way that comes close to some of the predictions of CT. See Karamanis (2001) for more details on the definition and the performance of those metrics. ciple of continuity to look for the class of optimal texts is a non-trivial search problem. Due to the factorial complexity of the exhaustive search in Karamanis (2001), the operation becomes impractical for an input that consist</context>
</contexts>
<marker>Prince, Smolensky, 1997</marker>
<rawString>Alan Prince and Paul Smolensky. 1997. Optimality: from neural networks to universal grammar. Science, 275:1604–1610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Spears</author>
<author>Kenneth A De Jong</author>
<author>Thomas Bck</author>
<author>David B Fogel</author>
<author>Hugo de Garis</author>
</authors>
<title>An overview of evolutionary computation.</title>
<date>1993</date>
<booktitle>In Machine Learning: ECML-93, Proceedings of the European Conference on Machine Learning,</booktitle>
<pages>442--459</pages>
<publisher>Springer.</publisher>
<marker>Spears, De Jong, Bck, Fogel, de Garis, 1993</marker>
<rawString>William M. Spears, Kenneth A. De Jong, Thomas Bck, David B. Fogel, and Hugo de Garis. 1993. An overview of evolutionary computation. In Machine Learning: ECML-93, Proceedings of the European Conference on Machine Learning, pages 442–459. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Aravind K Joshi</author>
<author>Ellen F Prince</author>
</authors>
<title>Centering in naturally occuring discourse: An overview.</title>
<date>1998</date>
<booktitle>Centering Theory in Discourse,</booktitle>
<pages>1--30</pages>
<editor>In Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince, editors,</editor>
<publisher>Clarendon Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="2658" citStr="Walker et al., 1998" startWordPosition="411" endWordPosition="414">using Rhetorical Structure Theory (RST) as a domain-independent framework for text structuring (Mann and Thompson, 1987). According to RST, a natural text can be described as a hierarchical structure with a rhetorical relation between each two consecutive spans of the text. More recently, Knott et al. (2001) identified a number of problems in the RST framework concerning the relation OBJECT-ATTRIBUTE ELABORATION. They suggest that ELABORATION be eliminated from the group of Rhetorical Relations and replaced by entity-based models of text coherence such as Centering Theory (Grosz et al., 1995; Walker et al., 1998). Although Knott et al. (2001) identified Centering Theory (henceforth CT) as one of the possible entity-based models that can be used in the context of text structuring for NLG, the exact formulation of CT in order to serve this purpose remains an open question. The next section presents the view of Kibble and Power (2000) on how CT can be translated into a part of an evaluation metric that selects the best structure out of a restricted number of candidate solutions. This approach is similar to our view of NLG as a formal search problem, already presented in Mellish et al. (1998). Then, we di</context>
</contexts>
<marker>Walker, Joshi, Prince, 1998</marker>
<rawString>Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince. 1998. Centering in naturally occuring discourse: An overview. In Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince, editors, Centering Theory in Discourse, pages 1–30. Clarendon Press, Oxford.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>