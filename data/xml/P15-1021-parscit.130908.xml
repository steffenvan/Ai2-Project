<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000621">
<title confidence="0.982595">
Efficient Top-Down BTG Parsing for Machine Translation Preordering
</title>
<author confidence="0.981979">
Tetsuji Nakagawa
</author>
<affiliation confidence="0.968339">
Google Japan Inc.
</affiliation>
<email confidence="0.98743">
tnaka@google.com
</email>
<sectionHeader confidence="0.997255" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947954545455">
We present an efficient incremental top-
down parsing method for preordering
based on Bracketing Transduction Gram-
mar (BTG). The BTG-based preordering
framework (Neubig et al., 2012) can be
applied to any language using only par-
allel text, but has the problem of compu-
tational efficiency. Our top-down parsing
algorithm allows us to use the early up-
date technique easily for the latent vari-
able structured Perceptron algorithm with
beam search, and solves the problem.
Experimental results showed that the top-
down method is more than 10 times faster
than a method using the CYK algorithm.
A phrase-based machine translation sys-
tem with the top-down method had statis-
tically significantly higher BLEU scores
for 7 language pairs without relying on
supervised syntactic parsers, compared to
baseline systems using existing preorder-
ing methods.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999871159090909">
The difference of the word order between source
and target languages is one of major problems in
phrase-based statistical machine translation. In or-
der to cope with the issue, many approaches have
been studied. Distortion models consider word re-
ordering in decoding time using such as distance
(Koehn et al., 2003) and lexical information (Till-
man, 2004). Another direction is to use more com-
plex translation models such as hierarchical mod-
els (Chiang, 2007). However, these approaches
suffer from the long-distance reordering issue and
computational complexity.
Preordering (reordering-as-preprocessing) (Xia
and McCord, 2004; Collins et al., 2005) is another
approach for tackling the problem, which modifies
the word order of an input sentence in a source lan-
guage to have the word order in a target language
(Figure 1(a)).
Various methods for preordering have been
studied, and a method based on Bracketing Trans-
duction Grammar (BTG) was proposed by Neubig
et al. (2012). It reorders source sentences by han-
dling sentence structures as latent variables. The
method can be applied to any language using only
parallel text. However, the method has the prob-
lem of computational efficiency.
In this paper, we propose an efficient incremen-
tal top-down BTG parsing method which can be
applied to preordering. Model parameters can
be learned using latent variable Perceptron with
the early update technique (Collins and Roark,
2004), since the parsing method provides an easy
way for checking the reachability of each parser
state to valid final states. We also try to use
forced-decoding instead of word alignment based
on Expectation Maximization (EM) algorithms in
order to create better training data for preorder-
ing. In experiments, preordering using the top-
down parsing algorithm was faster and gave higher
BLEU scores than BTG-based preordering using
the CYK algorithm. Compared to existing pre-
ordering methods, our method had better or com-
parable BLEU scores without using supervised
parsers.
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<subsectionHeader confidence="0.984386">
2.1 Preordering for Machine Translation
</subsectionHeader>
<bodyText confidence="0.969972125">
Many preordering methods which use syntactic
parse trees have been proposed, because syntac-
tic information is useful for determining the word
order in a target language, and it can be used to
restrict the search space against all the possible
permutations. Preordering methods using manu-
ally created rules on parse trees have been stud-
ied (Collins et al., 2005; Xu et al., 2009), but
</bodyText>
<page confidence="0.967351">
208
</page>
<note confidence="0.988714333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 208–218,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999637">
Figure 1: An example of preordering.
</figureCaption>
<bodyText confidence="0.999844578947369">
linguistic knowledge for a language pair is nec-
essary to create such rules. Preordering methods
which automatically create reordering rules or uti-
lize statistical classifiers have also been studied
(Xia and McCord, 2004; Li et al., 2007; Gen-
zel, 2010; Visweswariah et al., 2010; Yang et al.,
2012; Miceli Barone and Attardi, 2013; Lerner
and Petrov, 2013; Jehl et al., 2014). These meth-
ods rely on source-side parse trees and cannot be
applied to languages where no syntactic parsers
are available.
There are preordering methods that do not need
parse trees. They are usually trained only on auto-
matically word-aligned parallel text. It is possible
to mine parallel text from the Web (Uszkoreit et
al., 2010; Antonova and Misyurev, 2011), and the
preordering systems can be trained without man-
ually annotated language resources. Tromble and
Eisner (2009) studied preordering based on a Lin-
ear Ordering Problem by defining a pairwise pref-
erence matrix. Khalilov and Sima’an (2010) pro-
posed a method which swaps adjacent two words
using a maximum entropy model. Visweswariah
et al. (2011) regarded the preordering problem as
a Traveling Salesman Problem (TSP) and applied
TSP solvers for obtaining reordered words. These
methods do not consider sentence structures.
DeNero and Uszkoreit (2011) presented a pre-
ordering method which builds a monolingual pars-
ing model and a tree reordering model from par-
allel text. Neubig et al. (2012) proposed to train
a discriminative BTG parser for preordering di-
rectly from word-aligned parallel text by handling
underlying parse trees with latent variables. This
method is explained in detail in the next subsec-
tion. These two methods can use sentence struc-
tures for designing feature functions to score per-
mutations.
</bodyText>
<figureCaption confidence="0.954314">
Figure 2: Bracketing transduction grammar.
</figureCaption>
<subsectionHeader confidence="0.961536">
2.2 BTG-based Preordering
</subsectionHeader>
<bodyText confidence="0.999495714285714">
Neubig et al. (2012) proposed a BTG-based pre-
ordering method. Bracketing Transduction Gram-
mar (BTG) (Wu, 1997) is a binary synchronous
context-free grammar with only one non-terminal
symbol, and has three types of rules (Figure 2):
Straight which keeps the order of child nodes,
Inverted which reverses the order, and Terminal
which generates a terminal symbol.1
BTG can express word reordering. For exam-
ple, the word reordering in Figure 1(a) can be rep-
resented with the BTG parse tree in Figure 1(b).2
Therefore, the task to reorder an input source sen-
tence can be solved as a BTG parsing task to find
an appropriate BTG tree.
In order to find the best BTG tree among all
the possible ones, a score function is defined. Let
4b(m) denote the vector of feature functions for
the BTG tree node m, and A denote the vector of
feature weights. Then, for a given source sentence
x, the best BTG tree z� and the reordered sentence
x′ can be obtained as follows:
</bodyText>
<equation confidence="0.999988">
�z� = argmax A · 4b(m), (1)
zEZ(x) mENodes(z)
x′ = Proj(z), (2)
</equation>
<bodyText confidence="0.998956375">
where Z(x) is the set of all the possible BTG trees
for x, Nodes(z) is the set of all the nodes in the
tree z, and Proj(z) is the function which gener-
ates a reordered sentence from the BTG tree z.
The method was shown to improve transla-
tion performance. However, it has a problem of
processing speed. The CYK algorithm, whose
computational complexity is O(n3) for a sen-
</bodyText>
<footnote confidence="0.955022">
1Although Terminal produces a pair of source and target
words in the original BTG (Wu, 1997), the target-side words
are ignored here because both the input and the output of pre-
ordering systems are in the source language. In (Wu, 1997),
(DeNero and Uszkoreit, 2011) and (Neubig et al., 2012), Ter-
minal can produce multiple words. Here, we produce only
one word.
2There may be more than one BTG tree which repre-
sents the same word reordering (e.g., the word reordering
C3B2A1 to A1B2C3 has two possible BTG trees), and there
are permutations which cannot be represented with BTG
(e.g., B2D4A1C3 to A1B2C3D4, which is called the 2413
pattern).
</footnote>
<page confidence="0.99803">
209
</page>
<figureCaption confidence="0.998219">
Figure 3: Top-down BTG parsing.
</figureCaption>
<listItem confidence="0.900858">
(0) ([[0, 5)], [], 0)
(1) ([[0, 2), [2, 5)], [(2, S)], v1)
(2) ([[0, 2), [3, 5)], [(2, S), (3, I)], v2)
(3) ([[0, 2)], [(2, S), (3, I), (4, I)], v3)
(4) ([], [(2, S), (3, I), (4, I), (1, S)], v4)
</listItem>
<tableCaption confidence="0.997904">
Table 1: Parser states in top-down parsing.
</tableCaption>
<bodyText confidence="0.999878777777778">
tence of length n, is used to find the best parse
tree. Furthermore, due to the use of a complex
loss function, the complexity at training time is
O(n5) (Neubig et al., 2012). Since the compu-
tational cost is prohibitive, some techniques like
cube pruning and cube growing have been applied
(Neubig et al., 2012; Na and Lee, 2013). In this
study, we propose a top-down parsing algorithm
in order to achieve fast BTG-based preordering.
</bodyText>
<sectionHeader confidence="0.949325" genericHeader="method">
3 Preordering with Incremental
Top-Down BTG Parsing
</sectionHeader>
<subsectionHeader confidence="0.999955">
3.1 Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.926561541666667">
We explain an incremental top-down BTG parsing
algorithm using Figure 3, which illustrates how a
parse tree is built for the example sentence in Fig-
ure 1. At the beginning, a tree (span) which covers
all the words in the sentence is considered. Then,
a span which covers more than one word is split
in each step, and the node type (Straight or In-
verted) for the splitting point is determined. The
algorithm terminates after (n − 1) iterations for a
sentence with n words, because there are (n − 1)
positions which can be split.
We consider that the incremental parser has a
parser state in each step, and define the state
as a triple (P, C, v). P is a stack of unre-
solved spans. A span denoted by [p, q) covers
the words xp · · · xQ−1 for an input word sequence
x = x0 · · · x|x|−1. C is a list of past parser ac-
tions. A parser action denoted by (r, o) represents
the action to split a span at the position between
xr−1 and xr with the node type o E IS, I}, where
S and I indicate Straight and Inverted respectively.
v is the score of the state, which is the sum of the
Input: Sentence x, feature weights A, beam width k.
Output: BTG parse tree.
</bodyText>
<listItem confidence="0.979639217391304">
1: S0 +— {([[0, jxj)], [], 0) } //Initial state.
2: for i := 1,··· ,jxj-1do
3: S +— {} // Set of the next states.
4: foreach s E Si_1 do
5: S +— S U T.,A(s) // Generate next states.
6: Si +— Topk(S) // Select k-best states.
7: s&amp;quot; = argmax3ES|s|_1 Score(s)
8: return Tree(§)
9: function 7,,A((P, C, v))
10: [p, q) +— P.pop()
11: S +— {}
12: for r := p + 1,··· ,qdo
13: P′ +— P
14: if r — p &gt; 1 then
15: P′.push([p, r))
16: ifq — r &gt; 1then
17: P′.push([r, q))
18: vS +— v + A · 4)(x, C, p, q, r, S)
19: vI +— v + A · 4)(x, C, p, q, r, I)
20: CS +— C; CS.append((r, S))
21: CI +— C; CI.append((r, I))
22: S +— S U {(P′, CS, vS), (P′, CI, vI)}
23: return S
</listItem>
<figureCaption confidence="0.9713295">
Figure 4: Top-down BTG parsing with beam
search.
</figureCaption>
<bodyText confidence="0.99987136">
scores for the nodes constructed so far. Parsing
starts with the initial state ([[0, IxI)], [], 0), because
there is one span covering all the words at the be-
ginning. In each step, a span is popped from the
top of the stack, and a splitting point in the span
and its node type are determined. The new spans
generated by the split are pushed onto the stack if
their lengths are greater than 1, and the action is
added to the list. On termination, the parser has
the final state ([], [c0, · · · , c|x|−2], v), because the
stack is empty and there are (jx − 1) actions in
total. The parse tree can be obtained from the list
of actions. Table 1 shows the parser state for each
step in Figure 3.
The top-down parsing method can be used with
beam search as shown in Figure 4. Tx�A(s) is a
function which returns the set of all the possi-
ble next states for the state s. Top�(S) returns
the top k states from S in terms of their scores,
Score(s) returns the score of the state s, and
Tree(s) returns the BTG parse tree constructed
from s. 4b(x, C, p, q, r, o) is the feature vector for
the node created by splitting the span [p, q) at r
with the node type o, and is explained in Sec-
tion 3.3.
</bodyText>
<subsectionHeader confidence="0.999784">
3.2 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.915169">
Model parameters A are estimated from training
examples. We assume that each training example
</bodyText>
<page confidence="0.989171">
210
</page>
<bodyText confidence="0.999911695652174">
consists of a sentence x and its word order in a
target language y = y0 · · · y|x|−1, where yi is the
position of xi in the target language. For exam-
ple, the example sentence in Figure 1(a) will have
y = 0, 1, 4, 3, 2. y can have ambiguities. Multiple
words can be reordered to the same position on
the target side. The words whose target positions
are unknown are indicated by position −1, and we
consider such words can appear at any position.3
For example, the word alignment in Figure 5 gives
the target side word positions y = −1, 2, 1, 0, 0.
Statistical syntactic parsers are usually trained
on tree-annotated corpora. However, corpora an-
notated with BTG parse trees are unavailable, and
only the gold standard permutation y is available.
Neubig et al. (2012) proposed to train BTG parsers
for preordering by regarding BTG trees behind
word reordering as latent variables, and we use
latent variable Perceptron (Sun et al., 2009) to-
gether with beam search. In latent variable Percep-
tron, among the examples whose latent variables
are compatible with a gold standard label, the one
with the highest score is picked up as a positive
example. Such an approach was used for pars-
ing with multiple correct actions (Goldberg and
Elhadad, 2010; Sartorio et al., 2013).
Figure 6 describes the training algorithm.4
4b(x, s) is the feature vector for all the nodes in
the partial parse tree at the state s, and Tx,Λ,y(s)
is the set of all the next states for the state s.
The algorithm adopts the early update technique
(Collins and Roark, 2004) which terminates incre-
mental parsing if a correct state falls off the beam,
and there is no possibility to obtain a correct out-
put. Huang et al. (2012) proposed the violation-
fixing Perceptron framework which is guaranteed
to converge even if inexact search is used, and
also showed that early update is a special case
of the framework. We define that a parser state
is valid if the state can reach a final state whose
BTG parse tree is compatible with y. Since this
is a latent variable setting in which multiple states
can reach correct final states, early update occurs
when all the valid states fall off the beam (Ma et
al., 2013; Yu et al., 2013). In order to use early up-
date, we need to check the validity of each parser
</bodyText>
<footnote confidence="0.990688">
3In (Neubig et al., 2012), the positions of such words were
fixed by heuristics. In this study, the positions are not fixed,
and all the possibilities are considered by latent variables.
4Although the simple Perceptron algorithm is used for ex-
planation, we actually used the Passive Aggressive algorithm
(Crammer et al., 2006) with the parameter averaging tech-
nique (Freund and Schapire, 1999).
</footnote>
<bodyText confidence="0.9836998125">
state. We extend the parser state to the four tu-
ple ⟨P, A, v, w⟩, where w ∈ {true, false} is the
validity of the state. We remove training exam-
ples which cannot be represented with BTG be-
forehand and set w of the initial state to true. The
function Valid(s) in Figure 6 returns the validity
of state s. One advantage of the top-down pars-
ing algorithm is that it is easy to track the validity
of each state. The validity of a state can be cal-
culated using the following property, and we can
implement the function Tx,Λ,y(s) by modifying the
function Tx,Λ(s) in Figure 4.
Lemma 1. When a valid state s, which has [p, q)
in the top of the stack, transitions to a state s′ by
the action (r, o), s′ is also valid if and only if the
following condition holds:
</bodyText>
<equation confidence="0.9607365">
∀i ∈ {p, · · · ,r − 1}yi = −1 ∨
∀i ∈ {r, · · · ,q − 1}yi = −1 ∨
</equation>
<bodyText confidence="0.999663481481482">
Proof. Let Tri denote the position of xi after re-
ordering by BTG parsing. If Condition (3) does
not hold, there are i and j which satisfy Tri &lt;
Trj ∧ yi &gt; yj ∧ yi ≠ −1 ∧ yj ≠ −1, and Tri and Trj
are not compatible with y. Therefore, s′ is valid
only if Condition (3) holds.
When Condition (3) holds, a valid permutation
can be obtained if the spans [p, r) and [r, q) are
BTG-parsable. They are BTG-parsable as shown
below. Let us assume that y does not have am-
biguities. The class of the permutations which
can be represented by BTG is known as separable
permutations in combinatorics. It can be proven
(Bose et al., 1998) that a permutation is a sepa-
rable permutation if and only if it contains nei-
ther the 2413 nor the 3142 patterns. Since s is
valid, y is a separable permutation. y does not con-
tain the 2413 nor the 3142 patterns, and any sub-
sequence of y also does not contain the patterns.
Thus, [p, r) and [r, q) are separable permutations.
The above argument holds even if y has ambigui-
ties (duplicated positions or unaligned words). In
such a case, we can always make a word order y′
which specializes y and has no ambiguities (e.g.,
y′ = 2,1.0, 0.0, 0.1,1.1 for y = −1,1,0,0, 1),
because s is valid, and there is at least one BTG
parse tree which licenses y. Any subsequence in
</bodyText>
<equation confidence="0.982205333333333">
(o = S ∧ max yi ≤ min )yi ∨
i=p,··· ,r−1 i=r,··· ,q−1
yz̸=−1 yz̸=−1
(o = I ∧ max yi ≤ min )yi . (3)
i=r,··· ,q−1 i=p,··· ,r−1
yz̸=−1 yz̸=−1
</equation>
<page confidence="0.994446">
211
</page>
<figureCaption confidence="0.8560705">
Figure 5: An example of word reordering with am-
biguities.
</figureCaption>
<bodyText confidence="0.999843272727273">
y′ is a separable permutation, and [p, r) and [r, q)
are separable permutations. Therefore, s′ is valid
if Condition (3) holds.
For dependency parsing and constituent pars-
ing, incremental bottom-up parsing methods have
been studied (Yamada and Matsumoto, 2003;
Nivre, 2004; Goldberg and Elhadad, 2010; Sagae
and Lavie, 2005). Our top-down approach is
contrastive to the bottom-up approaches. In the
bottom-up approaches, spans which cover individ-
ual words are considered at the beginning, then
they are merged into larger spans in each step, and
a span which covers all the words is obtained at
the end. In the top-down approach, a span which
covers all the words is considered at the begin-
ning, then spans are split into smaller spans in
each step, and spans which cover individual words
are obtained at the end. The top-down BTG pars-
ing method has the advantage that the validity of
parser states can be easily tracked.
The computational complexity of the top-down
parsing algorithm is O(kn2) for sentence length n
and beam width k, because in Line 5 of Figure 4,
which is repeated at most k(n − 1) times, at most
2(n − 1) parser states are generated, and their
scores are calculated. The learning algorithm uses
the same decoding algorithm as in the parsing
phase, and has the same time complexity. Note
that the validity of a parser state can be calculated
in O(1) by pre-calculating mini=p,··· ,r∧yi̸=−1 yi,
maxi=p,··· ,r∧yi̸=−1 yi, mini=r,··· ,q−1∧yi̸=−1 yi,
and maxi=r,···,q−1∧yi̸=−1 yi for all r for the span
[p, q) when it is popped from the stack.
</bodyText>
<subsectionHeader confidence="0.932907">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.977817928571429">
We assume that each word xi in a sentence has
three attributes: word surface form xW , part-of-
speech (POS) tag xP and word class xz (Sec-
tion 4.1 explains how xP and xz are obtained).
Table 2 lists the features generated for the node
which is created by splitting the span [p, q) with
the action (r, o). o’ is the node type of the par-
ent node, d E {left, right} indicates whether this
node is the left-hand-side or the right-hand-side
child of the parent node, and Balance(p, q, r) re-
Input: Training data {⟨xl, yl⟩}�_1
l=0 ,
number of iterations T, beam width k.
Output: Feature weights A.
</bodyText>
<listItem confidence="0.9338989">
1: A ← 0
2: for t := 0,··· , T − 1 do
3: for k= 0,···L − 1 do
4: S0 ← {⟨[[0,, |xl|)], [], 0, true⟩}
5: for i := 1,··· ,|xl |− 1 do
6: S ← {}
7: foreach s ∈ Si_1 do//
8: S ← S ∪ Tal,Λ,yl(8)
9: Si ← Topk(S)
10: s&amp;quot; ← argmaxsES Score(s)
</listItem>
<figure confidence="0.802717333333333">
11: s* ← argmaxsESnV alid(s) Score(s)
12: if s* ∈� Si then
13: break // Early update.
14: if s&amp;quot; # s* then
15: A ← A + 4)(xl, s*) − 4)(xl, s&amp;quot;)
16: return A
</figure>
<figureCaption confidence="0.998279">
Figure 6: A training algorithm for latent variable
Perceptron with beam search.
</figureCaption>
<bodyText confidence="0.999536571428571">
turns a value among {‘&lt;’, ‘=’, ‘&gt;’} according to
the relation of the lengths of [p, r) and [r, q). The
baseline feature templates are those used by Neu-
big et al. (2012), and the additional feature tem-
plates are extended features that we introduce in
this study. The top-down parser is fast, and allows
us to use a larger number of features.
In order to make the feature generation efficient,
the attributes of all the words are converted to their
64-bit hash values beforehand, and concatenating
the attributes is executed not as string manipula-
tion but as faster integer calculation to generate a
hash value by merging two hash values. The hash
values are used as feature names. Therefore, when
accessing feature weights stored in a hash table
using the feature names as keys, the keys can be
used as their hash values. This technique is differ-
ent from the hashing trick (Ganchev and Dredze,
2008) which directly uses hash values as indices,
and no noticeable differences in accuracy were ob-
served by using this technique.
</bodyText>
<subsectionHeader confidence="0.993395">
3.4 Training Data for Preordering
</subsectionHeader>
<bodyText confidence="0.8786883">
As described in Section 3.2, each training example
has y which represents correct word positions after
reordering. However, only word alignment data is
generally available, and we need to convert it to
y. Let Ai denote the set of indices of the target-
side words which are aligned to the source-side
word xi. We define an order relation between two
words:
xi &lt; xj � ba E Ai \ Aj,bb E Aj a &lt; b ∧
ba E Ai,bb E Aj \ Ai a &lt; b. (4)
</bodyText>
<page confidence="0.976247">
212
</page>
<table confidence="0.9999631">
Baseline Feature Template
o(q − p), oBalance(p, q, r),
oxwp−1, oxwp , oxwr−1, oxwr , oxwq−1, oxwq , oxwp xwq−1, oxwr−1xwr ,
oxpp−1, oxpp, oxpr−1,oxpr, oxpq−1, oxpq, oxppxpq−1, oxpr−1xpr,
oxcp−1, oxcp, oxcr−1, oxcr, oxcq−1,oxcq, oxcpxcq−1, oxcr−1xcr.
Additional Feature Template
o min(r − p, 5) min(q − r, 5), oo′, oo′d,
oxw p−1xw p , oxw p xw r−1, oxw p xw r , oxw r−1xw q−1, oxw r xw q−1, oxw q−1xw q ,
oxwr−2xwr−1xw r , oxw p xw r−1xw r , oxw r−1xw r xwq−1, oxw r−1xw r xw r+1,
oxw p xw r−1xw r xw q−1,
oo′dxw p , oo′dxwr−1, oo′dxwr , oo′dxwq−1, oo′dxwp xwq−1,
p p, oxp pxp r−1, oxp pxp r, oxp r−1xp q−1, oxp rxp q−1, oxp q−1xp q,
oxp−1xp
oxp r−2xp r−1xp r, oxp pxp r−1xp r, oxp r−1xp rxp q−1, oxp r−1xp rxp r+1,
oxppxpr−1xprxpq−1,
oo′dxpp, oo′dxpr−1, oo′dxpr, oo′dxpq−1, oo′dxppxpq−1,
oxc p−1xc p, oxc pxc r−1, oxc pxc r, oxc r−1xc q−1, oxc rxc q−1, oxc q−1xc q,
oxcr−2xc r−1xc r, oxc pxc r−1xc r, oxc r−1xc rxc q−1, oxc r−1xc rxc r+1,
oxc pxc r−1xc rxc q−1,
oo′dxc p, oo′dxcr−1, oo′dxcr, oo′dxcq−1, oo′dxcpxcq−1.
</table>
<tableCaption confidence="0.998737">
Table 2: Feature templates.
</tableCaption>
<bodyText confidence="0.9997712">
Then, we sort x using the order relation and as-
sign the position of xi in the sorted result to yi.
If there are two words xi and xj in x which sat-
isfy neither xi G xj nor xj G xi (that is, x does
not make a totally ordered set with the order rela-
tion), then x cannot be sorted, and the example is
removed from the training data. −1 is assigned to
the words which do not have aligned target words.
Two words xi and xj are regarded to have the same
position if xi G xj and xj G xi.
The quality of training data is important to
make accurate preordering systems, but automat-
ically word-aligned data by EM algorithms tend
to have many wrong alignments. We use forced-
decoding in order to make training data for pre-
ordering. Given a parallel sentence pair and a
phrase table, forced-decoding tries to translate the
source sentence to the target sentence, and pro-
duces phrase alignments. We train the parameters
for forced-decoding using the same parallel data
used for training the final translation system. In-
frequent phrase translations are pruned when the
phrase table is created, and forced-decoding does
not always succeed for the parallel sentences in the
training data. Forced-decoding tends to succeed
for shorter sentences, and the phrase-alignment
data obtained by forced-decoding is biased to con-
tain more shorter sentences. Therefore, we apply
the following processing for the output of forced-
decoding to make training data for preordering:
</bodyText>
<listItem confidence="0.930978785714286">
1. Remove sentences which contain less than 3
or more than 50 words.
2. Remove sentences which contain less than 3
phrase alignments.
3. Remove sentences if they contain word 5-
grams which appear in other sentences in or-
der to drop boilerplates.
4. Lastly, randomly resample sentences from
the pool of filtered sentences to make the
distribution of the sentence lengths follow a
normal distribution with the mean of 20 and
the standard deviation of 8. The parame-
ters were determined from randomly sampled
sentences from the Web.
</listItem>
<sectionHeader confidence="0.999686" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998259">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999982210526316">
We conduct experiments for 12 language pairs:
Dutch (nl)-English (en), en-nl, en-French (fr), en-
Japanese (ja), en-Spanish (es), fr-en, Hindi (hi)-en,
ja-en, Korean (ko)-en, Turkish (tr)-en, Urdu (ur)-
en and Welsh (cy)-en.
We use a phrase-based statistical machine trans-
lation system which is similar to (Och and Ney,
2004). The decoder adopts the regular distance
distortion model, and also incorporates a maxi-
mum entropy based lexicalized phrase reordering
model (Zens and Ney, 2006). The distortion limit
is set to 5 words. Word alignments are learned
using 3 iterations of IBM Model-1 (Brown et al.,
1993) and 3 iterations of the HMM alignment
model (Vogel et al., 1996). Lattice-based mini-
mum error rate training (MERT) (Macherey et al.,
2008) is applied to optimize feature weights. 5-
gram language models trained on sentences col-
lected from various sources are used.
The translation system is trained with parallel
sentences automatically collected from the Web.
The parallel data for each language pair consists
of around 400 million source and target words. In
order to make the development data for MERT and
test data (3,000 and 5,000 sentences respectively
for each language), we created parallel sentences
by randomly collecting English sentences from the
Web, and translating them by humans into each
language.
As an evaluation metric for translation quality,
BLEU (Papineni et al., 2002) is used. As intrin-
sic evaluation metrics for preordering, Fuzzy Re-
ordering Score (FRS) (Talbot et al., 2011) and
Kendall’s T (Kendall, 1938; Birch et al., 2010;
Isozaki et al., 2010) are used. Let pi denote the po-
sition in the input sentence of the (i+1)-th token in
a preordered word sequence excluding unaligned
words in the gold-standard evaluation data. For
</bodyText>
<page confidence="0.998164">
213
</page>
<table confidence="0.999022166666667">
en-ja ja-en
Training Preordering FRS z Training Preordering FRS z
(min.) (sent./sec.) (min.) (sent./sec.)
Top-Down (EM-100k) 63 87.8 77.83 87.78 81 178.4 74.60 83.78
Top-Down (Basic Feat.) (EM-100k) 9 475.1 75.25 87.26 9 939.0 73.56 83.66
Lader (EM-100k) 1562 4.3 75.41 86.85 2087 12.3 74.89 82.15
</table>
<tableCaption confidence="0.986611">
Table 3: Speed and accuracy of preordering.
</tableCaption>
<table confidence="0.998704666666667">
en-ja ja-en
FRS z BLEU FRS z BLEU
Top-Down (Manual-8k) 81.57 90.44 18.13 79.26 86.47 14.26
(EM-10k) 74.79 85.87 17.07 72.51 82.65 14.55
(EM-100k) 77.83 87.78 17.66 74.60 83.78 14.84
(Forced-10k) 76.10 87.45 16.98 75.36 83.96 14.78
(Forced-100k) 78.76 89.22 17.88 76.58 85.25 15.54
Lader (EM-100k) 75.41 86.85 17.40 74.89 82.15 14.59
No-Preordering 46.17 65.07 13.80 59.35 65.30 10.31
Manual-Rules 80.59 90.30 18.68 73.65 81.72 14.02
Auto-Rules 64.13 84.17 16.80 60.60 75.49 12.59
Classifier 80.89 90.61 18.53 74.24 82.83 13.90
</table>
<tableCaption confidence="0.991295">
Table 4: Performance of preordering for various training data. Bold BLEU scores indicate no statistically
</tableCaption>
<bodyText confidence="0.9890908">
significant difference at p &lt; 0.05 from the best system (Koehn, 2004).
example, the preordering result “New York I to
went” for the gold-standard data in Figure 5 has
ρ = 3,4,2, 1. Then FRS and τ are calculated as
follows:
</bodyText>
<equation confidence="0.9989194">
B
FRS =
|ρ |+ 1, (5)
B= |ρ|−2 � δ(yρi=yρi+1 V yρi+1=yρi+1) +
i=0
</equation>
<bodyText confidence="0.989591760869565">
where δ(X) is the Kronecker’s delta function
which returns 1 if X is true or 0 otherwise. These
scores are calculated for each sentence, and are av-
eraged over all sentences in test data. As above,
FRS can be calculated as the precision of word bi-
grams (B is the number of the word bigrams which
exist both in the system output and the gold stan-
dard data). This formulation is equivalent to the
original formulation based on chunk fragmenta-
tion by Talbot et al. (2011). Equation (6) takes
into account the positions of the beginning and the
ending words (Neubig et al., 2012). Kendall’s τ is
equivalent to the (normalized) crossing alignment
link score used by Genzel (2010).
We prepared three types of training data for
learning model parameters of BTG-based pre-
ordering:
Manual-8k Manually word-aligned 8,000 sen-
tence pairs.
EM-10k, EM-100k These are the data obtained
with the EM-based word alignment learn-
ing. From the word alignment result
for phrase translation extraction described
above, 10,000 and 100,000 sentence pairs
were randomly sampled. Before the sam-
pling, the data filtering procedure 1 and 3
in Section 3.4 were applied, and also sen-
tences were removed if more than half of
source words do not have aligned target
words. Word alignment was obtained by
symmetrizing source-to-target and target-to-
source word alignment with the INTERSEC-
TION heuristic.5
Forced-10k, Forced-100k These are 10,000 and
100,000 word-aligned sentence pairs ob-
tained with forced-decoding as described in
Section 3.4.
As test data for intrinsic evaluation of preordering,
we manually word-aligned 2,000 sentence pairs
for en-ja and ja-en.
Several preordering systems were prepared in
order to compare the following six systems:
No-Preordering This is a system without pre-
ordering.
Manual-Rules This system uses the preordering
method based on manually created rules (Xu
</bodyText>
<footnote confidence="0.71981825">
5In our preliminary experiments, the UNION and GROW-
DIAG-FINAL heuristics were also applied to generate the
training data for preordering, but INTERSECTION per-
formed the best.
</footnote>
<equation confidence="0.971864">
δ(yρ0=0) + δ(yρ|ρ|−1=max
i yi), (6)
�|ρ|−2 �|ρ|−1
j=i+1 δ(yρi � yρj)
i=0|ρ |(|ρ |− 1) ,
1
τ =
2
</equation>
<page confidence="0.997552">
214
</page>
<table confidence="0.999447357142857">
No- Manual- Auto- Classifier Lader Top-Down Top-Down
Preordering Rules Rules (EM-100k) (EM-100k) (Forced-100k)
nl-en 34.01 - 34.24 35.42 33.83 35.49 35.51
en-nl 25.33 - 25.59 25.99 25.30 25.82 25.66
en-fr 25.86 - 26.39 26.35 26.50 26.75 26.81
en-ja 13.80 18.68 16.80 18.53 17.40 17.66 17.88
en-es 29.50 - 29.63 30.09 29.70 30.26 30.24
fr-en 32.33 - 32.09 32.28 32.43 33.00 32.99
hi-en 19.86 - - - 24.24 24.98 24.97
ja-en 10.31 14.02 12.59 13.90 14.59 14.84 15.54
ko-en 14.13 - 15.86 19.46 18.65 19.67 19.88
tr-en 18.26 - - - 22.80 23.91 24.18
ur-en 14.48 - - - 16.62 17.65 18.32
cy-en 41.68 - - - 41.79 41.95 41.86
</table>
<tableCaption confidence="0.966341">
Table 5: BLEU score comparison.
</tableCaption>
<table confidence="0.978229333333333">
Distortion No- Manual- Auto- Classifier Lader Top-Down Top-Down
Limit Preordering Rules Rules (EM-100k) (EM-100k) (Forced-100k)
en-ja 5 13.80 18.68 16.80 18.53 17.40 17.66 17.88
en-ja 0 11.99 18.34 16.87 18.31 16.95 17.36 17.88
ja-en 5 10.31 14.02 12.59 13.90 14.59 14.84 15.54
ja-en 0 10.03 12.43 11.33 13.09 14.38 14.72 15.34
</table>
<tableCaption confidence="0.996146">
Table 6: BLEU scores for different distortion limits.
</tableCaption>
<bodyText confidence="0.999187633333333">
et al., 2009). We made 43 precedence rules
for en-ja, and 24 for ja-en.
Auto-Rules This system uses the rule-based pre-
ordering method which automatically learns
the rules from word-aligned data using the
Variant 1 learning algorithm described in
(Genzel, 2010). 27 to 36 rules were automat-
ically learned for each language pair.
Classifier This system uses the preordering
method based on statistical classifiers (Lerner
and Petrov, 2013), and the 2-step algorithm
was implemented.
Lader This system uses Latent Derivation Re-
orderer (Neubig et al., 2012), which is a
BTG-based preordering system using the
CYK algorithm.6 The basic feature templates
in Table 2 are used as features.
Top-Down This system uses the preordering sys-
tem described in Section 3.
Among the six systems, Manual-Rules, Auto-
Rules and Classifier need dependency parsers for
source languages. A dependency parser based
on the shift-reduce algorithm with beam search
(Zhang and Nivre, 2011) is used. The dependency
parser and all the preordering systems need POS
taggers. A supervised POS tagger based on condi-
tional random fields (Lafferty et al., 2001) trained
with manually POS annotated data is used for nl,
en, fr, ja and ko. For other languages, we use a
POS tagger based on POS projection (T¨ackstr¨om
</bodyText>
<footnote confidence="0.979839">
6lader 0.1.4. http://www.phontron.com/lader/
</footnote>
<bodyText confidence="0.999636625">
et al., 2013) which does not need POS annotated
data. Word classes in Table 2 are obtained by us-
ing Brown clusters (Koo et al., 2008) (the number
of classes is set to 256). For both Lader and Top-
Down, the beam width is set to 20, and the number
of training iterations of online learning is set to 20.
The CPU time shown in this paper is measured
using Intel Xeon 3.20GHz with 32GB RAM.
</bodyText>
<sectionHeader confidence="0.666762" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<subsectionHeader confidence="0.798572">
4.2.1 Training and Preordering Speed
</subsectionHeader>
<bodyText confidence="0.99746">
Table 3 shows the training time and preordering
speed together with the intrinsic evaluation met-
rics. In this experiment, both Top-Down and Lader
were trained using the EM-100k data. Compared
to Lader, Top-Down was faster: more than 20
times in training, and more than 10 times in pre-
ordering. Top-down had higher preordering ac-
curacy in FRS and T for en-ja. Although Lader
uses sophisticated loss functions, Top-Down uses
a larger number of features.
Top-Down (Basic feats.) is the top-down
method using only the basic feature templates in
Table 2. It was much faster but less accurate
than Top-Down using the additional features. Top-
Down (Basic feats.) and Lader use exactly the
same features. However, there are differences in
the two systems, and they had different accuracies.
Top-Down uses the beam search-based top-down
method for parsing and the Passive-Aggressive al-
gorithm for parameter estimation, and Lader uses
the CYK algorithm with cube pruning and an on-
</bodyText>
<page confidence="0.997381">
215
</page>
<bodyText confidence="0.936366">
line SVM algorithm. Especially, Lader optimizes
FRS in the default setting, and it may be the reason
that Lader had higher FRS.
</bodyText>
<subsectionHeader confidence="0.975168">
4.2.2 Performance of Preordering for
Various Training Data
</subsectionHeader>
<bodyText confidence="0.999927142857143">
Table 4 shows the preordering accuracy and BLEU
scores when Top-Down was trained with various
data. The best BLEU score for Top-Down was ob-
tained by using manually annotated data for en-
ja and 100k forced-decoding data for ja-en. The
performance was improved by increasing the data
size.
</bodyText>
<subsectionHeader confidence="0.818062">
4.2.3 End-to-End Evaluation for Various
Language Pairs
</subsectionHeader>
<bodyText confidence="0.999964848484849">
Table 5 shows the BLEU score of each system for
12 language pairs. Some blank fields mean that
the results are unavailable due to the lack of rules
or dependency parsers. For all the language pairs,
Top-Down had higher BLEU scores than Lader.
For ja-en and ur-en, using Forced-100k instead
of EM-100k for Top-Down improved the BLEU
scores by more than 0.6, but it did not always im-
proved.
Manual-Rules performed the best for en-ja, but
it needs manually created rules and is difficult
to be applied to many language pairs. Auto-
Rules and Classifier had higher scores than No-
Preordering except for fr-en, but cannot be applied
to the languages with no available dependency
parsers. Top-Down (Forced-100k) can be applied
to any language, and had statistically significantly
better BLEU scores than No-Preordering, Manual-
Rules, Auto-Rules, Classifier and Lader for 7 lan-
guage pairs (en-fr, fr-en, hi-en, ja-en, ko-en, tr-en
and ur-en), and similar performance for other lan-
guage pairs except for en-ja, without dependency
parsers trained with manually annotated data.
In all the experiments so far, the decoder was
allowed to reorder even after preordering was car-
ried out. In order to see the performance without
reordering after preordering, we conducted exper-
iments by setting the distortion limit to 0. Table 6
shows the results. The effect of the distortion lim-
its varies for language pairs and preordering meth-
ods. The BLEU scores of Top-Down were not af-
fected largely even when relying only on preorder-
ing.
</bodyText>
<sectionHeader confidence="0.993937" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999374375">
In this paper, we proposed a top-down BTG pars-
ing method for preordering. The method in-
crementally builds parse trees by splitting larger
spans into smaller ones. The method provides an
easy way to check the validity of each parser state,
which allows us to use early update for latent vari-
able Perceptron with beam search. In the exper-
iments, it was shown that the top-down parsing
method is more than 10 times faster than a CYK-
based method. The top-down method had better
BLEU scores for 7 language pairs without relying
on supervised syntactic parsers compared to other
preordering methods. Future work includes devel-
oping a bottom-up BTG parser with latent vari-
ables, and comparing the results to the top-down
parser.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998977916666667">
Alexandra Antonova and Alexey Misyurev. 2011.
Building a Web-Based Parallel Corpus and Filtering
Out Machine-Translated Text. In Proceedings of the
4th Workshop on Building and Using Comparable
Corpora: Comparable Corpora and the Web, pages
136–144.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT Evaluation: Evaluating Re-
ordering. Machine Translation, 24(1):15–26.
Prosenjit Bose, Jonathan F. Buss, and Anna Lubiw.
1998. Pattern matching for permutations. Informa-
tion Processing Letters, 65(5):277–283.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2):263–311.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201–
228.
Michael Collins and Brian Roark. 2004. Incremental
Parsing with the Perceptron Algorithm. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics, pages 111–118.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 531–540.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line Passive-Aggressive Algorithms. Journal of Ma-
chine Learning Research, 7:551–585.
John DeNero and Jakob Uszkoreit. 2011. Inducing
Sentence Structure from Parallel Corpora for Re-
ordering. In Proceedings of the 2011 Conference on
</reference>
<page confidence="0.994296">
216
</page>
<reference confidence="0.999470495867768">
Empirical Methods in Natural Language Process-
ing, pages 193–203.
Yoav Freund and Robert E. Schapire. 1999. Large
Margin Classification Using the Perceptron Algo-
rithm. Machine Learning, 37(3):277–296.
Kuzman Ganchev and Mark Dredze. 2008. Small Sta-
tistical Models by Random Feature Mixing. In Pro-
ceedings of the ACL-08: HLT Workshop on Mobile
Language Processing, pages 19–20.
Dmitriy Genzel. 2010. Automatically Learning
Source-side Reordering Rules for Large Scale Ma-
chine Translation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 376–384.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-first Non-directional De-
pendency Parsing. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 742–750.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured Perceptron with Inexact Search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142–151.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
Evaluation of Translation Quality for Distant Lan-
guage Pairs. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 944–952.
Laura Jehl, Adri`a de Gispert, Mark Hopkins, and
Bill Byrne. 2014. Source-side Preordering for
Translation using Logistic Regression and Depth-
first Branch-and-Bound Search. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, pages
239–248.
Maurice G. Kendall. 1938. A New Measure of Rank
Correlation. Biometrika, 30(1/2):81–93.
Maxim Khalilov and Khalil Sima’an. 2010. Source
reordering using MaxEnt classifiers and supertags.
In Proceedings of the 14th Annual Conference of the
European Association for Machine Translation.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
48–54.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388–395.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Pars-
ing. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 595–603.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282–
289.
Uri Lerner and Slav Petrov. 2013. Source-Side Clas-
sifier Preordering for Machine Translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 513–
523.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A Probabilistic
Approach to Syntax-based Reordering for Statisti-
cal Machine Translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 720–727.
Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013.
Easy-First POS Tagging and Dependency Parsing
with Beam Search. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 110–
114.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based Minimum
Error Rate Training for Statistical Machine Trans-
lation. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 725–734.
Valerio Antonio Miceli Barone and Giuseppe Attardi.
2013. Pre-Reordering for Machine Translation Us-
ing Transition-Based Walks on Dependency Parse
Trees. In Proceedings of the 8th Workshop on Sta-
tistical Machine Translation, pages 164–169.
Hwidong Na and Jong-Hyeok Lee. 2013. A Dis-
criminative Reordering Parser for IWSLT 2013. In
Proceedings of the 10th International Workshop for
Spoken Language Translation, pages 83–86.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a Discriminative Parser to Optimize
Machine Translation Reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 843–853.
Joakim Nivre. 2004. Incrementality in Deterministic
Dependency Parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50–57.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine
Translation. Computational Linguistics, 30(4):417–
449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311–318.
Kenji Sagae and Alon Lavie. 2005. A Classifier-Based
Parser with Linear Run-Time Complexity. In Pro-
ceedings of the 9th International Workshop on Pars-
ing Technology, pages 125–132.
</reference>
<page confidence="0.975716">
217
</page>
<reference confidence="0.999883032608695">
Francesco Sartorio, Giorgio Satta, and Joakim Nivre.
2013. A Transition-Based Dependency Parser Us-
ing a Dynamic Parsing Strategy. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, pages 135–144.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and
Jun’ichi Tsujii. 2009. Latent Variable Perceptron
Algorithm for Structured Classification. In Proceed-
ings of the 21st International Joint Conference on
Artificial Intelligence, pages 1236–1242.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
Type Constraints for Cross-Lingual Part-of-Speech
Tagging. Transactions of the Association of Compu-
tational Linguistics, 1:1–12.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-
son Katz-Brown, Masakazu Seno, and Franz J. Och.
2011. A Lightweight Evaluation Framework for
Machine Translation Reordering. In Proceedings
of the 6th Workshop on Statistical Machine Trans-
lation, pages 12–21.
Christoph Tillman. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of the 2004 Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics (Short
Papers), pages 101–104.
Roy Tromble and Jason Eisner. 2009. Learning Linear
Ordering Problems for Better Translation. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1007–
1016.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large Scale Parallel Docu-
ment Mining for Machine Translation. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 1101–1109.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax Based Reordering with Au-
tomatically Derived Rules for Improved Statistical
Machine Translation. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1119–1127.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A Word Reordering Model for Im-
proved Machine Translation. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 486–496.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based Word Alignment in Statistical
Translation. In Proceedings of the 16th Conference
on Computational Linguistics, pages 836–841.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377–403.
Fei Xia and Michael McCord. 2004. Improving a
Statistical MT System with Automatically Learned
Rewrite Patterns. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 508–514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a Dependency Parser to Im-
prove SMT for Subject-Object-Verb Languages. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 245–253.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Sta-
tistical Dependency Analysis with Support Vector
Machines. In Proceedings of the 8th International
Workshop on Parsing Technologies, pages 195–206.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A Ranking-based Approach to Word Reorder-
ing for Statistical Machine Translation. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics, pages 912–920.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-Violation Perceptron and Forced Decod-
ing for Scalable MT Training. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1112–1123.
Richard Zens and Hermann Ney. 2006. Discriminative
Reordering Models for Statistical Machine Transla-
tion. In Proceedings on the Workshop on Statistical
Machine Translation, pages 55–63.
Yue Zhang and Joakim Nivre. 2011. Transition-based
Dependency Parsing with Rich Non-local Features.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Short Pa-
pers, pages 188–193.
</reference>
<page confidence="0.997168">
218
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.850652">
<title confidence="0.999683">Efficient Top-Down BTG Parsing for Machine Translation Preordering</title>
<author confidence="0.891393">Tetsuji</author>
<affiliation confidence="0.994588">Google Japan Inc.</affiliation>
<email confidence="0.999515">tnaka@google.com</email>
<abstract confidence="0.998104826086956">We present an efficient incremental topdown parsing method for preordering based on Bracketing Transduction Grammar (BTG). The BTG-based preordering framework (Neubig et al., 2012) can be applied to any language using only parallel text, but has the problem of computational efficiency. Our top-down parsing algorithm allows us to use the early update technique easily for the latent variable structured Perceptron algorithm with beam search, and solves the problem. Experimental results showed that the topdown method is more than 10 times faster than a method using the CYK algorithm. A phrase-based machine translation system with the top-down method had statistically significantly higher BLEU scores for 7 language pairs without relying on supervised syntactic parsers, compared to baseline systems using existing preordering methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Antonova</author>
<author>Alexey Misyurev</author>
</authors>
<title>Building a Web-Based Parallel Corpus and Filtering Out Machine-Translated Text.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,</booktitle>
<pages>136--144</pages>
<contexts>
<context position="4460" citStr="Antonova and Misyurev, 2011" startWordPosition="688" endWordPosition="691">ically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., 2012; Miceli Barone and Attardi, 2013; Lerner and Petrov, 2013; Jehl et al., 2014). These methods rely on source-side parse trees and cannot be applied to languages where no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble and Eisner (2009) studied preordering based on a Linear Ordering Problem by defining a pairwise preference matrix. Khalilov and Sima’an (2010) proposed a method which swaps adjacent two words using a maximum entropy model. Visweswariah et al. (2011) regarded the preordering problem as a Traveling Salesman Problem (TSP) and applied TSP solvers for obtaining reordered words. These methods do not consider sentence structures. DeNero and Uszkoreit (2011) presented a preordering method which builds a</context>
</contexts>
<marker>Antonova, Misyurev, 2011</marker>
<rawString>Alexandra Antonova and Alexey Misyurev. 2011. Building a Web-Based Parallel Corpus and Filtering Out Machine-Translated Text. In Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web, pages 136–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Phil Blunsom</author>
</authors>
<title>Metrics for MT Evaluation: Evaluating Reordering.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="25227" citStr="Birch et al., 2010" startWordPosition="4478" endWordPosition="4481">ly collected from the Web. The parallel data for each language pair consists of around 400 million source and target words. In order to make the development data for MERT and test data (3,000 and 5,000 sentences respectively for each language), we created parallel sentences by randomly collecting English sentences from the Web, and translating them by humans into each language. As an evaluation metric for translation quality, BLEU (Papineni et al., 2002) is used. As intrinsic evaluation metrics for preordering, Fuzzy Reordering Score (FRS) (Talbot et al., 2011) and Kendall’s T (Kendall, 1938; Birch et al., 2010; Isozaki et al., 2010) are used. Let pi denote the position in the input sentence of the (i+1)-th token in a preordered word sequence excluding unaligned words in the gold-standard evaluation data. For 213 en-ja ja-en Training Preordering FRS z Training Preordering FRS z (min.) (sent./sec.) (min.) (sent./sec.) Top-Down (EM-100k) 63 87.8 77.83 87.78 81 178.4 74.60 83.78 Top-Down (Basic Feat.) (EM-100k) 9 475.1 75.25 87.26 9 939.0 73.56 83.66 Lader (EM-100k) 1562 4.3 75.41 86.85 2087 12.3 74.89 82.15 Table 3: Speed and accuracy of preordering. en-ja ja-en FRS z BLEU FRS z BLEU Top-Down (Manual-</context>
</contexts>
<marker>Birch, Osborne, Blunsom, 2010</marker>
<rawString>Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for MT Evaluation: Evaluating Reordering. Machine Translation, 24(1):15–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prosenjit Bose</author>
<author>Jonathan F Buss</author>
<author>Anna Lubiw</author>
</authors>
<title>Pattern matching for permutations.</title>
<date>1998</date>
<journal>Information Processing Letters,</journal>
<volume>65</volume>
<issue>5</issue>
<contexts>
<context position="15619" citStr="Bose et al., 1998" startWordPosition="2762" endWordPosition="2765"> Let Tri denote the position of xi after reordering by BTG parsing. If Condition (3) does not hold, there are i and j which satisfy Tri &lt; Trj ∧ yi &gt; yj ∧ yi ≠ −1 ∧ yj ≠ −1, and Tri and Trj are not compatible with y. Therefore, s′ is valid only if Condition (3) holds. When Condition (3) holds, a valid permutation can be obtained if the spans [p, r) and [r, q) are BTG-parsable. They are BTG-parsable as shown below. Let us assume that y does not have ambiguities. The class of the permutations which can be represented by BTG is known as separable permutations in combinatorics. It can be proven (Bose et al., 1998) that a permutation is a separable permutation if and only if it contains neither the 2413 nor the 3142 patterns. Since s is valid, y is a separable permutation. y does not contain the 2413 nor the 3142 patterns, and any subsequence of y also does not contain the patterns. Thus, [p, r) and [r, q) are separable permutations. The above argument holds even if y has ambiguities (duplicated positions or unaligned words). In such a case, we can always make a word order y′ which specializes y and has no ambiguities (e.g., y′ = 2,1.0, 0.0, 0.1,1.1 for y = −1,1,0,0, 1), because s is valid, and there is</context>
</contexts>
<marker>Bose, Buss, Lubiw, 1998</marker>
<rawString>Prosenjit Bose, Jonathan F. Buss, and Anna Lubiw. 1998. Pattern matching for permutations. Information Processing Letters, 65(5):277–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="24274" citStr="Brown et al., 1993" startWordPosition="4327" endWordPosition="4330"> Settings We conduct experiments for 12 language pairs: Dutch (nl)-English (en), en-nl, en-French (fr), enJapanese (ja), en-Spanish (es), fr-en, Hindi (hi)-en, ja-en, Korean (ko)-en, Turkish (tr)-en, Urdu (ur)- en and Welsh (cy)-en. We use a phrase-based statistical machine translation system which is similar to (Och and Ney, 2004). The decoder adopts the regular distance distortion model, and also incorporates a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). The distortion limit is set to 5 words. Word alignments are learned using 3 iterations of IBM Model-1 (Brown et al., 1993) and 3 iterations of the HMM alignment model (Vogel et al., 1996). Lattice-based minimum error rate training (MERT) (Macherey et al., 2008) is applied to optimize feature weights. 5- gram language models trained on sentences collected from various sources are used. The translation system is trained with parallel sentences automatically collected from the Web. The parallel data for each language pair consists of around 400 million source and target words. In order to make the development data for MERT and test data (3,000 and 5,000 sentences respectively for each language), we created parallel </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical Phrase-Based Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>228</pages>
<contexts>
<context position="1441" citStr="Chiang, 2007" startWordPosition="220" endWordPosition="221">ores for 7 language pairs without relying on supervised syntactic parsers, compared to baseline systems using existing preordering methods. 1 Introduction The difference of the word order between source and target languages is one of major problems in phrase-based statistical machine translation. In order to cope with the issue, many approaches have been studied. Distortion models consider word reordering in decoding time using such as distance (Koehn et al., 2003) and lexical information (Tillman, 2004). Another direction is to use more complex translation models such as hierarchical models (Chiang, 2007). However, these approaches suffer from the long-distance reordering issue and computational complexity. Preordering (reordering-as-preprocessing) (Xia and McCord, 2004; Collins et al., 2005) is another approach for tackling the problem, which modifies the word order of an input sentence in a source language to have the word order in a target language (Figure 1(a)). Various methods for preordering have been studied, and a method based on Bracketing Transduction Grammar (BTG) was proposed by Neubig et al. (2012). It reorders source sentences by handling sentence structures as latent variables. </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2):201– 228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental Parsing with the Perceptron Algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="2412" citStr="Collins and Roark, 2004" startWordPosition="367" endWordPosition="370">nguage (Figure 1(a)). Various methods for preordering have been studied, and a method based on Bracketing Transduction Grammar (BTG) was proposed by Neubig et al. (2012). It reorders source sentences by handling sentence structures as latent variables. The method can be applied to any language using only parallel text. However, the method has the problem of computational efficiency. In this paper, we propose an efficient incremental top-down BTG parsing method which can be applied to preordering. Model parameters can be learned using latent variable Perceptron with the early update technique (Collins and Roark, 2004), since the parsing method provides an easy way for checking the reachability of each parser state to valid final states. We also try to use forced-decoding instead of word alignment based on Expectation Maximization (EM) algorithms in order to create better training data for preordering. In experiments, preordering using the topdown parsing algorithm was faster and gave higher BLEU scores than BTG-based preordering using the CYK algorithm. Compared to existing preordering methods, our method had better or comparable BLEU scores without using supervised parsers. 2 Previous Work 2.1 Preordering</context>
<context position="13054" citStr="Collins and Roark, 2004" startWordPosition="2273" endWordPosition="2276"> (Sun et al., 2009) together with beam search. In latent variable Perceptron, among the examples whose latent variables are compatible with a gold standard label, the one with the highest score is picked up as a positive example. Such an approach was used for parsing with multiple correct actions (Goldberg and Elhadad, 2010; Sartorio et al., 2013). Figure 6 describes the training algorithm.4 4b(x, s) is the feature vector for all the nodes in the partial parse tree at the state s, and Tx,Λ,y(s) is the set of all the next states for the state s. The algorithm adopts the early update technique (Collins and Roark, 2004) which terminates incremental parsing if a correct state falls off the beam, and there is no possibility to obtain a correct output. Huang et al. (2012) proposed the violationfixing Perceptron framework which is guaranteed to converge even if inexact search is used, and also showed that early update is a special case of the framework. We define that a parser state is valid if the state can reach a final state whose BTG parse tree is compatible with y. Since this is a latent variable setting in which multiple states can reach correct final states, early update occurs when all the valid states f</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental Parsing with the Perceptron Algorithm. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause Restructuring for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<contexts>
<context position="1632" citStr="Collins et al., 2005" startWordPosition="240" endWordPosition="243">rder between source and target languages is one of major problems in phrase-based statistical machine translation. In order to cope with the issue, many approaches have been studied. Distortion models consider word reordering in decoding time using such as distance (Koehn et al., 2003) and lexical information (Tillman, 2004). Another direction is to use more complex translation models such as hierarchical models (Chiang, 2007). However, these approaches suffer from the long-distance reordering issue and computational complexity. Preordering (reordering-as-preprocessing) (Xia and McCord, 2004; Collins et al., 2005) is another approach for tackling the problem, which modifies the word order of an input sentence in a source language to have the word order in a target language (Figure 1(a)). Various methods for preordering have been studied, and a method based on Bracketing Transduction Grammar (BTG) was proposed by Neubig et al. (2012). It reorders source sentences by handling sentence structures as latent variables. The method can be applied to any language using only parallel text. However, the method has the problem of computational efficiency. In this paper, we propose an efficient incremental top-dow</context>
<context position="3397" citStr="Collins et al., 2005" startWordPosition="522" endWordPosition="525">and gave higher BLEU scores than BTG-based preordering using the CYK algorithm. Compared to existing preordering methods, our method had better or comparable BLEU scores without using supervised parsers. 2 Previous Work 2.1 Preordering for Machine Translation Many preordering methods which use syntactic parse trees have been proposed, because syntactic information is useful for determining the word order in a target language, and it can be used to restrict the search space against all the possible permutations. Preordering methods using manually created rules on parse trees have been studied (Collins et al., 2005; Xu et al., 2009), but 208 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 208–218, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: An example of preordering. linguistic knowledge for a language pair is necessary to create such rules. Preordering methods which automatically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="14108" citStr="Crammer et al., 2006" startWordPosition="2459" endWordPosition="2462"> is compatible with y. Since this is a latent variable setting in which multiple states can reach correct final states, early update occurs when all the valid states fall off the beam (Ma et al., 2013; Yu et al., 2013). In order to use early update, we need to check the validity of each parser 3In (Neubig et al., 2012), the positions of such words were fixed by heuristics. In this study, the positions are not fixed, and all the possibilities are considered by latent variables. 4Although the simple Perceptron algorithm is used for explanation, we actually used the Passive Aggressive algorithm (Crammer et al., 2006) with the parameter averaging technique (Freund and Schapire, 1999). state. We extend the parser state to the four tuple ⟨P, A, v, w⟩, where w ∈ {true, false} is the validity of the state. We remove training examples which cannot be represented with BTG beforehand and set w of the initial state to true. The function Valid(s) in Figure 6 returns the validity of state s. One advantage of the top-down parsing algorithm is that it is easy to track the validity of each state. The validity of a state can be calculated using the following property, and we can implement the function Tx,Λ,y(s) by modif</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online Passive-Aggressive Algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing Sentence Structure from Parallel Corpora for Reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>193--203</pages>
<contexts>
<context position="5014" citStr="DeNero and Uszkoreit (2011)" startWordPosition="772" endWordPosition="775">l text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble and Eisner (2009) studied preordering based on a Linear Ordering Problem by defining a pairwise preference matrix. Khalilov and Sima’an (2010) proposed a method which swaps adjacent two words using a maximum entropy model. Visweswariah et al. (2011) regarded the preordering problem as a Traveling Salesman Problem (TSP) and applied TSP solvers for obtaining reordered words. These methods do not consider sentence structures. DeNero and Uszkoreit (2011) presented a preordering method which builds a monolingual parsing model and a tree reordering model from parallel text. Neubig et al. (2012) proposed to train a discriminative BTG parser for preordering directly from word-aligned parallel text by handling underlying parse trees with latent variables. This method is explained in detail in the next subsection. These two methods can use sentence structures for designing feature functions to score permutations. Figure 2: Bracketing transduction grammar. 2.2 BTG-based Preordering Neubig et al. (2012) proposed a BTG-based preordering method. Bracke</context>
<context position="7200" citStr="DeNero and Uszkoreit, 2011" startWordPosition="1150" endWordPosition="1153">(x) is the set of all the possible BTG trees for x, Nodes(z) is the set of all the nodes in the tree z, and Proj(z) is the function which generates a reordered sentence from the BTG tree z. The method was shown to improve translation performance. However, it has a problem of processing speed. The CYK algorithm, whose computational complexity is O(n3) for a sen1Although Terminal produces a pair of source and target words in the original BTG (Wu, 1997), the target-side words are ignored here because both the input and the output of preordering systems are in the source language. In (Wu, 1997), (DeNero and Uszkoreit, 2011) and (Neubig et al., 2012), Terminal can produce multiple words. Here, we produce only one word. 2There may be more than one BTG tree which represents the same word reordering (e.g., the word reordering C3B2A1 to A1B2C3 has two possible BTG trees), and there are permutations which cannot be represented with BTG (e.g., B2D4A1C3 to A1B2C3D4, which is called the 2413 pattern). 209 Figure 3: Top-down BTG parsing. (0) ([[0, 5)], [], 0) (1) ([[0, 2), [2, 5)], [(2, S)], v1) (2) ([[0, 2), [3, 5)], [(2, S), (3, I)], v2) (3) ([[0, 2)], [(2, S), (3, I), (4, I)], v3) (4) ([], [(2, S), (3, I), (4, I), (1, </context>
</contexts>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing Sentence Structure from Parallel Corpora for Reordering. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 193–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large Margin Classification Using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="14175" citStr="Freund and Schapire, 1999" startWordPosition="2469" endWordPosition="2472"> in which multiple states can reach correct final states, early update occurs when all the valid states fall off the beam (Ma et al., 2013; Yu et al., 2013). In order to use early update, we need to check the validity of each parser 3In (Neubig et al., 2012), the positions of such words were fixed by heuristics. In this study, the positions are not fixed, and all the possibilities are considered by latent variables. 4Although the simple Perceptron algorithm is used for explanation, we actually used the Passive Aggressive algorithm (Crammer et al., 2006) with the parameter averaging technique (Freund and Schapire, 1999). state. We extend the parser state to the four tuple ⟨P, A, v, w⟩, where w ∈ {true, false} is the validity of the state. We remove training examples which cannot be represented with BTG beforehand and set w of the initial state to true. The function Valid(s) in Figure 6 returns the validity of state s. One advantage of the top-down parsing algorithm is that it is easy to track the validity of each state. The validity of a state can be calculated using the following property, and we can implement the function Tx,Λ,y(s) by modifying the function Tx,Λ(s) in Figure 4. Lemma 1. When a valid state </context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Mark Dredze</author>
</authors>
<title>Small Statistical Models by Random Feature Mixing.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing,</booktitle>
<pages>pages</pages>
<contexts>
<context position="20013" citStr="Ganchev and Dredze, 2008" startWordPosition="3577" endWordPosition="3580">n parser is fast, and allows us to use a larger number of features. In order to make the feature generation efficient, the attributes of all the words are converted to their 64-bit hash values beforehand, and concatenating the attributes is executed not as string manipulation but as faster integer calculation to generate a hash value by merging two hash values. The hash values are used as feature names. Therefore, when accessing feature weights stored in a hash table using the feature names as keys, the keys can be used as their hash values. This technique is different from the hashing trick (Ganchev and Dredze, 2008) which directly uses hash values as indices, and no noticeable differences in accuracy were observed by using this technique. 3.4 Training Data for Preordering As described in Section 3.2, each training example has y which represents correct word positions after reordering. However, only word alignment data is generally available, and we need to convert it to y. Let Ai denote the set of indices of the targetside words which are aligned to the source-side word xi. We define an order relation between two words: xi &lt; xj � ba E Ai \ Aj,bb E Aj a &lt; b ∧ ba E Ai,bb E Aj \ Ai a &lt; b. (4) 212 Baseline F</context>
</contexts>
<marker>Ganchev, Dredze, 2008</marker>
<rawString>Kuzman Ganchev and Mark Dredze. 2008. Small Statistical Models by Random Feature Mixing. In Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 19–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically Learning Source-side Reordering Rules for Large Scale Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>376--384</pages>
<contexts>
<context position="3973" citStr="Genzel, 2010" startWordPosition="609" endWordPosition="611">e been studied (Collins et al., 2005; Xu et al., 2009), but 208 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 208–218, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: An example of preordering. linguistic knowledge for a language pair is necessary to create such rules. Preordering methods which automatically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., 2012; Miceli Barone and Attardi, 2013; Lerner and Petrov, 2013; Jehl et al., 2014). These methods rely on source-side parse trees and cannot be applied to languages where no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble and Eisner (2</context>
<context position="27370" citStr="Genzel (2010)" startWordPosition="4838" endWordPosition="4839"> if X is true or 0 otherwise. These scores are calculated for each sentence, and are averaged over all sentences in test data. As above, FRS can be calculated as the precision of word bigrams (B is the number of the word bigrams which exist both in the system output and the gold standard data). This formulation is equivalent to the original formulation based on chunk fragmentation by Talbot et al. (2011). Equation (6) takes into account the positions of the beginning and the ending words (Neubig et al., 2012). Kendall’s τ is equivalent to the (normalized) crossing alignment link score used by Genzel (2010). We prepared three types of training data for learning model parameters of BTG-based preordering: Manual-8k Manually word-aligned 8,000 sentence pairs. EM-10k, EM-100k These are the data obtained with the EM-based word alignment learning. From the word alignment result for phrase translation extraction described above, 10,000 and 100,000 sentence pairs were randomly sampled. Before the sampling, the data filtering procedure 1 and 3 in Section 3.4 were applied, and also sentences were removed if more than half of source words do not have aligned target words. Word alignment was obtained by sym</context>
<context position="30122" citStr="Genzel, 2010" startWordPosition="5278" endWordPosition="5279">Classifier Lader Top-Down Top-Down Limit Preordering Rules Rules (EM-100k) (EM-100k) (Forced-100k) en-ja 5 13.80 18.68 16.80 18.53 17.40 17.66 17.88 en-ja 0 11.99 18.34 16.87 18.31 16.95 17.36 17.88 ja-en 5 10.31 14.02 12.59 13.90 14.59 14.84 15.54 ja-en 0 10.03 12.43 11.33 13.09 14.38 14.72 15.34 Table 6: BLEU scores for different distortion limits. et al., 2009). We made 43 precedence rules for en-ja, and 24 for ja-en. Auto-Rules This system uses the rule-based preordering method which automatically learns the rules from word-aligned data using the Variant 1 learning algorithm described in (Genzel, 2010). 27 to 36 rules were automatically learned for each language pair. Classifier This system uses the preordering method based on statistical classifiers (Lerner and Petrov, 2013), and the 2-step algorithm was implemented. Lader This system uses Latent Derivation Reorderer (Neubig et al., 2012), which is a BTG-based preordering system using the CYK algorithm.6 The basic feature templates in Table 2 are used as features. Top-Down This system uses the preordering system described in Section 3. Among the six systems, Manual-Rules, AutoRules and Classifier need dependency parsers for source language</context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically Learning Source-side Reordering Rules for Large Scale Machine Translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 376–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An Efficient Algorithm for Easy-first Non-directional Dependency Parsing. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>742--750</pages>
<contexts>
<context position="12755" citStr="Goldberg and Elhadad, 2010" startWordPosition="2219" endWordPosition="2222"> corpora. However, corpora annotated with BTG parse trees are unavailable, and only the gold standard permutation y is available. Neubig et al. (2012) proposed to train BTG parsers for preordering by regarding BTG trees behind word reordering as latent variables, and we use latent variable Perceptron (Sun et al., 2009) together with beam search. In latent variable Perceptron, among the examples whose latent variables are compatible with a gold standard label, the one with the highest score is picked up as a positive example. Such an approach was used for parsing with multiple correct actions (Goldberg and Elhadad, 2010; Sartorio et al., 2013). Figure 6 describes the training algorithm.4 4b(x, s) is the feature vector for all the nodes in the partial parse tree at the state s, and Tx,Λ,y(s) is the set of all the next states for the state s. The algorithm adopts the early update technique (Collins and Roark, 2004) which terminates incremental parsing if a correct state falls off the beam, and there is no possibility to obtain a correct output. Huang et al. (2012) proposed the violationfixing Perceptron framework which is guaranteed to converge even if inexact search is used, and also showed that early update </context>
<context position="16787" citStr="Goldberg and Elhadad, 2010" startWordPosition="2979" endWordPosition="2982">1.1 for y = −1,1,0,0, 1), because s is valid, and there is at least one BTG parse tree which licenses y. Any subsequence in (o = S ∧ max yi ≤ min )yi ∨ i=p,··· ,r−1 i=r,··· ,q−1 yz̸=−1 yz̸=−1 (o = I ∧ max yi ≤ min )yi . (3) i=r,··· ,q−1 i=p,··· ,r−1 yz̸=−1 yz̸=−1 211 Figure 5: An example of word reordering with ambiguities. y′ is a separable permutation, and [p, r) and [r, q) are separable permutations. Therefore, s′ is valid if Condition (3) holds. For dependency parsing and constituent parsing, incremental bottom-up parsing methods have been studied (Yamada and Matsumoto, 2003; Nivre, 2004; Goldberg and Elhadad, 2010; Sagae and Lavie, 2005). Our top-down approach is contrastive to the bottom-up approaches. In the bottom-up approaches, spans which cover individual words are considered at the beginning, then they are merged into larger spans in each step, and a span which covers all the words is obtained at the end. In the top-down approach, a span which covers all the words is considered at the beginning, then spans are split into smaller spans in each step, and spans which cover individual words are obtained at the end. The top-down BTG parsing method has the advantage that the validity of parser states c</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An Efficient Algorithm for Easy-first Non-directional Dependency Parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 742–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured Perceptron with Inexact Search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--151</pages>
<contexts>
<context position="13206" citStr="Huang et al. (2012)" startWordPosition="2301" endWordPosition="2304">abel, the one with the highest score is picked up as a positive example. Such an approach was used for parsing with multiple correct actions (Goldberg and Elhadad, 2010; Sartorio et al., 2013). Figure 6 describes the training algorithm.4 4b(x, s) is the feature vector for all the nodes in the partial parse tree at the state s, and Tx,Λ,y(s) is the set of all the next states for the state s. The algorithm adopts the early update technique (Collins and Roark, 2004) which terminates incremental parsing if a correct state falls off the beam, and there is no possibility to obtain a correct output. Huang et al. (2012) proposed the violationfixing Perceptron framework which is guaranteed to converge even if inexact search is used, and also showed that early update is a special case of the framework. We define that a parser state is valid if the state can reach a final state whose BTG parse tree is compatible with y. Since this is a latent variable setting in which multiple states can reach correct final states, early update occurs when all the valid states fall off the beam (Ma et al., 2013; Yu et al., 2013). In order to use early update, we need to check the validity of each parser 3In (Neubig et al., 2012</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured Perceptron with Inexact Search. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Automatic Evaluation of Translation Quality for Distant Language Pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>944--952</pages>
<contexts>
<context position="25250" citStr="Isozaki et al., 2010" startWordPosition="4482" endWordPosition="4485">e Web. The parallel data for each language pair consists of around 400 million source and target words. In order to make the development data for MERT and test data (3,000 and 5,000 sentences respectively for each language), we created parallel sentences by randomly collecting English sentences from the Web, and translating them by humans into each language. As an evaluation metric for translation quality, BLEU (Papineni et al., 2002) is used. As intrinsic evaluation metrics for preordering, Fuzzy Reordering Score (FRS) (Talbot et al., 2011) and Kendall’s T (Kendall, 1938; Birch et al., 2010; Isozaki et al., 2010) are used. Let pi denote the position in the input sentence of the (i+1)-th token in a preordered word sequence excluding unaligned words in the gold-standard evaluation data. For 213 en-ja ja-en Training Preordering FRS z Training Preordering FRS z (min.) (sent./sec.) (min.) (sent./sec.) Top-Down (EM-100k) 63 87.8 77.83 87.78 81 178.4 74.60 83.78 Top-Down (Basic Feat.) (EM-100k) 9 475.1 75.25 87.26 9 939.0 73.56 83.66 Lader (EM-100k) 1562 4.3 75.41 86.85 2087 12.3 74.89 82.15 Table 3: Speed and accuracy of preordering. en-ja ja-en FRS z BLEU FRS z BLEU Top-Down (Manual-8k) 81.57 90.44 18.13 7</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, 2010</marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic Evaluation of Translation Quality for Distant Language Pairs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Jehl</author>
<author>Adri`a de Gispert</author>
<author>Mark Hopkins</author>
<author>Bill Byrne</author>
</authors>
<title>Source-side Preordering for Translation using Logistic Regression and Depthfirst Branch-and-Bound Search.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>239--248</pages>
<marker>Jehl, de Gispert, Hopkins, Byrne, 2014</marker>
<rawString>Laura Jehl, Adri`a de Gispert, Mark Hopkins, and Bill Byrne. 2014. Source-side Preordering for Translation using Logistic Regression and Depthfirst Branch-and-Bound Search. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice G Kendall</author>
</authors>
<title>A New Measure of Rank Correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>30--1</pages>
<contexts>
<context position="25207" citStr="Kendall, 1938" startWordPosition="4476" endWordPosition="4477">ces automatically collected from the Web. The parallel data for each language pair consists of around 400 million source and target words. In order to make the development data for MERT and test data (3,000 and 5,000 sentences respectively for each language), we created parallel sentences by randomly collecting English sentences from the Web, and translating them by humans into each language. As an evaluation metric for translation quality, BLEU (Papineni et al., 2002) is used. As intrinsic evaluation metrics for preordering, Fuzzy Reordering Score (FRS) (Talbot et al., 2011) and Kendall’s T (Kendall, 1938; Birch et al., 2010; Isozaki et al., 2010) are used. Let pi denote the position in the input sentence of the (i+1)-th token in a preordered word sequence excluding unaligned words in the gold-standard evaluation data. For 213 en-ja ja-en Training Preordering FRS z Training Preordering FRS z (min.) (sent./sec.) (min.) (sent./sec.) Top-Down (EM-100k) 63 87.8 77.83 87.78 81 178.4 74.60 83.78 Top-Down (Basic Feat.) (EM-100k) 9 475.1 75.25 87.26 9 939.0 73.56 83.66 Lader (EM-100k) 1562 4.3 75.41 86.85 2087 12.3 74.89 82.15 Table 3: Speed and accuracy of preordering. en-ja ja-en FRS z BLEU FRS z BL</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Maurice G. Kendall. 1938. A New Measure of Rank Correlation. Biometrika, 30(1/2):81–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxim Khalilov</author>
<author>Khalil Sima’an</author>
</authors>
<title>Source reordering using MaxEnt classifiers and supertags.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Annual Conference of the European Association for Machine Translation.</booktitle>
<marker>Khalilov, Sima’an, 2010</marker>
<rawString>Maxim Khalilov and Khalil Sima’an. 2010. Source reordering using MaxEnt classifiers and supertags. In Proceedings of the 14th Annual Conference of the European Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1297" citStr="Koehn et al., 2003" startWordPosition="195" endWordPosition="198">an a method using the CYK algorithm. A phrase-based machine translation system with the top-down method had statistically significantly higher BLEU scores for 7 language pairs without relying on supervised syntactic parsers, compared to baseline systems using existing preordering methods. 1 Introduction The difference of the word order between source and target languages is one of major problems in phrase-based statistical machine translation. In order to cope with the issue, many approaches have been studied. Distortion models consider word reordering in decoding time using such as distance (Koehn et al., 2003) and lexical information (Tillman, 2004). Another direction is to use more complex translation models such as hierarchical models (Chiang, 2007). However, these approaches suffer from the long-distance reordering issue and computational complexity. Preordering (reordering-as-preprocessing) (Xia and McCord, 2004; Collins et al., 2005) is another approach for tackling the problem, which modifies the word order of an input sentence in a source language to have the word order in a target language (Figure 1(a)). Various methods for preordering have been studied, and a method based on Bracketing Tra</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="26478" citStr="Koehn, 2004" startWordPosition="4675" endWordPosition="4676"> (EM-10k) 74.79 85.87 17.07 72.51 82.65 14.55 (EM-100k) 77.83 87.78 17.66 74.60 83.78 14.84 (Forced-10k) 76.10 87.45 16.98 75.36 83.96 14.78 (Forced-100k) 78.76 89.22 17.88 76.58 85.25 15.54 Lader (EM-100k) 75.41 86.85 17.40 74.89 82.15 14.59 No-Preordering 46.17 65.07 13.80 59.35 65.30 10.31 Manual-Rules 80.59 90.30 18.68 73.65 81.72 14.02 Auto-Rules 64.13 84.17 16.80 60.60 75.49 12.59 Classifier 80.89 90.61 18.53 74.24 82.83 13.90 Table 4: Performance of preordering for various training data. Bold BLEU scores indicate no statistically significant difference at p &lt; 0.05 from the best system (Koehn, 2004). example, the preordering result “New York I to went” for the gold-standard data in Figure 5 has ρ = 3,4,2, 1. Then FRS and τ are calculated as follows: B FRS = |ρ |+ 1, (5) B= |ρ|−2 � δ(yρi=yρi+1 V yρi+1=yρi+1) + i=0 where δ(X) is the Kronecker’s delta function which returns 1 if X is true or 0 otherwise. These scores are calculated for each sentence, and are averaged over all sentences in test data. As above, FRS can be calculated as the precision of word bigrams (B is the number of the word bigrams which exist both in the system output and the gold standard data). This formulation is equiv</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="31318" citStr="Koo et al., 2008" startWordPosition="5468" endWordPosition="5471">s for source languages. A dependency parser based on the shift-reduce algorithm with beam search (Zhang and Nivre, 2011) is used. The dependency parser and all the preordering systems need POS taggers. A supervised POS tagger based on conditional random fields (Lafferty et al., 2001) trained with manually POS annotated data is used for nl, en, fr, ja and ko. For other languages, we use a POS tagger based on POS projection (T¨ackstr¨om 6lader 0.1.4. http://www.phontron.com/lader/ et al., 2013) which does not need POS annotated data. Word classes in Table 2 are obtained by using Brown clusters (Koo et al., 2008) (the number of classes is set to 256). For both Lader and TopDown, the beam width is set to 20, and the number of training iterations of online learning is set to 20. The CPU time shown in this paper is measured using Intel Xeon 3.20GHz with 32GB RAM. 4.2 Results 4.2.1 Training and Preordering Speed Table 3 shows the training time and preordering speed together with the intrinsic evaluation metrics. In this experiment, both Top-Down and Lader were trained using the EM-100k data. Compared to Lader, Top-Down was faster: more than 20 times in training, and more than 10 times in preordering. Top-</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="30985" citStr="Lafferty et al., 2001" startWordPosition="5411" endWordPosition="5414">es Latent Derivation Reorderer (Neubig et al., 2012), which is a BTG-based preordering system using the CYK algorithm.6 The basic feature templates in Table 2 are used as features. Top-Down This system uses the preordering system described in Section 3. Among the six systems, Manual-Rules, AutoRules and Classifier need dependency parsers for source languages. A dependency parser based on the shift-reduce algorithm with beam search (Zhang and Nivre, 2011) is used. The dependency parser and all the preordering systems need POS taggers. A supervised POS tagger based on conditional random fields (Lafferty et al., 2001) trained with manually POS annotated data is used for nl, en, fr, ja and ko. For other languages, we use a POS tagger based on POS projection (T¨ackstr¨om 6lader 0.1.4. http://www.phontron.com/lader/ et al., 2013) which does not need POS annotated data. Word classes in Table 2 are obtained by using Brown clusters (Koo et al., 2008) (the number of classes is set to 256). For both Lader and TopDown, the beam width is set to 20, and the number of training iterations of online learning is set to 20. The CPU time shown in this paper is measured using Intel Xeon 3.20GHz with 32GB RAM. 4.2 Results 4.</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the 18th International Conference on Machine Learning, pages 282– 289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Lerner</author>
<author>Slav Petrov</author>
</authors>
<title>Source-Side Classifier Preordering for Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>513--523</pages>
<contexts>
<context position="4077" citStr="Lerner and Petrov, 2013" startWordPosition="625" endWordPosition="628">l Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 208–218, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: An example of preordering. linguistic knowledge for a language pair is necessary to create such rules. Preordering methods which automatically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., 2012; Miceli Barone and Attardi, 2013; Lerner and Petrov, 2013; Jehl et al., 2014). These methods rely on source-side parse trees and cannot be applied to languages where no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble and Eisner (2009) studied preordering based on a Linear Ordering Problem by defining a pairwise preference matrix. Kh</context>
<context position="30299" citStr="Lerner and Petrov, 2013" startWordPosition="5303" endWordPosition="5306">34 16.87 18.31 16.95 17.36 17.88 ja-en 5 10.31 14.02 12.59 13.90 14.59 14.84 15.54 ja-en 0 10.03 12.43 11.33 13.09 14.38 14.72 15.34 Table 6: BLEU scores for different distortion limits. et al., 2009). We made 43 precedence rules for en-ja, and 24 for ja-en. Auto-Rules This system uses the rule-based preordering method which automatically learns the rules from word-aligned data using the Variant 1 learning algorithm described in (Genzel, 2010). 27 to 36 rules were automatically learned for each language pair. Classifier This system uses the preordering method based on statistical classifiers (Lerner and Petrov, 2013), and the 2-step algorithm was implemented. Lader This system uses Latent Derivation Reorderer (Neubig et al., 2012), which is a BTG-based preordering system using the CYK algorithm.6 The basic feature templates in Table 2 are used as features. Top-Down This system uses the preordering system described in Section 3. Among the six systems, Manual-Rules, AutoRules and Classifier need dependency parsers for source languages. A dependency parser based on the shift-reduce algorithm with beam search (Zhang and Nivre, 2011) is used. The dependency parser and all the preordering systems need POS tagge</context>
</contexts>
<marker>Lerner, Petrov, 2013</marker>
<rawString>Uri Lerner and Slav Petrov. 2013. Source-Side Classifier Preordering for Machine Translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513– 523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Minghui Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Yi Guan</author>
</authors>
<title>A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>720--727</pages>
<contexts>
<context position="3959" citStr="Li et al., 2007" startWordPosition="605" endWordPosition="608">n parse trees have been studied (Collins et al., 2005; Xu et al., 2009), but 208 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 208–218, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: An example of preordering. linguistic knowledge for a language pair is necessary to create such rules. Preordering methods which automatically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., 2012; Miceli Barone and Attardi, 2013; Lerner and Petrov, 2013; Jehl et al., 2014). These methods rely on source-side parse trees and cannot be applied to languages where no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble</context>
</contexts>
<marker>Li, Li, Zhang, Li, Zhou, Guan, 2007</marker>
<rawString>Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 720–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Ma</author>
<author>Jingbo Zhu</author>
<author>Tong Xiao</author>
<author>Nan Yang</author>
</authors>
<title>Easy-First POS Tagging and Dependency Parsing with Beam Search.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>110--114</pages>
<contexts>
<context position="13687" citStr="Ma et al., 2013" startWordPosition="2387" endWordPosition="2390">incremental parsing if a correct state falls off the beam, and there is no possibility to obtain a correct output. Huang et al. (2012) proposed the violationfixing Perceptron framework which is guaranteed to converge even if inexact search is used, and also showed that early update is a special case of the framework. We define that a parser state is valid if the state can reach a final state whose BTG parse tree is compatible with y. Since this is a latent variable setting in which multiple states can reach correct final states, early update occurs when all the valid states fall off the beam (Ma et al., 2013; Yu et al., 2013). In order to use early update, we need to check the validity of each parser 3In (Neubig et al., 2012), the positions of such words were fixed by heuristics. In this study, the positions are not fixed, and all the possibilities are considered by latent variables. 4Although the simple Perceptron algorithm is used for explanation, we actually used the Passive Aggressive algorithm (Crammer et al., 2006) with the parameter averaging technique (Freund and Schapire, 1999). state. We extend the parser state to the four tuple ⟨P, A, v, w⟩, where w ∈ {true, false} is the validity of t</context>
</contexts>
<marker>Ma, Zhu, Xiao, Yang, 2013</marker>
<rawString>Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013. Easy-First POS Tagging and Dependency Parsing with Beam Search. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 110– 114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Och</author>
<author>Ignacio Thayer</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Lattice-based Minimum Error Rate Training for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>725--734</pages>
<contexts>
<context position="24413" citStr="Macherey et al., 2008" startWordPosition="4350" endWordPosition="4353">, fr-en, Hindi (hi)-en, ja-en, Korean (ko)-en, Turkish (tr)-en, Urdu (ur)- en and Welsh (cy)-en. We use a phrase-based statistical machine translation system which is similar to (Och and Ney, 2004). The decoder adopts the regular distance distortion model, and also incorporates a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). The distortion limit is set to 5 words. Word alignments are learned using 3 iterations of IBM Model-1 (Brown et al., 1993) and 3 iterations of the HMM alignment model (Vogel et al., 1996). Lattice-based minimum error rate training (MERT) (Macherey et al., 2008) is applied to optimize feature weights. 5- gram language models trained on sentences collected from various sources are used. The translation system is trained with parallel sentences automatically collected from the Web. The parallel data for each language pair consists of around 400 million source and target words. In order to make the development data for MERT and test data (3,000 and 5,000 sentences respectively for each language), we created parallel sentences by randomly collecting English sentences from the Web, and translating them by humans into each language. As an evaluation metric</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based Minimum Error Rate Training for Statistical Machine Translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 725–734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valerio Antonio Miceli Barone</author>
<author>Giuseppe Attardi</author>
</authors>
<title>Pre-Reordering for Machine Translation Using Transition-Based Walks on Dependency Parse Trees.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Statistical Machine Translation,</booktitle>
<pages>164--169</pages>
<contexts>
<context position="4052" citStr="Barone and Attardi, 2013" startWordPosition="621" endWordPosition="624">ceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 208–218, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: An example of preordering. linguistic knowledge for a language pair is necessary to create such rules. Preordering methods which automatically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., 2012; Miceli Barone and Attardi, 2013; Lerner and Petrov, 2013; Jehl et al., 2014). These methods rely on source-side parse trees and cannot be applied to languages where no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble and Eisner (2009) studied preordering based on a Linear Ordering Problem by defining a pairw</context>
</contexts>
<marker>Barone, Attardi, 2013</marker>
<rawString>Valerio Antonio Miceli Barone and Giuseppe Attardi. 2013. Pre-Reordering for Machine Translation Using Transition-Based Walks on Dependency Parse Trees. In Proceedings of the 8th Workshop on Statistical Machine Translation, pages 164–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwidong Na</author>
<author>Jong-Hyeok Lee</author>
</authors>
<title>A Discriminative Reordering Parser for IWSLT</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Workshop for Spoken Language Translation,</booktitle>
<pages>83--86</pages>
<contexts>
<context position="8182" citStr="Na and Lee, 2013" startWordPosition="1330" endWordPosition="1333">3 pattern). 209 Figure 3: Top-down BTG parsing. (0) ([[0, 5)], [], 0) (1) ([[0, 2), [2, 5)], [(2, S)], v1) (2) ([[0, 2), [3, 5)], [(2, S), (3, I)], v2) (3) ([[0, 2)], [(2, S), (3, I), (4, I)], v3) (4) ([], [(2, S), (3, I), (4, I), (1, S)], v4) Table 1: Parser states in top-down parsing. tence of length n, is used to find the best parse tree. Furthermore, due to the use of a complex loss function, the complexity at training time is O(n5) (Neubig et al., 2012). Since the computational cost is prohibitive, some techniques like cube pruning and cube growing have been applied (Neubig et al., 2012; Na and Lee, 2013). In this study, we propose a top-down parsing algorithm in order to achieve fast BTG-based preordering. 3 Preordering with Incremental Top-Down BTG Parsing 3.1 Parsing Algorithm We explain an incremental top-down BTG parsing algorithm using Figure 3, which illustrates how a parse tree is built for the example sentence in Figure 1. At the beginning, a tree (span) which covers all the words in the sentence is considered. Then, a span which covers more than one word is split in each step, and the node type (Straight or Inverted) for the splitting point is determined. The algorithm terminates aft</context>
</contexts>
<marker>Na, Lee, 2013</marker>
<rawString>Hwidong Na and Jong-Hyeok Lee. 2013. A Discriminative Reordering Parser for IWSLT 2013. In Proceedings of the 10th International Workshop for Spoken Language Translation, pages 83–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Shinsuke Mori</author>
</authors>
<title>Inducing a Discriminative Parser to Optimize Machine Translation Reordering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>843--853</pages>
<contexts>
<context position="1957" citStr="Neubig et al. (2012)" startWordPosition="296" endWordPosition="299">. Another direction is to use more complex translation models such as hierarchical models (Chiang, 2007). However, these approaches suffer from the long-distance reordering issue and computational complexity. Preordering (reordering-as-preprocessing) (Xia and McCord, 2004; Collins et al., 2005) is another approach for tackling the problem, which modifies the word order of an input sentence in a source language to have the word order in a target language (Figure 1(a)). Various methods for preordering have been studied, and a method based on Bracketing Transduction Grammar (BTG) was proposed by Neubig et al. (2012). It reorders source sentences by handling sentence structures as latent variables. The method can be applied to any language using only parallel text. However, the method has the problem of computational efficiency. In this paper, we propose an efficient incremental top-down BTG parsing method which can be applied to preordering. Model parameters can be learned using latent variable Perceptron with the early update technique (Collins and Roark, 2004), since the parsing method provides an easy way for checking the reachability of each parser state to valid final states. We also try to use forc</context>
<context position="5155" citStr="Neubig et al. (2012)" startWordPosition="797" endWordPosition="800">guage resources. Tromble and Eisner (2009) studied preordering based on a Linear Ordering Problem by defining a pairwise preference matrix. Khalilov and Sima’an (2010) proposed a method which swaps adjacent two words using a maximum entropy model. Visweswariah et al. (2011) regarded the preordering problem as a Traveling Salesman Problem (TSP) and applied TSP solvers for obtaining reordered words. These methods do not consider sentence structures. DeNero and Uszkoreit (2011) presented a preordering method which builds a monolingual parsing model and a tree reordering model from parallel text. Neubig et al. (2012) proposed to train a discriminative BTG parser for preordering directly from word-aligned parallel text by handling underlying parse trees with latent variables. This method is explained in detail in the next subsection. These two methods can use sentence structures for designing feature functions to score permutations. Figure 2: Bracketing transduction grammar. 2.2 BTG-based Preordering Neubig et al. (2012) proposed a BTG-based preordering method. Bracketing Transduction Grammar (BTG) (Wu, 1997) is a binary synchronous context-free grammar with only one non-terminal symbol, and has three type</context>
<context position="7226" citStr="Neubig et al., 2012" startWordPosition="1155" endWordPosition="1158">e BTG trees for x, Nodes(z) is the set of all the nodes in the tree z, and Proj(z) is the function which generates a reordered sentence from the BTG tree z. The method was shown to improve translation performance. However, it has a problem of processing speed. The CYK algorithm, whose computational complexity is O(n3) for a sen1Although Terminal produces a pair of source and target words in the original BTG (Wu, 1997), the target-side words are ignored here because both the input and the output of preordering systems are in the source language. In (Wu, 1997), (DeNero and Uszkoreit, 2011) and (Neubig et al., 2012), Terminal can produce multiple words. Here, we produce only one word. 2There may be more than one BTG tree which represents the same word reordering (e.g., the word reordering C3B2A1 to A1B2C3 has two possible BTG trees), and there are permutations which cannot be represented with BTG (e.g., B2D4A1C3 to A1B2C3D4, which is called the 2413 pattern). 209 Figure 3: Top-down BTG parsing. (0) ([[0, 5)], [], 0) (1) ([[0, 2), [2, 5)], [(2, S)], v1) (2) ([[0, 2), [3, 5)], [(2, S), (3, I)], v2) (3) ([[0, 2)], [(2, S), (3, I), (4, I)], v3) (4) ([], [(2, S), (3, I), (4, I), (1, S)], v4) Table 1: Parser s</context>
<context position="12279" citStr="Neubig et al. (2012)" startWordPosition="2140" endWordPosition="2143">example, the example sentence in Figure 1(a) will have y = 0, 1, 4, 3, 2. y can have ambiguities. Multiple words can be reordered to the same position on the target side. The words whose target positions are unknown are indicated by position −1, and we consider such words can appear at any position.3 For example, the word alignment in Figure 5 gives the target side word positions y = −1, 2, 1, 0, 0. Statistical syntactic parsers are usually trained on tree-annotated corpora. However, corpora annotated with BTG parse trees are unavailable, and only the gold standard permutation y is available. Neubig et al. (2012) proposed to train BTG parsers for preordering by regarding BTG trees behind word reordering as latent variables, and we use latent variable Perceptron (Sun et al., 2009) together with beam search. In latent variable Perceptron, among the examples whose latent variables are compatible with a gold standard label, the one with the highest score is picked up as a positive example. Such an approach was used for parsing with multiple correct actions (Goldberg and Elhadad, 2010; Sartorio et al., 2013). Figure 6 describes the training algorithm.4 4b(x, s) is the feature vector for all the nodes in th</context>
<context position="13807" citStr="Neubig et al., 2012" startWordPosition="2412" endWordPosition="2415">Huang et al. (2012) proposed the violationfixing Perceptron framework which is guaranteed to converge even if inexact search is used, and also showed that early update is a special case of the framework. We define that a parser state is valid if the state can reach a final state whose BTG parse tree is compatible with y. Since this is a latent variable setting in which multiple states can reach correct final states, early update occurs when all the valid states fall off the beam (Ma et al., 2013; Yu et al., 2013). In order to use early update, we need to check the validity of each parser 3In (Neubig et al., 2012), the positions of such words were fixed by heuristics. In this study, the positions are not fixed, and all the possibilities are considered by latent variables. 4Although the simple Perceptron algorithm is used for explanation, we actually used the Passive Aggressive algorithm (Crammer et al., 2006) with the parameter averaging technique (Freund and Schapire, 1999). state. We extend the parser state to the four tuple ⟨P, A, v, w⟩, where w ∈ {true, false} is the validity of the state. We remove training examples which cannot be represented with BTG beforehand and set w of the initial state to </context>
<context position="19283" citStr="Neubig et al. (2012)" startWordPosition="3452" endWordPosition="3456">t := 0,··· , T − 1 do 3: for k= 0,···L − 1 do 4: S0 ← {⟨[[0,, |xl|)], [], 0, true⟩} 5: for i := 1,··· ,|xl |− 1 do 6: S ← {} 7: foreach s ∈ Si_1 do// 8: S ← S ∪ Tal,Λ,yl(8) 9: Si ← Topk(S) 10: s&amp;quot; ← argmaxsES Score(s) 11: s* ← argmaxsESnV alid(s) Score(s) 12: if s* ∈� Si then 13: break // Early update. 14: if s&amp;quot; # s* then 15: A ← A + 4)(xl, s*) − 4)(xl, s&amp;quot;) 16: return A Figure 6: A training algorithm for latent variable Perceptron with beam search. turns a value among {‘&lt;’, ‘=’, ‘&gt;’} according to the relation of the lengths of [p, r) and [r, q). The baseline feature templates are those used by Neubig et al. (2012), and the additional feature templates are extended features that we introduce in this study. The top-down parser is fast, and allows us to use a larger number of features. In order to make the feature generation efficient, the attributes of all the words are converted to their 64-bit hash values beforehand, and concatenating the attributes is executed not as string manipulation but as faster integer calculation to generate a hash value by merging two hash values. The hash values are used as feature names. Therefore, when accessing feature weights stored in a hash table using the feature names</context>
<context position="27271" citStr="Neubig et al., 2012" startWordPosition="4821" endWordPosition="4824">) B= |ρ|−2 � δ(yρi=yρi+1 V yρi+1=yρi+1) + i=0 where δ(X) is the Kronecker’s delta function which returns 1 if X is true or 0 otherwise. These scores are calculated for each sentence, and are averaged over all sentences in test data. As above, FRS can be calculated as the precision of word bigrams (B is the number of the word bigrams which exist both in the system output and the gold standard data). This formulation is equivalent to the original formulation based on chunk fragmentation by Talbot et al. (2011). Equation (6) takes into account the positions of the beginning and the ending words (Neubig et al., 2012). Kendall’s τ is equivalent to the (normalized) crossing alignment link score used by Genzel (2010). We prepared three types of training data for learning model parameters of BTG-based preordering: Manual-8k Manually word-aligned 8,000 sentence pairs. EM-10k, EM-100k These are the data obtained with the EM-based word alignment learning. From the word alignment result for phrase translation extraction described above, 10,000 and 100,000 sentence pairs were randomly sampled. Before the sampling, the data filtering procedure 1 and 3 in Section 3.4 were applied, and also sentences were removed if </context>
<context position="30415" citStr="Neubig et al., 2012" startWordPosition="5321" endWordPosition="5324"> 14.72 15.34 Table 6: BLEU scores for different distortion limits. et al., 2009). We made 43 precedence rules for en-ja, and 24 for ja-en. Auto-Rules This system uses the rule-based preordering method which automatically learns the rules from word-aligned data using the Variant 1 learning algorithm described in (Genzel, 2010). 27 to 36 rules were automatically learned for each language pair. Classifier This system uses the preordering method based on statistical classifiers (Lerner and Petrov, 2013), and the 2-step algorithm was implemented. Lader This system uses Latent Derivation Reorderer (Neubig et al., 2012), which is a BTG-based preordering system using the CYK algorithm.6 The basic feature templates in Table 2 are used as features. Top-Down This system uses the preordering system described in Section 3. Among the six systems, Manual-Rules, AutoRules and Classifier need dependency parsers for source languages. A dependency parser based on the shift-reduce algorithm with beam search (Zhang and Nivre, 2011) is used. The dependency parser and all the preordering systems need POS taggers. A supervised POS tagger based on conditional random fields (Lafferty et al., 2001) trained with manually POS ann</context>
</contexts>
<marker>Neubig, Watanabe, Mori, 2012</marker>
<rawString>Graham Neubig, Taro Watanabe, and Shinsuke Mori. 2012. Inducing a Discriminative Parser to Optimize Machine Translation Reordering. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 843–853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in Deterministic Dependency Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="16759" citStr="Nivre, 2004" startWordPosition="2977" endWordPosition="2978">.0, 0.0, 0.1,1.1 for y = −1,1,0,0, 1), because s is valid, and there is at least one BTG parse tree which licenses y. Any subsequence in (o = S ∧ max yi ≤ min )yi ∨ i=p,··· ,r−1 i=r,··· ,q−1 yz̸=−1 yz̸=−1 (o = I ∧ max yi ≤ min )yi . (3) i=r,··· ,q−1 i=p,··· ,r−1 yz̸=−1 yz̸=−1 211 Figure 5: An example of word reordering with ambiguities. y′ is a separable permutation, and [p, r) and [r, q) are separable permutations. Therefore, s′ is valid if Condition (3) holds. For dependency parsing and constituent parsing, incremental bottom-up parsing methods have been studied (Yamada and Matsumoto, 2003; Nivre, 2004; Goldberg and Elhadad, 2010; Sagae and Lavie, 2005). Our top-down approach is contrastive to the bottom-up approaches. In the bottom-up approaches, spans which cover individual words are considered at the beginning, then they are merged into larger spans in each step, and a span which covers all the words is obtained at the end. In the top-down approach, a span which covers all the words is considered at the beginning, then spans are split into smaller spans in each step, and spans which cover individual words are obtained at the end. The top-down BTG parsing method has the advantage that the</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in Deterministic Dependency Parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>449</pages>
<contexts>
<context position="23988" citStr="Och and Ney, 2004" startWordPosition="4281" endWordPosition="4284"> sentences from the pool of filtered sentences to make the distribution of the sentence lengths follow a normal distribution with the mean of 20 and the standard deviation of 8. The parameters were determined from randomly sampled sentences from the Web. 4 Experiments 4.1 Experimental Settings We conduct experiments for 12 language pairs: Dutch (nl)-English (en), en-nl, en-French (fr), enJapanese (ja), en-Spanish (es), fr-en, Hindi (hi)-en, ja-en, Korean (ko)-en, Turkish (tr)-en, Urdu (ur)- en and Welsh (cy)-en. We use a phrase-based statistical machine translation system which is similar to (Och and Ney, 2004). The decoder adopts the regular distance distortion model, and also incorporates a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). The distortion limit is set to 5 words. Word alignments are learned using 3 iterations of IBM Model-1 (Brown et al., 1993) and 3 iterations of the HMM alignment model (Vogel et al., 1996). Lattice-based minimum error rate training (MERT) (Macherey et al., 2008) is applied to optimize feature weights. 5- gram language models trained on sentences collected from various sources are used. The translation system is trained with parallel </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417– 449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="25067" citStr="Papineni et al., 2002" startWordPosition="4451" endWordPosition="4454">weights. 5- gram language models trained on sentences collected from various sources are used. The translation system is trained with parallel sentences automatically collected from the Web. The parallel data for each language pair consists of around 400 million source and target words. In order to make the development data for MERT and test data (3,000 and 5,000 sentences respectively for each language), we created parallel sentences by randomly collecting English sentences from the Web, and translating them by humans into each language. As an evaluation metric for translation quality, BLEU (Papineni et al., 2002) is used. As intrinsic evaluation metrics for preordering, Fuzzy Reordering Score (FRS) (Talbot et al., 2011) and Kendall’s T (Kendall, 1938; Birch et al., 2010; Isozaki et al., 2010) are used. Let pi denote the position in the input sentence of the (i+1)-th token in a preordered word sequence excluding unaligned words in the gold-standard evaluation data. For 213 en-ja ja-en Training Preordering FRS z Training Preordering FRS z (min.) (sent./sec.) (min.) (sent./sec.) Top-Down (EM-100k) 63 87.8 77.83 87.78 81 178.4 74.60 83.78 Top-Down (Basic Feat.) (EM-100k) 9 475.1 75.25 87.26 9 939.0 73.56 </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A Classifier-Based Parser with Linear Run-Time Complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technology,</booktitle>
<pages>125--132</pages>
<contexts>
<context position="16811" citStr="Sagae and Lavie, 2005" startWordPosition="2983" endWordPosition="2986">cause s is valid, and there is at least one BTG parse tree which licenses y. Any subsequence in (o = S ∧ max yi ≤ min )yi ∨ i=p,··· ,r−1 i=r,··· ,q−1 yz̸=−1 yz̸=−1 (o = I ∧ max yi ≤ min )yi . (3) i=r,··· ,q−1 i=p,··· ,r−1 yz̸=−1 yz̸=−1 211 Figure 5: An example of word reordering with ambiguities. y′ is a separable permutation, and [p, r) and [r, q) are separable permutations. Therefore, s′ is valid if Condition (3) holds. For dependency parsing and constituent parsing, incremental bottom-up parsing methods have been studied (Yamada and Matsumoto, 2003; Nivre, 2004; Goldberg and Elhadad, 2010; Sagae and Lavie, 2005). Our top-down approach is contrastive to the bottom-up approaches. In the bottom-up approaches, spans which cover individual words are considered at the beginning, then they are merged into larger spans in each step, and a span which covers all the words is obtained at the end. In the top-down approach, a span which covers all the words is considered at the beginning, then spans are split into smaller spans in each step, and spans which cover individual words are obtained at the end. The top-down BTG parsing method has the advantage that the validity of parser states can be easily tracked. Th</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A Classifier-Based Parser with Linear Run-Time Complexity. In Proceedings of the 9th International Workshop on Parsing Technology, pages 125–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Sartorio</author>
<author>Giorgio Satta</author>
<author>Joakim Nivre</author>
</authors>
<title>A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>135--144</pages>
<contexts>
<context position="12779" citStr="Sartorio et al., 2013" startWordPosition="2223" endWordPosition="2226">nnotated with BTG parse trees are unavailable, and only the gold standard permutation y is available. Neubig et al. (2012) proposed to train BTG parsers for preordering by regarding BTG trees behind word reordering as latent variables, and we use latent variable Perceptron (Sun et al., 2009) together with beam search. In latent variable Perceptron, among the examples whose latent variables are compatible with a gold standard label, the one with the highest score is picked up as a positive example. Such an approach was used for parsing with multiple correct actions (Goldberg and Elhadad, 2010; Sartorio et al., 2013). Figure 6 describes the training algorithm.4 4b(x, s) is the feature vector for all the nodes in the partial parse tree at the state s, and Tx,Λ,y(s) is the set of all the next states for the state s. The algorithm adopts the early update technique (Collins and Roark, 2004) which terminates incremental parsing if a correct state falls off the beam, and there is no possibility to obtain a correct output. Huang et al. (2012) proposed the violationfixing Perceptron framework which is guaranteed to converge even if inexact search is used, and also showed that early update is a special case of the</context>
</contexts>
<marker>Sartorio, Satta, Nivre, 2013</marker>
<rawString>Francesco Sartorio, Giorgio Satta, and Joakim Nivre. 2013. A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 135–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Takuya Matsuzaki</author>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Latent Variable Perceptron Algorithm for Structured Classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1236--1242</pages>
<contexts>
<context position="12449" citStr="Sun et al., 2009" startWordPosition="2167" endWordPosition="2170">e words whose target positions are unknown are indicated by position −1, and we consider such words can appear at any position.3 For example, the word alignment in Figure 5 gives the target side word positions y = −1, 2, 1, 0, 0. Statistical syntactic parsers are usually trained on tree-annotated corpora. However, corpora annotated with BTG parse trees are unavailable, and only the gold standard permutation y is available. Neubig et al. (2012) proposed to train BTG parsers for preordering by regarding BTG trees behind word reordering as latent variables, and we use latent variable Perceptron (Sun et al., 2009) together with beam search. In latent variable Perceptron, among the examples whose latent variables are compatible with a gold standard label, the one with the highest score is picked up as a positive example. Such an approach was used for parsing with multiple correct actions (Goldberg and Elhadad, 2010; Sartorio et al., 2013). Figure 6 describes the training algorithm.4 4b(x, s) is the feature vector for all the nodes in the partial parse tree at the state s, and Tx,Λ,y(s) is the set of all the next states for the state s. The algorithm adopts the early update technique (Collins and Roark, </context>
</contexts>
<marker>Sun, Matsuzaki, Okanohara, Tsujii, 2009</marker>
<rawString>Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and Jun’ichi Tsujii. 2009. Latent Variable Perceptron Algorithm for Structured Classification. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, pages 1236–1242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging.</title>
<date>2013</date>
<journal>Transactions of the Association of Computational Linguistics,</journal>
<pages>1--1</pages>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging. Transactions of the Association of Computational Linguistics, 1:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Hideto Kazawa</author>
<author>Hiroshi Ichikawa</author>
<author>Jason Katz-Brown</author>
<author>Masakazu Seno</author>
<author>Franz J Och</author>
</authors>
<title>A Lightweight Evaluation Framework for Machine Translation Reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on Statistical Machine Translation,</booktitle>
<pages>12--21</pages>
<contexts>
<context position="25176" citStr="Talbot et al., 2011" startWordPosition="4469" endWordPosition="4472">system is trained with parallel sentences automatically collected from the Web. The parallel data for each language pair consists of around 400 million source and target words. In order to make the development data for MERT and test data (3,000 and 5,000 sentences respectively for each language), we created parallel sentences by randomly collecting English sentences from the Web, and translating them by humans into each language. As an evaluation metric for translation quality, BLEU (Papineni et al., 2002) is used. As intrinsic evaluation metrics for preordering, Fuzzy Reordering Score (FRS) (Talbot et al., 2011) and Kendall’s T (Kendall, 1938; Birch et al., 2010; Isozaki et al., 2010) are used. Let pi denote the position in the input sentence of the (i+1)-th token in a preordered word sequence excluding unaligned words in the gold-standard evaluation data. For 213 en-ja ja-en Training Preordering FRS z Training Preordering FRS z (min.) (sent./sec.) (min.) (sent./sec.) Top-Down (EM-100k) 63 87.8 77.83 87.78 81 178.4 74.60 83.78 Top-Down (Basic Feat.) (EM-100k) 9 475.1 75.25 87.26 9 939.0 73.56 83.66 Lader (EM-100k) 1562 4.3 75.41 86.85 2087 12.3 74.89 82.15 Table 3: Speed and accuracy of preordering. </context>
<context position="27164" citStr="Talbot et al. (2011)" startWordPosition="4803" endWordPosition="4806">d-standard data in Figure 5 has ρ = 3,4,2, 1. Then FRS and τ are calculated as follows: B FRS = |ρ |+ 1, (5) B= |ρ|−2 � δ(yρi=yρi+1 V yρi+1=yρi+1) + i=0 where δ(X) is the Kronecker’s delta function which returns 1 if X is true or 0 otherwise. These scores are calculated for each sentence, and are averaged over all sentences in test data. As above, FRS can be calculated as the precision of word bigrams (B is the number of the word bigrams which exist both in the system output and the gold standard data). This formulation is equivalent to the original formulation based on chunk fragmentation by Talbot et al. (2011). Equation (6) takes into account the positions of the beginning and the ending words (Neubig et al., 2012). Kendall’s τ is equivalent to the (normalized) crossing alignment link score used by Genzel (2010). We prepared three types of training data for learning model parameters of BTG-based preordering: Manual-8k Manually word-aligned 8,000 sentence pairs. EM-10k, EM-100k These are the data obtained with the EM-based word alignment learning. From the word alignment result for phrase translation extraction described above, 10,000 and 100,000 sentence pairs were randomly sampled. Before the samp</context>
</contexts>
<marker>Talbot, Kazawa, Ichikawa, Katz-Brown, Seno, Och, 2011</marker>
<rawString>David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason Katz-Brown, Masakazu Seno, and Franz J. Och. 2011. A Lightweight Evaluation Framework for Machine Translation Reordering. In Proceedings of the 6th Workshop on Statistical Machine Translation, pages 12–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A Unigram Orientation Model for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (Short Papers),</booktitle>
<pages>101--104</pages>
<contexts>
<context position="1337" citStr="Tillman, 2004" startWordPosition="202" endWordPosition="204">-based machine translation system with the top-down method had statistically significantly higher BLEU scores for 7 language pairs without relying on supervised syntactic parsers, compared to baseline systems using existing preordering methods. 1 Introduction The difference of the word order between source and target languages is one of major problems in phrase-based statistical machine translation. In order to cope with the issue, many approaches have been studied. Distortion models consider word reordering in decoding time using such as distance (Koehn et al., 2003) and lexical information (Tillman, 2004). Another direction is to use more complex translation models such as hierarchical models (Chiang, 2007). However, these approaches suffer from the long-distance reordering issue and computational complexity. Preordering (reordering-as-preprocessing) (Xia and McCord, 2004; Collins et al., 2005) is another approach for tackling the problem, which modifies the word order of an input sentence in a source language to have the word order in a target language (Figure 1(a)). Various methods for preordering have been studied, and a method based on Bracketing Transduction Grammar (BTG) was proposed by </context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (Short Papers), pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning Linear Ordering Problems for Better Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1007--1016</pages>
<contexts>
<context position="4577" citStr="Tromble and Eisner (2009)" startWordPosition="705" endWordPosition="708">., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., 2012; Miceli Barone and Attardi, 2013; Lerner and Petrov, 2013; Jehl et al., 2014). These methods rely on source-side parse trees and cannot be applied to languages where no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble and Eisner (2009) studied preordering based on a Linear Ordering Problem by defining a pairwise preference matrix. Khalilov and Sima’an (2010) proposed a method which swaps adjacent two words using a maximum entropy model. Visweswariah et al. (2011) regarded the preordering problem as a Traveling Salesman Problem (TSP) and applied TSP solvers for obtaining reordered words. These methods do not consider sentence structures. DeNero and Uszkoreit (2011) presented a preordering method which builds a monolingual parsing model and a tree reordering model from parallel text. Neubig et al. (2012) proposed to train a d</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning Linear Ordering Problems for Better Translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1007– 1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay M Ponte</author>
<author>Ashok C Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large Scale Parallel Document Mining for Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1101--1109</pages>
<contexts>
<context position="4430" citStr="Uszkoreit et al., 2010" startWordPosition="684" endWordPosition="687">ng methods which automatically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., 2012; Miceli Barone and Attardi, 2013; Lerner and Petrov, 2013; Jehl et al., 2014). These methods rely on source-side parse trees and cannot be applied to languages where no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble and Eisner (2009) studied preordering based on a Linear Ordering Problem by defining a pairwise preference matrix. Khalilov and Sima’an (2010) proposed a method which swaps adjacent two words using a maximum entropy model. Visweswariah et al. (2011) regarded the preordering problem as a Traveling Salesman Problem (TSP) and applied TSP solvers for obtaining reordered words. These methods do not consider sentence structures. DeNero and Uszkoreit (2011) presented a pre</context>
</contexts>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and Moshe Dubiner. 2010. Large Scale Parallel Document Mining for Machine Translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1101–1109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Jiri Navratil</author>
<author>Jeffrey Sorensen</author>
<author>Vijil Chenthamarakshan</author>
<author>Nandakishore Kambhatla</author>
</authors>
<title>Syntax Based Reordering with Automatically Derived Rules for Improved Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1119--1127</pages>
<contexts>
<context position="4000" citStr="Visweswariah et al., 2010" startWordPosition="612" endWordPosition="615"> (Collins et al., 2005; Xu et al., 2009), but 208 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 208–218, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: An example of preordering. linguistic knowledge for a language pair is necessary to create such rules. Preordering methods which automatically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., 2012; Miceli Barone and Attardi, 2013; Lerner and Petrov, 2013; Jehl et al., 2014). These methods rely on source-side parse trees and cannot be applied to languages where no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble and Eisner (2009) studied preordering ba</context>
</contexts>
<marker>Visweswariah, Navratil, Sorensen, Chenthamarakshan, Kambhatla, 2010</marker>
<rawString>Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nandakishore Kambhatla. 2010. Syntax Based Reordering with Automatically Derived Rules for Improved Statistical Machine Translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1119–1127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Rajakrishnan Rajkumar</author>
<author>Ankur Gandhe</author>
<author>Ananthakrishnan Ramanathan</author>
<author>Jiri Navratil</author>
</authors>
<title>A Word Reordering Model for Improved Machine Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>486--496</pages>
<contexts>
<context position="4809" citStr="Visweswariah et al. (2011)" startWordPosition="743" endWordPosition="746">re no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble and Eisner (2009) studied preordering based on a Linear Ordering Problem by defining a pairwise preference matrix. Khalilov and Sima’an (2010) proposed a method which swaps adjacent two words using a maximum entropy model. Visweswariah et al. (2011) regarded the preordering problem as a Traveling Salesman Problem (TSP) and applied TSP solvers for obtaining reordered words. These methods do not consider sentence structures. DeNero and Uszkoreit (2011) presented a preordering method which builds a monolingual parsing model and a tree reordering model from parallel text. Neubig et al. (2012) proposed to train a discriminative BTG parser for preordering directly from word-aligned parallel text by handling underlying parse trees with latent variables. This method is explained in detail in the next subsection. These two methods can use sentenc</context>
</contexts>
<marker>Visweswariah, Rajkumar, Gandhe, Ramanathan, Navratil, 2011</marker>
<rawString>Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and Jiri Navratil. 2011. A Word Reordering Model for Improved Machine Translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based Word Alignment in Statistical Translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Conference on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="24339" citStr="Vogel et al., 1996" startWordPosition="4339" endWordPosition="4342">)-English (en), en-nl, en-French (fr), enJapanese (ja), en-Spanish (es), fr-en, Hindi (hi)-en, ja-en, Korean (ko)-en, Turkish (tr)-en, Urdu (ur)- en and Welsh (cy)-en. We use a phrase-based statistical machine translation system which is similar to (Och and Ney, 2004). The decoder adopts the regular distance distortion model, and also incorporates a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). The distortion limit is set to 5 words. Word alignments are learned using 3 iterations of IBM Model-1 (Brown et al., 1993) and 3 iterations of the HMM alignment model (Vogel et al., 1996). Lattice-based minimum error rate training (MERT) (Macherey et al., 2008) is applied to optimize feature weights. 5- gram language models trained on sentences collected from various sources are used. The translation system is trained with parallel sentences automatically collected from the Web. The parallel data for each language pair consists of around 400 million source and target words. In order to make the development data for MERT and test data (3,000 and 5,000 sentences respectively for each language), we created parallel sentences by randomly collecting English sentences from the Web, </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based Word Alignment in Statistical Translation. In Proceedings of the 16th Conference on Computational Linguistics, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="5656" citStr="Wu, 1997" startWordPosition="874" endWordPosition="875">hich builds a monolingual parsing model and a tree reordering model from parallel text. Neubig et al. (2012) proposed to train a discriminative BTG parser for preordering directly from word-aligned parallel text by handling underlying parse trees with latent variables. This method is explained in detail in the next subsection. These two methods can use sentence structures for designing feature functions to score permutations. Figure 2: Bracketing transduction grammar. 2.2 BTG-based Preordering Neubig et al. (2012) proposed a BTG-based preordering method. Bracketing Transduction Grammar (BTG) (Wu, 1997) is a binary synchronous context-free grammar with only one non-terminal symbol, and has three types of rules (Figure 2): Straight which keeps the order of child nodes, Inverted which reverses the order, and Terminal which generates a terminal symbol.1 BTG can express word reordering. For example, the word reordering in Figure 1(a) can be represented with the BTG parse tree in Figure 1(b).2 Therefore, the task to reorder an input source sentence can be solved as a BTG parsing task to find an appropriate BTG tree. In order to find the best BTG tree among all the possible ones, a score function </context>
<context position="7027" citStr="Wu, 1997" startWordPosition="1123" endWordPosition="1124">e x, the best BTG tree z� and the reordered sentence x′ can be obtained as follows: �z� = argmax A · 4b(m), (1) zEZ(x) mENodes(z) x′ = Proj(z), (2) where Z(x) is the set of all the possible BTG trees for x, Nodes(z) is the set of all the nodes in the tree z, and Proj(z) is the function which generates a reordered sentence from the BTG tree z. The method was shown to improve translation performance. However, it has a problem of processing speed. The CYK algorithm, whose computational complexity is O(n3) for a sen1Although Terminal produces a pair of source and target words in the original BTG (Wu, 1997), the target-side words are ignored here because both the input and the output of preordering systems are in the source language. In (Wu, 1997), (DeNero and Uszkoreit, 2011) and (Neubig et al., 2012), Terminal can produce multiple words. Here, we produce only one word. 2There may be more than one BTG tree which represents the same word reordering (e.g., the word reordering C3B2A1 to A1B2C3 has two possible BTG trees), and there are permutations which cannot be represented with BTG (e.g., B2D4A1C3 to A1B2C3D4, which is called the 2413 pattern). 209 Figure 3: Top-down BTG parsing. (0) ([[0, 5)],</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a Statistical MT System with Automatically Learned Rewrite Patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>508--514</pages>
<contexts>
<context position="1609" citStr="Xia and McCord, 2004" startWordPosition="236" endWordPosition="239">fference of the word order between source and target languages is one of major problems in phrase-based statistical machine translation. In order to cope with the issue, many approaches have been studied. Distortion models consider word reordering in decoding time using such as distance (Koehn et al., 2003) and lexical information (Tillman, 2004). Another direction is to use more complex translation models such as hierarchical models (Chiang, 2007). However, these approaches suffer from the long-distance reordering issue and computational complexity. Preordering (reordering-as-preprocessing) (Xia and McCord, 2004; Collins et al., 2005) is another approach for tackling the problem, which modifies the word order of an input sentence in a source language to have the word order in a target language (Figure 1(a)). Various methods for preordering have been studied, and a method based on Bracketing Transduction Grammar (BTG) was proposed by Neubig et al. (2012). It reorders source sentences by handling sentence structures as latent variables. The method can be applied to any language using only parallel text. However, the method has the problem of computational efficiency. In this paper, we propose an effici</context>
<context position="3942" citStr="Xia and McCord, 2004" startWordPosition="601" endWordPosition="604">nually created rules on parse trees have been studied (Collins et al., 2005; Xu et al., 2009), but 208 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 208–218, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: An example of preordering. linguistic knowledge for a language pair is necessary to create such rules. Preordering methods which automatically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., 2012; Miceli Barone and Attardi, 2013; Lerner and Petrov, 2013; Jehl et al., 2014). These methods rely on source-side parse trees and cannot be applied to languages where no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language r</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a Statistical MT System with Automatically Learned Rewrite Patterns. In Proceedings of the 20th International Conference on Computational Linguistics, pages 508–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>245--253</pages>
<contexts>
<context position="3415" citStr="Xu et al., 2009" startWordPosition="526" endWordPosition="529">cores than BTG-based preordering using the CYK algorithm. Compared to existing preordering methods, our method had better or comparable BLEU scores without using supervised parsers. 2 Previous Work 2.1 Preordering for Machine Translation Many preordering methods which use syntactic parse trees have been proposed, because syntactic information is useful for determining the word order in a target language, and it can be used to restrict the search space against all the possible permutations. Preordering methods using manually created rules on parse trees have been studied (Collins et al., 2005; Xu et al., 2009), but 208 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 208–218, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: An example of preordering. linguistic knowledge for a language pair is necessary to create such rules. Preordering methods which automatically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., </context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 245–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="16746" citStr="Yamada and Matsumoto, 2003" startWordPosition="2973" endWordPosition="2976"> ambiguities (e.g., y′ = 2,1.0, 0.0, 0.1,1.1 for y = −1,1,0,0, 1), because s is valid, and there is at least one BTG parse tree which licenses y. Any subsequence in (o = S ∧ max yi ≤ min )yi ∨ i=p,··· ,r−1 i=r,··· ,q−1 yz̸=−1 yz̸=−1 (o = I ∧ max yi ≤ min )yi . (3) i=r,··· ,q−1 i=p,··· ,r−1 yz̸=−1 yz̸=−1 211 Figure 5: An example of word reordering with ambiguities. y′ is a separable permutation, and [p, r) and [r, q) are separable permutations. Therefore, s′ is valid if Condition (3) holds. For dependency parsing and constituent parsing, incremental bottom-up parsing methods have been studied (Yamada and Matsumoto, 2003; Nivre, 2004; Goldberg and Elhadad, 2010; Sagae and Lavie, 2005). Our top-down approach is contrastive to the bottom-up approaches. In the bottom-up approaches, spans which cover individual words are considered at the beginning, then they are merged into larger spans in each step, and a span which covers all the words is obtained at the end. In the top-down approach, a span which covers all the words is considered at the beginning, then spans are split into smaller spans in each step, and spans which cover individual words are obtained at the end. The top-down BTG parsing method has the advan</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical Dependency Analysis with Support Vector Machines. In Proceedings of the 8th International Workshop on Parsing Technologies, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Dongdong Zhang</author>
<author>Nenghai Yu</author>
</authors>
<title>A Ranking-based Approach to Word Reordering for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>912--920</pages>
<contexts>
<context position="4019" citStr="Yang et al., 2012" startWordPosition="616" endWordPosition="619">et al., 2009), but 208 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 208–218, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: An example of preordering. linguistic knowledge for a language pair is necessary to create such rules. Preordering methods which automatically create reordering rules or utilize statistical classifiers have also been studied (Xia and McCord, 2004; Li et al., 2007; Genzel, 2010; Visweswariah et al., 2010; Yang et al., 2012; Miceli Barone and Attardi, 2013; Lerner and Petrov, 2013; Jehl et al., 2014). These methods rely on source-side parse trees and cannot be applied to languages where no syntactic parsers are available. There are preordering methods that do not need parse trees. They are usually trained only on automatically word-aligned parallel text. It is possible to mine parallel text from the Web (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), and the preordering systems can be trained without manually annotated language resources. Tromble and Eisner (2009) studied preordering based on a Linear Ord</context>
</contexts>
<marker>Yang, Li, Zhang, Yu, 2012</marker>
<rawString>Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu. 2012. A Ranking-based Approach to Word Reordering for Statistical Machine Translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 912–920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Kai Zhao</author>
</authors>
<title>Max-Violation Perceptron and Forced Decoding for Scalable MT Training.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1112--1123</pages>
<contexts>
<context position="13705" citStr="Yu et al., 2013" startWordPosition="2391" endWordPosition="2394">ng if a correct state falls off the beam, and there is no possibility to obtain a correct output. Huang et al. (2012) proposed the violationfixing Perceptron framework which is guaranteed to converge even if inexact search is used, and also showed that early update is a special case of the framework. We define that a parser state is valid if the state can reach a final state whose BTG parse tree is compatible with y. Since this is a latent variable setting in which multiple states can reach correct final states, early update occurs when all the valid states fall off the beam (Ma et al., 2013; Yu et al., 2013). In order to use early update, we need to check the validity of each parser 3In (Neubig et al., 2012), the positions of such words were fixed by heuristics. In this study, the positions are not fixed, and all the possibilities are considered by latent variables. 4Although the simple Perceptron algorithm is used for explanation, we actually used the Passive Aggressive algorithm (Crammer et al., 2006) with the parameter averaging technique (Freund and Schapire, 1999). state. We extend the parser state to the four tuple ⟨P, A, v, w⟩, where w ∈ {true, false} is the validity of the state. We remov</context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao. 2013. Max-Violation Perceptron and Forced Decoding for Scalable MT Training. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1112–1123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Reordering Models for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>55--63</pages>
<contexts>
<context position="24150" citStr="Zens and Ney, 2006" startWordPosition="4305" endWordPosition="4308"> deviation of 8. The parameters were determined from randomly sampled sentences from the Web. 4 Experiments 4.1 Experimental Settings We conduct experiments for 12 language pairs: Dutch (nl)-English (en), en-nl, en-French (fr), enJapanese (ja), en-Spanish (es), fr-en, Hindi (hi)-en, ja-en, Korean (ko)-en, Turkish (tr)-en, Urdu (ur)- en and Welsh (cy)-en. We use a phrase-based statistical machine translation system which is similar to (Och and Ney, 2004). The decoder adopts the regular distance distortion model, and also incorporates a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). The distortion limit is set to 5 words. Word alignments are learned using 3 iterations of IBM Model-1 (Brown et al., 1993) and 3 iterations of the HMM alignment model (Vogel et al., 1996). Lattice-based minimum error rate training (MERT) (Macherey et al., 2008) is applied to optimize feature weights. 5- gram language models trained on sentences collected from various sources are used. The translation system is trained with parallel sentences automatically collected from the Web. The parallel data for each language pair consists of around 400 million source and target words. In order to make </context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. In Proceedings on the Workshop on Statistical Machine Translation, pages 55–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based Dependency Parsing with Rich Non-local Features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Short Papers,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="30821" citStr="Zhang and Nivre, 2011" startWordPosition="5384" endWordPosition="5387">r This system uses the preordering method based on statistical classifiers (Lerner and Petrov, 2013), and the 2-step algorithm was implemented. Lader This system uses Latent Derivation Reorderer (Neubig et al., 2012), which is a BTG-based preordering system using the CYK algorithm.6 The basic feature templates in Table 2 are used as features. Top-Down This system uses the preordering system described in Section 3. Among the six systems, Manual-Rules, AutoRules and Classifier need dependency parsers for source languages. A dependency parser based on the shift-reduce algorithm with beam search (Zhang and Nivre, 2011) is used. The dependency parser and all the preordering systems need POS taggers. A supervised POS tagger based on conditional random fields (Lafferty et al., 2001) trained with manually POS annotated data is used for nl, en, fr, ja and ko. For other languages, we use a POS tagger based on POS projection (T¨ackstr¨om 6lader 0.1.4. http://www.phontron.com/lader/ et al., 2013) which does not need POS annotated data. Word classes in Table 2 are obtained by using Brown clusters (Koo et al., 2008) (the number of classes is set to 256). For both Lader and TopDown, the beam width is set to 20, and th</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Short Papers, pages 188–193.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>