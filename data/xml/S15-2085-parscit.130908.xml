<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000828">
<title confidence="0.9963955">
DIEGOLab: An Approach for Message-level Sentiment Classification in
Twitter
</title>
<author confidence="0.992817">
Abeed Sarker, Azadeh Nikfarjam, Davy Weissenbacher, Graciela Gonzalez
</author>
<affiliation confidence="0.997753">
Department of Biomedical Informatics
Arizona State University
</affiliation>
<address confidence="0.575507">
Scottsdale, AZ 85281, USA
</address>
<email confidence="0.99964">
{abeed.sarker,anikfarj,dweissen,graciela.gonzalez}@asu.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987785">
We present our supervised sentiment classifi-
cation system which competed in SemEval-
2015 Task 10B: Sentiment Classification in
Twitter— Message Polarity Classification.
Our system employs a Support Vector Ma-
chine classifier trained using a number of fea-
tures including n-grams, dependency parses,
synset expansions, word prior polarities, and
embedding clusters. Using weighted Sup-
port Vector Machines, to address the issue of
class imbalance, our system obtains positive
class F-scores of 0.701 and 0.656, and nega-
tive class F-scores of 0.515 and 0.478 over the
training and test sets, respectively.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.928976">
Social media has seen unprecedented growth in
</subsectionHeader>
<bodyText confidence="0.998647642857143">
recent years. Twitter, for example, has over
645,750,000 users and grows by an estimated
135,000 users every day, generating 9,100 tweets
per second1). Users often express their views and
emotions regarding a range of topics on social me-
dia platforms. As such, social media has become
a crucial resource for obtaining information directly
from end-users, and data from social media has been
utilized for a variety of tasks ranging from person-
alized marketing to public health monitoring. While
the benefits of using a resource such as Twitter in-
clude large volumes of data and direct access to end-
user sentiments, there are several obstacles associ-
ated with the use of social media data. These include
</bodyText>
<footnote confidence="0.804337666666667">
1http://www.statisticbrain.com/
twitter-statistics/. Accessed on: 26th August,
2014.
</footnote>
<bodyText confidence="0.999734047619048">
the use of non-standard terminologies, misspellings,
short and ambiguous posts, and data imbalance, to
name a few.
In this paper, we present a supervised learning
approach, using Support Vector Machines (SVMs)
for the task of automatic sentiment classification
of Twitter posts. Our system participated in
the SemEval-2015 task Sentiment Classification in
Twitter— Message Polarity Classification. The goal
of the task was to automatically classify the polar-
ity of a Twitter post into one of three predefined
categories— positive, negative and neutral. In our
approach, we apply a small set of carefully extracted
lexical, semantic, and distributional features. The
features are used to train a SVM learner, and the
issue of data imbalance is addressed by using dis-
tinct weights for each of the three classes. The re-
sults of our system are promising, with positive class
F-scores of 0.701 and 0.656, and negative class F-
scores of 0.515 and 0.478 over the training and test
sets, respectively.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999841727272727">
Following the pioneering work on sentiment analy-
sis by Pang et. al. (2002), similar research has been
carried out under various umbrella terms such as: se-
mantic orientation (Turney, 2002), opinion mining
(Pang and Lee, 2008), polarity classification (Sarker
et al., 2013), and many more. Pang et al. (2002) uti-
lized machine learning models to predict sentiments
in text, and their approach showed that SVM clas-
sifiers trained using bag-of-words features produced
promising results. Similar approaches have been ap-
plied to texts of various granularities— documents,
</bodyText>
<page confidence="0.933899">
510
</page>
<bodyText confidence="0.92721195">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 510–514,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
sentences, and phrases.
Due to the availability of vast amounts of data,
there has been growing interest in utilizing social
media mining for obtaining information directly
from users (Liu and Zhang, 2012). However, so-
cial media sources, such as Twitter posts, present
various natural language processing (NLP) and ma-
chine learning challenges. The NLP challenges arise
from factors, such as, the use of informal language,
frequent misspellings, creative phrases and words,
abbreviations, short text lengths and others. From
the perspective of machine learning, some of the
key challenges include data imbalance, noise, and
feature sparseness. In recent research, these chal-
lenges have received significant attention (Jansen et
al., 2009; Barbosa and Feng, 2010; Davidov et al.,
2010; Kouloumpis et al., 2011; Sarker and Gonza-
lez, 2014).
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="method">
3 Methods
</sectionHeader>
<subsectionHeader confidence="0.996145">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999957642857143">
Our training and test data consists of the data made
available for SemEval 2015 task 10 (A–D). Each in-
stance of the data set made available consisted of a
tweet ID, a user ID, and a sentiment category for the
tweet. For training, we downloaded all the annotated
tweets that were publicly available at the time of de-
velopment of the system. We were able to obtain,
from the training and development sets released by
the organizers, a total of 9,289 tweets for which the
annotations were available. Of these, 4,445 (48%)
were annotated as neutral, 1,416 (15%) as negative,
and 3,428 (37%) as positive. The data is heavily im-
balanced with particularly small number of negative
instances.
</bodyText>
<subsectionHeader confidence="0.994146">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.99998325">
We derive a set of lexical, semantic, and distribu-
tional features from the training data. A brief de-
scription of each feature and preprocessing tech-
nique is described below.
</bodyText>
<subsectionHeader confidence="0.924932">
3.2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999860857142857">
We perform standard preprocessing such as tok-
enization, lowercasing and stemming of all the terms
using the Porter stemmer2 (Porter, 1980). Our pre-
liminary investigations suggested that stop words
can play a positive effect on classifier performances
by their presence in word 2-grams and 3-grams; so,
we do not remove stop words from the texts.
</bodyText>
<subsectionHeader confidence="0.813136">
3.2.2 N-grams
</subsectionHeader>
<bodyText confidence="0.8208044">
Our first feature set consists of word n-grams of
the tweets. A word n-gram is a sequence of con-
tiguous n words in a text segment, and this feature
enables us to represent a document using the union
of its terms. We use 1-, 2-, and 3-grams as features.
</bodyText>
<subsectionHeader confidence="0.911404">
3.2.3 Synset
</subsectionHeader>
<bodyText confidence="0.999941888888889">
It has been shown in past research that certain
terms, because of their prior polarities, play impor-
tant roles in determining the polarities of sentences
(Sarker et al., 2013). Certain adjectives, and some-
times nouns and verbs, or their synonyms, are almost
invariably associated with positive or non-positive
polarities. For each adjective, noun or verb in a
tweet, we use WordNet3 to identify the synonyms of
that term and add the synonymous terms as features.
</bodyText>
<subsectionHeader confidence="0.961552">
3.2.4 Average Sentiment Score
</subsectionHeader>
<bodyText confidence="0.999992263157895">
For this feature, we incorporate a score that at-
tempts to represent the general sentiment of a tweet
using the prior polarities of its terms. Each word-
POS pair in a comment is assigned a score and the
overall score assigned to the comment is equal to the
sum of all the individual term-POS sentiment scores
divided by the length of the sentence in words. For
term-POS pairs with multiple senses, the score for
the most common sense is chosen. To obtain a
score for each term, we use the lexicon proposed by
Guerini et al.. The lexicon contains approximately
155,000 English words associated with a sentiment
score between -1 and 1. The overall score a sentence
receives is therefore a floating point number with the
range [-1:1]. One problem faced, when using such
a lexicon on tweets, is words are frequently mis-
spelled and, thus, missed by the lexicon matching
process. We, therefore, used a fast, moderately accu-
rate, and publicly available spelling correction sys-
</bodyText>
<footnote confidence="0.99168475">
2We use the implementation provided by the NLTK toolkit
http://www.nltk.org/.
3http://wordnet.princeton.edu/. Accessed on
October 13, 2014.
</footnote>
<page confidence="0.99544">
511
</page>
<bodyText confidence="0.960989">
tem4 to process each tweet before performing lexi-
con matches.
</bodyText>
<subsectionHeader confidence="0.707029">
3.2.5 Grammatical Dependencies
</subsectionHeader>
<bodyText confidence="0.9999274">
Stanford grammatical dependencies have been
designed with a view to provide a simple and usable
analysis of the grammatical structure of a sentence
by people who are not (computational) linguists (de
Marneffe et al., 2006). In this schema, each rela-
tion between words of a sentence are encoded as
binary predicates between two words. A semantic
interpretation which uses the notions of traditional
grammar are attached to the relations to facilitate
their comprehension. For example, from the sen-
tence I love the banner, we expect in the analysis the
relations nsubj(love, I), det(banner, the), dobj(love,
banner) denoting subject, determinant and direct ob-
ject roles, respectively. Based on previous research
(Nikfarjam et al., 2012), our intuition is that depen-
dency relationships maybe useful for polarity clas-
sification. We used the Stanford parser integrated
in the Stanford CoreNLP 3.4 suite,5 and computed
collapsed and propagated dependency trees for each
tweet.
</bodyText>
<subsectionHeader confidence="0.967414">
3.2.6 Embedding Cluster Features
</subsectionHeader>
<bodyText confidence="0.999993294117647">
Considering the nature of the user posts in Twit-
ter, it is common to observe rarely occurring or un-
seen tokens in the test data. In order to address
this issue, we use embedding cluster features in-
troduced in (Nikfarjam et al., 2014). We catego-
rize the similar tokens into clusters, and as a result,
each token in the corpus has an associated cluster
number. Therefore, every tweet is represented with
a set of cluster numbers, with similar tokens hav-
ing the same cluster number. The word clusters are
generated based on K-means clustering of the to-
ken representative vectors (known as embeddings).
The embeddings are meaningful real-valued vectors
of configurable dimensions (usually, 150 to 500 di-
mensions) learned from large volumes of unlabeled
sentences. We generate 150-dimensional vectors
using the word2vec tool.6. Our corpus includes a
</bodyText>
<footnote confidence="0.997215666666667">
4http://norvig.com/spell-correct.html.
Accessed on January 7, 2015.
5http://nlp.stanford.edu/software/
corenlp.shtml. Accessed on January 8, 2015
6Available at: https://code.google.com/p/
word2vec/. Accessed on 13 January, 2015
</footnote>
<bodyText confidence="0.999164866666667">
large number of unlabeled sentences from the pro-
vided train/test tweets plus an additional 860,000
in-house set of collected tweets about user opinions
on medications. The vector and cluster dimensions
are selected based on extrinsic evaluation of differ-
ent configurations for the embedding clusters, gen-
erated from the same in-house Twitter corpus in our
previous study. Word2vec learns the embeddings
by training a neural network-based language model,
and mapping tokens from similar contexts into vec-
tors that can then be clustered using vector similarity
techniques. More information about generating the
embeddings can be found in the related papers (Ben-
gio et al., 2003; Turian et al., 2010; Mikolov et al.,
2013).
</bodyText>
<subsectionHeader confidence="0.502616">
3.2.7 Other Features
</subsectionHeader>
<bodyText confidence="0.999619666666667">
In addition to the abovementioned features, we
used the post lengths, in number of characters, as
a feature.
</bodyText>
<subsectionHeader confidence="0.996572">
3.3 Classification
</subsectionHeader>
<bodyText confidence="0.999975529411765">
Using the abovementioned features, we trained
SVM classifiers for the classification task. The per-
formance of SVMs can vary significantly based on
the kernel and specific parameter values. For our
work, based on some preliminary experimentation
on the training set, we used the RBF kernel. We
computed optimal values for the cost and -y parame-
ters via grid-search and 10-fold cross validation over
the training set. To address the problem of data im-
balance, we utilized the weighted SVM feature of
the LibSVM library (Chang and Lin, 2011), and we
attempted to find optimal values for the weights in
the same way using 10-fold cross validation over the
training set. We found that cost = 8.0, -y = 0.0,
W1 = 3.5, and W2 = 2.2 to produce the best results,
where W1 and W2 are the weights for the positive and
negative classes, respectively.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999913857142857">
Table 1 presents the performance of our system on
the training and test data sets. The table presents the
positive and negative class F-scores for the system,
and the average of the two scores— the metric that is
used for ranking systems in the SemEval evaluations
for this task. For the training set, the results are those
obtained via 10-fold cross validation. The test set
</bodyText>
<page confidence="0.994137">
512
</page>
<bodyText confidence="0.513034">
consists of 2,390 instances and the full training set
is used when performing classification on this set.
</bodyText>
<table confidence="0.9974104">
Data set Positive F- Negative F- P + N
score (P) score (N)
2
Training 0.701 0.515 0.608
Test 0.656 0.478 0.567
</table>
<tableCaption confidence="0.986631">
Table 1: Classification results for the DIEGOLab system
over the training and test sets.
</tableCaption>
<subsectionHeader confidence="0.996445">
4.1 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.969636538461539">
To assess the contribution of each feature towards
the final score, we performed leave-one-out feature
and single feature experiments. Tables 3 and 2 show
the P +N
2 values for the training and the test sets for
the two set of experiments. The first row of the
tables present the results when all the features are
used, and the following rows show the results when
a specific feature is removed or when a single fea-
ture is used. The tables illustrate that the most im-
portant feature set is n-grams, and there is a large
drop in the evaluation score when that feature is re-
moved (in Table 2). For all the other feature sets,
the drops in the evaluation scores shown in Table 3
are very low, meaning that their contribution to the
final evaluation score is quite limited. Table 3 sug-
gests that the sentiment score feature is the second
most useful feature after n-grams. The experiments
suggest that the classifier settings (i.e., the parameter
values and the class weights) play a more important
role in our final approach, as greater deviations from
the scores presented can be achieved by fine tuning
the parameter values than by adding, removing, or
modifying the feature sets. Further experimentation
is required to identify useful features and to config-
ure existing features to be more effective.
</bodyText>
<sectionHeader confidence="0.994701" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999843428571428">
Our system achieved moderate performance on the
SemEval sentiment analysis task utilizing very basic
settings. The F-scores were particularly low for the
negative class, which can be attributed to the class
imbalance. Considering that the performance of our
system was achieved by very basic settings, there
is promise of better performance via the utilization
</bodyText>
<table confidence="0.991372615384616">
Feature Training set Test set
removed average average
None 0.608 0.567
N-grams 0.575 0.527
Synset 0.606 0.565
Sentiment 0.608 0.561
Score
Grammatical 0.601 0.562
Dependen-
cies
Embedding 0.602 0.566
Clusters
Other 0.608 0.565
</table>
<tableCaption confidence="0.977085">
Table 2: Leave-one-out P +N
</tableCaption>
<table confidence="0.971637866666667">
2 feature scores for the train-
ing and test sets.
Feature Training set Test set
average average
All 0.608 0.567
N-grams 0.587 0.560
Synset 0.507 0.478
Sentiment 0.561 0.489
Score
Grammatical 0.435 0.436
Dependen-
cies
Embedding 0.482 0.461
Clusters
Other 0.303 0.272
</table>
<tableCaption confidence="0.999376">
Table 3: Single feature P +N
</tableCaption>
<bodyText confidence="0.941628285714286">
2 scores for the training and
test sets.
of various feature generation and engineering tech-
niques.
We have several planned future tasks to improve
the classification performance on this data set, and
for social media based sentiment analysis in general.
Following on from our past work on social media
data (Patki et al., 2014; Sarker and Gonzalez, 2014),
a significant portion of our future work will focus
on the application of more informative features for
automatic classification of social media text, includ-
ing sentiment analysis. We are also keen to explore
the use of text normalization techniques, at various
</bodyText>
<page confidence="0.994867">
513
</page>
<bodyText confidence="0.965632">
granularities, to improve classification performance
over social media data.
</bodyText>
<sectionHeader confidence="0.997707" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999006">
This work was supported by NIH National Li-
brary of Medicine under grant number NIH NLM
1R01LM011176. The content is solely the responsi-
bility of the authors and does not necessarily repre-
sent the official views of the NLM or NIH.
</bodyText>
<sectionHeader confidence="0.998951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999738147727273">
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In Proceedings of COLING, pages 36–44.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Research,
3:1137–1155.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1–
27:27. Software available at http://www.csie.
ntu.edu.tw/˜cjlin/libsvm.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced Sentiment Learning Using Twitter Hashtags
and Smileys. In Proceedings of COLING, pages 241–
249.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating Typed
Dependency Parsers from Phrase Structure Parses. In
Proceedings of the Fifth International Conference on
Language Resources and Evaluation, pages 449–454.
Marco Guerini, Lorenzo Gatti, and Marco Turchi. 2013.
Sentiment Analysis: How to Derive Prior Polarities
from SentiWordNet. In Proceedings of Empirical
Methods in Natural Language Processing (EMNLP),
pages 1259–1269.
Bernard J Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter Power: Tweets as
Electronic Word of Mouth. Journal of the Ameri-
can Society for Information Science and Technology,
60(11):2169–2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter Sentiment Analysis: The Good
the Bad and the OMG! In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media.
Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Charu C. Aggar-
wal and ChengXiang Zhai, editors, Mining Text Data,
pages 415–463.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Represen-
tations in Vector Space. arXiv:1301.3781v3.
Azadeh Nikfarjam, Ehsan Emadzadeh, and Graciela
Gonzalez. 2012. A Hybrid System for Emotion Ex-
traction from Suicide Notes. Biomedical Informatics
Insights, 5. Suppl 1:165–174.
Azadeh Nikfarjam, Abeed Sarker, Karen O’Connor,
Rachel Ginn, and Graciela Gonzalez. 2014. Pharma-
covigilance from social media: mining adverse drug
reaction mentions using sequence labeling with word
embedding cluster features. Journal of the American
Medical Informatics Association (JAMIA).
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of
the ACL conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79–86.
Apurv Patki, Abeed Sarker, Pranoti Pimpalkhute, Azadeh
Nikfarjam, Rachel Ginn, Karen O’Connor, Karen
Smith, and Graciela Gonzalez. 2014. Mining Adverse
Drug Reaction Signals from Social Media: Going Be-
yond Extraction. In Proceedings of BioLinkSig 2014.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Abeed Sarker and Graciela Gonzalez. 2014. Portable
Automatic Text Classification for Adverse Drug Reac-
tion Detection via Multi-corpus Training. Journal of
Biomedical Informatics.
Abeed Sarker, Diego Molla, and Cecile Paris. 2013.
Automatic Prediction of Evidence-based Recommen-
dations via Sentence-level Polarity Classification. In
Proceedings of the International Joint Conference on
Natural Language Processing (IJCNLP), pages 712–
718.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 384–394.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of ACL-02, 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 417–424.
</reference>
<page confidence="0.997924">
514
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.778330">
<title confidence="0.9427505">DIEGOLab: An Approach for Message-level Sentiment Classification in Twitter</title>
<author confidence="0.948078">Abeed Sarker</author>
<author confidence="0.948078">Azadeh Nikfarjam</author>
<author confidence="0.948078">Davy Weissenbacher</author>
<author confidence="0.948078">Graciela</author>
<affiliation confidence="0.980051">Department of Biomedical Arizona State</affiliation>
<address confidence="0.967094">Scottsdale, AZ 85281,</address>
<abstract confidence="0.997399733333333">We present our supervised sentiment classification system which competed in SemEval- 2015 Task 10B: Sentiment Classification in Twitter— Message Polarity Classification. Our system employs a Support Vector Machine classifier trained using a number of features including n-grams, dependency parses, synset expansions, word prior polarities, and embedding clusters. Using weighted Support Vector Machines, to address the issue of class imbalance, our system obtains positive class F-scores of 0.701 and 0.656, and negative class F-scores of 0.515 and 0.478 over the training and test sets, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust Sentiment Detection on Twitter from Biased and Noisy Data.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>36--44</pages>
<contexts>
<context position="4270" citStr="Barbosa and Feng, 2010" startWordPosition="629" endWordPosition="632">for obtaining information directly from users (Liu and Zhang, 2012). However, social media sources, such as Twitter posts, present various natural language processing (NLP) and machine learning challenges. The NLP challenges arise from factors, such as, the use of informal language, frequent misspellings, creative phrases and words, abbreviations, short text lengths and others. From the perspective of machine learning, some of the key challenges include data imbalance, noise, and feature sparseness. In recent research, these challenges have received significant attention (Jansen et al., 2009; Barbosa and Feng, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Sarker and Gonzalez, 2014). 3 Methods 3.1 Data Our training and test data consists of the data made available for SemEval 2015 task 10 (A–D). Each instance of the data set made available consisted of a tweet ID, a user ID, and a sentiment category for the tweet. For training, we downloaded all the annotated tweets that were publicly available at the time of development of the system. We were able to obtain, from the training and development sets released by the organizers, a total of 9,289 tweets for which the annotations were available. Of thes</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust Sentiment Detection on Twitter from Biased and Noisy Data. In Proceedings of COLING, pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="10333" citStr="Bengio et al., 2003" startWordPosition="1593" endWordPosition="1597">ain/test tweets plus an additional 860,000 in-house set of collected tweets about user opinions on medications. The vector and cluster dimensions are selected based on extrinsic evaluation of different configurations for the embedding clusters, generated from the same in-house Twitter corpus in our previous study. Word2vec learns the embeddings by training a neural network-based language model, and mapping tokens from similar contexts into vectors that can then be clustered using vector similarity techniques. More information about generating the embeddings can be found in the related papers (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013). 3.2.7 Other Features In addition to the abovementioned features, we used the post lengths, in number of characters, as a feature. 3.3 Classification Using the abovementioned features, we trained SVM classifiers for the classification task. The performance of SVMs can vary significantly based on the kernel and specific parameter values. For our work, based on some preliminary experimentation on the training set, we used the RBF kernel. We computed optimal values for the cost and -y parameters via grid-search and 10-fold cross validation over the tra</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<pages>27--27</pages>
<note>Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="11066" citStr="Chang and Lin, 2011" startWordPosition="1713" endWordPosition="1716"> used the post lengths, in number of characters, as a feature. 3.3 Classification Using the abovementioned features, we trained SVM classifiers for the classification task. The performance of SVMs can vary significantly based on the kernel and specific parameter values. For our work, based on some preliminary experimentation on the training set, we used the RBF kernel. We computed optimal values for the cost and -y parameters via grid-search and 10-fold cross validation over the training set. To address the problem of data imbalance, we utilized the weighted SVM feature of the LibSVM library (Chang and Lin, 2011), and we attempted to find optimal values for the weights in the same way using 10-fold cross validation over the training set. We found that cost = 8.0, -y = 0.0, W1 = 3.5, and W2 = 2.2 to produce the best results, where W1 and W2 are the weights for the positive and negative classes, respectively. 4 Results Table 1 presents the performance of our system on the training and test data sets. The table presents the positive and negative class F-scores for the system, and the average of the two scores— the metric that is used for ranking systems in the SemEval evaluations for this task. For the t</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1– 27:27. Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced Sentiment Learning Using Twitter Hashtags and Smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>241--249</pages>
<contexts>
<context position="4292" citStr="Davidov et al., 2010" startWordPosition="633" endWordPosition="636">n directly from users (Liu and Zhang, 2012). However, social media sources, such as Twitter posts, present various natural language processing (NLP) and machine learning challenges. The NLP challenges arise from factors, such as, the use of informal language, frequent misspellings, creative phrases and words, abbreviations, short text lengths and others. From the perspective of machine learning, some of the key challenges include data imbalance, noise, and feature sparseness. In recent research, these challenges have received significant attention (Jansen et al., 2009; Barbosa and Feng, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Sarker and Gonzalez, 2014). 3 Methods 3.1 Data Our training and test data consists of the data made available for SemEval 2015 task 10 (A–D). Each instance of the data set made available consisted of a tweet ID, a user ID, and a sentiment category for the tweet. For training, we downloaded all the annotated tweets that were publicly available at the time of development of the system. We were able to obtain, from the training and development sets released by the organizers, a total of 9,289 tweets for which the annotations were available. Of these, 4,445 (48%) were an</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced Sentiment Learning Using Twitter Hashtags and Smileys. In Proceedings of COLING, pages 241– 249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parsers from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D Manning. 2006. Generating Typed Dependency Parsers from Phrase Structure Parses. In Proceedings of the Fifth International Conference on Language Resources and Evaluation, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Guerini</author>
<author>Lorenzo Gatti</author>
<author>Marco Turchi</author>
</authors>
<title>Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet.</title>
<date>2013</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1259--1269</pages>
<marker>Guerini, Gatti, Turchi, 2013</marker>
<rawString>Marco Guerini, Lorenzo Gatti, and Marco Turchi. 2013. Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 1259–1269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<title>Twitter Power: Tweets as Electronic Word of Mouth.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>11</issue>
<contexts>
<context position="4246" citStr="Jansen et al., 2009" startWordPosition="625" endWordPosition="628"> social media mining for obtaining information directly from users (Liu and Zhang, 2012). However, social media sources, such as Twitter posts, present various natural language processing (NLP) and machine learning challenges. The NLP challenges arise from factors, such as, the use of informal language, frequent misspellings, creative phrases and words, abbreviations, short text lengths and others. From the perspective of machine learning, some of the key challenges include data imbalance, noise, and feature sparseness. In recent research, these challenges have received significant attention (Jansen et al., 2009; Barbosa and Feng, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Sarker and Gonzalez, 2014). 3 Methods 3.1 Data Our training and test data consists of the data made available for SemEval 2015 task 10 (A–D). Each instance of the data set made available consisted of a tweet ID, a user ID, and a sentiment category for the tweet. For training, we downloaded all the annotated tweets that were publicly available at the time of development of the system. We were able to obtain, from the training and development sets released by the organizers, a total of 9,289 tweets for which the annotations</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernard J Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter Power: Tweets as Electronic Word of Mouth. Journal of the American Society for Information Science and Technology, 60(11):2169–2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter Sentiment Analysis: The Good the Bad and the OMG!</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="4317" citStr="Kouloumpis et al., 2011" startWordPosition="637" endWordPosition="640">(Liu and Zhang, 2012). However, social media sources, such as Twitter posts, present various natural language processing (NLP) and machine learning challenges. The NLP challenges arise from factors, such as, the use of informal language, frequent misspellings, creative phrases and words, abbreviations, short text lengths and others. From the perspective of machine learning, some of the key challenges include data imbalance, noise, and feature sparseness. In recent research, these challenges have received significant attention (Jansen et al., 2009; Barbosa and Feng, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Sarker and Gonzalez, 2014). 3 Methods 3.1 Data Our training and test data consists of the data made available for SemEval 2015 task 10 (A–D). Each instance of the data set made available consisted of a tweet ID, a user ID, and a sentiment category for the tweet. For training, we downloaded all the annotated tweets that were publicly available at the time of development of the system. We were able to obtain, from the training and development sets released by the organizers, a total of 9,289 tweets for which the annotations were available. Of these, 4,445 (48%) were annotated as neutral, 1,416</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter Sentiment Analysis: The Good the Bad and the OMG! In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Lei Zhang</author>
</authors>
<title>A survey of opinion mining and sentiment analysis.</title>
<date>2012</date>
<booktitle>In Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data,</booktitle>
<pages>415--463</pages>
<contexts>
<context position="3715" citStr="Liu and Zhang, 2012" startWordPosition="548" endWordPosition="551">ls to predict sentiments in text, and their approach showed that SVM classifiers trained using bag-of-words features produced promising results. Similar approaches have been applied to texts of various granularities— documents, 510 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 510–514, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences, and phrases. Due to the availability of vast amounts of data, there has been growing interest in utilizing social media mining for obtaining information directly from users (Liu and Zhang, 2012). However, social media sources, such as Twitter posts, present various natural language processing (NLP) and machine learning challenges. The NLP challenges arise from factors, such as, the use of informal language, frequent misspellings, creative phrases and words, abbreviations, short text lengths and others. From the perspective of machine learning, some of the key challenges include data imbalance, noise, and feature sparseness. In recent research, these challenges have received significant attention (Jansen et al., 2009; Barbosa and Feng, 2010; Davidov et al., 2010; Kouloumpis et al., 20</context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis. In Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data, pages 415–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<contexts>
<context position="10377" citStr="Mikolov et al., 2013" startWordPosition="1602" endWordPosition="1605"> in-house set of collected tweets about user opinions on medications. The vector and cluster dimensions are selected based on extrinsic evaluation of different configurations for the embedding clusters, generated from the same in-house Twitter corpus in our previous study. Word2vec learns the embeddings by training a neural network-based language model, and mapping tokens from similar contexts into vectors that can then be clustered using vector similarity techniques. More information about generating the embeddings can be found in the related papers (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013). 3.2.7 Other Features In addition to the abovementioned features, we used the post lengths, in number of characters, as a feature. 3.3 Classification Using the abovementioned features, we trained SVM classifiers for the classification task. The performance of SVMs can vary significantly based on the kernel and specific parameter values. For our work, based on some preliminary experimentation on the training set, we used the RBF kernel. We computed optimal values for the cost and -y parameters via grid-search and 10-fold cross validation over the training set. To address the problem of data im</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. arXiv:1301.3781v3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Azadeh Nikfarjam</author>
<author>Ehsan Emadzadeh</author>
<author>Graciela Gonzalez</author>
</authors>
<title>A Hybrid System for Emotion Extraction from Suicide Notes. Biomedical Informatics Insights, 5. Suppl 1:165–174.</title>
<date>2012</date>
<contexts>
<context position="8315" citStr="Nikfarjam et al., 2012" startWordPosition="1293" endWordPosition="1296"> grammatical structure of a sentence by people who are not (computational) linguists (de Marneffe et al., 2006). In this schema, each relation between words of a sentence are encoded as binary predicates between two words. A semantic interpretation which uses the notions of traditional grammar are attached to the relations to facilitate their comprehension. For example, from the sentence I love the banner, we expect in the analysis the relations nsubj(love, I), det(banner, the), dobj(love, banner) denoting subject, determinant and direct object roles, respectively. Based on previous research (Nikfarjam et al., 2012), our intuition is that dependency relationships maybe useful for polarity classification. We used the Stanford parser integrated in the Stanford CoreNLP 3.4 suite,5 and computed collapsed and propagated dependency trees for each tweet. 3.2.6 Embedding Cluster Features Considering the nature of the user posts in Twitter, it is common to observe rarely occurring or unseen tokens in the test data. In order to address this issue, we use embedding cluster features introduced in (Nikfarjam et al., 2014). We categorize the similar tokens into clusters, and as a result, each token in the corpus has a</context>
</contexts>
<marker>Nikfarjam, Emadzadeh, Gonzalez, 2012</marker>
<rawString>Azadeh Nikfarjam, Ehsan Emadzadeh, and Graciela Gonzalez. 2012. A Hybrid System for Emotion Extraction from Suicide Notes. Biomedical Informatics Insights, 5. Suppl 1:165–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Azadeh Nikfarjam</author>
<author>Abeed Sarker</author>
<author>Karen O’Connor</author>
<author>Rachel Ginn</author>
<author>Graciela Gonzalez</author>
</authors>
<title>Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features.</title>
<date>2014</date>
<journal>Journal of the American Medical Informatics Association (JAMIA).</journal>
<marker>Nikfarjam, Sarker, O’Connor, Ginn, Gonzalez, 2014</marker>
<rawString>Azadeh Nikfarjam, Abeed Sarker, Karen O’Connor, Rachel Ginn, and Graciela Gonzalez. 2014. Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features. Journal of the American Medical Informatics Association (JAMIA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval,</title>
<date>2008</date>
<contexts>
<context position="2982" citStr="Pang and Lee, 2008" startWordPosition="442" endWordPosition="445"> lexical, semantic, and distributional features. The features are used to train a SVM learner, and the issue of data imbalance is addressed by using distinct weights for each of the three classes. The results of our system are promising, with positive class F-scores of 0.701 and 0.656, and negative class Fscores of 0.515 and 0.478 over the training and test sets, respectively. 2 Related Work Following the pioneering work on sentiment analysis by Pang et. al. (2002), similar research has been carried out under various umbrella terms such as: semantic orientation (Turney, 2002), opinion mining (Pang and Lee, 2008), polarity classification (Sarker et al., 2013), and many more. Pang et al. (2002) utilized machine learning models to predict sentiments in text, and their approach showed that SVM classifiers trained using bag-of-words features produced promising results. Similar approaches have been applied to texts of various granularities— documents, 510 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 510–514, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences, and phrases. Due to the availability of vast amounts of data,</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>79--86</pages>
<contexts>
<context position="3064" citStr="Pang et al. (2002)" startWordPosition="455" endWordPosition="458">VM learner, and the issue of data imbalance is addressed by using distinct weights for each of the three classes. The results of our system are promising, with positive class F-scores of 0.701 and 0.656, and negative class Fscores of 0.515 and 0.478 over the training and test sets, respectively. 2 Related Work Following the pioneering work on sentiment analysis by Pang et. al. (2002), similar research has been carried out under various umbrella terms such as: semantic orientation (Turney, 2002), opinion mining (Pang and Lee, 2008), polarity classification (Sarker et al., 2013), and many more. Pang et al. (2002) utilized machine learning models to predict sentiments in text, and their approach showed that SVM classifiers trained using bag-of-words features produced promising results. Similar approaches have been applied to texts of various granularities— documents, 510 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 510–514, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences, and phrases. Due to the availability of vast amounts of data, there has been growing interest in utilizing social media mining for obtaining in</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. In Proceedings of the ACL conference on Empirical Methods in Natural Language Processing (EMNLP), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apurv Patki</author>
<author>Abeed Sarker</author>
<author>Pranoti Pimpalkhute</author>
<author>Azadeh Nikfarjam</author>
<author>Rachel Ginn</author>
<author>Karen O’Connor</author>
<author>Karen Smith</author>
<author>Graciela Gonzalez</author>
</authors>
<title>Mining Adverse Drug Reaction Signals from Social Media: Going Beyond Extraction.</title>
<date>2014</date>
<booktitle>In Proceedings of BioLinkSig</booktitle>
<marker>Patki, Sarker, Pimpalkhute, Nikfarjam, Ginn, O’Connor, Smith, Gonzalez, 2014</marker>
<rawString>Apurv Patki, Abeed Sarker, Pranoti Pimpalkhute, Azadeh Nikfarjam, Rachel Ginn, Karen O’Connor, Karen Smith, and Graciela Gonzalez. 2014. Mining Adverse Drug Reaction Signals from Social Media: Going Beyond Extraction. In Proceedings of BioLinkSig 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="5396" citStr="Porter, 1980" startWordPosition="820" endWordPosition="821">e organizers, a total of 9,289 tweets for which the annotations were available. Of these, 4,445 (48%) were annotated as neutral, 1,416 (15%) as negative, and 3,428 (37%) as positive. The data is heavily imbalanced with particularly small number of negative instances. 3.2 Features We derive a set of lexical, semantic, and distributional features from the training data. A brief description of each feature and preprocessing technique is described below. 3.2.1 Preprocessing We perform standard preprocessing such as tokenization, lowercasing and stemming of all the terms using the Porter stemmer2 (Porter, 1980). Our preliminary investigations suggested that stop words can play a positive effect on classifier performances by their presence in word 2-grams and 3-grams; so, we do not remove stop words from the texts. 3.2.2 N-grams Our first feature set consists of word n-grams of the tweets. A word n-gram is a sequence of contiguous n words in a text segment, and this feature enables us to represent a document using the union of its terms. We use 1-, 2-, and 3-grams as features. 3.2.3 Synset It has been shown in past research that certain terms, because of their prior polarities, play important roles i</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abeed Sarker</author>
<author>Graciela Gonzalez</author>
</authors>
<title>Portable Automatic Text Classification for Adverse Drug Reaction Detection via Multi-corpus Training.</title>
<date>2014</date>
<journal>Journal of Biomedical Informatics.</journal>
<contexts>
<context position="4345" citStr="Sarker and Gonzalez, 2014" startWordPosition="641" endWordPosition="645">wever, social media sources, such as Twitter posts, present various natural language processing (NLP) and machine learning challenges. The NLP challenges arise from factors, such as, the use of informal language, frequent misspellings, creative phrases and words, abbreviations, short text lengths and others. From the perspective of machine learning, some of the key challenges include data imbalance, noise, and feature sparseness. In recent research, these challenges have received significant attention (Jansen et al., 2009; Barbosa and Feng, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Sarker and Gonzalez, 2014). 3 Methods 3.1 Data Our training and test data consists of the data made available for SemEval 2015 task 10 (A–D). Each instance of the data set made available consisted of a tweet ID, a user ID, and a sentiment category for the tweet. For training, we downloaded all the annotated tweets that were publicly available at the time of development of the system. We were able to obtain, from the training and development sets released by the organizers, a total of 9,289 tweets for which the annotations were available. Of these, 4,445 (48%) were annotated as neutral, 1,416 (15%) as negative, and 3,42</context>
<context position="14675" citStr="Sarker and Gonzalez, 2014" startWordPosition="2321" endWordPosition="2324">the training and test sets. Feature Training set Test set average average All 0.608 0.567 N-grams 0.587 0.560 Synset 0.507 0.478 Sentiment 0.561 0.489 Score Grammatical 0.435 0.436 Dependencies Embedding 0.482 0.461 Clusters Other 0.303 0.272 Table 3: Single feature P +N 2 scores for the training and test sets. of various feature generation and engineering techniques. We have several planned future tasks to improve the classification performance on this data set, and for social media based sentiment analysis in general. Following on from our past work on social media data (Patki et al., 2014; Sarker and Gonzalez, 2014), a significant portion of our future work will focus on the application of more informative features for automatic classification of social media text, including sentiment analysis. We are also keen to explore the use of text normalization techniques, at various 513 granularities, to improve classification performance over social media data. Acknowledgments This work was supported by NIH National Library of Medicine under grant number NIH NLM 1R01LM011176. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NLM or NIH. Reference</context>
</contexts>
<marker>Sarker, Gonzalez, 2014</marker>
<rawString>Abeed Sarker and Graciela Gonzalez. 2014. Portable Automatic Text Classification for Adverse Drug Reaction Detection via Multi-corpus Training. Journal of Biomedical Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abeed Sarker</author>
<author>Diego Molla</author>
<author>Cecile Paris</author>
</authors>
<title>Automatic Prediction of Evidence-based Recommendations via Sentence-level Polarity Classification.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>712--718</pages>
<contexts>
<context position="3029" citStr="Sarker et al., 2013" startWordPosition="448" endWordPosition="451">s. The features are used to train a SVM learner, and the issue of data imbalance is addressed by using distinct weights for each of the three classes. The results of our system are promising, with positive class F-scores of 0.701 and 0.656, and negative class Fscores of 0.515 and 0.478 over the training and test sets, respectively. 2 Related Work Following the pioneering work on sentiment analysis by Pang et. al. (2002), similar research has been carried out under various umbrella terms such as: semantic orientation (Turney, 2002), opinion mining (Pang and Lee, 2008), polarity classification (Sarker et al., 2013), and many more. Pang et al. (2002) utilized machine learning models to predict sentiments in text, and their approach showed that SVM classifiers trained using bag-of-words features produced promising results. Similar approaches have been applied to texts of various granularities— documents, 510 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 510–514, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences, and phrases. Due to the availability of vast amounts of data, there has been growing interest in utilizing s</context>
<context position="6059" citStr="Sarker et al., 2013" startWordPosition="934" endWordPosition="937">that stop words can play a positive effect on classifier performances by their presence in word 2-grams and 3-grams; so, we do not remove stop words from the texts. 3.2.2 N-grams Our first feature set consists of word n-grams of the tweets. A word n-gram is a sequence of contiguous n words in a text segment, and this feature enables us to represent a document using the union of its terms. We use 1-, 2-, and 3-grams as features. 3.2.3 Synset It has been shown in past research that certain terms, because of their prior polarities, play important roles in determining the polarities of sentences (Sarker et al., 2013). Certain adjectives, and sometimes nouns and verbs, or their synonyms, are almost invariably associated with positive or non-positive polarities. For each adjective, noun or verb in a tweet, we use WordNet3 to identify the synonyms of that term and add the synonymous terms as features. 3.2.4 Average Sentiment Score For this feature, we incorporate a score that attempts to represent the general sentiment of a tweet using the prior polarities of its terms. Each wordPOS pair in a comment is assigned a score and the overall score assigned to the comment is equal to the sum of all the individual t</context>
</contexts>
<marker>Sarker, Molla, Paris, 2013</marker>
<rawString>Abeed Sarker, Diego Molla, and Cecile Paris. 2013. Automatic Prediction of Evidence-based Recommendations via Sentence-level Polarity Classification. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP), pages 712– 718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="10354" citStr="Turian et al., 2010" startWordPosition="1598" endWordPosition="1601">an additional 860,000 in-house set of collected tweets about user opinions on medications. The vector and cluster dimensions are selected based on extrinsic evaluation of different configurations for the embedding clusters, generated from the same in-house Twitter corpus in our previous study. Word2vec learns the embeddings by training a neural network-based language model, and mapping tokens from similar contexts into vectors that can then be clustered using vector similarity techniques. More information about generating the embeddings can be found in the related papers (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013). 3.2.7 Other Features In addition to the abovementioned features, we used the post lengths, in number of characters, as a feature. 3.3 Classification Using the abovementioned features, we trained SVM classifiers for the classification task. The performance of SVMs can vary significantly based on the kernel and specific parameter values. For our work, based on some preliminary experimentation on the training set, we used the RBF kernel. We computed optimal values for the cost and -y parameters via grid-search and 10-fold cross validation over the training set. To address</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-02, 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="2945" citStr="Turney, 2002" startWordPosition="438" endWordPosition="439">mall set of carefully extracted lexical, semantic, and distributional features. The features are used to train a SVM learner, and the issue of data imbalance is addressed by using distinct weights for each of the three classes. The results of our system are promising, with positive class F-scores of 0.701 and 0.656, and negative class Fscores of 0.515 and 0.478 over the training and test sets, respectively. 2 Related Work Following the pioneering work on sentiment analysis by Pang et. al. (2002), similar research has been carried out under various umbrella terms such as: semantic orientation (Turney, 2002), opinion mining (Pang and Lee, 2008), polarity classification (Sarker et al., 2013), and many more. Pang et al. (2002) utilized machine learning models to predict sentiments in text, and their approach showed that SVM classifiers trained using bag-of-words features produced promising results. Similar approaches have been applied to texts of various granularities— documents, 510 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 510–514, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences, and phrases. Due to the </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of ACL-02, 40th Annual Meeting of the Association for Computational Linguistics, pages 417–424.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>