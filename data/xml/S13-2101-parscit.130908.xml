<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010399">
<title confidence="0.990034">
CU : Computational Assessment of Short Free Text Answers - A Tool for
Evaluating Students’ Understanding
</title>
<author confidence="0.993434">
Ifeyinwa Okoye
</author>
<affiliation confidence="0.8906755">
Institute of Cognitive Science
Dept. of Computer Science
University of Colorado
Boulder, CO 80309, USA
</affiliation>
<email confidence="0.998564">
okoye@colorado.edu
</email>
<author confidence="0.998793">
Steven Bethard
</author>
<affiliation confidence="0.890707">
Institute of Cognitive Science
Dept. of Computer Science
University of Colorado
Boulder, CO 80309, USA
</affiliation>
<email confidence="0.998553">
bethard@colorado.edu
</email>
<author confidence="0.992048">
Tamara Sumner
</author>
<affiliation confidence="0.89064975">
Institute of Cognitive Science
Dept. of Computer Science
University of Colorado
Boulder, CO 80309, USA
</affiliation>
<email confidence="0.998908">
sumner@colorado.edu
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993385">
Assessing student understanding by evaluat-
ing their free text answers to posed questions
is a very important task. However, manually,
it is time-consuming and computationally, it is
difficult. This paper details our shallow NLP
approach to computationally assessing student
free text answers when a reference answer is
provided. For four out of the five test sets, our
system achieved an overall accuracy above the
median and mean.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988">
Assessing student understanding is one of the holy
grails of education (Redecker et al., 2012). If we
(teachers, tutors, intelligent tutors, potential employ-
ers, parents and school administrators) know what
and how much a student knows, then we know what
the student still needs to learn. And then, can ef-
ficiently and effectively educate the student. How-
ever, the task of assessing what exactly a student un-
derstands about a particular topic can be expensive,
difficult and subjective.
Using multiple choice questionnaires is one of
the most prevalent forms of assessing student under-
standing because it is easy and fast, both manually
and computationally. However there has been a lot
of pushback from educators about the validity of re-
sults gotten from multiple choice questionnaires.
Assessing student understanding by evaluating
student free text answers either written or spoken is
one of the preferred alternatives to multiple choice
questionnaires. As an assessment tool, free text an-
swers can illuminate what and how much a student
knows since the student is forced to recall terms and
make connections between those terms rather than
just picking one out of several options. However,
assessing free text answers manually is tedious, ex-
pensive and time-consuming, hence the search for a
computational option.
There are three main issues that can limit the com-
putational approach and corresponding performance
when assessing free text answers: (1) the unit of
assessment, (2) the reference and (3) the level of
assessment. The unit of assessment can be words,
facets, phrases, sentences, short answers or essays.
The reference is the correct answer and what is
being compared to the student answer. Most re-
searchers generate the reference manually (Noorbe-
hbahani and Kardan, 2011; Graesser et al., 2004) but
some have focused on automatically generating the
reference (Ahmad, 2009). The level of assessment
can be coarse with 2 categories such as correct and
incorrect or more finer-grained with up to 19 cate-
gories as in (Ahmad, 2009). In general, the finer-
grained assessments are more difficult to assess.
</bodyText>
<sectionHeader confidence="0.889019" genericHeader="method">
2 The Student Response Analysis Task
</sectionHeader>
<bodyText confidence="0.9996241">
The student response analysis task was posed as fol-
lows: Given a question, a known correct/reference
answer and a 1 or 2 sentence student answer, classify
the student answer into two, three or five categories.
The two categories were correct and incorrect; the
three categories were correct, contradictory and in-
correct; while the five categories were correct, par-
tially correct but incomplete, contradictory, irrele-
vant and not in the domain (Dzikovska et al., 2013).
We chose to work on the 2-way response task only
</bodyText>
<page confidence="0.98641">
603
</page>
<bodyText confidence="0.9953393125">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 603–607, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
because for our application, we need to simply know
if a student answer is correct or incorrect. Our ap-
plication is an interactive essay-based personalized
learning environment (Bethard et al., 2012).
The overarching goal of our application is to cre-
ate a scalable online service that recommends re-
sources to users based on the their conceptual under-
standing expressed in an essay or short answer form.
Our application automatically constructs a domain
knowledge base from digital library resources and
identifies the core concepts in the domain knowl-
edge base.It detects flaws and gaps in users’ sci-
ence knowledge and recommends digital library re-
sources to address users’ misconceptions and knowl-
edge gaps. The gaps are detected by identifying the
core concepts which the user has not discussed. The
flaws (incorrect understanding/misconceptions) are
currently being identified by a process of (1) seg-
menting a student essay into sentences, (2) align-
ing the student sentence to a sentence in the domain
knowledge base and (3) using the system we devel-
oped for the student response analysis task to deter-
mine if the student sentence is correct or incorrect.
The development of our misconception detection
algorithm has been limited by the alignment task.
However, with the data set from the student response
analysis task containing correct alignments, we hope
to be able to use it to make improvements to our
misconception detection algorithm. We discuss our
current misconception detection system below.
</bodyText>
<sectionHeader confidence="0.990333" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.999967166666667">
Our system mainly exploits shallow NLP tech-
niques, in particular text overlap, to see how much
we can gain from using a simple system and how
much more some more semantic features could add
to the simple system. Although we have access to
the question which a 1-2 sentence student answer
corresponds to, we chose not to use that in our sys-
tem because in our application we do not have ac-
cess to that information. We were trying to build a
system that would work in our current essay-based
application.
Some of the student answers in the dataset have a
particular reference answer which they match. How-
ever, we do not make use of this information in our
system either. We assume that for a particular ques-
tion, all the corresponding reference answers can be
used to determine the correctness of any of the stu-
dent answers.
</bodyText>
<subsectionHeader confidence="0.976541">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.999573">
The features we use are:
</bodyText>
<listItem confidence="0.953826461538461">
1. CosineSimilarity : This is the average cosine
similarity (Jurafsky and James, 2000) between
a student answer vector and all the correspond-
ing reference answer vectors. The vectors are
based on word counts. The words were low-
ercased and included stopwords and punctua-
tions.
2. CosineSimilarityNormalized : This is the av-
erage cosine similarity between a student an-
swer vector and all the corresponding reference
answer vectors, with the word counts within
the vectors divided by the word counts in Gi-
gaword, a background corpus. We divided
the raw counts by the counts in Gigaword to
ensure that punctuations, stopwords and other
non-discriminatory words do not artificially in-
crease the cosine similarity.
3. UnigramRefStudent : This is the average un-
igram coverage of the reference answers by a
student answer. To calculate this, the student
answer and all the corresponding reference an-
swers are tokenized into unigrams. Next, for
each reference answer, we count the number of
unigrams in the reference answer that are con-
tained in the student answer and divide it by the
number of unigrams in the reference answer.
The value we get for this feature, is the aver-
age over all the reference answers.
4. UnigramStudentRef : This is the average uni-
gram coverage of the student answer by the ref-
erence answers. To calculate this, the student
answer and all the corresponding reference an-
swers are tokenized into unigrams. Next, for
each reference answer, we count the number
of unigrams in the student answer that are con-
tained in the reference answer and divide it by
the number of unigrams in the student answer.
The value we get for this feature, is the average
over all the reference answers.
</listItem>
<page confidence="0.893183">
604
</page>
<listItem confidence="0.995546307692308">
5. BigramRefStudent : This is similar to the Un-
igramRefStudent feature, but using bigrams.
6. BigramStudentRef : This is similar to the Un-
igramStudentRef feature, but using bigrams.
7. LemmaRefStudent : This is similar to the Un-
igramRefStudent feature, but in this case, the
lemmas are used in place of words.
8. LemmaStudentRef : This is similar to the Un-
igramStudentRef feature, but in this case, the
lemmas are used in place of words.
9. UnigramPosRefStudent : This is similar to
the UnigramRefStudent feature, but we use
part-of-speech unigrams for this feature in
place of word unigrams.
10. UnigramPosStudentRef : This is similar to
the UnigramStudentRef feature, but we use
part-of-speech unigrams for this feature in
place of word unigrams.
11. BigramPosRefStudent : This is similar to the
BigramRefStudent feature, but we use part-of-
speech bigrams for this feature in place of word
unigrams.
12. BigramPosStudentRef : This is similar to the
BigramStudentRef feature, but we use part-of-
speech bigrams for this feature in place of word
unigrams.
</listItem>
<subsectionHeader confidence="0.995694">
3.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999996181818182">
We used the ClearTK (Ogren et al., 2008) toolkit
within Eclipse to extract features from the student
and reference sentences. We trained a LibSVM
(Chang and Lin, 2011) binary classifier to classify a
feature vector into two classes, correct or incorrect.
We used the default parameters for LibSVM except
for the cost parameter, for which we tried different
values. However, the default value of 1 gave us the
best result on the training set. Our two runs/systems
are essentially the same system but with a cost pa-
rameter of 1 and 10.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.99957575">
The Student Response Analysis Task overall re-
sult can be found in the Task description paper
(Dzikovska et al., 2013). The CU system achieved
a ranking of above the mean and median for four
of the five different test sets. We perfomed below
the mean and median on the sciEntsBank unseen an-
swers. The accuracy result for the test data is shown
in Table 4. The results on our training data and
a breakdown of the contribution of each feature is
shown in Table 5. In Table 5 ALL refers to all the
features while ALL-CosineSimilarity is all the fea-
tures excluding the CosineSimilarity feature.
</bodyText>
<table confidence="0.896192333333333">
Sys beetle beetle sciEnts sciEnts sciEnts
tem un- un- Bank Bank Bank
seen seen un- un- un-
an- ques- seen seen seen
swers tions an- ques- do-
swers tions mains
CU 0.786 0.718 0.656 0.674 0.693
run
1
CU 0.784 0.717 0.654 0.671 0.691
run
2
</table>
<tableCaption confidence="0.9893735">
Table 1: Overall Accuracy results for CU system on the
test Data
</tableCaption>
<sectionHeader confidence="0.998084" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999381529411765">
As can be seen from Table 4 and further elaborated
on in (Dzikovska et al., 2013), there were two main
datasets, Beetle and SciEntsBank. The Beetle data
set has multiple reference answer per question while
the SciEntsBank has one reference answer per ques-
tion. Our system did better on the beetle data set
than the SciEntsBank data set, both during devel-
opment and on the final test sets. This leads us to
believe that our system will do well when there are
multiple reference answers rather than just one.
We analyzed the training data to understand
where our system was failing and what we could do
to make it better. We tried removing stopwords be-
fore constructing the feature vectors but that made
the results worse. Here are two examples where re-
moving the stopwords will make it impossible to as-
certain the validity of the student answer:
</bodyText>
<listItem confidence="0.975636">
• It was connected. becomes connected
</listItem>
<page confidence="0.948872">
605
</page>
<listItem confidence="0.9761555">
• It will work because that is closing the switch.
becomes work closing switch
</listItem>
<bodyText confidence="0.990102333333333">
Because the student answers are free text and use
pronouns in place of the nouns that were in the ques-
tion, the stop words are important to provide context.
</bodyText>
<table confidence="0.999773590909091">
Feature Type Beetle
&amp; sci-
Ents
Bank
1 ALL 0.703
2 ALL - CosineSimilarity 0.702
3 ALL - CosineSimilari- 0.700
tyNormalized
4 ALL - UnigramRefStudent 0.702
5 ALL - UnigramStudentRef 0.701
6 ALL - BigramRefStudent 0.702
7 ALL - BigramStudentRef 0.699
8 ALL - LemmaRefStudent 0.701
9 ALL - LemmaStudentRef 0.700
10 ALL - UnigramPosRefStu- 0.703
dent
11 ALL - UnigramPosStuden- 0.703
tRef
12 ALL - BigramPosRefStu- 0.702
dent
13 ALL - BigramPosStuden- 0.702
tRef
</table>
<tableCaption confidence="0.989025">
Table 2: Accuracy results for 5X cross validation on the
training data
</tableCaption>
<bodyText confidence="0.9979376">
Currently, we are working on extracting and
adding several features that we did not use for the
task due to time constraints, to see if they improve
our result. Some of the things we are working on
are:
</bodyText>
<sectionHeader confidence="0.79274" genericHeader="method">
1. Resolving Coreference
</sectionHeader>
<bodyText confidence="0.9999095">
We will use the current state-of-art coreference
system and assume that the question precedes
the student answer in a paragraph when resolv-
ing coreference.
</bodyText>
<sectionHeader confidence="0.898732" genericHeader="method">
2. Compare main predicates
</sectionHeader>
<bodyText confidence="0.999980333333333">
The question is how to assign a value to the se-
mantic similarity between the main predicates.
If the predicates are separate and connect, then
there should be a way to indicate that the men-
tion of one of them in the reference, precludes
the validity of the student answer being correct
if it mentions the other. However, we also have
to take negation into account here. not sepa-
rated and connected should be marked as very
similar if not equal. We plan to include the al-
gorithm from the best system in the semantic
similarity task to our current system.
</bodyText>
<listItem confidence="0.520072">
3. Compare main subject and object from a
</listItem>
<bodyText confidence="0.97274725">
syntactic parse or the numbered arguments
in semantic role label arguments
We have to resolve coreference for this to work
well. And again, we run into the problem of
how to assign a semantic similarity value to two
words that might not share the same synset in
ontologies such as Wordnet.
4. Optimize parameters and explore other clas-
sifiers Throughout developing and testing our
system, we used only the LibSVM classifier
and only optimized the cost parameter. How-
ever, there might be a different classifier or
set of options that can model the data better.
We hope to run through most of the classifiers
available and see if using a different one, with
different options improves our accuracy.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999978055555556">
We have shown that there is value in using shallow
NLP features to judge the validity of free answer text
when the reference answers are given. However,
looking at the sentences that our system labeled as
correct and the gold standard incorrect or vice versa,
it is clear that we have to delve into more seman-
tic features if we want our system to be more accu-
rate. We hope to keep working on this task in sub-
sequent years to ensure continuous improvements in
systems that can assess student knowledge by eval-
uating free answer texts. Such systems will be able
to give students the formative feedback they need
to help them learn better. In addition, such systems
will provide teachers, intelligent tutors and adminis-
trators with feedback about student knowledge, so as
to help them adapt their curriculum, teaching and tu-
toring methods to better serve students’ knowledge
needs.
</bodyText>
<page confidence="0.998671">
606
</page>
<sectionHeader confidence="0.995856" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999845181818182">
Faisal Ahmad. 2009. Generating conceptually personal-
ized interactions for educational digital libraries using
concept maps. Ph.D. thesis, University of Colorado at
Boulder.
Steven Bethard, Haojie Hang, Ifeyinwa Okoye, James H
Martin, Md Arafat Sultan, and Tamara Sumner. 2012.
Identifying science concepts and student misconcep-
tions in an interactive essay writing tutor. In Proceed-
ings of the Seventh Workshop on Building Educational
Applications Using NLP, pages 12–21. Association for
Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Arthur Graesser, Shulan Lu, George Jackson, Heather
Mitchell, Mathew Ventura, Andrew Olney, and Max
Louwerse. 2004. AutoTutor: A tutor with dialogue
in natural language. Behavior Research Methods,
36:180–192.
Daniel Jurafsky and H James. 2000. Speech and lan-
guage processing an introduction to natural language
processing, computational linguistics, and speech.
F Noorbehbahani and AA Kardan. 2011. The automatic
assessment of free text answers using a modified bleu
algorithm. Computers &amp; Education, 56(2):337–345.
Philip V Ogren, Philipp G Wetzler, and Steven J Bethard.
2008. Cleartk: A uima toolkit for statistical natu-
ral language processing. Towards Enhanced Inter-
operability for Large HLT Systems: UIMA for NLP,
page 32.
Christine Redecker, Yves Punie, and Anusca Ferrari.
2012. eassessment for 21st century learning and skills.
In 21st Century Learning for 21st Century Skills,
pages 292–305. Springer.
</reference>
<page confidence="0.99777">
607
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.518997">
<title confidence="0.9815455">CU : Computational Assessment of Short Free Text Answers - A Tool Evaluating Students’ Understanding</title>
<author confidence="0.710232">Ifeyinwa</author>
<affiliation confidence="0.999327">Institute of Cognitive Dept. of Computer University of</affiliation>
<address confidence="0.996115">Boulder, CO 80309,</address>
<email confidence="0.999683">okoye@colorado.edu</email>
<author confidence="0.972163">Steven</author>
<affiliation confidence="0.999836333333333">Institute of Cognitive Dept. of Computer University of</affiliation>
<address confidence="0.996055">Boulder, CO 80309,</address>
<email confidence="0.999716">bethard@colorado.edu</email>
<author confidence="0.777373">Tamara</author>
<affiliation confidence="0.999511333333333">Institute of Cognitive Dept. of Computer University of</affiliation>
<address confidence="0.995648">Boulder, CO 80309,</address>
<email confidence="0.999847">sumner@colorado.edu</email>
<abstract confidence="0.998868818181818">Assessing student understanding by evaluating their free text answers to posed questions is a very important task. However, manually, it is time-consuming and computationally, it is difficult. This paper details our shallow NLP approach to computationally assessing student free text answers when a reference answer is provided. For four out of the five test sets, our system achieved an overall accuracy above the median and mean.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Faisal Ahmad</author>
</authors>
<title>Generating conceptually personalized interactions for educational digital libraries using concept maps.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Colorado at Boulder.</institution>
<contexts>
<context position="2863" citStr="Ahmad, 2009" startWordPosition="435" endWordPosition="436">e the search for a computational option. There are three main issues that can limit the computational approach and corresponding performance when assessing free text answers: (1) the unit of assessment, (2) the reference and (3) the level of assessment. The unit of assessment can be words, facets, phrases, sentences, short answers or essays. The reference is the correct answer and what is being compared to the student answer. Most researchers generate the reference manually (Noorbehbahani and Kardan, 2011; Graesser et al., 2004) but some have focused on automatically generating the reference (Ahmad, 2009). The level of assessment can be coarse with 2 categories such as correct and incorrect or more finer-grained with up to 19 categories as in (Ahmad, 2009). In general, the finergrained assessments are more difficult to assess. 2 The Student Response Analysis Task The student response analysis task was posed as follows: Given a question, a known correct/reference answer and a 1 or 2 sentence student answer, classify the student answer into two, three or five categories. The two categories were correct and incorrect; the three categories were correct, contradictory and incorrect; while the five </context>
</contexts>
<marker>Ahmad, 2009</marker>
<rawString>Faisal Ahmad. 2009. Generating conceptually personalized interactions for educational digital libraries using concept maps. Ph.D. thesis, University of Colorado at Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>Haojie Hang</author>
<author>Ifeyinwa Okoye</author>
<author>James H Martin</author>
<author>Arafat Sultan</author>
<author>Tamara Sumner</author>
</authors>
<title>Identifying science concepts and student misconceptions in an interactive essay writing tutor.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>12--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4099" citStr="Bethard et al., 2012" startWordPosition="625" endWordPosition="628"> correct, partially correct but incomplete, contradictory, irrelevant and not in the domain (Dzikovska et al., 2013). We chose to work on the 2-way response task only 603 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 603–607, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics because for our application, we need to simply know if a student answer is correct or incorrect. Our application is an interactive essay-based personalized learning environment (Bethard et al., 2012). The overarching goal of our application is to create a scalable online service that recommends resources to users based on the their conceptual understanding expressed in an essay or short answer form. Our application automatically constructs a domain knowledge base from digital library resources and identifies the core concepts in the domain knowledge base.It detects flaws and gaps in users’ science knowledge and recommends digital library resources to address users’ misconceptions and knowledge gaps. The gaps are detected by identifying the core concepts which the user has not discussed. T</context>
</contexts>
<marker>Bethard, Hang, Okoye, Martin, Sultan, Sumner, 2012</marker>
<rawString>Steven Bethard, Haojie Hang, Ifeyinwa Okoye, James H Martin, Md Arafat Sultan, and Tamara Sumner. 2012. Identifying science concepts and student misconceptions in an interactive essay writing tutor. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 12–21. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="9228" citStr="Chang and Lin, 2011" startWordPosition="1473" endWordPosition="1476"> This is similar to the UnigramStudentRef feature, but we use part-of-speech unigrams for this feature in place of word unigrams. 11. BigramPosRefStudent : This is similar to the BigramRefStudent feature, but we use part-ofspeech bigrams for this feature in place of word unigrams. 12. BigramPosStudentRef : This is similar to the BigramStudentRef feature, but we use part-ofspeech bigrams for this feature in place of word unigrams. 3.2 Implementation We used the ClearTK (Ogren et al., 2008) toolkit within Eclipse to extract features from the student and reference sentences. We trained a LibSVM (Chang and Lin, 2011) binary classifier to classify a feature vector into two classes, correct or incorrect. We used the default parameters for LibSVM except for the cost parameter, for which we tried different values. However, the default value of 1 gave us the best result on the training set. Our two runs/systems are essentially the same system but with a cost parameter of 1 and 10. 4 Results The Student Response Analysis Task overall result can be found in the Task description paper (Dzikovska et al., 2013). The CU system achieved a ranking of above the mean and median for four of the five different test sets. </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Rodney Nielsen</author>
<author>Chris Brew</author>
<author>Claudia Leacock</author>
<author>Danilo Giampiccolo</author>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3594" citStr="Dzikovska et al., 2013" startWordPosition="552" endWordPosition="555">d with up to 19 categories as in (Ahmad, 2009). In general, the finergrained assessments are more difficult to assess. 2 The Student Response Analysis Task The student response analysis task was posed as follows: Given a question, a known correct/reference answer and a 1 or 2 sentence student answer, classify the student answer into two, three or five categories. The two categories were correct and incorrect; the three categories were correct, contradictory and incorrect; while the five categories were correct, partially correct but incomplete, contradictory, irrelevant and not in the domain (Dzikovska et al., 2013). We chose to work on the 2-way response task only 603 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 603–607, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics because for our application, we need to simply know if a student answer is correct or incorrect. Our application is an interactive essay-based personalized learning environment (Bethard et al., 2012). The overarching goal of our application is to create a scalable online service that recommend</context>
<context position="9722" citStr="Dzikovska et al., 2013" startWordPosition="1559" endWordPosition="1562">8) toolkit within Eclipse to extract features from the student and reference sentences. We trained a LibSVM (Chang and Lin, 2011) binary classifier to classify a feature vector into two classes, correct or incorrect. We used the default parameters for LibSVM except for the cost parameter, for which we tried different values. However, the default value of 1 gave us the best result on the training set. Our two runs/systems are essentially the same system but with a cost parameter of 1 and 10. 4 Results The Student Response Analysis Task overall result can be found in the Task description paper (Dzikovska et al., 2013). The CU system achieved a ranking of above the mean and median for four of the five different test sets. We perfomed below the mean and median on the sciEntsBank unseen answers. The accuracy result for the test data is shown in Table 4. The results on our training data and a breakdown of the contribution of each feature is shown in Table 5. In Table 5 ALL refers to all the features while ALL-CosineSimilarity is all the features excluding the CosineSimilarity feature. Sys beetle beetle sciEnts sciEnts sciEnts tem un- un- Bank Bank Bank seen seen un- un- unan- ques- seen seen seen swers tions a</context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, Leacock, Giampiccolo, Bentivogli, Clark, Dagan, Dang, 2013</marker>
<rawString>Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew, Claudia Leacock, Danilo Giampiccolo, Luisa Bentivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang. 2013. Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge. In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics, Atlanta, Georgia, USA, 13-14 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Graesser</author>
<author>Shulan Lu</author>
<author>George Jackson</author>
<author>Heather Mitchell</author>
<author>Mathew Ventura</author>
<author>Andrew Olney</author>
<author>Max Louwerse</author>
</authors>
<title>AutoTutor: A tutor with dialogue in natural language.</title>
<date>2004</date>
<journal>Behavior Research Methods,</journal>
<pages>36--180</pages>
<contexts>
<context position="2785" citStr="Graesser et al., 2004" startWordPosition="422" endWordPosition="425">ver, assessing free text answers manually is tedious, expensive and time-consuming, hence the search for a computational option. There are three main issues that can limit the computational approach and corresponding performance when assessing free text answers: (1) the unit of assessment, (2) the reference and (3) the level of assessment. The unit of assessment can be words, facets, phrases, sentences, short answers or essays. The reference is the correct answer and what is being compared to the student answer. Most researchers generate the reference manually (Noorbehbahani and Kardan, 2011; Graesser et al., 2004) but some have focused on automatically generating the reference (Ahmad, 2009). The level of assessment can be coarse with 2 categories such as correct and incorrect or more finer-grained with up to 19 categories as in (Ahmad, 2009). In general, the finergrained assessments are more difficult to assess. 2 The Student Response Analysis Task The student response analysis task was posed as follows: Given a question, a known correct/reference answer and a 1 or 2 sentence student answer, classify the student answer into two, three or five categories. The two categories were correct and incorrect; t</context>
</contexts>
<marker>Graesser, Lu, Jackson, Mitchell, Ventura, Olney, Louwerse, 2004</marker>
<rawString>Arthur Graesser, Shulan Lu, George Jackson, Heather Mitchell, Mathew Ventura, Andrew Olney, and Max Louwerse. 2004. AutoTutor: A tutor with dialogue in natural language. Behavior Research Methods, 36:180–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>H James</author>
</authors>
<title>Speech and language processing an introduction to natural language processing, computational linguistics, and speech.</title>
<date>2000</date>
<contexts>
<context position="6374" citStr="Jurafsky and James, 2000" startWordPosition="1003" endWordPosition="1006">se that in our system because in our application we do not have access to that information. We were trying to build a system that would work in our current essay-based application. Some of the student answers in the dataset have a particular reference answer which they match. However, we do not make use of this information in our system either. We assume that for a particular question, all the corresponding reference answers can be used to determine the correctness of any of the student answers. 3.1 Features The features we use are: 1. CosineSimilarity : This is the average cosine similarity (Jurafsky and James, 2000) between a student answer vector and all the corresponding reference answer vectors. The vectors are based on word counts. The words were lowercased and included stopwords and punctuations. 2. CosineSimilarityNormalized : This is the average cosine similarity between a student answer vector and all the corresponding reference answer vectors, with the word counts within the vectors divided by the word counts in Gigaword, a background corpus. We divided the raw counts by the counts in Gigaword to ensure that punctuations, stopwords and other non-discriminatory words do not artificially increase </context>
</contexts>
<marker>Jurafsky, James, 2000</marker>
<rawString>Daniel Jurafsky and H James. 2000. Speech and language processing an introduction to natural language processing, computational linguistics, and speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Noorbehbahani</author>
<author>AA Kardan</author>
</authors>
<title>The automatic assessment of free text answers using a modified bleu algorithm.</title>
<date>2011</date>
<journal>Computers &amp; Education,</journal>
<volume>56</volume>
<issue>2</issue>
<contexts>
<context position="2761" citStr="Noorbehbahani and Kardan, 2011" startWordPosition="417" endWordPosition="421">one out of several options. However, assessing free text answers manually is tedious, expensive and time-consuming, hence the search for a computational option. There are three main issues that can limit the computational approach and corresponding performance when assessing free text answers: (1) the unit of assessment, (2) the reference and (3) the level of assessment. The unit of assessment can be words, facets, phrases, sentences, short answers or essays. The reference is the correct answer and what is being compared to the student answer. Most researchers generate the reference manually (Noorbehbahani and Kardan, 2011; Graesser et al., 2004) but some have focused on automatically generating the reference (Ahmad, 2009). The level of assessment can be coarse with 2 categories such as correct and incorrect or more finer-grained with up to 19 categories as in (Ahmad, 2009). In general, the finergrained assessments are more difficult to assess. 2 The Student Response Analysis Task The student response analysis task was posed as follows: Given a question, a known correct/reference answer and a 1 or 2 sentence student answer, classify the student answer into two, three or five categories. The two categories were </context>
</contexts>
<marker>Noorbehbahani, Kardan, 2011</marker>
<rawString>F Noorbehbahani and AA Kardan. 2011. The automatic assessment of free text answers using a modified bleu algorithm. Computers &amp; Education, 56(2):337–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip V Ogren</author>
<author>Philipp G Wetzler</author>
<author>Steven J Bethard</author>
</authors>
<title>Cleartk: A uima toolkit for statistical natural language processing. Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP,</title>
<date>2008</date>
<pages>32</pages>
<contexts>
<context position="9101" citStr="Ogren et al., 2008" startWordPosition="1453" endWordPosition="1456">mRefStudent feature, but we use part-of-speech unigrams for this feature in place of word unigrams. 10. UnigramPosStudentRef : This is similar to the UnigramStudentRef feature, but we use part-of-speech unigrams for this feature in place of word unigrams. 11. BigramPosRefStudent : This is similar to the BigramRefStudent feature, but we use part-ofspeech bigrams for this feature in place of word unigrams. 12. BigramPosStudentRef : This is similar to the BigramStudentRef feature, but we use part-ofspeech bigrams for this feature in place of word unigrams. 3.2 Implementation We used the ClearTK (Ogren et al., 2008) toolkit within Eclipse to extract features from the student and reference sentences. We trained a LibSVM (Chang and Lin, 2011) binary classifier to classify a feature vector into two classes, correct or incorrect. We used the default parameters for LibSVM except for the cost parameter, for which we tried different values. However, the default value of 1 gave us the best result on the training set. Our two runs/systems are essentially the same system but with a cost parameter of 1 and 10. 4 Results The Student Response Analysis Task overall result can be found in the Task description paper (Dz</context>
</contexts>
<marker>Ogren, Wetzler, Bethard, 2008</marker>
<rawString>Philip V Ogren, Philipp G Wetzler, and Steven J Bethard. 2008. Cleartk: A uima toolkit for statistical natural language processing. Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP, page 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Redecker</author>
<author>Yves Punie</author>
<author>Anusca Ferrari</author>
</authors>
<title>eassessment for 21st century learning and skills.</title>
<date>2012</date>
<booktitle>In 21st Century Learning for 21st Century Skills,</booktitle>
<pages>292--305</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1068" citStr="Redecker et al., 2012" startWordPosition="151" endWordPosition="154">ience University of Colorado Boulder, CO 80309, USA sumner@colorado.edu Abstract Assessing student understanding by evaluating their free text answers to posed questions is a very important task. However, manually, it is time-consuming and computationally, it is difficult. This paper details our shallow NLP approach to computationally assessing student free text answers when a reference answer is provided. For four out of the five test sets, our system achieved an overall accuracy above the median and mean. 1 Introduction Assessing student understanding is one of the holy grails of education (Redecker et al., 2012). If we (teachers, tutors, intelligent tutors, potential employers, parents and school administrators) know what and how much a student knows, then we know what the student still needs to learn. And then, can efficiently and effectively educate the student. However, the task of assessing what exactly a student understands about a particular topic can be expensive, difficult and subjective. Using multiple choice questionnaires is one of the most prevalent forms of assessing student understanding because it is easy and fast, both manually and computationally. However there has been a lot of push</context>
</contexts>
<marker>Redecker, Punie, Ferrari, 2012</marker>
<rawString>Christine Redecker, Yves Punie, and Anusca Ferrari. 2012. eassessment for 21st century learning and skills. In 21st Century Learning for 21st Century Skills, pages 292–305. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>