<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004045">
<title confidence="0.987707">
Stochastic Language Generation for Spoken Dialogue Systems
</title>
<author confidence="0.911411">
Alice H. Oh
</author>
<affiliation confidence="0.943379">
Carnegie Mellon University
</affiliation>
<address confidence="0.7452995">
5000 Forbes Ave.
Pittsburgh, PA 15213
</address>
<email confidence="0.990012">
aliceo+@cs.cmu.edu
</email>
<author confidence="0.64804">
Alexander I. Rudnicky
</author>
<affiliation confidence="0.71828">
Carnegie Mellon University
</affiliation>
<address confidence="0.5291825">
5000 Forbes Ave.
Pittsburgh, PA 15213
</address>
<email confidence="0.696496">
air+Ocs.cmu.edu
</email>
<sectionHeader confidence="0.987843" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913666666667">
The two current approaches to language
generation, template-based and rule-based
(linguistic) NLG, have limitations when
applied to spoken dialogue systems, in part
because they were developed for text
generation. In this paper, we propose a new
corpus-based approach to natural language
generation, specifically designed for spoken
dialogue systems.
</bodyText>
<sectionHeader confidence="0.960382" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999504132075472">
Several general-purpose rule-based generation
systems have been developed, some of which
are available publicly (cf. Elhadad, 1992).
Unfortunately these systems, because of their
generality, can be difficult to adapt to small,
task-oriented applications. B aternan and
Henschel (1999) have described a lower cost and
more efficient generation system for a specific
application using an automatically customized
subgrammar. Busernann and Horacek (1998)
describe a system that mixes templates and rule-
based generation. This approach takes
advantages of templates and rule-based
generation as needed by specific sentences or
utterances. Stent (1999) has proposed a similar
approach for a spoken dialogue system.
However, there is still the burden of writing and
maintaining grammar rules, and processing time
is probably too slow for sentences using
grammar rules (only the average time for
templates and rule-based sentences combined is
reported in Busemann and Horacek, 1998), for
use in spoken dialogue systems.
Because comparatively less effort is needed,
many current dialogue systems use template-
based generation. But there is one obvious
disadvantage: the quality of the output depends
entirely on the set of templates. Even in a
relatively simple domain, such as travel
reservations, the number of templates necessary
for reasonable quality can become quite large
that maintenance becomes a serious problem.
There is an unavoidable trade-off between the
amount of time and effort in creating and
maintaining templates and the variety and
quality of the output utterances.
Given these shortcomings of the above
approaches, we developed a corpus-based
generation system, in which we model language
spoken by domain experts performing the task of
interest, and use that model to stochastically
generate system utterances. We have applied this
technique to sentence realization and content
planning, and have incorporated the resulting
generation component into a working natural
dialogue system (see Figure 1). In this paper, we
describe the technique and report the results of
two evaluations.
We used two corpora in the travel
reservations domain to build n-gram language
models. One corpus (henceforth, the CMU
corpus) consists of 39 dialogues between a travel
agent and clients (Eskenazi, et al. 1999).
</bodyText>
<figure confidence="0.960046285714286">
Surface
Realization
Content
Planning
Sentence
Planning
Dialogue Manager Generation Engine
</figure>
<figureCaption confidence="0.99979">
Figure 1: Overall Architecture
</figureCaption>
<page confidence="0.937256">
27
</page>
<figure confidence="0.998714083333333">
query_arrive_city inform_airport
query_arrive_time inform_confirm_utterance
query_confirm inform_flight
query_depart_date inform_flight_another
query_depart_time inform_flight_earlier
query_pay_by_eard nform_flight_earliest
query_preferred_airport inform_flight_later
query_return_date inform_flight_latest
query return_time inform_not_avail
hoteLear_info inform_num_flights
hotel_hotel_chain inform_price
hotel_hotel_info other
</figure>
<figureCaption confidence="0.690709">
Figure 2 : utterance classes
</figureCaption>
<figure confidence="0.998664">
airline depart_date
arrive_airport depart_time
arrive:city — flight_num
arrive_date — hotel_city
arrivetime hotel_price
Car_Company name
car_price num_flights
depart_airport pm
depart_city price
</figure>
<figureCaption confidence="0.981508">
Figure 3 : word classes
</figureCaption>
<bodyText confidence="0.999189916666667">
Another corpus (henceforth, the SRI corpus)
consists of 68 dialogues between a travel agent
and users in the SRI community (Kowtko and
Price 1989).
The utterances in the two corpora were
tagged with utterance classes and word classes
(see Figure 2 and Figure 3). The CMU corpus
was manually tagged, and back-off trigram
models built (using Clarkson and Rosenfeld,
1997). These language models were used to
automatically tag the SRI corpus; the tags were
manually checked.
</bodyText>
<sectionHeader confidence="0.7496" genericHeader="method">
1 Content Planning
</sectionHeader>
<bodyText confidence="0.999980807692308">
In content planning we decide which attributes
(represented as word classes, see Figure 3)
should be included in an utterance. In a task-
oriented dialogue, the number of attributes
generally increases during the course of the
dialogue. Therefore, as the dialogue progresses,
we need to decide which ones to include at each
system turn. If we include all of them every time
(indirect echoing, see Hayes and Reddy, 1983),
the utterances become overly lengthy, but if we
remove all unnecessary attributes, the user may
get confused. With a fairly high recognition
error rate, this becomes an even more important
issue.
The problem, then, is to find a compromise
between the two. We compared two ways to
systematically generate system utterances with
only selected attributes, such that the user hears
repetition of some of the constraints he/she has
specified, at appropriate points in the dialogue,
without sacrificing naturalness and efficiency.
The specific problems, then, are deciding what
should be repeated, and when. We first describe
a simple heuristic of old versus new information.
Then we present a statistical approach, based on
bigram models.
</bodyText>
<subsectionHeader confidence="0.999324">
1.1 First approach: old versus new
</subsectionHeader>
<bodyText confidence="0.999951227272727">
As a simple solution, we can use the previous
dialogue history, by tagging the attribute-value
pairs as old (previously said by the system)
information or new (not said by the system yet)
information. The generation module would
select only new information to be included in the
system utterances. Consequently, information
given by the user is repeated only once in the
dialogue, usually in the utterance immediately
following the user utterance in which the new
information was given&apos;.
Although this approach seems to work fairly
well, echoing user&apos;s constraints only once may
not be the right thing to do. Looking at human-
human dialogues, we observe that this is not
very natural for a conversation; humans often
repeat mutually known information, and they
also often do not repeat some information at all.
Also, this model does not capture the close
relationship between two consecutive utterances
within a dialogue. The second approach tries to
address these issues.
</bodyText>
<subsectionHeader confidence="0.993232">
1.2 Second approach: statistical model
</subsectionHeader>
<bodyText confidence="0.971194416666667">
For this approach, we adopt the first of the two
sub-maxims in (Oberlander, 1998) &amp;quot;Do the
human thing&amp;quot;. Oberlander (1998) talks about
generation of referring expressions, but it is
universally valid, at least within natural
language generation, to say the best we can do is
I When the system utterance uses a template that does
not contain the slots for the new information given in
the previous user utterance, then that new
information will be confirmed in the next available
system utterance in which the template contains those
slots.
</bodyText>
<page confidence="0.993337">
28
</page>
<bodyText confidence="0.999893142857143">
to mimic human behavior. Hence, we built a
two-stage statistical model of human-human
dialogues using the CMU corpus. The model
first predicts the number of attributes in the
system utterance given the utterance class, then
predicts the attributes given the attributes in the
previous user utterance.
</bodyText>
<subsubsectionHeader confidence="0.610434">
1.2.1 The number of attributes model
</subsubsectionHeader>
<bodyText confidence="0.9966515">
The first model will predict the number of
attributes in a system utterance given the
utterance class. The model is the probability
distribution P(nk) = P(nklek), where nk is the
number of attributes and ck is the utterance class
for system utterance k.
</bodyText>
<subsubsectionHeader confidence="0.998877">
1.2.2 The bigram model of the attributes
</subsubsectionHeader>
<bodyText confidence="0.996623636363636">
This model will predict which attributes to use
in a system utterance. Using a statistical model,
what we need to do is find the set of attributes
A* = ai, az, •.., an} such that
A * = arg max 11 P(ai, az, an)
We assume that the distributions of the ai&apos;s
are dependent on the attributes in the previous
utterances. As a simple model, we look only at
the utterance immediately preceding the current
utterance and build a bigram model of the
attributes. In other words, A* = arg max P(AIB),
where B = 1)1, b2, b.}, the set of m
attributes in the preceding user utterance.
If we took the above model and tried to
apply it directly, we would run into a serious
data sparseness problem, so we make two
independence assumptions. The first assumption
is that the attributes in the user utterance
contribute independently to the probabilities of
the attributes in the system utterance following
it. Applying this assumption to the model above,
we get the following:
</bodyText>
<equation confidence="0.668186">
A * arg max P(bk)P(A I bk)
k=1
</equation>
<bodyText confidence="0.99996625">
The second independence assumption is that
the attributes in the system utterance are
independent of each other. This gives the final
model that we used for selecting the attributes.
</bodyText>
<equation confidence="0.9383245">
A*. = arg max E.)(,,,k)ri P(ai I bk)
1c=1 i=1
</equation>
<bodyText confidence="0.99988675">
Although this independence assumption is
an oversimplification, this simple model is a
good starting point for our initial
implementation of this approach.
</bodyText>
<sectionHeader confidence="0.944074" genericHeader="method">
2 Stochastic Surface Realization
</sectionHeader>
<bodyText confidence="0.992824631578947">
We follow Busemann and Horacek (1998) in
designing our generation engine with &amp;quot;different
levels of granularity.&amp;quot; The different levels
contribute to the specific needs of the various
utterance classes. For example, at the beginning
of the dialogue., a system greeting can be simply
generated by a &amp;quot;canned&amp;quot; expression. Other short,
simple utterances can be generated efficiently by
templates. In Busemann and Horacek (1998), the
remaining output is generated by grammar rules.
We replace the generation grammar with a
simple statistical language model to generate
more complex utterances.
There are four aspects to our stochastic
surface realizer: building language models,
generating candidate utterances, scoring the
utterances, and filling in the slots. We explain
.
each of these below.
</bodyText>
<subsectionHeader confidence="0.997815">
2.1 Building Language Models
</subsectionHeader>
<bodyText confidence="0.9999428">
Using the tagged utterances as described in
the introduction, we built an unsmoothed n-gram
language model for each utterance class. Tokens
that belong in word classes (e.g., &amp;quot;U.S.
Airways&amp;quot; in class &amp;quot;airline&amp;quot;) were replaced by the
word classes before building the language
models. We selected 5 as the n in n-gram to
introduce some variability in the output
utterances while preventing nonsense utterances.
Note that language models are not used here
in the same way as in speech recognition. In
speech recognition, the language model
probability acts as a &apos;prior&apos; in determining the
most probable sequence of words given the
acoustics. In other words,
</bodyText>
<equation confidence="0.614953">
W&apos; = arg max P(W1A)
</equation>
<bodyText confidence="0.991409428571429">
= arg max P(Al W)Pr(W)
where W is the string of words, w„, and
A is the acoustic evidence (Jelinek 1998).
Although we use the same statistical tool,
we compute and use the language model
probability directly to predict the next word. In
other words, the most likely utterance is W* =
</bodyText>
<page confidence="0.996966">
29
</page>
<bodyText confidence="0.9999696">
arg max P(W1u), where u is the utterance class.
We do not, however, look for the most likely
hypothesis, but rather generate each word
randomly according to the distribution, as
illustrated in the next section.
</bodyText>
<subsectionHeader confidence="0.998725">
2.2 Generating Utterances
</subsectionHeader>
<bodyText confidence="0.998295333333333">
The input to NLG from the dialogue
manager is a frame of attribute-value pairs. The
first two attribute-value pairs specify the
utterance class. The rest of the frame contains
word classes and their values. Figure 4 is an
example of an input frame to NLG.
</bodyText>
<figure confidence="0.953057">
act-query
content depart_time
depart_city New York
arrive_oity San Francisco
depart_date 19991117
</figure>
<figureCaption confidence="0.999898">
Figure 4: an input frame to NLG
</figureCaption>
<bodyText confidence="0.999929166666667">
The generation engine uses the appropriate
language model for the utterance class and
generates word sequences randomly according
to the language model distributions. As in
speech recognition, the probability of a word
using the n-gram language model. is
</bodyText>
<equation confidence="0.613248">
Kw&apos;) := P(WilW1-1 W1-2 • • - 111)
</equation>
<bodyText confidence="0.999983">
where u is the utterance class. Since we have
built separate models for each of the utterance
classes, we can ignore u, and say that
</bodyText>
<equation confidence="0.923746">
P(w) = P(W11WI-15 WI-2, • • • Wi.(11-1))
</equation>
<bodyText confidence="0.9997298">
using the language model for u.
Since we use unsmoothed 5-grams, we will
not generate any unseen 5-grams (or smaller n-
grams at the beginning and end of an utterance).
This precludes generation of nonsense
utterances, at least within the 5-word window.
Using a smoothed n-gram would result in more
randomness, but using the conventional back-off
methods (Jelinek 1998), the probability mass
assigned to unseen 5-grams would be very
small, and those rare occurrences of unseen n-
grams may not make sense anyway. There is the
problem, as in speech recognition using n-gram
language models, that long-distance dependency
cannot be captured.
</bodyText>
<subsectionHeader confidence="0.99939">
2.3 Scoring Utterances -
</subsectionHeader>
<bodyText confidence="0.999647125">
For each randomly generated utterance, we
compute a penalty score. The score is based on
the heuristics we&apos;ve empirically selected.
Various penalty scores are assigned for an
utterance that 1. is too short or too long
(determined by utterance-class dependent
thresholds), 2. contains repetitions of any of the
slots, 3. contains slots for which there is no valid
value in the frame, or 4. does not have some
required slots (see section 2 for deciding which
slots are required).
The generation engine generates a candidate
utterance, scores it, keeping only the best-scored
utterance up to that point. It stops and returns the
best utterance when it finds an utterance with a
zero penalty score,-or runs out of time.
</bodyText>
<subsectionHeader confidence="0.999042">
2.4 Filling Slots
</subsectionHeader>
<bodyText confidence="0.9987682">
The last step is filling slots with the appropriate
values. For example, the utterance &amp;quot;What time
would you like to leave depart_city }?&amp;quot;
becomes &amp;quot;What time would you like to leave
New York?&amp;quot;.
</bodyText>
<sectionHeader confidence="0.996642" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9999673125">
It is generally difficult to empirically evaluate a
generation system. In the context of spoken
dialogue systems, evaluation of NLG becomes
an even more difficult problem. One reason is
simply that there has been very little effort in
building generation engines for spoken dialogue
systems. Another reason is that it is hard to
separate NLG from the rest of the system. It is
especially hard to separate evaluation of
language generation and speech synthesis.
As a simple solution, we have conducted a
comparative evaluation by running two identical
systems varying only the generation component.
In this section we present results from two
preliminary evaluations of our generation
algorithms described in the previous sections.
</bodyText>
<subsectionHeader confidence="0.989987">
3.1 Content Planning: Experiment
</subsectionHeader>
<bodyText confidence="0.999966666666667">
For the content planning part of the generation
system, we conducted a comparative evaluation
of the two different generation algorithms:
old/new and bigrams. Twelve subjects had two
dialogues each, one with the old/new generation
system, and another with the bigrams generation
</bodyText>
<page confidence="0.993769">
30
</page>
<bodyText confidence="0.999729571428571">
system (in counterbalanced order); all other
modules were held fixed. Afterwards, each
subject answered seven questions on a usability
survey. Immediately after, each subject was
given transcribed logs of his/her dialogues and
asked to rate each system utterance on a scale of
I to 3 (1 good; 2 okay; 3 = bad).
</bodyText>
<subsectionHeader confidence="0.9992">
3.2 Content Planning: Results
</subsectionHeader>
<bodyText confidence="0.999975625">
For the usability survey, the results seem to
indicate subjects&apos; preference for the old/new
system, but the difference is not statistically
significant (p = 0.06). However, six out of the
twelve subjects chose the bigram system to the
question &amp;quot;Duffingi the session, which system&apos;s
responses were easier to understand?&amp;quot; compared
to three subjects choosing the old/new system.
</bodyText>
<subsectionHeader confidence="0.991288">
3.3 Surface Realization: Experiment
</subsectionHeader>
<bodyText confidence="0.999968764705882">
For surface realization, we conducted a batch-
mode evaluation. We picked six recent calls to
our system and ran two generation algorithms
(template-based generation and stochastic
generation) on the input frames. We then
presented to seven subjects the generated
dialogues, consisting of decoder output of the
user utterances and corresponding system
responses, for each of the two generation
algorithms. Subjects then selected the output
utterance they would prefer, for each of the
utterances that differ between the two systems.
The results show a trend that subjects preferred
stochastic generation over template-based
generation, but a t-test shows no significant
difference (p = 0.18). We are in the process of
designing a larger evaluation.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999988833333333">
We have presented a new approach to language
generation for spoken dialogue systems. For
content planning, we built a simple bigram
model of attributes, and found that, in our first
implementation, it performs as well as a
heuristic of old vs. new information. For surface
realization, we used an n-gram language model
to stochastically generate each utterance and
found that the stochastic system performs at
least as well as the template-based system.
Our stochastic generation system has several
advantages. One of those, an important issue for
spoken dialogue systems, is the response time.
With stochastic surface realization, the average
generation time for the longest utterance class
(10 — 20 words long) is about 200 milliseconds,
which is much faster than any rule-based
systems. Another advantage is that by using a
corpus-based approach, we are directly
mimicking the language of a real domain expert,
rather than attempting to model it by rule.
Corpus collection is usually the first step in
building a dialogue system, so we are leveraging
the effort rather than creating more work. This
also means adapting this approach to new
domains and even new languages will be
relatively simple.
The approach we present does require some
amount of knowledge engineering, though this
appears to overlap with work needed for other
parts of the dialogue system. First, defining the
class of utterance and the attribute-value pairs
requires care. Second, tagging the human-human
corpus with the right classes and attributes
requires effort, However, we believe the tagging
effort is much less difficult than knowledge
acquisition for most rule-based systems or even
template-based systems. Finally, what may
sound right for a human speaker may sound
awkward for a computer, but we believe that
mimicking a human, especially a domain expert,
is the best we can do, at least for now.
</bodyText>
<sectionHeader confidence="0.996004" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999951285714286">
We are thankful for significant contribution by
other members of the CMU Communicator
Project, especially Eric Thayer, Wei Xu, and
Rande Shern. We would like to thank the
subjects who participated in our evaluations. We
also extend our thanks to two anonymous
reviewers.
</bodyText>
<sectionHeader confidence="0.999461" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999000888888889">
Bateman, J. and Henschel, R. (1999) From full
generation to &apos;near-templates&apos; without losing
generality. In Proceedings of the KI&apos;99 workshop,
&amp;quot;May I Speak Freely?&amp;quot;
Busemann, S. and Horacek, H. (1998) A flexible
shallow approach to text generation. In
Proceedings of the International Natural Language
Generation Workshop. Niagara-on-the-Lake,
Canada.
</reference>
<page confidence="0.999913">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.906908">
<title confidence="0.999863">Stochastic Language Generation for Spoken Dialogue Systems</title>
<author confidence="0.998291">H Alice</author>
<affiliation confidence="0.996598">Carnegie Mellon</affiliation>
<address confidence="0.9812005">5000 Forbes Pittsburgh, PA</address>
<email confidence="0.998929">aliceo+@cs.cmu.edu</email>
<author confidence="0.997313">I Alexander</author>
<affiliation confidence="0.996357">Carnegie Mellon</affiliation>
<address confidence="0.981497">5000 Forbes Pittsburgh, PA</address>
<email confidence="0.999197">air+Ocs.cmu.edu</email>
<abstract confidence="0.99909">The two current approaches to language generation, template-based and rule-based (linguistic) NLG, have limitations when applied to spoken dialogue systems, in part because they were developed for text generation. In this paper, we propose a new corpus-based approach to natural language generation, specifically designed for spoken dialogue systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bateman</author>
<author>R Henschel</author>
</authors>
<title>From full generation to &apos;near-templates&apos; without losing generality.</title>
<date>1999</date>
<booktitle>In Proceedings of the KI&apos;99 workshop,</booktitle>
<marker>Bateman, Henschel, 1999</marker>
<rawString>Bateman, J. and Henschel, R. (1999) From full generation to &apos;near-templates&apos; without losing generality. In Proceedings of the KI&apos;99 workshop, &amp;quot;May I Speak Freely?&amp;quot;</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Busemann</author>
<author>H Horacek</author>
</authors>
<title>A flexible shallow approach to text generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Natural Language Generation Workshop.</booktitle>
<location>Niagara-on-the-Lake, Canada.</location>
<contexts>
<context position="1605" citStr="Busemann and Horacek, 1998" startWordPosition="219" endWordPosition="222">em for a specific application using an automatically customized subgrammar. Busernann and Horacek (1998) describe a system that mixes templates and rulebased generation. This approach takes advantages of templates and rule-based generation as needed by specific sentences or utterances. Stent (1999) has proposed a similar approach for a spoken dialogue system. However, there is still the burden of writing and maintaining grammar rules, and processing time is probably too slow for sentences using grammar rules (only the average time for templates and rule-based sentences combined is reported in Busemann and Horacek, 1998), for use in spoken dialogue systems. Because comparatively less effort is needed, many current dialogue systems use templatebased generation. But there is one obvious disadvantage: the quality of the output depends entirely on the set of templates. Even in a relatively simple domain, such as travel reservations, the number of templates necessary for reasonable quality can become quite large that maintenance becomes a serious problem. There is an unavoidable trade-off between the amount of time and effort in creating and maintaining templates and the variety and quality of the output utterance</context>
<context position="9050" citStr="Busemann and Horacek (1998)" startWordPosition="1361" endWordPosition="1364">the probabilities of the attributes in the system utterance following it. Applying this assumption to the model above, we get the following: A * arg max P(bk)P(A I bk) k=1 The second independence assumption is that the attributes in the system utterance are independent of each other. This gives the final model that we used for selecting the attributes. A*. = arg max E.)(,,,k)ri P(ai I bk) 1c=1 i=1 Although this independence assumption is an oversimplification, this simple model is a good starting point for our initial implementation of this approach. 2 Stochastic Surface Realization We follow Busemann and Horacek (1998) in designing our generation engine with &amp;quot;different levels of granularity.&amp;quot; The different levels contribute to the specific needs of the various utterance classes. For example, at the beginning of the dialogue., a system greeting can be simply generated by a &amp;quot;canned&amp;quot; expression. Other short, simple utterances can be generated efficiently by templates. In Busemann and Horacek (1998), the remaining output is generated by grammar rules. We replace the generation grammar with a simple statistical language model to generate more complex utterances. There are four aspects to our stochastic surface r</context>
</contexts>
<marker>Busemann, Horacek, 1998</marker>
<rawString>Busemann, S. and Horacek, H. (1998) A flexible shallow approach to text generation. In Proceedings of the International Natural Language Generation Workshop. Niagara-on-the-Lake, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>