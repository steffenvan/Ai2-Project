<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000332">
<title confidence="0.9959275">
Retrieving Meaning-equivalent Sentences
for Example-based Rough Translation
</title>
<author confidence="0.90894">
Yuji Matsumoto
</author>
<affiliation confidence="0.7667225">
Nara Institute of
Science and Technology
</affiliation>
<email confidence="0.979011">
matsu@is.aist-nara.ac.jp
</email>
<title confidence="0.401305">
Mitsuo Shimohata Eiichiro Sumita
</title>
<author confidence="0.560405">
ATR Spoken Language Translation
Research Laboratories
</author>
<email confidence="0.9721725">
mitsuo.shimohata@atr.co.jp
eiichiro.sumita@atr.co.jp
</email>
<sectionHeader confidence="0.995606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999163722222222">
Example-based machine translation (EBMT)
is a promising translation method for speech-
to-speech translation because of its robust-
ness. It retrieves example sentences similar to
the input and adjusts their translations to ob-
tain the output. However, it has problems in
that the performance degrades when input sen-
tences are long and when the style of inputs
and that of the example corpus are different.
This paper proposes a method for retrieving
“meaning-equivalent sentences” to overcome
these two problems. A meaning-equivalent
sentence shares the main meaning with an in-
put despite lacking some unimportant informa-
tion. The translations of meaning-equivalent
sentences correspond to “rough translations.”
The retrieval is based on content words, modal-
ity, and tense.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99985841509434">
Speech-to-speech translation (S2ST) technologies con-
sist of speech recognition, machine translation (MT), and
speech synthesis (Waibel, 1996; Wahlster, 2000; Ya-
mamoto, 2000). The MT part receives speech texts rec-
ognized by a speech recognizer. The nature of speech
causes difficulty in translation since the styles of speech
are different from those of written text and are sometimes
ungrammatical (Lazzari, 2002). Therefore, rule-based
MT cannot translate speech accurately compared with its
performance for written-style text.
Example-based MT (EBMT) is one of the corpus-
based machine translation methods. It retrieves examples
similar to inputs and adjusts their translations to obtain
the output (Nagao, 1981). EBMT is a promising method
for S2ST in that it performs robust translation of ungram-
matical sentences and requires far less manual work than
rule-based MT.
However, there are two problems in applying EBMT
to S2ST. One is that the translation accuracy drastically
drops as input sentences become long. As the length of
a sentence becomes long, the number of retrieved similar
sentences greatly decreases. This often results in no out-
put when translating long sentences. The other problem
arises due to the differences in style between input sen-
tences and the example corpus. It is difficult to acquire
a large volume of natural speech data since it requires
much time and cost. Therefore, we cannot avoid using a
corpus with written-style text, which is different from that
of natural speech. This style difference makes retrieval of
similar sentences difficult and degrades the performance
of EBMT.
This paper proposes a method of retrieving sentences
whose meaning is equivalent to input sentences to over-
come the two problems. A meaning-equivalent sentence
means a sentence having the main meaning of an input
sentence despite lacking some unimportant information.
Such a sentence can be more easily retrieved than a simi-
lar sentence, and its translation is useful enough in S2ST.
We call this translation strategy example-based “rough
translation.”
Retrieval of meaning-equivalent sentences is based on
content words, modality, and tense. This provides robust-
ness against long inputs and in the differences in style be-
tween the input and the example corpus. This advantage
distinguishes our method from other translation methods.
We describe the difficulties in S2ST in Section 2. Then,
we describe our purpose, features for retrieval, and re-
trieval method for meaning-equivalent sentences in Sec-
tion 3. We report an experiment comparing our method
with two other methods in Section 4. The experiment
demonstrates the robustness of our method to length of
input and the style differences between inputs and the ex-
ample corpus.
</bodyText>
<figure confidence="0.957368095238096">
Language
English Japanese
Concise 5.4 6.2
Conversational 7.9 8.9
Table 1: Number of Words by Sentences
Language Model
Concise Conversational
Test Concise 16.4 58.3
Conversational 72.3 16.3
Table 2: Cross Perplexity
Number of Sentences
100
40
80
60
20
0
Untranslated
Translated
2-5 6-10 11-15 16-
Sentence Length (Words)
</figure>
<figureCaption confidence="0.999991">
Figure 1: Distribution of Untranslated Inputs by Length
</figureCaption>
<sectionHeader confidence="0.869493" genericHeader="introduction">
2 Difficulty in Example-based S2ST
</sectionHeader>
<subsectionHeader confidence="0.937242">
2.1 Translation Degradation by Input Length
</subsectionHeader>
<bodyText confidence="0.999980666666667">
A major problem with machine translation, regardless of
the translation method, is that performance drops rapidly
as input sentences become longer. For EBMT, the longer
input sentences become, the fewer similar example sen-
tences exist in the example corpus. Figure 1 shows
translation difficulty in long sentences in EBMT (Sumita,
2001). The EBMT system is given 591 test sentences
and returns translation result as translated/untranslated.
Untranslated means that there exists no similar example
sentences for the input. Although the EBMT is equipped
with a large example corpus (about 170K sentences), it
often failed to translate long inputs.
</bodyText>
<subsectionHeader confidence="0.9968175">
2.2 Style Differences between Concise and
Conversational
</subsectionHeader>
<bodyText confidence="0.999939483870968">
The performance of example-based S2ST greatly de-
pends on the example corpus. It is advantageous for an
example corpus to have a large volume and the same
style as the input sentences. A corpus of texts dictated
from conversational speech is favorable for S2ST. Un-
fortunately, it is very difficult to prepare such an exam-
ple corpus since this task requires laborious work such as
speech recording and speech transcription.
Therefore, we cannot avoid using a written-style cor-
pus, such as phrasebooks, to prepare a sufficiently large
volume of examples. Contained texts are almost gram-
matical and rarely contain unnecessary words. We call
the style used in such a corpus “concise” and the style
seen in conversational speech “conversational.”
Table 1 shows the average numbers of words in con-
cise (Takezawa et al., 2002) and conversational corpora
(Takezawa, 1999). Sentences in conversational style are
about 2.5 words longer than those in concise style in both
English and Japanese. This is because conversational
style sentences contain unnecessary words or subordinate
clauses, which have the effects of assisting the listener’s
comprehension and avoiding the possibility of giving the
listener a curt impression.
Table 2 shows cross perplexity between concise and
conversational corpora (Takezawa et al., 2002). Perplex-
ity is used as a metric for how well a language model
derived from a training set matches a test set (Jurafsky
and Martin, 2000). Cross perplexities between concise
and conversational corpora are much higher than the self-
perplexity of either of the two styles. This result also
illustrates the great difference between the two styles.
</bodyText>
<sectionHeader confidence="0.993095" genericHeader="method">
3 Meaning-equivalent Sentence
</sectionHeader>
<bodyText confidence="0.999136523809524">
Example-based S2ST has the difficulties described in
Section 2 when it attempts to translate inputs exactly.
Here, we set our translation goal to translating input sen-
tences not exactly but roughly. We assume that a rough
translation is useful enough for S2ST, since unimportant
information rarely disturbs the progress of dialogs and
can be recovered in the following dialog if needed. We
call this translation strategy “rough translation.”
We propose “meaning-equivalent sentence” to carry
out rough translation. Meaning-equivalent sentences are
defined as follows:
meaning-equivalent sentence
(to an input sentence)
A sentence that shares the main meaning with
the input sentence despite lacking some unim-
portant information. It does not contain infor-
mation additional to that in the input sentence.
Important information is subjectively recognized
mainly due to one of two reasons: (1) It can be surmised
from the general situation, or (2) It does not place a strong
restriction on the main information.
</bodyText>
<figure confidence="0.783430285714286">
Input Sentence Unimportant?
1 Would you take a picture of me? Yes
2 Would you take a picture of this painting? No
3 Could you tell me a Chinese restaurant around here? Yes
4 Could you tell me a Chinese restaurant around here? No
5 My baggage was stolen from my room while I was out. Yes
6 Please change my room because the room next door is noisy. Yes
</figure>
<figureCaption confidence="0.99976">
Figure 2: Examples of Unimportant Information
</figureCaption>
<bodyText confidence="0.993645375">
Figure 2 shows examples of unimportant/important in-
formation. Information to be examined is written in bold.
The information “of me” in (1) and “around here” in (3)
can be surmised from the general situation, while the in-
formation “of this painting” in (2) and “Chinese” would
not be surmised since it denotes a special object. The sub-
ordinate sentences in (4) and (5) are regarded as unimpor-
tant since they have small significance and are omittable.
</bodyText>
<subsectionHeader confidence="0.999185">
3.1 Basic Idea of Retrieval
</subsectionHeader>
<bodyText confidence="0.999975708333333">
The retrieval of meaning-equivalent sentence depends on
content words and basically does not depend on func-
tional words. Independence from functional words brings
robustness to the difference in styles.
However, functional words include important informa-
tion for sentence meaning: the case relation of content
words, modality, and tense. Lack of case relation infor-
mation is compensated by the nature of the restricted do-
main. A restricted domain, as a domain of S2ST, has a
relatively small lexicon and meaning variety. Therefore,
if content words included in an input are given, their re-
lation is almost determined in the domain. Information
of modality and tense is extracted from functional words
and utilized in classifying the meaning of a sentence (de-
scribed in Section 3.2.2).
This retrieval method is similar to information re-
trieval in that content words are used as clues for retrieval
(Frakes and Baeza-Yates, 1992). However, our task has
two difficulties: (1) Retrieval is carried out not by docu-
ments but by single sentences. This reduces the effective-
ness of word frequencies. (2) The differences in modality
and tense in sentences have to be considered since they
play an important role in determining a sentence’s com-
municative meaning.
</bodyText>
<sectionHeader confidence="0.70879" genericHeader="method">
3.2 Features for Retrieval
3.2.1 Content Words
</sectionHeader>
<bodyText confidence="0.6054715">
Words categorized as either noun1, adjective, adverb,
or verb are recognized as content words. Interrogatives
</bodyText>
<footnote confidence="0.634053">
1Number and pronoun are included.
</footnote>
<table confidence="0.999751916666667">
Modality Clues
Request tekudasai (auxiliary verb)
teitadakeru (auxiliary verb)
Desire shi-tai (expression)
te-hoshii (expression)
negau (verb)
Question ka (final particle)
ne (final particle)
Negation nai (auxiliary verb or adjective)
masen (auxiliary verb)
Tense Clues
Past ta (auxiliary verb)
</table>
<tableCaption confidence="0.993297">
Table 3: Clues for Discriminating Modalities in Japanese
</tableCaption>
<bodyText confidence="0.994225714285714">
are also included. Words such as particles, auxiliary
verbs, conjunctions, and interjections are recognized as
functional words.
We utilize a thesaurus to expand the coverage of the
example corpus. We call the relation of two words that
are the same “identical” and words that are synonymous
in the given thesaurus “synonymous.”
</bodyText>
<subsectionHeader confidence="0.742482">
3.2.2 Modality and Tense
</subsectionHeader>
<bodyText confidence="0.999826">
The meaning of a sentence is discriminated by its
modality and tense, since these factors obviously deter-
mine meaning. We defined two modality groups and
one tense group by examining our corpus. The modal-
ity groups are (“request”, “desire”, “question”, “confir-
mation”, “others”,) and (“negation”, “others”.) The tense
group is (“past”, “others”.) These modalities and tense
are distinguished by surface clues, mainly by particles
and auxiliary verbs. Table 3 shows a part of the clues
used for discriminating modalities in Japanese. Sentences
having no clues are classified as others. Figure 3 2 shows
</bodyText>
<footnote confidence="0.8851015">
2Japanese content words are written in sans serif style and
Japanese functional words in italic style.
</footnote>
<table confidence="0.9981499">
Sentence3 Modality &amp;
Tense4
hoteru wo yoyaku shi tekudasai request
(Will you reserve this hotel?)
hoteru wo yoyaku shi tai desire
(I want to reserve this hotel.)
hoteru wo yoyaku shi mashi ta ka? question
(Did you reserve this hotel?) past
hoteru wo yoyaku shi tei masen negation
(I do not reserve this hotel.)
</table>
<figureCaption confidence="0.997584">
Figure 3: Sentences and their Modality and Tense
</figureCaption>
<bodyText confidence="0.999450454545455">
sample sentences and their modality and tense. Clues are
underlined.
A speech act is a concept similar to modality in which
speakers’ intentions are represented. The two studies in-
troduced information of the speech act in their S2ST sys-
tems (Wahlster, 2000; Tanaka and Yokoo, 1999). The two
studies and our method differ in the effect of speech act
information. Their effect of speech act information is so
small that it is limited to generating the translation text.
Translation texts are refined by selecting proper expres-
sions according to the detected speakers’ intention.
</bodyText>
<subsectionHeader confidence="0.989804">
3.3 Retrieval and Ranking
</subsectionHeader>
<bodyText confidence="0.9910115">
Sentences that satisfy the conditions below are recog-
nized as meaning-equivalent sentences.
</bodyText>
<listItem confidence="0.9989415">
1. It is required to have the same modality and tense as
the input sentence.
2. All content words are included (identical or synony-
mous) in the input sentence. This means that the set
of content words of a meaning-equivalent sentence
is a subset of the input.
3. At least one content word is included (identical) in
the input sentence.
</listItem>
<bodyText confidence="0.999105888888889">
If more than one sentence is retrieved, we must rank
them to select the most similar one. We introduce “focus
area” in the ranking process to select sentences that are
meaning-equivalent to the main sentence in complex sen-
tences. We set the focus area as the last N words from the
word list of an input sentence. N denotes the number of
content words in meaning-equivalent sentences. This is
because main sentences in complex sentences tend to be
placed at the end in Japanese.
</bodyText>
<footnote confidence="0.997793666666667">
3Space characters are inserted into word boundaries in
Japanese texts.
4The value “others” in all modality/tense groups is omitted.
</footnote>
<table confidence="0.761345071428571">
Input
gaishutsu shi teiru aida ni,
(While I was out),
kaban wo nusuma re mashi ta
(my baggage was stolen.)
Meaning-equivalent Sentence
baggu wo nusuma re ta
(My bag was stolen).
C1 nusumu5 1
C2 (kaban = baggu) 1
C3 - 0
C4 - 0
C5 wo, re, ta 3
C6 suru, teiru, ni, masu 4
</table>
<figureCaption confidence="0.999576">
Figure 4: Example of Conditions for Ranking
</figureCaption>
<bodyText confidence="0.9510188">
Retrieved sentences are ranked by the conditions de-
scribed below. Conditions are described in order of prior-
ity. If there is more than one sentence having the highest
score under these conditions, the most similar sentence is
selected randomly.
</bodyText>
<listItem confidence="0.9691255">
C1: # of identical words in focus area.
C2: # of synonymous words in focus area.
C3: # of identical words in non-focus area.
C4: # of synonymous words in non-focus area.
C5: # of common functional words.
C6: # of different functional words.
</listItem>
<bodyText confidence="0.9809845">
(the fewer, the higher priority)
Figure 4 shows an example of conditions for ranking.
Content word in a focus area of input are underlined and
functional words are written in italic.
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999102">
4.1 Test Data
</subsectionHeader>
<bodyText confidence="0.999795909090909">
We used a bilingual corpus of travel conversation, which
has Japanese sentences and their English translations
(Takezawa et al., 2002). This corpus was sentence-
aligned, and a morphological analysis was done on both
languages by our morphological analysis tools. The bilin-
gual corpus was divided into example data (Example) and
test data (Concise) by extracting test data randomly from
the whole set of data.
In addition to this, we used a conversational speech
corpus for another set of test data (Takezawa, 1999). This
corpus contains dialogs between a traveler and a hotel
</bodyText>
<footnote confidence="0.960681">
5Words are converted to base form.
</footnote>
<note confidence="0.843765">
Method-1 Method-2 Proposed
</note>
<figure confidence="0.965667">
(Strict with Func. ) (Rough with Func.) (Rough w/o Func.)
Input Length (Words) Input Length (Words) Input Length (Words)
</figure>
<figureCaption confidence="0.97328">
Figure 5: Results
</figureCaption>
<figure confidence="0.989579384615385">
Accuracy (%)
100
40
20
80
60
0
1-5 6-10 11-15 16-
Accuracy (%)
100
40
20
80
60
0
1-5 6-10 11-15 16-
1-5 6-10 11-15 16-
Accuracy (%)
100
40
20
80
60
0
Conversational
Concise
</figure>
<table confidence="0.8317304">
Corpus # of Average
Sentences Length
Example 92,397 7.4
Concise 1,588 6.6
Conversational 800 10.1
</table>
<tableCaption confidence="0.996532">
Table 4: Statistics of the Corpora
</tableCaption>
<bodyText confidence="0.9990761">
receptionist. It tests the robustness in styles. We call this
test corpus “Conversational.”
We use sentences including more than one content
word among the three corpora. The statistics of the three
corpora are shown in Table 4.
The thesaurus used in the experiment was “Kadokawa-
Ruigo-Jisho” (Ohno and Hamanishi, 1984). Each word
has semantic code consisting of three digits, that is, this
thesaurus has three hierarchies. We defined “synony-
mous” words as sharing exact semantic codes.
</bodyText>
<subsectionHeader confidence="0.997688">
4.2 Compared Retrieval Methods
</subsectionHeader>
<bodyText confidence="0.999952444444445">
We use two example-based retrieval methods to show the
characteristic of the proposed method. The first method
(Method-1) uses “strict” retrieval, which does not al-
low missing words in input. The method takes func-
tional words into account on retrieval. This method cor-
responds to the conventional EBMT method. The second
method (Method-2) uses “rough” retrieval, which does
allow missing words in input, but still takes functional
words into account.
</bodyText>
<subsectionHeader confidence="0.99488">
4.3 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999932666666667">
Evaluation was carried out by judging whether retrieved
sentences are meaning-equivalent to inputs. It must be
noted that inputs and retrieved sentences are both in
Japanese. We did not compare inputs and translations of
retrieved sentences, since translation accuracy is a matter
of the example corpus and does not concern our method.
The sentence with the highest score among retrieved
sentences was taken and evaluated. The sentences are
marked manually as meaning-equivalent or not by a
Japanese native. A meaning-equivalent sentence includes
all important information in the input but may lack some
unimportant information.
</bodyText>
<subsectionHeader confidence="0.822382">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999973391304348">
Figure 5 shows the accuracy of the three methods with
the concise and conversational style data. Accuracy is
defined as the ratio of the number of correctly equivalent
sentences to that of total inputs. Inputs are classified into
four types by their word length.
The performance of Method-1 reflects the narrow cov-
erage and style-dependency of conventional EBMT. The
longer input sentences become, the more steeply its per-
formance degrades in both styles. The method can re-
trieve no similar sentence for inputs longer than eleven
words in conversational style.
Method-2 adopts a “rough” strategy in retrieval. It
attains higher accuracy than Method-1, especially with
longer inputs. This indicates the robustness of the rough
retrieval strategy to longer inputs. However, the method
still has an accuracy difference of about 15% between the
two styles.
The accuracy of the proposed method is better than
that of Method-2, especially in conversational style. The
accuracy difference in longer inputs becomes smaller
(about 4%) than that of Method-2. This indicates the ro-
bustness of the proposed method to the differences be-
tween the two styles.
</bodyText>
<sectionHeader confidence="0.816858" genericHeader="method">
5 Related Work
5.1 EBMT
</sectionHeader>
<bodyText confidence="0.999958428571429">
The rough translation proposed in this paper is a type of
EBMT (Sumita, 2001; Veale and Way, 1997; Carl, 1999;
Brown, 2000). The basic idea of EBMT is that sentences
similar to the inputs are retrieved from an example corpus
and their translations become the basis of outputs.
Here, let us consider the difference between our
method and other EBMT methods by dividing similar-
ity into a content-word part and a functional-word part.
In the content-word part, our method and other EBMT
methods are almost the same. Content words are im-
portant information in a similarity measure process, and
thesauri are utilized to extend lexical coverage. In the
functional-word part, our method is characterized by dis-
regarding functional words, while other EBMT meth-
ods still rely on them for the similarity measure. In our
method, the lack of functional word information is com-
pensated by the semantically narrow variety in S2ST do-
mains and the use of information on modality and tense.
Consequently, our method gains robustness to length and
the style differences between inputs and the example cor-
pus.
</bodyText>
<subsectionHeader confidence="0.994347">
5.2 Translation Memory
</subsectionHeader>
<bodyText confidence="0.9999845">
Translation memory (TM) is aimed at retrieving infor-
mative translation example from example corpus. TM
and our method share the retrieval strategy of rough and
wide coverage. However, recall is more highly weighted
than precision in TM, while recall and precision should
be equally considered in our method. To carry out
wide coverage retrieval, TM relaxed various conditions
on inputs: Preserving only mono-gram and bi-gram on
words/characters (Baldwin, 2001; Sato, 1992), remov-
ing functional words (Kumano et al., 2002; Wakita et al.,
2000), and removing content words (Sumita and Tsut-
sumi, 1988). In our method, information on functional
words is removed and that on modality and tense is in-
troduced instead. Information on word order is also re-
moved while instead we preserve information on whether
each word is located in the focus area.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999979416666667">
In this paper, we introduced the idea of meaning-
equivalent sentences for robust example-based S2ST.
Meaning-equivalent sentences have the same main mean-
ing as the input despite lacking some unimportant infor-
mation. Translation of meaning-equivalent sentences cor-
responds to rough translations, which aim not at exact
translation with narrow coverage but at rough translation
with wide coverage. For S2ST, we assume that this trans-
lation strategy is sufficiently useful.
Then, we described a method for retrieving meaning-
equivalent sentences from an example corpus. Retrieval
is based on content words, modality, and tense. This
strategy is feasible owing to the restricted domains, of-
ten adopted in S2ST, which have relatively small variety
in lexicon and meaning. An experiment demonstrated the
robustness of our method to input length and the style dif-
ferences between inputs and the example corpus.
Most MT systems aim to achieve exact translation, but
unfortunately they often output bad or no translation for
long conversational speeches. The rough translation pro-
posed in this paper achieves robustness in translation for
such inputs. This method compensates for the shortcom-
ings of conventional MT and makes S2ST technology
more practical.
</bodyText>
<sectionHeader confidence="0.996933" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999843">
The research reported here was supported in part by a
contract with the Telecommunications Advancement Or-
ganization of Japan entitled, ”A study of speech dialogue
translation technology based on a large corpus”.
</bodyText>
<sectionHeader confidence="0.999109" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985889514705882">
T. Baldwin. 2001. Low-cost, high-performance transla-
tion retrieval: Dumber is better. In Proc. of the 39th
ACL, pages 18–25.
R. D. Brown. 2000. Automated generalization of trans-
lation examples. In Proc. of the 18th COLING.
M. Carl. 1999. Inducing translation templates for
example-based machine translation. In Proc. of the MT
Summit VII, pages 250–258.
W. B. Frakes and R. Baeza-Yates, editors. 1992. Infor-
mation Retrieval Data Structures &amp; Algorithms. Pren-
tice Hall.
D. Jurafsky and J. H. Martin, editors. 2000. Speech and
Language Processing. Prentice Hall.
T. Kumano, I. Goto, H. Tanaka, N. Uratani, and T. Ehara.
2002. A translation aid system by retrieving bilingual
news database. In System and Computers in Japan,
pages 19–29.
G. Lazzari. 2002. The V1 framework program in Eu-
rope: Some thoughts about speech to speech trans-
lation research. In Proc. of 40th ACL Workshop on
Speech-to-Speech Translation, pages 129–135.
M. Nagao. 1981. A framework of a mechanical transla-
tion between Japanese and English by analogy princi-
ple. In Artificial and Human Intelligence, pages 173–
180.
S. Ohno and M. Hamanishi, editors. 1984. Ruigo-Shin-
Jiten. Kadokawa. (in Japanese).
S. Sato. 1992. CTM: An example-based translation aid
system. In Proc. of the 14th COLING, pages 1259–
1263.
E. Sumita and Y. Tsutsumi. 1988. A translation aid
system using flexible text retrieval based on syntax-
matching. In TRL Research Report TR87-1019. IBM
Tokyo Research Laboratory.
E. Sumita. 2001. Example-based machine translation
using DP-matching between work sequences. In Proc.
of the ACL 2001 Workshop on Data-Driven Methods
in Machine Translation, pages 1–8.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proc. of the 3rd LREC, pages
147–152.
T. Takezawa. 1999. Building a bilingual travel con-
versation database for speech translation research. In
Proc. of the 2nd international workshop on East-Asian
resources and evaluation conference on language re-
sources and evaluation, pages 17–20.
H. Tanaka and A. Yokoo. 1999. An efficient statistical
speech act type tagging system for a speech translation
systems. In Proc. of the Association for Computational
Linguistics, pages 381–388.
T. Veale and A. Way. 1997. Gaijin: A bootstrapping,
template-driven approach to example-based MT. In
Proc. of the NeMNLP97.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
Alex Waibel. 1996. Interactive translation of conversa-
tional speech. IEEE Computer, 29(7):41–48.
Y. Wakita, K. Matsui, and Y. Sagisaka. 2000. Fine
keyword clustering using a thesaurus and example se-
tences for speech translation. In Proc. ofInternational
Conference of Speech Language Processing, pages
390–393.
S. Yamamoto. 2000. Toward speech communications
beyond language barrier - research of spoken language
translation technologies at ATR -. In Proc. of ICSLP,
volume 4, pages 406–411.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.225771">
<title confidence="0.9997755">Retrieving Meaning-equivalent for Example-based Rough Translation</title>
<author confidence="0.926821">Yuji</author>
<affiliation confidence="0.986563">Nara Institute Science and</affiliation>
<email confidence="0.614272">matsu@is.aist-nara.ac.jp</email>
<author confidence="0.938295">Mitsuo Shimohata Eiichiro</author>
<affiliation confidence="0.6559465">ATR Spoken Language Research</affiliation>
<email confidence="0.911971">eiichiro.sumita@atr.co.jp</email>
<abstract confidence="0.993197894736842">Example-based machine translation (EBMT) is a promising translation method for speechto-speech translation because of its robustness. It retrieves example sentences similar to the input and adjusts their translations to obtain the output. However, it has problems in that the performance degrades when input sentences are long and when the style of inputs and that of the example corpus are different. This paper proposes a method for retrieving “meaning-equivalent sentences” to overcome these two problems. A meaning-equivalent sentence shares the main meaning with an input despite lacking some unimportant information. The translations of meaning-equivalent sentences correspond to “rough translations.” The retrieval is based on content words, modality, and tense.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Baldwin</author>
</authors>
<title>Low-cost, high-performance translation retrieval: Dumber is better.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th ACL,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="19892" citStr="Baldwin, 2001" startWordPosition="3135" endWordPosition="3136">ty and tense. Consequently, our method gains robustness to length and the style differences between inputs and the example corpus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant </context>
</contexts>
<marker>Baldwin, 2001</marker>
<rawString>T. Baldwin. 2001. Low-cost, high-performance translation retrieval: Dumber is better. In Proc. of the 39th ACL, pages 18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Brown</author>
</authors>
<title>Automated generalization of translation examples.</title>
<date>2000</date>
<booktitle>In Proc. of the 18th COLING.</booktitle>
<contexts>
<context position="18442" citStr="Brown, 2000" startWordPosition="2905" endWordPosition="2906">s. This indicates the robustness of the rough retrieval strategy to longer inputs. However, the method still has an accuracy difference of about 15% between the two styles. The accuracy of the proposed method is better than that of Method-2, especially in conversational style. The accuracy difference in longer inputs becomes smaller (about 4%) than that of Method-2. This indicates the robustness of the proposed method to the differences between the two styles. 5 Related Work 5.1 EBMT The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Veale and Way, 1997; Carl, 1999; Brown, 2000). The basic idea of EBMT is that sentences similar to the inputs are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functional-word part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the functional-word part, our method is characterized by disregarding functional</context>
</contexts>
<marker>Brown, 2000</marker>
<rawString>R. D. Brown. 2000. Automated generalization of translation examples. In Proc. of the 18th COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carl</author>
</authors>
<title>Inducing translation templates for example-based machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of the MT Summit VII,</booktitle>
<pages>250--258</pages>
<contexts>
<context position="18428" citStr="Carl, 1999" startWordPosition="2903" endWordPosition="2904">longer inputs. This indicates the robustness of the rough retrieval strategy to longer inputs. However, the method still has an accuracy difference of about 15% between the two styles. The accuracy of the proposed method is better than that of Method-2, especially in conversational style. The accuracy difference in longer inputs becomes smaller (about 4%) than that of Method-2. This indicates the robustness of the proposed method to the differences between the two styles. 5 Related Work 5.1 EBMT The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Veale and Way, 1997; Carl, 1999; Brown, 2000). The basic idea of EBMT is that sentences similar to the inputs are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functional-word part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the functional-word part, our method is characterized by disregard</context>
</contexts>
<marker>Carl, 1999</marker>
<rawString>M. Carl. 1999. Inducing translation templates for example-based machine translation. In Proc. of the MT Summit VII, pages 250–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Frakes</author>
<author>R Baeza-Yates</author>
<author>editors</author>
</authors>
<date>1992</date>
<booktitle>Information Retrieval Data Structures &amp;</booktitle>
<publisher>Algorithms. Prentice Hall.</publisher>
<marker>Frakes, Baeza-Yates, editors, 1992</marker>
<rawString>W. B. Frakes and R. Baeza-Yates, editors. 1992. Information Retrieval Data Structures &amp; Algorithms. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<date>2000</date>
<booktitle>Speech and Language Processing.</booktitle>
<editor>D. Jurafsky and J. H. Martin, editors.</editor>
<publisher>Prentice Hall.</publisher>
<marker>2000</marker>
<rawString>D. Jurafsky and J. H. Martin, editors. 2000. Speech and Language Processing. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kumano</author>
<author>I Goto</author>
<author>H Tanaka</author>
<author>N Uratani</author>
<author>T Ehara</author>
</authors>
<title>A translation aid system by retrieving bilingual news database.</title>
<date>2002</date>
<booktitle>In System and Computers in Japan,</booktitle>
<pages>pages</pages>
<contexts>
<context position="19953" citStr="Kumano et al., 2002" startWordPosition="3143" endWordPosition="3146"> to length and the style differences between inputs and the example corpus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant information. Translation of meaning-equivalent sentences corr</context>
</contexts>
<marker>Kumano, Goto, Tanaka, Uratani, Ehara, 2002</marker>
<rawString>T. Kumano, I. Goto, H. Tanaka, N. Uratani, and T. Ehara. 2002. A translation aid system by retrieving bilingual news database. In System and Computers in Japan, pages 19–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lazzari</author>
</authors>
<title>The V1 framework program in Europe: Some thoughts about speech to speech translation research.</title>
<date>2002</date>
<booktitle>In Proc. of 40th ACL Workshop on Speech-to-Speech Translation,</booktitle>
<pages>129--135</pages>
<contexts>
<context position="1504" citStr="Lazzari, 2002" startWordPosition="204" endWordPosition="205"> lacking some unimportant information. The translations of meaning-equivalent sentences correspond to “rough translations.” The retrieval is based on content words, modality, and tense. 1 Introduction Speech-to-speech translation (S2ST) technologies consist of speech recognition, machine translation (MT), and speech synthesis (Waibel, 1996; Wahlster, 2000; Yamamoto, 2000). The MT part receives speech texts recognized by a speech recognizer. The nature of speech causes difficulty in translation since the styles of speech are different from those of written text and are sometimes ungrammatical (Lazzari, 2002). Therefore, rule-based MT cannot translate speech accurately compared with its performance for written-style text. Example-based MT (EBMT) is one of the corpusbased machine translation methods. It retrieves examples similar to inputs and adjusts their translations to obtain the output (Nagao, 1981). EBMT is a promising method for S2ST in that it performs robust translation of ungrammatical sentences and requires far less manual work than rule-based MT. However, there are two problems in applying EBMT to S2ST. One is that the translation accuracy drastically drops as input sentences become lon</context>
</contexts>
<marker>Lazzari, 2002</marker>
<rawString>G. Lazzari. 2002. The V1 framework program in Europe: Some thoughts about speech to speech translation research. In Proc. of 40th ACL Workshop on Speech-to-Speech Translation, pages 129–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagao</author>
</authors>
<title>A framework of a mechanical translation between Japanese and English by analogy principle.</title>
<date>1981</date>
<booktitle>In Artificial and Human Intelligence,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="1804" citStr="Nagao, 1981" startWordPosition="246" endWordPosition="247">MT), and speech synthesis (Waibel, 1996; Wahlster, 2000; Yamamoto, 2000). The MT part receives speech texts recognized by a speech recognizer. The nature of speech causes difficulty in translation since the styles of speech are different from those of written text and are sometimes ungrammatical (Lazzari, 2002). Therefore, rule-based MT cannot translate speech accurately compared with its performance for written-style text. Example-based MT (EBMT) is one of the corpusbased machine translation methods. It retrieves examples similar to inputs and adjusts their translations to obtain the output (Nagao, 1981). EBMT is a promising method for S2ST in that it performs robust translation of ungrammatical sentences and requires far less manual work than rule-based MT. However, there are two problems in applying EBMT to S2ST. One is that the translation accuracy drastically drops as input sentences become long. As the length of a sentence becomes long, the number of retrieved similar sentences greatly decreases. This often results in no output when translating long sentences. The other problem arises due to the differences in style between input sentences and the example corpus. It is difficult to acqui</context>
</contexts>
<marker>Nagao, 1981</marker>
<rawString>M. Nagao. 1981. A framework of a mechanical translation between Japanese and English by analogy principle. In Artificial and Human Intelligence, pages 173– 180.</rawString>
</citation>
<citation valid="false">
<date>1984</date>
<editor>S. Ohno and M. Hamanishi, editors.</editor>
<note>Ruigo-ShinJiten. Kadokawa. (in Japanese).</note>
<marker>1984</marker>
<rawString>S. Ohno and M. Hamanishi, editors. 1984. Ruigo-ShinJiten. Kadokawa. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
</authors>
<title>CTM: An example-based translation aid system.</title>
<date>1992</date>
<booktitle>In Proc. of the 14th COLING,</booktitle>
<pages>1259--1263</pages>
<contexts>
<context position="19905" citStr="Sato, 1992" startWordPosition="3137" endWordPosition="3138">onsequently, our method gains robustness to length and the style differences between inputs and the example corpus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant information. </context>
</contexts>
<marker>Sato, 1992</marker>
<rawString>S. Sato. 1992. CTM: An example-based translation aid system. In Proc. of the 14th COLING, pages 1259– 1263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>Y Tsutsumi</author>
</authors>
<title>A translation aid system using flexible text retrieval based on syntaxmatching.</title>
<date>1988</date>
<booktitle>In TRL Research Report TR87-1019. IBM</booktitle>
<institution>Tokyo Research Laboratory.</institution>
<contexts>
<context position="20031" citStr="Sumita and Tsutsumi, 1988" startWordPosition="3155" endWordPosition="3159">pus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant information. Translation of meaning-equivalent sentences corresponds to rough translations, which aim not at exact translation with narrow </context>
</contexts>
<marker>Sumita, Tsutsumi, 1988</marker>
<rawString>E. Sumita and Y. Tsutsumi. 1988. A translation aid system using flexible text retrieval based on syntaxmatching. In TRL Research Report TR87-1019. IBM Tokyo Research Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
</authors>
<title>Example-based machine translation using DP-matching between work sequences.</title>
<date>2001</date>
<booktitle>In Proc. of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4627" citStr="Sumita, 2001" startWordPosition="687" endWordPosition="688"> 72.3 16.3 Table 2: Cross Perplexity Number of Sentences 100 40 80 60 20 0 Untranslated Translated 2-5 6-10 11-15 16- Sentence Length (Words) Figure 1: Distribution of Untranslated Inputs by Length 2 Difficulty in Example-based S2ST 2.1 Translation Degradation by Input Length A major problem with machine translation, regardless of the translation method, is that performance drops rapidly as input sentences become longer. For EBMT, the longer input sentences become, the fewer similar example sentences exist in the example corpus. Figure 1 shows translation difficulty in long sentences in EBMT (Sumita, 2001). The EBMT system is given 591 test sentences and returns translation result as translated/untranslated. Untranslated means that there exists no similar example sentences for the input. Although the EBMT is equipped with a large example corpus (about 170K sentences), it often failed to translate long inputs. 2.2 Style Differences between Concise and Conversational The performance of example-based S2ST greatly depends on the example corpus. It is advantageous for an example corpus to have a large volume and the same style as the input sentences. A corpus of texts dictated from conversational sp</context>
<context position="18395" citStr="Sumita, 2001" startWordPosition="2897" endWordPosition="2898">acy than Method-1, especially with longer inputs. This indicates the robustness of the rough retrieval strategy to longer inputs. However, the method still has an accuracy difference of about 15% between the two styles. The accuracy of the proposed method is better than that of Method-2, especially in conversational style. The accuracy difference in longer inputs becomes smaller (about 4%) than that of Method-2. This indicates the robustness of the proposed method to the differences between the two styles. 5 Related Work 5.1 EBMT The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Veale and Way, 1997; Carl, 1999; Brown, 2000). The basic idea of EBMT is that sentences similar to the inputs are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functional-word part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the functional-word part, our met</context>
</contexts>
<marker>Sumita, 2001</marker>
<rawString>E. Sumita. 2001. Example-based machine translation using DP-matching between work sequences. In Proc. of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
<author>E Sumita</author>
<author>F Sugaya</author>
<author>H Yamamoto</author>
<author>S Yamamoto</author>
</authors>
<title>Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world.</title>
<date>2002</date>
<booktitle>In Proc. of the 3rd LREC,</booktitle>
<pages>147--152</pages>
<contexts>
<context position="5812" citStr="Takezawa et al., 2002" startWordPosition="872" endWordPosition="875">texts dictated from conversational speech is favorable for S2ST. Unfortunately, it is very difficult to prepare such an example corpus since this task requires laborious work such as speech recording and speech transcription. Therefore, we cannot avoid using a written-style corpus, such as phrasebooks, to prepare a sufficiently large volume of examples. Contained texts are almost grammatical and rarely contain unnecessary words. We call the style used in such a corpus “concise” and the style seen in conversational speech “conversational.” Table 1 shows the average numbers of words in concise (Takezawa et al., 2002) and conversational corpora (Takezawa, 1999). Sentences in conversational style are about 2.5 words longer than those in concise style in both English and Japanese. This is because conversational style sentences contain unnecessary words or subordinate clauses, which have the effects of assisting the listener’s comprehension and avoiding the possibility of giving the listener a curt impression. Table 2 shows cross perplexity between concise and conversational corpora (Takezawa et al., 2002). Perplexity is used as a metric for how well a language model derived from a training set matches a test</context>
<context position="14566" citStr="Takezawa et al., 2002" startWordPosition="2284" endWordPosition="2287">milar sentence is selected randomly. C1: # of identical words in focus area. C2: # of synonymous words in focus area. C3: # of identical words in non-focus area. C4: # of synonymous words in non-focus area. C5: # of common functional words. C6: # of different functional words. (the fewer, the higher priority) Figure 4 shows an example of conditions for ranking. Content word in a focus area of input are underlined and functional words are written in italic. 4 Experiment 4.1 Test Data We used a bilingual corpus of travel conversation, which has Japanese sentences and their English translations (Takezawa et al., 2002). This corpus was sentencealigned, and a morphological analysis was done on both languages by our morphological analysis tools. The bilingual corpus was divided into example data (Example) and test data (Concise) by extracting test data randomly from the whole set of data. In addition to this, we used a conversational speech corpus for another set of test data (Takezawa, 1999). This corpus contains dialogs between a traveler and a hotel 5Words are converted to base form. Method-1 Method-2 Proposed (Strict with Func. ) (Rough with Func.) (Rough w/o Func.) Input Length (Words) Input Length (Word</context>
</contexts>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and S. Yamamoto. 2002. Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world. In Proc. of the 3rd LREC, pages 147–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
</authors>
<title>Building a bilingual travel conversation database for speech translation research.</title>
<date>1999</date>
<booktitle>In Proc. of the 2nd international workshop on East-Asian resources and evaluation conference on language resources and evaluation,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="5856" citStr="Takezawa, 1999" startWordPosition="879" endWordPosition="880">ble for S2ST. Unfortunately, it is very difficult to prepare such an example corpus since this task requires laborious work such as speech recording and speech transcription. Therefore, we cannot avoid using a written-style corpus, such as phrasebooks, to prepare a sufficiently large volume of examples. Contained texts are almost grammatical and rarely contain unnecessary words. We call the style used in such a corpus “concise” and the style seen in conversational speech “conversational.” Table 1 shows the average numbers of words in concise (Takezawa et al., 2002) and conversational corpora (Takezawa, 1999). Sentences in conversational style are about 2.5 words longer than those in concise style in both English and Japanese. This is because conversational style sentences contain unnecessary words or subordinate clauses, which have the effects of assisting the listener’s comprehension and avoiding the possibility of giving the listener a curt impression. Table 2 shows cross perplexity between concise and conversational corpora (Takezawa et al., 2002). Perplexity is used as a metric for how well a language model derived from a training set matches a test set (Jurafsky and Martin, 2000). Cross perp</context>
<context position="14945" citStr="Takezawa, 1999" startWordPosition="2348" endWordPosition="2349">s area of input are underlined and functional words are written in italic. 4 Experiment 4.1 Test Data We used a bilingual corpus of travel conversation, which has Japanese sentences and their English translations (Takezawa et al., 2002). This corpus was sentencealigned, and a morphological analysis was done on both languages by our morphological analysis tools. The bilingual corpus was divided into example data (Example) and test data (Concise) by extracting test data randomly from the whole set of data. In addition to this, we used a conversational speech corpus for another set of test data (Takezawa, 1999). This corpus contains dialogs between a traveler and a hotel 5Words are converted to base form. Method-1 Method-2 Proposed (Strict with Func. ) (Rough with Func.) (Rough w/o Func.) Input Length (Words) Input Length (Words) Input Length (Words) Figure 5: Results Accuracy (%) 100 40 20 80 60 0 1-5 6-10 11-15 16- Accuracy (%) 100 40 20 80 60 0 1-5 6-10 11-15 16- 1-5 6-10 11-15 16- Accuracy (%) 100 40 20 80 60 0 Conversational Concise Corpus # of Average Sentences Length Example 92,397 7.4 Concise 1,588 6.6 Conversational 800 10.1 Table 4: Statistics of the Corpora receptionist. It tests the robu</context>
</contexts>
<marker>Takezawa, 1999</marker>
<rawString>T. Takezawa. 1999. Building a bilingual travel conversation database for speech translation research. In Proc. of the 2nd international workshop on East-Asian resources and evaluation conference on language resources and evaluation, pages 17–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tanaka</author>
<author>A Yokoo</author>
</authors>
<title>An efficient statistical speech act type tagging system for a speech translation systems.</title>
<date>1999</date>
<booktitle>In Proc. of the Association for Computational Linguistics,</booktitle>
<pages>381--388</pages>
<contexts>
<context position="12063" citStr="Tanaka and Yokoo, 1999" startWordPosition="1856" endWordPosition="1859"> &amp; Tense4 hoteru wo yoyaku shi tekudasai request (Will you reserve this hotel?) hoteru wo yoyaku shi tai desire (I want to reserve this hotel.) hoteru wo yoyaku shi mashi ta ka? question (Did you reserve this hotel?) past hoteru wo yoyaku shi tei masen negation (I do not reserve this hotel.) Figure 3: Sentences and their Modality and Tense sample sentences and their modality and tense. Clues are underlined. A speech act is a concept similar to modality in which speakers’ intentions are represented. The two studies introduced information of the speech act in their S2ST systems (Wahlster, 2000; Tanaka and Yokoo, 1999). The two studies and our method differ in the effect of speech act information. Their effect of speech act information is so small that it is limited to generating the translation text. Translation texts are refined by selecting proper expressions according to the detected speakers’ intention. 3.3 Retrieval and Ranking Sentences that satisfy the conditions below are recognized as meaning-equivalent sentences. 1. It is required to have the same modality and tense as the input sentence. 2. All content words are included (identical or synonymous) in the input sentence. This means that the set of</context>
</contexts>
<marker>Tanaka, Yokoo, 1999</marker>
<rawString>H. Tanaka and A. Yokoo. 1999. An efficient statistical speech act type tagging system for a speech translation systems. In Proc. of the Association for Computational Linguistics, pages 381–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
<author>A Way</author>
</authors>
<title>Gaijin: A bootstrapping, template-driven approach to example-based MT.</title>
<date>1997</date>
<booktitle>In Proc. of the NeMNLP97.</booktitle>
<contexts>
<context position="18416" citStr="Veale and Way, 1997" startWordPosition="2899" endWordPosition="2902">d-1, especially with longer inputs. This indicates the robustness of the rough retrieval strategy to longer inputs. However, the method still has an accuracy difference of about 15% between the two styles. The accuracy of the proposed method is better than that of Method-2, especially in conversational style. The accuracy difference in longer inputs becomes smaller (about 4%) than that of Method-2. This indicates the robustness of the proposed method to the differences between the two styles. 5 Related Work 5.1 EBMT The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Veale and Way, 1997; Carl, 1999; Brown, 2000). The basic idea of EBMT is that sentences similar to the inputs are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functional-word part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the functional-word part, our method is characterized </context>
</contexts>
<marker>Veale, Way, 1997</marker>
<rawString>T. Veale and A. Way. 1997. Gaijin: A bootstrapping, template-driven approach to example-based MT. In Proc. of the NeMNLP97.</rawString>
</citation>
<citation valid="true">
<title>Verbmobil: Foundations of Speech-to-Speech Translation.</title>
<date>2000</date>
<editor>W. Wahlster, editor.</editor>
<publisher>Springer.</publisher>
<marker>2000</marker>
<rawString>W. Wahlster, editor. 2000. Verbmobil: Foundations of Speech-to-Speech Translation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Waibel</author>
</authors>
<title>Interactive translation of conversational speech.</title>
<date>1996</date>
<journal>IEEE Computer,</journal>
<volume>29</volume>
<issue>7</issue>
<contexts>
<context position="1231" citStr="Waibel, 1996" startWordPosition="161" endWordPosition="162">nces are long and when the style of inputs and that of the example corpus are different. This paper proposes a method for retrieving “meaning-equivalent sentences” to overcome these two problems. A meaning-equivalent sentence shares the main meaning with an input despite lacking some unimportant information. The translations of meaning-equivalent sentences correspond to “rough translations.” The retrieval is based on content words, modality, and tense. 1 Introduction Speech-to-speech translation (S2ST) technologies consist of speech recognition, machine translation (MT), and speech synthesis (Waibel, 1996; Wahlster, 2000; Yamamoto, 2000). The MT part receives speech texts recognized by a speech recognizer. The nature of speech causes difficulty in translation since the styles of speech are different from those of written text and are sometimes ungrammatical (Lazzari, 2002). Therefore, rule-based MT cannot translate speech accurately compared with its performance for written-style text. Example-based MT (EBMT) is one of the corpusbased machine translation methods. It retrieves examples similar to inputs and adjusts their translations to obtain the output (Nagao, 1981). EBMT is a promising metho</context>
</contexts>
<marker>Waibel, 1996</marker>
<rawString>Alex Waibel. 1996. Interactive translation of conversational speech. IEEE Computer, 29(7):41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wakita</author>
<author>K Matsui</author>
<author>Y Sagisaka</author>
</authors>
<title>Fine keyword clustering using a thesaurus and example setences for speech translation.</title>
<date>2000</date>
<booktitle>In Proc. ofInternational Conference of Speech Language Processing,</booktitle>
<pages>390--393</pages>
<contexts>
<context position="19975" citStr="Wakita et al., 2000" startWordPosition="3147" endWordPosition="3150">yle differences between inputs and the example corpus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant information. Translation of meaning-equivalent sentences corresponds to rough trans</context>
</contexts>
<marker>Wakita, Matsui, Sagisaka, 2000</marker>
<rawString>Y. Wakita, K. Matsui, and Y. Sagisaka. 2000. Fine keyword clustering using a thesaurus and example setences for speech translation. In Proc. ofInternational Conference of Speech Language Processing, pages 390–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yamamoto</author>
</authors>
<title>Toward speech communications beyond language barrier - research of spoken language translation technologies at ATR -.</title>
<date>2000</date>
<booktitle>In Proc. of ICSLP,</booktitle>
<volume>4</volume>
<pages>406--411</pages>
<contexts>
<context position="1264" citStr="Yamamoto, 2000" startWordPosition="165" endWordPosition="167">le of inputs and that of the example corpus are different. This paper proposes a method for retrieving “meaning-equivalent sentences” to overcome these two problems. A meaning-equivalent sentence shares the main meaning with an input despite lacking some unimportant information. The translations of meaning-equivalent sentences correspond to “rough translations.” The retrieval is based on content words, modality, and tense. 1 Introduction Speech-to-speech translation (S2ST) technologies consist of speech recognition, machine translation (MT), and speech synthesis (Waibel, 1996; Wahlster, 2000; Yamamoto, 2000). The MT part receives speech texts recognized by a speech recognizer. The nature of speech causes difficulty in translation since the styles of speech are different from those of written text and are sometimes ungrammatical (Lazzari, 2002). Therefore, rule-based MT cannot translate speech accurately compared with its performance for written-style text. Example-based MT (EBMT) is one of the corpusbased machine translation methods. It retrieves examples similar to inputs and adjusts their translations to obtain the output (Nagao, 1981). EBMT is a promising method for S2ST in that it performs ro</context>
</contexts>
<marker>Yamamoto, 2000</marker>
<rawString>S. Yamamoto. 2000. Toward speech communications beyond language barrier - research of spoken language translation technologies at ATR -. In Proc. of ICSLP, volume 4, pages 406–411.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>