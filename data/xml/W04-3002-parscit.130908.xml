<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000028">
<title confidence="0.996822">
Hybrid Statistical and Structural Semantic Modeling for Thai Multi-
Stage Spoken Language Understanding
</title>
<author confidence="0.996954">
Chai Wutiwiwatchai and Sadaoki Furui
</author>
<affiliation confidence="0.999752">
Department of Computer Science, Tokyo Institute of Technology
</affiliation>
<address confidence="0.942062">
2-12-1 Ookayama, Meguro-ku, Tokyo, 152-8552 Japan.
</address>
<email confidence="0.996456">
{chai, furui}@furui.cs.titech.ac.jp
</email>
<sectionHeader confidence="0.997351" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999885">
This article proposes a hybrid statistical and
structural semantic model for multi-stage
spoken language understanding (SLU). The
first stage of this SLU utilizes a weighted fi-
nite-state transducer (WFST)-based parser,
which encodes the regular grammar of con-
cepts to be extracted. The proposed method
improves the regular grammar model by in-
corporating a well-known n-gram semantic
tagger. This hybrid model thus enhances the
syntax of n-gram outputs while providing
robustness against speech-recognition errors.
With applications to a Thai hotel reservation
domain, it is shown to outperform both indi-
vidual models at every stage of the SLU sys-
tem. Under the probabilistic WFST
framework, the use of N-best hypotheses
from the speech recognizer instead of the 1-
best can further improve performance requir-
ing only a small additional processing time.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958596774194">
Automatic speech recognition (ASR) for Thai lan-
guage is still in the first stage, where Thai researchers
in related fields have worked towards creating funda-
mental tools for language processing such as phono-
logical and morphological analyzers. Although Thai
writing is an alphabetic system, a problem of writing
without sentence markers or spaces between words has
obstructed initiation of development of ASR. Pioneer-
ing a Thai spoken dialogue system has therefore be-
come a challenging task, where several unique
components need to be developed specifically for a
Thai system.
Our prototype dialogue system, namely Thai Inter-
active Hotel Reservation Agent (TIRA), was created
mainly by handcrafted rules. The first user evaluation
(Wutiwiwatchai and Furui, 2003a) showed that the
spoken language understanding (SLU) part of the sys-
tem proved the most problematic as it could not cover
the variety of contents supplied by the users, especially
when they talked in a mixed-initiative style.
To rapidly improve performance, a trainable SLU
model is preferable and it needs to be able to learn
from a partially annotated corpus, where only essential
keywords are given. This is particularly important for
Thai where no large corpus is available.
Recently, a novel multi-stage SLU model has been
developed (Wutiwiwatchai and Furui, 2003b), which
combines two different practices used for SLU-related
tasks, robust semantic parsing and topic classification.
The former paradigm was implemented in the concept
extraction and concept-value recognition component,
whereas the latter was applied for the goal identifica-
tion component. The concept extraction utilizes a set
of weighted finite-state transducers (WFST) to encode
possible word-syntax (or regular grammar) expressed
for each concept. The concept WFST not only deter-
mines the existence of a concept in an input utterance,
but also labels keywords used to construct its value in
the concept-value recognition component. Given the
extracted concepts, the goal of the utterance can be
identified in the goal identification component using a
generalized pattern classifier.
This article reports an improvement of the concept
extraction and concept-value recognition parts by con-
ducting a well-known statistical n-gram parser to com-
pensate for the concept expressions, which cannot be
recognized by the ordinary concept WFST. The n-
gram modeling alone lacks structural information as it
captures only up to n-word dependencies. Combining
the statistical and structural model for SLU hence be-
comes a better alternative. Motivated by Béchet et al.
(2002), we propose a strategic way called logical n-
gram modeling, which combines the statistical n-gram
with the existing regular grammar. In contrast to the
regular-grammar approach, the probabilistic model
allows the SLU to deal with ASR N-best hypotheses,
resulting in an increment of the overall performance.
Some related works are reviewed in the next sec-
tion, followed by a description of our multi-stage SLU
model. Section 4 explains the proposed hybrid model.
Section 5 shows the evaluation results with a conclu-
sion in section 6.
</bodyText>
<sectionHeader confidence="0.999497" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.999979965517242">
In the technology of trainable or data-driven SLU, two
different practices for different applications have been
widely investigated. The first practice aims to tag the
words (or group of words) in the utterance with se-
mantic labels, which are later converted to a certain
format of semantic representation. To generate such a
semantic frame, words in the utterance are usually
aligned to a semantic tree by a parsing algorithm such
as a probabilistic context free grammar or a recursive
network whose nodes represent semantic symbols of
the words and arcs consist of transition probabilities.
During parsing, these probabilities are summed up,
and used to determine the most likely parsed tree.
Many understanding engines have been successfully
implemented based on this paradigm (Seneff, 1992;
Potamianos et al., 2000; Miller et al., 1994). A draw-
back of this method is, however, the requirement of a
large, fully annotated corpus, i.e. a corpus with seman-
tic tags on every word, to ensure training reliability.
The second practice has been utilized in applica-
tions such as call classification (Gorin et al., 1997). In
this application, the understanding module aims to
classify an input utterance to one of predefined user
goals (if an utterance is supposed to have one goal)
directly from the words contained in the utterance.
This problem can be considered a simple pattern clas-
sification task. An advantage of this method is the
need for training utterances tagged only with their
goals, one for each utterance. However, another proc-
ess is required if one needs to obtain more detailed
information. Our motivation for combining the two
practices described above is that this allows the use of
an only partially annotated corpus, while still allowing
the system to capture sufficient information. The idea
of combination has also been investigated in other
works such as Wang et al. (2002).
Another issue related to this article is the combina-
tion of a statistical and rule-based approach for SLU, a
system which is expected to improve the overall per-
formance over both individual approaches. The closest
approach to our work was proposed by Béchet et al.
(2002), aiming to extract named-entities (NEs) from
an input utterance. NE extraction is performed in two
steps, detecting the NEs by a statistical tagger and ex-
tracting NE values using local models. Estève et al.
(2003) proposed a tighter coupling method that em-
beds conceptual structures into the ASR decoding
network. Wang et al. (2000), and Hacioglu and Ward
(2001) proposed similar ideas for unified models that
incorporated domain-specific context-free grammars
(CFGs) into domain-independent n-gram models. The
hybrid models thus improved the generalized ability of
the CFG and specificity of the n-gram. With the exist-
ing regular grammar model in a weighted finite-state
transducer (WFST) framework, we propose another
strategy to incorporate the statistical n-gram model
into the concept extraction and concept-value recogni-
tion components of our multi-stage SLU.
</bodyText>
<sectionHeader confidence="0.997291" genericHeader="method">
3 Multi-Stage SLU
</sectionHeader>
<bodyText confidence="0.999969647058824">
In the design of our spoken dialogue system, the dia-
logue manager decides to respond to the user after
perceiving the user goal. In some types of goal, infor-
mation items contained in the utterance are required
for communication. For example the goal “request for
facilities” must come with the facilities the user is ask-
ing for, and the goal “request for prerequisite keys”
aims to have the user state the reserved date and the
number of participants. Hence, the SLU module must
be able to identify the goal and extract the required
information items.
We proposed a novel SLU model (Wutiwiwatchai
and Furui, 2003b) that processes an input utterance in
three stages, concept extraction, goal identification,
and concept-value recognition. Figure 1 illustrates the
overall architecture of the SLU model, in which its
components are described in detail as follows:
</bodyText>
<figure confidence="0.973654833333333">
Word string
Concept extraction
Concepts
Concept-value
recognition
Goal Concept-values
</figure>
<figureCaption confidence="0.999528">
Figure 1. Overall architecture of the multi-stage SLU.
</figureCaption>
<subsectionHeader confidence="0.997762">
3.1 Concept extraction
</subsectionHeader>
<bodyText confidence="0.9990095">
The function of concept extraction is similar to that of
other works, aiming to extract a set of concepts from
an input utterance. However, our way to define a con-
cept is rather different.
A concept has a unique semantic meaning.
The order of concepts is not important.
Each type of concept occurs only once in an ut-
terance.
The semantic meaning of a concept can be inter-
preted from a sequence of words arbitrarily
placed in the utterance (the sequence can overlap
or cross each other).
Examples of utterances and concepts contained in the
utterances are shown in Table 1. A word sequence or
</bodyText>
<equation confidence="0.3358135">
Accepted
substrings
Goal
identification
</equation>
<bodyText confidence="0.999121947368421">
substring corresponding to the concept is presented in
the form of a label sequence. The ‘’ and two-alphabet
symbols such as ‘fd’ denote the words required to in-
dicate the concept. The two-alphabet symbols addi-
tionally specify keywords used for concept-value
recognition. The ‘-’ is for other words not related to
the concept. As defined above, a concept such as
‘reqprovide’ (asking whether something is provided) is
expressed by the substring “there is ... right”, which
contains two separated strings, “there is” and “right”.
In the same utterance, another concept ‘yesnoq’ (ask-
ing by a yes-no question) also possesses the word
‘right’. We considered this method of definition to
have more impact for presenting the meaning of con-
cepts, compared to what has been defined in other
works. It must be noted that some concepts contain
values such as the concept ‘numperson’ (the number of
people), whereas some do not, such as the concept
‘yesnoq’.
</bodyText>
<figure confidence="0.829938">
...
</figure>
<figureCaption confidence="0.9985415">
Figure 2. A portion of regular grammar WFST for the
concept ‘numperson’ (the number of people).
</figureCaption>
<bodyText confidence="0.999644652173913">
We implemented the concept extraction component
by using weighted finite-state transducers (WFSTs).
Similar to the implementation of salient grammar
fragments in Gorin et al. (1997), the possible word
sequences expressed for a concept are encoded in a
WFST, one for each type of concept. Figure 2 demon-
strates a portion of WFST for the concept ‘numperson’.
Each arc or transition of the WFST is labeled with an
input word (or word class) followed after a colon by
an output semantic label, and enclosed after a slash by
a weight. A special symbol ‘NIL’ represents any word
not included in the concept. The transitions, linking
between the start and end node, characterize the ac-
ceptable word syntax. Weights of these transitions,
except those containing ‘NIL’, are assigned to be -1.
The rest are assigned to have zero weights. The output
labels indicate keywords as shown in Table 1. These
labels will be used later by the concept-value recogni-
tion component.
In the training step, each concept WFST was cre-
ated separately. The training utterances were tagged by
marking just the words required by the concept. Then
the WFST was constructed by:
</bodyText>
<listItem confidence="0.9958006">
1. replacing the unmarked words in each training
utterance by the symbols ‘NIL’,
2. making an individual FST for the preprocessed
utterance,
3. performing the union operation of all FSTs and
determinizing the resulting FST,
4. attaching the recursive-arcs of every word to
the start and end node as illustrated in Fig. 2,
5. assigning the weights to the transitions as
described previously.
</listItem>
<bodyText confidence="0.999875166666667">
In the parsing step, an input utterance is fed to
every concept WFST in parallel. For each WFST, the
words in the utterance that are not included in the
WFST are replaced by the symbols ‘NIL’ and the pre-
processed word string is parsed by the WFST using the
composition operation. By minimizing the cumulative
weight, the longest accepted substring is chosen. A
concept is considered to exist if at least one substring
is accepted. Since this model is a kind of word-
grammar representation for a particular concept, we
have called it the concept regular grammar or ‘Reg’
model in short.
</bodyText>
<listItem confidence="0.630009909090909">
“two nights from the sixth of July”
Concept Keyword labels of accepted substring
(1) reservedate - - fd fm
(2) numnight nn - - - - -
Goal inform_prerequisite-keys
Label sequence 2:nn 2: 1: 1: 1:fd 1: 1:fm
“there is a pool, right?”
Concept Keyword labels of accepted substring
(1) reqprovide - -
(2) facility - - fc -
(3) yesnoq - - - -
</listItem>
<table confidence="0.7230855">
Goal request_facility
Label sequence 1: 1: 2: 2:fc 1:,3:
</table>
<tableCaption confidence="0.948967">
Table 1. Examples of defined goals, concepts and their
corresponding substrings presented by keyword labels.
</tableCaption>
<subsectionHeader confidence="0.998696">
3.2 Goal identification
</subsectionHeader>
<bodyText confidence="0.999844529411765">
Having extracted the concepts, the goal of the utter-
ance can be identified. The goal in our case can be
considered as a derivative of the dialogue act coupled
with additional information. As the examples show in
Table 1, the goal ‘request_facility’ means a request
(dialogue act) for some facilities (additional informa-
tion). Since we observed in our largest corpus that
only 1.1% were multiple-goal utterances, an utterance
could be supposed to have only one goal.
The goal identification task can be viewed as a
simple pattern classification problem, where a goal is
identified given an input vector of binary values indi-
cating the existence of predefined concepts. Our previ-
ous work (Wutiwiwatchai and Furui, 2003b) showed
that this task could be efficiently achieved by the sim-
ple multi-layer perceptron type of artificial neural net-
work (ANN).
</bodyText>
<figure confidence="0.996607846153846">
I: /0
NIL: /0
DGT: /0
NIL: /0
...
I: ac /-1 DGT: nc /-1
person: /-1
S E
DGT: np /-1
I : /0
NIL: /0
DGT: /0
friend: ac /-1
</figure>
<subsectionHeader confidence="0.982013">
3.3 Concept-value recognition
</subsectionHeader>
<bodyText confidence="0.999792275">
Recall again that some concepts contain values such as
the concept ‘numperson’, whose value is the number
of people, whereas some concepts do not, such as the
concept ‘yesnoq’. Given an input utterance, the SLU
module must be able to identify the goal and extract
information items such as the reserved date, the num-
ber of people, the name of facility, etc. The concepts
extracted in the first stage are not only used to identify
the goal, but also strongly related to the described in-
formation items, that is, the values of concepts are
actually the required information items. Hence, ex-
tracting the information items is to recognize the con-
cept values.
Since the keywords within a concept have already
been labeled by WFST composition in the concept
extraction step, recognizing the concept-value is just a
matter of converting the labeled keywords to a certain
format. For sake of explanation, let’s consider the ut-
terance “two nights from the sixth of July” in Table 1.
After parsing by the ‘reservedate’ (the reserved date)
concept WFST, the substring “from the sixth of July”
is accepted with the words “sixth” and “July” labeled
by the symbols ‘fd’ and ‘fm’ respectively. These label
symbols are specifically defined for each type of con-
cept and have their unique meanings, e.g. ‘fd’ for the
check-in date, ‘fm’ for the check-in month, etc. The
labeled keywords are then converted to a predefined
format for the concept value. The value of ‘reserve-
date’ concept is in a form of &lt;fy-fm-fd_ty-tm-td&gt;, and
thus the labeled keywords “sixth(fd) July(fm)” is con-
verted to &lt;04-07-06_ty-tm-td&gt;. It must be noted that
although the check-in year is not stated in the utterance,
the concept-value recognition process under its knowl-
edge-base inherently assigns the value ‘04’ (the year
2004) to the ‘fy’. This process can greatly help in solv-
ing anaphoric expressions in natural conversation. Ta-
ble 2 gives more examples of substrings accepted and
labeled by ‘reservedate’ WFST, and their correspond-
ing values. Currently, this conversion task is per-
formed by simple rules.
</bodyText>
<subsubsectionHeader confidence="0.70307">
Accepted substring Concept-value
</subsubsectionHeader>
<table confidence="0.683145">
“sixth(fd) to eighth(td) of July(tm)” &lt;04-07-06_04-07-08&gt;
“check-in tomorrow(fd)” &lt;04-06-10_ty-tm-td&gt;
“until next Tuesday(td)” &lt;fy-fm-fd_04-06-18&gt;
</table>
<tableCaption confidence="0.8076525">
Table 2. Examples of substrings accepted by the ‘re-
servedate’ WFST with their corresponding values.
</tableCaption>
<sectionHeader confidence="0.8258815" genericHeader="method">
4 Hybrid Statistical and Structural Se-
mantic Modeling
</sectionHeader>
<bodyText confidence="0.999692571428572">
Although the Reg model described in Sect. 3.1 has an
ability to capture long-distant dependencies for seen
grammar, it certainly fails to parse an unseen-grammar
utterance, especially when it is distorted by speech
recognition errors. This article thus presents an effort
to improve concept extraction and concept-value
recognition by incorporating a statistical approach.
</bodyText>
<subsectionHeader confidence="0.96846">
4.1 N-gram modeling
</subsectionHeader>
<bodyText confidence="0.99989325">
We can view the concept extraction process as a se-
quence labeling task, where a label sequence L = (l1 ...
lT) as shown in the “Label sequence” lines of Table 1
is determined given a word string W = (w1...wT). Each
label, in the form of {c:l}, refers to the cth-concept
with keyword label l. A word is allowed to be in mul-
tiple concepts, hence having multiple keyword labels
such as {1:,3: } as shown in the last line of Table 1.
Finding the most probable sequence L is equivalent to
maximizing the joint probability P(W,L), which can be
simplified using n-gram modeling (n = 2 for bigram)
as follows:
</bodyText>
<equation confidence="0.9991148">
T
~
L argmaxP(W,L) argmax P(wt,lt
L L t 1
(1)
</equation>
<bodyText confidence="0.999996833333333">
The described n-gram model, called ‘Ngram’
hereafter, can be implemented also by a WFST, whose
weights are the smoothed n-gram probabilities. Parsing
an utterance by the Ngram WFST is performed simply
by applying the WFST composition in the same way
as operated with the Reg model.
</bodyText>
<subsectionHeader confidence="0.991898">
4.2 Logical n-gram modeling
</subsectionHeader>
<bodyText confidence="0.996258208333333">
Although the n-gram model can assign a likelihood
score to any input utterance, it cannot distinguish be-
tween valid and invalid grammar structure. On the
other hand, the regular grammar model can give se-
mantic tags to an utterance that is permitted by the
grammar, but always rejects an ungrammatical utter-
ance. Thus, another probabilistic approach that inte-
grates the advantages of both models is optimum.
Our proposed model, motivated mainly by (Béchet
et al. 2002), combines the statistical and structural
models in two-pass processing. Firstly, the conven-
tional n-gram model is used to generate M-best hy-
potheses of label sequences given an input word string.
The likelihood score of each hypothesis is then en-
hanced once its word-and-label syntax is permitted by
the regular grammar model. By rescoring the M-best
list using the modified scores, the syntactically valid
sequence that has the highest n-gram probability is
reordered to the top. Even if no label sequence is per-
mitted by the regular grammar, the hybrid model is
still able to output the best sequence based on the
original n-gram scores. Since the proposed model aims
to enhance the logic of n-gram outputs, it is named the
logical n-gram model.
</bodyText>
<equation confidence="0.948948">
 |wt
1, 1)
lt
</equation>
<bodyText confidence="0.999978965517242">
This idea can be implemented efficiently in the
framework of WFST as depicted in Fig. 3. At first, the
concept-specific Reg WFST is modified from the one
shown in Fig. 2 by replacing the weight -1 by a vari-
able -, which can be empirically adjusted to gain the
best result. An unknown word string in the form of a
finite state machine is parsed by the Ngram WFST,
producing a WFST of M-best label-sequence hypothe-
ses. Concepts are detected in the top hypothesis. Then,
the concept-value recognition process is applied for
each detected concept separately. In the concept-value
recognition process, the M-best WFST is intersected
by the concept-specific Reg WFST. Rescoring the
result offers a new WFST of P-best (P &lt; M) hypothe-
ses with a score in logarithmic domain for each hy-
pothesis assigned by
where t { , 0} . If is set to 0, the intersection op-
eration is just to filter out the hypotheses that violate
the regular grammar, while the original scores from n-
gram model are left unaltered. If a larger is used, the
hypothesis that contains a longer valid syntax is given
a higher score. When no hypothesis in the M-best list
is permitted by the grammar (P = 0), the top hypothe-
sis of the M-best list is outputted. It is noted that the
strategy of eliminating unacceptable paths of n-gram
due to syntactical violation has also successfully been
used in a WFST-based speech recognition system
(Szarvas and Furui, 2003). Hereafter, we will refer to
the logical n-gram modeling as ‘LNgram’.
</bodyText>
<subsectionHeader confidence="0.984224">
4.3 The use of ASR N-best hypotheses
</subsectionHeader>
<bodyText confidence="0.999654454545454">
The probabilistic model allows the use of N-best hy-
potheses from the automatic speech recognition (ASR)
engine. As described in Sect. 4.1, our Ngram semantic
model produces a joint probability P(W,L), which in-
dicates the chance that the semantic-label sequence L
occurs with the word hypothesis W. When the N-best
word hypotheses generated from the ASR are fed into
the Ngram semantic parser, the parsed scores are
combined with the ASR likelihood scores in a log-
linear interpolation fashion (Klakow, 1998) as shown
in Eq. 3.
</bodyText>
<sectionHeader confidence="0.4075915" genericHeader="method">
L W N
,
</sectionHeader>
<bodyText confidence="0.9998805">
where A is an acoustic speech signal, and P(A,W) is a
product of an acoustic score P(A|W) and a language
score P(W). N denotes the N-best list and is an in-
terpolation weight, which can be adjusted experimen-
tally to give the best result. This interpolation method
can be easily implemented in a WFST framework
compared to normal linear interpolation.
An N-best list can be used in the LNgram using
the same criterion as well. The only necessary precau-
tion is an appropriate size of M in the M-best seman-
tic-label list, which is rescored in the second pass to
improve the concept-value result.
</bodyText>
<figureCaption confidence="0.940265">
Figure 5. Logical n-gram modeling.
</figureCaption>
<sectionHeader confidence="0.987387" genericHeader="evaluation">
5 Evaluation and Discussion
</sectionHeader>
<subsectionHeader confidence="0.895976">
5.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999983846153846">
Collecting and annotating a corpus is an especially
serious problem for language like Thai, where only
few databases are available. To shorten the collection
time, we created a specific web page simulating our
expected conversational dialogues, and asked Thai
native users to answer the dialogue questions by typ-
ing. As we asked the users to try answering the ques-
tions using spoken language, we could obtain a fairly
good corpus for training the SLU.
Currently, 5,869 typed-in utterances from 150 us-
ers have been completely annotated. To reduce the
effort of manual annotation, we conducted a semi-
automatic annotation method. The prototype rule-
based SLU was used to roughly tag each utterance
with a goal and concepts, which were then manually
corrected. Words or phases that were relevant to the
concept were marked automatically based on their
frequencies and information mutual to the concept.
Finally the tags were manually checked and the key-
words within each concept were additionally marked
by the defined label symbols.
All 5,869 utterances described above were used as
a training set (TR) for the SLU system. We also col-
lected a set of speech utterances during an evaluation
of our prototype dialogue system. It contained 1,101
speech utterances from 96 dialogues. By balancing the
</bodyText>
<figure confidence="0.955573862068965">
(log ( ,  |1, 1) λ, ) (2)
P wt lt wt lt t
T
Score
t
1
~
L argmax ( , ) ( , )
P A W P W L
1
(3)
Concept
extraction
Concept-value
recognition
Concept
Reg models
Semantic-label tagging
by the Ngram model
Word string
Converting keyword seq.
to concept values
Concept values
Rescoring by each
concept Reg model
The top hypothesis
M-best hypotheses
Extracted
concepts
</figure>
<bodyText confidence="0.998917833333333">
occurrence of goals, we reserved 500 utterances for a
development set (DS), which was used for tuning pa-
rameters. The remaining 601 utterances were used for
an evaluation set (ES). Table 3 shows the characteris-
tics of each data set. From the TR set, 75 types of con-
cepts and 42 types of goals were defined. The out-of-
goal and out-of-concept denote goals and concepts that
are not defined in the TR set, and thus cannot be rec-
ognized by the trained SLU. Since concepts that con-
tain no value are not counted for concept-value
evaluation, Table 3 also shows the number of concepts
that contain values in the line “# Concept-values”.
</bodyText>
<table confidence="0.999650363636364">
Characteristic TR DS ES
# Utterances 5,869 500 601
# Words / utterance 7.3 6.2 5.8
# Goal types 42 40 40
# Concept types 75 58 57
# Concept-value types 20 18 18
# Concepts 10,041 791 949
# Concept-values 6,365 366 439
% Out-of-goal 5.2 5.3
% Out-of-concept 2.8 3.3
% Word accuracy 77.2 79.0
</table>
<tableCaption confidence="0.999303">
Table 3. Characteristics of data sets
</tableCaption>
<subsectionHeader confidence="0.997252">
5.2 Evaluation measures
</subsectionHeader>
<bodyText confidence="0.997494">
Four measures were used for evaluation:
</bodyText>
<listItem confidence="0.993262363636364">
1. Word accuracy (WAcc) – the standard measure
for evaluating the ASR,
2. Concept F-measure (ConF) – the F-measure of
detected concepts,
3. Goal accuracy (GAcc) – the number of
utterances with correctly identified goals,
divided by the total number of test utterances,
4. Concept-value accuracy (CAcc) – the number
of concepts, whose values are correctly
matched to their references, divided by the total
number of concepts that contain values.
</listItem>
<subsectionHeader confidence="0.996539">
5.3 The use of logical n-gram modeling
</subsectionHeader>
<bodyText confidence="0.999091277777778">
The first experiment was to inspect improvement
gained after conducting the statistical approaches for
concept extraction and concept-value recognition.
Only the 1-best word hypothesis from the ASR was
experimented in this section. The AT&amp;T generalized
FSM library (Mohri et al., 1997) was used to construct
and operate all WFSTs, and the SNNS toolkit (Zell et
al., 1994) was used to create the ANN classifiers for
the goal identification task.
The baseline system utilized the Reg model for
concept extraction and concept-value recognition, and
the multi-layer perceptron ANN for goal identification.
75 WFSTs corresponding to the number of defined
concepts were created from the TR set. The ANN con-
sisted of a 75-node input layer, a 100-node hidden
layer (Wutiwiwatchai and Furui, 2003b), and a 42-
node output layer equal to the number of goals to be
identified.
</bodyText>
<figure confidence="0.99034525">
76
74
72
70
68
66
10 20 30 40 50 60 70 80 90 100
M-best
</figure>
<figureCaption confidence="0.9639995">
Figure 4. CAcc results with respect to values of M in
an oracle test for the DS set.
</figureCaption>
<figure confidence="0.977154571428572">
64
63
62
61
60
59
58
</figure>
<figureCaption confidence="0.990804">
Figure 5. CAcc results with variation of for the DS
set when M is set to 80.
</figureCaption>
<table confidence="0.9991266">
Measure Recognition Orthography
Reg Ngram LNgram Reg LNgram
ConF 76.5 88.6 78.9 91.4
GAcc 71.4 76.0 81.2 83.5
CAcc 65.1 52.4 67.2 75.7 76.8
</table>
<tableCaption confidence="0.978757">
Table 4. Evaluation results for the ES set using the
Reg, Ngram, and LNgram models.
</tableCaption>
<bodyText confidence="0.999853933333333">
Another WFST was constructed for the n-gram
semantic parser (n = 2 in our experiment), which was
used for the Ngram model and the first pass of the
LNgram model. Two parameters, M and , in the
LNgram approach need to be adjusted. To determine
an appropriate value of M, we plotted in an oracle
mode the CAcc of the DS set with respect to M, as
shown in Figure 4. According to the graph, an M of 80
was considered optimum and set for the rest of the
experiments. Figure 5 then shows the CAcc obtained
for rescored M-best hypotheses when the weight as
defined in Eq. 2 is varied. Here, the larger value of
means to assign a higher score to the hypothesis that
contains longer valid word-and-label syntax. Hence,
we concluded by Fig. 5 that reordering the hypotheses,
</bodyText>
<figure confidence="0.9886884">
CAcc (%)
CAcc (%)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
</figure>
<bodyText confidence="0.999635227272727">
which contain longer valid syntaxes, could improve
the CAcc significantly. Since the CAcc results become
steady when the value of is greater than 0.7, a of
1.0 is used henceforth to ensure the best performance.
The overall evaluation results on the ES set are
shown in Table 4, where M and in the LNgram
model are set to 80 and 1.0 respectively. ‘Recognition’
denotes the experiments on automatic speech-
recognized utterances (at 79% WAcc), whereas ‘Or-
thography’ means their exact manual transcriptions. It
is noted that the LNgram approach utilizes the same
process of Ngram in its first pass, where the concepts
are determined. Therefore, the ConF and GAcc results
of both approaches are the same.
According to the results, the Ngram tagger worked
well for the concept extraction task as it increased the
ConF by over 10%. The improvement mainly came
from reduction of redundant concepts often accepted
by the Reg model. The better extraction of concepts
could give better goal identification accuracy reasona-
bly. However, as we expected, the conventional
Ngram model itself had no syntactic information and
thus often produced a confusing label sequence, espe-
cially for ill-formed utterances. A typical error oc-
curred for words that could be tagged with one of
several semantic labels, such as the word ‘MNT’ (re-
ferring to the name of the month), which could be
identified as ‘check-in month’ or ‘check-out month’.
These two alternatives could only be clarified by a
context word, which sometimes located far from the
word ‘MNT’. This problem could be solved by using
the Reg model. The Reg model, however, could not
provide a label sequence to any out-of-syntax sentence.
The LNgram as an integration of both models thus
obviously outperformed the others.
In conclusion, the LNgram model could improve
the ConF, GAcc, and CAcc by 15.8%, 6.4%, and 3.2%
relative to the baseline Reg model. Moreover, if we
considered the orthography result an upperbound of
the underlying model, the GAcc and CAcc results pro-
duced by the LNgram model are relatively closer to
their upperbounds compared to the Reg model. This
verifies robustness improvement of the proposed
model against speech-recognition errors.
</bodyText>
<subsectionHeader confidence="0.968623">
5.4 The use of ASR N-best hypotheses
</subsectionHeader>
<bodyText confidence="0.999826571428571">
To incorporate N-best hypotheses from the ASR to the
LNgram model, we need to firstly determine an ap-
propriate value of N. An oracle test that measures
WAcc and ConF for the DS set with variation of N is
shown in Fig. 6. Although we can select a proper value
of N by considering only the WAcc, we also examine
the ConF to ensure that the selected N provides possi-
bility to improve the understanding performance as
well. According to Fig. 6, the ConF highly correlates
to the WAcc, and an N of 50 is considered optimum
for our task. At this operating point, we plot another
curve of ConF for the DS set with a variation of θ, the
interpolation weight in Eq. 3, as shown in Fig. 7. The
appropriate value of is 0.6, as the highest ConF is
obtained at this point. The last parameter we need to
adjust is the value of M. Although we have tuned the
value of M for the case of 1-best word hypothesis, the
appropriate value of M may change when the N-best
hypotheses are used instead. However, in our trial, we
found that the optimum value of M is again in the
same range as that operated for the 1-best case. A
probable reason is that rescoring the N-best word hy-
potheses by the Ngram model can reorder the good
hypotheses to a certain upper portion of the N-best list,
and thus rescoring in the second pass of the LNgram
is independent to the value of N. Consequently, an M
of 80 as that selected for the 1-best hypothesis is also
used for the N-best case.
</bodyText>
<figure confidence="0.486128">
N-best
</figure>
<figureCaption confidence="0.98110125">
Figure 6. WAcc and ConF results with respect to val-
ues of N in an oracle test for the DS set.
Figure 7. ConF results with variation of for the DS
set when N is set to 50.
</figureCaption>
<bodyText confidence="0.999858666666667">
Given all tuned parameters, an evaluation on the
ES set is carried out as shown in Fig. 8. With the Reg
model as a baseline system, the use of N-best hypothe-
ses further improves the the ConF, GAcc, and CAcc
by 0.9%, 0.6%, and 3.9% from the only 1-best, and
hence reduces the gap between the speech-recognized
</bodyText>
<figure confidence="0.880761974358974">
100
95
90
%
85
80
75
1
11
21
31
41
51
61
71
81
91
WAcc
ConF
0.0
0.1
0.2
0.3
0.4
0.5
0.6
ConF (%)
90
89
88
87
86
85
50-best
1-best
0.7
0.8
0.9
1.0
</figure>
<bodyText confidence="0.999549357142857">
test set and the orthography test set by 25%, 5.3%, and
26% respectively.
Finally, we would like to note that the proposed
LNgram approach provided the significant advantage
of a much smaller computational time compared to the
original Reg approach. While the Reg model requires
C times (C denotes the number of defined concepts) of
WFST operations to determine concepts, the LNgram
needs only D+1 times (D &lt;&lt; C), where D is the num-
ber of concepts appearing in the top hypothesis pro-
duced by the n-gram semantic model. Moreover, under
the framework of WFST, incorporating ASR N-best
hypotheses required only a small increment of addi-
tional processing time compared to the use of 1-best.
</bodyText>
<figure confidence="0.450667">
ConF GAcc CAcc
</figure>
<figureCaption confidence="0.9702275">
Figure 8. Comparative results for the ES set between
the use of ASR 1-best and N-best (N = 50) hypotheses.
</figureCaption>
<sectionHeader confidence="0.996649" genericHeader="conclusions">
6 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.999957913043478">
Recently, a multi-stage spoken language understanding
(SLU) approach has been proposed for the first Thai
spoken dialogue system. This article reported an im-
provement on the SLU system by replacing the regular
grammar-based semantic model by a hybrid n-gram
and regular grammar approach, which not only cap-
tures long-distant dependencies of word syntax, but
also provides robustness against speech-recognition
errors. The proposed model, called logical n-gram
modeling, obviously improved the performance in
every SLU stage, while reducing the computational
time compared to the original regular-grammar ap-
proach. Under the probabilistic WFST framework, the
system was improved further by using N-best word-
hypotheses from the ASR, requiring only a small addi-
tional processing time compared to the use of 1-best.
Further improvement of overall speech understand-
ing as well as a spoken dialogue system in the future
can be expected by introducing dialogue-state depend-
ent modeling in the ASR and/or the SLU. A better way
to utilize the first P-best goal hypotheses produced by
the goal identifier instead of 1-best would also en-
hance the understanding performance.
</bodyText>
<sectionHeader confidence="0.996445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99928847826087">
Béchet, F., Gorin, A., Wright, J., and Tur, D. H. 2002.
Named entity extraction from spontaneous speech in How
May I Help You. Proc. ICSLP 2002, 597-600.
Estève, Y., Raymond, C., Béchet, F., and De Mori, R. 2003.
Conceptual decoding for spoken dialogue systems. Proc.
Eurospeech 2003, 617-620.
Gorin, A. L., Riccardi, G., and Wright, J. H. 1997. How May
I Help You. Speech Communication, 23, 113-127.
Hacioglu, K., and Ward, W. 2001. Dialog-context dependent
language modeling combining n-grams and stochastic
context-free grammars. Proc. ICASSP 2001, 537-540.
Klakow, D. 1998. Log-linear interpolation of language mod-
els. Proc. ICSLP 1998, 1695-1699.
Miller, S., Bobrow, R., Ingria, R., and Schwartz, R. 1994.
Hidden understanding models of natural language. Proc.
ACL 1994, 25-32.
Mohri, M., Pereira, F., and Riley, M. 1997. General-purpose
finite-state machine software tools.
http://www.research.att.com/sw/tools/fsm, AT&amp;T Labs –
Research.
Potamianos, A., Kwang, H., and Kuo, J. 2000. Statistical
recursive finite state machine parsing for speech under-
standing. Proc. ICSLP 2000, vol.3, 510-513.
Seneff, S. 1992. TINA: A natural language system for spoken
language applications. Computational Linguistics, 18(1),
61-86.
Szarvas, M. and Furui, S. Finite-state transducer based
modeling of morphosyntax with applications to Hungar-
ian LVCSR. Proc. ICASSP 2003, 368-371.
Wang, Y. Y., Mahajan, M., and Huang, X. 2000. A unified
context-free grammar and n-gram model for spoken lan-
guage processing. Proc. ICASSP 2000, 1639-1642.
Wang, Y. Y., Acero, A., Chelba, C., Frey, B., and Wong, L.
2002. Combination of statistical and rule-based ap-
proaches for spoken language understanding. Proc.
ICSLP 2002, 609-612.
Wutiwiwatchai, C. and Furui, S. 2003a. Pioneering a Thai
Language Spoken Dialogue System. Spring Meeting of
Acoustic Society of Japan, 2-4-15, 87-88.
Wutiwiwatchai, C., and Furui, S. 2003b. Combination of
finite state automata and neural network for spoken lan-
guage understanding. Proc. EuroSpeech 2003, 2761-2764.
Zell, A., Mamier, G., Vogt, M., Mach, N., Huebner, R.,
Herrmann, K. U., Doering, S., and Posselt, D. SNNS
Stuttgart neural network simulator, user manual. Univer-
sity of Stuttgart.
</reference>
<figure confidence="0.998412461538462">
%
95
90
75
70
65
60
85
80
Reg
LNgram (1-best)
LNgram (50-best)
Orthgraphy
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955114">
<title confidence="0.9976945">Hybrid Statistical and Structural Semantic Modeling for Thai Multi- Stage Spoken Language Understanding</title>
<author confidence="0.998338">Wutiwiwatchai Furui</author>
<affiliation confidence="0.999992">Department of Computer Science, Tokyo Institute of Technology</affiliation>
<address confidence="0.982865">2-12-1 Ookayama, Meguro-ku, Tokyo, 152-8552 Japan.</address>
<email confidence="0.989904">chai@furui.cs.titech.ac.jp</email>
<email confidence="0.989904">furui@furui.cs.titech.ac.jp</email>
<abstract confidence="0.999392047619048">This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU). The first stage of this SLU utilizes a weighted finite-state transducer (WFST)-based parser, which encodes the regular grammar of concepts to be extracted. The proposed method improves the regular grammar model by ina well-known semantic tagger. This hybrid model thus enhances the of outputs while providing robustness against speech-recognition errors. With applications to a Thai hotel reservation domain, it is shown to outperform both individual models at every stage of the SLU system. Under the probabilistic WFST the use of hypotheses from the speech recognizer instead of the 1best can further improve performance requiring only a small additional processing time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Béchet</author>
<author>A Gorin</author>
<author>J Wright</author>
<author>D H Tur</author>
</authors>
<title>Named entity extraction from spontaneous speech in How May I Help You.</title>
<date>2002</date>
<booktitle>Proc. ICSLP</booktitle>
<pages>597--600</pages>
<contexts>
<context position="3763" citStr="Béchet et al. (2002)" startWordPosition="554" endWordPosition="557">he extracted concepts, the goal of the utterance can be identified in the goal identification component using a generalized pattern classifier. This article reports an improvement of the concept extraction and concept-value recognition parts by conducting a well-known statistical n-gram parser to compensate for the concept expressions, which cannot be recognized by the ordinary concept WFST. The ngram modeling alone lacks structural information as it captures only up to n-word dependencies. Combining the statistical and structural model for SLU hence becomes a better alternative. Motivated by Béchet et al. (2002), we propose a strategic way called logical ngram modeling, which combines the statistical n-gram with the existing regular grammar. In contrast to the regular-grammar approach, the probabilistic model allows the SLU to deal with ASR N-best hypotheses, resulting in an increment of the overall performance. Some related works are reviewed in the next section, followed by a description of our multi-stage SLU model. Section 4 explains the proposed hybrid model. Section 5 shows the evaluation results with a conclusion in section 6. 2 Related Works In the technology of trainable or data-driven SLU, </context>
<context position="6478" citStr="Béchet et al. (2002)" startWordPosition="993" endWordPosition="996">equired if one needs to obtain more detailed information. Our motivation for combining the two practices described above is that this allows the use of an only partially annotated corpus, while still allowing the system to capture sufficient information. The idea of combination has also been investigated in other works such as Wang et al. (2002). Another issue related to this article is the combination of a statistical and rule-based approach for SLU, a system which is expected to improve the overall performance over both individual approaches. The closest approach to our work was proposed by Béchet et al. (2002), aiming to extract named-entities (NEs) from an input utterance. NE extraction is performed in two steps, detecting the NEs by a statistical tagger and extracting NE values using local models. Estève et al. (2003) proposed a tighter coupling method that embeds conceptual structures into the ASR decoding network. Wang et al. (2000), and Hacioglu and Ward (2001) proposed similar ideas for unified models that incorporated domain-specific context-free grammars (CFGs) into domain-independent n-gram models. The hybrid models thus improved the generalized ability of the CFG and specificity of the n-</context>
<context position="17939" citStr="Béchet et al. 2002" startWordPosition="2869" endWordPosition="2872">ance by the Ngram WFST is performed simply by applying the WFST composition in the same way as operated with the Reg model. 4.2 Logical n-gram modeling Although the n-gram model can assign a likelihood score to any input utterance, it cannot distinguish between valid and invalid grammar structure. On the other hand, the regular grammar model can give semantic tags to an utterance that is permitted by the grammar, but always rejects an ungrammatical utterance. Thus, another probabilistic approach that integrates the advantages of both models is optimum. Our proposed model, motivated mainly by (Béchet et al. 2002), combines the statistical and structural models in two-pass processing. Firstly, the conventional n-gram model is used to generate M-best hypotheses of label sequences given an input word string. The likelihood score of each hypothesis is then enhanced once its word-and-label syntax is permitted by the regular grammar model. By rescoring the M-best list using the modified scores, the syntactically valid sequence that has the highest n-gram probability is reordered to the top. Even if no label sequence is permitted by the regular grammar, the hybrid model is still able to output the best seque</context>
</contexts>
<marker>Béchet, Gorin, Wright, Tur, 2002</marker>
<rawString>Béchet, F., Gorin, A., Wright, J., and Tur, D. H. 2002. Named entity extraction from spontaneous speech in How May I Help You. Proc. ICSLP 2002, 597-600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Estève</author>
<author>C Raymond</author>
<author>F Béchet</author>
<author>R De Mori</author>
</authors>
<title>Conceptual decoding for spoken dialogue systems.</title>
<date>2003</date>
<booktitle>Proc. Eurospeech</booktitle>
<pages>617--620</pages>
<marker>Estève, Raymond, Béchet, De Mori, 2003</marker>
<rawString>Estève, Y., Raymond, C., Béchet, F., and De Mori, R. 2003. Conceptual decoding for spoken dialogue systems. Proc. Eurospeech 2003, 617-620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
<author>G Riccardi</author>
<author>J H Wright</author>
</authors>
<title>How May I Help You.</title>
<date>1997</date>
<journal>Speech Communication,</journal>
<volume>23</volume>
<pages>113--127</pages>
<contexts>
<context position="5427" citStr="Gorin et al., 1997" startWordPosition="820" endWordPosition="823">hose nodes represent semantic symbols of the words and arcs consist of transition probabilities. During parsing, these probabilities are summed up, and used to determine the most likely parsed tree. Many understanding engines have been successfully implemented based on this paradigm (Seneff, 1992; Potamianos et al., 2000; Miller et al., 1994). A drawback of this method is, however, the requirement of a large, fully annotated corpus, i.e. a corpus with semantic tags on every word, to ensure training reliability. The second practice has been utilized in applications such as call classification (Gorin et al., 1997). In this application, the understanding module aims to classify an input utterance to one of predefined user goals (if an utterance is supposed to have one goal) directly from the words contained in the utterance. This problem can be considered a simple pattern classification task. An advantage of this method is the need for training utterances tagged only with their goals, one for each utterance. However, another process is required if one needs to obtain more detailed information. Our motivation for combining the two practices described above is that this allows the use of an only partially</context>
<context position="10250" citStr="Gorin et al. (1997)" startWordPosition="1590" endWordPosition="1593">o possesses the word ‘right’. We considered this method of definition to have more impact for presenting the meaning of concepts, compared to what has been defined in other works. It must be noted that some concepts contain values such as the concept ‘numperson’ (the number of people), whereas some do not, such as the concept ‘yesnoq’. ... Figure 2. A portion of regular grammar WFST for the concept ‘numperson’ (the number of people). We implemented the concept extraction component by using weighted finite-state transducers (WFSTs). Similar to the implementation of salient grammar fragments in Gorin et al. (1997), the possible word sequences expressed for a concept are encoded in a WFST, one for each type of concept. Figure 2 demonstrates a portion of WFST for the concept ‘numperson’. Each arc or transition of the WFST is labeled with an input word (or word class) followed after a colon by an output semantic label, and enclosed after a slash by a weight. A special symbol ‘NIL’ represents any word not included in the concept. The transitions, linking between the start and end node, characterize the acceptable word syntax. Weights of these transitions, except those containing ‘NIL’, are assigned to be -</context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>Gorin, A. L., Riccardi, G., and Wright, J. H. 1997. How May I Help You. Speech Communication, 23, 113-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hacioglu</author>
<author>W Ward</author>
</authors>
<title>Dialog-context dependent language modeling combining n-grams and stochastic context-free grammars.</title>
<date>2001</date>
<booktitle>Proc. ICASSP</booktitle>
<pages>537--540</pages>
<contexts>
<context position="6841" citStr="Hacioglu and Ward (2001)" startWordPosition="1052" endWordPosition="1055">sue related to this article is the combination of a statistical and rule-based approach for SLU, a system which is expected to improve the overall performance over both individual approaches. The closest approach to our work was proposed by Béchet et al. (2002), aiming to extract named-entities (NEs) from an input utterance. NE extraction is performed in two steps, detecting the NEs by a statistical tagger and extracting NE values using local models. Estève et al. (2003) proposed a tighter coupling method that embeds conceptual structures into the ASR decoding network. Wang et al. (2000), and Hacioglu and Ward (2001) proposed similar ideas for unified models that incorporated domain-specific context-free grammars (CFGs) into domain-independent n-gram models. The hybrid models thus improved the generalized ability of the CFG and specificity of the n-gram. With the existing regular grammar model in a weighted finite-state transducer (WFST) framework, we propose another strategy to incorporate the statistical n-gram model into the concept extraction and concept-value recognition components of our multi-stage SLU. 3 Multi-Stage SLU In the design of our spoken dialogue system, the dialogue manager decides to r</context>
</contexts>
<marker>Hacioglu, Ward, 2001</marker>
<rawString>Hacioglu, K., and Ward, W. 2001. Dialog-context dependent language modeling combining n-grams and stochastic context-free grammars. Proc. ICASSP 2001, 537-540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klakow</author>
</authors>
<title>Log-linear interpolation of language models.</title>
<date>1998</date>
<booktitle>Proc. ICSLP</booktitle>
<pages>1695--1699</pages>
<contexts>
<context position="20732" citStr="Klakow, 1998" startWordPosition="3346" endWordPosition="3347"> Hereafter, we will refer to the logical n-gram modeling as ‘LNgram’. 4.3 The use of ASR N-best hypotheses The probabilistic model allows the use of N-best hypotheses from the automatic speech recognition (ASR) engine. As described in Sect. 4.1, our Ngram semantic model produces a joint probability P(W,L), which indicates the chance that the semantic-label sequence L occurs with the word hypothesis W. When the N-best word hypotheses generated from the ASR are fed into the Ngram semantic parser, the parsed scores are combined with the ASR likelihood scores in a loglinear interpolation fashion (Klakow, 1998) as shown in Eq. 3. L W N , where A is an acoustic speech signal, and P(A,W) is a product of an acoustic score P(A|W) and a language score P(W). N denotes the N-best list and is an interpolation weight, which can be adjusted experimentally to give the best result. This interpolation method can be easily implemented in a WFST framework compared to normal linear interpolation. An N-best list can be used in the LNgram using the same criterion as well. The only necessary precaution is an appropriate size of M in the M-best semantic-label list, which is rescored in the second pass to improve the co</context>
</contexts>
<marker>Klakow, 1998</marker>
<rawString>Klakow, D. 1998. Log-linear interpolation of language models. Proc. ICSLP 1998, 1695-1699.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>R Bobrow</author>
<author>R Ingria</author>
<author>R Schwartz</author>
</authors>
<title>Hidden understanding models of natural language.</title>
<date>1994</date>
<booktitle>Proc. ACL</booktitle>
<pages>25--32</pages>
<contexts>
<context position="5152" citStr="Miller et al., 1994" startWordPosition="773" endWordPosition="776">ntic labels, which are later converted to a certain format of semantic representation. To generate such a semantic frame, words in the utterance are usually aligned to a semantic tree by a parsing algorithm such as a probabilistic context free grammar or a recursive network whose nodes represent semantic symbols of the words and arcs consist of transition probabilities. During parsing, these probabilities are summed up, and used to determine the most likely parsed tree. Many understanding engines have been successfully implemented based on this paradigm (Seneff, 1992; Potamianos et al., 2000; Miller et al., 1994). A drawback of this method is, however, the requirement of a large, fully annotated corpus, i.e. a corpus with semantic tags on every word, to ensure training reliability. The second practice has been utilized in applications such as call classification (Gorin et al., 1997). In this application, the understanding module aims to classify an input utterance to one of predefined user goals (if an utterance is supposed to have one goal) directly from the words contained in the utterance. This problem can be considered a simple pattern classification task. An advantage of this method is the need f</context>
</contexts>
<marker>Miller, Bobrow, Ingria, Schwartz, 1994</marker>
<rawString>Miller, S., Bobrow, R., Ingria, R., and Schwartz, R. 1994. Hidden understanding models of natural language. Proc. ACL 1994, 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>F Pereira</author>
<author>M Riley</author>
</authors>
<title>General-purpose finite-state machine software tools. http://www.research.att.com/sw/tools/fsm,</title>
<date>1997</date>
<journal>AT&amp;T Labs – Research.</journal>
<contexts>
<context position="24872" citStr="Mohri et al., 1997" startWordPosition="4050" endWordPosition="4053">acy (GAcc) – the number of utterances with correctly identified goals, divided by the total number of test utterances, 4. Concept-value accuracy (CAcc) – the number of concepts, whose values are correctly matched to their references, divided by the total number of concepts that contain values. 5.3 The use of logical n-gram modeling The first experiment was to inspect improvement gained after conducting the statistical approaches for concept extraction and concept-value recognition. Only the 1-best word hypothesis from the ASR was experimented in this section. The AT&amp;T generalized FSM library (Mohri et al., 1997) was used to construct and operate all WFSTs, and the SNNS toolkit (Zell et al., 1994) was used to create the ANN classifiers for the goal identification task. The baseline system utilized the Reg model for concept extraction and concept-value recognition, and the multi-layer perceptron ANN for goal identification. 75 WFSTs corresponding to the number of defined concepts were created from the TR set. The ANN consisted of a 75-node input layer, a 100-node hidden layer (Wutiwiwatchai and Furui, 2003b), and a 42- node output layer equal to the number of goals to be identified. 76 74 72 70 68 66 1</context>
</contexts>
<marker>Mohri, Pereira, Riley, 1997</marker>
<rawString>Mohri, M., Pereira, F., and Riley, M. 1997. General-purpose finite-state machine software tools. http://www.research.att.com/sw/tools/fsm, AT&amp;T Labs – Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Potamianos</author>
<author>H Kwang</author>
<author>J Kuo</author>
</authors>
<title>Statistical recursive finite state machine parsing for speech understanding.</title>
<date>2000</date>
<booktitle>Proc. ICSLP</booktitle>
<volume>3</volume>
<pages>510--513</pages>
<contexts>
<context position="5130" citStr="Potamianos et al., 2000" startWordPosition="769" endWordPosition="772">n the utterance with semantic labels, which are later converted to a certain format of semantic representation. To generate such a semantic frame, words in the utterance are usually aligned to a semantic tree by a parsing algorithm such as a probabilistic context free grammar or a recursive network whose nodes represent semantic symbols of the words and arcs consist of transition probabilities. During parsing, these probabilities are summed up, and used to determine the most likely parsed tree. Many understanding engines have been successfully implemented based on this paradigm (Seneff, 1992; Potamianos et al., 2000; Miller et al., 1994). A drawback of this method is, however, the requirement of a large, fully annotated corpus, i.e. a corpus with semantic tags on every word, to ensure training reliability. The second practice has been utilized in applications such as call classification (Gorin et al., 1997). In this application, the understanding module aims to classify an input utterance to one of predefined user goals (if an utterance is supposed to have one goal) directly from the words contained in the utterance. This problem can be considered a simple pattern classification task. An advantage of thi</context>
</contexts>
<marker>Potamianos, Kwang, Kuo, 2000</marker>
<rawString>Potamianos, A., Kwang, H., and Kuo, J. 2000. Statistical recursive finite state machine parsing for speech understanding. Proc. ICSLP 2000, vol.3, 510-513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>TINA: A natural language system for spoken language applications.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<pages>61--86</pages>
<contexts>
<context position="5105" citStr="Seneff, 1992" startWordPosition="767" endWordPosition="768">up of words) in the utterance with semantic labels, which are later converted to a certain format of semantic representation. To generate such a semantic frame, words in the utterance are usually aligned to a semantic tree by a parsing algorithm such as a probabilistic context free grammar or a recursive network whose nodes represent semantic symbols of the words and arcs consist of transition probabilities. During parsing, these probabilities are summed up, and used to determine the most likely parsed tree. Many understanding engines have been successfully implemented based on this paradigm (Seneff, 1992; Potamianos et al., 2000; Miller et al., 1994). A drawback of this method is, however, the requirement of a large, fully annotated corpus, i.e. a corpus with semantic tags on every word, to ensure training reliability. The second practice has been utilized in applications such as call classification (Gorin et al., 1997). In this application, the understanding module aims to classify an input utterance to one of predefined user goals (if an utterance is supposed to have one goal) directly from the words contained in the utterance. This problem can be considered a simple pattern classification </context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>Seneff, S. 1992. TINA: A natural language system for spoken language applications. Computational Linguistics, 18(1), 61-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Szarvas</author>
<author>S Furui</author>
</authors>
<title>Finite-state transducer based modeling of morphosyntax with applications to Hungarian LVCSR.</title>
<date>2003</date>
<booktitle>Proc. ICASSP</booktitle>
<pages>368--371</pages>
<contexts>
<context position="20118" citStr="Szarvas and Furui, 2003" startWordPosition="3245" endWordPosition="3248"> by where t { , 0} . If is set to 0, the intersection operation is just to filter out the hypotheses that violate the regular grammar, while the original scores from ngram model are left unaltered. If a larger is used, the hypothesis that contains a longer valid syntax is given a higher score. When no hypothesis in the M-best list is permitted by the grammar (P = 0), the top hypothesis of the M-best list is outputted. It is noted that the strategy of eliminating unacceptable paths of n-gram due to syntactical violation has also successfully been used in a WFST-based speech recognition system (Szarvas and Furui, 2003). Hereafter, we will refer to the logical n-gram modeling as ‘LNgram’. 4.3 The use of ASR N-best hypotheses The probabilistic model allows the use of N-best hypotheses from the automatic speech recognition (ASR) engine. As described in Sect. 4.1, our Ngram semantic model produces a joint probability P(W,L), which indicates the chance that the semantic-label sequence L occurs with the word hypothesis W. When the N-best word hypotheses generated from the ASR are fed into the Ngram semantic parser, the parsed scores are combined with the ASR likelihood scores in a loglinear interpolation fashion </context>
</contexts>
<marker>Szarvas, Furui, 2003</marker>
<rawString>Szarvas, M. and Furui, S. Finite-state transducer based modeling of morphosyntax with applications to Hungarian LVCSR. Proc. ICASSP 2003, 368-371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Y Wang</author>
<author>M Mahajan</author>
<author>X Huang</author>
</authors>
<title>A unified context-free grammar and n-gram model for spoken language processing.</title>
<date>2000</date>
<booktitle>Proc. ICASSP</booktitle>
<pages>1639--1642</pages>
<contexts>
<context position="6811" citStr="Wang et al. (2000)" startWordPosition="1047" endWordPosition="1050">t al. (2002). Another issue related to this article is the combination of a statistical and rule-based approach for SLU, a system which is expected to improve the overall performance over both individual approaches. The closest approach to our work was proposed by Béchet et al. (2002), aiming to extract named-entities (NEs) from an input utterance. NE extraction is performed in two steps, detecting the NEs by a statistical tagger and extracting NE values using local models. Estève et al. (2003) proposed a tighter coupling method that embeds conceptual structures into the ASR decoding network. Wang et al. (2000), and Hacioglu and Ward (2001) proposed similar ideas for unified models that incorporated domain-specific context-free grammars (CFGs) into domain-independent n-gram models. The hybrid models thus improved the generalized ability of the CFG and specificity of the n-gram. With the existing regular grammar model in a weighted finite-state transducer (WFST) framework, we propose another strategy to incorporate the statistical n-gram model into the concept extraction and concept-value recognition components of our multi-stage SLU. 3 Multi-Stage SLU In the design of our spoken dialogue system, the</context>
</contexts>
<marker>Wang, Mahajan, Huang, 2000</marker>
<rawString>Wang, Y. Y., Mahajan, M., and Huang, X. 2000. A unified context-free grammar and n-gram model for spoken language processing. Proc. ICASSP 2000, 1639-1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Y Wang</author>
<author>A Acero</author>
<author>C Chelba</author>
<author>B Frey</author>
<author>L Wong</author>
</authors>
<title>Combination of statistical and rule-based approaches for spoken language understanding.</title>
<date>2002</date>
<booktitle>Proc. ICSLP</booktitle>
<pages>609--612</pages>
<contexts>
<context position="6205" citStr="Wang et al. (2002)" startWordPosition="947" endWordPosition="950">l) directly from the words contained in the utterance. This problem can be considered a simple pattern classification task. An advantage of this method is the need for training utterances tagged only with their goals, one for each utterance. However, another process is required if one needs to obtain more detailed information. Our motivation for combining the two practices described above is that this allows the use of an only partially annotated corpus, while still allowing the system to capture sufficient information. The idea of combination has also been investigated in other works such as Wang et al. (2002). Another issue related to this article is the combination of a statistical and rule-based approach for SLU, a system which is expected to improve the overall performance over both individual approaches. The closest approach to our work was proposed by Béchet et al. (2002), aiming to extract named-entities (NEs) from an input utterance. NE extraction is performed in two steps, detecting the NEs by a statistical tagger and extracting NE values using local models. Estève et al. (2003) proposed a tighter coupling method that embeds conceptual structures into the ASR decoding network. Wang et al. </context>
</contexts>
<marker>Wang, Acero, Chelba, Frey, Wong, 2002</marker>
<rawString>Wang, Y. Y., Acero, A., Chelba, C., Frey, B., and Wong, L. 2002. Combination of statistical and rule-based approaches for spoken language understanding. Proc. ICSLP 2002, 609-612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wutiwiwatchai</author>
<author>S Furui</author>
</authors>
<title>Pioneering a Thai Language Spoken Dialogue System. Spring Meeting of Acoustic</title>
<date>2003</date>
<journal>Society of Japan,</journal>
<pages>2--4</pages>
<contexts>
<context position="1926" citStr="Wutiwiwatchai and Furui, 2003" startWordPosition="277" endWordPosition="280">ed towards creating fundamental tools for language processing such as phonological and morphological analyzers. Although Thai writing is an alphabetic system, a problem of writing without sentence markers or spaces between words has obstructed initiation of development of ASR. Pioneering a Thai spoken dialogue system has therefore become a challenging task, where several unique components need to be developed specifically for a Thai system. Our prototype dialogue system, namely Thai Interactive Hotel Reservation Agent (TIRA), was created mainly by handcrafted rules. The first user evaluation (Wutiwiwatchai and Furui, 2003a) showed that the spoken language understanding (SLU) part of the system proved the most problematic as it could not cover the variety of contents supplied by the users, especially when they talked in a mixed-initiative style. To rapidly improve performance, a trainable SLU model is preferable and it needs to be able to learn from a partially annotated corpus, where only essential keywords are given. This is particularly important for Thai where no large corpus is available. Recently, a novel multi-stage SLU model has been developed (Wutiwiwatchai and Furui, 2003b), which combines two differe</context>
<context position="7975" citStr="Wutiwiwatchai and Furui, 2003" startWordPosition="1229" endWordPosition="1232"> Multi-Stage SLU In the design of our spoken dialogue system, the dialogue manager decides to respond to the user after perceiving the user goal. In some types of goal, information items contained in the utterance are required for communication. For example the goal “request for facilities” must come with the facilities the user is asking for, and the goal “request for prerequisite keys” aims to have the user state the reserved date and the number of participants. Hence, the SLU module must be able to identify the goal and extract the required information items. We proposed a novel SLU model (Wutiwiwatchai and Furui, 2003b) that processes an input utterance in three stages, concept extraction, goal identification, and concept-value recognition. Figure 1 illustrates the overall architecture of the SLU model, in which its components are described in detail as follows: Word string Concept extraction Concepts Concept-value recognition Goal Concept-values Figure 1. Overall architecture of the multi-stage SLU. 3.1 Concept extraction The function of concept extraction is similar to that of other works, aiming to extract a set of concepts from an input utterance. However, our way to define a concept is rather differen</context>
<context position="13433" citStr="Wutiwiwatchai and Furui, 2003" startWordPosition="2128" endWordPosition="2131"> our case can be considered as a derivative of the dialogue act coupled with additional information. As the examples show in Table 1, the goal ‘request_facility’ means a request (dialogue act) for some facilities (additional information). Since we observed in our largest corpus that only 1.1% were multiple-goal utterances, an utterance could be supposed to have only one goal. The goal identification task can be viewed as a simple pattern classification problem, where a goal is identified given an input vector of binary values indicating the existence of predefined concepts. Our previous work (Wutiwiwatchai and Furui, 2003b) showed that this task could be efficiently achieved by the simple multi-layer perceptron type of artificial neural network (ANN). I: /0 NIL: /0 DGT: /0 NIL: /0 ... I: ac /-1 DGT: nc /-1 person: /-1 S E DGT: np /-1 I : /0 NIL: /0 DGT: /0 friend: ac /-1 3.3 Concept-value recognition Recall again that some concepts contain values such as the concept ‘numperson’, whose value is the number of people, whereas some concepts do not, such as the concept ‘yesnoq’. Given an input utterance, the SLU module must be able to identify the goal and extract information items such as the reserved date, the nu</context>
<context position="25374" citStr="Wutiwiwatchai and Furui, 2003" startWordPosition="4131" endWordPosition="4134">the 1-best word hypothesis from the ASR was experimented in this section. The AT&amp;T generalized FSM library (Mohri et al., 1997) was used to construct and operate all WFSTs, and the SNNS toolkit (Zell et al., 1994) was used to create the ANN classifiers for the goal identification task. The baseline system utilized the Reg model for concept extraction and concept-value recognition, and the multi-layer perceptron ANN for goal identification. 75 WFSTs corresponding to the number of defined concepts were created from the TR set. The ANN consisted of a 75-node input layer, a 100-node hidden layer (Wutiwiwatchai and Furui, 2003b), and a 42- node output layer equal to the number of goals to be identified. 76 74 72 70 68 66 10 20 30 40 50 60 70 80 90 100 M-best Figure 4. CAcc results with respect to values of M in an oracle test for the DS set. 64 63 62 61 60 59 58 Figure 5. CAcc results with variation of for the DS set when M is set to 80. Measure Recognition Orthography Reg Ngram LNgram Reg LNgram ConF 76.5 88.6 78.9 91.4 GAcc 71.4 76.0 81.2 83.5 CAcc 65.1 52.4 67.2 75.7 76.8 Table 4. Evaluation results for the ES set using the Reg, Ngram, and LNgram models. Another WFST was constructed for the n-gram semantic parse</context>
</contexts>
<marker>Wutiwiwatchai, Furui, 2003</marker>
<rawString>Wutiwiwatchai, C. and Furui, S. 2003a. Pioneering a Thai Language Spoken Dialogue System. Spring Meeting of Acoustic Society of Japan, 2-4-15, 87-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wutiwiwatchai</author>
<author>S Furui</author>
</authors>
<title>Combination of finite state automata and neural network for spoken language understanding.</title>
<date>2003</date>
<booktitle>Proc. EuroSpeech</booktitle>
<pages>2761--2764</pages>
<contexts>
<context position="1926" citStr="Wutiwiwatchai and Furui, 2003" startWordPosition="277" endWordPosition="280">ed towards creating fundamental tools for language processing such as phonological and morphological analyzers. Although Thai writing is an alphabetic system, a problem of writing without sentence markers or spaces between words has obstructed initiation of development of ASR. Pioneering a Thai spoken dialogue system has therefore become a challenging task, where several unique components need to be developed specifically for a Thai system. Our prototype dialogue system, namely Thai Interactive Hotel Reservation Agent (TIRA), was created mainly by handcrafted rules. The first user evaluation (Wutiwiwatchai and Furui, 2003a) showed that the spoken language understanding (SLU) part of the system proved the most problematic as it could not cover the variety of contents supplied by the users, especially when they talked in a mixed-initiative style. To rapidly improve performance, a trainable SLU model is preferable and it needs to be able to learn from a partially annotated corpus, where only essential keywords are given. This is particularly important for Thai where no large corpus is available. Recently, a novel multi-stage SLU model has been developed (Wutiwiwatchai and Furui, 2003b), which combines two differe</context>
<context position="7975" citStr="Wutiwiwatchai and Furui, 2003" startWordPosition="1229" endWordPosition="1232"> Multi-Stage SLU In the design of our spoken dialogue system, the dialogue manager decides to respond to the user after perceiving the user goal. In some types of goal, information items contained in the utterance are required for communication. For example the goal “request for facilities” must come with the facilities the user is asking for, and the goal “request for prerequisite keys” aims to have the user state the reserved date and the number of participants. Hence, the SLU module must be able to identify the goal and extract the required information items. We proposed a novel SLU model (Wutiwiwatchai and Furui, 2003b) that processes an input utterance in three stages, concept extraction, goal identification, and concept-value recognition. Figure 1 illustrates the overall architecture of the SLU model, in which its components are described in detail as follows: Word string Concept extraction Concepts Concept-value recognition Goal Concept-values Figure 1. Overall architecture of the multi-stage SLU. 3.1 Concept extraction The function of concept extraction is similar to that of other works, aiming to extract a set of concepts from an input utterance. However, our way to define a concept is rather differen</context>
<context position="13433" citStr="Wutiwiwatchai and Furui, 2003" startWordPosition="2128" endWordPosition="2131"> our case can be considered as a derivative of the dialogue act coupled with additional information. As the examples show in Table 1, the goal ‘request_facility’ means a request (dialogue act) for some facilities (additional information). Since we observed in our largest corpus that only 1.1% were multiple-goal utterances, an utterance could be supposed to have only one goal. The goal identification task can be viewed as a simple pattern classification problem, where a goal is identified given an input vector of binary values indicating the existence of predefined concepts. Our previous work (Wutiwiwatchai and Furui, 2003b) showed that this task could be efficiently achieved by the simple multi-layer perceptron type of artificial neural network (ANN). I: /0 NIL: /0 DGT: /0 NIL: /0 ... I: ac /-1 DGT: nc /-1 person: /-1 S E DGT: np /-1 I : /0 NIL: /0 DGT: /0 friend: ac /-1 3.3 Concept-value recognition Recall again that some concepts contain values such as the concept ‘numperson’, whose value is the number of people, whereas some concepts do not, such as the concept ‘yesnoq’. Given an input utterance, the SLU module must be able to identify the goal and extract information items such as the reserved date, the nu</context>
<context position="25374" citStr="Wutiwiwatchai and Furui, 2003" startWordPosition="4131" endWordPosition="4134">the 1-best word hypothesis from the ASR was experimented in this section. The AT&amp;T generalized FSM library (Mohri et al., 1997) was used to construct and operate all WFSTs, and the SNNS toolkit (Zell et al., 1994) was used to create the ANN classifiers for the goal identification task. The baseline system utilized the Reg model for concept extraction and concept-value recognition, and the multi-layer perceptron ANN for goal identification. 75 WFSTs corresponding to the number of defined concepts were created from the TR set. The ANN consisted of a 75-node input layer, a 100-node hidden layer (Wutiwiwatchai and Furui, 2003b), and a 42- node output layer equal to the number of goals to be identified. 76 74 72 70 68 66 10 20 30 40 50 60 70 80 90 100 M-best Figure 4. CAcc results with respect to values of M in an oracle test for the DS set. 64 63 62 61 60 59 58 Figure 5. CAcc results with variation of for the DS set when M is set to 80. Measure Recognition Orthography Reg Ngram LNgram Reg LNgram ConF 76.5 88.6 78.9 91.4 GAcc 71.4 76.0 81.2 83.5 CAcc 65.1 52.4 67.2 75.7 76.8 Table 4. Evaluation results for the ES set using the Reg, Ngram, and LNgram models. Another WFST was constructed for the n-gram semantic parse</context>
</contexts>
<marker>Wutiwiwatchai, Furui, 2003</marker>
<rawString>Wutiwiwatchai, C., and Furui, S. 2003b. Combination of finite state automata and neural network for spoken language understanding. Proc. EuroSpeech 2003, 2761-2764.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Zell</author>
<author>G Mamier</author>
<author>M Vogt</author>
<author>N Mach</author>
<author>R Huebner</author>
<author>K U Herrmann</author>
<author>S Doering</author>
<author>D Posselt</author>
</authors>
<title>SNNS Stuttgart neural network simulator, user manual.</title>
<institution>University of Stuttgart.</institution>
<marker>Zell, Mamier, Vogt, Mach, Huebner, Herrmann, Doering, Posselt, </marker>
<rawString>Zell, A., Mamier, G., Vogt, M., Mach, N., Huebner, R., Herrmann, K. U., Doering, S., and Posselt, D. SNNS Stuttgart neural network simulator, user manual. University of Stuttgart.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>