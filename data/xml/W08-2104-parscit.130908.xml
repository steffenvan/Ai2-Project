<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000051">
<title confidence="0.997176">
Linguistic features in data-driven dependency parsing
</title>
<author confidence="0.983406">
Lilja Øvrelid
</author>
<affiliation confidence="0.9813425">
NLP-unit, Dept. of Swedish
University of Gothenburg
</affiliation>
<address confidence="0.533546">
Sweden
</address>
<email confidence="0.996031">
lilja.ovrelid@svenska.gu.se
</email>
<sectionHeader confidence="0.993806" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999775307692308">
This article investigates the effect of a set
of linguistically motivated features on ar-
gument disambiguation in data-driven de-
pendency parsing of Swedish. We present
results from experiments with gold stan-
dard features, such as animacy, definite-
ness and finiteness, as well as correspond-
ing experiments where these features have
been acquired automatically and show
significant improvements both in overall
parse results and in the analysis of specific
argument relations, such as subjects, ob-
jects and predicatives.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999867944444444">
Data-driven dependency parsing has recently re-
ceived extensive attention in the parsing commu-
nity and impressive results have been obtained for
a range of languages (Nivre et al., 2007). Even
with high overall parsing accuracy, however, data-
driven parsers often make errors in the assign-
ment of argument relations such as subject and
object and the exact influence of data-derived fea-
tures on the parsing accuracy for specific linguistic
constructions is still relatively poorly understood.
There are a number of studies that investigate the
influence of different features or representational
choices on overall parsing accuracy, (Bod, 1998;
Klein and Manning, 2003). There are also attempts
at a more fine-grained analysis of accuracy, target-
ing specific linguistic constructions or grammati-
cal functions (Carroll and Briscoe, 2002; K¨ubler
and Proki´c, 2006; McDonald and Nivre, 2007).
</bodyText>
<note confidence="0.644566">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.883351666666667">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999966655172414">
But there are few studies that combine the two per-
spectives and try to tease apart the influence of dif-
ferent features on the analysis of specific construc-
tions, let alone motivated by a thorough linguistic
analysis.
In this paper, we investigate the influence of a
set of linguistically motivated features on parse re-
sults for Swedish, and in particular on the analysis
of argument relations such as subjects, objects and
subject predicatives. Motivated by an error anal-
ysis of the best performing parser for Swedish in
the CoNLL-X shared task, we extend the feature
model employed by the parser with a set of lin-
guistically motivated features and go on to show
how these features may be acquired automatically.
We then present results from corresponding parse
experiments with automatic features.
The rest of the paper is structured as follows. In
section 2 we present relevant properties of Swedish
morphosyntax, as well as the treebank and parser
employed in the experiments. Section 3 presents
an error analysis of the baseline parser and we go
on to motivate a set of linguistic features in sec-
tion 4, which are employed in a set of experiments
with gold standard features, discussed in section
5. Section 6 presents the automatic acquisition of
these features, with a particular focus on animacy
classification and in section 7 we report parse ex-
periments with automatic features.
</bodyText>
<sectionHeader confidence="0.955289" genericHeader="introduction">
2 Parsing Swedish
</sectionHeader>
<bodyText confidence="0.999770571428572">
Before we turn to a description of the treebank
and the parser used in the experiments, we want to
point to a few grammatical properties of Swedish
that will be important in the following:
Verb second (V2) Swedish is, like the majority of
Germanic languages a V2-language; the fi-
nite verb always resides in second position in
</bodyText>
<page confidence="0.983473">
25
</page>
<note confidence="0.87966">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 25–32
Manchester, August 2008
</note>
<bodyText confidence="0.986230666666667">
declarative main clauses.
Word order variation Pretty much any con-
stituent may occupy the sentence-initial po-
sition, but subjects are most common.
Limited case marking Nouns are only inflected
for genitive case. Personal pronouns dis-
tinguish nominative and accusative case, but
demonstratives and quantifying pronouns are
case ambiguous (like nouns).
</bodyText>
<subsectionHeader confidence="0.7656">
2.1 Treebank: Talbanken05
</subsectionHeader>
<bodyText confidence="0.994203416666667">
Talbanken05 is a Swedish treebank converted to
dependency format, containing both written and
spoken language (Nivre et al., 2006a).1 For each
token, Talbanken05 contains information on word
form, part of speech, head and dependency rela-
tion, as well as various morphosyntactic and/or
lexical semantic features. The nature of this ad-
ditional information varies depending on part of
speech:
NOUN: definiteness, animacy, case (Ø/GEN)
PRO: animacy, case (Ø/ACC)
VERB: tense, voice (Ø/PA)
</bodyText>
<subsectionHeader confidence="0.998968">
2.2 Parser: MaltParser
</subsectionHeader>
<bodyText confidence="0.998816260869565">
We use the freely available MaltParser,2 which
is a language-independent system for data-driven
dependency parsing. MaltParser is based on
a deterministic parsing strategy, first proposed
by Nivre (2003), in combination with treebank-
induced classifiers for predicting the next parsing
action. Classifiers can be trained using any ma-
chine learning approach, but the best results have
so far been obtained with support vector machines,
using LIBSVM (Chang and Lin, 2001). Malt-
Parser has a wide range of parameters that need to
be optimized when parsing a new language. As
our baseline, we use the settings optimized for
Swedish in the CoNLL-X shared task (Nivre et al.,
2006b), where this parser was the best perform-
ing parser for Swedish. The only parameter that
will be varied in the later experiments is the fea-
ture model used for the prediction of the next pars-
ing action. Hence, we need to describe the feature
model in a little more detail.
MaltParser uses two main data structures, a
stack (S) and an input queue (I), and builds a de-
pendency graph (G) incrementally in a single left-
</bodyText>
<footnote confidence="0.9981815">
1The written sections of the treebank consist of profes-
sional prose and student essays and amount to 197,123 run-
ning tokens, spread over 11,431 sentences.
2http://w3.msi.vxu.se/users/nivre/research/MaltParser.html
</footnote>
<equation confidence="0.692472">
FORM POS DEP FEATS
S:top + + + +
S:top+1 +
I:next + + +
I:next−1 + +
I:next+1 + + +
I:next+2 +
</equation>
<table confidence="0.89765825">
G: head of top + +
G: left dep of top +
G: right dep of top +
G: left dep of next + + +
G: left dep of head of top +
G: left sibling of right dep of top +
G: right sibling of left dep of top + +
G: right sibling of left dep of next + +
</table>
<tableCaption confidence="0.762009333333333">
Table 1: Baseline and extended (FEATS) feature
model for Swedish; S: stack, I: input, G: graph;
±n = n positions to the left(−) or right (+)
</tableCaption>
<bodyText confidence="0.999628578947369">
to-right pass over the input. The decision that
needs to be made at any point during this deriva-
tion is (a) whether to add a dependency arc (with
some label) between the token on top of the stack
(top) and the next token in the input queue (next),
and (b) whether to pop top from the stack or push
next onto the stack. The features fed to the classi-
fier for making these decisions naturally focus on
attributes of top, next and neighbouring tokens in
S, I or G. In the baseline feature model, these at-
tributes are limited to the word form (FORM), part
of speech (POS), and dependency relation (DEP) of
a given token, but in later experiments we will add
other linguistic features (FEATS). The baseline fea-
ture model is depicted as a matrix in Table 1, where
rows denote tokens in the parser configuration (de-
fined relative to S, I and G) and columns denote
attributes. Each cell containing a + corresponds to
a feature of the model.
</bodyText>
<sectionHeader confidence="0.989716" genericHeader="method">
3 Baseline and Error Analysis
</sectionHeader>
<bodyText confidence="0.999905">
The written part of Talbanken05 was parsed em-
ploying the baseline feature model detailed above,
using 10-fold cross validation for training and test-
ing. The overall result for unlabeled and labeled
dependency accuracy is 89.87 and 84.92 respec-
tively.3
Error analysis shows that the overall most fre-
quent errors in terms of dependency relations in-
volve either various adverbial relations, due to PP-
attachment ambiguities and a large number of ad-
</bodyText>
<footnote confidence="0.9996948">
3Note that these results are slightly better than the official
CoNLL-X shared task scores (89.50/84.58), which were ob-
tained using a single training-test split, not cross-validation.
Note also that, in both cases, the parser input contained gold
standard part-of-speech tags.
</footnote>
<page confidence="0.99246">
26
</page>
<table confidence="0.999131333333333">
Gold Sys before after Total
SS OO 103 (23.1%) 343 (76.9%) 446 (100%)
OO SS 103 (33.3%) 206 (66.7%) 309 (100%)
</table>
<tableCaption confidence="0.945479">
Table 2: Position relative to verb for confused sub-
jects and objects
</tableCaption>
<bodyText confidence="0.98276374025974">
verbial labels, or the argument relations, such as
subjects, direct objects, formal subjects and sub-
ject predicatives. In particular, confusion of argu-
ment relations are among the most frequent error
types with respect to dependency assignment.4
Swedish exhibits some ambiguities in word or-
der and morphology which follow from the proper-
ties discussed above. We will exemplify these fac-
tors through an analysis of the errors where sub-
jects are assigned object status (SS OO) and vice
versa (OO SS). The confusion of subjects and ob-
jects follows from lack of sufficient formal disam-
biguation, i.e., simple clues such as word order,
part-of-speech and word form do not clearly indi-
cate syntactic function.
With respect to word order, subjects and objects
may both precede or follow their verbal head. Sub-
jects, however, are more likely to occur prever-
bally (77%), whereas objects typically occupy a
postverbal position (94%). We would therefore ex-
pect postverbal subjects and preverbal objects to be
more dominant among the errors than in the tree-
bank as a whole (23% and 6% respectively). Table
2 shows a breakdown of the errors for confused
subjects and objects and their position with respect
to the verbal head. We find that postverbal subjects
(after) are in clear majority among the subjects er-
roneously assigned the object relation. Due to the
V2 property of Swedish, the subject must reside
in the position directly following the finite verb
whenever another constituent occupies the prever-
bal position, as in (1) where a direct object resides
sentence-initially:
(1) Samma erfarenhet gjorde engelsm¨annen
same experience made englishmen-DEF
‘The same experience, the Englishmen had’
For the confused objects we find a larger propor-
tion of preverbal elements than for subjects, which
4We define argument relations as dependency relations
which obtain between a verb and a dependent which is
subcategorized for and/or thematically entailed by the verb.
Note that arguments are not distinguished structurally from
non-arguments, like adverbials, in dependency grammar, but
through dependency label.
is the mirror image of the normal distribution of
syntactic functions among preverbal elements. As
Table 2 shows, the proportion of preverbal ele-
ments among the subject-assigned objects (33.3%)
is notably higher than in the corpus as a whole,
where preverbal objects account for a miniscule
6% of all objects.
In addition to the word order variation dis-
cussed above, Swedish also has limited morpho-
logical marking of syntactic function. Nouns are
marked only for genitive case and only pronouns
are marked for accusative case. There is also syn-
cretism in the pronominal paradigm where the pro-
noun is invariant for case, e.g. det, den ‘it’, in-
gen/inga ‘no’, and may, in fact, also function as
a determiner. This means that, with respect to
word form, only the set of unambiguous pronouns
clearly indicate syntactic function. In the errors,
we find that nouns and functionally ambiguous
pronouns dominate the errors where subjects and
objects are confused, accounting for 84.5% of the
SS OO and 93.5% of the OO SS errors.
The initial error analysis shows that the confu-
sion of argument relations constitutes a frequent
and consistent error during parsing. Ambiguities
in word order and morphological marking consti-
tute a complicating factor and we find cases that
deviate from the most frequent word order pat-
terns and are not formally disambiguated by part-
of-speech information. It is clear that we in order
to resolve these ambiguities have to examine fea-
tures beyond syntactic category and linear word or-
der.
</bodyText>
<sectionHeader confidence="0.9620825" genericHeader="method">
4 Linguistic features for argument
disambiguation
</sectionHeader>
<bodyText confidence="0.9999483125">
Argument relations tend to differ along several lin-
guistic dimensions. These differences are found
as statistical tendencies, rather than absolute re-
quirements on syntactic structure. The property
of animacy, a referential property of nominal el-
ements, has been argued to play a role in argument
realization in a range of languages see de Swart
et.al. (2008) for an overview. It is closely cor-
related with the semantic property of agentivity,
hence subjects will tend to be referentially animate
more often than objects. Another property which
may differentiate between the argument functions
is the property of definiteness, which can be linked
with a notion of givenness, (Weber and M¨uller,
2004). This is reflected in the choice of refer-
ring expression for the various argument types in
</bodyText>
<page confidence="0.987474">
27
</page>
<bodyText confidence="0.999785647058824">
Talbanken05 – subjects are more often pronominal
(49.2%), whereas objects and subject predicatives
are typically realized by an indefinite noun (67.6%
and 89.6%, respectively). As mentioned in section
2, there are categorical constraints which are char-
acteristic for Swedish morphosyntax. Even if the
morphological marking of arguments in Scandina-
vian is not extensive or unambiguous, case may
distinguish arguments. Only subjects may follow
a finite verb and precede a non-finite verb and only
complements may follow a non-finite verb. Infor-
mation on tense or the related finiteness is there-
fore something that one might assume to be ben-
eficial for argument analysis. Another property of
the verb which clearly influences the assignment
of core argument functions is the voice of the verb,
i.e., whether it is passive or active.5
</bodyText>
<sectionHeader confidence="0.9926495" genericHeader="method">
5 Experiments with gold standard
features
</sectionHeader>
<bodyText confidence="0.999985">
We perform a set of experiments with an extended
feature model and added, gold standard informa-
tion on animacy, definiteness, case, finiteness and
voice, where the features were employed individu-
ally as well as in combination.
</bodyText>
<subsectionHeader confidence="0.909138">
5.1 Experimental methodology
</subsectionHeader>
<bodyText confidence="0.999779857142857">
All parsing experiments are performed using 10-
fold cross-validation for training and testing on
the entire written part of Talbanken05. The fea-
ture model used throughout is the extended fea-
ture model depicted in Table 1, including all four
columns.6 Hence, what is varied in the exper-
iments is only the information contained in the
FEATS features (animacy, definiteness, etc.), while
the tokens for which these features are defined re-
mains constant. Overall parsing accuracy will be
reported using the standard metrics of labeled at-
tachment score (LAS) and unlabeled attachment
score (UAS).7 Statistical significance is checked
using Dan Bikel’s randomized parsing evaluation
</bodyText>
<footnote confidence="0.650490285714286">
5We experimented with the use of tense as well as finite-
ness, a binary feature which was obtained by a mapping from
tense to finite/non-finite. Finiteness gave significantly better
results (p&lt;.03) and was therefore employed in the following,
see (Øvrelid, 2008b) for details.
6Preliminary experiments showed that it was better to tie
FEATS features to the same tokens as FORM features (rather
than POS or DEP features). Backward selection from this
model was tried for several different instantiations of FEATS
but with no significant improvement.
7LAS and UAS report the percentage of tokens that are as-
signed the correct head with (labeled) or without (unlabeled)
the correct dependency label, calculated using eval.pl with de-
fault settings (http://nextens.uvt.nl/—conll/software.html)
</footnote>
<bodyText confidence="0.99634525">
comparator.8 Since the main focus of this article is
on the disambiguation of grammatical functions,
we report accuracy for specific dependency rela-
tions, measured as a balanced F-score.
</bodyText>
<subsectionHeader confidence="0.707597">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999985527777778">
The overall results for these experiments are pre-
sented in table 3, along with p-scores. The exper-
iments show that each feature individually causes
a significant improvement in terms of overall la-
beled accuracy as well as performance for argu-
ment relations. Error analysis comparing the base-
line parser (NoFeats) with new parsers trained with
individual features reveal the influence of these
features on argument disambiguation. We find
that animacy influences the disambiguation of sub-
jects from objects, objects from indirect objects
as well as the general distinction of arguments
from non-arguments. Definiteness has a notable
effect on the disambiguation of subjects and sub-
ject predicatives. Information on morphological
case shows a clear effect in distinguishing between
arguments and non-arguments, and in particular,
in distinguishing nominal modifiers with genitive
case. The added verbal features, finiteness and
voice, have a positive effect on the verbal depen-
dency relations, as well as an overall effect on the
assignment of the SS and OO argument relations.
Information on voice also benefits the relation ex-
pressing the demoted agent (AG) in passive con-
structions, headed by the preposition av ‘by’, as in
English.
The ADCV experiment which combines infor-
mation on animacy, definiteness, case and verbal
features shows a cumulative effect of the added
features with results which differ significantly
from the baseline, as well as from each of the in-
dividual experiments (p&lt;.0001). We observe clear
improvements for the analysis of all argument re-
lations, as shown by the third column in table 4
which presents F-scores for the various argument
relations.
</bodyText>
<sectionHeader confidence="0.981189" genericHeader="method">
6 Acquiring features
</sectionHeader>
<bodyText confidence="0.999825571428571">
A possible objection to the general applicability
of the results presented above is that the added
information consists of gold standard annotation
from a treebank. However, the morphosyntactic
features examined here (definiteness, case, tense,
voice) represent standard output from most part-
of-speech taggers. In the following we will also
</bodyText>
<footnote confidence="0.951875">
8http://www.cis.upenn.edu/—dbikel/software.html
</footnote>
<page confidence="0.995067">
28
</page>
<table confidence="0.999286625">
UAS LAS p-value
NoFeats 89.87 84.92 –
Anim 89.93 85.10 p&lt;.0002
Def 89.87 85.02 p&lt;.02
Case 89.99 85.13 p&lt;.0001
Verb 90.24 85.38 p&lt;.0001
ADC 90.13 85.35 p&lt;.0001
ADCV 90.40 85.68 p&lt;.0001
</table>
<tableCaption confidence="0.981732333333333">
Table 3: Overall results in gold standard ex-
periments expressed as unlabeled and labeled
attachment scores.
</tableCaption>
<figure confidence="0.828953142857143">
Feature Application
Definiteness POS-tagger
Case POS-tagger
Animacy - NN Animacy classifier
Animacy - PN Named Entity Tagger
Animacy - PO Majority class
Tense (finiteness), voice POS-tagger
</figure>
<tableCaption confidence="0.789688">
Table 5: Overview of applications employed for
automatic feature acquisition.
</tableCaption>
<bodyText confidence="0.999965">
show that the property of animacy can be fairly
robustly acquired for common nouns by means
of distributional features from an automatically
parsed corpus.
Table 5 shows an overview of the applications
employed for the automatic acquisition of our lin-
guistic features. For part-of-speech tagging, we
employ MaltTagger – a HMM part-of-speech tag-
ger for Swedish (Hall, 2003). The POS-tagger dis-
tinguishes tense and voice for verbs, nominative
and accusative case for pronouns, as well as defi-
niteness and genitive case for nouns.
</bodyText>
<subsectionHeader confidence="0.995904">
6.1 Animacy
</subsectionHeader>
<bodyText confidence="0.995062777777778">
The feature of animacy is clearly the most chal-
lenging feature to acquire automatically. Recall
that Talbanken05 distinguishes animacy for all
nominal constituents. In the following we describe
the automatic acquisition of animacy information
for common nouns, proper nouns and pronouns.
Common nouns Table 6 presents an overview
of the animacy data for common nouns in Tal-
banken05. It is clear that the data is highly skewed
</bodyText>
<table confidence="0.999604727272727">
NoFeats Gold Auto
SS subject 90.25 91.80 91.32
OO object 84.53 86.27 86.10
SP subj.pred. 84.82 85.87 85.80
AG pass. agent 73.56 81.34 81.02
ES logical subj. 71.82 73.44 72.60
FO formal obj. 56.68 65.64 65.38
VO obj. small clause 72.10 83.40 83.12
VS subj. small clause 58.75 65.56 68.75
FS formal subj. 71.31 72.10 71.31
IO indir. obj. 76.14 77.76 76.29
</table>
<tableCaption confidence="0.925616666666667">
Table 4: F-scores for argument relations with
combined features (ADCV).
Class Types Tokens covered
</tableCaption>
<table confidence="0.662597333333333">
Animate 644 6010
Inanimate 6910 34822
Total 7554 40832
</table>
<tableCaption confidence="0.772236">
Table 6: The animacy data set from Talbanken05;
</tableCaption>
<bodyText confidence="0.975335166666667">
number of noun lemmas (Types) and tokens in
each class.
towards the non-person class, which accounts for
91.5% of the data instances. Due to the small size
of the treebank we classify common noun lem-
mas based on their morphosyntactic distribution
in a considerably larger corpus. For the animacy
classification of common nouns, we construct a
general feature space for animacy classification,
which makes use of distributional data regarding
syntactic properties of the noun, as well as various
morphological properties. The syntactic and mor-
phological features in the general feature space are
presented below:
Syntactic features A feature for each dependency
relation with nominal potential: (transitive)
subject (SUBJ), object (OBJ), prepositional
complement (PA), root (ROOT)9, apposition
(APP), conjunct (CC), determiner (DET), pred-
icative (PRD), complement of comparative
subjunction (UK). We also include a feature
for the complement of a genitive modifier, the
so-called ‘possessee’, (GENHD).
Morphological features A feature for each mor-
</bodyText>
<footnote confidence="0.9474665">
9Nominal elements may be assigned the root relation in
sentence fragments which do not include a finite verb.
</footnote>
<page confidence="0.999297">
29
</page>
<bodyText confidence="0.999963378378378">
phological distinction relevant for a noun:
gender (NEU/UTR), number (SIN/PLU), defi-
niteness (DEF/IND), case (NOM/GEN). Also,
the part-of-speech tags distinguish dates
(DAT) and quantifying nouns (SET), e.g. del,
rad ‘part, row’, so these are also included as
features.
For extraction of distributional data for the Tal-
banken05 nouns we make use of the Swedish Pa-
role corpus of 21.5M tokens.10 To facilitate feature
extraction, we part-of-speech tag the corpus and
parse it with MaltParser, which assigns a depen-
dency analysis.11 For classification, we make use
of the Tilburg Memory-Based Learner (TiMBL)
(Daelemans et al., 2004).12 and optimize the
TiMBL parameters on a subset of the full data
set.13
We obtain results for animacy classification of
noun lemmas, ranging from 97.3% accuracy to
94.0% depending on the sparsity of the data. With
an absolute frequency threshold of 10, we obtain
an accuracy of 95.4%, which constitutes a 50%
reduction of error rate over a majority baseline.
We find that classification of the inanimate class is
quite stable throughout the experiments, whereas
the classification of the minority class of animate
nouns suffers from sparse data. We obtain a F-
score of 71.8% F-score for the animate class and
97.5% for the inanimate class with a threshold of
10. The common nouns in Talbanken05 are classi-
fied for animacy following a leave-one-out training
and testing scheme where each of the n nouns in
Talbanken05 are classified with a classifier trained
on n − 1 instances. This ensures that the training
and test instances are disjoint at all times. More-
over, the fact that the distributional data is taken
from a separate data set ensures non-circularity
</bodyText>
<footnote confidence="0.9800545">
10Parole is available at http://spraakbanken.gu.se
11For part-of-speech tagging, we employ the MaltTagger –
a HMM part-of-speech tagger for Swedish (Hall, 2003). For
parsing, we employ MaltParser with a pretrained model for
Swedish, which has been trained on the tags output by the
tagger. It makes use of a smaller set of dependency relations
than those found in Talbanken05.
12TiMBL is freely available at
http://ilk.uvt.nl/software.html
13For parameter optimization we employ the
paramsearch tool, supplied with TiMBL, see
http://ilk.uvt.nl/software.html. Paramsearch implements
</footnote>
<bodyText confidence="0.966997333333334">
a hill climbing search for the optimal settings on iteratively
larger parts of the supplied data. We performed parameter
optimization on 20% of the total &gt;0 data set, where we
balanced the data with respect to frequency. The resulting
settings are k = 11, GainRatio feature weighting and Inverse
Linear (IL) class voting weights.
since we are not basing the classification on gold
standard parses.
Proper nouns In the task of named entity recog-
nition (NER), proper nouns are classified accord-
ing to a set of semantic categories. For the annota-
tion of proper nouns, we make use of a named en-
tity tagger for Swedish (Kokkinakis, 2004), which
is a rule-based tagger based on finite-state rules,
supplied with name lists, so-called “gazetteers”.
The tagger distinguishes the category ‘Person’ for
human referring proper nouns and we extract in-
formation on this category.
Pronouns A subset of the personal pronouns in
Scandinavian, as in English, clearly distinguish
their referent with regard to animacy, e.g. han,
det ‘he, it’. There is, however, a quite large group
of third person plural pronouns which are ambigu-
ous with regards to the animacy of their referent,
e.g., de, dem, deras ‘they, them, theirs’. Pronom-
inal reference resolution is a complex task which
we will not attempt to solve in the present context.
The pronominal part-of-speech tags from the part-
of-speech tagger distinguish number and gender
and in the animacy classification of the personal
pronouns we classify based on these tags only. We
employ a simple heuristic where the pronominal
tags which had more than 85% human instances in
the gold standard are annotated as human.14 The
pronouns which are ambiguous with respect to an-
imacy are not annotated as animate.
In table 7 we see an overview of the accuracy
of the acquired features, i.e., the percentage of
correct instances out of all instances. Note that
we adhere to the general annotation strategy in
Talbanken05, where each dimension (definiteness,
case etc.) contains a null category Ø, which ex-
presses the lack of a certain property. The acqui-
sition of the morphological features (definiteness,
case, finiteness and voice) are very reliable, with
accuracies from 96.9% for voice to 98.5% for the
case feature.
It is not surprising that we observe the largest
discrepancies from the gold standard annotation
in the automatic animacy annotation. In general,
the annotation of animate nominals exhibits a de-
cent precision (95.7) and a lower recall (61.3). The
automatic classification of human common nouns
14A manual classification of the individual pronoun lem-
mas was also considered. However, the treebank has a total of
324 different pronoun forms, hence we opted for a heuristic
classification of the part-of-speech tags instead.
</bodyText>
<page confidence="0.989648">
30
</page>
<table confidence="0.999863">
Dimension Features Instances Correct Accuracy
Definiteness DD, Ø 40832 40010 98.0
Case GG, AA, Ø 68313 67289 98.5
AnimacyNNPNPO HH, Ø 68313 61295 89.7
AnimacyNN HH, Ø 40832 37952 92.9
AnimacyPN HH, Ø 2078 1902 91.5
AnimacyPO HH, Ø 25403 21441 84.4
Finiteness FV, Ø 30767 30035 97.6
Voice PA, Ø 30767 29805 96.9
</table>
<tableCaption confidence="0.853142">
Table 7: Accuracy for automatically acquired linguistic features.
</tableCaption>
<table confidence="0.999889777777778">
Gold Automatic p-value
UAS LAS UAS LAS
NoFeats 89.87 84.92 89.87 84.92 –
Def 89.87 85.02 89.88 85.03 p&lt;0.01
Case 89.99 85.13 89.95 85.11 p&lt;.0001
Verb 90.24 85.38 90.12 85.26 p&lt;.0001
Anim 89.93 85.10 89.86 85.01 p&lt;.03
ADC 90.13 85.35 90.01 85.21 p&lt;.0001
ADCV 90.40 85.68 90.27 85.54 p&lt;.0001
</table>
<tableCaption confidence="0.964044">
Table 8: Overall results in experiments with auto-
matic features compared to gold standard features.
</tableCaption>
<bodyText confidence="0.994604625">
(AnimacyNN) also has a quite high precision
(94.2) in combination with a lower recall (55.5).
The named-entity recognizer (AnimacyPN) shows
more balanced results with a precision of 97.8 and
a recall of 85.2 and the heuristic classification of
the pronominal part-of-speech tags (AnimacyPO)
gives us high precision (96.3) combined with lower
recall (62.0) for the animate class.
</bodyText>
<sectionHeader confidence="0.986655" genericHeader="method">
7 Experiments with acquired features
</sectionHeader>
<bodyText confidence="0.999974980392157">
The experimental methodology is identical to the
one described in 5.1 above, the only difference be-
ing that the linguistic features are acquired auto-
matically, rather than being gold standard. In order
to enable a direct comparison with the results from
the earlier experiments, we employ the gold stan-
dard part-of-speech tags, as before. This means
that the set for which the various linguistic features
are defined is identical, whereas the feature values
may differ.
Table 8 presents the overall results with auto-
matic features, compared to the gold standard re-
sults and p-scores for the difference of the auto-
matic results from the NoFeats baseline. As ex-
pected, we find that the effect of the automatic fea-
tures is generally lower than their gold standard
counterparts. However, all automatic features im-
prove significantly on the NoFeats baseline. In the
error analysis we find the same tendencies in terms
of improvement for specific dependency relations.
The morphological argument features from the
POS-tagger are reliable, as we saw above, and
we observe almost identical results to the gold
standard results. The addition of information
on definiteness causes a significant improvement
(p&lt;.01), and so does the addition of information
on case (p&lt;.0001). The addition of the automat-
ically acquired animacy information results in a
smaller, but significant improvement of overall re-
sults even though the annotation is less reliable
(p&lt;.03). An interesting result is that the automat-
ically acquired information on animacy for com-
mon nouns actually has a significantly better effect
than the gold standard counterparts due to captur-
ing distributional tendencies (Øvrelid, 2008a). As
in the gold standard experiments, we find that the
features which have the most notable effect on per-
formance are the verbal features (p&lt;.0001).
In parallel with the results achieved with the
combination of gold standard features, we observe
improvement of overall results compared to the
baseline (p&lt;.0001) and each of the individual fea-
tures when we combine the features of the argu-
ments (ADC; p&lt;.01) and the argument and ver-
bal features (ADCV; p&lt;.0001). Column 4 in Ta-
ble 4 shows an overview of performance for the
argument relations, compared to the gold standard
experiments. We find overall somewhat lower re-
sults in the experiment with automatic features, but
find the same tendencies with the automatically ac-
quired features.
</bodyText>
<page confidence="0.999825">
31
</page>
<sectionHeader confidence="0.996716" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999963275">
An error analysis of the best performing data-
driven dependency parser for Swedish revealed
consistent errors in dependency assignment,
namely the confusion of argument functions. We
established a set of features expressing distinguish-
ing semantic and structural properties of argu-
ments such as animacy, definiteness and finiteness
and performed a set of experiments with gold stan-
dard features taken from a treebank of Swedish.
The experiments showed that each feature individ-
ually caused an improvement in terms of overall la-
beled accuracy and performance for the argument
relations. We furthermore found that the results
may largely be replicated with automatic features
and a generic part-of-speech tagger. The features
were acquired automatically employing a part-of-
speech tagger, a named-entity recognizer and an
animacy classifier of common noun lemmas em-
ploying morphosyntactic distributional features. A
set of corresponding experiments with automatic
features gave significant improvement from the ad-
dition of individual features and a cumulative ef-
fect of the same features in combination. In partic-
ular, we show that the very same tendencies in im-
provement for specific argument relations such as
subjects, objects and predicatives may be obtained
using automatically acquired features.
Properties of the Scandinavian languages con-
nected with errors in argument assignment are not
isolated phenomena. A range of other languages
exhibit similar properties, for instance, Italian ex-
hibits word order variation, little case, syncretism
in agreement morphology, as well as pro-drop;
German exhibits a larger degree of word order
variation in combination with quite a bit of syn-
cretism in case morphology; Dutch has word order
variation, little case and syncretism in agreement
morphology. These are all examples of other lan-
guages for which the results described here are rel-
evant.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999559426229508">
Bod, Rens. 1998. Beyond Grammar: An experience-based
theory of language. CSLI Publications, Stanford, CA.
Carroll, John and Edward Briscoe. 2002. High precision ex-
traction of grammatical relations. In Proceedings of the
19th International Conference on Computational Linguis-
tics (COLING), pages 134–140.
Chang, Chih-Chung and Chih-Jen Lin. 2001. LIBSVM: A
library for support vector machines. Software available at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm.
Daelemans, Walter, Jakub Zavrel, Ko Van der Sloot, and An-
tal Van den Bosch. 2004. TiMBL: Tilburg Memory Based
Learner, version 5.1, Reference Guide. Technical report,
ILK Technical Report Series 04-02.
de Swart, Peter, Monique Lamers, and Sander Lestrade.
2008. Animacy, argument structure and argument encod-
ing: Introduction to the special issue on animacy. Lingua,
118(2):131–140.
Hall, Johan. 2003. A probabilistic part-of-speech tagger
with suffix probabilities. Master’s thesis, V¨axj¨o Univer-
sity, Sweden.
Klein, Dan and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics
(ACL), pages 423–430.
Kokkinakis, Dimitrios. 2004. Reducing the effect of name
explosion. In Proceedings of the LREC Workshop: Be-
yond Named Entity Recognition, Semantic labelling for
NLP tasks.
K¨ubler, Sandra and Jelena Proki´c. 2006. Why is German de-
pendency parsing more reliable than constituent parsing?
In Proceedings of the Fifth Workshop on Treebanks and
Linguistic Theories (TLT), pages 7–18.
McDonald, Ryan and Joakim Nivre. 2007. Characterizing
the errors of data-driven dependency parsing. In Proceed-
ings of the Eleventh Conference on Computational Natural
Language Learning (CoNLL), pages 122–131.
Nivre, Joakim, Jens Nilsson, and Johan Hall. 2006a. Tal-
banken05: A Swedish treebank with phrase structure and
dependency annotation. In Proceedings of the fifth Inter-
national Conference on Language Resources and Evalua-
tion (LREC), pages 1392–1395.
Nivre, Joakim, Jens Nilsson, Johan Hall, G¨uls¸en Eryiˇgit, and
Svetoslav Marinov. 2006b. Labeled pseudo-projective
dependency parsing with Support Vector Machines. In
Proceedings of the Conference on Computational Natural
Language Learning (CoNLL).
Nivre, Joakim, Johan Hall, Sandra K¨ubler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. CoNLL 2007 Shared Task on Dependency Pars-
ing. In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915–932.
Øvrelid, Lilja. 2008a. Argument Differentiation. Soft con-
straints and data-driven models. Ph.D. thesis, University
of Gothenburg.
Øvrelid, Lilja. 2008b. Finite matters: Verbal features in data-
driven parsing of Swedish. In Proceedings of the Interna-
tional Conference on NLP, GoTAL 2008.
Weber, Andrea and Karin M¨uller. 2004. Word order varia-
tion in German main clauses: A corpus analysis. In Pro-
ceedings of the 20th International Conference on Compu-
tational Linguistics, pages 71–77.
</reference>
<page confidence="0.999294">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.774157">
<title confidence="0.997203">Linguistic features in data-driven dependency parsing</title>
<author confidence="0.973467">Lilja</author>
<affiliation confidence="0.9654725">NLP-unit, Dept. of University of</affiliation>
<email confidence="0.890444">lilja.ovrelid@svenska.gu.se</email>
<abstract confidence="0.995616142857143">This article investigates the effect of a set of linguistically motivated features on argument disambiguation in data-driven dependency parsing of Swedish. We present results from experiments with gold standard features, such as animacy, definiteness and finiteness, as well as corresponding experiments where these features have been acquired automatically and show significant improvements both in overall parse results and in the analysis of specific argument relations, such as subjects, objects and predicatives.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Beyond Grammar: An experience-based theory of language.</title>
<date>1998</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="1338" citStr="Bod, 1998" startWordPosition="192" endWordPosition="193">as recently received extensive attention in the parsing community and impressive results have been obtained for a range of languages (Nivre et al., 2007). Even with high overall parsing accuracy, however, datadriven parsers often make errors in the assignment of argument relations such as subject and object and the exact influence of data-derived features on the parsing accuracy for specific linguistic constructions is still relatively poorly understood. There are a number of studies that investigate the influence of different features or representational choices on overall parsing accuracy, (Bod, 1998; Klein and Manning, 2003). There are also attempts at a more fine-grained analysis of accuracy, targeting specific linguistic constructions or grammatical functions (Carroll and Briscoe, 2002; K¨ubler and Proki´c, 2006; McDonald and Nivre, 2007). © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. But there are few studies that combine the two perspectives and try to tease apart the influence of different features on the analysis of specific constructions, let alone mo</context>
</contexts>
<marker>Bod, 1998</marker>
<rawString>Bod, Rens. 1998. Beyond Grammar: An experience-based theory of language. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Edward Briscoe</author>
</authors>
<title>High precision extraction of grammatical relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>134--140</pages>
<contexts>
<context position="1530" citStr="Carroll and Briscoe, 2002" startWordPosition="218" endWordPosition="221">l parsing accuracy, however, datadriven parsers often make errors in the assignment of argument relations such as subject and object and the exact influence of data-derived features on the parsing accuracy for specific linguistic constructions is still relatively poorly understood. There are a number of studies that investigate the influence of different features or representational choices on overall parsing accuracy, (Bod, 1998; Klein and Manning, 2003). There are also attempts at a more fine-grained analysis of accuracy, targeting specific linguistic constructions or grammatical functions (Carroll and Briscoe, 2002; K¨ubler and Proki´c, 2006; McDonald and Nivre, 2007). © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. But there are few studies that combine the two perspectives and try to tease apart the influence of different features on the analysis of specific constructions, let alone motivated by a thorough linguistic analysis. In this paper, we investigate the influence of a set of linguistically motivated features on parse results for Swedish, and in particular on the anal</context>
</contexts>
<marker>Carroll, Briscoe, 2002</marker>
<rawString>Carroll, John and Edward Briscoe. 2002. High precision extraction of grammatical relations. In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines. Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="4979" citStr="Chang and Lin, 2001" startWordPosition="747" endWordPosition="750">ries depending on part of speech: NOUN: definiteness, animacy, case (Ø/GEN) PRO: animacy, case (Ø/ACC) VERB: tense, voice (Ø/PA) 2.2 Parser: MaltParser We use the freely available MaltParser,2 which is a language-independent system for data-driven dependency parsing. MaltParser is based on a deterministic parsing strategy, first proposed by Nivre (2003), in combination with treebankinduced classifiers for predicting the next parsing action. Classifiers can be trained using any machine learning approach, but the best results have so far been obtained with support vector machines, using LIBSVM (Chang and Lin, 2001). MaltParser has a wide range of parameters that need to be optimized when parsing a new language. As our baseline, we use the settings optimized for Swedish in the CoNLL-X shared task (Nivre et al., 2006b), where this parser was the best performing parser for Swedish. The only parameter that will be varied in the later experiments is the feature model used for the prediction of the next parsing action. Hence, we need to describe the feature model in a little more detail. MaltParser uses two main data structures, a stack (S) and an input queue (I), and builds a dependency graph (G) incremental</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chang, Chih-Chung and Chih-Jen Lin. 2001. LIBSVM: A library for support vector machines. Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko Van der Sloot</author>
<author>Antal Van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide.</title>
<date>2004</date>
<tech>Technical report, ILK Technical Report Series 04-02.</tech>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2004</marker>
<rawString>Daelemans, Walter, Jakub Zavrel, Ko Van der Sloot, and Antal Van den Bosch. 2004. TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide. Technical report, ILK Technical Report Series 04-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter de Swart</author>
<author>Monique Lamers</author>
<author>Sander Lestrade</author>
</authors>
<title>Animacy, argument structure and argument encoding: Introduction to the special issue on animacy.</title>
<date>2008</date>
<journal>Lingua,</journal>
<volume>118</volume>
<issue>2</issue>
<marker>de Swart, Lamers, Lestrade, 2008</marker>
<rawString>de Swart, Peter, Monique Lamers, and Sander Lestrade. 2008. Animacy, argument structure and argument encoding: Introduction to the special issue on animacy. Lingua, 118(2):131–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
</authors>
<title>A probabilistic part-of-speech tagger with suffix probabilities. Master’s thesis,</title>
<date>2003</date>
<institution>V¨axj¨o University, Sweden.</institution>
<contexts>
<context position="18420" citStr="Hall, 2003" startWordPosition="2911" endWordPosition="2912">ess POS-tagger Case POS-tagger Animacy - NN Animacy classifier Animacy - PN Named Entity Tagger Animacy - PO Majority class Tense (finiteness), voice POS-tagger Table 5: Overview of applications employed for automatic feature acquisition. show that the property of animacy can be fairly robustly acquired for common nouns by means of distributional features from an automatically parsed corpus. Table 5 shows an overview of the applications employed for the automatic acquisition of our linguistic features. For part-of-speech tagging, we employ MaltTagger – a HMM part-of-speech tagger for Swedish (Hall, 2003). The POS-tagger distinguishes tense and voice for verbs, nominative and accusative case for pronouns, as well as definiteness and genitive case for nouns. 6.1 Animacy The feature of animacy is clearly the most challenging feature to acquire automatically. Recall that Talbanken05 distinguishes animacy for all nominal constituents. In the following we describe the automatic acquisition of animacy information for common nouns, proper nouns and pronouns. Common nouns Table 6 presents an overview of the animacy data for common nouns in Talbanken05. It is clear that the data is highly skewed NoFeat</context>
<context position="22581" citStr="Hall, 2003" startWordPosition="3562" endWordPosition="3563">7.5% for the inanimate class with a threshold of 10. The common nouns in Talbanken05 are classified for animacy following a leave-one-out training and testing scheme where each of the n nouns in Talbanken05 are classified with a classifier trained on n − 1 instances. This ensures that the training and test instances are disjoint at all times. Moreover, the fact that the distributional data is taken from a separate data set ensures non-circularity 10Parole is available at http://spraakbanken.gu.se 11For part-of-speech tagging, we employ the MaltTagger – a HMM part-of-speech tagger for Swedish (Hall, 2003). For parsing, we employ MaltParser with a pretrained model for Swedish, which has been trained on the tags output by the tagger. It makes use of a smaller set of dependency relations than those found in Talbanken05. 12TiMBL is freely available at http://ilk.uvt.nl/software.html 13For parameter optimization we employ the paramsearch tool, supplied with TiMBL, see http://ilk.uvt.nl/software.html. Paramsearch implements a hill climbing search for the optimal settings on iteratively larger parts of the supplied data. We performed parameter optimization on 20% of the total &gt;0 data set, where we ba</context>
</contexts>
<marker>Hall, 2003</marker>
<rawString>Hall, Johan. 2003. A probabilistic part-of-speech tagger with suffix probabilities. Master’s thesis, V¨axj¨o University, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>423--430</pages>
<contexts>
<context position="1364" citStr="Klein and Manning, 2003" startWordPosition="194" endWordPosition="197"> received extensive attention in the parsing community and impressive results have been obtained for a range of languages (Nivre et al., 2007). Even with high overall parsing accuracy, however, datadriven parsers often make errors in the assignment of argument relations such as subject and object and the exact influence of data-derived features on the parsing accuracy for specific linguistic constructions is still relatively poorly understood. There are a number of studies that investigate the influence of different features or representational choices on overall parsing accuracy, (Bod, 1998; Klein and Manning, 2003). There are also attempts at a more fine-grained analysis of accuracy, targeting specific linguistic constructions or grammatical functions (Carroll and Briscoe, 2002; K¨ubler and Proki´c, 2006; McDonald and Nivre, 2007). © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. But there are few studies that combine the two perspectives and try to tease apart the influence of different features on the analysis of specific constructions, let alone motivated by a thorough ling</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Kokkinakis</author>
</authors>
<title>Reducing the effect of name explosion.</title>
<date>2004</date>
<booktitle>In Proceedings of the LREC Workshop: Beyond Named Entity Recognition, Semantic labelling for NLP tasks.</booktitle>
<contexts>
<context position="23635" citStr="Kokkinakis, 2004" startWordPosition="3728" endWordPosition="3729"> search for the optimal settings on iteratively larger parts of the supplied data. We performed parameter optimization on 20% of the total &gt;0 data set, where we balanced the data with respect to frequency. The resulting settings are k = 11, GainRatio feature weighting and Inverse Linear (IL) class voting weights. since we are not basing the classification on gold standard parses. Proper nouns In the task of named entity recognition (NER), proper nouns are classified according to a set of semantic categories. For the annotation of proper nouns, we make use of a named entity tagger for Swedish (Kokkinakis, 2004), which is a rule-based tagger based on finite-state rules, supplied with name lists, so-called “gazetteers”. The tagger distinguishes the category ‘Person’ for human referring proper nouns and we extract information on this category. Pronouns A subset of the personal pronouns in Scandinavian, as in English, clearly distinguish their referent with regard to animacy, e.g. han, det ‘he, it’. There is, however, a quite large group of third person plural pronouns which are ambiguous with regards to the animacy of their referent, e.g., de, dem, deras ‘they, them, theirs’. Pronominal reference resol</context>
</contexts>
<marker>Kokkinakis, 2004</marker>
<rawString>Kokkinakis, Dimitrios. 2004. Reducing the effect of name explosion. In Proceedings of the LREC Workshop: Beyond Named Entity Recognition, Semantic labelling for NLP tasks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Jelena Proki´c</author>
</authors>
<title>Why is German dependency parsing more reliable than constituent parsing?</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth Workshop on Treebanks and Linguistic Theories (TLT),</booktitle>
<pages>7--18</pages>
<marker>K¨ubler, Proki´c, 2006</marker>
<rawString>K¨ubler, Sandra and Jelena Proki´c. 2006. Why is German dependency parsing more reliable than constituent parsing? In Proceedings of the Fifth Workshop on Treebanks and Linguistic Theories (TLT), pages 7–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>122--131</pages>
<contexts>
<context position="1584" citStr="McDonald and Nivre, 2007" startWordPosition="226" endWordPosition="229">make errors in the assignment of argument relations such as subject and object and the exact influence of data-derived features on the parsing accuracy for specific linguistic constructions is still relatively poorly understood. There are a number of studies that investigate the influence of different features or representational choices on overall parsing accuracy, (Bod, 1998; Klein and Manning, 2003). There are also attempts at a more fine-grained analysis of accuracy, targeting specific linguistic constructions or grammatical functions (Carroll and Briscoe, 2002; K¨ubler and Proki´c, 2006; McDonald and Nivre, 2007). © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. But there are few studies that combine the two perspectives and try to tease apart the influence of different features on the analysis of specific constructions, let alone motivated by a thorough linguistic analysis. In this paper, we investigate the influence of a set of linguistically motivated features on parse results for Swedish, and in particular on the analysis of argument relations such as subjects, objects a</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>McDonald, Ryan and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing. In Proceedings of the Eleventh Conference on Computational Natural Language Learning (CoNLL), pages 122–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
<author>Johan Hall</author>
</authors>
<title>Talbanken05: A Swedish treebank with phrase structure and dependency annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the fifth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1392--1395</pages>
<contexts>
<context position="4132" citStr="Nivre et al., 2006" startWordPosition="623" endWordPosition="626">ceedings of the 12th Conference on Computational Natural Language Learning, pages 25–32 Manchester, August 2008 declarative main clauses. Word order variation Pretty much any constituent may occupy the sentence-initial position, but subjects are most common. Limited case marking Nouns are only inflected for genitive case. Personal pronouns distinguish nominative and accusative case, but demonstratives and quantifying pronouns are case ambiguous (like nouns). 2.1 Treebank: Talbanken05 Talbanken05 is a Swedish treebank converted to dependency format, containing both written and spoken language (Nivre et al., 2006a).1 For each token, Talbanken05 contains information on word form, part of speech, head and dependency relation, as well as various morphosyntactic and/or lexical semantic features. The nature of this additional information varies depending on part of speech: NOUN: definiteness, animacy, case (Ø/GEN) PRO: animacy, case (Ø/ACC) VERB: tense, voice (Ø/PA) 2.2 Parser: MaltParser We use the freely available MaltParser,2 which is a language-independent system for data-driven dependency parsing. MaltParser is based on a deterministic parsing strategy, first proposed by Nivre (2003), in combination w</context>
</contexts>
<marker>Nivre, Nilsson, Hall, 2006</marker>
<rawString>Nivre, Joakim, Jens Nilsson, and Johan Hall. 2006a. Talbanken05: A Swedish treebank with phrase structure and dependency annotation. In Proceedings of the fifth International Conference on Language Resources and Evaluation (LREC), pages 1392–1395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
<author>Johan Hall</author>
<author>G¨uls¸en Eryiˇgit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with Support Vector Machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<marker>Nivre, Nilsson, Hall, Eryiˇgit, Marinov, 2006</marker>
<rawString>Nivre, Joakim, Jens Nilsson, Johan Hall, G¨uls¸en Eryiˇgit, and Svetoslav Marinov. 2006b. Labeled pseudo-projective dependency parsing with Support Vector Machines. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>Shared Task on Dependency Parsing.</title>
<date>2007</date>
<journal>CoNLL</journal>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Nivre, Joakim, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. CoNLL 2007 Shared Task on Dependency Parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lilja Øvrelid</author>
</authors>
<title>Argument Differentiation. Soft constraints and data-driven models.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Gothenburg.</institution>
<contexts>
<context position="14673" citStr="Øvrelid, 2008" startWordPosition="2351" endWordPosition="2352">d in the FEATS features (animacy, definiteness, etc.), while the tokens for which these features are defined remains constant. Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).7 Statistical significance is checked using Dan Bikel’s randomized parsing evaluation 5We experimented with the use of tense as well as finiteness, a binary feature which was obtained by a mapping from tense to finite/non-finite. Finiteness gave significantly better results (p&lt;.03) and was therefore employed in the following, see (Øvrelid, 2008b) for details. 6Preliminary experiments showed that it was better to tie FEATS features to the same tokens as FORM features (rather than POS or DEP features). Backward selection from this model was tried for several different instantiations of FEATS but with no significant improvement. 7LAS and UAS report the percentage of tokens that are assigned the correct head with (labeled) or without (unlabeled) the correct dependency label, calculated using eval.pl with default settings (http://nextens.uvt.nl/—conll/software.html) comparator.8 Since the main focus of this article is on the disambiguati</context>
<context position="28636" citStr="Øvrelid, 2008" startWordPosition="4519" endWordPosition="4520">t identical results to the gold standard results. The addition of information on definiteness causes a significant improvement (p&lt;.01), and so does the addition of information on case (p&lt;.0001). The addition of the automatically acquired animacy information results in a smaller, but significant improvement of overall results even though the annotation is less reliable (p&lt;.03). An interesting result is that the automatically acquired information on animacy for common nouns actually has a significantly better effect than the gold standard counterparts due to capturing distributional tendencies (Øvrelid, 2008a). As in the gold standard experiments, we find that the features which have the most notable effect on performance are the verbal features (p&lt;.0001). In parallel with the results achieved with the combination of gold standard features, we observe improvement of overall results compared to the baseline (p&lt;.0001) and each of the individual features when we combine the features of the arguments (ADC; p&lt;.01) and the argument and verbal features (ADCV; p&lt;.0001). Column 4 in Table 4 shows an overview of performance for the argument relations, compared to the gold standard experiments. We find over</context>
</contexts>
<marker>Øvrelid, 2008</marker>
<rawString>Øvrelid, Lilja. 2008a. Argument Differentiation. Soft constraints and data-driven models. Ph.D. thesis, University of Gothenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lilja Øvrelid</author>
</authors>
<title>Finite matters: Verbal features in datadriven parsing of Swedish.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on NLP, GoTAL</booktitle>
<contexts>
<context position="14673" citStr="Øvrelid, 2008" startWordPosition="2351" endWordPosition="2352">d in the FEATS features (animacy, definiteness, etc.), while the tokens for which these features are defined remains constant. Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).7 Statistical significance is checked using Dan Bikel’s randomized parsing evaluation 5We experimented with the use of tense as well as finiteness, a binary feature which was obtained by a mapping from tense to finite/non-finite. Finiteness gave significantly better results (p&lt;.03) and was therefore employed in the following, see (Øvrelid, 2008b) for details. 6Preliminary experiments showed that it was better to tie FEATS features to the same tokens as FORM features (rather than POS or DEP features). Backward selection from this model was tried for several different instantiations of FEATS but with no significant improvement. 7LAS and UAS report the percentage of tokens that are assigned the correct head with (labeled) or without (unlabeled) the correct dependency label, calculated using eval.pl with default settings (http://nextens.uvt.nl/—conll/software.html) comparator.8 Since the main focus of this article is on the disambiguati</context>
<context position="28636" citStr="Øvrelid, 2008" startWordPosition="4519" endWordPosition="4520">t identical results to the gold standard results. The addition of information on definiteness causes a significant improvement (p&lt;.01), and so does the addition of information on case (p&lt;.0001). The addition of the automatically acquired animacy information results in a smaller, but significant improvement of overall results even though the annotation is less reliable (p&lt;.03). An interesting result is that the automatically acquired information on animacy for common nouns actually has a significantly better effect than the gold standard counterparts due to capturing distributional tendencies (Øvrelid, 2008a). As in the gold standard experiments, we find that the features which have the most notable effect on performance are the verbal features (p&lt;.0001). In parallel with the results achieved with the combination of gold standard features, we observe improvement of overall results compared to the baseline (p&lt;.0001) and each of the individual features when we combine the features of the arguments (ADC; p&lt;.01) and the argument and verbal features (ADCV; p&lt;.0001). Column 4 in Table 4 shows an overview of performance for the argument relations, compared to the gold standard experiments. We find over</context>
</contexts>
<marker>Øvrelid, 2008</marker>
<rawString>Øvrelid, Lilja. 2008b. Finite matters: Verbal features in datadriven parsing of Swedish. In Proceedings of the International Conference on NLP, GoTAL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Weber</author>
<author>Karin M¨uller</author>
</authors>
<title>Word order variation in German main clauses: A corpus analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>71--77</pages>
<marker>Weber, M¨uller, 2004</marker>
<rawString>Weber, Andrea and Karin M¨uller. 2004. Word order variation in German main clauses: A corpus analysis. In Proceedings of the 20th International Conference on Computational Linguistics, pages 71–77.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>