<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000612">
<title confidence="0.456443">
On Using Ensemble Methods for Chinese Named Entity Recognition
</title>
<author confidence="0.54065">
Chia-Wei Wu Shyh-Yi Jan Richard Tzong-Han Wen-Lian Hsu
Tsai
</author>
<affiliation confidence="0.695877">
Institute of Information Science, Academia Sinica, Nankang, Taipei,115, Taiwan
</affiliation>
<email confidence="0.970098">
{cwwu,shihyi,thtsai,Hsu}@iis.sinica.edu.tw
</email>
<sectionHeader confidence="0.997101" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997935">
In sequence labeling tasks, applying dif-
ferent machine learning models and fea-
ture sets usually leads to different results.
In this paper, we exploit two ensemble
methods in order to integrate multiple
results generated under different condi-
tions. One method is based on majority
vote, while the other is a memory-based
approach that integrates maximum en-
tropy and conditional random field clas-
sifiers. Our results indicate that the
memory-based method can outperform
the individual classifiers, but the major-
ity vote method cannot.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958903846154">
Sequence labeling and segmentation tasks have
been studied extensively in the fields of computa-
tional linguistics and information extraction. Sev-
eral tasks, including, word segmentation, and
semantic role labeling, provide rich information
for various applications, such as segmentation in
Chinese information retrieval and named entity
recognition in biomedical literature mining.
Probabilistic state automata models, such as the
Hidden Markov model (HMM) [6] and condi-
tional random fields (CRF) [5] are some of best,
and therefore most popular, approaches for se-
quence labeling tasks. Both HMM and CRF con-
sider that the state transition and the state
prediction are conditional on the observation of
data. The advantage of the CRF model is that
richer feature sets can be considered, because,
unlike HMM, it does not make a dependence as-
sumption. However, the obvious drawback of the
CRF model is that it needs more computing re-
sources, so we can not apply all the features of
the model. One possible way to resolve this prob-
lem is to effectively combine the results of vari-
ous individual classifiers trained with different
feature sets. In this paper, we use two ensemble
methods to combine the results of the classifiers.
We also combine the results generated by two
machine learning models: maximum entropy
(ME) [1] and CRF. One ensemble method is
based on the majority vote [3], and the other is
the memory based learner [7]. Although the en-
semble methods have been applied in some se-
quence labeling tasks [2],[3], similar work in
Chinese named entity recognition is scarce.
Our Chinese named entity tagger uses a charac-
ter-based model. For English named entity tasks,
a character-based NER model proposed by Dan
Klein [4] proves the usefulness of substrings
within words. In Chinese NER, the character-
based model is more straightforward, since there
are no spaces between Chinese words and each
Chinese character is actually meaningful. An-
other reason for using a character-based model is
that it can avoid the errors sometimes made by a
Chinese word segmentor.
The remainder of this paper is organized as fol-
lows. In the Section 2, we introduce the machine
learning models, the features we apply in the ma-
chine learning models, and the ensemble methods.
In Section 3, we briefly describe the experimental
data and the experiment results. Then, in Section
4, we present our conclusions..
</bodyText>
<sectionHeader confidence="0.993886" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.996589">
2.1 Machine Learning Models
</subsectionHeader>
<bodyText confidence="0.933891">
In this section, we introduce ME and CRF.
</bodyText>
<subsectionHeader confidence="0.39209">
Maximum Entropy
</subsectionHeader>
<bodyText confidence="0.999146">
ME[1] is a statistical modeling technique used
for estimating the conditional probability of a
target label based on given information. The
technique computes the probability p(y|x), where
y denotes all possible outcomes of the space, and
x denotes all possible features of the space. The
computation of p(y|x) depends on a set of fea-
</bodyText>
<page confidence="0.974281">
142
</page>
<bodyText confidence="0.955048875">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 142–145,
Sydney, July 2006. c�2006 Association for Computational Linguistics
tures in x; the features are helpful for making
predictions about the outcomes, y.
Given a set of features and a training set, the ME
estimation process produces a model, in which
every feature fi has a weight λi. The ME model
can be represented by the following formula:
</bodyText>
<equation confidence="0.983253888888889">
p(y  |x)= z1(
x ⎝ i ⎠
,
z( x) ∑ ∑ ( )⎟
⎛ ⎞
= exp λ ,
⎜ i f i x y
y ⎝ i ⎠
.
</equation>
<bodyText confidence="0.998845333333333">
The probability is derived by multiplying the
weights of the active features (i.e., those fi (y,x) =
1).
</bodyText>
<subsectionHeader confidence="0.703855">
Conditional Random Field
</subsectionHeader>
<bodyText confidence="0.999046">
A conditional random field (CRF)[5] can be seen
as an undirected graph model in which the nodes
corresponding to the label sequence y are condi-
tional on the observed sequence x. The goal of
CRF is to find the label sequence y that has the
maximized probability, given an observation se-
quence x. The formula for the CRF model can be
written as:
</bodyText>
<equation confidence="0.926197333333333">
exp ( ( y x ) )
∑ λ
j j F j , ,
</equation>
<bodyText confidence="0.998101333333333">
where λj is the parameter of a corresponding fea-
ture Fj , Z(x) is an normalizing factor, and Fj can
be written as:
</bodyText>
<equation confidence="0.97683925">
F y, x
( ) = ∑ n = ( −
0 f y 1 , y , x, i),
j i i i i
</equation>
<bodyText confidence="0.999937888888889">
where i means the relative position in the se-
quence, and yi-1 and yi denote the label at position
i-1 and i respectively. In this paper, we only con-
sider linear chain and first-order Markov assump-
tion CRFs. In NER applications, a feature
function fj (yi-1, yi, x, i) can be set to check
whether x is a specific character, and whether yi-1
is a label (such as Location) and yi is a label (such
as Others).
</bodyText>
<subsectionHeader confidence="0.998818">
2.2 Chinese Named Entity Recognition
</subsectionHeader>
<bodyText confidence="0.999969333333333">
In this section, we present the features applied in
our CRF and ME models, namely, characters,
words, and chuck information.
</bodyText>
<subsectionHeader confidence="0.652013">
Character Features
</subsectionHeader>
<bodyText confidence="0.999967222222222">
The character features we apply in the CRF
model and the ME model are presented in Tables
1 and 2 respectively. The numbers listed in the
feature type column indicate the relative position
of a character in the sliding window. For example,
-1 means the previous character of the target
character. Therefore, the characters in those posi-
tions are applied in the model. The numbers in
parentheses mean that the feature includes a
combination of the characters in those positions.
The unigrams in Tables 1 and 2 indicate that the
listed features only consider to their own labels,
whereas the bigram model considers the combi-
nation of the current label and the previous label.
Since ME does not consider multiple states in a
single feature, there are only unigrams in Table 2.
In addition, as ME can handle more features than
CRF, we apply extra features in the ME model
</bodyText>
<tableCaption confidence="0.950811">
Table 1 Character features for CRF
</tableCaption>
<table confidence="0.99604775">
Feature Types
unigram -2, -1, 0, 1, 2, (-2,-1), (-1,0), (0,1),
(1,2), (-1,0,1)
bigram -2 -1 0 +1 +2, (0,1)
</table>
<tableCaption confidence="0.9127">
Table 2 Character features for ME
</tableCaption>
<table confidence="0.947185666666667">
Feature Types
unigram -2, -1, 0, 1, 2, (-2,-1), (-1,0), (0,1),
(1,2), (-1,0,1) (-1,1)
</table>
<sectionHeader confidence="0.275171" genericHeader="method">
Word Information
</sectionHeader>
<bodyText confidence="0.985814090909091">
Because of the limitations of the closed task, we
use the NER corpus to train the segmentors based
on the CRF model. To simulate noisy word in-
formation in the test corpus, we use a ten-fold
method for training segmentors to tag the training
corpus. The word features we apply in our NER
systems are presented in Tables 3 and 4.
In addition to the word itself, chuck information,
i.e., the relative position of a character in a word,
is also valuable information. Hence, we also add
chuck information to our models. As the diversity
of Chinese words is greater than that of Chinese
characters, the number of features that can be
used in CRF is much lower than the number that
can be used in ME.
Table 3 Word features for CRF
Feature Types
unigram 0
bigram 0
Table 4 Word features for ME
Feature Types
unigram -1, 0, 1, (-2,-1), (-1,0), (0,1), (1,2)
</bodyText>
<sectionHeader confidence="0.5957965" genericHeader="method">
2.3 Ensemble Methods
Majority vote
</sectionHeader>
<bodyText confidence="0.9999625">
We can not put all the features into the CRF
model because of its limited resources. Therefore,
we train several CRF classifiers with different
feature sets so that we can use as many features
</bodyText>
<equation confidence="0.982355">
1
P(y  |x)=
Z( x)
</equation>
<page confidence="0.99504">
143
</page>
<bodyText confidence="0.99887975">
as possible. Then, we use the following simple,
equally weighted linear equation, called majority
vote, to combine the results of the CRF classifi-
ers.
</bodyText>
<equation confidence="0.9854445">
��++
s(y,x)=L,T 0Ci(y,x),
</equation>
<bodyText confidence="0.999895571428571">
where S(y,x) is the score of a label y and a char-
acter x respectively; T denotes the total number
of CRF models; and the value of Ci(y,x) is 1 if
the decision of the result of the ith CRF model is
y, otherwise it is zero. The highest score of y is
chosen as the label of x. The results are incorpo-
rated into the Viterbi algorithm to search for the
path with the maximum scores.
In this paper, the first step in the majority vote
experiment is to train three CRF classifiers with
different feature sets. Then, in the second step,
we use the results obtained in the first step to
generate the voting scores for the Viterbi algo-
rithm.
</bodyText>
<subsectionHeader confidence="0.622799">
Memory Based learner
</subsectionHeader>
<bodyText confidence="0.999881380952381">
The memory-based learning method memorizes
all examples in a training corpus. If a word is
unknown, the memory-based classifier uses the
k-nearest neighbors to find the most similar ex-
ample as the answer. Instead of using the com-
plete algorithm of the memory-based learner, we
do not handle unseen data. In our memory- based
combination method, the learner remembers all
named entities from the results of the various
classifiers and then tags the characters that were
originally tagged as “Other”. For example, if a
character x is tagged by one classifier as “0”
(“Others” tag) and if the memory-based classifier
learns from another classifier that this character
is tagged as PER, then x will be tagged as “B-
PER” by the memory-based classifier.
The obvious drawback of this method is that the
precision rate might decrease as the recall rate
increases. Therefore, we set the following three
rules to filter out samples that are likely to have a
high error rate.
</bodyText>
<listItem confidence="0.993759">
1. Named entities can not be tagged as differ-
ent named entity tags by different classifiers.
2. We set an absolute frequency threshold to
filter out examples that occur less than the
threshold.
3. We set a relative frequency threshold to
filter out examples that occur less than the
threshold. For example, if a word x appears
10 times in the corpus, then half of the in-
stances of x have to be tagged as named en-
tities; otherwise, x will be filtered out of the
memory classifier.
</listItem>
<bodyText confidence="0.999821">
In our experiment, we used the memory-based
learner to memorize the named entities from the
tagging results of an ME classifier and a CRF
classifier, and then tagged the tagging results of
the CRF classifier.
</bodyText>
<sectionHeader confidence="0.997921" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99586">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.9982168">
We selected the corpora of City University of
Hong Kong (CityU) and Microsoft Research
(MSRA) corpora to evaluate our methods. CityU
is a Traditional Chinese corpus, and MSRA is
Simplified Chinese corpus.
</bodyText>
<subsectionHeader confidence="0.904907">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.998089125">
Table 5 shows the results of several methods ap-
plied to the MSRA corpus. The memory-based
ensemble method, which combines the results of
a maximum entropy model and those of a CRF
classifier, achieves the best performance. The
majority vote combined with the results of three
CRF models based on different feature sets has
the worst performance.
</bodyText>
<tableCaption confidence="0.96101">
Table 5 msra
</tableCaption>
<table confidence="0.99927">
Precision Recall FB1
Memory based 86.21 78.14 81.98
Majority Vote 85.83 76.06 80.65
Only-Character 86.70 75.54 80.74
CRF 86.23 77.40 81.58
</table>
<bodyText confidence="0.9617736">
The results obtained on Cityu, presented in Table
6, show that the single CRF classifier achieved
the best performance. None of the ensemble
methods can outperform the non-ensemble meth-
ods.
</bodyText>
<tableCaption confidence="0.948241">
Table 6 ci
</tableCaption>
<table confidence="0.9991496">
Precision Recall FB1
Memory based 90.79 86.26 88.47
Majority Vote 90.52 84.15 87.22
Only-Character 91.32 84.55 87.80
CRF 92.01 85.45 88.61
</table>
<bodyText confidence="0.9890464">
Tables 7 and 8 show the results of the memory-
based ensemble methods under different rules.
We set the frequency threshold as 2 and the rela-
tive frequency threshold as 0.5. The results show
that the relative frequencies rule effectively re-
duces the loss of precision caused by more enti-
ties being tagged by the memory-based classifier.
The memory-based ensemble method works well
on the MSRA corpus, but not on the CityU cor-
pus. In the MSRA corpus, the memory-based
</bodyText>
<page confidence="0.99589">
144
</page>
<bodyText confidence="0.9998294">
ensemble method outperforms the individual
CRF model by approximately 0.4 % in FB1. We
found that the memory-based classifier can not
achieve a better performance than the CRF model
because it misclassifies many organizations’
names. Therefore, we chose another strategy that
restricts the memory-based classifier to tagging
person names only. Under this restriction, the
performance of the memory-based classifier im-
proves FB1 by approximately 0.2%.
</bodyText>
<tableCaption confidence="0.8868535">
Table 7 msra- The performances of memory
based ensemble methods under different rules.
</tableCaption>
<table confidence="0.999976">
Precision Recall FB1
Frequency Threshold 86.18 78.16 81.97
Relative Frequency 86.21 78.14 81.98
Threshold
Only Person 86.27 77.58 81.69
</table>
<tableCaption confidence="0.889158">
Table 8 cityu- The performances of memory
based ensemble methods under different rules.
</tableCaption>
<table confidence="0.999698">
Precision Recall FB1
Frequency Threshold 90.69 86.55 88.57
Relative Frequency 90.87 86.29 88.52
Threshold
Only Person 92.00 85.66 88.72
</table>
<sectionHeader confidence="0.999002" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999966083333333">
In this paper, we use ME and CRF models to
train a Chinese named entity tagger. Like previ-
ous researchers, we found that CRF models out-
perform ME models. We also apply two
ensemble methods, namely, majority vote and
memory-based approaches, to the closed NER
shared task. Our results show that integrating
individual classifiers as the majority vote ap-
proach does not outperform the individual classi-
fiers. Furthermore, a memory-based combination
only seems to work when we restrict the mem-
ory-based classifier to handling person names.
</bodyText>
<sectionHeader confidence="0.980356" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.996245333333333">
We are grateful for the support of National Sci-
ence Council under Grant NSC 95-2752-E-001-
001-PAE.
</bodyText>
<sectionHeader confidence="0.999557" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999872764705883">
1. Berger, A., Pietra, S.A.D. and Pietra, V.J.D. A
Maximum Entropy Approach to Natural Language
Processing. Computer Linguistic, 22. 1996 39-71.
2. Florian, R., Ittycheriah, A., Jing, H. and Zhang, T.,
Named Entity Recognition through Classifier
Combination. in Proceedings of Conference on
Computational Natural Language Learning, 2003,
168-171.
3. Halteren, H.v., Zavrel, J. and Daelemans, W. Im-
proving accuracy in word class tagging through
combination of machine learning systems. Compu-
tational Linguistics, 27 (2). 2001 199-230.
4. Klein, D., Smarr, J., Nguyen, H. and Manning,
C.D., Named Entity Recognition with Character-
Level Models. in Conference on Computational
Natural Language Learning, 2003, 180-183.
5. Lafferty, J., McCallum, A. and Pereira, F. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. International
Conference on Machine Learning. 2001 282-289.
6. Rabiner, L. A tutorial on hidden Markov models
and selected applications in speech recognition.
Proceedings of the IEEE, 77 (2). 1989 257-286.
7. Sutton, C., Rohanimanesh, K. and McCallum, A.,
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting
Sequence Data. in Proceedings of the Twenty-First
International Conference on Machine Learning,
2004, 99-107.
8. Zavrel, J. and Daelemans, W. Memory-based learn-
ing: using similarity for smoothing. Proceedings of
the eighth conference on European chapter of the
Association for Computational Linguistics. 1997
436 - 443.
</reference>
<page confidence="0.998812">
145
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.634332">
<title confidence="0.999981">On Using Ensemble Methods for Chinese Named Entity Recognition</title>
<author confidence="0.8438475">Chia-Wei Wu Shyh-Yi Jan Richard Tzong-Han Wen-Lian Hsu Tsai</author>
<affiliation confidence="0.968533">Institute of Information Science, Academia Sinica, Nankang, Taipei,115,</affiliation>
<email confidence="0.986921">cwwu@iis.sinica.edu.tw</email>
<email confidence="0.986921">shihyi@iis.sinica.edu.tw</email>
<email confidence="0.986921">thtsai@iis.sinica.edu.tw</email>
<email confidence="0.986921">Hsu@iis.sinica.edu.tw</email>
<abstract confidence="0.9964878">In sequence labeling tasks, applying different machine learning models and feature sets usually leads to different results. In this paper, we exploit two ensemble methods in order to integrate multiple results generated under different conditions. One method is based on majority vote, while the other is a memory-based approach that integrates maximum entropy and conditional random field classifiers. Our results indicate that the memory-based method can outperform the individual classifiers, but the majority vote method cannot.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computer Linguistic,</journal>
<volume>22</volume>
<pages>39--71</pages>
<contexts>
<context position="2122" citStr="[1]" startWordPosition="322" endWordPosition="322">tage of the CRF model is that richer feature sets can be considered, because, unlike HMM, it does not make a dependence assumption. However, the obvious drawback of the CRF model is that it needs more computing resources, so we can not apply all the features of the model. One possible way to resolve this problem is to effectively combine the results of various individual classifiers trained with different feature sets. In this paper, we use two ensemble methods to combine the results of the classifiers. We also combine the results generated by two machine learning models: maximum entropy (ME) [1] and CRF. One ensemble method is based on the majority vote [3], and the other is the memory based learner [7]. Although the ensemble methods have been applied in some sequence labeling tasks [2],[3], similar work in Chinese named entity recognition is scarce. Our Chinese named entity tagger uses a character-based model. For English named entity tasks, a character-based NER model proposed by Dan Klein [4] proves the usefulness of substrings within words. In Chinese NER, the characterbased model is more straightforward, since there are no spaces between Chinese words and each Chinese character </context>
</contexts>
<marker>1.</marker>
<rawString>Berger, A., Pietra, S.A.D. and Pietra, V.J.D. A Maximum Entropy Approach to Natural Language Processing. Computer Linguistic, 22. 1996 39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>T Zhang</author>
</authors>
<title>Named Entity Recognition through Classifier Combination.</title>
<date>2003</date>
<booktitle>in Proceedings of Conference on Computational Natural Language Learning,</booktitle>
<pages>168--171</pages>
<contexts>
<context position="2317" citStr="[2]" startWordPosition="358" endWordPosition="358"> more computing resources, so we can not apply all the features of the model. One possible way to resolve this problem is to effectively combine the results of various individual classifiers trained with different feature sets. In this paper, we use two ensemble methods to combine the results of the classifiers. We also combine the results generated by two machine learning models: maximum entropy (ME) [1] and CRF. One ensemble method is based on the majority vote [3], and the other is the memory based learner [7]. Although the ensemble methods have been applied in some sequence labeling tasks [2],[3], similar work in Chinese named entity recognition is scarce. Our Chinese named entity tagger uses a character-based model. For English named entity tasks, a character-based NER model proposed by Dan Klein [4] proves the usefulness of substrings within words. In Chinese NER, the characterbased model is more straightforward, since there are no spaces between Chinese words and each Chinese character is actually meaningful. Another reason for using a character-based model is that it can avoid the errors sometimes made by a Chinese word segmentor. The remainder of this paper is organized as fo</context>
</contexts>
<marker>2.</marker>
<rawString>Florian, R., Ittycheriah, A., Jing, H. and Zhang, T., Named Entity Recognition through Classifier Combination. in Proceedings of Conference on Computational Natural Language Learning, 2003, 168-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H v Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving accuracy in word class tagging through combination of machine learning systems.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<pages>199--230</pages>
<contexts>
<context position="2185" citStr="[3]" startWordPosition="334" endWordPosition="334">ered, because, unlike HMM, it does not make a dependence assumption. However, the obvious drawback of the CRF model is that it needs more computing resources, so we can not apply all the features of the model. One possible way to resolve this problem is to effectively combine the results of various individual classifiers trained with different feature sets. In this paper, we use two ensemble methods to combine the results of the classifiers. We also combine the results generated by two machine learning models: maximum entropy (ME) [1] and CRF. One ensemble method is based on the majority vote [3], and the other is the memory based learner [7]. Although the ensemble methods have been applied in some sequence labeling tasks [2],[3], similar work in Chinese named entity recognition is scarce. Our Chinese named entity tagger uses a character-based model. For English named entity tasks, a character-based NER model proposed by Dan Klein [4] proves the usefulness of substrings within words. In Chinese NER, the characterbased model is more straightforward, since there are no spaces between Chinese words and each Chinese character is actually meaningful. Another reason for using a character-ba</context>
</contexts>
<marker>3.</marker>
<rawString>Halteren, H.v., Zavrel, J. and Daelemans, W. Improving accuracy in word class tagging through combination of machine learning systems. Computational Linguistics, 27 (2). 2001 199-230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>J Smarr</author>
<author>H Nguyen</author>
<author>C D Manning</author>
</authors>
<title>Named Entity Recognition with CharacterLevel Models.</title>
<date>2003</date>
<booktitle>in Conference on Computational Natural Language Learning,</booktitle>
<pages>180--183</pages>
<contexts>
<context position="2530" citStr="[4]" startWordPosition="391" endWordPosition="391"> feature sets. In this paper, we use two ensemble methods to combine the results of the classifiers. We also combine the results generated by two machine learning models: maximum entropy (ME) [1] and CRF. One ensemble method is based on the majority vote [3], and the other is the memory based learner [7]. Although the ensemble methods have been applied in some sequence labeling tasks [2],[3], similar work in Chinese named entity recognition is scarce. Our Chinese named entity tagger uses a character-based model. For English named entity tasks, a character-based NER model proposed by Dan Klein [4] proves the usefulness of substrings within words. In Chinese NER, the characterbased model is more straightforward, since there are no spaces between Chinese words and each Chinese character is actually meaningful. Another reason for using a character-based model is that it can avoid the errors sometimes made by a Chinese word segmentor. The remainder of this paper is organized as follows. In the Section 2, we introduce the machine learning models, the features we apply in the machine learning models, and the ensemble methods. In Section 3, we briefly describe the experimental data and the ex</context>
</contexts>
<marker>4.</marker>
<rawString>Klein, D., Smarr, J., Nguyen, H. and Manning, C.D., Named Entity Recognition with CharacterLevel Models. in Conference on Computational Natural Language Learning, 2003, 180-183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>International Conference on Machine Learning.</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1302" citStr="[5]" startWordPosition="181" endWordPosition="181"> method can outperform the individual classifiers, but the majority vote method cannot. 1 Introduction Sequence labeling and segmentation tasks have been studied extensively in the fields of computational linguistics and information extraction. Several tasks, including, word segmentation, and semantic role labeling, provide rich information for various applications, such as segmentation in Chinese information retrieval and named entity recognition in biomedical literature mining. Probabilistic state automata models, such as the Hidden Markov model (HMM) [6] and conditional random fields (CRF) [5] are some of best, and therefore most popular, approaches for sequence labeling tasks. Both HMM and CRF consider that the state transition and the state prediction are conditional on the observation of data. The advantage of the CRF model is that richer feature sets can be considered, because, unlike HMM, it does not make a dependence assumption. However, the obvious drawback of the CRF model is that it needs more computing resources, so we can not apply all the features of the model. One possible way to resolve this problem is to effectively combine the results of various individual classifie</context>
<context position="4297" citStr="[5]" startWordPosition="697" endWordPosition="697">cessing, pages 142–145, Sydney, July 2006. c�2006 Association for Computational Linguistics tures in x; the features are helpful for making predictions about the outcomes, y. Given a set of features and a training set, the ME estimation process produces a model, in which every feature fi has a weight λi. The ME model can be represented by the following formula: p(y |x)= z1( x ⎝ i ⎠ , z( x) ∑ ∑ ( )⎟ ⎛ ⎞ = exp λ , ⎜ i f i x y y ⎝ i ⎠ . The probability is derived by multiplying the weights of the active features (i.e., those fi (y,x) = 1). Conditional Random Field A conditional random field (CRF)[5] can be seen as an undirected graph model in which the nodes corresponding to the label sequence y are conditional on the observed sequence x. The goal of CRF is to find the label sequence y that has the maximized probability, given an observation sequence x. The formula for the CRF model can be written as: exp ( ( y x ) ) ∑ λ j j F j , , where λj is the parameter of a corresponding feature Fj , Z(x) is an normalizing factor, and Fj can be written as: F y, x ( ) = ∑ n = ( − 0 f y 1 , y , x, i), j i i i i where i means the relative position in the sequence, and yi-1 and yi denote the label at p</context>
</contexts>
<marker>5.</marker>
<rawString>Lafferty, J., McCallum, A. and Pereira, F. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. International Conference on Machine Learning. 2001 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>257--286</pages>
<contexts>
<context position="1262" citStr="[6]" startWordPosition="174" endWordPosition="174">r results indicate that the memory-based method can outperform the individual classifiers, but the majority vote method cannot. 1 Introduction Sequence labeling and segmentation tasks have been studied extensively in the fields of computational linguistics and information extraction. Several tasks, including, word segmentation, and semantic role labeling, provide rich information for various applications, such as segmentation in Chinese information retrieval and named entity recognition in biomedical literature mining. Probabilistic state automata models, such as the Hidden Markov model (HMM) [6] and conditional random fields (CRF) [5] are some of best, and therefore most popular, approaches for sequence labeling tasks. Both HMM and CRF consider that the state transition and the state prediction are conditional on the observation of data. The advantage of the CRF model is that richer feature sets can be considered, because, unlike HMM, it does not make a dependence assumption. However, the obvious drawback of the CRF model is that it needs more computing resources, so we can not apply all the features of the model. One possible way to resolve this problem is to effectively combine the</context>
</contexts>
<marker>6.</marker>
<rawString>Rabiner, L. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77 (2). 1989 257-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>K Rohanimanesh</author>
<author>A McCallum</author>
</authors>
<title>Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data.</title>
<date>2004</date>
<booktitle>in Proceedings of the Twenty-First International Conference on Machine Learning,</booktitle>
<pages>99--107</pages>
<contexts>
<context position="2232" citStr="[7]" startWordPosition="343" endWordPosition="343">ependence assumption. However, the obvious drawback of the CRF model is that it needs more computing resources, so we can not apply all the features of the model. One possible way to resolve this problem is to effectively combine the results of various individual classifiers trained with different feature sets. In this paper, we use two ensemble methods to combine the results of the classifiers. We also combine the results generated by two machine learning models: maximum entropy (ME) [1] and CRF. One ensemble method is based on the majority vote [3], and the other is the memory based learner [7]. Although the ensemble methods have been applied in some sequence labeling tasks [2],[3], similar work in Chinese named entity recognition is scarce. Our Chinese named entity tagger uses a character-based model. For English named entity tasks, a character-based NER model proposed by Dan Klein [4] proves the usefulness of substrings within words. In Chinese NER, the characterbased model is more straightforward, since there are no spaces between Chinese words and each Chinese character is actually meaningful. Another reason for using a character-based model is that it can avoid the errors somet</context>
</contexts>
<marker>7.</marker>
<rawString>Sutton, C., Rohanimanesh, K. and McCallum, A., Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data. in Proceedings of the Twenty-First International Conference on Machine Learning, 2004, 99-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Memory-based learning: using similarity for smoothing.</title>
<date>1997</date>
<booktitle>Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics.</booktitle>
<pages>436--443</pages>
<marker>8.</marker>
<rawString>Zavrel, J. and Daelemans, W. Memory-based learning: using similarity for smoothing. Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics. 1997 436 - 443.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>