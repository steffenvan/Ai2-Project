<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007050">
<title confidence="0.982372">
Extra-Linguistic Constraints on Stance Recognition in Ideological Debates
</title>
<author confidence="0.909631">
Kazi Saidul Hasan and Vincent Ng
</author>
<affiliation confidence="0.963453">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.899209">
Richardson, TX 75083-0688
</address>
<email confidence="0.999702">
{saidul,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858307692308">
Determining the stance expressed by an
author from a post written for a two-
sided debate in an online debate forum
is a relatively new problem. We seek to
improve Anand et al.’s (2011) approach
to debate stance classification by model-
ing two types of soft extra-linguistic con-
straints on the stance labels of debate
posts, user-interaction constraints and ide-
ology constraints. Experimental results on
four datasets demonstrate the effectiveness
of these inter-post constraints in improv-
ing debate stance classification.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999671243243243">
While a lot of work on document-level opinion
mining has involved determining the polarity ex-
pressed in a customer review (e.g., whether a re-
view is “thumbs up” or “thumbs down”) (see Pang
and Lee (2008) and Liu (2012) for an overview
of the field), researchers have begun exploring
new opinion mining tasks in recent years. One
such task is debate stance classification: given
a post written for a two-sided topic discussed in
an online debate forum (e.g., “Should abortion be
banned?”), determine which of the two sides (i.e.,
for and against) its author is taking.
Debate stance classification is potentially more
interesting and challenging than polarity classifi-
cation for at least two reasons. First, while in po-
larity classification sentiment-bearing words and
phrases have proven to be useful (e.g., “excellent”
correlates strongly with the positive polarity), in
debate stance classification it is not uncommon to
find debate posts where stances are not expressed
in terms of sentiment words, as exemplified in Fig-
ure 1, where the author is for abortion.
Second, while customer reviews are typically
written independently of other reviews in an on-
line forum, the same is not true for debate posts. In
The fetus is simply a part of the mother’s body and she
can have an abortion because it is her human rights. Also
I take this view because every woman can face with sit-
uation when two lives are at stake and the moral obli-
gation is to save the one closest at hand — namely, that
of the mother, whose life is always more immediate than
that of the unborn child within her body. Permission for
an abortion could then be based on psychiatric consider-
ations such as prepartum depression, especially if there
is responsible psychiatric opinion that a continued preg-
nancy raises the strong probability of suicide in a clini-
cally depressed patient.
</bodyText>
<figureCaption confidence="0.999411">
Figure 1: A sample post on abortion.
</figureCaption>
<bodyText confidence="0.999879366666666">
a debate forum, debate posts form threads, where
later posts often support or oppose the viewpoints
raised in earlier posts in the same thread.
Previous approaches to debate stance classifica-
tion have focused on three debate settings, namely
congressional floor debates (Thomas et al., 2006;
Bansal et al., 2008; Balahur et al., 2009; Yesse-
nalina et al., 2010; Burfoot et al., 2011), company-
internal discussions (Murakami and Raymond,
2010), and online social, political, and ideologi-
cal debates in public forums (Agrawal et al., 2003;
Somasundaran and Wiebe, 2010; Wang and Ros´e,
2010; Biran and Rambow, 2011; Hasan and Ng,
2012). As Walker et al. (2012) point out, debates
in public forums differ from congressional debates
and company-internal discussions in terms of lan-
guage use. Specifically, online debaters use color-
ful and emotional language to express their points,
which may involve sarcasm, insults, and question-
ing another debater’s assumptions and evidence.
These properties can potentially make stance clas-
sification of online debates more challenging than
that of the other two types of debates.
Our goal in this paper is to improve the state-
of-the-art supervised learning approach to debate
stance classification of online debates proposed by
Anand et al. (2011), focusing in particular on ideo-
logical debates. Specifically, we hypothesize that
there are two types of soft extra-linguistic con-
straints on the stance labels of debate posts that,
</bodyText>
<page confidence="0.98184">
816
</page>
<note confidence="0.5301925">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 816–821,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.999482333333333">
Domain Number “for” % of posts Average thread
of posts posts (%) in a thread length
ABO 1741 54.9 75.1 4.1
GAY 1376 63.4 74.5 4.0
OBA 985 53.9 57.1 2.6
MAR 626 69.5 58.0 2.5
</table>
<tableCaption confidence="0.999914">
Table 1: Statistics of the four datasets.
</tableCaption>
<bodyText confidence="0.999800571428571">
if explicitly modeled, could improve a learning-
based stance classification system. We refer to
these two types of inter-post constraints as user-
interaction constraints and ideology constraints.
We show how they can be learned from stance-
annotated debate posts in Sections 4.1 and 4.2, re-
spectively.
</bodyText>
<sectionHeader confidence="0.998103" genericHeader="introduction">
2 Datasets
</sectionHeader>
<bodyText confidence="0.999894421052632">
For our experiments, we collect debate posts
from four popular domains, Abortion (ABO),
Gay Rights (GAY), Obama (OBA), and Marijuana
(MAR), from an online debate forum1. All de-
bates are two-sided, so each post receives one of
two domain labels, for or against, depending on
whether the author of the post supports or opposes
abortion, gay rights, Obama, or the legalization of
marijuana.
We construct one dataset for each domain (see
Table 1 for statistics). The fourth column of the
table shows the percentage of posts in each domain
that appear in a thread. More precisely, a thread
is a tree with one or more nodes such that (1) each
node corresponds to a debate post, and (2) a post yi
is the parent of another post yj if yj is a reply to yi.
Given a thread, we can generate post sequences,
each of which is a path from the root of the thread
to one of its leaves.
</bodyText>
<sectionHeader confidence="0.986321" genericHeader="method">
3 Baseline Systems
</sectionHeader>
<bodyText confidence="0.999831384615385">
We employ as baselines two stance classification
systems, Anand et al.’s (2011) approach and an en-
hanced version of it, as described below.
Our first baseline, Anand et al.’s approach is a
supervised method that trains a stance classifier
for determining whether the stance expressed in
a debate post is for or against the topic. Hence,
we create one training instance from each post in
the training set, using the stance it expresses as
its class label. Following Anand et al., we repre-
sent a training instance using three types of lexico-
syntactic features, which are briefly summarized
in Table 2. In our implementation, we train the
</bodyText>
<footnote confidence="0.988569">
1http://www.createdebate.com/
</footnote>
<table confidence="0.9999036">
Feature type Features
Basic Unigrams, bigrams, syntactic and POS-
generalized dependencies
Sentiment LIWC counts, opinion dependencies
Argument Cue words, repeated punctuation, context
</table>
<tableCaption confidence="0.998804">
Table 2: Anand et al.’s features.
</tableCaption>
<bodyText confidence="0.9996794">
stance classifier using SVMlight (Joachims, 1999).
After training, we can apply the classifier to clas-
sify the test instances, which are generated in the
same way as the training instances.
Related work on stance classification of con-
gressional debates has found that enforcing au-
thor constraints (ACs) can improve classification
performance (e.g., Thomas et al. (2006), Bansal et
al. (2008), Burfoot et al. (2011), Lu et al. (2012),
Walker et al. (2012)). ACs are a type of inter-
post constraints that specify that two posts written
by the same author for the same debate domain
should have the same stance. We hypothesize that
ACs could similarly be used to improve stance
classification of ideological debates, and therefore
propose a second baseline where we enhance the
first baseline with ACs. Enforcing ACs is simple.
We first use the learned stance classifier to classify
the test posts as in the first baseline, and then post-
process the labels of the test posts. Specifically,
we sum up the confidence values2 assigned to the
set of test posts written by the same author for the
same debate domain. If the sum is positive, then
we label all the posts in this set as for; otherwise
we label them as against.
</bodyText>
<sectionHeader confidence="0.999403" genericHeader="method">
4 Extra-Linguistic Constraints
</sectionHeader>
<bodyText confidence="0.999559">
In this section, we introduce two types of inter-
post constraints on debate stance classification.
</bodyText>
<subsectionHeader confidence="0.993823">
4.1 User-Interaction Constraints
</subsectionHeader>
<bodyText confidence="0.990948857142857">
We call the first type of constraints user-
interaction constraints (UCs). UCs are motivated
by the observation that the stance labels of the
posts in a post sequence are not independent of
each other. Consider the post sequence in Fig-
ure 2, where each post is a response to the preced-
ing post. It shows an opening anti-abortion post
(P1), followed by a pro-abortion comment (P2),
which is in turn followed by another anti-abortion
view (P3). While this sequence contains alternat-
ing posts from opposing stances, in general there
is no hard constraint on the stance of a post given
2We use as the confidence value the signed distance of the
associated test point from the SVM hyperplane.
</bodyText>
<page confidence="0.982961">
817
</page>
<bodyText confidence="0.883780692307692">
[P1: Anti-abortion] There are thousands of people who
want to take these children because they cannot have their
own. If you do not want a child, have it and put it up for
adoption. At least you will be preserving a human life rather
than killing one.
[P2: Pro-abortion] I agree that if people don’t want
their babies, they should have the choice of putting it
up for adoption. But it should not be made compulsory,
which is essentially what happens if you ban abortion.
[P3: Anti-abortion] Why should it not be made
compulsory? Those children have as much right to
live as you and I. Besides, no one loses with adop-
tion, so why wouldn’t you utilize it?
</bodyText>
<figureCaption confidence="0.9962295">
Figure 2: A sample post sequence. P2 and P3 are
replies to P1 and P2, respectively.
</figureCaption>
<bodyText confidence="0.999892172413793">
the preceding sequence of posts. Nevertheless, we
found that in our training data, a for (against) post
is followed by a against (for) post 80% of the time.
UCs aim to model the regularities in how users
interact with each other in a post sequence as soft
constraints. These kinds of soft constraints can be
naturally encoded as factors over adjacent posts in
a post sequence (see Kschischang et al. (2001)),
which can in turn be learned by recasting stance
classification as a sequence labeling task. In our
experiments, we seek to derive the best sequence
of stance labels for each post sequence of length &gt;
1 using a Conditional Random Field (CRF) (Laf-
ferty et al., 2001).
We train the CRF model using the CRF im-
plementation in Mallet (McCallum, 2002). Each
training sequence corresponds to a post sequence.
Each post in a sequence is represented using the
same set of features as in the baselines.
After training, the resulting CRF model can be
used to assign a stance sequence to each test post
sequence. There is a caveat, however. Since a
given test post may appear in more than one se-
quence, different occurrences of it may be as-
signed different stance labels by the CRF. To deter-
mine the final stance label for the post, we average
the probabilities assigned to the for stance over all
its occurrences; if the average is &gt; 0.5, then its
final label is for; otherwise, its label is against.
</bodyText>
<subsectionHeader confidence="0.889186">
4.2 Ideology Constraints
</subsectionHeader>
<bodyText confidence="0.999974733333333">
Next, we introduce our second type of inter-post
constraints, ideology constraints (ICs). ICs are
cross-domain, author-based constraints: they are
only applicable to debate posts written by the same
author in different domains. ICs model the fact
that for some authors, their stances on various is-
sues are determined in part by their ideological
values, and in particular, their stances on different
issues may be correlated. For example, someone
who opposes abortion is likely to be a conserva-
tive and has a good chance of opposing gay rights.
ICs aim to capture this kind of inter-domain corre-
lation of stances. Below we describe how we im-
plement ICs and show how they can be integrated
with ACs.
</bodyText>
<subsectionHeader confidence="0.678142">
4.2.1 Implementing Ideology Constraints
</subsectionHeader>
<bodyText confidence="0.5446352">
We first compute a set of conditional probabil-
ities, P(stance(dq)=sd|stance(dp)=sc), where (1)
dp, dq E Domains (i.e., the set of four domains),
(2) sc, sd E {for, against}, and (3) dp 7� dq.
To compute P(stance(dq)=sd|stance(dp)=sc), we
</bodyText>
<listItem confidence="0.888443">
(1) determine for each author a in the train-
ing set and each domain dp the stance of a
in dp (denoted by author-stance(dp,a)), where
author-stance(dp,a) is computed as the majority
stance labels associated with the debate posts
in the training set that a wrote for dp; and
(2) compute P(stance(dq)=sd|stance(dp)=sc) as
the ratio of Ea∈A Count(author-stance(dp,a)=sc,
author-stance(dq,a)=sd) to Ea∈A Count(author-
stance(dp,a)=sc), where A is the set of authors in
the training set who posted in both dp and dq. It
should be fairly easy to see that these conditional
probabilities measure the degree of correlation be-
tween the stances in different domains.
</listItem>
<subsubsectionHeader confidence="0.893286">
4.2.2 Inference Using ILP
</subsubsectionHeader>
<bodyText confidence="0.999345904761905">
Recall that in our second baseline, we employ
ACs to postprocess the output of the stance clas-
sifier simply by summing up the confidence val-
ues assigned to the posts written by the same au-
thor for the same debate domain. However, since
we now want to enforce two types of inter-post
constraints (namely, ACs and ICs), we will have
to employ a more sophisticated inference mecha-
nism. Previous work has focused on employing
graph minimum cut (MinCut) as the inference al-
gorithm. However, since MinCut suffers from the
weakness of not being able to enforce negative
constraints (i.e., two posts cannot receive the same
label) (Bansal et al., 2008), we propose to use in-
teger linear programming (ILP) as the underlying
inference mechanism. Below we show how to im-
plement ACs and ICs within the ILP framework.
Owing to space limitations, we refer the reader
to Roth and Yih (2004) for details of the ILP
framework. Briefly, ILP seeks to optimize an
objective function subject to a set of linear con-
</bodyText>
<page confidence="0.990473">
818
</page>
<bodyText confidence="0.9992882">
straints. Below we focus on describing the ILP
program and how the ACs and ICs can be encoded.
Let Y = yi, ... , yn be the set of debate posts.
For each yi, we create one (binary-valued) indi-
cator variable xi, which will be used in the ILP
program. Let pi = P(for|yi) be the “benefit” of
setting xi to 1, where P(for|yi) is provided by the
CRF. Consequently, after optimization, yi’s stance
is for if its xi is set to 1. We optimize the following
objective function:
</bodyText>
<sectionHeader confidence="0.999851" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.99014">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.921848">
Results are expressed in terms of accuracy ob-
tained via 5-fold cross validation, where accuracy
</bodyText>
<footnote confidence="0.588311">
3Intuitively, if this condition is satisfied, it means that
there is sufficient evidence that the two nodes from differ-
ent domains should have the same stance, and so we convert
the soft ICs into (hard) linear constraints in ILP. Note that t is
a threshold to be tuned using development data.
</footnote>
<table confidence="0.9717774">
System ABO GAY OBA MAR
Anand 61.4 62.6 58.1 66.9
Anand+AC 72.0 64.9 62.7 67.8
Anand+AC+UC 73.7 69.9 64.1 75.4
Anand+AC+UC+IC 74.9 70.9 72.7 75.4
</table>
<tableCaption confidence="0.999677">
Table 3: 5-fold cross-validation accuracies.
</tableCaption>
<bodyText confidence="0.9991392">
is the percentage of test instances correctly classi-
fied. Since all experiments require the use of de-
velopment data for parameter tuning, we use three
folds for model training, one fold for development,
and one fold for testing in each fold experiment.
</bodyText>
<subsectionHeader confidence="0.782579">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999587575757576">
Results are shown in Table 3. Row 1 shows the
results of the Anand et al. (2011) baseline (see
Section 3) on the four datasets, obtained by train-
ing a SVM stance classifier using the SVMlight
software.4 Row 2 shows the results of the sec-
ond baseline, Anand et al.’s system enhanced with
ACs. As we can see, incorporating ACs into
Anand et al.’s system improves its performance
significantly on all datasets and yields a system
that achieves an average improvement of 4.6 ac-
curacy points.5
Next, we incorporate our first type of con-
straints, UCs, into the better of the two baselines
(i.e., the second baseline). Results of applying the
CRF for modeling UCs to the test posts and post-
processing them using the ACs are shown in row 3
of Table 3. As we can see, incorporating UCs into
the second baseline significantly improves its per-
formance and yields a system that achieves an av-
erage improvement of 3.93 accuracy points.
Finally, we incorporate our second type of con-
straints, ICs, effectively performing inference over
the CRF output using ILP with ACs and ICs as the
inter-post constraints. Results of this experiment
are shown in row 4 of Table 3. As we can see, in-
corporating the ICs significantly improves the per-
formance of the system on all but MAR and yields
a system that achieves an average improvement of
2.7 accuracy points.
Overall, our inter-post constraints yield a stance
classification system that significantly outper-
forms the better baseline on all four datasets, with
an average improvement of 6.63 accuracy points.
</bodyText>
<footnote confidence="0.99784325">
4For all SVM experiments, the regularization parameter C
is tuned using development data, but the remaining learning
parameters are set to their default values.
5All significance tests are paired t-tests, with p &lt; 0.05.
</footnote>
<equation confidence="0.912147">
�max pixi + (1 − pi)(1 − xi)
i
</equation>
<bodyText confidence="0.992561818181818">
subject to a set of linear constraints, which encode
the ACs and the ICs, as described below.
Implementing author constraints. If yi and yj
are composed by the same author, we ensure that
xi and xj will be assigned the same value by em-
ploying the linear constraint |xi − xj |= 0.
Implementing ideology constraints. For con-
venience, below we use the notation introduced in
Section 4.2.1, and assume that yi and yj are two
arbitrary posts written by the same author in do-
mains dp and dq, respectively.
</bodyText>
<figureCaption confidence="0.990827">
Case 1: If P(stance(dq)=for|stance(dp)=for) ≥ t,
we want to ensure that xi=1 =⇒ xj=1.3 This can
be achieved using the constraint (1−xj) ≤ (1−xi).
Case 2: If P(stance(dq)=against|stance(dp)=against)
≥ t, we want to ensure that xi=0 =⇒ xj=0. This
can be achieved using the constraint xj ≤ xi.
Case 3: If P(stance(dq)=against|stance(dp)=for)
≥ t, we want to ensure that xi=1 =⇒ xj=0. This
can be achieved using the constraint xj ≤ (1−xi).
Case 4: If P(stance(dq)=for|stance(dp)=against)
≥ t, we want to ensure that xi=0 =⇒ xj=1. This
can be achieved using the constraint (1−xj) ≤ xi.
</figureCaption>
<bodyText confidence="0.9946095">
Two points deserve mention. First, cases 3 and
4 correspond to negative constraints, and unlike in
MinCut, they can be implemented easily in ILP.
Second, if ICs are used, one ILP program will be
created to perform inference over the debate posts
in all four domains.
</bodyText>
<page confidence="0.99602">
819
</page>
<subsectionHeader confidence="0.951214">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999996958333333">
Next, we make some observations on the results of
applying ICs to our datasets.
First, ICs do not improve the MAR dataset. An
examination of the domains reveals the reason. We
find three pairs of ICs involving the other three do-
mains — ABO, GAY, and OBA — in our training
data. More specifically, the stances of the posts
written by an author for these three domains are
all positively co-related. In other words, if an au-
thor supports abortion, it is likely that she supports
both gay rights and Obama as well. On the other
hand, we find no co-relation between MAR and
the remaining domains. This means that no ICs
can be established between the posts in MAR and
those in the remaining domains.
Second, the improvement resulting from the ap-
plication of ICs is much larger on the OBA dataset
than on ABO and GAY. The reason can be at-
tributed to the fact that ICs exist more frequently
between OBA and ABO and between OBA and
GAY than between ABO and GAY. Specifically,
ICs are seen in all five folds of the data in the
first two pairs of domains, whereas they are seen
in only two folds in the last pair of domains.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999981152542373">
Previous work has investigated the use of extra-
linguistic constraints to improve stance classifica-
tion. Introduced by Thomas et al. (2006), ACs are
arguably the most commonly used extra-linguistic
constraints. Since then, they have been employed
and extended in different ways (see, for example,
Bansal et al. (2008), Burfoot et al. (2011), Lu et al.
(2012), and Walker et al. (2012)).
ICs are different from ACs in at least two re-
spects. First, ICs are softer than ACs, so accu-
rate modeling of ICs has to be based on stance-
annotated data. Although we employ ICs as hard
constraints (owing in part to our use of the ILP
framework), they can be used directly as soft con-
straints in other frameworks, such as MinCut. Sec-
ond, ICs are inter-domain constraints, whereas
ACs are intra-domain constraints. To our knowl-
edge, this is the first time inter-domain constraints
are employed for stance classification.
There has been work related to the modeling of
user interaction in a post sequence. Recall that be-
tween two adjacent posts in a post sequence that
have opposing stances, there exists a rebuttal link.
Walker et al. (2012) employ manually identified
rebuttal links as hard inter-post constraints dur-
ing inference. However, since automatic discov-
ery of rebuttal links is a non-trivial problem, em-
ploying gold rebuttal links substantially simplifies
the stance classification task. Lu et al. (2012), on
the other hand, predict whether a link is of type
agreement or disagreement using a bootstrapped
classifier. Anand et al. (2011) do not predict links.
Instead, hypothesizing that the content of the pre-
ceding post in a post sequence would be useful
for predicting the stance of the current post, they
employ features computed based on the preceding
post when training a stance classifier. Hence, un-
like us, they classify each post independently of
the others, whereas we classify the posts in a se-
quence in dependent relation to each other.
The ILP framework has been applied to perform
joint inference for a variety of stance prediction
tasks. Lu et al. (2012) address the task of discov-
ering opposing opinion networks, where the goal
is to partition the authors in a debate (e.g., gay
rights) based on whether they support or oppose
the given issue. To this end, they employ ILP
to coordinate different sources of information. In
our previous work on debate stance classification
(Hasan and Ng, 2012), we employ ILP to coor-
dinate the output of two classifiers: a post-stance
classifier, which determines the stance of a debate
post written for a domain (e.g., gay rights); and
a topic-stance classifier, which determines the au-
thor’s stance on each topic mentioned in her post
(e.g., gay marriage, gay adoption). In this work,
on the other hand, we train only one classifier,
but use ILP to coordinate two types of constraints,
ACs and ICs.
</bodyText>
<sectionHeader confidence="0.999232" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999993">
We examined the under-studied task of stance
classification of ideological debates. Employing
our two types of extra-linguistic constraints yields
a system that outperforms an improved version of
Anand et al.’s approach by 2.9–10 accuracy points.
While the effectiveness of ideology constraints de-
pends to some extent on the “relatedness” of the
underlying ideological domains, we believe that
the gains they offer will increase with the num-
ber of authors posting in different domains and the
number of related domains.6
</bodyText>
<footnote confidence="0.921742666666667">
6Only a small fraction of the authors posted in multiple
domains in our datasets: 12% and 5% of them posted in two
and three domains, respectively.
</footnote>
<page confidence="0.995624">
820
</page>
<sectionHeader confidence="0.995753" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999664019417476">
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In Pro-
ceedings of the 12th International Conference on
World Wide Web, WWW ’03, pages 529–535.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis (WASSA 2011), pages 1–9.
Alexandra Balahur, Zornitsa Kozareva, and Andr´es
Montoyo. 2009. Determining the polarity and
source of opinions expressed in political debates. In
Proceedings of the 10th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, CICLing ’09, pages 468–480.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In Proceedings of the 22nd International
Conference on Computational Linguistics: Com-
panion volume: Posters, pages 15–18.
Or Biran and Owen Rambow. 2011. Identifying justi-
fications in written dialogs. In Proceedings of the
2011 IEEE Fifth International Conference on Se-
mantic Computing, ICSC ’11, pages 162–168.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506–1515.
Kazi Saidul Hasan and Vincent Ng. 2012. Predict-
ing stance in ideological debate with rich linguistic
knowledge. In Proceedings ofthe 24th International
Conference on Computational Linguistics: Posters,
pages 451–460.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning, pages 44–56. MIT Press.
Frank Kschischang, Brendan J. Frey, and Hans-Andrea
Loeliger. 2001. Factor graphs and the sum-product
algorithm. IEEE Transactions on Information The-
ory, 47:498–519.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282–
289.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan &amp; Claypool Publishers.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of the 21st ACM International Conference
on Information and Knowledge Management, CIKM
’12, pages 1642–1646.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit. http://
mallet.cs.umass.edu.
Akiko Murakami and Rudy Raymond. 2010. Support
or oppose? Classifying positions in online debates
from reply activities and opinion expressions. In
Proceedings ofthe 23rd International Conference on
Computational Linguistics: Posters, pages 869–875.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1–2):1–135.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of the Eighth Confer-
ence on ComputationalNatural Language Learning,
pages 1–8.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116–124.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327–335.
Marilyn Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012. Stance classification using dialogic
properties ofpersuasion. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 592–596.
Yi-Chia Wang and Carolyn P. Ros´e. 2010. Making
conversational structure explicit: Identification of
initiation-response pairs within online discussions.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 673–676.
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010. Multi-level structured models for document-
level sentiment classification. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 1046–1056.
</reference>
<page confidence="0.998403">
821
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.522444">
<title confidence="0.999606">Extra-Linguistic Constraints on Stance Recognition in Ideological Debates</title>
<author confidence="0.997356">Saidul Hasan</author>
<affiliation confidence="0.9900935">Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.549024">Richardson, TX</address>
<abstract confidence="0.997844214285714">Determining the stance expressed by an author from a post written for a twosided debate in an online debate forum is a relatively new problem. We seek to improve Anand et al.’s (2011) approach to debate stance classification by modeling two types of soft extra-linguistic constraints on the stance labels of debate posts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Sridhar Rajagopalan</author>
<author>Ramakrishnan Srikant</author>
<author>Yirong Xu</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th International Conference on World Wide Web, WWW ’03,</booktitle>
<pages>529--535</pages>
<contexts>
<context position="3206" citStr="Agrawal et al., 2003" startWordPosition="509" endWordPosition="512">ty of suicide in a clinically depressed patient. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this pape</context>
</contexts>
<marker>Agrawal, Rajagopalan, Srikant, Xu, 2003</marker>
<rawString>Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan Srikant, and Yirong Xu. 2003. Mining newsgroups using networks arising from social behavior. In Proceedings of the 12th International Conference on World Wide Web, WWW ’03, pages 529–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranav Anand</author>
<author>Marilyn Walker</author>
<author>Rob Abbott</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Michael Minor</author>
</authors>
<title>Cats rule and dogs drool!: Classifying stance in online debate.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA</booktitle>
<pages>1--9</pages>
<contexts>
<context position="3952" citStr="Anand et al. (2011)" startWordPosition="624" endWordPosition="627">t out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this paper is to improve the stateof-the-art supervised learning approach to debate stance classification of online debates proposed by Anand et al. (2011), focusing in particular on ideological debates. Specifically, we hypothesize that there are two types of soft extra-linguistic constraints on the stance labels of debate posts that, 816 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 816–821, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Domain Number “for” % of posts Average thread of posts posts (%) in a thread length ABO 1741 54.9 75.1 4.1 GAY 1376 63.4 74.5 4.0 OBA 985 53.9 57.1 2.6 MAR 626 69.5 58.0 2.5 Table 1: Statistics of the four datasets. if explici</context>
<context position="14933" citStr="Anand et al. (2011)" startWordPosition="2487" endWordPosition="2490"> constraints in ILP. Note that t is a threshold to be tuned using development data. System ABO GAY OBA MAR Anand 61.4 62.6 58.1 66.9 Anand+AC 72.0 64.9 62.7 67.8 Anand+AC+UC 73.7 69.9 64.1 75.4 Anand+AC+UC+IC 74.9 70.9 72.7 75.4 Table 3: 5-fold cross-validation accuracies. is the percentage of test instances correctly classified. Since all experiments require the use of development data for parameter tuning, we use three folds for model training, one fold for development, and one fold for testing in each fold experiment. 5.2 Results Results are shown in Table 3. Row 1 shows the results of the Anand et al. (2011) baseline (see Section 3) on the four datasets, obtained by training a SVM stance classifier using the SVMlight software.4 Row 2 shows the results of the second baseline, Anand et al.’s system enhanced with ACs. As we can see, incorporating ACs into Anand et al.’s system improves its performance significantly on all datasets and yields a system that achieves an average improvement of 4.6 accuracy points.5 Next, we incorporate our first type of constraints, UCs, into the better of the two baselines (i.e., the second baseline). Results of applying the CRF for modeling UCs to the test posts and p</context>
<context position="20672" citStr="Anand et al. (2011)" startWordPosition="3473" endWordPosition="3476">s been work related to the modeling of user interaction in a post sequence. Recall that between two adjacent posts in a post sequence that have opposing stances, there exists a rebuttal link. Walker et al. (2012) employ manually identified rebuttal links as hard inter-post constraints during inference. However, since automatic discovery of rebuttal links is a non-trivial problem, employing gold rebuttal links substantially simplifies the stance classification task. Lu et al. (2012), on the other hand, predict whether a link is of type agreement or disagreement using a bootstrapped classifier. Anand et al. (2011) do not predict links. Instead, hypothesizing that the content of the preceding post in a post sequence would be useful for predicting the stance of the current post, they employ features computed based on the preceding post when training a stance classifier. Hence, unlike us, they classify each post independently of the others, whereas we classify the posts in a sequence in dependent relation to each other. The ILP framework has been applied to perform joint inference for a variety of stance prediction tasks. Lu et al. (2012) address the task of discovering opposing opinion networks, where th</context>
</contexts>
<marker>Anand, Walker, Abbott, Tree, Bowmani, Minor, 2011</marker>
<rawString>Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox Tree, Robeson Bowmani, and Michael Minor. 2011. Cats rule and dogs drool!: Classifying stance in online debate. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2011), pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Zornitsa Kozareva</author>
<author>Andr´es Montoyo</author>
</authors>
<title>Determining the polarity and source of opinions expressed in political debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing ’09,</booktitle>
<pages>468--480</pages>
<contexts>
<context position="3005" citStr="Balahur et al., 2009" startWordPosition="478" endWordPosition="481">an abortion could then be based on psychiatric considerations such as prepartum depression, especially if there is responsible psychiatric opinion that a continued pregnancy raises the strong probability of suicide in a clinically depressed patient. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning anot</context>
</contexts>
<marker>Balahur, Kozareva, Montoyo, 2009</marker>
<rawString>Alexandra Balahur, Zornitsa Kozareva, and Andr´es Montoyo. 2009. Determining the polarity and source of opinions expressed in political debates. In Proceedings of the 10th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing ’09, pages 468–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Claire Cardie</author>
<author>Lillian Lee</author>
</authors>
<title>The power of negative thinking: Exploiting label disagreement in the min-cut classification framework.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics: Companion volume: Posters,</booktitle>
<pages>15--18</pages>
<contexts>
<context position="2983" citStr="Bansal et al., 2008" startWordPosition="474" endWordPosition="477">body. Permission for an abortion could then be based on psychiatric considerations such as prepartum depression, especially if there is responsible psychiatric opinion that a continued pregnancy raises the strong probability of suicide in a clinically depressed patient. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults</context>
<context position="7014" citStr="Bansal et al. (2008)" startWordPosition="1127" endWordPosition="1130">debate.com/ Feature type Features Basic Unigrams, bigrams, syntactic and POSgeneralized dependencies Sentiment LIWC counts, opinion dependencies Argument Cue words, repeated punctuation, context Table 2: Anand et al.’s features. stance classifier using SVMlight (Joachims, 1999). After training, we can apply the classifier to classify the test instances, which are generated in the same way as the training instances. Related work on stance classification of congressional debates has found that enforcing author constraints (ACs) can improve classification performance (e.g., Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), Walker et al. (2012)). ACs are a type of interpost constraints that specify that two posts written by the same author for the same debate domain should have the same stance. We hypothesize that ACs could similarly be used to improve stance classification of ideological debates, and therefore propose a second baseline where we enhance the first baseline with ACs. Enforcing ACs is simple. We first use the learned stance classifier to classify the test posts as in the first baseline, and then postprocess the labels of the test posts. Specifically, we sum</context>
<context position="13153" citStr="Bansal et al., 2008" startWordPosition="2173" endWordPosition="2176">d baseline, we employ ACs to postprocess the output of the stance classifier simply by summing up the confidence values assigned to the posts written by the same author for the same debate domain. However, since we now want to enforce two types of inter-post constraints (namely, ACs and ICs), we will have to employ a more sophisticated inference mechanism. Previous work has focused on employing graph minimum cut (MinCut) as the inference algorithm. However, since MinCut suffers from the weakness of not being able to enforce negative constraints (i.e., two posts cannot receive the same label) (Bansal et al., 2008), we propose to use integer linear programming (ILP) as the underlying inference mechanism. Below we show how to implement ACs and ICs within the ILP framework. Owing to space limitations, we refer the reader to Roth and Yih (2004) for details of the ILP framework. Briefly, ILP seeks to optimize an objective function subject to a set of linear con818 straints. Below we focus on describing the ILP program and how the ACs and ICs can be encoded. Let Y = yi, ... , yn be the set of debate posts. For each yi, we create one (binary-valued) indicator variable xi, which will be used in the ILP program</context>
<context position="19456" citStr="Bansal et al. (2008)" startWordPosition="3270" endWordPosition="3273"> be attributed to the fact that ICs exist more frequently between OBA and ABO and between OBA and GAY than between ABO and GAY. Specifically, ICs are seen in all five folds of the data in the first two pairs of domains, whereas they are seen in only two folds in the last pair of domains. 6 Related Work Previous work has investigated the use of extralinguistic constraints to improve stance classification. Introduced by Thomas et al. (2006), ACs are arguably the most commonly used extra-linguistic constraints. Since then, they have been employed and extended in different ways (see, for example, Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), and Walker et al. (2012)). ICs are different from ACs in at least two respects. First, ICs are softer than ACs, so accurate modeling of ICs has to be based on stanceannotated data. Although we employ ICs as hard constraints (owing in part to our use of the ILP framework), they can be used directly as soft constraints in other frameworks, such as MinCut. Second, ICs are inter-domain constraints, whereas ACs are intra-domain constraints. To our knowledge, this is the first time inter-domain constraints are employed for stance classification. There has b</context>
</contexts>
<marker>Bansal, Cardie, Lee, 2008</marker>
<rawString>Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. In Proceedings of the 22nd International Conference on Computational Linguistics: Companion volume: Posters, pages 15–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Owen Rambow</author>
</authors>
<title>Identifying justifications in written dialogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 IEEE Fifth International Conference on Semantic Computing, ICSC ’11,</booktitle>
<pages>162--168</pages>
<contexts>
<context position="3282" citStr="Biran and Rambow, 2011" startWordPosition="521" endWordPosition="524">n abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this paper is to improve the stateof-the-art supervised learning approach to debate s</context>
</contexts>
<marker>Biran, Rambow, 2011</marker>
<rawString>Or Biran and Owen Rambow. 2011. Identifying justifications in written dialogs. In Proceedings of the 2011 IEEE Fifth International Conference on Semantic Computing, ICSC ’11, pages 162–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clinton Burfoot</author>
<author>Steven Bird</author>
<author>Timothy Baldwin</author>
</authors>
<title>Collective classification of congressional floor-debate transcripts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1506--1515</pages>
<contexts>
<context position="3054" citStr="Burfoot et al., 2011" startWordPosition="487" endWordPosition="490">onsiderations such as prepartum depression, especially if there is responsible psychiatric opinion that a continued pregnancy raises the strong probability of suicide in a clinically depressed patient. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These pro</context>
<context position="7037" citStr="Burfoot et al. (2011)" startWordPosition="1131" endWordPosition="1134">pe Features Basic Unigrams, bigrams, syntactic and POSgeneralized dependencies Sentiment LIWC counts, opinion dependencies Argument Cue words, repeated punctuation, context Table 2: Anand et al.’s features. stance classifier using SVMlight (Joachims, 1999). After training, we can apply the classifier to classify the test instances, which are generated in the same way as the training instances. Related work on stance classification of congressional debates has found that enforcing author constraints (ACs) can improve classification performance (e.g., Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), Walker et al. (2012)). ACs are a type of interpost constraints that specify that two posts written by the same author for the same debate domain should have the same stance. We hypothesize that ACs could similarly be used to improve stance classification of ideological debates, and therefore propose a second baseline where we enhance the first baseline with ACs. Enforcing ACs is simple. We first use the learned stance classifier to classify the test posts as in the first baseline, and then postprocess the labels of the test posts. Specifically, we sum up the confidence valu</context>
<context position="19479" citStr="Burfoot et al. (2011)" startWordPosition="3274" endWordPosition="3277">fact that ICs exist more frequently between OBA and ABO and between OBA and GAY than between ABO and GAY. Specifically, ICs are seen in all five folds of the data in the first two pairs of domains, whereas they are seen in only two folds in the last pair of domains. 6 Related Work Previous work has investigated the use of extralinguistic constraints to improve stance classification. Introduced by Thomas et al. (2006), ACs are arguably the most commonly used extra-linguistic constraints. Since then, they have been employed and extended in different ways (see, for example, Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), and Walker et al. (2012)). ICs are different from ACs in at least two respects. First, ICs are softer than ACs, so accurate modeling of ICs has to be based on stanceannotated data. Although we employ ICs as hard constraints (owing in part to our use of the ILP framework), they can be used directly as soft constraints in other frameworks, such as MinCut. Second, ICs are inter-domain constraints, whereas ACs are intra-domain constraints. To our knowledge, this is the first time inter-domain constraints are employed for stance classification. There has been work related to the</context>
</contexts>
<marker>Burfoot, Bird, Baldwin, 2011</marker>
<rawString>Clinton Burfoot, Steven Bird, and Timothy Baldwin. 2011. Collective classification of congressional floor-debate transcripts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1506–1515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Predicting stance in ideological debate with rich linguistic knowledge.</title>
<date>2012</date>
<booktitle>In Proceedings ofthe 24th International Conference on Computational Linguistics: Posters,</booktitle>
<pages>451--460</pages>
<contexts>
<context position="3303" citStr="Hasan and Ng, 2012" startWordPosition="525" endWordPosition="528">um, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this paper is to improve the stateof-the-art supervised learning approach to debate stance classification </context>
<context position="21545" citStr="Hasan and Ng, 2012" startWordPosition="3621" endWordPosition="3624">fier. Hence, unlike us, they classify each post independently of the others, whereas we classify the posts in a sequence in dependent relation to each other. The ILP framework has been applied to perform joint inference for a variety of stance prediction tasks. Lu et al. (2012) address the task of discovering opposing opinion networks, where the goal is to partition the authors in a debate (e.g., gay rights) based on whether they support or oppose the given issue. To this end, they employ ILP to coordinate different sources of information. In our previous work on debate stance classification (Hasan and Ng, 2012), we employ ILP to coordinate the output of two classifiers: a post-stance classifier, which determines the stance of a debate post written for a domain (e.g., gay rights); and a topic-stance classifier, which determines the author’s stance on each topic mentioned in her post (e.g., gay marriage, gay adoption). In this work, on the other hand, we train only one classifier, but use ILP to coordinate two types of constraints, ACs and ICs. 7 Conclusions We examined the under-studied task of stance classification of ideological debates. Employing our two types of extra-linguistic constraints yield</context>
</contexts>
<marker>Hasan, Ng, 2012</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2012. Predicting stance in ideological debate with rich linguistic knowledge. In Proceedings ofthe 24th International Conference on Computational Linguistics: Posters, pages 451–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods -Support Vector Learning,</booktitle>
<pages>44--56</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6672" citStr="Joachims, 1999" startWordPosition="1075" endWordPosition="1076">t the topic. Hence, we create one training instance from each post in the training set, using the stance it expresses as its class label. Following Anand et al., we represent a training instance using three types of lexicosyntactic features, which are briefly summarized in Table 2. In our implementation, we train the 1http://www.createdebate.com/ Feature type Features Basic Unigrams, bigrams, syntactic and POSgeneralized dependencies Sentiment LIWC counts, opinion dependencies Argument Cue words, repeated punctuation, context Table 2: Anand et al.’s features. stance classifier using SVMlight (Joachims, 1999). After training, we can apply the classifier to classify the test instances, which are generated in the same way as the training instances. Related work on stance classification of congressional debates has found that enforcing author constraints (ACs) can improve classification performance (e.g., Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), Walker et al. (2012)). ACs are a type of interpost constraints that specify that two posts written by the same author for the same debate domain should have the same stance. We hypothesize that ACs could similarly b</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Advances in Kernel Methods -Support Vector Learning, pages 44–56. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Kschischang</author>
<author>Brendan J Frey</author>
<author>Hans-Andrea Loeliger</author>
</authors>
<title>Factor graphs and the sum-product algorithm.</title>
<date>2001</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>47--498</pages>
<contexts>
<context position="9834" citStr="Kschischang et al. (2001)" startWordPosition="1619" endWordPosition="1622">mpulsory? Those children have as much right to live as you and I. Besides, no one loses with adoption, so why wouldn’t you utilize it? Figure 2: A sample post sequence. P2 and P3 are replies to P1 and P2, respectively. the preceding sequence of posts. Nevertheless, we found that in our training data, a for (against) post is followed by a against (for) post 80% of the time. UCs aim to model the regularities in how users interact with each other in a post sequence as soft constraints. These kinds of soft constraints can be naturally encoded as factors over adjacent posts in a post sequence (see Kschischang et al. (2001)), which can in turn be learned by recasting stance classification as a sequence labeling task. In our experiments, we seek to derive the best sequence of stance labels for each post sequence of length &gt; 1 using a Conditional Random Field (CRF) (Lafferty et al., 2001). We train the CRF model using the CRF implementation in Mallet (McCallum, 2002). Each training sequence corresponds to a post sequence. Each post in a sequence is represented using the same set of features as in the baselines. After training, the resulting CRF model can be used to assign a stance sequence to each test post sequen</context>
</contexts>
<marker>Kschischang, Frey, Loeliger, 2001</marker>
<rawString>Frank Kschischang, Brendan J. Frey, and Hans-Andrea Loeliger. 2001. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, 47:498–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="10102" citStr="Lafferty et al., 2001" startWordPosition="1665" endWordPosition="1669">und that in our training data, a for (against) post is followed by a against (for) post 80% of the time. UCs aim to model the regularities in how users interact with each other in a post sequence as soft constraints. These kinds of soft constraints can be naturally encoded as factors over adjacent posts in a post sequence (see Kschischang et al. (2001)), which can in turn be learned by recasting stance classification as a sequence labeling task. In our experiments, we seek to derive the best sequence of stance labels for each post sequence of length &gt; 1 using a Conditional Random Field (CRF) (Lafferty et al., 2001). We train the CRF model using the CRF implementation in Mallet (McCallum, 2002). Each training sequence corresponds to a post sequence. Each post in a sequence is represented using the same set of features as in the baselines. After training, the resulting CRF model can be used to assign a stance sequence to each test post sequence. There is a caveat, however. Since a given test post may appear in more than one sequence, different occurrences of it may be assigned different stance labels by the CRF. To determine the final stance label for the post, we average the probabilities assigned to the</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282– 289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1002" citStr="Liu (2012)" startWordPosition="148" endWordPosition="149">We seek to improve Anand et al.’s (2011) approach to debate stance classification by modeling two types of soft extra-linguistic constraints on the stance labels of debate posts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification. 1 Introduction While a lot of work on document-level opinion mining has involved determining the polarity expressed in a customer review (e.g., whether a review is “thumbs up” or “thumbs down”) (see Pang and Lee (2008) and Liu (2012) for an overview of the field), researchers have begun exploring new opinion mining tasks in recent years. One such task is debate stance classification: given a post written for a two-sided topic discussed in an online debate forum (e.g., “Should abortion be banned?”), determine which of the two sides (i.e., for and against) its author is taking. Debate stance classification is potentially more interesting and challenging than polarity classification for at least two reasons. First, while in polarity classification sentiment-bearing words and phrases have proven to be useful (e.g., “excellent</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Lu</author>
<author>Hongning Wang</author>
<author>ChengXiang Zhai</author>
<author>Dan Roth</author>
</authors>
<title>Unsupervised discovery of opposing opinion networks from forum discussions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12,</booktitle>
<pages>1642--1646</pages>
<contexts>
<context position="7055" citStr="Lu et al. (2012)" startWordPosition="1135" endWordPosition="1138">ams, bigrams, syntactic and POSgeneralized dependencies Sentiment LIWC counts, opinion dependencies Argument Cue words, repeated punctuation, context Table 2: Anand et al.’s features. stance classifier using SVMlight (Joachims, 1999). After training, we can apply the classifier to classify the test instances, which are generated in the same way as the training instances. Related work on stance classification of congressional debates has found that enforcing author constraints (ACs) can improve classification performance (e.g., Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), Walker et al. (2012)). ACs are a type of interpost constraints that specify that two posts written by the same author for the same debate domain should have the same stance. We hypothesize that ACs could similarly be used to improve stance classification of ideological debates, and therefore propose a second baseline where we enhance the first baseline with ACs. Enforcing ACs is simple. We first use the learned stance classifier to classify the test posts as in the first baseline, and then postprocess the labels of the test posts. Specifically, we sum up the confidence values2 assigned to th</context>
<context position="19497" citStr="Lu et al. (2012)" startWordPosition="3278" endWordPosition="3281">e frequently between OBA and ABO and between OBA and GAY than between ABO and GAY. Specifically, ICs are seen in all five folds of the data in the first two pairs of domains, whereas they are seen in only two folds in the last pair of domains. 6 Related Work Previous work has investigated the use of extralinguistic constraints to improve stance classification. Introduced by Thomas et al. (2006), ACs are arguably the most commonly used extra-linguistic constraints. Since then, they have been employed and extended in different ways (see, for example, Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), and Walker et al. (2012)). ICs are different from ACs in at least two respects. First, ICs are softer than ACs, so accurate modeling of ICs has to be based on stanceannotated data. Although we employ ICs as hard constraints (owing in part to our use of the ILP framework), they can be used directly as soft constraints in other frameworks, such as MinCut. Second, ICs are inter-domain constraints, whereas ACs are intra-domain constraints. To our knowledge, this is the first time inter-domain constraints are employed for stance classification. There has been work related to the modeling of user </context>
<context position="21204" citStr="Lu et al. (2012)" startWordPosition="3564" endWordPosition="3567"> type agreement or disagreement using a bootstrapped classifier. Anand et al. (2011) do not predict links. Instead, hypothesizing that the content of the preceding post in a post sequence would be useful for predicting the stance of the current post, they employ features computed based on the preceding post when training a stance classifier. Hence, unlike us, they classify each post independently of the others, whereas we classify the posts in a sequence in dependent relation to each other. The ILP framework has been applied to perform joint inference for a variety of stance prediction tasks. Lu et al. (2012) address the task of discovering opposing opinion networks, where the goal is to partition the authors in a debate (e.g., gay rights) based on whether they support or oppose the given issue. To this end, they employ ILP to coordinate different sources of information. In our previous work on debate stance classification (Hasan and Ng, 2012), we employ ILP to coordinate the output of two classifiers: a post-stance classifier, which determines the stance of a debate post written for a domain (e.g., gay rights); and a topic-stance classifier, which determines the author’s stance on each topic ment</context>
</contexts>
<marker>Lu, Wang, Zhai, Roth, 2012</marker>
<rawString>Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan Roth. 2012. Unsupervised discovery of opposing opinion networks from forum discussions. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12, pages 1642–1646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http:// mallet.cs.umass.edu.</note>
<contexts>
<context position="10182" citStr="McCallum, 2002" startWordPosition="1682" endWordPosition="1683"> 80% of the time. UCs aim to model the regularities in how users interact with each other in a post sequence as soft constraints. These kinds of soft constraints can be naturally encoded as factors over adjacent posts in a post sequence (see Kschischang et al. (2001)), which can in turn be learned by recasting stance classification as a sequence labeling task. In our experiments, we seek to derive the best sequence of stance labels for each post sequence of length &gt; 1 using a Conditional Random Field (CRF) (Lafferty et al., 2001). We train the CRF model using the CRF implementation in Mallet (McCallum, 2002). Each training sequence corresponds to a post sequence. Each post in a sequence is represented using the same set of features as in the baselines. After training, the resulting CRF model can be used to assign a stance sequence to each test post sequence. There is a caveat, however. Since a given test post may appear in more than one sequence, different occurrences of it may be assigned different stance labels by the CRF. To determine the final stance label for the post, we average the probabilities assigned to the for stance over all its occurrences; if the average is &gt; 0.5, then its final la</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http:// mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akiko Murakami</author>
<author>Rudy Raymond</author>
</authors>
<title>Support or oppose? Classifying positions in online debates from reply activities and opinion expressions.</title>
<date>2010</date>
<booktitle>In Proceedings ofthe 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>869--875</pages>
<contexts>
<context position="3112" citStr="Murakami and Raymond, 2010" startWordPosition="494" endWordPosition="497">ly if there is responsible psychiatric opinion that a continued pregnancy raises the strong probability of suicide in a clinically depressed patient. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of onli</context>
</contexts>
<marker>Murakami, Raymond, 2010</marker>
<rawString>Akiko Murakami and Rudy Raymond. 2010. Support or oppose? Classifying positions in online debates from reply activities and opinion expressions. In Proceedings ofthe 23rd International Conference on Computational Linguistics: Posters, pages 869–875.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="987" citStr="Pang and Lee (2008)" startWordPosition="143" endWordPosition="146">relatively new problem. We seek to improve Anand et al.’s (2011) approach to debate stance classification by modeling two types of soft extra-linguistic constraints on the stance labels of debate posts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification. 1 Introduction While a lot of work on document-level opinion mining has involved determining the polarity expressed in a customer review (e.g., whether a review is “thumbs up” or “thumbs down”) (see Pang and Lee (2008) and Liu (2012) for an overview of the field), researchers have begun exploring new opinion mining tasks in recent years. One such task is debate stance classification: given a post written for a two-sided topic discussed in an online debate forum (e.g., “Should abortion be banned?”), determine which of the two sides (i.e., for and against) its author is taking. Debate stance classification is potentially more interesting and challenging than polarity classification for at least two reasons. First, while in polarity classification sentiment-bearing words and phrases have proven to be useful (e</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1–2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth Conference on ComputationalNatural Language Learning,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="13384" citStr="Roth and Yih (2004)" startWordPosition="2214" endWordPosition="2217">ce two types of inter-post constraints (namely, ACs and ICs), we will have to employ a more sophisticated inference mechanism. Previous work has focused on employing graph minimum cut (MinCut) as the inference algorithm. However, since MinCut suffers from the weakness of not being able to enforce negative constraints (i.e., two posts cannot receive the same label) (Bansal et al., 2008), we propose to use integer linear programming (ILP) as the underlying inference mechanism. Below we show how to implement ACs and ICs within the ILP framework. Owing to space limitations, we refer the reader to Roth and Yih (2004) for details of the ILP framework. Briefly, ILP seeks to optimize an objective function subject to a set of linear con818 straints. Below we focus on describing the ILP program and how the ACs and ICs can be encoded. Let Y = yi, ... , yn be the set of debate posts. For each yi, we create one (binary-valued) indicator variable xi, which will be used in the ILP program. Let pi = P(for|yi) be the “benefit” of setting xi to 1, where P(for|yi) is provided by the CRF. Consequently, after optimization, yi’s stance is for if its xi is set to 1. We optimize the following objective function: 5 Evaluatio</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of the Eighth Conference on ComputationalNatural Language Learning, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in ideological on-line debates.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>116--124</pages>
<contexts>
<context position="3236" citStr="Somasundaran and Wiebe, 2010" startWordPosition="513" endWordPosition="516">nically depressed patient. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this paper is to improve the stateof-th</context>
</contexts>
<marker>Somasundaran, Wiebe, 2010</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2010. Recognizing stances in ideological on-line debates. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>327--335</pages>
<contexts>
<context position="2962" citStr="Thomas et al., 2006" startWordPosition="470" endWordPosition="473">orn child within her body. Permission for an abortion could then be based on psychiatric considerations such as prepartum depression, especially if there is responsible psychiatric opinion that a continued pregnancy raises the strong probability of suicide in a clinically depressed patient. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may inv</context>
<context position="6992" citStr="Thomas et al. (2006)" startWordPosition="1123" endWordPosition="1126">the 1http://www.createdebate.com/ Feature type Features Basic Unigrams, bigrams, syntactic and POSgeneralized dependencies Sentiment LIWC counts, opinion dependencies Argument Cue words, repeated punctuation, context Table 2: Anand et al.’s features. stance classifier using SVMlight (Joachims, 1999). After training, we can apply the classifier to classify the test instances, which are generated in the same way as the training instances. Related work on stance classification of congressional debates has found that enforcing author constraints (ACs) can improve classification performance (e.g., Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), Walker et al. (2012)). ACs are a type of interpost constraints that specify that two posts written by the same author for the same debate domain should have the same stance. We hypothesize that ACs could similarly be used to improve stance classification of ideological debates, and therefore propose a second baseline where we enhance the first baseline with ACs. Enforcing ACs is simple. We first use the learned stance classifier to classify the test posts as in the first baseline, and then postprocess the labels of the test posts</context>
<context position="19278" citStr="Thomas et al. (2006)" startWordPosition="3243" endWordPosition="3246">sts in MAR and those in the remaining domains. Second, the improvement resulting from the application of ICs is much larger on the OBA dataset than on ABO and GAY. The reason can be attributed to the fact that ICs exist more frequently between OBA and ABO and between OBA and GAY than between ABO and GAY. Specifically, ICs are seen in all five folds of the data in the first two pairs of domains, whereas they are seen in only two folds in the last pair of domains. 6 Related Work Previous work has investigated the use of extralinguistic constraints to improve stance classification. Introduced by Thomas et al. (2006), ACs are arguably the most commonly used extra-linguistic constraints. Since then, they have been employed and extended in different ways (see, for example, Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), and Walker et al. (2012)). ICs are different from ACs in at least two respects. First, ICs are softer than ACs, so accurate modeling of ICs has to be based on stanceannotated data. Although we employ ICs as hard constraints (owing in part to our use of the ILP framework), they can be used directly as soft constraints in other frameworks, such as MinCut. Second, ICs are inter-d</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 327–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Pranav Anand</author>
<author>Rob Abbott</author>
<author>Ricky Grant</author>
</authors>
<title>Stance classification using dialogic properties ofpersuasion.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>592--596</pages>
<contexts>
<context position="3328" citStr="Walker et al. (2012)" startWordPosition="530" endWordPosition="533">reads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this paper is to improve the stateof-the-art supervised learning approach to debate stance classification of online debates propose</context>
<context position="7077" citStr="Walker et al. (2012)" startWordPosition="1139" endWordPosition="1142">actic and POSgeneralized dependencies Sentiment LIWC counts, opinion dependencies Argument Cue words, repeated punctuation, context Table 2: Anand et al.’s features. stance classifier using SVMlight (Joachims, 1999). After training, we can apply the classifier to classify the test instances, which are generated in the same way as the training instances. Related work on stance classification of congressional debates has found that enforcing author constraints (ACs) can improve classification performance (e.g., Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), Walker et al. (2012)). ACs are a type of interpost constraints that specify that two posts written by the same author for the same debate domain should have the same stance. We hypothesize that ACs could similarly be used to improve stance classification of ideological debates, and therefore propose a second baseline where we enhance the first baseline with ACs. Enforcing ACs is simple. We first use the learned stance classifier to classify the test posts as in the first baseline, and then postprocess the labels of the test posts. Specifically, we sum up the confidence values2 assigned to the set of test posts wr</context>
<context position="19523" citStr="Walker et al. (2012)" startWordPosition="3283" endWordPosition="3286">BA and ABO and between OBA and GAY than between ABO and GAY. Specifically, ICs are seen in all five folds of the data in the first two pairs of domains, whereas they are seen in only two folds in the last pair of domains. 6 Related Work Previous work has investigated the use of extralinguistic constraints to improve stance classification. Introduced by Thomas et al. (2006), ACs are arguably the most commonly used extra-linguistic constraints. Since then, they have been employed and extended in different ways (see, for example, Bansal et al. (2008), Burfoot et al. (2011), Lu et al. (2012), and Walker et al. (2012)). ICs are different from ACs in at least two respects. First, ICs are softer than ACs, so accurate modeling of ICs has to be based on stanceannotated data. Although we employ ICs as hard constraints (owing in part to our use of the ILP framework), they can be used directly as soft constraints in other frameworks, such as MinCut. Second, ICs are inter-domain constraints, whereas ACs are intra-domain constraints. To our knowledge, this is the first time inter-domain constraints are employed for stance classification. There has been work related to the modeling of user interaction in a post sequ</context>
</contexts>
<marker>Walker, Anand, Abbott, Grant, 2012</marker>
<rawString>Marilyn Walker, Pranav Anand, Rob Abbott, and Ricky Grant. 2012. Stance classification using dialogic properties ofpersuasion. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Chia Wang</author>
<author>Carolyn P Ros´e</author>
</authors>
<title>Making conversational structure explicit: Identification of initiation-response pairs within online discussions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>673--676</pages>
<marker>Wang, Ros´e, 2010</marker>
<rawString>Yi-Chia Wang and Carolyn P. Ros´e. 2010. Making conversational structure explicit: Identification of initiation-response pairs within online discussions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 673–676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Yisong Yue</author>
<author>Claire Cardie</author>
</authors>
<title>Multi-level structured models for documentlevel sentiment classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1046--1056</pages>
<contexts>
<context position="3031" citStr="Yessenalina et al., 2010" startWordPosition="482" endWordPosition="486"> be based on psychiatric considerations such as prepartum depression, especially if there is responsible psychiatric opinion that a continued pregnancy raises the strong probability of suicide in a clinically depressed patient. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions </context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010. Multi-level structured models for documentlevel sentiment classification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1046–1056.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>