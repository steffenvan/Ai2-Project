<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.952831">
A Telephone-Based Railway Information System for Spanish:
Development of a Methodology for Spoken Dialogue Design
</title>
<note confidence="0.420413">
R. San-Segundo, J.M. Montero, J.M. Gutierrez, A. Gallardo, J.D. Romeral and J.M. Pardo
Grupo de Tecnologia del Habla. Departamento de Ingenier�a Electronica. UPM
E.T.S.I. Telecomunicacion. Ciudad Universitaria s/n, 28040 Madrid, Spain
</note>
<email confidence="0.899852">
{lapizuuanchouuanalgallardoudromerallpardo}@die.upm.es
</email>
<sectionHeader confidence="0.969097" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909357142857">
In this paper, we describe the steps
carried out for developing a Railway
Information Service for Spanish. This
work introduces a methodology for
designing dialogue managers in spoken
dialogue systems for restricted domains.
In this methodology several sources of
information are combined: intuition,
observation and simulation for defining
several dialogue strategies, evaluating
them and choosing the best. Users in a
final evaluation gave the system a 3.9
score in a 5-point scale with an average
call duration of 204 s.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988640625">
The important improvements in Speech
Technology have allowed developing automatic
systems that can work under real conditions. At
this point, telephone-based spoken dialogue
systems have appeared as an important field for
applying these technologies. Last decade, a great
amount of restricted domain systems have been
developed: JUPITER (Zue, 1997) that provides
weather information; travel information and
ticket reservation services like RailTel (Lamel,
1997) and ARISE European projects (Lamel,
2000; Baggia, 2000). Another important spoken
dialogue project is the DARPA Communicator
(http://fofoca.mitre.org). Some systems
developed in this project are the CU (Pellom,
2000) and the CMU (Rudnicky, 2000)
Communicator systems. These services enable
natural conversational interaction with telephone
callers that access information about airline
flights, hotels and rental cars. Other services are
the Directory-Assistance systems like
(Schramm, 2000) or the automatic call
redirection system as implemented in AT&amp;T
(Riccardi, 2000). In this paper, we present a
methodology for designing the dialogue
manager in spoken dialogue system for
restricted domains. We use this methodology to
develop a train timetable information system for
the main Spanish intercity connections. In this
paper, we also present an approach for designing
the confirmation mechanisms and incorporating
user-modelling techniques.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="introduction">
2 Methodology for Dialogue Design
</sectionHeader>
<bodyText confidence="0.999466285714285">
The methodology proposed in this paper
consists of 5 steps. The first step is the database
analysis where the information contained in the
knowledge database is described by an Entity-
Relationship diagram. In the design by intuition,
a &amp;quot;brain-storming&amp;quot; over the E-R is performed for
proposing different dialogue alternatives. The
third step is the design by observation, where we
evaluate each proposal using user-operator
dialogue transcriptions. The next step we
simulate the system with a Wizard of Oz in
order to learn the specific characteristics of
human-system interactions. The fifth step is the
design by iterative improvement where the
confirmation strategies are designed in order to
implement a fully automatic system. The
railway information system developed in this
work uses isolated speech recognition, but the
methodology is general and it can be used to
develop system with continuous speech
reconigiton and understanding.
This methodology is similar to the Life-Cycle
Model presented in (Bernsen, 1998) and
(www.disc2.dk), but we incorporate the step
&amp;quot;design by observation&amp;quot; where human-human
interactions are analysed and we present
measures to evaluate the different design
alternatives at every step of the methodology.
</bodyText>
<sectionHeader confidence="0.9986765" genericHeader="method">
3 Database Analysis and Design by
Intuition
</sectionHeader>
<bodyText confidence="0.999986933333333">
The database, as the initial point in our
methodology contains the information or
knowledge we want to provide in our service.
For example in a railway information system,
this database contains the time information for
all the trains, their prices, their services,
etc...The aim of database analysis is to describe
this information, in order to offer the service.
This description consists of an Entity-
Relationship Diagram (E-R) that shows a
semantic representation of the data. In this
diagram, the main entity sets, their attributes and
keys (attributes that uniquely define an element
in an entity set), and entity set relationships must
be defined. We must pay attention to the entity
sets because they will be the possible goals that
the system can provide (parts of service such as
timetable information, reservations, fares, etc.).
The keys are the main mandatory items to ask
the user and they define the dialogue
interactions. The E-R diagram is not unique and
it depends strongly on the system designer and
the service.
After the database analysis, we propose a
&amp;quot;brain-storming&amp;quot; (design by intuition) over the
E-R to propose alternatives. These proposals are
concerned with the goals to provide, the
sequence of offering them and the items that are
needed to satisfy each goal (pieces of
information that must be obtained from the user,
such as departure and arrival cities, departing
date, etc.).
The ways the user can specify each item are
also an important issue. For example, for the
departure place, the user can say the station
name or the city name. In the second case, the
system should provide the information for all the
train stations in that city.
The result of this analysis is a table or work
sheet that includes all the alternatives. This table
will be used at next step of the methodology to
compute the frequency of each alternative
concerning with the goals, the information that
the system must provide, the ways the user
specifies an item, etc.
</bodyText>
<sectionHeader confidence="0.995361" genericHeader="method">
4 Design by Observation
</sectionHeader>
<bodyText confidence="0.9998498">
It aims at analysing user-operator dialogues in a
similar service and tracking off the observed
events. This design phase evaluates and
measures the impact of the alternatives proposed
at previous step.
</bodyText>
<subsectionHeader confidence="0.999548">
4.1 Goals analysis
</subsectionHeader>
<bodyText confidence="0.978295538461538">
a) Which goals are most frequently required by
the user, and which is the sequence for
asking them.
The result of this analysis is the number of
times that a specific goal is required and its
position. In Table 1, the goals analysis for
100 call transcriptions is presented. Several
goals can appear in the same call. In our
final system, we offer the goals that appear
in more than 10% calls, except Itinerary
(there is no information about that in the
database) and Others (it contains very
different goals but none reaches 10%).
</bodyText>
<tableCaption confidence="0.999599">
Table 1: Goals analysis.
</tableCaption>
<table confidence="0.995565181818182">
% Position
1st 2nd 3rd 4th Sth
Timetable 64 57 6 1 - -
Round trip 20 - 14 5 1 -
Timetable
Fares 46 6 30 10 - -
Reservations 26 14 4 2 3 3
Train frequency 2 2 - - - -
Itinerary 14 8 4 1 1 -
Bargains 5 1 2 1 1 -
Others 12 8 2 - 2 -
</table>
<construct confidence="0.386473">
b) The information given by the operator to
satisfy each goal.
</construct>
<bodyText confidence="0.99992125">
We annotate these data (i.e. departure and
arrival times, trip duration, train type, etc.)
and their importance (number of times that
the operator provides each item).
</bodyText>
<subsectionHeader confidence="0.999504">
4.2 Items analysis
</subsectionHeader>
<bodyText confidence="0.994553">
To satisfy a goal, the system must ask the user
some items. These items have to be also
analysed at this step:
</bodyText>
<listItem confidence="0.9669161875">
a) Which items are needed to satisfy each goal
and the sequence for asking them.
b) To classify items as mandatory or optional:
• Mandatory: it is an item needed to
satisfy the goal and must be asked to the
user (e.g. &amp;quot;What date are you
departing?&amp;quot;). If the user does not
provide this information, it must be
assigned a default value (e.g. &amp;quot;today&amp;quot;)
or the goals have to be satisfied
depending on its value (e.g. for all days
in current week).
• Optional: the system does not ask it to
the user. When the user specifies it, the
provided information should match this
value.
</listItem>
<bodyText confidence="0.95849219047619">
c) To classify each item as simple or complex.
A complex item can be divided into several
simple ones (e.g. &amp;quot;departure date&amp;quot; can be
divided into DAY, MONTH and YEAR).
With isolated speech recognition, each
simple item needs one interaction. A
continuous speech recognizer could get
several simple items simultaneously.
d) To analyse the different ways a user can
specify an item value and its importance.
Table 2 shows the results for the departure
date. A high percentage of people travel on
current week and a relevant percentage do
not specify any date, because they just want
general travel information between two
cities. 9.4% of people travelled in the same
month and specified the day with the day
number (e.g. the 20th). 4.7% of people
travelled on another month and specified the
date by the name of the month and the day
number.
</bodyText>
<tableCaption confidence="0.921958">
Table 2:Analysis for the Departure Date.
</tableCaption>
<table confidence="0.8287566">
Current week This Other An y
month month
today 25.0%
tomorrow 15.6% 9.4% 4.7% 28.1%
weekday 17.2%
</table>
<listItem confidence="0.825586">
e) Item ordering and grouping.
One step in the dialogue is a group of items
that can be asked to the user without
confirmation. Our recommendations are:
• To group items that are contiguous in the
item sequence and have a semantic
relationship between them.
• Not to group more than 4 consecutive items
(without leaving simple items alone).
</listItem>
<bodyText confidence="0.997728">
Just one confirmation can validate a group of
items. We can also incorporate rest zones to
provide help or a summary of the interaction.
The steps of the dialogue impose a certain
rhythm to the user-system interaction.
</bodyText>
<subsectionHeader confidence="0.999963">
4.3 Negotiation analysis
</subsectionHeader>
<bodyText confidence="0.989910916666667">
There is a &amp;quot;negotiation&amp;quot; when the user has to
choose one of the possible travel options:
a) What information helps the user to take a
decision and its importance.
Table 3 shows the analysis for negotiation
criteria. The number of connections is an
important factor, but it does not appear
frequently in user-operator dialogues
because the operator only offers direct trips
without connections. Train Type provides a
hint about fares and the services offered in
the train, generally known by the user.
</bodyText>
<tableCaption confidence="0.998632">
Table 3: Criteria analysis for negotiation.
</tableCaption>
<table confidence="0.999206333333333">
Criteria % Criteria %
Depart. time 41.0 # Connections 2.6
Arrival time 15.4 Connection 2.6
Depart. station 2.6 Class 10.3
Fares 7.7 Duration 5.0
Train Type 12.8
</table>
<subsubsectionHeader confidence="0.311006">
b) To Choose the negotiation strategy.
</subsubsectionHeader>
<bodyText confidence="0.999650368421053">
You can present the best option and let the
user ask for the previous/next one, or you
can present several alternatives at the same
time and ask the user to choose one of them.
In this case, it is important to analyse the
number of alternatives the operator gives
simultaneously (and the user can manage),
during the conversation.
In the observation analysis, only human-human
interactions are considered, and they can be
severely different from human-system ones.
Because of this, high level dialogue
characteristics can be learnt, but specific
behaviours when interacting with an automatic
system are not detected (e.g. changes in the user
speaking rate, usage of formal vs. colloquial
phrases, etc.). On the other hand, this analysis
permits to evaluate several alternatives without
having implemented any system.
</bodyText>
<sectionHeader confidence="0.984235" genericHeader="method">
5 Design by Simulation
</sectionHeader>
<bodyText confidence="0.999946117647059">
Now, the specific characteristics of human-
system interactions are analysed by simulating
the system with a Wizard of Oz approach. We
must focus on the design of the dialogue flow,
the questions to ask for obtaining the needed
item and the information the system should
provide to satisfy each goal. Dialogue
alternatives are implemented and evaluated. In
this evaluation, 15 users called the system,
completed 6 scenarios and fill a questionnaire.
The evaluating measures come from system
annotations (referred as System) and from user
answers in the questionnaire (referred as
Questionnaire), where the user answers about
subjective aspects difficult to be measured by
the system. The user answers are recorded in
audio files. The evaluating measures were:
</bodyText>
<subsectionHeader confidence="0.989771">
5.1 Goal evaluation
</subsectionHeader>
<bodyText confidence="0.9759585">
a) The measures used to validate the goal
sequence and the goal coverage were:
</bodyText>
<listItem confidence="0.999388">
• System: number of times a goal is
required by the user, time and number of
questions (interactions) to satisfy a goal.
• Questionnaire: new goals suggested by
the user.
</listItem>
<subsectionHeader confidence="0.91556">
5.2 Item evaluation
</subsectionHeader>
<bodyText confidence="0.8103775">
a) To design the questions and recognition
vocabulary.
</bodyText>
<listItem confidence="0.94170325">
• System: number of times the user keeps in
silence (does not answer the system
question) and, recognition rate for each
question (if a recognizer is already
available).
• Questionnaire: how easy is it to specify an
item value.
b) For the item sequence evaluation.
</listItem>
<bodyText confidence="0.9756365">
In some parts of the dialogue, several
sequences of questions for asking the items
are proposed and randomly selected in each
call. The evaluation measures are:
</bodyText>
<listItem confidence="0.971447">
• System: when it is possible, the Sequence
Recognition Rate (SRR) computed as the
product of all independent item
recognition rates.
</listItem>
<bodyText confidence="0.999557090909091">
Table 4 shows the sequence analysis for the
9departure and arrival cities. The SRR
difference is not significant, but we observed
the following effect during user calls: when
the system asks the Arrival City first, the
user assumes that the system knows the
Departure City and he/she gets confused
when the system asks about it. This explains
the great difference between the item
recognition rates for the Arrival-Departure
City sequence.
</bodyText>
<tableCaption confidence="0.847634333333333">
Table 4: Sequence evaluation for Departure and
Arrival cities (item recognition rate and
Sequence Recognition Rate).
</tableCaption>
<table confidence="0.99631025">
Recognition Rates (%)
Sequence 1st item 2nd item SRR
Depart.-Arrival City 75.6 80.0 60.5
Arrival-Depart. City 94.3 60.0 56.6
</table>
<listItem confidence="0.931713">
• Questionnaire: asking the user for his/her
preference.
c) Analysis of different ways to split complex
items into simple ones.
• System: number of interactions and time
to get a complex item, and the Sequence
Recognition Rate of the simple items.
• Questionnaire: how easy is it to specify a
complex item with the proposed sequence
of interactions.
d) How can we manage the mandatory items
when the user does not specify any value for
them
We should parameterise the information by
this item values or fix a default value.
• Questionnaire: to evaluate the best option,
we ask the user in the questionnaire if the
</listItem>
<bodyText confidence="0.800902">
information given by the system exceeds
his/her expectations (and it is necessary to
fix a default value) or it is adequate.
</bodyText>
<subsectionHeader confidence="0.989682">
5.3 Negotiation
</subsectionHeader>
<bodyText confidence="0.9999835">
In our case, we have decided to present several
travel options at the same time and ask the user
to choose one of them. Randomly, the system
presents the options one by one, two by two or
three by three. For the information provided per
option, several patterns are designed and
randomly selected for each group of options.
The measures considered for evaluation were:
</bodyText>
<listItem confidence="0.9971126">
• System: number of questions and time for
negotiation.
• Questionnaire: What information helps the
user to choose and number of options he/she
can manage at the same time.
</listItem>
<bodyText confidence="0.998715">
Users preferred to manage 3 options at the
same time (less interactions) but negotiation
takes more time. We decided to keep the
negotiation in a 3 by 3 basis reducing the
information provided per option to Train Type
plus Departure and Arrival Times.
</bodyText>
<tableCaption confidence="0.976471">
Table 5: Negotiation analysis in the simulation
step.
</tableCaption>
<table confidence="0.999875">
Negotiation analysis (User Questionnaire)
1 by 1 2 by 2 3 by 3
21.4% 21.4% 57.2%
Negotiation Criteria
Train Depart. Arrival Fares Duration
T Time Time
Type
15.6% 37.5% 25.0% 15.6% 6.2%
</table>
<bodyText confidence="0.9577135">
For global dialogue evaluation, we propose the
following measures:
</bodyText>
<listItem confidence="0.9631924">
• System: average number of interactions and
time per call, and recognition rate for all the
vocabularies.
• Questionnaire: the user has to evaluate in a
scale how fast did the user obtain the
</listItem>
<bodyText confidence="0.997702529411765">
information, how easy was to learn, which
part would you change, and how do you
compare the system to other ways of
obtaining the service: web, going to the
station, etc.
Whenever it is possible, it is better to use only
the measures computed by the system because
they are objective. The questionnaire can not be
longer than one sheet and should only address
issues that cannot be resolved by automatic
system annotations, such as subjective
evaluations or default values for mandatory
items. If we want to include many questions, it
is better to design several questionnaires that
evaluate different aspects. Every user should fill
only one of the questionnaires proposed. About
the scenarios to complete, it is important that the
users can define their own scenarios for testing a
greater diversity of situations. Some scenarios
should be imposed, in order to have enough data
to evaluate a specific situation.
The Wizard of Oz permits us to analyse
different dialogue designs without any system
implemented. Moreover, the system does not
need to confirm the item values, so we can make
independent designs for the dialogue and for the
confirmation mechanisms.
On the other hand, the main problem in this
step is the difficulty for simulating an automatic
system, especially the answer time. A person
spends more time when selecting the item values
than an automatic system. It is necessary to
develop tools that help the Wizard to simulate
the system in a realistic way.
</bodyText>
<sectionHeader confidence="0.996786" genericHeader="method">
6 Design by Iterative Improvement
</sectionHeader>
<bodyText confidence="0.99982175">
At this step, we implement the first version of a
fully automatic system in an iterative modify-
and-test process. With the Wizard of Oz, a first
version of the dialogue flow was defined and it
was necessary to design confirmation
mechanisms that take into account the
recognition confidence: an analysis of
confidence measures is required.
</bodyText>
<subsectionHeader confidence="0.999051">
6.1 Confirmation Strategies
</subsectionHeader>
<bodyText confidence="0.985863">
Depending on the number of items to confirm:
</bodyText>
<listItem confidence="0.977617692307692">
• One item. (&amp;quot;Have you said Madrid?&amp;quot;)
• Several items. (&amp;quot;Do you want to go from
Madrid to Sevilla?&amp;quot;)
Depending on the possibility to correct (Lavelle,
1999):
• Explicit confirmation: the system confirms
one or several item values through a direct
question. (&amp;quot;I understood you want to depart
from Madrid. Is that correct?&amp;quot;)
• Implicit confirmation: the system does not
permit the user a correction, it only reports
about the recognition result. (&amp;quot;You want to
leave from Madrid. Where are you arriving
at?&amp;quot;)
• Semi-implicit confirmation: it is similar to
the implicit confirmation, but the user can
correct (&amp;quot;You want to leave from Madrid. In
case of error, say correct, otherwise, indicate
your arrival city&amp;quot;).
• No confirmation: the system does not
provides feedback about the recognized
value (i.e. in yes/no questions).
• Item value rejection and repeat question:
when the confidence is low, the system does
not present the value to the user and repeats
the question (&amp;quot;Sorry, I can not understand.
</listItem>
<bodyText confidence="0.679324">
Where are you departing from?&amp;quot;).
</bodyText>
<subsectionHeader confidence="0.999478">
6.2 Confidence Measures in recognition
</subsectionHeader>
<bodyText confidence="0.999670166666667">
Confidence measures in recognition are vey
useful for designing the confirmations (Sturm,
1999). The recognition module used in our
system is a large vocabulary telephone speech
recognizer, that can recognize isolate words and
simple expressions such as &amp;quot;On monday&amp;quot;, &amp;quot;Next
week&amp;quot; or &amp;quot;In the morning&amp;quot;. The recognizer
(Macias, 2000a) is based on a hypothesis-
verification approach. The best features for
confidence annotation are concerned with the
verification step and are based on (Macias,
2000b):
</bodyText>
<listItem confidence="0.963187833333333">
• First Candidate Score: acoustic score of the
best verification candidate.
• Candidate Score Difference: difference of
acoustic score between the 1st and 2nd
verification candidates.
• Candidate Score Mean and Variance:
average score and variance over the 10 best
candidate names.
• Score Ratio: difference between the score
of the phone sequence (hypothesis stage)
and the score of the best candidate word
(verification stage).
</listItem>
<bodyText confidence="0.996219172413793">
All the features are divided by the number of
frames. We have considered a Multi-Layer
Perceptron (MLP) to combine the features in
order to obtain a unique confidence value. In
this case we use the features directly as inputs to
the MLP. With this solution, a preprocessing is
required to limit the dynamic range of each
measure to the (0,1) interval. Here, the
normalization scales the features using the
minimum and maximum value obtained for each
measure in the training set. The hidden layer
contains of 10 units and one output node was
used to model the word confidence. During
weight estimation, a target value of 1 is assigned
when the decoder correctly recognizes the name
and a value of 0, when incorrect recognition
occurs. The database used in the confidence
experiments is built with the results obtained in
the recognizer evaluation for a 1,100 name
dictionary. In this case we have 2,204 examples,
considering 1,450 for training the MLP, 370 as
the evaluation set and 370 for testing. We have
repeated it six times providing a 6-Round Robin
training to verify the results. In this paper we
present the average results of these experiments.
A 39.1% of wrong recognized words are
detected at 5% false rejection rate (Figure 1),
reducing the minimum classification error from
15.8% (recognition error rate) to 14.0%.
</bodyText>
<figure confidence="0.634541">
False rejection (%)
</figure>
<figureCaption confidence="0.9981">
Figure 1. False acceptance vs False Rejection.
</figureCaption>
<subsectionHeader confidence="0.99011">
6.3 Confirmation Mechanism design
</subsectionHeader>
<bodyText confidence="0.99922">
For designing the confirmation mechanism, it is
necessary to plot the correct words and errors
distributions as a function of the confidence
value and to define different confidence
thresholds.
</bodyText>
<figureCaption confidence="0.995112">
Figure 2. Bottom detail for Errors and Correct
words distributions vs. confidence.
</figureCaption>
<bodyText confidence="0.9937555">
We have defined 4 levels (3 thresholds) of
confidence (Figure 1):
</bodyText>
<listItem confidence="0.981735307692308">
1. Very High Confidence: the number of
correctly recognized words is much higher
than the number of errors.
2. High Confidence: the number of correctly
recognized words is higher than the number
of errors.
3. Low Confidence: both distributions are
similar. The system is not sure about the
correctness of the recognized word.
4. Very Low Confidence: in this case, there are
much more errors than correctly recognized
words, so we reject the recognized word and
the system must ask again.
</listItem>
<bodyText confidence="0.9793376">
For the departure and arrival cities step, we
define CL(D) and CL(A) as departure/arrival
city Confidence Levels (CL). Depending on the
CL, we implemented the following confirmation
strategies:
</bodyText>
<listItem confidence="0.973533461538462">
• CL(D)=1 and CL(A)=1: implicit
confirmation of both items. &amp;quot;You want to
travel from Madrid to Sevilla, When do you
want to leave?&amp;quot;
• CL(D)=2 or CL(A)=2: explicit confirmation
of both items. &amp;quot;Do you want a Madrid-
Sevilla trip?&amp;quot;
• CL(D)=3 or CL(A)=3: explicit confirmation
of each item. &amp;quot;Do you want to travel from
Madrid?&amp;quot;
• CL(D)=4 or CL(A)=4: rejection of each
item. &amp;quot;Sorry, I do not understand. Where are
you departing from?&amp;quot;
</listItem>
<bodyText confidence="0.9996095">
Under hard conditions, the system asks the
user to spell the city name (San-Segundo, 2000).
In this design, we have not considered the semi-
implicit confirmation because it is not very
friendly for the user. In Section 5.5, we describe
the CORRECT command that permits the same
functionality without increasing the prompt
length.
</bodyText>
<subsectionHeader confidence="0.9314215">
6.4 Confirmation Mechanism
Evaluation
</subsectionHeader>
<bodyText confidence="0.99999025">
For the iterative improvement, it is necessary to
evaluate the confirmation mechanisms in order
to adjust them when we changes the dialogue.
The proposed measures are:
</bodyText>
<listItem confidence="0.970512">
• For Implicit confirmation: the recognition
rate obtained for each item when comparing
the recorded audio files and the word that
was recognized as 1st candidate.
• For Semi-implicit confirmation: number of
times the user wants to correct. If this
</listItem>
<figure confidence="0.96156145">
False acceptance (%)
40
00
80
60
20
0
4 3 2 1
0,05 0,15 0,25 0,35 0,45 0,55 0,65 0,75 0,85 0,95
Confidence
250
200
150
100
50
0
Correct words
Errors
Errors and Correct words
distributions
</figure>
<bodyText confidence="0.906015642857143">
number is high, we should change to an
explicit confirmation; otherwise, we should
use an implicit confirmation strategy.
• For Explicit confirmation: number of times
the user denies the system proposal. If this
number is low we could think on relaxing
the confirmation strategy to an implicit one.
For all confirmations, we can compute the
number of questions and time needed to confirm
the item values and the number of times the user
deny the system proposal. To analyze the impact
of the confirmation mechanisms in the dialogue
speed, the system can compute the percentage of
implicit vs. explicit confirmations (see Table 6).
</bodyText>
<subsectionHeader confidence="0.932797">
6.5 Error recovering mechanisms
</subsectionHeader>
<bodyText confidence="0.965976740740741">
When an automatic system uses implicit
confirmations, it is necessary to define some
mechanisms to permit the user to recover from
system errors:
START OVER: this command permits the
user to start from scratch. Instead of resetting all
the items, our system begins confirming groups
of items explicitly (dialogue steps). When one
group is not confirmed, the system starts from
that point:
S: &amp;quot;The selected option is an Intercity train... &amp;quot;
U: &amp;quot;start over&amp;quot;
S: &amp;quot;Let us start the call. Do you want to go from
Madrid to Barcelona?&amp;quot;
U: &amp;quot;yes&amp;quot;
S: &amp;quot;Do you want to travel on 19th of July?&amp;quot;
U: &amp;quot;No&amp;quot;
S: &amp;quot;Do you want to leave this week, next week
or later?&amp;quot;
CORRECT: When the system makes a
mistake and takes a wrong item value as right
(in an implicit confirmation), the user can
correct the system by saying this command at
any point of the dialogue. In this case, the
system asks again the last introduced item.
When considering these commands, some
aspects must be kept in mind:
</bodyText>
<listItem confidence="0.993853333333333">
• The selected commands must be very
different from the words contained in the
recognition vocabularies, but it must be very
friendly to the user.
• The sentence used by the system to report to
the user the recognition of these commands
has to be carefully designed to permit a
smooth transition in the dialogue.
• It is necessary to specify the dialogue points
</listItem>
<bodyText confidence="0.993943666666667">
where the system should report to the user
about these possibilities: e.g. in the initial
help and in the help provided when the
system detects problems in the interaction.
When one of these commands is recognized,
the confirmation strategies have to be more
pessimist about the confidence because one
error, recognizing these commands, can
produces delays in the interaction.
</bodyText>
<subsectionHeader confidence="0.999905">
6.6 User-Modelling
</subsectionHeader>
<bodyText confidence="0.99995775">
The user modelling technique applied is based
on (Veldhuijzen van Zanten, 1999) using 4 user
skill levels. Depending on current level, the
prompt sentences are clearer (they contain more
information about how the user should answer,
possible values, etc...) or the system provides
more or less information per time unit. The
levels considered are:
</bodyText>
<listItem confidence="0.9831774">
• 1s&apos; level. The prompts explain how to
interact with the system, the asked item, the
possible accepted values and how to specify
one of the them (for the period of the day:
&amp;quot;Please speak after the tone. Say the period
of the day you want to travel in; in the
morning, in the afternoon or in the
evening.&amp;quot;).
• 2nd level. The prompts include the item
needed, the accepted values for it and the
way to specify them. (&amp;quot;Say the period of the
day you want to travel in; in the morning, in
the afternoon or in the evening.&amp;quot;)
• Yd level. Only the required item is included
in the prompt. (&amp;quot;Say the period of the day
you want to travel in.&amp;quot;)
• 4&apos;h level. The user knows everything (the
accepted values and the way to specify one
of them) and we can relax the question
(&amp;quot;When do you want to leave?&amp;quot;).
</listItem>
<bodyText confidence="0.996432268292683">
Current level depends on the initial state, the
number of errors and positive confirmations
along the interaction. In our case, the system
starts at the 2nd level (after providing a general
explanation about how to interact with the
system). When several errors (or positive
confirmations) occur, the system decreases (or
increases) the level. The number of errors or
positive confirmations that forces a change
depends on current level. Thus, the system
adapts dynamically the interaction to the user
skill, making more explicit questions when
recognition errors occur.
Example:
[The system is in the level 3]
S: &amp;quot;Say the period of the day you want to travel
in.&amp;quot;
U: &amp;quot;After lunch&amp;quot;
[The system recognizes &amp;quot;in the evening&amp;quot;]
S: &amp;quot;Have you said in the evening?&amp;quot;
U: &amp;quot;No&amp;quot;
[The system decreases the level from 3 to 2]
S: &amp;quot;Say the period of the day you want to travel
in; in the morning, in the afternoon or in the
evening.&amp;quot;
U: &amp;quot;In the afternoon&amp;quot;
When the user makes several scenarios in the
same call, the system could start the new
scenario one level higher than the level at the
end of the previous one. This is possible because
when the user has done a complete scenario it
means that the user has answered sucessfully the
system questions once.
For simplicity, we have not made the
confirmation strategies to depend on the user
ability level but it could be possible redefining
the confirmation mechanism in each level.
Considering this user-modeling technique, one
interesting measure to evaluate the adaptability
of the system are the average ability level during
the interaction (see Table 6).
</bodyText>
<sectionHeader confidence="0.994179" genericHeader="method">
7 Field Evaluation
</sectionHeader>
<bodyText confidence="0.999768">
In this evaluation, 30 users called the system for
completing 4 scenarios (120 calls). The
evaluating measures come from the system
annotations (table 6) and from user answers in a
questionnaire (table 7).
</bodyText>
<tableCaption confidence="0.965458">
Table 6 Measures calculated by the system.
</tableCaption>
<table confidence="0.999579777777778">
Measure Value
Call duration (seconds) 204
Number of questions per call 21.2
% of implicit confirmations 61.3
Number of START OVER commands 0.08
Number of CORRECT commands 0.43
Average User-Modeling level along 1.95
the calls
Duration of Negotiation(seconds) 58
</table>
<bodyText confidence="0.999593230769231">
35.4% of the calls asked information of the
two legs in a round-trip and 31.5% completed
the reservation for single or round-trips. As we
can see, average call duration is 204 seconds,
higher than in the operator based service (152
seconds), but similar to other automatic services
(Baggia, 2000). About the recognition rates, we
got more than 95% for small vocabularies (less
than 50 words/expressions: weekdays, period of
the day...). For departure and arrival cities (770-
words vocabulary), we obtained 90.1%
recognition rate for in-vocabulary cities and not
rejected answers. 25% of the wrong cases were
solved with the second candidate and 36% with
the spelled name recogniser. For the remaining
cases, more interactions were necessary.
In these experiments, we got 4.1% out-of-
vocabulary cities, detecting 32% of the cases
with the spelled name recognizer. For the
remaining 68% cases, the user hung up after
several trials.
We asked the user for his/her preference when
obtaining train information: 75.4% of the people
preferred the system, 24.6% preferred web
access and nobody preferred to go to the ticket
office.
</bodyText>
<tableCaption confidence="0.900993">
Table 7. Measures (out of 5) obtained from the
questionnaire.
</tableCaption>
<table confidence="0.700366">
Measure Score
User experience in these kind of 1.8
</table>
<bodyText confidence="0.9266545">
systems.
The system understands what I say. 3.6
I understand what the systems says. 4.5
I get train information fast. 4.0
The system is easy to learn. 3.9
In case of error the correction was 3.1
easy.
The system asks me in a logical order. 4.6
Generally, it is a good system. 3.9
The dialogue point with more problems was
the departing date specification, because we
used isolated speech recognition and it needs
several interactions to get a date. System
intelligibility obtained a score of 4.5 (out of 5)
due to our restricted-domain female-voice
synthesis (Montero, 2000). The dialogue flow
obtained the best score (4.6) because of the
detailed analysis performed.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999805243902439">
In this work, we propose a new methodology for
designing dialogue managers in automatic
telephone-based spoken services. This
methodology has been successfully applied to a
train information system for Spanish. A
combination of several sources of information is
proposed: intuition, observation and simulation
for defining and evaluating several dialogue
strategies, and choosing the best one. The first
steps are database analysis (E-R diagram) and
design by intuition, where a &amp;quot;brain-storming&amp;quot;
over the E-R is performed for proposing
different dialogue alternatives and for defining
an evaluation table. In design by observation,
we evaluate each proposal using user-operator
dialogue transcriptions, without having any
system implemented. The limitation of the
observation step is that human-human
interactions are different from human-system
ones. This problem is solved by the Wizard of
Oz, that simulates the human-system interaction.
In design by iterative improvement, we
describe an approach to incorporate recognition
confidence measures for defining and managing
the confirmation mechanisms. For all
vocabularies, this approach obtained a
recognition rate higher than 90%. Two
mechanisms for error recovering are described:
Start-Over and Correct. User-modelling
techniques are incorporated for adapting the
system dynamically to the user ability.
With this proposed methodology, we have
implemented a fully automatic system with a
good user acceptability: mean call duration was
204 s, similar to (Baggia, 2000). The users
validated the applicability and usability of the
system giving a general score of 3.9 (out of 5).
In a future work, we will apply this
methodology to develop automatic systems with
continuous speech recognition and
understanding modules.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999174">
This work has been partially supported by the
grant 2FD1997-1062-C02 (EU and Spanish
CICYT). Authors want to thank the
contributions of colleagues at GTH and RENFE
railway company.
</bodyText>
<sectionHeader confidence="0.998387" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999875863013699">
Baggia, P., Castagneri, G., Danieli, M., 2000.
&amp;quot;Field trails of the italian ARISE train timetable
system&amp;quot;. Speech Communication. Vol. 31, pp. 356-
368.
Bernsen, N.O. Dykjaer, H. and Daykjaer, L. 1998
&amp;quot;Designing interactive speech systems. From first
ideas to user testing&amp;quot; Springer Verlag.
Lamel, L.F., Bennacef, S.K., Rosset, S.,
Devillers, L., Foukia, S., Gangolf, J.J., Gauvain, J.L.,
1997. &amp;quot;The LIMSI RailTel system: Field trial of a
telephone service for rail travel information&amp;quot;. Speech
Communication. Vol. 23, pp. 67-82.
Lamel, L., Rosset, S., Gauvain, J.L., Bennacef, S.,
Garnier-Rizet, H., Prouts, B., 2000. &amp;quot;The LIMSI
ARISE system&amp;quot;. Speech Communication. Vol. 31, pp
339-355.
Lavelle, A.C., Calmes, H., P6rennov, G.,
1999.&amp;quot;Confirmation strategies to improve correction
rates in a telephonic inquiry dialogue system&amp;quot;. Proc.
of EUROSPEECH, Budapest, Hungary. Vol. 3, pp.
1399-1402.
Macias-Guarasa, J., Ferreiros, J. Colas, J.,
Gallardo, A., and Pardo. JM., 2000a.&amp;quot;Improved
Variable List Preselection List Length Estimation
Using NNs in a Large Vocabulary Telephone Speech
Recognition System&amp;quot;. Proc. of ICSLP, Beijing,
China. Vol. II, pp. 823-826.
Macias-Guarasa, J., Ferreiros, J., San-Segundo,
R., Montero, JM., and Pardo, JM., 2000b.
&amp;quot;Acoustical and Lexical Based Confidence Measures
for a Very Large Vocabulary Telephone Speech
Hypothesis-Verification System&amp;quot;. Proc. of ICSLP.
Beijing, China. Vol. IV, pp. 446-449.
Montero, J.M., C6rdoba, R., Vallejo, J.A.,
Guti6rrez-Arriola, J., Enriquez, E., Pardo, J.M.,
2000.&amp;quot;Restricted-domain female-voice synthesis in
Spanish: from database design to a prosodic
modeling&amp;quot;. Proc. of ICSLP. Beijing, China.
Pellom, B., Ward, W., Pradhan, S., 2000. &amp;quot;The
CU Communicator: An Architecture for Dialogue
Systems&amp;quot;. Proc. of ICSLP, Beijing, China.
Riccardi, G., Gorin, A., 2000. &amp;quot;Stochatic
Language Adaptation over time and state in natural
spoken dialog systems&amp;quot;. IEEE Trans. on Speech and
Audio Processing, Vol 8, pp. 3-10.
Rudnicky, A., Bennet, C., Black, A.,
Chotomongcol, A., Lenzo, K., Oh, A., Singh, R.
2000. &amp;quot;Task and domain specific modelling in the
Carnegie Mellon Communicator system&amp;quot;. Proc. of
ICSLP, Beijing, China.
San-Segundo, R., Colas, J., Ferreiros, J., Macias-
Guarasa, J., Pardo, J.M., 2000. &amp;quot;Spanish Recognizer
of continuously spelled names over the telephone&amp;quot;.
Proc. of ICSLP, Beijing, China.
Schramm, H., Rueber, B., and Kellner, A., 2000.
&amp;quot;Strategies for name recognition in automatic
directory assistance systems&amp;quot;. Speech
Communication. Vol 31, No 4 pp. 329-338.
Sturm, J., den Os, E., and Boves, L., 1999 &amp;quot;Issues
in Spoken Dialogue Systems: Experiences with the
Dutch ARISE System&amp;quot;. Proceedings of ESCA
Workshop on Interactive Dialogue in MultiModal
Systems. Kloter Irsee, Germany, 1-4.
Veldhuijzen van Zanten, G., &amp;quot;User modelling in
adaptive dialogue management&amp;quot;. 1999. Proc. of
EUROSPEECH, Budapest, Hungary. Vol. 3, pp.
1183-1186.
Zue, V., Seneff, S., Glass, J., Hetherington, L.,
Hurley, E., Meng, H., Pao, C., Polifroni, J.,
Schloming, R., Schmid, P., 1997. &amp;quot;From interface to
content: transclingual access and delivery of on-line
information&amp;quot;. Proc. of EUROSPEECH, Athenas,
Greece.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993879">A Telephone-Based Railway Information System for Development of a Methodology for Spoken Dialogue Design</title>
<note confidence="0.348678">R. San-Segundo, J.M. Montero, J.M. Gutierrez, A. Gallardo, J.D. Romeral and J.M. Grupo de Tecnologia del Habla. Departamento de Ingenier�a Electronica. E.T.S.I. Telecomunicacion. Ciudad Universitaria s/n, 28040 Madrid,</note>
<email confidence="0.958873">{lapizuuanchouuanalgallardoudromerallpardo}@die.upm.es</email>
<abstract confidence="0.998377745689655">In this paper, we describe the steps carried out for developing a Railway Information Service for Spanish. This work introduces a methodology for designing dialogue managers in spoken dialogue systems for restricted domains. In this methodology several sources of information are combined: intuition, observation and simulation for defining several dialogue strategies, evaluating them and choosing the best. Users in a final evaluation gave the system a 3.9 score in a 5-point scale with an average call duration of 204 s. The important improvements in Speech Technology have allowed developing automatic systems that can work under real conditions. At this point, telephone-based spoken dialogue systems have appeared as an important field for applying these technologies. Last decade, a great amount of restricted domain systems have been developed: JUPITER (Zue, 1997) that provides weather information; travel information and ticket reservation services like RailTel (Lamel, 1997) and ARISE European projects (Lamel, 2000; Baggia, 2000). Another important spoken dialogue project is the DARPA Communicator (http://fofoca.mitre.org).Some developed in this project are the CU (Pellom, 2000) and the CMU (Rudnicky, 2000) Communicator systems. These services enable natural conversational interaction with telephone callers that access information about airline flights, hotels and rental cars. Other services are the Directory-Assistance systems like (Schramm, 2000) or the automatic call redirection system as implemented in AT&amp;T (Riccardi, 2000). In this paper, we present a methodology for designing the dialogue manager in spoken dialogue system for restricted domains. We use this methodology to develop a train timetable information system for the main Spanish intercity connections. In this paper, we also present an approach for designing the confirmation mechanisms and incorporating user-modelling techniques. for Dialogue Design The methodology proposed in this paper consists of 5 steps. The first step is the database analysis where the information contained in the knowledge database is described by an Entity- Relationship diagram. In the design by intuition, a &amp;quot;brain-storming&amp;quot; over the E-R is performed for proposing different dialogue alternatives. The third step is the design by observation, where we evaluate each proposal using user-operator dialogue transcriptions. The next step we simulate the system with a Wizard of Oz in order to learn the specific characteristics of human-system interactions. The fifth step is the design by iterative improvement where the confirmation strategies are designed in order to implement a fully automatic system. The railway information system developed in this work uses isolated speech recognition, but the methodology is general and it can be used to develop system with continuous speech reconigiton and understanding. This methodology is similar to the Life-Cycle Model presented in (Bernsen, 1998) and (www.disc2.dk),but we incorporate the step &amp;quot;design by observation&amp;quot; where human-human interactions are analysed and we present measures to evaluate the different design alternatives at every step of the methodology. Analysis and Design by Intuition The database, as the initial point in our methodology contains the information or knowledge we want to provide in our service. For example in a railway information system, this database contains the time information for all the trains, their prices, their services, etc...The aim of database analysis is to describe this information, in order to offer the service. This description consists of an Entity- Relationship Diagram (E-R) that shows a semantic representation of the data. In this diagram, the main entity sets, their attributes and keys (attributes that uniquely define an element in an entity set), and entity set relationships must be defined. We must pay attention to the entity sets because they will be the possible goals that the system can provide (parts of service such as timetable information, reservations, fares, etc.). The keys are the main mandatory items to ask the user and they define the dialogue interactions. The E-R diagram is not unique and it depends strongly on the system designer and the service. After the database analysis, we propose a &amp;quot;brain-storming&amp;quot; (design by intuition) over the to propose These proposals are concerned with the goals to provide, the sequence of offering them and the items that are needed to satisfy each goal (pieces of information that must be obtained from the user, such as departure and arrival cities, departing date, etc.). The ways the user can specify each item are also an important issue. For example, for the departure place, the user can say the station name or the city name. In the second case, the system should provide the information for all the train stations in that city. The result of this analysis is a table or work sheet that includes all the alternatives. This table will be used at next step of the methodology to compute the frequency of each alternative concerning with the goals, the information that the system must provide, the ways the user specifies an item, etc. by Observation It aims at analysing user-operator dialogues in a similar service and tracking off the observed events. This design phase evaluates and measures the impact of the alternatives proposed at previous step. 4.1 Goals analysis a) Which goals are most frequently required by the user, and which is the sequence for asking them. The result of this analysis is the number of times that a specific goal is required and its position. In Table 1, the goals analysis for 100 call transcriptions is presented. Several goals can appear in the same call. In our final system, we offer the goals that appear more than 10% calls, except (there is no information about that in the and contains very different goals but none reaches 10%). Table 1: Goals analysis. % Position Timetable 64 57 6 1 - - Round trip Timetable 20 - 14 5 1 - Fares 46 6 30 10 - - Reservations 26 14 4 2 3 3 Train frequency 2 2 - - - - Itinerary 14 8 4 1 1 - Bargains 5 1 2 1 1 - Others 12 8 2 - 2 b) The information given by the operator to each We annotate these data (i.e. departure and arrival times, trip duration, train type, etc.) and their importance (number of times that the operator provides each item). 4.2 Items analysis To satisfy a goal, the system must ask the user some items. These items have to be also analysed at this step: a) Which items are needed to satisfy each goal the sequence for asking b) To classify items as mandatory or optional: • it is an item needed to satisfy the goal and must be asked to the user (e.g. &amp;quot;What date are you departing?&amp;quot;). If the user does not provide this information, it must be assigned a default value (e.g. &amp;quot;today&amp;quot;) or the goals have to be satisfied depending on its value (e.g. for all days in current week). • the system does not ask it to the user. When the user specifies it, the provided information should match this value. c) To classify each item as simple or complex. A complex item can be divided into several simple ones (e.g. &amp;quot;departure date&amp;quot; can be divided into DAY, MONTH and YEAR). With isolated speech recognition, each simple item needs one interaction. A continuous speech recognizer could get several simple items simultaneously. d) To analyse the different ways a user can specify an item value and its importance. Table 2 shows the results for the departure date. A high percentage of people travel on current week and a relevant percentage do not specify any date, because they just want general travel information between two cities. 9.4% of people travelled in the same month and specified the day with the day (e.g. the 4.7% of people travelled on another month and specified the date by the name of the month and the day number. Table 2:Analysis for the Departure Date. Current week month month An y today 25.0% tomorrow 15.6% 9.4% 4.7% 28.1% weekday 17.2% Item ordering and One step in the dialogue is a group of items that can be asked to the user without confirmation. Our recommendations are: • To group items that are contiguous in the item sequence and have a semantic relationship between them. • Not to group more than 4 consecutive items (without leaving simple items alone). Just one confirmation can validate a group of items. We can also incorporate rest zones to provide help or a summary of the interaction. The steps of the dialogue impose a certain rhythm to the user-system interaction. 4.3 Negotiation analysis There is a &amp;quot;negotiation&amp;quot; when the user has to choose one of the possible travel options: a) What information helps the user to take a decision and its importance. Table 3 shows the analysis for negotiation criteria. The number of connections is an important factor, but it does not appear frequently in user-operator dialogues because the operator only offers direct trips without connections. Train Type provides a hint about fares and the services offered in the train, generally known by the user.</abstract>
<note confidence="0.8308946">Table 3: Criteria analysis for negotiation. Criteria % Criteria % Depart. time 41.0 # Connections 2.6 Arrival time 15.4 Connection 2.6 Depart. station 2.6 Class 10.3</note>
<abstract confidence="0.956923590163934">Fares 7.7 Duration 5.0 Train Type 12.8 b) To Choose the negotiation strategy. You can present the best option and let the user ask for the previous/next one, or you can present several alternatives at the same time and ask the user to choose one of them. In this case, it is important to analyse the of alternatives operator gives simultaneously (and the user can manage), during the conversation. In the observation analysis, only human-human interactions are considered, and they can be severely different from human-system ones. Because of this, high level dialogue characteristics can be learnt, but specific behaviours when interacting with an automatic system are not detected (e.g. changes in the user speaking rate, usage of formal vs. colloquial phrases, etc.). On the other hand, this analysis permits to evaluate several alternatives without having implemented any system. by Simulation Now, the specific characteristics of humansystem interactions are analysed by simulating the system with a Wizard of Oz approach. We must focus on the design of the dialogue flow, the questions to ask for obtaining the needed item and the information the system should provide to satisfy each goal. Dialogue alternatives are implemented and evaluated. In this evaluation, 15 users called the system, completed 6 scenarios and fill a questionnaire. The evaluating measures come from system (referred as System)and from user answers in the questionnaire (referred as Questionnaire),where the user answers about subjective aspects difficult to be measured by the system. The user answers are recorded in audio files. The evaluating measures were: 5.1 Goal evaluation a) The measures used to validate the goal sequence and the goal coverage were: • System:number of times a goal is required by the user, time and number of questions (interactions) to satisfy a goal. • Questionnaire:new goals suggested by the user. 5.2 Item evaluation a) To design the questions and recognition • System:number of times the user keeps in silence (does not answer the system question) and, recognition rate for each question (if a recognizer is already available). • Questionnaire:how easy is it to specify an item value. b) For the item sequence evaluation. In some parts of the dialogue, several sequences of questions for asking the items are proposed and randomly selected in each call. The evaluation measures are: • System:when it is possible, the Sequence Recognition Rate (SRR) computed as the product of all independent item recognition rates. Table 4 shows the sequence analysis for the 9departure and arrival cities. The SRR difference is not significant, but we observed the following effect during user calls: when the system asks the Arrival City first, the user assumes that the system knows the Departure City and he/she gets confused when the system asks about it. This explains the great difference between the item recognition rates for the Arrival-Departure City sequence. Table 4: Sequence evaluation for Departure and Arrival cities (item recognition rate and Sequence Recognition Rate). Recognition Rates (%) Sequence item item SRR Depart.-Arrival City 75.6 80.0 60.5 Arrival-Depart. City 94.3 60.0 56.6 • Questionnaire:asking the user for his/her preference. c) Analysis of different ways to split complex items into simple ones. • System:number of interactions and time to get a complex item, and the Sequence Recognition Rate of the simple items. • Questionnaire:how easy is it to specify a complex item with the proposed sequence of interactions. d) How can we manage the mandatory items when the user does not specify any value for them We should parameterise the information by this item values or fix a default value. • Questionnaire:to evaluate the best option, we ask the user in the questionnaire if the information given by the system exceeds his/her expectations (and it is necessary to fix a default value) or it is adequate. 5.3 Negotiation In our case, we have decided to present several travel options at the same time and ask the user to choose one of them. Randomly, the system presents the options one by one, two by two or three by three. For the information provided per option, several patterns are designed and randomly selected for each group of options. The measures considered for evaluation were: • System:number of questions and time for negotiation. • Questionnaire:What information helps the user to choose and number of options he/she can manage at the same time. Users preferred to manage 3 options at the same time (less interactions) but negotiation takes more time. We decided to keep the negotiation in a 3 by 3 basis reducing the information provided per option to Train Type plus Departure and Arrival Times. Table 5: Negotiation analysis in the simulation step. Negotiation analysis (User Questionnaire) 1 by 1 2 by 2 3 by 3 21.4% 21.4% 57.2% Negotiation Criteria Train T Time Arrival Time Fares Duration Type 15.6% 37.5% 25.0% 15.6% 6.2% For global dialogue evaluation, we propose the following measures: • System:average number of interactions and time per call, and recognition rate for all the vocabularies. • Questionnaire:the user has to evaluate in a scale how fast did the user obtain the information, how easy was to learn, which part would you change, and how do you compare the system to other ways of obtaining the service: web, going to the station, etc. Whenever it is possible, it is better to use only the measures computed by the system because they are objective. The questionnaire can not be longer than one sheet and should only address issues that cannot be resolved by automatic system annotations, such as subjective evaluations or default values for mandatory items. If we want to include many questions, it is better to design several questionnaires that evaluate different aspects. Every user should fill only one of the questionnaires proposed. About the scenarios to complete, it is important that the users can define their own scenarios for testing a greater diversity of situations. Some scenarios should be imposed, in order to have enough data to evaluate a specific situation. The Wizard of Oz permits us to analyse different dialogue designs without any system implemented. Moreover, the system does not need to confirm the item values, so we can make independent designs for the dialogue and for the confirmation mechanisms. On the other hand, the main problem in this step is the difficulty for simulating an automatic system, especially the answer time. A person spends more time when selecting the item values than an automatic system. It is necessary to develop tools that help the Wizard to simulate the system in a realistic way. by Iterative Improvement At this step, we implement the first version of a fully automatic system in an iterative modifyand-test process. With the Wizard of Oz, a first version of the dialogue flow was defined and it was necessary to design confirmation mechanisms that take into account the recognition confidence: an analysis of confidence measures is required.</abstract>
<note confidence="0.715660285714286">6.1 Confirmation Strategies Depending on the number of items to confirm: • One item. (&amp;quot;Have you said Madrid?&amp;quot;) • Several items. (&amp;quot;Do you want to go from Madrid to Sevilla?&amp;quot;) Depending on the possibility to correct (Lavelle, 1999):</note>
<abstract confidence="0.993795021428571">Explicit the system confirms one or several item values through a direct question. (&amp;quot;I understood you want to depart from Madrid. Is that correct?&amp;quot;) Implicit the system does not permit the user a correction, it only reports about the recognition result. (&amp;quot;You want to leave from Madrid. Where are you arriving at?&amp;quot;) Semi-implicit it is similar to the implicit confirmation, but the user can correct (&amp;quot;You want to leave from Madrid. In case of error, say correct, otherwise, indicate your arrival city&amp;quot;). No the system does not provides feedback about the recognized value (i.e. in yes/no questions). Item value rejection repeat question: when the confidence is low, the system does not present the value to the user and repeats the question (&amp;quot;Sorry, I can not understand. Where are you departing from?&amp;quot;). 6.2 Confidence Measures in recognition Confidence measures in recognition are vey useful for designing the confirmations (Sturm, 1999). The recognition module used in our system is a large vocabulary telephone speech recognizer, that can recognize isolate words and simple expressions such as &amp;quot;On monday&amp;quot;, &amp;quot;Next week&amp;quot; or &amp;quot;In the morning&amp;quot;. The recognizer 2000a) is based on a hypothesisverification approach. The best features for confidence annotation are concerned with the verification step and are based on (Macias, 2000b): First Candidate Score: score of the best verification candidate. Candidate Score Difference: of score between the and verification candidates. • Candidate Score Mean and Variance: average score and variance over the 10 best candidate names. Score Ratio: between the score of the phone sequence (hypothesis stage) and the score of the best candidate word (verification stage). All the features are divided by the number of frames. We have considered a Multi-Layer Perceptron (MLP) to combine the features in order to obtain a unique confidence value. In this case we use the features directly as inputs to the MLP. With this solution, a preprocessing is required to limit the dynamic range of each measure to the (0,1) interval. Here, the normalization scales the features using the minimum and maximum value obtained for each measure in the training set. The hidden layer contains of 10 units and one output node was used to model the word confidence. During weight estimation, a target value of 1 is assigned when the decoder correctly recognizes the name and a value of 0, when incorrect recognition occurs. The database used in the confidence experiments is built with the results obtained in the recognizer evaluation for a 1,100 name dictionary. In this case we have 2,204 examples, considering 1,450 for training the MLP, 370 as the evaluation set and 370 for testing. We have repeated it six times providing a 6-Round Robin training to verify the results. In this paper we present the average results of these experiments. A 39.1% of wrong recognized words are detected at 5% false rejection rate (Figure 1), reducing the minimum classification error from 15.8% (recognition error rate) to 14.0%. False rejection (%) Figure 1. False acceptance vs False Rejection. 6.3 Confirmation Mechanism design For designing the confirmation mechanism, it is necessary to plot the correct words and errors distributions as a function of the confidence value and to define different confidence thresholds. Figure 2. Bottom detail for Errors and Correct words distributions vs. confidence. We have defined 4 levels (3 thresholds) of confidence (Figure 1): Very High Confidence: number of correctly recognized words is much higher than the number of errors. High Confidence: number of correctly recognized words is higher than the number of errors. Low Confidence: distributions are similar. The system is not sure about the correctness of the recognized word. Very Low Confidence: this case, there are much more errors than correctly recognized words, so we reject the recognized word and the system must ask again. For the departure and arrival cities step, we define CL(D) and CL(A) as departure/arrival city Confidence Levels (CL). Depending on the CL, we implemented the following confirmation strategies: CL(D)=1 and CL(A)=1: of both &amp;quot;You want to travel from Madrid to Sevilla, When do you want to leave?&amp;quot; CL(D)=2 or CL(A)=2: confirmation both &amp;quot;Do you want a Madrid- Sevilla trip?&amp;quot; CL(D)=3 or CL(A)=3: confirmation each &amp;quot;Do you want to travel from Madrid?&amp;quot; CL(D)=4 or CL(A)=4: of each &amp;quot;Sorry, I do not understand. Where are you departing from?&amp;quot; Under hard conditions, the system asks the user to spell the city name (San-Segundo, 2000). In this design, we have not considered the semiimplicit confirmation because it is not very friendly for the user. In Section 5.5, we describe the CORRECT command that permits the same functionality without increasing the prompt length. 6.4 Confirmation Mechanism Evaluation For the iterative improvement, it is necessary to evaluate the confirmation mechanisms in order to adjust them when we changes the dialogue. The proposed measures are: For Implicit the recognition rate obtained for each item when comparing the recorded audio files and the word that recognized as candidate. For Semi-implicit number of times the user wants to correct. If this False acceptance (%)</abstract>
<note confidence="0.887988833333333">40 00 80 60 20 0</note>
<date confidence="0.541781">4 3 2 1</date>
<phone confidence="0.466006">0,05 0,15 0,25 0,35 0,45 0,55 0,65 0,75 0,85 0,95</phone>
<note confidence="0.873186125">Confidence 250 200 150 100 50 0 Correct words</note>
<abstract confidence="0.988086415254237">Errors Errors and Correct distributions number is high, we should change to an explicit confirmation; otherwise, we should use an implicit confirmation strategy. For Explicit number of times the user denies the system proposal. If this number is low we could think on relaxing the confirmation strategy to an implicit one. For all confirmations, we can compute the number of questions and time needed to confirm the item values and the number of times the user deny the system proposal. To analyze the impact of the confirmation mechanisms in the dialogue speed, the system can compute the percentage of implicit vs. explicit confirmations (see Table 6). 6.5 Error recovering mechanisms When an automatic system uses implicit confirmations, it is necessary to define some mechanisms to permit the user to recover from system errors: START OVER: this command permits the user to start from scratch. Instead of resetting all the items, our system begins confirming groups of items explicitly (dialogue steps). When one group is not confirmed, the system starts from that point: S: &amp;quot;The selected option is an Intercity train... &amp;quot; U: &amp;quot;start over&amp;quot; S: &amp;quot;Let us start the call. Do you want to go from Madrid to Barcelona?&amp;quot; U: &amp;quot;yes&amp;quot; &amp;quot;Do you want to travel on of July?&amp;quot; U: &amp;quot;No&amp;quot; S: &amp;quot;Do you want to leave this week, next week or later?&amp;quot; CORRECT: When the system makes a mistake and takes a wrong item value as right (in an implicit confirmation), the user can correct the system by saying this command at any point of the dialogue. In this case, the system asks again the last introduced item. When considering these commands, some aspects must be kept in mind: • The selected commands must be very different from the words contained in the recognition vocabularies, but it must be very friendly to the user. • The sentence used by the system to report to the user the recognition of these commands has to be carefully designed to permit a smooth transition in the dialogue. • It is necessary to specify the dialogue points where the system should report to the user about these possibilities: e.g. in the initial help and in the help provided when the system detects problems in the interaction. When one of these commands is recognized, the confirmation strategies have to be more pessimist about the confidence because one error, recognizing these commands, can produces delays in the interaction. 6.6 User-Modelling The user modelling technique applied is based on (Veldhuijzen van Zanten, 1999) using 4 user skill levels. Depending on current level, the prompt sentences are clearer (they contain more information about how the user should answer, possible values, etc...) or the system provides more or less information per time unit. The levels considered are: • level. The prompts explain to the system, the the values to specify one of the them (for the period of the day: after the Say the the day want to travel in; in the morning, in the afternoon or in the evening.&amp;quot;). • level. The prompts include the the values it and the to specify (&amp;quot;Say the of the want to travel in; the morning, in afternoon or in the • level. Only the required item is included the prompt. (&amp;quot;Say the of the day you want to travel in.&amp;quot;) • level. The user knows everything (the accepted values and the way to specify one of them) and we can relax the question (&amp;quot;When do you want to leave?&amp;quot;). Current level depends on the initial state, the number of errors and positive confirmations along the interaction. In our case, the system at the level (after providing a general explanation about how to interact with the system). When several errors (or positive confirmations) occur, the system decreases (or increases) the level. The number of errors or positive confirmations that forces a change depends on current level. Thus, the system adapts dynamically the interaction to the user skill, making more explicit questions when recognition errors occur. Example: [The system is in the level 3] S: &amp;quot;Say the period of the day you want to travel in.&amp;quot; U: &amp;quot;After lunch&amp;quot; [The system recognizes &amp;quot;in the evening&amp;quot;] S: &amp;quot;Have you said in the evening?&amp;quot; U: &amp;quot;No&amp;quot; [The system decreases the level from 3 to 2] S: &amp;quot;Say the period of the day you want to travel in; in the morning, in the afternoon or in the evening.&amp;quot; U: &amp;quot;In the afternoon&amp;quot; When the user makes several scenarios in the same call, the system could start the new scenario one level higher than the level at the end of the previous one. This is possible because when the user has done a complete scenario it means that the user has answered sucessfully the system questions once. For simplicity, we have not made the confirmation strategies to depend on the user ability level but it could be possible redefining the confirmation mechanism in each level. Considering this user-modeling technique, one interesting measure to evaluate the adaptability of the system are the average ability level during the interaction (see Table 6). Evaluation In this evaluation, 30 users called the system for completing 4 scenarios (120 calls). The evaluating measures come from the system annotations (table 6) and from user answers in a questionnaire (table 7). Table 6 Measures calculated by the system. Measure Value Call duration (seconds) 204 Number of questions per call 21.2 % of implicit confirmations 61.3 Number of START OVER commands 0.08 Number of CORRECT commands 0.43 Average User-Modeling level along the calls 1.95 Duration of Negotiation(seconds) 58 35.4% of the calls asked information of the two legs in a round-trip and 31.5% completed the reservation for single or round-trips. As we can see, average call duration is 204 seconds, higher than in the operator based service (152 seconds), but similar to other automatic services (Baggia, 2000). About the recognition rates, we got more than 95% for small vocabularies (less than 50 words/expressions: weekdays, period of the day...). For departure and arrival cities (770words vocabulary), we obtained 90.1% recognition rate for in-vocabulary cities and not rejected answers. 25% of the wrong cases were solved with the second candidate and 36% with the spelled name recogniser. For the remaining cases, more interactions were necessary. In these experiments, we got 4.1% out-ofvocabulary cities, detecting 32% of the cases with the spelled name recognizer. For the remaining 68% cases, the user hung up after several trials. We asked the user for his/her preference when obtaining train information: 75.4% of the people preferred the system, 24.6% preferred web access and nobody preferred to go to the ticket office. Table 7. Measures (out of 5) obtained from the questionnaire. Measure Score User experience in these kind of systems. 1.8 The system understands what I say. 3.6 I understand what the systems says. 4.5 I get train information fast. 4.0 The system is easy to learn. 3.9 In case of error the correction was easy. 3.1 The system asks me in a logical order. 4.6 Generally, it is a good system. 3.9 The dialogue point with more problems was the departing date specification, because we used isolated speech recognition and it needs several interactions to get a date. System intelligibility obtained a score of 4.5 (out of 5) due to our restricted-domain female-voice synthesis (Montero, 2000). The dialogue flow obtained the best score (4.6) because of the detailed analysis performed. and Future Work In this work, we propose a new methodology for designing dialogue managers in automatic telephone-based spoken services. This methodology has been successfully applied to a train information system for Spanish. A combination of several sources of information is proposed: intuition, observation and simulation for defining and evaluating several dialogue strategies, and choosing the best one. The first steps are database analysis (E-R diagram) and design by intuition, where a &amp;quot;brain-storming&amp;quot; over the E-R is performed for proposing different dialogue alternatives and for defining an evaluation table. In design by observation, we evaluate each proposal using user-operator dialogue transcriptions, without having any system implemented. The limitation of the observation step is that human-human interactions are different from human-system ones. This problem is solved by the Wizard of Oz, that simulates the human-system interaction. In design by iterative improvement, we describe an approach to incorporate recognition confidence measures for defining and managing the confirmation mechanisms. For all vocabularies, this approach obtained a recognition rate higher than 90%. Two mechanisms for error recovering are described: Start-Over and Correct. User-modelling techniques are incorporated for adapting the system dynamically to the user ability. With this proposed methodology, we have implemented a fully automatic system with a good user acceptability: mean call duration was 204 s, similar to (Baggia, 2000). The users validated the applicability and usability of the system giving a general score of 3.9 (out of 5). In a future work, we will apply this methodology to develop automatic systems with continuous speech recognition understanding modules.</abstract>
<note confidence="0.874718689655172">This work has been partially supported by the grant 2FD1997-1062-C02 (EU and Spanish CICYT). Authors want to thank the contributions of colleagues at GTH and RENFE railway company. References Baggia, P., Castagneri, G., Danieli, M., 2000. &amp;quot;Field trails of the italian ARISE train timetable system&amp;quot;. Speech Communication. Vol. 31, pp. 356- 368. Bernsen, N.O. Dykjaer, H. and Daykjaer, L. 1998 &amp;quot;Designing interactive speech systems. From first ideas to user testing&amp;quot; Springer Verlag. Lamel, L.F., Bennacef, S.K., Rosset, S., Devillers, L., Foukia, S., Gangolf, J.J., Gauvain, J.L., 1997. &amp;quot;The LIMSI RailTel system: Field trial of a telephone service for rail travel information&amp;quot;. Speech Communication. Vol. 23, pp. 67-82. Lamel, L., Rosset, S., Gauvain, J.L., Bennacef, S., Garnier-Rizet, H., Prouts, B., 2000. &amp;quot;The LIMSI ARISE system&amp;quot;. Speech Communication. Vol. 31, pp 339-355. Lavelle, A.C., Calmes, H., P6rennov, G., 1999.&amp;quot;Confirmation strategies to improve correction rates in a telephonic inquiry dialogue system&amp;quot;. Proc. of EUROSPEECH, Budapest, Hungary. Vol. 3, pp. 1399-1402. Macias-Guarasa, J., Ferreiros, J. Colas, J., Gallardo, A., and Pardo. JM., 2000a.&amp;quot;Improved</note>
<title confidence="0.952578">Variable List Preselection List Length Estimation Using NNs in a Large Vocabulary Telephone Speech</title>
<affiliation confidence="0.521967">Recognition System&amp;quot;. Proc. of ICSLP, Beijing,</affiliation>
<address confidence="0.515317">China. Vol. II, pp. 823-826. Macias-Guarasa, J., Ferreiros, J., San-Segundo, R., Montero, JM., and Pardo, JM., 2000b.</address>
<title confidence="0.919914">amp;quot;Acoustical and Lexical Based Confidence Measures for a Very Large Vocabulary Telephone Speech</title>
<note confidence="0.915983214285714">Hypothesis-Verification System&amp;quot;. Proc. of ICSLP. Beijing, China. Vol. IV, pp. 446-449. Montero, J.M., C6rdoba, R., Vallejo, J.A., Guti6rrez-Arriola, J., Enriquez, E., Pardo, J.M., 2000.&amp;quot;Restricted-domain female-voice synthesis in Spanish: from database design to a prosodic modeling&amp;quot;. Proc. of ICSLP. Beijing, China. Pellom, B., Ward, W., Pradhan, S., 2000. &amp;quot;The CU Communicator: An Architecture for Dialogue Systems&amp;quot;. Proc. of ICSLP, Beijing, China. Riccardi, G., Gorin, A., 2000. &amp;quot;Stochatic Language Adaptation over time and state in natural spoken dialog systems&amp;quot;. IEEE Trans. on Speech and Audio Processing, Vol 8, pp. 3-10. Rudnicky, A., Bennet, C., Black, A., Chotomongcol, A., Lenzo, K., Oh, A., Singh, R. 2000. &amp;quot;Task and domain specific modelling in the Carnegie Mellon Communicator system&amp;quot;. Proc. of ICSLP, Beijing, China. San-Segundo, R., Colas, J., Ferreiros, J., Macias- Guarasa, J., Pardo, J.M., 2000. &amp;quot;Spanish Recognizer of continuously spelled names over the telephone&amp;quot;. Proc. of ICSLP, Beijing, China. Schramm, H., Rueber, B., and Kellner, A., 2000. &amp;quot;Strategies for name recognition in automatic directory assistance systems&amp;quot;. Communication. Vol 31, No 4 pp. 329-338. J., den Os, E., and Boves, L., 1999 in Spoken Dialogue Systems: Experiences with the ARISE Proceedings of ESCA Workshop on Interactive Dialogue in MultiModal Systems. Kloter Irsee, Germany, 1-4. Veldhuijzen van Zanten, G., &amp;quot;User modelling in adaptive dialogue management&amp;quot;. 1999. Proc. of EUROSPEECH, Budapest, Hungary. Vol. 3, pp. 1183-1186. Zue, V., Seneff, S., Glass, J., Hetherington, L., Hurley, E., Meng, H., Pao, C., Polifroni, J., Schloming, R., Schmid, P., 1997. &amp;quot;From interface to content: transclingual access and delivery of on-line information&amp;quot;. Proc. of EUROSPEECH, Athenas, Greece.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Baggia</author>
<author>G Castagneri</author>
<author>M Danieli</author>
</authors>
<title>Field trails of the italian ARISE train timetable system&amp;quot;.</title>
<date>2000</date>
<journal>Speech Communication.</journal>
<volume>31</volume>
<pages>356--368</pages>
<marker>Baggia, Castagneri, Danieli, 2000</marker>
<rawString>Baggia, P., Castagneri, G., Danieli, M., 2000. &amp;quot;Field trails of the italian ARISE train timetable system&amp;quot;. Speech Communication. Vol. 31, pp. 356-368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N O Dykjaer Bernsen</author>
<author>H</author>
<author>L Daykjaer</author>
</authors>
<title>Designing interactive speech systems. From first ideas to user testing&amp;quot;</title>
<date>1998</date>
<publisher>Springer Verlag.</publisher>
<marker>Bernsen, H, Daykjaer, 1998</marker>
<rawString>Bernsen, N.O. Dykjaer, H. and Daykjaer, L. 1998 &amp;quot;Designing interactive speech systems. From first ideas to user testing&amp;quot; Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L F Lamel</author>
<author>S K Bennacef</author>
<author>S Rosset</author>
<author>L Devillers</author>
<author>S Foukia</author>
<author>J J Gangolf</author>
<author>J L Gauvain</author>
</authors>
<title>The LIMSI RailTel system: Field trial of a telephone service for rail travel information&amp;quot;.</title>
<date>1997</date>
<journal>Speech Communication.</journal>
<volume>23</volume>
<pages>67--82</pages>
<marker>Lamel, Bennacef, Rosset, Devillers, Foukia, Gangolf, Gauvain, 1997</marker>
<rawString>Lamel, L.F., Bennacef, S.K., Rosset, S., Devillers, L., Foukia, S., Gangolf, J.J., Gauvain, J.L., 1997. &amp;quot;The LIMSI RailTel system: Field trial of a telephone service for rail travel information&amp;quot;. Speech Communication. Vol. 23, pp. 67-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
<author>S Rosset</author>
<author>J L Gauvain</author>
<author>S Bennacef</author>
<author>H Garnier-Rizet</author>
<author>B Prouts</author>
</authors>
<title>The LIMSI ARISE system&amp;quot;.</title>
<date>2000</date>
<journal>Speech Communication.</journal>
<volume>31</volume>
<pages>339--355</pages>
<marker>Lamel, Rosset, Gauvain, Bennacef, Garnier-Rizet, Prouts, 2000</marker>
<rawString>Lamel, L., Rosset, S., Gauvain, J.L., Bennacef, S., Garnier-Rizet, H., Prouts, B., 2000. &amp;quot;The LIMSI ARISE system&amp;quot;. Speech Communication. Vol. 31, pp 339-355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Lavelle</author>
<author>H Calmes</author>
<author>G P6rennov</author>
</authors>
<title>strategies to improve correction rates in a telephonic inquiry dialogue system&amp;quot;.</title>
<date>1999</date>
<booktitle>Proc. of EUROSPEECH,</booktitle>
<volume>3</volume>
<pages>1399--1402</pages>
<location>Budapest,</location>
<marker>Lavelle, Calmes, P6rennov, 1999</marker>
<rawString>Lavelle, A.C., Calmes, H., P6rennov, G., 1999.&amp;quot;Confirmation strategies to improve correction rates in a telephonic inquiry dialogue system&amp;quot;. Proc. of EUROSPEECH, Budapest, Hungary. Vol. 3, pp. 1399-1402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JM</author>
</authors>
<title>Variable List Preselection List Length Estimation Using NNs in a Large Vocabulary Telephone Speech Recognition System&amp;quot;.</title>
<date>2000</date>
<booktitle>Proc. of ICSLP,</booktitle>
<pages>823--826</pages>
<location>Beijing, China. Vol. II,</location>
<marker>JM, 2000</marker>
<rawString>Macias-Guarasa, J., Ferreiros, J. Colas, J., Gallardo, A., and Pardo. JM., 2000a.&amp;quot;Improved Variable List Preselection List Length Estimation Using NNs in a Large Vocabulary Telephone Speech Recognition System&amp;quot;. Proc. of ICSLP, Beijing, China. Vol. II, pp. 823-826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Macias-Guarasa</author>
<author>J Ferreiros</author>
<author>R San-Segundo</author>
<author>JM Montero</author>
<author>JM Pardo</author>
</authors>
<title>Acoustical and Lexical Based Confidence Measures for a Very Large Vocabulary Telephone Speech Hypothesis-Verification System&amp;quot;.</title>
<date>2000</date>
<booktitle>Proc. of ICSLP.</booktitle>
<volume>Vol. IV,</volume>
<pages>446--449</pages>
<location>Beijing,</location>
<marker>Macias-Guarasa, Ferreiros, San-Segundo, Montero, Pardo, 2000</marker>
<rawString>Macias-Guarasa, J., Ferreiros, J., San-Segundo, R., Montero, JM., and Pardo, JM., 2000b. &amp;quot;Acoustical and Lexical Based Confidence Measures for a Very Large Vocabulary Telephone Speech Hypothesis-Verification System&amp;quot;. Proc. of ICSLP. Beijing, China. Vol. IV, pp. 446-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Montero</author>
<author>R C6rdoba</author>
<author>J A Vallejo</author>
<author>J Guti6rrez-Arriola</author>
<author>E Enriquez</author>
<author>J M Pardo</author>
</authors>
<title>female-voice synthesis in Spanish: from database design to a prosodic modeling&amp;quot;.</title>
<date>2000</date>
<booktitle>Proc. of ICSLP.</booktitle>
<location>Beijing, China.</location>
<marker>Montero, C6rdoba, Vallejo, Guti6rrez-Arriola, Enriquez, Pardo, 2000</marker>
<rawString>Montero, J.M., C6rdoba, R., Vallejo, J.A., Guti6rrez-Arriola, J., Enriquez, E., Pardo, J.M., 2000.&amp;quot;Restricted-domain female-voice synthesis in Spanish: from database design to a prosodic modeling&amp;quot;. Proc. of ICSLP. Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pellom</author>
<author>W Ward</author>
<author>S Pradhan</author>
</authors>
<title>The CU Communicator: An Architecture for Dialogue Systems&amp;quot;.</title>
<date>2000</date>
<booktitle>Proc. of ICSLP,</booktitle>
<location>Beijing, China.</location>
<marker>Pellom, Ward, Pradhan, 2000</marker>
<rawString>Pellom, B., Ward, W., Pradhan, S., 2000. &amp;quot;The CU Communicator: An Architecture for Dialogue Systems&amp;quot;. Proc. of ICSLP, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>A Gorin</author>
</authors>
<title>Stochatic Language Adaptation over time and state in natural spoken dialog systems&amp;quot;.</title>
<date>2000</date>
<journal>IEEE Trans. on Speech and Audio Processing,</journal>
<volume>8</volume>
<pages>3--10</pages>
<marker>Riccardi, Gorin, 2000</marker>
<rawString>Riccardi, G., Gorin, A., 2000. &amp;quot;Stochatic Language Adaptation over time and state in natural spoken dialog systems&amp;quot;. IEEE Trans. on Speech and Audio Processing, Vol 8, pp. 3-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rudnicky</author>
<author>C Bennet</author>
<author>A Black</author>
<author>A Chotomongcol</author>
<author>K Lenzo</author>
<author>A Oh</author>
<author>R Singh</author>
</authors>
<title>Task and domain specific modelling in the Carnegie Mellon Communicator system&amp;quot;.</title>
<date>2000</date>
<booktitle>Proc. of ICSLP,</booktitle>
<location>Beijing, China.</location>
<marker>Rudnicky, Bennet, Black, Chotomongcol, Lenzo, Oh, Singh, 2000</marker>
<rawString>Rudnicky, A., Bennet, C., Black, A., Chotomongcol, A., Lenzo, K., Oh, A., Singh, R. 2000. &amp;quot;Task and domain specific modelling in the Carnegie Mellon Communicator system&amp;quot;. Proc. of ICSLP, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R San-Segundo</author>
<author>J Colas</author>
<author>J Ferreiros</author>
<author>J MaciasGuarasa</author>
<author>J M Pardo</author>
</authors>
<title>Spanish Recognizer of continuously spelled names over the telephone&amp;quot;.</title>
<date>2000</date>
<booktitle>Proc. of ICSLP,</booktitle>
<location>Beijing, China.</location>
<marker>San-Segundo, Colas, Ferreiros, MaciasGuarasa, Pardo, 2000</marker>
<rawString>San-Segundo, R., Colas, J., Ferreiros, J., MaciasGuarasa, J., Pardo, J.M., 2000. &amp;quot;Spanish Recognizer of continuously spelled names over the telephone&amp;quot;. Proc. of ICSLP, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schramm</author>
<author>B Rueber</author>
<author>A Kellner</author>
</authors>
<title>Strategies for name recognition in automatic directory assistance systems&amp;quot;.</title>
<date>2000</date>
<journal>Speech Communication. Vol</journal>
<volume>31</volume>
<pages>329--338</pages>
<marker>Schramm, Rueber, Kellner, 2000</marker>
<rawString>Schramm, H., Rueber, B., and Kellner, A., 2000. &amp;quot;Strategies for name recognition in automatic directory assistance systems&amp;quot;. Speech Communication. Vol 31, No 4 pp. 329-338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sturm</author>
<author>E den Os</author>
<author>L Boves</author>
</authors>
<title>Issues in Spoken Dialogue Systems: Experiences with the Dutch ARISE System&amp;quot;.</title>
<date>1999</date>
<booktitle>Proceedings of ESCA Workshop on Interactive Dialogue in MultiModal Systems. Kloter Irsee,</booktitle>
<pages>1--4</pages>
<marker>Sturm, den Os, Boves, 1999</marker>
<rawString>Sturm, J., den Os, E., and Boves, L., 1999 &amp;quot;Issues in Spoken Dialogue Systems: Experiences with the Dutch ARISE System&amp;quot;. Proceedings of ESCA Workshop on Interactive Dialogue in MultiModal Systems. Kloter Irsee, Germany, 1-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veldhuijzen van Zanten</author>
<author>G</author>
</authors>
<title>User modelling in adaptive dialogue management&amp;quot;.</title>
<date>1999</date>
<booktitle>Proc. of EUROSPEECH, Budapest, Hungary.</booktitle>
<volume>3</volume>
<pages>1183--1186</pages>
<marker>van Zanten, G, 1999</marker>
<rawString>Veldhuijzen van Zanten, G., &amp;quot;User modelling in adaptive dialogue management&amp;quot;. 1999. Proc. of EUROSPEECH, Budapest, Hungary. Vol. 3, pp. 1183-1186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Zue</author>
<author>S Seneff</author>
<author>J Glass</author>
<author>L Hetherington</author>
<author>E Hurley</author>
<author>H Meng</author>
<author>C Pao</author>
<author>J Polifroni</author>
<author>R Schloming</author>
<author>P Schmid</author>
</authors>
<title>From interface to content: transclingual access and delivery of on-line information&amp;quot;.</title>
<date>1997</date>
<booktitle>Proc. of EUROSPEECH, Athenas,</booktitle>
<marker>Zue, Seneff, Glass, Hetherington, Hurley, Meng, Pao, Polifroni, Schloming, Schmid, 1997</marker>
<rawString>Zue, V., Seneff, S., Glass, J., Hetherington, L., Hurley, E., Meng, H., Pao, C., Polifroni, J., Schloming, R., Schmid, P., 1997. &amp;quot;From interface to content: transclingual access and delivery of on-line information&amp;quot;. Proc. of EUROSPEECH, Athenas, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>