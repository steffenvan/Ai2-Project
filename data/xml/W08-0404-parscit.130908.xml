<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016363">
<title confidence="0.993244">
Generalizing local translation models
</title>
<author confidence="0.995994">
Michael Subotin
</author>
<affiliation confidence="0.997843666666667">
Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland
</affiliation>
<address confidence="0.972196">
College Park, MD 20742
</address>
<email confidence="0.999704">
msubotin@umiacs.umd.edu
</email>
<sectionHeader confidence="0.998532" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.987066826086956">
We investigate translation modeling based on
exponential estimates which generalize essen-
tial components of standard translation mod-
els. In application to a hierarchical phrase-
based system the simplest generalization al-
lows its models of lexical selection and re-
ordering to be conditioned on arbitrary at-
tributes of the source sentence and its anno-
tation. Viewing these estimates as approxi-
mations of sentence-level probabilities moti-
vates further elaborations that seek to exploit
general syntactic and morphological patterns.
Dimensionality control with Bl regularizers
makes it possible to negotiate the tradeoff be-
tween translation quality and decoding speed.
Putting together and extending several recent
advances in phrase-based translation we ar-
rive at a flexible modeling framework that al-
lows efficient leveraging of monolingual re-
sources and tools. Experiments with features
derived from the output of Chinese and Arabic
parsers and an Arabic lemmatizer show signif-
icant improvements over a strong baseline.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999784444444444">
Effective handling of large and diverse inventories
of feature functions is one of the most pressing
open problems in machine translation. While min-
imum error training (Och, 2003) has by now be-
come a standard tool for interpolating a small num-
ber of aggregate scores, it is not well suited for
learning in high-dimensional feature spaces. At the
same time, although recent years have seen consid-
erable progress in development of general methods
</bodyText>
<page confidence="0.977138">
28
</page>
<bodyText confidence="0.999978205882353">
for large-scale prediction of complex outputs (Bakır
et al., 2007), their application to language transla-
tion has presented considerable challenges. Sev-
eral studies have shown that large-margin methods
can be adapted to the special complexities of the
task (Liang et al., 2006; Tillmann and Zhang, 2006;
Cowan et al., 2006) . However, the capacity of these
algorithms to improve over state-of-the-art baselines
is currently limited by their lack of robust dimen-
sionality reduction. Performance gains are closely
tied to the number and variety of candidate features
that enter into the model, and increasing the size of
the feature space not only slows down training in
terms of the number of iterations required for con-
vergence, but can also considerably reduce decod-
ing speed, leading to run-time costs that may be un-
acceptable in industrial settings. Vector space re-
gression has shown impressive performance in other
tasks involving string-to-string mappings (Cortes et
al., 2007), but its application to language transla-
tion presents a different set of open problems (Wang
et al., 2007). Other promising formalisms, which
have not yet produced end-to-end systems compet-
itive with standard baselines, include the approach
due to Turian et al (2006), the hidden-state syn-
chronous grammar-based exponential model studied
by Blunsom et al (2008), and a similar model in-
corporating target-side n-gram features proposed in
Subotin (2008).
Taken together the results of these studies point
to a striking overarching conclusion: the humble
relative frequency estimate of phrase-based mod-
els makes for a surprisingly strong baseline. The
present paper investigates a family of models that
</bodyText>
<note confidence="0.9760675">
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 28–36,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999936770833334">
capitalize on this practical insight to allow efficient
optimization of weights for a virtually unlimited
number of features. We take as a point of depar-
ture the observation that the essential translation
model scores comprising standard decoding deci-
sion rules can be recovered as special cases of a
more general family of models. As we discuss be-
low, they are equal to maximum likelihood solu-
tions for locally normalized ”piecewise” approxi-
mations to sentence-level probabilities, where word
alignment is used to determine the subset of fea-
tures observed in each training example. The cases
for which such solutions have a closed form corre-
spond to particular restrictions placed on the feature
space. Thus, relative frequency phrase models can
be obtained by limiting the feature space to indica-
tor functions for the phrase pairs consistent with an
alignment. By removing unnecessary restrictions we
restore the full flexibility of local exponential mod-
els, including their ability to use features depending
on arbitrary aspects of the source sentence and its
annotation. The availability of robust algorithms for
dimensionality reduction with Ei regularizers (Ng,
2004) means that we can start with a virtually un-
limited number of candidate features and negotiate
the tradeoff between translation quality and decod-
ing speed in a way appropriate for a given setting.
A further attractive property of locally normalized
models is the modest computational cost of their
training and ease of its parallelization. This is par-
ticularly so for the models we concentrate on in this
paper, defined so that parameter estimation decom-
poses into a large number of small optimization sub-
problems which can be solved independently.
Several variants of these models beyond rela-
tive frequencies have appeared in the literature be-
fore. Maximum entropy estimation for transla-
tion of individual words dates back to Berger et
al (1996), and the idea of using multi-class classi-
fiers to sharpen predictions normally made through
relative frequency estimates has been recently rein-
troduced under the rubric of word sense disambigua-
tion and generalized to substrings (Chan et al 2007;
Carpuat and Wu 2007a; Carpuat and Wu 2007b).
Maximum entropy models for non-lexicalized re-
ordering rules for a phrase-based system with CKY
decoding has been described by Xiong et al (2006).
Some of our experiments, where exponential models
conditioned on the source sentence and its parse an-
notation are associated with all rewrite rules in a hi-
erarchical phrase-based system (Chiang, 2007) and
all word-level probabilities in standard lexical mod-
els, may be seen as a synthesis of these ideas.
The broader perspective of viewing the product of
such local probabilities as a particular approxima-
tion of sentence-level likelihood points the way be-
yond multi-class classification, and this type of gen-
eralization is the main original contribution of the
present work. Training a classifier to predict the tar-
get phrase for every source phrase is equivalent to
conjoining all contextual features of the model with
an indicator function for the surface form of some
rule in the grammar. We can also use features based
on less specific representation of a rule. Of par-
ticular importance for machine translations are rep-
resentations which generalize reordering informa-
tion beyond identity of individual words – a type of
generalization that presents a challenge in hierarchi-
cal phrase-based translation. With generalized local
models this can be accomplished by adding features
tracking only ordering patterns of rules. We exper-
iment with a case of such models which allows us
to preserve decomposition of parameter estimation
into independent subproblems.
Besides varying the structure of the feature space,
we can also extend the range of normalization for
the exponential models beyond target phrases co-
occurring with a given source phrase in the phrase
table. This choice is especially natural for richly in-
flected languages, since it enables us to model mul-
tiple levels of morphological representation at once
and estimate probabilities for rules whose surface
forms have not been observed in training. We apply
a simple variant of this approach to Arabic-English
lexical models.
Experimental results across eight test sets in two
language pairs support the intuition that features
conjoined with indicator functions for surface forms
of rules yield higher gains for test sets with better
coverage in training data, while features based on
less specific representations become more useful for
test sets with lower baselines.
The types of features explored in this paper rep-
resent only a small portion of available options, and
much practical experimentation remains to be done,
particularly in order to find the most effective ex-
</bodyText>
<page confidence="0.99759">
29
</page>
<bodyText confidence="0.999795888888889">
tensions of the feature space beyond multiclass clas-
sification. However, the results reported here show
considerable promise and we believe that the flexi-
bility of these models combined with their computa-
tional efficiency makes them potentially valuable as
an extension for a variety of systems using transla-
tion models with local conditional probabilities and
as a feature selection method for globally trained
models.
</bodyText>
<sectionHeader confidence="0.804373" genericHeader="method">
2 Hierarchical phrase-based translation
</sectionHeader>
<bodyText confidence="0.9974948">
We take as our starting point David Chiang’s Hiero
system, which generalizes phrase-based translation
to substrings with gaps (Chiang, 2007). Consider
for instance the following set of context-free rules
with a single non-terminal symbol:
</bodyText>
<equation confidence="0.929943166666667">
(A, A) (A1 A2 , A1 A2 )
(A, A) ( d&apos; A1 id´ees A2 , A1 A2 ideas)
(A, A) (incolores , colorless)
(A, A) ( vertes , green)
(A, A) (dormentA, sleepA)
(A, A) (furieusement , furiously)
</equation>
<bodyText confidence="0.97729948">
It is one of many rule sets that would suffice to
generate the English translation 1b for the French
sentence 1a.
1a. d’ incolores idees vertes dorment furieusement
1b. colorless green ideas sleep furiously
As shown by Chiang (2007), a weighted gram-
mar of this form can be collected and scored by
simple extensions of standard methods for phrase-
based translation and efficiently combined with a
language model in a CKY decoder to achieve large
improvements over a state-of-the-art phrase-based
system. The translation is chosen to be the target-
side yield of the highest-scoring synchronous parse
consistent with the source sentence. Although a va-
riety of scores interpolated into the decision rule for
phrase-based systems have been investigated over
the years, only a handful have been discovered to be
consistently useful, as is in our experience also the
case for the hierarchical variant. Setting aside spe-
cialized components such as number translators, we
concentrate on the essential sub-models1 comprising
1To avoid confusion with features of the exponential models
described below we shall use the term ”model” for the terms
the translation model: the phrase models and lexical
models.
</bodyText>
<sectionHeader confidence="0.946082" genericHeader="method">
3 Local exponential translation models
</sectionHeader>
<subsectionHeader confidence="0.993874">
3.1 Relative frequency solutions
</subsectionHeader>
<bodyText confidence="0.9999585">
Standard phrase models associate conditional proba-
bilities with subparts of translation hypotheses, usu-
ally computed as relative frequencies of counts of
extracted phrases.2 Let ry be the target side of a rule
and rx its source side. The weight of the rule in the
”reverse” phrase model would then be computed as
</bodyText>
<equation confidence="0.936253666666667">
ry r, _ count((rx, ry))
1
p Er,y&apos; count((rx, ry )) ( )
</equation>
<bodyText confidence="0.9988162">
When used to score a translation hypothesis cor-
responding to some synchronous parse tree T, the
phrase model may be conceived as an approxima-
tion of the probability of a target sentence Y given a
source sentence X
</bodyText>
<equation confidence="0.9861285">
p(Y |X) ^ 11 p(ry|rx) (2)
rET
</equation>
<bodyText confidence="0.9973102">
Although there is nothing in current learning the-
ory that would prompt one to expect that expressions
of this form should be effective, their surprisingly
strong performance in machine translation in an em-
pirical observation borne out by many studies. In
order to build on this practical insight it is useful to
gain a clearer understanding of their formal proper-
ties.
We start by writing out an expression for the like-
lihood of training data which would give rise to max-
imum likelihood solutions like those in eq. 1. Con-
sider a feature vector whose components are indi-
cator functions for rules in the grammar, and let
us define an exponential model for a sentence pair
(Xm, Ym) of the form
</bodyText>
<equation confidence="0.993203">
p (Ym|Xm) ^ 11 p(ry|rx) (3)
rE(X,,Y,)
exp{w · fr(Xm,Ym)}
E(4)
r:rx=rx exp{w · fr(Xm,Ym)}
</equation>
<bodyText confidence="0.93379725">
interpolated using MERT.
2Chiang (2007) uses a heuristic estimate of fractional counts
in these computations. For completeness we report both vari-
ants in the experiments.
</bodyText>
<equation confidence="0.910916">
11 =
rE(X,,Y,)
</equation>
<page confidence="0.942905">
30
</page>
<bodyText confidence="0.999982777777778">
where fr(X,,t, Y,,t) is a restriction of the feature
vector such that all of its entries except for the one
corresponding to the rule r are zero and the summa-
tion is over all rules in the grammar with the same
source side. As can be verified by writing out the
likelihood for the training set and setting its gradi-
ent to zero, maximum likelihood estimation based
on eq. 4 yields estimates equal to relative frequency
solutions. In fact, because its normalization fac-
tors have non-zero parameters in common only for
rules which share the same source phrase, param-
eter estimation decomposes into independent opti-
mization subproblems, one for each source phrase
in the grammar. However, recovering relative fre-
quencies of the needed form requires further atten-
tion to the relationship between the definition of fea-
ture functions and phrase extraction. Computation
of phrase models in machine translation crucially re-
lies on a form of feature selection not widely known
in other contexts. A rule is considered to be ob-
served in a sentence pair only if it is consistent with
predictions of a word alignment model according to
heuristics for alignment combination and phrase ex-
traction. The standard recipes in translation model-
ing can thus be seen to include a feature selection
procedure that applies individually to each training
example.
</bodyText>
<subsectionHeader confidence="0.999021">
3.2 Classifier solutions
</subsectionHeader>
<bodyText confidence="0.999946686567164">
We can now generalize these relative frequency
estimates by relaxing the restrictions they implic-
itly place on the form of permissible feature func-
tions. The simplest elaboration involves allow-
ing indicator functions for rules to be conjoined
with indicator functions for arbitrary attributes of
the source sentence or its annotation. This pre-
serves a decomposition of parameter estimation of
optimization subproblems associated with individ-
ual source phrase, but effectively replaces proba-
bilities p(ry|r&apos;) in eqs. 2 and 3 with probabili-
ties conditioned on the source phrase together with
some of its source-side context. We may, for ex-
ample, conjoin an indicator function for the rule
(A, A) —* ( d� A1 idees A2 , A1 A2 ideas) with a
function telling us whether a part-of-speech tagger
has identified the word at the left edge of the source-
side gap A2 as an adjective, which would provide
additional evidence for the target side of this rule.
Combining a grammar-based formalism with con-
textual features raises a subtle question of whether
rules which have gaps at the edges and can match at
multiple positions of a training example should be
counted as having occurred together with their re-
spective contextual features once for each possible
match. To avoid favoring monotone rules, which
tend to match at many positions, over reordering
rules, which tend to match at a single span, we ran-
domly sample only one of such multiple matches for
training.
Unlike conventional phrase models, contextually-
conditioned probabilities cannot be stored in a pre-
computed phrase table. Instead, we store informa-
tion about features and their weights and compute
the normalization factors at run-time at the point
when they are first needed by the decoder.
At the expense of more complicated decoding
procedures we could also apply the same line of
reasoning to generalize the ”noisy channel” phrase
model p(rflry) to be conditioned on local target-
side context in a translation hypothesis, possibly
combining target-side annotation of the training set
with surface form of rules. We do not pursue this
elaboration in part because we are skeptical about its
potential for success. The current state of machine
translation rarely permits constructing well-formed
translations, so that most of the contextual features
on the target side would be rarely if at all observed
in the training data, resulting in sparse and noisy es-
timates. Furthermore, we have yet to find a case
where relative frequency estimates p(rflry) make a
useful contribution to the system when contextually-
conditioned ”reverse” probabilities are used, sug-
gesting that viewing translation modeling as approx-
imating sentence-level probabilities p(Y |X) may be
a more fruitful avenue in the long term.
For translation with phrases without gaps classi-
fier solutions of eq. 4 are equivalent to a maximum
entropy variant of the phrase sense disambigua-
tion approach studied by Carpuat &amp; Wu (2007b).
These solutions are also closely related to the ap-
proximation known as piecewise training in graph-
ical model literature (Sutton and McCallum, 2005;
Sutton and Minka, 2006) and independently stated
in a more general form by P´erez-Cruz et al (2007).
Aside from formal differences between feature tem-
plates defined by graphical models and grammars,
</bodyText>
<page confidence="0.999443">
31
</page>
<bodyText confidence="0.999974636363636">
which are beyond the scope of our discussion, there
are several further contrasts between these studies
and standard practice in machine translation in how
the learned parameters are used to make predic-
tions. Unlike inference in piecewise-trained graphi-
cal models, where all parameters for a given output
are added together without normalization, features
that enter into the score for a translation hypothe-
sis are restricted to be consistent with a single syn-
chronous parse and the local probabilities are nor-
malized in decoding as in training.
</bodyText>
<subsectionHeader confidence="0.999153">
3.3 Lexical models
</subsectionHeader>
<bodyText confidence="0.999864571428571">
The use of conditional probabilities in standard lex-
ical models also gives us a straightforward way to
generalize them in the same way as phrase models.
Consider the lexical model pw(ry|rx), defined fol-
lowing Koehn et al (2003), with a denoting the most
frequent word alignment observed for the rule in the
training set.
</bodyText>
<equation confidence="0.971776333333333">
1
|j|(i,j) E a |(iYap(wyi |wxj )
(5)
</equation>
<bodyText confidence="0.999520307692308">
We replace p(wyi |wxj ) with context-conditioned
probabilities, computed similarly to eq. 4, but at the
level of individual words. Our experience suggests
that, unlike the analogous phrase model, the stan-
dard lexical model pw(rx|ry) is not made redundant
by this elaboration, and we use its baseline variant
in all our experiments. While this approach seeks to
make the most of practical insights underlying state-
of-the-art baselines, it is of course not the only way
to combine rule-based and word-based features. See
for example Sutton &amp; Minka (2006) for a discussion
of alternatives that are closer in spirit to the idea of
approximating global probabilities.
</bodyText>
<subsectionHeader confidence="0.963339">
3.4 Further generalizations
</subsectionHeader>
<bodyText confidence="0.999992357142858">
An immediate practical benefit of interpreting rela-
tive frequency and classifier estimates of translation
models as special cases is the possibility of gener-
alizing them further by introducing additional fea-
tures based on less specific representations of rules
and words.
Among the least specific and most potentially use-
ful representations of hierarchical phrases are those
limited to the patterns formed by gaps and words,
allowing the model to generalize reordering infor-
mation beyond individual tokens. We study two
types of ordering patterns. For rules with two gaps
we form features by conjoining contextual indicator
functions with functions indicating whether the gap
pattern is monotone or inverting. We also use an-
other type of ordering features, representing the pat-
tern formed by gaps and contiguous subsequences
of words. For example, the rule with the right-hand
side ( d&apos; A1 idees A2 , A1 A2 ideas) might be asso-
ciated with the pattern ( a A1 a A2 , A1 A2 a). Be-
cause some source-side patterns of this type apply
to many different rules it is no longer possible to de-
compose parameter estimation into small indepen-
dent optimization subproblems. For practical conve-
nience we enforce decomposition in the experiments
reported below in the following way. We define indi-
cator functions for sequences of closed-class words
and the most frequent part-of-speech tag for open-
class words on the source side. For the rule above
and a simple tag-set the pattern tracked by such an
indicator function would be d&apos; A1 N A2 . We require
all reordering features to be conjoined with an indi-
cator function of this type, ensuring that each cor-
responds to a separate optimization subproblem. We
further split larger optimization subproblems, so that
parameters for identical reordering features are in
some cases estimated separately for different subsets
of rules.
Morphological inflection provides motivation for
another class of features not bound to surface repre-
sentations. In this paper we explore a particularly
simple example of this approach, adding features
conjoined with indicator functions for Arabic lem-
mas to the lexical models in Arabic-English trans-
lation. This preserves decomposition of parameter
estimation, with subproblems now associated with
individual lemmas rather than words. Lemma-based
features suggest another extension of the modeling
framework. Instead of computing the sums in nor-
malization factors over all English words aligned to
a given Arabic token in the training data, we let the
sum range over all English words aligned to Arabic
words sharing its lemma. This also defines probabil-
ities for Arabic words whose surface forms have not
been observed in training, although we do not take
advantage of estimates for out-of-vocabulary words
</bodyText>
<equation confidence="0.985719">
n
pw(ry|rx) _
i�1
</equation>
<page confidence="0.979024">
32
</page>
<bodyText confidence="0.986956">
in the experiments below.
</bodyText>
<subsectionHeader confidence="0.912652">
3.5 Regularization
</subsectionHeader>
<bodyText confidence="0.999944666666667">
We apply Ei regularization (Ng, 2004; Gao et al.,
2007) to make learning more robust to noise and
control the effective dimensionality of the feature
space by subtracting a weighted sum of absolute val-
ues of parameter weights from the log-likelihood of
the training data
</bodyText>
<equation confidence="0.991211333333333">
w* = arg max LL(w) −
W � Ci|wi |(6)
i
</equation>
<bodyText confidence="0.999948333333334">
We optimize the objective using a variant of the
orthant-wise limited-memory quasi-Newton algo-
rithm proposed by Andrew &amp; Gao (2007).3 All val-
ues Ci are set to 1 in most of the experiments below,
although we apply stronger regularization (Ci = 3)
to reordering features. Tuning regularization trade-
offs individually for different feature types is an at-
tractive option, but our experiments suggest that us-
ing cross-entropy on a held-out portion of training
data for that purpose does not help performance.
We leave investigation of the alternatives for future
work.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998504">
4.1 Data and methods
</subsectionHeader>
<bodyText confidence="0.999846666666667">
We apply the models to Arabic-English and
Chinese-English translation, with training sets con-
sisting of 108,268 and 1,017,930 sentence pairs, re-
spectively.4 All conditions use word alignments
produced by sequential iterations of IBM model 1,
HMM, and IBM model 4 in GIZA++ , followed
</bodyText>
<subsectionHeader confidence="0.571558">
3Our implementation of the algo-
</subsectionHeader>
<bodyText confidence="0.6057935">
rithm as a SciPy routine is available at
http://www.umiacs.umd.edu/∼msubotin/owlqn.py
</bodyText>
<footnote confidence="0.94477625">
4The Arabic-English data came from Arabic News Transla-
tion Text Part 1 (LDC2004T17), Arabic English Parallel News
Text (LDC2004T18), and Arabic Treebank English Translation
(LDC2005E46). Chinese-English data came from Xinhua Chi-
nese English Parallel News Text Version 1 beta (LDC2002E18),
Chinese Treebank English Parallel Corpus (LDC2003E07),
Chinese English News Magazine Parallel Text (LDC2005T10),
FBIS Multilanguage Texts (LDC2003E14), Chinese News
Translation Text Part 1 (LDC2005T06), and the HKNews por-
tion of Hong Kong Parallel Text (LDC2004T08). Some sen-
tence pairs were not included in the training sets due to large
length discrepancies.
</footnote>
<bodyText confidence="0.999904375">
by ”diag-and” symmetrization (Koehn et al., 2003).
Thresholds for phrase extraction and decoder prun-
ing were set to values typical for the baseline sys-
tem (Chiang, 2007). Unaligned words at the outer
edges of rules or gaps were disallowed. A trigram
language model with modified interpolated Kneser-
Ney smoothing (Chen and Goodman, 1998) was
trained by the SRILM toolkit on the Xinhua por-
tion of the Gigaword corpus and the English side of
the parallel training set. Evaluation was based on
the BLEU score with 95% bootstrap confidence in-
tervals for the score and difference between scores,
calculated by scripts in version 11a of the NIST dis-
tribution. The 2002 NIST MT evaluation sets was
used for development. The 2003, 2004, 2005, and
2006 sets were used for testing.
The decision rule was based on the standard log-
linear interpolation of several models, with weights
tuned by MERT on the development set (Och, 2003).
The baseline consisted of the language model, two
phrase translation models, two lexical models, and
a brevity penalty. In the runs where generalized ex-
ponential models were used they replaced both of
the baseline phrase translation models. The feature
set used for exponential phrase models in the exper-
iments included all the rules in the grammar and all
aligned word pairs for lexical models. Elementary
contextual features were based on Viterbi parses ob-
tained from the Stanford parser. Word features in-
cluded identities of word unigrams and bigrams ad-
jacent to a given rule, possibly including rule words.
Part-of-speech features included similar ngrams up
to the length of 3 and the tags for rule tokens. These
features were collected for training by a straightfor-
ward extension of rule extraction algorithms imple-
mented in the baseline system for each possible lo-
cation of ngrams with respect to the rule: namely, at
the outer edges of the rule and at the edges of any
gaps that it has. Our models also include a subset
of contextual features formed by pairwise combina-
tions of these elementary features. A final type of
contextual features in these experiments was the se-
quence of the highest nodes in the parse tree that fill
the span of the rule and the sequences that fill its
gaps. We used an in-house Arabic tokenizer based
on a Java implementation of Buckwalter’s morpho-
logical analyzer and incorporating simple statistics
from the Penn Arabic treebank, also extending it to
</bodyText>
<page confidence="0.997986">
33
</page>
<bodyText confidence="0.999840058823529">
perform lemmatization.
The total number of candidate features thus de-
fined is very large, and we use a number of sim-
ple heuristics to reduce it prior to training. They
are not essential to the estimates and were chosen
so that the models could be trained in a few hours
on a small cluster. With the exception of discarding
all except the 10 most frequent target phrases ob-
served with each source phrase,5 which benefits per-
formance, we expect that relaxing these restrictions
would improve the score. These limitations included
count-based thresholds on the frequency of contex-
tual features included into the model, the frequency
of rules and reordering patterns conjoined with other
features, and the size of optimization subproblems to
which contextual features are added. We don’t con-
join contextual features to rules whose source phrase
terminals are all punctuation symbols. For subprob-
lems of size exceeding a certain threshold, we train
on a subsample of available training instances. For
the Chinese-English task we do not add reorder-
ing features to problems with low-entropy distribu-
tions of inversion and reordering patterns and dis-
card rules with two non-terminals altogether if the
entropy of their reordering patterns falls under a
threshold. None of these restrictions were applied to
the baselines. Finally, we solve only those optimiza-
tion subproblems which include parameters needed
in the development and training sets. This leads to a
reduction of costs that is similar to phrase table fil-
tering and likewise does not affect the solution. At
decoding time all features for the translation models
and their weights are accessed from a disk-mapped
trie.
</bodyText>
<subsectionHeader confidence="0.964738">
4.2 Results and discussion
</subsectionHeader>
<bodyText confidence="0.999968222222222">
The results are shown in tables 1 and 2. For both lan-
guage pairs we had a choice between using a base-
line that is computed in the same way as the other ex-
ponential models, with the exception of its use of rel-
ative frequency estimates and a baseline that incor-
porates averaged fractional counts for phrase mod-
els and lexical models, as used by Chiang (2007).
For the sake of completeness we report both (though
without performing statistical comparisons between
</bodyText>
<footnote confidence="0.970208333333333">
5This has prompted us to add an additional target-side token
to lexical models, which subsumes the discarded items under a
single category.
</footnote>
<table confidence="0.999875833333333">
Condition MT03 MT04 MT05 MT06
Rel. freq. 48.24 43.92 47.53 37.94
Frac. 48.34 45.68 47.95 39.41
Context 49.47* 45.65 48.76 39.49
+lex 50.42* 46.07* 49.66* 39.32
+lex+lemma 49.86* 47.02* 49.29* 40.81*
</table>
<tableCaption confidence="0.7594701">
Table 1: Arabic-English translation, BLEU scores on
testing. Conditions include two baselines: simple rela-
tive frequency (rel. freq.) and fractional estimates (frac.).
Experimental conditions: contextual features in phrase
models (context); same and contextual features in lexi-
cal models (+lex); same and lemma based features in lex-
ical models (+lex+lemma). Stars mark statistically sig-
nificant improvements over the fractional baseline which
produced a higher score on the dev-test MT02 set than
the other baseline (59.75 vs. 59.66).
</tableCaption>
<table confidence="0.998901">
Condition MT03 MT04 MT05 MT06
Rel. freq. 32.62 27.53 30.50 22.78
Frac. 32.56 27.98 30.42 23.16
Context 33.16* 28.35* 31.52* 23.67*
+lex 33.50* 28.14* 31.98* 23.05
+lex+reord 33.12* 28.27* 31.73* 23.45*
</table>
<tableCaption confidence="0.9544045">
Table 2: Chinese-English translation, BLEU scores on
testing. Conditions include two baselines: simple rela-
</tableCaption>
<bodyText confidence="0.940007">
tive frequency (rel. freq.) and fractional estimates (frac.).
Experimental conditions: contextual features in phrase
models (context); same and contextual features in lexi-
cal models (+lex); same and reordering features in phrase
models (+lex+reord). Stars mark statistically significant
improvements over the simple relative frequency baseline
which produced a higher score on the dev-test MT02 set
than the other baseline (33.62 vs. 33.53).
</bodyText>
<page confidence="0.996548">
34
</page>
<bodyText confidence="0.999937666666667">
them). Statistical tests for experimental conditions
were performed in comparison to the baseline which
achieved higher score on the test-dev MT02 set: the
fractional count baseline for Arabic-English and the
simple relative count baseline for Chinese-English.
We test models with classifier solutions for phrase
models alone and for phrase models together with
lexical models in both language pairs. For Arabic-
English translation we also experiment with adding
features based on lemmas to lexical models, while
for Chinese-English we add ”reordering” features –
features based on the ordering pattern of gaps for
rules with two gaps and features based on ordering
of gaps and words for rules with a single gap.
For both language pairs the results show con-
sistent distinctions in behavior of different mod-
els between the test sets giving rise to generally
higher scores (MT03 and MT05) and generally
lower scores (MT04 and MT06). The fractional
counts seem to be consistently more helpful for
test sets with poorer coverage, although the rea-
son for this is not immediately clear. For exponen-
tial models the two type of sets present two pos-
sible sources of difference. The lower-performing
sets have poorer coverage in the training data, and
they also may suffer from lower-quality annotation,
since the training sets for both the translation mod-
els and the annotation tools are dominated by text
in the same, newswire domain. Overall, the use of
features based on surface forms is more beneficial
for MT03 and MT05. Indeed, using lexical models
with contextual features in addition to phrase models
hurts performance on MT06 for Arabic-English and
on both MT04 and MT06 for Chinese-English. In
contrast, using features based on less specific repre-
sentations is more beneficial on test sets with poorer
coverage, while hurting performance on MT03 and
MT05. This agrees with our intuitions and also sug-
gests that the differences in coverage of training data
for the translation models may be playing a more
important role in these trends than coverage for an-
notation tools.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985833333334">
We have outlined a framework for translation
modeling that synthesizes several recent advances
in phrase-based machine translation and suggests
many other ways to leverage sub-token representa-
tions of words as well as syntactic and morpholog-
ical annotation tools, of which the experiments re-
ported here explore only a small fraction. Indeed,
the range and practicality of the available options is
perhaps its most attractive feature. The inital results
are promising and we are optimistic that continued
exploration of this class of models will uncover even
more effective uses.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99986075">
I would like to thank David Chiang, Chris Dyer, Lise
Getoor, Kevin Gimpel, Adam Lopez, Nitin Mad-
nani, Smaranda Muresan, Noah Smith, Amy Wein-
berg and especially Philip Resnik for discussions re-
lating to this work. I am also grateful to David Chi-
ang for sharing source code of the Hiero translation
system and to the two anonymous reviewers for their
constructive comments.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999586034482759">
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Proc.
ICML 2007
G¨okhan H. Bakır, Thomas Hofmann, Bernhard
Sch¨olkopf, Alexander J. Smola, Ben Taskar and
S. V. N. Vishwanathan, eds. 2007. Predicting
Structured Data. MIT Press.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing Computational Linguistics,
22(1).
Phil Blunsom, Trevor Cohn and Miles Osborne. 2008.
Discriminative Synchronous Transduction for Statisti-
cal Machine Translation In proc. ACL 2008.
Marine Carpuat and Dekai Wu. 2007a. Improving Sta-
tistical Machine Translation using Word Sense Disam-
biguation In Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL 2007)..
Marine Carpuat and Dekai Wu. 2007b. How Phrase
Sense Disambiguation outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation. In
11th Conference on Theoretical and Methodological
Issues in Machine Translation (TMI 2007).
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. ACL.
Stanley F. Chen and Joshua T. Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
</reference>
<page confidence="0.983031">
35
</page>
<reference confidence="0.99969215">
Modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201-228.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2007. A General Regression Framework for Learning
String-to-String Mappings. In Predicting Structured
Data. MIT Press.
Brooke Cowan, Ivona Kucerova, and Michael Collins.
2006. A Discriminative Model for Tree-to-Tree Trans-
lation. In proceedings of EMNLP 2006.
J. Gao, G. Andrew, M. Johnson and K. Toutanova 2007.
A Comparative Study of Parameter Estimation Meth-
ods for Statistical Natural Language Processing. In
Proc. ACL 2007.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. Proceed-
ings of the Human Language Technology Conference
(HLT-NAACL 2003).
P. Liang, Alexandre Bouchard-Cote, D. Klein and B.
Taskar. 2006. An End-to-End Discriminative Ap-
proach to Machine Translation. In Association for
Computational Linguistics (ACL06).
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-
ization, and rotational invariance In Proceedings of
the Twenty-first International Conference on Machine
Learning
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In ACL 2003:
Proc. of the 41st Annual Meeting of the Association for
Computational Linguistics.
F. P´erez-Cruz, Z. Ghahramani and M. Pontil. 2007. Ker-
nel Conditional Graphical Models In Predicting Struc-
tured Data. MIT Press.
Michael Subotin. 2008. Exponential models for machine
translation. Generals paper, Department of Linguis-
tics, University of Maryland.
Charles Sutton and Andrew McCallum. 2005. Piecewise
training for undirected models. In Conference on Un-
certainty in Artificial Intelligence (UAI).
Charles Sutton and Tom Minka. 2006. Local Training
and Belief Propagation. Microsoft Research Technical
Report TR-2006-121..
Christoph Tillmann and Tong Zhang 2006. A Dis-
criminative Global Training Algorithm for Statistical
MT. In Association for Computational Linguistics
(ACL06).
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed 2006. Scalable Discriminative Learning for
Natural Language Parsing and Translation In Proceed-
ings of the 20th Annual Conference on Neural Infor-
mation Processing Systems (NIPS).
Zhuoran Wang, John Shawe-Taylor, and Sandor Szedmak
2007. Kernel Regression Based Machine Translation.
In Proceedings of NAACL HLT.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy
based phrase reordering model for statistical machine
translation. In Proceedings of the 21st international
Conference on Computational Linguistics and the 44th
Annual Meeting of the ACL.
</reference>
<page confidence="0.998937">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916005">
<title confidence="0.999297">Generalizing local translation models</title>
<author confidence="0.997593">Michael</author>
<affiliation confidence="0.995629333333333">Laboratory for Computational Linguistics and Information Department of University of</affiliation>
<address confidence="0.952739">College Park, MD</address>
<email confidence="0.999162">msubotin@umiacs.umd.edu</email>
<abstract confidence="0.998987791666667">We investigate translation modeling based on exponential estimates which generalize essential components of standard translation models. In application to a hierarchical phrasebased system the simplest generalization allows its models of lexical selection and reordering to be conditioned on arbitrary attributes of the source sentence and its annotation. Viewing these estimates as approximations of sentence-level probabilities motivates further elaborations that seek to exploit general syntactic and morphological patterns. control with makes it possible to negotiate the tradeoff between translation quality and decoding speed. Putting together and extending several recent advances in phrase-based translation we arrive at a flexible modeling framework that allows efficient leveraging of monolingual resources and tools. Experiments with features derived from the output of Chinese and Arabic parsers and an Arabic lemmatizer show significant improvements over a strong baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of L1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In Proc. ICML</booktitle>
<contexts>
<context position="21671" citStr="Andrew &amp; Gao (2007)" startWordPosition="3433" endWordPosition="3436">s have not been observed in training, although we do not take advantage of estimates for out-of-vocabulary words n pw(ry|rx) _ i�1 32 in the experiments below. 3.5 Regularization We apply Ei regularization (Ng, 2004; Gao et al., 2007) to make learning more robust to noise and control the effective dimensionality of the feature space by subtracting a weighted sum of absolute values of parameter weights from the log-likelihood of the training data w* = arg max LL(w) − W � Ci|wi |(6) i We optimize the objective using a variant of the orthant-wise limited-memory quasi-Newton algorithm proposed by Andrew &amp; Gao (2007).3 All values Ci are set to 1 in most of the experiments below, although we apply stronger regularization (Ci = 3) to reordering features. Tuning regularization tradeoffs individually for different feature types is an attractive option, but our experiments suggest that using cross-entropy on a held-out portion of training data for that purpose does not help performance. We leave investigation of the alternatives for future work. 4 Experiments 4.1 Data and methods We apply the models to Arabic-English and Chinese-English translation, with training sets consisting of 108,268 and 1,017,930 senten</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of L1-regularized log-linear models. In Proc. ICML 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨okhan H Bakır</author>
<author>Thomas Hofmann</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>Predicting Structured Data.</title>
<date>2007</date>
<editor>Alexander J. Smola, Ben Taskar and S. V. N. Vishwanathan, eds.</editor>
<publisher>MIT Press.</publisher>
<marker>Bakır, Hofmann, Sch¨olkopf, 2007</marker>
<rawString>G¨okhan H. Bakır, Thomas Hofmann, Bernhard Sch¨olkopf, Alexander J. Smola, Ben Taskar and S. V. N. Vishwanathan, eds. 2007. Predicting Structured Data. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="5517" citStr="Berger et al (1996)" startWordPosition="833" endWordPosition="836"> quality and decoding speed in a way appropriate for a given setting. A further attractive property of locally normalized models is the modest computational cost of their training and ease of its parallelization. This is particularly so for the models we concentrate on in this paper, defined so that parameter estimation decomposes into a large number of small optimization subproblems which can be solved independently. Several variants of these models beyond relative frequencies have appeared in the literature before. Maximum entropy estimation for translation of individual words dates back to Berger et al (1996), and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroduced under the rubric of word sense disambiguation and generalized to substrings (Chan et al 2007; Carpuat and Wu 2007a; Carpuat and Wu 2007b). Maximum entropy models for non-lexicalized reordering rules for a phrase-based system with CKY decoding has been described by Xiong et al (2006). Some of our experiments, where exponential models conditioned on the source sentence and its parse annotation are associated with all rewrite rules in a hierarchi</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing Computational Linguistics, 22(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Discriminative Synchronous Transduction for Statistical Machine Translation In</title>
<date>2008</date>
<booktitle>proc. ACL</booktitle>
<contexts>
<context position="3052" citStr="Blunsom et al (2008)" startWordPosition="453" endWordPosition="456">nce, but can also considerably reduce decoding speed, leading to run-time costs that may be unacceptable in industrial settings. Vector space regression has shown impressive performance in other tasks involving string-to-string mappings (Cortes et al., 2007), but its application to language translation presents a different set of open problems (Wang et al., 2007). Other promising formalisms, which have not yet produced end-to-end systems competitive with standard baselines, include the approach due to Turian et al (2006), the hidden-state synchronous grammar-based exponential model studied by Blunsom et al (2008), and a similar model incorporating target-side n-gram features proposed in Subotin (2008). Taken together the results of these studies point to a striking overarching conclusion: the humble relative frequency estimate of phrase-based models makes for a surprisingly strong baseline. The present paper investigates a family of models that Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 28–36, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics capitalize on this practical insight to allow efficien</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn and Miles Osborne. 2008. Discriminative Synchronous Transduction for Statistical Machine Translation In proc. ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving Statistical Machine Translation using Word Sense Disambiguation</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<contexts>
<context position="5783" citStr="Carpuat and Wu 2007" startWordPosition="875" endWordPosition="878"> on in this paper, defined so that parameter estimation decomposes into a large number of small optimization subproblems which can be solved independently. Several variants of these models beyond relative frequencies have appeared in the literature before. Maximum entropy estimation for translation of individual words dates back to Berger et al (1996), and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroduced under the rubric of word sense disambiguation and generalized to substrings (Chan et al 2007; Carpuat and Wu 2007a; Carpuat and Wu 2007b). Maximum entropy models for non-lexicalized reordering rules for a phrase-based system with CKY decoding has been described by Xiong et al (2006). Some of our experiments, where exponential models conditioned on the source sentence and its parse annotation are associated with all rewrite rules in a hierarchical phrase-based system (Chiang, 2007) and all word-level probabilities in standard lexical models, may be seen as a synthesis of these ideas. The broader perspective of viewing the product of such local probabilities as a particular approximation of sentence-level </context>
<context position="16426" citStr="Carpuat &amp; Wu (2007" startWordPosition="2593" endWordPosition="2596">d be rarely if at all observed in the training data, resulting in sparse and noisy estimates. Furthermore, we have yet to find a case where relative frequency estimates p(rflry) make a useful contribution to the system when contextuallyconditioned ”reverse” probabilities are used, suggesting that viewing translation modeling as approximating sentence-level probabilities p(Y |X) may be a more fruitful avenue in the long term. For translation with phrases without gaps classifier solutions of eq. 4 are equivalent to a maximum entropy variant of the phrase sense disambiguation approach studied by Carpuat &amp; Wu (2007b). These solutions are also closely related to the approximation known as piecewise training in graphical model literature (Sutton and McCallum, 2005; Sutton and Minka, 2006) and independently stated in a more general form by P´erez-Cruz et al (2007). Aside from formal differences between feature templates defined by graphical models and grammars, 31 which are beyond the scope of our discussion, there are several further contrasts between these studies and standard practice in machine translation in how the learned parameters are used to make predictions. Unlike inference in piecewise-trained</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007a. Improving Statistical Machine Translation using Word Sense Disambiguation In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007)..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>How Phrase Sense Disambiguation outperforms Word Sense Disambiguation for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI</booktitle>
<contexts>
<context position="5783" citStr="Carpuat and Wu 2007" startWordPosition="875" endWordPosition="878"> on in this paper, defined so that parameter estimation decomposes into a large number of small optimization subproblems which can be solved independently. Several variants of these models beyond relative frequencies have appeared in the literature before. Maximum entropy estimation for translation of individual words dates back to Berger et al (1996), and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroduced under the rubric of word sense disambiguation and generalized to substrings (Chan et al 2007; Carpuat and Wu 2007a; Carpuat and Wu 2007b). Maximum entropy models for non-lexicalized reordering rules for a phrase-based system with CKY decoding has been described by Xiong et al (2006). Some of our experiments, where exponential models conditioned on the source sentence and its parse annotation are associated with all rewrite rules in a hierarchical phrase-based system (Chiang, 2007) and all word-level probabilities in standard lexical models, may be seen as a synthesis of these ideas. The broader perspective of viewing the product of such local probabilities as a particular approximation of sentence-level </context>
<context position="16426" citStr="Carpuat &amp; Wu (2007" startWordPosition="2593" endWordPosition="2596">d be rarely if at all observed in the training data, resulting in sparse and noisy estimates. Furthermore, we have yet to find a case where relative frequency estimates p(rflry) make a useful contribution to the system when contextuallyconditioned ”reverse” probabilities are used, suggesting that viewing translation modeling as approximating sentence-level probabilities p(Y |X) may be a more fruitful avenue in the long term. For translation with phrases without gaps classifier solutions of eq. 4 are equivalent to a maximum entropy variant of the phrase sense disambiguation approach studied by Carpuat &amp; Wu (2007b). These solutions are also closely related to the approximation known as piecewise training in graphical model literature (Sutton and McCallum, 2005; Sutton and Minka, 2006) and independently stated in a more general form by P´erez-Cruz et al (2007). Aside from formal differences between feature templates defined by graphical models and grammars, 31 which are beyond the scope of our discussion, there are several further contrasts between these studies and standard practice in machine translation in how the learned parameters are used to make predictions. Unlike inference in piecewise-trained</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007b. How Phrase Sense Disambiguation outperforms Word Sense Disambiguation for Statistical Machine Translation. In 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="5762" citStr="Chan et al 2007" startWordPosition="871" endWordPosition="874">ls we concentrate on in this paper, defined so that parameter estimation decomposes into a large number of small optimization subproblems which can be solved independently. Several variants of these models beyond relative frequencies have appeared in the literature before. Maximum entropy estimation for translation of individual words dates back to Berger et al (1996), and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroduced under the rubric of word sense disambiguation and generalized to substrings (Chan et al 2007; Carpuat and Wu 2007a; Carpuat and Wu 2007b). Maximum entropy models for non-lexicalized reordering rules for a phrase-based system with CKY decoding has been described by Xiong et al (2006). Some of our experiments, where exponential models conditioned on the source sentence and its parse annotation are associated with all rewrite rules in a hierarchical phrase-based system (Chiang, 2007) and all word-level probabilities in standard lexical models, may be seen as a synthesis of these ideas. The broader perspective of viewing the product of such local probabilities as a particular approximati</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua T Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University.</institution>
<contexts>
<context position="23526" citStr="Chen and Goodman, 1998" startWordPosition="3711" endWordPosition="3714">s Magazine Parallel Text (LDC2005T10), FBIS Multilanguage Texts (LDC2003E14), Chinese News Translation Text Part 1 (LDC2005T06), and the HKNews portion of Hong Kong Parallel Text (LDC2004T08). Some sentence pairs were not included in the training sets due to large length discrepancies. by ”diag-and” symmetrization (Koehn et al., 2003). Thresholds for phrase extraction and decoder pruning were set to values typical for the baseline system (Chiang, 2007). Unaligned words at the outer edges of rules or gaps were disallowed. A trigram language model with modified interpolated KneserNey smoothing (Chen and Goodman, 1998) was trained by the SRILM toolkit on the Xinhua portion of the Gigaword corpus and the English side of the parallel training set. Evaluation was based on the BLEU score with 95% bootstrap confidence intervals for the score and difference between scores, calculated by scripts in version 11a of the NIST distribution. The 2002 NIST MT evaluation sets was used for development. The 2003, 2004, 2005, and 2006 sets were used for testing. The decision rule was based on the standard loglinear interpolation of several models, with weights tuned by MERT on the development set (Och, 2003). The baseline co</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua T. Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Computer Science Group, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--2</pages>
<contexts>
<context position="6155" citStr="Chiang, 2007" startWordPosition="935" endWordPosition="936">ulti-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroduced under the rubric of word sense disambiguation and generalized to substrings (Chan et al 2007; Carpuat and Wu 2007a; Carpuat and Wu 2007b). Maximum entropy models for non-lexicalized reordering rules for a phrase-based system with CKY decoding has been described by Xiong et al (2006). Some of our experiments, where exponential models conditioned on the source sentence and its parse annotation are associated with all rewrite rules in a hierarchical phrase-based system (Chiang, 2007) and all word-level probabilities in standard lexical models, may be seen as a synthesis of these ideas. The broader perspective of viewing the product of such local probabilities as a particular approximation of sentence-level likelihood points the way beyond multi-class classification, and this type of generalization is the main original contribution of the present work. Training a classifier to predict the target phrase for every source phrase is equivalent to conjoining all contextual features of the model with an indicator function for the surface form of some rule in the grammar. We can </context>
<context position="8996" citStr="Chiang, 2007" startWordPosition="1374" endWordPosition="1375">fective ex29 tensions of the feature space beyond multiclass classification. However, the results reported here show considerable promise and we believe that the flexibility of these models combined with their computational efficiency makes them potentially valuable as an extension for a variety of systems using translation models with local conditional probabilities and as a feature selection method for globally trained models. 2 Hierarchical phrase-based translation We take as our starting point David Chiang’s Hiero system, which generalizes phrase-based translation to substrings with gaps (Chiang, 2007). Consider for instance the following set of context-free rules with a single non-terminal symbol: (A, A) (A1 A2 , A1 A2 ) (A, A) ( d&apos; A1 id´ees A2 , A1 A2 ideas) (A, A) (incolores , colorless) (A, A) ( vertes , green) (A, A) (dormentA, sleepA) (A, A) (furieusement , furiously) It is one of many rule sets that would suffice to generate the English translation 1b for the French sentence 1a. 1a. d’ incolores idees vertes dorment furieusement 1b. colorless green ideas sleep furiously As shown by Chiang (2007), a weighted grammar of this form can be collected and scored by simple extensions of sta</context>
<context position="11972" citStr="Chiang (2007)" startWordPosition="1875" endWordPosition="1876">irical observation borne out by many studies. In order to build on this practical insight it is useful to gain a clearer understanding of their formal properties. We start by writing out an expression for the likelihood of training data which would give rise to maximum likelihood solutions like those in eq. 1. Consider a feature vector whose components are indicator functions for rules in the grammar, and let us define an exponential model for a sentence pair (Xm, Ym) of the form p (Ym|Xm) ^ 11 p(ry|rx) (3) rE(X,,Y,) exp{w · fr(Xm,Ym)} E(4) r:rx=rx exp{w · fr(Xm,Ym)} interpolated using MERT. 2Chiang (2007) uses a heuristic estimate of fractional counts in these computations. For completeness we report both variants in the experiments. 11 = rE(X,,Y,) 30 where fr(X,,t, Y,,t) is a restriction of the feature vector such that all of its entries except for the one corresponding to the rule r are zero and the summation is over all rules in the grammar with the same source side. As can be verified by writing out the likelihood for the training set and setting its gradient to zero, maximum likelihood estimation based on eq. 4 yields estimates equal to relative frequency solutions. In fact, because its n</context>
<context position="23359" citStr="Chiang, 2007" startWordPosition="3687" endWordPosition="3688">a came from Xinhua Chinese English Parallel News Text Version 1 beta (LDC2002E18), Chinese Treebank English Parallel Corpus (LDC2003E07), Chinese English News Magazine Parallel Text (LDC2005T10), FBIS Multilanguage Texts (LDC2003E14), Chinese News Translation Text Part 1 (LDC2005T06), and the HKNews portion of Hong Kong Parallel Text (LDC2004T08). Some sentence pairs were not included in the training sets due to large length discrepancies. by ”diag-and” symmetrization (Koehn et al., 2003). Thresholds for phrase extraction and decoder pruning were set to values typical for the baseline system (Chiang, 2007). Unaligned words at the outer edges of rules or gaps were disallowed. A trigram language model with modified interpolated KneserNey smoothing (Chen and Goodman, 1998) was trained by the SRILM toolkit on the Xinhua portion of the Gigaword corpus and the English side of the parallel training set. Evaluation was based on the BLEU score with 95% bootstrap confidence intervals for the score and difference between scores, calculated by scripts in version 11a of the NIST distribution. The 2002 NIST MT evaluation sets was used for development. The 2003, 2004, 2005, and 2006 sets were used for testing</context>
<context position="27657" citStr="Chiang (2007)" startWordPosition="4403" endWordPosition="4404">s leads to a reduction of costs that is similar to phrase table filtering and likewise does not affect the solution. At decoding time all features for the translation models and their weights are accessed from a disk-mapped trie. 4.2 Results and discussion The results are shown in tables 1 and 2. For both language pairs we had a choice between using a baseline that is computed in the same way as the other exponential models, with the exception of its use of relative frequency estimates and a baseline that incorporates averaged fractional counts for phrase models and lexical models, as used by Chiang (2007). For the sake of completeness we report both (though without performing statistical comparisons between 5This has prompted us to add an additional target-side token to lexical models, which subsumes the discarded items under a single category. Condition MT03 MT04 MT05 MT06 Rel. freq. 48.24 43.92 47.53 37.94 Frac. 48.34 45.68 47.95 39.41 Context 49.47* 45.65 48.76 39.49 +lex 50.42* 46.07* 49.66* 39.32 +lex+lemma 49.86* 47.02* 49.29* 40.81* Table 1: Arabic-English translation, BLEU scores on testing. Conditions include two baselines: simple relative frequency (rel. freq.) and fractional estimat</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Mehryar Mohri</author>
<author>Jason Weston</author>
</authors>
<title>A General Regression Framework for Learning String-to-String Mappings. In Predicting Structured Data.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2690" citStr="Cortes et al., 2007" startWordPosition="398" endWordPosition="401">ithms to improve over state-of-the-art baselines is currently limited by their lack of robust dimensionality reduction. Performance gains are closely tied to the number and variety of candidate features that enter into the model, and increasing the size of the feature space not only slows down training in terms of the number of iterations required for convergence, but can also considerably reduce decoding speed, leading to run-time costs that may be unacceptable in industrial settings. Vector space regression has shown impressive performance in other tasks involving string-to-string mappings (Cortes et al., 2007), but its application to language translation presents a different set of open problems (Wang et al., 2007). Other promising formalisms, which have not yet produced end-to-end systems competitive with standard baselines, include the approach due to Turian et al (2006), the hidden-state synchronous grammar-based exponential model studied by Blunsom et al (2008), and a similar model incorporating target-side n-gram features proposed in Subotin (2008). Taken together the results of these studies point to a striking overarching conclusion: the humble relative frequency estimate of phrase-based mod</context>
</contexts>
<marker>Cortes, Mohri, Weston, 2007</marker>
<rawString>Corinna Cortes, Mehryar Mohri, and Jason Weston. 2007. A General Regression Framework for Learning String-to-String Mappings. In Predicting Structured Data. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Ivona Kucerova</author>
<author>Michael Collins</author>
</authors>
<title>A Discriminative Model for Tree-to-Tree Translation.</title>
<date>2006</date>
<booktitle>In proceedings of EMNLP</booktitle>
<contexts>
<context position="2031" citStr="Cowan et al., 2006" startWordPosition="295" endWordPosition="298"> error training (Och, 2003) has by now become a standard tool for interpolating a small number of aggregate scores, it is not well suited for learning in high-dimensional feature spaces. At the same time, although recent years have seen considerable progress in development of general methods 28 for large-scale prediction of complex outputs (Bakır et al., 2007), their application to language translation has presented considerable challenges. Several studies have shown that large-margin methods can be adapted to the special complexities of the task (Liang et al., 2006; Tillmann and Zhang, 2006; Cowan et al., 2006) . However, the capacity of these algorithms to improve over state-of-the-art baselines is currently limited by their lack of robust dimensionality reduction. Performance gains are closely tied to the number and variety of candidate features that enter into the model, and increasing the size of the feature space not only slows down training in terms of the number of iterations required for convergence, but can also considerably reduce decoding speed, leading to run-time costs that may be unacceptable in industrial settings. Vector space regression has shown impressive performance in other task</context>
</contexts>
<marker>Cowan, Kucerova, Collins, 2006</marker>
<rawString>Brooke Cowan, Ivona Kucerova, and Michael Collins. 2006. A Discriminative Model for Tree-to-Tree Translation. In proceedings of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>G Andrew</author>
<author>M Johnson</author>
<author>K Toutanova</author>
</authors>
<title>A Comparative Study of Parameter Estimation Methods for Statistical Natural Language Processing.</title>
<date>2007</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="21286" citStr="Gao et al., 2007" startWordPosition="3367" endWordPosition="3370">mas rather than words. Lemma-based features suggest another extension of the modeling framework. Instead of computing the sums in normalization factors over all English words aligned to a given Arabic token in the training data, we let the sum range over all English words aligned to Arabic words sharing its lemma. This also defines probabilities for Arabic words whose surface forms have not been observed in training, although we do not take advantage of estimates for out-of-vocabulary words n pw(ry|rx) _ i�1 32 in the experiments below. 3.5 Regularization We apply Ei regularization (Ng, 2004; Gao et al., 2007) to make learning more robust to noise and control the effective dimensionality of the feature space by subtracting a weighted sum of absolute values of parameter weights from the log-likelihood of the training data w* = arg max LL(w) − W � Ci|wi |(6) i We optimize the objective using a variant of the orthant-wise limited-memory quasi-Newton algorithm proposed by Andrew &amp; Gao (2007).3 All values Ci are set to 1 in most of the experiments below, although we apply stronger regularization (Ci = 3) to reordering features. Tuning regularization tradeoffs individually for different feature types is </context>
</contexts>
<marker>Gao, Andrew, Johnson, Toutanova, 2007</marker>
<rawString>J. Gao, G. Andrew, M. Johnson and K. Toutanova 2007. A Comparative Study of Parameter Estimation Methods for Statistical Natural Language Processing. In Proc. ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>Proceedings of the Human Language Technology Conference (HLT-NAACL</booktitle>
<contexts>
<context position="17572" citStr="Koehn et al (2003)" startWordPosition="2775" endWordPosition="2778">ters are used to make predictions. Unlike inference in piecewise-trained graphical models, where all parameters for a given output are added together without normalization, features that enter into the score for a translation hypothesis are restricted to be consistent with a single synchronous parse and the local probabilities are normalized in decoding as in training. 3.3 Lexical models The use of conditional probabilities in standard lexical models also gives us a straightforward way to generalize them in the same way as phrase models. Consider the lexical model pw(ry|rx), defined following Koehn et al (2003), with a denoting the most frequent word alignment observed for the rule in the training set. 1 |j|(i,j) E a |(iYap(wyi |wxj ) (5) We replace p(wyi |wxj ) with context-conditioned probabilities, computed similarly to eq. 4, but at the level of individual words. Our experience suggests that, unlike the analogous phrase model, the standard lexical model pw(rx|ry) is not made redundant by this elaboration, and we use its baseline variant in all our experiments. While this approach seeks to make the most of practical insights underlying stateof-the-art baselines, it is of course not the only way t</context>
<context position="23239" citStr="Koehn et al., 2003" startWordPosition="3665" endWordPosition="3668">17), Arabic English Parallel News Text (LDC2004T18), and Arabic Treebank English Translation (LDC2005E46). Chinese-English data came from Xinhua Chinese English Parallel News Text Version 1 beta (LDC2002E18), Chinese Treebank English Parallel Corpus (LDC2003E07), Chinese English News Magazine Parallel Text (LDC2005T10), FBIS Multilanguage Texts (LDC2003E14), Chinese News Translation Text Part 1 (LDC2005T06), and the HKNews portion of Hong Kong Parallel Text (LDC2004T08). Some sentence pairs were not included in the training sets due to large length discrepancies. by ”diag-and” symmetrization (Koehn et al., 2003). Thresholds for phrase extraction and decoder pruning were set to values typical for the baseline system (Chiang, 2007). Unaligned words at the outer edges of rules or gaps were disallowed. A trigram language model with modified interpolated KneserNey smoothing (Chen and Goodman, 1998) was trained by the SRILM toolkit on the Xinhua portion of the Gigaword corpus and the English side of the parallel training set. Evaluation was based on the BLEU score with 95% bootstrap confidence intervals for the score and difference between scores, calculated by scripts in version 11a of the NIST distributi</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. Proceedings of the Human Language Technology Conference (HLT-NAACL 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>Alexandre Bouchard-Cote</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An End-to-End Discriminative Approach to Machine Translation.</title>
<date>2006</date>
<booktitle>In Association for Computational Linguistics (ACL06).</booktitle>
<contexts>
<context position="1984" citStr="Liang et al., 2006" startWordPosition="287" endWordPosition="290">problems in machine translation. While minimum error training (Och, 2003) has by now become a standard tool for interpolating a small number of aggregate scores, it is not well suited for learning in high-dimensional feature spaces. At the same time, although recent years have seen considerable progress in development of general methods 28 for large-scale prediction of complex outputs (Bakır et al., 2007), their application to language translation has presented considerable challenges. Several studies have shown that large-margin methods can be adapted to the special complexities of the task (Liang et al., 2006; Tillmann and Zhang, 2006; Cowan et al., 2006) . However, the capacity of these algorithms to improve over state-of-the-art baselines is currently limited by their lack of robust dimensionality reduction. Performance gains are closely tied to the number and variety of candidate features that enter into the model, and increasing the size of the feature space not only slows down training in terms of the number of iterations required for convergence, but can also considerably reduce decoding speed, leading to run-time costs that may be unacceptable in industrial settings. Vector space regression</context>
</contexts>
<marker>Liang, Bouchard-Cote, Klein, Taskar, 2006</marker>
<rawString>P. Liang, Alexandre Bouchard-Cote, D. Klein and B. Taskar. 2006. An End-to-End Discriminative Approach to Machine Translation. In Association for Computational Linguistics (ACL06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Y Ng</author>
</authors>
<title>Feature selection, L1 vs. L2 regularization, and rotational invariance</title>
<date>2004</date>
<booktitle>In Proceedings of the Twenty-first International Conference on Machine Learning</booktitle>
<contexts>
<context position="4771" citStr="Ng, 2004" startWordPosition="714" endWordPosition="715"> training example. The cases for which such solutions have a closed form correspond to particular restrictions placed on the feature space. Thus, relative frequency phrase models can be obtained by limiting the feature space to indicator functions for the phrase pairs consistent with an alignment. By removing unnecessary restrictions we restore the full flexibility of local exponential models, including their ability to use features depending on arbitrary aspects of the source sentence and its annotation. The availability of robust algorithms for dimensionality reduction with Ei regularizers (Ng, 2004) means that we can start with a virtually unlimited number of candidate features and negotiate the tradeoff between translation quality and decoding speed in a way appropriate for a given setting. A further attractive property of locally normalized models is the modest computational cost of their training and ease of its parallelization. This is particularly so for the models we concentrate on in this paper, defined so that parameter estimation decomposes into a large number of small optimization subproblems which can be solved independently. Several variants of these models beyond relative fr</context>
<context position="21267" citStr="Ng, 2004" startWordPosition="3365" endWordPosition="3366">vidual lemmas rather than words. Lemma-based features suggest another extension of the modeling framework. Instead of computing the sums in normalization factors over all English words aligned to a given Arabic token in the training data, we let the sum range over all English words aligned to Arabic words sharing its lemma. This also defines probabilities for Arabic words whose surface forms have not been observed in training, although we do not take advantage of estimates for out-of-vocabulary words n pw(ry|rx) _ i�1 32 in the experiments below. 3.5 Regularization We apply Ei regularization (Ng, 2004; Gao et al., 2007) to make learning more robust to noise and control the effective dimensionality of the feature space by subtracting a weighted sum of absolute values of parameter weights from the log-likelihood of the training data w* = arg max LL(w) − W � Ci|wi |(6) i We optimize the objective using a variant of the orthant-wise limited-memory quasi-Newton algorithm proposed by Andrew &amp; Gao (2007).3 All values Ci are set to 1 in most of the experiments below, although we apply stronger regularization (Ci = 3) to reordering features. Tuning regularization tradeoffs individually for differen</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>A. Y. Ng. 2004. Feature selection, L1 vs. L2 regularization, and rotational invariance In Proceedings of the Twenty-first International Conference on Machine Learning</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In ACL 2003: Proc. of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1439" citStr="Och, 2003" startWordPosition="201" endWordPosition="202"> the tradeoff between translation quality and decoding speed. Putting together and extending several recent advances in phrase-based translation we arrive at a flexible modeling framework that allows efficient leveraging of monolingual resources and tools. Experiments with features derived from the output of Chinese and Arabic parsers and an Arabic lemmatizer show significant improvements over a strong baseline. 1 Introduction Effective handling of large and diverse inventories of feature functions is one of the most pressing open problems in machine translation. While minimum error training (Och, 2003) has by now become a standard tool for interpolating a small number of aggregate scores, it is not well suited for learning in high-dimensional feature spaces. At the same time, although recent years have seen considerable progress in development of general methods 28 for large-scale prediction of complex outputs (Bakır et al., 2007), their application to language translation has presented considerable challenges. Several studies have shown that large-margin methods can be adapted to the special complexities of the task (Liang et al., 2006; Tillmann and Zhang, 2006; Cowan et al., 2006) . Howev</context>
<context position="24109" citStr="Och, 2003" startWordPosition="3814" endWordPosition="3815">ng (Chen and Goodman, 1998) was trained by the SRILM toolkit on the Xinhua portion of the Gigaword corpus and the English side of the parallel training set. Evaluation was based on the BLEU score with 95% bootstrap confidence intervals for the score and difference between scores, calculated by scripts in version 11a of the NIST distribution. The 2002 NIST MT evaluation sets was used for development. The 2003, 2004, 2005, and 2006 sets were used for testing. The decision rule was based on the standard loglinear interpolation of several models, with weights tuned by MERT on the development set (Och, 2003). The baseline consisted of the language model, two phrase translation models, two lexical models, and a brevity penalty. In the runs where generalized exponential models were used they replaced both of the baseline phrase translation models. The feature set used for exponential phrase models in the experiments included all the rules in the grammar and all aligned word pairs for lexical models. Elementary contextual features were based on Viterbi parses obtained from the Stanford parser. Word features included identities of word unigrams and bigrams adjacent to a given rule, possibly including</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training for Statistical Machine Translation. In ACL 2003: Proc. of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F P´erez-Cruz</author>
<author>Z Ghahramani</author>
<author>M Pontil</author>
</authors>
<title>Kernel Conditional Graphical Models In Predicting Structured Data.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<marker>P´erez-Cruz, Ghahramani, Pontil, 2007</marker>
<rawString>F. P´erez-Cruz, Z. Ghahramani and M. Pontil. 2007. Kernel Conditional Graphical Models In Predicting Structured Data. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Subotin</author>
</authors>
<title>Exponential models for machine translation. Generals paper,</title>
<date>2008</date>
<institution>Department of Linguistics, University of Maryland.</institution>
<contexts>
<context position="3142" citStr="Subotin (2008)" startWordPosition="468" endWordPosition="469">ceptable in industrial settings. Vector space regression has shown impressive performance in other tasks involving string-to-string mappings (Cortes et al., 2007), but its application to language translation presents a different set of open problems (Wang et al., 2007). Other promising formalisms, which have not yet produced end-to-end systems competitive with standard baselines, include the approach due to Turian et al (2006), the hidden-state synchronous grammar-based exponential model studied by Blunsom et al (2008), and a similar model incorporating target-side n-gram features proposed in Subotin (2008). Taken together the results of these studies point to a striking overarching conclusion: the humble relative frequency estimate of phrase-based models makes for a surprisingly strong baseline. The present paper investigates a family of models that Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 28–36, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics capitalize on this practical insight to allow efficient optimization of weights for a virtually unlimited number of features. We take as a point</context>
</contexts>
<marker>Subotin, 2008</marker>
<rawString>Michael Subotin. 2008. Exponential models for machine translation. Generals paper, Department of Linguistics, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Piecewise training for undirected models.</title>
<date>2005</date>
<booktitle>In Conference on Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="16576" citStr="Sutton and McCallum, 2005" startWordPosition="2616" endWordPosition="2619">ative frequency estimates p(rflry) make a useful contribution to the system when contextuallyconditioned ”reverse” probabilities are used, suggesting that viewing translation modeling as approximating sentence-level probabilities p(Y |X) may be a more fruitful avenue in the long term. For translation with phrases without gaps classifier solutions of eq. 4 are equivalent to a maximum entropy variant of the phrase sense disambiguation approach studied by Carpuat &amp; Wu (2007b). These solutions are also closely related to the approximation known as piecewise training in graphical model literature (Sutton and McCallum, 2005; Sutton and Minka, 2006) and independently stated in a more general form by P´erez-Cruz et al (2007). Aside from formal differences between feature templates defined by graphical models and grammars, 31 which are beyond the scope of our discussion, there are several further contrasts between these studies and standard practice in machine translation in how the learned parameters are used to make predictions. Unlike inference in piecewise-trained graphical models, where all parameters for a given output are added together without normalization, features that enter into the score for a translat</context>
</contexts>
<marker>Sutton, McCallum, 2005</marker>
<rawString>Charles Sutton and Andrew McCallum. 2005. Piecewise training for undirected models. In Conference on Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Tom Minka</author>
</authors>
<title>Local Training and Belief Propagation. Microsoft Research</title>
<date>2006</date>
<tech>Technical Report TR-2006-121..</tech>
<contexts>
<context position="16601" citStr="Sutton and Minka, 2006" startWordPosition="2620" endWordPosition="2623">(rflry) make a useful contribution to the system when contextuallyconditioned ”reverse” probabilities are used, suggesting that viewing translation modeling as approximating sentence-level probabilities p(Y |X) may be a more fruitful avenue in the long term. For translation with phrases without gaps classifier solutions of eq. 4 are equivalent to a maximum entropy variant of the phrase sense disambiguation approach studied by Carpuat &amp; Wu (2007b). These solutions are also closely related to the approximation known as piecewise training in graphical model literature (Sutton and McCallum, 2005; Sutton and Minka, 2006) and independently stated in a more general form by P´erez-Cruz et al (2007). Aside from formal differences between feature templates defined by graphical models and grammars, 31 which are beyond the scope of our discussion, there are several further contrasts between these studies and standard practice in machine translation in how the learned parameters are used to make predictions. Unlike inference in piecewise-trained graphical models, where all parameters for a given output are added together without normalization, features that enter into the score for a translation hypothesis are restri</context>
<context position="18255" citStr="Sutton &amp; Minka (2006)" startWordPosition="2887" endWordPosition="2890">r the rule in the training set. 1 |j|(i,j) E a |(iYap(wyi |wxj ) (5) We replace p(wyi |wxj ) with context-conditioned probabilities, computed similarly to eq. 4, but at the level of individual words. Our experience suggests that, unlike the analogous phrase model, the standard lexical model pw(rx|ry) is not made redundant by this elaboration, and we use its baseline variant in all our experiments. While this approach seeks to make the most of practical insights underlying stateof-the-art baselines, it is of course not the only way to combine rule-based and word-based features. See for example Sutton &amp; Minka (2006) for a discussion of alternatives that are closer in spirit to the idea of approximating global probabilities. 3.4 Further generalizations An immediate practical benefit of interpreting relative frequency and classifier estimates of translation models as special cases is the possibility of generalizing them further by introducing additional features based on less specific representations of rules and words. Among the least specific and most potentially useful representations of hierarchical phrases are those limited to the patterns formed by gaps and words, allowing the model to generalize reo</context>
</contexts>
<marker>Sutton, Minka, 2006</marker>
<rawString>Charles Sutton and Tom Minka. 2006. Local Training and Belief Propagation. Microsoft Research Technical Report TR-2006-121..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A Discriminative Global Training Algorithm for Statistical MT.</title>
<date>2006</date>
<booktitle>In Association for Computational Linguistics (ACL06).</booktitle>
<contexts>
<context position="2010" citStr="Tillmann and Zhang, 2006" startWordPosition="291" endWordPosition="294">translation. While minimum error training (Och, 2003) has by now become a standard tool for interpolating a small number of aggregate scores, it is not well suited for learning in high-dimensional feature spaces. At the same time, although recent years have seen considerable progress in development of general methods 28 for large-scale prediction of complex outputs (Bakır et al., 2007), their application to language translation has presented considerable challenges. Several studies have shown that large-margin methods can be adapted to the special complexities of the task (Liang et al., 2006; Tillmann and Zhang, 2006; Cowan et al., 2006) . However, the capacity of these algorithms to improve over state-of-the-art baselines is currently limited by their lack of robust dimensionality reduction. Performance gains are closely tied to the number and variety of candidate features that enter into the model, and increasing the size of the feature space not only slows down training in terms of the number of iterations required for convergence, but can also considerably reduce decoding speed, leading to run-time costs that may be unacceptable in industrial settings. Vector space regression has shown impressive perf</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>Christoph Tillmann and Tong Zhang 2006. A Discriminative Global Training Algorithm for Statistical MT. In Association for Computational Linguistics (ACL06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Benjamin Wellington</author>
<author>I Dan Melamed</author>
</authors>
<title>Scalable Discriminative Learning for Natural Language Parsing and Translation</title>
<date>2006</date>
<booktitle>In Proceedings of the 20th Annual Conference on Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="2958" citStr="Turian et al (2006)" startWordPosition="440" endWordPosition="443">space not only slows down training in terms of the number of iterations required for convergence, but can also considerably reduce decoding speed, leading to run-time costs that may be unacceptable in industrial settings. Vector space regression has shown impressive performance in other tasks involving string-to-string mappings (Cortes et al., 2007), but its application to language translation presents a different set of open problems (Wang et al., 2007). Other promising formalisms, which have not yet produced end-to-end systems competitive with standard baselines, include the approach due to Turian et al (2006), the hidden-state synchronous grammar-based exponential model studied by Blunsom et al (2008), and a similar model incorporating target-side n-gram features proposed in Subotin (2008). Taken together the results of these studies point to a striking overarching conclusion: the humble relative frequency estimate of phrase-based models makes for a surprisingly strong baseline. The present paper investigates a family of models that Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 28–36, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 As</context>
</contexts>
<marker>Turian, Wellington, Melamed, 2006</marker>
<rawString>Joseph Turian, Benjamin Wellington, and I. Dan Melamed 2006. Scalable Discriminative Learning for Natural Language Parsing and Translation In Proceedings of the 20th Annual Conference on Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoran Wang</author>
<author>John Shawe-Taylor</author>
<author>Sandor Szedmak</author>
</authors>
<title>Kernel Regression Based Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT.</booktitle>
<contexts>
<context position="2797" citStr="Wang et al., 2007" startWordPosition="416" endWordPosition="419">reduction. Performance gains are closely tied to the number and variety of candidate features that enter into the model, and increasing the size of the feature space not only slows down training in terms of the number of iterations required for convergence, but can also considerably reduce decoding speed, leading to run-time costs that may be unacceptable in industrial settings. Vector space regression has shown impressive performance in other tasks involving string-to-string mappings (Cortes et al., 2007), but its application to language translation presents a different set of open problems (Wang et al., 2007). Other promising formalisms, which have not yet produced end-to-end systems competitive with standard baselines, include the approach due to Turian et al (2006), the hidden-state synchronous grammar-based exponential model studied by Blunsom et al (2008), and a similar model incorporating target-side n-gram features proposed in Subotin (2008). Taken together the results of these studies point to a striking overarching conclusion: the humble relative frequency estimate of phrase-based models makes for a surprisingly strong baseline. The present paper investigates a family of models that Procee</context>
</contexts>
<marker>Wang, Shawe-Taylor, Szedmak, 2007</marker>
<rawString>Zhuoran Wang, John Shawe-Taylor, and Sandor Szedmak 2007. Kernel Regression Based Machine Translation. In Proceedings of NAACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Xiong</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st international Conference on Computational Linguistics and the 44th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="5953" citStr="Xiong et al (2006)" startWordPosition="902" endWordPosition="905">s of these models beyond relative frequencies have appeared in the literature before. Maximum entropy estimation for translation of individual words dates back to Berger et al (1996), and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroduced under the rubric of word sense disambiguation and generalized to substrings (Chan et al 2007; Carpuat and Wu 2007a; Carpuat and Wu 2007b). Maximum entropy models for non-lexicalized reordering rules for a phrase-based system with CKY decoding has been described by Xiong et al (2006). Some of our experiments, where exponential models conditioned on the source sentence and its parse annotation are associated with all rewrite rules in a hierarchical phrase-based system (Chiang, 2007) and all word-level probabilities in standard lexical models, may be seen as a synthesis of these ideas. The broader perspective of viewing the product of such local probabilities as a particular approximation of sentence-level likelihood points the way beyond multi-class classification, and this type of generalization is the main original contribution of the present work. Training a classifier </context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 21st international Conference on Computational Linguistics and the 44th Annual Meeting of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>