<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000047">
<title confidence="0.6504845">
E-Assessment using Latent Semantic Analysis in the Computer Science
Domain: A Pilot Study
</title>
<author confidence="0.510043">
Pete Thomas, Debra Haley, Anne deRoeck, Marian Petre
</author>
<affiliation confidence="0.496133">
Computing Research Centre, Department of Computing
The Open University, Walton Hall, Milton Keynes, UK MK7 6AA
</affiliation>
<email confidence="0.368084">
P.G.Thomas;D.T.Haley;A.Deroeck;M.Petre [at] open.ac.uk
</email>
<sectionHeader confidence="0.975755" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994498866666667">
Latent Semantic Analysis (LSA) is a statistical
Natural Language Processing (NLP) technique for
inferring meaning from a text. Existing LSA-based
applications focus on formative assessment in
general domains. The suitability of LSA for
summative assessment in the domain of computer
science is not well known. The results from the
pilot study reported in this paper encourage us to
pursue further research in the use of LSA in the
narrow, technical domain of computer science.
This paper explains the theory behind LSA,
describes some existing LSA applications, and
presents some results using LSA for automatic
marking of short essays for a graduate class in
architectures of computing systems.
</bodyText>
<sectionHeader confidence="0.998687" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999855772727273">
This paper describes a pilot study undertaken to
investigate the feasibility of using Latent Semantic
Analysis (LSA) for automatic marking of short
essays in the domain of computer science. These
short essays are free-form answers to exam
questions - not multiple choice questions (MCQ).
Exams in the form of MCQs, although easy to
mark, do not provide the opportunity for deeper
assessment made possible with essays.
This study employs LSA in several areas that are
under-researched. First, it uses very small corpora
– less than 2,000 words compared to about 11
million words in one of the existing, successful
applications (Wade-Stein &amp; Kintsch, 2003).
Second, it involves the specific, technical domain
of computer science. LSA research usually
involves more heterogeneous text with a broad
vocabulary. Finally, it focuses on summative
assessment where the accuracy of results is
paramount. Most LSA research has involved
formative assessment for which more general
evaluations are sufficient.
The study investigates one of the shortcomings
of LSA mentioned by Manning and Schütze (1999,
p. 564). They report that LSA has high recall but
low precision. The precision declines because of
spurious co-occurrences. They claim that LSA
does better on heterogeneous text with a broad
vocabulary. Computer science is a technical
domain with a more homogeneous vocabulary,
which results, possibly, in fewer spurious co-
occurrences. A major question of this research is
how LSA will behave when the technique is
stretched by applying it to a narrow domain.
Section 2 gives the history of LSA and explains
how it works. Section 3 describes several existing
LSA applications related to e-assessment. Section
4 provides the motivation for more LSA research
and reports on a pilot study undertaken to assess
the feasibility of using LSA for automatic marking
of short essays in the domain of computer science.
Section 5 lists several open issues and areas for
improvement that future studies will address.
Finally, Section 6 summarises the paper.
</bodyText>
<sectionHeader confidence="0.624002" genericHeader="method">
2 What is Latent Semantic Analysis?
</sectionHeader>
<bodyText confidence="0.999298333333333">
“Latent Semantic Analysis is a theory and
method for extracting and representing the
contextual-usage meaning of words by statistical
computations applied to a large corpus of text”
(Landauer, Foltz &amp; Laham, 1998). It is a
statistical-based natural language processing (NLP)
method for inferring meaning from a text1. It was
developed by researchers at Bellcore as an
information retrieval technique (Deerwester,
Dumais, Furnas, Landauer &amp; Harshman, 1990) in
the late 1980s. The earliest application of LSA was
Latent Semantic Indexing (LSI) (Furnas, et al.,
1988; Deerwester, et al., 1990). LSI provided an
advantage over keyword-based methods in that it
could induce associative meanings of the query
(Foltz, 1996) rather than relying on exact matches.
Landauer and Dumais (1997) promoted LSA as
a model for the human acquisition of knowledge.
They developed their theory after creating an
information retrieval tool and observing
unexpected results from its use. They claimed that
</bodyText>
<footnote confidence="0.9592645">
1 The researchers originally used the term LSI (Latent
Semantic Indexing) to refer to the method. The
information retrieval community continues to use the
term LSI.
</footnote>
<bodyText confidence="0.989459027777778">
LSA solves Plato’s problem, that is, how do people
learn so much when presented with so little? Their
answer is the inductive process: LSA “induces
global knowledge indirectly from local co-
occurrence data in a large body of representative
text” (Landauer &amp; Dumais, 1997).
From the original application for retrieving
information, the applications of LSA have evolved
to systems that more fully exploit its ability to
extract and represent meaning. Recent applications
based on LSA compare a sample text with a pre-
existing, very large corpus to judge the meaning of
the sample.
To use LSA, researchers amass a suitable corpus
of text. They create a term-by-document matrix
where the columns are documents and the rows are
terms (Deerwester, et al., 1990). A term is a
subdivision of a document; it can be a word,
phrase, or some other unit. A document can be a
sentence, a paragraph, a textbook, or some other
unit. In other words, documents contain terms. The
elements of the matrix are weighted word counts of
how many times each term appears in each
document. More formally, each element, aij in an i
x j matrix is the weighted count of term i in
document j.
LSA decomposes the matrix into three matrices
using Singular Value Decomposition (SVD), a
well-known technique (Miller, 2003) that is the
general case of factor analysis. Deerwester et. al.,
(1990) describe the process as follows.
Let t = the number of terms, or rows
d = the number of documents, or columns
X = a t by d matrix
Then, after applying SVD, X = TSD, where
m = the number of dimensions, m &lt;= min(t,d)
</bodyText>
<equation confidence="0.9211895">
T = a t by m matrix
S = an m by m diagonal matrix, i.e., only
diagonal entries have non-zero values
D = an m by d matrix
</equation>
<bodyText confidence="0.999944311111111">
LSA reduces S, the diagonal matrix created by
SVD, to an appropriate number of dimensions k,
where k &lt;&lt; m, resulting in S&apos;. The product of TS&apos;D
is the least-squares best fit to X, the original matrix
(Deerwester, et al., 1990).
The literature often describes LSA as analyzing
co-occurring terms. Landauer and Dumais (1997)
argue it does more and explain that the new matrix
captures the “latent transitivity relations” among
the terms. Terms not appearing in an original
document are represented in the new matrix as if
they actually were in the original document
(Landauer &amp; Dumais, 1997). LSA’s ability to
induce transitive meanings is considered especially
important given that Furnas et. al. (1982) report
fewer than 20% of paired individuals will use the
same term to refer to the same common concept.
LSA exploits what can be named the transitive
property of semantic relationships: If A→B and
B→C, then A→C (where → stands for is
semantically related to). However, the similarity to
the transitive property of equality is not perfect.
Two words widely separated in the transitivity
chain can have a weaker relationship than closer
words. For example, LSA might find that copy →
duplicate → double → twin → sibling. Copy and
duplicate are much closer semantically than copy
and sibling.
Finding the correct number of dimensions for the
new matrix created by SVD is critical; if it is too
small, the structure of the data is not captured.
Conversely, if it is too large, sampling error and
unimportant details remain, e.g., grammatical
variants (Deerwester, et al., 1990; Miller, 2003;
Wade-Stein &amp; Kintsch, 2003). Empirical work
involving very large corpora shows the correct
number of dimensions to be about 300 (Landauer
&amp; Dumais, 1997; Wade-Stein &amp; Kintsch, 2003).
Creating the matrices using SVD and reducing
the number of dimensions, often referred to as
training the system, requires a lot of computing
power; it can take hours or days to complete the
processing (Miller, 2003). Fortunately, once the
training is complete, it takes just seconds for LSA
to evaluate a text sample (Miller, 2003).
</bodyText>
<sectionHeader confidence="0.86644" genericHeader="method">
3 Using LSA for assessment
</sectionHeader>
<subsectionHeader confidence="0.984187">
3.1 Types of assessment
</subsectionHeader>
<bodyText confidence="0.999800153846154">
Electronic feedback, or e-assessment, is an
important component of e-learning. LSA, with its
ability to provide immediate, accurate,
personalised, and content-based feedback, can be
an important component of an e-learning
environment.
Formative assessment provides direction, focus,
and guidance concurrent with the learner engaging
in some learning process. E-assessment can
provide ample help to a learner without requiring
added work by a human tutor. A learner can
benefit from private, immediate, and convenient
feedback.
Summative assessment, on the other hand,
happens at the conclusion of a learning episode or
activity. It evaluates a learner’s achievement and
communicates that achievement to interested
parties. Summative assessment using LSA shares
the virtues of formative assessment and can
produce more objective grading results than those
that can occur when many markers are assessing
hundreds of student essays.
The applications described in the next section
use LSA to provide formative assessment. Section
4 discusses a pilot study that focuses on summative
assessment.
</bodyText>
<subsectionHeader confidence="0.99983">
3.2 Existing applications
</subsectionHeader>
<bodyText confidence="0.9999862">
Much work is being done in the area of using
LSA to mark essays automatically and to provide
content-based feedback. One of the great
advantages of automatic assessment of essays is its
ability to provide helpful, immediate feedback to
the learner without burdening the teacher. This
application is particularly suited to distance
education, where opportunities for one-on-one
tutoring are infrequent or non-existent (Steinhart,
2001). Existing systems include Apex (Lemaire &amp;
Dessus, 2001), Autotutor (Wiemer-Hastings,
Wiemer-Hastings &amp; Graesser, 1999), Intelligent
Essay Assessor (Foltz, Laham &amp; Landauer, 1999),
Select-a-Kibitzer (Miller, 2003), and Summary
Street (Steinhart, 2001; Wade-Stein &amp; Kintsch,
2003). They differ in details of audience addressed,
subject domain, and advanced training required by
the system (Miller, 2003). They are similar in that
they are LSA-based, web-based, and provide
scaffolding, feedback, and unlimited practice
opportunities without increasing a teacher’s
workload (Steinhart, 2001). All of them claim that
LSA correlates as well to human markers as human
markers correlate to one another. See (Miller,
2003) for an excellent analysis of these systems.
</bodyText>
<sectionHeader confidence="0.927093" genericHeader="method">
4 E-Assessment pilot study
</sectionHeader>
<bodyText confidence="0.999931105263158">
Although research using Latent Semantic
Analysis (LSA) to assess essays automatically has
shown promising results (Chung &amp; O&apos;Neil, 1997;
Foltz, et al., 1999; Foltz, 1996; Lemaire &amp; Dessus,
2001; Landauer, et al., 1998; Miller, 2003;
Steinhart, 2001; Wade-Stein &amp; Kintsch, 2003), not
enough research has been done on using LSA for
instructional software (Lemaire &amp; Dessus, 2001).
Previous studies involved both young students and
university-age students, and several different
knowledge domains. An open question is how LSA
can be used to improve the learning of university-
age, computer science students. This section offers
three characteristics that distinguish this research
from existing research involving the use of LSA to
analyse expository writing texts and reports on a
pilot study to determine the feasibility of using
LSA to mark students’ short essay answers to
exam questions.
</bodyText>
<subsectionHeader confidence="0.997299">
4.1 Focuses of the experiment
</subsectionHeader>
<bodyText confidence="0.999990666666667">
This subsection describes three facets of the
experiment that involve under-researched areas, in
the cases of the domain and the type of assessment,
and an unsolved research question in the case of
the appropriate dimension reduction value for
small corpora.
The study involves essays written by computer
science (CS) students. CS, being a technical
domain, has a limited, specialist vocabulary. Thus,
essays written for CS exams are thought to have a
more restricted terminology than do the expository
writing texts usually analysed by LSA researchers.
Nevertheless, the essays are written in English
using a mixture of technical terms and general
terms. Will LSA produce valid results?
Accuracy is paramount in summative
assessment. Whereas formative assessment can be
general and informative, summative assessment
requires a high degree of precision. Can LSA
produce results with a high degree of correlation
with human markers?
The consensus among LSA researchers, who
customarily use very large corpora, is that the
number of dimensions that produces the best result
is about 300. But because this study involved just
17 graded samples, the number of reduced
dimensions has to be less than 17. Can LSA work
with many fewer dimensions than 300? A broader
question is whether LSA can work with a small
corpus in a restricted domain.
</bodyText>
<subsectionHeader confidence="0.988142">
4.2 The Data
</subsectionHeader>
<bodyText confidence="0.9992196">
The data for this experiment consisted of
answers from six students to three questions in a
single electronic exam held at the Open University
in April 2002. The answers are free-form short
essays. The training corpus for each question
comprised 16 documents consisting of student
answers to the same question and a specimen
solution. Table 1 gives the average size (in words)
of both the student answers graded by LSA and the
corpus essays.
</bodyText>
<table confidence="0.997870166666667">
Question Question Question
A B C
Corpus
documents 112 35 131
Student 108 31 88
answers
</table>
<tableCaption confidence="0.999901">
Table 1: Average document size
</tableCaption>
<bodyText confidence="0.999641571428571">
The corpus training documents had been marked
previously by three trained human markers. The
average marks were assigned to each corpus
document. To provide a standard on which to
judge the LSA results, each of the answers from
the six students was marked by three human
markers and awarded the average mark.
</bodyText>
<subsectionHeader confidence="0.994398">
4.3 The LSA Method
</subsectionHeader>
<bodyText confidence="0.9998975">
The following steps were taken three times, once
for each question on the exam.
</bodyText>
<listItem confidence="0.887706840909091">
• Determine the words, or terms, in the corpus
documents after removing punctuation and
stop words. (No attempt has yet been made
to deal with synonyms or word forms, such
as plurals, via stemming.)
• Construct a t x d term frequency matrix M,
where t is the number of terms in the corpus
and d is the number of documents – 17 in
this experiment. Each entry tfij is the number
of times term i appears in document j.
• Weight each entry tfij in M using the simple
weighting scheme: 1 + log(tfij).
• Perform singular value decomposition of the
weighted term frequency matrix resulting in
Mweighted = TSDT.
• Choose an optimum dimension, k, to reduce
Mweighted. (see the next subsection for details)
• Compute B = SDT - the reduced weighted
frequency document
• Construct a vector, a, of weighted term
frequencies in a student-answer document.
• Compute the reduced student-answer vector
a&apos; = aTST
• Determine the corpus document that best
matches the student-answer by comparing a&apos;
with the column vectors in B.
• Award the student-answer the mark
associated with the most similar corpus
document using the cosine similarity
measure.
4.4 Determining the optimum dimension
reduction (k)
• This experiment reduced the SVD matrices
using k = 2 .. number of corpus documents –
1, or k = 2 .. 16. For each value of k, the
LSA method produced a mark for each
student-answer.
• The experiment compared the six LSA
marks for the student-answers with the
corresponding average human mark using
Euclidean distance.
• The experiment revealed that, for this
corpus, k = about 10 gave the best matches
across the three questions.
</listItem>
<sectionHeader confidence="0.798182" genericHeader="method">
4.5 Results
</sectionHeader>
<bodyText confidence="0.973176">
The four graphs below show the results obtained.
</bodyText>
<figureCaption confidence="0.999749666666667">
Figure 1: LSA marks for question A
Figure 2: LSA marks for question B
Figure 4: LSA marks for total
</figureCaption>
<subsectionHeader confidence="0.756667">
4.6 Discussion
</subsectionHeader>
<bodyText confidence="0.990631777777778">
This experiment investigated the feasibility of
using LSA to assess short essay answers. The
results shown in Figures 1 – 3 suggest that LSA-
marked answers were similar to human-marked
answers in 83% (15 of 18) of the answers tested.
LSA seemed to work well on five of the six
student-answers for Question A, all the answers for
Question B, and four of the six answers for
Question C. For the three clearly incorrect
</bodyText>
<figure confidence="0.998629916666666">
Points Awarded
7
6
5
4
3
2
0
1
1 2 3 4 5 6
Student
Question A
Graded by Human
Graded by LSA
Points Awarded
4
2
7
6
5
3
0
1
1 2 3 4 5 6
Student
Graded by Human
Graded by LSA
Question B
Points Awarded
Points Awarded
16
14
12
10
4
2
8
7
6
5
3
0
1
8
6
4
2
0
</figure>
<figureCaption confidence="0.901431">
Figure 3: LSA marks for question C
</figureCaption>
<figure confidence="0.9941549">
1 2 3 4 5 6
Student
1 2 3 4 5 6
Student
Graded by Human
Graded by LSA
Graded by Human
Graded by LSA
Question C
Total
</figure>
<bodyText confidence="0.99991175">
answers, LSA gave a higher score than did the
human markers for the answer to question A and
one higher mark and one lower mark than did the
human markers for the answers to question C.
To quantify these visual impressions, the study
used the Spearman’s rho statistical test for each of
the three questions. Only one of the three questions
shows a statistical correlation between LSA and
human marks: question B shows a statistical
correlation significant at the 95% level.
These results, while unacceptable for a real-
world application, are encouraging given the
extremely small corpus size of only 17 documents,
or about 2,000 words for questions A and C and
about 600 words for question B. This pilot study
solidified our understanding of how to use LSA,
the importance of a large corpus, and how to
approach further research to improve the results
and increase the applicability of the results of this
pilot study.
</bodyText>
<sectionHeader confidence="0.898938" genericHeader="method">
5 A roadmap for further research
</sectionHeader>
<subsectionHeader confidence="0.866836">
5.1 The corpus
</subsectionHeader>
<bodyText confidence="0.998747">
LSA results depend on both corpus size and
corpus content.
</bodyText>
<subsectionHeader confidence="0.552933">
5.1.1 Corpus size
</subsectionHeader>
<bodyText confidence="0.999962777777778">
Existing LSA research stresses the need for a
large corpus. The corpora for the pilot study
described in this paper were very small. In
addition, the documents are too few in number to
be representative of the student population. An
ideal corpus would provide documents that give a
spread of marks across the mark range and a
variety of answers for each mark. Future studies
will use a larger corpus.
</bodyText>
<subsubsectionHeader confidence="0.608064">
5.1.2 Corpus content
</subsubsectionHeader>
<bodyText confidence="0.999972857142857">
Wiemer-Hastings, et. al (1999) report that size is
not the only important characteristic of the corpus.
Not surprisingly, the composition of the corpus
effects the results of essay grading by LSA. In
addition to specific documents directly related to
their essay questions, Wiemer-Hastings, et. al used
more general documents. They found the best
composition to be about 40% general documents
and 60% specific documents.
The corpora used for this pilot study comprised
only specific documents - the human marked short
essays. Future work will involve adding sections of
text books to enlarge and enrich the corpus with
more general documents.
</bodyText>
<subsectionHeader confidence="0.999248">
5.2 Weighting function
</subsectionHeader>
<bodyText confidence="0.999226">
The pilot study used local weighting - the most
basic form of term weighting. Local weighting is
defined as tfij (the number of times term i is found
in document j) dampened by the log function: local
weighting = 1 + log (tfij ). This dampening reflects
the fact that a term that appears in a document x
times more frequently than another term is not x
times more important.
The study selected this simple weighting
function to provide a basis on which to compare
more sophisticated functions in future work. Many
variations of weighting functions exist; two are
described next.
</bodyText>
<subsectionHeader confidence="0.963435">
5.2.1 Log-entropy
</subsectionHeader>
<bodyText confidence="0.9997765">
Dumais (1991) recommended using log-entropy
weighting, which is local weighting times global
weighting. Global weighting is defined as 1 – the
entropy or noise. Global weighting attempts to
quantify the fact that a term appearing in many
documents is less important than a term appearing
in fewer documents.
The log-entropy term weight for term i in doc j =
</bodyText>
<equation confidence="0.97380075">
⎡ tflogf,⎤
log(1 + fij )∗⎢1− gfi or&apos;
log(numdocs) ⎥
⎥⎦
</equation>
<bodyText confidence="0.993344">
where
tfij – term frequency – the frequency of term i in
document j
gfi – global frequency – the total number of
times term i occurs in the whole collection
</bodyText>
<subsectionHeader confidence="0.455026">
5.2.2 tfidf
</subsectionHeader>
<bodyText confidence="0.994137">
Sebastiani (2002) claims the most common
weighting is tfidf, or term frequency inverse
document frequency.
</bodyText>
<equation confidence="0.998262">
tfidf(tk,dj) = #(tk,dj)∗log Tr
#Tr(tk)
</equation>
<bodyText confidence="0.999332428571429">
where #( tk, dj ) denotes the number of times tk
occurs in dj
#Tr(tk) denotes the document frequency of term tk,
that is, the number of documents in Tr in which tk
occurs.
Future studies will examine the effects of
applying various term weighting functions.
</bodyText>
<subsectionHeader confidence="0.995377">
5.3 Similarity measures
</subsectionHeader>
<bodyText confidence="0.999654555555556">
The pilot study used two different similarity
measures. It used the cosine measure to compare
the test document with the corpus documents. It
used Euclidean distance to choose k, the number of
reduced dimensions that produced the best results
overall. Other measures exist and will be tried in
future studies.
Ljungstrand and Johansson (1998) define the
following similarity measures:
</bodyText>
<figureCaption confidence="0.952644">
Figure 5. Similarity Measures
</figureCaption>
<subsectionHeader confidence="0.948131">
5.4 Corpus pre-processing
</subsectionHeader>
<bodyText confidence="0.999958">
Removing stop words is one type of pre-
processing performed for this study. Explicitly
adding synonym knowledge and stemming are two
additional ways of preparing the corpus that future
research will consider. Stemming involves
conflating word forms to a common string, e.g.,
write, writing, writes, written, writer would be
represented in the corpus as writ.
</bodyText>
<subsectionHeader confidence="0.958463">
5.5 Dimension reduction
</subsectionHeader>
<bodyText confidence="0.999993777777778">
Choosing the appropriate dimension, k, for
reducing the matrices in LSA is a well known open
issue. The current consensus is that k should be
about 300. No theory yet exists to suggest the
appropriate value for k. Currently, researchers
determine k by empirically testing various values
of k and selecting the best one. The only heuristic
says that k &lt;&lt; min(terms, documents). An
interesting result from the study reported in this
paper is that even though k had to be less than 17,
the number of documents in our corpora and thus
much less than the recommended value of 300,
LSA produced statistically significant results for
one of the three questions tested.
Future studies will continue to investigate the
relationship among k, the size of the corpus, the
number of documents in the corpus, and the type of
documents in the corpus.
</bodyText>
<sectionHeader confidence="0.997536" genericHeader="method">
6 Summary
</sectionHeader>
<bodyText confidence="0.999961684210526">
This paper introduced and explained LSA and
how it can be used to provide e-assessment by both
formative and summative assessment. It provided
examples of existing research that uses LSA for e-
assessment. It reported the results of a pilot study
to determine the feasibility of using LSA to assess
automatically essays written in the domain of
computer science. Although just one of the three
essay questions tested showed that LSA marks
were statistically correlated to the average of three
human marks, the results are promising because
the experiment used very small corpora.
Future studies will attempt to improve the results
of LSA by increasing the size of the corpora,
improving the content of the corpora,
experimenting with different weighting functions
and similarity measures, pre-processing the corpus,
and using various values of k for dimension
reduction.
</bodyText>
<sectionHeader confidence="0.998405" genericHeader="conclusions">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999418428571429">
The work reported in this study was partially supported by
the European Community under the Innovation Society
Technologies (IST) programme of the 6th Framework
Programme for RTD - project ELeGI, contract IST-002205.
This document does not represent the opinion of the European
Community, and the European Community is not responsible
for any use that might be made of data appearing therein.
</bodyText>
<sectionHeader confidence="0.998337" genericHeader="references">
8 References
</sectionHeader>
<reference confidence="0.9195992">
Chung, G., &amp; O&apos;Neil, G. (1997). Methodological
approaches to online scoring of essays (Center
for the Study of Evaluation, CRESST No. 461).
Los Angeles.
Deerwester, S., Dumais, S. T., Furnas, G. W.,
Landauer, T. K., &amp; Harshman, R. (1990).
Indexing by Latent Semantic Analysis. Journal
of the American Society for Information Science,
41(6), 391-407.
Dumais, S. T. (1991). Improving the retrieval of
information from external sources. Behavioral
Research Methods, Instruments &amp; Computers,
23(2), 229-236.
Foltz, P. W. (1996). Latent semantic analysis for
text-based research. Behavior Research Methods,
Instruments and Computers, 28(2), 197-202.
Foltz, P. W., Laham, D., &amp; Landauer, T. K. (1999).
Automated Essay Scoring: Applications to
Educational Technology. In Proceedings of
EdMedia &apos;99.
</reference>
<figure confidence="0.844750666666667">
Inner product (dot) measure:
n
M( X, Y ) = E
i =1
Cosine measure:
n
E
M( X, Y ) =
1
n n
E E
x i
</figure>
<equation confidence="0.852837846153846">
= 1 i = 1
yi
i
Manhattan distance measure:
n
M( X, Y ) = E xi −
i =1
Euclidean distance measure (2-norm):
n
M( X, Y ) = E (xi −yi)
i=1
m-norm measure:
1
M( X, Y ) = ⎜⎝⎛E(xi − yi)m
m ⎠⎟, m ∈ N
i=1
Where X = (x1,x2,...,xn) and Y = (y1,y2,...,yn) are two
n-dimensional vectors.
yi
2
i
xiy
xi
yi
i
=
</equation>
<reference confidence="0.9744375">
Furnas, G. W., Deerwester, S., Dumais, S. T.,
Landauer, T. K., Harshman, R. A., Streeter, L.
A., et al. (1988). Information retrieval using a
singular value decomposition model of latent
semantic structure. ACM, pp. 465-480.
Furnas, G. W., Gomez, L. M., Landauer, T. K., &amp;
Dumais, S. T. (1982). Statistical semantics: How
can a computer use what people name things to
guess what things people mean when they name
things? In Proceedings of the SIGCHI
Conference on Human Factors in Computing
Systems (pp. 251-253). ACM.
Landauer, T. K., &amp; Dumais, S. T. (1997). A
solution to Plato&apos;s problem: The Latent Semantic
Analysis theory of acquisition, induction and
representation of knowledge. Psychological
Review, 104(2), 211-240.
Landauer, T. K., Foltz, P. W., &amp; Laham, D. (1998).
An introduction to Latent Semantic Analysis.
Discourse Processes, 25, 259-284.
Lemaire, B., &amp; Dessus, P. (2001). A system to
assess the semantic content of student essays.
Journal of Educational Computing Research,
24(3), 305-320.
Ljungstrand, P., &amp; Johansson, H. (1998, May).
Intranet indexing using semantic document
clustering. Retrieved 5/4/2004, from
http://www.handels.gu.se/epc/archive/00002294/
01/ljungstrand.IA7400.pdf.
Manning, C., &amp; Schütze, H. (1999). Foundations
of Statistical Natural Language Processing.
Cambridge, Massachusetts: MIT Press.
Miller, T. (2003). Essay assessment with Latent
Semantic Analysis. Journal of Educational
Computing Research, 28.
Sebastiani, F. (2002, March). Machine Learning in
Automated Text Categorization. ACM
Computing Surveys, 34(1), 1-47.
Steinhart, D. J. (2001). Summary Street: An
intelligent tutoring system for improving student
writing through the use of Latent Semantic
Analysis. Unpublished doctoral dissertation,
University of Colorado, Boulder, Department of
Psychology.
Wade-Stein, D., &amp; Kintsch, E. (2003). Summary
Street: Interactive computer support for writing
(Tech Report from the Institute for Cognitive
Science). University of Colorado, USA.
Wiemer-Hastings, P., Wiemer-Hastings, K., &amp;
Graesser, A. C. (1999). Improving an intelligent
tutor&apos;s comprehension of students with Latent
Semantic Analysis. In S. Lajoie &amp; M. Vivet
(Eds.), Artificial Intelligence in Education.
Amsterdam: IOS Press.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.223050">
<title confidence="0.9086815">E-Assessment using Latent Semantic Analysis in the Computer Science Domain: A Pilot Study</title>
<author confidence="0.950462">Pete Thomas</author>
<author confidence="0.950462">Debra Haley</author>
<author confidence="0.950462">Anne deRoeck</author>
<author confidence="0.950462">Marian</author>
<affiliation confidence="0.453737">Computing Research Centre, Department of The Open University, Walton Hall, Milton Keynes, UK MK7</affiliation>
<email confidence="0.888593">P.G.Thomas;D.T.Haley;A.Deroeck;M.Petre[at]open.ac.uk</email>
<abstract confidence="0.999767625">Latent Semantic Analysis (LSA) is a statistical Natural Language Processing (NLP) technique for inferring meaning from a text. Existing LSA-based applications focus on formative assessment in general domains. The suitability of LSA for summative assessment in the domain of computer science is not well known. The results from the pilot study reported in this paper encourage us to pursue further research in the use of LSA in the narrow, technical domain of computer science. This paper explains the theory behind LSA, describes some existing LSA applications, and presents some results using LSA for automatic marking of short essays for a graduate class in architectures of computing systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Chung</author>
<author>G O&apos;Neil</author>
</authors>
<title>Methodological approaches to online scoring of essays (Center for the Study of Evaluation,</title>
<date>1997</date>
<journal>CRESST</journal>
<volume>461</volume>
<location>Los Angeles.</location>
<contexts>
<context position="10577" citStr="Chung &amp; O&apos;Neil, 1997" startWordPosition="1654" endWordPosition="1657">s of audience addressed, subject domain, and advanced training required by the system (Miller, 2003). They are similar in that they are LSA-based, web-based, and provide scaffolding, feedback, and unlimited practice opportunities without increasing a teacher’s workload (Steinhart, 2001). All of them claim that LSA correlates as well to human markers as human markers correlate to one another. See (Miller, 2003) for an excellent analysis of these systems. 4 E-Assessment pilot study Although research using Latent Semantic Analysis (LSA) to assess essays automatically has shown promising results (Chung &amp; O&apos;Neil, 1997; Foltz, et al., 1999; Foltz, 1996; Lemaire &amp; Dessus, 2001; Landauer, et al., 1998; Miller, 2003; Steinhart, 2001; Wade-Stein &amp; Kintsch, 2003), not enough research has been done on using LSA for instructional software (Lemaire &amp; Dessus, 2001). Previous studies involved both young students and university-age students, and several different knowledge domains. An open question is how LSA can be used to improve the learning of universityage, computer science students. This section offers three characteristics that distinguish this research from existing research involving the use of LSA to analyse</context>
</contexts>
<marker>Chung, O&apos;Neil, 1997</marker>
<rawString>Chung, G., &amp; O&apos;Neil, G. (1997). Methodological approaches to online scoring of essays (Center for the Study of Evaluation, CRESST No. 461). Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>391--407</pages>
<contexts>
<context position="3693" citStr="Deerwester, et al., 1990" startWordPosition="559" endWordPosition="562"> What is Latent Semantic Analysis? “Latent Semantic Analysis is a theory and method for extracting and representing the contextual-usage meaning of words by statistical computations applied to a large corpus of text” (Landauer, Foltz &amp; Laham, 1998). It is a statistical-based natural language processing (NLP) method for inferring meaning from a text1. It was developed by researchers at Bellcore as an information retrieval technique (Deerwester, Dumais, Furnas, Landauer &amp; Harshman, 1990) in the late 1980s. The earliest application of LSA was Latent Semantic Indexing (LSI) (Furnas, et al., 1988; Deerwester, et al., 1990). LSI provided an advantage over keyword-based methods in that it could induce associative meanings of the query (Foltz, 1996) rather than relying on exact matches. Landauer and Dumais (1997) promoted LSA as a model for the human acquisition of knowledge. They developed their theory after creating an information retrieval tool and observing unexpected results from its use. They claimed that 1 The researchers originally used the term LSI (Latent Semantic Indexing) to refer to the method. The information retrieval community continues to use the term LSI. LSA solves Plato’s problem, that is, how </context>
<context position="5005" citStr="Deerwester, et al., 1990" startWordPosition="767" endWordPosition="770">ocess: LSA “induces global knowledge indirectly from local cooccurrence data in a large body of representative text” (Landauer &amp; Dumais, 1997). From the original application for retrieving information, the applications of LSA have evolved to systems that more fully exploit its ability to extract and represent meaning. Recent applications based on LSA compare a sample text with a preexisting, very large corpus to judge the meaning of the sample. To use LSA, researchers amass a suitable corpus of text. They create a term-by-document matrix where the columns are documents and the rows are terms (Deerwester, et al., 1990). A term is a subdivision of a document; it can be a word, phrase, or some other unit. A document can be a sentence, a paragraph, a textbook, or some other unit. In other words, documents contain terms. The elements of the matrix are weighted word counts of how many times each term appears in each document. More formally, each element, aij in an i x j matrix is the weighted count of term i in document j. LSA decomposes the matrix into three matrices using Singular Value Decomposition (SVD), a well-known technique (Miller, 2003) that is the general case of factor analysis. Deerwester et. al., (</context>
<context position="7522" citStr="Deerwester, et al., 1990" startWordPosition="1209" endWordPosition="1212">elated to). However, the similarity to the transitive property of equality is not perfect. Two words widely separated in the transitivity chain can have a weaker relationship than closer words. For example, LSA might find that copy → duplicate → double → twin → sibling. Copy and duplicate are much closer semantically than copy and sibling. Finding the correct number of dimensions for the new matrix created by SVD is critical; if it is too small, the structure of the data is not captured. Conversely, if it is too large, sampling error and unimportant details remain, e.g., grammatical variants (Deerwester, et al., 1990; Miller, 2003; Wade-Stein &amp; Kintsch, 2003). Empirical work involving very large corpora shows the correct number of dimensions to be about 300 (Landauer &amp; Dumais, 1997; Wade-Stein &amp; Kintsch, 2003). Creating the matrices using SVD and reducing the number of dimensions, often referred to as training the system, requires a lot of computing power; it can take hours or days to complete the processing (Miller, 2003). Fortunately, once the training is complete, it takes just seconds for LSA to evaluate a text sample (Miller, 2003). 3 Using LSA for assessment 3.1 Types of assessment Electronic feedba</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., &amp; Harshman, R. (1990). Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6), 391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Dumais</author>
</authors>
<title>Improving the retrieval of information from external sources.</title>
<date>1991</date>
<journal>Behavioral Research Methods, Instruments &amp; Computers,</journal>
<volume>23</volume>
<issue>2</issue>
<pages>229--236</pages>
<contexts>
<context position="19032" citStr="Dumais (1991)" startWordPosition="3109" endWordPosition="3110">on The pilot study used local weighting - the most basic form of term weighting. Local weighting is defined as tfij (the number of times term i is found in document j) dampened by the log function: local weighting = 1 + log (tfij ). This dampening reflects the fact that a term that appears in a document x times more frequently than another term is not x times more important. The study selected this simple weighting function to provide a basis on which to compare more sophisticated functions in future work. Many variations of weighting functions exist; two are described next. 5.2.1 Log-entropy Dumais (1991) recommended using log-entropy weighting, which is local weighting times global weighting. Global weighting is defined as 1 – the entropy or noise. Global weighting attempts to quantify the fact that a term appearing in many documents is less important than a term appearing in fewer documents. The log-entropy term weight for term i in doc j = ⎡ tflogf,⎤ log(1 + fij )∗⎢1− gfi or&apos; log(numdocs) ⎥ ⎥⎦ where tfij – term frequency – the frequency of term i in document j gfi – global frequency – the total number of times term i occurs in the whole collection 5.2.2 tfidf Sebastiani (2002) claims the mo</context>
</contexts>
<marker>Dumais, 1991</marker>
<rawString>Dumais, S. T. (1991). Improving the retrieval of information from external sources. Behavioral Research Methods, Instruments &amp; Computers, 23(2), 229-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Foltz</author>
</authors>
<title>Latent semantic analysis for text-based research.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments and Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<pages>197--202</pages>
<contexts>
<context position="3819" citStr="Foltz, 1996" startWordPosition="580" endWordPosition="581">aning of words by statistical computations applied to a large corpus of text” (Landauer, Foltz &amp; Laham, 1998). It is a statistical-based natural language processing (NLP) method for inferring meaning from a text1. It was developed by researchers at Bellcore as an information retrieval technique (Deerwester, Dumais, Furnas, Landauer &amp; Harshman, 1990) in the late 1980s. The earliest application of LSA was Latent Semantic Indexing (LSI) (Furnas, et al., 1988; Deerwester, et al., 1990). LSI provided an advantage over keyword-based methods in that it could induce associative meanings of the query (Foltz, 1996) rather than relying on exact matches. Landauer and Dumais (1997) promoted LSA as a model for the human acquisition of knowledge. They developed their theory after creating an information retrieval tool and observing unexpected results from its use. They claimed that 1 The researchers originally used the term LSI (Latent Semantic Indexing) to refer to the method. The information retrieval community continues to use the term LSI. LSA solves Plato’s problem, that is, how do people learn so much when presented with so little? Their answer is the inductive process: LSA “induces global knowledge in</context>
<context position="10611" citStr="Foltz, 1996" startWordPosition="1662" endWordPosition="1663">d advanced training required by the system (Miller, 2003). They are similar in that they are LSA-based, web-based, and provide scaffolding, feedback, and unlimited practice opportunities without increasing a teacher’s workload (Steinhart, 2001). All of them claim that LSA correlates as well to human markers as human markers correlate to one another. See (Miller, 2003) for an excellent analysis of these systems. 4 E-Assessment pilot study Although research using Latent Semantic Analysis (LSA) to assess essays automatically has shown promising results (Chung &amp; O&apos;Neil, 1997; Foltz, et al., 1999; Foltz, 1996; Lemaire &amp; Dessus, 2001; Landauer, et al., 1998; Miller, 2003; Steinhart, 2001; Wade-Stein &amp; Kintsch, 2003), not enough research has been done on using LSA for instructional software (Lemaire &amp; Dessus, 2001). Previous studies involved both young students and university-age students, and several different knowledge domains. An open question is how LSA can be used to improve the learning of universityage, computer science students. This section offers three characteristics that distinguish this research from existing research involving the use of LSA to analyse expository writing texts and repo</context>
</contexts>
<marker>Foltz, 1996</marker>
<rawString>Foltz, P. W. (1996). Latent semantic analysis for text-based research. Behavior Research Methods, Instruments and Computers, 28(2), 197-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Foltz</author>
<author>D Laham</author>
<author>T K Landauer</author>
</authors>
<title>Automated Essay Scoring: Applications to Educational Technology.</title>
<date>1999</date>
<booktitle>In Proceedings of EdMedia &apos;99.</booktitle>
<contexts>
<context position="10598" citStr="Foltz, et al., 1999" startWordPosition="1658" endWordPosition="1661">d, subject domain, and advanced training required by the system (Miller, 2003). They are similar in that they are LSA-based, web-based, and provide scaffolding, feedback, and unlimited practice opportunities without increasing a teacher’s workload (Steinhart, 2001). All of them claim that LSA correlates as well to human markers as human markers correlate to one another. See (Miller, 2003) for an excellent analysis of these systems. 4 E-Assessment pilot study Although research using Latent Semantic Analysis (LSA) to assess essays automatically has shown promising results (Chung &amp; O&apos;Neil, 1997; Foltz, et al., 1999; Foltz, 1996; Lemaire &amp; Dessus, 2001; Landauer, et al., 1998; Miller, 2003; Steinhart, 2001; Wade-Stein &amp; Kintsch, 2003), not enough research has been done on using LSA for instructional software (Lemaire &amp; Dessus, 2001). Previous studies involved both young students and university-age students, and several different knowledge domains. An open question is how LSA can be used to improve the learning of universityage, computer science students. This section offers three characteristics that distinguish this research from existing research involving the use of LSA to analyse expository writing t</context>
</contexts>
<marker>Foltz, Laham, Landauer, 1999</marker>
<rawString>Foltz, P. W., Laham, D., &amp; Landauer, T. K. (1999). Automated Essay Scoring: Applications to Educational Technology. In Proceedings of EdMedia &apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Furnas</author>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>R A Harshman</author>
<author>L A Streeter</author>
</authors>
<title>Information retrieval using a singular value decomposition model of latent semantic structure.</title>
<date>1988</date>
<pages>465--480</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="3666" citStr="Furnas, et al., 1988" startWordPosition="555" endWordPosition="558">ummarises the paper. 2 What is Latent Semantic Analysis? “Latent Semantic Analysis is a theory and method for extracting and representing the contextual-usage meaning of words by statistical computations applied to a large corpus of text” (Landauer, Foltz &amp; Laham, 1998). It is a statistical-based natural language processing (NLP) method for inferring meaning from a text1. It was developed by researchers at Bellcore as an information retrieval technique (Deerwester, Dumais, Furnas, Landauer &amp; Harshman, 1990) in the late 1980s. The earliest application of LSA was Latent Semantic Indexing (LSI) (Furnas, et al., 1988; Deerwester, et al., 1990). LSI provided an advantage over keyword-based methods in that it could induce associative meanings of the query (Foltz, 1996) rather than relying on exact matches. Landauer and Dumais (1997) promoted LSA as a model for the human acquisition of knowledge. They developed their theory after creating an information retrieval tool and observing unexpected results from its use. They claimed that 1 The researchers originally used the term LSI (Latent Semantic Indexing) to refer to the method. The information retrieval community continues to use the term LSI. LSA solves Pla</context>
</contexts>
<marker>Furnas, Deerwester, Dumais, Landauer, Harshman, Streeter, 1988</marker>
<rawString>Furnas, G. W., Deerwester, S., Dumais, S. T., Landauer, T. K., Harshman, R. A., Streeter, L. A., et al. (1988). Information retrieval using a singular value decomposition model of latent semantic structure. ACM, pp. 465-480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Furnas</author>
<author>L M Gomez</author>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>Statistical semantics: How can a computer use what people name things to guess what things people mean when they name things?</title>
<date>1982</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</booktitle>
<pages>251--253</pages>
<publisher>ACM.</publisher>
<marker>Furnas, Gomez, Landauer, Dumais, 1982</marker>
<rawString>Furnas, G. W., Gomez, L. M., Landauer, T. K., &amp; Dumais, S. T. (1982). Statistical semantics: How can a computer use what people name things to guess what things people mean when they name things? In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 251-253). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>211--240</pages>
<contexts>
<context position="3884" citStr="Landauer and Dumais (1997)" startWordPosition="588" endWordPosition="591">to a large corpus of text” (Landauer, Foltz &amp; Laham, 1998). It is a statistical-based natural language processing (NLP) method for inferring meaning from a text1. It was developed by researchers at Bellcore as an information retrieval technique (Deerwester, Dumais, Furnas, Landauer &amp; Harshman, 1990) in the late 1980s. The earliest application of LSA was Latent Semantic Indexing (LSI) (Furnas, et al., 1988; Deerwester, et al., 1990). LSI provided an advantage over keyword-based methods in that it could induce associative meanings of the query (Foltz, 1996) rather than relying on exact matches. Landauer and Dumais (1997) promoted LSA as a model for the human acquisition of knowledge. They developed their theory after creating an information retrieval tool and observing unexpected results from its use. They claimed that 1 The researchers originally used the term LSI (Latent Semantic Indexing) to refer to the method. The information retrieval community continues to use the term LSI. LSA solves Plato’s problem, that is, how do people learn so much when presented with so little? Their answer is the inductive process: LSA “induces global knowledge indirectly from local cooccurrence data in a large body of represen</context>
<context position="6269" citStr="Landauer and Dumais (1997)" startWordPosition="1003" endWordPosition="1006"> Let t = the number of terms, or rows d = the number of documents, or columns X = a t by d matrix Then, after applying SVD, X = TSD, where m = the number of dimensions, m &lt;= min(t,d) T = a t by m matrix S = an m by m diagonal matrix, i.e., only diagonal entries have non-zero values D = an m by d matrix LSA reduces S, the diagonal matrix created by SVD, to an appropriate number of dimensions k, where k &lt;&lt; m, resulting in S&apos;. The product of TS&apos;D is the least-squares best fit to X, the original matrix (Deerwester, et al., 1990). The literature often describes LSA as analyzing co-occurring terms. Landauer and Dumais (1997) argue it does more and explain that the new matrix captures the “latent transitivity relations” among the terms. Terms not appearing in an original document are represented in the new matrix as if they actually were in the original document (Landauer &amp; Dumais, 1997). LSA’s ability to induce transitive meanings is considered especially important given that Furnas et. al. (1982) report fewer than 20% of paired individuals will use the same term to refer to the same common concept. LSA exploits what can be named the transitive property of semantic relationships: If A→B and B→C, then A→C (where →</context>
<context position="4522" citStr="Landauer &amp; Dumais, 1997" startWordPosition="689" endWordPosition="692">as a model for the human acquisition of knowledge. They developed their theory after creating an information retrieval tool and observing unexpected results from its use. They claimed that 1 The researchers originally used the term LSI (Latent Semantic Indexing) to refer to the method. The information retrieval community continues to use the term LSI. LSA solves Plato’s problem, that is, how do people learn so much when presented with so little? Their answer is the inductive process: LSA “induces global knowledge indirectly from local cooccurrence data in a large body of representative text” (Landauer &amp; Dumais, 1997). From the original application for retrieving information, the applications of LSA have evolved to systems that more fully exploit its ability to extract and represent meaning. Recent applications based on LSA compare a sample text with a preexisting, very large corpus to judge the meaning of the sample. To use LSA, researchers amass a suitable corpus of text. They create a term-by-document matrix where the columns are documents and the rows are terms (Deerwester, et al., 1990). A term is a subdivision of a document; it can be a word, phrase, or some other unit. A document can be a sentence, </context>
<context position="6536" citStr="Landauer &amp; Dumais, 1997" startWordPosition="1047" endWordPosition="1050">n-zero values D = an m by d matrix LSA reduces S, the diagonal matrix created by SVD, to an appropriate number of dimensions k, where k &lt;&lt; m, resulting in S&apos;. The product of TS&apos;D is the least-squares best fit to X, the original matrix (Deerwester, et al., 1990). The literature often describes LSA as analyzing co-occurring terms. Landauer and Dumais (1997) argue it does more and explain that the new matrix captures the “latent transitivity relations” among the terms. Terms not appearing in an original document are represented in the new matrix as if they actually were in the original document (Landauer &amp; Dumais, 1997). LSA’s ability to induce transitive meanings is considered especially important given that Furnas et. al. (1982) report fewer than 20% of paired individuals will use the same term to refer to the same common concept. LSA exploits what can be named the transitive property of semantic relationships: If A→B and B→C, then A→C (where → stands for is semantically related to). However, the similarity to the transitive property of equality is not perfect. Two words widely separated in the transitivity chain can have a weaker relationship than closer words. For example, LSA might find that copy → dupl</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, T. K., &amp; Dumais, S. T. (1997). A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2), 211-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>An introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<pages>259--284</pages>
<contexts>
<context position="10659" citStr="Landauer, et al., 1998" startWordPosition="1668" endWordPosition="1671">ystem (Miller, 2003). They are similar in that they are LSA-based, web-based, and provide scaffolding, feedback, and unlimited practice opportunities without increasing a teacher’s workload (Steinhart, 2001). All of them claim that LSA correlates as well to human markers as human markers correlate to one another. See (Miller, 2003) for an excellent analysis of these systems. 4 E-Assessment pilot study Although research using Latent Semantic Analysis (LSA) to assess essays automatically has shown promising results (Chung &amp; O&apos;Neil, 1997; Foltz, et al., 1999; Foltz, 1996; Lemaire &amp; Dessus, 2001; Landauer, et al., 1998; Miller, 2003; Steinhart, 2001; Wade-Stein &amp; Kintsch, 2003), not enough research has been done on using LSA for instructional software (Lemaire &amp; Dessus, 2001). Previous studies involved both young students and university-age students, and several different knowledge domains. An open question is how LSA can be used to improve the learning of universityage, computer science students. This section offers three characteristics that distinguish this research from existing research involving the use of LSA to analyse expository writing texts and reports on a pilot study to determine the feasibilit</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Landauer, T. K., Foltz, P. W., &amp; Laham, D. (1998). An introduction to Latent Semantic Analysis. Discourse Processes, 25, 259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lemaire</author>
<author>P Dessus</author>
</authors>
<title>A system to assess the semantic content of student essays.</title>
<date>2001</date>
<journal>Journal of Educational Computing Research,</journal>
<volume>24</volume>
<issue>3</issue>
<pages>305--320</pages>
<contexts>
<context position="9711" citStr="Lemaire &amp; Dessus, 2001" startWordPosition="1534" endWordPosition="1537">A to provide formative assessment. Section 4 discusses a pilot study that focuses on summative assessment. 3.2 Existing applications Much work is being done in the area of using LSA to mark essays automatically and to provide content-based feedback. One of the great advantages of automatic assessment of essays is its ability to provide helpful, immediate feedback to the learner without burdening the teacher. This application is particularly suited to distance education, where opportunities for one-on-one tutoring are infrequent or non-existent (Steinhart, 2001). Existing systems include Apex (Lemaire &amp; Dessus, 2001), Autotutor (Wiemer-Hastings, Wiemer-Hastings &amp; Graesser, 1999), Intelligent Essay Assessor (Foltz, Laham &amp; Landauer, 1999), Select-a-Kibitzer (Miller, 2003), and Summary Street (Steinhart, 2001; Wade-Stein &amp; Kintsch, 2003). They differ in details of audience addressed, subject domain, and advanced training required by the system (Miller, 2003). They are similar in that they are LSA-based, web-based, and provide scaffolding, feedback, and unlimited practice opportunities without increasing a teacher’s workload (Steinhart, 2001). All of them claim that LSA correlates as well to human markers as</context>
</contexts>
<marker>Lemaire, Dessus, 2001</marker>
<rawString>Lemaire, B., &amp; Dessus, P. (2001). A system to assess the semantic content of student essays. Journal of Educational Computing Research, 24(3), 305-320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ljungstrand</author>
<author>H Johansson</author>
</authors>
<title>Intranet indexing using semantic document clustering. Retrieved 5/4/2004, from http://www.handels.gu.se/epc/archive/00002294/ 01/ljungstrand.IA7400.pdf.</title>
<date>1998</date>
<contexts>
<context position="20371" citStr="Ljungstrand and Johansson (1998)" startWordPosition="3331" endWordPosition="3334">#Tr(tk) where #( tk, dj ) denotes the number of times tk occurs in dj #Tr(tk) denotes the document frequency of term tk, that is, the number of documents in Tr in which tk occurs. Future studies will examine the effects of applying various term weighting functions. 5.3 Similarity measures The pilot study used two different similarity measures. It used the cosine measure to compare the test document with the corpus documents. It used Euclidean distance to choose k, the number of reduced dimensions that produced the best results overall. Other measures exist and will be tried in future studies. Ljungstrand and Johansson (1998) define the following similarity measures: Figure 5. Similarity Measures 5.4 Corpus pre-processing Removing stop words is one type of preprocessing performed for this study. Explicitly adding synonym knowledge and stemming are two additional ways of preparing the corpus that future research will consider. Stemming involves conflating word forms to a common string, e.g., write, writing, writes, written, writer would be represented in the corpus as writ. 5.5 Dimension reduction Choosing the appropriate dimension, k, for reducing the matrices in LSA is a well known open issue. The current consens</context>
</contexts>
<marker>Ljungstrand, Johansson, 1998</marker>
<rawString>Ljungstrand, P., &amp; Johansson, H. (1998, May). Intranet indexing using semantic document clustering. Retrieved 5/4/2004, from http://www.handels.gu.se/epc/archive/00002294/ 01/ljungstrand.IA7400.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schütze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="2119" citStr="Manning and Schütze (1999" startWordPosition="313" endWordPosition="316">e under-researched. First, it uses very small corpora – less than 2,000 words compared to about 11 million words in one of the existing, successful applications (Wade-Stein &amp; Kintsch, 2003). Second, it involves the specific, technical domain of computer science. LSA research usually involves more heterogeneous text with a broad vocabulary. Finally, it focuses on summative assessment where the accuracy of results is paramount. Most LSA research has involved formative assessment for which more general evaluations are sufficient. The study investigates one of the shortcomings of LSA mentioned by Manning and Schütze (1999, p. 564). They report that LSA has high recall but low precision. The precision declines because of spurious co-occurrences. They claim that LSA does better on heterogeneous text with a broad vocabulary. Computer science is a technical domain with a more homogeneous vocabulary, which results, possibly, in fewer spurious cooccurrences. A major question of this research is how LSA will behave when the technique is stretched by applying it to a narrow domain. Section 2 gives the history of LSA and explains how it works. Section 3 describes several existing LSA applications related to e-assessmen</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>Manning, C., &amp; Schütze, H. (1999). Foundations of Statistical Natural Language Processing. Cambridge, Massachusetts: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Miller</author>
</authors>
<title>Essay assessment with Latent Semantic Analysis.</title>
<date>2003</date>
<journal>Journal of Educational Computing Research,</journal>
<volume>28</volume>
<contexts>
<context position="5538" citStr="Miller, 2003" startWordPosition="864" endWordPosition="865">here the columns are documents and the rows are terms (Deerwester, et al., 1990). A term is a subdivision of a document; it can be a word, phrase, or some other unit. A document can be a sentence, a paragraph, a textbook, or some other unit. In other words, documents contain terms. The elements of the matrix are weighted word counts of how many times each term appears in each document. More formally, each element, aij in an i x j matrix is the weighted count of term i in document j. LSA decomposes the matrix into three matrices using Singular Value Decomposition (SVD), a well-known technique (Miller, 2003) that is the general case of factor analysis. Deerwester et. al., (1990) describe the process as follows. Let t = the number of terms, or rows d = the number of documents, or columns X = a t by d matrix Then, after applying SVD, X = TSD, where m = the number of dimensions, m &lt;= min(t,d) T = a t by m matrix S = an m by m diagonal matrix, i.e., only diagonal entries have non-zero values D = an m by d matrix LSA reduces S, the diagonal matrix created by SVD, to an appropriate number of dimensions k, where k &lt;&lt; m, resulting in S&apos;. The product of TS&apos;D is the least-squares best fit to X, the origina</context>
<context position="7536" citStr="Miller, 2003" startWordPosition="1213" endWordPosition="1214">imilarity to the transitive property of equality is not perfect. Two words widely separated in the transitivity chain can have a weaker relationship than closer words. For example, LSA might find that copy → duplicate → double → twin → sibling. Copy and duplicate are much closer semantically than copy and sibling. Finding the correct number of dimensions for the new matrix created by SVD is critical; if it is too small, the structure of the data is not captured. Conversely, if it is too large, sampling error and unimportant details remain, e.g., grammatical variants (Deerwester, et al., 1990; Miller, 2003; Wade-Stein &amp; Kintsch, 2003). Empirical work involving very large corpora shows the correct number of dimensions to be about 300 (Landauer &amp; Dumais, 1997; Wade-Stein &amp; Kintsch, 2003). Creating the matrices using SVD and reducing the number of dimensions, often referred to as training the system, requires a lot of computing power; it can take hours or days to complete the processing (Miller, 2003). Fortunately, once the training is complete, it takes just seconds for LSA to evaluate a text sample (Miller, 2003). 3 Using LSA for assessment 3.1 Types of assessment Electronic feedback, or e-asses</context>
<context position="9868" citStr="Miller, 2003" startWordPosition="1553" endWordPosition="1554">a of using LSA to mark essays automatically and to provide content-based feedback. One of the great advantages of automatic assessment of essays is its ability to provide helpful, immediate feedback to the learner without burdening the teacher. This application is particularly suited to distance education, where opportunities for one-on-one tutoring are infrequent or non-existent (Steinhart, 2001). Existing systems include Apex (Lemaire &amp; Dessus, 2001), Autotutor (Wiemer-Hastings, Wiemer-Hastings &amp; Graesser, 1999), Intelligent Essay Assessor (Foltz, Laham &amp; Landauer, 1999), Select-a-Kibitzer (Miller, 2003), and Summary Street (Steinhart, 2001; Wade-Stein &amp; Kintsch, 2003). They differ in details of audience addressed, subject domain, and advanced training required by the system (Miller, 2003). They are similar in that they are LSA-based, web-based, and provide scaffolding, feedback, and unlimited practice opportunities without increasing a teacher’s workload (Steinhart, 2001). All of them claim that LSA correlates as well to human markers as human markers correlate to one another. See (Miller, 2003) for an excellent analysis of these systems. 4 E-Assessment pilot study Although research using La</context>
</contexts>
<marker>Miller, 2003</marker>
<rawString>Miller, T. (2003). Essay assessment with Latent Semantic Analysis. Journal of Educational Computing Research, 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine Learning in Automated Text Categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<pages>1--47</pages>
<contexts>
<context position="19618" citStr="Sebastiani (2002)" startWordPosition="3212" endWordPosition="3213">5.2.1 Log-entropy Dumais (1991) recommended using log-entropy weighting, which is local weighting times global weighting. Global weighting is defined as 1 – the entropy or noise. Global weighting attempts to quantify the fact that a term appearing in many documents is less important than a term appearing in fewer documents. The log-entropy term weight for term i in doc j = ⎡ tflogf,⎤ log(1 + fij )∗⎢1− gfi or&apos; log(numdocs) ⎥ ⎥⎦ where tfij – term frequency – the frequency of term i in document j gfi – global frequency – the total number of times term i occurs in the whole collection 5.2.2 tfidf Sebastiani (2002) claims the most common weighting is tfidf, or term frequency inverse document frequency. tfidf(tk,dj) = #(tk,dj)∗log Tr #Tr(tk) where #( tk, dj ) denotes the number of times tk occurs in dj #Tr(tk) denotes the document frequency of term tk, that is, the number of documents in Tr in which tk occurs. Future studies will examine the effects of applying various term weighting functions. 5.3 Similarity measures The pilot study used two different similarity measures. It used the cosine measure to compare the test document with the corpus documents. It used Euclidean distance to choose k, the number</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Sebastiani, F. (2002, March). Machine Learning in Automated Text Categorization. ACM Computing Surveys, 34(1), 1-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Steinhart</author>
</authors>
<title>Summary Street: An intelligent tutoring system for improving student writing through the use of Latent Semantic Analysis. Unpublished doctoral dissertation,</title>
<date>2001</date>
<institution>University of Colorado, Boulder, Department of Psychology.</institution>
<contexts>
<context position="9655" citStr="Steinhart, 2001" startWordPosition="1528" endWordPosition="1529">applications described in the next section use LSA to provide formative assessment. Section 4 discusses a pilot study that focuses on summative assessment. 3.2 Existing applications Much work is being done in the area of using LSA to mark essays automatically and to provide content-based feedback. One of the great advantages of automatic assessment of essays is its ability to provide helpful, immediate feedback to the learner without burdening the teacher. This application is particularly suited to distance education, where opportunities for one-on-one tutoring are infrequent or non-existent (Steinhart, 2001). Existing systems include Apex (Lemaire &amp; Dessus, 2001), Autotutor (Wiemer-Hastings, Wiemer-Hastings &amp; Graesser, 1999), Intelligent Essay Assessor (Foltz, Laham &amp; Landauer, 1999), Select-a-Kibitzer (Miller, 2003), and Summary Street (Steinhart, 2001; Wade-Stein &amp; Kintsch, 2003). They differ in details of audience addressed, subject domain, and advanced training required by the system (Miller, 2003). They are similar in that they are LSA-based, web-based, and provide scaffolding, feedback, and unlimited practice opportunities without increasing a teacher’s workload (Steinhart, 2001). All of th</context>
</contexts>
<marker>Steinhart, 2001</marker>
<rawString>Steinhart, D. J. (2001). Summary Street: An intelligent tutoring system for improving student writing through the use of Latent Semantic Analysis. Unpublished doctoral dissertation, University of Colorado, Boulder, Department of Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wade-Stein</author>
<author>E Kintsch</author>
</authors>
<title>Summary Street: Interactive computer support for writing (Tech Report from the</title>
<date>2003</date>
<institution>Institute for Cognitive Science). University of Colorado, USA.</institution>
<contexts>
<context position="1683" citStr="Wade-Stein &amp; Kintsch, 2003" startWordPosition="250" endWordPosition="253">t study undertaken to investigate the feasibility of using Latent Semantic Analysis (LSA) for automatic marking of short essays in the domain of computer science. These short essays are free-form answers to exam questions - not multiple choice questions (MCQ). Exams in the form of MCQs, although easy to mark, do not provide the opportunity for deeper assessment made possible with essays. This study employs LSA in several areas that are under-researched. First, it uses very small corpora – less than 2,000 words compared to about 11 million words in one of the existing, successful applications (Wade-Stein &amp; Kintsch, 2003). Second, it involves the specific, technical domain of computer science. LSA research usually involves more heterogeneous text with a broad vocabulary. Finally, it focuses on summative assessment where the accuracy of results is paramount. Most LSA research has involved formative assessment for which more general evaluations are sufficient. The study investigates one of the shortcomings of LSA mentioned by Manning and Schütze (1999, p. 564). They report that LSA has high recall but low precision. The precision declines because of spurious co-occurrences. They claim that LSA does better on het</context>
<context position="7565" citStr="Wade-Stein &amp; Kintsch, 2003" startWordPosition="1215" endWordPosition="1218">he transitive property of equality is not perfect. Two words widely separated in the transitivity chain can have a weaker relationship than closer words. For example, LSA might find that copy → duplicate → double → twin → sibling. Copy and duplicate are much closer semantically than copy and sibling. Finding the correct number of dimensions for the new matrix created by SVD is critical; if it is too small, the structure of the data is not captured. Conversely, if it is too large, sampling error and unimportant details remain, e.g., grammatical variants (Deerwester, et al., 1990; Miller, 2003; Wade-Stein &amp; Kintsch, 2003). Empirical work involving very large corpora shows the correct number of dimensions to be about 300 (Landauer &amp; Dumais, 1997; Wade-Stein &amp; Kintsch, 2003). Creating the matrices using SVD and reducing the number of dimensions, often referred to as training the system, requires a lot of computing power; it can take hours or days to complete the processing (Miller, 2003). Fortunately, once the training is complete, it takes just seconds for LSA to evaluate a text sample (Miller, 2003). 3 Using LSA for assessment 3.1 Types of assessment Electronic feedback, or e-assessment, is an important compon</context>
<context position="9934" citStr="Wade-Stein &amp; Kintsch, 2003" startWordPosition="1560" endWordPosition="1563">rovide content-based feedback. One of the great advantages of automatic assessment of essays is its ability to provide helpful, immediate feedback to the learner without burdening the teacher. This application is particularly suited to distance education, where opportunities for one-on-one tutoring are infrequent or non-existent (Steinhart, 2001). Existing systems include Apex (Lemaire &amp; Dessus, 2001), Autotutor (Wiemer-Hastings, Wiemer-Hastings &amp; Graesser, 1999), Intelligent Essay Assessor (Foltz, Laham &amp; Landauer, 1999), Select-a-Kibitzer (Miller, 2003), and Summary Street (Steinhart, 2001; Wade-Stein &amp; Kintsch, 2003). They differ in details of audience addressed, subject domain, and advanced training required by the system (Miller, 2003). They are similar in that they are LSA-based, web-based, and provide scaffolding, feedback, and unlimited practice opportunities without increasing a teacher’s workload (Steinhart, 2001). All of them claim that LSA correlates as well to human markers as human markers correlate to one another. See (Miller, 2003) for an excellent analysis of these systems. 4 E-Assessment pilot study Although research using Latent Semantic Analysis (LSA) to assess essays automatically has sh</context>
</contexts>
<marker>Wade-Stein, Kintsch, 2003</marker>
<rawString>Wade-Stein, D., &amp; Kintsch, E. (2003). Summary Street: Interactive computer support for writing (Tech Report from the Institute for Cognitive Science). University of Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wiemer-Hastings</author>
<author>K Wiemer-Hastings</author>
<author>A C Graesser</author>
</authors>
<title>Improving an intelligent tutor&apos;s comprehension of students with Latent Semantic Analysis.</title>
<date>1999</date>
<booktitle>In S. Lajoie &amp; M. Vivet (Eds.), Artificial Intelligence in Education.</booktitle>
<publisher>IOS Press.</publisher>
<location>Amsterdam:</location>
<marker>Wiemer-Hastings, Wiemer-Hastings, Graesser, 1999</marker>
<rawString>Wiemer-Hastings, P., Wiemer-Hastings, K., &amp; Graesser, A. C. (1999). Improving an intelligent tutor&apos;s comprehension of students with Latent Semantic Analysis. In S. Lajoie &amp; M. Vivet (Eds.), Artificial Intelligence in Education. Amsterdam: IOS Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>