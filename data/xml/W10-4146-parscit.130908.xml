<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003833">
<title confidence="0.997866">
Dependency Parser for Chinese Constituent Parsing ∗
</title>
<author confidence="0.998934">
Xuezhe Ma, Xiaotian Zhang, Hai Zhao, Bao-Liang Lu
</author>
<affiliation confidence="0.99684875">
1Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai 200240, China
</affiliation>
<email confidence="0.99663">
{xuezhe.ma,xtian.zh}@gmail.com, {zhaohai,blu}@cs.sjtu.edu.cn
</email>
<sectionHeader confidence="0.993856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893538461539">
This paper presents our work for participation
in the 2010 CIPS-ParsEval shared task on Chi-
nese syntactic constituent tree parsing. We
use dependency parsers for this constituent
parsing task based on a formal dependency-
constituent transformation method which con-
verts dependency to constituent structures us-
ing a machine learning approach. A condi-
tional random fields (CRF) tagger is adopted
for head information recognition. Our ex-
periments shows that acceptable parsing and
head tagging results are obtained on our ap-
proaches.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999873782608696">
Constituent parsing is a challenging but useful task
aiming at analyzing the constituent structure of a sen-
tence. Recently, it is widely adopted by the popular ap-
plications of natural language processing techniques,
such as machine translation (Ding and Palmer, 2005),
synonym generation (Shinyama et al., 2002), relation
extraction (Culotta and Sorensen, 2004) and lexical re-
source augmentation (Snow et al., 2004). A great deal
of researches have been conducted on this topic with
promising progress (Magerman, 1995; Collins, 1999;
Charniak, 2000; Charniak and Johnson, 2005; Sagae
and Lavie, 2006; Petrov and Klein, 2007; Finkel et al.,
2008; Huang, 2008).
Recently, several effective dependency parsing al-
gorithms has been developed and shows excellent per-
formance in the responding parsing tasks (McDonald,
2006; Nivre and Scholz, 2004). Since graph struc-
tures of dependency and constituent parsing over a
sentence are strongly related, they should be benefited
from each other. It is true that constituent parsing may
be smoothly altered to fit dependency parsing. How-
ever, due to the inconvenience from dependency to
constituent structure, it is not so easy to adopt the latter
</bodyText>
<footnote confidence="0.9830225">
∗This work is partially supported by the National Natu-
ral Science Foundation of China (Grant No. 60903119, Grant
No. 60773090 and Grant No. 90820018), the National Ba-
sic Research Program of China (Grant No. 2009CB320901),
and the National High-Tech Research Program of China
(Grant No.2008AA02Z315).
</footnote>
<bodyText confidence="0.999977888888889">
for the former. This means that most of these popular
and effective dependency parsing models can not be di-
rectly extended to constituents parsing. This paper pro-
poses an formal method for such a conversion which
adoptively solves the problem of ambiguity. Based on
the proposed method, a dependency parsing algorithm
can be used to solve tasks of constituent parsing.
A part of Tsinghua Chinese Treebank (TCT) (Zhou,
2004; Zhou, 2007; Chen et al., 2008) is used as
the training and test data for the 2010 CIPS-ParsEval
shared task. Being different from the annotation
scheme of the Penn Chinese Treebank (CTB), the TCT
has another annotation scheme, which combines both
the constituent tree structure and the head informa-
tion of each constituent. Specifically, there can be al-
ways multiple heads in a constituent. For the 2010
CIPS-ParsEval shared task, only segmented sentences
are given in test data without part-of-speech (POS)
tags, a POS tagger is required for this task. There-
fore, we divide our system into three major cascade
stages, namely POS tagging, constituent parsing and
head information recognition, which are connected as
a pipeline of processing. For the POS tagging, we
adopt the SVMTool tagger (Gimenez and Marquez,
2004); for the constituent parsing, we use the Maxi-
mum Spanning Tree (MST) (McDonald, 2006) parser
combined with a dependencies-to-constituents conver-
sion; and for the head information recognition, we ap-
ply a sequence labeling method to label head informa-
tion.
Section 2 presents the POS tagger in our approach.
The details of our parsing method is presented in sec-
tion 3. The head information recognition is described
in section 4. The data and experimental results are
shown in section 5. The last section is the conclusion
and future work.
</bodyText>
<sectionHeader confidence="0.936119" genericHeader="method">
2 POS Tagging
</sectionHeader>
<footnote confidence="0.724649714285714">
The SVMTool tagger (Gimenez and Marquez, 2004) is
used as our POS tagging tool for the first stage. It is a
POS tagger based on SVM classifier, written in Perl. It
can be trained on standardized collection of hand POS-
tagged sentences. It uses SVM-Light1 toolkit as the
1http://www.cs.cornell.edu/People/tj/
svm_light/.
</footnote>
<bodyText confidence="0.999621">
implementation of SVM classifier and achieves 97.2%
accuracy on the Penn English Treebank. We test the
accuracy of the SVMTool tagger on the development
set of the TCT (see section 5.1) and achieve accuracy
of 94.98%.
</bodyText>
<sectionHeader confidence="0.95983" genericHeader="method">
3 Parsing Constituents Using
Dependency Parsing Algorithms
</sectionHeader>
<subsectionHeader confidence="0.999811">
3.1 Convert Dependencies to Constituents
</subsectionHeader>
<bodyText confidence="0.999964916666667">
The conversion from constituent to dependency struc-
tures is straightforward with some specific rules based
on linguistic theory. However, there is not an effective
method which can accurately accomplish the opposite
transformation, from the dependency structures back
into constituent ones due to the existence of ambiguity
introduced by the former transformation.
Aimed at the above difficulty, our solution is to in-
troduce a formal dependency structure and a machine
learning method so that the ambiguity from depen-
dency structures to constituent structures can be dealt
with automatically.
</bodyText>
<subsectionHeader confidence="0.934312">
3.1.1 Binarization
</subsectionHeader>
<bodyText confidence="0.999983928571429">
We first transform constituent trees into the form
that all productions for all subtrees are either unary or
binary, before converting them to dependency struc-
tures. Due to the binarization, the target constituent
trees of the conversion from dependency back to con-
stituent structures are binary branching.
This binarization is done by the left-factoring ap-
proach described in (Charniak et al., 1998; Petrov and
Klein, 2008), which converts each production with n
children, where n &gt; 2, into n − 1 binary productions.
Additional non-terminal nodes introduced in this con-
version must be clearly marked. Transforming the bi-
nary branching trees into arbitrary branching trees is
accomplished by using the reverse process.
</bodyText>
<subsectionHeader confidence="0.987097">
3.1.2 Using Binary Classifier
</subsectionHeader>
<bodyText confidence="0.9999038">
We train a classifier to decide which dependency
edges should be transformed first at each step of con-
version automatically. After the binarization described
in the previous section, only one dependency edge
should be transformed at each step. Therefore the
classifier only need to decide which dependency edge
should be transformed at each step during the conver-
sion.
As a result of the projective property of constituent
structures, this problem only happens in the cases that
modifiers are at both sides of their heads. And for these
cases that one head has multiple modifiers, only the
leftmost or the rightmost dependency edge could be
transformed first. Therefore, a binary classifier is al-
ways enough for the disambiguation at each step.
</bodyText>
<listItem confidence="0.99642275">
1. Word form of the parent
2. Part-of-speech (POS) tag of the parent
3. Word form of the leftmost child
4. POS tag of the leftmost child
5. Dependency label of the leftmost child
6. Word form of the rightmost child
7. POS tag of the rightmost child
8. Dependency label of the rightmost child
9. Distance between the leftmost child and
the parent
10. Distance between the rightmost child
and the parent
</listItem>
<tableCaption confidence="0.998149">
Table 1: Features used for conversion classifier.
</tableCaption>
<bodyText confidence="0.989363">
Support Vector Machine (SVM) is adopted as the
learning algorithm for the binary classifier and the fea-
tures are in Table 1.
</bodyText>
<subsectionHeader confidence="0.924379">
3.1.3 Convert Constituent Labels
</subsectionHeader>
<bodyText confidence="0.999981294117647">
The rest problem is that we should restore the label
for each constituent when dependency structure trees
are again converted to constituent structures. The prob-
lem is solved by storing constituent labels as labels of
dependency types. The label for each constituent is just
used as the label dependency type for each dependency
edge.
The conversion method is tested on the develop-
ment, too. Constituent trees are firstly converted into
dependency structures using the head rules described
in (Li and Zhou, 2009). Then, we transform those
trees back to constituent structure using our conversion
method and use the PARSEVAL (Black et al., 1991)
measures to evaluate the performance of the conver-
sion method. Our conversion method obtains 99.76%
precision and 99.76% recall, which is a great perfor-
mance.
</bodyText>
<subsectionHeader confidence="0.999748">
3.2 Dependency Parser for Constituent Parsing
</subsectionHeader>
<bodyText confidence="0.999764923076923">
Based on the proposed conversion method, depen-
dency parsing algorithms can be used for constituent
parsing. This can be done by firstly transforming train-
ing data from constituents into dependencies and ex-
tract training instances to train a binary classifier for
dependency-constituent conversion, then training a de-
pendency parser using the transformed training data.
On the test step, parse the test data using the depen-
dency parser and convert output dependencies to con-
stituents using the binary classifier trained in advance.
In addition, since our conversion method needs depen-
dency types, labeled dependency parsing algorithms
are always required.
</bodyText>
<listItem confidence="0.998640666666667">
1. Constituent label of the constituent
2. Constituent label of each child of
the constituent.
3. Wether it is a terminal for each
child of the constituent
4. The leftmost word in the sentence
of each child of the constituent.
5. The leftmost word in the sentence
of each child of the constituent.
</listItem>
<tableCaption confidence="0.512897">
Table 2: CRF features for head information recogni-
tion.
</tableCaption>
<listItem confidence="0.996805">
1. Word form and POS tag of the parent.
2. Word form and POS tag of each child.
3. POS tag of the leftmost child of each child.
4. POS tag of the rightmost child of each child.
5. Dependency label between the parent and
its parent
</listItem>
<tableCaption confidence="0.986482">
Table 3: CRF features for dependency type labeling.
</tableCaption>
<sectionHeader confidence="0.968148" genericHeader="method">
4 Head Information Recognition
</sectionHeader>
<bodyText confidence="0.999987619047619">
Since head information of each constituent is always
determined by the syntactic label of its own and the
categories of the constituents in subtrees, the order and
relations between the productions of each constituent
strongly affects the head information labeling. It is
natural to apply a sequential labeling strategy to tackle
this problem. The linear chain CRF model is adopted
for the head information labeling, and the implemen-
tation of CRF model we used is the 0.53 version of
the CRF++ toolkit2. We assume that head information
is independent between different constituents, which
could decrease the length of sequence to be labeled for
the CRF model.
We use a binary tag set to determine whether a con-
stituent is a head, e.g. H for a head, O for a non-head,
which is the same as (Song and Kit, 2009). The fea-
tures in Table 2 are used for CRF model.
To test our CRF tagger, we remove all head informa-
tion from the development set, and use the CRF tagger
to retrieve the head. The result strongly proves its ef-
fectiveness by showing an accuracy of 99.52%.
</bodyText>
<sectionHeader confidence="0.998411" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.8120445">
All experiments reported here were performed on a
Core 2 Quad 2.83Ghz CPU with 8GB of RAM.
</bodyText>
<footnote confidence="0.9485635">
2The CRF++ toolkit is publicly available from
http://crfpp.sourceforge.net/.
</footnote>
<subsectionHeader confidence="0.959238">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999589">
There are 37,219 short sentences in official released
training data for the first sub-task and 17,744 long sen-
tences for the second sub-task (for the second sub-task,
one line in the training data set may contain more than
one sentence). We split one eighth of the data as our
development set. On the other hand, there are both
1,000 sentences in released test data for the first and
second sub-tasks.
</bodyText>
<subsectionHeader confidence="0.999219">
5.2 Constituent Parsing
</subsectionHeader>
<bodyText confidence="0.9999307">
As mentioned in section 3, constituent parsing is
done by using a dependency parser combined with
our conversion method. We choose the second or-
der maximum spanning tree parser with k-best online
large-margin learning algorithm (Crammer and Singer,
2003; Crammer et al., 2003). The MST parser we use
is in the form of an open source program implemented
in C++3.
The features used for MST parser is the same as
(McDonald, 2006). Both the single-stage and two-
stage dependency type labeling approaches are applied
in our experiments. For the two-stage dependency type
labeling, The linear chain CRF model is adopted in-
stead of the first-order Markov model used in (McDon-
ald, 2006). The features in Table 3 are used for CRF
model. It takes about 7 hours for training the MST
parser, and about 24 hours for training the CRF model.
As mentioned in section 3.1.2, SVM is adopted as
the learning algorithm for the binary classifier. There
are about 40,000 training instances in the first sub-task
and about 80,000 in the second sub-task. Develop-
ment sets are used for tuning parameter C of SVM
and the training time of the SVM classifier for the first
and second sub-task is about 8 and 24 hours, respec-
tively. However, the conversion from dependencies to
constituents is extremely fast. Converting more than
2,000 trees takes less than 1 second.
To transform the constituent trees in training set into
dependency structures, we use the head rules of (Li and
Zhou, 2009).
</bodyText>
<subsectionHeader confidence="0.624146">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.99901">
The evaluation metrics used in 2010 CIPS-ParsEval
shared task is shown in following:
</bodyText>
<footnote confidence="0.461711222222222">
1. syntactic parsing
Precision = number of correct constituents in proposed parse
number of constituents in proposed parse
Recall = number of correct constituents in proposed parse
number of constituents in standard parse
F1 = 2*Precision*Recall
Precision+Recall
3The Max-MSTParser is publicly available from
http://max-mstparser.sourceforge.net/.
</footnote>
<table confidence="0.999784">
Precision without head F1 Precision with head F1
Recall Recall
single-stage 77.78 78.13 77.96 75.78 76.13 75.95
two-stage 78.61 78.76 78.69 76.61 76.75 76.68
</table>
<tableCaption confidence="0.99912">
Table 4: Official scores of syntactic parsing. single-stage and two-stage are for single-stage and two-stage depen-
dency type labeling approached, respectively.
Table 5: Official scores of event recognition
</tableCaption>
<figure confidence="0.9761174">
Micro-R Macro-R
single-stage
two-stage
62.74 62.47
63.14 62.48
</figure>
<bodyText confidence="0.975847">
The correctness of syntactic constituents is judged
based on the following two criteria:
</bodyText>
<listItem confidence="0.622534916666667">
(a) the boundary, the POS tags of all the words
in the constituent and the constituent type la-
bel should match that of the constituent in
the gold standard data.
(b) the boundary, the POS tags of all the words
in the constituent, the constituent type la-
bel and head child index of the constituent
should match that of the constituent in the
gold standard data. (if the constituent con-
tains more than one head child index, at least
one of them should be correct.)
2. event pattern recognition
</listItem>
<equation confidence="0.831642">
Micro-R = number of all correct events in proposed parse
number of all events in standard parse
Macro-R = sum of recall of different target verbs
</equation>
<bodyText confidence="0.9978805">
number of target verbs
Here the event pattern of a sentence is defined to
be the sequence of event blocks controlled by the
target verb in a sentence. The criteria for judging
the correctness of event pattern recognition is:
• the event pattern should be completely con-
sistent with gold standard data (information
of each event block should completely match
and the order of event blocks should also
consistent).
There are both two submissions for the first and sec-
ond sub-tasks. One is using the single-stage depen-
dency type labeling and the other is two-stage. Since
there are some mistakes in our models for the second
sub-task, the results of our submissions are unexpect-
edly poor and are not shown in this paper. All the re-
sults in this paper is reported by the official organizer
of the 2010 CIPS-ParsEval shared task.
The accuracy of POS tagging on the official test data
is 92.77%. The results of syntactic parsing for the first
sub-task is shown in Table 4. And results of event
recognition is shown in Table 5.
From the Table 4 and 5, we can see that our system
achieves acceptable parsing and head tagging results,
and the results of event recognition is also reasonably
high.
</bodyText>
<subsectionHeader confidence="0.999712">
5.4 Comparison with Previous Works
</subsectionHeader>
<bodyText confidence="0.99995925">
We comparison our approach with previous works of
2009 CIPS-ParsEval shared task. The data set and
evaluation measures of 2009 CIPS-ParsEval shared
task, which are quite different from that of 2010 CIPS-
ParsEval shared task, are used in this experiment for
the comparison purpose. Table 6 shows the compari-
son.
We compare our method with several main parsers
on the official data set of 2009 CIPS-ParsEval shared
task. All these results are evaluated with official
evaluation tool by the 2009 CIPS-ParsEval shared
task. Bikel’s parser4 (Bikel, 2004) in Table 6 is
a implementation of Collins’ head-driven statistical
model (Collins, 2003). The Stanford parser5 is based
on the factored model described in (Klein and Man-
ning, 2002). The Charniak’s parser6 is based on the
parsing model described in (Charniak, 2000). Berke-
ley parser7 is based on unlexicalized parsing model de-
scribed in (Petrov and Klein, 2007). According to Ta-
ble 6, the performance of our method is better than all
the four parsers described above. Chen et al. (2009)
and Jiang et al. (2009) both make use of combination
of multiple parsers and achieve considerably high per-
formance.
</bodyText>
<footnote confidence="0.997434666666667">
4http://www.cis.upenn.edu/˜dbikel/
software.html
5http://nlp.stanford.edu/software/
lex-parser.shtml/
6ftp://ftp.cs.brown.edu/pub/nlparser/
7http://nlp.cs.berkeley.edu/Main.html
</footnote>
<table confidence="0.994739625">
F1
Bikel’s parser 81.8
Stanford parser 83.3
Charniak’s parser 83.9
Berkeley parser 85.2
this paper 85.6
Jiang et al (2009). 87.2
Chen et al (2009). 88.8
</table>
<tableCaption confidence="0.999904">
Table 6: Comparison with previous works
</tableCaption>
<sectionHeader confidence="0.996896" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999898">
This paper describes our approaches for the parsing
task in CIPS-ParsEval 2010 shared task. A pipeline
system is used to solve the POS tagging, constituent
parsing and head information recognition. SVMTool
tagger is used for the POS tagging. For constituent
parsing, we proposes a conversion based method,
which can use dependency parsers for constituent pars-
ing. MST parser is chosen as our dependency parser.
A CRF tagger is used for head information recognition.
The official scores indicate that our system obtains ac-
ceptable results on constituent parsing and high perfor-
mance on head information tagging.
One of future work should apply parser combination
and reranking approaches to leverage this in producing
more accurate parsers.
</bodyText>
<sectionHeader confidence="0.998594" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998740514285714">
Bikel, Daniel M. 2004. Intricacies of collins parsing
model. Computational Linguistics, 30(4):480–511.
Black, Ezra W., Steven P. Abney, Daniel P. Flickinger,
Cluadia Gdaniec, Ralph Grishman, Philio Harrison,
Donald Hindle, Robert J.P. Inqria, Frederick Jelinek,
Judith L. Klavans, Mark Y. Liberman, Mitchell P.
Marcus, Salim Roukos, and B Santorini. 1991. A
procedure for quantitatively comparing the syntac-
tic coverage of english grammars. In Proceedings
of the February 1991 DARPA Speech and Natural
Language Workshop.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the 43rd ACLL, pages
132–139.
Charniak, Eugene, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132–139, seattle, WA.
Chen, Yi, Qiang Zhou, and Hang Yu. 2008. Anal-
ysis of the hierarchical Chinese funcitional chunk
bank. Journal of Chinese Information Processing,
22(3):24–31.
Chen, Xiao, Changning Huang, Mu Li, and Chunyu
Kit. 2009. Better parser combination. In CIPS-
ParsEval-2009 shared task.
Collins, Michael. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589–637.
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learining.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2003. Online
passive aggressive algorithms. In Proceedings of
NIPS.
Culotta, Aron and Jeffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of ACL.
Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL.
Finkel, Jenny Rose, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. pages 959–967, The
Ohio State University, Columbus, Ohio, USA.
Gimenez and Marquez. 2004. Svmtool: A general
POS tagger generator based on support vector ma-
chines. In Proceedings of the 4th International Con-
ference of Language Resources and Evaluation, Lis-
bon, Portugal.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL/HLT.
Jiang, Wenbin, Hao Xiong, and Qun Liu. 2009. Muti-
path shift-reduce parsing with online training. In
CIPS-ParsEval-2009 shared task.
Klein, Dan and Christopher Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In In Advances in NIPS 2002, pages
3–10.
Li, Junhui and Guodong Zhou. 2009. Soochow uni-
versity report for the 1st china workshop on syntac-
tic parsing. In CIPS-ParsEval-2009 shared task.
Magerman, David M. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276–283, MIT, Cambridge, Massachusetts, USA.
McDonald, Ryan. 2006. Discriminative Learning
Spanning Tree Algorithm for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nivre, Joakim and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics (COLING-2004), pages 64–70,
Geneva, Switzerland, August 23rd-27th.
Petrov, Slav and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404–411, Rochester, New York.
Petrov, Slav and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. In Proceed-
ings of NIPS 20.
Sagae, Kenji and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of COL-
ING/ACL, pages 689–691, Sydney, Australia.
Shinyama, Yusuke, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In HLT-2002.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.
2004. Learning syntactic patterns for automatic hy-
pernym discovery. In Proceedings of NIPS.
Song, Yan and Chunyu Kit. 2009. PCFG parsing with
crf tagging for head recognition. In CIPS-ParsEval-
2009 shared task.
Zhou, Qiang. 2004. Annotation scheme for Chinese
treebank. Journal of Chinese Information Process-
ing, 18(4):1–8.
Zhou, Qiang. 2007. Base chuck scheme for the Chi-
nese language. Journal of Chinese Information Pro-
cessing, 21(3):21–27.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.263794">
<title confidence="0.5650998">Parser for Chinese Constituent Parsing Xuezhe Ma, Xiaotian Zhang, Hai Zhao, Bao-Liang for Brain-Like Computing and Machine Department of Computer Science and Engineering, Shanghai Jiao Tong Key Laboratory for Intelligent Computing and Intelligent</title>
<author confidence="0.521325">Shanghai Jiao Tong University</author>
<author confidence="0.521325">Dong Chuan Rd</author>
<author confidence="0.521325">Shanghai</author>
<abstract confidence="0.988875142857143">This paper presents our work for participation in the 2010 CIPS-ParsEval shared task on Chinese syntactic constituent tree parsing. We use dependency parsers for this constituent parsing task based on a formal dependencyconstituent transformation method which converts dependency to constituent structures using a machine learning approach. A conditional random fields (CRF) tagger is adopted for head information recognition. Our experiments shows that acceptable parsing and head tagging results are obtained on our approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Intricacies of collins parsing model.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="16326" citStr="Bikel, 2004" startWordPosition="2618" endWordPosition="2619">ion is also reasonably high. 5.4 Comparison with Previous Works We comparison our approach with previous works of 2009 CIPS-ParsEval shared task. The data set and evaluation measures of 2009 CIPS-ParsEval shared task, which are quite different from that of 2010 CIPSParsEval shared task, are used in this experiment for the comparison purpose. Table 6 shows the comparison. We compare our method with several main parsers on the official data set of 2009 CIPS-ParsEval shared task. All these results are evaluated with official evaluation tool by the 2009 CIPS-ParsEval shared task. Bikel’s parser4 (Bikel, 2004) in Table 6 is a implementation of Collins’ head-driven statistical model (Collins, 2003). The Stanford parser5 is based on the factored model described in (Klein and Manning, 2002). The Charniak’s parser6 is based on the parsing model described in (Charniak, 2000). Berkeley parser7 is based on unlexicalized parsing model described in (Petrov and Klein, 2007). According to Table 6, the performance of our method is better than all the four parsers described above. Chen et al. (2009) and Jiang et al. (2009) both make use of combination of multiple parsers and achieve considerably high performanc</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Bikel, Daniel M. 2004. Intricacies of collins parsing model. Computational Linguistics, 30(4):480–511.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ezra W Black</author>
<author>Steven P Abney</author>
<author>Daniel P Flickinger</author>
<author>Cluadia Gdaniec</author>
<author>Ralph Grishman</author>
<author>Philio Harrison</author>
<author>Donald Hindle</author>
<author>Robert J P Inqria</author>
<author>Frederick Jelinek</author>
<author>Judith L Klavans</author>
<author>Mark Y Liberman</author>
<author>Mitchell P Marcus</author>
<author>Salim Roukos</author>
<author>B Santorini</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of english grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the February</booktitle>
<contexts>
<context position="8283" citStr="Black et al., 1991" startWordPosition="1283" endWordPosition="1286">we should restore the label for each constituent when dependency structure trees are again converted to constituent structures. The problem is solved by storing constituent labels as labels of dependency types. The label for each constituent is just used as the label dependency type for each dependency edge. The conversion method is tested on the development, too. Constituent trees are firstly converted into dependency structures using the head rules described in (Li and Zhou, 2009). Then, we transform those trees back to constituent structure using our conversion method and use the PARSEVAL (Black et al., 1991) measures to evaluate the performance of the conversion method. Our conversion method obtains 99.76% precision and 99.76% recall, which is a great performance. 3.2 Dependency Parser for Constituent Parsing Based on the proposed conversion method, dependency parsing algorithms can be used for constituent parsing. This can be done by firstly transforming training data from constituents into dependencies and extract training instances to train a binary classifier for dependency-constituent conversion, then training a dependency parser using the transformed training data. On the test step, parse t</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Inqria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, 1991</marker>
<rawString>Black, Ezra W., Steven P. Abney, Daniel P. Flickinger, Cluadia Gdaniec, Ralph Grishman, Philio Harrison, Donald Hindle, Robert J.P. Inqria, Frederick Jelinek, Judith L. Klavans, Mark Y. Liberman, Mitchell P. Marcus, Salim Roukos, and B Santorini. 1991. A procedure for quantitatively comparing the syntactic coverage of english grammars. In Proceedings of the February 1991 DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine-grained n-best parsing and discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACLL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1583" citStr="Charniak and Johnson, 2005" startWordPosition="219" endWordPosition="222">ts are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt th</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, Eugene and Mark Johnson. 2005. Coarseto-fine-grained n-best parsing and discriminative reranking. In Proceedings of the 43rd ACLL, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Edge-based best-first chart parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="5934" citStr="Charniak et al., 1998" startWordPosition="900" endWordPosition="903">ulty, our solution is to introduce a formal dependency structure and a machine learning method so that the ambiguity from dependency structures to constituent structures can be dealt with automatically. 3.1.1 Binarization We first transform constituent trees into the form that all productions for all subtrees are either unary or binary, before converting them to dependency structures. Due to the binarization, the target constituent trees of the conversion from dependency back to constituent structures are binary branching. This binarization is done by the left-factoring approach described in (Charniak et al., 1998; Petrov and Klein, 2008), which converts each production with n children, where n &gt; 2, into n − 1 binary productions. Additional non-terminal nodes introduced in this conversion must be clearly marked. Transforming the binary branching trees into arbitrary branching trees is accomplished by using the reverse process. 3.1.2 Using Binary Classifier We train a classifier to decide which dependency edges should be transformed first at each step of conversion automatically. After the binarization described in the previous section, only one dependency edge should be transformed at each step. Theref</context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>Charniak, Eugene, Sharon Goldwater, and Mark Johnson. 1998. Edge-based best-first chart parsing. In Proceedings of the Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>132--139</pages>
<location>seattle, WA.</location>
<contexts>
<context position="1555" citStr="Charniak, 2000" startWordPosition="217" endWordPosition="218">ad tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, i</context>
<context position="16591" citStr="Charniak, 2000" startWordPosition="2660" endWordPosition="2661">CIPSParsEval shared task, are used in this experiment for the comparison purpose. Table 6 shows the comparison. We compare our method with several main parsers on the official data set of 2009 CIPS-ParsEval shared task. All these results are evaluated with official evaluation tool by the 2009 CIPS-ParsEval shared task. Bikel’s parser4 (Bikel, 2004) in Table 6 is a implementation of Collins’ head-driven statistical model (Collins, 2003). The Stanford parser5 is based on the factored model described in (Klein and Manning, 2002). The Charniak’s parser6 is based on the parsing model described in (Charniak, 2000). Berkeley parser7 is based on unlexicalized parsing model described in (Petrov and Klein, 2007). According to Table 6, the performance of our method is better than all the four parsers described above. Chen et al. (2009) and Jiang et al. (2009) both make use of combination of multiple parsers and achieve considerably high performance. 4http://www.cis.upenn.edu/˜dbikel/ software.html 5http://nlp.stanford.edu/software/ lex-parser.shtml/ 6ftp://ftp.cs.brown.edu/pub/nlparser/ 7http://nlp.cs.berkeley.edu/Main.html F1 Bikel’s parser 81.8 Stanford parser 83.3 Charniak’s parser 83.9 Berkeley parser 8</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, Eugene. 2000. A maximum-entropyinspired parser. In Proceedings of NAACL, pages 132–139, seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Chen</author>
<author>Qiang Zhou</author>
<author>Hang Yu</author>
</authors>
<title>Analysis of the hierarchical Chinese funcitional chunk bank.</title>
<date>2008</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="2946" citStr="Chen et al., 2008" startWordPosition="435" endWordPosition="438">No. 90820018), the National Basic Research Program of China (Grant No. 2009CB320901), and the National High-Tech Research Program of China (Grant No.2008AA02Z315). for the former. This means that most of these popular and effective dependency parsing models can not be directly extended to constituents parsing. This paper proposes an formal method for such a conversion which adoptively solves the problem of ambiguity. Based on the proposed method, a dependency parsing algorithm can be used to solve tasks of constituent parsing. A part of Tsinghua Chinese Treebank (TCT) (Zhou, 2004; Zhou, 2007; Chen et al., 2008) is used as the training and test data for the 2010 CIPS-ParsEval shared task. Being different from the annotation scheme of the Penn Chinese Treebank (CTB), the TCT has another annotation scheme, which combines both the constituent tree structure and the head information of each constituent. Specifically, there can be always multiple heads in a constituent. For the 2010 CIPS-ParsEval shared task, only segmented sentences are given in test data without part-of-speech (POS) tags, a POS tagger is required for this task. Therefore, we divide our system into three major cascade stages, namely POS </context>
</contexts>
<marker>Chen, Zhou, Yu, 2008</marker>
<rawString>Chen, Yi, Qiang Zhou, and Hang Yu. 2008. Analysis of the hierarchical Chinese funcitional chunk bank. Journal of Chinese Information Processing, 22(3):24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Chen</author>
<author>Changning Huang</author>
<author>Mu Li</author>
<author>Chunyu Kit</author>
</authors>
<title>Better parser combination.</title>
<date>2009</date>
<booktitle>In CIPSParsEval-2009 shared task.</booktitle>
<contexts>
<context position="16812" citStr="Chen et al. (2009)" startWordPosition="2697" endWordPosition="2700">. All these results are evaluated with official evaluation tool by the 2009 CIPS-ParsEval shared task. Bikel’s parser4 (Bikel, 2004) in Table 6 is a implementation of Collins’ head-driven statistical model (Collins, 2003). The Stanford parser5 is based on the factored model described in (Klein and Manning, 2002). The Charniak’s parser6 is based on the parsing model described in (Charniak, 2000). Berkeley parser7 is based on unlexicalized parsing model described in (Petrov and Klein, 2007). According to Table 6, the performance of our method is better than all the four parsers described above. Chen et al. (2009) and Jiang et al. (2009) both make use of combination of multiple parsers and achieve considerably high performance. 4http://www.cis.upenn.edu/˜dbikel/ software.html 5http://nlp.stanford.edu/software/ lex-parser.shtml/ 6ftp://ftp.cs.brown.edu/pub/nlparser/ 7http://nlp.cs.berkeley.edu/Main.html F1 Bikel’s parser 81.8 Stanford parser 83.3 Charniak’s parser 83.9 Berkeley parser 85.2 this paper 85.6 Jiang et al (2009). 87.2 Chen et al (2009). 88.8 Table 6: Comparison with previous works 6 Conclusion This paper describes our approaches for the parsing task in CIPS-ParsEval 2010 shared task. A pipel</context>
</contexts>
<marker>Chen, Huang, Li, Kit, 2009</marker>
<rawString>Chen, Xiao, Changning Huang, Mu Li, and Chunyu Kit. 2009. Better parser combination. In CIPSParsEval-2009 shared task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1539" citStr="Collins, 1999" startWordPosition="215" endWordPosition="216"> parsing and head tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constitu</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, Michael. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="16415" citStr="Collins, 2003" startWordPosition="2631" endWordPosition="2632">ach with previous works of 2009 CIPS-ParsEval shared task. The data set and evaluation measures of 2009 CIPS-ParsEval shared task, which are quite different from that of 2010 CIPSParsEval shared task, are used in this experiment for the comparison purpose. Table 6 shows the comparison. We compare our method with several main parsers on the official data set of 2009 CIPS-ParsEval shared task. All these results are evaluated with official evaluation tool by the 2009 CIPS-ParsEval shared task. Bikel’s parser4 (Bikel, 2004) in Table 6 is a implementation of Collins’ head-driven statistical model (Collins, 2003). The Stanford parser5 is based on the factored model described in (Klein and Manning, 2002). The Charniak’s parser6 is based on the parsing model described in (Charniak, 2000). Berkeley parser7 is based on unlexicalized parsing model described in (Petrov and Klein, 2007). According to Table 6, the performance of our method is better than all the four parsers described above. Chen et al. (2009) and Jiang et al. (2009) both make use of combination of multiple parsers and achieve considerably high performance. 4http://www.cis.upenn.edu/˜dbikel/ software.html 5http://nlp.stanford.edu/software/ le</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Collins, Michael. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learining.</journal>
<contexts>
<context position="11745" citStr="Crammer and Singer, 2003" startWordPosition="1861" endWordPosition="1864">raining data for the first sub-task and 17,744 long sentences for the second sub-task (for the second sub-task, one line in the training data set may contain more than one sentence). We split one eighth of the data as our development set. On the other hand, there are both 1,000 sentences in released test data for the first and second sub-tasks. 5.2 Constituent Parsing As mentioned in section 3, constituent parsing is done by using a dependency parser combined with our conversion method. We choose the second order maximum spanning tree parser with k-best online large-margin learning algorithm (Crammer and Singer, 2003; Crammer et al., 2003). The MST parser we use is in the form of an open source program implemented in C++3. The features used for MST parser is the same as (McDonald, 2006). Both the single-stage and twostage dependency type labeling approaches are applied in our experiments. For the two-stage dependency type labeling, The linear chain CRF model is adopted instead of the first-order Markov model used in (McDonald, 2006). The features in Table 3 are used for CRF model. It takes about 7 hours for training the MST parser, and about 24 hours for training the CRF model. As mentioned in section 3.1</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Crammer, Koby and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive aggressive algorithms.</title>
<date>2003</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="11768" citStr="Crammer et al., 2003" startWordPosition="1865" endWordPosition="1868"> sub-task and 17,744 long sentences for the second sub-task (for the second sub-task, one line in the training data set may contain more than one sentence). We split one eighth of the data as our development set. On the other hand, there are both 1,000 sentences in released test data for the first and second sub-tasks. 5.2 Constituent Parsing As mentioned in section 3, constituent parsing is done by using a dependency parser combined with our conversion method. We choose the second order maximum spanning tree parser with k-best online large-margin learning algorithm (Crammer and Singer, 2003; Crammer et al., 2003). The MST parser we use is in the form of an open source program implemented in C++3. The features used for MST parser is the same as (McDonald, 2006). Both the single-stage and twostage dependency type labeling approaches are applied in our experiments. For the two-stage dependency type labeling, The linear chain CRF model is adopted instead of the first-order Markov model used in (McDonald, 2006). The features in Table 3 are used for CRF model. It takes about 7 hours for training the MST parser, and about 24 hours for training the CRF model. As mentioned in section 3.1.2, SVM is adopted as t</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2003</marker>
<rawString>Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2003. Online passive aggressive algorithms. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1368" citStr="Culotta and Sorensen, 2004" startWordPosition="186" endWordPosition="189"> to constituent structures using a machine learning approach. A conditional random fields (CRF) tagger is adopted for head information recognition. Our experiments shows that acceptable parsing and head tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be bene</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Culotta, Aron and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1274" citStr="Ding and Palmer, 2005" startWordPosition="174" endWordPosition="177">k based on a formal dependencyconstituent transformation method which converts dependency to constituent structures using a machine learning approach. A conditional random fields (CRF) tagger is adopted for head information recognition. Our experiments shows that acceptable parsing and head tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures o</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Ding, Yuan and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<pages>959--967</pages>
<institution>The Ohio State University,</institution>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="1651" citStr="Finkel et al., 2008" startWordPosition="231" endWordPosition="234">challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗This work is partially supported by the National Natural S</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Finkel, Jenny Rose, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. pages 959–967, The Ohio State University, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gimenez</author>
<author>Marquez</author>
</authors>
<title>Svmtool: A general POS tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference of Language Resources and Evaluation,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="3734" citStr="Gimenez and Marquez, 2004" startWordPosition="560" endWordPosition="563">TCT has another annotation scheme, which combines both the constituent tree structure and the head information of each constituent. Specifically, there can be always multiple heads in a constituent. For the 2010 CIPS-ParsEval shared task, only segmented sentences are given in test data without part-of-speech (POS) tags, a POS tagger is required for this task. Therefore, we divide our system into three major cascade stages, namely POS tagging, constituent parsing and head information recognition, which are connected as a pipeline of processing. For the POS tagging, we adopt the SVMTool tagger (Gimenez and Marquez, 2004); for the constituent parsing, we use the Maximum Spanning Tree (MST) (McDonald, 2006) parser combined with a dependencies-to-constituents conversion; and for the head information recognition, we apply a sequence labeling method to label head information. Section 2 presents the POS tagger in our approach. The details of our parsing method is presented in section 3. The head information recognition is described in section 4. The data and experimental results are shown in section 5. The last section is the conclusion and future work. 2 POS Tagging The SVMTool tagger (Gimenez and Marquez, 2004) i</context>
</contexts>
<marker>Gimenez, Marquez, 2004</marker>
<rawString>Gimenez and Marquez. 2004. Svmtool: A general POS tagger generator based on support vector machines. In Proceedings of the 4th International Conference of Language Resources and Evaluation, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="1665" citStr="Huang, 2008" startWordPosition="235" endWordPosition="236">l task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗This work is partially supported by the National Natural Science Foundat</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Huang, Liang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Hao Xiong</author>
<author>Qun Liu</author>
</authors>
<title>Mutipath shift-reduce parsing with online training.</title>
<date>2009</date>
<booktitle>In CIPS-ParsEval-2009 shared task.</booktitle>
<contexts>
<context position="16836" citStr="Jiang et al. (2009)" startWordPosition="2702" endWordPosition="2705"> evaluated with official evaluation tool by the 2009 CIPS-ParsEval shared task. Bikel’s parser4 (Bikel, 2004) in Table 6 is a implementation of Collins’ head-driven statistical model (Collins, 2003). The Stanford parser5 is based on the factored model described in (Klein and Manning, 2002). The Charniak’s parser6 is based on the parsing model described in (Charniak, 2000). Berkeley parser7 is based on unlexicalized parsing model described in (Petrov and Klein, 2007). According to Table 6, the performance of our method is better than all the four parsers described above. Chen et al. (2009) and Jiang et al. (2009) both make use of combination of multiple parsers and achieve considerably high performance. 4http://www.cis.upenn.edu/˜dbikel/ software.html 5http://nlp.stanford.edu/software/ lex-parser.shtml/ 6ftp://ftp.cs.brown.edu/pub/nlparser/ 7http://nlp.cs.berkeley.edu/Main.html F1 Bikel’s parser 81.8 Stanford parser 83.3 Charniak’s parser 83.9 Berkeley parser 85.2 this paper 85.6 Jiang et al (2009). 87.2 Chen et al (2009). 88.8 Table 6: Comparison with previous works 6 Conclusion This paper describes our approaches for the parsing task in CIPS-ParsEval 2010 shared task. A pipeline system is used to so</context>
</contexts>
<marker>Jiang, Xiong, Liu, 2009</marker>
<rawString>Jiang, Wenbin, Hao Xiong, and Qun Liu. 2009. Mutipath shift-reduce parsing with online training. In CIPS-ParsEval-2009 shared task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In In Advances in NIPS</booktitle>
<pages>3--10</pages>
<contexts>
<context position="16507" citStr="Klein and Manning, 2002" startWordPosition="2644" endWordPosition="2648">tion measures of 2009 CIPS-ParsEval shared task, which are quite different from that of 2010 CIPSParsEval shared task, are used in this experiment for the comparison purpose. Table 6 shows the comparison. We compare our method with several main parsers on the official data set of 2009 CIPS-ParsEval shared task. All these results are evaluated with official evaluation tool by the 2009 CIPS-ParsEval shared task. Bikel’s parser4 (Bikel, 2004) in Table 6 is a implementation of Collins’ head-driven statistical model (Collins, 2003). The Stanford parser5 is based on the factored model described in (Klein and Manning, 2002). The Charniak’s parser6 is based on the parsing model described in (Charniak, 2000). Berkeley parser7 is based on unlexicalized parsing model described in (Petrov and Klein, 2007). According to Table 6, the performance of our method is better than all the four parsers described above. Chen et al. (2009) and Jiang et al. (2009) both make use of combination of multiple parsers and achieve considerably high performance. 4http://www.cis.upenn.edu/˜dbikel/ software.html 5http://nlp.stanford.edu/software/ lex-parser.shtml/ 6ftp://ftp.cs.brown.edu/pub/nlparser/ 7http://nlp.cs.berkeley.edu/Main.html </context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Klein, Dan and Christopher Manning. 2002. Fast exact inference with a factored model for natural language parsing. In In Advances in NIPS 2002, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Guodong Zhou</author>
</authors>
<title>Soochow university report for the 1st china workshop on syntactic parsing. In CIPS-ParsEval-2009 shared task.</title>
<date>2009</date>
<contexts>
<context position="8151" citStr="Li and Zhou, 2009" startWordPosition="1262" endWordPosition="1265">ing algorithm for the binary classifier and the features are in Table 1. 3.1.3 Convert Constituent Labels The rest problem is that we should restore the label for each constituent when dependency structure trees are again converted to constituent structures. The problem is solved by storing constituent labels as labels of dependency types. The label for each constituent is just used as the label dependency type for each dependency edge. The conversion method is tested on the development, too. Constituent trees are firstly converted into dependency structures using the head rules described in (Li and Zhou, 2009). Then, we transform those trees back to constituent structure using our conversion method and use the PARSEVAL (Black et al., 1991) measures to evaluate the performance of the conversion method. Our conversion method obtains 99.76% precision and 99.76% recall, which is a great performance. 3.2 Dependency Parser for Constituent Parsing Based on the proposed conversion method, dependency parsing algorithms can be used for constituent parsing. This can be done by firstly transforming training data from constituents into dependencies and extract training instances to train a binary classifier for</context>
<context position="12954" citStr="Li and Zhou, 2009" startWordPosition="2070" endWordPosition="2073">ction 3.1.2, SVM is adopted as the learning algorithm for the binary classifier. There are about 40,000 training instances in the first sub-task and about 80,000 in the second sub-task. Development sets are used for tuning parameter C of SVM and the training time of the SVM classifier for the first and second sub-task is about 8 and 24 hours, respectively. However, the conversion from dependencies to constituents is extremely fast. Converting more than 2,000 trees takes less than 1 second. To transform the constituent trees in training set into dependency structures, we use the head rules of (Li and Zhou, 2009). 5.3 Results The evaluation metrics used in 2010 CIPS-ParsEval shared task is shown in following: 1. syntactic parsing Precision = number of correct constituents in proposed parse number of constituents in proposed parse Recall = number of correct constituents in proposed parse number of constituents in standard parse F1 = 2*Precision*Recall Precision+Recall 3The Max-MSTParser is publicly available from http://max-mstparser.sourceforge.net/. Precision without head F1 Precision with head F1 Recall Recall single-stage 77.78 78.13 77.96 75.78 76.13 75.95 two-stage 78.61 78.76 78.69 76.61 76.75 7</context>
</contexts>
<marker>Li, Zhou, 2009</marker>
<rawString>Li, Junhui and Guodong Zhou. 2009. Soochow university report for the 1st china workshop on syntactic parsing. In CIPS-ParsEval-2009 shared task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>276--283</pages>
<location>MIT, Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="1524" citStr="Magerman, 1995" startWordPosition="213" endWordPosition="214"> that acceptable parsing and head tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from depende</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>Magerman, David M. 1995. Statistical decision-tree models for parsing. In Proceedings of ACL, pages 276–283, MIT, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Learning Spanning Tree Algorithm for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1823" citStr="McDonald, 2006" startWordPosition="257" endWordPosition="258">g techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗This work is partially supported by the National Natural Science Foundation of China (Grant No. 60903119, Grant No. 60773090 and Grant No. 90820018), the National Basic Research Program of China (Grant No. 2009CB320901), and the N</context>
<context position="3820" citStr="McDonald, 2006" startWordPosition="576" endWordPosition="577">d information of each constituent. Specifically, there can be always multiple heads in a constituent. For the 2010 CIPS-ParsEval shared task, only segmented sentences are given in test data without part-of-speech (POS) tags, a POS tagger is required for this task. Therefore, we divide our system into three major cascade stages, namely POS tagging, constituent parsing and head information recognition, which are connected as a pipeline of processing. For the POS tagging, we adopt the SVMTool tagger (Gimenez and Marquez, 2004); for the constituent parsing, we use the Maximum Spanning Tree (MST) (McDonald, 2006) parser combined with a dependencies-to-constituents conversion; and for the head information recognition, we apply a sequence labeling method to label head information. Section 2 presents the POS tagger in our approach. The details of our parsing method is presented in section 3. The head information recognition is described in section 4. The data and experimental results are shown in section 5. The last section is the conclusion and future work. 2 POS Tagging The SVMTool tagger (Gimenez and Marquez, 2004) is used as our POS tagging tool for the first stage. It is a POS tagger based on SVM cl</context>
<context position="11918" citStr="McDonald, 2006" startWordPosition="1896" endWordPosition="1897"> We split one eighth of the data as our development set. On the other hand, there are both 1,000 sentences in released test data for the first and second sub-tasks. 5.2 Constituent Parsing As mentioned in section 3, constituent parsing is done by using a dependency parser combined with our conversion method. We choose the second order maximum spanning tree parser with k-best online large-margin learning algorithm (Crammer and Singer, 2003; Crammer et al., 2003). The MST parser we use is in the form of an open source program implemented in C++3. The features used for MST parser is the same as (McDonald, 2006). Both the single-stage and twostage dependency type labeling approaches are applied in our experiments. For the two-stage dependency type labeling, The linear chain CRF model is adopted instead of the first-order Markov model used in (McDonald, 2006). The features in Table 3 are used for CRF model. It takes about 7 hours for training the MST parser, and about 24 hours for training the CRF model. As mentioned in section 3.1.2, SVM is adopted as the learning algorithm for the binary classifier. There are about 40,000 training instances in the first sub-task and about 80,000 in the second sub-ta</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>McDonald, Ryan. 2006. Discriminative Learning Spanning Tree Algorithm for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of english text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics (COLING-2004),</booktitle>
<pages>64--70</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1848" citStr="Nivre and Scholz, 2004" startWordPosition="259" endWordPosition="262">ch as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗This work is partially supported by the National Natural Science Foundation of China (Grant No. 60903119, Grant No. 60773090 and Grant No. 90820018), the National Basic Research Program of China (Grant No. 2009CB320901), and the National High-Tech Researc</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Nivre, Joakim and Mario Scholz. 2004. Deterministic dependency parsing of english text. In Proceedings of the 20th international conference on Computational Linguistics (COLING-2004), pages 64–70, Geneva, Switzerland, August 23rd-27th.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="1630" citStr="Petrov and Klein, 2007" startWordPosition="227" endWordPosition="230">onstituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗This work is partially supported by t</context>
<context position="16687" citStr="Petrov and Klein, 2007" startWordPosition="2674" endWordPosition="2677">6 shows the comparison. We compare our method with several main parsers on the official data set of 2009 CIPS-ParsEval shared task. All these results are evaluated with official evaluation tool by the 2009 CIPS-ParsEval shared task. Bikel’s parser4 (Bikel, 2004) in Table 6 is a implementation of Collins’ head-driven statistical model (Collins, 2003). The Stanford parser5 is based on the factored model described in (Klein and Manning, 2002). The Charniak’s parser6 is based on the parsing model described in (Charniak, 2000). Berkeley parser7 is based on unlexicalized parsing model described in (Petrov and Klein, 2007). According to Table 6, the performance of our method is better than all the four parsers described above. Chen et al. (2009) and Jiang et al. (2009) both make use of combination of multiple parsers and achieve considerably high performance. 4http://www.cis.upenn.edu/˜dbikel/ software.html 5http://nlp.stanford.edu/software/ lex-parser.shtml/ 6ftp://ftp.cs.brown.edu/pub/nlparser/ 7http://nlp.cs.berkeley.edu/Main.html F1 Bikel’s parser 81.8 Stanford parser 83.3 Charniak’s parser 83.9 Berkeley parser 85.2 this paper 85.6 Jiang et al (2009). 87.2 Chen et al (2009). 88.8 Table 6: Comparison with pr</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Petrov, Slav and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT/NAACL, pages 404–411, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative loglinear grammars with latent variables.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS 20.</booktitle>
<contexts>
<context position="5959" citStr="Petrov and Klein, 2008" startWordPosition="904" endWordPosition="907">o introduce a formal dependency structure and a machine learning method so that the ambiguity from dependency structures to constituent structures can be dealt with automatically. 3.1.1 Binarization We first transform constituent trees into the form that all productions for all subtrees are either unary or binary, before converting them to dependency structures. Due to the binarization, the target constituent trees of the conversion from dependency back to constituent structures are binary branching. This binarization is done by the left-factoring approach described in (Charniak et al., 1998; Petrov and Klein, 2008), which converts each production with n children, where n &gt; 2, into n − 1 binary productions. Additional non-terminal nodes introduced in this conversion must be clearly marked. Transforming the binary branching trees into arbitrary branching trees is accomplished by using the reverse process. 3.1.2 Using Binary Classifier We train a classifier to decide which dependency edges should be transformed first at each step of conversion automatically. After the binarization described in the previous section, only one dependency edge should be transformed at each step. Therefore the classifier only n</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Petrov, Slav and Dan Klein. 2008. Discriminative loglinear grammars with latent variables. In Proceedings of NIPS 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A best-first probabilistic shift-reduce parser.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>689--691</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1606" citStr="Sagae and Lavie, 2006" startWordPosition="223" endWordPosition="226">aches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗This work is </context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Sagae, Kenji and Alon Lavie. 2006. A best-first probabilistic shift-reduce parser. In Proceedings of COLING/ACL, pages 689–691, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
</authors>
<title>Automatic paraphrase acquisition from news articles.</title>
<date>2002</date>
<booktitle>In HLT-2002.</booktitle>
<contexts>
<context position="1318" citStr="Shinyama et al., 2002" startWordPosition="180" endWordPosition="183">ansformation method which converts dependency to constituent structures using a machine learning approach. A conditional random fields (CRF) tagger is adopted for head information recognition. Our experiments shows that acceptable parsing and head tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a </context>
</contexts>
<marker>Shinyama, Sekine, Sudo, 2002</marker>
<rawString>Shinyama, Yusuke, Satoshi Sekine, and Kiyoshi Sudo. 2002. Automatic paraphrase acquisition from news articles. In HLT-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2004</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="1422" citStr="Snow et al., 2004" startWordPosition="195" endWordPosition="198"> conditional random fields (CRF) tagger is adopted for head information recognition. Our experiments shows that acceptable parsing and head tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent par</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2004</marker>
<rawString>Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Song</author>
<author>Chunyu Kit</author>
</authors>
<title>PCFG parsing with crf tagging for head recognition.</title>
<date>2009</date>
<booktitle>In CIPS-ParsEval2009 shared task.</booktitle>
<contexts>
<context position="10620" citStr="Song and Kit, 2009" startWordPosition="1672" endWordPosition="1675">ductions of each constituent strongly affects the head information labeling. It is natural to apply a sequential labeling strategy to tackle this problem. The linear chain CRF model is adopted for the head information labeling, and the implementation of CRF model we used is the 0.53 version of the CRF++ toolkit2. We assume that head information is independent between different constituents, which could decrease the length of sequence to be labeled for the CRF model. We use a binary tag set to determine whether a constituent is a head, e.g. H for a head, O for a non-head, which is the same as (Song and Kit, 2009). The features in Table 2 are used for CRF model. To test our CRF tagger, we remove all head information from the development set, and use the CRF tagger to retrieve the head. The result strongly proves its effectiveness by showing an accuracy of 99.52%. 5 Experiments All experiments reported here were performed on a Core 2 Quad 2.83Ghz CPU with 8GB of RAM. 2The CRF++ toolkit is publicly available from http://crfpp.sourceforge.net/. 5.1 Data There are 37,219 short sentences in official released training data for the first sub-task and 17,744 long sentences for the second sub-task (for the seco</context>
</contexts>
<marker>Song, Kit, 2009</marker>
<rawString>Song, Yan and Chunyu Kit. 2009. PCFG parsing with crf tagging for head recognition. In CIPS-ParsEval2009 shared task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Zhou</author>
</authors>
<title>Annotation scheme for Chinese treebank.</title>
<date>2004</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="2914" citStr="Zhou, 2004" startWordPosition="431" endWordPosition="432"> No. 60773090 and Grant No. 90820018), the National Basic Research Program of China (Grant No. 2009CB320901), and the National High-Tech Research Program of China (Grant No.2008AA02Z315). for the former. This means that most of these popular and effective dependency parsing models can not be directly extended to constituents parsing. This paper proposes an formal method for such a conversion which adoptively solves the problem of ambiguity. Based on the proposed method, a dependency parsing algorithm can be used to solve tasks of constituent parsing. A part of Tsinghua Chinese Treebank (TCT) (Zhou, 2004; Zhou, 2007; Chen et al., 2008) is used as the training and test data for the 2010 CIPS-ParsEval shared task. Being different from the annotation scheme of the Penn Chinese Treebank (CTB), the TCT has another annotation scheme, which combines both the constituent tree structure and the head information of each constituent. Specifically, there can be always multiple heads in a constituent. For the 2010 CIPS-ParsEval shared task, only segmented sentences are given in test data without part-of-speech (POS) tags, a POS tagger is required for this task. Therefore, we divide our system into three m</context>
</contexts>
<marker>Zhou, 2004</marker>
<rawString>Zhou, Qiang. 2004. Annotation scheme for Chinese treebank. Journal of Chinese Information Processing, 18(4):1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Zhou</author>
</authors>
<title>Base chuck scheme for the Chinese language.</title>
<date>2007</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="2926" citStr="Zhou, 2007" startWordPosition="433" endWordPosition="434">0 and Grant No. 90820018), the National Basic Research Program of China (Grant No. 2009CB320901), and the National High-Tech Research Program of China (Grant No.2008AA02Z315). for the former. This means that most of these popular and effective dependency parsing models can not be directly extended to constituents parsing. This paper proposes an formal method for such a conversion which adoptively solves the problem of ambiguity. Based on the proposed method, a dependency parsing algorithm can be used to solve tasks of constituent parsing. A part of Tsinghua Chinese Treebank (TCT) (Zhou, 2004; Zhou, 2007; Chen et al., 2008) is used as the training and test data for the 2010 CIPS-ParsEval shared task. Being different from the annotation scheme of the Penn Chinese Treebank (CTB), the TCT has another annotation scheme, which combines both the constituent tree structure and the head information of each constituent. Specifically, there can be always multiple heads in a constituent. For the 2010 CIPS-ParsEval shared task, only segmented sentences are given in test data without part-of-speech (POS) tags, a POS tagger is required for this task. Therefore, we divide our system into three major cascade</context>
</contexts>
<marker>Zhou, 2007</marker>
<rawString>Zhou, Qiang. 2007. Base chuck scheme for the Chinese language. Journal of Chinese Information Processing, 21(3):21–27.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>