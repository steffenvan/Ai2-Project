<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.088709">
<note confidence="0.992201">
Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
Philadelphia, July 2002, pp. 184-187. Association for Computational Linguistics.
</note>
<sectionHeader confidence="0.3028015" genericHeader="abstract">
2.1 Contextual Reference of Military
Echelon
</sectionHeader>
<bodyText confidence="0.999799692307693">
The name delta in utterance 2 refers to delta
company of the 1st marines division, 5th bat-
talion. Military forces are organized into hierar-
chies. A company is composed of platoons which
is composed of squads which is composed of sol-
diers. Thus, at any given time, context infor-
mation about the various values at each level of
the hierarchy must be assumed or ascertained
as needed. Therefore, if the next user utter-
ance had been, &amp;quot;First platoon will lead.&amp;quot;,
the established context should imply that this
is first platoon of delta company. If ambiguity
arises, clarification is needed.
</bodyText>
<subsectionHeader confidence="0.998253">
2.2 Knowledge of Data Sources
</subsectionHeader>
<bodyText confidence="0.999987444444444">
Besides the user, the dialog controller must be
able to take advantage of system knowledge
bases as needed. In utterance 2, a system knowl-
edge base must be consulted to determine both
the type of entity and the location of the entity
four pack (it is a cluster of buildings) in order
to properly represent the commander&apos;s expecta-
tion in the story. Our organization of knowledge
bases is described in section 4.
</bodyText>
<subsectionHeader confidence="0.934012">
2.3 Multimodal Integration
</subsectionHeader>
<bodyText confidence="0.999917789473684">
The utility of gestural input in conjunction
with speech, particularly for spatial references,
has been clearly demonstrated (Oviatt, 1996).
Utterances 4 and 8 illustrate contexts where
multimodal integration is needed. Dynamic
association of names with locations (such as
checkpoint alpha one in utterance 4) and
specifications of force movements are just two of
the many possibilities for multimodal input. An-
other under development includes specification
of geographic regions for various purposes (such
as direction of enemy movement, designation of
flanks, etc). An analysis of a videotape of an
exercise performed by Brigadier General Keith
Holcomb, U.S. Marine Corps, Ret., as part of
a DARPA CPOF exercise, revealed a total of
at least 7 different semantic classes for gestures.
The capability for multimodal input for any map
based task is essential.
</bodyText>
<subsectionHeader confidence="0.989611">
2.4 Relative Time References
</subsectionHeader>
<bodyText confidence="0.999966714285714">
While the schedule for military actions often has
precise timing, the timing is frequently relative
rather than absolute. Furthermore, the uncer-
tainties of military actions (often referred to as
the &amp;quot;fog of war&amp;quot;) virtually ensures that no pre-
cise scheduling will ever be achieved during the
actual action. Consequently, any system that
attempts to assist commanders with their situ-
ational awareness must be able to reason about
the relative time of events.
In utterance 4 of figure 1, the time in con-
text comes from utterance 2, the time at
which delta company gains position in the
four pack. In utterance 8 the time reference,
when alpha at checkpoint refers to the time
when alpha company gains position at check-
point alpha one.
Any system dealing with coordination of com-
plex sequences of events must be capable of han-
dling pronominal as well as more complex lin-
guistic references to other times and events.
</bodyText>
<sectionHeader confidence="0.994931" genericHeader="introduction">
3 Activity-Based Dialog Model
</sectionHeader>
<bodyText confidence="0.989747956521739">
The interaction is based on the notion of an ac-
tivity. Various types of activities in which a mil-
itary force can be engaged include movement,
position establishment, and reconnaissance. As-
sociated with an activity type are parameters,
some of which are mandatory and some are op-
tional. The dialog system will attempt to con-
tinue interaction about the current activity un-
til values for all mandatory parameters are sup-
plied. This approach is an instantiation of the
Missing Axiom Theory of dialog that we have
used in the past ((Smith et al., 1995) and (Bier-
mann et al., 1997)).
In the prototype system we have focused on
movement activities. There are a total of five
required parameters: force, start time, start lo-
cation, end time, and end location. We will illus-
trate how the model functions from the scenario
in figure 1.
Utterance 2: Move delta to the four
pack in two hours.
Initial context specifies start time and start
location for delta company. Since initial context
</bodyText>
<figure confidence="0.999007793103448">
XML
XML-&gt;Prolog
Prolog
XML
XML
Story Server
Fusion
Prolog
XML XML
Geo-Forces Server
Semantic Bus
Prolog XML
Text
Typing Agent
Text Text XML
A N/A
Voice MDT Capture
Text Tool
(stdin)
XML
File
System Drive
Prolog-&gt;XML
XML
Prolog
XML
Prolog
Vocal
Utterance
</figure>
<figureCaption confidence="0.999929">
Figure 2: System Architecture
</figureCaption>
<bodyText confidence="0.96057121875">
is discussing only one battalion, there is only one
company named delta; thus, it is a unique ref-
erence. The end location is four pack and the
end time is two hours after the start. Since all
mandatory parameters are specified, the system
simply issues a generic query. Future work will
include dealing with issues concerning confirma-
tion and verification.
Utterance 4: Move alpha to checkpoint
alpha one (MOUSE CLICK) starting one
hour after that.
Context provides initial location of alpha
company. Start time is specified linguistically
while the end location is specified by mouse
click, but end time is not, which leads to
utterance 5, What is the destination time?
In the response Start plus one hour, current
time refers to when alpha begins its movement.
After processing each utterance, the dialog con-
troller must update its context model to cor-
rectly reflect current time.
Utterance 8: Initiate when alpha at
checkpoint (DRAG ACTION).
Force, start location, and end location are all
specified via mouse gesture. Start time is speci-
fied linguistically. Again, end time is not speci-
fied and must be requested.
In general, we believe that using the idea of
activity for dialog context together with param-
eter specification as the engine for choosing what
to say will provide a robust interaction model as
we expand its capabilities.
</bodyText>
<sectionHeader confidence="0.990457" genericHeader="background">
4 Architecture
</sectionHeader>
<bodyText confidence="0.994973">
A block diagram of the system is shown in fig-
ure 2. The arrow labels specify the type of data
transmitted. Prolog refers to attribute-value
(AV) lists in Prolog syntax. A brief description
of each module is included below.
</bodyText>
<listItem confidence="0.992970333333333">
• Semantic Bus - handles interprocess com-
munication between agents.
• System Driver - provides a graphical user
interface (GUI) from which other system
components can be initiated.
• Dialog Controller - manages the interaction
with the user through contextual interpre-
tation of the user&apos;s multimodal inputs.
• Capture Tool - provides a GUI for viewing
the map and story graph and specifying ges-
tural inputs.
• Fusion - handles multimodal integration of
input via a unification-based approach (e.g.
QuickSet (Johnston et al., 1997))
• Voice- handles speech recognition using Vi-
aVoice from IBM.
• Typing Agent - allows for typed inputs
when speech recognition is not functioning.
• MDT - handles parsing via minimum dis-
tance translation.
• Geo-Forces Server - provides a database for
all geographic entities and military forces.
• Story Server - provides a database for the
story graph.
• XML -&gt; Prolog and Prolog-&gt;XML - con-
verts messages between XML format and
Prolog attribute-value (AV) list format.
</listItem>
<sectionHeader confidence="0.996224" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99997625">
The two most relevant systems previously devel-
oped are QuickSet ((Johnston et al., 1997)) and
CommandTalk ((Stent et al., 1999)). QuickSet
is a multimodal interface for designing military
simulations. The emphasis of the work is on
proper integration of the various input modal-
ities. QuickSet uses a unification based ap-
proach over typed feature structures for deter-
mining the appropriate interpretation. Due to
the command-driven nature of the application, a
great deal of functionality can be achieved with-
out a complex model of the ongoing dialog.
CommandTalk is primarily a spoken natural
language dialog system for the same application.
Its gestural capabilities are limited to specifica-
tion of points on the map analogous to utter-
ance 4 of our scenario in figure 1. Notable fea-
tures of CommandTalk include its bidirectional
grammar and its dialog manager which consists
of a collection of finite state machines (FSM).
The main purpose of these FSM&apos;s is to coordi-
nate initiative and handle parameter specifica-
tion. Neither system is designed to be able to
maintain a representation of an actual planned
military action for purposes of helping comman-
ders maintain appropriate situational awareness
via use of intelligent information filtering and
reporting.
</bodyText>
<sectionHeader confidence="0.990769" genericHeader="conclusions">
6 Status
</sectionHeader>
<bodyText confidence="0.99998025">
A prototype system has been implemented for
location activities. There is currently limited
linguistic capability but it can process inputs
such as those specified in the sample scenario.
Work continues on expanding its capabilities, in-
cluding specification of geographic regions, bat-
tle resources such as helicopters and ships, and
development of a more robust dialog model.
</bodyText>
<sectionHeader confidence="0.996706" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999695">
This work has been supported in part by
DARPA contract F30602-99-C-0060 (through
subcontract from General Dynamics: Advanced
Information Systems). We would also like to ex-
press our appreciation to our colleagues at Duke
University under the guidance of Dr. Alan Bier-
mann who have collaborated with us on the idea
of military stories and information filtering.
</bodyText>
<sectionHeader confidence="0.998132" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889259259259">
A.W. Biermann, C.I. Guinn, M.S. Fulkerson,
G. Keim, Z. Liang, D. Melamed, and K. Ra-
jagopalan. 1997. Goal-oriented multimedia dia-
logue with variable initiative. In Z.W. Ras and
A. Skowron, editors, Foundations of Intelligent
Systems, pages 1-16. Springer-Verlag, Berlin.
Nahum Gershon and Ward Page. 2001. What story-
telling can do for information visualization. Com-
munications of the ACM, pages 31-37, August.
M. Johnston, P.R. Cohen, D. McGee, S.L. Oviatt,
J.A. Pittman, and I. Smith. 1997. Unification-
based multimodal integration. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, pages 281-288.
S.L. Oviatt. 1996. Multimodal interfaces for dy-
namic interactive maps. In Proceedings of Con-
ference on Human Factors in Computing Systems:
CHF 96, pages 95-102, New York. ACM Press.
R.W. Smith, D.R. Hipp, and A.W. Biermann. 1995.
An architecture for voice dialog systems based
on Prolog-style theorem-proving. Computational
Linguistics, 21:281-320.
A. Stent, Dowding J., Gawron J. M., Bratt E.O., and
R. Moore. 1999. The commandtalk spoken dia-
logue system. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics, pages 183-190.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000063">
<note confidence="0.6797315">Proceedings of the Third SIGdial Workshop on Discourse and Dialogue, Philadelphia, July 2002, pp. 184-187. Association for Computational Linguistics. 2.1 Contextual Reference of Military Echelon</note>
<abstract confidence="0.991308032967032">name utterance 2 refers to delta company of the 1st marines division, 5th battalion. Military forces are organized into hierarchies. A company is composed of platoons which is composed of squads which is composed of soldiers. Thus, at any given time, context information about the various values at each level of the hierarchy must be assumed or ascertained as needed. Therefore, if the next user utterance had been, &amp;quot;First platoon will lead.&amp;quot;, the established context should imply that this is first platoon of delta company. If ambiguity arises, clarification is needed. 2.2 Knowledge of Data Sources Besides the user, the dialog controller must be able to take advantage of system knowledge bases as needed. In utterance 2, a system knowledge base must be consulted to determine both the type of entity and the location of the entity pack (it a cluster of buildings) in order to properly represent the commander&apos;s expectation in the story. Our organization of knowledge bases is described in section 4. 2.3 Multimodal Integration of gestural input in conjunction with speech, particularly for spatial references, has been clearly demonstrated (Oviatt, 1996). Utterances 4 and 8 illustrate contexts where multimodal integration is needed. Dynamic association of names with locations (such as one in utterance 4) and specifications of force movements are just two of the many possibilities for multimodal input. Another under development includes specification of geographic regions for various purposes (such as direction of enemy movement, designation of flanks, etc). An analysis of a videotape of an exercise performed by Brigadier General Keith Holcomb, U.S. Marine Corps, Ret., as part of a DARPA CPOF exercise, revealed a total of at least 7 different semantic classes for gestures. The capability for multimodal input for any map based task is essential. 2.4 Relative Time References While the schedule for military actions often has precise timing, the timing is frequently relative rather than absolute. Furthermore, the uncertainties of military actions (often referred to as the &amp;quot;fog of war&amp;quot;) virtually ensures that no precise scheduling will ever be achieved during the actual action. Consequently, any system that attempts to assist commanders with their situational awareness must be able to reason about the relative time of events. In utterance 4 of figure 1, the time in context comes from utterance 2, the time at which delta company gains position in the four pack. In utterance 8 the time reference, alpha at checkpoint to the time when alpha company gains position at checkpoint alpha one. Any system dealing with coordination of complex sequences of events must be capable of handling pronominal as well as more complex linguistic references to other times and events. Dialog Model interaction is based on the notion of an actypes of activities in which a military force can be engaged include movement, position establishment, and reconnaissance. Associated with an activity type are parameters, some of which are mandatory and some are optional. The dialog system will attempt to continue interaction about the current activity until values for all mandatory parameters are supplied. This approach is an instantiation of the Missing Axiom Theory of dialog that we have used in the past ((Smith et al., 1995) and (Biermann et al., 1997)). In the prototype system we have focused on movement activities. There are a total of five required parameters: force, start time, start location, end time, and end location. We will illustrate how the model functions from the scenario in figure 1. Move delta to the pack in two hours. Initial context specifies start time and start location for delta company. Since initial context XML XML-&gt;Prolog</abstract>
<title confidence="0.9099142">Prolog XML XML Story Server Fusion Prolog XML XML Geo-Forces Server Semantic Bus Prolog XML Text Typing Agent Text Text XML A N/A Voice MDT Capture</title>
<author confidence="0.603326">Text Tool</author>
<email confidence="0.565775">(stdin)</email>
<title confidence="0.920190571428571">File System Drive Prolog-&gt;XML XML Prolog XML Prolog</title>
<abstract confidence="0.98314629245283">Vocal Utterance Figure 2: System Architecture is discussing only one battalion, there is only one company named delta; thus, it is a unique ref- The end location is pack the end time is two hours after the start. Since all mandatory parameters are specified, the system simply issues a generic query. Future work will include dealing with issues concerning confirmation and verification. 4: alpha to checkpoint alpha one (MOUSE CLICK) starting one hour after that. Context provides initial location of alpha company. Start time is specified linguistically while the end location is specified by mouse click, but end time is not, which leads to 5, is the destination time? the response plus one hour, time refers to when alpha begins its movement. After processing each utterance, the dialog controller must update its context model to correctly reflect current time. 8: when alpha checkpoint (DRAG ACTION). Force, start location, and end location are all specified via mouse gesture. Start time is specified linguistically. Again, end time is not specified and must be requested. In general, we believe that using the idea of activity for dialog context together with parameter specification as the engine for choosing what to say will provide a robust interaction model as we expand its capabilities. 4 Architecture A block diagram of the system is shown in figure 2. The arrow labels specify the type of data transmitted. Prolog refers to attribute-value (AV) lists in Prolog syntax. A brief description of each module is included below. Semantic Bus interprocess communication between agents. System Driver a graphical user interface (GUI) from which other system components can be initiated. Dialog Controller the interaction with the user through contextual interpretation of the user&apos;s multimodal inputs. Capture Tool a GUI for viewing the map and story graph and specifying gestural inputs. Fusion multimodal integration of input via a unification-based approach (e.g. QuickSet (Johnston et al., 1997)) Voicespeech recognition using ViaVoice from IBM. Typing Agent for typed inputs when speech recognition is not functioning. MDT handles parsing via minimum distance translation. Geo-Forces Server a database for all geographic entities and military forces. Story Server a database for the story graph. XML -&gt; Prolog converts messages between XML format and Prolog attribute-value (AV) list format. 5 Related Work The two most relevant systems previously developed are QuickSet ((Johnston et al., 1997)) and CommandTalk ((Stent et al., 1999)). QuickSet is a multimodal interface for designing military simulations. The emphasis of the work is on proper integration of the various input modalities. QuickSet uses a unification based approach over typed feature structures for determining the appropriate interpretation. Due to the command-driven nature of the application, a great deal of functionality can be achieved without a complex model of the ongoing dialog. CommandTalk is primarily a spoken natural language dialog system for the same application. Its gestural capabilities are limited to specification of points on the map analogous to utterance 4 of our scenario in figure 1. Notable features of CommandTalk include its bidirectional grammar and its dialog manager which consists of a collection of finite state machines (FSM). The main purpose of these FSM&apos;s is to coordinate initiative and handle parameter specification. Neither system is designed to be able to maintain a representation of an actual planned military action for purposes of helping commanders maintain appropriate situational awareness via use of intelligent information filtering and reporting. 6 Status A prototype system has been implemented for location activities. There is currently limited linguistic capability but it can process inputs such as those specified in the sample scenario. Work continues on expanding its capabilities, including specification of geographic regions, battle resources such as helicopters and ships, and development of a more robust dialog model.</abstract>
<note confidence="0.714901142857143">7 Acknowledgements This work has been supported in part by DARPA contract F30602-99-C-0060 (through subcontract from General Dynamics: Advanced Information Systems). We would also like to express our appreciation to our colleagues at Duke University under the guidance of Dr. Alan Bier-</note>
<abstract confidence="0.9621885">mann who have collaborated with us on the idea of military stories and information filtering.</abstract>
<title confidence="0.879755">References</title>
<author confidence="0.8976725">A W Biermann</author>
<author confidence="0.8976725">C I Guinn</author>
<author confidence="0.8976725">M S Fulkerson</author>
<author confidence="0.8976725">G Keim</author>
<author confidence="0.8976725">Z Liang</author>
<author confidence="0.8976725">D Melamed</author>
<author confidence="0.8976725">K Ra-</author>
<note confidence="0.851849208333333">jagopalan. 1997. Goal-oriented multimedia dialogue with variable initiative. In Z.W. Ras and Skowron, editors, of Intelligent 1-16. Springer-Verlag, Berlin. Nahum Gershon and Ward Page. 2001. What storycan do for information visualization. Comof the ACM, August. M. Johnston, P.R. Cohen, D. McGee, S.L. Oviatt, J.A. Pittman, and I. Smith. 1997. Unificationmultimodal integration. In of the 35th Annual Meeting of the Association for Linguistics, S.L. Oviatt. 1996. Multimodal interfaces for dyinteractive maps. In of Conference on Human Factors in Computing Systems: 96, 95-102, New York. ACM Press. R.W. Smith, D.R. Hipp, and A.W. Biermann. 1995. An architecture for voice dialog systems based Prolog-style theorem-proving. A. Stent, Dowding J., Gawron J. M., Bratt E.O., and R. Moore. 1999. The commandtalk spoken diasystem. In of the 37th Annual Meeting of the Association for Computational Lin- 183-190.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A W Biermann</author>
<author>C I Guinn</author>
<author>M S Fulkerson</author>
<author>G Keim</author>
<author>Z Liang</author>
<author>D Melamed</author>
<author>K Rajagopalan</author>
</authors>
<title>Goal-oriented multimedia dialogue with variable initiative.</title>
<date>1997</date>
<booktitle>Foundations of Intelligent Systems,</booktitle>
<pages>1--16</pages>
<editor>In Z.W. Ras and A. Skowron, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="3655" citStr="Biermann et al., 1997" startWordPosition="591" endWordPosition="595">imes and events. 3 Activity-Based Dialog Model The interaction is based on the notion of an activity. Various types of activities in which a military force can be engaged include movement, position establishment, and reconnaissance. Associated with an activity type are parameters, some of which are mandatory and some are optional. The dialog system will attempt to continue interaction about the current activity until values for all mandatory parameters are supplied. This approach is an instantiation of the Missing Axiom Theory of dialog that we have used in the past ((Smith et al., 1995) and (Biermann et al., 1997)). In the prototype system we have focused on movement activities. There are a total of five required parameters: force, start time, start location, end time, and end location. We will illustrate how the model functions from the scenario in figure 1. Utterance 2: Move delta to the four pack in two hours. Initial context specifies start time and start location for delta company. Since initial context XML XML-&gt;Prolog Prolog XML XML Story Server Fusion Prolog XML XML Geo-Forces Server Semantic Bus Prolog XML Text Typing Agent Text Text XML A N/A Voice MDT Capture Text Tool (stdin) XML File System</context>
</contexts>
<marker>Biermann, Guinn, Fulkerson, Keim, Liang, Melamed, Rajagopalan, 1997</marker>
<rawString>A.W. Biermann, C.I. Guinn, M.S. Fulkerson, G. Keim, Z. Liang, D. Melamed, and K. Rajagopalan. 1997. Goal-oriented multimedia dialogue with variable initiative. In Z.W. Ras and A. Skowron, editors, Foundations of Intelligent Systems, pages 1-16. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nahum Gershon</author>
<author>Ward Page</author>
</authors>
<title>What storytelling can do for information visualization.</title>
<date>2001</date>
<journal>Communications of the ACM,</journal>
<pages>31--37</pages>
<marker>Gershon, Page, 2001</marker>
<rawString>Nahum Gershon and Ward Page. 2001. What storytelling can do for information visualization. Communications of the ACM, pages 31-37, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>P R Cohen</author>
<author>D McGee</author>
<author>S L Oviatt</author>
<author>J A Pittman</author>
<author>I Smith</author>
</authors>
<title>Unificationbased multimodal integration.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>281--288</pages>
<contexts>
<context position="6450" citStr="Johnston et al., 1997" startWordPosition="1052" endWordPosition="1055">te-value (AV) lists in Prolog syntax. A brief description of each module is included below. • Semantic Bus - handles interprocess communication between agents. • System Driver - provides a graphical user interface (GUI) from which other system components can be initiated. • Dialog Controller - manages the interaction with the user through contextual interpretation of the user&apos;s multimodal inputs. • Capture Tool - provides a GUI for viewing the map and story graph and specifying gestural inputs. • Fusion - handles multimodal integration of input via a unification-based approach (e.g. QuickSet (Johnston et al., 1997)) • Voice- handles speech recognition using ViaVoice from IBM. • Typing Agent - allows for typed inputs when speech recognition is not functioning. • MDT - handles parsing via minimum distance translation. • Geo-Forces Server - provides a database for all geographic entities and military forces. • Story Server - provides a database for the story graph. • XML -&gt; Prolog and Prolog-&gt;XML - converts messages between XML format and Prolog attribute-value (AV) list format. 5 Related Work The two most relevant systems previously developed are QuickSet ((Johnston et al., 1997)) and CommandTalk ((Stent </context>
</contexts>
<marker>Johnston, Cohen, McGee, Oviatt, Pittman, Smith, 1997</marker>
<rawString>M. Johnston, P.R. Cohen, D. McGee, S.L. Oviatt, J.A. Pittman, and I. Smith. 1997. Unificationbased multimodal integration. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 281-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
</authors>
<title>Multimodal interfaces for dynamic interactive maps.</title>
<date>1996</date>
<booktitle>In Proceedings of Conference on Human Factors in Computing Systems: CHF 96,</booktitle>
<pages>95--102</pages>
<publisher>ACM Press.</publisher>
<location>New York.</location>
<contexts>
<context position="1388" citStr="Oviatt, 1996" startWordPosition="221" endWordPosition="222">eded. 2.2 Knowledge of Data Sources Besides the user, the dialog controller must be able to take advantage of system knowledge bases as needed. In utterance 2, a system knowledge base must be consulted to determine both the type of entity and the location of the entity four pack (it is a cluster of buildings) in order to properly represent the commander&apos;s expectation in the story. Our organization of knowledge bases is described in section 4. 2.3 Multimodal Integration The utility of gestural input in conjunction with speech, particularly for spatial references, has been clearly demonstrated (Oviatt, 1996). Utterances 4 and 8 illustrate contexts where multimodal integration is needed. Dynamic association of names with locations (such as checkpoint alpha one in utterance 4) and specifications of force movements are just two of the many possibilities for multimodal input. Another under development includes specification of geographic regions for various purposes (such as direction of enemy movement, designation of flanks, etc). An analysis of a videotape of an exercise performed by Brigadier General Keith Holcomb, U.S. Marine Corps, Ret., as part of a DARPA CPOF exercise, revealed a total of at l</context>
</contexts>
<marker>Oviatt, 1996</marker>
<rawString>S.L. Oviatt. 1996. Multimodal interfaces for dynamic interactive maps. In Proceedings of Conference on Human Factors in Computing Systems: CHF 96, pages 95-102, New York. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Smith</author>
<author>D R Hipp</author>
<author>A W Biermann</author>
</authors>
<title>An architecture for voice dialog systems based on Prolog-style theorem-proving.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--281</pages>
<contexts>
<context position="3627" citStr="Smith et al., 1995" startWordPosition="586" endWordPosition="589">tic references to other times and events. 3 Activity-Based Dialog Model The interaction is based on the notion of an activity. Various types of activities in which a military force can be engaged include movement, position establishment, and reconnaissance. Associated with an activity type are parameters, some of which are mandatory and some are optional. The dialog system will attempt to continue interaction about the current activity until values for all mandatory parameters are supplied. This approach is an instantiation of the Missing Axiom Theory of dialog that we have used in the past ((Smith et al., 1995) and (Biermann et al., 1997)). In the prototype system we have focused on movement activities. There are a total of five required parameters: force, start time, start location, end time, and end location. We will illustrate how the model functions from the scenario in figure 1. Utterance 2: Move delta to the four pack in two hours. Initial context specifies start time and start location for delta company. Since initial context XML XML-&gt;Prolog Prolog XML XML Story Server Fusion Prolog XML XML Geo-Forces Server Semantic Bus Prolog XML Text Typing Agent Text Text XML A N/A Voice MDT Capture Text </context>
</contexts>
<marker>Smith, Hipp, Biermann, 1995</marker>
<rawString>R.W. Smith, D.R. Hipp, and A.W. Biermann. 1995. An architecture for voice dialog systems based on Prolog-style theorem-proving. Computational Linguistics, 21:281-320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>J Dowding</author>
<author>J M Gawron</author>
<author>E O Bratt</author>
<author>R Moore</author>
</authors>
<title>The commandtalk spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="7063" citStr="Stent et al., 1999" startWordPosition="1153" endWordPosition="1156"> 1997)) • Voice- handles speech recognition using ViaVoice from IBM. • Typing Agent - allows for typed inputs when speech recognition is not functioning. • MDT - handles parsing via minimum distance translation. • Geo-Forces Server - provides a database for all geographic entities and military forces. • Story Server - provides a database for the story graph. • XML -&gt; Prolog and Prolog-&gt;XML - converts messages between XML format and Prolog attribute-value (AV) list format. 5 Related Work The two most relevant systems previously developed are QuickSet ((Johnston et al., 1997)) and CommandTalk ((Stent et al., 1999)). QuickSet is a multimodal interface for designing military simulations. The emphasis of the work is on proper integration of the various input modalities. QuickSet uses a unification based approach over typed feature structures for determining the appropriate interpretation. Due to the command-driven nature of the application, a great deal of functionality can be achieved without a complex model of the ongoing dialog. CommandTalk is primarily a spoken natural language dialog system for the same application. Its gestural capabilities are limited to specification of points on the map analogous</context>
</contexts>
<marker>Stent, Dowding, Gawron, Bratt, Moore, 1999</marker>
<rawString>A. Stent, Dowding J., Gawron J. M., Bratt E.O., and R. Moore. 1999. The commandtalk spoken dialogue system. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 183-190.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>