<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002156">
<title confidence="0.9988775">
Content Planner Construction via Evolutionary Algorithms and a
Corpus-based Fitness Function
</title>
<author confidence="0.989038">
Pablo A. Duboue
</author>
<affiliation confidence="0.9978335">
Department of Computer Science
Columbia University
</affiliation>
<email confidence="0.996009">
pablo@cs.columbia.edu
</email>
<author confidence="0.995337">
Kathleen R. McKeown
</author>
<affiliation confidence="0.9978885">
Department of Computer Science
Columbia University
</affiliation>
<email confidence="0.998456">
kathy@cs.columbia.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918375">
In this paper, we present a novel technique
to learn a tree-like structure for a con-
tent planner from an aligned corpus of se-
mantic inputs and corresponding, human-
produced, outputs. We apply a stochas-
tic search mechanism with a two-level fit-
ness function. As a first stage, we use high
level order constraints to quickly discard
unpromising planners. As a second stage,
alignments between regenerated text and
human output are employed. We evaluate
our approach by using the existing sym-
bolic planner in our system as a gold stan-
dard, obtaining a 66% improvement over
a random baseline in just 20 generations
of genetic search.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969370967742">
In a standard generation pipeline (Reiter, 1994),
a content planner is responsible for the higher
level document structuring and information selec-
tion. Any non-trivial multi-sentential/multi-para-
graph generator will require a complex content plan-
ner, responsible for deciding, for instance, the distri-
bution of the information among the different para-
graphs, bulleted lists, and other textual elements.
Information-rich inputs require a thorough filtering,
resulting in a small amount of the available data be-
ing conveyed in the output. Furthermore, the task
of building a content planner is normally recognized
as tightly coupled with the semantics and idiosyn-
crasies of each particular domain.
The AI planning community is aware that ma-
chine learning techniques can bring a general solu-
tion to problems that require customization for ev-
ery particular instantiation (Minton, 1993). The au-
tomatic (or semi-automatic) construction of a com-
plete content planner for unrestricted domains is
a highly desirable goal. While there are general
tools and techniques to deal with surface realiza-
tion (Elhadad and Robin, 1996; Lavoie and Ram-
bow, 1997) and sentence planning (Shaw, 1998), the
inherent dependency on each domain makes the con-
tent planning problem difficult to deal with in a uni-
fied framework; it requires sophisticated planning
methodologies, for example, DPOCL (Young and
Moore, 1994). The main problem is that the space of
possible planners is so large. For example, in the ex-
periments reported here, it contains all the possible
orderings of 82 units of information.
In this paper, we present a technique for learning
the structure of tree-like planners, similar to the one
manually built for our MAGIC system (McKeown
et al., 1997). The overall architecture for our learn-
ing of content planners is shown in Figure 1. As in-
put we utilize an aligned corpus of semantic inputs
aligned with human-produced discourse. We also
take advantage of the definition of the atomic opera-
tors (messages) from our existing system. We learn
these tree-like planners by means of a genetic search
process. The plan produced as output by such plan-
ners is a sequence of semantic structures, defined by
the atomic operators. The learning technique is com-
plementary to approaches proposed for generation
in summarization (Kan and McKeown, 2002), that
utilize semantically annotated text to build content
planners.
Our domain is the generation of post cardiac-
surgery medical reports or briefings. MAGIC pro-
duces such a briefing given the output from infer-
ences computed over raw data collected in the op-
erating room (Jordan et al., 2001). Since we have
a fully operational system, it serves as a devel-
opment environment in which we can experiment
with the automatic reproduction of the existing plan-
ner. Once the learning system has been fully devel-
oped, we can move to other domains and learn new
planners. We will also eventually experiment with
learning improved versions of the MAGIC planner
through evaluation with health care providers.
</bodyText>
<subsectionHeader confidence="0.989793">
1.1 Data
</subsectionHeader>
<bodyText confidence="0.999944444444444">
The corpus we are using in our experiments consists
of the data collected in the evaluation reported in
(McKeown et al., 2000). Normal work-flow in the
hospital requires a medical specialist to give brief-
ings when the patient arrives in the Intensive Care
Unit. In our past evaluation, 23 briefings were col-
lected and transcribed and these were used, along
with the admission note, another gold standard, to
quantify the quality of MAGIC output (100% preci-
sion, 78% recall).
In our work, we align the briefings with the se-
mantic input for the same patient; this input can be
used to produce MAGIC output for this patient. In
a later stage of the learning process we also align
system output with the briefings. An example of
the semantic input, system output and the briefing
is given in Figure 6. Note that there are quite a
few differences among them. In particular, the brief-
ings (c) are normally occurring speech. Aside from
being much more colorful than our system output,
they also include a considerable amount of informa-
tion not present in our semantic input. And there is
also some information present in our system that is
not being said by the doctors. This is because at the
time the briefing is given, data such as the name of
the patient is available in paper format to the target
audience.
</bodyText>
<subsectionHeader confidence="0.953695">
1.2 The Current Planner
</subsectionHeader>
<bodyText confidence="0.999718">
The planner currently used in the MAGIC system
was developed with efficiency in mind, but it lacks
flexibility and it is more appropriate for formal
</bodyText>
<figure confidence="0.369595">
planner
</figure>
<figureCaption confidence="0.998517">
Figure 1: Overall learning architecture
</figureCaption>
<bodyText confidence="0.999430190476191">
speech or text. It has a total of 274 operators; 192
are structure-defining (discourse or topic levels) and
82 are data defining (atomic level) operatorsl. An
atomic operator may select, for example, the age of
the patient, querying the semantic input for the pres-
ence of a given piece of information and instanti-
ating of some semantic structures. Those semantic
structures can be as complex as desired, referring to
constants, and function invocations. It is also possi-
ble for an atomic operator to span several nodes in
the output plan if its specified data is multi-valued.
During the execution of the planner, the input is then
checked for the existence of the datum specified by
the operator. If there is data available, the corre-
sponding semantic structures are inserted in the out-
put. The internal nodes, on the other hand, form a
tree representing the discourse plan; they provide a
structural frame for the placement of the atomic op-
erators. Thus, the execution of the planner involves
a traversal of the tree while querying the input and
instantiating the necessary nodes.
</bodyText>
<sectionHeader confidence="0.954269" genericHeader="method">
2 Our Approach
</sectionHeader>
<bodyText confidence="0.830904">
Our task is to learn a tree representing a planner that
performs as well as the planner developed manually
for MAGIC. We explore the large space of possible
&apos;equivalent to the notion of messages (Reiter and Dale,
2000).
</bodyText>
<figure confidence="0.997200846153846">
semantic input transcripts
order constraints
genetic search
genetic pool
atomic operators
structure
atomic operators
structure
atomic operators
structure
mutations
crossover
fitness fn
</figure>
<figureCaption confidence="0.845322666666667">
Figure 2: A planner tree-like structure, in our plan-
ning formalism, together with an input/output exam-
ple.
</figureCaption>
<bodyText confidence="0.994938868421053">
trees by means of evolutionary algorithms. While
we use them to learn a content planner, they have
also proven useful in the past for implementing con-
tent planners (Mellish et al., 1998). Note that both
tasks are different in nature, as ours is done off-line,
only once through the life-time of a system, while
their use of the GA search will be performed on ev-
ery execution of the system.
In a genetic search, a population of putative solu-
tions, known as chromosomes, is kept. In our case,
each chromosome is a tree representing a possible
content planner. Figure 2 shows an example plan-
ner and how it realizes semantic input when data is
missing (B) or duplicated (D). In each cycle, chro-
mosomes are allowed to reproduce themselves, with
well-fitted chromosomes reproducing more often.
Normally two types of reproductive mechanisms are
provided: mutations (that produces a new chromo-
some by modifying an old one) and cross-over (that
produces a new chromosome by combining two ex-
isting ones, its ‘parents’).
Each chromosome has an associated fitness value,
that specifies how well or promising the chromo-
some looks. A main contribution of our work is the
use of two corpus-based fitness functions, FC and
FA. We use an approximate evaluation function, FC
that allows us to efficiently determine whether or-
der constraints over plan operators are met in the
current chromosome. We use the constraints we
acquired on this domain (Duboue and McKeown,
2001),2 Figure 4). These constraints relate sets of
2In particular, we set fitness = −1 * N, where N is the
number of violated constraints over on the training set.
patterns by specifying strict restrictions on their rel-
ative placements. Note that a chromosome that vi-
olates any of these constraints ought to be consid-
ered invalid. However, instead of discarding it com-
pletely, we follow Richardson et al. (1989) and pro-
vide a penalty function, in order to allow the useful
information contained in it to be preserved in future
generations.
Once a tree has been evolved so that it conforms
to all order constraints, we switch to a computa-
tionally intensive fitness function, FA. In this last
stage, we use MAGIC to generate output text using
the current chromosome. We then compare that text
against the briefing produced by the physician for
the same patient. We use alignment to measure how
close the two texts are. This procedure is shown
in Figure 5. The fitness is then the average of the
alignment scores produced for a set of semantic in-
puts. This approach avoids some of the problems
typically found with gold standards. By averaging
the fitness function over different semantic inputs, it
evaluates the system against different subjects (since
each briefing was produced by a different person) in
one fell swoop. By capturing similarity in a scalar
value (the average itself), it avoids penalizing the
system for small discrepancies between system out-
put and gold standard.
For computing each pairwise alignment, we
use a global alignment3 with affine gap penalty,
the Needleman–Wunsch algorithm, as defined by
Durbin et al. (1998). These alignments do not allow
flipping (i.e., when aligning A–B–C and C–B–A
they will align both Bs but neither A nor C4) and
capture the notion of ordering more appropriately
for our needs. We adapted their algorithm by using
the information contents of words, as measured in
a 1M-token corpus of related discourse, to estimate
the goodness of substituting one word by another.
An important point to note here is that both FC
and FA are data-dependent, as they analyze the
goodness or badness of output plans, i.e., sequences
of instantiated atomic operators. They require run-
ning the planner multiple times in order to do the
</bodyText>
<footnote confidence="0.9269845">
3We employ global alignments because we are comparing
two discourses derived from identical semantic input.
4or they will align only the As or the Cs, depending on the
score of aligning correctly any of them.
</footnote>
<bodyText confidence="0.680197">
Sas computed by Pan and McKeown (1999).
</bodyText>
<figure confidence="0.999076333333333">
atomic operators
Input
{A;C;D;D}
structure
A
B
C
D
Output
A,D,D,C
plan
order constraint
</figure>
<figureCaption confidence="0.999963">
Figure 4: Fitness function: Constraints.
</figureCaption>
<bodyText confidence="0.999574857142857">
evaluation. We do this because, for one instance, as
the planning process may delete (because there is no
data available) or duplicate nodes (because of multi-
valued data).
An advantage of Fc is that it can be tested on
a much wider range of semantic inputs than it is
trained on6.
</bodyText>
<subsectionHeader confidence="0.998201">
2.1 Operations over chromosomes
</subsectionHeader>
<bodyText confidence="0.999941142857143">
We define three mutation operators and one cross-
over operation. The mutations include node inser-
tion, which picks an internal node at random and
moves a subset of its children to a newly created
subnode, and node deletion, which randomly picks
an internal node different from the root and removes
it by making its parent absorb its children. Both
operators are order-preserving. To include order
variations, a shuffle mutation is provided that ran-
domly picks an internal node and randomizes the
order of its children. The cross-over operation is
sketched in Figure 3. We choose an uniform cross-
over instead of a single or double point one follow-
ing Syswerda (1989).
</bodyText>
<sectionHeader confidence="0.994804" genericHeader="method">
3 Experiment results
</sectionHeader>
<bodyText confidence="0.999922636363636">
The framework described in the paper was imple-
mented as follows: We employed a population of
2000 chromosomes, discarding 25% of the worse-
fitted ones in each cycle. The vacant places were
filled with 40% chromosomes generated by muta-
tion and 60% by cross-over. The mutation operator
was applied with a 40% probability of performing a
node insertion or deletion and 60% chance of choos-
ing a shuffle mutation. The population was started
from a chromosome with one root node connected
to a random ordering of the 82 operators and then
</bodyText>
<footnote confidence="0.786756">
6not implemented in the current set of experiments.
</footnote>
<bodyText confidence="0.999403342857143">
nodes were inserted and shuffled 40 times.7
The search algorithm was executed by 20 gener-
ations in 8d 14h (total CPU time, using about 20
machines in parallel to compute the verbalizations8).
The best chromosome obtained was compared with
the current planner in an intrinsic evaluation, de-
scribed below.
Given the size of both planners, an automatic
evaluation process was needed. We use a met-
ric that captures the structural similarities between
the two planners. In our metric, we recorded for
each pair of atomic operators, the list of internal
nodes that dominates both of them. This infor-
mation was used to build a matrix of counts with
the size of such lists (for example, in the original
planner “age-node” and “name-node” are both dom-
inated by “demographics-node”, “overview-node”
and “discourse-node”; therefore, their entry in the
table is the size of that list, i.e., 3). Given the fact
that both the MAGIC planner and our learned plan-
ners have the same set of atomic operators, it is pos-
sible to score their similarity by subtracting the as-
sociated matrices and then computing the average of
the absolute values of this difference matrix. Lower
values will indicate closer similarity, with a perfect
match receiving the value of 0. Applying this metric
to the MAGIC planner and our best planner we ob-
tained the score 1.16. We compare this score against
2000 random planners from the initial population of
the genetic search. Taking the average of their scores
we obtain 3.08.
Finally, if we compare our learned planner against
the random ones we obtain 2.92. We clearly improve
over this baseline. An example of the learned plan-
ner is seen in Figure 6 (d).
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999922142857143">
In recent years, there has been a surge of in-
terest in empirical methods applied to natural
language generation (Columbia, 2001). Some
work on content planning also deals with con-
structing plans from semantically annotated input.
Kan and McKeown (2002) use an n-gram model for
ordering constraints. The approach is complemen-
</bodyText>
<footnote confidence="0.971309">
7this figure was picked to obtain trees with height Pz� 4.
8each regeneration involves complex unification processes
timing 31’ on average in a PIII 1Ghz
</footnote>
<figure confidence="0.999413739583333">
A
A
B
A
C
D
C
F
A
?
A
D
C
initial configuration
parents
c iv
iii ii
b +
i
1 2 3 4 5 6
a
6
1 2 3 4 5
offspring
first step
1
2
a
3
c
b
4
5
6
1
i
2
iii
3
iv
ii
4
5
6
4
b
5
second step
1
2
i
3
iv
ii
6
2
ii
4
b
5
6
1
2
a
3
c
6
1
1
2
2
a
a
3
3
ii
ii
c
4
4
b
b
5
5
6
6
third step
1
a
3
c
1
i
3
iv
final result
</figure>
<figureCaption confidence="0.861152">
Figure 3: Cross over between chromosomes. Two trees are merged by picking subtrees of them (trees
rooted at the dark circled nodes in 1st and 2nd step). Once a tree is moved to the offspring, all its leaves
</figureCaption>
<bodyText confidence="0.95681575">
(and connecting nodes) are removed from both trees (note that node 6 is removed from both trees in step 3).
The process continues until the parents are empty. This algorithm was chosen to maximize the amount of
structure from the parents preserved in the offspring, its inspired on Bickel and Bickel (1987). Note that the
first parent represents the ordering 123456, the second parent, 134265 while the offspring is 132645.
</bodyText>
<footnote confidence="0.818789">
inputs generated human
outputs transcripts
</footnote>
<figureCaption confidence="0.999294">
Figure 5: Fitness function: Alignment architecture.
</figureCaption>
<figure confidence="0.851100636363637">
instance
score
. . .
planner
MAGIC
. . .
ALIGNER
. . .
(patient-info-12865, c-patient, (a-age, age-12865), (a-name, name-12865), (a-gen-
der, gender-12865), (a-birth-date, ...), ..., (r-receive-blood-product, received-
BloodProduct1-12865), ...)
</figure>
<bodyText confidence="0.896228333333333">
(age-12865, c-measurement, (a-value, 38), (a-unit, &amp;quot;year&amp;quot;)) maps to sentence 1 (b)
(ht-12865, c-measurement, (a-value, 175), (a-unitm &amp;quot;centimeter&amp;quot;)) maps to sentence 1 (b)
(name-12865, c-name, (a-first-name, &amp;quot;John&amp;quot;), (a-last-name, &amp;quot;Doe&amp;quot;)) maps to sentence 1 (b)
. . .
(received-BloodProduct1-12865, c-receive-blood-product, (r-arg2, BloodProcut1-
12865), (a-dosage, Measure-BloodProduct1-12865)) maps to sentence 5 to last (b)
(BloodProduct1-12865, c-blood-product, (a-name, “Cell Savers’’)) maps tosentence5tolast(b)
(Measure-BloodProduct1-12865, c-measurement, (a-value, 3.0), (a-unit, “unit’’)) maps
to sentence 5 to last (b)
</bodyText>
<equation confidence="0.5798755">
. . .
John Doe is a 41 year-old male patient of Dr. Smith undergoing mitral valve repair. His weight is 92 kilograms and his height
</equation>
<figureCaption confidence="0.672952666666667">
175 centimeters. Drips in protocol concentrations include Dobutamine, Nitroglycerine and Levophed. He received 1000 mg of
Vancomycin and 160 mg of Gentamicin for antibiotics. Around induction, he was anesthetized with 130.0 mg of Rocuronium,
11.0 mg of Etomidate, 500.0 mcg of Fentanyl and 1.0 mg of Midazolam. Before start of bypass , he had hypotension, at start
of bypass, alkalosis, before coming off bypass, bradycardia and after coming off bypass, hypotension and relative-anemia. He
received three units of cell savers. His total cross clamp time was 2.0 hour 1.0 minute. His total bypass time was 2.0 hour 33.0
minutes. His pre-op cardiac output was 4.13. Cardiac output immediately off was 4.73 .
</figureCaption>
<bodyText confidence="0.995861384615385">
Approximately 175-cm gentleman. History of rheumatic fever and polio. He is nonambulatory but can move his legs. History of
acute renal insufficiency with a hematocrit of 1.4. History of mixed mr/ms lesion, tricuspid regurg and ai. Decreased right and
left sided function, 4 chamber dilatation. Tricuspid repair with the ringand mvr with a st. jude’s valve. History of pulmonary
hypertension with a baseline of 90/40 catheter. He was on heparin nph preop. No allergies. Feed and lines were
extubated he was on bypass approximately 2.5 hours. His ischemic time was 2 hours and 2 minutes. No problems. He came off
on dobutamine because of poor function. No problems post-bypass. Maintained on levo, nitro and dobutamine at 4.5 mcg per
kilo. Got vancomycin and gentamicin at 9 o’clock, standard iv anesthetics. He received a liter of albumin, 3 units of cell saver,
no exogenous blood. Last po2 was 453, potassium of 4.6, hematocrit of 26, before getting any blood gas. His cardiac output
with his chest closed
The patient is male. He had an easy intubation. Before coming off bypass, he had bradycardia. Drips in protocol concentrations
include Dobutamine, Nitroglycerine and Levophed. At start of bypass, he had alkalosis. After coming off bypass, he had
relative-anemia. Around induction, he was anesthetized with 130.0 mg of Rocuronium, 11.0 mg of Etomidate, 500.0 mcg of
Fentanyl and 1.0 mg of Midazolam. His weight is 92 kilograms and his pre-op cardiac output 4.13.
</bodyText>
<figureCaption confidence="0.970062">
Figure 6: Examples. (a) Semantic input excerpt. (b) MAGIC output. (c) Physician briefing. (d) Learned
planner output.
</figureCaption>
<bodyText confidence="0.999979235294117">
tary and suitable for scenarios where semantic an-
notation is an inexpensive task, such as in the auto-
matic summarization tasks presented in that paper.
Also working on order constraints for summariza-
tion, Barzilay et al. (2002) collect a corpus of order-
ing preferences among subjects and use them to es-
timate a preferred ordering.
Evolutionary algorithms were also employed be
Mellish et al. (1998) for content planning purposes.
While their intention was to push stochastic search
as a feasible method for implementing a content
planner and we pursue the automatic construction
of the planner itself, both systems produce a tree
as output. Our system, however, uses a corpus-
based fitness function, while they use a rhetorically-
motivated heuristic function with hand-tuned param-
eters.
Our approach is similar to techniques employed in
evolutionary algorithms to implement general pur-
pose planners,such as SYNERGY (Muslea, 1997); or
to induce grammars (Smith and Witten, 1996). In
general, all these approaches are deeply tied to Ge-
netic Programming (Koza, 1994), that deals with the
issue of how to let a computer program itself.
Our two-level fitness-function employs a lower-
order function for the initial approximation of so-
lutions in a process similar to the one taken by
Haupt (1995) in a very different domain. This tech-
nique is appropriate for dealing with expensive fit-
ness functions.
Finally, our approach with respect to human-
produced discourse as gold standard is similar to
Papinini et al. (2001) as it avoids adhering to the
particularities of one specific person or discourse.
</bodyText>
<sectionHeader confidence="0.996304" genericHeader="conclusions">
5 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999966">
The task of learning a general content planner such
as Moore and Paris (1992) is well beyond the state
of the art. We have identified reduced content plan-
ning tasks that are feasible for learning. In learning
for traditional AI planning, e.g., learning of plan-
ning operators (Garcia-Martinez and Borrajo, 1997),
the focus is on reduced planning environments. The
kind of discourse targeted by our current techniques
has been identified in the past as rich in domain com-
munication knowledge (Kittredge et al., 1991). We
identify such discourse as non-trivial scenarios in
which investigation of learning in content planners
is feasible.
In searching for an appropriate solution, a func-
tion that enables the computer tell from one solution
to another is always needed. A second contribution
of this paper falls in our proposed fitness function,
motivated by generation issues behind the content
planning in generation. By means of a powerful 2-
level fitness function we obtain a 66% improvement
over a random baseline (the one we started from) in
just 20 generations.
The problem with costly functions is always exe-
cution time. By using Fc, the constraint-based fit-
ness function, we speed up the process, computing
only 55K regenerations from a total of 187.5K. Our
proposed cross-over and mutation operators are also
well-suited for the task and are the result of our anal-
ysis of the generation domain.
Moreover, our technique achieves results without
the need of extensive semantic annotation nor large
sized input.
We are interested in extending this work by mi-
grating it to other domains. We also plan to inves-
tigate the quality of the obtained planners head to
head with our existing system. We want to see if it
is possible to improve the existing planner (exten-
sive evaluation with human subjects is required), as
the newly obtained output resembles more normally
occurring discourse in the domain.
Aside from the structure discovery, techniques
are required for automatically detecting and build-
ing messages (Reiter and Dale, 2000). This task has
been reported as extremely costly in the past (Ku-
kich, 1983). We believe there are good chances of
combining techniques such as (Duboue and McKe-
own, 2001) and (Barzilay and Lee, 2002) to semi-
automate its creation process.
</bodyText>
<sectionHeader confidence="0.998957" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999748528846154">
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
EMNLP-2002, Philadelphia, PA.
Regina Barzilay, Noemie Elhadad, and Katheleen McK-
eown. 2002. Inferring strategies for sentence ordering
in multidocument news summarization. JAIR.
A.S. Bickel and R.W. Bickel. 1987. Tree structured in
genetic algorithms. In L. Davis, editor, Genetic Algo-
rithms and Simulated Annealing. Pittman.
NLP Group Columbia. 2001. Columbia statistical gener-
ation day. http://www.cs.columbia.edu/ nlp/
sgd/.
Pablo A. Duboue and Kathleen R. McKeown. 2001. Em-
pirically estimating order constraints for content plan-
ning in generation. In Proceedings of the 39th Annual
Meeting ofthe Association for Computational Linguis-
tics (ACL-EACL 2001), Toulouse, France, July.
Richard Durbin, Sean Eddy, Anders Krogh, and Graeme
Mitchison, 1998. Biological sequence analysis, pages
17–28. Cambridge Univeristy Press.
Michael Elhadad and Jacques Robin. 1996. An overview
of surge. Technical report, Dept. of Mathematics and
C.S., Ben Gurion University, Beer Sheva, Israel.
Ram´on Garcia-Martinez and Daniel Borrajo. 1997.
Planning, learning and executing in autonomous sys-
tems. In Steel and Alami, editors, Recent Advances in
AI Planning, pages 208–220. Springer.
L.R. Haupt. 1995. Optimization of highly aperiodic con-
duction grids. In 11th Annual Review of Progress in
Applied Comp. Electromag. Conf., Monterrey, CA.
Desmond Jordan, Kathleen McKeown, K.J. Concepcion,
S.K. Feiner, and V. Hatzivassiloglou. 2001. Genera-
tion and evaluation of intraoperative inferences for au-
tomated health care briefings on patient status after by-
pass surgery. J. Am. Med. Inform. Assoc., 8:267–280.
Min-Yen Kan and Kathleen R. McKeown. 2002.
Corpus-trained text generation for summarization. In
Proceedings ofINLG-2002, Ramapo Mountains, NY.
Richard Kittredge, Tanya Korelsky, and Owen Rambow.
1991. On the need for domain communication lan-
guage. Computational Intelligence, 7(4):305–314.
J. Koza. 1994. Genetic Programming II. MIT Press.
Karen Kukich. 1983. Knowledge-based report genera-
tions: A technique for automatically generating nat-
ural language reports from databases. In Sixth ACM
SIGIR Conference, pages 246–250, Bethesda, MA.
Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In Proc.
ofANLP’97, Washington, DC.
Katheleen McKeown, Shimei Pan, James Shaw, Jordan
D., and Barry A. 1997. Language generation for mul-
timedia healthcare briefings. In Proc. ofANLP’97.
Kathleen McKeown, Desmond Jordan, Steven Feiner,
J. Shaw, E. Chen, S. Ahmad, A. Kushniruk, and V. Pa-
tel. 2000. A study of communication in the cardiac
surgery intensive care unit and its implications for au-
tomated briefing. In Proc. of the AMIA 2000.
Chris Mellish, Alistair Knott, Jon Oberlander, and Mick
O’Donnell. 1998. Experiments using stochastic
search for text planning. In Proc. of the 9th Inter-
national Workshop on Natural Language Generation,
pages 98–107, Niagra-on-the-Lake, Ontario, Canada.
Steven Minton, editor. 1993. Machine learning methods
for planning. Morgan Kaufmann series in machine
learning. M. Kaufmann, San Mateo, CA.
Johanna D. Moore and Cecile L. Paris. 1992. Plan-
ning text for advisory dialogues: Capturing intentional
and rhetorical information. Computational Linguis-
tics, 19(4):651–695.
Ian Muslea. 1997. A general-purpose AI planning sys-
tem based on the genetic programming paradigm. In
Late Breaking Papers at GP-97, pages 157–164.
Shimei Pan and Katheleen McKeown. 1999. Word infor-
mativeness and automatic pitch accent modeling. In
Proc. ofEMNLP/VLC’99, College Park, MD.
Kishore Papinini, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. Technical report, IBM.
Ehud Reiter and Robert Dale, 2000. Building Natural
Language Generation Systems, pages 61–63. Cam-
bridge University Press.
Ehud Reiter. 1994. Has a consensus nlg architecture ap-
peared and is it psychologically plausible? In Proc. of
7th IWNLG, pages 163–170.
J.T. Richardson, M.R. Palmer, G. Liepins, and
M. Hilliard. 1989. Some guidelines for genetic algo-
rithms with penalty functions. In J.D. Schaffer, editor,
Proc. of the Third Intl. Conf. in Genetic Algorithms,
pages 191–197. Morgan Kaufmann, Los Altos, CA.
James Shaw. 1998. Clause aggregation using linguistic
knowledge. In Proc. of 9th International Workshop on
Natural Language Generation, pages 138–147.
Tony C. Smith and Ian H. Witten. 1996. Learning lan-
guage using genetic algorithms. In Riloff and Scheler,
editors, Connectionist, statistical, and symbolic ap-
proaches to learning for natural language processing,
pages 133–145. Springer.
Gilbert Syswerda. 1989. Uniform crossover in genetic
algorithms. In J.D. Schaffer, editor, Proc. of the Third
Intl. Conf. in Genetic Algorithms, pages 2–9. Morgan
Kaufmann, Los Altos, CA.
Michael R. Young and Johanna D. Moore. 1994.
DPOCL: A principled approach to discourse planning.
In Proc. of 7th IWNLG, Kennebunkport, ME.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.587525">
<title confidence="0.9993145">Content Planner Construction via Evolutionary Algorithms and Corpus-based Fitness Function</title>
<author confidence="0.999192">A Pablo</author>
<affiliation confidence="0.889728">Department of Computer Columbia</affiliation>
<email confidence="0.998746">pablo@cs.columbia.edu</email>
<author confidence="0.999546">R Kathleen</author>
<affiliation confidence="0.8865695">Department of Computer Columbia</affiliation>
<email confidence="0.999869">kathy@cs.columbia.edu</email>
<abstract confidence="0.998694882352941">In this paper, we present a novel technique to learn a tree-like structure for a content planner from an aligned corpus of semantic inputs and corresponding, humanproduced, outputs. We apply a stochastic search mechanism with a two-level fitness function. As a first stage, we use high level order constraints to quickly discard unpromising planners. As a second stage, alignments between regenerated text and human output are employed. We evaluate our approach by using the existing symbolic planner in our system as a gold standard, obtaining a 66% improvement over a random baseline in just 20 generations of genetic search.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Bootstrapping lexical choice via multiple-sequence alignment.</title>
<date>2002</date>
<booktitle>In EMNLP-2002,</booktitle>
<location>Philadelphia, PA.</location>
<marker>Barzilay, Lee, 2002</marker>
<rawString>Regina Barzilay and Lillian Lee. 2002. Bootstrapping lexical choice via multiple-sequence alignment. In EMNLP-2002, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Katheleen McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument news summarization.</title>
<date>2002</date>
<publisher>JAIR.</publisher>
<contexts>
<context position="19497" citStr="Barzilay et al. (2002)" startWordPosition="3217" endWordPosition="3220">of bypass, he had alkalosis. After coming off bypass, he had relative-anemia. Around induction, he was anesthetized with 130.0 mg of Rocuronium, 11.0 mg of Etomidate, 500.0 mcg of Fentanyl and 1.0 mg of Midazolam. His weight is 92 kilograms and his pre-op cardiac output 4.13. Figure 6: Examples. (a) Semantic input excerpt. (b) MAGIC output. (c) Physician briefing. (d) Learned planner output. tary and suitable for scenarios where semantic annotation is an inexpensive task, such as in the automatic summarization tasks presented in that paper. Also working on order constraints for summarization, Barzilay et al. (2002) collect a corpus of ordering preferences among subjects and use them to estimate a preferred ordering. Evolutionary algorithms were also employed be Mellish et al. (1998) for content planning purposes. While their intention was to push stochastic search as a feasible method for implementing a content planner and we pursue the automatic construction of the planner itself, both systems produce a tree as output. Our system, however, uses a corpusbased fitness function, while they use a rhetoricallymotivated heuristic function with hand-tuned parameters. Our approach is similar to techniques empl</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Regina Barzilay, Noemie Elhadad, and Katheleen McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. JAIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Bickel</author>
<author>R W Bickel</author>
</authors>
<title>Tree structured in genetic algorithms.</title>
<date>1987</date>
<booktitle>Genetic Algorithms and Simulated Annealing.</booktitle>
<editor>In L. Davis, editor,</editor>
<publisher>Pittman.</publisher>
<contexts>
<context position="15760" citStr="Bickel and Bickel (1987)" startWordPosition="2657" endWordPosition="2660">ii 6 2 ii 4 b 5 6 1 2 a 3 c 6 1 1 2 2 a a 3 3 ii ii c 4 4 b b 5 5 6 6 third step 1 a 3 c 1 i 3 iv final result Figure 3: Cross over between chromosomes. Two trees are merged by picking subtrees of them (trees rooted at the dark circled nodes in 1st and 2nd step). Once a tree is moved to the offspring, all its leaves (and connecting nodes) are removed from both trees (note that node 6 is removed from both trees in step 3). The process continues until the parents are empty. This algorithm was chosen to maximize the amount of structure from the parents preserved in the offspring, its inspired on Bickel and Bickel (1987). Note that the first parent represents the ordering 123456, the second parent, 134265 while the offspring is 132645. inputs generated human outputs transcripts Figure 5: Fitness function: Alignment architecture. instance score . . . planner MAGIC . . . ALIGNER . . . (patient-info-12865, c-patient, (a-age, age-12865), (a-name, name-12865), (a-gender, gender-12865), (a-birth-date, ...), ..., (r-receive-blood-product, receivedBloodProduct1-12865), ...) (age-12865, c-measurement, (a-value, 38), (a-unit, &amp;quot;year&amp;quot;)) maps to sentence 1 (b) (ht-12865, c-measurement, (a-value, 175), (a-unitm &amp;quot;centimeter</context>
</contexts>
<marker>Bickel, Bickel, 1987</marker>
<rawString>A.S. Bickel and R.W. Bickel. 1987. Tree structured in genetic algorithms. In L. Davis, editor, Genetic Algorithms and Simulated Annealing. Pittman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NLP Group Columbia</author>
</authors>
<title>Columbia statistical generation day.</title>
<date>2001</date>
<note>http://www.cs.columbia.edu/ nlp/ sgd/.</note>
<contexts>
<context position="14594" citStr="Columbia, 2001" startWordPosition="2401" endWordPosition="2402">with a perfect match receiving the value of 0. Applying this metric to the MAGIC planner and our best planner we obtained the score 1.16. We compare this score against 2000 random planners from the initial population of the genetic search. Taking the average of their scores we obtain 3.08. Finally, if we compare our learned planner against the random ones we obtain 2.92. We clearly improve over this baseline. An example of the learned planner is seen in Figure 6 (d). 4 Related work In recent years, there has been a surge of interest in empirical methods applied to natural language generation (Columbia, 2001). Some work on content planning also deals with constructing plans from semantically annotated input. Kan and McKeown (2002) use an n-gram model for ordering constraints. The approach is complemen7this figure was picked to obtain trees with height Pz� 4. 8each regeneration involves complex unification processes timing 31’ on average in a PIII 1Ghz A A B A C D C F A ? A D C initial configuration parents c iv iii ii b + i 1 2 3 4 5 6 a 6 1 2 3 4 5 offspring first step 1 2 a 3 c b 4 5 6 1 i 2 iii 3 iv ii 4 5 6 4 b 5 second step 1 2 i 3 iv ii 6 2 ii 4 b 5 6 1 2 a 3 c 6 1 1 2 2 a a 3 3 ii ii c 4 4 </context>
</contexts>
<marker>Columbia, 2001</marker>
<rawString>NLP Group Columbia. 2001. Columbia statistical generation day. http://www.cs.columbia.edu/ nlp/ sgd/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo A Duboue</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Empirically estimating order constraints for content planning in generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics (ACL-EACL 2001),</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="8555" citStr="Duboue and McKeown, 2001" startWordPosition="1386" endWordPosition="1389">isms are provided: mutations (that produces a new chromosome by modifying an old one) and cross-over (that produces a new chromosome by combining two existing ones, its ‘parents’). Each chromosome has an associated fitness value, that specifies how well or promising the chromosome looks. A main contribution of our work is the use of two corpus-based fitness functions, FC and FA. We use an approximate evaluation function, FC that allows us to efficiently determine whether order constraints over plan operators are met in the current chromosome. We use the constraints we acquired on this domain (Duboue and McKeown, 2001),2 Figure 4). These constraints relate sets of 2In particular, we set fitness = −1 * N, where N is the number of violated constraints over on the training set. patterns by specifying strict restrictions on their relative placements. Note that a chromosome that violates any of these constraints ought to be considered invalid. However, instead of discarding it completely, we follow Richardson et al. (1989) and provide a penalty function, in order to allow the useful information contained in it to be preserved in future generations. Once a tree has been evolved so that it conforms to all order co</context>
</contexts>
<marker>Duboue, McKeown, 2001</marker>
<rawString>Pablo A. Duboue and Kathleen R. McKeown. 2001. Empirically estimating order constraints for content planning in generation. In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics (ACL-EACL 2001), Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Durbin</author>
<author>Sean Eddy</author>
<author>Anders Krogh</author>
<author>Graeme Mitchison</author>
</authors>
<title>Biological sequence analysis,</title>
<date>1998</date>
<pages>17--28</pages>
<publisher>Cambridge Univeristy Press.</publisher>
<contexts>
<context position="10190" citStr="Durbin et al. (1998)" startWordPosition="1658" endWordPosition="1661">for a set of semantic inputs. This approach avoids some of the problems typically found with gold standards. By averaging the fitness function over different semantic inputs, it evaluates the system against different subjects (since each briefing was produced by a different person) in one fell swoop. By capturing similarity in a scalar value (the average itself), it avoids penalizing the system for small discrepancies between system output and gold standard. For computing each pairwise alignment, we use a global alignment3 with affine gap penalty, the Needleman–Wunsch algorithm, as defined by Durbin et al. (1998). These alignments do not allow flipping (i.e., when aligning A–B–C and C–B–A they will align both Bs but neither A nor C4) and capture the notion of ordering more appropriately for our needs. We adapted their algorithm by using the information contents of words, as measured in a 1M-token corpus of related discourse, to estimate the goodness of substituting one word by another. An important point to note here is that both FC and FA are data-dependent, as they analyze the goodness or badness of output plans, i.e., sequences of instantiated atomic operators. They require running the planner mult</context>
</contexts>
<marker>Durbin, Eddy, Krogh, Mitchison, 1998</marker>
<rawString>Richard Durbin, Sean Eddy, Anders Krogh, and Graeme Mitchison, 1998. Biological sequence analysis, pages 17–28. Cambridge Univeristy Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
<author>Jacques Robin</author>
</authors>
<title>An overview of surge.</title>
<date>1996</date>
<tech>Technical report,</tech>
<institution>Dept. of Mathematics and C.S., Ben Gurion University, Beer Sheva, Israel.</institution>
<contexts>
<context position="2042" citStr="Elhadad and Robin, 1996" startWordPosition="303" endWordPosition="306">available data being conveyed in the output. Furthermore, the task of building a content planner is normally recognized as tightly coupled with the semantics and idiosyncrasies of each particular domain. The AI planning community is aware that machine learning techniques can bring a general solution to problems that require customization for every particular instantiation (Minton, 1993). The automatic (or semi-automatic) construction of a complete content planner for unrestricted domains is a highly desirable goal. While there are general tools and techniques to deal with surface realization (Elhadad and Robin, 1996; Lavoie and Rambow, 1997) and sentence planning (Shaw, 1998), the inherent dependency on each domain makes the content planning problem difficult to deal with in a unified framework; it requires sophisticated planning methodologies, for example, DPOCL (Young and Moore, 1994). The main problem is that the space of possible planners is so large. For example, in the experiments reported here, it contains all the possible orderings of 82 units of information. In this paper, we present a technique for learning the structure of tree-like planners, similar to the one manually built for our MAGIC sys</context>
</contexts>
<marker>Elhadad, Robin, 1996</marker>
<rawString>Michael Elhadad and Jacques Robin. 1996. An overview of surge. Technical report, Dept. of Mathematics and C.S., Ben Gurion University, Beer Sheva, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ram´on Garcia-Martinez</author>
<author>Daniel Borrajo</author>
</authors>
<title>Planning, learning and executing in autonomous systems.</title>
<date>1997</date>
<booktitle>In Steel and Alami, editors, Recent Advances in AI Planning,</booktitle>
<pages>208--220</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="21201" citStr="Garcia-Martinez and Borrajo, 1997" startWordPosition="3490" endWordPosition="3493"> different domain. This technique is appropriate for dealing with expensive fitness functions. Finally, our approach with respect to humanproduced discourse as gold standard is similar to Papinini et al. (2001) as it avoids adhering to the particularities of one specific person or discourse. 5 Conclusions and Further Work The task of learning a general content planner such as Moore and Paris (1992) is well beyond the state of the art. We have identified reduced content planning tasks that are feasible for learning. In learning for traditional AI planning, e.g., learning of planning operators (Garcia-Martinez and Borrajo, 1997), the focus is on reduced planning environments. The kind of discourse targeted by our current techniques has been identified in the past as rich in domain communication knowledge (Kittredge et al., 1991). We identify such discourse as non-trivial scenarios in which investigation of learning in content planners is feasible. In searching for an appropriate solution, a function that enables the computer tell from one solution to another is always needed. A second contribution of this paper falls in our proposed fitness function, motivated by generation issues behind the content planning in gener</context>
</contexts>
<marker>Garcia-Martinez, Borrajo, 1997</marker>
<rawString>Ram´on Garcia-Martinez and Daniel Borrajo. 1997. Planning, learning and executing in autonomous systems. In Steel and Alami, editors, Recent Advances in AI Planning, pages 208–220. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Haupt</author>
</authors>
<title>Optimization of highly aperiodic conduction grids.</title>
<date>1995</date>
<booktitle>In 11th Annual Review of Progress in Applied Comp. Electromag. Conf.,</booktitle>
<location>Monterrey, CA.</location>
<contexts>
<context position="20557" citStr="Haupt (1995)" startWordPosition="3387" endWordPosition="3388">based fitness function, while they use a rhetoricallymotivated heuristic function with hand-tuned parameters. Our approach is similar to techniques employed in evolutionary algorithms to implement general purpose planners,such as SYNERGY (Muslea, 1997); or to induce grammars (Smith and Witten, 1996). In general, all these approaches are deeply tied to Genetic Programming (Koza, 1994), that deals with the issue of how to let a computer program itself. Our two-level fitness-function employs a lowerorder function for the initial approximation of solutions in a process similar to the one taken by Haupt (1995) in a very different domain. This technique is appropriate for dealing with expensive fitness functions. Finally, our approach with respect to humanproduced discourse as gold standard is similar to Papinini et al. (2001) as it avoids adhering to the particularities of one specific person or discourse. 5 Conclusions and Further Work The task of learning a general content planner such as Moore and Paris (1992) is well beyond the state of the art. We have identified reduced content planning tasks that are feasible for learning. In learning for traditional AI planning, e.g., learning of planning o</context>
</contexts>
<marker>Haupt, 1995</marker>
<rawString>L.R. Haupt. 1995. Optimization of highly aperiodic conduction grids. In 11th Annual Review of Progress in Applied Comp. Electromag. Conf., Monterrey, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Jordan</author>
<author>Kathleen McKeown</author>
<author>K J Concepcion</author>
<author>S K Feiner</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Generation and evaluation of intraoperative inferences for automated health care briefings on patient status after bypass surgery.</title>
<date>2001</date>
<journal>J. Am. Med. Inform. Assoc.,</journal>
<pages>8--267</pages>
<contexts>
<context position="3550" citStr="Jordan et al., 2001" startWordPosition="550" endWordPosition="553"> our existing system. We learn these tree-like planners by means of a genetic search process. The plan produced as output by such planners is a sequence of semantic structures, defined by the atomic operators. The learning technique is complementary to approaches proposed for generation in summarization (Kan and McKeown, 2002), that utilize semantically annotated text to build content planners. Our domain is the generation of post cardiacsurgery medical reports or briefings. MAGIC produces such a briefing given the output from inferences computed over raw data collected in the operating room (Jordan et al., 2001). Since we have a fully operational system, it serves as a development environment in which we can experiment with the automatic reproduction of the existing planner. Once the learning system has been fully developed, we can move to other domains and learn new planners. We will also eventually experiment with learning improved versions of the MAGIC planner through evaluation with health care providers. 1.1 Data The corpus we are using in our experiments consists of the data collected in the evaluation reported in (McKeown et al., 2000). Normal work-flow in the hospital requires a medical speci</context>
</contexts>
<marker>Jordan, McKeown, Concepcion, Feiner, Hatzivassiloglou, 2001</marker>
<rawString>Desmond Jordan, Kathleen McKeown, K.J. Concepcion, S.K. Feiner, and V. Hatzivassiloglou. 2001. Generation and evaluation of intraoperative inferences for automated health care briefings on patient status after bypass surgery. J. Am. Med. Inform. Assoc., 8:267–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Corpus-trained text generation for summarization.</title>
<date>2002</date>
<booktitle>In Proceedings ofINLG-2002,</booktitle>
<location>Ramapo Mountains, NY.</location>
<contexts>
<context position="3258" citStr="Kan and McKeown, 2002" startWordPosition="502" endWordPosition="505">IC system (McKeown et al., 1997). The overall architecture for our learning of content planners is shown in Figure 1. As input we utilize an aligned corpus of semantic inputs aligned with human-produced discourse. We also take advantage of the definition of the atomic operators (messages) from our existing system. We learn these tree-like planners by means of a genetic search process. The plan produced as output by such planners is a sequence of semantic structures, defined by the atomic operators. The learning technique is complementary to approaches proposed for generation in summarization (Kan and McKeown, 2002), that utilize semantically annotated text to build content planners. Our domain is the generation of post cardiacsurgery medical reports or briefings. MAGIC produces such a briefing given the output from inferences computed over raw data collected in the operating room (Jordan et al., 2001). Since we have a fully operational system, it serves as a development environment in which we can experiment with the automatic reproduction of the existing planner. Once the learning system has been fully developed, we can move to other domains and learn new planners. We will also eventually experiment wi</context>
<context position="14718" citStr="Kan and McKeown (2002)" startWordPosition="2418" endWordPosition="2421">ined the score 1.16. We compare this score against 2000 random planners from the initial population of the genetic search. Taking the average of their scores we obtain 3.08. Finally, if we compare our learned planner against the random ones we obtain 2.92. We clearly improve over this baseline. An example of the learned planner is seen in Figure 6 (d). 4 Related work In recent years, there has been a surge of interest in empirical methods applied to natural language generation (Columbia, 2001). Some work on content planning also deals with constructing plans from semantically annotated input. Kan and McKeown (2002) use an n-gram model for ordering constraints. The approach is complemen7this figure was picked to obtain trees with height Pz� 4. 8each regeneration involves complex unification processes timing 31’ on average in a PIII 1Ghz A A B A C D C F A ? A D C initial configuration parents c iv iii ii b + i 1 2 3 4 5 6 a 6 1 2 3 4 5 offspring first step 1 2 a 3 c b 4 5 6 1 i 2 iii 3 iv ii 4 5 6 4 b 5 second step 1 2 i 3 iv ii 6 2 ii 4 b 5 6 1 2 a 3 c 6 1 1 2 2 a a 3 3 ii ii c 4 4 b b 5 5 6 6 third step 1 a 3 c 1 i 3 iv final result Figure 3: Cross over between chromosomes. Two trees are merged by picki</context>
</contexts>
<marker>Kan, McKeown, 2002</marker>
<rawString>Min-Yen Kan and Kathleen R. McKeown. 2002. Corpus-trained text generation for summarization. In Proceedings ofINLG-2002, Ramapo Mountains, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Kittredge</author>
<author>Tanya Korelsky</author>
<author>Owen Rambow</author>
</authors>
<title>On the need for domain communication language.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="21405" citStr="Kittredge et al., 1991" startWordPosition="3523" endWordPosition="3526">t avoids adhering to the particularities of one specific person or discourse. 5 Conclusions and Further Work The task of learning a general content planner such as Moore and Paris (1992) is well beyond the state of the art. We have identified reduced content planning tasks that are feasible for learning. In learning for traditional AI planning, e.g., learning of planning operators (Garcia-Martinez and Borrajo, 1997), the focus is on reduced planning environments. The kind of discourse targeted by our current techniques has been identified in the past as rich in domain communication knowledge (Kittredge et al., 1991). We identify such discourse as non-trivial scenarios in which investigation of learning in content planners is feasible. In searching for an appropriate solution, a function that enables the computer tell from one solution to another is always needed. A second contribution of this paper falls in our proposed fitness function, motivated by generation issues behind the content planning in generation. By means of a powerful 2- level fitness function we obtain a 66% improvement over a random baseline (the one we started from) in just 20 generations. The problem with costly functions is always exe</context>
</contexts>
<marker>Kittredge, Korelsky, Rambow, 1991</marker>
<rawString>Richard Kittredge, Tanya Korelsky, and Owen Rambow. 1991. On the need for domain communication language. Computational Intelligence, 7(4):305–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Koza</author>
</authors>
<title>Genetic Programming II.</title>
<date>1994</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="20331" citStr="Koza, 1994" startWordPosition="3348" endWordPosition="3349">n was to push stochastic search as a feasible method for implementing a content planner and we pursue the automatic construction of the planner itself, both systems produce a tree as output. Our system, however, uses a corpusbased fitness function, while they use a rhetoricallymotivated heuristic function with hand-tuned parameters. Our approach is similar to techniques employed in evolutionary algorithms to implement general purpose planners,such as SYNERGY (Muslea, 1997); or to induce grammars (Smith and Witten, 1996). In general, all these approaches are deeply tied to Genetic Programming (Koza, 1994), that deals with the issue of how to let a computer program itself. Our two-level fitness-function employs a lowerorder function for the initial approximation of solutions in a process similar to the one taken by Haupt (1995) in a very different domain. This technique is appropriate for dealing with expensive fitness functions. Finally, our approach with respect to humanproduced discourse as gold standard is similar to Papinini et al. (2001) as it avoids adhering to the particularities of one specific person or discourse. 5 Conclusions and Further Work The task of learning a general content p</context>
</contexts>
<marker>Koza, 1994</marker>
<rawString>J. Koza. 1994. Genetic Programming II. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Knowledge-based report generations: A technique for automatically generating natural language reports from databases.</title>
<date>1983</date>
<booktitle>In Sixth ACM SIGIR Conference,</booktitle>
<pages>246--250</pages>
<location>Bethesda, MA.</location>
<marker>Kukich, 1983</marker>
<rawString>Karen Kukich. 1983. Knowledge-based report generations: A technique for automatically generating natural language reports from databases. In Sixth ACM SIGIR Conference, pages 246–250, Bethesda, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Lavoie</author>
<author>Owen Rambow</author>
</authors>
<title>A fast and portable realizer for text generation systems.</title>
<date>1997</date>
<booktitle>In Proc. ofANLP’97,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="2068" citStr="Lavoie and Rambow, 1997" startWordPosition="307" endWordPosition="311">eyed in the output. Furthermore, the task of building a content planner is normally recognized as tightly coupled with the semantics and idiosyncrasies of each particular domain. The AI planning community is aware that machine learning techniques can bring a general solution to problems that require customization for every particular instantiation (Minton, 1993). The automatic (or semi-automatic) construction of a complete content planner for unrestricted domains is a highly desirable goal. While there are general tools and techniques to deal with surface realization (Elhadad and Robin, 1996; Lavoie and Rambow, 1997) and sentence planning (Shaw, 1998), the inherent dependency on each domain makes the content planning problem difficult to deal with in a unified framework; it requires sophisticated planning methodologies, for example, DPOCL (Young and Moore, 1994). The main problem is that the space of possible planners is so large. For example, in the experiments reported here, it contains all the possible orderings of 82 units of information. In this paper, we present a technique for learning the structure of tree-like planners, similar to the one manually built for our MAGIC system (McKeown et al., 1997)</context>
</contexts>
<marker>Lavoie, Rambow, 1997</marker>
<rawString>Benoit Lavoie and Owen Rambow. 1997. A fast and portable realizer for text generation systems. In Proc. ofANLP’97, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katheleen McKeown</author>
<author>Shimei Pan</author>
<author>James Shaw</author>
<author>D Jordan</author>
<author>A Barry</author>
</authors>
<title>Language generation for multimedia healthcare briefings.</title>
<date>1997</date>
<booktitle>In Proc. ofANLP’97.</booktitle>
<contexts>
<context position="2668" citStr="McKeown et al., 1997" startWordPosition="406" endWordPosition="409">oie and Rambow, 1997) and sentence planning (Shaw, 1998), the inherent dependency on each domain makes the content planning problem difficult to deal with in a unified framework; it requires sophisticated planning methodologies, for example, DPOCL (Young and Moore, 1994). The main problem is that the space of possible planners is so large. For example, in the experiments reported here, it contains all the possible orderings of 82 units of information. In this paper, we present a technique for learning the structure of tree-like planners, similar to the one manually built for our MAGIC system (McKeown et al., 1997). The overall architecture for our learning of content planners is shown in Figure 1. As input we utilize an aligned corpus of semantic inputs aligned with human-produced discourse. We also take advantage of the definition of the atomic operators (messages) from our existing system. We learn these tree-like planners by means of a genetic search process. The plan produced as output by such planners is a sequence of semantic structures, defined by the atomic operators. The learning technique is complementary to approaches proposed for generation in summarization (Kan and McKeown, 2002), that uti</context>
</contexts>
<marker>McKeown, Pan, Shaw, Jordan, Barry, 1997</marker>
<rawString>Katheleen McKeown, Shimei Pan, James Shaw, Jordan D., and Barry A. 1997. Language generation for multimedia healthcare briefings. In Proc. ofANLP’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Desmond Jordan</author>
<author>Steven Feiner</author>
<author>J Shaw</author>
<author>E Chen</author>
<author>S Ahmad</author>
<author>A Kushniruk</author>
<author>V Patel</author>
</authors>
<title>A study of communication in the cardiac surgery intensive care unit and its implications for automated briefing.</title>
<date>2000</date>
<booktitle>In Proc. of the AMIA</booktitle>
<contexts>
<context position="4091" citStr="McKeown et al., 2000" startWordPosition="640" endWordPosition="643">ces computed over raw data collected in the operating room (Jordan et al., 2001). Since we have a fully operational system, it serves as a development environment in which we can experiment with the automatic reproduction of the existing planner. Once the learning system has been fully developed, we can move to other domains and learn new planners. We will also eventually experiment with learning improved versions of the MAGIC planner through evaluation with health care providers. 1.1 Data The corpus we are using in our experiments consists of the data collected in the evaluation reported in (McKeown et al., 2000). Normal work-flow in the hospital requires a medical specialist to give briefings when the patient arrives in the Intensive Care Unit. In our past evaluation, 23 briefings were collected and transcribed and these were used, along with the admission note, another gold standard, to quantify the quality of MAGIC output (100% precision, 78% recall). In our work, we align the briefings with the semantic input for the same patient; this input can be used to produce MAGIC output for this patient. In a later stage of the learning process we also align system output with the briefings. An example of t</context>
</contexts>
<marker>McKeown, Jordan, Feiner, Shaw, Chen, Ahmad, Kushniruk, Patel, 2000</marker>
<rawString>Kathleen McKeown, Desmond Jordan, Steven Feiner, J. Shaw, E. Chen, S. Ahmad, A. Kushniruk, and V. Patel. 2000. A study of communication in the cardiac surgery intensive care unit and its implications for automated briefing. In Proc. of the AMIA 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Alistair Knott</author>
<author>Jon Oberlander</author>
<author>Mick O’Donnell</author>
</authors>
<title>Experiments using stochastic search for text planning.</title>
<date>1998</date>
<booktitle>In Proc. of the 9th International Workshop on Natural Language Generation,</booktitle>
<pages>98--107</pages>
<location>Niagra-on-the-Lake, Ontario, Canada.</location>
<marker>Mellish, Knott, Oberlander, O’Donnell, 1998</marker>
<rawString>Chris Mellish, Alistair Knott, Jon Oberlander, and Mick O’Donnell. 1998. Experiments using stochastic search for text planning. In Proc. of the 9th International Workshop on Natural Language Generation, pages 98–107, Niagra-on-the-Lake, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<title>Machine learning methods for planning.</title>
<date>1993</date>
<booktitle>series in machine learning. M.</booktitle>
<editor>Steven Minton, editor.</editor>
<publisher>Morgan Kaufmann</publisher>
<location>San Mateo, CA.</location>
<marker>1993</marker>
<rawString>Steven Minton, editor. 1993. Machine learning methods for planning. Morgan Kaufmann series in machine learning. M. Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
<author>Cecile L Paris</author>
</authors>
<title>Planning text for advisory dialogues: Capturing intentional and rhetorical information.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="20968" citStr="Moore and Paris (1992)" startWordPosition="3453" endWordPosition="3456">th the issue of how to let a computer program itself. Our two-level fitness-function employs a lowerorder function for the initial approximation of solutions in a process similar to the one taken by Haupt (1995) in a very different domain. This technique is appropriate for dealing with expensive fitness functions. Finally, our approach with respect to humanproduced discourse as gold standard is similar to Papinini et al. (2001) as it avoids adhering to the particularities of one specific person or discourse. 5 Conclusions and Further Work The task of learning a general content planner such as Moore and Paris (1992) is well beyond the state of the art. We have identified reduced content planning tasks that are feasible for learning. In learning for traditional AI planning, e.g., learning of planning operators (Garcia-Martinez and Borrajo, 1997), the focus is on reduced planning environments. The kind of discourse targeted by our current techniques has been identified in the past as rich in domain communication knowledge (Kittredge et al., 1991). We identify such discourse as non-trivial scenarios in which investigation of learning in content planners is feasible. In searching for an appropriate solution,</context>
</contexts>
<marker>Moore, Paris, 1992</marker>
<rawString>Johanna D. Moore and Cecile L. Paris. 1992. Planning text for advisory dialogues: Capturing intentional and rhetorical information. Computational Linguistics, 19(4):651–695.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Muslea</author>
</authors>
<title>A general-purpose AI planning system based on the genetic programming paradigm.</title>
<date>1997</date>
<booktitle>In Late Breaking Papers at GP-97,</booktitle>
<pages>157--164</pages>
<contexts>
<context position="20197" citStr="Muslea, 1997" startWordPosition="3326" endWordPosition="3327">ferred ordering. Evolutionary algorithms were also employed be Mellish et al. (1998) for content planning purposes. While their intention was to push stochastic search as a feasible method for implementing a content planner and we pursue the automatic construction of the planner itself, both systems produce a tree as output. Our system, however, uses a corpusbased fitness function, while they use a rhetoricallymotivated heuristic function with hand-tuned parameters. Our approach is similar to techniques employed in evolutionary algorithms to implement general purpose planners,such as SYNERGY (Muslea, 1997); or to induce grammars (Smith and Witten, 1996). In general, all these approaches are deeply tied to Genetic Programming (Koza, 1994), that deals with the issue of how to let a computer program itself. Our two-level fitness-function employs a lowerorder function for the initial approximation of solutions in a process similar to the one taken by Haupt (1995) in a very different domain. This technique is appropriate for dealing with expensive fitness functions. Finally, our approach with respect to humanproduced discourse as gold standard is similar to Papinini et al. (2001) as it avoids adheri</context>
</contexts>
<marker>Muslea, 1997</marker>
<rawString>Ian Muslea. 1997. A general-purpose AI planning system based on the genetic programming paradigm. In Late Breaking Papers at GP-97, pages 157–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimei Pan</author>
<author>Katheleen McKeown</author>
</authors>
<title>Word informativeness and automatic pitch accent modeling.</title>
<date>1999</date>
<booktitle>In Proc. ofEMNLP/VLC’99,</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="11067" citStr="Pan and McKeown (1999)" startWordPosition="1806" endWordPosition="1809">f words, as measured in a 1M-token corpus of related discourse, to estimate the goodness of substituting one word by another. An important point to note here is that both FC and FA are data-dependent, as they analyze the goodness or badness of output plans, i.e., sequences of instantiated atomic operators. They require running the planner multiple times in order to do the 3We employ global alignments because we are comparing two discourses derived from identical semantic input. 4or they will align only the As or the Cs, depending on the score of aligning correctly any of them. Sas computed by Pan and McKeown (1999). atomic operators Input {A;C;D;D} structure A B C D Output A,D,D,C plan order constraint Figure 4: Fitness function: Constraints. evaluation. We do this because, for one instance, as the planning process may delete (because there is no data available) or duplicate nodes (because of multivalued data). An advantage of Fc is that it can be tested on a much wider range of semantic inputs than it is trained on6. 2.1 Operations over chromosomes We define three mutation operators and one crossover operation. The mutations include node insertion, which picks an internal node at random and moves a sub</context>
</contexts>
<marker>Pan, McKeown, 1999</marker>
<rawString>Shimei Pan and Katheleen McKeown. 1999. Word informativeness and automatic pitch accent modeling. In Proc. ofEMNLP/VLC’99, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papinini</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical report, IBM.</tech>
<contexts>
<context position="20777" citStr="Papinini et al. (2001)" startWordPosition="3421" endWordPosition="3424">se planners,such as SYNERGY (Muslea, 1997); or to induce grammars (Smith and Witten, 1996). In general, all these approaches are deeply tied to Genetic Programming (Koza, 1994), that deals with the issue of how to let a computer program itself. Our two-level fitness-function employs a lowerorder function for the initial approximation of solutions in a process similar to the one taken by Haupt (1995) in a very different domain. This technique is appropriate for dealing with expensive fitness functions. Finally, our approach with respect to humanproduced discourse as gold standard is similar to Papinini et al. (2001) as it avoids adhering to the particularities of one specific person or discourse. 5 Conclusions and Further Work The task of learning a general content planner such as Moore and Paris (1992) is well beyond the state of the art. We have identified reduced content planning tasks that are feasible for learning. In learning for traditional AI planning, e.g., learning of planning operators (Garcia-Martinez and Borrajo, 1997), the focus is on reduced planning environments. The kind of discourse targeted by our current techniques has been identified in the past as rich in domain communication knowle</context>
</contexts>
<marker>Papinini, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papinini, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical report, IBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems,</title>
<date>2000</date>
<pages>61--63</pages>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6807" citStr="Reiter and Dale, 2000" startWordPosition="1104" endWordPosition="1107">or. If there is data available, the corresponding semantic structures are inserted in the output. The internal nodes, on the other hand, form a tree representing the discourse plan; they provide a structural frame for the placement of the atomic operators. Thus, the execution of the planner involves a traversal of the tree while querying the input and instantiating the necessary nodes. 2 Our Approach Our task is to learn a tree representing a planner that performs as well as the planner developed manually for MAGIC. We explore the large space of possible &apos;equivalent to the notion of messages (Reiter and Dale, 2000). semantic input transcripts order constraints genetic search genetic pool atomic operators structure atomic operators structure atomic operators structure mutations crossover fitness fn Figure 2: A planner tree-like structure, in our planning formalism, together with an input/output example. trees by means of evolutionary algorithms. While we use them to learn a content planner, they have also proven useful in the past for implementing content planners (Mellish et al., 1998). Note that both tasks are different in nature, as ours is done off-line, only once through the life-time of a system, w</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale, 2000. Building Natural Language Generation Systems, pages 61–63. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>Has a consensus nlg architecture appeared and is it psychologically plausible?</title>
<date>1994</date>
<booktitle>In Proc. of 7th IWNLG,</booktitle>
<pages>163--170</pages>
<contexts>
<context position="975" citStr="Reiter, 1994" startWordPosition="145" endWordPosition="146"> a content planner from an aligned corpus of semantic inputs and corresponding, humanproduced, outputs. We apply a stochastic search mechanism with a two-level fitness function. As a first stage, we use high level order constraints to quickly discard unpromising planners. As a second stage, alignments between regenerated text and human output are employed. We evaluate our approach by using the existing symbolic planner in our system as a gold standard, obtaining a 66% improvement over a random baseline in just 20 generations of genetic search. 1 Introduction In a standard generation pipeline (Reiter, 1994), a content planner is responsible for the higher level document structuring and information selection. Any non-trivial multi-sentential/multi-paragraph generator will require a complex content planner, responsible for deciding, for instance, the distribution of the information among the different paragraphs, bulleted lists, and other textual elements. Information-rich inputs require a thorough filtering, resulting in a small amount of the available data being conveyed in the output. Furthermore, the task of building a content planner is normally recognized as tightly coupled with the semantic</context>
</contexts>
<marker>Reiter, 1994</marker>
<rawString>Ehud Reiter. 1994. Has a consensus nlg architecture appeared and is it psychologically plausible? In Proc. of 7th IWNLG, pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Richardson</author>
<author>M R Palmer</author>
<author>G Liepins</author>
<author>M Hilliard</author>
</authors>
<title>Some guidelines for genetic algorithms with penalty functions.</title>
<date>1989</date>
<booktitle>Proc. of the Third Intl. Conf. in Genetic Algorithms,</booktitle>
<pages>191--197</pages>
<editor>In J.D. Schaffer, editor,</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>Los Altos, CA.</location>
<contexts>
<context position="8962" citStr="Richardson et al. (1989)" startWordPosition="1455" endWordPosition="1458">uation function, FC that allows us to efficiently determine whether order constraints over plan operators are met in the current chromosome. We use the constraints we acquired on this domain (Duboue and McKeown, 2001),2 Figure 4). These constraints relate sets of 2In particular, we set fitness = −1 * N, where N is the number of violated constraints over on the training set. patterns by specifying strict restrictions on their relative placements. Note that a chromosome that violates any of these constraints ought to be considered invalid. However, instead of discarding it completely, we follow Richardson et al. (1989) and provide a penalty function, in order to allow the useful information contained in it to be preserved in future generations. Once a tree has been evolved so that it conforms to all order constraints, we switch to a computationally intensive fitness function, FA. In this last stage, we use MAGIC to generate output text using the current chromosome. We then compare that text against the briefing produced by the physician for the same patient. We use alignment to measure how close the two texts are. This procedure is shown in Figure 5. The fitness is then the average of the alignment scores p</context>
</contexts>
<marker>Richardson, Palmer, Liepins, Hilliard, 1989</marker>
<rawString>J.T. Richardson, M.R. Palmer, G. Liepins, and M. Hilliard. 1989. Some guidelines for genetic algorithms with penalty functions. In J.D. Schaffer, editor, Proc. of the Third Intl. Conf. in Genetic Algorithms, pages 191–197. Morgan Kaufmann, Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
</authors>
<title>Clause aggregation using linguistic knowledge.</title>
<date>1998</date>
<booktitle>In Proc. of 9th International Workshop on Natural Language Generation,</booktitle>
<pages>138--147</pages>
<contexts>
<context position="2103" citStr="Shaw, 1998" startWordPosition="315" endWordPosition="316">lding a content planner is normally recognized as tightly coupled with the semantics and idiosyncrasies of each particular domain. The AI planning community is aware that machine learning techniques can bring a general solution to problems that require customization for every particular instantiation (Minton, 1993). The automatic (or semi-automatic) construction of a complete content planner for unrestricted domains is a highly desirable goal. While there are general tools and techniques to deal with surface realization (Elhadad and Robin, 1996; Lavoie and Rambow, 1997) and sentence planning (Shaw, 1998), the inherent dependency on each domain makes the content planning problem difficult to deal with in a unified framework; it requires sophisticated planning methodologies, for example, DPOCL (Young and Moore, 1994). The main problem is that the space of possible planners is so large. For example, in the experiments reported here, it contains all the possible orderings of 82 units of information. In this paper, we present a technique for learning the structure of tree-like planners, similar to the one manually built for our MAGIC system (McKeown et al., 1997). The overall architecture for our </context>
</contexts>
<marker>Shaw, 1998</marker>
<rawString>James Shaw. 1998. Clause aggregation using linguistic knowledge. In Proc. of 9th International Workshop on Natural Language Generation, pages 138–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony C Smith</author>
<author>Ian H Witten</author>
</authors>
<title>Learning language using genetic algorithms.</title>
<date>1996</date>
<booktitle>In Riloff and Scheler,</booktitle>
<pages>133--145</pages>
<editor>editors, Connectionist, statistical, and</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="20245" citStr="Smith and Witten, 1996" startWordPosition="3332" endWordPosition="3335">ms were also employed be Mellish et al. (1998) for content planning purposes. While their intention was to push stochastic search as a feasible method for implementing a content planner and we pursue the automatic construction of the planner itself, both systems produce a tree as output. Our system, however, uses a corpusbased fitness function, while they use a rhetoricallymotivated heuristic function with hand-tuned parameters. Our approach is similar to techniques employed in evolutionary algorithms to implement general purpose planners,such as SYNERGY (Muslea, 1997); or to induce grammars (Smith and Witten, 1996). In general, all these approaches are deeply tied to Genetic Programming (Koza, 1994), that deals with the issue of how to let a computer program itself. Our two-level fitness-function employs a lowerorder function for the initial approximation of solutions in a process similar to the one taken by Haupt (1995) in a very different domain. This technique is appropriate for dealing with expensive fitness functions. Finally, our approach with respect to humanproduced discourse as gold standard is similar to Papinini et al. (2001) as it avoids adhering to the particularities of one specific person</context>
</contexts>
<marker>Smith, Witten, 1996</marker>
<rawString>Tony C. Smith and Ian H. Witten. 1996. Learning language using genetic algorithms. In Riloff and Scheler, editors, Connectionist, statistical, and symbolic approaches to learning for natural language processing, pages 133–145. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilbert Syswerda</author>
</authors>
<title>Uniform crossover in genetic algorithms.</title>
<date>1989</date>
<booktitle>Proc. of the Third Intl. Conf. in Genetic Algorithms,</booktitle>
<pages>2--9</pages>
<editor>In J.D. Schaffer, editor,</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>Los Altos, CA.</location>
<contexts>
<context position="12175" citStr="Syswerda (1989)" startWordPosition="1993" endWordPosition="1994">ver operation. The mutations include node insertion, which picks an internal node at random and moves a subset of its children to a newly created subnode, and node deletion, which randomly picks an internal node different from the root and removes it by making its parent absorb its children. Both operators are order-preserving. To include order variations, a shuffle mutation is provided that randomly picks an internal node and randomizes the order of its children. The cross-over operation is sketched in Figure 3. We choose an uniform crossover instead of a single or double point one following Syswerda (1989). 3 Experiment results The framework described in the paper was implemented as follows: We employed a population of 2000 chromosomes, discarding 25% of the worsefitted ones in each cycle. The vacant places were filled with 40% chromosomes generated by mutation and 60% by cross-over. The mutation operator was applied with a 40% probability of performing a node insertion or deletion and 60% chance of choosing a shuffle mutation. The population was started from a chromosome with one root node connected to a random ordering of the 82 operators and then 6not implemented in the current set of experi</context>
</contexts>
<marker>Syswerda, 1989</marker>
<rawString>Gilbert Syswerda. 1989. Uniform crossover in genetic algorithms. In J.D. Schaffer, editor, Proc. of the Third Intl. Conf. in Genetic Algorithms, pages 2–9. Morgan Kaufmann, Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Young</author>
<author>Johanna D Moore</author>
</authors>
<title>DPOCL: A principled approach to discourse planning.</title>
<date>1994</date>
<booktitle>In Proc. of 7th IWNLG,</booktitle>
<location>Kennebunkport, ME.</location>
<contexts>
<context position="2318" citStr="Young and Moore, 1994" startWordPosition="346" endWordPosition="349">ring a general solution to problems that require customization for every particular instantiation (Minton, 1993). The automatic (or semi-automatic) construction of a complete content planner for unrestricted domains is a highly desirable goal. While there are general tools and techniques to deal with surface realization (Elhadad and Robin, 1996; Lavoie and Rambow, 1997) and sentence planning (Shaw, 1998), the inherent dependency on each domain makes the content planning problem difficult to deal with in a unified framework; it requires sophisticated planning methodologies, for example, DPOCL (Young and Moore, 1994). The main problem is that the space of possible planners is so large. For example, in the experiments reported here, it contains all the possible orderings of 82 units of information. In this paper, we present a technique for learning the structure of tree-like planners, similar to the one manually built for our MAGIC system (McKeown et al., 1997). The overall architecture for our learning of content planners is shown in Figure 1. As input we utilize an aligned corpus of semantic inputs aligned with human-produced discourse. We also take advantage of the definition of the atomic operators (me</context>
</contexts>
<marker>Young, Moore, 1994</marker>
<rawString>Michael R. Young and Johanna D. Moore. 1994. DPOCL: A principled approach to discourse planning. In Proc. of 7th IWNLG, Kennebunkport, ME.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>