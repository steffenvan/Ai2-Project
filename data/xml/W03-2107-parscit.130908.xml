<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.871685333333333">
Flexible Spoken Dialogue System based on User Models
and Dynamic Generation of VoiceXML Scripts
Kazunori Komatani Fumihiro Adachi Shinichi Ueno
</title>
<author confidence="0.971508">
Tatsuya Kawahara Hiroshi G. Okuno
</author>
<affiliation confidence="0.9930095">
Graduate School of Informatics
Kyoto University
</affiliation>
<address confidence="0.685445">
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
</address>
<email confidence="0.998875">
{komatani,adachi,ueno,kawahara,okuno}@kuis.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.984530346153846">
We realize a telephone-based collab-
orative natural language dialogue sys-
tem. Since natural language involves
very various expressions, a large num-
ber of VoiceXML scripts need to be pre-
pared to handle all possible input patterns.
We realize flexible dialogue management
for various user utterances by generating
VoiceXML scripts dynamically. More-
over, we address appropriate user mod-
eling in order to generate cooperative re-
sponses to each user. Specifically, we set
up three dimensions of user models: skill
level to the system, knowledge level on
the target domain and the degree of hasti-
ness. The models are automatically de-
rived by decision tree learning using real
dialogue data collected by the system. Ex-
perimental evaluation shows that the co-
operative responses adapted to individual
users serve as good guidance for novice
users without increasing the dialogue du-
ration for skilled users.
Keywords: spoken dialogue system, user model,
VoiceXML, cooperative responses, dialogue
strategy
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984978947369">
A Spoken dialogue system is one of the promising
applications of the speech recognition and natural
language understanding technologies. A typical task
of spoken dialogue systems is database retrieval.
Some IVR (interactive voice response) systems us-
ing the speech recognition technology are being put
into practical use as its simplest form. According to
the spread of cellular phones, spoken dialogue sys-
tems via telephone enable us to obtain information
from various places without any other special appa-
ratuses.
In order to realize user-friendly interaction, spo-
ken dialogue systems should be able to (1) accept
various user utterances to enable mixed-initiative di-
alogue and (2) generate cooperative responses. Cur-
rently, a lot of IVR systems via telephone operate
by using VoiceXML, which is a script language to
prescribe procedures of spoken dialogues. How-
ever, only the next behaviors corresponding to ev-
ery input are prescribed in the VoiceXML scripts,
so the dialogue procedure is basically designed as
system-initiated one, in which the system asked re-
quired items one by one. In order to realize mixed-
initiative dialogue, the system should be able to ac-
cept various user-initiated utterances. By allowing
to accept various user utterances, the combination
of words included in the utterances accordingly gets
enormous, and then it is practically impossible to
prepare VoiceXML scripts that correspond to the
enormous combinations of input words in advance.
It is also difficult to generate cooperative responses
adaptively in the above framework.
We propose a framework to generate VoiceXML
scripts dynamically in order to realize the mixed-
initiative dialogue, in which the system is needed to
accept various user utterances. This framework real-
izes flexible dialogue management without requiring
preparation for a large number of VoiceXML scripts
in advance. Furthermore, it enables various behav-
iors adaptive to the dialogue situations such as ob-
tained query results.
Another problem to realize user-friendly interac-
tion is how to generate cooperative responses. When
we consider the responses generated from the sys-
tem side, the dialogue strategies, which determine
when to make guidance and what the system should
tell to the user, are the essential factors in spoken di-
alogue systems. There are many studies in respect of
the dialogue strategy such as confirmation manage-
ment using confidence measures of speech recogni-
tion results (Komatani and Kawahara, 2000; Hazen
et al., 2000), dynamic change of dialogue initiative
(Litman and Pan, 2000; Chu-Carroll, 2000; Lamel
et al., 1999), and addition of cooperative contents
to system responses (Sadek, 1999). Nevertheless,
whether a particular response is cooperative or not
depends on individual user’s characteristic.
In order to adapt the system’s behavior to individ-
ual users, it is necessary to model the user’s patterns
(Kass and Finin, 1988). Most of conventional stud-
ies on user models have focused on the knowledge
of users. Others tried to infer and utilize user’s goals
to generate responses adapted to the user (van Beek,
1987; Paris, 1988). Elzer et al. (2000) proposed
a method to generate adaptive suggestions accord-
ing to users’ preferences. However, these studies
depend on knowledge of the target domain greatly,
and therefore the user models need to be deliberated
manually to be applied to new domains. Moreover,
they assumed that the input is text only, which does
not contain errors.
We propose more comprehensive user models to
generate user-adapted responses in spoken dialogue
systems taking account of information specific to
spoken dialogue. Spoken utterances include vari-
ous information such as the interval between the ut-
terances, the presence of barge-in and so on, which
can be utilized to judge the user’s character. These
features also possess generality in spoken dialogue
systems because they are not dependent on domain-
specific knowledge. As user models in spoken di-
alogue systems, Eckert et al. (1997) defined stereo-
types of users such as patient, submissive and ex-
perienced, in order to evaluate spoken dialogue sys-
tems by simulation. We introduce user models not
for defining users’ behaviors beforehand, but for de-
tecting users’ patterns in real-time interaction.
We define three dimensions in the user models:
‘skill level to the system’, ‘knowledge level on the
target domain’ and ‘degree of hastiness’. The user
models are trained by decision tree learning algo-
rithm, using real data collected from the Kyoto city
bus information system. Then, we implement the
user models on the system and evaluate them using
data collected with 20 novice users.
</bodyText>
<sectionHeader confidence="0.604603" genericHeader="introduction">
2 Flexible Spoken Dialogue System based
on Dynamic Generation of VoiceXML
Scripts
</sectionHeader>
<listItem confidence="0.840300714285714">
VoiceXML1 is a script language to prescribe pro-
cedures in spoken dialogues mainly on telephone,
and is becoming to a standard language of IVR sys-
tems. The VoiceXML scripts consist of three parts:
(1) specifications of system’s prompts, (2) specifica-
tions of grammars to accept a user’s utterance, and
(3) description of the next behaviors.
</listItem>
<bodyText confidence="0.999912">
However, most of existing services using the
VoiceXML imposes rigid interaction, in which user
utterances are restricted by system-initiated prompts
and a user is accordingly allowed to specify only re-
quested items one by one. It is more user-friendly
that users can freely convey their requests by natural
language expressions.
We present a framework to realize flexible inter-
action by generating VoiceXML scripts dynamically
(Pargellis et al., 1999; Nyberg et al., 2002). The
framework enables users to express their requests
by natural language even in VoiceXML-based sys-
tems. Furthermore, cooperative responses in the Ky-
oto city bus information system that has been devel-
oped in our laboratory are also presented in this sec-
tion.
</bodyText>
<subsectionHeader confidence="0.988058">
2.1 Dynamic Generation of VoiceXML Scripts
</subsectionHeader>
<bodyText confidence="0.998655571428572">
In VoiceXML scripts, acceptable keywords and cor-
responding next states must be explicitly specified.
However, since there exists enormous combinations
of keywords in natural language expressions, it is
practically impossible to describe all VoiceXML
scripts that correspond to the combinations. Then,
we introduce the framework in which VoiceXML
</bodyText>
<footnote confidence="0.99583">
1VoiceXML Forum. http://www.voicexml.org/
</footnote>
<figureCaption confidence="0.9514585">
Figure 1: Overview of spoken dialogue system
based on dynamic generation of VoiceXML scripts
</figureCaption>
<bodyText confidence="0.957551592592593">
scripts are generated dynamically to enable the sys-
tem to accept natural language expressions.
Figure 1 shows the overview of the framework.
The front end that operates based on VoiceXML
scripts is separated from the dialogue management
portion, which accepts speech recognition results
and generates corresponding VoiceXML scripts.
The user utterance is recognized based on a gram-
mar rule specified in VoiceXML scripts, and key-
words extracted from a speech recognition result are
passed to a CGI script. The CGI script retrieves cor-
responding information from the database on Web,
and generates a VoiceXML script for the succeed-
ing interaction. If sufficient information is not ob-
tained from a user utterance, a script that prompts to
fill remaining contents is generated, and if the user
utterance contains ambiguity, a script that makes a
disambiguating question is generated.
Consequently, the generation of VoiceXML
scripts enables to accept natural language expres-
sions without preparing a large number of the scripts
corresponding to various inputs beforehand. The
framework also enables to generate cooperative re-
sponses adapted to the situation such as retrieval re-
sults without spoiling portability.
Sys: Please tell me your current bus stop, your destination
or the specific bus route.
</bodyText>
<figure confidence="0.341914111111111">
User: Shijo-Kawaramachi.
Sys: Do you take a bus from Shijo-Kawaramachi?
User: Yes.
Sys: Where will you get off the bus?
User: Arashiyama.
Sys: Do you go from Shijo-Kawaramachi to Arashiyama?
User: Yes.
Sys: Bus number 11 bound for Arashiyama has departed
Sanjo-Keihanmae, two bus stops away.
</figure>
<figureCaption confidence="0.987899">
Figure 2: Example dialogue of the bus system
</figureCaption>
<subsectionHeader confidence="0.989699">
2.2 Kyoto City Bus Information System
</subsectionHeader>
<bodyText confidence="0.999974076923077">
We have developed the Kyoto city bus information
system, which locates the bus a user wants to take,
and tell him/her how long it will take before arriving.
The system can be accessed via telephone including
cellular phones2. From any places, users can easily
get the information on the bus that changes every
minute. Users are requested to input the bus stop to
get on, the destination, or the bus route number by
speech, and consequently get the bus information.
There are 652 bus stops and 85 bus routes in Kyoto
city. The bus stops can also be specified by the name
of famous places or public facilities nearby. Figure
2 shows a simple example of the dialogue.
</bodyText>
<subsectionHeader confidence="0.9992205">
2.3 Generation of Cooperative Responses in
Kyoto City Bus Information System
</subsectionHeader>
<bodyText confidence="0.999978333333333">
In spoken dialogue systems, it is important to gener-
ate cooperative responses by guessing the user’s in-
tention. The cooperative responses consist of adding
information more than the user has requested, telling
the reason when there is no query result, present-
ing the alternatives, and so on (Sadek, 1999). In
order to generate cooperative responses, it is neces-
sary to change responses according to the situations.
Since we separate language-understanding modules
from individual VoiceXML scripts that are gener-
ated dynamically, our system can generate appropri-
ate responses cooperatively according to query re-
sults. The examples of the cooperative responses
implemented in our Kyoto city bus information sys-
tem are listed below.
</bodyText>
<page confidence="0.847367">
2+81-75-326-3116
</page>
<figure confidence="0.989363909090909">
keywords
speech
recognizer
synthesized
speech
speech
database
dialogue
manager
VWS (Voice Web Server)
grammar
rules
user
VoiceXML
generator
TTS engine
response
sentences
VoiceXML
CGI script
Front end
(1) Suggestive answers
</figure>
<bodyText confidence="0.956929625">
The information related to a query result is offered
when the query result is negative.
(Example)
User: From Hyakumanben to Kyoto Station by bus
number 17.
Sys: Bus number 17 does not come for a while, but
bus number 206 has departed Takano, four bus
stops away.
</bodyText>
<listItem confidence="0.713992">
(2) Corrective answers
</listItem>
<bodyText confidence="0.933467666666667">
The reason is presented when there is no query result
or when a user utterance contains an error.
(Example)
</bodyText>
<listItem confidence="0.737784">
User: From Kyoto Station to Arashiyama.
Sys: There is no bus that directly goes from Kyoto
station to Arashiyama.
(3) Intensional answers
</listItem>
<bodyText confidence="0.909921333333333">
If several query results are obtained, generating in-
tensional answer is more cooperative than listing all
the results.
</bodyText>
<listItem confidence="0.804316">
(Example)
User: From Shijo-Kawaramachi to Shijo-Karasuma.
Sys: All buses to Shijo-Karasuma do not come for a
while.
(4) Completion answers
</listItem>
<bodyText confidence="0.904812222222222">
It is cooperative to generate responses including in-
formation more than the user’s request. However,
whether the information should be presented or not
differs in individual users.
(Example)
User: From Kyoto station by bus number 5.
Sys: Bus number 5 will depart at 17:34 from plat-
form ‘A1’.
(There are a lot of platforms in Kyoto station.)
</bodyText>
<sectionHeader confidence="0.990207" genericHeader="method">
3 Response Generation Using User Models
</sectionHeader>
<bodyText confidence="0.999977428571428">
We propose comprehensive user models to generate
user-adaptive responses taking account of informa-
tion specific to spoken dialogue. Whether a partic-
ular response is regarded as cooperative depends on
individual user’s characteristics. So, we address ap-
propriate user modeling in order to generate cooper-
ative responses to the users.
</bodyText>
<subsectionHeader confidence="0.999876">
3.1 Classification of User Models
</subsectionHeader>
<bodyText confidence="0.998927">
We define three dimensions as user models listed be-
low.
</bodyText>
<listItem confidence="0.981041">
• Skill level to the system
• Knowledge level on the target domain
• Degree of hastiness
Skill Level to the System
</listItem>
<bodyText confidence="0.991946565217391">
Since spoken dialogue systems are not
widespread yet, there arises a difference in the
skill level of users in operating the systems. It
is desirable that the system changes its behavior
including response generation and initiative man-
agement in accordance with the skill level of the
user. In conventional systems, a system-initiated
guidance has been invoked on the spur of the
moment either when the user says nothing or
when speech recognition is not successful. In our
framework, we address a radical solution for the
unskilled users by modeling the skill level as the
user’s property before such a problem arises.
Knowledge Level on the Target Domain
There also exists a difference in the knowledge
level on the target domain among users. Thus, it is
necessary for the system to change information to
present to users. For example, it is not cooperative
to tell too detailed information to strangers. On the
other hand, for inhabitants, it is useful to omit too
obvious information and to output more detailed in-
formation. Therefore, we introduce a dimension that
represents the knowledge level on the target domain.
</bodyText>
<subsectionHeader confidence="0.849127">
Degree of Hastiness
</subsectionHeader>
<bodyText confidence="0.999992375">
In speech communications, it is more important
to present information promptly and concisely com-
pared with the other communication modes such as
browsing. Especially in the bus system, the concise-
ness is preferred because the bus information is ur-
gent to most users. Therefore, we also take account
of degree of hastiness of the user, and accordingly
change the system’s responses.
</bodyText>
<subsectionHeader confidence="0.7551465">
3.2 Response Generation Strategy Using User
Models
</subsectionHeader>
<bodyText confidence="0.999965111111111">
Next, we describe the response generation strategies
adapted to individual users based on the proposed
user models: skill level, knowledge level and hasti-
ness. Basic design of dialogue management is based
on mixed-initiative dialogue, in which the system
makes follow-up questions and guidance if neces-
sary while allowing a user to utter freely. It is in-
vestigated to add various contents to the system re-
sponses as cooperative responses in (Sadek, 1999).
Such additional information is usually cooperative,
but some people may feel such a response redun-
dant.
Thus, we introduce the user models and con-
trol the generation of additional information. By
introducing the proposed user models, the system
changes generated responses by the following two
aspects: dialogue procedure and contents of re-
sponses.
</bodyText>
<subsectionHeader confidence="0.590202">
Dialogue Procedure
</subsectionHeader>
<bodyText confidence="0.999983928571429">
The dialogue procedure is changed based on the
skill level and the hastiness. If a user is identified as
having the high skill level, the dialogue management
is carried out in a user-initiated manner; namely, the
system generates only open-ended prompts. On the
other hand, when user’s skill level is detected as low,
the system takes an initiative and prompts necessary
items in order.
When the degree of hastiness is low, the system
makes confirmation on the input contents. Con-
versely, when the hastiness is detected as high, such
a confirmation procedure is omitted; namely, the
system immediately makes a query and outputs the
result without making such a confirmation.
</bodyText>
<subsectionHeader confidence="0.814252">
Contents of Responses
</subsectionHeader>
<bodyText confidence="0.999159333333333">
Information that should be included in the sys-
tem response can be classified into the following two
items.
</bodyText>
<listItem confidence="0.902359">
1. Dialogue management information
2. Domain-specific information
</listItem>
<bodyText confidence="0.94314325">
The dialogue management information specifies
how to carry out the dialogue including the instruc-
tion on user’s expression for yes/no questions like
“Please reply with either yes or no.” and the expla-
nation about the following dialogue procedure like
“Now I will ask in order.” This dialogue manage-
ment information is determined by the user’s skill
the maximum number of filled slots
</bodyText>
<figureCaption confidence="0.99915">
Figure 3: Decision tree for the skill level
</figureCaption>
<bodyText confidence="0.999957466666667">
level to the system, and is added to system responses
when the skill level is considered as low.
The domain-specific information is generated ac-
cording to the user’s knowledge level on the target
domain. Namely, for users unacquainted with the
local information, the system adds the explanation
about the nearest bus stop, and omits complicated
contents such as a proposal of another route.
The contents described above are also controlled
by the hastiness. For users who are not in hurry, the
system generates the additional contents that corre-
spond to their skill level and knowledge level as co-
operative responses. On the other hand, for hasty
users, the contents are omitted to prevent the dia-
logue from being redundant.
</bodyText>
<subsectionHeader confidence="0.9545915">
3.3 Classification of User based on Decision
Tree
</subsectionHeader>
<bodyText confidence="0.999981894736842">
In order to implement the proposed user models as
classifiers, we adopt a decision tree. It is constructed
by decision tree learning algorithm C5.0 (Quinlan,
1993) with data collected by our dialogue system.
Figure 3 shows an example of the derived decision
tree for the skill level.
We use the features listed in Figure 4. They in-
clude not only semantic information contained in the
utterances but also information specific to spoken di-
alogue systems such as the silence duration prior to
the utterance, the presence of barge-in and so on.
Except for the last category of Figure 4 including
“attribute of specified bus stops”, most of the fea-
tures are domain-independent.
The classification of each dimension is done for
every user utterance except for knowledge level. The
model of a user can change during a dialogue. Fea-
tures extracted from utterances are accumulated as
history information during the session.
</bodyText>
<figure confidence="0.760698857142857">
presense of barge-in
skill level
low
otherwise
initial state
dialogue state
0 1 2
skill level
low
skill level
high
0.07&gt;
rate of no input
58.8&gt;=
average of
recognition score
3
skill level
high
58.8&lt;
• features obtained from a single utterance
– dialogue state (defined by already filled slots)
– presence of barge-in
– lapsed time of the current utterance
– recognition result (something recognized / un-
certain / no input)
– score of speech recognizer
– the number of filled slots by the current utter-
ance
• features obtained from the session
– the number of utterances
– dialogue state of the previous utterance
– lapsed time from the beginning of the session
– the number of the repetition of a same question
– the average number of the repetition of a same
question
– ratio of the time of user utterances in whole
elapsed time
– ratio of the occurrence of barge-in out of the
whole number of utterances
– recognition result of the previous utterance
– ratio of something recognized
</figure>
<listItem confidence="0.725201">
– ratio of getting uncertain results
– ratio of no input
– the number of barge-in
– the number of something recognized
– the number of getting uncertain results
– the number of no input
– average of recognition scores
– the maximum number of slots filled by a single
utterance
• features obtained from the session (used only in clas-
sification of knowledge level)
</listItem>
<tableCaption confidence="0.4793202">
– attribute of specified bus stops
– the way of specifying bus stops (whether a bus
stop is specified by its correct name or not)
– ratio of each attribute of specified bus stops
– ratio of each way of specifying bus stops
</tableCaption>
<figureCaption confidence="0.973925">
Figure 4: Features to classify the user models
</figureCaption>
<figure confidence="0.744833925925926">
case: [skill level: low][knowledge level: low]
[hastiness: low]
Sys: Please tell me your current bus stop, your destination
or the specific bus route number.
User: Maruyama Park.
Sys: Let me confirm one by one.
(+— The explanation is added by [skill level: low].)
Will you get on at Maruyama Park?
(+— The content is confirmed by [hastiness: low].)
Please reply with yes or no.
(+— The instruction is added by [skill level: low].)
User: Yes.
Sys: Where will you get off the bus? Please specify by a
bus stop name or a well-known landmark.
(+— The explanation is added by [skill level: low].)
User: Tofukuji temple.
Sys: Will you get off the bus at Tofukuji temple?
Please reply with yes or no.
User: Yes.
Sys: The following buses are located.
(+— The explanation is added by [skill level: low].)
The nearest bus stop to Maruyama Park is Gion.
(+— The domain information is added by [knowl-
edge level: low].)
Bus number 202 bound for Nishioji-Kujo has de-
parted Higashiyama-Sanjo, which is two stops away.
...
</figure>
<figureCaption confidence="0.9906805">
Figure 5: An example dialogue with the proposed
user models
</figureCaption>
<table confidence="0.96869575">
low indeterminable high total
skill level 743 253 496 1492
knowledge level 275 808 409 1492
hastiness 421 932 139 1492
</table>
<tableCaption confidence="0.9612465">
Table 1: Number of manually annotated labels for
decision tree learning
</tableCaption>
<bodyText confidence="0.999727181818182">
Figure 5 shows an example of the system behav-
ior with the proposed user models. The skill level is
classified as being low by the decision tree, because
the first user’s utterance includes only one content
word. Then, dialogue procedure is changed to the
system-initiated one. Similarly, the hastiness is clas-
sified as being low by the decision tree, and the sys-
tem includes the explanation on the dialogue pro-
cedure and the instruction on the expression in the
responses. They are omitted if the hastiness is iden-
tified as high.
</bodyText>
<subsectionHeader confidence="0.966742">
3.4 Decision Tree Learning for User Models
</subsectionHeader>
<bodyText confidence="0.972477375">
We train and evaluate the decision tree for the user
models using dialogue data collected by our sys-
tem. The data was collected from December 10th
2001 to May 10th 2002. The number of the ses-
sions (telephone calls) is 215, and the total number
of utterances included in the sessions is 1492. We
annotated the subjective labels of the user models
by hand. The annotator judges the user models for
every utterance based on the recorded speech data
and logs. The labels were given to the three dimen-
sions described in section 3.1 among ‘high’, ‘inde-
terminable’ or ‘low’. It is possible that the annotated
model of a user changes during a dialogue, espe-
cially from ‘indeterminable’ to ‘low’ or ‘high’. The
number of the labeled utterances is shown in Table
1.
</bodyText>
<table confidence="0.999689">
condition #1 #2 #3
skill level 80.8% 75.3% 85.6%
knowledge level 73.9% 63.7% 78.2%
hastiness 74.9% 73.7% 78.6%
</table>
<tableCaption confidence="0.9472245">
Table 2: Classification accuracy of the proposed user
models
</tableCaption>
<bodyText confidence="0.995905918918919">
Using the labeled data, we trained the decision
tree and evaluated the classification accuracy of the
proposed user models. All the experiments were car-
ried out by the method of 10-fold cross validation.
The process, in which one tenth of all data is used as
the test data, and the remainder is used as the train-
ing data, is repeated ten times, and the average of
the classification accuracy is computed. The result
is shown in Table 2. The conditions #1, #2 and #3 in
Table 2 are described as follows.
#1: The 10-fold cross validation is carried out per
utterance.
#2: The 10-fold cross validation is carried out per
session (call).
#3: We calculate the accuracy under more realis-
tic condition. The accuracy is calculated not
in three classes (high / indeterminable / low)
but in two classes that actually affect the dia-
logue strategies. For example, the accuracy for
the skill level is calculated for the two classes:
low and the others. As to the classification of
knowledge level, the accuracy is calculated for
dialogue sessions, because the features such as
the attribute of a specified bus stop are not ob-
tained in every utterance. Moreover, in order
to smooth unbalanced distribution of the train-
ing data, a cost corresponding to the reciprocal
ratio of the number of samples in each class is
introduced. By the cost, the chance rate of two
classes becomes 50%.
The difference between condition #1 and #2 is
that the training was carried out in a speaker-closed
or speaker-open manner. The former shows better
performance.
The result in condition #3 shows useful accuracy
in the skill level. The following features play im-
portant part in the decision tree for the skill level:
</bodyText>
<figureCaption confidence="0.7385235">
the system except for
proposed user models
Figure 6: Overview of the Kyoto city bus informa-
tion system with user models
</figureCaption>
<bodyText confidence="0.999578666666667">
the number of filled slots by the current utterance,
presence of barge-in and ratio of no input. For the
knowledge level, recognition result (something rec-
ognized / uncertain / no input), ratio of no input and
the way to specify bus stops (whether a bus stop is
specified by its exact name or not) are effective. The
hastiness is classified mainly by the three features:
presence of barge-in, ratio of no input and lapsed
time of the current utterance.
</bodyText>
<subsectionHeader confidence="0.988017">
3.5 System Overview
</subsectionHeader>
<bodyText confidence="0.999800857142857">
Figure 6 shows an overview of the Kyoto city bus in-
formation system with the user models. The system
operates by generating VoiceXML scripts dynami-
cally as described in section 2.1. The real-time bus
information database is provided on the Web, which
can be accessed via Internet. Then, we explain the
modules in the following.
</bodyText>
<subsectionHeader confidence="0.812873">
VWS (Voice Web Server)
</subsectionHeader>
<bodyText confidence="0.99918325">
The Voice Web Server drives the speech recog-
nition engine and the TTS (Text-To-Speech)
module accordingly to the specifications by the
generated VoiceXML script.
</bodyText>
<subsectionHeader confidence="0.940462">
Speech Recognizer
</subsectionHeader>
<bodyText confidence="0.99987825">
The speech recognizer decodes user utterances
based on specified grammar rules and vocabu-
lary, which are defined by VoiceXML at each
dialogue state.
</bodyText>
<figure confidence="0.950396523809524">
recognition results
(including features other
than language info.)
user model
identifier
CGI
user
profiles
recognition results
(keywords)
database
on Web
dialogue
manager
VWS
(Voice Web Server)
user
VoiceXML
VoiceXML
generator
Dialogue Manager
</figure>
<bodyText confidence="0.999544">
The dialogue manager generates response sen-
tences based on recognition results (bus stop
names or a route number) received from the
VWS. If sufficient information to locate a bus
is obtained, it retrieves the corresponding bus
information on the Web.
</bodyText>
<subsectionHeader confidence="0.623456">
VoiceXML Generator
</subsectionHeader>
<bodyText confidence="0.99984025">
This module dynamically generates VoiceXML
scripts that contain response sentences and
specifications of speech recognition grammars,
which are given by the dialogue manager.
</bodyText>
<subsectionHeader confidence="0.754655">
User model identifier
</subsectionHeader>
<bodyText confidence="0.999873833333333">
This module classifies user’s characters based
on the user models using features specific to
spoken dialogue as well as semantic attributes.
The obtained user profiles are sent to the dia-
logue manager, and are utilized in the dialogue
management and response generation.
</bodyText>
<sectionHeader confidence="0.9975375" genericHeader="method">
4 Experimental Evaluation of the System
with User Models
</sectionHeader>
<bodyText confidence="0.9999624">
We evaluated the system with the proposed user
models using 20 novice subjects who had not used
the system. The experiment was performed in the
laboratory under adequate control. For the speech
input, the headset microphone was used.
</bodyText>
<subsectionHeader confidence="0.97403">
4.1 Experiment Procedure
</subsectionHeader>
<bodyText confidence="0.999953176470588">
First, we explained the outline of the system to sub-
jects and gave a document in which experiment con-
ditions and scenarios were described. We prepared
two sets of eight scenarios. Subjects were requested
to acquire the bus information according to the sce-
narios using the system with/without the user mod-
els. In the scenarios, neither the concrete names of
bus stops nor the bus number were given. For exam-
ple, one of the scenarios was as follows: “You are in
Kyoto for sightseeing. After visiting the Ginkakuji
temple, you go to Maruyama Park. Supposing such
a situation, please get information on the bus.” We
also set the constraint in order to vary the subjects’
hastiness such as “Please hurry as much as possible
in order to save the charge of your cellular phone.”
The subjects were also told to look over question-
naire items before the experiment, and filled in them
</bodyText>
<table confidence="0.995895">
duration (sec.) # turn
group 1 with UM 51.9 4.03
(with UM → w/o UM)
w/o UM 47.1 4.18
group 2 w/o UM 85.4 8.23
(w/o UM → with UM)
with UM 46.7 4.08
UM: User Model
</table>
<tableCaption confidence="0.9185055">
Table 3: Duration and the number of turns in dia-
logue
</tableCaption>
<bodyText confidence="0.998831206896552">
after using each system. This aims to reduce the sub-
ject’s cognitive load and possible confusion due to
switching the systems (Over, 1999). The question-
naire consisted of eight items, for example, “When
the dialogue did not go well, did the system guide in-
telligibly?” We set seven steps for evaluation about
each item, and the subject selected one of them.
Furthermore, subjects were asked to write down
the obtained information: the name of the bus stop
to get on, the bus number and how much time it
takes before the bus arrives. With this procedure,
we planned to make the experiment condition close
to the realistic one.
The subjects were divided into two groups; a half
(group 1) used the system in the order of “with
user models → without user models”, the other half
(group 2) used in the reverse order.
The dialogue management in the system without
user models is also based on the mixed-initiative
dialogue. The system generates follow-up ques-
tions and guidance if necessary, but behaves in a
fixed manner. Namely, additional cooperative con-
tents corresponding to skill level described in section
3.2 are not generated and the dialogue procedure is
changed only after recognition errors occur. The
system without user models behaves equivalently to
the initial state of the user models: the hastiness is
low, the knowledge level is low and the skill level is
high.
</bodyText>
<sectionHeader confidence="0.840668" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.995812975">
All of the subjects successfully completed the given
task, although they had been allowed to give up if the
system did not work well. Namely, the task success
rate is 100%.
Average dialogue duration and the number of
turns in respective cases are shown in Table 3.
Though the users had not experienced the system at
Table 4: Ratio of utterances for which the skill level
was judged as high
all, they got accustomed to the system very rapidly.
Therefore, as shown in Table 3, the duration and
the number of turns were decreased obviously in the
latter half of the experiment in both groups. How-
ever, in the initial half of the experiment, the group
1 completed with significantly shorter dialogue than
group 2. This means that the incorporation of the
user models is effective for novice users. Table 4
shows a ratio of utterances for which the skill level
was identified as high. The ratio is calculated by di-
viding the number of utterances that were judged as
high skill level by the number of all utterances. The
ratio is much larger for group 1 who initially used
the system with user models. This fact means that
the novice users got accustomed to the system more
rapidly with the user models, because they were in-
structed on the usage by cooperative responses gen-
erated when the skill level is low. The results demon-
strate that cooperative responses generated accord-
ing to the proposed user models can serve as good
guidance for novice users.
In the latter half of the experiment, the dialogue
duration and the number of turns were almost same
between the two groups. This result shows that the
proposed models prevent the dialogue from becom-
ing redundant for skilled users, although generating
cooperative responses for all users made the dia-
logue verbose in general. It suggests that the pro-
posed user models appropriately control the genera-
tion of cooperative responses by detecting characters
of individual users.
</bodyText>
<sectionHeader confidence="0.999787" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99998603030303">
We have presented a framework to realize flexible
interaction by dynamically generating VoiceXML
scripts. This framework realizes mixed-initiative di-
alogues and the generation of cooperative responses
in VoiceXML-based systems.
We have also proposed and evaluated user mod-
els for generating cooperative responses adaptively
to individual users. The proposed user models con-
sist of the three dimensions: skill level to the sys-
tem, knowledge level on the target domain and the
degree of hastiness. The user models are identified
by decision tree using features specific to spoken di-
alogue systems as well as semantic attributes. They
are automatically derived by decision tree learning,
and all features used for skill level and hastiness are
independent of domain-specific knowledge. So, it is
expected that the derived user models can be gener-
ally used in other domains.
The experimental evaluation with 20 novice users
shows that the skill level of novice users was im-
proved more rapidly by incorporating user mod-
els, and accordingly the dialogue duration becomes
shorter more immediately. The result is achieved
by the generated cooperative responses based on the
proposed user models. The proposed user models
also suppress the redundancy by changing the dia-
logue procedure and selecting contents of responses.
Thus, the framework generating VoiceXML
scripts dynamically and the proposed user models
realize a user-adaptive dialogue strategies, in which
the generated cooperative responses serve as good
guidance for novice users without increasing the di-
alogue duration for skilled users.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998317882352941">
Jennifer Chu-Carroll. 2000. MIMIC: An adaptive
mixed initiative spoken dialogue system for informa-
tion queries. In Proc. of the 6th Conf. on applied Nat-
ural Language Processing, pages 97–104.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Proc. IEEE Workshop on Automatic Speech
Recognition and Understanding, pages 80–87.
Stephanie Elzer, Jennifer Chu-Carroll, and Sandra Car-
berry. 2000. Recognizing and utilizing user prefer-
ences in collaborative consultation dialogues. In Proc.
of the 4th Int’l Conf. on User Modeling, pages 19–24.
Timothy J. Hazen, Theresa Burianek, Joseph Polifroni,
and Stephanie Seneff. 2000. Integrating recognition
confidence scoring with language understanding and
dialogue modeling. In Proc. Int’l Conf. Spoken Lan-
guage Processing (ICSLP).
</reference>
<figure confidence="0.99807775">
group 1
(with UM → w/o UM)
group 2
w/o UM
0.72
0.70
0.41
(w/o UM → with UM)
with UM
0.63
with UM
w/o UM
</figure>
<reference confidence="0.998611341463415">
Robert Kass and Tim Finin. 1988. Modeling the user in
natural language systems. Computational Linguistics,
14(3):5–22.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. Int’l Conf. Computational Lin-
guistics (COLING), pages 467–473.
Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and Samir
Bennacef. 1999. The LIMSI ARISE system for
train travel information. In IEEE Int’l Conf. Acoust.,
Speech &amp; Signal Processing (ICASSP).
Diane J. Litman and Shimei Pan. 2000. Predicting and
adapting to poor speech recognition in a spoken dia-
logue system. In Proc. of the 17th National Confer-
ence on Artificial Intelligence (AAAI2000).
Eric Nyberg, Teruko Mitamura, Paul Placeway, Michael
Duggan, and Nobuo Hataoka. 2002. Dialogxml:
Extending voicexml for dynamic dialog manage-
ment. In Proc. of Human Language Technology 2002
(HLT2002), pages 286–291.
Paul Over. 1999. Trec-7 interactive track report. In Proc.
of the 7th Text REtrieval Conference (TREC7).
Andrew Pargellis, Jeff Kuo, and Chin-Hui Lee. 1999.
Automatic dialogue generator creates user defined ap-
plications. In Proc. European Conf. Speech Commun.
&amp; Tech. (EUROSPEECH).
Cecile L. Paris. 1988. Tailoring object descriptions to
a user’s level of expertise. Computational Linguistics,
14(3):64–78.
J. Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo, CA.
http://www.rulequest.com/see5-info.html.
David Sadek. 1999. Design considerations on dia-
logue systems: From theory to technology -the case
of artimis-. In Proc. ESCA workshop on Interactive
Dialogue in Multi-Modal Systems.
Peter van Beek. 1987. A model for generating better
explanations. In Proc. of the 25th Annual Meeting of
the Association for Computational Linguistics (ACL-
87), pages 215–220.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.253926">
<title confidence="0.997218">Flexible Spoken Dialogue System based on User and Dynamic Generation of VoiceXML Scripts</title>
<author confidence="0.9914095">Kazunori Komatani Fumihiro Adachi Shinichi Ueno Tatsuya Kawahara Hiroshi G Okuno</author>
<affiliation confidence="0.674054">Graduate School of Kyoto</affiliation>
<address confidence="0.514332">Yoshida-Hommachi, Sakyo, Kyoto 606-8501,</address>
<abstract confidence="0.990644846153846">We realize a telephone-based collaborative natural language dialogue system. Since natural language involves very various expressions, a large number of VoiceXML scripts need to be prepared to handle all possible input patterns. We realize flexible dialogue management for various user utterances by generating VoiceXML scripts dynamically. Moreover, we address appropriate user modeling in order to generate cooperative responses to each user. Specifically, we set three dimensions of user models: the system, level target domain and the degree of hasti- The models are automatically derived by decision tree learning using real dialogue data collected by the system. Experimental evaluation shows that the cooperative responses adapted to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users. dialogue system, user model, VoiceXML, cooperative responses, dialogue</abstract>
<intro confidence="0.848693">strategy</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>MIMIC: An adaptive mixed initiative spoken dialogue system for information queries.</title>
<date>2000</date>
<booktitle>In Proc. of the 6th Conf. on applied Natural Language Processing,</booktitle>
<pages>97--104</pages>
<contexts>
<context position="3927" citStr="Chu-Carroll, 2000" startWordPosition="586" endWordPosition="587">ained query results. Another problem to realize user-friendly interaction is how to generate cooperative responses. When we consider the responses generated from the system side, the dialogue strategies, which determine when to make guidance and what the system should tell to the user, are the essential factors in spoken dialogue systems. There are many studies in respect of the dialogue strategy such as confirmation management using confidence measures of speech recognition results (Komatani and Kawahara, 2000; Hazen et al., 2000), dynamic change of dialogue initiative (Litman and Pan, 2000; Chu-Carroll, 2000; Lamel et al., 1999), and addition of cooperative contents to system responses (Sadek, 1999). Nevertheless, whether a particular response is cooperative or not depends on individual user’s characteristic. In order to adapt the system’s behavior to individual users, it is necessary to model the user’s patterns (Kass and Finin, 1988). Most of conventional studies on user models have focused on the knowledge of users. Others tried to infer and utilize user’s goals to generate responses adapted to the user (van Beek, 1987; Paris, 1988). Elzer et al. (2000) proposed a method to generate adaptive s</context>
</contexts>
<marker>Chu-Carroll, 2000</marker>
<rawString>Jennifer Chu-Carroll. 2000. MIMIC: An adaptive mixed initiative spoken dialogue system for information queries. In Proc. of the 6th Conf. on applied Natural Language Processing, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wieland Eckert</author>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>User modeling for spoken dialogue system evaluation.</title>
<date>1997</date>
<booktitle>In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="5351" citStr="Eckert et al. (1997)" startWordPosition="809" endWordPosition="812">ins. Moreover, they assumed that the input is text only, which does not contain errors. We propose more comprehensive user models to generate user-adapted responses in spoken dialogue systems taking account of information specific to spoken dialogue. Spoken utterances include various information such as the interval between the utterances, the presence of barge-in and so on, which can be utilized to judge the user’s character. These features also possess generality in spoken dialogue systems because they are not dependent on domainspecific knowledge. As user models in spoken dialogue systems, Eckert et al. (1997) defined stereotypes of users such as patient, submissive and experienced, in order to evaluate spoken dialogue systems by simulation. We introduce user models not for defining users’ behaviors beforehand, but for detecting users’ patterns in real-time interaction. We define three dimensions in the user models: ‘skill level to the system’, ‘knowledge level on the target domain’ and ‘degree of hastiness’. The user models are trained by decision tree learning algorithm, using real data collected from the Kyoto city bus information system. Then, we implement the user models on the system and eval</context>
</contexts>
<marker>Eckert, Levin, Pieraccini, 1997</marker>
<rawString>Wieland Eckert, Esther Levin, and Roberto Pieraccini. 1997. User modeling for spoken dialogue system evaluation. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Elzer</author>
<author>Jennifer Chu-Carroll</author>
<author>Sandra Carberry</author>
</authors>
<title>Recognizing and utilizing user preferences in collaborative consultation dialogues.</title>
<date>2000</date>
<booktitle>In Proc. of the 4th Int’l Conf. on User Modeling,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4486" citStr="Elzer et al. (2000)" startWordPosition="674" endWordPosition="677">dialogue initiative (Litman and Pan, 2000; Chu-Carroll, 2000; Lamel et al., 1999), and addition of cooperative contents to system responses (Sadek, 1999). Nevertheless, whether a particular response is cooperative or not depends on individual user’s characteristic. In order to adapt the system’s behavior to individual users, it is necessary to model the user’s patterns (Kass and Finin, 1988). Most of conventional studies on user models have focused on the knowledge of users. Others tried to infer and utilize user’s goals to generate responses adapted to the user (van Beek, 1987; Paris, 1988). Elzer et al. (2000) proposed a method to generate adaptive suggestions according to users’ preferences. However, these studies depend on knowledge of the target domain greatly, and therefore the user models need to be deliberated manually to be applied to new domains. Moreover, they assumed that the input is text only, which does not contain errors. We propose more comprehensive user models to generate user-adapted responses in spoken dialogue systems taking account of information specific to spoken dialogue. Spoken utterances include various information such as the interval between the utterances, the presence </context>
</contexts>
<marker>Elzer, Chu-Carroll, Carberry, 2000</marker>
<rawString>Stephanie Elzer, Jennifer Chu-Carroll, and Sandra Carberry. 2000. Recognizing and utilizing user preferences in collaborative consultation dialogues. In Proc. of the 4th Int’l Conf. on User Modeling, pages 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy J Hazen</author>
<author>Theresa Burianek</author>
<author>Joseph Polifroni</author>
<author>Stephanie Seneff</author>
</authors>
<title>Integrating recognition confidence scoring with language understanding and dialogue modeling.</title>
<date>2000</date>
<booktitle>In Proc. Int’l Conf. Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context position="3847" citStr="Hazen et al., 2000" startWordPosition="573" endWordPosition="576">more, it enables various behaviors adaptive to the dialogue situations such as obtained query results. Another problem to realize user-friendly interaction is how to generate cooperative responses. When we consider the responses generated from the system side, the dialogue strategies, which determine when to make guidance and what the system should tell to the user, are the essential factors in spoken dialogue systems. There are many studies in respect of the dialogue strategy such as confirmation management using confidence measures of speech recognition results (Komatani and Kawahara, 2000; Hazen et al., 2000), dynamic change of dialogue initiative (Litman and Pan, 2000; Chu-Carroll, 2000; Lamel et al., 1999), and addition of cooperative contents to system responses (Sadek, 1999). Nevertheless, whether a particular response is cooperative or not depends on individual user’s characteristic. In order to adapt the system’s behavior to individual users, it is necessary to model the user’s patterns (Kass and Finin, 1988). Most of conventional studies on user models have focused on the knowledge of users. Others tried to infer and utilize user’s goals to generate responses adapted to the user (van Beek, </context>
</contexts>
<marker>Hazen, Burianek, Polifroni, Seneff, 2000</marker>
<rawString>Timothy J. Hazen, Theresa Burianek, Joseph Polifroni, and Stephanie Seneff. 2000. Integrating recognition confidence scoring with language understanding and dialogue modeling. In Proc. Int’l Conf. Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Kass</author>
<author>Tim Finin</author>
</authors>
<title>Modeling the user in natural language systems.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="4261" citStr="Kass and Finin, 1988" startWordPosition="635" endWordPosition="638">e systems. There are many studies in respect of the dialogue strategy such as confirmation management using confidence measures of speech recognition results (Komatani and Kawahara, 2000; Hazen et al., 2000), dynamic change of dialogue initiative (Litman and Pan, 2000; Chu-Carroll, 2000; Lamel et al., 1999), and addition of cooperative contents to system responses (Sadek, 1999). Nevertheless, whether a particular response is cooperative or not depends on individual user’s characteristic. In order to adapt the system’s behavior to individual users, it is necessary to model the user’s patterns (Kass and Finin, 1988). Most of conventional studies on user models have focused on the knowledge of users. Others tried to infer and utilize user’s goals to generate responses adapted to the user (van Beek, 1987; Paris, 1988). Elzer et al. (2000) proposed a method to generate adaptive suggestions according to users’ preferences. However, these studies depend on knowledge of the target domain greatly, and therefore the user models need to be deliberated manually to be applied to new domains. Moreover, they assumed that the input is text only, which does not contain errors. We propose more comprehensive user models </context>
</contexts>
<marker>Kass, Finin, 1988</marker>
<rawString>Robert Kass and Tim Finin. 1988. Modeling the user in natural language systems. Computational Linguistics, 14(3):5–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Flexible mixed-initiative dialogue management using concept-level confidence measures of speech recognizer output.</title>
<date>2000</date>
<booktitle>In Proc. Int’l Conf. Computational Linguistics (COLING),</booktitle>
<pages>467--473</pages>
<contexts>
<context position="3826" citStr="Komatani and Kawahara, 2000" startWordPosition="569" endWordPosition="572">L scripts in advance. Furthermore, it enables various behaviors adaptive to the dialogue situations such as obtained query results. Another problem to realize user-friendly interaction is how to generate cooperative responses. When we consider the responses generated from the system side, the dialogue strategies, which determine when to make guidance and what the system should tell to the user, are the essential factors in spoken dialogue systems. There are many studies in respect of the dialogue strategy such as confirmation management using confidence measures of speech recognition results (Komatani and Kawahara, 2000; Hazen et al., 2000), dynamic change of dialogue initiative (Litman and Pan, 2000; Chu-Carroll, 2000; Lamel et al., 1999), and addition of cooperative contents to system responses (Sadek, 1999). Nevertheless, whether a particular response is cooperative or not depends on individual user’s characteristic. In order to adapt the system’s behavior to individual users, it is necessary to model the user’s patterns (Kass and Finin, 1988). Most of conventional studies on user models have focused on the knowledge of users. Others tried to infer and utilize user’s goals to generate responses adapted to</context>
</contexts>
<marker>Komatani, Kawahara, 2000</marker>
<rawString>Kazunori Komatani and Tatsuya Kawahara. 2000. Flexible mixed-initiative dialogue management using concept-level confidence measures of speech recognizer output. In Proc. Int’l Conf. Computational Linguistics (COLING), pages 467–473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori Lamel</author>
<author>Sophie Rosset</author>
<author>Jean-Luc Gauvain</author>
<author>Samir Bennacef</author>
</authors>
<title>The LIMSI ARISE system for train travel information.</title>
<date>1999</date>
<booktitle>In IEEE Int’l Conf. Acoust., Speech &amp; Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="3948" citStr="Lamel et al., 1999" startWordPosition="588" endWordPosition="591">. Another problem to realize user-friendly interaction is how to generate cooperative responses. When we consider the responses generated from the system side, the dialogue strategies, which determine when to make guidance and what the system should tell to the user, are the essential factors in spoken dialogue systems. There are many studies in respect of the dialogue strategy such as confirmation management using confidence measures of speech recognition results (Komatani and Kawahara, 2000; Hazen et al., 2000), dynamic change of dialogue initiative (Litman and Pan, 2000; Chu-Carroll, 2000; Lamel et al., 1999), and addition of cooperative contents to system responses (Sadek, 1999). Nevertheless, whether a particular response is cooperative or not depends on individual user’s characteristic. In order to adapt the system’s behavior to individual users, it is necessary to model the user’s patterns (Kass and Finin, 1988). Most of conventional studies on user models have focused on the knowledge of users. Others tried to infer and utilize user’s goals to generate responses adapted to the user (van Beek, 1987; Paris, 1988). Elzer et al. (2000) proposed a method to generate adaptive suggestions according </context>
</contexts>
<marker>Lamel, Rosset, Gauvain, Bennacef, 1999</marker>
<rawString>Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and Samir Bennacef. 1999. The LIMSI ARISE system for train travel information. In IEEE Int’l Conf. Acoust., Speech &amp; Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Shimei Pan</author>
</authors>
<title>Predicting and adapting to poor speech recognition in a spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proc. of the 17th National Conference on Artificial Intelligence (AAAI2000).</booktitle>
<contexts>
<context position="3908" citStr="Litman and Pan, 2000" startWordPosition="582" endWordPosition="585">situations such as obtained query results. Another problem to realize user-friendly interaction is how to generate cooperative responses. When we consider the responses generated from the system side, the dialogue strategies, which determine when to make guidance and what the system should tell to the user, are the essential factors in spoken dialogue systems. There are many studies in respect of the dialogue strategy such as confirmation management using confidence measures of speech recognition results (Komatani and Kawahara, 2000; Hazen et al., 2000), dynamic change of dialogue initiative (Litman and Pan, 2000; Chu-Carroll, 2000; Lamel et al., 1999), and addition of cooperative contents to system responses (Sadek, 1999). Nevertheless, whether a particular response is cooperative or not depends on individual user’s characteristic. In order to adapt the system’s behavior to individual users, it is necessary to model the user’s patterns (Kass and Finin, 1988). Most of conventional studies on user models have focused on the knowledge of users. Others tried to infer and utilize user’s goals to generate responses adapted to the user (van Beek, 1987; Paris, 1988). Elzer et al. (2000) proposed a method to </context>
</contexts>
<marker>Litman, Pan, 2000</marker>
<rawString>Diane J. Litman and Shimei Pan. 2000. Predicting and adapting to poor speech recognition in a spoken dialogue system. In Proc. of the 17th National Conference on Artificial Intelligence (AAAI2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Nyberg</author>
<author>Teruko Mitamura</author>
<author>Paul Placeway</author>
<author>Michael Duggan</author>
<author>Nobuo Hataoka</author>
</authors>
<title>Dialogxml: Extending voicexml for dynamic dialog management.</title>
<date>2002</date>
<booktitle>In Proc. of Human Language Technology</booktitle>
<pages>286--291</pages>
<contexts>
<context position="6898" citStr="Nyberg et al., 2002" startWordPosition="1053" endWordPosition="1056">ts: (1) specifications of system’s prompts, (2) specifications of grammars to accept a user’s utterance, and (3) description of the next behaviors. However, most of existing services using the VoiceXML imposes rigid interaction, in which user utterances are restricted by system-initiated prompts and a user is accordingly allowed to specify only requested items one by one. It is more user-friendly that users can freely convey their requests by natural language expressions. We present a framework to realize flexible interaction by generating VoiceXML scripts dynamically (Pargellis et al., 1999; Nyberg et al., 2002). The framework enables users to express their requests by natural language even in VoiceXML-based systems. Furthermore, cooperative responses in the Kyoto city bus information system that has been developed in our laboratory are also presented in this section. 2.1 Dynamic Generation of VoiceXML Scripts In VoiceXML scripts, acceptable keywords and corresponding next states must be explicitly specified. However, since there exists enormous combinations of keywords in natural language expressions, it is practically impossible to describe all VoiceXML scripts that correspond to the combinations. </context>
</contexts>
<marker>Nyberg, Mitamura, Placeway, Duggan, Hataoka, 2002</marker>
<rawString>Eric Nyberg, Teruko Mitamura, Paul Placeway, Michael Duggan, and Nobuo Hataoka. 2002. Dialogxml: Extending voicexml for dynamic dialog management. In Proc. of Human Language Technology 2002 (HLT2002), pages 286–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Over</author>
</authors>
<title>Trec-7 interactive track report.</title>
<date>1999</date>
<booktitle>In Proc. of the 7th Text REtrieval Conference (TREC7).</booktitle>
<contexts>
<context position="28027" citStr="Over, 1999" startWordPosition="4523" endWordPosition="4524">et the constraint in order to vary the subjects’ hastiness such as “Please hurry as much as possible in order to save the charge of your cellular phone.” The subjects were also told to look over questionnaire items before the experiment, and filled in them duration (sec.) # turn group 1 with UM 51.9 4.03 (with UM → w/o UM) w/o UM 47.1 4.18 group 2 w/o UM 85.4 8.23 (w/o UM → with UM) with UM 46.7 4.08 UM: User Model Table 3: Duration and the number of turns in dialogue after using each system. This aims to reduce the subject’s cognitive load and possible confusion due to switching the systems (Over, 1999). The questionnaire consisted of eight items, for example, “When the dialogue did not go well, did the system guide intelligibly?” We set seven steps for evaluation about each item, and the subject selected one of them. Furthermore, subjects were asked to write down the obtained information: the name of the bus stop to get on, the bus number and how much time it takes before the bus arrives. With this procedure, we planned to make the experiment condition close to the realistic one. The subjects were divided into two groups; a half (group 1) used the system in the order of “with user models → </context>
</contexts>
<marker>Over, 1999</marker>
<rawString>Paul Over. 1999. Trec-7 interactive track report. In Proc. of the 7th Text REtrieval Conference (TREC7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Pargellis</author>
<author>Jeff Kuo</author>
<author>Chin-Hui Lee</author>
</authors>
<title>Automatic dialogue generator creates user defined applications. In</title>
<date>1999</date>
<booktitle>Proc. European Conf. Speech Commun. &amp; Tech.</booktitle>
<publisher>(EUROSPEECH).</publisher>
<contexts>
<context position="6876" citStr="Pargellis et al., 1999" startWordPosition="1049" endWordPosition="1052">pts consist of three parts: (1) specifications of system’s prompts, (2) specifications of grammars to accept a user’s utterance, and (3) description of the next behaviors. However, most of existing services using the VoiceXML imposes rigid interaction, in which user utterances are restricted by system-initiated prompts and a user is accordingly allowed to specify only requested items one by one. It is more user-friendly that users can freely convey their requests by natural language expressions. We present a framework to realize flexible interaction by generating VoiceXML scripts dynamically (Pargellis et al., 1999; Nyberg et al., 2002). The framework enables users to express their requests by natural language even in VoiceXML-based systems. Furthermore, cooperative responses in the Kyoto city bus information system that has been developed in our laboratory are also presented in this section. 2.1 Dynamic Generation of VoiceXML Scripts In VoiceXML scripts, acceptable keywords and corresponding next states must be explicitly specified. However, since there exists enormous combinations of keywords in natural language expressions, it is practically impossible to describe all VoiceXML scripts that correspond</context>
</contexts>
<marker>Pargellis, Kuo, Lee, 1999</marker>
<rawString>Andrew Pargellis, Jeff Kuo, and Chin-Hui Lee. 1999. Automatic dialogue generator creates user defined applications. In Proc. European Conf. Speech Commun. &amp; Tech. (EUROSPEECH).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecile L Paris</author>
</authors>
<title>Tailoring object descriptions to a user’s level of expertise.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="4465" citStr="Paris, 1988" startWordPosition="672" endWordPosition="673">mic change of dialogue initiative (Litman and Pan, 2000; Chu-Carroll, 2000; Lamel et al., 1999), and addition of cooperative contents to system responses (Sadek, 1999). Nevertheless, whether a particular response is cooperative or not depends on individual user’s characteristic. In order to adapt the system’s behavior to individual users, it is necessary to model the user’s patterns (Kass and Finin, 1988). Most of conventional studies on user models have focused on the knowledge of users. Others tried to infer and utilize user’s goals to generate responses adapted to the user (van Beek, 1987; Paris, 1988). Elzer et al. (2000) proposed a method to generate adaptive suggestions according to users’ preferences. However, these studies depend on knowledge of the target domain greatly, and therefore the user models need to be deliberated manually to be applied to new domains. Moreover, they assumed that the input is text only, which does not contain errors. We propose more comprehensive user models to generate user-adapted responses in spoken dialogue systems taking account of information specific to spoken dialogue. Spoken utterances include various information such as the interval between the utte</context>
</contexts>
<marker>Paris, 1988</marker>
<rawString>Cecile L. Paris. 1988. Tailoring object descriptions to a user’s level of expertise. Computational Linguistics, 14(3):64–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA. http://www.rulequest.com/see5-info.html.</location>
<contexts>
<context position="17413" citStr="Quinlan, 1993" startWordPosition="2719" endWordPosition="2720">nd omits complicated contents such as a proposal of another route. The contents described above are also controlled by the hastiness. For users who are not in hurry, the system generates the additional contents that correspond to their skill level and knowledge level as cooperative responses. On the other hand, for hasty users, the contents are omitted to prevent the dialogue from being redundant. 3.3 Classification of User based on Decision Tree In order to implement the proposed user models as classifiers, we adopt a decision tree. It is constructed by decision tree learning algorithm C5.0 (Quinlan, 1993) with data collected by our dialogue system. Figure 3 shows an example of the derived decision tree for the skill level. We use the features listed in Figure 4. They include not only semantic information contained in the utterances but also information specific to spoken dialogue systems such as the silence duration prior to the utterance, the presence of barge-in and so on. Except for the last category of Figure 4 including “attribute of specified bus stops”, most of the features are domain-independent. The classification of each dimension is done for every user utterance except for knowledge</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA. http://www.rulequest.com/see5-info.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sadek</author>
</authors>
<title>Design considerations on dialogue systems: From theory to technology -the case of artimis-.</title>
<date>1999</date>
<booktitle>In Proc. ESCA workshop on Interactive Dialogue in Multi-Modal Systems.</booktitle>
<contexts>
<context position="4020" citStr="Sadek, 1999" startWordPosition="600" endWordPosition="601">rative responses. When we consider the responses generated from the system side, the dialogue strategies, which determine when to make guidance and what the system should tell to the user, are the essential factors in spoken dialogue systems. There are many studies in respect of the dialogue strategy such as confirmation management using confidence measures of speech recognition results (Komatani and Kawahara, 2000; Hazen et al., 2000), dynamic change of dialogue initiative (Litman and Pan, 2000; Chu-Carroll, 2000; Lamel et al., 1999), and addition of cooperative contents to system responses (Sadek, 1999). Nevertheless, whether a particular response is cooperative or not depends on individual user’s characteristic. In order to adapt the system’s behavior to individual users, it is necessary to model the user’s patterns (Kass and Finin, 1988). Most of conventional studies on user models have focused on the knowledge of users. Others tried to infer and utilize user’s goals to generate responses adapted to the user (van Beek, 1987; Paris, 1988). Elzer et al. (2000) proposed a method to generate adaptive suggestions according to users’ preferences. However, these studies depend on knowledge of the</context>
<context position="10402" citStr="Sadek, 1999" startWordPosition="1605" endWordPosition="1606">ently get the bus information. There are 652 bus stops and 85 bus routes in Kyoto city. The bus stops can also be specified by the name of famous places or public facilities nearby. Figure 2 shows a simple example of the dialogue. 2.3 Generation of Cooperative Responses in Kyoto City Bus Information System In spoken dialogue systems, it is important to generate cooperative responses by guessing the user’s intention. The cooperative responses consist of adding information more than the user has requested, telling the reason when there is no query result, presenting the alternatives, and so on (Sadek, 1999). In order to generate cooperative responses, it is necessary to change responses according to the situations. Since we separate language-understanding modules from individual VoiceXML scripts that are generated dynamically, our system can generate appropriate responses cooperatively according to query results. The examples of the cooperative responses implemented in our Kyoto city bus information system are listed below. 2+81-75-326-3116 keywords speech recognizer synthesized speech speech database dialogue manager VWS (Voice Web Server) grammar rules user VoiceXML generator TTS engine respon</context>
<context position="14807" citStr="Sadek, 1999" startWordPosition="2303" endWordPosition="2304">e, we also take account of degree of hastiness of the user, and accordingly change the system’s responses. 3.2 Response Generation Strategy Using User Models Next, we describe the response generation strategies adapted to individual users based on the proposed user models: skill level, knowledge level and hastiness. Basic design of dialogue management is based on mixed-initiative dialogue, in which the system makes follow-up questions and guidance if necessary while allowing a user to utter freely. It is investigated to add various contents to the system responses as cooperative responses in (Sadek, 1999). Such additional information is usually cooperative, but some people may feel such a response redundant. Thus, we introduce the user models and control the generation of additional information. By introducing the proposed user models, the system changes generated responses by the following two aspects: dialogue procedure and contents of responses. Dialogue Procedure The dialogue procedure is changed based on the skill level and the hastiness. If a user is identified as having the high skill level, the dialogue management is carried out in a user-initiated manner; namely, the system generates </context>
</contexts>
<marker>Sadek, 1999</marker>
<rawString>David Sadek. 1999. Design considerations on dialogue systems: From theory to technology -the case of artimis-. In Proc. ESCA workshop on Interactive Dialogue in Multi-Modal Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter van Beek</author>
</authors>
<title>A model for generating better explanations.</title>
<date>1987</date>
<booktitle>In Proc. of the 25th Annual Meeting of the Association for Computational Linguistics (ACL87),</booktitle>
<pages>215--220</pages>
<marker>van Beek, 1987</marker>
<rawString>Peter van Beek. 1987. A model for generating better explanations. In Proc. of the 25th Annual Meeting of the Association for Computational Linguistics (ACL87), pages 215–220.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>