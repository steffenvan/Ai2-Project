<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014411">
<title confidence="0.995867">
ASOBEK: Twitter Paraphrase Identification with Simple Overlap
Features and SVMs
</title>
<author confidence="0.993346">
Asli Eyecioglu and Bill Keller
</author>
<affiliation confidence="0.999489">
Department of Informatics
The University of Sussex
</affiliation>
<address confidence="0.562284">
Brighton, UK
</address>
<email confidence="0.9685685">
A.Eyecioglu@sussex.ac.uk,
billk@sussex.ac.uk
</email>
<sectionHeader confidence="0.995241" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982066666667">
We present an approach to identifying
Twitter paraphrases using simple lexical over-
lap features. The work is part of ongoing re-
search into the applicability of knowledge-
lean techniques to paraphrase identification.
We utilize features based on overlap of word
and character n-grams and train support vector
machine (SVM). Our results demonstrate that
character and word level overlap features in
combination can give performance compara-
ble to methods employing more sophisticated
NLP processing tools and external resources.
We achieve the highest F-score for identifying
paraphrases on the Twitter Paraphrase Corpus
as part of the SemEval-2015 Task1.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997785625">
This paper presents an approach to identifying
Twitter paraphrase pairs using lexical overlap fea-
tures. Paraphrase identification (PI) may be de-
fined as “the task of deciding whether two given
text fragments have the same meaning” (Lintean &amp;
Rus 2011). Methods for identifying paraphrases
thus take a pair of texts and make a binary judg-
ment. The PI task has practical importance in the
Natural Language Processing (NLP) community
because of the pervasive problem of linguistic var-
iation. Accurate methods for PI should help im-
prove the performance of NLP systems that would
seem to require language understanding. This in-
cludes key applications such as question answer-
ing, information retrieval and machine translation,
amongst others. Acquired paraphrases have been
</bodyText>
<page confidence="0.990691">
64
</page>
<bodyText confidence="0.985362709090909">
shown to improve the performance of Statistical
Machine Translation (SMT) systems, for example
(Callison-Burch et al. 2006, Owczarzak et al.,
2006; Madnani et al., 2007)
Many researchers on PI make use of existing
NLP tools and other resources to identify para-
phrases. For example, Duclaye et al., (2002) ex-
ploits the NLP tools of a question answering
system for reformulating rules to identify para-
phrases. Other researchers (Finch et al 2005,
Mihalcea et al 2006, Fernando &amp; Stevenson 2008,
Malakasiotis 2009, Das &amp; Smith 2009) have em-
ployed lexical semantic similarity information
based on resources such as WordNet (Miller,
1995).
Although the PI task aims to identify sentenc-
es that are semantically equivalent, a number of
researchers have shown that classifiers trained on
lexical overlap features may achieve relatively
high accuracy. Good performance is achieved
without the use of knowledge-based semantic fea-
tures or other external knowledge sources such as
parallel corpora (Lintean &amp; Rus 2011, Blacoe &amp;
Lapata, 2012). We consider methods as
knowledge-lean if they make use of just the text at
hand and avoid the use of external processing tools
and other resources. Knowledge-lean PI methods
may thus employ shallow overlap measures based
on lexical items or n-grams, but they might also
make use of distributional techniques where these
are based on simple text statistics.
The work described here is part of ongoing
research that is investigating the extent to which
knowledge-lean techniques may help to identify
paraphrases. Preliminary work has been conducted
using the Microsoft Research Paraphrase Corpus
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 64–69,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
(MSRPC) (Dolan &amp; Brockett, 2005). However, the
approach may be of particular value where
knowledge-based language resources are not readi-
ly available or applicable. In this context, Twitter
presents interesting challenges. Its short texts
(tweets), widespread use of non-standard grammar,
spelling and punctuation, as well as slang, abbrevi-
ations and neologisms, etc. make syntactic and se-
mantic analysis difficult.
We apply a supervised learning approach us-
ing SVMs and learn classifiers based on simple
lexical and character n-gram overlap features.
SVM classifiers benefit from features that are in-
terdependent and informative, so good choice of
feature combinations is crucial. We also experi-
mented with different kernels to find out whether a
non-linear kernel works well for this task.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.993787586206897">
A number of researchers have investigated whether
near state-of-the-art PI results can be obtained
without use of external sources. Blacoe &amp; Lapata
(2012) use distributional methods to find composi-
tional meaning of phrases and sentences. They find
that performance of shallow approaches is compa-
rable to methods that are computationally intensive
or that use very large corpora. Lintean &amp; Rus
(2011) apply word unigrams and bigrams. Bigrams
capture word order information, which can in turn
capture syntactic similarities between two text
fragments. Finch et al. (2005) combines several
MT metrics and uses them as features. Madnani et
al. (2012) also shows that good results are obtained
by combining different MT metrics. Ji &amp; Eisen-
stein (2013) attains state-of-the-art results based on
latent semantic analysis and a new term-weighting
metric, TF-KLD.1
A variety of classifiers has been employed for
the purpose of identifying paraphrases. Kozarova
&amp; Montoyo (2006) measures lexical and semantic
similarity with the combination of different classi-
fiers: k-Nearest Neighbours, Support Vector Ma-
chines, and Maximum Entropy. The SVM
Classifiers remains the most applicable in recent
research whether applied solely (Finch et al., 2005;
Wan et al., 2006) or part of combined classifiers
(Kozoreva &amp; Montotyo, 2006; Lintean &amp; Rus,
2011; Madnani et al, 2012).
</bodyText>
<page confidence="0.477613">
1 State-of-the-art results are shown in Section 5.
</page>
<sectionHeader confidence="0.980194" genericHeader="method">
3 The Task
</sectionHeader>
<bodyText confidence="0.9999335">
The Semeval-2015 task “Paraphrase and Semantic
Similarity in Twitter” involves predicting whether
two tweets have the same meaning. Training and
test data are provided in the form of a Twitter Par-
aphrase Corpus (TPC) (Xu, 2014). The TPC is
constructed semi-randomly and annotated via Am-
azon Mechanical Turk by 5 annotators. It consists
of around 35% paraphrases and 65% non-
paraphrases. Training and development data con-
sists of 18K tweet pairs and 1K test data. Test data
is drawn from a different time period and annotat-
ed by an expert.
</bodyText>
<sectionHeader confidence="0.99909" genericHeader="method">
4 Approach
</sectionHeader>
<subsectionHeader confidence="0.989179">
4.1 Text Preprocessing
</subsectionHeader>
<bodyText confidence="0.99988353125">
Text preprocessing is essential to many NLP appli-
cations. It may involve tokenizing, removal of
punctuation, PoS-tagging, and so on. For identify-
ing paraphrases, this may not always be appropri-
ate. Removing punctuation and stop words, as
commonly done for many NLP applications, argu-
ably results in the loss of information that may be
critical in terms of PI. We therefor keep text pre-
processing to a minimum.
The TPC is already tokenized (O’Connor et
al., 2010), part-of-speech tagged (Derczynski et al.,
2013), and named entity tagged (Ritter et al.,
2011). Here we only experiment on tokenized data,
ignoring part-of-speech and named entity tagged
data. In the next section we also report results for
the MSR Paraphrase Corpus. We used the Rasp
Toolkit (Briscoe et al., 2006) to perform tokeniza-
tion in this case.
A particular issue in dealing with Twitter is
the use of capitalization. Variability in the use of
capitals (some tweets may be uncapitalised, others
written in all uppercase) presents a problem for
simple lexical overlap measures between candidate
paraphrase pairs. To help overcome this, tokenized
tweets are lowercased. Although this potentially
causes confusion between proper nouns and com-
mon nouns (e.g. apple the fruit v. Apple the com-
pany) our experimental work shows that it most
likely increases the quantity of identified para-
phrase pairs.
Tweets tend to have a higher proportion of
out-of-vocabulary (OOV) words than other texts.
</bodyText>
<page confidence="0.997891">
65
</page>
<bodyText confidence="0.999990181818182">
Due to the character limit, words are often short-
ened or abbreviated and standard spelling rules
ignored. In addition, characters may be added for
emphasis. Nevertheless, we have not normalized
the original texts to compensate for this.
A novel aspect of the TPC compared to other
paraphrase corpora is the inclusion of topic infor-
mation, which is also used during the construction
process. Despite the possibility that topic features
might be utilized, we have not made use of this
information in our approach.
</bodyText>
<subsectionHeader confidence="0.992073">
4.2 Features and Instances
</subsectionHeader>
<bodyText confidence="0.996318761904762">
As the basis for deriving a number of overlap fea-
tures, we consider different representations of a
text as a set of tokens, where a token may be either
a word or character n-gram. For the work de-
scribed here we restrict attention to word and char-
acter unigrams and bigrams. Use of a variety of
machine translation techniques (Madnani et al.,
2012) that utilise word n-grams motivated their use
in representing texts for this task. In particular,
word bigrams may provide potentially useful syn-
tactic information about a text. Character bigrams,
on the other hand, allow us to capture similarity
between related word forms. Possible overlap fea-
tures are constructed using basic set operations:
Size of union: the size of the union of the tokens
in the two texts of a candidate paraphrase pair.
Size of intersection: the number of tokens com-
mon to the texts of a candidate paraphrase pair.
Text Size: the size of the set of tokens representing
a given text.
This yields a total of eight possible overlap fea-
tures for a pair of texts, plus four ways of measur-
ing text size. Each data instance is a vector of
features representing a pair of tweets. In order to
select an optimal set of features we ran a number
of preliminary experiments. Table 1 presents the
results for different features and combinations of
features on the development data. We present re-
sults obtained for a linear kernel. The general pat-
tern for an RBF kernel is similar.
Intuitively, knowing about the union, intersec-
tion or size of a text in isolation may not be very
informative. However, for a given token type, the-
se four features in combination provide potentially
useful information about similarity of texts. In the
following, C1 and C2 each denote four features
(union, intersection, sizes of tweet 1 and tweet 2)
produced by character unigrams and bigrams, re-
spectively. Similarly, W1 and W2 denote the four
features generated by word unigrams and bigrams,
respectively. Combinations (e.g. C1W2) represent
eight features: those for C1 plus those for W2.
</bodyText>
<table confidence="0.999884083333333">
Features Acc Pre. Rec. F-sc.
C1 64.5 0.0 0.0 0.0
C2 74.5 70.2 48.4 57.7
C1C2 74.5 70.3 48.5 57.4
W1 74.1 70.5 46.5 56.0
W2 70.5 63.9 38.8 48.3
W1W2 74.0 69.9 46.9 56.2
C1W1 74.2 70.4 47.2 56.5
C2W2 74.9 71.1 49.1 58.1
C1W2 71.4 72.0 31.9 44.2
C2W1 75.6 72.4 50.6 59.6
Baseline 72.6 70.4 38.9 50.1
</table>
<tableCaption confidence="0.999807">
Table 1: Individual and Combined Results by Linear SVM
</tableCaption>
<bodyText confidence="0.999976625">
It is clear that features based on character bi-
grams are more informative than character uni-
grams (for C1, all instances are classified
negative). For words, on the other hand, use of bi-
grams did not improve performance over uni-
grams. However, combining features for words and
characters proved beneficial. Although, the combi-
nation of character and word bigrams increases
performance, combining word unigrams and char-
acter bigrams is more informative. We therefore
chose to represent instances using a combination of
character bigrams and word unigrams. 2
An important step in SVM classification is
rescaling of the features. Apart from a simple scal-
ing mechanism, which is applied during the classi-
fication process, features are kept as they are.
</bodyText>
<subsectionHeader confidence="0.997536">
4.3 SVM Classifiers
</subsectionHeader>
<bodyText confidence="0.861837615384615">
An SVM classifier maps the feature vectors into
high dimensional vector space and computes the
dot product of the two vectors inside the kernel. Its
applicability to both linear and non-linear systems
has been proven for different NLP applications.
We used SVM implementations from scikit-learn
(Pedregosa et al., 2011) and experimented with a
number of classifiers. We report here on results
obtained using SVC adapted from libsvm (Chang
&amp; Lin, 2011) by embedding different kernels. We
2 The submitted system used just six features: four character
bigram features together with just the union and intersection of
word unigrams. This had no impact on performance.
</bodyText>
<page confidence="0.986062">
66
</page>
<bodyText confidence="0.999954142857143">
experimented with linear and Radial Basis Func-
tion (RBF) kernels. Linear kernels are known
to work well with large datasets and RBF kernels
are the first choice if small number of features are
applied (Hsu et al., 2003), which both cases to ap-
ply our datasets. Classifiers are used with their de-
fault parameters and trained on the data provided.
</bodyText>
<sectionHeader confidence="0.999921" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9989915">
Table 2 shows that SVC with a linear kernel
achieved an F-score of 67.4. This represent the
highest score amongst those systems participating
in Task 1, though still some way below Xu et al
(2014) and the human upper-bound. Xu et al.
(2014)’s approach constructs a joint word-sentence
paraphrase model (MULTIP) and utilizes topic
information, which outperforms other features in-
dividually. Table 2 also shows the result for the
RBF kernel, which was not submitted for the task.
For this task the non-linear kernel does not provide
any performance gain over the linear SVM.
</bodyText>
<table confidence="0.999451">
Model Acc. Pre. Rec. F-sc.
Human Upperbound -- 75.2 90.8 82.3
Xu et al. (2014) -- 72.2 72.6 72.4
SVC (linear kernel) 86.5 68.0 66.9 67.4
SVC (rbf kernel) 85.7 64.9 68.6 66.7
Baseline -- 67.9 52.0 58.9
</table>
<tableCaption confidence="0.998729">
Table 2: TPC Results
</tableCaption>
<bodyText confidence="0.999479142857143">
For comparison, Table 3 shows state-of-the-
art results for the PI task on the MSRPC, together
with our classifiers trained using of same set of
features as for the TPC. Our method performs well
above baseline, but with relatively lower precision
than other systems. In contrast to Table 2, our
highest result is obtained using the RBF kernel.
</bodyText>
<table confidence="0.999549615384615">
Model Acc. Pre. Rec. F-sc.
Ji&amp;Eisenstein(2013) 80.4 85.96
Madnani et al (2012) 77.4 - - 84.1
Socher et al. (2011) 76.8 - - 83.6
Wan et al. (2006) 75.6 77.0 90.0 83.0
SVC(rbf kernel) 74.4 74.8 92.9 82.8
Das &amp; Smith (2009) 76.1 79.6 86.1 82.7
Finch et al. (2005) 75.0 76.6 89.8 82.7
Fernando&amp;Stevenson (2008) 74.1 75.2 91.3 82.4
SVC (linear kernel) 73.7 75.0 90.1 82.1
Qiu et al. (2006) 72.0 72.5 93.4 81.6
Zhang and Patrick (2005) 71.9 74.3 88.2 80.7
BASELINE 65.4 71.6 79.5 75.3
</table>
<tableCaption confidence="0.999835">
Table 3: Paraphrase Identification State-of-the-art Results on MSRPC
</tableCaption>
<bodyText confidence="0.9999812">
We note that the features that we adopt as in-
formative for the Twitter PI task outperform some
recent approaches to PI on the MSRPC. This is
encouraging and indicates applicability of
knowledge-lean approaches to other data sets.
</bodyText>
<sectionHeader confidence="0.998459" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999986783783784">
Our results demonstrate that knowledge-lean
methods based on character and word level overlap
features in combination can give good results in
terms of the identification of Twitter paraphrases.
SVM classifiers were successfully used to identify
paraphrase pairs given just a few simple features.
Our approach performed as well as and generally
much better (in terms of F-score) than other, more
sophisticated participating systems.
Overlap of character bigrams was more in-
formative than that of character unigrams. We hy-
pothesize that measuring overlap of character
bigrams provides a way of detecting similarity of
related word-forms. It thus performs a similar
function to stemming or lemmatization in other
domains, whilst retaining some information about
difference. This may be especially helpful with
Twitter, where a variety of idiosyncratic spellings
and short forms may be observed alongside the
usual morphological variants.
A strength of our approach is that pre-
processing is kept to a minimum. This may explain
why our system outperforms other approaches that
use a similar set of overlap features. Methods that
require the removal of stop words, punctuation,
OOV words etc., lose potentially useful infor-
mation. On the other hand, we found that normaliz-
ing tweets with regard to capitalization enhanced
performance of the classifier.
The current work on paraphrase identification
is ongoing. There is clearly room for reaching to
human upper bound shown in Table 2. Our latest
work shows that extending character and word n-
grams to up to 4 is promising and gives perfor-
mance that is close to the state–of-the-art results on
TPC obtained by Xu et al. (2014). We intend to
report on these results in a future paper.
</bodyText>
<sectionHeader confidence="0.897784" genericHeader="references">
References
</sectionHeader>
<footnote confidence="0.814714333333333">
Blacoe, W., &amp; Lapata, M. (2012). A Comparison of
Vector-based Representations for Semantic Com-
position. Proceedings of the 2012 Joint Confer-
</footnote>
<page confidence="0.996627">
67
</page>
<note confidence="0.846138285714286">
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL ’12), (July), 546–556.
Briscoe, E., J. Carroll and R. Watson (2006) The Se-
cond Release of the RASP System. In Proceed-
ings of the COLING/ACL 2006 Interactive
Presentation Sessions. Sydney, Australia, 77-80.
</note>
<reference confidence="0.993885012048193">
Callison-burch, C., Koehn, P., &amp; Osborne, M. (2006).
Improved Statistical Machine Translation Using
Paraphrases. In Proceedings of the main confer-
ence on Human Language Technology Confer-
ence of the North American Chapter of the
Association of Computational Linguistics (HLT-
NAACL ’06). Association for Computational Lin-
guistics (pp. 17-24). Stroudsburg, PA, USA
Chang, C.C. &amp; Lin, C.J. (2011). LIBSVM: a library for
support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2:27:1--
27:27. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm
Das, D., &amp; Smith, N. A. (2009). Paraphrase Identifica-
tion as Probabilistic Quasi-Synchronous Recogni-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1 -
Volume 1 (ACL ’09), Vol. 1. Association for
Computational Linguistics (pp. 468-476).
Stroudsburg, PA, USA.
Derczynski, L., Ritter, A., Clark, S., &amp; Bontcheva, K.
(2013). Twitter Part-of-Speech Tagging for All:
Overcoming Sparse and Noisy Data. In Proceed-
ings of the International Conference on Recent
Advances in Natural Language Processing.
Dolan, W. B. &amp; Brockett, C. (2005). Automatically
Constructing a Corpus of Sentential Paraphrases,
9-16. In Proceedings of The Third International
Workshop on Paraphrasing (IWP2005), Jeju, Re-
public of Korea.
Duclaye, F., Yvon, F., Collin, O., R, F. T., Marzin, P.,
&amp; Cedex, L. (2002). Using the Web as a Linguis-
tic Resource for Learning Reformulations Auto-
matically. In Proceedings of the Third
International Conference on Language Resources
and Evaluation. (pp. 390-396).
Fernando, S., &amp; Stevenson, M. (2008). A Semantic Sim-
ilarity Approach to Paraphrase Detection. In In
Proceedings of the 11th Annual Research Collo-
quium of the UK Special Interest Group for Com-
putational Linguistics (pp. 45–52).
Finch, A., Hwang, Y.-S., &amp; Sumita, E. (2005). Using
Machine Translation Evaluation Techniques to
Determine Sentence-level Semantic Equivalence.
In Proceedings of the Third International Work-
shop on Paraphrasing (IWP2005) (pp. 17–24).
C.-W. Hsu, C.-C. Chang, C.-J. Lin. (2003) A practical
guide to support vector classification. Technical
report, Department of Computer Science, Nation-
al Taiwan University. July.
Ji, Y., &amp; Eisenstein, J. (2013). Discriminative Im-
provements to Distributional Sentence Similarity.
In Proceedings of the 2013 Conference on Empir-
ical Methods in Natural Language Processing
(pp. 891–896). Seattle, Washington, USA: Asso-
ciation for Computational Linguistics.
Kozareva, Z., &amp; Montoyo, A. (2006). Paraphrase Identi-
fication on the Basis of Supervised Machine
Learning Techniques. in Proceedings of the 5th
International Conference on Natural Language
Processing (FinTAL 2006),Lecture Notes in Arti-
ficial Intelligence (pp. 524-533). Turku, Finland.
Lintean, M., &amp; Rus, V. (2011). Dissimilarity Kernels for
Paraphrase Identification. Proceedings of the 24th
International Florida Artificial Intelligence Re-
search Society Conference. (pp. 263-268). Palm
Beach, FL.
Madnani, N., Ayan, N. F., Resnik, P., Dorr, B. J., &amp;
Park, C. (2007). Using Paraphrases for Parameter
Tuning in Statistical Machine Translation. Pro-
ceedings of the Second Workshop on Statistical
Machine Translation (WMT’07). (Vol. 20742).
Prague, Czech Republic: Association for Compu-
tational Linguistics.
Madnani, N., Tetreault, J., &amp; Chodorow, M. (2012). Re-
examining Machine Translation Metrics for Para-
phrase Identification. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies (NAACL HLT ’12)
(pp. 182–190). Stroudsburg, PA, USA
</reference>
<page confidence="0.995251">
68
</page>
<reference confidence="0.999465289855072">
Malakasiotis, P. (2009). Paraphrase Recognition Using
Machine Learning to Combine Similarity
Measures. Proceedings of the ACL-IJCNLP 2009
Student Research Workshop (pp. 27-35). Suntec,
Singapore.
Mihalcea, R., Corley, C., &amp; Strapparava, C. (2006).
Corpus-based and Knowledge-based Measures of
Text Semantic Similarity. In A. Cohn (Ed.), Pro-
ceedings of the 21st national conference on Artifi-
cial intelligence- Volume 1 (pp. 775–780). AAAI
Press.
Miller, G.A. (1995). WordNet: A Lexical Database for
English. Communications of the ACM Vol. 38, No.
11: 39:41.
O’Connor, B., Krieger, M., &amp; Ahn, D. (2010). Tweet-
Motif: Exploratory Search and Topic Summariza-
tion for Twitter. In Proceedings of the Fourth
International AAAI Conference on Weblogs and
Social Media (pp. 384–385). Association for the
Advancement of Artificial Intelligence.
Owczarzak, K., Gorves, D., Genabith, J. V., &amp; Way, A.
(2006). Contextual Bitext-Derived Paraphrases in
Automatic MT Evaluation. In Proceedings of the
Workshop on Statistical Machine Translation
(StatMT ’06). Association for Computational
Linguistics (pp. 86-93). Stroudsburg, PA, USA.
Pedregosa, F., Varoquaux, G., Gramfort, A. , Michel,
V.,Thirion, B.,Grisel, O., Blondel, M., Prettenho-
fer, P., Weiss, R., Dubourg, V., Vanderplas, J.,
Passos, A., Cournapeau, D., Brucher, M., Perrot,
M., Duchesnay, ƒ. (2011). Scikit-learn: Machine
Learning in Python. Journal of Machine Learning
Research, 12, 2825-2830
Qiu, L., Kan, M.-Y., &amp; Chua, T.-S. (2006). Paraphrase
Recognition via Dissimilarity Significance Classi-
fication. Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Pro-
cessing (EMNLP ’06), (July), 18–26.
doi:10.3115/1610075.1610079
Ritter, A., Clark, S., Mausam &amp; Etzioni, O. (2011).
Named entity recognition in tweets: an experi-
mental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP’11). Association for Computi-
tional Linguistics. Stroudsburg, PA, USA, 1524-
1534.
Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., &amp;
Manning, C. D. (2011) Dynamic Pooling and Un-
folding Recursive Autoencoders for Paraphrase
Detection. Science, 1-9.
Wan, S., Dras, M., &amp; Dale, R. (2006). Using Dependen-
cy-Based Features to Take the “ Para-farce ” out
of Paraphrase. In proceedings of the Australasian
Language Technology Workshop (pp. 131-138).
Sydney, Australia.
Xu, W. (2014). Data-Drive Aprroches for Paraphrasing
Across Language Variations. PhD Thesis. Depart-
ment of Computer Science, New York University.
Xu, W., Ritter, A., Callison-Burch, C., Dolan, W., &amp; Ji,
Y. (2014). Extracting Lexically Divergent Para-
phrases from Twitter. Transactions Of The Associ-
ation For Computational Linguistics, 2, 435-448.
Retrieved
fromhttps://tacl2013.cs.columbia.edu/ojs/index.ph
p/tacl/article/view/498
Zhang, Y., &amp; Patrick, J. (2005). Paraphrase Identifica-
tion by Text Canonicalization. In proceedings of
the Australasian Language Technology Workshop
(pp. 160-166). Sydney, Australia.
</reference>
<page confidence="0.999316">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.257566">
<title confidence="0.998543">ASOBEK: Twitter Paraphrase Identification with Simple Overlap Features and SVMs</title>
<author confidence="0.862634">Eyecioglu</author>
<affiliation confidence="0.9985115">Department of The University of</affiliation>
<address confidence="0.651103">Brighton,</address>
<email confidence="0.999477">billk@sussex.ac.uk</email>
<abstract confidence="0.975893933333333">We present an approach to identifying Twitter paraphrases using simple lexical overlap features. The work is part of ongoing reinto the applicability of knowledgeto paraphrase identification. We utilize features based on overlap of word and character n-grams and train support vector machine (SVM). Our results demonstrate that character and word level overlap features in combination can give performance comparable to methods employing more sophisticated NLP processing tools and external resources. We achieve the highest F-score for identifying paraphrases on the Twitter Paraphrase Corpus</abstract>
<note confidence="0.784301">as part of the SemEval-2015 Task1.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Callison-burch</author>
<author>P Koehn</author>
<author>M Osborne</author>
</authors>
<title>Improved Statistical Machine Translation Using Paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLTNAACL ’06). Association for Computational Linguistics</booktitle>
<pages>17--24</pages>
<location>Stroudsburg, PA, USA</location>
<marker>Callison-burch, Koehn, Osborne, 2006</marker>
<rawString>Callison-burch, C., Koehn, P., &amp; Osborne, M. (2006). Improved Statistical Machine Translation Using Paraphrases. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLTNAACL ’06). Association for Computational Linguistics (pp. 17-24). Stroudsburg, PA, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm</note>
<contexts>
<context position="11876" citStr="Chang &amp; Lin, 2011" startWordPosition="1878" endWordPosition="1881">scaling of the features. Apart from a simple scaling mechanism, which is applied during the classification process, features are kept as they are. 4.3 SVM Classifiers An SVM classifier maps the feature vectors into high dimensional vector space and computes the dot product of the two vectors inside the kernel. Its applicability to both linear and non-linear systems has been proven for different NLP applications. We used SVM implementations from scikit-learn (Pedregosa et al., 2011) and experimented with a number of classifiers. We report here on results obtained using SVC adapted from libsvm (Chang &amp; Lin, 2011) by embedding different kernels. We 2 The submitted system used just six features: four character bigram features together with just the union and intersection of word unigrams. This had no impact on performance. 66 experimented with linear and Radial Basis Function (RBF) kernels. Linear kernels are known to work well with large datasets and RBF kernels are the first choice if small number of features are applied (Hsu et al., 2003), which both cases to apply our datasets. Classifiers are used with their default parameters and trained on the data provided. 5 Results Table 2 shows that SVC with </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chang, C.C. &amp; Lin, C.J. (2011). LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition.</title>
<date>2009</date>
<journal>Association for Computational Linguistics</journal>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>468--476</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2196" citStr="Das &amp; Smith 2009" startWordPosition="326" endWordPosition="329">on retrieval and machine translation, amongst others. Acquired paraphrases have been 64 shown to improve the performance of Statistical Machine Translation (SMT) systems, for example (Callison-Burch et al. 2006, Owczarzak et al., 2006; Madnani et al., 2007) Many researchers on PI make use of existing NLP tools and other resources to identify paraphrases. For example, Duclaye et al., (2002) exploits the NLP tools of a question answering system for reformulating rules to identify paraphrases. Other researchers (Finch et al 2005, Mihalcea et al 2006, Fernando &amp; Stevenson 2008, Malakasiotis 2009, Das &amp; Smith 2009) have employed lexical semantic similarity information based on resources such as WordNet (Miller, 1995). Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have shown that classifiers trained on lexical overlap features may achieve relatively high accuracy. Good performance is achieved without the use of knowledge-based semantic features or other external knowledge sources such as parallel corpora (Lintean &amp; Rus 2011, Blacoe &amp; Lapata, 2012). We consider methods as knowledge-lean if they make use of just the text at hand and avoid the use </context>
<context position="13805" citStr="Das &amp; Smith (2009)" startWordPosition="2210" endWordPosition="2213">.6 66.7 Baseline -- 67.9 52.0 58.9 Table 2: TPC Results For comparison, Table 3 shows state-of-theart results for the PI task on the MSRPC, together with our classifiers trained using of same set of features as for the TPC. Our method performs well above baseline, but with relatively lower precision than other systems. In contrast to Table 2, our highest result is obtained using the RBF kernel. Model Acc. Pre. Rec. F-sc. Ji&amp;Eisenstein(2013) 80.4 85.96 Madnani et al (2012) 77.4 - - 84.1 Socher et al. (2011) 76.8 - - 83.6 Wan et al. (2006) 75.6 77.0 90.0 83.0 SVC(rbf kernel) 74.4 74.8 92.9 82.8 Das &amp; Smith (2009) 76.1 79.6 86.1 82.7 Finch et al. (2005) 75.0 76.6 89.8 82.7 Fernando&amp;Stevenson (2008) 74.1 75.2 91.3 82.4 SVC (linear kernel) 73.7 75.0 90.1 82.1 Qiu et al. (2006) 72.0 72.5 93.4 81.6 Zhang and Patrick (2005) 71.9 74.3 88.2 80.7 BASELINE 65.4 71.6 79.5 75.3 Table 3: Paraphrase Identification State-of-the-art Results on MSRPC We note that the features that we adopt as informative for the Twitter PI task outperform some recent approaches to PI on the MSRPC. This is encouraging and indicates applicability of knowledge-lean approaches to other data sets. 6 Conclusions Our results demonstrate that</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Das, D., &amp; Smith, N. A. (2009). Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 -Volume 1 (ACL ’09), Vol. 1. Association for Computational Linguistics (pp. 468-476). Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Derczynski</author>
<author>A Ritter</author>
<author>S Clark</author>
<author>K Bontcheva</author>
</authors>
<title>Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="6782" citStr="Derczynski et al., 2013" startWordPosition="1032" endWordPosition="1035">a. Test data is drawn from a different time period and annotated by an expert. 4 Approach 4.1 Text Preprocessing Text preprocessing is essential to many NLP applications. It may involve tokenizing, removal of punctuation, PoS-tagging, and so on. For identifying paraphrases, this may not always be appropriate. Removing punctuation and stop words, as commonly done for many NLP applications, arguably results in the loss of information that may be critical in terms of PI. We therefor keep text preprocessing to a minimum. The TPC is already tokenized (O’Connor et al., 2010), part-of-speech tagged (Derczynski et al., 2013), and named entity tagged (Ritter et al., 2011). Here we only experiment on tokenized data, ignoring part-of-speech and named entity tagged data. In the next section we also report results for the MSR Paraphrase Corpus. We used the Rasp Toolkit (Briscoe et al., 2006) to perform tokenization in this case. A particular issue in dealing with Twitter is the use of capitalization. Variability in the use of capitals (some tweets may be uncapitalised, others written in all uppercase) presents a problem for simple lexical overlap measures between candidate paraphrase pairs. To help overcome this, toke</context>
</contexts>
<marker>Derczynski, Ritter, Clark, Bontcheva, 2013</marker>
<rawString>Derczynski, L., Ritter, A., Clark, S., &amp; Bontcheva, K. (2013). Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data. In Proceedings of the International Conference on Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Dolan</author>
<author>C Brockett</author>
</authors>
<title>Automatically Constructing a Corpus of Sentential Paraphrases,</title>
<date>2005</date>
<booktitle>In Proceedings of The Third International Workshop on Paraphrasing (IWP2005), Jeju,</booktitle>
<location>Republic of</location>
<contexts>
<context position="3505" citStr="Dolan &amp; Brockett, 2005" startWordPosition="521" endWordPosition="524">mploy shallow overlap measures based on lexical items or n-grams, but they might also make use of distributional techniques where these are based on simple text statistics. The work described here is part of ongoing research that is investigating the extent to which knowledge-lean techniques may help to identify paraphrases. Preliminary work has been conducted using the Microsoft Research Paraphrase Corpus Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 64–69, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics (MSRPC) (Dolan &amp; Brockett, 2005). However, the approach may be of particular value where knowledge-based language resources are not readily available or applicable. In this context, Twitter presents interesting challenges. Its short texts (tweets), widespread use of non-standard grammar, spelling and punctuation, as well as slang, abbreviations and neologisms, etc. make syntactic and semantic analysis difficult. We apply a supervised learning approach using SVMs and learn classifiers based on simple lexical and character n-gram overlap features. SVM classifiers benefit from features that are interdependent and informative, s</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>Dolan, W. B. &amp; Brockett, C. (2005). Automatically Constructing a Corpus of Sentential Paraphrases, 9-16. In Proceedings of The Third International Workshop on Paraphrasing (IWP2005), Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Duclaye</author>
<author>F Yvon</author>
<author>O Collin</author>
<author>F T R</author>
<author>P Marzin</author>
<author>L Cedex</author>
</authors>
<title>Using the Web as a Linguistic Resource for Learning Reformulations Automatically.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation.</booktitle>
<pages>390--396</pages>
<contexts>
<context position="1971" citStr="Duclaye et al., (2002)" startWordPosition="289" endWordPosition="292">ve problem of linguistic variation. Accurate methods for PI should help improve the performance of NLP systems that would seem to require language understanding. This includes key applications such as question answering, information retrieval and machine translation, amongst others. Acquired paraphrases have been 64 shown to improve the performance of Statistical Machine Translation (SMT) systems, for example (Callison-Burch et al. 2006, Owczarzak et al., 2006; Madnani et al., 2007) Many researchers on PI make use of existing NLP tools and other resources to identify paraphrases. For example, Duclaye et al., (2002) exploits the NLP tools of a question answering system for reformulating rules to identify paraphrases. Other researchers (Finch et al 2005, Mihalcea et al 2006, Fernando &amp; Stevenson 2008, Malakasiotis 2009, Das &amp; Smith 2009) have employed lexical semantic similarity information based on resources such as WordNet (Miller, 1995). Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have shown that classifiers trained on lexical overlap features may achieve relatively high accuracy. Good performance is achieved without the use of knowledge-bas</context>
</contexts>
<marker>Duclaye, Yvon, Collin, R, Marzin, Cedex, 2002</marker>
<rawString>Duclaye, F., Yvon, F., Collin, O., R, F. T., Marzin, P., &amp; Cedex, L. (2002). Using the Web as a Linguistic Resource for Learning Reformulations Automatically. In Proceedings of the Third International Conference on Language Resources and Evaluation. (pp. 390-396).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fernando</author>
<author>M Stevenson</author>
</authors>
<title>A Semantic Similarity Approach to Paraphrase Detection. In</title>
<date>2008</date>
<booktitle>In Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics</booktitle>
<pages>45--52</pages>
<contexts>
<context position="2158" citStr="Fernando &amp; Stevenson 2008" startWordPosition="320" endWordPosition="323">ications such as question answering, information retrieval and machine translation, amongst others. Acquired paraphrases have been 64 shown to improve the performance of Statistical Machine Translation (SMT) systems, for example (Callison-Burch et al. 2006, Owczarzak et al., 2006; Madnani et al., 2007) Many researchers on PI make use of existing NLP tools and other resources to identify paraphrases. For example, Duclaye et al., (2002) exploits the NLP tools of a question answering system for reformulating rules to identify paraphrases. Other researchers (Finch et al 2005, Mihalcea et al 2006, Fernando &amp; Stevenson 2008, Malakasiotis 2009, Das &amp; Smith 2009) have employed lexical semantic similarity information based on resources such as WordNet (Miller, 1995). Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have shown that classifiers trained on lexical overlap features may achieve relatively high accuracy. Good performance is achieved without the use of knowledge-based semantic features or other external knowledge sources such as parallel corpora (Lintean &amp; Rus 2011, Blacoe &amp; Lapata, 2012). We consider methods as knowledge-lean if they make use of ju</context>
</contexts>
<marker>Fernando, Stevenson, 2008</marker>
<rawString>Fernando, S., &amp; Stevenson, M. (2008). A Semantic Similarity Approach to Paraphrase Detection. In In Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics (pp. 45–52).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Finch</author>
<author>Y-S Hwang</author>
<author>E Sumita</author>
</authors>
<title>Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</booktitle>
<pages>17--24</pages>
<contexts>
<context position="2110" citStr="Finch et al 2005" startWordPosition="312" endWordPosition="315">e understanding. This includes key applications such as question answering, information retrieval and machine translation, amongst others. Acquired paraphrases have been 64 shown to improve the performance of Statistical Machine Translation (SMT) systems, for example (Callison-Burch et al. 2006, Owczarzak et al., 2006; Madnani et al., 2007) Many researchers on PI make use of existing NLP tools and other resources to identify paraphrases. For example, Duclaye et al., (2002) exploits the NLP tools of a question answering system for reformulating rules to identify paraphrases. Other researchers (Finch et al 2005, Mihalcea et al 2006, Fernando &amp; Stevenson 2008, Malakasiotis 2009, Das &amp; Smith 2009) have employed lexical semantic similarity information based on resources such as WordNet (Miller, 1995). Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have shown that classifiers trained on lexical overlap features may achieve relatively high accuracy. Good performance is achieved without the use of knowledge-based semantic features or other external knowledge sources such as parallel corpora (Lintean &amp; Rus 2011, Blacoe &amp; Lapata, 2012). We consider </context>
<context position="4849" citStr="Finch et al. (2005)" startWordPosition="723" endWordPosition="726">r kernel works well for this task. 2 Related Work A number of researchers have investigated whether near state-of-the-art PI results can be obtained without use of external sources. Blacoe &amp; Lapata (2012) use distributional methods to find compositional meaning of phrases and sentences. They find that performance of shallow approaches is comparable to methods that are computationally intensive or that use very large corpora. Lintean &amp; Rus (2011) apply word unigrams and bigrams. Bigrams capture word order information, which can in turn capture syntactic similarities between two text fragments. Finch et al. (2005) combines several MT metrics and uses them as features. Madnani et al. (2012) also shows that good results are obtained by combining different MT metrics. Ji &amp; Eisenstein (2013) attains state-of-the-art results based on latent semantic analysis and a new term-weighting metric, TF-KLD.1 A variety of classifiers has been employed for the purpose of identifying paraphrases. Kozarova &amp; Montoyo (2006) measures lexical and semantic similarity with the combination of different classifiers: k-Nearest Neighbours, Support Vector Machines, and Maximum Entropy. The SVM Classifiers remains the most applica</context>
<context position="13845" citStr="Finch et al. (2005)" startWordPosition="2218" endWordPosition="2221">e 2: TPC Results For comparison, Table 3 shows state-of-theart results for the PI task on the MSRPC, together with our classifiers trained using of same set of features as for the TPC. Our method performs well above baseline, but with relatively lower precision than other systems. In contrast to Table 2, our highest result is obtained using the RBF kernel. Model Acc. Pre. Rec. F-sc. Ji&amp;Eisenstein(2013) 80.4 85.96 Madnani et al (2012) 77.4 - - 84.1 Socher et al. (2011) 76.8 - - 83.6 Wan et al. (2006) 75.6 77.0 90.0 83.0 SVC(rbf kernel) 74.4 74.8 92.9 82.8 Das &amp; Smith (2009) 76.1 79.6 86.1 82.7 Finch et al. (2005) 75.0 76.6 89.8 82.7 Fernando&amp;Stevenson (2008) 74.1 75.2 91.3 82.4 SVC (linear kernel) 73.7 75.0 90.1 82.1 Qiu et al. (2006) 72.0 72.5 93.4 81.6 Zhang and Patrick (2005) 71.9 74.3 88.2 80.7 BASELINE 65.4 71.6 79.5 75.3 Table 3: Paraphrase Identification State-of-the-art Results on MSRPC We note that the features that we adopt as informative for the Twitter PI task outperform some recent approaches to PI on the MSRPC. This is encouraging and indicates applicability of knowledge-lean approaches to other data sets. 6 Conclusions Our results demonstrate that knowledge-lean methods based on charact</context>
</contexts>
<marker>Finch, Hwang, Sumita, 2005</marker>
<rawString>Finch, A., Hwang, Y.-S., &amp; Sumita, E. (2005). Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005) (pp. 17–24).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-W Hsu</author>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>A practical guide to support vector classification.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, National Taiwan University.</institution>
<contexts>
<context position="12311" citStr="Hsu et al., 2003" startWordPosition="1950" endWordPosition="1953">tations from scikit-learn (Pedregosa et al., 2011) and experimented with a number of classifiers. We report here on results obtained using SVC adapted from libsvm (Chang &amp; Lin, 2011) by embedding different kernels. We 2 The submitted system used just six features: four character bigram features together with just the union and intersection of word unigrams. This had no impact on performance. 66 experimented with linear and Radial Basis Function (RBF) kernels. Linear kernels are known to work well with large datasets and RBF kernels are the first choice if small number of features are applied (Hsu et al., 2003), which both cases to apply our datasets. Classifiers are used with their default parameters and trained on the data provided. 5 Results Table 2 shows that SVC with a linear kernel achieved an F-score of 67.4. This represent the highest score amongst those systems participating in Task 1, though still some way below Xu et al (2014) and the human upper-bound. Xu et al. (2014)’s approach constructs a joint word-sentence paraphrase model (MULTIP) and utilizes topic information, which outperforms other features individually. Table 2 also shows the result for the RBF kernel, which was not submitted</context>
</contexts>
<marker>Hsu, Chang, Lin, 2003</marker>
<rawString>C.-W. Hsu, C.-C. Chang, C.-J. Lin. (2003) A practical guide to support vector classification. Technical report, Department of Computer Science, National Taiwan University. July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ji</author>
<author>J Eisenstein</author>
</authors>
<title>Discriminative Improvements to Distributional Sentence Similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</booktitle>
<pages>891--896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA:</location>
<contexts>
<context position="5026" citStr="Ji &amp; Eisenstein (2013)" startWordPosition="752" endWordPosition="756">ources. Blacoe &amp; Lapata (2012) use distributional methods to find compositional meaning of phrases and sentences. They find that performance of shallow approaches is comparable to methods that are computationally intensive or that use very large corpora. Lintean &amp; Rus (2011) apply word unigrams and bigrams. Bigrams capture word order information, which can in turn capture syntactic similarities between two text fragments. Finch et al. (2005) combines several MT metrics and uses them as features. Madnani et al. (2012) also shows that good results are obtained by combining different MT metrics. Ji &amp; Eisenstein (2013) attains state-of-the-art results based on latent semantic analysis and a new term-weighting metric, TF-KLD.1 A variety of classifiers has been employed for the purpose of identifying paraphrases. Kozarova &amp; Montoyo (2006) measures lexical and semantic similarity with the combination of different classifiers: k-Nearest Neighbours, Support Vector Machines, and Maximum Entropy. The SVM Classifiers remains the most applicable in recent research whether applied solely (Finch et al., 2005; Wan et al., 2006) or part of combined classifiers (Kozoreva &amp; Montotyo, 2006; Lintean &amp; Rus, 2011; Madnani et </context>
</contexts>
<marker>Ji, Eisenstein, 2013</marker>
<rawString>Ji, Y., &amp; Eisenstein, J. (2013). Discriminative Improvements to Distributional Sentence Similarity. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (pp. 891–896). Seattle, Washington, USA: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>A Montoyo</author>
</authors>
<title>Paraphrase Identification on the Basis of Supervised Machine Learning Techniques.</title>
<date>2006</date>
<booktitle>in Proceedings of the 5th International Conference on Natural Language Processing (FinTAL 2006),Lecture Notes in Artificial Intelligence</booktitle>
<pages>524--533</pages>
<location>Turku, Finland.</location>
<marker>Kozareva, Montoyo, 2006</marker>
<rawString>Kozareva, Z., &amp; Montoyo, A. (2006). Paraphrase Identification on the Basis of Supervised Machine Learning Techniques. in Proceedings of the 5th International Conference on Natural Language Processing (FinTAL 2006),Lecture Notes in Artificial Intelligence (pp. 524-533). Turku, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lintean</author>
<author>V Rus</author>
</authors>
<title>Dissimilarity Kernels for Paraphrase Identification.</title>
<date>2011</date>
<booktitle>Proceedings of the 24th International Florida Artificial Intelligence Research Society Conference.</booktitle>
<pages>263--268</pages>
<location>Palm Beach, FL.</location>
<contexts>
<context position="1147" citStr="Lintean &amp; Rus 2011" startWordPosition="161" endWordPosition="164"> vector machine (SVM). Our results demonstrate that character and word level overlap features in combination can give performance comparable to methods employing more sophisticated NLP processing tools and external resources. We achieve the highest F-score for identifying paraphrases on the Twitter Paraphrase Corpus as part of the SemEval-2015 Task1. 1 Introduction This paper presents an approach to identifying Twitter paraphrase pairs using lexical overlap features. Paraphrase identification (PI) may be defined as “the task of deciding whether two given text fragments have the same meaning” (Lintean &amp; Rus 2011). Methods for identifying paraphrases thus take a pair of texts and make a binary judgment. The PI task has practical importance in the Natural Language Processing (NLP) community because of the pervasive problem of linguistic variation. Accurate methods for PI should help improve the performance of NLP systems that would seem to require language understanding. This includes key applications such as question answering, information retrieval and machine translation, amongst others. Acquired paraphrases have been 64 shown to improve the performance of Statistical Machine Translation (SMT) system</context>
<context position="2672" citStr="Lintean &amp; Rus 2011" startWordPosition="397" endWordPosition="400">dentify paraphrases. Other researchers (Finch et al 2005, Mihalcea et al 2006, Fernando &amp; Stevenson 2008, Malakasiotis 2009, Das &amp; Smith 2009) have employed lexical semantic similarity information based on resources such as WordNet (Miller, 1995). Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have shown that classifiers trained on lexical overlap features may achieve relatively high accuracy. Good performance is achieved without the use of knowledge-based semantic features or other external knowledge sources such as parallel corpora (Lintean &amp; Rus 2011, Blacoe &amp; Lapata, 2012). We consider methods as knowledge-lean if they make use of just the text at hand and avoid the use of external processing tools and other resources. Knowledge-lean PI methods may thus employ shallow overlap measures based on lexical items or n-grams, but they might also make use of distributional techniques where these are based on simple text statistics. The work described here is part of ongoing research that is investigating the extent to which knowledge-lean techniques may help to identify paraphrases. Preliminary work has been conducted using the Microsoft Researc</context>
<context position="4679" citStr="Lintean &amp; Rus (2011)" startWordPosition="698" endWordPosition="701">res that are interdependent and informative, so good choice of feature combinations is crucial. We also experimented with different kernels to find out whether a non-linear kernel works well for this task. 2 Related Work A number of researchers have investigated whether near state-of-the-art PI results can be obtained without use of external sources. Blacoe &amp; Lapata (2012) use distributional methods to find compositional meaning of phrases and sentences. They find that performance of shallow approaches is comparable to methods that are computationally intensive or that use very large corpora. Lintean &amp; Rus (2011) apply word unigrams and bigrams. Bigrams capture word order information, which can in turn capture syntactic similarities between two text fragments. Finch et al. (2005) combines several MT metrics and uses them as features. Madnani et al. (2012) also shows that good results are obtained by combining different MT metrics. Ji &amp; Eisenstein (2013) attains state-of-the-art results based on latent semantic analysis and a new term-weighting metric, TF-KLD.1 A variety of classifiers has been employed for the purpose of identifying paraphrases. Kozarova &amp; Montoyo (2006) measures lexical and semantic </context>
</contexts>
<marker>Lintean, Rus, 2011</marker>
<rawString>Lintean, M., &amp; Rus, V. (2011). Dissimilarity Kernels for Paraphrase Identification. Proceedings of the 24th International Florida Artificial Intelligence Research Society Conference. (pp. 263-268). Palm Beach, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>N F Ayan</author>
<author>P Resnik</author>
<author>B J Dorr</author>
<author>C Park</author>
</authors>
<title>Using Paraphrases for Parameter Tuning in Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>Proceedings of the Second Workshop on Statistical Machine Translation (WMT’07).</booktitle>
<volume>20742</volume>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic:</location>
<contexts>
<context position="1836" citStr="Madnani et al., 2007" startWordPosition="266" endWordPosition="269">make a binary judgment. The PI task has practical importance in the Natural Language Processing (NLP) community because of the pervasive problem of linguistic variation. Accurate methods for PI should help improve the performance of NLP systems that would seem to require language understanding. This includes key applications such as question answering, information retrieval and machine translation, amongst others. Acquired paraphrases have been 64 shown to improve the performance of Statistical Machine Translation (SMT) systems, for example (Callison-Burch et al. 2006, Owczarzak et al., 2006; Madnani et al., 2007) Many researchers on PI make use of existing NLP tools and other resources to identify paraphrases. For example, Duclaye et al., (2002) exploits the NLP tools of a question answering system for reformulating rules to identify paraphrases. Other researchers (Finch et al 2005, Mihalcea et al 2006, Fernando &amp; Stevenson 2008, Malakasiotis 2009, Das &amp; Smith 2009) have employed lexical semantic similarity information based on resources such as WordNet (Miller, 1995). Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have shown that classifiers </context>
</contexts>
<marker>Madnani, Ayan, Resnik, Dorr, Park, 2007</marker>
<rawString>Madnani, N., Ayan, N. F., Resnik, P., Dorr, B. J., &amp; Park, C. (2007). Using Paraphrases for Parameter Tuning in Statistical Machine Translation. Proceedings of the Second Workshop on Statistical Machine Translation (WMT’07). (Vol. 20742). Prague, Czech Republic: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>Reexamining Machine Translation Metrics for Paraphrase Identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT ’12)</booktitle>
<pages>182--190</pages>
<location>Stroudsburg, PA, USA</location>
<contexts>
<context position="4926" citStr="Madnani et al. (2012)" startWordPosition="736" endWordPosition="739">ave investigated whether near state-of-the-art PI results can be obtained without use of external sources. Blacoe &amp; Lapata (2012) use distributional methods to find compositional meaning of phrases and sentences. They find that performance of shallow approaches is comparable to methods that are computationally intensive or that use very large corpora. Lintean &amp; Rus (2011) apply word unigrams and bigrams. Bigrams capture word order information, which can in turn capture syntactic similarities between two text fragments. Finch et al. (2005) combines several MT metrics and uses them as features. Madnani et al. (2012) also shows that good results are obtained by combining different MT metrics. Ji &amp; Eisenstein (2013) attains state-of-the-art results based on latent semantic analysis and a new term-weighting metric, TF-KLD.1 A variety of classifiers has been employed for the purpose of identifying paraphrases. Kozarova &amp; Montoyo (2006) measures lexical and semantic similarity with the combination of different classifiers: k-Nearest Neighbours, Support Vector Machines, and Maximum Entropy. The SVM Classifiers remains the most applicable in recent research whether applied solely (Finch et al., 2005; Wan et al.</context>
<context position="8619" citStr="Madnani et al., 2012" startWordPosition="1333" endWordPosition="1336">mpared to other paraphrase corpora is the inclusion of topic information, which is also used during the construction process. Despite the possibility that topic features might be utilized, we have not made use of this information in our approach. 4.2 Features and Instances As the basis for deriving a number of overlap features, we consider different representations of a text as a set of tokens, where a token may be either a word or character n-gram. For the work described here we restrict attention to word and character unigrams and bigrams. Use of a variety of machine translation techniques (Madnani et al., 2012) that utilise word n-grams motivated their use in representing texts for this task. In particular, word bigrams may provide potentially useful syntactic information about a text. Character bigrams, on the other hand, allow us to capture similarity between related word forms. Possible overlap features are constructed using basic set operations: Size of union: the size of the union of the tokens in the two texts of a candidate paraphrase pair. Size of intersection: the number of tokens common to the texts of a candidate paraphrase pair. Text Size: the size of the set of tokens representing a giv</context>
<context position="13663" citStr="Madnani et al (2012)" startWordPosition="2180" endWordPosition="2183">-sc. Human Upperbound -- 75.2 90.8 82.3 Xu et al. (2014) -- 72.2 72.6 72.4 SVC (linear kernel) 86.5 68.0 66.9 67.4 SVC (rbf kernel) 85.7 64.9 68.6 66.7 Baseline -- 67.9 52.0 58.9 Table 2: TPC Results For comparison, Table 3 shows state-of-theart results for the PI task on the MSRPC, together with our classifiers trained using of same set of features as for the TPC. Our method performs well above baseline, but with relatively lower precision than other systems. In contrast to Table 2, our highest result is obtained using the RBF kernel. Model Acc. Pre. Rec. F-sc. Ji&amp;Eisenstein(2013) 80.4 85.96 Madnani et al (2012) 77.4 - - 84.1 Socher et al. (2011) 76.8 - - 83.6 Wan et al. (2006) 75.6 77.0 90.0 83.0 SVC(rbf kernel) 74.4 74.8 92.9 82.8 Das &amp; Smith (2009) 76.1 79.6 86.1 82.7 Finch et al. (2005) 75.0 76.6 89.8 82.7 Fernando&amp;Stevenson (2008) 74.1 75.2 91.3 82.4 SVC (linear kernel) 73.7 75.0 90.1 82.1 Qiu et al. (2006) 72.0 72.5 93.4 81.6 Zhang and Patrick (2005) 71.9 74.3 88.2 80.7 BASELINE 65.4 71.6 79.5 75.3 Table 3: Paraphrase Identification State-of-the-art Results on MSRPC We note that the features that we adopt as informative for the Twitter PI task outperform some recent approaches to PI on the MSRP</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Madnani, N., Tetreault, J., &amp; Chodorow, M. (2012). Reexamining Machine Translation Metrics for Paraphrase Identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT ’12) (pp. 182–190). Stroudsburg, PA, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
</authors>
<title>Paraphrase Recognition Using Machine Learning to Combine Similarity Measures.</title>
<date>2009</date>
<booktitle>Proceedings of the ACL-IJCNLP 2009 Student Research Workshop</booktitle>
<pages>27--35</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="2177" citStr="Malakasiotis 2009" startWordPosition="324" endWordPosition="325">nswering, information retrieval and machine translation, amongst others. Acquired paraphrases have been 64 shown to improve the performance of Statistical Machine Translation (SMT) systems, for example (Callison-Burch et al. 2006, Owczarzak et al., 2006; Madnani et al., 2007) Many researchers on PI make use of existing NLP tools and other resources to identify paraphrases. For example, Duclaye et al., (2002) exploits the NLP tools of a question answering system for reformulating rules to identify paraphrases. Other researchers (Finch et al 2005, Mihalcea et al 2006, Fernando &amp; Stevenson 2008, Malakasiotis 2009, Das &amp; Smith 2009) have employed lexical semantic similarity information based on resources such as WordNet (Miller, 1995). Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have shown that classifiers trained on lexical overlap features may achieve relatively high accuracy. Good performance is achieved without the use of knowledge-based semantic features or other external knowledge sources such as parallel corpora (Lintean &amp; Rus 2011, Blacoe &amp; Lapata, 2012). We consider methods as knowledge-lean if they make use of just the text at hand</context>
</contexts>
<marker>Malakasiotis, 2009</marker>
<rawString>Malakasiotis, P. (2009). Paraphrase Recognition Using Machine Learning to Combine Similarity Measures. Proceedings of the ACL-IJCNLP 2009 Student Research Workshop (pp. 27-35). Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In A. Cohn (Ed.), Proceedings of the 21st national conference on Artificial intelligence- Volume</booktitle>
<volume>1</volume>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="2131" citStr="Mihalcea et al 2006" startWordPosition="316" endWordPosition="319">his includes key applications such as question answering, information retrieval and machine translation, amongst others. Acquired paraphrases have been 64 shown to improve the performance of Statistical Machine Translation (SMT) systems, for example (Callison-Burch et al. 2006, Owczarzak et al., 2006; Madnani et al., 2007) Many researchers on PI make use of existing NLP tools and other resources to identify paraphrases. For example, Duclaye et al., (2002) exploits the NLP tools of a question answering system for reformulating rules to identify paraphrases. Other researchers (Finch et al 2005, Mihalcea et al 2006, Fernando &amp; Stevenson 2008, Malakasiotis 2009, Das &amp; Smith 2009) have employed lexical semantic similarity information based on resources such as WordNet (Miller, 1995). Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have shown that classifiers trained on lexical overlap features may achieve relatively high accuracy. Good performance is achieved without the use of knowledge-based semantic features or other external knowledge sources such as parallel corpora (Lintean &amp; Rus 2011, Blacoe &amp; Lapata, 2012). We consider methods as knowledge-</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Mihalcea, R., Corley, C., &amp; Strapparava, C. (2006). Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In A. Cohn (Ed.), Proceedings of the 21st national conference on Artificial intelligence- Volume 1 (pp. 775–780). AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM</journal>
<volume>38</volume>
<pages>39--41</pages>
<contexts>
<context position="2300" citStr="Miller, 1995" startWordPosition="343" endWordPosition="344"> performance of Statistical Machine Translation (SMT) systems, for example (Callison-Burch et al. 2006, Owczarzak et al., 2006; Madnani et al., 2007) Many researchers on PI make use of existing NLP tools and other resources to identify paraphrases. For example, Duclaye et al., (2002) exploits the NLP tools of a question answering system for reformulating rules to identify paraphrases. Other researchers (Finch et al 2005, Mihalcea et al 2006, Fernando &amp; Stevenson 2008, Malakasiotis 2009, Das &amp; Smith 2009) have employed lexical semantic similarity information based on resources such as WordNet (Miller, 1995). Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have shown that classifiers trained on lexical overlap features may achieve relatively high accuracy. Good performance is achieved without the use of knowledge-based semantic features or other external knowledge sources such as parallel corpora (Lintean &amp; Rus 2011, Blacoe &amp; Lapata, 2012). We consider methods as knowledge-lean if they make use of just the text at hand and avoid the use of external processing tools and other resources. Knowledge-lean PI methods may thus employ shallow over</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller, G.A. (1995). WordNet: A Lexical Database for English. Communications of the ACM Vol. 38, No. 11: 39:41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B O’Connor</author>
<author>M Krieger</author>
<author>D Ahn</author>
</authors>
<title>TweetMotif: Exploratory Search and Topic Summarization for Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media (pp. 384–385). Association for the Advancement of Artificial Intelligence.</booktitle>
<marker>O’Connor, Krieger, Ahn, 2010</marker>
<rawString>O’Connor, B., Krieger, M., &amp; Ahn, D. (2010). TweetMotif: Exploratory Search and Topic Summarization for Twitter. In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media (pp. 384–385). Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Owczarzak</author>
<author>D Gorves</author>
<author>J V Genabith</author>
<author>A Way</author>
</authors>
<title>Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation (StatMT ’06). Association for Computational Linguistics</booktitle>
<pages>86--93</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1813" citStr="Owczarzak et al., 2006" startWordPosition="262" endWordPosition="265">ake a pair of texts and make a binary judgment. The PI task has practical importance in the Natural Language Processing (NLP) community because of the pervasive problem of linguistic variation. Accurate methods for PI should help improve the performance of NLP systems that would seem to require language understanding. This includes key applications such as question answering, information retrieval and machine translation, amongst others. Acquired paraphrases have been 64 shown to improve the performance of Statistical Machine Translation (SMT) systems, for example (Callison-Burch et al. 2006, Owczarzak et al., 2006; Madnani et al., 2007) Many researchers on PI make use of existing NLP tools and other resources to identify paraphrases. For example, Duclaye et al., (2002) exploits the NLP tools of a question answering system for reformulating rules to identify paraphrases. Other researchers (Finch et al 2005, Mihalcea et al 2006, Fernando &amp; Stevenson 2008, Malakasiotis 2009, Das &amp; Smith 2009) have employed lexical semantic similarity information based on resources such as WordNet (Miller, 1995). Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have </context>
</contexts>
<marker>Owczarzak, Gorves, Genabith, Way, 2006</marker>
<rawString>Owczarzak, K., Gorves, D., Genabith, J. V., &amp; Way, A. (2006). Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation. In Proceedings of the Workshop on Statistical Machine Translation (StatMT ’06). Association for Computational Linguistics (pp. 86-93). Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>ƒ Duchesnay</author>
</authors>
<title>Scikit-learn: Machine Learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2825--2830</pages>
<marker>Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>Pedregosa, F., Varoquaux, G., Gramfort, A. , Michel, V.,Thirion, B.,Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, ƒ. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825-2830</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M-Y Kan</author>
<author>T-S Chua</author>
</authors>
<title>Paraphrase Recognition via Dissimilarity Significance Classification.</title>
<date>2006</date>
<booktitle>Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP ’06),</booktitle>
<pages>10--3115</pages>
<contexts>
<context position="13969" citStr="Qiu et al. (2006)" startWordPosition="2239" endWordPosition="2242">iers trained using of same set of features as for the TPC. Our method performs well above baseline, but with relatively lower precision than other systems. In contrast to Table 2, our highest result is obtained using the RBF kernel. Model Acc. Pre. Rec. F-sc. Ji&amp;Eisenstein(2013) 80.4 85.96 Madnani et al (2012) 77.4 - - 84.1 Socher et al. (2011) 76.8 - - 83.6 Wan et al. (2006) 75.6 77.0 90.0 83.0 SVC(rbf kernel) 74.4 74.8 92.9 82.8 Das &amp; Smith (2009) 76.1 79.6 86.1 82.7 Finch et al. (2005) 75.0 76.6 89.8 82.7 Fernando&amp;Stevenson (2008) 74.1 75.2 91.3 82.4 SVC (linear kernel) 73.7 75.0 90.1 82.1 Qiu et al. (2006) 72.0 72.5 93.4 81.6 Zhang and Patrick (2005) 71.9 74.3 88.2 80.7 BASELINE 65.4 71.6 79.5 75.3 Table 3: Paraphrase Identification State-of-the-art Results on MSRPC We note that the features that we adopt as informative for the Twitter PI task outperform some recent approaches to PI on the MSRPC. This is encouraging and indicates applicability of knowledge-lean approaches to other data sets. 6 Conclusions Our results demonstrate that knowledge-lean methods based on character and word level overlap features in combination can give good results in terms of the identification of Twitter paraphrase</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>Qiu, L., Kan, M.-Y., &amp; Chua, T.-S. (2006). Paraphrase Recognition via Dissimilarity Significance Classification. Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP ’06), (July), 18–26. doi:10.3115/1610075.1610079</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>S Clark</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’11). Association for Computitional Linguistics.</booktitle>
<pages>1524--1534</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="6829" citStr="Ritter et al., 2011" startWordPosition="1040" endWordPosition="1043">and annotated by an expert. 4 Approach 4.1 Text Preprocessing Text preprocessing is essential to many NLP applications. It may involve tokenizing, removal of punctuation, PoS-tagging, and so on. For identifying paraphrases, this may not always be appropriate. Removing punctuation and stop words, as commonly done for many NLP applications, arguably results in the loss of information that may be critical in terms of PI. We therefor keep text preprocessing to a minimum. The TPC is already tokenized (O’Connor et al., 2010), part-of-speech tagged (Derczynski et al., 2013), and named entity tagged (Ritter et al., 2011). Here we only experiment on tokenized data, ignoring part-of-speech and named entity tagged data. In the next section we also report results for the MSR Paraphrase Corpus. We used the Rasp Toolkit (Briscoe et al., 2006) to perform tokenization in this case. A particular issue in dealing with Twitter is the use of capitalization. Variability in the use of capitals (some tweets may be uncapitalised, others written in all uppercase) presents a problem for simple lexical overlap measures between candidate paraphrase pairs. To help overcome this, tokenized tweets are lowercased. Although this pote</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Ritter, A., Clark, S., Mausam &amp; Etzioni, O. (2011). Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’11). Association for Computitional Linguistics. Stroudsburg, PA, USA, 1524-1534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>E H Huang</author>
<author>J Pennington</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<journal>Science,</journal>
<pages>1--9</pages>
<contexts>
<context position="13698" citStr="Socher et al. (2011)" startWordPosition="2188" endWordPosition="2191">82.3 Xu et al. (2014) -- 72.2 72.6 72.4 SVC (linear kernel) 86.5 68.0 66.9 67.4 SVC (rbf kernel) 85.7 64.9 68.6 66.7 Baseline -- 67.9 52.0 58.9 Table 2: TPC Results For comparison, Table 3 shows state-of-theart results for the PI task on the MSRPC, together with our classifiers trained using of same set of features as for the TPC. Our method performs well above baseline, but with relatively lower precision than other systems. In contrast to Table 2, our highest result is obtained using the RBF kernel. Model Acc. Pre. Rec. F-sc. Ji&amp;Eisenstein(2013) 80.4 85.96 Madnani et al (2012) 77.4 - - 84.1 Socher et al. (2011) 76.8 - - 83.6 Wan et al. (2006) 75.6 77.0 90.0 83.0 SVC(rbf kernel) 74.4 74.8 92.9 82.8 Das &amp; Smith (2009) 76.1 79.6 86.1 82.7 Finch et al. (2005) 75.0 76.6 89.8 82.7 Fernando&amp;Stevenson (2008) 74.1 75.2 91.3 82.4 SVC (linear kernel) 73.7 75.0 90.1 82.1 Qiu et al. (2006) 72.0 72.5 93.4 81.6 Zhang and Patrick (2005) 71.9 74.3 88.2 80.7 BASELINE 65.4 71.6 79.5 75.3 Table 3: Paraphrase Identification State-of-the-art Results on MSRPC We note that the features that we adopt as informative for the Twitter PI task outperform some recent approaches to PI on the MSRPC. This is encouraging and indicate</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., &amp; Manning, C. D. (2011) Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. Science, 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wan</author>
<author>M Dras</author>
<author>R Dale</author>
</authors>
<title>Using Dependency-Based Features to Take the “ Para-farce ” out of Paraphrase.</title>
<date>2006</date>
<booktitle>In proceedings of the Australasian Language Technology Workshop</booktitle>
<pages>131--138</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="5533" citStr="Wan et al., 2006" startWordPosition="826" endWordPosition="829">al. (2012) also shows that good results are obtained by combining different MT metrics. Ji &amp; Eisenstein (2013) attains state-of-the-art results based on latent semantic analysis and a new term-weighting metric, TF-KLD.1 A variety of classifiers has been employed for the purpose of identifying paraphrases. Kozarova &amp; Montoyo (2006) measures lexical and semantic similarity with the combination of different classifiers: k-Nearest Neighbours, Support Vector Machines, and Maximum Entropy. The SVM Classifiers remains the most applicable in recent research whether applied solely (Finch et al., 2005; Wan et al., 2006) or part of combined classifiers (Kozoreva &amp; Montotyo, 2006; Lintean &amp; Rus, 2011; Madnani et al, 2012). 1 State-of-the-art results are shown in Section 5. 3 The Task The Semeval-2015 task “Paraphrase and Semantic Similarity in Twitter” involves predicting whether two tweets have the same meaning. Training and test data are provided in the form of a Twitter Paraphrase Corpus (TPC) (Xu, 2014). The TPC is constructed semi-randomly and annotated via Amazon Mechanical Turk by 5 annotators. It consists of around 35% paraphrases and 65% nonparaphrases. Training and development data consists of 18K tw</context>
<context position="13730" citStr="Wan et al. (2006)" startWordPosition="2196" endWordPosition="2199">72.4 SVC (linear kernel) 86.5 68.0 66.9 67.4 SVC (rbf kernel) 85.7 64.9 68.6 66.7 Baseline -- 67.9 52.0 58.9 Table 2: TPC Results For comparison, Table 3 shows state-of-theart results for the PI task on the MSRPC, together with our classifiers trained using of same set of features as for the TPC. Our method performs well above baseline, but with relatively lower precision than other systems. In contrast to Table 2, our highest result is obtained using the RBF kernel. Model Acc. Pre. Rec. F-sc. Ji&amp;Eisenstein(2013) 80.4 85.96 Madnani et al (2012) 77.4 - - 84.1 Socher et al. (2011) 76.8 - - 83.6 Wan et al. (2006) 75.6 77.0 90.0 83.0 SVC(rbf kernel) 74.4 74.8 92.9 82.8 Das &amp; Smith (2009) 76.1 79.6 86.1 82.7 Finch et al. (2005) 75.0 76.6 89.8 82.7 Fernando&amp;Stevenson (2008) 74.1 75.2 91.3 82.4 SVC (linear kernel) 73.7 75.0 90.1 82.1 Qiu et al. (2006) 72.0 72.5 93.4 81.6 Zhang and Patrick (2005) 71.9 74.3 88.2 80.7 BASELINE 65.4 71.6 79.5 75.3 Table 3: Paraphrase Identification State-of-the-art Results on MSRPC We note that the features that we adopt as informative for the Twitter PI task outperform some recent approaches to PI on the MSRPC. This is encouraging and indicates applicability of knowledge-lea</context>
</contexts>
<marker>Wan, Dras, Dale, 2006</marker>
<rawString>Wan, S., Dras, M., &amp; Dale, R. (2006). Using Dependency-Based Features to Take the “ Para-farce ” out of Paraphrase. In proceedings of the Australasian Language Technology Workshop (pp. 131-138). Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xu</author>
</authors>
<title>Data-Drive Aprroches for Paraphrasing Across Language Variations.</title>
<date>2014</date>
<tech>PhD Thesis.</tech>
<institution>Department of Computer Science, New York University.</institution>
<contexts>
<context position="5926" citStr="Xu, 2014" startWordPosition="892" endWordPosition="893">ferent classifiers: k-Nearest Neighbours, Support Vector Machines, and Maximum Entropy. The SVM Classifiers remains the most applicable in recent research whether applied solely (Finch et al., 2005; Wan et al., 2006) or part of combined classifiers (Kozoreva &amp; Montotyo, 2006; Lintean &amp; Rus, 2011; Madnani et al, 2012). 1 State-of-the-art results are shown in Section 5. 3 The Task The Semeval-2015 task “Paraphrase and Semantic Similarity in Twitter” involves predicting whether two tweets have the same meaning. Training and test data are provided in the form of a Twitter Paraphrase Corpus (TPC) (Xu, 2014). The TPC is constructed semi-randomly and annotated via Amazon Mechanical Turk by 5 annotators. It consists of around 35% paraphrases and 65% nonparaphrases. Training and development data consists of 18K tweet pairs and 1K test data. Test data is drawn from a different time period and annotated by an expert. 4 Approach 4.1 Text Preprocessing Text preprocessing is essential to many NLP applications. It may involve tokenizing, removal of punctuation, PoS-tagging, and so on. For identifying paraphrases, this may not always be appropriate. Removing punctuation and stop words, as commonly done for</context>
</contexts>
<marker>Xu, 2014</marker>
<rawString>Xu, W. (2014). Data-Drive Aprroches for Paraphrasing Across Language Variations. PhD Thesis. Department of Computer Science, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xu</author>
<author>A Ritter</author>
<author>C Callison-Burch</author>
<author>W Dolan</author>
<author>Y Ji</author>
</authors>
<title>Extracting Lexically Divergent Paraphrases from Twitter.</title>
<date>2014</date>
<journal>Transactions Of The Association For Computational Linguistics,</journal>
<volume>2</volume>
<pages>435--448</pages>
<contexts>
<context position="12644" citStr="Xu et al (2014)" startWordPosition="2009" endWordPosition="2012">ection of word unigrams. This had no impact on performance. 66 experimented with linear and Radial Basis Function (RBF) kernels. Linear kernels are known to work well with large datasets and RBF kernels are the first choice if small number of features are applied (Hsu et al., 2003), which both cases to apply our datasets. Classifiers are used with their default parameters and trained on the data provided. 5 Results Table 2 shows that SVC with a linear kernel achieved an F-score of 67.4. This represent the highest score amongst those systems participating in Task 1, though still some way below Xu et al (2014) and the human upper-bound. Xu et al. (2014)’s approach constructs a joint word-sentence paraphrase model (MULTIP) and utilizes topic information, which outperforms other features individually. Table 2 also shows the result for the RBF kernel, which was not submitted for the task. For this task the non-linear kernel does not provide any performance gain over the linear SVM. Model Acc. Pre. Rec. F-sc. Human Upperbound -- 75.2 90.8 82.3 Xu et al. (2014) -- 72.2 72.6 72.4 SVC (linear kernel) 86.5 68.0 66.9 67.4 SVC (rbf kernel) 85.7 64.9 68.6 66.7 Baseline -- 67.9 52.0 58.9 Table 2: TPC Results F</context>
</contexts>
<marker>Xu, Ritter, Callison-Burch, Dolan, Ji, 2014</marker>
<rawString>Xu, W., Ritter, A., Callison-Burch, C., Dolan, W., &amp; Ji, Y. (2014). Extracting Lexically Divergent Paraphrases from Twitter. Transactions Of The Association For Computational Linguistics, 2, 435-448. p/tacl/article/view/498</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Patrick</author>
</authors>
<title>Paraphrase Identification by Text Canonicalization.</title>
<date>2005</date>
<booktitle>In proceedings of the Australasian Language Technology Workshop</booktitle>
<pages>160--166</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="14014" citStr="Zhang and Patrick (2005)" startWordPosition="2247" endWordPosition="2250">ures as for the TPC. Our method performs well above baseline, but with relatively lower precision than other systems. In contrast to Table 2, our highest result is obtained using the RBF kernel. Model Acc. Pre. Rec. F-sc. Ji&amp;Eisenstein(2013) 80.4 85.96 Madnani et al (2012) 77.4 - - 84.1 Socher et al. (2011) 76.8 - - 83.6 Wan et al. (2006) 75.6 77.0 90.0 83.0 SVC(rbf kernel) 74.4 74.8 92.9 82.8 Das &amp; Smith (2009) 76.1 79.6 86.1 82.7 Finch et al. (2005) 75.0 76.6 89.8 82.7 Fernando&amp;Stevenson (2008) 74.1 75.2 91.3 82.4 SVC (linear kernel) 73.7 75.0 90.1 82.1 Qiu et al. (2006) 72.0 72.5 93.4 81.6 Zhang and Patrick (2005) 71.9 74.3 88.2 80.7 BASELINE 65.4 71.6 79.5 75.3 Table 3: Paraphrase Identification State-of-the-art Results on MSRPC We note that the features that we adopt as informative for the Twitter PI task outperform some recent approaches to PI on the MSRPC. This is encouraging and indicates applicability of knowledge-lean approaches to other data sets. 6 Conclusions Our results demonstrate that knowledge-lean methods based on character and word level overlap features in combination can give good results in terms of the identification of Twitter paraphrases. SVM classifiers were successfully used to </context>
</contexts>
<marker>Zhang, Patrick, 2005</marker>
<rawString>Zhang, Y., &amp; Patrick, J. (2005). Paraphrase Identification by Text Canonicalization. In proceedings of the Australasian Language Technology Workshop (pp. 160-166). Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>