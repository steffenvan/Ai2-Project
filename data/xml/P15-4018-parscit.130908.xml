<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.031518">
<title confidence="0.996726">
JOBIMVIZ: A Web-based Visualization
for Graph-based Distributional Semantic Models
</title>
<author confidence="0.989917">
Eugen Ruppert and Manuel Kaufmann and Martin Riedl and Chris Biemann
</author>
<affiliation confidence="0.98196">
FG Language Technology
Computer Science Department, Technische Universit¨at Darmstadt,
</affiliation>
<address confidence="0.922925">
Hochschulsstrasse 10, D-62489 Darmstadt, Germany
</address>
<email confidence="0.997649">
{eugen.ruppert,riedl,biem}@cs.tu-darmstadt.de, mtk@kisad.de
</email>
<sectionHeader confidence="0.993845" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921363636364">
This paper introduces a web-based visual-
ization framework for graph-based distri-
butional semantic models. The visualiza-
tion supports a wide range of data struc-
tures, including term similarities, sim-
ilarities of contexts, support of multi-
word expressions, sense clusters for terms
and sense labels. In contrast to other
browsers of semantic resources, our visu-
alization accepts input sentences, which
are subsequently processed with language-
independent or language-dependent ways
to compute term-context representations.
Our web demonstrator currently contains
models for multiple languages, based on
different preprocessing such as depen-
dency parsing and n-gram context repre-
sentations. These models can be accessed
from a database, the web interface and via
a RESTful API. The latter facilitates the
quick integration of such models in re-
search prototypes.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996798090909091">
Statistical semantics has emerged as a field of
computational linguistics, aiming at automatically
computing semantic representations for words,
sentences and phrases from (large) corpora. While
early approaches to distributional semantics were
split into symbolic, graph-based approaches (Lin,
1998) and vector-based approaches (Sch¨utze,
1993), recent trends have mainly concentrated on
optimizing the representation of word meanings in
vector spaces and how these account for composi-
tionality (cf. Baroni and Lenci (2010); Turney and
Pantel (2010)).
While dense vector representations, obtained by
singular value decomposition (cf. Rapp (2002))
or neural embeddings (Mikolov et al., 2010), have
gained popularity due to successes in modelling
semantic and relational similarity, we propose to
revisit graph-based approaches to distributional
semantics in the tradition of Lin (1998), Curran
(2002) and Biemann and Riedl (2013) – at least as
an additional alternative – for the following rea-
sons:
</bodyText>
<listItem confidence="0.951533555555555">
• (dense) vector representations are not inter-
pretable, thus it cannot be traced why two
terms are similar
• vectors do not make word senses explicit, but
represent ambiguous words as a mix of their
senses
• graph-based models can be straightforwardly
structured and extended, e.g. to represent tax-
onomic or other relationships
</listItem>
<bodyText confidence="0.999935647058824">
In this demonstration paper, we describe JO-
BIMVIZ, a visualization and interactive demon-
strator for graph-based models of distributional
semantics. Our models comprise similarities be-
tween terms (a.k.a. distributional thesaurus) and
multiword units, similarities between context fea-
tures, clustered word senses and their labeling
with taxonomic is-a relations. The demonstra-
tor transforms input sentences into a term-context
representation and allows to browse parts of the
underlying model relevant to the sentence at hand.
Requests are handled through a RESTful API,
which allows to use all available information
in custom prototypes via HTTP requests. The
demonstrator, as well as the computation of the
models, is fully available open source under a per-
missive license.
</bodyText>
<page confidence="0.992223">
103
</page>
<note confidence="0.7853745">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 103–108,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<sectionHeader confidence="0.998887" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997748022727273">
Aside from providing a convenient lookup for hu-
man users, the visualization of semantic models
provides an important tool for debugging models
visually. Additionally, prototyping of semantic ap-
plications based on these models is substantially
accelerated.
VISUALSYNONYMS1 and THESAURUS.COM2
offer lookup possibilities to retrieve various in-
formation for input words. Users can view in-
formation, like WordNet or Wikipedia definitions
and related words (synonyms, hypernyms, etc.).
BABELNET3 (Navigli and Ponzetto, 2012) uses
such information to compile multilingual defini-
tions and translations for input words. Here, the
words are differentiated by senses, with taxonomi-
cal labels. BABELNET offers a SPARQL endpoint
and APIs for web access.
SERELEX4 (Panchenko et al., 2013) is a graph-
ical thesaurus viewer. Users can enter a term
for different languages and retrieve related words.
The similarity graph displays the similarity links
between similar items (‘secondary links’). The
items can be expanded for a denser graph. The
input terms map to nominal phrases, allowing the
interface to display short definitions and disam-
biguations from Wikipedia. An API with JSON
output for similar words is provided.
SKETCH ENGINE5 (Kilgarriff et al., 2004) of-
fers access to pre-processed corpora. For each
corpus, the user can view concordances and sim-
ilar terms (thesaurus) for a given term. SKETCH
ENGINE also features statistical information, like
word frequency and co-occurrence counts. Fur-
thermore it shows meta information for a corpus.
One drawback of the SKETCH ENGINE is that the
tools and the models are not freely available.
NETSPEAK6 (Stein et al., 2010) is a search en-
gine for words in context. Access is possible via a
graphical UI, a RESTful interface and a Java API.
Users can enter wildcards and other meta sym-
bols into the input phrases and thus retrieve all the
words and phrases that occur in a given context.
The information is displayed with corpus statis-
tics.
</bodyText>
<footnote confidence="0.999965833333333">
1http://www.visualsynonyms.com
2http://www.thesaurus.com
3http://babelnet.org/
4http://serelex.org/
5http://www.sketchengine.co.uk/
6http://www.netspeak.org/
</footnote>
<bodyText confidence="0.999973714285714">
A novel aspect of JOBIMVIZ is that it in-
corporates several different aspects of symbolic
methods (distributional thesaurus, context feature
scores, sense clusters), and all of these methods
are derived from the input corpus alone, without
relying on external resources, which are not avail-
able for every language/domain. Furthermore, we
provide domain-based sense clusterings with is-a
labels (cf. Figure 4), which is not performed by
the other discussed tools.
Our interface features a generic interactive vi-
sualization that is adaptable to different kinds of
parses and also handles multiword units in the vi-
sualization. All of this information is made freely
available by the API, enabling rapid prototyp-
ing techniques. To our knowledge, the presented
demonstrator is the only online tool that combines
technical accessibility (open source project, open
API) with rich, flexible preprocessing options (cf.
Section 3) and graph-based, structured semantic
models that contain context similarities.
</bodyText>
<sectionHeader confidence="0.841714" genericHeader="method">
3 Computation of distributional models
</sectionHeader>
<bodyText confidence="0.999979230769231">
The visualization is based on distributional mod-
els computed with the JoBimText framework (Bie-
mann and Riedl, 2013)7; however it can also be
used for other semantic models of similar struc-
ture. One of the major components of the frame-
work is a method for the computation of distribu-
tional thesauri. This method consists of two steps:
a holing operation and a similarity computation.
The holing operation processes text and yields
a representation consisting of jos and bims. Jos
and bims are normally instantiated by a term (jo)
and its context features (bims), but the definition
extends to arbitrary splits of the input perception
that mutually characterize each other distribution-
ally. The holing operation executes a preprocess-
ing pipeline that can be as simple as text segmen-
tation or complex like POS tagging and depen-
dency parsing, depending on the complexity of the
context features. As the preprocessing is defined
in UIMA (Ferrucci and Lally, 2004) pipeline de-
scriptors, it is straightforward to exchange com-
ponents or define new preprocessing operations.
Using this processed and annotated text, the hol-
ing annotator generates the term–feature represen-
tation of the input text, e.g. by using the neighbor-
ing words (‘trigram holing’) or dependency paths
</bodyText>
<footnote confidence="0.9974805">
7The framework is available under the permissive ASL
2.0 license at http://sf.net/p/jobimtext.
</footnote>
<page confidence="0.996254">
104
</page>
<figureCaption confidence="0.9473845">
Figure 1: Architecture of JOBIMVIZ, with the
three components GUI, Web Server and Workers.
</figureCaption>
<bodyText confidence="0.99807">
between terms (‘dependency holing’). A graphi-
cal example is given in Figure 5, where the holing
operation yields four context features for the term
example#NN. Different term representations are
possible, like surface text, lemma, lemma+POS
and also multiword expressions. This flexibility
allows, on the one hand, domain- and language-
independent similarity computations using general
holing operations, while on the other hand allow-
ing complex, language-specific processing in the
holing operations for higher quality models (e.g.
using parsing and lemmatization).
The second part consists of the similarity com-
putation, which relies on MapReduce (Dean and
Ghemawat, 2004) for scalability and employs ef-
ficient pruning strategies to keep runtime feasible
even for very large corpora. After the computa-
tion of similarities, sense clusters for each term
are computed using Chinese Whispers graph clus-
tering (Biemann, 2006), as described in Biemann
et al. (2013). Furthermore, the sense clusters are
also labeled with hypernyms (is-a relations) de-
rived from Hearst patterns (Hearst, 1992), imple-
mented using the UIMA Ruta (Kluegl et al., 2014)
pattern matching engine8.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="method">
4 Web-based Demonstrator
</sectionHeader>
<subsectionHeader confidence="0.981875">
4.1 Architecture and Technology
</subsectionHeader>
<bodyText confidence="0.999910777777778">
The architecture of JOBIMVIZ consists of three
main components (see Figure 1). The central el-
ement is a Java EE based web server, which pro-
vides a RESTful interface (Fielding, 2000) for ac-
cessing API resources, such as sentence holing op-
erations and the distributional models.
To handle many parallel requests for long run-
ning holing operations like dependency parsing,
we use an Apache ActiveMQ9 based request/reply
</bodyText>
<footnote confidence="0.999743">
8https://uima.apache.org/ruta.html
9http://activemq.apache.org/
</footnote>
<bodyText confidence="0.99966692">
queue. All requests to the web server are stored
in the queue and processed by one of the worker
processes, which can be distributed on differ-
ent machines and handle the requests in parallel.
These workers form the second component of our
system. The holing workers execute the UIMA
pipelines that define the holing operations. Us-
age of UIMA descriptors provides great flexibil-
ity, since one type of workers can run every holing
operation. Every model defines a custom UIMA
pipeline to ensure the same holing operation for
the input and the model. To speed up the hol-
ing operation we cache frequently queried holing
outputs into the in-memory database Redis10. Re-
quests to the API are processed by another type of
workers, which retrieve the relevant data from the
models database.
The third component of the software is a
HTML5-based GUI with an overall layout based
on the Bootstrap11 framework. The front-end uses
Ajax requests to connect to the RESTful interface
of the web server and retrieves responses in the
JSON format. The received data is processed by a
Javascript application, which uses D3.js (Bostock
et al., 2011) to visualize the graphs.
</bodyText>
<subsectionHeader confidence="0.958964">
4.2 Visualization
</subsectionHeader>
<bodyText confidence="0.999791190476191">
In the demonstrator, users can enter sentences or
phrases, run different holing operations on the in-
put and browse distributional models. The demon-
strator can be accessed online12.
Figure 2 shows the application layout. First,
the user can decide on the model, which consists
of a corpus and a holing operation, and select it
via the dropdown holing operation selector (3).
Then, the user enters a sentence in the text field
(1) and activates the processing by clicking the
Parse button (2). The holing output is presented
as a graph (4) with marked terms and context fea-
tures. Other views are available in tab bar at the
top (5). To retrieve term similarities, a term in
the displayed sentence can be selected (4a), acti-
vating the information boxes (6, 7, 8, 9). Context
similarities can be viewed by selecting the corre-
sponding feature arc (4b). The frequency of the
selected term/feature is presented on the top right
(6). Similar items are displayed in the first box
in the lower pane (7). Similarity scores between
</bodyText>
<footnote confidence="0.992492">
10http://redis.io
11http://www.getbootstrap.com/
12http://goo.gl/V2ZEly
</footnote>
<figure confidence="0.997699666666667">
User
Java EE Web-
RESTful
API
GUI
Request-/
Reply
Queue
API-
Workere
Holing
_rkere
Database
JIMA-
Pipeline
</figure>
<page confidence="0.895452">
105
</page>
<figureCaption confidence="0.918907">
Figure 2: Overview of the visualization with a collapsed dependency parse of the input sentence This is
an example for the paper; the selected term is paper#NN.
Figure 3: Similar bims (left) and most signifi-
cant jos (right) for the dependency parse context
give#VB-dobj (direct object of give).
</figureCaption>
<bodyText confidence="0.997557842105263">
the selected and similar terms are shown, includ-
ing self-similarities as an upper limit for similarity
of the selected item.
The most relevant context features for the term
are displayed with the significance score and their
term-feature count in the corpus (8). When select-
ing a context feature, the most relevant terms for
a context feature are shown (cf. Figure 3). For
terms, there is a box displaying sense clusters (9).
These are often available in different granularities
to match the application requirements (e.g. ‘CW’
or ‘CW-finer’)13. When a user selects a sense clus-
ter, a list of related terms for the selected cluster
and a list of hypernyms (is-a relations) with fre-
quency scores are displayed (cf. Figure 4). But-
tons for API calls are displayed for all data display
in the GUI (10). This enables users to get comfort-
able with the models and the API before deploying
it in an application. The buttons feature selectors
</bodyText>
<footnote confidence="0.983483">
13Here, ‘CW’ is referring to sense clusters computed with
Chinese Whispers (Biemann, 2006).
</footnote>
<figureCaption confidence="0.887234666666667">
Figure 4: Different word senses for paper, with
hypernym terms (sense 1 paper:publication, sense
2 paper:material), as accessed from field (9) in
Figure 2. The tabs “CW/CW-finer” provide access
to different sense clustering inventories, e.g. with
a different granularity.
</figureCaption>
<bodyText confidence="0.999939">
for different output data format options, i.e. TSV,
XML, JSON and RDF. For the boxes with list con-
tent, there is a ‘maximize’ button next to the API
button that brings up a screen-filling overlay.
</bodyText>
<subsectionHeader confidence="0.881832">
4.2.1 Sentence Holing Operations
</subsectionHeader>
<bodyText confidence="0.999942916666667">
For the graphical representation of holing opera-
tions, the web demo offers views for single terms
(Figure 5 and 6) as well as support for n-grams
(Figure 7). Figure 5 shows the tree representation
for a dependency parser using collapsed depen-
dencies. Figure 6 exemplifies a holing operation
considering the left and right neighboring words
as one context feature (‘trigram holing’). Figure 7
shows the result of the same holing operation, ap-
plied for n-grams, where several different possi-
ble left and right multiword items can be selected
as context. Here, the demonstrator identified mul-
</bodyText>
<page confidence="0.997384">
106
</page>
<figureCaption confidence="0.925604666666667">
Figure 5: Collapsed dependency (de Marneffe et
al., 2006) holing result for This is an example for
the paper; the preposition for is collapsed into the
prep for dependency.
Figure 6: Trigram holing (unigram) result for This
is an example for the paper.
</figureCaption>
<bodyText confidence="0.9990255">
tiword expressions that are present in the corre-
sponding distributional model (acute lymphoblas-
tic leukemia, lymphoblastic leukemia cells and hu-
man bones). These expressions can be selected
like single word items. Furthermore, there is a fil-
tering function for n-grams, where users can refine
the display of n-grams by selecting the desired n-
gram lengths.
</bodyText>
<subsectionHeader confidence="0.91807">
4.2.2 Model Access
</subsectionHeader>
<bodyText confidence="0.9990813">
The demonstrator features a selection of mod-
els for different languages (currently: German,
English, Hindi, Bengali) and different domains,
like news, encyclopedia or medical domain. Be-
sides term similarities, typical context features and
sense clusters are also part of these models. Dis-
tributional similarities for context features can be
viewed as well. By selecting an arc that represents
the feature relation, the user can view similar fea-
tures in the GUI. In Figure 3, the context features
</bodyText>
<figureCaption confidence="0.747459">
Figure 7: Trigram holing (n-gram) result for mi-
</figureCaption>
<bodyText confidence="0.980238823529412">
gration of acute lymphoblastic leukemia cells into
human bones with display of multiwords that are
part of the model.
that are most similar to give#VB#-dobj (direct
object of verb give) are displayed. Here, the most
significant words for a feature are shown. To our
knowledge, we are the first ones to explicitly pro-
vide similarities of contexts in distributional se-
mantic models.
Holing operations and models can be accessed
via an open RESTful API. The access URIs con-
tain the model identifier (consisting of dataset and
holing operation), the desired method, like sen-
tence holing or similar terms, and the input sen-
tence, term or context feature. The distributional
project also features a Java API to access models
via the web-based API14.
</bodyText>
<sectionHeader confidence="0.990617" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999015214285714">
In this paper we have introduced a new web-based
tool that can be used to browse and to access
graph-based semantic models based on distribu-
tional semantics. In its current form, it can dis-
play data from a distributional thesaurus, simi-
larities of context features, sense clusters labeled
with taxonomic relations, and provides the dis-
play of multiword expressions. Additionally, it
provides the functionality to transform sentences
into term–context representations. The web demo
can give a first impression for people who are
interested in the JoBimText framework for dis-
tributional semantics. Providing a RESTful in-
terface for accessing all information with state-
</bodyText>
<footnote confidence="0.984176">
14For an overview of available API methods, see http:
//goo.gl/l6K6Gu.
</footnote>
<page confidence="0.99769">
107
</page>
<bodyText confidence="0.999956611111111">
less requests allows for an easy integration into
prototypes. The RESTful API can also be ac-
cessed using our Java API, which also can access
other back-ends such as on-disk and in-memory
databases. The complete project is available under
the permissive Apache ASL 2.0 license and mod-
els for several languages are available for down-
load15.
Whereas at the moment similar terms are glob-
ally ranked, we will add visualization support for
a contextualization method, in order to rank sim-
ilar terms regarding their context within the sen-
tence. Furthermore, we are working on incorpo-
rating more complex pre-processing for the hol-
ing operation in the visualization, e.g. aggregating
context features over co-reference chains, as well
as relation extraction and frame-semantic parsing
for term–context representations.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9935982">
This work has been supported by the German Fed-
eral Ministry of Education and Research (BMBF)
within the context of the Software Campus project
LiCoRes under grant No. 01IS12054 and by an
IBM Shared University Research award.
</bodyText>
<sectionHeader confidence="0.998084" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999811871794871">
Marco Baroni and Alessandro Lenci. 2010. Distributional
memory: A general framework for corpus-based seman-
tics. Computational Linguistics, 36(4):673–721.
Chris Biemann and Martin Riedl. 2013. Text: Now in 2D! a
framework for lexical expansion with contextual similar-
ity. Journal of Language Modelling, 1(1):55–95.
Chris Biemann, Bonaventura Coppola, Michael R. Glass, Al-
fio Gliozzo, Matthew Hatem, and Martin Riedl. 2013.
JoBimText Visualizer: A graph-based approach to con-
textualizing distributional similarity. In Proc. TextGraphs
2013, pages 6–10, Seattle, Washington, USA.
Chris Biemann. 2006. Chinese Whispers – an efficient
graph clustering algorithm and its application to natural
language processing problems. In Proc. TextGraphs 2006,
pages 73–80, New York City, NY, USA.
Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer. 2011.
D3: Data-driven documents. IEEE Transactions on Visu-
alization &amp; Computer Graphics, 17(12):2301–2309.
James R. Curran. 2002. Ensemble methods for automatic
thesaurus extraction. In Proc. EMNLP’02/ACL-2002,
pages 222–229, Philadelphia, PA, USA.
15Selection of models available under http:
//sf.net/projects/jobimtext/files/data/
models/.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proc. LREC-2006,
pages 449–454, Genova, Italy.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified Data Processing on Large Clusters. In Proc.
OSDI ’04, pages 137–150, San Francisco, CA, USA.
David Ferrucci and Adam Lally. 2004. UIMA: An Architec-
tural Approach to Unstructured Information Processing in
the Corporate Research Environment. Natural Language
Engineering, 10(3-4):327–348.
Roy Thomas Fielding. 2000. Architectural Styles and the
Design of Network-based Software Architectures. Doc-
toral dissertation, University of California, Irivne.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. COLING-1992, pages
539–545, Nantes, France.
Adam Kilgarriff, Pavel Rychl´y, Pavel Smrˇz, and David Tug-
well. 2004. The Sketch Engine. In Proc. EURALEX
2004, pages 105–116, Lorient, France.
Peter Kluegl, Martin Toepfer, Philip-Daniel Beck, Georg
Fette, and Frank Puppe. 2014. UIMA Ruta: Rapid devel-
opment of rule-based information extraction applications.
Natural Language Engineering.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. COLING-98, pages 768–774,
Montr´eal, Quebec, Canada.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cer-
nock`y, and Sanjeev Khudanpur. 2010. Recurrent neural
network based language model. In Proc. INTERSPEECH
2010, pages 1045–1048, Makuhari, Chiba, Japan.
Roberto Navigli and Simone P. Ponzetto. 2012. BabelNet:
The automatic construction, evaluation and application of
a wide-coverage multilingual semantic network. Artificial
Intelligence, 193:217–250.
Alexander Panchenko, Pavel Romanov, Olga Morozova, Hu-
bert Naets, Andrey Philippovich, Alexey Romanov, and
Fairon C´edrick. 2013. Serelex: Search and visualization
of semantically related words. In Proc. ECIR 2013, pages
837–840, Moscow, Russia.
Reinhard Rapp. 2002. The computation of word asso-
ciations: Comparing syntagmatic and paradigmatic ap-
proaches. In Proc. COLING ’02, pages 1–7, Taipei, Tai-
wan.
Hinrich Sch¨utze. 1993. Word space. In Advances in Neural
Information Processing Systems 5, pages 895–902, Den-
ver, Colorado, USA.
Benno Stein, Martin Potthast, and Martin Trenkmann. 2010.
Retrieving Customary Web Language to Assist Writers.
In Advances in Information Retrieval, ECIR 10, pages
631–635, Milton Keynes, UK.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Journal of
artificial intelligence research, 37(1):141–188.
</reference>
<page confidence="0.998365">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493625">
<title confidence="0.9990565">A Web-based for Graph-based Distributional Semantic Models</title>
<author confidence="0.997123">Ruppert Kaufmann Riedl</author>
<affiliation confidence="0.991165">FG Language Computer Science Department, Technische Universit¨at</affiliation>
<address confidence="0.510531">Hochschulsstrasse 10, D-62489 Darmstadt,</address>
<email confidence="0.997909">mtk@kisad.de</email>
<abstract confidence="0.999355913043478">This paper introduces a web-based visualization framework for graph-based distributional semantic models. The visualization supports a wide range of data structures, including term similarities, similarities of contexts, support of multiword expressions, sense clusters for terms and sense labels. In contrast to other browsers of semantic resources, our visualization accepts input sentences, which are subsequently processed with languageindependent or language-dependent ways to compute term-context representations. Our web demonstrator currently contains models for multiple languages, based on different preprocessing such as depenparsing and context representations. These models can be accessed from a database, the web interface and via a RESTful API. The latter facilitates the quick integration of such models in research prototypes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="1754" citStr="Baroni and Lenci (2010)" startWordPosition="229" endWordPosition="232">The latter facilitates the quick integration of such models in research prototypes. 1 Introduction Statistical semantics has emerged as a field of computational linguistics, aiming at automatically computing semantic representations for words, sentences and phrases from (large) corpora. While early approaches to distributional semantics were split into symbolic, graph-based approaches (Lin, 1998) and vector-based approaches (Sch¨utze, 1993), recent trends have mainly concentrated on optimizing the representation of word meanings in vector spaces and how these account for compositionality (cf. Baroni and Lenci (2010); Turney and Pantel (2010)). While dense vector representations, obtained by singular value decomposition (cf. Rapp (2002)) or neural embeddings (Mikolov et al., 2010), have gained popularity due to successes in modelling semantic and relational similarity, we propose to revisit graph-based approaches to distributional semantics in the tradition of Lin (1998), Curran (2002) and Biemann and Riedl (2013) – at least as an additional alternative – for the following reasons: • (dense) vector representations are not interpretable, thus it cannot be traced why two terms are similar • vectors do not m</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Martin Riedl</author>
</authors>
<title>Text: Now in 2D! a framework for lexical expansion with contextual similarity.</title>
<date>2013</date>
<journal>Journal of Language Modelling,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2159" citStr="Biemann and Riedl (2013)" startWordPosition="286" endWordPosition="289">vector-based approaches (Sch¨utze, 1993), recent trends have mainly concentrated on optimizing the representation of word meanings in vector spaces and how these account for compositionality (cf. Baroni and Lenci (2010); Turney and Pantel (2010)). While dense vector representations, obtained by singular value decomposition (cf. Rapp (2002)) or neural embeddings (Mikolov et al., 2010), have gained popularity due to successes in modelling semantic and relational similarity, we propose to revisit graph-based approaches to distributional semantics in the tradition of Lin (1998), Curran (2002) and Biemann and Riedl (2013) – at least as an additional alternative – for the following reasons: • (dense) vector representations are not interpretable, thus it cannot be traced why two terms are similar • vectors do not make word senses explicit, but represent ambiguous words as a mix of their senses • graph-based models can be straightforwardly structured and extended, e.g. to represent taxonomic or other relationships In this demonstration paper, we describe JOBIMVIZ, a visualization and interactive demonstrator for graph-based models of distributional semantics. Our models comprise similarities between terms (a.k.a.</context>
<context position="6759" citStr="Biemann and Riedl, 2013" startWordPosition="969" endWordPosition="973">adaptable to different kinds of parses and also handles multiword units in the visualization. All of this information is made freely available by the API, enabling rapid prototyping techniques. To our knowledge, the presented demonstrator is the only online tool that combines technical accessibility (open source project, open API) with rich, flexible preprocessing options (cf. Section 3) and graph-based, structured semantic models that contain context similarities. 3 Computation of distributional models The visualization is based on distributional models computed with the JoBimText framework (Biemann and Riedl, 2013)7; however it can also be used for other semantic models of similar structure. One of the major components of the framework is a method for the computation of distributional thesauri. This method consists of two steps: a holing operation and a similarity computation. The holing operation processes text and yields a representation consisting of jos and bims. Jos and bims are normally instantiated by a term (jo) and its context features (bims), but the definition extends to arbitrary splits of the input perception that mutually characterize each other distributionally. The holing operation execu</context>
</contexts>
<marker>Biemann, Riedl, 2013</marker>
<rawString>Chris Biemann and Martin Riedl. 2013. Text: Now in 2D! a framework for lexical expansion with contextual similarity. Journal of Language Modelling, 1(1):55–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Bonaventura Coppola</author>
<author>Michael R Glass</author>
<author>Alfio Gliozzo</author>
<author>Matthew Hatem</author>
<author>Martin Riedl</author>
</authors>
<title>JoBimText Visualizer: A graph-based approach to contextualizing distributional similarity.</title>
<date>2013</date>
<booktitle>In Proc. TextGraphs</booktitle>
<pages>6--10</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="9058" citStr="Biemann et al. (2013)" startWordPosition="1322" endWordPosition="1325">pendent similarity computations using general holing operations, while on the other hand allowing complex, language-specific processing in the holing operations for higher quality models (e.g. using parsing and lemmatization). The second part consists of the similarity computation, which relies on MapReduce (Dean and Ghemawat, 2004) for scalability and employs efficient pruning strategies to keep runtime feasible even for very large corpora. After the computation of similarities, sense clusters for each term are computed using Chinese Whispers graph clustering (Biemann, 2006), as described in Biemann et al. (2013). Furthermore, the sense clusters are also labeled with hypernyms (is-a relations) derived from Hearst patterns (Hearst, 1992), implemented using the UIMA Ruta (Kluegl et al., 2014) pattern matching engine8. 4 Web-based Demonstrator 4.1 Architecture and Technology The architecture of JOBIMVIZ consists of three main components (see Figure 1). The central element is a Java EE based web server, which provides a RESTful interface (Fielding, 2000) for accessing API resources, such as sentence holing operations and the distributional models. To handle many parallel requests for long running holing o</context>
</contexts>
<marker>Biemann, Coppola, Glass, Gliozzo, Hatem, Riedl, 2013</marker>
<rawString>Chris Biemann, Bonaventura Coppola, Michael R. Glass, Alfio Gliozzo, Matthew Hatem, and Martin Riedl. 2013. JoBimText Visualizer: A graph-based approach to contextualizing distributional similarity. In Proc. TextGraphs 2013, pages 6–10, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese Whispers – an efficient graph clustering algorithm and its application to natural language processing problems.</title>
<date>2006</date>
<booktitle>In Proc. TextGraphs</booktitle>
<pages>73--80</pages>
<location>New York City, NY, USA.</location>
<contexts>
<context position="9019" citStr="Biemann, 2006" startWordPosition="1317" endWordPosition="1318">e hand, domain- and languageindependent similarity computations using general holing operations, while on the other hand allowing complex, language-specific processing in the holing operations for higher quality models (e.g. using parsing and lemmatization). The second part consists of the similarity computation, which relies on MapReduce (Dean and Ghemawat, 2004) for scalability and employs efficient pruning strategies to keep runtime feasible even for very large corpora. After the computation of similarities, sense clusters for each term are computed using Chinese Whispers graph clustering (Biemann, 2006), as described in Biemann et al. (2013). Furthermore, the sense clusters are also labeled with hypernyms (is-a relations) derived from Hearst patterns (Hearst, 1992), implemented using the UIMA Ruta (Kluegl et al., 2014) pattern matching engine8. 4 Web-based Demonstrator 4.1 Architecture and Technology The architecture of JOBIMVIZ consists of three main components (see Figure 1). The central element is a Java EE based web server, which provides a RESTful interface (Fielding, 2000) for accessing API resources, such as sentence holing operations and the distributional models. To handle many para</context>
<context position="13492" citStr="Biemann, 2006" startWordPosition="2040" endWordPosition="2041">e clusters (9). These are often available in different granularities to match the application requirements (e.g. ‘CW’ or ‘CW-finer’)13. When a user selects a sense cluster, a list of related terms for the selected cluster and a list of hypernyms (is-a relations) with frequency scores are displayed (cf. Figure 4). Buttons for API calls are displayed for all data display in the GUI (10). This enables users to get comfortable with the models and the API before deploying it in an application. The buttons feature selectors 13Here, ‘CW’ is referring to sense clusters computed with Chinese Whispers (Biemann, 2006). Figure 4: Different word senses for paper, with hypernym terms (sense 1 paper:publication, sense 2 paper:material), as accessed from field (9) in Figure 2. The tabs “CW/CW-finer” provide access to different sense clustering inventories, e.g. with a different granularity. for different output data format options, i.e. TSV, XML, JSON and RDF. For the boxes with list content, there is a ‘maximize’ button next to the API button that brings up a screen-filling overlay. 4.2.1 Sentence Holing Operations For the graphical representation of holing operations, the web demo offers views for single term</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Chinese Whispers – an efficient graph clustering algorithm and its application to natural language processing problems. In Proc. TextGraphs 2006, pages 73–80, New York City, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bostock</author>
<author>Vadim Ogievetsky</author>
<author>Jeffrey Heer</author>
</authors>
<title>D3: Data-driven documents.</title>
<date>2011</date>
<journal>IEEE Transactions on Visualization &amp; Computer Graphics,</journal>
<volume>17</volume>
<issue>12</issue>
<contexts>
<context position="10932" citStr="Bostock et al., 2011" startWordPosition="1618" endWordPosition="1621">g operation for the input and the model. To speed up the holing operation we cache frequently queried holing outputs into the in-memory database Redis10. Requests to the API are processed by another type of workers, which retrieve the relevant data from the models database. The third component of the software is a HTML5-based GUI with an overall layout based on the Bootstrap11 framework. The front-end uses Ajax requests to connect to the RESTful interface of the web server and retrieves responses in the JSON format. The received data is processed by a Javascript application, which uses D3.js (Bostock et al., 2011) to visualize the graphs. 4.2 Visualization In the demonstrator, users can enter sentences or phrases, run different holing operations on the input and browse distributional models. The demonstrator can be accessed online12. Figure 2 shows the application layout. First, the user can decide on the model, which consists of a corpus and a holing operation, and select it via the dropdown holing operation selector (3). Then, the user enters a sentence in the text field (1) and activates the processing by clicking the Parse button (2). The holing output is presented as a graph (4) with marked terms </context>
</contexts>
<marker>Bostock, Ogievetsky, Heer, 2011</marker>
<rawString>Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer. 2011. D3: Data-driven documents. IEEE Transactions on Visualization &amp; Computer Graphics, 17(12):2301–2309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>Ensemble methods for automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP’02/ACL-2002,</booktitle>
<pages>222--229</pages>
<location>Philadelphia, PA, USA.</location>
<note>15Selection of models available under http: //sf.net/projects/jobimtext/files/data/ models/.</note>
<contexts>
<context position="2130" citStr="Curran (2002)" startWordPosition="283" endWordPosition="284">s (Lin, 1998) and vector-based approaches (Sch¨utze, 1993), recent trends have mainly concentrated on optimizing the representation of word meanings in vector spaces and how these account for compositionality (cf. Baroni and Lenci (2010); Turney and Pantel (2010)). While dense vector representations, obtained by singular value decomposition (cf. Rapp (2002)) or neural embeddings (Mikolov et al., 2010), have gained popularity due to successes in modelling semantic and relational similarity, we propose to revisit graph-based approaches to distributional semantics in the tradition of Lin (1998), Curran (2002) and Biemann and Riedl (2013) – at least as an additional alternative – for the following reasons: • (dense) vector representations are not interpretable, thus it cannot be traced why two terms are similar • vectors do not make word senses explicit, but represent ambiguous words as a mix of their senses • graph-based models can be straightforwardly structured and extended, e.g. to represent taxonomic or other relationships In this demonstration paper, we describe JOBIMVIZ, a visualization and interactive demonstrator for graph-based models of distributional semantics. Our models comprise simil</context>
</contexts>
<marker>Curran, 2002</marker>
<rawString>James R. Curran. 2002. Ensemble methods for automatic thesaurus extraction. In Proc. EMNLP’02/ACL-2002, pages 222–229, Philadelphia, PA, USA. 15Selection of models available under http: //sf.net/projects/jobimtext/files/data/ models/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proc. LREC-2006,</booktitle>
<pages>449--454</pages>
<location>Genova, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proc. LREC-2006, pages 449–454, Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>MapReduce: Simplified Data Processing on Large Clusters.</title>
<date>2004</date>
<booktitle>In Proc. OSDI ’04,</booktitle>
<pages>137--150</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="8771" citStr="Dean and Ghemawat, 2004" startWordPosition="1277" endWordPosition="1280">example is given in Figure 5, where the holing operation yields four context features for the term example#NN. Different term representations are possible, like surface text, lemma, lemma+POS and also multiword expressions. This flexibility allows, on the one hand, domain- and languageindependent similarity computations using general holing operations, while on the other hand allowing complex, language-specific processing in the holing operations for higher quality models (e.g. using parsing and lemmatization). The second part consists of the similarity computation, which relies on MapReduce (Dean and Ghemawat, 2004) for scalability and employs efficient pruning strategies to keep runtime feasible even for very large corpora. After the computation of similarities, sense clusters for each term are computed using Chinese Whispers graph clustering (Biemann, 2006), as described in Biemann et al. (2013). Furthermore, the sense clusters are also labeled with hypernyms (is-a relations) derived from Hearst patterns (Hearst, 1992), implemented using the UIMA Ruta (Kluegl et al., 2014) pattern matching engine8. 4 Web-based Demonstrator 4.1 Architecture and Technology The architecture of JOBIMVIZ consists of three m</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: Simplified Data Processing on Large Clusters. In Proc. OSDI ’04, pages 137–150, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<pages>10--3</pages>
<contexts>
<context position="7602" citStr="Ferrucci and Lally, 2004" startWordPosition="1108" endWordPosition="1111"> a holing operation and a similarity computation. The holing operation processes text and yields a representation consisting of jos and bims. Jos and bims are normally instantiated by a term (jo) and its context features (bims), but the definition extends to arbitrary splits of the input perception that mutually characterize each other distributionally. The holing operation executes a preprocessing pipeline that can be as simple as text segmentation or complex like POS tagging and dependency parsing, depending on the complexity of the context features. As the preprocessing is defined in UIMA (Ferrucci and Lally, 2004) pipeline descriptors, it is straightforward to exchange components or define new preprocessing operations. Using this processed and annotated text, the holing annotator generates the term–feature representation of the input text, e.g. by using the neighboring words (‘trigram holing’) or dependency paths 7The framework is available under the permissive ASL 2.0 license at http://sf.net/p/jobimtext. 104 Figure 1: Architecture of JOBIMVIZ, with the three components GUI, Web Server and Workers. between terms (‘dependency holing’). A graphical example is given in Figure 5, where the holing operatio</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment. Natural Language Engineering, 10(3-4):327–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Thomas Fielding</author>
</authors>
<title>Architectural Styles and the Design of Network-based Software Architectures. Doctoral dissertation,</title>
<date>2000</date>
<institution>University of California, Irivne.</institution>
<contexts>
<context position="9504" citStr="Fielding, 2000" startWordPosition="1393" endWordPosition="1394">the computation of similarities, sense clusters for each term are computed using Chinese Whispers graph clustering (Biemann, 2006), as described in Biemann et al. (2013). Furthermore, the sense clusters are also labeled with hypernyms (is-a relations) derived from Hearst patterns (Hearst, 1992), implemented using the UIMA Ruta (Kluegl et al., 2014) pattern matching engine8. 4 Web-based Demonstrator 4.1 Architecture and Technology The architecture of JOBIMVIZ consists of three main components (see Figure 1). The central element is a Java EE based web server, which provides a RESTful interface (Fielding, 2000) for accessing API resources, such as sentence holing operations and the distributional models. To handle many parallel requests for long running holing operations like dependency parsing, we use an Apache ActiveMQ9 based request/reply 8https://uima.apache.org/ruta.html 9http://activemq.apache.org/ queue. All requests to the web server are stored in the queue and processed by one of the worker processes, which can be distributed on different machines and handle the requests in parallel. These workers form the second component of our system. The holing workers execute the UIMA pipelines that de</context>
</contexts>
<marker>Fielding, 2000</marker>
<rawString>Roy Thomas Fielding. 2000. Architectural Styles and the Design of Network-based Software Architectures. Doctoral dissertation, University of California, Irivne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. COLING-1992,</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="9184" citStr="Hearst, 1992" startWordPosition="1342" endWordPosition="1343">g in the holing operations for higher quality models (e.g. using parsing and lemmatization). The second part consists of the similarity computation, which relies on MapReduce (Dean and Ghemawat, 2004) for scalability and employs efficient pruning strategies to keep runtime feasible even for very large corpora. After the computation of similarities, sense clusters for each term are computed using Chinese Whispers graph clustering (Biemann, 2006), as described in Biemann et al. (2013). Furthermore, the sense clusters are also labeled with hypernyms (is-a relations) derived from Hearst patterns (Hearst, 1992), implemented using the UIMA Ruta (Kluegl et al., 2014) pattern matching engine8. 4 Web-based Demonstrator 4.1 Architecture and Technology The architecture of JOBIMVIZ consists of three main components (see Figure 1). The central element is a Java EE based web server, which provides a RESTful interface (Fielding, 2000) for accessing API resources, such as sentence holing operations and the distributional models. To handle many parallel requests for long running holing operations like dependency parsing, we use an Apache ActiveMQ9 based request/reply 8https://uima.apache.org/ruta.html 9http://a</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. COLING-1992, pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Pavel Rychl´y</author>
<author>Pavel Smrˇz</author>
<author>David Tugwell</author>
</authors>
<title>The Sketch Engine.</title>
<date>2004</date>
<booktitle>In Proc. EURALEX 2004,</booktitle>
<pages>105--116</pages>
<location>Lorient, France.</location>
<marker>Kilgarriff, Rychl´y, Smrˇz, Tugwell, 2004</marker>
<rawString>Adam Kilgarriff, Pavel Rychl´y, Pavel Smrˇz, and David Tugwell. 2004. The Sketch Engine. In Proc. EURALEX 2004, pages 105–116, Lorient, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Kluegl</author>
<author>Martin Toepfer</author>
<author>Philip-Daniel Beck</author>
<author>Georg Fette</author>
<author>Frank Puppe</author>
</authors>
<title>UIMA Ruta: Rapid development of rule-based information extraction applications. Natural Language Engineering.</title>
<date>2014</date>
<contexts>
<context position="9239" citStr="Kluegl et al., 2014" startWordPosition="1350" endWordPosition="1353">dels (e.g. using parsing and lemmatization). The second part consists of the similarity computation, which relies on MapReduce (Dean and Ghemawat, 2004) for scalability and employs efficient pruning strategies to keep runtime feasible even for very large corpora. After the computation of similarities, sense clusters for each term are computed using Chinese Whispers graph clustering (Biemann, 2006), as described in Biemann et al. (2013). Furthermore, the sense clusters are also labeled with hypernyms (is-a relations) derived from Hearst patterns (Hearst, 1992), implemented using the UIMA Ruta (Kluegl et al., 2014) pattern matching engine8. 4 Web-based Demonstrator 4.1 Architecture and Technology The architecture of JOBIMVIZ consists of three main components (see Figure 1). The central element is a Java EE based web server, which provides a RESTful interface (Fielding, 2000) for accessing API resources, such as sentence holing operations and the distributional models. To handle many parallel requests for long running holing operations like dependency parsing, we use an Apache ActiveMQ9 based request/reply 8https://uima.apache.org/ruta.html 9http://activemq.apache.org/ queue. All requests to the web serv</context>
</contexts>
<marker>Kluegl, Toepfer, Beck, Fette, Puppe, 2014</marker>
<rawString>Peter Kluegl, Martin Toepfer, Philip-Daniel Beck, Georg Fette, and Frank Puppe. 2014. UIMA Ruta: Rapid development of rule-based information extraction applications. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. COLING-98,</booktitle>
<pages>768--774</pages>
<location>Montr´eal, Quebec, Canada.</location>
<contexts>
<context position="1530" citStr="Lin, 1998" startWordPosition="199" endWordPosition="200">els for multiple languages, based on different preprocessing such as dependency parsing and n-gram context representations. These models can be accessed from a database, the web interface and via a RESTful API. The latter facilitates the quick integration of such models in research prototypes. 1 Introduction Statistical semantics has emerged as a field of computational linguistics, aiming at automatically computing semantic representations for words, sentences and phrases from (large) corpora. While early approaches to distributional semantics were split into symbolic, graph-based approaches (Lin, 1998) and vector-based approaches (Sch¨utze, 1993), recent trends have mainly concentrated on optimizing the representation of word meanings in vector spaces and how these account for compositionality (cf. Baroni and Lenci (2010); Turney and Pantel (2010)). While dense vector representations, obtained by singular value decomposition (cf. Rapp (2002)) or neural embeddings (Mikolov et al., 2010), have gained popularity due to successes in modelling semantic and relational similarity, we propose to revisit graph-based approaches to distributional semantics in the tradition of Lin (1998), Curran (2002)</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. COLING-98, pages 768–774, Montr´eal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proc. INTERSPEECH 2010,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan.</location>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proc. INTERSPEECH 2010, pages 1045–1048, Makuhari, Chiba, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone P Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<pages>193--217</pages>
<contexts>
<context position="3992" citStr="Navigli and Ponzetto, 2012" startWordPosition="557" endWordPosition="560"> System Demonstrations, pages 103–108, Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP 2 Related Work Aside from providing a convenient lookup for human users, the visualization of semantic models provides an important tool for debugging models visually. Additionally, prototyping of semantic applications based on these models is substantially accelerated. VISUALSYNONYMS1 and THESAURUS.COM2 offer lookup possibilities to retrieve various information for input words. Users can view information, like WordNet or Wikipedia definitions and related words (synonyms, hypernyms, etc.). BABELNET3 (Navigli and Ponzetto, 2012) uses such information to compile multilingual definitions and translations for input words. Here, the words are differentiated by senses, with taxonomical labels. BABELNET offers a SPARQL endpoint and APIs for web access. SERELEX4 (Panchenko et al., 2013) is a graphical thesaurus viewer. Users can enter a term for different languages and retrieve related words. The similarity graph displays the similarity links between similar items (‘secondary links’). The items can be expanded for a denser graph. The input terms map to nominal phrases, allowing the interface to display short definitions and</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone P. Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Panchenko</author>
<author>Pavel Romanov</author>
<author>Olga Morozova</author>
<author>Hubert Naets</author>
<author>Andrey Philippovich</author>
</authors>
<title>Alexey Romanov, and Fairon C´edrick.</title>
<date>2013</date>
<booktitle>In Proc. ECIR 2013,</booktitle>
<pages>837--840</pages>
<location>Moscow, Russia.</location>
<contexts>
<context position="4248" citStr="Panchenko et al., 2013" startWordPosition="596" endWordPosition="599">. Additionally, prototyping of semantic applications based on these models is substantially accelerated. VISUALSYNONYMS1 and THESAURUS.COM2 offer lookup possibilities to retrieve various information for input words. Users can view information, like WordNet or Wikipedia definitions and related words (synonyms, hypernyms, etc.). BABELNET3 (Navigli and Ponzetto, 2012) uses such information to compile multilingual definitions and translations for input words. Here, the words are differentiated by senses, with taxonomical labels. BABELNET offers a SPARQL endpoint and APIs for web access. SERELEX4 (Panchenko et al., 2013) is a graphical thesaurus viewer. Users can enter a term for different languages and retrieve related words. The similarity graph displays the similarity links between similar items (‘secondary links’). The items can be expanded for a denser graph. The input terms map to nominal phrases, allowing the interface to display short definitions and disambiguations from Wikipedia. An API with JSON output for similar words is provided. SKETCH ENGINE5 (Kilgarriff et al., 2004) offers access to pre-processed corpora. For each corpus, the user can view concordances and similar terms (thesaurus) for a giv</context>
</contexts>
<marker>Panchenko, Romanov, Morozova, Naets, Philippovich, 2013</marker>
<rawString>Alexander Panchenko, Pavel Romanov, Olga Morozova, Hubert Naets, Andrey Philippovich, Alexey Romanov, and Fairon C´edrick. 2013. Serelex: Search and visualization of semantically related words. In Proc. ECIR 2013, pages 837–840, Moscow, Russia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>The computation of word associations: Comparing syntagmatic and paradigmatic approaches.</title>
<date>2002</date>
<booktitle>In Proc. COLING ’02,</booktitle>
<pages>1--7</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1876" citStr="Rapp (2002)" startWordPosition="247" endWordPosition="248">s a field of computational linguistics, aiming at automatically computing semantic representations for words, sentences and phrases from (large) corpora. While early approaches to distributional semantics were split into symbolic, graph-based approaches (Lin, 1998) and vector-based approaches (Sch¨utze, 1993), recent trends have mainly concentrated on optimizing the representation of word meanings in vector spaces and how these account for compositionality (cf. Baroni and Lenci (2010); Turney and Pantel (2010)). While dense vector representations, obtained by singular value decomposition (cf. Rapp (2002)) or neural embeddings (Mikolov et al., 2010), have gained popularity due to successes in modelling semantic and relational similarity, we propose to revisit graph-based approaches to distributional semantics in the tradition of Lin (1998), Curran (2002) and Biemann and Riedl (2013) – at least as an additional alternative – for the following reasons: • (dense) vector representations are not interpretable, thus it cannot be traced why two terms are similar • vectors do not make word senses explicit, but represent ambiguous words as a mix of their senses • graph-based models can be straightforwa</context>
</contexts>
<marker>Rapp, 2002</marker>
<rawString>Reinhard Rapp. 2002. The computation of word associations: Comparing syntagmatic and paradigmatic approaches. In Proc. COLING ’02, pages 1–7, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Word space.</title>
<date>1993</date>
<booktitle>In Advances in Neural Information Processing Systems 5,</booktitle>
<pages>895--902</pages>
<location>Denver, Colorado, USA.</location>
<marker>Sch¨utze, 1993</marker>
<rawString>Hinrich Sch¨utze. 1993. Word space. In Advances in Neural Information Processing Systems 5, pages 895–902, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benno Stein</author>
<author>Martin Potthast</author>
<author>Martin Trenkmann</author>
</authors>
<title>Retrieving Customary Web Language to Assist Writers.</title>
<date>2010</date>
<booktitle>In Advances in Information Retrieval, ECIR 10,</booktitle>
<pages>631--635</pages>
<location>Milton Keynes, UK.</location>
<contexts>
<context position="5131" citStr="Stein et al., 2010" startWordPosition="736" endWordPosition="739"> map to nominal phrases, allowing the interface to display short definitions and disambiguations from Wikipedia. An API with JSON output for similar words is provided. SKETCH ENGINE5 (Kilgarriff et al., 2004) offers access to pre-processed corpora. For each corpus, the user can view concordances and similar terms (thesaurus) for a given term. SKETCH ENGINE also features statistical information, like word frequency and co-occurrence counts. Furthermore it shows meta information for a corpus. One drawback of the SKETCH ENGINE is that the tools and the models are not freely available. NETSPEAK6 (Stein et al., 2010) is a search engine for words in context. Access is possible via a graphical UI, a RESTful interface and a Java API. Users can enter wildcards and other meta symbols into the input phrases and thus retrieve all the words and phrases that occur in a given context. The information is displayed with corpus statistics. 1http://www.visualsynonyms.com 2http://www.thesaurus.com 3http://babelnet.org/ 4http://serelex.org/ 5http://www.sketchengine.co.uk/ 6http://www.netspeak.org/ A novel aspect of JOBIMVIZ is that it incorporates several different aspects of symbolic methods (distributional thesaurus, c</context>
</contexts>
<marker>Stein, Potthast, Trenkmann, 2010</marker>
<rawString>Benno Stein, Martin Potthast, and Martin Trenkmann. 2010. Retrieving Customary Web Language to Assist Writers. In Advances in Information Retrieval, ECIR 10, pages 631–635, Milton Keynes, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1780" citStr="Turney and Pantel (2010)" startWordPosition="233" endWordPosition="236">e quick integration of such models in research prototypes. 1 Introduction Statistical semantics has emerged as a field of computational linguistics, aiming at automatically computing semantic representations for words, sentences and phrases from (large) corpora. While early approaches to distributional semantics were split into symbolic, graph-based approaches (Lin, 1998) and vector-based approaches (Sch¨utze, 1993), recent trends have mainly concentrated on optimizing the representation of word meanings in vector spaces and how these account for compositionality (cf. Baroni and Lenci (2010); Turney and Pantel (2010)). While dense vector representations, obtained by singular value decomposition (cf. Rapp (2002)) or neural embeddings (Mikolov et al., 2010), have gained popularity due to successes in modelling semantic and relational similarity, we propose to revisit graph-based approaches to distributional semantics in the tradition of Lin (1998), Curran (2002) and Biemann and Riedl (2013) – at least as an additional alternative – for the following reasons: • (dense) vector representations are not interpretable, thus it cannot be traced why two terms are similar • vectors do not make word senses explicit, </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>