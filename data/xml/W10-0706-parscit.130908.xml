<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.046803">
<title confidence="0.9955455">
Shared Task: Crowdsourced Accessibility
Elicitation of Wikipedia Articles
</title>
<author confidence="0.992002">
Scott Novotney and Chris Callison-Burch
</author>
<affiliation confidence="0.918447">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.911144">
3400 North Charles Street
Baltimore, MD, USA
</address>
<email confidence="0.999362">
snovotne@bbn.com ccb@jhu.edu
</email>
<sectionHeader confidence="0.99389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999210210526316">
Mechanical Turk is useful for generating
complex speech resources like conversational
speech transcription. In this work, we ex-
plore the next step of eliciting narrations of
Wikipedia articles to improve accessibility for
low-literacy users. This task proves a use-
ful test-bed to implement qualitative vetting
of workers based on difficult to define metrics
like narrative quality. Working with the Me-
chanical Turk API, we collected sample nar-
rations, had other Turkers rate these samples
and then granted access to full narration HITs
depending on aggregate quality. While narrat-
ing full articles proved too onerous a task to
be viable, using other Turkers to perform vet-
ting was very successful. Elicitation is possi-
ble on Mechanical Turk, but it should conform
to suggested best practices of simple tasks that
can be completed in a streamlined workflow.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997194545454545">
The rise of Mechanical Turk publications in the NLP
community leaves no doubt that non-experts can
provide useful annotations for low cost. Emerging
best practices suggest designing short, simple tasks
that require little amount of upfront effort to most ef-
fectively use Mechanical Turk’s labor pool. Suitable
tasks are best limited to those easily accomplished
in ‘short bites’ requiring little context switching. For
instance, most annotation tasks in prior work (Snow
et al., 2008) required selection from an enumerated
list, allowing for easy automated quality control and
data collection.
More recent work to collect speech transcrip-
tion (Novotney and Callison-Burch, 2010) or paral-
41
lel text translations (Callison-Burch, 2009) demon-
strated that Turkers can provide useful free-form an-
notation.
In this paper, we extend open ended collec-
tion even further by eliciting narrations of English
Wikipedia articles. To vet prospective narrators,
we use qualitative qualifications by aggregating the
opinions of other Turkers on narrative style, thus
avoiding quantification of qualitative tasks.
The Spoken Wikipedia Project1 aims to increase
the accessibility of Wikipedia by recording articles
for use by blind or illiterate users. Since 2008, over
1600 English articles covering topics from art to
technology have been narrated by volunteers. The
charitable nature of this work should provide addi-
tional incentive for Turkers to complete this task.
We use Wikipedia narrations as an initial proof-of-
concept for other more challenging elicitation tasks
such as spontaneous or conversational speech.
While previous work used other Turkers in
second-pass filtering for quality control, we flip this
process and instead require that narrators be judged
favorably before working on full narration tasks. Re-
lying on human opinion sidesteps the difficult task
of automatically judging narrative quality. This re-
quires a multi-pass workflow to manage potential
narrators and grant them access to the full narration
HITs through Mechanical Turk’s Qualifications.
In this paper, we make the following points:
</bodyText>
<listItem confidence="0.996835666666667">
• Vetting based on qualitative criteria like nar-
ration quality can be effectively implemented
through Turker-provided ratings.
</listItem>
<footnote confidence="0.921818">
1http://en.wikipedia.org/wiki/Wikipedia:
WikiProject_Spoken_Wikipedia
</footnote>
<note confidence="0.710237">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 41–44,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<listItem confidence="0.949973428571428">
• Narrating full articles is too complex and time-
consuming for timely task throughput - best
practices are worth following.
• HITs should be streamlined as much as possi-
ble. Requiring Turkers to perform work outside
of the web interface seemingly hurt task com-
pletion rate.
</listItem>
<sectionHeader confidence="0.955003" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.999961458333333">
The research community has demonstrated that
complex annotations (like speech transcription and
elicitation) can be provided through Mechanical
Turk.
Callison-Burch (2009) showed that Turkers could
accomplish complex tasks like translating Urdu or
creating reading comprehension tests.
McGraw et al. (2009) used Mechanical Turk to
improve an English isolated word speech recognizer
by having Turkers listen to a word and select from
a list of probable words at a cost of $20 per hour of
transcription.
Marge et al. (2010) collected transcriptions of
clean speech and demonstrated that duplicate tran-
scription of non-experts can match expert transcrip-
tion.
Novotney and Callison-Burch (2010) collected
transcriptions of conversational speech for as little
as $5 / hour of transcription and demonstrated that
resources are better spent annotating more data than
improving data quality.
McGraw et al. (2010) elicited short snippets of
English street addresses through a web interface.
103 hours were elicted in just over three days.
</bodyText>
<sectionHeader confidence="0.993527" genericHeader="method">
3 Narration Task
</sectionHeader>
<bodyText confidence="0.999652909090909">
Using a python library for parsing Wikipedia2, we
extracted all text under the &lt;p&gt; tag as a heuristic
for readable content. We ignored all other content
like lists, info boxes or headings. Since we wanted
to preserve narrative flow, each article was posted
as one HIT, paying $0.05 per paragraph. Articles
averaged 40 paragraphs, so each HIT averaged $2 in
payment - some as little as $0.25.
We provided instructions for using recording
software and asked Turkers to record one para-
graph at a time. Using Mechanical Turk’s API,
</bodyText>
<footnote confidence="0.671501">
2http://github.com/j2labs/wikipydia
</footnote>
<bodyText confidence="0.9996376">
we generated an XML template for each para-
graph and let the Turker upload a file through the
FileUploadAnswer form. The API supports
constraints on file extensions, so we were able to re-
quire that all files be in mp3 format before the Turker
could submit the work.
Mechanical Turk’s API supports file requests
through the GetFileUploadURL call. A URL is
dynamically generated on Amazon’s servers which
stays active for one minute. We then fetched each
audio file and stored them locally on our own servers
for later processing.
Since these narrations are meant for public con-
sumption and are difficult to quality control, we re-
quired prospective Turkers first qualify.
</bodyText>
<sectionHeader confidence="0.990832" genericHeader="method">
4 Granting Qualitative Qualifications
</sectionHeader>
<bodyText confidence="0.9996905625">
Qualifications are prerequisites that limit which
Turkers can work on a HIT. A common qualifica-
tion provided by Mechanical Turk is a minimum ap-
proval rating for a Turker, indicating what percent-
age of submitted work was approved. We created a
qualification for our narration tasks since we wanted
to ensure only those turkers with a good speaking
voice would complete our tasks.
However, the definition of a “good speaking
voice” is not easy to quantify. Luckily, this task is
well suited to Mechanical Turk’s concept of artifi-
cial artificial intelligence. Humans can easily decide
a narrator’s quality while automatic methods would
be impractical. Additionally, we never define what
a ‘good’ narration voice is, relying instead on public
opinion.
</bodyText>
<subsectionHeader confidence="0.980232">
4.1 Workflow
</subsectionHeader>
<bodyText confidence="0.990039">
We implemented the qualification ratings using the
API with three different steps. Turkers who wish
to complete the full narration HITs are first directed
to a ‘qualification’ HIT with one sample paragraph
paying $0.05. We then use other Turkers to rate the
quality of the narrator, asking them to judge based
on speaking style, audio clarity and pronunciation.
Post Qualification The narration qualification and
full narration HITs are posted.
Sample HIT A prospective narrator uploads a
recording of a sample paragraph earning $0.05.
</bodyText>
<page confidence="0.984602">
42
</page>
<bodyText confidence="0.975197260869565">
The audio is downloaded and hosted on our • muddy audio quality; narrator has a tired and a very
web host. low tone quality.
Rating HIT A HIT is created to be completed ten • Very solemn voice - didn’t like listening to it.
times. Turkers make a binary decision as to 5 Data Analysis
whether they would listen to a full article by Of the thirteen qualified Turkers, only two went on
the narrator and optionally suggest feedback. to complete full narrations. This happened only af-
Grant Qualification The ten ratings are collected ter we shortened the articles to the initial five para-
and if five or more are positive we grant the graphs and raised payment to $0.25 per paragraph.
qualification. The narrator is then automati- While the audio was clear, both authors exhibited
cally contacted with the decision and provided mispronunciations of domain-specific terms. For in-
with any feedback from the rating Turkers. stance, one author narrating Isaac Newton mispro-
Although not straightforward, the API made it nounced Principia with a soft c (/prInsIpi9/) instead
possible to dynamically create HITs, approve as- of a hard c (/prInkIpi9/) and indices as /Ind&gt;aIsez/.
signments, sync audio files and ratings,notify work- Since the text is known ahead of time, one could in-
ers and grant qualifications. It does not, however, clude a pronunciation guide for rare words to assist
manage state across HITs, requiring us to implement the narrator.
our own control logic for associating workers with The more disapointing result, however, is the very
narration and rating HITs. Once implemented, man- slow return of the narration task. Contrasting with
aging the process was as simple as invoking three the successful elicitation of (McGraw et al., 2010),
perl scripts a few times a day. These could easily two reasons clearly stand out.
be rolled into one background process automatically First, these tasks were much too long in length.
controlling the entire workflow. This was due to constraints we placed on collection
4.2 Effectiveness of Turker Ratings to improve data quality. We assumed that multiple
Thirteen Turkers submitted sample audio files over narrators for a single article would ruin the narrative
the course of a week. Collecting the ten ratings took flow. Since few workers were willing to complete
a few hours per Turker. The average rating for the five recordings, future work could chop each article
narrators was 7.5, with three of the thirteen being into smaller chunks to be completed by multiple nar-
rejected for having a score less than 5. The authors rators. In contrast, eliciting spoken addresses has no
agreed with the sentiment of the raters and feel that need for continuity across samples, thus the individ-
the qualification process correctly filtered out the ual HITs in (McGraw et al., 2010) could be much
poor narrators. smaller.
Below is a sample of the comments for an ap- Second, and more importantly, our HITs required
proved narrator and a rejected narrator. much more effort on the part of the Turker. We chose
This Turker was approved with 9/10 votes. to fully use Mechanical Turk’s API to manage data
• The narration was very easy to understand. The and did not implement audio recording or data trans-
speaker’s tone was even, well-paced, and clear. mission through the browser. Turkers were required
Great narration. to record audio in a separate program and then up-
• Very good voice, good pace and modulation. load the files. We thought the added ability to re-
• Very nice voice and pleasant to listen to. I would record and review audio would be a plus compared
have guessed that this was a professional voice ac- to in-browser recording. In contrast, (McGraw et al.,
tor. 2010) used a javascript package to record narrations
This Turker was rejected with 3/10 votes. directly in the browser window. While it was sim-
• Monotone voice, uninterested and barely literate. I ple to use the API, it raised too much of a barrier for
would never listen to this voice for any length of Turkers to complete the task.
time.
43
</bodyText>
<subsectionHeader confidence="0.419678">
5.1 Feasability for Full Narration
</subsectionHeader>
<bodyText confidence="0.99992">
Regardless of the task effectiveness, it is not clear
that Mechanical Turk is cost effective for large scale
narration. A reasonable first task would be to nar-
rate the 2500 featured articles on Wikipedia’s home
page. They average 44 paragraphs in length with
around 4311 words per article. Narrating this corpus
would cost $5500 at the rate of $0.05 per paragraph -
if workers would be willing to complete at that rate.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999836956521739">
Our experiments with Mechanical Turk attempted
to find the limits of data collection and nebulous
task definitions. Long-form narration was unsuc-
cessful due to the length of the tasks and the lack
of a streamlined workflow for the Turkers. How-
ever, assigning qualifications based upon aggregat-
ing qualitative opinions was very successful. This
task exploited the strenghts of Mechanical Turk by
quickly gathering judgements that are easy for hu-
mans to make but near impossible to reliably auto-
mate.
The contrast between the failure of this narration
task and the success of previous elicitation is due
to the nature of the underlying task. Our desire to
have one narrator per article prevented elicitation in
short bites of a few seconds long. Additionally, our
efforts to solely use Mechanical Turk’s API limited
the simplicity of the workflow. While our backend
work was greatly simplified since we relied on ex-
isting data management code, the lack of in-browser
recording placed too much burden on the Turkers.
We would make the following changes if we were
to reimplement this task:
</bodyText>
<listItem confidence="0.973256076923077">
1. Integrate the workflow into the browser.
2. Perform post-process quality control to block
bad narrators from completing more HITs.
3. Drop the requirement of one narrator per ar-
ticle. A successful compromise might be one
section, averaging around five paragraphs.
4. Only narrate the lead in to an article (first par-
gagraph) first. If a user requests a full narration,
then seek out the rest of the article.
5. Place qualification as a much larger set of as-
signments. Turkers often sort HITs by avail-
able assignments, so the qualification HIT was
rarely seen.
</listItem>
<sectionHeader confidence="0.992437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993145">
Chris Callison-Burch. 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazons Me-
chanical Turk. EMNLP.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010. Using the amazon mechanical turk
for transcription of spoken language. ICASSP, March.
Ian McGraw, Alexander Gruenstein, and Andrew Suther-
land. 2009. A self-labeling speech corpus: Collecting
spoken words with an online educational game. In IN-
TERSPEECH.
Ian McGraw, Chia ying Lee, Lee Hetherington, and Jim
Glass. 2010. Collecting Voices from the Crowd.
LREC, May.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
Fast and Good Enough: Automatic Speech Recogni-
tion with Non-Expert Transcription. NAACL, June.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast—but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP.
</reference>
<page confidence="0.999294">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.482679">
<title confidence="0.983323">Shared Task: Crowdsourced Elicitation of Wikipedia Articles</title>
<author confidence="0.819785">Novotney</author>
<affiliation confidence="0.934475">Center for Language and Speech</affiliation>
<address confidence="0.835733333333333">Johns Hopkins 3400 North Charles Baltimore, MD,</address>
<email confidence="0.999008">snovotne@bbn.comccb@jhu.edu</email>
<abstract confidence="0.99979915">Mechanical Turk is useful for generating complex speech resources like conversational speech transcription. In this work, we explore the next step of eliciting narrations of Wikipedia articles to improve accessibility for low-literacy users. This task proves a useful test-bed to implement qualitative vetting of workers based on difficult to define metrics like narrative quality. Working with the Mechanical Turk API, we collected sample narrations, had other Turkers rate these samples and then granted access to full narration HITs depending on aggregate quality. While narrating full articles proved too onerous a task to be viable, using other Turkers to perform vetting was very successful. Elicitation is possible on Mechanical Turk, but it should conform to suggested best practices of simple tasks that can be completed in a streamlined workflow.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazons Mechanical Turk.</title>
<date>2009</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="1870" citStr="Callison-Burch, 2009" startWordPosition="276" endWordPosition="277"> annotations for low cost. Emerging best practices suggest designing short, simple tasks that require little amount of upfront effort to most effectively use Mechanical Turk’s labor pool. Suitable tasks are best limited to those easily accomplished in ‘short bites’ requiring little context switching. For instance, most annotation tasks in prior work (Snow et al., 2008) required selection from an enumerated list, allowing for easy automated quality control and data collection. More recent work to collect speech transcription (Novotney and Callison-Burch, 2010) or paral41 lel text translations (Callison-Burch, 2009) demonstrated that Turkers can provide useful free-form annotation. In this paper, we extend open ended collection even further by eliciting narrations of English Wikipedia articles. To vet prospective narrators, we use qualitative qualifications by aggregating the opinions of other Turkers on narrative style, thus avoiding quantification of qualitative tasks. The Spoken Wikipedia Project1 aims to increase the accessibility of Wikipedia by recording articles for use by blind or illiterate users. Since 2008, over 1600 English articles covering topics from art to technology have been narrated by</context>
<context position="4103" citStr="Callison-Burch (2009)" startWordPosition="597" endWordPosition="598">Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 41–44, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics • Narrating full articles is too complex and timeconsuming for timely task throughput - best practices are worth following. • HITs should be streamlined as much as possible. Requiring Turkers to perform work outside of the web interface seemingly hurt task completion rate. 2 Prior Work The research community has demonstrated that complex annotations (like speech transcription and elicitation) can be provided through Mechanical Turk. Callison-Burch (2009) showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests. McGraw et al. (2009) used Mechanical Turk to improve an English isolated word speech recognizer by having Turkers listen to a word and select from a list of probable words at a cost of $20 per hour of transcription. Marge et al. (2010) collected transcriptions of clean speech and demonstrated that duplicate transcription of non-experts can match expert transcription. Novotney and Callison-Burch (2010) collected transcriptions of conversational speech for as little as $5 / hour of </context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazons Mechanical Turk. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Marge</author>
<author>Satanjeev Banerjee</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Using the amazon mechanical turk for transcription of spoken language.</title>
<date>2010</date>
<publisher>ICASSP,</publisher>
<contexts>
<context position="4453" citStr="Marge et al. (2010)" startWordPosition="653" endWordPosition="656">rs to perform work outside of the web interface seemingly hurt task completion rate. 2 Prior Work The research community has demonstrated that complex annotations (like speech transcription and elicitation) can be provided through Mechanical Turk. Callison-Burch (2009) showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests. McGraw et al. (2009) used Mechanical Turk to improve an English isolated word speech recognizer by having Turkers listen to a word and select from a list of probable words at a cost of $20 per hour of transcription. Marge et al. (2010) collected transcriptions of clean speech and demonstrated that duplicate transcription of non-experts can match expert transcription. Novotney and Callison-Burch (2010) collected transcriptions of conversational speech for as little as $5 / hour of transcription and demonstrated that resources are better spent annotating more data than improving data quality. McGraw et al. (2010) elicited short snippets of English street addresses through a web interface. 103 hours were elicted in just over three days. 3 Narration Task Using a python library for parsing Wikipedia2, we extracted all text under</context>
</contexts>
<marker>Marge, Banerjee, Rudnicky, 2010</marker>
<rawString>Matthew Marge, Satanjeev Banerjee, and Alexander Rudnicky. 2010. Using the amazon mechanical turk for transcription of spoken language. ICASSP, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian McGraw</author>
<author>Alexander Gruenstein</author>
<author>Andrew Sutherland</author>
</authors>
<title>A self-labeling speech corpus: Collecting spoken words with an online educational game.</title>
<date>2009</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="4238" citStr="McGraw et al. (2009)" startWordPosition="614" endWordPosition="617"> Computational Linguistics • Narrating full articles is too complex and timeconsuming for timely task throughput - best practices are worth following. • HITs should be streamlined as much as possible. Requiring Turkers to perform work outside of the web interface seemingly hurt task completion rate. 2 Prior Work The research community has demonstrated that complex annotations (like speech transcription and elicitation) can be provided through Mechanical Turk. Callison-Burch (2009) showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests. McGraw et al. (2009) used Mechanical Turk to improve an English isolated word speech recognizer by having Turkers listen to a word and select from a list of probable words at a cost of $20 per hour of transcription. Marge et al. (2010) collected transcriptions of clean speech and demonstrated that duplicate transcription of non-experts can match expert transcription. Novotney and Callison-Burch (2010) collected transcriptions of conversational speech for as little as $5 / hour of transcription and demonstrated that resources are better spent annotating more data than improving data quality. McGraw et al. (2010) e</context>
</contexts>
<marker>McGraw, Gruenstein, Sutherland, 2009</marker>
<rawString>Ian McGraw, Alexander Gruenstein, and Andrew Sutherland. 2009. A self-labeling speech corpus: Collecting spoken words with an online educational game. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian McGraw</author>
<author>Chia ying Lee</author>
<author>Lee Hetherington</author>
<author>Jim Glass</author>
</authors>
<date>2010</date>
<booktitle>Collecting Voices from the Crowd. LREC,</booktitle>
<contexts>
<context position="4836" citStr="McGraw et al. (2010)" startWordPosition="707" endWordPosition="710">. McGraw et al. (2009) used Mechanical Turk to improve an English isolated word speech recognizer by having Turkers listen to a word and select from a list of probable words at a cost of $20 per hour of transcription. Marge et al. (2010) collected transcriptions of clean speech and demonstrated that duplicate transcription of non-experts can match expert transcription. Novotney and Callison-Burch (2010) collected transcriptions of conversational speech for as little as $5 / hour of transcription and demonstrated that resources are better spent annotating more data than improving data quality. McGraw et al. (2010) elicited short snippets of English street addresses through a web interface. 103 hours were elicted in just over three days. 3 Narration Task Using a python library for parsing Wikipedia2, we extracted all text under the &lt;p&gt; tag as a heuristic for readable content. We ignored all other content like lists, info boxes or headings. Since we wanted to preserve narrative flow, each article was posted as one HIT, paying $0.05 per paragraph. Articles averaged 40 paragraphs, so each HIT averaged $2 in payment - some as little as $0.25. We provided instructions for using recording software and asked T</context>
<context position="9296" citStr="McGraw et al., 2010" startWordPosition="1427" endWordPosition="1430">prInkIpi9/) and indices as /Ind&gt;aIsez/. signments, sync audio files and ratings,notify work- Since the text is known ahead of time, one could iners and grant qualifications. It does not, however, clude a pronunciation guide for rare words to assist manage state across HITs, requiring us to implement the narrator. our own control logic for associating workers with The more disapointing result, however, is the very narration and rating HITs. Once implemented, man- slow return of the narration task. Contrasting with aging the process was as simple as invoking three the successful elicitation of (McGraw et al., 2010), perl scripts a few times a day. These could easily two reasons clearly stand out. be rolled into one background process automatically First, these tasks were much too long in length. controlling the entire workflow. This was due to constraints we placed on collection 4.2 Effectiveness of Turker Ratings to improve data quality. We assumed that multiple Thirteen Turkers submitted sample audio files over narrators for a single article would ruin the narrative the course of a week. Collecting the ten ratings took flow. Since few workers were willing to complete a few hours per Turker. The averag</context>
</contexts>
<marker>McGraw, Lee, Hetherington, Glass, 2010</marker>
<rawString>Ian McGraw, Chia ying Lee, Lee Hetherington, and Jim Glass. 2010. Collecting Voices from the Crowd. LREC, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Novotney</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Cheap, Fast and Good Enough: Automatic Speech Recognition with Non-Expert Transcription.</title>
<date>2010</date>
<location>NAACL,</location>
<contexts>
<context position="1814" citStr="Novotney and Callison-Burch, 2010" startWordPosition="266" endWordPosition="269">the NLP community leaves no doubt that non-experts can provide useful annotations for low cost. Emerging best practices suggest designing short, simple tasks that require little amount of upfront effort to most effectively use Mechanical Turk’s labor pool. Suitable tasks are best limited to those easily accomplished in ‘short bites’ requiring little context switching. For instance, most annotation tasks in prior work (Snow et al., 2008) required selection from an enumerated list, allowing for easy automated quality control and data collection. More recent work to collect speech transcription (Novotney and Callison-Burch, 2010) or paral41 lel text translations (Callison-Burch, 2009) demonstrated that Turkers can provide useful free-form annotation. In this paper, we extend open ended collection even further by eliciting narrations of English Wikipedia articles. To vet prospective narrators, we use qualitative qualifications by aggregating the opinions of other Turkers on narrative style, thus avoiding quantification of qualitative tasks. The Spoken Wikipedia Project1 aims to increase the accessibility of Wikipedia by recording articles for use by blind or illiterate users. Since 2008, over 1600 English articles cove</context>
<context position="4622" citStr="Novotney and Callison-Burch (2010)" startWordPosition="675" endWordPosition="678">otations (like speech transcription and elicitation) can be provided through Mechanical Turk. Callison-Burch (2009) showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests. McGraw et al. (2009) used Mechanical Turk to improve an English isolated word speech recognizer by having Turkers listen to a word and select from a list of probable words at a cost of $20 per hour of transcription. Marge et al. (2010) collected transcriptions of clean speech and demonstrated that duplicate transcription of non-experts can match expert transcription. Novotney and Callison-Burch (2010) collected transcriptions of conversational speech for as little as $5 / hour of transcription and demonstrated that resources are better spent annotating more data than improving data quality. McGraw et al. (2010) elicited short snippets of English street addresses through a web interface. 103 hours were elicted in just over three days. 3 Narration Task Using a python library for parsing Wikipedia2, we extracted all text under the &lt;p&gt; tag as a heuristic for readable content. We ignored all other content like lists, info boxes or headings. Since we wanted to preserve narrative flow, each artic</context>
</contexts>
<marker>Novotney, Callison-Burch, 2010</marker>
<rawString>Scott Novotney and Chris Callison-Burch. 2010. Cheap, Fast and Good Enough: Automatic Speech Recognition with Non-Expert Transcription. NAACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>