<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.950828">
Single Character Chinese Named Entity Recognition
</title>
<author confidence="0.985221">
Xiaodan Zhu, Mu Li, Jianfeng Gao and Chang-Ning Huang
</author>
<affiliation confidence="0.974665">
Microsoft Research, Asia
</affiliation>
<address confidence="0.470766">
Beijing 100080, China
</address>
<email confidence="0.989623">
xdzhu@msrchina.research.microsoft.com
{t-muli,jfgao,cnhuang}@microsoft.com
</email>
<sectionHeader confidence="0.997337" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999823695652174">
Single character named entity (SCNE) is a
name entity (NE) composed of one Chinese
character, such as “” (zhong1, China)
and “ ” e2,Russia . SCNE is very
common in written Chinese text. However,
due to the lack of in-depth research, SCNE
is a major source of errors in named entity
recognition (NER). This paper formulates
the SCNE recognition within the source-
channel model framework. Our experi-
ments show very encouraging results: an F-
score of 81.01% for single character loca-
tion name recognition, and an F-score of
68.02% for single character person name
recognition. An alternative view of the
SCNE recognition problem is to formulate
it as a classification task. We construct two
classifiers based on maximum entropy
model (ME) and vector space model
(VSM), respectively. We compare all pro-
posed approaches, showing that the source-
channel model performs the best in most
cases.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999360416666667">
The research of named entity recognition (NER)
becomes very popular in recent years due to its
wide applications and the Message Understanding
Conference (MUC) which provides a standard test-
bed for NER evaluation. Recent research on Eng-
lish NER includes (Collins, 2002; Isozaki, 2002;
Zhou, 2002; etc.). Chinese NER research includes
(Liu, 2001; Zheng, 2000; Yu, 1998; Chen, 1998;
Shen, 1995; Sun, 1994; Zhang, 1992 etc.)
In Chinese NEs, there is a special kind of NE,
called single character named entity (SCNE), on
which there is little in-depth research. SCNE is a
NE composed of only one Chinese character, such
as the location name “ ” (zhong1,China) and
“ ” (e2,Russia) in the phrase “ ”
(zhong1-e2-mao4-yi4, trade between China and
Russia). SCNE is very common in written Chinese
text. For instance, SCNE accounts for 8.17% of all
NE tokens according to our statistics on a 10MB
corpus. However, due to the lack of research,
SCNE is a major source of errors in NER. Among
three state-of-the-art systems we have, the best F-
scores of single character location (SCL) and sin-
gle character person (SCP) are 43.63% and 43.48%
respectively. This paper formulates the SCNE rec-
ognition within the source-channel model frame-
work. Our results show very encouraging
performance. We achieve an F-score of 81.01% for
SCL recognition and an F-score of 68.02% for
SCP recognition. An alternative view of the SCNE
recognition problem is to formulate it as a
classification task. For example, “” is a SCNE
in “”, but not in “ ”(bei3-jing1-
si4-zhong1, Beijing No.4 High School). We then
construct two classifiers respectively based on two
statistical models: maximum entropy model (ME)
and vector space model (VSM). We compare these
two classifiers with the source-channel model,
showing that the source-channel model is slightly
better. We then compare the source-channel model
with other three state-of-the-art NER systems.
The remainder of this paper is structured as
follows: Section 2 introduces the task of SCNE
recognition and related work. Section 3 and 4 pro-
pose the source-channel model and two classifiers
for SCNE recognition, respectively. Section 5 pre-
sents experimental results and error analysis. Sec-
tion 6 gives conclusion.
</bodyText>
<sectionHeader confidence="0.787232" genericHeader="method">
2 SCNE Recognition and Related Work
</sectionHeader>
<bodyText confidence="0.98397075">
We consider three types of SCNE in this paper:
single character location name (SCL), person
name (SCP), and organization name (SCO). Be-
low are examples:
</bodyText>
<listItem confidence="0.756913142857143">
2. SCP: “” (zhou1, Zhou) in “
(zhou1-zong3-li3,Premier Zhou),
3. SCO: “” (guo2, Kuomingtang Party)
and “” (gong4, Communist Party ) in
“ ” (Guo2-gong4-he2-zuo4,
Cooperation between Kuomingtang Part
and Communist Party)
</listItem>
<bodyText confidence="0.870174222222222">
SCNE is very common in written Chinese text.
As shown in Table 1, SCNE accounts for 8.17%
of all NE tokens on the 10MB corpus. Especially,
14.65% of location names are SCLs. However,
due to the lack of research, SCNE is a major
source of errors in NER. In our experiments
described below, we focus on SCL and SCP,
while SCO is not considered because of its small
number in the data.
</bodyText>
<table confidence="0.999249">
# SCNE # NE #SCNE / #NE
PN 5,892 129,317 4.56%
LN 32,483 221,713 14.65%
ON 356 122,779 0.29%
Total 38,731 473,809 8.17%
</table>
<tableCaption confidence="0.999945">
Table 1. Proportion of SCNE in NE
</tableCaption>
<bodyText confidence="0.99937725">
To our knowledge, most NER systems do not
report SCNE recognition results separately.
Some systems (e.g. Liu, 2001) even do not in-
clude SCNE in recognition task. SCNE recogni-
tion is achieved using the same technologies as
for NER, which can be roughly classified into
rule-based methods and statistical-based methods,
while most of state-of-the-art systems use hybrid
approaches.
Wang (1999) and Chen (1998) used linguistic
rules to detect NE with the help of the statistics
from dictionary. Ji(2001), Zheng (2000),
Shen(1995) and Sun(1994) used statistics from
dictionaries and large corpus to generate PN or
LN candidates, and used linguistic rules to filter
the result, and Yu (1998) used language model to
filter. Liu (2001) applied statistical and linguistic
knowledge alternatively in a seven-step proce-
dure. Unfortunately, most of these results are
incomparable due to the different test sets used,
except the results of Chen (1998) and Yu (1998).
They took part in Multilingual Entity Task
(MET-2) on Chinese, held together with MUC-7.
Between them, Yu (1998)’s results are slightly
better. However, these two comparable sys-
tems did not report their results on SCNE sepa-
rately. To evaluate our results, we compare with
three state-of-the-art system we have. These sys-
tems include: MSWS, PBWS and LCWS. The
former two are developed by Microsoft® and the
last one comes from by Beijing Language Uni-
versity.
</bodyText>
<sectionHeader confidence="0.9739455" genericHeader="method">
3 SCNE Recognition Using an Improved
Source-Channel Model
</sectionHeader>
<subsectionHeader confidence="0.99778">
3.1 Improved Source-Channel Model1
</subsectionHeader>
<bodyText confidence="0.999956">
We first conduct SCNE recognition within a
framework of improved source-channel models,
which is applied to Chinese word segmentation.
We define Chinese words as one of the following
four types: (1) entries in a lexicon, (2) morpho-
logically derived words, (3) named entity (NE)
</bodyText>
<listItem confidence="0.9268575">
,
and (4) factoid. Examples are
1. lexicon word: (peng2-you3, friend).
2. morph-derived word: (gao1-gao1-
xing4-xing4 , happily)
3. named entity: ✍(wei1-ruan3-gong1-
si1, Microsoft Corporation)
4. factoid2: (yi1-yue4-jiu3-ri4, Jan 9th)
</listItem>
<bodyText confidence="0.999657">
Chinese NER is achieved within the framework.
To make our later discussion on SCNE clear, we
introduce the model briefly.
We are given Chinese sentence S, which is
a character string. For all possible word segmen-
tations W, we will choose the one which
achieves the highest conditional probability W*
= argmax w P(W|S). According to Bayes’ law and
dropping the constant denominator, we acquire
the following equation:
</bodyText>
<footnote confidence="0.71339925">
1 This follows the description of (Gao, 2003).
2 We define ten types of factoid: date, time (TIME), percent-
age, money, number (NUM), measure, e-mail, phone number,
and WWW.
</footnote>
<equation confidence="0.881540666666667">
1. SCL: “”and “” in “
”
”
W * = arg max ( ) ( |
P W P S W
W
</equation>
<bodyText confidence="0.999705727272727">
Following our Chinese word definition, we de-
fine word class C as follows: (1) each lexicon
word is defined as a class; (2) each morphologi-
cally derived word is defined as a class; (3) each
type of named entities is defined as a class, e.g.
all person names belong to a class PN, and (4)
each type of factoids is defined as a class, e.g. all
time expressions belong to a class TIME. We
therefore convert the word segmentation W into
a word class sequence C. Eq. 1 can then be
rewritten as:
</bodyText>
<equation confidence="0.998129333333333">
* = arg max ( ) (  |)
P C P S C
C
</equation>
<bodyText confidence="0.998436169811321">
Eq. 2 is the basic form of the source-channel
models for Chinese word segmentation. The
models assume that a Chinese sentence S is gen-
erated as follows: First, a person chooses a se-
quence of concepts (i.e., word classes C) to
output, according to the probability distribution
P(C); then the person attempts to express each
concept by choosing a sequence of characters,
according to the probability distribution P(S|C).
We use different types of channel models
for different types of Chinese words. This brings
several advantages. First, different linguistic
constraints can be easily added to corresponding
channel models (see Figure 1). These constraints
can be dynamic linguistic knowledge acquired
through statistics or intuitive rules compiled by
linguists. Second, this framework is data-driven,
which makes it easy to adapt to other languages.
We have three channel models for PN, LN and
ON respectively. (see Figure 1)
However, although Eq. 2 suggests that channel
model probability and source model probability
can be combined through simple multiplication, in
practice some weighting is desirable. There are two
reasons. First, some channel models are poorly
estimated, owing to the sub-optimal assumptions
we make for simplicity and the insufficiency of the
training corpus. Combining the channel model
probability with poorly estimated source model
probabilities according to Eq. 2 would give the
context model too little weight. Second, as seen in
Figure 1, the channel models of different word
classes are constructed in different ways (e.g. name
entity models are n-gram models trained on cor-
pora, and factoid models are compiled using lin-
guistic knowledge). Therefore, the quantities of
channel model probabilities are likely to have
vastly different dynamic ranges among different
word classes. One way to balance these probability
quantities is to add several channel model weight
CW, each for one word class, to adjust the channel
model probability P(S|C) to P(S|C)CW. In our ex-
periments, these weights are determined empiri-
cally on a development set.
Given the source-channel models, the procedure
of word segmentation involves two steps: first,
given an input string S, all word candidates are
generated (and stored in a lattice). Each candidate
is tagged with its class and the probability P(S’|C),
where S’ is any substring of S. Second, Viterbi
search is used to select (from the lattice) the most
probable word segmentation (i.e. word class se-
quence C*) according to Eq. 2.
</bodyText>
<equation confidence="0.996916333333333">
) (1)
C
. (2)
</equation>
<table confidence="0.964507">
Word class Channel model Linguistic Constraints
Lexicon word (LW) P(S|LW)=1 if S forms a lexicon entry, Word lexicon
0 otherwise.
Morphologically derived word P(S|MW)=1 if S forms a morph lexicon Morph-lexicon
(MW) entry, 0 otherwise.
Person name (PN) Character bigram family name list, Chinese PN patterns
Location name (LN) Character bigram LN keyword list, LN lexicon, LN abbr. list
Organization name (ON) Word class bigram ON keyword list, ON abbr. List
Factoid (FT) P(S|G)=1 if S can be parsed using a Factoid rules (presented by FSTs).
factoid grammar G, 0 otherwise
</table>
<figureCaption confidence="0.986918">
Figure 1. Channel models (Gao, 2003)
</figureCaption>
<subsectionHeader confidence="0.942502">
3.2 Improved Model for SCNE Recognition
</subsectionHeader>
<bodyText confidence="0.999979444444444">
Although our results show that the source-
channel models achieve the state-of-the-art word
segmentation performance, they cannot handle
SCNE very well. Error analysis shows that
11.6% person name errors come from SCP, and
47.7% location names come from SCL. There
are two reasons accounting for it: First, SCNE is
generated in a different way from that of multi-
character NE. Second, the context of SCNE is
different from other NE. For example, SCNE
usually appears one after another such as “
”. But this is not the case for multi-
character NE.
To solve the first problem, we add two new
channel models to Figure 1, that is, define each
type of SCNE (i.e. SCL and SCP) as a individual
class (i.e. NE_SCL and NE_SCP) with its chan-
nel probability P(Sj |NE_SCL), and P(Sj
|NE_SCP). P(Sj |NE_SCL) is calculated by Eq. 3.
We also use two CW to balance their channel
probabilities with other NE’s.
To solve the second problem, we trained a new
source model P(C) on the re-annotated training
corpus, where all SCNE are tagged by SCL or SCP.
For example, “” in “”is tagged as SCP
instead of PN, and “” in “” is tagged as
SCL in stead of LN.
</bodyText>
<sectionHeader confidence="0.999547" genericHeader="method">
4 Character-based Classifiers
</sectionHeader>
<bodyText confidence="0.967347352941176">
In this section, SCNE recognition is formulated
as a binary classification problem. Our motiva-
tions are two folds. First, most NER systems do
not use source-channel model, so our method
described in the previous section cannot be ap-
plied. However, if we define SCNE as a binary
classification problem, it would be possible to
build a separate recognizer which can be used
together with any NER systems. Second, we are
interested in comparing the performance of
source-channel models with that of other meth-
ods.
For each Chinese character, a classifier is
built to estimate how likely an occurrence of this
Chinese character in a text is a SCNE. Some ex-
amples of these Chinese character as well as
their probabilities of being a SCNE is shown in
</bodyText>
<tableCaption confidence="0.998631">
Table 2.
Table 2. The probability of a character as SCNE
</tableCaption>
<bodyText confidence="0.9999527">
We can see that the probabilities of being a
SCNE of many characters are very small. Thus,
SCNE recognition is an ‘unbalanced’ classifica-
tion problem. That is, in most cases, it is safer to
assume that a character is not a SCNE.
We construct two classifiers respectively
based on two statistical models: maximum
entropy model (ME) and vector space model
(VSM). Local context characters (i.e. left or right
characters within a window) are used as features.
</bodyText>
<subsectionHeader confidence="0.993062">
4.1 Maximum Entropy
</subsectionHeader>
<bodyText confidence="0.999806">
ME provides a good framework to integrate
various features from different knowledge
sources. Each feature is typically represented as
a binary constraint f. All features are then com-
bined using a log-linear model shown in Eq. 5.
</bodyText>
<equation confidence="0.962436111111111">
1
Pλ (y|x) = Zλ (x) exp( λi fi (x, y)) i
P(Sj|NE_ SCL) =  |SCL (Sj)  |(3) n =|
i
|i 1
SCL (S
)
(
NE_
)
SCP
P
Sj|
SCP
=|
(Sj)  |(4) n=|
|i 1
SCP vi)
</equation>
<bodyText confidence="0.705467">
Here,
is a character in SCL list which is ex-
tracted from training corpus.
is the
number of tokens
are labeled as SCL
in training corpus. n is the size of SCL list,
which includes 177 SCL. Similarly, P(Sj
is calculated by Eq. 4, an
</bodyText>
<figure confidence="0.758421857142857">
Sj
|SCL(Sj)|
Sj, which
|NE_SCP)
d the SCP
list includes 151 SCP.
(5)
</figure>
<bodyText confidence="0.999685058823529">
where i is a weight of the feature fi , and Z(x) is
a normalization factor.
Weights ( ) are estimated using the maximum
entropy principle: to satisfy constraints on ob-
served data and assume a uniform distribution
(with the maximum entropy) on unseen data. The
training algorithm we used is the improved itera-
tive scaling (IIS) described in (Berger et al,
1996)3. The context features include six charac-
ters: three on the left of the SCNE, and three on
the right. Given the context features, the ME
classifier would estimate the probability of the
candidate being a SCNE. In our example, we
treat candidates with the probability larger than
0.5 as SCNEs. To get the precision-recall curve,
we can vary the probability threshold from 0.1 to
0.9.
</bodyText>
<subsectionHeader confidence="0.892455">
4.2 Vector Space Model
</subsectionHeader>
<bodyText confidence="0.996530333333333">
VSM is another model we used to detect SCNE.
Similar to ME, we use six surrounding characters
as the features, as shown in Figure 2.
</bodyText>
<figureCaption confidence="0.997882">
Figure 2. Context window
</figureCaption>
<bodyText confidence="0.999936125">
In this approach, we apply the standard tf-idf
weighting technique with one minor adaptation:
the same character appearing in different
positions within the context window is
considered as different terms. For example,
character Cj appearing at position i, i{-3,-2,-
1,1,2,3}, is regarded as term Cji,. Term weight-
ing of Cji is acquired with Eq.6.
</bodyText>
<equation confidence="0.819577">
Wei ( Cji)=TF(Cji) *IDF(Cji) *PW i (6)
</equation>
<bodyText confidence="0.999625857142857">
With this adaptation, we can apply an additional
weighting coefficient PWi to different position, so
as to reflect the importance of different positions.
PWi is determined in a heuristic way as shown in
Table.3 with the underlying principle that the
closer the context character is to the SCNE candi-
date, the larger PWi is.
</bodyText>
<page confidence="0.583132">
3 Thank Joshua Goodman for providing the ME toolkit.
</page>
<table confidence="0.981164">
Pos -3 -2 -1 1 2 3
PWi 1 4 7 7 4 1
</table>
<tableCaption confidence="0.999723">
Table 3. Weights assigned to deferent positions
</tableCaption>
<bodyText confidence="0.999935333333333">
A precision/recall curve can be obtained by mul-
tiplying a factor to one of the two cosine dis-
tances we get, before comparing them.
</bodyText>
<sectionHeader confidence="0.998849" genericHeader="evaluation">
5 Experiment results
</sectionHeader>
<subsectionHeader confidence="0.997531">
5.1 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.99738">
To achieve a reliable evaluation, we developed
an annotated test set. First, we discuss a standard
of Chinese NE and SCNE. Most previous re-
searches define their own standards; hence re-
sults of different systems are not comparable.
Recently, two widely accepted standards were
developed. They are (1) MET-2 (Multilingual
Entity Task)4 for Chinese and Japanese NE, and
(2) IEER-99’5 for Chinese NE. IEER-99 is a
slightly modified version of MET-2. Our
NE/SCNE standard is based on these two well-
known standards.
Second, we manually annotated a 10MB
training corpus and a 1MB test corpus. The texts
are randomly selected from People’s Daily, in-
cluding articles from 10 subjects and 5 writing
styles. This test set is much larger than MET-2
test data (which is about 106 KB), and contains
more SCNE for evaluation.
The evaluation metrics we used include
precision (P), recall (R), and F-score. F-score is
experiments.
</bodyText>
<subsectionHeader confidence="0.997915">
5.2 Results of Source-Channel Models
</subsectionHeader>
<bodyText confidence="0.9951608">
We show the SCNE recognition results using the
source-channel models described in Section 3.
Two versions of NE models are used. M1 is the
original model described in Section 3.1. M2 is the
one adapted for SCNE, shown in Section 3.2. The
results in Table 4 show that obvious improvement
can be achieved on SCL and SCP after adapting
source-channel models for SCNE. As shown in
Table 5, the improvement of SCL and SCP has
significant impact on performance of LN and PN
</bodyText>
<footnote confidence="0.983105">
4 http://www.itl.nist.gov/iaui/894.02/related_projects/muc/
5 http://www.nist.gov/speech/tests/ie-er/er_99/er_99.htm
</footnote>
<equation confidence="0.930115833333333">
2
(β
*P)+R
calculated as
( 1.0) * *
β 2 + P R , while ✂=1 in our
</equation>
<bodyText confidence="0.90894">
recognition. We can see that the increase of F-
score of LN is 5.13%, and PN is 0.92% absolutely.
</bodyText>
<table confidence="0.9931806">
SCL SCP
P% R% F P% R% F
M1 83.77 15.25 25.80 * * *
M2 84.38 77.90 81.01 76.14 61.47 68.02
* No SCP is detected
</table>
<tableCaption confidence="0.997607">
Table 4. Improvement of SCL and SCP recognition
</tableCaption>
<table confidence="0.99979">
LN PN
P% R% F P% R% F
M1 88.60 71.35 79.04 83.23 76.17 79.54
M2 88.20 80.49 84.17 83.51 77.63 80.46
</table>
<tableCaption confidence="0.999666">
Table 5. Improvement of LN and PN recognition
</tableCaption>
<subsectionHeader confidence="0.977701">
5.3 Results of Different Methods
</subsectionHeader>
<bodyText confidence="0.998554666666667">
In Figures 3 and 4, we compare the results of the
source-channel models with two classifiers de-
scribed in Section 4: ME and VSM.
</bodyText>
<figureCaption confidence="0.9999885">
Figure 3. Result of different methods on SCL
Figure 4. Result of different methods on SCP
</figureCaption>
<bodyText confidence="0.999934">
We can see that source-channel model achieves
the best result. This can be interpreted as follows.
The source-channel models use more informa-
tion than the other two methods. The feature set
of ME or VSM classifiers includes only six sur-
rounding characters while the source-channel
models use much rich global and local informa-
tion as shown in Figure 1. Based on our analysis,
we believe that even enlarging the window size
of the local context, the performance gain of
these classifiers is very limited, because most
error tags cannot be correctly classified using
local context features. We can then say with con-
fidence that the source-channel models can
achieve comparable results with ME and VSM
even if they used more local context.
</bodyText>
<subsectionHeader confidence="0.9953085">
5.4 Comparison with Other State-of-The-Art
Systems
</subsectionHeader>
<bodyText confidence="0.996244">
The section compares the performance of the
source-channel models M2, with three state-of-
the-art systems: MSWS, LCWS and PBWS.
</bodyText>
<listItem confidence="0.93511725">
1. The MSWS system is one of the best avail-
able products. It is released by Microsoft® (as
a set of Windows APIs). MSWS first con-
ducts the word breaking using MM (aug-
mented by heuristic rules for disambiguation),
then conducts factoid detection and NER us-
ing rules.
2. The LCWS system is one of the best research
</listItem>
<bodyText confidence="0.905379">
systems in mainland China. It is released by
Beijing Language University. The system
works similarly to MSWS, but has a larger
dictionary containing more PNs and LNs.
3. The PBWS system is a rule-based Chinese
parser, which can also output the NER results.
It explores high-level linguistic knowledge
such as syntactic structure for Chinese word
segmentation and NER.
To compare the results across different systems,
we have to consider the problem that they might
have different tagging format or spec. For exam-
ple, the LCWS system tags the two-character
string “” as a location name, and tags “
” other than “” as a person name. We
then manually convert all tagging results of these
three systems according to our spec. The results
are shown in Table 6.
</bodyText>
<table confidence="0.979499">
SCL SCP
P% R% F R% P% F
* No SCL is detected
</table>
<tableCaption confidence="0.999508">
Table 6. Comparison with other systems
</tableCaption>
<bodyText confidence="0.999772666666667">
We can see that our system (M2) achieves the
best results in both SCL and SCP recognition.
PBWS has the second best result in recognizing
SCL (43.63%), and MSWS in SCP (43.48%).
However, they achieved the worst result on SCP
and SCL, respectively.
</bodyText>
<subsectionHeader confidence="0.515723">
5.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99399137037037">
Through human checking, we list the typical er-
rors as follows:
resovle this ambiguity well. Errors in (3) come
from another kind of ambiguities such as
ambiguity between SCNE and normal lexicon
words. They are partly caused by noises in training
data, because SCNE are very likely to be neglected
by annotators, which makes training data more
sparse. Both errors in (1) and (3) are not easy to
handle.
Our immediate work is to cope with errors in
(2), which account for about 8.9% of all errors. We
can obtain additional SCNE entries from resources
such as abbreviation dictionaries. However, the
procedure to select SENE entries should be careful,
because the SCNE characters we do not cover
currently might be rare to act as SCNE, and
difficult to recall. Besides, unsupervised methods
can be applied to the task, considering
insufficiency of the training data of the task.
ba1-xi1, Brazil.
Errors in (1) account for about 40% of all errors.
SCNE is usually a part of multi-character NE, such
as “”, “” in “”.Viterbi
search has to make a decision: recognizing the
multi-character NE, or recognizing SCNE. Current
features we used seem not powful enough to
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999178125">
Although SCNE is very common in written Chi-
nese text, due to the lack of in-depth research,
SCNE is a major source of errors in NER. This
paper formulates the SCNE recognition within the
source-channel model framework. Our experi-
ments show very encouraging results: an F-score
of 81.01% for single character location name rec-
ognition, an F-score of 68.02% for single character
person name recognition. An alternative view of
the SCNE recognition problem is to formulate it as
a classification task. We construct two classifiers
respectively based on maximum entropy model
(ME) and vector space model (VSM), respectively.
We compare all proposed approaches, showing
that the source-channel model performs the best in
most cases.
</bodyText>
<sectionHeader confidence="0.997989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989792166666667">
A. Berger, S. Della Pietra and V. Della Pietra, 1996. A
maximum entropy approach to natural language
processing. Computational Linguistics 22(1):39-71.
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. NYU: Description of the
MENE Named Entity System as Used in MUC-7.
Proceedings of the Seventh Message Understanding
Conference (MUC-7)
Hsin-Hsi Chen, Yung-Wei Ding, Shih-Chung Tsai and
Guo-Wei Bian, 1998. Description of the NTU Sys-
tem used for MET-2, Proceedings of the Seventh
Message Understanding Conference (MUC-7).
</reference>
<figure confidence="0.9750456">
1. Ambiguity between NE: “
”(mei3-zhong1-mao4-yi4-quan2-guo2-
wei3-yuan2-hui4, National Committee on
United States-China Relations) is a ON, but
“” are usually wrongly recognized
</figure>
<bodyText confidence="0.810343263157895">
as SCL. On the contrary, “
”(ri4- zhong1-you3-hao3-qi1-tuan2-ti3,
seven Janpan-China friendship organiza-
tions ) is tagged as ON falsely. So
“”(ri4,Japan), “” are missed.
2. SCNE list acquire from training data cannot
covers some cases in test data: “
”(zhong1-ka3-zu2-qiu2-sai4, China-
Qatar soccer match), “ ”(ka3, Qatar)
here is stand for “ ”(ka3-ta1-er3,
Qatar), which is out of SCL list.
3. Other errors: “”(zhong1-ba1-
shi3-chu1-da4-men2, the middle bus drives
out from the gate),“ ”(zhong1, middle),
“ ” (ba1, bus)are recognized falsely as
SCL. Because“” and “” can also stand
for China and Pakistan.“ ” can even
stand for other countries such as “”
“”,
</bodyText>
<reference confidence="0.999824327272727">
Michael Collins, 2002. Ranking Algorithm for Named-
Entity Extraction: Boosting and Voted Perceptron.
Proceedings of 40th Annual Meeting of Association
for Computational Linguistics.
Jianfeng Gao, Mu Li and Chang-Ning Huang, 2003.
Improved Source-Channel Models for Chinese Word
Segmentation. Proceedings of 41th Annual Meeting
of Association for Computational Linguistics.
Hideki Isozaki and Hideto Kazawa, 2002. Efficient
Support Vector Classifier for Named Entity Recogni-
tion, Proceedings of 19th International Conference
on Computational Linguistics.
Heng Ji, Zhensheng Luo, 2001. Inverse Name Fre-
quency Model and Rule Based Chinese Name Identi-
fication. (In Chinese) Natural Language
Understanding and Machine Translation. Tsinghua
University Press. pp. 123 -128.
Kaiying Liu, 2001. Research on Chinese Proper Noun
and Internet Words Recognition. (In Chinese) Pro-
ceedings of Conference of the 20th Anniversary of
CIPSC. Tsinghua University Press. pp. 7-13.
Song Rou, Benjamin K T’sou, 2001. Primary Study on
Chinese Proper Noun. (In Chinese) Proceedings of
Conference of the 20th Anniversary of CIPSC.
Tsinghua University Press,14-19.
Dayang Shen, Maosong Sun, 1995. Chinese Location
Name Recognition. (In Chinese) Development and
Applications of Computational Linguistics. Tsinghua
University Press.
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou, and
Changning Huang, 2002. Chinese named entity iden-
tification using class-based language model.
COLING 2002. Taipei, Taiwan, August 24-25, 2002.
Maosong Sun, Changning Huang, Haiyan Gao, Jie Fang,
1994. Chinese Person Name Recognition. (In Chi-
nese) Journal of Chinese Information Processing.
Xing Wang, Degen Huang, Yuansheng Yang, 1999.
Identifying Chinese Names Based on Combination of
Statistics and Rules. (In Chinese) proceedings of
JSCL-99. Tsinghua University Press. pp. 155 -161
Shihong Yu, Shuanhu Bai and Paul Wu. Description of
the Kent Ridge Digital Labs System Used for MUC-
7. Proceedings of the Seventh Message Understand-
ing Conference (MUC-7), 1998.
Junsheng Zhang, 1992. Chinese Person Name Recogni-
tion. (In Chinese) Journal of Chinese Information
Processing. 9(2) .
Jiahen Zheng, Xin Li, Hongye Tan, 2000. The Researh
of Chinese Names Recognition Based on Corpus,. (In
Chinese) Journal of Chinese Information Processing.
14(1): 7-12
Guodong Zhou and Jian Su, 2002. Named Entity Rec-
ognition using an HMM-based Chunk Tagger. Pro-
ceedings of 40th Annual Meeting of Association for
Computational Linguistics.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.574334">
<title confidence="0.999577">Single Character Chinese Named Entity Recognition</title>
<author confidence="0.630302">Xiaodan Zhu</author>
<author confidence="0.630302">Mu Li</author>
<author confidence="0.630302">Jianfeng Gao</author>
<author confidence="0.630302">Chang-Ning</author>
<affiliation confidence="0.987919">Microsoft Research,</affiliation>
<address confidence="0.971156">Beijing 100080,</address>
<email confidence="0.99968">t-muli@microsoft.com</email>
<email confidence="0.99968">jfgao@microsoft.com</email>
<email confidence="0.99968">cnhuang@microsoft.com</email>
<abstract confidence="0.997090375">Single character named entity (SCNE) is a name entity (NE) composed of one Chinese such as (zhong1, China) ” e2,Russia . is very common in written Chinese text. However, due to the lack of in-depth research, SCNE is a major source of errors in named entity recognition (NER). This paper formulates the SCNE recognition within the sourcechannel model framework. Our experiments show very encouraging results: an Fscore of 81.01% for single character location name recognition, and an F-score of 68.02% for single character person name recognition. An alternative view of the SCNE recognition problem is to formulate it as a classification task. We construct two classifiers based on maximum entropy model (ME) and vector space model (VSM), respectively. We compare all proposed approaches, showing that the sourcechannel model performs the best in most cases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<pages>22--1</pages>
<contexts>
<context position="14136" citStr="Berger et al, 1996" startWordPosition="2348" endWordPosition="2351"> extracted from training corpus. is the number of tokens are labeled as SCL in training corpus. n is the size of SCL list, which includes 177 SCL. Similarly, P(Sj is calculated by Eq. 4, an Sj |SCL(Sj)| Sj, which |NE_SCP) d the SCP list includes 151 SCP. (5) where i is a weight of the feature fi , and Z(x) is a normalization factor. Weights ( ) are estimated using the maximum entropy principle: to satisfy constraints on observed data and assume a uniform distribution (with the maximum entropy) on unseen data. The training algorithm we used is the improved iterative scaling (IIS) described in (Berger et al, 1996)3. The context features include six characters: three on the left of the SCNE, and three on the right. Given the context features, the ME classifier would estimate the probability of the candidate being a SCNE. In our example, we treat candidates with the probability larger than 0.5 as SCNEs. To get the precision-recall curve, we can vary the probability threshold from 0.1 to 0.9. 4.2 Vector Space Model VSM is another model we used to detect SCNE. Similar to ME, we use six surrounding characters as the features, as shown in Figure 2. Figure 2. Context window In this approach, we apply the stan</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra and V. Della Pietra, 1996. A maximum entropy approach to natural language processing. Computational Linguistics 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
<author>John Sterling</author>
<author>Eugene Agichtein</author>
<author>Ralph Grishman</author>
</authors>
<date>1998</date>
<booktitle>NYU: Description of the MENE Named Entity System as Used in MUC-7. Proceedings of the Seventh Message Understanding Conference (MUC-7)</booktitle>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>Andrew Borthwick, John Sterling, Eugene Agichtein, and Ralph Grishman. 1998. NYU: Description of the MENE Named Entity System as Used in MUC-7. Proceedings of the Seventh Message Understanding Conference (MUC-7)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-Hsi Chen</author>
<author>Yung-Wei Ding</author>
<author>Shih-Chung Tsai</author>
<author>Guo-Wei Bian</author>
</authors>
<date>1998</date>
<booktitle>Description of the NTU System used for MET-2, Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<marker>Chen, Ding, Tsai, Bian, 1998</marker>
<rawString>Hsin-Hsi Chen, Yung-Wei Ding, Shih-Chung Tsai and Guo-Wei Bian, 1998. Description of the NTU System used for MET-2, Proceedings of the Seventh Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking Algorithm for NamedEntity Extraction: Boosting and Voted Perceptron.</title>
<date>2002</date>
<booktitle>Proceedings of 40th Annual Meeting of Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1402" citStr="Collins, 2002" startWordPosition="211" endWordPosition="212">name recognition. An alternative view of the SCNE recognition problem is to formulate it as a classification task. We construct two classifiers based on maximum entropy model (ME) and vector space model (VSM), respectively. We compare all proposed approaches, showing that the sourcechannel model performs the best in most cases. 1 Introduction The research of named entity recognition (NER) becomes very popular in recent years due to its wide applications and the Message Understanding Conference (MUC) which provides a standard testbed for NER evaluation. Recent research on English NER includes (Collins, 2002; Isozaki, 2002; Zhou, 2002; etc.). Chinese NER research includes (Liu, 2001; Zheng, 2000; Yu, 1998; Chen, 1998; Shen, 1995; Sun, 1994; Zhang, 1992 etc.) In Chinese NEs, there is a special kind of NE, called single character named entity (SCNE), on which there is little in-depth research. SCNE is a NE composed of only one Chinese character, such as the location name “ ” (zhong1,China) and “ ” (e2,Russia) in the phrase “ ” (zhong1-e2-mao4-yi4, trade between China and Russia). SCNE is very common in written Chinese text. For instance, SCNE accounts for 8.17% of all NE tokens according to our sta</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins, 2002. Ranking Algorithm for NamedEntity Extraction: Boosting and Voted Perceptron. Proceedings of 40th Annual Meeting of Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Improved Source-Channel Models for Chinese Word Segmentation.</title>
<date>2003</date>
<booktitle>Proceedings of 41th Annual Meeting of Association for Computational Linguistics.</booktitle>
<marker>Gao, Li, Huang, 2003</marker>
<rawString>Jianfeng Gao, Mu Li and Chang-Ning Huang, 2003. Improved Source-Channel Models for Chinese Word Segmentation. Proceedings of 41th Annual Meeting of Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Efficient Support Vector Classifier for Named Entity Recognition,</title>
<date>2002</date>
<booktitle>Proceedings of 19th International Conference on Computational Linguistics.</booktitle>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>Hideki Isozaki and Hideto Kazawa, 2002. Efficient Support Vector Classifier for Named Entity Recognition, Proceedings of 19th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Zhensheng Luo</author>
</authors>
<title>Inverse Name Frequency Model and Rule Based Chinese Name Identification. (In Chinese) Natural Language Understanding and Machine Translation.</title>
<date>2001</date>
<pages>123--128</pages>
<publisher>Tsinghua University Press.</publisher>
<marker>Ji, Luo, 2001</marker>
<rawString>Heng Ji, Zhensheng Luo, 2001. Inverse Name Frequency Model and Rule Based Chinese Name Identification. (In Chinese) Natural Language Understanding and Machine Translation. Tsinghua University Press. pp. 123 -128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaiying Liu</author>
</authors>
<title>Research on Chinese Proper Noun and Internet Words Recognition. (In Chinese)</title>
<date>2001</date>
<booktitle>Proceedings of Conference of the 20th Anniversary of CIPSC.</booktitle>
<pages>7--13</pages>
<publisher>Tsinghua University Press.</publisher>
<contexts>
<context position="1478" citStr="Liu, 2001" startWordPosition="222" endWordPosition="223">ulate it as a classification task. We construct two classifiers based on maximum entropy model (ME) and vector space model (VSM), respectively. We compare all proposed approaches, showing that the sourcechannel model performs the best in most cases. 1 Introduction The research of named entity recognition (NER) becomes very popular in recent years due to its wide applications and the Message Understanding Conference (MUC) which provides a standard testbed for NER evaluation. Recent research on English NER includes (Collins, 2002; Isozaki, 2002; Zhou, 2002; etc.). Chinese NER research includes (Liu, 2001; Zheng, 2000; Yu, 1998; Chen, 1998; Shen, 1995; Sun, 1994; Zhang, 1992 etc.) In Chinese NEs, there is a special kind of NE, called single character named entity (SCNE), on which there is little in-depth research. SCNE is a NE composed of only one Chinese character, such as the location name “ ” (zhong1,China) and “ ” (e2,Russia) in the phrase “ ” (zhong1-e2-mao4-yi4, trade between China and Russia). SCNE is very common in written Chinese text. For instance, SCNE accounts for 8.17% of all NE tokens according to our statistics on a 10MB corpus. However, due to the lack of research, SCNE is a ma</context>
<context position="4424" citStr="Liu, 2001" startWordPosition="711" endWordPosition="712">t. As shown in Table 1, SCNE accounts for 8.17% of all NE tokens on the 10MB corpus. Especially, 14.65% of location names are SCLs. However, due to the lack of research, SCNE is a major source of errors in NER. In our experiments described below, we focus on SCL and SCP, while SCO is not considered because of its small number in the data. # SCNE # NE #SCNE / #NE PN 5,892 129,317 4.56% LN 32,483 221,713 14.65% ON 356 122,779 0.29% Total 38,731 473,809 8.17% Table 1. Proportion of SCNE in NE To our knowledge, most NER systems do not report SCNE recognition results separately. Some systems (e.g. Liu, 2001) even do not include SCNE in recognition task. SCNE recognition is achieved using the same technologies as for NER, which can be roughly classified into rule-based methods and statistical-based methods, while most of state-of-the-art systems use hybrid approaches. Wang (1999) and Chen (1998) used linguistic rules to detect NE with the help of the statistics from dictionary. Ji(2001), Zheng (2000), Shen(1995) and Sun(1994) used statistics from dictionaries and large corpus to generate PN or LN candidates, and used linguistic rules to filter the result, and Yu (1998) used language model to filte</context>
</contexts>
<marker>Liu, 2001</marker>
<rawString>Kaiying Liu, 2001. Research on Chinese Proper Noun and Internet Words Recognition. (In Chinese) Proceedings of Conference of the 20th Anniversary of CIPSC. Tsinghua University Press. pp. 7-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Rou</author>
<author>Benjamin K T’sou</author>
</authors>
<title>Primary Study on Chinese Proper Noun. (In Chinese)</title>
<date>2001</date>
<booktitle>Proceedings of Conference of the 20th Anniversary of CIPSC.</booktitle>
<publisher>Tsinghua University Press,14-19.</publisher>
<marker>Rou, T’sou, 2001</marker>
<rawString>Song Rou, Benjamin K T’sou, 2001. Primary Study on Chinese Proper Noun. (In Chinese) Proceedings of Conference of the 20th Anniversary of CIPSC. Tsinghua University Press,14-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayang Shen</author>
</authors>
<title>Maosong Sun,</title>
<date>1995</date>
<booktitle>(In Chinese) Development and Applications of Computational Linguistics.</booktitle>
<publisher>Tsinghua University Press.</publisher>
<contexts>
<context position="1525" citStr="Shen, 1995" startWordPosition="230" endWordPosition="231">t two classifiers based on maximum entropy model (ME) and vector space model (VSM), respectively. We compare all proposed approaches, showing that the sourcechannel model performs the best in most cases. 1 Introduction The research of named entity recognition (NER) becomes very popular in recent years due to its wide applications and the Message Understanding Conference (MUC) which provides a standard testbed for NER evaluation. Recent research on English NER includes (Collins, 2002; Isozaki, 2002; Zhou, 2002; etc.). Chinese NER research includes (Liu, 2001; Zheng, 2000; Yu, 1998; Chen, 1998; Shen, 1995; Sun, 1994; Zhang, 1992 etc.) In Chinese NEs, there is a special kind of NE, called single character named entity (SCNE), on which there is little in-depth research. SCNE is a NE composed of only one Chinese character, such as the location name “ ” (zhong1,China) and “ ” (e2,Russia) in the phrase “ ” (zhong1-e2-mao4-yi4, trade between China and Russia). SCNE is very common in written Chinese text. For instance, SCNE accounts for 8.17% of all NE tokens according to our statistics on a 10MB corpus. However, due to the lack of research, SCNE is a major source of errors in NER. Among three state-</context>
</contexts>
<marker>Shen, 1995</marker>
<rawString>Dayang Shen, Maosong Sun, 1995. Chinese Location Name Recognition. (In Chinese) Development and Applications of Computational Linguistics. Tsinghua University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Sun</author>
<author>Jianfeng Gao</author>
<author>Lei Zhang</author>
<author>Ming Zhou</author>
<author>Changning Huang</author>
</authors>
<title>Chinese named entity identification using class-based language model. COLING</title>
<date>2002</date>
<location>Taipei, Taiwan,</location>
<marker>Sun, Gao, Zhang, Zhou, Huang, 2002</marker>
<rawString>Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou, and Changning Huang, 2002. Chinese named entity identification using class-based language model. COLING 2002. Taipei, Taiwan, August 24-25, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
</authors>
<title>Changning Huang, Haiyan Gao, Jie Fang,</title>
<date>1994</date>
<journal>(In Chinese) Journal of Chinese Information Processing.</journal>
<contexts>
<context position="1536" citStr="Sun, 1994" startWordPosition="232" endWordPosition="233">fiers based on maximum entropy model (ME) and vector space model (VSM), respectively. We compare all proposed approaches, showing that the sourcechannel model performs the best in most cases. 1 Introduction The research of named entity recognition (NER) becomes very popular in recent years due to its wide applications and the Message Understanding Conference (MUC) which provides a standard testbed for NER evaluation. Recent research on English NER includes (Collins, 2002; Isozaki, 2002; Zhou, 2002; etc.). Chinese NER research includes (Liu, 2001; Zheng, 2000; Yu, 1998; Chen, 1998; Shen, 1995; Sun, 1994; Zhang, 1992 etc.) In Chinese NEs, there is a special kind of NE, called single character named entity (SCNE), on which there is little in-depth research. SCNE is a NE composed of only one Chinese character, such as the location name “ ” (zhong1,China) and “ ” (e2,Russia) in the phrase “ ” (zhong1-e2-mao4-yi4, trade between China and Russia). SCNE is very common in written Chinese text. For instance, SCNE accounts for 8.17% of all NE tokens according to our statistics on a 10MB corpus. However, due to the lack of research, SCNE is a major source of errors in NER. Among three state-of-the-art </context>
</contexts>
<marker>Sun, 1994</marker>
<rawString>Maosong Sun, Changning Huang, Haiyan Gao, Jie Fang, 1994. Chinese Person Name Recognition. (In Chinese) Journal of Chinese Information Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Wang</author>
<author>Degen Huang</author>
<author>Yuansheng Yang</author>
</authors>
<title>Identifying Chinese Names Based on Combination of Statistics and Rules.</title>
<date>1999</date>
<booktitle>(In Chinese) proceedings of JSCL-99. Tsinghua</booktitle>
<pages>155--161</pages>
<publisher>University Press.</publisher>
<marker>Wang, Huang, Yang, 1999</marker>
<rawString>Xing Wang, Degen Huang, Yuansheng Yang, 1999. Identifying Chinese Names Based on Combination of Statistics and Rules. (In Chinese) proceedings of JSCL-99. Tsinghua University Press. pp. 155 -161</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shihong Yu</author>
<author>Shuanhu Bai</author>
<author>Paul Wu</author>
</authors>
<title>Description of the Kent Ridge Digital Labs System Used for MUC7.</title>
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference (MUC-7),</booktitle>
<marker>Yu, Bai, Wu, 1998</marker>
<rawString>Shihong Yu, Shuanhu Bai and Paul Wu. Description of the Kent Ridge Digital Labs System Used for MUC7. Proceedings of the Seventh Message Understanding Conference (MUC-7), 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junsheng Zhang</author>
</authors>
<title>Chinese Person Name Recognition.</title>
<date>1992</date>
<journal>(In Chinese) Journal of Chinese Information Processing.</journal>
<volume>9</volume>
<issue>2</issue>
<pages>.</pages>
<contexts>
<context position="1549" citStr="Zhang, 1992" startWordPosition="234" endWordPosition="235"> on maximum entropy model (ME) and vector space model (VSM), respectively. We compare all proposed approaches, showing that the sourcechannel model performs the best in most cases. 1 Introduction The research of named entity recognition (NER) becomes very popular in recent years due to its wide applications and the Message Understanding Conference (MUC) which provides a standard testbed for NER evaluation. Recent research on English NER includes (Collins, 2002; Isozaki, 2002; Zhou, 2002; etc.). Chinese NER research includes (Liu, 2001; Zheng, 2000; Yu, 1998; Chen, 1998; Shen, 1995; Sun, 1994; Zhang, 1992 etc.) In Chinese NEs, there is a special kind of NE, called single character named entity (SCNE), on which there is little in-depth research. SCNE is a NE composed of only one Chinese character, such as the location name “ ” (zhong1,China) and “ ” (e2,Russia) in the phrase “ ” (zhong1-e2-mao4-yi4, trade between China and Russia). SCNE is very common in written Chinese text. For instance, SCNE accounts for 8.17% of all NE tokens according to our statistics on a 10MB corpus. However, due to the lack of research, SCNE is a major source of errors in NER. Among three state-of-the-art systems we ha</context>
</contexts>
<marker>Zhang, 1992</marker>
<rawString>Junsheng Zhang, 1992. Chinese Person Name Recognition. (In Chinese) Journal of Chinese Information Processing. 9(2) .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiahen Zheng</author>
<author>Xin Li</author>
<author>Hongye Tan</author>
</authors>
<title>The Researh of Chinese Names Recognition Based on Corpus,.</title>
<date>2000</date>
<journal>(In Chinese) Journal of Chinese Information Processing.</journal>
<volume>14</volume>
<issue>1</issue>
<pages>7--12</pages>
<marker>Zheng, Li, Tan, 2000</marker>
<rawString>Jiahen Zheng, Xin Li, Hongye Tan, 2000. The Researh of Chinese Names Recognition Based on Corpus,. (In Chinese) Journal of Chinese Information Processing. 14(1): 7-12</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
</authors>
<title>Named Entity Recognition using an HMM-based Chunk Tagger.</title>
<date>2002</date>
<booktitle>Proceedings of 40th Annual Meeting of Association for Computational Linguistics.</booktitle>
<marker>Zhou, Su, 2002</marker>
<rawString>Guodong Zhou and Jian Su, 2002. Named Entity Recognition using an HMM-based Chunk Tagger. Proceedings of 40th Annual Meeting of Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>