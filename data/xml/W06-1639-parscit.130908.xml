<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000929">
<title confidence="0.9922495">
Get out the vote: Determining support or opposition from Congressional
floor-debate transcripts
</title>
<author confidence="0.999416">
Matt Thomas, Bo Pang, and Lillian Lee
</author>
<affiliation confidence="0.999533">
Department of Computer Science, Cornell University
</affiliation>
<address confidence="0.577275">
Ithaca, NY 14853-7501
</address>
<email confidence="0.997107">
mattthomas84@gmail.com, pabo@cs.cornell.edu, llee@cs.cornell.edu
</email>
<sectionHeader confidence="0.99384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997744">
We investigate whether one can determine
from the transcripts of U.S. Congressional
floor debates whether the speeches repre-
sent support of or opposition to proposed
legislation. To address this problem, we
exploit the fact that these speeches occur
as part of a discussion; this allows us to
use sources of information regarding re-
lationships between discourse segments,
such as whether a given utterance indicates
agreement with the opinion expressed by
another. We find that the incorporation
of such information yields substantial im-
provements over classifying speeches in
isolation.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999821733333333">
One ought to recognize that the present
political chaos is connected with the de-
cay of language, and that one can prob-
ably bring about some improvement by
starting at the verbal end. — Orwell,
“Politics and the English language”
We have entered an era where very large
amounts of politically oriented text are now avail-
able online. This includes both official documents,
such as the full text of laws and the proceedings of
legislative bodies, and unofficial documents, such
as postings on weblogs (blogs) devoted to politics.
In some sense, the availability of such data is sim-
ply a manifestation of a general trend of “every-
body putting their records on the Internet”.1 The
</bodyText>
<footnote confidence="0.988280666666667">
1It is worth pointing out that the United States’ Library of
Congress was an extremely early adopter of Web technology:
the THOMAS database (http://thomas.loc.gov) of congres-
</footnote>
<bodyText confidence="0.9731002">
online accessibility of politically oriented texts in
particular, however, is a phenomenon that some
have gone so far as to say will have a potentially
society-changing effect.
In the United States, for example, governmen-
tal bodies are providing and soliciting political
documents via the Internet, with lofty goals in
mind: electronic rulemaking (eRulemaking) ini-
tiatives involving the “electronic collection, dis-
tribution, synthesis, and analysis of public com-
mentary in the regulatory rulemaking process”,
may “[alter] the citizen-government relationship”
(Shulman and Schlosberg, 2002). Additionally,
much media attention has been focused recently
on the potential impact that Internet sites may have
on politics2, or at least on political journalism3.
Regardless of whether one views such claims as
clear-sighted prophecy or mere hype, it is obvi-
ously important to help people understand and an-
alyze politically oriented text, given the impor-
tance of enabling informed participation in the po-
litical process.
Evaluative and persuasive documents, such as
a politician’s speech regarding a bill or a blog-
ger’s commentary on a legislative proposal, form a
particularly interesting type of politically oriented
text. People are much more likely to consult such
evaluative statements than the actual text of a bill
or law under discussion, given the dense nature of
legislative language and the fact that (U.S.) bills
often reach several hundred pages in length (Smith
et al., 2005). Moreover, political opinions are ex-
sional bills and related data was launched in January 1995,
when Mosaic was not quite two years old and Altavista did
not yet exist.
</bodyText>
<affiliation confidence="0.74128175">
2E.g., “Internet injects sweeping change into U.S. poli-
tics”, Adam Nagourney, The New York Times, April 2, 2006.
3E.g., “The End of News?”, Michael Massing, The New
York Review of Books, December 1, 2005.
</affiliation>
<page confidence="0.983509">
327
</page>
<note confidence="0.858798">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 327–335,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998588115384615">
plicitly solicited in the eRulemaking scenario.
In the analysis of evaluative language, it is fun-
damentally necessary to determine whether the au-
thor/speaker supports or disapproves of the topic
of discussion. In this paper, we investigate the
following specific instantiation of this problem:
we seek to determine from the transcripts of
U.S. Congressional floor debates whether each
“speech” (continuous single-speaker segment of
text) represents support for or opposition to a pro-
posed piece of legislation. Note that from an ex-
perimental point of view, this is a very convenient
problem to work with because we can automati-
cally determine ground truth (and thus avoid the
need for manual annotation) simply by consulting
publicly available voting records.
Task properties Determining whether or not a
speaker supports a proposal falls within the realm
of sentiment analysis, an extremely active re-
search area devoted to the computational treatment
of subjective or opinion-oriented language (early
work includes Wiebe and Rapaport (1988), Hearst
(1992), Sack (1994), and Wiebe (1994); see Esuli
(2006) for an active bibliography). In particu-
lar, since we treat each individual speech within
a debate as a single “document”, we are consider-
ing a version of document-level sentiment-polarity
classification, namely, automatically distinguish-
ing between positive and negative documents (Das
and Chen, 2001; Pang et al., 2002; Turney, 2002;
Dave et al., 2003).
Most sentiment-polarity classifiers proposed in
the recent literature categorize each document in-
dependently. A few others incorporate various
measures of inter-document similarity between the
texts to be labeled (Agarwal and Bhattacharyya,
2005; Pang and Lee, 2005; Goldberg and Zhu,
2006). Many interesting opinion-oriented docu-
ments, however, can be linked through certain re-
lationships that occur in the context of evaluative
discussions. For example, we may find textual4
evidence of a high likelihood of agreement be-
4Because we are most interested in techniques applicable
across domains, we restrict consideration to NLP aspects of
the problem, ignoring external problem-specific information.
For example, although most votes in our corpus were almost
completely along party lines (and despite the fact that same-
party information is easily incorporated via the methods we
propose), we did not use party-affiliation data. Indeed, in
other settings (e.g., a movie-discussion listserv) one may not
be able to determine the participants’ political leanings, and
such information may not lead to significantly improved re-
sults even if it were available.
tween two speakers, such as explicit assertions (“I
second that!”) or quotation of messages in emails
or postings (see Mullen and Malouf (2006) but cf.
Agrawal et al. (2003)). Agreement evidence can
be a powerful aid in our classification task: for ex-
ample, we can easily categorize a complicated (or
overly terse) document if we find within it indica-
tions of agreement with a clearly positive text.
Obviously, incorporating agreement informa-
tion provides additional benefit only when the in-
put documents are relatively difficult to classify
individually. Intuition suggests that this is true
of the data with which we experiment, for several
reasons. First, U.S. congressional debates contain
very rich language and cover an extremely wide
variety of topics, ranging from flag burning to in-
ternational policy to the federal budget. Debates
are also subject to digressions, some fairly natural
and others less so (e.g., “Why are we discussing
this bill when the plight of my constituents regard-
ing this other issue is being ignored?”)
Second, an important characteristic of persua-
sive language is that speakers may spend more
time presenting evidence in support of their po-
sitions (or attacking the evidence presented by
others) than directly stating their attitudes. An
extreme example will illustrate the problems in-
volved. Consider a speech that describes the U.S.
flag as deeply inspirational, and thus contains only
positive language. If the bill under discussion is a
proposed flag-burning ban, then the speech is sup-
portive; but if the bill under discussion is aimed at
rescinding an existing flag-burning ban, the speech
may represent opposition to the legislation. Given
the current state of the art in sentiment analysis,
it is doubtful that one could determine the (proba-
bly topic-specific) relationship between presented
evidence and speaker opinion.
Qualitative summary of results The above dif-
ficulties underscore the importance of enhancing
standard classification techniques with new infor-
mation sources that promise to improve accuracy,
such as inter-document relationships between the
documents to be labeled. In this paper, we demon-
strate that the incorporation of agreement model-
ing can provide substantial improvements over the
application of support vector machines (SVMs) in
isolation, which represents the state of the art in
the individual classification of documents. The en-
hanced accuracies are obtained via a fairly primi-
tive automatically-acquired “agreement detector”
</bodyText>
<page confidence="0.997713">
328
</page>
<table confidence="0.9898344">
total train test development
speech segments 3857 2740 860 257
debates 53 38 10 5
average number of speech segments per debate 72.8 72.1 86.0 51.4
average number of speakers per debate 32.1 30.9 41.1 22.6
</table>
<tableCaption confidence="0.999935">
Table 1: Corpus statistics.
</tableCaption>
<bodyText confidence="0.99974">
and a conceptually simple method for integrat-
ing isolated-document and agreement-based in-
formation. We thus view our results as demon-
strating the potentially large benefits of exploiting
sentiment-related discourse-segment relationships
in sentiment-analysis tasks.
</bodyText>
<sectionHeader confidence="0.988208" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999966087719299">
This section outlines the main steps of the process
by which we created our corpus (download site:
www.cs.cornell.edu/home/llee/data/convote.html).
GovTrack (http://govtrack.us) is an independent
website run by Joshua Tauberer that collects pub-
licly available data on the legislative and fund-
raising activities of U.S. congresspeople. Due to
its extensive cross-referencing and collating of in-
formation, it was nominated for a 2006 “Webby”
award. A crucial characteristic of GovTrack from
our point of view is that the information is pro-
vided in a very convenient format; for instance,
the floor-debate transcripts are broken into sepa-
rate HTML files according to the subject of the
debate, so we can trivially derive long sequences
of speeches guaranteed to cover the same topic.
We extracted from GovTrack all available tran-
scripts of U.S. floor debates in the House of Rep-
resentatives for the year 2005 (3268 pages of tran-
scripts in total), together with voting records for all
roll-call votes during that year. We concentrated
on debates regarding “controversial” bills (ones in
which the losing side generated at least 20% of the
speeches) because these debates should presum-
ably exhibit more interesting discourse structure.
Each debate consists of a series of speech seg-
ments, where each segment is a sequence of un-
interrupted utterances by a single speaker. Since
speech segments represent natural discourse units,
we treat them as the basic unit to be classified.
Each speech segment was labeled by the vote
(“yea” or “nay”) cast for the proposed bill by the
person who uttered the speech segment.
We automatically discarded those speech seg-
ments belonging to a class of formulaic, generally
one-sentence utterances focused on the yielding
of time on the house floor (for example, “Madam
Speaker, I am pleased to yield 5 minutes to the
gentleman from Massachusetts”), as such speech
segments are clearly off-topic. We also removed
speech segments containing the term “amend-
ment”, since we found during initial inspection
that these speeches generally reflect a speaker’s
opinion on an amendment, and this opinion may
differ from the speaker’s opinion on the underly-
ing bill under discussion.
We randomly split the data into training, test,
and development (parameter-tuning) sets repre-
senting roughly 70%, 20%, and 10% of our data,
respectively (see Table 1). The speech segments
remained grouped by debate, with 38 debates as-
signed to the training set, 10 to the test set, and 5
to the development set; we require that the speech
segments from an individual debate all appear in
the same set because our goal is to examine clas-
sification of speech segments in the context of the
surrounding discussion.
</bodyText>
<sectionHeader confidence="0.984755" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999995944444444">
The support/oppose classification problem can be
approached through the use of standard classifiers
such as support vector machines (SVMs), which
consider each text unit in isolation. As discussed
in Section 1, however, the conversational nature
of our data implies the existence of various rela-
tionships that can be exploited to improve cumu-
lative classification accuracy for speech segments
belonging to the same debate. Our classification
framework, directly inspired by Blum and Chawla
(2001), integrates both perspectives, optimizing
its labeling of speech segments based on both in-
dividual speech-segment classification scores and
preferences for groups of speech segments to re-
ceive the same label. In this section, we discuss
the specific classification framework that we adopt
and the set of mechanisms that we propose for
modeling specific types of relationships.
</bodyText>
<page confidence="0.994814">
329
</page>
<subsectionHeader confidence="0.990098">
3.1 Classification framework
</subsectionHeader>
<bodyText confidence="0.995996428571428">
Let s1, s2, ... , sn be the sequence of speech seg-
ments within a given debate, and let Y and
N stand for the “yea” and “nay” class, respec-
tively. Assume we have a non-negative func-
tion ind(s, C) indicating the degree of preference
that an individual-document classifier, such as an
SVM, has for placing speech-segment s in class
C. Also, assume that some pairs of speech seg-
ments have weighted links between them, where
the non-negative strength (weight) str(`) for a
link ` indicates the degree to which it is prefer-
able that the linked speech segments receive the
same label. Then, any class assignment c =
c(s1), c(s2), ... , c(sn) can be assigned a cost
</bodyText>
<equation confidence="0.9984005">
� ind(s, c(s))+ � � str(`),
s s,s&apos;: c(s)6=c(s&apos;) t betweens,s&apos;
</equation>
<bodyText confidence="0.999974631578948">
where c(s) is the “opposite” class from c(s). A
minimum-cost assignment thus represents an opti-
mum way to classify the speech segments so that
each one tends not to be put into the class that
the individual-document classifier disprefers, but
at the same time, highly associated speech seg-
ments tend not to be put in different classes.
As has been previously observed and exploited
in the NLP literature (Pang and Lee, 2004; Agar-
wal and Bhattacharyya, 2005; Barzilay and Lap-
ata, 2005), the above optimization function, unlike
many others that have been proposed for graph or
set partitioning, can be solved exactly in an prov-
ably efficient manner via methods for finding min-
imum cuts in graphs. In our view, the contribution
of our work is the examination of new types of
relationships, not the method by which such re-
lationships are incorporated into the classification
decision.
</bodyText>
<subsectionHeader confidence="0.999967">
3.2 Classifying speech segments in isolation
</subsectionHeader>
<bodyText confidence="0.842973733333333">
In our experiments, we employed the well-known
classifier SVM&amp;quot;ght to obtain individual-document
classification scores, treating Y as the positive
class and using plain unigrams as features.5 Fol-
lowing standard practice in sentiment analysis
(Pang et al., 2002), the input to SVM&amp;quot;ght con-
sisted of normalized presence-of-feature (rather
than frequency-of-feature) vectors. The ind value
5SVMlight is available at svmlight.joachims.org. Default
parameters were used, although experimentation with differ-
ent parameter settings is an important direction for future
work (Daelemans and Hoste, 2002; Munson et al., 2005).
for each speech segment s was based on the signed
distance d(s) from the vector representing s to the
trained SVM decision plane:
</bodyText>
<equation confidence="0.999362666666667">
ind(s, Y) def = { 1 d(s) &gt; 2σs7
0 d(s) &lt; −2σs
(1 + 2a3,) /2  |d(s) |≤ 2σs7
</equation>
<bodyText confidence="0.965229666666667">
where σs is the standard deviation of d(s) over all
speech segments s in the debate in question, and
ind(s, N) def = 1 − ind(s, Y).
We now turn to the more interesting problem of
representing the preferences that speech segments
may have for being assigned to the same class.
</bodyText>
<subsectionHeader confidence="0.996866">
3.3 Relationships between speech segments
</subsectionHeader>
<bodyText confidence="0.999531066666667">
A wide range of relationships between text seg-
ments can be modeled as positive-strength links.
Here we discuss two types of constraints that are
considered in this work.
Same-speaker constraints: In Congressional
debates and in general social-discourse contexts,
a single speaker may make a number of comments
regarding a topic. It is reasonable to expect that in
many settings, the participants in a discussion may
be convinced to change their opinions midway
through a debate. Hence, in the general case we
wish to be able to express “soft” preferences for all
of an author’s statements to receive the same label,
where the strengths of such constraints could, for
instance, vary according to the time elapsed be-
tween the statements. Weighted links are an ap-
propriate means to express such variation.
However, if we assume that most speakers do
not change their positions in the course of a dis-
cussion, we can conclude that all comments made
by the same speaker must receive the same label.
This assumption holds by fiat for the ground-truth
labels in our dataset because these labels were
derived from the single vote cast by the speaker
on the bill being discussed.6 We can implement
this assumption via links whose weights are essen-
tially infinite. Although one can also implement
this assumption via concatenation of same-speaker
speech segments (see Section 4.3), we view the
fact that our graph-based framework incorporates
</bodyText>
<footnote confidence="0.6401375">
6We are attempting to determine whether a speech seg-
ment represents support or not. This differs from the problem
of determining what the speaker’s actual opinion is, a prob-
lem that, as an anonymous reviewer put it, is complicated by
“grandstanding, backroom deals, or, more innocently, plain
change of mind (‘I voted for it before I voted against it’)”.
</footnote>
<page confidence="0.993102">
330
</page>
<bodyText confidence="0.963701384615385">
both hard and soft constraints in a principled fash-
ion as an advantage of our approach.
Different-speaker agreements In House dis-
course, it is common for one speaker to make ref-
erence to another in the context of an agreement
or disagreement over the topic of discussion. The
systematic identification of instances of agreement
can, as we have discussed, be a powerful tool for
the development of intelligently selected weights
for links between speech segments.
The problem of agreement identification can be
decomposed into two sub-problems: identifying
references and their targets, and deciding whether
each reference represents an instance of agree-
ment. In our case, the first task is straightfor-
ward because we focused solely on by-name ref-
erences.7 Hence, we will now concentrate on the
second, more interesting task.
We approach the problem of classifying refer-
ences by representing each reference with a word-
presence vector derived from a window of text
surrounding the reference.8 In the training set,
we classify each reference connecting two speak-
ers with a positive or negative label depending on
whether the two voted the same way on the bill un-
der discussion9. These labels are then used to train
an SVM classifier, the output of which is subse-
quently used to create weights on agreement links
in the test set as follows.
Let d(r) denote the distance from the vector
representing reference r to the agreement-detector
SVM’s decision plane, and let u, be the standard
deviation of d(r) over all references in the debate
in question. We then define the strength agr of the
agreement link corresponding to the reference as:
agr(r) def = { 0 d(r) &lt; Bagr;
α · d(r)/4u, Bagr &lt; d(r) &lt; 4u,;
α d(r) &gt; 4u,
The free parameter α specifies the relative impor-
</bodyText>
<footnote confidence="0.979370846153846">
7One subtlety is that for the purposes of mining agree-
ment cues (but not for evaluating overall support/oppose
classification accuracy), we temporarily re-inserted into our
dataset previously filtered speech segments containing the
term “yield”, since the yielding of time on the House floor
typically indicates agreement even though the yield state-
ments contain little relevant text on their own.
8We found good development-set performance using the
30 tokens before, 20 tokens after, and the name itself.
9Since we are concerned with references that potentially
represent relationships between speech segments, we ignore
references for which the target of the reference did not speak
in the debate in which the reference was made.
</footnote>
<table confidence="0.9996182">
Agreement classifier Devel. Test
(“reference agreement?”) set set
majority baseline 81.51 80.26
Train: no amdmts; Bagr = 0 84.25 81.07
Train: with amdmts; Bagr = 0 86.99 80.10
</table>
<tableCaption confidence="0.96423225">
Table 2: Agreement-classifier accuracy, in per-
cent. “Amdmts”=“speech segments containing the
word ‘amendment’”. Recall that boldface indi-
cates results for development-set-optimal settings.
</tableCaption>
<bodyText confidence="0.9195296">
tance of the agr scores. The threshold Bagr con-
trols the precision of the agreement links, in that
values of Bagr greater than zero mean that greater
confidence is required before an agreement link
can be added.10
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999986111111111">
This section presents experiments testing the util-
ity of using speech-segment relationships, evalu-
ating against a number of baselines. All reported
results use values for the free parameter α derived
via tuning on the development set. In the tables,
boldface indicates the development- and test-set
results for the development-set-optimal parameter
settings, as one would make algorithmic choices
based on development-set performance.
</bodyText>
<subsectionHeader confidence="0.989132">
4.1 Preliminaries: Reference classification
</subsectionHeader>
<bodyText confidence="0.921811434782609">
Recall that to gather inter-speaker agreement in-
formation, the strategy employed in this paper is
to classify by-name references to other speakers
as to whether they indicate agreement or not.
To train our agreement classifier, we experi-
mented with undoing the deletion of amendment-
related speech segments in the training set. Note
that such speech segments were never included in
the development or test set, since, as discussed in
Section 2, their labels are probably noisy; how-
ever, including them in the training set allows the
classifier to examine more instances even though
some of them are labeled incorrectly. As Table
2 shows, using more, if noisy, data yields bet-
ter agreement-classification results on the devel-
opment set, and so we use that policy in all subse-
quent experiments.11
10Our implementation puts a link between just one arbi-
trary pair of speech segments among all those uttered by a
given pair of apparently agreeing speakers. The “infinite-
weight” same-speaker links propagate the agreement infor-
mation to all other such pairs.
11Unfortunately, this policy leads to inferior test-set agree-
</bodyText>
<page confidence="0.99487">
331
</page>
<table confidence="0.9952715">
Agreement classifier Precision (in percent):
Devel. set Test set
Bagr = 0 86.23 82.55
Bagr = µ 89.41 88.47
</table>
<tableCaption confidence="0.999786">
Table 3: Agreement-classifier precision.
</tableCaption>
<bodyText confidence="0.9996398">
An important observation is that precision may
be more important than accuracy in deciding
which agreement links to add: false positives with
respect to agreement can cause speech segments
to be incorrectly assigned the same label, whereas
false negatives mean only that agreement-based
information about other speech segments is not
employed. As described above, we can raise
agreement precision by increasing the threshold
Bagr, which specifies the required confidence for
the addition of an agreement link. Indeed, Table
3 shows that we can improve agreement precision
by setting Bagr to the (positive) mean agreement
score µ assigned by the SVM agreement-classifier
over all references in the given debate12. How-
ever, this comes at the cost of greatly reducing
agreement accuracy (development: 64.38%; test:
66.18%) due to lowered recall levels. Whether
or not better speech-segment classification is ulti-
mately achieved is discussed in the next sections.
</bodyText>
<subsectionHeader confidence="0.6109665">
4.2 Segment-based speech-segment
classification
</subsectionHeader>
<bodyText confidence="0.997690555555556">
Baselines The first two data rows of Table
4 depict baseline performance results. The
#(“support”) − #(“oppos”) baseline is meant
to explore whether the speech-segment classifica-
tion task can be reduced to simple lexical checks.
Specifically, this method uses the signed differ-
ence between the number of words containing the
stem “support” and the number of words contain-
ing the stem “oppos” (returning the majority class
if the difference is 0). No better than 62.67% test-
set accuracy is obtained by either baseline.
Using relationship information Applying an
SVM to classify each speech segment in isolation
leads to clear improvements over the two base-
line methods, as demonstrated in Table 4. When
we impose the constraint that all speech segments
uttered by the same speaker receive the same la-
bel via “same-speaker links”, both test-set and
</bodyText>
<footnote confidence="0.74346">
ment classification. Section 4.5 contains further discussion.
12We elected not to explicitly tune the value of Bagr in or-
der to minimize the number of free parameters to deal with.
</footnote>
<table confidence="0.998896">
Support/oppose classifer Devel. Test
(“speech segment yea?”) set set
majority baseline 54.09 58.37
#(“support”) − #(“oppos”) 59.14 62.67
SVM [speech segment] 70.04 66.05
SVM + same-speaker links 79.77 67.21
SVM + same-speaker links ... 89.11 70.81
+ agreement links, Bagr = 0
+ agreement links, Bagr = µ 87.94 71.16
</table>
<tableCaption confidence="0.8375065">
Table 4: Segment-based speech-segment classifi-
cation accuracy, in percent.
</tableCaption>
<table confidence="0.999751833333333">
Support/oppose classifer Devel. Test
(“speech segment yea?”) set set
SVM [speaker] 71.60 70.00
SVM + agreement links ... 88.72 71.28
with Bagr = 0
with Bagr = µ 84.44 76.05
</table>
<tableCaption confidence="0.997626">
Table 5: Speaker-based speech-segment classifica-
</tableCaption>
<bodyText confidence="0.996167">
tion accuracy, in percent. Here, the initial SVM is
run on the concatenation of all of a given speaker’s
speech segments, but the results are computed
over speech segments (not speakers), so that they
can be compared to those in Table 4.
development-set accuracy increase even more, in
the latter case quite substantially so.
The last two lines of Table 4 show that the
best results are obtained by incorporating agree-
ment information as well. The highest test-set re-
sult, 71.16%, is obtained by using a high-precision
threshold to determine which agreement links to
add. While the development-set results would in-
duce us to utilize the standard threshold value of 0,
which is sub-optimal on the test set, the Bagr = 0
agreement-link policy still achieves noticeable im-
provement over not using agreement links (test set:
70.81% vs. 67.21%).
</bodyText>
<subsectionHeader confidence="0.820637">
4.3 Speaker-based speech-segment
classification
</subsectionHeader>
<bodyText confidence="0.99970025">
We use speech segments as the unit of classifica-
tion because they represent natural discourse units.
As a consequence, we are able to exploit relation-
ships at the speech-segment level. However, it is
interesting to consider whether we really need to
consider relationships specifically between speech
segments themselves, or whether it suffices to sim-
ply consider relationships between the speakers
</bodyText>
<page confidence="0.995701">
332
</page>
<bodyText confidence="0.999964541666667">
of the speech segments. In particular, as an al-
ternative to using same-speaker links, we tried a
speaker-based approach wherein the way we de-
termine the initial individual-document classifica-
tion score for each speech segment uttered by a
person p in a given debate is to run an SVM on the
concatenation of all of p’s speech segments within
that debate. (We also ensure that agreement-link
information is propagated from speech-segment to
speaker pairs.)
How does the use of same-speaker links com-
pare to the concatenation of each speaker’s speech
segments? Tables 4 and 5 show that, not sur-
prisingly, the SVM individual-document classifier
works better on the concatenated speech segments
than on the speech segments in isolation. How-
ever, the effect on overall classification accuracy
is less clear: the development set favors same-
speaker links over concatenation, while the test set
does not.
But we stress that the most important obser-
vation we can make from Table 5 is that once
again, the addition of agreement information leads
to substantial improvements in accuracy.
</bodyText>
<subsectionHeader confidence="0.999311">
4.4 “Hard” agreement constraints
</subsectionHeader>
<bodyText confidence="0.999977076923077">
Recall that in in our experiments, we created
finite-weight agreement links, so that speech seg-
ments appearing in pairs flagged by our (imper-
fect) agreement detector can potentially receive
different labels. We also experimented with forc-
ing such speech segments to receive the same la-
bel, either through infinite-weight agreement links
or through a speech-segment concatenation strat-
egy similar to that described in the previous sub-
section. Both strategies resulted in clear degrada-
tion in performance on both the development and
test sets, a finding that validates our encoding of
agreement information as “soft” preferences.
</bodyText>
<subsectionHeader confidence="0.994846">
4.5 On the development/test set split
</subsectionHeader>
<bodyText confidence="0.997486333333333">
We have seen several cases in which the method
that performs best on the development set does
not yield the best test-set performance. However,
we felt that it would be illegitimate to change the
train/development/test sets in a post hoc fashion,
that is, after seeing the experimental results.
Moreover, and crucially, it is very clear that
using agreement information, encoded as prefer-
ences within our graph-based approach rather than
as hard constraints, yields substantial improve-
ments on both the development and test set; this,
we believe, is our most important finding.
</bodyText>
<sectionHeader confidence="0.997356" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.998240239130435">
Politically-oriented text Sentiment analysis has
specifically been proposed as a key enabling tech-
nology in eRulemaking, allowing the automatic
analysis of the opinions that people submit (Shul-
man et al., 2005; Cardie et al., 2006; Kwon et al.,
2006). There has also been work focused upon de-
termining the political leaning (e.g., “liberal” vs.
“conservative”) of a document or author, where
most previously-proposed methods make no di-
rect use of relationships between the documents to
be classified (the “unlabeled” texts) (Laver et al.,
2003; Efron, 2004; Mullen and Malouf, 2006). An
exception is Grefenstette et al. (2004), who exper-
imented with determining the political orientation
of websites essentially by classifying the concate-
nation of all the documents found on that site.
Others have applied the NLP technologies of
near-duplicate detection and topic-based text cat-
egorization to politically oriented text (Yang and
Callan, 2005; Purpura and Hillard, 2006).
Detecting agreement We used a simple method
to learn to identify cross-speaker references indi-
cating agreement. More sophisticated approaches
have been proposed (Hillard et al., 2003), in-
cluding an extension that, in an interesting re-
versal of our problem, makes use of sentiment-
polarity indicators within speech segments (Gal-
ley et al., 2004). Also relevant is work on the gen-
eral problems of dialog-act tagging (Stolcke et al.,
2000), citation analysis (Lehnert et al., 1990), and
computational rhetorical analysis (Marcu, 2000;
Teufel and Moens, 2002).
We currently do not have an efficient means
to encode disagreement information as hard con-
straints; we plan to investigate incorporating such
information in future work.
Relationships between the unlabeled items
Carvalho and Cohen (2005) consider sequential
relations between different types of emails (e.g.,
between requests and satisfactions thereof) to clas-
sify messages, and thus also explicitly exploit the
structure of conversations.
Previous sentiment-analysis work in different
domains has considered inter-document similar-
ity (Agarwal and Bhattacharyya, 2005; Pang and
Lee, 2005; Goldberg and Zhu, 2006) or explicit
</bodyText>
<page confidence="0.998352">
333
</page>
<bodyText confidence="0.999813736842105">
inter-document references in the form of hyper-
links (Agrawal et al., 2003).
Notable early papers on graph-based semi-
supervised learning include Blum and Chawla
(2001), Bansal et al. (2002), Kondor and Lafferty
(2002), and Joachims (2003). Zhu (2005) main-
tains a survey of this area.
Recently, several alternative, often quite sophis-
ticated approaches to collective classification have
been proposed (Neville and Jensen, 2000; Laf-
ferty et al., 2001; Getoor et al., 2002; Taskar et
al., 2002; Taskar et al., 2003; Taskar et al., 2004;
McCallum and Wellner, 2004). It would be inter-
esting to investigate the application of such meth-
ods to our problem. However, we also believe
that our approach has important advantages, in-
cluding conceptual simplicity and the fact that it is
based on an underlying optimization problem that
is provably and in practice easy to solve.
</bodyText>
<sectionHeader confidence="0.991114" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999976977777778">
In this study, we focused on very general types
of cross-document classification preferences, uti-
lizing constraints based only on speaker identity
and on direct textual references between state-
ments. We showed that the integration of even
very limited information regarding inter-document
relationships can significantly increase the accu-
racy of support/opposition classification.
The simple constraints modeled in our study,
however, represent just a small portion of the
rich network of relationships that connect state-
ments and speakers across the political universe
and in the wider realm of opinionated social dis-
course. One intriguing possibility is to take ad-
vantage of (readily identifiable) information re-
garding interpersonal relationships, making use of
speaker/author affiliations, positions within a so-
cial hierarchy, and so on. Or, we could even at-
tempt to model relationships between topics or
concepts, in a kind of extension of collaborative
filtering. For example, perhaps we could infer that
two speakers sharing a common opinion on evo-
lutionary biologist Richard Dawkins (a.k.a. “Dar-
win’s rottweiler”) will be likely to agree in a de-
bate centered on Intelligent Design. While such
functionality is well beyond the scope of our cur-
rent study, we are optimistic that we can develop
methods to exploit additional types of relation-
ships in future work.
Acknowledgments We thank Claire Cardie, Jon
Kleinberg, Michael Macy, Andrew Myers, and the
six anonymous EMNLP referees for valuable dis-
cussions and comments. We also thank Reviewer
1 for generously providing additional post hoc
feedback, and the EMNLP chairs Eric Gaussier
and Dan Jurafsky for facilitating the process (as
well as for allowing authors an extra proceedings
page...). This paper is based upon work sup-
ported in part by the National Science Founda-
tion under grant no. IIS-0329064. Any opinions,
findings, and conclusions or recommendations ex-
pressed are those of the authors and do not neces-
sarily reflect the views or official policies, either
expressed or implied, of any sponsoring institu-
tions, the U.S. government, or any other entity.
</bodyText>
<sectionHeader confidence="0.997798" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98991712195122">
A. Agarwal, P. Bhattacharyya. 2005. Sentiment anal-
ysis: A new approach for effective use of linguis-
tic knowledge and exploiting similarities in a set of
documents to be classified. In Proceedings of the
International Conference on Natural Language Pro-
cessing (ICON).
R. Agrawal, S. Rajagopalan, R. Srikant, Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of WWW, 529–535.
N. Bansal, A. Blum, S. Chawla. 2002. Correla-
tion clustering. In Proceedings of the Symposium
on Foundations of Computer Science (FOCS), 238–
247. Journal version in Machine Learning Journal,
special issue on theoretical advances in data cluster-
ing, 56(1-3):89–113 (2004).
R. Barzilay, M. Lapata. 2005. Collective content selec-
tion for concept-to-text generation. In Proceedings
ofHLT/EMNLP, 331–338.
A. Blum, S. Chawla. 2001. Learning from labeled and
unlabeled data using graph mincuts. In Proceedings
ofICML, 19–26.
C. Cardie, C. Farina, T. Bruce, E. Wagner. 2006. Us-
ing natural language processing to improve eRule-
making. In Proceedings of Digital Government Re-
search (dg.o).
V. Carvalho, W. W. Cohen. 2005. On the collective
classification of email “speech acts”. In Proceedings
of SIGIR, 345–352.
W. Daelemans, V. Hoste. 2002. Evaluation of ma-
chine learning methods for natural language pro-
cessing tasks. In Proceedings of the Third Interna-
tional Conference on Language Resources and Eval-
uation (LREC), 755–760.
S. Das, M. Chen. 2001. Yahoo! for Amazon: Extract-
ing market sentiment from stock message boards. In
Proceedings of the Asia Pacific Finance Association
Annual Conference (APFA).
K. Dave, S. Lawrence, D. M. Pennock. 2003. Mining
the peanut gallery: Opinion extraction and semantic
classification of product reviews. In Proceedings of
WWW, 519–528.
</reference>
<page confidence="0.995169">
334
</page>
<reference confidence="0.998286078124999">
M. Efron. 2004. Cultural orientation: Classifying sub-
jective documents by cociation [sic] analysis. In
Proceedings of the AAAI Fall Symposium on Style
and Meaning in Language, Art, Music, and Design,
41–48.
A. Esuli. 2006. Sentiment classification bibliography.
liinwww.ira.uka.de/bibliography/Misc/Sentiment.html.
M. Galley, K. McKeown, J. Hirschberg, E. Shriberg.
2004. Identifying agreement and disagreement in
conversational speech: Use of Bayesian networks to
model pragmatic dependencies. In Proceedings of
the 42nd ACL, 669–676.
L. Getoor, N. Friedman, D. Koller, B. Taskar. 2002.
Learning probabilistic models of relational structure.
Journal of Machine Learning Research, 3:679–707.
Special issue on the Eighteenth ICML.
A. B. Goldberg, J. Zhu. 2006. Seeing stars
when there aren’t many stars: Graph-based semi-
supervised learning for sentiment categorization.
In TextGraphs: HLT/NAACL Workshop on Graph-
based Algorithms for Natural Language Processing.
G. Grefenstette, Y. Qu, J. G. Shanahan, D. A. Evans.
2004. Coupling niche browsers and affect analysis
for an opinion mining application. In Proceedings
of RIAO.
M. Hearst. 1992. Direction-based text interpretation as
an information access refinement. In P. Jacobs, ed.,
Text-Based Intelligent Systems, 257–274. Lawrence
Erlbaum Associates.
D. Hillard, M. Ostendorf, E. Shriberg. 2003. Detection
of agreement vs. disagreement in meetings: Train-
ing with unlabeled data. In Proceedings of HLT-
NAACL.
T. Joachims. 2003. Transductive learning via spectral
graph partitioning. In Proceedings of ICML, 290–
297.
R. I. Kondor, J. D. Lafferty. 2002. Diffusion kernels
on graphs and other discrete input spaces. In Pro-
ceedings ofICML, 315–322.
N. Kwon, S. Shulman, E. Hovy. 2006. Multidimen-
sional text analysis for eRulemaking. In Proceed-
ings ofDigital Government Research (dg.o).
J. Lafferty, A. McCallum, F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
ofICML, 282–289.
M. Laver, K. Benoit, J. Garry. 2003. Extracting policy
positions from political texts using words as data.
American Political Science Review.
W. Lehnert, C. Cardie, E. Riloff. 1990. Analyzing re-
search papers using citation sentences. In Program
of the Twelfth Annual Conference of the Cognitive
Science Society, 511–18.
D. Marcu. 2000. The theory and practice of discourse
parsing and summarization. MIT Press.
A. McCallum, B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Proceedings ofNIPS.
T. Mullen, R. Malouf. 2006. A preliminary investiga-
tion into sentiment analysis of informal political dis-
course. In Proceedings of the AAAI Symposium on
Computational Approaches to Analyzing Weblogs,
159–162.
A. Munson, C. Cardie, R. Caruana. 2005. Optimizing
to arbitrary NLP metrics using ensemble selection.
In Proceedings ofHLT-EMNLP, 539–546.
J. Neville, D. Jensen. 2000. Iterative classification in
relational data. In Proceedings of the AAAI Work-
shop on Learning Statistical Models from Relational
Data, 13–20.
B. Pang, L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the ACL,
271–278.
B. Pang, L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with re-
spect to rating scales. In Proceedings of the ACL.
B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proceedings of EMNLP, 79–86.
S. Purpura, D. Hillard. 2006. Automated classifica-
tion of congressional legislation. In Proceedings of
Digital Government Research (dg.o).
W. Sack. 1994. On the computation of point of view.
In Proceedings ofAAAI, pg. 1488. Student abstract.
S. Shulman, D. Schlosberg. 2002. Electronic rulemak-
ing: New frontiers in public participation. Prepared
for the Annual Meeting of the American Political
Science Association.
S. Shulman, J. Callan, E. Hovy, S. Zavestoski. 2005.
Language processing technologies for electronic
rulemaking: A project highlight. In Proceedings of
Digital Government Research (dg.o), 87–88.
S. S. Smith, J. M. Roberts, R. J. Vander Wielen. 2005.
The American Congress. Cambridge University
Press, fourth edition.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339–373.
B. Taskar, P. Abbeel, D. Koller. 2002. Discriminative
probabilistic models for relational data. In Proceed-
ings of UAI, Edmonton, Canada.
B. Taskar, C. Guestrin, D. Koller. 2003. Max-margin
Markov networks. In Proceedings ofNIPS.
B. Taskar, V. Chatalbashev, D. Koller. 2004. Learn-
ing associative Markov networks. In Proceedings of
ICML.
S. Teufel, M. Moens. 2002. Summarizing scientific
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409–445.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the ACL, 417–424.
J. M. Wiebe, W. J. Rapaport. 1988. A computational
theory of perspective and reference in narrative. In
Proceedings of the ACL, 131–138.
J. M. Wiebe. 1994. Tracking point of view in narrative.
Computational Linguistics, 20(2):233–287.
H. Yang, J. Callan. 2005. Near-duplicate detection
for eRulemaking. In Proceedings of Digital Gov-
ernment Research (dg.o).
J. Zhu. 2005. Semi-supervised learning literature
survey. Computer Sciences Technical Report TR
1530, University of Wisconsin-Madison. Available
at http://www.cs.wisc.edu/∼jerryzhu/pub/ssl survey.pdf;
has been updated since the initial 2005 version.
</reference>
<page confidence="0.999026">
335
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.608783">
<title confidence="0.9798505">Get out the vote: Determining support or opposition from Congressional floor-debate transcripts</title>
<author confidence="0.998123">Matt Thomas</author>
<author confidence="0.998123">Bo Pang</author>
<author confidence="0.998123">Lillian</author>
<affiliation confidence="0.9993">Department of Computer Science, Cornell</affiliation>
<address confidence="0.722774">Ithaca, NY</address>
<email confidence="0.994867">mattthomas84@gmail.com,pabo@cs.cornell.edu,llee@cs.cornell.edu</email>
<abstract confidence="0.9910045625">We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation. To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another. We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Agarwal</author>
<author>P Bhattacharyya</author>
</authors>
<title>Sentiment analysis: A new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Natural Language Processing (ICON).</booktitle>
<contexts>
<context position="5485" citStr="Agarwal and Bhattacharyya, 2005" startWordPosition="822" endWordPosition="825">Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easil</context>
<context position="14211" citStr="Agarwal and Bhattacharyya, 2005" startWordPosition="2191" endWordPosition="2195">segments receive the same label. Then, any class assignment c = c(s1), c(s2), ... , c(sn) can be assigned a cost � ind(s, c(s))+ � � str(`), s s,s&apos;: c(s)6=c(s&apos;) t betweens,s&apos; where c(s) is the “opposite” class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVM&amp;quot;ght to obtain individual-document classification scores, treating Y </context>
<context position="30841" citStr="Agarwal and Bhattacharyya, 2005" startWordPosition="4802" endWordPosition="4805">onal rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCal</context>
</contexts>
<marker>Agarwal, Bhattacharyya, 2005</marker>
<rawString>A. Agarwal, P. Bhattacharyya. 2005. Sentiment analysis: A new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified. In Proceedings of the International Conference on Natural Language Processing (ICON).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>S Rajagopalan</author>
<author>R Srikant</author>
<author>Y Xu</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>529--535</pages>
<contexts>
<context position="6569" citStr="Agrawal et al. (2003)" startWordPosition="988" endWordPosition="991">ple, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data. Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants’ political leanings, and such information may not lead to significantly improved results even if it were available. tween two speakers, such as explicit assertions (“I second that!”) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf. Agrawal et al. (2003)). Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text. Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually. Intuition suggests that this is true of the data with which we experiment, for several reasons. First, U.S. congressional debates contain very rich language and cover an extremely wide variety of topics, ranging from </context>
<context position="30977" citStr="Agrawal et al., 2003" startWordPosition="4825" endWordPosition="4828">hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe </context>
</contexts>
<marker>Agrawal, Rajagopalan, Srikant, Xu, 2003</marker>
<rawString>R. Agrawal, S. Rajagopalan, R. Srikant, Y. Xu. 2003. Mining newsgroups using networks arising from social behavior. In Proceedings of WWW, 529–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bansal</author>
<author>A Blum</author>
<author>S Chawla</author>
</authors>
<title>Correlation clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of the Symposium on Foundations of Computer Science (FOCS), 238– 247. Journal version in Machine Learning Journal, special issue on theoretical advances in data clustering,</booktitle>
<pages>56--1</pages>
<contexts>
<context position="31091" citStr="Bansal et al. (2002)" startWordPosition="4842" endWordPosition="4845">nlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an un</context>
</contexts>
<marker>Bansal, Blum, Chawla, 2002</marker>
<rawString>N. Bansal, A. Blum, S. Chawla. 2002. Correlation clustering. In Proceedings of the Symposium on Foundations of Computer Science (FOCS), 238– 247. Journal version in Machine Learning Journal, special issue on theoretical advances in data clustering, 56(1-3):89–113 (2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Collective content selection for concept-to-text generation.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT/EMNLP,</booktitle>
<pages>331--338</pages>
<contexts>
<context position="14239" citStr="Barzilay and Lapata, 2005" startWordPosition="2196" endWordPosition="2200">Then, any class assignment c = c(s1), c(s2), ... , c(sn) can be assigned a cost � ind(s, c(s))+ � � str(`), s s,s&apos;: c(s)6=c(s&apos;) t betweens,s&apos; where c(s) is the “opposite” class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVM&amp;quot;ght to obtain individual-document classification scores, treating Y as the positive class and us</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>R. Barzilay, M. Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings ofHLT/EMNLP, 331–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>S Chawla</author>
</authors>
<title>Learning from labeled and unlabeled data using graph mincuts.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML,</booktitle>
<contexts>
<context position="12624" citStr="Blum and Chawla (2001)" startWordPosition="1925" endWordPosition="1928">e set because our goal is to examine classification of speech segments in the context of the surrounding discussion. 3 Method The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation. As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate. Our classification framework, directly inspired by Blum and Chawla (2001), integrates both perspectives, optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label. In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships. 329 3.1 Classification framework Let s1, s2, ... , sn be the sequence of speech segments within a given debate, and let Y and N stand for the “yea” and “nay” class, respectively. Assume we have a non-negative function ind(</context>
<context position="31069" citStr="Blum and Chawla (2001)" startWordPosition="4838" endWordPosition="4841">ationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact tha</context>
</contexts>
<marker>Blum, Chawla, 2001</marker>
<rawString>A. Blum, S. Chawla. 2001. Learning from labeled and unlabeled data using graph mincuts. In Proceedings ofICML, 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>C Farina</author>
<author>T Bruce</author>
<author>E Wagner</author>
</authors>
<title>Using natural language processing to improve eRulemaking.</title>
<date>2006</date>
<booktitle>In Proceedings of Digital Government Research (dg.o).</booktitle>
<contexts>
<context position="28976" citStr="Cardie et al., 2006" startWordPosition="4528" endWordPosition="4531">velopment/test sets in a post hoc fashion, that is, after seeing the experimental results. Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006). There has also been work focused upon determining the political leaning (e.g., “liberal” vs. “conservative”) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the “unlabeled” texts) (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of </context>
</contexts>
<marker>Cardie, Farina, Bruce, Wagner, 2006</marker>
<rawString>C. Cardie, C. Farina, T. Bruce, E. Wagner. 2006. Using natural language processing to improve eRulemaking. In Proceedings of Digital Government Research (dg.o).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Carvalho</author>
<author>W W Cohen</author>
</authors>
<title>On the collective classification of email “speech acts”.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>345--352</pages>
<contexts>
<context position="30511" citStr="Carvalho and Cohen (2005)" startWordPosition="4760" endWordPosition="4763">d et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Laffert</context>
</contexts>
<marker>Carvalho, Cohen, 2005</marker>
<rawString>V. Carvalho, W. W. Cohen. 2005. On the collective classification of email “speech acts”. In Proceedings of SIGIR, 345–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>V Hoste</author>
</authors>
<title>Evaluation of machine learning methods for natural language processing tasks.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>755--760</pages>
<contexts>
<context position="15272" citStr="Daelemans and Hoste, 2002" startWordPosition="2348" endWordPosition="2351">ying speech segments in isolation In our experiments, we employed the well-known classifier SVM&amp;quot;ght to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al., 2002), the input to SVM&amp;quot;ght consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al., 2005). for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s, Y) def = { 1 d(s) &gt; 2σs7 0 d(s) &lt; −2σs (1 + 2a3,) /2 |d(s) |≤ 2σs7 where σs is the standard deviation of d(s) over all speech segments s in the debate in question, and ind(s, N) def = 1 − ind(s, Y). We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class. 3.3 Relationships between speech segments A wide range of relationships between text segments ca</context>
</contexts>
<marker>Daelemans, Hoste, 2002</marker>
<rawString>W. Daelemans, V. Hoste. 2002. Evaluation of machine learning methods for natural language processing tasks. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC), 755–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Das</author>
<author>M Chen</author>
</authors>
<title>Yahoo! for Amazon: Extracting market sentiment from stock message boards.</title>
<date>2001</date>
<booktitle>In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA).</booktitle>
<contexts>
<context position="5185" citStr="Das and Chen, 2001" startWordPosition="780" endWordPosition="783">whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most in</context>
</contexts>
<marker>Das, Chen, 2001</marker>
<rawString>S. Das, M. Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dave</author>
<author>S Lawrence</author>
<author>D M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>519--528</pages>
<contexts>
<context position="5238" citStr="Dave et al., 2003" startWordPosition="790" endWordPosition="793">thin the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we </context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>K. Dave, S. Lawrence, D. M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of WWW, 519–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Efron</author>
</authors>
<title>Cultural orientation: Classifying subjective documents by cociation [sic] analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Style and Meaning in Language, Art, Music, and Design,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="29302" citStr="Efron, 2004" startWordPosition="4581" endWordPosition="4582">believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006). There has also been work focused upon determining the political leaning (e.g., “liberal” vs. “conservative”) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the “unlabeled” texts) (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003),</context>
</contexts>
<marker>Efron, 2004</marker>
<rawString>M. Efron. 2004. Cultural orientation: Classifying subjective documents by cociation [sic] analysis. In Proceedings of the AAAI Fall Symposium on Style and Meaning in Language, Art, Music, and Design, 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
</authors>
<date>2006</date>
<note>Sentiment classification bibliography. liinwww.ira.uka.de/bibliography/Misc/Sentiment.html.</note>
<contexts>
<context position="4884" citStr="Esuli (2006)" startWordPosition="739" endWordPosition="740">e of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 200</context>
</contexts>
<marker>Esuli, 2006</marker>
<rawString>A. Esuli. 2006. Sentiment classification bibliography. liinwww.ira.uka.de/bibliography/Misc/Sentiment.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>J Hirschberg</author>
<author>E Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: Use of Bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd ACL,</booktitle>
<pages>669--676</pages>
<contexts>
<context position="30061" citStr="Galley et al., 2004" startWordPosition="4692" endWordPosition="4696">ites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus als</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>M. Galley, K. McKeown, J. Hirschberg, E. Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use of Bayesian networks to model pragmatic dependencies. In Proceedings of the 42nd ACL, 669–676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Getoor</author>
<author>N Friedman</author>
<author>D Koller</author>
<author>B Taskar</author>
</authors>
<title>Learning probabilistic models of relational structure.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<booktitle>Special issue on the Eighteenth ICML.</booktitle>
<pages>3--679</pages>
<contexts>
<context position="31371" citStr="Getoor et al., 2002" startWordPosition="4885" endWordPosition="4888">fferent domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual refe</context>
</contexts>
<marker>Getoor, Friedman, Koller, Taskar, 2002</marker>
<rawString>L. Getoor, N. Friedman, D. Koller, B. Taskar. 2002. Learning probabilistic models of relational structure. Journal of Machine Learning Research, 3:679–707. Special issue on the Eighteenth ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Goldberg</author>
<author>J Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: Graph-based semisupervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In TextGraphs: HLT/NAACL Workshop on Graphbased Algorithms for Natural Language Processing.</booktitle>
<contexts>
<context position="5530" citStr="Goldberg and Zhu, 2006" startWordPosition="830" endWordPosition="833">aphy). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), w</context>
<context position="30886" citStr="Goldberg and Zhu, 2006" startWordPosition="4810" endWordPosition="4813">ns, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesti</context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>A. B. Goldberg, J. Zhu. 2006. Seeing stars when there aren’t many stars: Graph-based semisupervised learning for sentiment categorization. In TextGraphs: HLT/NAACL Workshop on Graphbased Algorithms for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
<author>Y Qu</author>
<author>J G Shanahan</author>
<author>D A Evans</author>
</authors>
<title>Coupling niche browsers and affect analysis for an opinion mining application.</title>
<date>2004</date>
<booktitle>In Proceedings of RIAO.</booktitle>
<contexts>
<context position="29372" citStr="Grefenstette et al. (2004)" startWordPosition="4590" endWordPosition="4593">Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006). There has also been work focused upon determining the political leaning (e.g., “liberal” vs. “conservative”) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the “unlabeled” texts) (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our proble</context>
</contexts>
<marker>Grefenstette, Qu, Shanahan, Evans, 2004</marker>
<rawString>G. Grefenstette, Y. Qu, J. G. Shanahan, D. A. Evans. 2004. Coupling niche browsers and affect analysis for an opinion mining application. In Proceedings of RIAO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Direction-based text interpretation as an information access refinement.</title>
<date>1992</date>
<booktitle>Text-Based Intelligent Systems, 257–274. Lawrence Erlbaum Associates.</booktitle>
<editor>In P. Jacobs, ed.,</editor>
<contexts>
<context position="4835" citStr="Hearst (1992)" startWordPosition="731" endWordPosition="732">sents support for or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the te</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Direction-based text interpretation as an information access refinement. In P. Jacobs, ed., Text-Based Intelligent Systems, 257–274. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>E Shriberg</author>
</authors>
<title>Detection of agreement vs. disagreement in meetings: Training with unlabeled data.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="29901" citStr="Hillard et al., 2003" startWordPosition="4666" endWordPosition="4669">al., 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Co</context>
</contexts>
<marker>Hillard, Ostendorf, Shriberg, 2003</marker>
<rawString>D. Hillard, M. Ostendorf, E. Shriberg. 2003. Detection of agreement vs. disagreement in meetings: Training with unlabeled data. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Transductive learning via spectral graph partitioning.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>290--297</pages>
<contexts>
<context position="31140" citStr="Joachims (2003)" startWordPosition="4851" endWordPosition="4852">ential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably an</context>
</contexts>
<marker>Joachims, 2003</marker>
<rawString>T. Joachims. 2003. Transductive learning via spectral graph partitioning. In Proceedings of ICML, 290– 297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R I Kondor</author>
<author>J D Lafferty</author>
</authors>
<title>Diffusion kernels on graphs and other discrete input spaces.</title>
<date>2002</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>315--322</pages>
<contexts>
<context position="31119" citStr="Kondor and Lafferty (2002)" startWordPosition="4846" endWordPosition="4849">o and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization proble</context>
</contexts>
<marker>Kondor, Lafferty, 2002</marker>
<rawString>R. I. Kondor, J. D. Lafferty. 2002. Diffusion kernels on graphs and other discrete input spaces. In Proceedings ofICML, 315–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kwon</author>
<author>S Shulman</author>
<author>E Hovy</author>
</authors>
<title>Multidimensional text analysis for eRulemaking.</title>
<date>2006</date>
<booktitle>In Proceedings ofDigital Government Research (dg.o).</booktitle>
<contexts>
<context position="28996" citStr="Kwon et al., 2006" startWordPosition="4532" endWordPosition="4535">n a post hoc fashion, that is, after seeing the experimental results. Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006). There has also been work focused upon determining the political leaning (e.g., “liberal” vs. “conservative”) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the “unlabeled” texts) (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detec</context>
</contexts>
<marker>Kwon, Shulman, Hovy, 2006</marker>
<rawString>N. Kwon, S. Shulman, E. Hovy. 2006. Multidimensional text analysis for eRulemaking. In Proceedings ofDigital Government Research (dg.o).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="31350" citStr="Lafferty et al., 2001" startWordPosition="4880" endWordPosition="4884">ent-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and o</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML, 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Laver</author>
<author>K Benoit</author>
<author>J Garry</author>
</authors>
<title>Extracting policy positions from political texts using words as data. American Political Science Review.</title>
<date>2003</date>
<contexts>
<context position="29289" citStr="Laver et al., 2003" startWordPosition="4577" endWordPosition="4580"> test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006). There has also been work focused upon determining the political leaning (e.g., “liberal” vs. “conservative”) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the “unlabeled” texts) (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard e</context>
</contexts>
<marker>Laver, Benoit, Garry, 2003</marker>
<rawString>M. Laver, K. Benoit, J. Garry. 2003. Extracting policy positions from political texts using words as data. American Political Science Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
<author>C Cardie</author>
<author>E Riloff</author>
</authors>
<title>Analyzing research papers using citation sentences.</title>
<date>1990</date>
<booktitle>In Program of the Twelfth Annual Conference of the Cognitive Science Society,</booktitle>
<pages>511--18</pages>
<contexts>
<context position="30195" citStr="Lehnert et al., 1990" startWordPosition="4715" endWordPosition="4718">f near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-docume</context>
</contexts>
<marker>Lehnert, Cardie, Riloff, 1990</marker>
<rawString>W. Lehnert, C. Cardie, E. Riloff. 1990. Analyzing research papers using citation sentences. In Program of the Twelfth Annual Conference of the Cognitive Science Society, 511–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The theory and practice of discourse parsing and summarization.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="30247" citStr="Marcu, 2000" startWordPosition="4723" endWordPosition="4724"> to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000. The theory and practice of discourse parsing and summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2004</date>
<booktitle>In Proceedings ofNIPS.</booktitle>
<contexts>
<context position="31463" citStr="McCallum and Wellner, 2004" startWordPosition="4901" endWordPosition="4904"> 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements. We showed that the integration of even very limited information r</context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>A. McCallum, B. Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. In Proceedings ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mullen</author>
<author>R Malouf</author>
</authors>
<title>A preliminary investigation into sentiment analysis of informal political discourse.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Symposium on Computational Approaches to Analyzing Weblogs,</booktitle>
<pages>159--162</pages>
<contexts>
<context position="6539" citStr="Mullen and Malouf (2006)" startWordPosition="982" endWordPosition="985">em-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data. Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants’ political leanings, and such information may not lead to significantly improved results even if it were available. tween two speakers, such as explicit assertions (“I second that!”) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf. Agrawal et al. (2003)). Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text. Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually. Intuition suggests that this is true of the data with which we experiment, for several reasons. First, U.S. congressional debates contain very rich language and cover an extremely wide va</context>
<context position="29328" citStr="Mullen and Malouf, 2006" startWordPosition="4583" endWordPosition="4586">ur most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006). There has also been work focused upon determining the political leaning (e.g., “liberal” vs. “conservative”) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the “unlabeled” texts) (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension th</context>
</contexts>
<marker>Mullen, Malouf, 2006</marker>
<rawString>T. Mullen, R. Malouf. 2006. A preliminary investigation into sentiment analysis of informal political discourse. In Proceedings of the AAAI Symposium on Computational Approaches to Analyzing Weblogs, 159–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Munson</author>
<author>C Cardie</author>
<author>R Caruana</author>
</authors>
<title>Optimizing to arbitrary NLP metrics using ensemble selection.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT-EMNLP,</booktitle>
<pages>539--546</pages>
<contexts>
<context position="15294" citStr="Munson et al., 2005" startWordPosition="2352" endWordPosition="2355">lation In our experiments, we employed the well-known classifier SVM&amp;quot;ght to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al., 2002), the input to SVM&amp;quot;ght consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al., 2005). for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s, Y) def = { 1 d(s) &gt; 2σs7 0 d(s) &lt; −2σs (1 + 2a3,) /2 |d(s) |≤ 2σs7 where σs is the standard deviation of d(s) over all speech segments s in the debate in question, and ind(s, N) def = 1 − ind(s, Y). We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class. 3.3 Relationships between speech segments A wide range of relationships between text segments can be modeled as positi</context>
</contexts>
<marker>Munson, Cardie, Caruana, 2005</marker>
<rawString>A. Munson, C. Cardie, R. Caruana. 2005. Optimizing to arbitrary NLP metrics using ensemble selection. In Proceedings ofHLT-EMNLP, 539–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Neville</author>
<author>D Jensen</author>
</authors>
<title>Iterative classification in relational data.</title>
<date>2000</date>
<booktitle>In Proceedings of the AAAI Workshop on Learning Statistical Models from Relational Data,</booktitle>
<pages>13--20</pages>
<contexts>
<context position="31327" citStr="Neville and Jensen, 2000" startWordPosition="4876" endWordPosition="4879">ersations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on</context>
</contexts>
<marker>Neville, Jensen, 2000</marker>
<rawString>J. Neville, D. Jensen. 2000. Iterative classification in relational data. In Proceedings of the AAAI Workshop on Learning Statistical Models from Relational Data, 13–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="14178" citStr="Pang and Lee, 2004" startWordPosition="2187" endWordPosition="2190">t the linked speech segments receive the same label. Then, any class assignment c = c(s1), c(s2), ... , c(sn) can be assigned a cost � ind(s, c(s))+ � � str(`), s s,s&apos;: c(s)6=c(s&apos;) t betweens,s&apos; where c(s) is the “opposite” class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVM&amp;quot;ght to obtain individual-document c</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang, L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the ACL, 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="5505" citStr="Pang and Lee, 2005" startWordPosition="826" endWordPosition="829">r an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via t</context>
<context position="30861" citStr="Pang and Lee, 2005" startWordPosition="4806" endWordPosition="4809">2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 200</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>B. Pang, L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="5204" citStr="Pang et al., 2002" startWordPosition="784" endWordPosition="787">aker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniq</context>
<context position="14941" citStr="Pang et al., 2002" startWordPosition="2305" endWordPosition="2308">aph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. 3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVM&amp;quot;ght to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al., 2002), the input to SVM&amp;quot;ght consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al., 2005). for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s, Y) def = { 1 d(s) &gt; 2σs7 0 d(s) &lt; −2σs (1 + 2a3,) /2 |d(s) |≤ 2σs7 where σs is the standard deviation of d(s</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP, 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Purpura</author>
<author>D Hillard</author>
</authors>
<title>Automated classification of congressional legislation.</title>
<date>2006</date>
<booktitle>In Proceedings of Digital Government Research (dg.o).</booktitle>
<contexts>
<context position="29716" citStr="Purpura and Hillard, 2006" startWordPosition="4640" endWordPosition="4643"> “conservative”) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the “unlabeled” texts) (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient mean</context>
</contexts>
<marker>Purpura, Hillard, 2006</marker>
<rawString>S. Purpura, D. Hillard. 2006. Automated classification of congressional legislation. In Proceedings of Digital Government Research (dg.o).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sack</author>
</authors>
<title>On the computation of point of view.</title>
<date>1994</date>
<booktitle>In Proceedings ofAAAI,</booktitle>
<pages>1488</pages>
<note>Student abstract.</note>
<contexts>
<context position="4848" citStr="Sack (1994)" startWordPosition="733" endWordPosition="734">or or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be lab</context>
</contexts>
<marker>Sack, 1994</marker>
<rawString>W. Sack. 1994. On the computation of point of view. In Proceedings ofAAAI, pg. 1488. Student abstract.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shulman</author>
<author>D Schlosberg</author>
</authors>
<title>Electronic rulemaking: New frontiers in public participation. Prepared for the Annual Meeting of the American Political Science Association.</title>
<date>2002</date>
<contexts>
<context position="2323" citStr="Shulman and Schlosberg, 2002" startWordPosition="341" endWordPosition="344">abase (http://thomas.loc.gov) of congresonline accessibility of politically oriented texts in particular, however, is a phenomenon that some have gone so far as to say will have a potentially society-changing effect. In the United States, for example, governmental bodies are providing and soliciting political documents via the Internet, with lofty goals in mind: electronic rulemaking (eRulemaking) initiatives involving the “electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process”, may “[alter] the citizen-government relationship” (Shulman and Schlosberg, 2002). Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics2, or at least on political journalism3. Regardless of whether one views such claims as clear-sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process. Evaluative and persuasive documents, such as a politician’s speech regarding a bill or a blogger’s commentary on a legislative proposal, form a particularly interesting type o</context>
</contexts>
<marker>Shulman, Schlosberg, 2002</marker>
<rawString>S. Shulman, D. Schlosberg. 2002. Electronic rulemaking: New frontiers in public participation. Prepared for the Annual Meeting of the American Political Science Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shulman</author>
<author>J Callan</author>
<author>E Hovy</author>
<author>S Zavestoski</author>
</authors>
<title>Language processing technologies for electronic rulemaking: A project highlight.</title>
<date>2005</date>
<booktitle>In Proceedings of Digital Government Research (dg.o),</booktitle>
<pages>87--88</pages>
<contexts>
<context position="28955" citStr="Shulman et al., 2005" startWordPosition="4523" endWordPosition="4527">to change the train/development/test sets in a post hoc fashion, that is, after seeing the experimental results. Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding. 5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006). There has also been work focused upon determining the political leaning (e.g., “liberal” vs. “conservative”) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the “unlabeled” texts) (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the</context>
</contexts>
<marker>Shulman, Callan, Hovy, Zavestoski, 2005</marker>
<rawString>S. Shulman, J. Callan, E. Hovy, S. Zavestoski. 2005. Language processing technologies for electronic rulemaking: A project highlight. In Proceedings of Digital Government Research (dg.o), 87–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Smith</author>
<author>J M Roberts</author>
<author>R J Vander Wielen</author>
</authors>
<title>The American Congress.</title>
<date>2005</date>
<publisher>Cambridge University Press,</publisher>
<note>fourth edition.</note>
<contexts>
<context position="3216" citStr="Smith et al., 2005" startWordPosition="482" endWordPosition="485">ant to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process. Evaluative and persuasive documents, such as a politician’s speech regarding a bill or a blogger’s commentary on a legislative proposal, form a particularly interesting type of politically oriented text. People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that (U.S.) bills often reach several hundred pages in length (Smith et al., 2005). Moreover, political opinions are exsional bills and related data was launched in January 1995, when Mosaic was not quite two years old and Altavista did not yet exist. 2E.g., “Internet injects sweeping change into U.S. politics”, Adam Nagourney, The New York Times, April 2, 2006. 3E.g., “The End of News?”, Michael Massing, The New York Review of Books, December 1, 2005. 327 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 327–335, Sydney, July 2006. c�2006 Association for Computational Linguistics plicitly solicited in the eRulemaking</context>
</contexts>
<marker>Smith, Roberts, Wielen, 2005</marker>
<rawString>S. S. Smith, J. M. Roberts, R. J. Vander Wielen. 2005. The American Congress. Cambridge University Press, fourth edition.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Stolcke</author>
<author>N Coccaro</author>
<author>R Bates</author>
<author>P Taylor</author>
<author>C Van EssDykema</author>
<author>K Ries</author>
<author>E Shriberg</author>
<author>D Jurafsky</author>
<author>R Mar-</author>
</authors>
<marker>Stolcke, Coccaro, Bates, Taylor, Van EssDykema, Ries, Shriberg, Jurafsky, Mar-, </marker>
<rawString>A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van EssDykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer tin</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>tin, 2000</marker>
<rawString>tin, M. Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>P Abbeel</author>
<author>D Koller</author>
</authors>
<title>Discriminative probabilistic models for relational data.</title>
<date>2002</date>
<booktitle>In Proceedings of UAI,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="31392" citStr="Taskar et al., 2002" startWordPosition="4889" endWordPosition="4892">onsidered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statem</context>
</contexts>
<marker>Taskar, Abbeel, Koller, 2002</marker>
<rawString>B. Taskar, P. Abbeel, D. Koller. 2002. Discriminative probabilistic models for relational data. In Proceedings of UAI, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In Proceedings ofNIPS.</booktitle>
<contexts>
<context position="31413" citStr="Taskar et al., 2003" startWordPosition="4893" endWordPosition="4896">ent similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements. We showed that </context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, D. Koller. 2003. Max-margin Markov networks. In Proceedings ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>V Chatalbashev</author>
<author>D Koller</author>
</authors>
<title>Learning associative Markov networks.</title>
<date>2004</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="31434" citStr="Taskar et al., 2004" startWordPosition="4897" endWordPosition="4900">al and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve. 6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements. We showed that the integration of ev</context>
</contexts>
<marker>Taskar, Chatalbashev, Koller, 2004</marker>
<rawString>B. Taskar, V. Chatalbashev, D. Koller. 2004. Learning associative Markov networks. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Summarizing scientific articles: Experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="30272" citStr="Teufel and Moens, 2002" startWordPosition="4725" endWordPosition="4728">ly oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg </context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>S. Teufel, M. Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="5218" citStr="Turney, 2002" startWordPosition="788" endWordPosition="789">posal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the ACL, 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Wiebe</author>
<author>W J Rapaport</author>
</authors>
<title>A computational theory of perspective and reference in narrative.</title>
<date>1988</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>131--138</pages>
<contexts>
<context position="4820" citStr="Wiebe and Rapaport (1988)" startWordPosition="727" endWordPosition="730">aker segment of text) represents support for or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity</context>
</contexts>
<marker>Wiebe, Rapaport, 1988</marker>
<rawString>J. M. Wiebe, W. J. Rapaport. 1988. A computational theory of perspective and reference in narrative. In Proceedings of the ACL, 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Wiebe</author>
</authors>
<title>Tracking point of view in narrative.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="4866" citStr="Wiebe (1994)" startWordPosition="736" endWordPosition="737">to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and </context>
</contexts>
<marker>Wiebe, 1994</marker>
<rawString>J. M. Wiebe. 1994. Tracking point of view in narrative. Computational Linguistics, 20(2):233–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yang</author>
<author>J Callan</author>
</authors>
<title>Near-duplicate detection for eRulemaking.</title>
<date>2005</date>
<booktitle>In Proceedings of Digital Government Research (dg.o).</booktitle>
<contexts>
<context position="29688" citStr="Yang and Callan, 2005" startWordPosition="4636" endWordPosition="4639">ng (e.g., “liberal” vs. “conservative”) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the “unlabeled” texts) (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006). An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006). Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently d</context>
</contexts>
<marker>Yang, Callan, 2005</marker>
<rawString>H. Yang, J. Callan. 2005. Near-duplicate detection for eRulemaking. In Proceedings of Digital Government Research (dg.o).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
</authors>
<title>Semi-supervised learning literature survey. Computer Sciences</title>
<date>2005</date>
<tech>Technical Report TR 1530,</tech>
<institution>University of Wisconsin-Madison.</institution>
<contexts>
<context position="31152" citStr="Zhu (2005)" startWordPosition="4853" endWordPosition="4854">between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al., 2003). Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor and Lafferty (2002), and Joachims (2003). Zhu (2005) maintains a survey of this area. Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004). It would be interesting to investigate the application of such methods to our problem. However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practic</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>J. Zhu. 2005. Semi-supervised learning literature survey. Computer Sciences Technical Report TR 1530, University of Wisconsin-Madison. Available at http://www.cs.wisc.edu/∼jerryzhu/pub/ssl survey.pdf; has been updated since the initial 2005 version.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>