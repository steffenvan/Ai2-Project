<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003562">
<title confidence="0.996467">
A Chinese LPCFG Parser with Hybrid Character Information
</title>
<author confidence="0.999182">
Wenzhi Xu, Chaobo Sun and Caixia Yuan
</author>
<affiliation confidence="0.834079">
School of Computer,
Beijing University of Posts and Telecommunications,
Beijing, 100876 China
</affiliation>
<email confidence="0.9314845">
{ earl808, sunchaobo}@gmail.com
yuancx@bupt.edu.cn
</email>
<sectionHeader confidence="0.993776" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858">
We present a new probabilistic model
based on the lexical PCFG model, which
can easily utilize the Chinese character in-
formation to solve the lexical information
sparseness in lexical PCFG model. We
discuss in particular some important fea-
tures that can improve the parsing perfor-
mance, and describe the strategy of mod-
ifying original label structure to reduce
the label ambiguities. Final experiment
demonstrates that the character informa-
tion and label modification improve the
parsing performance.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986161290323">
Parsing is an important and fundamental task in
natural language processing. The challenge of
Chinese parser has been the focus of attention in
recent years, and many different kinds of Chi-
nese parsing models are investigated. (Bikel,
2000) adopts Head-Driven model to parse Chi-
nese. (Levy, 2003) analyzes the difficulties of
Chinese parsing through comparing the differ-
ences between Chinese and English. (Wang,
2006) utilizes shift-reduce approach, dramatically
improved the decoding speed of parsing. All these
research adopted the same models which are also
used in English parser – the models based on the
words.
However, there is a big difference between En-
glish and Chinese: the expressing unit in English
is word, while character is the smallest unit in
Chinese. Due to difficulties of word segmenta-
tion, especially for different segmenting criteria,
many researchers explored parsing Chinese based
on characters. The parser of (Luo, 2003) received
sentence as input and conducted word segmenta-
tion and syntactic parsing at the same time, but
they did not utilize the character information in
generating subtree; (Zhao, 2009)’s dependency
parsing tree totally abandoned the word concept,
so the dependency relations are the relations be-
tween characters.
We combine both word and character informa-
tion to gain better performance of parsing. Al-
though the criteria of segmentation are difficult to
be unified, different criteria conflict only within
the phrases which have little influence on the
structure between phrases. So we still use word
as our basic unit of parsing. Although word
has been proved to be effective in head-driven
parser (Collins,1999), the data of word depen-
dence is very sparse. While it is worthy to note
that words with similar concept always share the
same characters in Chinese. For instance, “W�
*(scientist)”, “fjP_?j�*(historian)”, etc., share
the same character “*(expert)”, since they belong
to the same concept “expert in a certain field”. So
the problem of word sparseness can be solved by
combining the character information to some ex-
tent.
Throughout this paper, we use TCT Treebank
(Zhou, 2004) as experimental data. TCT mainly
consists of binary trees, with a few of multi-
branch and single-branch trees. Thus, we first
transfer all trees to binary trees. Then we use
Lexical-PCFG model to exploit the word and
character information, and Maximum Entropy
Model to calculate the probability of induced trees
as (Charinak, 2000). Finally we use CKY-based
decoder.
In the following section, we will introduce how
to utilize character information in our parsing
model and the other features in detail. Section 3
gives experiment results and analysis, which show
improvement of our parsing approach. Section 4
presents the conclusion and future work.
</bodyText>
<sectionHeader confidence="0.986332" genericHeader="method">
2 Lexical PCFG model
</sectionHeader>
<subsectionHeader confidence="0.986586">
2.1 Model Introduction
</subsectionHeader>
<bodyText confidence="0.99111425">
Starting from the Lexical-PCFG model (Model 2
in Collins, 1999), we propose a new generative
process which can conveniently exploit the char-
acter information and other features.
Assume P is the label of parent, H is the head
child of the rule, and L1,..., Ln and R1, ..., Rn is
the left and right modifiers of H. Then the rule of
Lexical-PCFG (LPCFG) can be written as:
</bodyText>
<equation confidence="0.996354">
P(hw, ht) --+ (1)
Ln(lwn, ltn)...L1(lw1, lt1)H(hw, ht)
R1(rw1, rt1)...Rn(rwn, rtn),
</equation>
<bodyText confidence="0.999966625">
where (hw, ht) represents the head word and
head tag of head child, (lw1, lt1), ..., (lwn, ltn)
and (rw1, rt1), ..., (rwn, rtn) are the head words
and head tags of left and right modifiers, and par-
ent node P’s head word and tag are the same as
that of H.
As mentioned above, our trees are all binary
trees. In this case, the LPCFG can be written as:
</bodyText>
<equation confidence="0.9992505">
P(hw, ht) → H(hw, ht)R(rw, rt), (2)
P(hw, ht) → L(lw, lt)H(hw, ht). (3)
</equation>
<bodyText confidence="0.9994572">
Formula 2 and 3 represent that the head child is
the left or right child respectively. The probability
of the rule is conditioned on the head words and
tags of head and its modified children, which is
specified as:
</bodyText>
<equation confidence="0.599888">
Pr(P, L, H|hw, ht, lw, lt), (4)
and
Pr(P, R, H|hw, ht, rw, rt). (5)
</equation>
<bodyText confidence="0.999791">
To calculate these probabilities, we rewrite
Equation 4 and 5 by three factors in 6 and 7 us-
ing the chain rule.
</bodyText>
<equation confidence="0.997138">
Pr(P, R, H|hw, ht, lw, lt) = (6)
Prd(P − DIR|hw, ht, lw, lt) * Prh(H|P, hw, ht)
*Pr.(L − DIR|P, H, hw, ht),
Pr(P, R, H|hw, ht, rw, rt) = (7)
Prd(P − DIR|hw, ht, rw, rt) * Prh(H|P, hw, ht)
*Pr.(R − DIR|P, H, hw, ht).
</equation>
<bodyText confidence="0.9998654375">
in which, DIR=LEFT/RIGHT. DIR in P-DIR
is used to discriminate different positions of head
child, DIR in L-DIR and R-DIR are used to repre-
sent different positions of modifiers.
The calculation processes of Equation 6 and 7
can be interpreted by following generative pro-
cess. Firstly, the head words and tags of chil-
dren generate the parent and the head position (the
first probability in Equation 6 and 7). We define
this probability as the word dependency probabil-
ity Prd: if two words (or characters in words)
always appear together in the training data, this
probability will be large (&lt; 1); if two words (or
characters in words) do not have any dependence
in the training data, this probability will be ap-
proximately equal to 1/|Y |, where |Y  |is the pre-
dicted number of the Prd. The second probability
generates the head child label (defined as the head
child probability Prh), we hold that the head word
and tag of modifier do not provide information to
determine the head child label, so we omit them.
The third one produces the modifier label, which
is defined as the modifier probability Prm, and
evaluates the dependency relation between modi-
fier and the head child. We also omit the influence
of the head word and tag of modifier.
For example, assume there is a tree as shown
in Figure 1. For the rule “vp → v np”, head
child of parent vp and np are the left child v and
right child n respectively, so “_.R(organize)” and
“-V*(expert)” are the head word of vp and np.
Thus the LPCFG rule is “vp(_.R,v) → v(_.R,v)
</bodyText>
<figureCaption confidence="0.998372">
Figure 1: Tree representation of LPCFG rule.
</figureCaption>
<bodyText confidence="0.724568">
np(;*,n)”. The probability can be written as:
</bodyText>
<equation confidence="0.9691845">
Prd(vp-LEFT  ||„,v,;*,n) * Prh(v|vp,|
„,v) * Prm(np-RIGHT  |vp, v, |„,v).
</equation>
<subsectionHeader confidence="0.993529">
2.2 Probability Model and Feature Set
</subsectionHeader>
<bodyText confidence="0.999142486111111">
We use Maximum Entropy (ME) Model to com-
pute probabilities of candidate trees. ME model
estimate parameters that would maximize the en-
tropy over distributions, meanwhile satisfy certain
constraints. These constraints will force the model
to reflect characteristic of training data. With the
feature function, Maximum Entropy can exploit
kinds of features flexibly, some of which are very
important to improve the performance of tasks at
hand. ME model has been applied successfully
in many tasks, such as parser (Charniak, 2000;
Luo, 2003), POS tagging (Ratnaparkhi,1996), etc.
In our experiment, we use Maxent toolkit de-
velopped by Zhang (Zhang, 2004), which uses
the LBFGS algorithm for parameter estimation.
Details of the model and toolkit can be seen in
(Berger, 1996; Zhang, 2004).
Our features consist of four parts: basic fea-
tures, character features, context features and
overlapping features of character and context. Ba-
sic features are traditional LPCFG features, in-
cluding head word, head tag and the label. We
extract the first and last characters of a word as the
Character features, of course for a single character
word the first and last character are the same. Con-
text features are defined as the previous and fol-
lowing POS tags of the current subtree, and these
features utilize the information outside of the sub-
tree very well without increasing the complex-
ity of parsing decoder. Overlapping features are
the combinations of character features and context
features.
Take Chinese sentence “�§i5iI/n Eh/p A
uf/n |„/v ;&apos;)�/b ;*/n  |/v 7&amp;quot;b /v (Com-
mittee is composed of experts organized by the
Ministry of Agriculture)” for example, the corre-
sponding rule is “vp(|„,v) —* v(|„,v) np(;
*,n)”, the feature template of the example sen-
tence is shown in Table 1.
When applying the character information, it is
worthwhile to note that character is always com-
bined with the POS tag of the word since the
sense of single character varies as word’s POS tag
changes. For example, the sense of “O(love)”
in verb “O4&apos;(care and protect)” and noun “O
&apos;*(love)” is different. Of course the sense dis-
cussed here is reflected in the dependency of
words: “ONcare and protect)” can be followed
by some nouns which are objects, while “O
&apos;*(love)” can not.
For the multi-branch tree, (Collins,1999) calcu-
lates the probability of the left or right modifier
with a feature which represent whether there are
modifiers between current modifier and the head
child (distance feature). But in the situation of
binary tree, it is obvious that current modifier is
unlikely to follow other modifiers. Since the rep-
resentation of binary tree conforms to the X-bar
theory of Chomsky, we can modify the head child
label to get this non-local information in binary
tree. For instance, a multi-branche tree rule “vp1
—* pp d vp2” corresponding to these two binary
tree rules: “vp3 —* pp vp4” and “vp4 —* d vp5”
(the index numbers of the vb here stand for dif-
ferent vp). So when we calculate the probability
of pp with the multi-branch situations, d lies be-
tween pp and vp2. While in binary tree situation,
we cannot catch this information between pp and
vp4. However, we can modify vp4 to vp-LEFT,
which means there is a modifier at the left child
of vp4, then we get the similar effect in (Collins,
1999). We call this as the head position labeling.
</bodyText>
<subsectionHeader confidence="0.9905735">
2.3 Label splitting and Head Position
Modifying
</subsectionHeader>
<bodyText confidence="0.999691666666667">
(Klein, 2003) improves the performance of parser
via splitting the POS tag in corpus. We split the
non-terminal label using the same approaches (as-
suming the POS tag is terminal label). The need
of label splitting is that the corpus does not suffi-
ciently consider different situations and treat them
</bodyText>
<tableCaption confidence="0.996331">
Table 1: Feature templates and symbol explanation.
</tableCaption>
<table confidence="0.845685">
Prd (vp-LEFT) Prh (v) Prm (np-RIGHT)
basic lw rw |„;* p vp p h vp v
feature lw lt rw rt |„v ;*n p hw vp |„ p h hw vp v|„
phwht vp |„v phhwht vp v |„v
pht vpv phht vpvv
char. lw frc rt |„;n p fhc ht vp |v p h fhc ht vp v |v
feature other combinations ••• p lhc ht vp „v p h lhc ht vp v „v
flc lt frc rt |v ;n
other combinations •••
context lw rw pt1 at1 |„;*n v p pt1 vp n p h pt1 vp v n
feature lw lt rw rt pt1 at1 |„v ;*n n v p pt1 pt2 vp n p p h pt1 pt2 vp v n p
p at1 vpv p h at1 vpvv
p at1 at2 vpvv p h at1 at2 vpvvv
p pt1 at1 vpnv p h pt1 at1 vpvnv
p hw pt1 at1 vp |„v n p h hw pt1 at1 vp v|„v n
overlap lw frc rt pt1 at1 |„;n n v p fhc ht pt1 at1 vp |v n v p h fhc ht pt1 at1 vp v |v n v
feature other combinations ... p lhc ht pt1 at1 vp „v n v p h lhc ht pt1 at1 vp v „v n v
flc lt frc rt pt1 at1 |v ;n n v
other combinations ...
</table>
<subsectionHeader confidence="0.749922">
Symbol Explanation
</subsectionHeader>
<bodyText confidence="0.997988557377049">
frc lrc the first and last characters of the head word of the right child
pt at previous and following POS tag of current subtree, number indicates the position
flc llc the first and last characters of the head word of the left child
fhc lhc the first and last characters of the head word
as the same label which results in ambiguity. Fur-
thermore, in our experiment, the corpus that we
adopt is binary tree. Though the rule set in binary
tree is closed, it brings stronger independent as-
sumption (Jonson,1998). Thus splitting the label
can make the node label represent more informa-
tion from descendants. Just like the intuition of
head position labeling, this is also one method to
utilize the non-local information. We mainly con-
sider these modifying as follows.
First of all, we split the label vp. There are
three kinds of verb phrases: the first one is that
there is modifier ahead (such as advp); the second
phrase consists of an object; while the third one
has the form of two verbs or verb plus an auxil-
iary word. The formal two situations can not fol-
low any object any more (some double-object verb
phrase may be continued to contain object, but
their POS label is different with common verb),
the vp in last situation can be followed by object
(there maybe actually no object). If we do not dis-
criminate these situations, it will be easy to result
in dividing the object into two objects during pars-
ing test, just as shown in Figure 2. However, if
we modify vp in the third situation into vb, then
this difference can be discriminated well. We take
a simple statistics as an example to illustrate the
sense. Assume our object is np, rule “vp → vp
np” appears for 5,284 times in corpus before mod-
ifying, while it present only 166 times after mod-
ifying.
Figure 2: Parsing Result Example: (a) is a correct
tree, (b) is a wrong one, while the probability may
be not small enough, (c) is also wrong, but the
probability is very small due to the symbol vb.
Secondly, we also split the np tag. We notice
that a noun phrase, which consists of non-noun
(phrase) modifier (such as ADJP, PP) and a noun
(phrase), is always the final noun phrase but rarely
part of another noun phrase. So we transform the
np, which has the non-noun (phrase) modifier, to
nm. From the statistics of corpus, we find rule “np
→ np n” occurs for 4,502 times, while “np → nm
n” only appears 826 times.
Finally, we change the head position of prepo-
sition phrase. The head position of preposition
phrases in corpus mostly is the phrase behind the
preposition, but we found the grammar of prepo-
sition phrase is much related to the preposition.
Take the preposition “�k(by)” and “%&apos;(to)” as ex-
ample, these two prepositions occur for 755 and
1,300 times respectively. In our corpus, 98.7% of
preposition phrases with �k(by)” are the modifiers
of verb phrases, while only 57.2% of phrases with
“%&apos;(to)” appear as the modifiers of verb phrases,
and the remaining 42.8% are the modifiers of noun
phrases.
</bodyText>
<sectionHeader confidence="0.970321" genericHeader="method">
3 Experiment Result and Analysis
</sectionHeader>
<bodyText confidence="0.999921576923077">
Our experiments are conducted on the TCT cor-
pus, which is used as the standard data of the
CIPS-SIGHAN Parser 2010 bakeoff. We omit the
sentences with length 1 during training and test-
ing. Performance on the test corpus is evaluated
with the standard measures from (SIGHAN RE-
PORT, 2010).
We submit two results for the parsing bakeoff:
one is single model we described in Section 2, an-
other is reranking model, which is an attempt to
apply a perceptron algorithm to rerank the 50-best
result produced by the ME model.1 Table 2 shows
the result of our parser compared with the top one
in this bakeoff. Since the parser we built is strictly
dependent on the POS tags, the precision of POS
tagging has a harsh effect on the overall parsing
performance.
The performance of the rerank model is lightly
lower than that of the single model. The most
likely reason is that the features we count on
are far from enough, and the informative features
proved to be useful in (Charniak and Johnson,
2003) are not yet included in our discriminative
ranker. Besides, the rank model we used is a
simple perceptron learner, more delicated model,
such as ME model used in (Charniak and Johnson,
</bodyText>
<footnote confidence="0.97238675">
1More details can be found in (Charniak and Johnson,
2003; Huang, 2008). The features we used include Paren-
tRule, RightBranch, Rule, Heads, WProj described in (Char-
niak and Johnson, 2003).
</footnote>
<tableCaption confidence="0.984657">
Table 3: Results of different features with no limit
sentence length.
</tableCaption>
<table confidence="0.946781166666667">
feature set LR LP F CB 0CB 2CB
basic 80.19 79.61 79.90 1.20 56.10 83.49
+ch 81.91 81.38. 81.65 1.10 58.34 84.95
+cont 85.53 85.34 85.44 0.83 65.62 88.86
+ch+cont 86.17 85.94 86.06 0.80 66.61 89.62
+ch + cont + ol 86.34 86.13 86.24 0.79 66.65 89.81
+ch + cont + ol + cwd 86.47 86.26 86.37 0.78 66.73 89.87
+ch + cont + ol + cwd + cm 87.03 86.77 86.90 0.75 67.06 90.36
+ch + cont + ol + cwd + cm + hpl 87.20 86.94 87.07 0.74 67.43 90.40
ch=character feature, cont=context feature
ol=overlap feature, cwd=coordinate word dependence
cm=corpus modifying, hpl= head position label
</table>
<bodyText confidence="0.997998259259259">
2003), might improve the result.
In order to make clear how different features
effect the parser performance, we conducted ex-
periments on the TCT data provided by CIPS-
ParEval-2009 for Chinese parser bakeoff 2, since
the sentences in CIPSParEval-2009 are given with
head words and gold-standard POS tags. The re-
sults of our parser are given in Table 3. From Ta-
ble 3 we can see that character features bring the
improvement of F score for 1.75 compared with
the basic features (line 2 vs line 3), and for 0.8
after adding the context features (line 4 vs line
6). These results show that character features can
improve the model with basic features very well.
After applying the context features, character fea-
tures can still bring improvement, which states
that character features can solve the ambiguities
that can not be solved by the context features.
One likely reason why character information is
helpful is that the character can partly represent
the meaning of word and can partly resolve the
sparseness problem of word dependence as been
observed in the work of (Kang, 2005). Kang cal-
culated the statistics for 50,000 double characters
words and divided the methods of constructing
word into 8 types according to the relations of
meaning between word and characters:
</bodyText>
<listItem confidence="0.86226075">
(1) A+B=A=B (2) A+B=A
(3) A+B=B (4) A+B=C
(5) A+B=A+B (6) A+B=A+B+D
(7) A+B=A+D (8) A+B=D+B
</listItem>
<bodyText confidence="0.871789666666667">
A and B stand for the meaning of the two char-
acters which are used to construct the word. C
is a totally new meaning and D represents an ad-
</bodyText>
<footnote confidence="0.838953">
2http://www.ncmmsc.org/CIPS-ParsEval-
2009/index.asp, the first workshop on Chinese Syntactic
Parsing Evaluation, November, 2009.
</footnote>
<tableCaption confidence="0.993495">
Table 2: Results of different features with no limit sentence length.
</tableCaption>
<table confidence="0.999914">
“B+C”-P “B+C”-R “B+C”-F1 “B+C+H”-P “B+C+H”-R “B+C+H”-F1 POS-P
Top one 85.42 85.35 85.39 83.69 83.63 83.66 93.96
Single 74.86 76.05 75.45 71.06 72.20 71.63 87.00
Rerank 74.48 75.64 75.05 70.72 71.81 71.26 87.00
</table>
<tableCaption confidence="0.989445">
Table 4: The relation between the meaning of
words and characters.
</tableCaption>
<table confidence="0.995368666666667">
type 1 2 3 4 5 6 7 8
word number 4035 1031 297 4201 14455 23562 2780 1886
rate(%) 7.71 1.97 0.57 8.02 27.60 44.99 5.31 3.60
</table>
<bodyText confidence="0.9956938">
ditional meaning. The expression after the first
” is the meaning of the word, and the symbol
“+” indicates the melding of meaning. For exam-
ple, A+B=A+D indicates that the word retains the
meaning of character A, and adds new meaning
D. The distribution of each type in the dataset is
shown in Table 4. From Table 4 we can see that
type 4, i.e., there are no relation between charac-
ters and word, occupies only 8.02%. This data
proves that the word inherits the meaning from the
characters which are used to construct the word.
However, the relations are really complicated. For
example, some words only inherit the meaning of
formal characters and others of the last characters.
This might be the reason why character informa-
tion does not have very obvious effect as expected.
In our parsing model, context features are re-
ally helpful to the parsing accuracy. Different with
the decision method in (Rantnaparkhi, 1999) and
(Wang, 2006), and reranking in (Collins, 2000)
which all can utilize the context of current sub-
tree very well (not only the POS tag), the CYK
decoding algorithm restricts our context features.
However, we can conveniently exploit the POS
tags around the current subtree without increas-
ing the complexity of decoding and thus improve
the performance.
Commonly, each subtree has only one head
word. However, we notice that the two head
words of two coordinate children are equivalent,
as illustrated in Figure 3. We assume that the par-
ent node of these two children is A and the two
head word are all the head words of A. When
A is the child of the parent node B, all the head
words in A can be dependent with the other head
</bodyText>
<figureCaption confidence="0.9372635">
Figure 3: Example of dependence between coor-
dinate words.
</figureCaption>
<bodyText confidence="0.999654541666667">
words of another child C. When A is still the
head child of B, the head words of B are also the
same as A. Then we can extract more word depen-
dence data. For example, A have two head words
“ A(construct)” and “5AG&apos;�(complete)”, and “*j
ft(rule)” is the head word of C, then we consider
that “ A(construct)” and “5AG&apos;*(complete)” are
all dependent with “Oft(rule)”. Meanwhile, A is
also the head child of B, and the head words of B
are also “ A(construct)” and “5AG&apos;�(complete)”.
During the decoding, we choose the most proba-
ble dependence as the dependence probability of
B. From the result, we can see that this strategy
yields 0.17 improvements in the F score.
Label splitting can also improve the perfor-
mance. However, modifying the labels need
much linguistic knowledge and manual work.
(Petrov, 2006) proposed an automated splitting
and merging method. As an attempt, we tested
the effectiveness of it in our parser empirically.
When tested on the TCT data provided by CIPS-
ParsEval-2009 for Chinese parser, bakeoff the la-
bel spitting improve the F1 measure from 0.864 to
0.869.
</bodyText>
<sectionHeader confidence="0.994369" genericHeader="evaluation">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.996582388888889">
This paper presents a new lexical PCFG model,
which can synthetically exploit the word and char-
acter information. The results of experiment
prove the effectiveness of character information.
“=
Also our model can utilize the context features
and some non-local features which can dramati-
cally improve the performance.
In future work, we need improve the decod-
ing algorithm to exploit more complex features.
As the parser we build is greatly dependent on
the preprocessing result of word segmentation,
POS tagging and head labeling, a critical direc-
tion of future work is to do word-segmentation,
POS tagging, head detection and parsing in a uni-
fied framework. Besides, as for the K-best rerank-
ing, we should take into account more informative
features and more powerful reranking model.
</bodyText>
<sectionHeader confidence="0.970945" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9993582">
This research has been partially supported by
the National Science Foundation of China (NO.
NSFC90920006). We also thank Xiaojie Wang,
Huixing Jiang, Jiashen Sun and Bichuan Zhang
for useful discussion of this work.
</bodyText>
<sectionHeader confidence="0.998171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999572098591549">
A.L. Beger, S. A. D Pietra, and V.J.D Pietra. 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 39–71.
D.M. Bikel and D. Chiang. 2000. Two statistical pars-
ing models applied to the Chinese Treebank. In Pro-
ceedings of the Second Chinese Language Process-
ing Workshop, 1–6.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st NAACL, Seattle,
WA, 132–139.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceddings
of the Fourteenth National Conference on Artificial
Intelligence, Menlo Park, CA. 598–603.
X. Chen, C.N. Huang, M. Li, and C.Y. Kit. 2009. Bet-
ter Parser Combination. In CIPS ParsEval, Beijing,
China. 81–90.
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. Dissertation,
University of Pennsylvania.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of ICML 2000,
175–182.
S.Y. Kang, X.X. Xu, and M.S. Sun. 2005. The
Research on the Modern Chinese Semantic Word-
Formation. Journal of Chinese Language and Com-
puting, 103–112.
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of ACL 2003, 423–
430.
L. Huang. 2008. Forest Reranking: Discriminative
Parsing with Non-Local Features. In Proceedings
of ACL 2008, 586–594.
D. Klein and C.D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL 2003, 423–
430.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of ACL 2005, 173–180.
R. Levy and C.D. Manning. 2003. Is it harder to parse
Chinese, or the Chinese Treebank? In Proceedings
of ACL 2003, 439–446.
X.Q. Luo. 2003. A maximum entropy Chinese
character-based parser. In Proceedings of EMNLP
2003, 192–199.
M. Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 613–
632.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of ACL 2006, 433–440.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of EMNLP 1996,
133–142.
A. Ratnaparkhi. 1999. Learning to parse natural lan-
guage with maximum entropy models. Machine
Learning, 503–512.
M.W. Wang, K. Sagae, and T. Mitamura. 2006. A
Fast, Accurate Deterministic Parser for Chinese. In
Proceedings of ACL 2006, 425–432.
L. Zhang. 2004. Reference Manual. Maximum En-
tropy Modeling Toolkit for Python and C++.
H. Zhao. 2009. Character-Level Dependencies in Chi-
nese: Usefulness and Learning. In Proceedings of
12th ECACL 2009, 879–887.
SIGHAN REPORT. 2010. SIGHAN REPORT ON
TASK2. In Proceedings of CIPS-SIGHAN 2010,
Beijing, China.
Q. Zhou. 2004. Annotation Scheme for Chinese Tree-
bank. Journal of Chinese Information Processing,
1–8.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.913618">
<title confidence="0.999567">A Chinese LPCFG Parser with Hybrid Character Information</title>
<author confidence="0.982951">Wenzhi Xu</author>
<author confidence="0.982951">Chaobo Sun</author>
<author confidence="0.982951">Caixia</author>
<affiliation confidence="0.9976645">School of Beijing University of Posts and</affiliation>
<address confidence="0.998673">Beijing, 100876</address>
<email confidence="0.944749">yuancx@bupt.edu.cn</email>
<abstract confidence="0.999151785714286">We present a new probabilistic model based on the lexical PCFG model, which can easily utilize the Chinese character information to solve the lexical information sparseness in lexical PCFG model. We discuss in particular some important features that can improve the parsing performance, and describe the strategy of modifying original label structure to reduce the label ambiguities. Final experiment demonstrates that the character information and label modification improve the parsing performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Beger</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>39--71</pages>
<marker>Beger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Beger, S. A. D Pietra, and V.J.D Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>D Chiang</author>
</authors>
<title>Two statistical parsing models applied to the Chinese Treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Chinese Language Processing Workshop,</booktitle>
<volume>1</volume>
<marker>Bikel, Chiang, 2000</marker>
<rawString>D.M. Bikel and D. Chiang. 2000. Two statistical parsing models applied to the Chinese Treebank. In Proceedings of the Second Chinese Language Processing Workshop, 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="7335" citStr="Charniak, 2000" startWordPosition="1220" endWordPosition="1221"> * Prh(v|vp,| „,v) * Prm(np-RIGHT |vp, v, |„,v). 2.2 Probability Model and Feature Set We use Maximum Entropy (ME) Model to compute probabilities of candidate trees. ME model estimate parameters that would maximize the entropy over distributions, meanwhile satisfy certain constraints. These constraints will force the model to reflect characteristic of training data. With the feature function, Maximum Entropy can exploit kinds of features flexibly, some of which are very important to improve the performance of tasks at hand. ME model has been applied successfully in many tasks, such as parser (Charniak, 2000; Luo, 2003), POS tagging (Ratnaparkhi,1996), etc. In our experiment, we use Maxent toolkit developped by Zhang (Zhang, 2004), which uses the LBFGS algorithm for parameter estimation. Details of the model and toolkit can be seen in (Berger, 1996; Zhang, 2004). Our features consist of four parts: basic features, character features, context features and overlapping features of character and context. Basic features are traditional LPCFG features, including head word, head tag and the label. We extract the first and last characters of a word as the Character features, of course for a single charac</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st NAACL, Seattle, WA, 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a contextfree grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceddings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>598--603</pages>
<location>Menlo Park, CA.</location>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical parsing with a contextfree grammar and word statistics. In Proceddings of the Fourteenth National Conference on Artificial Intelligence, Menlo Park, CA. 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Chen</author>
<author>C N Huang</author>
<author>M Li</author>
<author>C Y Kit</author>
</authors>
<title>Better Parser Combination.</title>
<date>2009</date>
<booktitle>In CIPS ParsEval,</booktitle>
<location>Beijing,</location>
<marker>Chen, Huang, Li, Kit, 2009</marker>
<rawString>X. Chen, C.N. Huang, M. Li, and C.Y. Kit. 2009. Better Parser Combination. In CIPS ParsEval, Beijing, China. 81–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. Dissertation,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3659" citStr="Collins, 1999" startWordPosition="561" endWordPosition="562"> to binary trees. Then we use Lexical-PCFG model to exploit the word and character information, and Maximum Entropy Model to calculate the probability of induced trees as (Charinak, 2000). Finally we use CKY-based decoder. In the following section, we will introduce how to utilize character information in our parsing model and the other features in detail. Section 3 gives experiment results and analysis, which show improvement of our parsing approach. Section 4 presents the conclusion and future work. 2 Lexical PCFG model 2.1 Model Introduction Starting from the Lexical-PCFG model (Model 2 in Collins, 1999), we propose a new generative process which can conveniently exploit the character information and other features. Assume P is the label of parent, H is the head child of the rule, and L1,..., Ln and R1, ..., Rn is the left and right modifiers of H. Then the rule of Lexical-PCFG (LPCFG) can be written as: P(hw, ht) --+ (1) Ln(lwn, ltn)...L1(lw1, lt1)H(hw, ht) R1(rw1, rt1)...Rn(rwn, rtn), where (hw, ht) represents the head word and head tag of head child, (lw1, lt1), ..., (lwn, ltn) and (rw1, rt1), ..., (rwn, rtn) are the head words and head tags of left and right modifiers, and parent node P’s</context>
<context position="10093" citStr="Collins, 1999" startWordPosition="1689" endWordPosition="1690">an modify the head child label to get this non-local information in binary tree. For instance, a multi-branche tree rule “vp1 —* pp d vp2” corresponding to these two binary tree rules: “vp3 —* pp vp4” and “vp4 —* d vp5” (the index numbers of the vb here stand for different vp). So when we calculate the probability of pp with the multi-branch situations, d lies between pp and vp2. While in binary tree situation, we cannot catch this information between pp and vp4. However, we can modify vp4 to vp-LEFT, which means there is a modifier at the left child of vp4, then we get the similar effect in (Collins, 1999). We call this as the head position labeling. 2.3 Label splitting and Head Position Modifying (Klein, 2003) improves the performance of parser via splitting the POS tag in corpus. We split the non-terminal label using the same approaches (assuming the POS tag is terminal label). The need of label splitting is that the corpus does not sufficiently consider different situations and treat them Table 1: Feature templates and symbol explanation. Prd (vp-LEFT) Prh (v) Prm (np-RIGHT) basic lw rw |„;* p vp p h vp v feature lw lt rw rt |„v ;*n p hw vp |„ p h hw vp v|„ phwht vp |„v phhwht vp v |„v pht v</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. Dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML 2000,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="19485" citStr="Collins, 2000" startWordPosition="3408" endWordPosition="3409">no relation between characters and word, occupies only 8.02%. This data proves that the word inherits the meaning from the characters which are used to construct the word. However, the relations are really complicated. For example, some words only inherit the meaning of formal characters and others of the last characters. This might be the reason why character information does not have very obvious effect as expected. In our parsing model, context features are really helpful to the parsing accuracy. Different with the decision method in (Rantnaparkhi, 1999) and (Wang, 2006), and reranking in (Collins, 2000) which all can utilize the context of current subtree very well (not only the POS tag), the CYK decoding algorithm restricts our context features. However, we can conveniently exploit the POS tags around the current subtree without increasing the complexity of decoding and thus improve the performance. Commonly, each subtree has only one head word. However, we notice that the two head words of two coordinate children are equivalent, as illustrated in Figure 3. We assume that the parent node of these two children is A and the two head word are all the head words of A. When A is the child of the</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of ICML 2000, 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Y Kang</author>
<author>X X Xu</author>
<author>M S Sun</author>
</authors>
<title>The Research on the Modern Chinese Semantic WordFormation.</title>
<date>2005</date>
<journal>Journal of Chinese Language and Computing,</journal>
<pages>103--112</pages>
<marker>Kang, Xu, Sun, 2005</marker>
<rawString>S.Y. Kang, X.X. Xu, and M.S. Sun. 2005. The Research on the Modern Chinese Semantic WordFormation. Journal of Chinese Language and Computing, 103–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>423--430</pages>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL 2003, 423– 430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Forest Reranking: Discriminative Parsing with Non-Local Features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL 2008,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="15649" citStr="Huang, 2008" startWordPosition="2753" endWordPosition="2754">ly dependent on the POS tags, the precision of POS tagging has a harsh effect on the overall parsing performance. The performance of the rerank model is lightly lower than that of the single model. The most likely reason is that the features we count on are far from enough, and the informative features proved to be useful in (Charniak and Johnson, 2003) are not yet included in our discriminative ranker. Besides, the rank model we used is a simple perceptron learner, more delicated model, such as ME model used in (Charniak and Johnson, 1More details can be found in (Charniak and Johnson, 2003; Huang, 2008). The features we used include ParentRule, RightBranch, Rule, Heads, WProj described in (Charniak and Johnson, 2003). Table 3: Results of different features with no limit sentence length. feature set LR LP F CB 0CB 2CB basic 80.19 79.61 79.90 1.20 56.10 83.49 +ch 81.91 81.38. 81.65 1.10 58.34 84.95 +cont 85.53 85.34 85.44 0.83 65.62 88.86 +ch+cont 86.17 85.94 86.06 0.80 66.61 89.62 +ch + cont + ol 86.34 86.13 86.24 0.79 66.65 89.81 +ch + cont + ol + cwd 86.47 86.26 86.37 0.78 66.73 89.87 +ch + cont + ol + cwd + cm 87.03 86.77 86.90 0.75 67.06 90.36 +ch + cont + ol + cwd + cm + hpl 87.20 86.94 </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>L. Huang. 2008. Forest Reranking: Discriminative Parsing with Non-Local Features. In Proceedings of ACL 2008, 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>423--430</pages>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL 2003, 423– 430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL 2005,</booktitle>
<pages>173--180</pages>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and MaxEnt discriminative reranking. In Proceedings of ACL 2005, 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>C D Manning</author>
</authors>
<title>Is it harder to parse Chinese, or the Chinese Treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of ACL 2003,</booktitle>
<pages>439--446</pages>
<marker>Levy, Manning, 2003</marker>
<rawString>R. Levy and C.D. Manning. 2003. Is it harder to parse Chinese, or the Chinese Treebank? In Proceedings of ACL 2003, 439–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Q Luo</author>
</authors>
<title>A maximum entropy Chinese character-based parser.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP 2003,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="1711" citStr="Luo, 2003" startWordPosition="254" endWordPosition="255">rsing through comparing the differences between Chinese and English. (Wang, 2006) utilizes shift-reduce approach, dramatically improved the decoding speed of parsing. All these research adopted the same models which are also used in English parser – the models based on the words. However, there is a big difference between English and Chinese: the expressing unit in English is word, while character is the smallest unit in Chinese. Due to difficulties of word segmentation, especially for different segmenting criteria, many researchers explored parsing Chinese based on characters. The parser of (Luo, 2003) received sentence as input and conducted word segmentation and syntactic parsing at the same time, but they did not utilize the character information in generating subtree; (Zhao, 2009)’s dependency parsing tree totally abandoned the word concept, so the dependency relations are the relations between characters. We combine both word and character information to gain better performance of parsing. Although the criteria of segmentation are difficult to be unified, different criteria conflict only within the phrases which have little influence on the structure between phrases. So we still use wo</context>
<context position="7347" citStr="Luo, 2003" startWordPosition="1222" endWordPosition="1223">v) * Prm(np-RIGHT |vp, v, |„,v). 2.2 Probability Model and Feature Set We use Maximum Entropy (ME) Model to compute probabilities of candidate trees. ME model estimate parameters that would maximize the entropy over distributions, meanwhile satisfy certain constraints. These constraints will force the model to reflect characteristic of training data. With the feature function, Maximum Entropy can exploit kinds of features flexibly, some of which are very important to improve the performance of tasks at hand. ME model has been applied successfully in many tasks, such as parser (Charniak, 2000; Luo, 2003), POS tagging (Ratnaparkhi,1996), etc. In our experiment, we use Maxent toolkit developped by Zhang (Zhang, 2004), which uses the LBFGS algorithm for parameter estimation. Details of the model and toolkit can be seen in (Berger, 1996; Zhang, 2004). Our features consist of four parts: basic features, character features, context features and overlapping features of character and context. Basic features are traditional LPCFG features, including head word, head tag and the label. We extract the first and last characters of a word as the Character features, of course for a single character word the</context>
</contexts>
<marker>Luo, 2003</marker>
<rawString>X.Q. Luo. 2003. A maximum entropy Chinese character-based parser. In Proceedings of EMNLP 2003, 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>613</volume>
<pages>632</pages>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 613– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL 2006,</booktitle>
<pages>433--440</pages>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of ACL 2006, 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-ofspeech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>133--142</pages>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy part-ofspeech tagger. In Proceedings of EMNLP 1996, 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>503--512</pages>
<marker>Ratnaparkhi, 1999</marker>
<rawString>A. Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 503–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Wang</author>
<author>K Sagae</author>
<author>T Mitamura</author>
</authors>
<title>A Fast, Accurate Deterministic Parser for Chinese.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL 2006,</booktitle>
<pages>425--432</pages>
<marker>Wang, Sagae, Mitamura, 2006</marker>
<rawString>M.W. Wang, K. Sagae, and T. Mitamura. 2006. A Fast, Accurate Deterministic Parser for Chinese. In Proceedings of ACL 2006, 425–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhang</author>
</authors>
<title>Reference Manual. Maximum Entropy Modeling Toolkit for Python and C++.</title>
<date>2004</date>
<contexts>
<context position="7460" citStr="Zhang, 2004" startWordPosition="1239" endWordPosition="1240">pute probabilities of candidate trees. ME model estimate parameters that would maximize the entropy over distributions, meanwhile satisfy certain constraints. These constraints will force the model to reflect characteristic of training data. With the feature function, Maximum Entropy can exploit kinds of features flexibly, some of which are very important to improve the performance of tasks at hand. ME model has been applied successfully in many tasks, such as parser (Charniak, 2000; Luo, 2003), POS tagging (Ratnaparkhi,1996), etc. In our experiment, we use Maxent toolkit developped by Zhang (Zhang, 2004), which uses the LBFGS algorithm for parameter estimation. Details of the model and toolkit can be seen in (Berger, 1996; Zhang, 2004). Our features consist of four parts: basic features, character features, context features and overlapping features of character and context. Basic features are traditional LPCFG features, including head word, head tag and the label. We extract the first and last characters of a word as the Character features, of course for a single character word the first and last character are the same. Context features are defined as the previous and following POS tags of th</context>
</contexts>
<marker>Zhang, 2004</marker>
<rawString>L. Zhang. 2004. Reference Manual. Maximum Entropy Modeling Toolkit for Python and C++.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhao</author>
</authors>
<title>Character-Level Dependencies in Chinese: Usefulness and Learning.</title>
<date>2009</date>
<booktitle>In Proceedings of 12th ECACL 2009,</booktitle>
<pages>879--887</pages>
<contexts>
<context position="1897" citStr="Zhao, 2009" startWordPosition="283" endWordPosition="284"> adopted the same models which are also used in English parser – the models based on the words. However, there is a big difference between English and Chinese: the expressing unit in English is word, while character is the smallest unit in Chinese. Due to difficulties of word segmentation, especially for different segmenting criteria, many researchers explored parsing Chinese based on characters. The parser of (Luo, 2003) received sentence as input and conducted word segmentation and syntactic parsing at the same time, but they did not utilize the character information in generating subtree; (Zhao, 2009)’s dependency parsing tree totally abandoned the word concept, so the dependency relations are the relations between characters. We combine both word and character information to gain better performance of parsing. Although the criteria of segmentation are difficult to be unified, different criteria conflict only within the phrases which have little influence on the structure between phrases. So we still use word as our basic unit of parsing. Although word has been proved to be effective in head-driven parser (Collins,1999), the data of word dependence is very sparse. While it is worthy to not</context>
</contexts>
<marker>Zhao, 2009</marker>
<rawString>H. Zhao. 2009. Character-Level Dependencies in Chinese: Usefulness and Learning. In Proceedings of 12th ECACL 2009, 879–887.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SIGHAN REPORT</author>
</authors>
<date>2010</date>
<booktitle>SIGHAN REPORT ON TASK2. In Proceedings of CIPS-SIGHAN 2010,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="14688" citStr="REPORT, 2010" startWordPosition="2583" endWordPosition="2585">wo prepositions occur for 755 and 1,300 times respectively. In our corpus, 98.7% of preposition phrases with �k(by)” are the modifiers of verb phrases, while only 57.2% of phrases with “%&apos;(to)” appear as the modifiers of verb phrases, and the remaining 42.8% are the modifiers of noun phrases. 3 Experiment Result and Analysis Our experiments are conducted on the TCT corpus, which is used as the standard data of the CIPS-SIGHAN Parser 2010 bakeoff. We omit the sentences with length 1 during training and testing. Performance on the test corpus is evaluated with the standard measures from (SIGHAN REPORT, 2010). We submit two results for the parsing bakeoff: one is single model we described in Section 2, another is reranking model, which is an attempt to apply a perceptron algorithm to rerank the 50-best result produced by the ME model.1 Table 2 shows the result of our parser compared with the top one in this bakeoff. Since the parser we built is strictly dependent on the POS tags, the precision of POS tagging has a harsh effect on the overall parsing performance. The performance of the rerank model is lightly lower than that of the single model. The most likely reason is that the features we count </context>
</contexts>
<marker>REPORT, 2010</marker>
<rawString>SIGHAN REPORT. 2010. SIGHAN REPORT ON TASK2. In Proceedings of CIPS-SIGHAN 2010, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhou</author>
</authors>
<title>Annotation Scheme for Chinese Treebank.</title>
<date>2004</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>1</volume>
<contexts>
<context position="2901" citStr="Zhou, 2004" startWordPosition="442" endWordPosition="443">s. So we still use word as our basic unit of parsing. Although word has been proved to be effective in head-driven parser (Collins,1999), the data of word dependence is very sparse. While it is worthy to note that words with similar concept always share the same characters in Chinese. For instance, “W� *(scientist)”, “fjP_?j�*(historian)”, etc., share the same character “*(expert)”, since they belong to the same concept “expert in a certain field”. So the problem of word sparseness can be solved by combining the character information to some extent. Throughout this paper, we use TCT Treebank (Zhou, 2004) as experimental data. TCT mainly consists of binary trees, with a few of multibranch and single-branch trees. Thus, we first transfer all trees to binary trees. Then we use Lexical-PCFG model to exploit the word and character information, and Maximum Entropy Model to calculate the probability of induced trees as (Charinak, 2000). Finally we use CKY-based decoder. In the following section, we will introduce how to utilize character information in our parsing model and the other features in detail. Section 3 gives experiment results and analysis, which show improvement of our parsing approach. </context>
</contexts>
<marker>Zhou, 2004</marker>
<rawString>Q. Zhou. 2004. Annotation Scheme for Chinese Treebank. Journal of Chinese Information Processing, 1–8.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>