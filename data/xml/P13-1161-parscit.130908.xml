<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000067">
<title confidence="0.992472">
Joint Inference for Fine-grained Opinion Extraction
</title>
<author confidence="0.997797">
Bishan Yang
</author>
<affiliation confidence="0.9943685">
Department of Computer Science
Cornell University
</affiliation>
<email confidence="0.991331">
bishan@cs.cornell.edu
</email>
<author confidence="0.992355">
Claire Cardie
</author>
<affiliation confidence="0.994383">
Department of Computer Science
Cornell University
</affiliation>
<email confidence="0.994382">
cardie@cs.cornell.edu
</email>
<sectionHeader confidence="0.993793" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982476190476">
This paper addresses the task of fine-
grained opinion extraction – the identi-
fication of opinion-related entities: the
opinion expressions, the opinion hold-
ers, and the targets of the opinions, and
the relations between opinion expressions
and their targets and holders. Most ex-
isting approaches tackle the extraction
of opinion entities and opinion relations
in a pipelined manner, where the inter-
dependencies among different extraction
stages are not captured. We propose a joint
inference model that leverages knowledge
from predictors that optimize subtasks
of opinion extraction, and seeks a glob-
ally optimal solution. Experimental re-
sults demonstrate that our joint inference
approach significantly outperforms tradi-
tional pipeline methods and baselines that
tackle subtasks in isolation for the problem
of opinion extraction.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999623454545455">
Fine-grained opinion analysis is concerned with
identifying opinions in text at the expression level;
this includes identifying the subjective (i.e., opin-
ion) expression itself, the opinion holder and the
target of the opinion (Wiebe et al., 2005). The
task has received increasing attention as many nat-
ural language processing applications would ben-
efit from the ability to identify text spans that cor-
respond to these key components of opinions. In
question-answering systems, for example, users
may submit questions in the form “What does en-
tity A think about target B?”; opinion-oriented
summarization systems also need to recognize
opinions and their targets and holders.
In this paper, we address the task of identifying
opinion-related entities and opinion relations. We
consider three types of opinion entities: opinion
expressions or direct subjective expressions as de-
fined in Wiebe et al. (2005) — expressions that ex-
plicitly indicate emotions, sentiment, opinions or
other private states (Quirk et al., 1985) or speech
events expressing private states; opinion targets
— expressions that indicate what the opinion is
about; and opinion holders — mentions of whom
or what the opinion is from. Consider the follow-
ing examples in which opinion expressions (O) are
underlined and targets (T) and holders (H) of the
opinion are bracketed.
S1: [The workers][H1,2] were irked[O1]
by [the government report][T1] and
were worried[O2] as they went about
their daily chores.
S2: From the very start it could be
predicted[O1] that on the subject of
economic globalization, [the developed
states][T1,2] were going to come across
fierce opposition[O2].
The numeric subscripts denote linking relations,
one of IS-ABOUT or IS-FROM. In S1, for in-
stance, opinion expression “were irked” (O1) IS-
ABOUT “the government report” (T1). Note that
the IS-ABOUT relation can contain an empty tar-
get (e.g. “were worried” in S1); similarly for IS-
FROM w.r.t. the opinion holder (e.g. “predicted” in
S2). We also allow an opinion entity to be involved
in multiple relations (e.g. “the developed states” in
S2).
Not surprisingly, fine-grained opinion extrac-
tion is a challenging task due to the complexity
and variety of the language used to express opin-
ions and their components (Pang and Lee, 2008).
Nevertheless, much progress has been made in ex-
tracting opinion information from text. Sequence
labeling models have been successfully employed
to identify opinion expressions (e.g. (Breck et al.,
</bodyText>
<page confidence="0.913789">
1640
</page>
<note confidence="0.9146845">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640–1649,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998762755555556">
2007; Yang and Cardie, 2012)) and relation ex-
traction techniques have been proposed to extract
opinion holders and targets based on their link-
ing relations to the opinion expressions (e.g. Kim
and Hovy (2006), Kobayashi et al. (2007)). How-
ever, most existing work treats the extraction of
different opinion entities and opinion relations in a
pipelined manner: the interaction between differ-
ent extraction tasks is not modeled jointly and er-
ror propagation is not considered. One exception
is Choi et al. (2006), which proposed an ILP ap-
proach to jointly identify opinion holders, opinion
expressions and their IS-FROM linking relations,
and demonstrated the effectiveness of joint infer-
ence. Their ILP formulation, however, does not
handle implicit linking relations, i.e. opinion ex-
pressions with no explicit opinion holder; nor does
it consider IS-ABOUT relations.
In this paper, we present a model that jointly
identifies opinion-related entities, including opin-
ion expressions, opinion targets and opinion hold-
ers as well as the associated opinion linking rela-
tions, IS-ABOUT and IS-FROM. For each type of
opinion relation, we allow implicit (i.e. empty) ar-
guments for cases when the opinion holder or tar-
get is not explicitly expressed in text. We model
entity identification as a sequence tagging prob-
lem and relation extraction as binary classifica-
tion. A joint inference framework is proposed to
jointly optimize the predictors for different sub-
problems with constraints that enforce global con-
sistency. We hypothesize that the ambiguity in
the extraction results will be reduced and thus,
performance increased. For example, uncertainty
w.r.t. the spans of opinion entities can adversely
affect the prediction of opinion relations; and evi-
dence of opinion relations might provide clues to
guide the accurate extraction of opinion entities.
We evaluate our approach using a standard cor-
pus for fine-grained opinion analysis (the MPQA
corpus (Wiebe et al., 2005)) and demonstrate that
our model outperforms by a significant margin tra-
ditional baselines that do not employ joint infer-
ence for extracting opinion entities and different
types of opinion relations.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999845709090909">
Significant research effort has been invested into
fine-grained opinion extraction for open-domain
text such as news articles (Wiebe et al., 2005; Wil-
son et al., 2009). Many techniques were proposed
to identify the text spans for opinion expressions
(e.g. (Breck et al., 2007; Johansson and Moschitti,
2010b; Yang and Cardie, 2012)), opinion hold-
ers (e.g. (Choi et al., 2005)) and topics of opin-
ions (Stoyanov and Cardie, 2008). Some consider
extracting opinion targets/holders along with their
relation to the opinion expressions. Kim and Hovy
(2006) identifies opinion holders and targets by us-
ing their semantic roles related to opinion words.
Ruppenhofer et al. (2008) argued that semantic
role labeling is not sufficient for identifying opin-
ion holders and targets. Johansson and Moschitti
(2010a) extract opinion expressions and holders
by applying reranking on top of sequence label-
ing methods. Kobayashi et al. (2007) considered
extracting “aspect-evaluation” relations (relations
between opinion expressions and targets) by iden-
tifying opinion expressions first and then search-
ing for the most likely target for each opinion ex-
pression via a binary relation classifier. All these
methods extract opinion arguments and opinion
relations in separate stages instead of extracting
them jointly.
Most similar to our method is Choi et al. (2006),
which jointly extracts opinion expressions, hold-
ers and their IS-FROM relations using an ILP ap-
proach. In contrast, our approach (1) also consid-
ers the IS-ABOUT relation which is arguably more
complex due to the larger variety in the syntac-
tic structure exhibited by opinion expressions and
their targets, (2) handles implicit opinion relations
(opinion expressions without any associated argu-
ment), and (3) uses a simpler ILP formulation.
There has also been substantial interest in opin-
ion extraction from product reviews (Liu, 2012).
Most existing approaches focus on the extrac-
tion of opinion targets and their associated opin-
ion expressions and usually employ a pipeline
architecture: generate candidates of opinion ex-
pressions and opinion targets first, and then use
rule-based or machine-learning-based approaches
to identify potential relations between opinions
and targets (Hu and Liu, 2004; Wu et al., 2009;
Liu et al., 2012). In addition to pipeline ap-
proaches, bootstrapping-based approaches were
proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang
et al., 2010) to identify opinion expressions and
targets iteratively; however, they suffer from the
problem of error propagation.
There is much work demonstrating the bene-
fit of performing global inference. Roth and Yih
</bodyText>
<page confidence="0.984103">
1641
</page>
<bodyText confidence="0.9998574375">
(2004) proposed a global inference approach in the
formulation of a linear program (LP) and applied
it to the task of extracting named entities and re-
lations simultaneously. Their problem is similar
to ours — the difference is that Roth and Yih Roth
and Yih (2004) assume that named entity spans are
known a priori and only their labels need to be as-
signed. Joint inference has also been applied to
semantic role labeling (Punyakanok et al., 2008;
Srikumar and Roth, 2011; Das et al., 2012), where
the goal is to jointly identify semantic arguments
for given lexical predicates. The problem is con-
ceptually similar to identifying opinion arguments
for opinion expressions, however, we do not as-
sume prior knowledge of opinion expressions (un-
like in SRL, where predicates are given).
</bodyText>
<sectionHeader confidence="0.996426" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999965857142857">
As proposed in Section 1, we consider the task of
jointly identifying opinion entities and opinion re-
lations. Specifically, given a sentence, our goal is
to identify spans of opinion expressions, opinion
arguments (targets and holders) and their associ-
ated linking relations. Training data consists of
text with manually annotated opinion expression
and argument spans, each with a list of relation
ids specifying the linking relation between opin-
ion expressions and their arguments.
In this section, we will describe how we model
opinion entity identification and opinion relation
extraction, and how we combine them in a joint
inference model.
</bodyText>
<subsectionHeader confidence="0.999578">
3.1 Opinion Entity Identification
</subsectionHeader>
<bodyText confidence="0.990268578947369">
We formulate the task of opinion entity identifica-
tion as a sequence labeling problem and employ
conditional random fields (CRFs) (Lafferty et al.,
2001) to learn the probability of a sequence as-
signment y for a given sentence x. Through in-
ference we can find the best sequence assignment
for sentence x and recover the opinion entities ac-
cording to the standard “IOB” encoding scheme.
We consider four entity labels: D, T, H, N, where
D denotes opinion expressions, T denotes opinion
targets, H denotes opinion holders and N denotes
“NONE” entities.
We define potential function fiz that gives the
probability of assigning a span i with entity label
z, and the probability is estimated based on the
learned parameters from CRFs. Formally, given
a within-sentence span i = (a, b), where a is the
starting position and b is the end position, and la-
bel z E {D, T, H}, we have
</bodyText>
<equation confidence="0.999587666666667">
fiz = p(ya = Bz, ya+1 = Iz, ...,
yb = Iz, yb+1 =� Iz|x)
fiN = p(ya = O, ..., yb = O|x)
</equation>
<bodyText confidence="0.999405">
These probabilities can be efficiently computed
using the forward-backward algorithm.
</bodyText>
<subsectionHeader confidence="0.999569">
3.2 Opinion Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999988647058824">
We consider extracting the IS-ABOUT and IS-
FROM opinion relations. In the following we will
not distinguish these two relations, since they can
both be characterized as relations between opinion
expressions and opinion arguments, and the meth-
ods for relation extraction are the same.
We treat the relation extraction problem as a
combination of two binary classification prob-
lems: opinion-arg classification, which decides
whether a pair consisting of an opinion candidate o
and an argument candidate a forms a relation; and
opinion-implicit-arg classification, which decides
whether an opinion candidate o is linked to an im-
plicit argument, i.e. no argument is mentioned. We
define a potential function r to capture the strength
of association between an opinion candidate o and
an argument candidate a,
</bodyText>
<equation confidence="0.737323">
roa = p(y = 1|x) − p(y = 0|x)
</equation>
<bodyText confidence="0.9999634">
where p(y = 1|x) and p(y = 0|x) are the logistic
regression estimates of the positive and negative
relations. Similarly, we define potential ro∅ to de-
note the confidence of predicting opinion span o
associated with an implicit argument.
</bodyText>
<subsectionHeader confidence="0.873562">
3.2.1 Opinion-Arg Relations
</subsectionHeader>
<bodyText confidence="0.9999791">
For opinion-arg classification, we construct can-
didates of opinion expressions and opinion argu-
ments and consider each pair of an opinion can-
didate and an argument candidate as a potential
opinion relation. Conceptually, all possible sub-
sequences in the sentence are candidates. To filter
out candidates that are less reasonable, we con-
sider the opinion expressions and arguments ob-
tained from the n-best predictions by CRFs1. We
also employ syntactic patterns from dependency
</bodyText>
<footnote confidence="0.7949694">
1We randomly split the training data into 10 parts and ob-
tained the 50-best CRF predictions on each part for the gen-
eration of candidates. We also experimented with candidates
generated from more CRF predictions, but did not find any
performance improvement for the task.
</footnote>
<page confidence="0.988526">
1642
</page>
<bodyText confidence="0.942820222222222">
trees to generate candidates. Specifically, we se-
lected the most common patterns of the shortest
dependency paths2 between an opinion candidate
o and an argument candidate a in our dataset, and
include all pairs of candidates that satisfy at least
one dependency pattern. For the IS-ABOUT rela-
tion, the top three patterns are (1) o Tdobj a, (2)
o Tccomp x Tnsubj a (x is a word in the path that is
not covered by either o nor a), (3) o Tccomp a; for
the IS-FROM relation, the top three patterns are (1)
o Tnsubj a, (2) o Tposs a, (3) o �ccomp x Tnsubj a.
Note that generating candidates this way will
give us a large number of negative examples. Sim-
ilar to the preprocessing approach in (Choi et al.,
2006), we filter pairs of opinion and argument can-
didates that do not overlap with any gold standard
relation in our training data.
Many features we use are common features
in the SRL tasks (Punyakanok et al., 2008)
due to the similarity of opinion relations to the
predicate-argument relations in SRL (Ruppen-
hofer et al., 2008; Choi et al., 2006). In general,
the features aim to capture (a) local properties of
the candidate opinion expressions and arguments
and (b) syntactic and semantic attributes of their
relation.
Words and POS tags: the words contained in the
candidate and their POS tags.
Lexicon: For each word in the candidate, we
include its WordNet hypernyms and its strength
of subjectivity in the Subjectivity Lexicon3
(e.g. weaksubj, strongsubj).
Phrase type: the syntactic category of the deepest
constituent that covers the candidate in the parse
tree, e.g. NP, VP.
Semantic frames: For each verb in the opinion
candidate, we include its frame types according to
FrameNet4.
Distance: the relative distance (number of words)
between the opinion and argument candidates.
Dependency Path: the shortest path in the
dependency tree between the opinion candidate
and the target candidate, e.g. ccompTnsubjT. We
also include word types and POS types in the
paths, e.g. opinionTccompsufferingTnsubjpatient,
</bodyText>
<footnote confidence="0.9312195">
2We use the Stanford Parser to generate parse trees and
dependency graphs.
3http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
4https://framenet.icsi.berkeley.edu/
fndrupal/
</footnote>
<bodyText confidence="0.99984525">
NNTccompVBGTnsubjNN. The dependency path
has been shown to be very useful in extracting
opinion expressions and opinion holders (Johans-
son and Moschitti, 2010a).
</bodyText>
<subsectionHeader confidence="0.620766">
3.2.2 Opinion-Implicit-Arg Relations
</subsectionHeader>
<bodyText confidence="0.988032121212121">
When the opinion-arg relation classifier predicts
that there is no suitable argument for the opinion
expression candidate, it does not capture the possi-
bility that an opinion candidate may associate with
an implicit argument. To incorporate knowledge
of implicit relations, we build an opinion-implicit-
arg classifier to identify an opinion candidate with
an implicit argument based on its own properties
and context information.
For training, we consider all gold-standard
opinion expressions as training examples —
including those with implicit arguments — as
positive examples and those associated with
explicit arguments as negative examples. For
features, we use words, POS tags, phrase types,
lexicon and semantic frames (see Section 3.2.1
for details) to capture the properties of the opinion
expression, and also features that capture the
context of the opinion expression:
Neighboring constituents: The words and gram-
matical roles of neighboring constituents of the
opinion expression in the parse tree — the left and
right sibling of the deepest constituent containing
the opinion expression in the parse tree.
Parent Constituent: The grammatical role of
the parent constituent of the deepest constituent
containing the opinion expression.
Dependency Argument: The word types and
POS types of the arguments of the dependency
patterns in which the opinion expression is
involved. We consider the same dependency
patterns that are used to generate candidates for
opinion-arg classification.
</bodyText>
<subsectionHeader confidence="0.995825">
3.3 Joint Inference
</subsectionHeader>
<bodyText confidence="0.999892625">
The inference goal is to find the optimal prediction
for both opinion entity identification and opinion
relation extraction. For a given sentence, we de-
note O as a set of opinion candidates, Ak as a set
of argument candidates, where k denotes the type
of opinion relation — IS-ABOUT or IS-FROM —
and S as a set of within-sentence spans that cover
all of the opinion candidates and argument can-
</bodyText>
<page confidence="0.936281">
1643
</page>
<bodyText confidence="0.999897333333333">
didates. We introduce binary variable xiz, where
xiz = 1 means span i is associated with label z.
We also introduce binary variable uij for every
pair of opinion candidate i and argument candidate
j, where uij = 1 means i forms an opinion rela-
tion with j, and binary variable vik for every opin-
ion candidate i in relation type k, where vik = 1
means i associates with an implicit argument in
relation k. Given the binary variables xiy, uij, vik,
it is easy to recover the entity and relation assign-
ment by checking which spans are labeled as opin-
ion entities, and which opinion span and argument
span form an opinion relation.
The objective function is defined as a linear
combination of the potentials from different pre-
dictors with a parameter A to balance the contribu-
tion of two components: opinion entity identifica-
tion and opinion relation extraction.
</bodyText>
<equation confidence="0.653072666666667">
arg max X X fizxiz
x,u,v A z
iES
X X ⎛ ⎞ (1)
+ (1 − A) iEO ⎝Xrijuij + ri∅vik ⎠
k jEAk
</equation>
<bodyText confidence="0.64268825">
It is subject to the following linear constraints:
Constraint 1: Uniqueness. For each span i, we
must assign one and only one label z, where z E
{H, D, T, N}.
</bodyText>
<equation confidence="0.960551666666667">
X
xiz = 1
z
</equation>
<bodyText confidence="0.732087333333333">
Constraint 2: Non-overlapping. If two spans i and
j overlap, then at most one of the spans can be
assigned to a non-NONE entity label: H, D, T.
</bodyText>
<equation confidence="0.984894">
X Xxiz + xjz G 1
z=,4N z=,4N
</equation>
<bodyText confidence="0.9837018">
Constraint 3: Consistency between the opinion-
arg and opinion-implicit-arg classifiers. For an
opinion candidate i, if it is predicted to have an
implicit argument in relation k, vik = 1, then no
argument candidate should form a relation with i.
If vik = 0, then there exists some argument can-
didate j E Ak such that uij = 1. We introduce
two auxiliary binary variables aik and bik to limit
the maximum number of relations associated with
each opinion candidate to be less than or equal to
</bodyText>
<equation confidence="0.958837">
three5. When vik = 1, aik and bik have to be 0.
X uij = 1 − vik + aik + bik
jEAk
aik G 1 − vik, bik G 1 − vik
</equation>
<bodyText confidence="0.962767636363636">
Constraint 4: Consistency between opinion-arg
classifier and opinion entity extractor. Suppose
an argument candidate j in relation k is assigned
an argument label by the entity extractor, that is
xjz = 1 (z = T for IS-ABOUT relation and z = H
for IS-FROM relation), then there exists some opin-
ion candidates that associate with j. Similar to
constraint 3, we introduce auxiliary binary vari-
ables cj and dj to enforce that an argument j links
to at most three opinion expressions. If xjz = 0,
then no relations should be extracted for j.
</bodyText>
<equation confidence="0.915326333333333">
X uij = xjz + cjk + djk
iEO
cjk G xjz, djk G xjz
</equation>
<bodyText confidence="0.97383">
Constraint 5: Consistency between the opinion-
implicit-arg classifier and opinion entity extractor.
When an opinion candidate i is predicted to asso-
ciate with an implicit argument in relation k, that
is vik = 1, then we allow xiD to be either 1 or
0 depending on the confidence of labeling i as an
opinion expression. When vik = 0, there exisits
some opinion argument associated with the opin-
ion candidate, and we enforce xiD = 1, which
means the entity extractor agrees to label i as an
opinion expression.
</bodyText>
<equation confidence="0.833479">
vik + xiD &gt; 1
</equation>
<bodyText confidence="0.999898583333333">
Note that in our ILP formulation, the label
assignment for a candidate span involves one
multiple-choice decision among different opinion
entity labels and the “NONE” entity label. The
scores of different label assignments are compara-
ble for the same span since they come from one
entity extraction model. This makes our ILP for-
mulation advantageous over the ILP formulation
proposed in Choi et al. (2006), which needs m bi-
nary decisions for a candidate span, where m is the
number of types of opinion entities, and the score
for each possible label assignment is obtained by
</bodyText>
<footnote confidence="0.9519912">
5It is possible to add more auxiliary variables to allow
more than three arguments to link to an opinion expression,
but this rarely happens in our experiments. For the IS-FROM
relation, we set aik = 0, bik = 0 since an opinion expression
usually has only one holder.
</footnote>
<page confidence="0.996212">
1644
</page>
<bodyText confidence="0.999815">
the sum of raw scores from m independent extrac-
tion models. This design choice also allows us
to easily deal with multiple types of opinion ar-
guments and opinion relations.
</bodyText>
<sectionHeader confidence="0.999206" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.998727">
For evaluation, we used version 2.0 of the MPQA
corpus (Wiebe et al., 2005; Wilson, 2008), a
widely used data set for fine-grained opinion anal-
ysis.6 We considered the subset of 482 docu-
ments7 that contain attitude and target annotations.
There are a total of 9,471 sentences with opinion-
related labels at the phrase level. We set aside 132
documents as a development set and use 350 doc-
uments as the evaluation set. All experiments em-
ploy 10-fold cross validation on the evaluation set;
the average over the 10 runs is reported.
Our gold standard opinion expressions, opinion
targets and opinion holders correspond to the di-
rect subjective annotations, target annotations and
agent annotations, respectively. The IS-FROM re-
lation is obtained from the agent attribute of each
opinion expression. The IS-ABOUT relation is ob-
tained from the attitude annotations: each opinion
expression is annotated with attitude frames and
each attitude frame is associated with a list of tar-
gets. The relations may overlap: for example, in
the following sentence, the target of relation 1 con-
tains relation 2.
[John]H1 is happyO1 because [[he]H2
lovesO2 [being at Enderly Park]T2]T1.
We discard relations that contain sub-relations be-
cause we believe that identifying the sub-relations
usually is sufficient to recover the discarded rela-
tions. (Prediction of overlapping relations is con-
sidered as future work.) In the example above, we
will identify (loves, being at Enderly Park) as an
IS-ABOUT relation and happy as an opinion ex-
pression associated with an implicit target. Table 1
shows some statistics of the corpus.
We adopted the evaluation metrics for entity and
relation extraction from Choi et al. (2006), which
include precision, recall, and F1-measure accord-
ing to overlap and exact matching metrics.8 We
</bodyText>
<footnote confidence="0.998029285714286">
6Available at http://www.cs.pitt.edu/mpqa/.
7349 news articles from the original MPQA corpus, 84
Wall Street Journal articles (Xbank), and 48 articles from the
American National Corpus.
8Overlap matching considers two spans to match if they
overlap, while exact matching requires two spans to be ex-
actly the same.
</footnote>
<table confidence="0.9995684">
Opinion Target Holder
TotalNum 5849 4676 4244
Opinion-arg Relations Implicit Relations
IS-ABOUT 4823 1302
IS-FROM 4662 1187
</table>
<tableCaption confidence="0.999555">
Table 1: Data Statistics of the MPQA Corpus.
</tableCaption>
<bodyText confidence="0.999908833333333">
will focus our discussion on results obtained us-
ing overlap matching, since the exact boundaries
of opinion entities are hard to define even for hu-
man annotators (Wiebe et al., 2005).
We trained CRFs for opinion entity identifica-
tion using the following features: indicators for
words, POS tags, and lexicon features (the sub-
jectivity strength of the word in the Subjectivity
Lexicon). All features are computed for the cur-
rent token and tokens in a [−1, +1] window. We
used L2-regularization; the regularization param-
eter was tuned using the development set. We
trained the classifiers for relation extraction using
L1-regularized logistic regression with default pa-
rameters using the LIBLINEAR (Fan et al., 2008)
package. For joint inference, we used GLPK9 to
provide the optimal ILP solution. The parameter
λ was tuned using the development set.
</bodyText>
<subsectionHeader confidence="0.584733">
4.1 Baseline Methods
</subsectionHeader>
<bodyText confidence="0.993649571428571">
We compare our approach to several pipeline base-
lines. Each extracts opinion entities first using
the same CRF employed in our approach, and
then predicts opinion relations on the opinion en-
tity candidates obtained from the CRF prediction.
Three relation extraction techniques were used in
the baselines:
</bodyText>
<listItem confidence="0.991921384615385">
• Adj: Inspired by the adjacency rule used
in Hu and Liu (2004), it links each argu-
ment candidate to its nearest opinion candi-
date. Arguments that do not link to any opin-
ion candidate are discarded. This is also used
as a strong baseline in Choi et al. (2006).
• Syn: Links pairs of opinion and argument
candidates that present prominent syntactic
patterns. (We consider the syntactic patterns
listed in Section 3.2.1.) Previous work also
demonstrates the effectiveness of syntactic
information in opinion extraction (Johansson
and Moschitti, 2012).
</listItem>
<footnote confidence="0.956648">
9http://www.gnu.org/software/glpk/
</footnote>
<page confidence="0.952679">
1645
</page>
<table confidence="0.99990225">
Opinion Expression Opinion Target Opinion Holder
Method P R F1 P R F1 P R F1
CRF 82.21 66.15 73.31 73.22 48.58 58.41 72.32 49.09 58.48
CRF+Adj 82.21 66.15 73.31 80.87 42.31 55.56 75.24 48.48 58.97
CRF+Syn 82.21 66.15 73.31 81.87 30.36 44.29 78.97 40.20 53.28
CRF+RE 83.02 48.99 61.62 85.07 22.01 34.97 78.13 40.40 53.26
Joint-Model 71.16 77.85 74.35* 75.18 57.12 64.92** 67.01 66.46 66.73**
CRF 66.60 52.57 58.76 44.44 29.60 35.54 65.18 44.24 52.71
CRF+Adj 66.60 52.57 58.76 49.10 25.81 33.83 68.03 43.84 53.32
CRF+Syn 66.60 52.57 58.76 50.26 18.41 26.94 74.60 37.98 50.33
CRF+RE 69.27 40.09 50.79 60.45 15.37 24.51 75 38.79 51.13
Joint-Model 57.39 62.40 59.79* 49.15 38.33 43.07** 62.73 62.22 62.47**
</table>
<tableCaption confidence="0.780841333333333">
Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and
the bottom table uses exact). Two-tailed t-test results are shown on F1 measure for our method compared to the other baselines
(statistical significance is indicated with *(p &lt; 0.05), **(p &lt; 0.005)).
</tableCaption>
<table confidence="0.999865777777778">
IS-ABOUT IS-FROM
Method P R F1 P R F1
CRF+Adj 73.65 37.34 49.55 70.22 41.58 52.23
CRF+Syn 76.21 28.28 41.25 77.48 36.63 49.74
CRF+RE 78.26 20.33 32.28 74.81 37.55 50.00
CRF+Adj-merged-10-best 25.05 61.18 35.55 30.28 62.82 40.87
CRF+Syn-merged-10-best 41.60 45.66 43.53 48.08 54.03 50.88
CRF+RE-merged-10-best 51.60 33.09 40.32 47.73 54.40 50.84
Joint-Model 64.38 51.20 57.04** 64.97 58.61 61.63**
</table>
<tableCaption confidence="0.999753">
Table 3: Performance on opinion relation extraction using the overlap metric.
</tableCaption>
<listItem confidence="0.975141">
• RE: Predicts opinion relations by employ-
</listItem>
<bodyText confidence="0.903766538461538">
ing the opinion-arg classifier and opinion-
implicit-arg classifier. First, the opinion-arg
classifier identifies pairs of opinion and argu-
ment candidates that form valid opinion rela-
tions, and then the opinion-implicit-arg clas-
sifier is used on the remaining opinion candi-
dates to further identify opinion expressions
without explicit arguments.
We report results using opinion entity candi-
dates from the best CRF output and from the
merged 10-best CRF output.10 The motivation of
merging the 10-best output is to increase recall for
the pipeline methods.
</bodyText>
<sectionHeader confidence="0.999821" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.99103982051282">
Table 2 shows the results of opinion entity identi-
fication using both overlap and exact metrics. We
compare our approach with the pipeline baselines
and CRF (the first step of the pipeline). We can
see that our joint inference approach significantly
outperforms all the baselines in F1 measure on ex-
tracting all types of opinion entities. In general,
10It is similar to the merged 10-best baseline in Choi et
al. (2006). If an entity Ei extracted by the ith-best sequence
overlaps with an entity Ej extracted by the jth-best sequence,
where i ≤ j, then we discard Ej. If Ei and Ej do not over-
lap, then we consider both entities.
by adding the relation extraction step, the pipeline
baselines are able to improve precision over the
CRF but fail at recall. CRF+Syn and CRF+Adj
provide the same performance as CRF, since the
relation extraction step only affects the results of
opinion arguments. By incorporating syntactic
information, CRF+Syn provides better precision
than CRF+Adj on extracting arguments at the ex-
pense of recall. This indicates that using simple
syntactic rules would mistakenly filter many cor-
rect relations. By using binary classifiers to pre-
dict relations, CRF+RE produces high precision
on opinion and target extraction but also results in
very low recall. Using the exact metric, we ob-
serve the same general trend in the results as the
overlap metric. The scores are lower since the
metric is much stricter.
Table 3 shows the results of opinion relation ex-
traction using the overlap metric. We compare our
approach with pipelined baselines in two settings:
one employs relation extraction on 1-best output
of CRF (top half of table) and the other employs
the merged 10-best output of CRF (bottom half of
table). We can see that in general, using merged
10-best CRF outputs boosts the recall while sac-
rificing precision. This is expected since merging
the 10-best CRF outputs favors candidates that are
</bodyText>
<page confidence="0.967258">
1646
</page>
<table confidence="0.999331166666667">
IS-ABOUT Relation Extraction IS-FROM Relation Extraction
Method P R F1 P R F1
ILP-W/O-ENTITY 49.10 40.48 44.38 44.77 58.24 50.63
ILP-W-SINGLE-RE 63.88 49.35 55.68 53.64 65.02 58.78
ILP-W/O-IMPLICIT-RE 62.00 44.73 51.97 73.23 51.28 60.32
Joint-Model 64.38 51.20 57.04** 64.97 58.61 61.63*
</table>
<tableCaption confidence="0.999973">
Table 4: Comparison between our approach and ILP baselines that omit some potentials in our approach.
</tableCaption>
<bodyText confidence="0.999931708333333">
believed to be more accurate by the CRF predictor.
If CRF makes mistakes, the mistakes will propa-
gate to the relation extraction step. The poor per-
formance on precision further confirms the error
propagation problem in the pipeline approaches.
In contrast, our joint-inference method success-
fully boosts the recall while maintaining reason-
able precision. This demonstrates that joint infer-
ence can effectively leverage the advantage of in-
dividual predictors and limit error propagation.
To demonstrate the effectiveness of different
potentials in our joint inference model, we con-
sider three variants of our ILP formulation that
omit some potentials in the joint inference: one
is ILP-W/O-ENTITY, which extracts opinion rela-
tions without integrating information from opin-
ion entity identification; one is ILP-W-SINGLE-RE,
which focuses on extracting a single opinion re-
lation and ignores the information from the other
relation; the third one is ILP-W/O-IMPLICIT-RE,
which omits the potential for opinion-implicit-arg
relation and assumes every opinion expression is
linked to an explicit argument. The objective func-
tion of ILP-W/O-ENTITY can be represented as
</bodyText>
<equation confidence="0.573818">
rijuij (2)
</equation>
<bodyText confidence="0.9987208">
which is subject to constraints on uij to enforce
relations to not overlap and limit the maximum
number of relations that can be extracted for each
opinion expression and each argument. For ILP-
W-SINGLE-RE, we simply remove the variables as-
sociated with one opinion relation in the objective
function (1) and constraints. The formulation of
ILP-W/O-IMPLICIT-RE removes the variables as-
sociated with potential ri in the objective function
and corresponding constraints. It can be viewed
as an extension to the ILP approach in Choi et al.
(2006) that includes opinion targets and uses sim-
pler ILP formulation with only one parameter and
fewer binary variables and constraints to represent
entity label assignments 11.
11We compared the proposed ILP formulation with the ILP
Table 4 shows the results of these methods on
opinion relation extraction. We can see that with-
out the knowledge of the entity extractor, ILP-
W/O-ENTITY provides poor performance on both
relation extraction tasks. This confirms the effec-
tiveness of leveraging knowledge from entity ex-
tractor and relation extractor. The improvement
yielded by our approach over ILP-W-SINGLE-RE
demonstrates the benefit of jointly optimizing dif-
ferent types of opinion relations. Our approach
also outperforms ILP-W/O-IMPLICIT-RE, which
does not take into account implicit relations. The
results demonstrate that incorporating knowledge
of implicit opinion relations is important.
</bodyText>
<sectionHeader confidence="0.999753" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.967712714285714">
We note that the joint inference model yields a
clear improvement on recall but not on precision
compared to the CRF-based baselines. Analyz-
ing the errors, we found that the joint model ex-
tracts comparable number of opinion entities com-
pared to the gold standard, while the CRF-based
baselines extract significantly fewer opinion enti-
ties (around 60% of the number of entities in the
gold standard). With more extracted opinion enti-
ties, the precision is sacriced but recall is boosted
substantially, and overall we see an increase in
F-measure. We also found that a good portion
of errors were made because the generated candi-
dates failed to cover the correct solutions. Recall
that the joint model finds the global optimal solu-
tion over a set of opinion entity and relation can-
didates, which are obtained from the n-best CRF
predictions and constituents in the parse tree that
satisfy certain syntactic patterns. It is possible
that the generated candidates do not contain the
gold standard answers. For example, our model
failed to identify the IS-ABOUT relation (offers,
general aid) from the following sentence Powell
had contacted ... and received offersO1 of [gen-
formulation in Choi et al. (2006) on extracting opinion hold-
ers, opinion expressions and IS-FROM relations, and showed
that the proposed ILP formulation performs better on all three
extraction tasks.
</bodyText>
<figure confidence="0.99238175">
�
arg max
u
k
�
iEO
�
jEAk
</figure>
<page confidence="0.985835">
1647
</page>
<bodyText confidence="0.999984045454545">
eral aid]T,... because both the CRF predictor and
syntactic heuristics fail to capture (offers, general
aid) as a potential relation candidate. By applying
simple heuristics such as treating all verbs or verb
phrases as opinion candidates would not help be-
cause it would introduce a large number of nega-
tive candidates and lower the accuracy of relation
extraction (only 52% of the opinion expressions
are verbs or verb phrases and 64% of the opinion
targets are noun or noun phrases in the corpus we
used). Therefore a more effective candidate gen-
eration method is needed to allow more candidates
while limiting the number of negative candidates.
We also observed incorrect parsing to be a cause of
error. We hope to study ways to account for such
errors in our approach as future work.
For computational time, our ILP formulation
can be solved very efficiently using advanced ILP
solvers. In our experiment, using GLPK’s branch-
and-cut solver took 0.2 seconds to produce opti-
mal ILP solutions for 1000 sentences on a machine
with Intel Core 2 Duo CPU and 4GB RAM.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999449866666667">
In this paper we propose a joint inference ap-
proach for extracting opinion-related entities and
opinion relations. We decompose the task into
different subproblems, and jointly optimize them
using constraints that aim to encourage their con-
sistency and reduce prediction uncertainty. We
show that our approach can effectively integrate
knowledge from different predictors and achieve
significant improvements in overall performance
for opinion extraction. For future work, we plan to
extend our model to handle more complex opinion
relations, e.g. nesting or cross-sentential relations.
This can be potentially addressed by incorporat-
ing more powerful predictors and more complex
linguistic constraints.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999877833333333">
This work was supported in part by DARPA-BAA-
12-47 DEFT grant 12475008 and NSF grant BCS-
0904822. We thank Igor Labutov for helpful dis-
cussion and suggestions, Ainur Yessenalina for
early discussion of the work, as well as the reviews
for helpful comments.
</bodyText>
<sectionHeader confidence="0.990285" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999844944444445">
E. Breck, Y. Choi, and C. Cardie. 2007. Identifying
expressions of opinion in context. In Proceedings of
the 20th international joint conference on Artifical
intelligence, pages 2683–2688. Morgan Kaufmann
Publishers Inc.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opin-
ions with conditional random fields and extraction
patterns. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 355–362.
Association for Computational Linguistics.
Y. Choi, E. Breck, and C. Cardie. 2006. Joint ex-
traction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 431–439. Association for Computational
Linguistics.
D. Das, A.F.T. Martins, and N.A. Smith. 2012. An
exact dual decomposition algorithm for shallow se-
mantic parsing with constraints. Proceedings of*
SEM.[ii, 10, 50].
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
M. Hu and B. Liu. 2004. Mining opinion features
in customer reviews. In Proceedings of the Na-
tional Conference on Artificial Intelligence, pages
755–760. Menlo Park, CA; Cambridge, MA; Lon-
don; AAAI Press; MIT Press; 1999.
Richard Johansson and Alessandro Moschitti. 2010a.
Reranking models in fine-grained opinion analysis.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 519–527. As-
sociation for Computational Linguistics.
Richard Johansson and Alessandro Moschitti. 2010b.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 67–76. Association for Com-
putational Linguistics.
Richard Johansson and Alessandro Moschitti. 2012.
Relational features in fine-grained opinion analysis.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, pages 1–
8. Association for Computational Linguistics.
N. Kobayashi, K. Inui, and Y. Matsumoto. 2007.
Extracting aspect-evaluation and aspect-of relations
in opinion mining. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
</reference>
<page confidence="0.811444">
1648
</page>
<reference confidence="0.999880666666667">
Language Learning (EMNLP-CoNLL), pages 1065–
1074.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
K. Liu, L. Xu, and J. Zhao. 2012. Opinion target
extraction using word-based translation model. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Pub.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2):257–287.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st
international jont conference on Artifical intelli-
gence, pages 1199–1204. Morgan Kaufmann Pub-
lishers Inc.
G. Qiu, B. Liu, J. Bu, and C. Chen. 2011. Opinion
word expansion and target extraction through double
propagation. Computational linguistics, 37(1):9–
27.
R. Quirk, S. Greenbaum, G. Leech, J. Svartvik, and
D. Crystal. 1985. A comprehensive grammar of
the English language, volume 397. Cambridge Univ
Press.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. Defense Technical Information Center.
J. Ruppenhofer, S. Somasundaran, and J. Wiebe. 2008.
Finding the sources and targets of subjective expres-
sions. In Proceedings of LREC.
Vivek Srikumar and Dan Roth. 2011. A joint model
for extended semantic role labeling. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 129–139. Association
for Computational Linguistics.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 817–824. Asso-
ciation for Computational Linguistics.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2):165–
210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational linguistics, 35(3):399–433.
Theresa Wilson. 2008. Fine-Grained Subjectivity
Analysis. Ph.D. thesis, Ph. D. thesis, University of
Pittsburgh. Intelligent Systems Program.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. 2009. Phrase
dependency parsing for opinion mining. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 3-Volume
3, pages 1533–1541. Association for Computational
Linguistics.
B. Yang and C. Cardie. 2012. Extracting opinion
expressions with semi-markov conditional random
fields. In Proceedings of the conference on Empiri-
cal Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O’Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1462–1470. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.995769">
1649
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.721844">
<title confidence="0.999453">Joint Inference for Fine-grained Opinion Extraction</title>
<author confidence="0.984453">Bishan</author>
<affiliation confidence="0.928862">Department of Computer Cornell</affiliation>
<email confidence="0.99856">bishan@cs.cornell.edu</email>
<author confidence="0.97692">Claire</author>
<affiliation confidence="0.9352705">Department of Computer Cornell</affiliation>
<email confidence="0.998819">cardie@cs.cornell.edu</email>
<abstract confidence="0.999490272727273">This paper addresses the task of finegrained opinion extraction – the identification of opinion-related entities: the opinion expressions, the opinion holders, and the targets of the opinions, and the relations between opinion expressions and their targets and holders. Most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the interdependencies among different extraction stages are not captured. We propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction, and seeks a globally optimal solution. Experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Breck</author>
<author>Y Choi</author>
<author>C Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence,</booktitle>
<pages>2683--2688</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="6211" citStr="Breck et al., 2007" startWordPosition="934" endWordPosition="937">tities. We evaluate our approach using a standard corpus for fine-grained opinion analysis (the MPQA corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of </context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>E. Breck, Y. Choi, and C. Cardie. 2007. Identifying expressions of opinion in context. In Proceedings of the 20th international joint conference on Artifical intelligence, pages 2683–2688. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6311" citStr="Choi et al., 2005" startWordPosition="950" endWordPosition="953"> corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Kobayashi et al. (2007) considered extracting “aspect-evaluation” relatio</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 355–362. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choi</author>
<author>E Breck</author>
<author>C Cardie</author>
</authors>
<title>Joint extraction of entities and relations for opinion recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>431--439</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4262" citStr="Choi et al. (2006)" startWordPosition="636" endWordPosition="639">stics, pages 1640–1649, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2007; Yang and Cardie, 2012)) and relation extraction techniques have been proposed to extract opinion holders and targets based on their linking relations to the opinion expressions (e.g. Kim and Hovy (2006), Kobayashi et al. (2007)). However, most existing work treats the extraction of different opinion entities and opinion relations in a pipelined manner: the interaction between different extraction tasks is not modeled jointly and error propagation is not considered. One exception is Choi et al. (2006), which proposed an ILP approach to jointly identify opinion holders, opinion expressions and their IS-FROM linking relations, and demonstrated the effectiveness of joint inference. Their ILP formulation, however, does not handle implicit linking relations, i.e. opinion expressions with no explicit opinion holder; nor does it consider IS-ABOUT relations. In this paper, we present a model that jointly identifies opinion-related entities, including opinion expressions, opinion targets and opinion holders as well as the associated opinion linking relations, IS-ABOUT and IS-FROM. For each type of </context>
<context position="7284" citStr="Choi et al. (2006)" startWordPosition="1095" endWordPosition="1098">identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Kobayashi et al. (2007) considered extracting “aspect-evaluation” relations (relations between opinion expressions and targets) by identifying opinion expressions first and then searching for the most likely target for each opinion expression via a binary relation classifier. All these methods extract opinion arguments and opinion relations in separate stages instead of extracting them jointly. Most similar to our method is Choi et al. (2006), which jointly extracts opinion expressions, holders and their IS-FROM relations using an ILP approach. In contrast, our approach (1) also considers the IS-ABOUT relation which is arguably more complex due to the larger variety in the syntactic structure exhibited by opinion expressions and their targets, (2) handles implicit opinion relations (opinion expressions without any associated argument), and (3) uses a simpler ILP formulation. There has also been substantial interest in opinion extraction from product reviews (Liu, 2012). Most existing approaches focus on the extraction of opinion t</context>
<context position="13682" citStr="Choi et al., 2006" startWordPosition="2145" endWordPosition="2148">shortest dependency paths2 between an opinion candidate o and an argument candidate a in our dataset, and include all pairs of candidates that satisfy at least one dependency pattern. For the IS-ABOUT relation, the top three patterns are (1) o Tdobj a, (2) o Tccomp x Tnsubj a (x is a word in the path that is not covered by either o nor a), (3) o Tccomp a; for the IS-FROM relation, the top three patterns are (1) o Tnsubj a, (2) o Tposs a, (3) o �ccomp x Tnsubj a. Note that generating candidates this way will give us a large number of negative examples. Similar to the preprocessing approach in (Choi et al., 2006), we filter pairs of opinion and argument candidates that do not overlap with any gold standard relation in our training data. Many features we use are common features in the SRL tasks (Punyakanok et al., 2008) due to the similarity of opinion relations to the predicate-argument relations in SRL (Ruppenhofer et al., 2008; Choi et al., 2006). In general, the features aim to capture (a) local properties of the candidate opinion expressions and arguments and (b) syntactic and semantic attributes of their relation. Words and POS tags: the words contained in the candidate and their POS tags. Lexico</context>
<context position="20680" citStr="Choi et al. (2006)" startWordPosition="3323" endWordPosition="3326">ression. When vik = 0, there exisits some opinion argument associated with the opinion candidate, and we enforce xiD = 1, which means the entity extractor agrees to label i as an opinion expression. vik + xiD &gt; 1 Note that in our ILP formulation, the label assignment for a candidate span involves one multiple-choice decision among different opinion entity labels and the “NONE” entity label. The scores of different label assignments are comparable for the same span since they come from one entity extraction model. This makes our ILP formulation advantageous over the ILP formulation proposed in Choi et al. (2006), which needs m binary decisions for a candidate span, where m is the number of types of opinion entities, and the score for each possible label assignment is obtained by 5It is possible to add more auxiliary variables to allow more than three arguments to link to an opinion expression, but this rarely happens in our experiments. For the IS-FROM relation, we set aik = 0, bik = 0 since an opinion expression usually has only one holder. 1644 the sum of raw scores from m independent extraction models. This design choice also allows us to easily deal with multiple types of opinion arguments and op</context>
<context position="23009" citStr="Choi et al. (2006)" startWordPosition="3707" endWordPosition="3710">s relation 2. [John]H1 is happyO1 because [[he]H2 lovesO2 [being at Enderly Park]T2]T1. We discard relations that contain sub-relations because we believe that identifying the sub-relations usually is sufficient to recover the discarded relations. (Prediction of overlapping relations is considered as future work.) In the example above, we will identify (loves, being at Enderly Park) as an IS-ABOUT relation and happy as an opinion expression associated with an implicit target. Table 1 shows some statistics of the corpus. We adopted the evaluation metrics for entity and relation extraction from Choi et al. (2006), which include precision, recall, and F1-measure according to overlap and exact matching metrics.8 We 6Available at http://www.cs.pitt.edu/mpqa/. 7349 news articles from the original MPQA corpus, 84 Wall Street Journal articles (Xbank), and 48 articles from the American National Corpus. 8Overlap matching considers two spans to match if they overlap, while exact matching requires two spans to be exactly the same. Opinion Target Holder TotalNum 5849 4676 4244 Opinion-arg Relations Implicit Relations IS-ABOUT 4823 1302 IS-FROM 4662 1187 Table 1: Data Statistics of the MPQA Corpus. will focus our</context>
<context position="25029" citStr="Choi et al. (2006)" startWordPosition="4029" endWordPosition="4032">tuned using the development set. 4.1 Baseline Methods We compare our approach to several pipeline baselines. Each extracts opinion entities first using the same CRF employed in our approach, and then predicts opinion relations on the opinion entity candidates obtained from the CRF prediction. Three relation extraction techniques were used in the baselines: • Adj: Inspired by the adjacency rule used in Hu and Liu (2004), it links each argument candidate to its nearest opinion candidate. Arguments that do not link to any opinion candidate are discarded. This is also used as a strong baseline in Choi et al. (2006). • Syn: Links pairs of opinion and argument candidates that present prominent syntactic patterns. (We consider the syntactic patterns listed in Section 3.2.1.) Previous work also demonstrates the effectiveness of syntactic information in opinion extraction (Johansson and Moschitti, 2012). 9http://www.gnu.org/software/glpk/ 1645 Opinion Expression Opinion Target Opinion Holder Method P R F1 P R F1 P R F1 CRF 82.21 66.15 73.31 73.22 48.58 58.41 72.32 49.09 58.48 CRF+Adj 82.21 66.15 73.31 80.87 42.31 55.56 75.24 48.48 58.97 CRF+Syn 82.21 66.15 73.31 81.87 30.36 44.29 78.97 40.20 53.28 CRF+RE 83.</context>
<context position="27886" citStr="Choi et al. (2006)" startWordPosition="4469" endWordPosition="4472">ng opinion entity candidates from the best CRF output and from the merged 10-best CRF output.10 The motivation of merging the 10-best output is to increase recall for the pipeline methods. 5 Results Table 2 shows the results of opinion entity identification using both overlap and exact metrics. We compare our approach with the pipeline baselines and CRF (the first step of the pipeline). We can see that our joint inference approach significantly outperforms all the baselines in F1 measure on extracting all types of opinion entities. In general, 10It is similar to the merged 10-best baseline in Choi et al. (2006). If an entity Ei extracted by the ith-best sequence overlaps with an entity Ej extracted by the jth-best sequence, where i ≤ j, then we discard Ej. If Ei and Ej do not overlap, then we consider both entities. by adding the relation extraction step, the pipeline baselines are able to improve precision over the CRF but fail at recall. CRF+Syn and CRF+Adj provide the same performance as CRF, since the relation extraction step only affects the results of opinion arguments. By incorporating syntactic information, CRF+Syn provides better precision than CRF+Adj on extracting arguments at the expense</context>
<context position="31502" citStr="Choi et al. (2006)" startWordPosition="5036" endWordPosition="5039">. The objective function of ILP-W/O-ENTITY can be represented as rijuij (2) which is subject to constraints on uij to enforce relations to not overlap and limit the maximum number of relations that can be extracted for each opinion expression and each argument. For ILPW-SINGLE-RE, we simply remove the variables associated with one opinion relation in the objective function (1) and constraints. The formulation of ILP-W/O-IMPLICIT-RE removes the variables associated with potential ri in the objective function and corresponding constraints. It can be viewed as an extension to the ILP approach in Choi et al. (2006) that includes opinion targets and uses simpler ILP formulation with only one parameter and fewer binary variables and constraints to represent entity label assignments 11. 11We compared the proposed ILP formulation with the ILP Table 4 shows the results of these methods on opinion relation extraction. We can see that without the knowledge of the entity extractor, ILPW/O-ENTITY provides poor performance on both relation extraction tasks. This confirms the effectiveness of leveraging knowledge from entity extractor and relation extractor. The improvement yielded by our approach over ILP-W-SINGL</context>
<context position="33609" citStr="Choi et al. (2006)" startWordPosition="5364" endWordPosition="5367">rors were made because the generated candidates failed to cover the correct solutions. Recall that the joint model finds the global optimal solution over a set of opinion entity and relation candidates, which are obtained from the n-best CRF predictions and constituents in the parse tree that satisfy certain syntactic patterns. It is possible that the generated candidates do not contain the gold standard answers. For example, our model failed to identify the IS-ABOUT relation (offers, general aid) from the following sentence Powell had contacted ... and received offersO1 of [genformulation in Choi et al. (2006) on extracting opinion holders, opinion expressions and IS-FROM relations, and showed that the proposed ILP formulation performs better on all three extraction tasks. � arg max u k � iEO � jEAk 1647 eral aid]T,... because both the CRF predictor and syntactic heuristics fail to capture (offers, general aid) as a potential relation candidate. By applying simple heuristics such as treating all verbs or verb phrases as opinion candidates would not help because it would introduce a large number of negative candidates and lower the accuracy of relation extraction (only 52% of the opinion expressions</context>
</contexts>
<marker>Choi, Breck, Cardie, 2006</marker>
<rawString>Y. Choi, E. Breck, and C. Cardie. 2006. Joint extraction of entities and relations for opinion recognition. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 431–439. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>A F T Martins</author>
<author>N A Smith</author>
</authors>
<title>An exact dual decomposition algorithm for shallow semantic parsing with constraints.</title>
<date>2012</date>
<journal>Proceedings of* SEM.[ii,</journal>
<volume>10</volume>
<pages>50</pages>
<contexts>
<context position="9063" citStr="Das et al., 2012" startWordPosition="1378" endWordPosition="1381">e problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be assigned. Joint inference has also been applied to semantic role labeling (Punyakanok et al., 2008; Srikumar and Roth, 2011; Das et al., 2012), where the goal is to jointly identify semantic arguments for given lexical predicates. The problem is conceptually similar to identifying opinion arguments for opinion expressions, however, we do not assume prior knowledge of opinion expressions (unlike in SRL, where predicates are given). 3 Model As proposed in Section 1, we consider the task of jointly identifying opinion entities and opinion relations. Specifically, given a sentence, our goal is to identify spans of opinion expressions, opinion arguments (targets and holders) and their associated linking relations. Training data consists </context>
</contexts>
<marker>Das, Martins, Smith, 2012</marker>
<rawString>D. Das, A.F.T. Martins, and N.A. Smith. 2012. An exact dual decomposition algorithm for shallow semantic parsing with constraints. Proceedings of* SEM.[ii, 10, 50].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="24309" citStr="Fan et al., 2008" startWordPosition="3907" endWordPosition="3910">of opinion entities are hard to define even for human annotators (Wiebe et al., 2005). We trained CRFs for opinion entity identification using the following features: indicators for words, POS tags, and lexicon features (the subjectivity strength of the word in the Subjectivity Lexicon). All features are computed for the current token and tokens in a [−1, +1] window. We used L2-regularization; the regularization parameter was tuned using the development set. We trained the classifiers for relation extraction using L1-regularized logistic regression with default parameters using the LIBLINEAR (Fan et al., 2008) package. For joint inference, we used GLPK9 to provide the optimal ILP solution. The parameter λ was tuned using the development set. 4.1 Baseline Methods We compare our approach to several pipeline baselines. Each extracts opinion entities first using the same CRF employed in our approach, and then predicts opinion relations on the opinion entity candidates obtained from the CRF prediction. Three relation extraction techniques were used in the baselines: • Adj: Inspired by the adjacency rule used in Hu and Liu (2004), it links each argument candidate to its nearest opinion candidate. Argumen</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>755--760</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="8185" citStr="Hu and Liu, 2004" startWordPosition="1230" endWordPosition="1233">ons and their targets, (2) handles implicit opinion relations (opinion expressions without any associated argument), and (3) uses a simpler ILP formulation. There has also been substantial interest in opinion extraction from product reviews (Liu, 2012). Most existing approaches focus on the extraction of opinion targets and their associated opinion expressions and usually employ a pipeline architecture: generate candidates of opinion expressions and opinion targets first, and then use rule-based or machine-learning-based approaches to identify potential relations between opinions and targets (Hu and Liu, 2004; Wu et al., 2009; Liu et al., 2012). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the</context>
<context position="24833" citStr="Hu and Liu (2004)" startWordPosition="3992" endWordPosition="3995">egularized logistic regression with default parameters using the LIBLINEAR (Fan et al., 2008) package. For joint inference, we used GLPK9 to provide the optimal ILP solution. The parameter λ was tuned using the development set. 4.1 Baseline Methods We compare our approach to several pipeline baselines. Each extracts opinion entities first using the same CRF employed in our approach, and then predicts opinion relations on the opinion entity candidates obtained from the CRF prediction. Three relation extraction techniques were used in the baselines: • Adj: Inspired by the adjacency rule used in Hu and Liu (2004), it links each argument candidate to its nearest opinion candidate. Arguments that do not link to any opinion candidate are discarded. This is also used as a strong baseline in Choi et al. (2006). • Syn: Links pairs of opinion and argument candidates that present prominent syntactic patterns. (We consider the syntactic patterns listed in Section 3.2.1.) Previous work also demonstrates the effectiveness of syntactic information in opinion extraction (Johansson and Moschitti, 2012). 9http://www.gnu.org/software/glpk/ 1645 Opinion Expression Opinion Target Opinion Holder Method P R F1 P R F1 P R</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining opinion features in customer reviews. In Proceedings of the National Conference on Artificial Intelligence, pages 755–760. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Reranking models in fine-grained opinion analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>519--527</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6242" citStr="Johansson and Moschitti, 2010" startWordPosition="938" endWordPosition="941">our approach using a standard corpus for fine-grained opinion analysis (the MPQA corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Koba</context>
<context position="15324" citStr="Johansson and Moschitti, 2010" startWordPosition="2388" endWordPosition="2392">stance (number of words) between the opinion and argument candidates. Dependency Path: the shortest path in the dependency tree between the opinion candidate and the target candidate, e.g. ccompTnsubjT. We also include word types and POS types in the paths, e.g. opinionTccompsufferingTnsubjpatient, 2We use the Stanford Parser to generate parse trees and dependency graphs. 3http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 4https://framenet.icsi.berkeley.edu/ fndrupal/ NNTccompVBGTnsubjNN. The dependency path has been shown to be very useful in extracting opinion expressions and opinion holders (Johansson and Moschitti, 2010a). 3.2.2 Opinion-Implicit-Arg Relations When the opinion-arg relation classifier predicts that there is no suitable argument for the opinion expression candidate, it does not capture the possibility that an opinion candidate may associate with an implicit argument. To incorporate knowledge of implicit relations, we build an opinion-implicitarg classifier to identify an opinion candidate with an implicit argument based on its own properties and context information. For training, we consider all gold-standard opinion expressions as training examples — including those with implicit arguments — a</context>
</contexts>
<marker>Johansson, Moschitti, 2010</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2010a. Reranking models in fine-grained opinion analysis. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 519–527. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Syntactic and semantic structure for opinion expression detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>67--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6242" citStr="Johansson and Moschitti, 2010" startWordPosition="938" endWordPosition="941">our approach using a standard corpus for fine-grained opinion analysis (the MPQA corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Koba</context>
<context position="15324" citStr="Johansson and Moschitti, 2010" startWordPosition="2388" endWordPosition="2392">stance (number of words) between the opinion and argument candidates. Dependency Path: the shortest path in the dependency tree between the opinion candidate and the target candidate, e.g. ccompTnsubjT. We also include word types and POS types in the paths, e.g. opinionTccompsufferingTnsubjpatient, 2We use the Stanford Parser to generate parse trees and dependency graphs. 3http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 4https://framenet.icsi.berkeley.edu/ fndrupal/ NNTccompVBGTnsubjNN. The dependency path has been shown to be very useful in extracting opinion expressions and opinion holders (Johansson and Moschitti, 2010a). 3.2.2 Opinion-Implicit-Arg Relations When the opinion-arg relation classifier predicts that there is no suitable argument for the opinion expression candidate, it does not capture the possibility that an opinion candidate may associate with an implicit argument. To incorporate knowledge of implicit relations, we build an opinion-implicitarg classifier to identify an opinion candidate with an implicit argument based on its own properties and context information. For training, we consider all gold-standard opinion expressions as training examples — including those with implicit arguments — a</context>
</contexts>
<marker>Johansson, Moschitti, 2010</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2010b. Syntactic and semantic structure for opinion expression detection. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 67–76. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Relational features in fine-grained opinion analysis.</title>
<date>2012</date>
<contexts>
<context position="25318" citStr="Johansson and Moschitti, 2012" startWordPosition="4068" endWordPosition="4071"> CRF prediction. Three relation extraction techniques were used in the baselines: • Adj: Inspired by the adjacency rule used in Hu and Liu (2004), it links each argument candidate to its nearest opinion candidate. Arguments that do not link to any opinion candidate are discarded. This is also used as a strong baseline in Choi et al. (2006). • Syn: Links pairs of opinion and argument candidates that present prominent syntactic patterns. (We consider the syntactic patterns listed in Section 3.2.1.) Previous work also demonstrates the effectiveness of syntactic information in opinion extraction (Johansson and Moschitti, 2012). 9http://www.gnu.org/software/glpk/ 1645 Opinion Expression Opinion Target Opinion Holder Method P R F1 P R F1 P R F1 CRF 82.21 66.15 73.31 73.22 48.58 58.41 72.32 49.09 58.48 CRF+Adj 82.21 66.15 73.31 80.87 42.31 55.56 75.24 48.48 58.97 CRF+Syn 82.21 66.15 73.31 81.87 30.36 44.29 78.97 40.20 53.28 CRF+RE 83.02 48.99 61.62 85.07 22.01 34.97 78.13 40.40 53.26 Joint-Model 71.16 77.85 74.35* 75.18 57.12 64.92** 67.01 66.46 66.73** CRF 66.60 52.57 58.76 44.44 29.60 35.54 65.18 44.24 52.71 CRF+Adj 66.60 52.57 58.76 49.10 25.81 33.83 68.03 43.84 53.32 CRF+Syn 66.60 52.57 58.76 50.26 18.41 26.94 74.</context>
</contexts>
<marker>Johansson, Moschitti, 2012</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2012. Relational features in fine-grained opinion analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Sentiment and Subjectivity in Text,</booktitle>
<pages>pages</pages>
<contexts>
<context position="3959" citStr="Kim and Hovy (2006)" startWordPosition="588" endWordPosition="591">ang and Lee, 2008). Nevertheless, much progress has been made in extracting opinion information from text. Sequence labeling models have been successfully employed to identify opinion expressions (e.g. (Breck et al., 1640 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640–1649, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2007; Yang and Cardie, 2012)) and relation extraction techniques have been proposed to extract opinion holders and targets based on their linking relations to the opinion expressions (e.g. Kim and Hovy (2006), Kobayashi et al. (2007)). However, most existing work treats the extraction of different opinion entities and opinion relations in a pipelined manner: the interaction between different extraction tasks is not modeled jointly and error propagation is not considered. One exception is Choi et al. (2006), which proposed an ILP approach to jointly identify opinion holders, opinion expressions and their IS-FROM linking relations, and demonstrated the effectiveness of joint inference. Their ILP formulation, however, does not handle implicit linking relations, i.e. opinion expressions with no explic</context>
<context position="6487" citStr="Kim and Hovy (2006)" startWordPosition="976" endWordPosition="979">n entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Kobayashi et al. (2007) considered extracting “aspect-evaluation” relations (relations between opinion expressions and targets) by identifying opinion expressions first and then searching for the most likely target for each opinion expression via a </context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1– 8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kobayashi</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1065--1074</pages>
<contexts>
<context position="3984" citStr="Kobayashi et al. (2007)" startWordPosition="592" endWordPosition="595">evertheless, much progress has been made in extracting opinion information from text. Sequence labeling models have been successfully employed to identify opinion expressions (e.g. (Breck et al., 1640 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640–1649, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2007; Yang and Cardie, 2012)) and relation extraction techniques have been proposed to extract opinion holders and targets based on their linking relations to the opinion expressions (e.g. Kim and Hovy (2006), Kobayashi et al. (2007)). However, most existing work treats the extraction of different opinion entities and opinion relations in a pipelined manner: the interaction between different extraction tasks is not modeled jointly and error propagation is not considered. One exception is Choi et al. (2006), which proposed an ILP approach to jointly identify opinion holders, opinion expressions and their IS-FROM linking relations, and demonstrated the effectiveness of joint inference. Their ILP formulation, however, does not handle implicit linking relations, i.e. opinion expressions with no explicit opinion holder; nor do</context>
<context position="6861" citStr="Kobayashi et al. (2007)" startWordPosition="1033" endWordPosition="1036">2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Kobayashi et al. (2007) considered extracting “aspect-evaluation” relations (relations between opinion expressions and targets) by identifying opinion expressions first and then searching for the most likely target for each opinion expression via a binary relation classifier. All these methods extract opinion arguments and opinion relations in separate stages instead of extracting them jointly. Most similar to our method is Choi et al. (2006), which jointly extracts opinion expressions, holders and their IS-FROM relations using an ILP approach. In contrast, our approach (1) also considers the IS-ABOUT relation which</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>N. Kobayashi, K. Inui, and Y. Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1065– 1074.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<contexts>
<context position="10197" citStr="Lafferty et al., 2001" startWordPosition="1552" endWordPosition="1555">ts (targets and holders) and their associated linking relations. Training data consists of text with manually annotated opinion expression and argument spans, each with a list of relation ids specifying the linking relation between opinion expressions and their arguments. In this section, we will describe how we model opinion entity identification and opinion relation extraction, and how we combine them in a joint inference model. 3.1 Opinion Entity Identification We formulate the task of opinion entity identification as a sequence labeling problem and employ conditional random fields (CRFs) (Lafferty et al., 2001) to learn the probability of a sequence assignment y for a given sentence x. Through inference we can find the best sequence assignment for sentence x and recover the opinion entities according to the standard “IOB” encoding scheme. We consider four entity labels: D, T, H, N, where D denotes opinion expressions, T denotes opinion targets, H denotes opinion holders and N denotes “NONE” entities. We define potential function fiz that gives the probability of assigning a span i with entity label z, and the probability is estimated based on the learned parameters from CRFs. Formally, given a withi</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Liu</author>
<author>L Xu</author>
<author>J Zhao</author>
</authors>
<title>Opinion target extraction using word-based translation model.</title>
<date>2012</date>
<booktitle>In Proceedings of the conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8221" citStr="Liu et al., 2012" startWordPosition="1238" endWordPosition="1241">implicit opinion relations (opinion expressions without any associated argument), and (3) uses a simpler ILP formulation. There has also been substantial interest in opinion extraction from product reviews (Liu, 2012). Most existing approaches focus on the extraction of opinion targets and their associated opinion expressions and usually employ a pipeline architecture: generate candidates of opinion expressions and opinion targets first, and then use rule-based or machine-learning-based approaches to identify potential relations between opinions and targets (Hu and Liu, 2004; Wu et al., 2009; Liu et al., 2012). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Rot</context>
</contexts>
<marker>Liu, Xu, Zhao, 2012</marker>
<rawString>K. Liu, L. Xu, and J. Zhao. 2012. Opinion target extraction using word-based translation model. In Proceedings of the conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="7821" citStr="Liu, 2012" startWordPosition="1180" endWordPosition="1181">extracting them jointly. Most similar to our method is Choi et al. (2006), which jointly extracts opinion expressions, holders and their IS-FROM relations using an ILP approach. In contrast, our approach (1) also considers the IS-ABOUT relation which is arguably more complex due to the larger variety in the syntactic structure exhibited by opinion expressions and their targets, (2) handles implicit opinion relations (opinion expressions without any associated argument), and (3) uses a simpler ILP formulation. There has also been substantial interest in opinion extraction from product reviews (Liu, 2012). Most existing approaches focus on the extraction of opinion targets and their associated opinion expressions and usually employ a pipeline architecture: generate candidates of opinion expressions and opinion targets first, and then use rule-based or machine-learning-based approaches to identify potential relations between opinions and targets (Hu and Liu, 2004; Wu et al., 2009; Liu et al., 2012). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; how</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Now Pub.</title>
<date>2008</date>
<contexts>
<context position="3358" citStr="Pang and Lee, 2008" startWordPosition="501" endWordPosition="504">c subscripts denote linking relations, one of IS-ABOUT or IS-FROM. In S1, for instance, opinion expression “were irked” (O1) ISABOUT “the government report” (T1). Note that the IS-ABOUT relation can contain an empty target (e.g. “were worried” in S1); similarly for ISFROM w.r.t. the opinion holder (e.g. “predicted” in S2). We also allow an opinion entity to be involved in multiple relations (e.g. “the developed states” in S2). Not surprisingly, fine-grained opinion extraction is a challenging task due to the complexity and variety of the language used to express opinions and their components (Pang and Lee, 2008). Nevertheless, much progress has been made in extracting opinion information from text. Sequence labeling models have been successfully employed to identify opinion expressions (e.g. (Breck et al., 1640 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640–1649, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2007; Yang and Cardie, 2012)) and relation extraction techniques have been proposed to extract opinion holders and targets based on their linking relations to the opinion expressions (e.g. Kim and Hovy (2006</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Now Pub.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="9019" citStr="Punyakanok et al., 2008" startWordPosition="1370" endWordPosition="1373"> targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be assigned. Joint inference has also been applied to semantic role labeling (Punyakanok et al., 2008; Srikumar and Roth, 2011; Das et al., 2012), where the goal is to jointly identify semantic arguments for given lexical predicates. The problem is conceptually similar to identifying opinion arguments for opinion expressions, however, we do not assume prior knowledge of opinion expressions (unlike in SRL, where predicates are given). 3 Model As proposed in Section 1, we consider the task of jointly identifying opinion entities and opinion relations. Specifically, given a sentence, our goal is to identify spans of opinion expressions, opinion arguments (targets and holders) and their associate</context>
<context position="13892" citStr="Punyakanok et al., 2008" startWordPosition="2182" endWordPosition="2185">ation, the top three patterns are (1) o Tdobj a, (2) o Tccomp x Tnsubj a (x is a word in the path that is not covered by either o nor a), (3) o Tccomp a; for the IS-FROM relation, the top three patterns are (1) o Tnsubj a, (2) o Tposs a, (3) o �ccomp x Tnsubj a. Note that generating candidates this way will give us a large number of negative examples. Similar to the preprocessing approach in (Choi et al., 2006), we filter pairs of opinion and argument candidates that do not overlap with any gold standard relation in our training data. Many features we use are common features in the SRL tasks (Punyakanok et al., 2008) due to the similarity of opinion relations to the predicate-argument relations in SRL (Ruppenhofer et al., 2008; Choi et al., 2006). In general, the features aim to capture (a) local properties of the candidate opinion expressions and arguments and (b) syntactic and semantic attributes of their relation. Words and POS tags: the words contained in the candidate and their POS tags. Lexicon: For each word in the candidate, we include its WordNet hypernyms and its strength of subjectivity in the Subjectivity Lexicon3 (e.g. weaksubj, strongsubj). Phrase type: the syntactic category of the deepest </context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Expanding domain sentiment lexicon through double propagation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st international jont conference on Artifical intelligence,</booktitle>
<pages>1199--1204</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="8321" citStr="Qiu et al., 2009" startWordPosition="1252" endWordPosition="1255">ler ILP formulation. There has also been substantial interest in opinion extraction from product reviews (Liu, 2012). Most existing approaches focus on the extraction of opinion targets and their associated opinion expressions and usually employ a pipeline architecture: generate candidates of opinion expressions and opinion targets first, and then use rule-based or machine-learning-based approaches to identify potential relations between opinions and targets (Hu and Liu, 2004; Wu et al., 2009; Liu et al., 2012). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be </context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2009</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double propagation. In Proceedings of the 21st international jont conference on Artifical intelligence, pages 1199–1204. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Qiu</author>
<author>B Liu</author>
<author>J Bu</author>
<author>C Chen</author>
</authors>
<title>Opinion word expansion and target extraction through double propagation.</title>
<date>2011</date>
<journal>Computational linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>27</pages>
<contexts>
<context position="8339" citStr="Qiu et al., 2011" startWordPosition="1256" endWordPosition="1259">n. There has also been substantial interest in opinion extraction from product reviews (Liu, 2012). Most existing approaches focus on the extraction of opinion targets and their associated opinion expressions and usually employ a pipeline architecture: generate candidates of opinion expressions and opinion targets first, and then use rule-based or machine-learning-based approaches to identify potential relations between opinions and targets (Hu and Liu, 2004; Wu et al., 2009; Liu et al., 2012). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be assigned. Joint in</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2011</marker>
<rawString>G. Qiu, B. Liu, J. Bu, and C. Chen. 2011. Opinion word expansion and target extraction through double propagation. Computational linguistics, 37(1):9– 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
<author>D Crystal</author>
</authors>
<title>A comprehensive grammar of the English language, volume 397.</title>
<date>1985</date>
<publisher>Cambridge Univ Press.</publisher>
<contexts>
<context position="2098" citStr="Quirk et al., 1985" startWordPosition="299" endWordPosition="302">orrespond to these key components of opinions. In question-answering systems, for example, users may submit questions in the form “What does entity A think about target B?”; opinion-oriented summarization systems also need to recognize opinions and their targets and holders. In this paper, we address the task of identifying opinion-related entities and opinion relations. We consider three types of opinion entities: opinion expressions or direct subjective expressions as defined in Wiebe et al. (2005) — expressions that explicitly indicate emotions, sentiment, opinions or other private states (Quirk et al., 1985) or speech events expressing private states; opinion targets — expressions that indicate what the opinion is about; and opinion holders — mentions of whom or what the opinion is from. Consider the following examples in which opinion expressions (O) are underlined and targets (T) and holders (H) of the opinion are bracketed. S1: [The workers][H1,2] were irked[O1] by [the government report][T1] and were worried[O2] as they went about their daily chores. S2: From the very start it could be predicted[O1] that on the subject of economic globalization, [the developed states][T1,2] were going to come</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, Crystal, 1985</marker>
<rawString>R. Quirk, S. Greenbaum, G. Leech, J. Svartvik, and D. Crystal. 1985. A comprehensive grammar of the English language, volume 397. Cambridge Univ Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks. Defense Technical Information Center.</title>
<date>2004</date>
<contexts>
<context position="8837" citStr="Roth and Yih (2004)" startWordPosition="1338" endWordPosition="1341">12). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be assigned. Joint inference has also been applied to semantic role labeling (Punyakanok et al., 2008; Srikumar and Roth, 2011; Das et al., 2012), where the goal is to jointly identify semantic arguments for given lexical predicates. The problem is conceptually similar to identifying opinion arguments for opinion expressions, however, we do not assume prior knowledge of opinion expressions (unlike in SRL, where predicates are given). 3 Model As proposed in Section 1, we consider the task of jointly identifying opi</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. Defense Technical Information Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Finding the sources and targets of subjective expressions.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="6608" citStr="Ruppenhofer et al. (2008)" startWordPosition="995" endWordPosition="998">nto fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Kobayashi et al. (2007) considered extracting “aspect-evaluation” relations (relations between opinion expressions and targets) by identifying opinion expressions first and then searching for the most likely target for each opinion expression via a binary relation classifier. All these methods extract opinion arguments and opinion relations in separate stages instead </context>
<context position="14004" citStr="Ruppenhofer et al., 2008" startWordPosition="2199" endWordPosition="2203">vered by either o nor a), (3) o Tccomp a; for the IS-FROM relation, the top three patterns are (1) o Tnsubj a, (2) o Tposs a, (3) o �ccomp x Tnsubj a. Note that generating candidates this way will give us a large number of negative examples. Similar to the preprocessing approach in (Choi et al., 2006), we filter pairs of opinion and argument candidates that do not overlap with any gold standard relation in our training data. Many features we use are common features in the SRL tasks (Punyakanok et al., 2008) due to the similarity of opinion relations to the predicate-argument relations in SRL (Ruppenhofer et al., 2008; Choi et al., 2006). In general, the features aim to capture (a) local properties of the candidate opinion expressions and arguments and (b) syntactic and semantic attributes of their relation. Words and POS tags: the words contained in the candidate and their POS tags. Lexicon: For each word in the candidate, we include its WordNet hypernyms and its strength of subjectivity in the Subjectivity Lexicon3 (e.g. weaksubj, strongsubj). Phrase type: the syntactic category of the deepest constituent that covers the candidate in the parse tree, e.g. NP, VP. Semantic frames: For each verb in the opin</context>
</contexts>
<marker>Ruppenhofer, Somasundaran, Wiebe, 2008</marker>
<rawString>J. Ruppenhofer, S. Somasundaran, and J. Wiebe. 2008. Finding the sources and targets of subjective expressions. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Srikumar</author>
<author>Dan Roth</author>
</authors>
<title>A joint model for extended semantic role labeling.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9044" citStr="Srikumar and Roth, 2011" startWordPosition="1374" endWordPosition="1377">ever, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be assigned. Joint inference has also been applied to semantic role labeling (Punyakanok et al., 2008; Srikumar and Roth, 2011; Das et al., 2012), where the goal is to jointly identify semantic arguments for given lexical predicates. The problem is conceptually similar to identifying opinion arguments for opinion expressions, however, we do not assume prior knowledge of opinion expressions (unlike in SRL, where predicates are given). 3 Model As proposed in Section 1, we consider the task of jointly identifying opinion entities and opinion relations. Specifically, given a sentence, our goal is to identify spans of opinion expressions, opinion arguments (targets and holders) and their associated linking relations. Trai</context>
</contexts>
<marker>Srikumar, Roth, 2011</marker>
<rawString>Vivek Srikumar and Dan Roth. 2011. A joint model for extended semantic role labeling. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 129–139. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
</authors>
<title>Topic identification for fine-grained opinion analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>817--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6363" citStr="Stoyanov and Cardie, 2008" startWordPosition="959" endWordPosition="962">e that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Kobayashi et al. (2007) considered extracting “aspect-evaluation” relations (relations between opinion expressions and target</context>
</contexts>
<marker>Stoyanov, Cardie, 2008</marker>
<rawString>V. Stoyanov and C. Cardie. 2008. Topic identification for fine-grained opinion analysis. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 817–824. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<volume>39</volume>
<issue>2</issue>
<pages>210</pages>
<contexts>
<context position="1325" citStr="Wiebe et al., 2005" startWordPosition="181" endWordPosition="184">joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction, and seeks a globally optimal solution. Experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction. 1 Introduction Fine-grained opinion analysis is concerned with identifying opinions in text at the expression level; this includes identifying the subjective (i.e., opinion) expression itself, the opinion holder and the target of the opinion (Wiebe et al., 2005). The task has received increasing attention as many natural language processing applications would benefit from the ability to identify text spans that correspond to these key components of opinions. In question-answering systems, for example, users may submit questions in the form “What does entity A think about target B?”; opinion-oriented summarization systems also need to recognize opinions and their targets and holders. In this paper, we address the task of identifying opinion-related entities and opinion relations. We consider three types of opinion entities: opinion expressions or dire</context>
<context position="5721" citStr="Wiebe et al., 2005" startWordPosition="858" endWordPosition="861">ification. A joint inference framework is proposed to jointly optimize the predictors for different subproblems with constraints that enforce global consistency. We hypothesize that the ambiguity in the extraction results will be reduced and thus, performance increased. For example, uncertainty w.r.t. the spans of opinion entities can adversely affect the prediction of opinion relations; and evidence of opinion relations might provide clues to guide the accurate extraction of opinion entities. We evaluate our approach using a standard corpus for fine-grained opinion analysis (the MPQA corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topi</context>
<context position="21385" citStr="Wiebe et al., 2005" startWordPosition="3449" endWordPosition="3452">f opinion entities, and the score for each possible label assignment is obtained by 5It is possible to add more auxiliary variables to allow more than three arguments to link to an opinion expression, but this rarely happens in our experiments. For the IS-FROM relation, we set aik = 0, bik = 0 since an opinion expression usually has only one holder. 1644 the sum of raw scores from m independent extraction models. This design choice also allows us to easily deal with multiple types of opinion arguments and opinion relations. 4 Experiments For evaluation, we used version 2.0 of the MPQA corpus (Wiebe et al., 2005; Wilson, 2008), a widely used data set for fine-grained opinion analysis.6 We considered the subset of 482 documents7 that contain attitude and target annotations. There are a total of 9,471 sentences with opinionrelated labels at the phrase level. We set aside 132 documents as a development set and use 350 documents as the evaluation set. All experiments employ 10-fold cross validation on the evaluation set; the average over the 10 runs is reported. Our gold standard opinion expressions, opinion targets and opinion holders correspond to the direct subjective annotations, target annotations a</context>
<context position="23777" citStr="Wiebe et al., 2005" startWordPosition="3825" endWordPosition="3828">49 news articles from the original MPQA corpus, 84 Wall Street Journal articles (Xbank), and 48 articles from the American National Corpus. 8Overlap matching considers two spans to match if they overlap, while exact matching requires two spans to be exactly the same. Opinion Target Holder TotalNum 5849 4676 4244 Opinion-arg Relations Implicit Relations IS-ABOUT 4823 1302 IS-FROM 4662 1187 Table 1: Data Statistics of the MPQA Corpus. will focus our discussion on results obtained using overlap matching, since the exact boundaries of opinion entities are hard to define even for human annotators (Wiebe et al., 2005). We trained CRFs for opinion entity identification using the following features: indicators for words, POS tags, and lexicon features (the subjectivity strength of the word in the Subjectivity Lexicon). All features are computed for the current token and tokens in a [−1, +1] window. We used L2-regularization; the regularization parameter was tuned using the development set. We trained the classifiers for relation extraction using L1-regularized logistic regression with default parameters using the LIBLINEAR (Fan et al., 2008) package. For joint inference, we used GLPK9 to provide the optimal </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2):165– 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<booktitle>Computational linguistics,</booktitle>
<pages>35--3</pages>
<contexts>
<context position="6103" citStr="Wilson et al., 2009" startWordPosition="916" endWordPosition="920">elations; and evidence of opinion relations might provide clues to guide the accurate extraction of opinion entities. We evaluate our approach using a standard corpus for fine-grained opinion analysis (the MPQA corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targe</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational linguistics, 35(3):399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Fine-Grained Subjectivity Analysis.</title>
<date>2008</date>
<tech>Ph.D. thesis, Ph. D. thesis,</tech>
<institution>University of Pittsburgh. Intelligent Systems Program.</institution>
<contexts>
<context position="21400" citStr="Wilson, 2008" startWordPosition="3453" endWordPosition="3454">and the score for each possible label assignment is obtained by 5It is possible to add more auxiliary variables to allow more than three arguments to link to an opinion expression, but this rarely happens in our experiments. For the IS-FROM relation, we set aik = 0, bik = 0 since an opinion expression usually has only one holder. 1644 the sum of raw scores from m independent extraction models. This design choice also allows us to easily deal with multiple types of opinion arguments and opinion relations. 4 Experiments For evaluation, we used version 2.0 of the MPQA corpus (Wiebe et al., 2005; Wilson, 2008), a widely used data set for fine-grained opinion analysis.6 We considered the subset of 482 documents7 that contain attitude and target annotations. There are a total of 9,471 sentences with opinionrelated labels at the phrase level. We set aside 132 documents as a development set and use 350 documents as the evaluation set. All experiments employ 10-fold cross validation on the evaluation set; the average over the 10 runs is reported. Our gold standard opinion expressions, opinion targets and opinion holders correspond to the direct subjective annotations, target annotations and agent annota</context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>Theresa Wilson. 2008. Fine-Grained Subjectivity Analysis. Ph.D. thesis, Ph. D. thesis, University of Pittsburgh. Intelligent Systems Program.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wu</author>
<author>Q Zhang</author>
<author>X Huang</author>
<author>L Wu</author>
</authors>
<title>Phrase dependency parsing for opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1533--1541</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8202" citStr="Wu et al., 2009" startWordPosition="1234" endWordPosition="1237">ets, (2) handles implicit opinion relations (opinion expressions without any associated argument), and (3) uses a simpler ILP formulation. There has also been substantial interest in opinion extraction from product reviews (Liu, 2012). Most existing approaches focus on the extraction of opinion targets and their associated opinion expressions and usually employ a pipeline architecture: generate candidates of opinion expressions and opinion targets first, and then use rule-based or machine-learning-based approaches to identify potential relations between opinions and targets (Hu and Liu, 2004; Wu et al., 2009; Liu et al., 2012). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is th</context>
</contexts>
<marker>Wu, Zhang, Huang, Wu, 2009</marker>
<rawString>Y. Wu, Q. Zhang, X. Huang, and L. Wu. 2009. Phrase dependency parsing for opinion mining. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1533–1541. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Yang</author>
<author>C Cardie</author>
</authors>
<title>Extracting opinion expressions with semi-markov conditional random fields.</title>
<date>2012</date>
<booktitle>In Proceedings of the conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3779" citStr="Yang and Cardie, 2012" startWordPosition="559" endWordPosition="562">tes” in S2). Not surprisingly, fine-grained opinion extraction is a challenging task due to the complexity and variety of the language used to express opinions and their components (Pang and Lee, 2008). Nevertheless, much progress has been made in extracting opinion information from text. Sequence labeling models have been successfully employed to identify opinion expressions (e.g. (Breck et al., 1640 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640–1649, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2007; Yang and Cardie, 2012)) and relation extraction techniques have been proposed to extract opinion holders and targets based on their linking relations to the opinion expressions (e.g. Kim and Hovy (2006), Kobayashi et al. (2007)). However, most existing work treats the extraction of different opinion entities and opinion relations in a pipelined manner: the interaction between different extraction tasks is not modeled jointly and error propagation is not considered. One exception is Choi et al. (2006), which proposed an ILP approach to jointly identify opinion holders, opinion expressions and their IS-FROM linking r</context>
<context position="6267" citStr="Yang and Cardie, 2012" startWordPosition="942" endWordPosition="945">rpus for fine-grained opinion analysis (the MPQA corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Kobayashi et al. (2007) consi</context>
</contexts>
<marker>Yang, Cardie, 2012</marker>
<rawString>B. Yang and C. Cardie. 2012. Extracting opinion expressions with semi-markov conditional random fields. In Proceedings of the conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Zhang</author>
<author>Bing Liu</author>
<author>Suk Hwan Lim</author>
<author>Eamonn O’Brien-Strain</author>
</authors>
<title>Extracting and ranking product features in opinion documents.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>1462--1470</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Zhang, Liu, Lim, O’Brien-Strain, 2010</marker>
<rawString>Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O’Brien-Strain. 2010. Extracting and ranking product features in opinion documents. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 1462–1470. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>