<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008316">
<title confidence="0.979863">
Cross-lingual Model Transfer Using Feature Representation Projection
</title>
<author confidence="0.992797">
Mikhail Kozhevnikov Ivan Titov
</author>
<affiliation confidence="0.7774585">
MMCI, University of Saarland ILLC, University of Amsterdam
Saarbr¨ucken, Germany Amsterdam, Netherlands
</affiliation>
<email confidence="0.99214">
mkozhevn@mmci.uni-saarland.de titov@uva.nl
</email>
<sectionHeader confidence="0.99369" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989">
We propose a novel approach to cross-
lingual model transfer based on feature
representation projection. First, a com-
pact feature representation relevant for the
task in question is constructed for either
language independently and then the map-
ping between the two representations is
determined using parallel data. The tar-
get instance can then be mapped into
the source-side feature representation us-
ing the derived mapping and handled di-
rectly by the source-side model. This ap-
proach displays competitive performance
on model transfer for semantic role label-
ing when compared to direct model trans-
fer and annotation projection and suggests
interesting directions for further research.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999876629032258">
Cross-lingual model transfer approaches are con-
cerned with creating statistical models for var-
ious tasks for languages poor in annotated re-
sources, utilising resources or models available
for these tasks in other languages. That includes
approaches such as direct model transfer (Ze-
man and Resnik, 2008) and annotation projec-
tion (Yarowsky et al., 2001). Such methods have
been successfully applied to a variety of tasks,
including POS tagging (Xi and Hwa, 2005; Das
and Petrov, 2011; T¨ackstr¨om et al., 2013), syntac-
tic parsing (Ganchev et al., 2009; Smith and Eis-
ner, 2009; Hwa et al., 2005; Durrett et al., 2012;
Søgaard, 2011), semantic role labeling (Pad´o and
Lapata, 2009; Annesi and Basili, 2010; Tonelli
and Pianta, 2008; Kozhevnikov and Titov, 2013)
and others.
Direct model transfer attempts to find a shared
feature representation for samples from the two
languages, usually generalizing and abstract-
ing away from language-specific representations.
Once this is achieved, instances from both lan-
guages can be mapped into this space and a model
trained on the source-language data directly ap-
plied to the target language. If parallel data is
available, it can be further used to enforce model
agreement on this data to adjust for discrepancies
between the two languages, for example by means
of projected transfer (McDonald et al., 2011).
The shared feature representation depends on
the task in question, but most often each aspect
of the original feature representation is handled
separately. Word types, for example, may be re-
placed by cross-lingual word clusters (T¨ackstr¨om
et al., 2012) or cross-lingual distributed word rep-
resentations (Klementiev et al., 2012). Part-of-
speech tags, which are often language-specific,
can be converted into universal part-of-speech
tags (Petrov et al., 2012) and morpho-syntactic
information can also be represented in a unified
way (Zeman et al., 2012; McDonald et al., 2013;
Tsarfaty, 2013). Unfortunately, the design of such
representations and corresponding conversion pro-
cedures is by no means trivial.
Annotation projection, on the other hand, does
not require any changes to the feature represen-
tation. Instead, it operates on translation pairs,
usually on sentence level, applying the available
source-side model to the source sentence and
transferring the resulting annotations through the
word alignment links to the target one. The quality
of predictions on source sentences depends heav-
ily on the quality of parallel data and the domain
it belongs to (or, rather, the similarity between this
domain and that of the corpus the source-language
model was trained on). The transfer itself also
introduces errors due to translation shifts (Cyrus,
2006) and word alignment errors, which may lead
to inaccurate predictions. These issues are gen-
erally handled using heuristics (Pad´o and Lapata,
2006) and filtering, for example based on align-
ment coverage (van der Plas et al., 2011).
</bodyText>
<page confidence="0.97754">
579
</page>
<note confidence="0.602914">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9959375">
Figure 1: Dependency-based semantic role labeling example. The top arcs depict dependency relations,
the bottom ones – semantic role structure. Rendered with https://code.google.com/p/whatswrong/.
</figureCaption>
<subsectionHeader confidence="0.999008">
1.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999903529411765">
The approach proposed here, which we will refer
to as feature representation projection (FRP), con-
stitutes an alternative to direct model transfer and
annotation projection and can be seen as a com-
promise between the two.
It is similar to direct transfer in that we also
use a shared feature representation. Instead of
designing this representation manually, however,
we create compact monolingual feature represen-
tations for source and target languages separately
and automatically estimate the mapping between
the two from parallel data. This allows us to make
use of language-specific annotations and account
for the interplay between different types of infor-
mation. For example, a certain preposition at-
tached to a token in the source language might
map into a morphological tag in the target lan-
guage, which would be hard to handle for tradi-
tional direct model transfer other than using some
kind of refinement procedure involving parallel
data. Note also that any such refinement procedure
applicable to direct transfer would likely work for
FRP as well.
Compared to annotation projection, our ap-
proach may be expected to be less sensitive to par-
allel data quality, since we do not have to com-
mit to a particular prediction on a given instance
from parallel data. We also believe that FRP
may profit from using other sources of informa-
tion about the correspondence between source and
target feature representations, such as dictionary
entries, and thus have an edge over annotation pro-
jection in those cases where the amount of parallel
data available is limited.
</bodyText>
<sectionHeader confidence="0.993306" genericHeader="introduction">
2 Evaluation
</sectionHeader>
<bodyText confidence="0.999946433333333">
We evaluate feature representation projection on
the task of dependency-based semantic role label-
ing (SRL) (Hajiˇc et al., 2009).
This task consists in identifying predicates and
their arguments in sentences and assigning each
argument a semantic role with respect to its pred-
icate (see figure 1). Note that only a single word
– the syntactic head of the argument phrase – is
marked as an argument in this case, as opposed
to constituent- or span-based SRL (Carreras and
M`arquez, 2005). We focus on the assignment of
semantic roles to identified arguments.
For the sake of simplicity we cast it as a multi-
class classification problem, ignoring the interac-
tion between different arguments in a predicate. It
is well known that such interaction plays an impor-
tant part in SRL (Punyakanok et al., 2008), but it
is not well understood which kinds of interactions
are preserved across languages and which are not.
Also, should one like to apply constraints on the
set of semantic roles in a given predicate, or, for
example, use a reranker (Bj¨orkelund et al., 2009),
this can be done using a factorized model obtained
by cross-lingual transfer.
In our setting, each instance includes the word
type and part-of-speech and morphological tags (if
any) of argument token, its parent and correspond-
ing predicate token, as well as their dependency
relations to their respective parents. This repre-
sentation is further denoted W0.
</bodyText>
<subsectionHeader confidence="0.989464">
2.1 Approach
</subsectionHeader>
<bodyText confidence="0.999839454545455">
We consider a pair of languages (Ls, Lt) and
assume that an annotated training set DsT =
{(xs, ys)} is available in the source language as
well as a parallel corpus of instance pairs Dst =
{(xs, xt)} and a target dataset DtE = {xt} that
needs to be labeled.
We design a pair of intermediate compact
monolingual feature representations Ws and Wt
and models Ms and Mt to map source and target
samples xs and xt from their original representa-
tions, Ws0 and Wt0, to the new ones. We use the par-
</bodyText>
<page confidence="0.836052">
580
</page>
<equation confidence="0.683655">
allel instances in the new feature representation
Dst = (xs1, xi~ = (Ms (xs), Mt(xt)~
</equation>
<bodyText confidence="0.9968952">
to determine the mapping Mts (usually, linear) be-
tween the two spaces:
xs 1 − M(xt 1) 2
Then a classification model My is trained on the
source training data
</bodyText>
<equation confidence="0.998123">
¯DsT = {(xs1,ys)} = {(Ms(xs),ys)}
</equation>
<bodyText confidence="0.6750485">
and the labels are assigned to the target samples
xt E Dt E using a composition of the models:
</bodyText>
<equation confidence="0.892142">
yt = My(Mts(Mt(xt)))
</equation>
<subsectionHeader confidence="0.996716">
2.2 Feature Representation
</subsectionHeader>
<bodyText confidence="0.999992222222222">
Our objective is to make the feature represen-
tation sufficiently compact that the mapping be-
tween source and target feature spaces could be
reliably estimated from a limited amount of paral-
lel data, while preserving, insofar as possible, the
information relevant for classification.
Estimating the mapping directly from raw cat-
egorical features (w0) is both computationally ex-
pensive and likely inaccurate – using one-hot en-
coding the feature vectors in our experiments
would have tens of thousands of components.
There is a number of ways to make this repre-
sentation more compact. To start with, we re-
place word types with corresponding neural lan-
guage model representations estimated using the
skip-gram model (Mikolov et al., 2013a). This
corresponds to Ms and Mt above and reduces the
dimension of the feature space, making direct es-
timation of the mapping practical. We will refer to
this representation as w1.
To go further, one can, for example, apply
dimensionality reduction techniques to obtain a
more compact representation of w1 by eliminating
redundancy or define auxiliary tasks and produce
a vector representation useful for those tasks. In
source language, one can even directly tune an in-
termediate representation for the target problem.
</bodyText>
<subsectionHeader confidence="0.998713">
2.3 Baselines
</subsectionHeader>
<bodyText confidence="0.99998865625">
As mentioned above we compare the performance
of this approach to that of direct transfer and an-
notation projection. Both baselines are using the
same set of features as the proposed model, as de-
scribed earlier.
The shared feature representation for di-
rect transfer is derived from w0 by replacing
language-specific part-of-speech tags with univer-
sal ones (Petrov et al., 2012) and adding cross-
lingual word clusters (T¨ackstr¨om et al., 2012) to
word types. The word types themselves are left as
they are in the source language and replaced with
their gloss translations in the target one (Zeman
and Resnik, 2008). In English-Czech and Czech-
English we also use the dependency relation infor-
mation, since the annotations are partly compati-
ble.
The annotation projection baseline implementa-
tion is straightforward. The source-side instances
from a parallel corpus are labeled using a classi-
fier trained on source-language training data and
transferred to the target side. The resulting anno-
tations are then used to train a target-side classifier
for evaluation. Note that predicate and argument
identification in both languages is performed us-
ing monolingual classifiers and only aligned pairs
are used in projection. A more common approach
would be to project the whole structure from the
source language, but in our case this may give
unfair advantage to feature representation projec-
tion, which relies on target-side argument identifi-
cation.
</bodyText>
<subsectionHeader confidence="0.991875">
2.4 Tools
</subsectionHeader>
<bodyText confidence="0.99991195">
We use the same type of log-linear classifiers
in the model itself and the two baselines to
avoid any discrepancy due to learning proce-
dure. These classifiers are implemented using
PYLEARN2 (Goodfellow et al., 2013), based on
THEANO (Bergstra et al., 2010). We also use this
framework to estimate the linear mapping Mts be-
tween source and target feature spaces in FRP.
The 250-dimensional word representations for
w1 are obtained using WORD2VEC tool. Both
monolingual data and that from the parallel cor-
pus are included in the training. In Mikolov et al.
(2013b) the authors consider embeddings of up to
800 dimensions, but we would not expect to bene-
fit as much from larger vectors since we are using
a much smaller corpus to train them. We did not
tune the size of the word representation to our task,
as this would not be appropriate in a cross-lingual
transfer setup, but we observe that the classifier
is relatively robust to their dimension when evalu-
</bodyText>
<equation confidence="0.9677295">
XMts = argmaxM
(xs1,xt1∈�Dst)
</equation>
<page confidence="0.956849">
581
</page>
<bodyText confidence="0.999974111111111">
ated on source language – in our experiments the
performance of the monolingual classifier does not
improve significantly if the dimension is increased
past 300 and decreases only by a small margin
(less than one absolute point) if it is reduced to
100. It should be noted, however, that the dimen-
sion that is optimal in this sense is not necessarily
the best choice for FRP, especially if the amount
of available parallel data is limited.
</bodyText>
<subsectionHeader confidence="0.95587">
2.5 Data
</subsectionHeader>
<bodyText confidence="0.998648227272728">
We use two language pairs for evaluation:
English-Czech and English-French. In the first
case, the data is converted from Prague Czech-
English Dependency Treebank 2.0 (Hajiˇc et al.,
2012) using the script from Kozhevnikov and
Titov (2013). In the second, we use CoNLL 2009
shared task (Hajiˇc et al., 2009) corpus for English
and the manually corrected dataset from van der
Plas et al. (2011) for French. Since the size of
the latter dataset is relatively small – one thou-
sand sentences – we reserve the whole dataset for
testing and only evaluate transfer from English to
French, but not the other way around. Datasets for
other languages are sufficiently large, so we take
30 thousand samples for testing and use the rest
as training data. The validation set in each exper-
iment is withheld from the corresponding training
corpus and contains 10 thousand samples.
Parallel data for both language pairs is de-
rived from Europarl (Koehn, 2005), which we pre-
process using MATE-TOOLS (Bj¨orkelund et al.,
2009; Bohnet, 2010).
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.9999720625">
The classification error of FRP and the baselines
given varying amount of parallel data is reported
in figures 2, 3 and 4. The training set for each
language is fixed. We denote the two baselines AP
(annotation projection) and DT (direct transfer).
The number of parallel instances in these exper-
iments is shown on a logarithmic scale, the values
considered are 2, 5, 10, 20 and 50 thousand pairs.
Please note that we report only a single value
for direct transfer, since this approach does not ex-
plicitly rely on parallel data. Although some of
the features – namely, gloss translations and cross-
lingual clusters – used in direct transfer are, in fact,
derived from parallel data, we consider the effect
of this on the performance of direct transfer to be
indirect and outside the scope of this work.
</bodyText>
<figure confidence="0.836044">
Error
Number of parallel instances, ×103
</figure>
<figureCaption confidence="0.899303">
Figure 2: English-Czech transfer results
</figureCaption>
<figure confidence="0.9377435">
Error
Number of parallel instances, ×103
</figure>
<figureCaption confidence="0.999901">
Figure 3: Czech-English transfer results
</figureCaption>
<bodyText confidence="0.9991964">
The rather inferior performance of direct trans-
fer baseline on English-French may be partially
attributed to the fact that it cannot rely on depen-
dency relation features, as the corpora we consider
make use of different dependency relation inven-
tories. Replacing language-specific dependency
annotations with the universal ones (McDonald
et al., 2013) may help somewhat, but we would
still expect the methods directly relying on paral-
lel data to achieve better results given a sufficiently
large parallel corpus.
Overall, we observe that the proposed method
with w1 representation demonstrates performance
competitive to direct transfer and annotation pro-
jection baselines.
</bodyText>
<figure confidence="0.97451515">
2 5 10 20 50
0.42
0.40
0.38
0.36
0.34
FRP
AP
DT
2 5 10 20 50
0.40
0.38
0.36
0.34
0.32
FRP
AP
DT
582
Error
</figure>
<figureCaption confidence="0.7900905">
Number of parallel instances, ×103
Figure 4: English-French transfer results
</figureCaption>
<sectionHeader confidence="0.991694" genericHeader="method">
4 Additional Related Work
</sectionHeader>
<bodyText confidence="0.999852454545454">
Apart from the work on direct/projected transfer
and annotation projection mentioned above, the
proposed method can be seen as a more explicit
kind of domain adaptation, similar to Titov (2011)
or Blitzer et al. (2006).
It is also somewhat similar in spirit to Mikolov
et al. (2013b), where a small number of word
translation pairs are used to estimate a mapping
between distributed representations of words in
two different languages and build a word transla-
tion model.
</bodyText>
<sectionHeader confidence="0.999071" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999551857142857">
In this paper we propose a new method of cross-
lingual model transfer, report initial evaluation re-
sults and highlight directions for its further devel-
opment.
We observe that the performance of this method
is competitive with that of established cross-
lingual transfer approaches and its application re-
quires very little manual adjustment – no heuris-
tics or filtering and no explicit shared feature rep-
resentation design. It also retains compatibility
with any refinement procedures similar to pro-
jected transfer (McDonald et al., 2011) that may
have been designed to work in conjunction with
direct model transfer.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.9957785">
This paper reports work in progress and there is
a number of directions we would like to pursue
further.
Better Monolingual Representations The rep-
resentation we used in the initial evaluation does
not discriminate between aspects that are relevant
for the assignment of semantic roles and those that
are not. Since we are using a relatively small set of
features to start with, this does not present much of
a problem. In general, however, retaining only rel-
evant aspects of intermediate monolingual repre-
sentations would simplify the estimation of map-
ping between them and make FRP more robust.
For source language, this is relatively straight-
forward, as the intermediate representation can be
directly tuned for the problem in question using
labeled training data. For target language, how-
ever, we assume that no labeled data is available
and auxiliary tasks have to be used to achieve this.
Alternative Sources of Information The
amount of parallel data available for many
language pairs is growing steadily. However,
cross-lingual transfer methods are often applied
in cases where parallel resources are scarce or of
poor quality and must be used with care. In such
situations an ability to use alternative sources of
information may be crucial. Potential sources
of such information include dictionary entries or
information about the mapping between certain
elements of syntactic structure, for example a
known part-of-speech tag mapping.
The available parallel data itself may also be
used more comprehensively – aligned arguments
of aligned predicates, for example, constitute only
a small part of it, while the mapping of vector rep-
resentations of individual tokens is likely to be the
same for all aligned words.
Multi-source Transfer One of the strong points
of direct model transfer is that it naturally fits the
multi-source transfer setting. There are several
possible ways of adapting FRP to such a setting.
It remains to be seen which one would produce
the best results and how multi-source feature rep-
resentation projection would compare to, for ex-
ample, multi-source projected transfer (McDonald
et al., 2011).
</bodyText>
<sectionHeader confidence="0.980161" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996744">
The authors would like to acknowledge the
support of MMCI Cluster of Excellence and
Saarbr¨ucken Graduate School of Computer Sci-
ence and thank the anonymous reviewers for their
suggestions.
</bodyText>
<figure confidence="0.98146825">
2 5 10 20 50
0.40
0.38
0.36
0.34
FRP
AP
DT
</figure>
<page confidence="0.997753">
583
</page>
<sectionHeader confidence="0.960285" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996754154545454">
Paolo Annesi and Roberto Basili. 2010. Cross-lingual
alignment of FrameNet annotations through hidden
Markov models. In Proceedings of the 11th interna-
tional conference on Computational Linguistics and
Intelligent Text Processing, CICLing’10, pages 12–
25. Springer-Verlag.
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), Austin, TX.
Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43–48, Boulder, Colorado, June.
Association for Computational Linguistics.
John Blitzer, Ryan McDonal, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. Conference on Empirical
Methods in Natural Language Processing, Sydney,
Australia.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89–97, Bei-
jing, China, August.
Xavier Carreras and Llufs M`arquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI USA.
Lea Cyrus. 2006. Building a resource for studying
translation shifts. CoRR, abs/cs/0606096.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600–609, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1–11,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
369–377, Suntec, Singapore, August. Association
for Computational Linguistics.
Ian J. Goodfellow, David Warde-Farley, Pascal Lam-
blin, Vincent Dumoulin, Mehdi Mirza, Razvan Pas-
canu, James Bergstra, Fr´ed´eric Bastien, and Yoshua
Bengio. 2013. Pylearn2: a machine learning re-
search library. CoRR, abs/1308.4214.
Jan Haji&amp;quot;c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Martf, Llufs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan &amp;quot;St&amp;quot;ep´anek, Pavel Stra&amp;quot;n´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1–18, Boulder, Colorado.
Jan Haji&amp;quot;c, Eva Haji&amp;quot;cov´a, Jarmila Panevov´a, Petr
Sgall, Ond&amp;quot;rej Bojar, Silvie Cinkov´a, Eva Fu&amp;quot;c´ıkov´a,
Marie Mikulov´a, Petr Pajas, Jan Popelka, Ji&amp;quot;rfSemeck´y, Jana &amp;quot;Sindlerov´a, Jan &amp;quot;St&amp;quot;ep´anek, Josef
Toman, Zde&amp;quot;nka Ure&amp;quot;sov´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y.
2012. Announcing Prague Czech-English depen-
dency treebank 2.0. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet U&amp;quot;gur Do&amp;quot;gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12), Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA).
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel text.
Natural Language Engineering, 11(3):311–325.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), Bombay, India.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand. AAMT.
Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1190–1200, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 62–72, Edinburgh, United King-
dom. Association for Computational Linguistics.
</reference>
<page confidence="0.993766">
584
</page>
<reference confidence="0.999707297297297">
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria
Bertomeu Castell´o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92–97, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages for
machine translation. CoRR, abs/1309.4168.
Sebastian Pad´o and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for semantic
projection. In Proc. 44th Annual Meeting of Associ-
ation for Computational Linguistics and 21st Inter-
national Conf. on Computational Linguistics, ACL-
COLING 2006, pages 1161–1168, Sydney, Aus-
tralia.
Sebastian Pad´o and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36:307–
340.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287.
David A Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 822–831. Association for Com-
putational Linguistics.
Anders Søgaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, volume 2 of HLT ’11, pages
682–686, Portland, Oregon. Association for Com-
putational Linguistics.
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proc. of the An-
nual Meeting of the North American Association
of Computational Linguistics (NAACL), pages 477–
487, Montr´eal, Canada.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1–12.
Ivan Titov. 2011. Domain adaptation by constraining
inter-domain variability of latent feature representa-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 62–71, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Sara Tonelli and Emanuele Pianta. 2008. Frame infor-
mation transfer from English to Italian. In Proceed-
ings of LREC 2008.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 578–584, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
HLT ’11, pages 299–304, Portland, Oregon, USA.
Association for Computational Linguistics.
Chenhai Xi and Rebecca Hwa. 2005. A backoff
model for bootstrapping resources for non-english
languages. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
851–858, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, pages 1–
8. Association for Computational Linguistics.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of the IJCNLP-08 Workshop
on NLP for Less Privileged Languages, pages 35–
42, Hyderabad, India, January. Asian Federation of
Natural Language Processing.
Daniel Zeman, David Mareˇcek, Martin Popel,
Loganathan Ramasamy, Jan ˇStˇep´anek, Zdenˇek
ˇZabokrtsk´y, and Jan Hajiˇc. 2012. Hamledt: To
parse or not to parse? In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uˇgur Doˇgan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12), Is-
tanbul, Turkey, may. European Language Resources
Association (ELRA).
</reference>
<page confidence="0.99875">
585
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931830">
<title confidence="0.998701">Cross-lingual Model Transfer Using Feature Representation Projection</title>
<author confidence="0.999623">Mikhail Kozhevnikov Ivan Titov</author>
<affiliation confidence="0.999302">MMCI, University of Saarland ILLC, University of Amsterdam</affiliation>
<address confidence="0.985635">Saarbr¨ucken, Germany Amsterdam, Netherlands</address>
<email confidence="0.96829">mkozhevn@mmci.uni-saarland.detitov@uva.nl</email>
<abstract confidence="0.998707777777778">We propose a novel approach to crossmodel transfer based on First, a compact feature representation relevant for the task in question is constructed for either language independently and then the mapping between the two representations is determined using parallel data. The target instance can then be mapped into the source-side feature representation using the derived mapping and handled directly by the source-side model. This approach displays competitive performance on model transfer for semantic role labeling when compared to direct model transfer and annotation projection and suggests interesting directions for further research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paolo Annesi</author>
<author>Roberto Basili</author>
</authors>
<title>Cross-lingual alignment of FrameNet annotations through hidden Markov models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th international conference on Computational Linguistics and Intelligent Text Processing, CICLing’10,</booktitle>
<pages>12--25</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1660" citStr="Annesi and Basili, 2010" startWordPosition="241" endWordPosition="244">odels for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language. If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means </context>
</contexts>
<marker>Annesi, Basili, 2010</marker>
<rawString>Paolo Annesi and Roberto Basili. 2010. Cross-lingual alignment of FrameNet annotations through hidden Markov models. In Proceedings of the 11th international conference on Computational Linguistics and Intelligent Text Processing, CICLing’10, pages 12– 25. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference</booktitle>
<location>(SciPy), Austin, TX.</location>
<contexts>
<context position="11236" citStr="Bergstra et al., 2010" startWordPosition="1767" endWordPosition="1770">ument identification in both languages is performed using monolingual classifiers and only aligned pairs are used in projection. A more common approach would be to project the whole structure from the source language, but in our case this may give unfair advantage to feature representation projection, which relies on target-side argument identification. 2.4 Tools We use the same type of log-linear classifiers in the model itself and the two baselines to avoid any discrepancy due to learning procedure. These classifiers are implemented using PYLEARN2 (Goodfellow et al., 2013), based on THEANO (Bergstra et al., 2010). We also use this framework to estimate the linear mapping Mts between source and target feature spaces in FRP. The 250-dimensional word representations for w1 are obtained using WORD2VEC tool. Both monolingual data and that from the parallel corpus are included in the training. In Mikolov et al. (2013b) the authors consider embeddings of up to 800 dimensions, but we would not expect to benefit as much from larger vectors since we are using a much smaller corpus to train them. We did not tune the size of the word representation to our task, as this would not be appropriate in a cross-lingual </context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task,</booktitle>
<pages>43--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task, pages 43–48, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonal</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proc. Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="15519" citStr="Blitzer et al. (2006)" startWordPosition="2481" endWordPosition="2484">ficiently large parallel corpus. Overall, we observe that the proposed method with w1 representation demonstrates performance competitive to direct transfer and annotation projection baselines. 2 5 10 20 50 0.42 0.40 0.38 0.36 0.34 FRP AP DT 2 5 10 20 50 0.40 0.38 0.36 0.34 0.32 FRP AP DT 582 Error Number of parallel instances, ×103 Figure 4: English-French transfer results 4 Additional Related Work Apart from the work on direct/projected transfer and annotation projection mentioned above, the proposed method can be seen as a more explicit kind of domain adaptation, similar to Titov (2011) or Blitzer et al. (2006). It is also somewhat similar in spirit to Mikolov et al. (2013b), where a small number of word translation pairs are used to estimate a mapping between distributed representations of words in two different languages and build a word translation model. 5 Conclusions In this paper we propose a new method of crosslingual model transfer, report initial evaluation results and highlight directions for its further development. We observe that the performance of this method is competitive with that of established crosslingual transfer approaches and its application requires very little manual adjustm</context>
</contexts>
<marker>Blitzer, McDonal, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonal, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proc. Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>89--97</pages>
<location>Beijing, China,</location>
<contexts>
<context position="13440" citStr="Bohnet, 2010" startWordPosition="2144" endWordPosition="2145">nce the size of the latter dataset is relatively small – one thousand sentences – we reserve the whole dataset for testing and only evaluate transfer from English to French, but not the other way around. Datasets for other languages are sufficiently large, so we take 30 thousand samples for testing and use the rest as training data. The validation set in each experiment is withheld from the corresponding training corpus and contains 10 thousand samples. Parallel data for both language pairs is derived from Europarl (Koehn, 2005), which we preprocess using MATE-TOOLS (Bj¨orkelund et al., 2009; Bohnet, 2010). 3 Results The classification error of FRP and the baselines given varying amount of parallel data is reported in figures 2, 3 and 4. The training set for each language is fixed. We denote the two baselines AP (annotation projection) and DT (direct transfer). The number of parallel instances in these experiments is shown on a logarithmic scale, the values considered are 2, 5, 10, 20 and 50 thousand pairs. Please note that we report only a single value for direct transfer, since this approach does not explicitly rely on parallel data. Although some of the features – namely, gloss translations </context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89–97, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llufs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<location>Ann Arbor, MI USA.</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llufs M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of CoNLL-2005, Ann Arbor, MI USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lea Cyrus</author>
</authors>
<title>Building a resource for studying translation shifts.</title>
<date>2006</date>
<location>CoRR, abs/cs/0606096.</location>
<contexts>
<context position="3668" citStr="Cyrus, 2006" startWordPosition="549" endWordPosition="550">d, does not require any changes to the feature representation. Instead, it operates on translation pairs, usually on sentence level, applying the available source-side model to the source sentence and transferring the resulting annotations through the word alignment links to the target one. The quality of predictions on source sentences depends heavily on the quality of parallel data and the domain it belongs to (or, rather, the similarity between this domain and that of the corpus the source-language model was trained on). The transfer itself also introduces errors due to translation shifts (Cyrus, 2006) and word alignment errors, which may lead to inaccurate predictions. These issues are generally handled using heuristics (Pad´o and Lapata, 2006) and filtering, for example based on alignment coverage (van der Plas et al., 2011). 579 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Figure 1: Dependency-based semantic role labeling example. The top arcs depict dependency relations, the bottom ones – semantic role structure. Rendered w</context>
</contexts>
<marker>Cyrus, 2006</marker>
<rawString>Lea Cyrus. 2006. Building a resource for studying translation shifts. CoRR, abs/cs/0606096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graphbased projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>600--609</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1439" citStr="Das and Petrov, 2011" startWordPosition="204" endWordPosition="207">hen compared to direct model transfer and annotation projection and suggests interesting directions for further research. 1 Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-lang</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graphbased projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 600–609, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Syntactic transfer using a bilingual lexicon.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1--11</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="1571" citStr="Durrett et al., 2012" startWordPosition="228" endWordPosition="231">tion Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language. If parallel data is available, it can be further used to enforce model agreement </context>
</contexts>
<marker>Durrett, Pauls, Klein, 2012</marker>
<rawString>Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntactic transfer using a bilingual lexicon. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1–11, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>369--377</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1507" citStr="Ganchev et al., 2009" startWordPosition="215" endWordPosition="218">suggests interesting directions for further research. 1 Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language. If parallel data </context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 369–377, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian J Goodfellow</author>
<author>David Warde-Farley</author>
<author>Pascal Lamblin</author>
<author>Vincent Dumoulin</author>
<author>Mehdi Mirza</author>
<author>Razvan Pascanu</author>
<author>James Bergstra</author>
<author>Fr´ed´eric Bastien</author>
<author>Yoshua Bengio</author>
</authors>
<title>Pylearn2: a machine learning research library.</title>
<date>2013</date>
<location>CoRR, abs/1308.4214.</location>
<contexts>
<context position="11195" citStr="Goodfellow et al., 2013" startWordPosition="1760" endWordPosition="1763">for evaluation. Note that predicate and argument identification in both languages is performed using monolingual classifiers and only aligned pairs are used in projection. A more common approach would be to project the whole structure from the source language, but in our case this may give unfair advantage to feature representation projection, which relies on target-side argument identification. 2.4 Tools We use the same type of log-linear classifiers in the model itself and the two baselines to avoid any discrepancy due to learning procedure. These classifiers are implemented using PYLEARN2 (Goodfellow et al., 2013), based on THEANO (Bergstra et al., 2010). We also use this framework to estimate the linear mapping Mts between source and target feature spaces in FRP. The 250-dimensional word representations for w1 are obtained using WORD2VEC tool. Both monolingual data and that from the parallel corpus are included in the training. In Mikolov et al. (2013b) the authors consider embeddings of up to 800 dimensions, but we would not expect to benefit as much from larger vectors since we are using a much smaller corpus to train them. We did not tune the size of the word representation to our task, as this wou</context>
</contexts>
<marker>Goodfellow, Warde-Farley, Lamblin, Dumoulin, Mirza, Pascanu, Bergstra, Bastien, Bengio, 2013</marker>
<rawString>Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Fr´ed´eric Bastien, and Yoshua Bengio. 2013. Pylearn2: a machine learning research library. CoRR, abs/1308.4214.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajic</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Martf</author>
<author>Llufs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan Step´anek</author>
<author>Pavel Stran´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task,</booktitle>
<pages>1--18</pages>
<location>Boulder, Colorado.</location>
<marker>Hajic, Ciaramita, Johansson, Kawahara, Martf, M`arquez, Meyers, Nivre, Pad´o, Step´anek, Stran´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Haji&amp;quot;c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Martf, Llufs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan &amp;quot;St&amp;quot;ep´anek, Pavel Stra&amp;quot;n´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task, pages 1–18, Boulder, Colorado.</rawString>
</citation>
<citation valid="false">
<title>Ure&amp;quot;sov´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<editor>Jan Haji&amp;quot;c, Eva Haji&amp;quot;cov´a, Jarmila Panevov´a, Petr Sgall, Ond&amp;quot;rej Bojar, Silvie Cinkov´a, Eva Fu&amp;quot;c´ıkov´a, Marie Mikulov´a, Petr Pajas, Jan Popelka, Ji&amp;quot;rfSemeck´y, Jana &amp;quot;Sindlerov´a, Jan &amp;quot;St&amp;quot;ep´anek, Josef Toman, Zde&amp;quot;nka</editor>
<location>Istanbul, Turkey,</location>
<marker>2012</marker>
<rawString>Jan Haji&amp;quot;c, Eva Haji&amp;quot;cov´a, Jarmila Panevov´a, Petr Sgall, Ond&amp;quot;rej Bojar, Silvie Cinkov´a, Eva Fu&amp;quot;c´ıkov´a, Marie Mikulov´a, Petr Pajas, Jan Popelka, Ji&amp;quot;rfSemeck´y, Jana &amp;quot;Sindlerov´a, Jan &amp;quot;St&amp;quot;ep´anek, Josef Toman, Zde&amp;quot;nka Ure&amp;quot;sov´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2012. Announcing Prague Czech-English dependency treebank 2.0. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet U&amp;quot;gur Do&amp;quot;gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel text.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="1549" citStr="Hwa et al., 2005" startWordPosition="224" endWordPosition="227">search. 1 Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language. If parallel data is available, it can be further used to en</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel text. Natural Language Engineering, 11(3):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ivan Titov</author>
<author>Binod Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING),</booktitle>
<location>Bombay, India.</location>
<contexts>
<context position="2637" citStr="Klementiev et al., 2012" startWordPosition="391" endWordPosition="394">trained on the source-language data directly applied to the target language. If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al., 2011). The shared feature representation depends on the task in question, but most often each aspect of the original feature representation is handled separately. Word types, for example, may be replaced by cross-lingual word clusters (T¨ackstr¨om et al., 2012) or cross-lingual distributed word representations (Klementiev et al., 2012). Part-ofspeech tags, which are often language-specific, can be converted into universal part-of-speech tags (Petrov et al., 2012) and morpho-syntactic information can also be represented in a unified way (Zeman et al., 2012; McDonald et al., 2013; Tsarfaty, 2013). Unfortunately, the design of such representations and corresponding conversion procedures is by no means trivial. Annotation projection, on the other hand, does not require any changes to the feature representation. Instead, it operates on translation pairs, usually on sentence level, applying the available source-side model to the </context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words. In Proceedings of the International Conference on Computational Linguistics (COLING), Bombay, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<publisher>AAMT.</publisher>
<location>Phuket, Thailand.</location>
<contexts>
<context position="13361" citStr="Koehn, 2005" startWordPosition="2132" endWordPosition="2133"> the manually corrected dataset from van der Plas et al. (2011) for French. Since the size of the latter dataset is relatively small – one thousand sentences – we reserve the whole dataset for testing and only evaluate transfer from English to French, but not the other way around. Datasets for other languages are sufficiently large, so we take 30 thousand samples for testing and use the rest as training data. The validation set in each experiment is withheld from the corresponding training corpus and contains 10 thousand samples. Parallel data for both language pairs is derived from Europarl (Koehn, 2005), which we preprocess using MATE-TOOLS (Bj¨orkelund et al., 2009; Bohnet, 2010). 3 Results The classification error of FRP and the baselines given varying amount of parallel data is reported in figures 2, 3 and 4. The training set for each language is fixed. We denote the two baselines AP (annotation projection) and DT (direct transfer). The number of parallel instances in these experiments is shown on a logarithmic scale, the values considered are 2, 5, 10, 20 and 50 thousand pairs. Please note that we report only a single value for direct transfer, since this approach does not explicitly rel</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. AAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Kozhevnikov</author>
<author>Ivan Titov</author>
</authors>
<title>Crosslingual transfer of semantic role labeling models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1190--1200</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1716" citStr="Kozhevnikov and Titov, 2013" startWordPosition="249" endWordPosition="252">ated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language. If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al., 2011). The share</context>
<context position="12658" citStr="Kozhevnikov and Titov (2013)" startWordPosition="2009" endWordPosition="2012">e of the monolingual classifier does not improve significantly if the dimension is increased past 300 and decreases only by a small margin (less than one absolute point) if it is reduced to 100. It should be noted, however, that the dimension that is optimal in this sense is not necessarily the best choice for FRP, especially if the amount of available parallel data is limited. 2.5 Data We use two language pairs for evaluation: English-Czech and English-French. In the first case, the data is converted from Prague CzechEnglish Dependency Treebank 2.0 (Hajiˇc et al., 2012) using the script from Kozhevnikov and Titov (2013). In the second, we use CoNLL 2009 shared task (Hajiˇc et al., 2009) corpus for English and the manually corrected dataset from van der Plas et al. (2011) for French. Since the size of the latter dataset is relatively small – one thousand sentences – we reserve the whole dataset for testing and only evaluate transfer from English to French, but not the other way around. Datasets for other languages are sufficiently large, so we take 30 thousand samples for testing and use the rest as training data. The validation set in each experiment is withheld from the corresponding training corpus and con</context>
</contexts>
<marker>Kozhevnikov, Titov, 2013</marker>
<rawString>Mikhail Kozhevnikov and Ivan Titov. 2013. Crosslingual transfer of semantic role labeling models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1190–1200, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>62--72</pages>
<institution>Edinburgh, United Kingdom. Association for Computational Linguistics.</institution>
<contexts>
<context position="2305" citStr="McDonald et al., 2011" startWordPosition="342" endWordPosition="345">008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language. If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al., 2011). The shared feature representation depends on the task in question, but most often each aspect of the original feature representation is handled separately. Word types, for example, may be replaced by cross-lingual word clusters (T¨ackstr¨om et al., 2012) or cross-lingual distributed word representations (Klementiev et al., 2012). Part-ofspeech tags, which are often language-specific, can be converted into universal part-of-speech tags (Petrov et al., 2012) and morpho-syntactic information can also be represented in a unified way (Zeman et al., 2012; McDonald et al., 2013; Tsarfaty, 2013). Un</context>
<context position="16320" citStr="McDonald et al., 2011" startWordPosition="2609" endWordPosition="2612">s of words in two different languages and build a word translation model. 5 Conclusions In this paper we propose a new method of crosslingual model transfer, report initial evaluation results and highlight directions for its further development. We observe that the performance of this method is competitive with that of established crosslingual transfer approaches and its application requires very little manual adjustment – no heuristics or filtering and no explicit shared feature representation design. It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al., 2011) that may have been designed to work in conjunction with direct model transfer. 6 Future Work This paper reports work in progress and there is a number of directions we would like to pursue further. Better Monolingual Representations The representation we used in the initial evaluation does not discriminate between aspects that are relevant for the assignment of semantic roles and those that are not. Since we are using a relatively small set of features to start with, this does not present much of a problem. In general, however, retaining only relevant aspects of intermediate monolingual repre</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 62–72, Edinburgh, United Kingdom. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
<author>Yvonne QuirmbachBrundage</author>
<author>Yoav Goldberg</author>
<author>Dipanjan Das</author>
<author>Kuzman Ganchev</author>
<author>Keith Hall</author>
<author>Slav Petrov</author>
<author>Hao Zhang</author>
<author>Oscar T¨ackstr¨om</author>
<author>Claudia Bedini</author>
<author>N´uria Bertomeu Castell´o</author>
<author>Jungmee Lee</author>
</authors>
<title>Universal dependency annotation for multilingual parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>92--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>McDonald, Nivre, QuirmbachBrundage, Goldberg, Das, Ganchev, Hall, Petrov, Zhang, T¨ackstr¨om, Bedini, Castell´o, Lee, 2013</marker>
<rawString>Ryan McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria Bertomeu Castell´o, and Jungmee Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92–97, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="8991" citStr="Mikolov et al., 2013" startWordPosition="1413" endWordPosition="1416">et feature spaces could be reliably estimated from a limited amount of parallel data, while preserving, insofar as possible, the information relevant for classification. Estimating the mapping directly from raw categorical features (w0) is both computationally expensive and likely inaccurate – using one-hot encoding the feature vectors in our experiments would have tens of thousands of components. There is a number of ways to make this representation more compact. To start with, we replace word types with corresponding neural language model representations estimated using the skip-gram model (Mikolov et al., 2013a). This corresponds to Ms and Mt above and reduces the dimension of the feature space, making direct estimation of the mapping practical. We will refer to this representation as w1. To go further, one can, for example, apply dimensionality reduction techniques to obtain a more compact representation of w1 by eliminating redundancy or define auxiliary tasks and produce a vector representation useful for those tasks. In source language, one can even directly tune an intermediate representation for the target problem. 2.3 Baselines As mentioned above we compare the performance of this approach t</context>
<context position="11540" citStr="Mikolov et al. (2013" startWordPosition="1818" endWordPosition="1821">ch relies on target-side argument identification. 2.4 Tools We use the same type of log-linear classifiers in the model itself and the two baselines to avoid any discrepancy due to learning procedure. These classifiers are implemented using PYLEARN2 (Goodfellow et al., 2013), based on THEANO (Bergstra et al., 2010). We also use this framework to estimate the linear mapping Mts between source and target feature spaces in FRP. The 250-dimensional word representations for w1 are obtained using WORD2VEC tool. Both monolingual data and that from the parallel corpus are included in the training. In Mikolov et al. (2013b) the authors consider embeddings of up to 800 dimensions, but we would not expect to benefit as much from larger vectors since we are using a much smaller corpus to train them. We did not tune the size of the word representation to our task, as this would not be appropriate in a cross-lingual transfer setup, but we observe that the classifier is relatively robust to their dimension when evaluXMts = argmaxM (xs1,xt1∈�Dst) 581 ated on source language – in our experiments the performance of the monolingual classifier does not improve significantly if the dimension is increased past 300 and decr</context>
<context position="15582" citStr="Mikolov et al. (2013" startWordPosition="2493" endWordPosition="2496">oposed method with w1 representation demonstrates performance competitive to direct transfer and annotation projection baselines. 2 5 10 20 50 0.42 0.40 0.38 0.36 0.34 FRP AP DT 2 5 10 20 50 0.40 0.38 0.36 0.34 0.32 FRP AP DT 582 Error Number of parallel instances, ×103 Figure 4: English-French transfer results 4 Additional Related Work Apart from the work on direct/projected transfer and annotation projection mentioned above, the proposed method can be seen as a more explicit kind of domain adaptation, similar to Titov (2011) or Blitzer et al. (2006). It is also somewhat similar in spirit to Mikolov et al. (2013b), where a small number of word translation pairs are used to estimate a mapping between distributed representations of words in two different languages and build a word translation model. 5 Conclusions In this paper we propose a new method of crosslingual model transfer, report initial evaluation results and highlight directions for its further development. We observe that the performance of this method is competitive with that of established crosslingual transfer approaches and its application requires very little manual adjustment – no heuristics or filtering and no explicit shared feature</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation.</title>
<date>2013</date>
<location>CoRR, abs/1309.4168.</location>
<contexts>
<context position="8991" citStr="Mikolov et al., 2013" startWordPosition="1413" endWordPosition="1416">et feature spaces could be reliably estimated from a limited amount of parallel data, while preserving, insofar as possible, the information relevant for classification. Estimating the mapping directly from raw categorical features (w0) is both computationally expensive and likely inaccurate – using one-hot encoding the feature vectors in our experiments would have tens of thousands of components. There is a number of ways to make this representation more compact. To start with, we replace word types with corresponding neural language model representations estimated using the skip-gram model (Mikolov et al., 2013a). This corresponds to Ms and Mt above and reduces the dimension of the feature space, making direct estimation of the mapping practical. We will refer to this representation as w1. To go further, one can, for example, apply dimensionality reduction techniques to obtain a more compact representation of w1 by eliminating redundancy or define auxiliary tasks and produce a vector representation useful for those tasks. In source language, one can even directly tune an intermediate representation for the target problem. 2.3 Baselines As mentioned above we compare the performance of this approach t</context>
<context position="11540" citStr="Mikolov et al. (2013" startWordPosition="1818" endWordPosition="1821">ch relies on target-side argument identification. 2.4 Tools We use the same type of log-linear classifiers in the model itself and the two baselines to avoid any discrepancy due to learning procedure. These classifiers are implemented using PYLEARN2 (Goodfellow et al., 2013), based on THEANO (Bergstra et al., 2010). We also use this framework to estimate the linear mapping Mts between source and target feature spaces in FRP. The 250-dimensional word representations for w1 are obtained using WORD2VEC tool. Both monolingual data and that from the parallel corpus are included in the training. In Mikolov et al. (2013b) the authors consider embeddings of up to 800 dimensions, but we would not expect to benefit as much from larger vectors since we are using a much smaller corpus to train them. We did not tune the size of the word representation to our task, as this would not be appropriate in a cross-lingual transfer setup, but we observe that the classifier is relatively robust to their dimension when evaluXMts = argmaxM (xs1,xt1∈�Dst) 581 ated on source language – in our experiments the performance of the monolingual classifier does not improve significantly if the dimension is increased past 300 and decr</context>
<context position="15582" citStr="Mikolov et al. (2013" startWordPosition="2493" endWordPosition="2496">oposed method with w1 representation demonstrates performance competitive to direct transfer and annotation projection baselines. 2 5 10 20 50 0.42 0.40 0.38 0.36 0.34 FRP AP DT 2 5 10 20 50 0.40 0.38 0.36 0.34 0.32 FRP AP DT 582 Error Number of parallel instances, ×103 Figure 4: English-French transfer results 4 Additional Related Work Apart from the work on direct/projected transfer and annotation projection mentioned above, the proposed method can be seen as a more explicit kind of domain adaptation, similar to Titov (2011) or Blitzer et al. (2006). It is also somewhat similar in spirit to Mikolov et al. (2013b), where a small number of word translation pairs are used to estimate a mapping between distributed representations of words in two different languages and build a word translation model. 5 Conclusions In this paper we propose a new method of crosslingual model transfer, report initial evaluation results and highlight directions for its further development. We observe that the performance of this method is competitive with that of established crosslingual transfer approaches and its application requires very little manual adjustment – no heuristics or filtering and no explicit shared feature</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Optimal constituent alignment with edge covers for semantic projection.</title>
<date>2006</date>
<booktitle>In Proc. 44th Annual Meeting of Association for Computational Linguistics and 21st International Conf. on Computational Linguistics, ACLCOLING</booktitle>
<pages>1161--1168</pages>
<location>Sydney, Australia.</location>
<marker>Pad´o, Lapata, 2006</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2006. Optimal constituent alignment with edge covers for semantic projection. In Proc. 44th Annual Meeting of Association for Computational Linguistics and 21st International Conf. on Computational Linguistics, ACLCOLING 2006, pages 1161–1168, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Crosslingual annotation projection for semantic roles.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>36</volume>
<pages>340</pages>
<marker>Pad´o, Lapata, 2009</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2009. Crosslingual annotation projection for semantic roles. Journal of Artificial Intelligence Research, 36:307– 340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC,</booktitle>
<contexts>
<context position="2767" citStr="Petrov et al., 2012" startWordPosition="409" endWordPosition="412">enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al., 2011). The shared feature representation depends on the task in question, but most often each aspect of the original feature representation is handled separately. Word types, for example, may be replaced by cross-lingual word clusters (T¨ackstr¨om et al., 2012) or cross-lingual distributed word representations (Klementiev et al., 2012). Part-ofspeech tags, which are often language-specific, can be converted into universal part-of-speech tags (Petrov et al., 2012) and morpho-syntactic information can also be represented in a unified way (Zeman et al., 2012; McDonald et al., 2013; Tsarfaty, 2013). Unfortunately, the design of such representations and corresponding conversion procedures is by no means trivial. Annotation projection, on the other hand, does not require any changes to the feature representation. Instead, it operates on translation pairs, usually on sentence level, applying the available source-side model to the source sentence and transferring the resulting annotations through the word alignment links to the target one. The quality of pred</context>
<context position="9904" citStr="Petrov et al., 2012" startWordPosition="1558" endWordPosition="1561">entation of w1 by eliminating redundancy or define auxiliary tasks and produce a vector representation useful for those tasks. In source language, one can even directly tune an intermediate representation for the target problem. 2.3 Baselines As mentioned above we compare the performance of this approach to that of direct transfer and annotation projection. Both baselines are using the same set of features as the proposed model, as described earlier. The shared feature representation for direct transfer is derived from w0 by replacing language-specific part-of-speech tags with universal ones (Petrov et al., 2012) and adding crosslingual word clusters (T¨ackstr¨om et al., 2012) to word types. The word types themselves are left as they are in the source language and replaced with their gloss translations in the target one (Zeman and Resnik, 2008). In English-Czech and CzechEnglish we also use the dependency relation information, since the annotations are partly compatible. The annotation projection baseline implementation is straightforward. The source-side instances from a parallel corpus are labeled using a classifier trained on source-language training data and transferred to the target side. The res</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of LREC, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="6709" citStr="Punyakanok et al., 2008" startWordPosition="1026" endWordPosition="1029">nd their arguments in sentences and assigning each argument a semantic role with respect to its predicate (see figure 1). Note that only a single word – the syntactic head of the argument phrase – is marked as an argument in this case, as opposed to constituent- or span-based SRL (Carreras and M`arquez, 2005). We focus on the assignment of semantic roles to identified arguments. For the sake of simplicity we cast it as a multiclass classification problem, ignoring the interaction between different arguments in a predicate. It is well known that such interaction plays an important part in SRL (Punyakanok et al., 2008), but it is not well understood which kinds of interactions are preserved across languages and which are not. Also, should one like to apply constraints on the set of semantic roles in a given predicate, or, for example, use a reranker (Bj¨orkelund et al., 2009), this can be done using a factorized model obtained by cross-lingual transfer. In our setting, each instance includes the word type and part-of-speech and morphological tags (if any) of argument token, its parent and corresponding predicate token, as well as their dependency relations to their respective parents. This representation is</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>822--831</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1531" citStr="Smith and Eisner, 2009" startWordPosition="219" endWordPosition="223">irections for further research. 1 Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language. If parallel data is available, it can be </context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David A Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 822–831. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Data point selection for crosslanguage adaptation of dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>2</volume>
<pages>682--686</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<contexts>
<context position="1587" citStr="Søgaard, 2011" startWordPosition="232" endWordPosition="233">el transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language. If parallel data is available, it can be further used to enforce model agreement on this data to </context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Data point selection for crosslanguage adaptation of dependency parsers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, volume 2 of HLT ’11, pages 682–686, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL),</booktitle>
<pages>477--487</pages>
<location>Montr´eal, Canada.</location>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL), pages 477– 487, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--1</pages>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. Transactions of the Association for Computational Linguistics, 1:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
</authors>
<title>Domain adaptation by constraining inter-domain variability of latent feature representation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>62--71</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="15494" citStr="Titov (2011)" startWordPosition="2478" endWordPosition="2479">ults given a sufficiently large parallel corpus. Overall, we observe that the proposed method with w1 representation demonstrates performance competitive to direct transfer and annotation projection baselines. 2 5 10 20 50 0.42 0.40 0.38 0.36 0.34 FRP AP DT 2 5 10 20 50 0.40 0.38 0.36 0.34 0.32 FRP AP DT 582 Error Number of parallel instances, ×103 Figure 4: English-French transfer results 4 Additional Related Work Apart from the work on direct/projected transfer and annotation projection mentioned above, the proposed method can be seen as a more explicit kind of domain adaptation, similar to Titov (2011) or Blitzer et al. (2006). It is also somewhat similar in spirit to Mikolov et al. (2013b), where a small number of word translation pairs are used to estimate a mapping between distributed representations of words in two different languages and build a word translation model. 5 Conclusions In this paper we propose a new method of crosslingual model transfer, report initial evaluation results and highlight directions for its further development. We observe that the performance of this method is competitive with that of established crosslingual transfer approaches and its application requires v</context>
</contexts>
<marker>Titov, 2011</marker>
<rawString>Ivan Titov. 2011. Domain adaptation by constraining inter-domain variability of latent feature representation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 62–71, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Tonelli</author>
<author>Emanuele Pianta</author>
</authors>
<title>Frame information transfer from English to Italian.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="1686" citStr="Tonelli and Pianta, 2008" startWordPosition="245" endWordPosition="248">or languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language. If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McD</context>
</contexts>
<marker>Tonelli, Pianta, 2008</marker>
<rawString>Sara Tonelli and Emanuele Pianta. 2008. Frame information transfer from English to Italian. In Proceedings of LREC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
</authors>
<title>A unified morpho-syntactic scheme of stanford dependencies.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>578--584</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2901" citStr="Tsarfaty, 2013" startWordPosition="432" endWordPosition="433">onald et al., 2011). The shared feature representation depends on the task in question, but most often each aspect of the original feature representation is handled separately. Word types, for example, may be replaced by cross-lingual word clusters (T¨ackstr¨om et al., 2012) or cross-lingual distributed word representations (Klementiev et al., 2012). Part-ofspeech tags, which are often language-specific, can be converted into universal part-of-speech tags (Petrov et al., 2012) and morpho-syntactic information can also be represented in a unified way (Zeman et al., 2012; McDonald et al., 2013; Tsarfaty, 2013). Unfortunately, the design of such representations and corresponding conversion procedures is by no means trivial. Annotation projection, on the other hand, does not require any changes to the feature representation. Instead, it operates on translation pairs, usually on sentence level, applying the available source-side model to the source sentence and transferring the resulting annotations through the word alignment links to the target one. The quality of predictions on source sentences depends heavily on the quality of parallel data and the domain it belongs to (or, rather, the similarity b</context>
</contexts>
<marker>Tsarfaty, 2013</marker>
<rawString>Reut Tsarfaty. 2013. A unified morpho-syntactic scheme of stanford dependencies. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 578–584, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>Paola Merlo</author>
<author>James Henderson</author>
</authors>
<title>Scaling up automatic cross-lingual semantic role annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, HLT ’11,</booktitle>
<pages>299--304</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<marker>van der Plas, Merlo, Henderson, 2011</marker>
<rawString>Lonneke van der Plas, Paola Merlo, and James Henderson. 2011. Scaling up automatic cross-lingual semantic role annotation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, HLT ’11, pages 299–304, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhai Xi</author>
<author>Rebecca Hwa</author>
</authors>
<title>A backoff model for bootstrapping resources for non-english languages.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>851--858</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1417" citStr="Xi and Hwa, 2005" startWordPosition="200" endWordPosition="203">ic role labeling when compared to direct model transfer and annotation projection and suggests interesting directions for further research. 1 Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. Once this is achieved, instances from both languages can be mapped into this space and a model trai</context>
</contexts>
<marker>Xi, Hwa, 2005</marker>
<rawString>Chenhai Xi and Rebecca Hwa. 2005. A backoff model for bootstrapping resources for non-english languages. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 851–858, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the first international conference on Human language technology research,</booktitle>
<pages>pages</pages>
<contexts>
<context position="1309" citStr="Yarowsky et al., 2001" startWordPosition="182" endWordPosition="185">led directly by the source-side model. This approach displays competitive performance on model transfer for semantic role labeling when compared to direct model transfer and annotation projection and suggests interesting directions for further research. 1 Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific represen</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the first international conference on Human language technology research, pages 1– 8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Crosslanguage parser adaptation between related languages.</title>
<date>2008</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages,</booktitle>
<pages>35--42</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="1259" citStr="Zeman and Resnik, 2008" startWordPosition="173" endWordPosition="177">e representation using the derived mapping and handled directly by the source-side model. This approach displays competitive performance on model transfer for semantic role labeling when compared to direct model transfer and annotation projection and suggests interesting directions for further research. 1 Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing an</context>
<context position="10140" citStr="Zeman and Resnik, 2008" startWordPosition="1598" endWordPosition="1601">Baselines As mentioned above we compare the performance of this approach to that of direct transfer and annotation projection. Both baselines are using the same set of features as the proposed model, as described earlier. The shared feature representation for direct transfer is derived from w0 by replacing language-specific part-of-speech tags with universal ones (Petrov et al., 2012) and adding crosslingual word clusters (T¨ackstr¨om et al., 2012) to word types. The word types themselves are left as they are in the source language and replaced with their gloss translations in the target one (Zeman and Resnik, 2008). In English-Czech and CzechEnglish we also use the dependency relation information, since the annotations are partly compatible. The annotation projection baseline implementation is straightforward. The source-side instances from a parallel corpus are labeled using a classifier trained on source-language training data and transferred to the target side. The resulting annotations are then used to train a target-side classifier for evaluation. Note that predicate and argument identification in both languages is performed using monolingual classifiers and only aligned pairs are used in projectio</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Crosslanguage parser adaptation between related languages. In Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 35– 42, Hyderabad, India, January. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel Zeman</author>
<author>David Mareˇcek</author>
<author>Martin Popel</author>
<author>Loganathan Ramasamy</author>
<author>Jan ˇStˇep´anek</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Hamledt: To parse or not to parse?</title>
<date>2012</date>
<booktitle>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uˇgur Doˇgan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul, Turkey,</location>
<marker>Zeman, Mareˇcek, Popel, Ramasamy, ˇStˇep´anek, ˇZabokrtsk´y, Hajiˇc, 2012</marker>
<rawString>Daniel Zeman, David Mareˇcek, Martin Popel, Loganathan Ramasamy, Jan ˇStˇep´anek, Zdenˇek ˇZabokrtsk´y, and Jan Hajiˇc. 2012. Hamledt: To parse or not to parse? In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uˇgur Doˇgan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>