<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001277">
<title confidence="0.899776">
Radical Embedding: Delving Deeper to Chinese Radicals
</title>
<author confidence="0.93794">
Xinlei Shi, Junjie Zhai, Xudong Yang, Zehua Xie, Chao Liu
</author>
<affiliation confidence="0.945743">
Sogou Technology Inc., Beijing, China
</affiliation>
<keyword confidence="0.519292">
{shixinlei, zhaijunjie, yangxudong, xiezehua, liuchao}@sogou-inc.com
</keyword>
<sectionHeader confidence="0.990018" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994477222222222">
Languages using Chinese characters
are mostly processed at word level. In-
spired by recent success of deep learn-
ing, we delve deeper to character and
radical levels for Chinese language pro-
cessing. We propose a new deep learn-
ing technique, called “radical embed-
ding”, with justifications based on Chi-
nese linguistics, and validate its fea-
sibility and utility through a set of
three experiments: two in-house stan-
dard experiments on short-text catego-
rization (STC) and Chinese word seg-
mentation (CWS), and one in-field ex-
periment on search ranking. We show
that radical embedding achieves com-
parable, and sometimes even better, re-
sults than competing methods.
</bodyText>
<sectionHeader confidence="0.998521" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998590315789474">
Chinese is one of the oldest written languages
in the world, but it does not attract much at-
tention in top NLP research forums, proba-
bly because of its peculiarities and drastic dif-
ferences from English. There are sentences,
words, characters in Chinese, as illustrated in
Figure 1. The top row is a Chinese sentence,
whose English translation is at the bottom. In
between is the pronunciation of the sentence
in Chinese, called PinYin, which is a form of
Romanian phonetic representation of Chinese,
similar to the International Phonetic Alpha-
bet (IPA) for English. Each squared symbol
is a distinct Chinese character, and there are
no separators between characters calls for Chi-
nese Word Segmentation (CWS) techniques to
group adjacent characters into words.
In most current applications (e.g., catego-
rization and recommendation etc.), Chinese is
</bodyText>
<figure confidence="0.883247">
a word a character
</figure>
<figureCaption confidence="0.76534125">
Chinese: 7 )�/ )� &apos;�V A/ ffo
Pinyin: jīn tiān/ tiān qì/ zhēn/ hǎo0
English: It is a nice day today.
Figure 1: Illustration of Chinese Language
</figureCaption>
<bodyText confidence="0.9996000625">
represented at the word level. Inspired by re-
cent success of delving deep (Szegedy et al.,
2014; Zhang and LeCun, 2015; Collobert et
al., 2011), an interesting question arises then:
can we delve deeper than word level represen-
tation for better Chinese language processing?
If the answer is yes, how deep can it be done
for fun and for profit?
Intuitively, the answer should be positive.
Nevertheless, each Chinese character is seman-
tically meaningful, thanks to its pictographic
root from ancient Chinese as depicted in Fig-
ure 2. We could delve deeper by decomposing
each character into character radicals.
The right part of Figure 2 illustrates the de-
composition. This Chinese character (mean-
ing “morning”) is decomposed into 4 radicals
that consists of 12 strokes in total. In Chi-
nese linguistics, each Chinese character can be
decomposed into no more than four radicals
based on a set of preset rules&apos;. As depicted by
the pictograms in the right part of Figure 2,
the 1st radical (and the 3rd that happens to
be the same) means “grass”, and the 2nd and
the 4th mean the “sun” and the “moon”, re-
spectively. These four radicals altogether con-
vey the meaning that “the moment when sun
arises from the grass while the moon wanes
away”, which is exactly “morning”. On the
other hand, it is hard to decipher the seman-
tics of strokes, and radicals are the minimum
semantic unit for Chinese. Building deep mod-
</bodyText>
<footnote confidence="0.441742">
lhttp://en.wikipedia.org/wiki/Wubi_method
</footnote>
<page confidence="0.808707">
594
</page>
<note confidence="0.252655">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 594–598,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.916221">
character
radical
pictogram
stroker
</figure>
<figureCaption confidence="0.99967">
Figure 2: Decomposition of Chinese Character
</figureCaption>
<bodyText confidence="0.999182896551724">
els from radicals could lead to interesting re-
sults.
In sum, this paper makes the following
three-fold contributions: (1) we propose a
new deep learning technique, called “radical
embedding”, for Chinese language processing
with proper justifications based on Chinese
linguistics; (2) we validate the feasibility and
utility of radical embedding through a set of
three experiments, which include not only two
in-house standard experiments on short-text
categorization (STC) and Chinese word seg-
mentation (CWS), but an in-field experiment
on search ranking as well; (3) this initial suc-
cess of radical embedding could shed some
light on new approaches to better language
processing for Chinese and other languages
alike.
The rest of this paper is organized as fol-
lows. Section 2 presents the radical embed-
ding technique and the accompanying deep
neural network components, which are com-
bined and stacked to solve three application
problems. Section 3 elaborates on the three
applications and reports on the experiment re-
sults. With related work briefly discussed in
Section 4, Section 5 concludes this study. For
clarity, we limit the study to Simplified Chi-
nese in this paper.
</bodyText>
<sectionHeader confidence="0.9727325" genericHeader="method">
2 Deep Networks with Radical
Embeddings
</sectionHeader>
<bodyText confidence="0.999702818181818">
This section presents the radical embedding
technique, and the accompanying deep neu-
ral network components. These components
are combined to solve the three applications
in Section 3.
Word embedding is a popular technique in
NLP (Collobert et al., 2011). It maps words to
vectors of real numbers in a relatively low di-
mensional space. It is shown that the proxim-
ity in this numeric space actually embodies al-
gebraic semantic relationship, such as “Queen
</bodyText>
<table confidence="0.555305481481482">
input output
Convolution m V ∈ Rm+n−1
f ∈ i+n−1
k ∈ Rn s=i fs · ks−i
0 ≤ i ≤ m − n + 1
Max-pooling x ∈ Rd y = max(x) ∈ R
Lookup M ∈ Rdx|D |vi = MIi ∈ Rd
Table Ii ∈ R|D|x1
Tanh x ∈ Rd V ∈ Rd
exi −e−xi
yi = exi +e−xi
0 ≤ i ≤ d − 1
Linear x ∈ Rd V = x ∈ Rd
ReLU x ∈ Rd V ∈ Rd
yi = 0 if xi ≤ 0
yi = xi if xi &gt; 0
0 ≤ i ≤ d − 1
Softmax x ∈ Rd V ∈ Rd
exi
yi = j=1 exj
0 ≤ i ≤ d − 1
Concatenate xi ∈ Rd (x0, x1 ..., xn−1)
0 ≤ i ≤ n − 1 E Rddxn
D: radical vocabulary
M: a matrix containing |D |columns, each column
is a d-dimensional vector represent radical in D.
Ii: a one hot vector stands for the ith radical in vocabulary
</table>
<tableCaption confidence="0.998334">
Table 1: Neural Network Components
</tableCaption>
<bodyText confidence="0.997375037037037">
− Woman + Man ≈ King” (Mikolov et al.,
2013). As demonstrated in previous work,
this numeric representation of words has led to
big improvements in many NLP tasks such as
machine translation (Sutskever et al., 2014),
question answering (Iyyer et al., 2014) and
document ranking (Shen et al., 2014).
Radical embedding is similar to word em-
bedding except that the embedding is at rad-
ical level. There are two ways of embedding:
C13OW and skip-gram (Mikolov et al., 2013).
We here use C13OW for radical embedding be-
cause the two methods exhibit few differences,
and C13OW is slightly faster in experiments.
Specifically, a sequence of Chinese characters
is decomposed into a sequence of radicals, to
which C13OW is applied. We use the word2vec
package (Mikolov et al., 2013) to train radical
vectors, and then initialize the lookup table
with these radical vectors.
We list the network components in Table 1,
which are combined and stacked in Figure 3
to solve different problems in Section 3. Each
component is a function, the input column of
Table 1 demonstrates input parameters and
their dimensions of these functions, the out-
put column shows the formulas and outputs.
</bodyText>
<sectionHeader confidence="0.98216" genericHeader="method">
3 Applications and Experiments
</sectionHeader>
<bodyText confidence="0.99955925">
In this section, we explain how to stack the
components in Table 1 to solve three prob-
lems: short-text categorization, Chinese word
segmentation and search ranking, respectively.
</bodyText>
<figure confidence="0.94111196">
Oracle Bone Script
ca. 1200-1050 BCE
Bronze Script
ca. 800 BCE
Small Seal Script
ca. 220 BCE
Clerical Script
ca. 50 BCE
Regular Script
ca. 200 CE
1 2 3 4
t E t A
595
Label 3
Max-Pooling
Linear 128
Lookup Table 30K
Convolution 1×3
ReLU 256
ReLU 256
Softmax 3
Short Text
LossCal
Lookup Table 30K
Concatenate 300
ReLU 256
Tanh 256
Softmax 2
Label 2
Input Text
LossCal
Max-Pooling
Linear 100 Linear 100
ReLU 512 ReLU 512
ReLU 512 ReLU 512
Linear 256 Linear 256
Convolution 1×3 Convolution 1×3
Query
Lookup Table 30K
Max-Pooling
LossCal
Titlea
Max-Pooling
Convolution 1×3
Linear 100
ReLU 512
Linear 256
ReLU 512
Titleb
(a) STC (b) CWS (c) Search Ranking
</figure>
<figureCaption confidence="0.999553">
Figure 3: Application Models using Radical Embedding
</figureCaption>
<table confidence="0.996064833333333">
Accuracy(%) Competing Methods Deep Neural Networks with Embedding
LR SVM wrd chr rdc wrd+rdc chr+rdc
Finance 93.52 94.06 94.89 95.85 94.75 95.70 95.74
Sports 92.40 92.83 95.10 95.01 92.24 95.87 95.91
Entertainment 91.72 92.24 94.32 94.77 93.21 95.11 94.78
Average 92.55 93.04 94.77 95.21 93.40 95.56 95.46
</table>
<tableCaption confidence="0.996977">
Table 2: Short Text Categorization Results
</tableCaption>
<subsectionHeader confidence="0.930382">
3.1 Short-Text Categorization
</subsectionHeader>
<bodyText confidence="0.999923642857143">
Figure 3(a) presents the network structure of
the model for short-text categorization, where
the width of each layer is marked out as well.
From the top down, a piece of short text,
e.g., the title of a URL, is fed into the net-
work, which goes through radical decomposi-
tion, table-lookup (i.e., locating the embed-
ding vector corresponding to each radical),
convolution, max pooling, two ReLU layers
and one fully connected layer, all the way to
the final softmax layer, where the loss is cal-
culated against the given label. The stan-
dard back-propagation algorithm is used to
fine tune all the parameters.
The experiment uses the top-3 categories
of the SogouCA and SogouCS news corpus
(Wang et al., 2008). 100,000 samples of each
category are randomly selected for training
and 10,000 for testing. Hyper-parameters
for SVM and LR are selected through cross-
validation. Table 2 presents the accuracy of
different methods, where “wrd”, “chr”, and
“rdc” denote word, character, and radical em-
bedding, respectively. As can be seen, embed-
ding methods outperform competing LR and
SVM algorithms uniformly, and the fusion of
radicals with words and characters improves
both.
</bodyText>
<subsectionHeader confidence="0.998078">
3.2 Chinese Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999739592592593">
Figure 3(b) presents the CWS network ar-
chitecture. It uses softmax as well because
it essentially classifies whether each charac-
ter should be a segmentation boundary. The
input is firstly decomposed into a radical se-
quence, on which a sliding window of size
3 is applied to extract features, which are
pipelined to downstream levels of the network.
We evaluate the performance using two
standard datasets: PKU and MSR, as pro-
vided by (Emerson, 2005). The PKU dataset
contains 1.1M training words and 104K test
words, and the MSR dataset contains 2.37M
training words and 107K test words. We use
the first 90% sentences for training and the
rest 10% sentences for testing. We compare
radical embedding with the CRF method2,
FNLM (Mansur et al., 2013) and PSA (Zheng
et al., 2013), and present the results in Table
3. Note that no dictionary is used in any of
these algorithms.
We see that the radical embedding (RdE)
method, as the first attempt to segment words
at radical level, actually achieves very compet-
itive results. It outperforms both CRF and
FNLM on both datasets, and is comparable
with PSA.
</bodyText>
<footnote confidence="0.9904315">
2http://crfpp.googlecode.com/svn/trunk/doc/
index.html?source=navbar
</footnote>
<page confidence="0.993715">
596
</page>
<table confidence="0.999907">
Data Approach Precision Recall F1
CRF 88.1 86.2 87.1
PKU FNLM 87.1 87.9 87.5
PSA 92.8 92.0 92.4
RdE 92.6 92.1 92.3
CRF 89.3 87.5 88.4
MSR FNLM 92.3 92.2 92.2
PSA 92.9 93.6 93.3
RdE 93.4 93.3 93.3
</table>
<tableCaption confidence="0.999367">
Table 3: CWS Result Comparison
</tableCaption>
<figure confidence="0.996540133333333">
Radical Embedding
Word Embedding
56
55
5 10 50
dataset percentage (%)
54
1
100
accuracy (%)
58
57
61
60
59
</figure>
<figureCaption confidence="0.999729">
Figure 4: Search Ranking Results
</figureCaption>
<subsectionHeader confidence="0.996612">
3.3 Web Search Ranking
</subsectionHeader>
<bodyText confidence="0.99999335">
Finally, we report on an in-field experiment
with Web search ranking. Web search lever-
ages many kinds of ranking signals, an impor-
tant one of which is the preference signals ex-
tracted from click-through logs. Given a set of
triplets {query, titlea, titleb} discovered from
click logs, where the URL titlea is preferred
to titleb for the query. The goal of learning is
to produce a matching model between query
and title that maximally agrees with the pref-
erence triplets. This learnt matching model is
combined with other signals, e.g., PageRank,
BM25F, etc. in the general ranking. The deep
network model for this task is depicted in Fig-
ure 3(c), where each triplet goes through seven
layers to compute the loss using Equation (1),
where qi, ai, bi are the output vectors for the
query and two titles right before computing
the loss. The calculated loss is then back prop-
agated to fine tune all the parameters.
</bodyText>
<equation confidence="0.913817">
log 1 + exp
−. .
qTi ai
|qi||ai|
</equation>
<bodyText confidence="0.999057259259259">
The evaluation is carried out on a propri-
etary data set provided by a leading Chi-
nese search engine company. It contains
95,640,311 triplets, which involve 14,919,928
distinct queries and 65,125,732 distinct titles.
95,502,506 triplets are used for training, with
the rest 137,805 triplets as testing. It is worth
noting that the testing triplets are hard cases,
mostly involving long queries and short title
texts.
Figure 4 presents the results, where we vary
the amount of training data to see how the per-
formance varies. The x-axis lists the percent-
age of training dataset used, and 100% means
using the entire training dataset, and the y-
axis is the accuracy of the predicted prefer-
ences. We see that word embedding is over-
all superior to radical embedding, but it is
interesting to see that word embedding sat-
urates using half of the data, while ranking
with radical embedding catches up using the
entire dataset, getting very close in accuracy
(60.78% vs. 60.47%). Because no more data
is available beyond the 95,640,311 triplets, un-
fortunately we cannot tell if radical embed-
ding would eventually surpass word embed-
ding with more data.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999987105263158">
This paper presents the first piece of work on
embedding radicals for fun and for profit, and
we are mostly inspired by fellow researchers
delving deeper in various domains (Zheng et
al., 2013; Zhang and LeCun, 2015; Collobert
et al., 2011; Kim, 2014; Johnson and Zhang,
2014; dos Santos and Gatti, 2014). For exam-
ple, Huang et al.’s work (Huang et al., 2013) on
DSSM uses letter trigram as the basic repre-
sentation, which somehow resembles radicals.
Zhang and Yann’s recent work (Zhang and Le-
Cun, 2015) represents Chinese at PinYin level,
thus taking Chinese as a western language.
Although working at PinYin level might be
a viable approach, using radicals should be
more reasonable from a linguistic point of
view. Nevertheless, PinYin only represents the
pronunciation, which is arguably further away
from semantics than radicals.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999983875">
This study presents the first piece of evidence
on the feasibility and utility of radical embed-
ding for Chinese language processing. It is in-
spired by recent success of delving deep in var-
ious domains, and roots on the rationale that
radicals, as the minimum semantic unit, could
be appropriate for deep learning. We demon-
strate the utility of radical embedding through
</bodyText>
<equation confidence="0.944147">
Xm
i=1
|qi  ||bi |! ! !
</equation>
<page confidence="0.992719">
597
</page>
<bodyText confidence="0.999913153846154">
two standard in-house and one in-field exper-
iments. While some promising results are ob-
tained, there are still many problems to be ex-
plored further, e.g., how to leverage the lay-
out code in radical decomposition that is cur-
rently neglected to improve performance. An
even more exciting topic could be to train rad-
ical, character and word embedding in a uni-
fied hierarchical model as they are naturally
hierarchical. In sum, we hope this preliminary
work could shed some light on new approaches
to Chinese language processing and other lan-
guages alike.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859794117647">
Ronan Collobert, Jason Weston, L´eon Bot-
tou, Michael Karlen, Koray Kavukcuoglu, and
Pavel P. Kuksa. 2011. Natural language pro-
cessing (almost) from scratch. Journal of Ma-
chine Learning Research, 12:2493–2537.
C´ıcero Nogueira dos Santos and Maira Gatti. 2014.
Deep convolutional neural networks for senti-
ment analysis of short texts. In 25th Inter-
national Conference on Computational Linguis-
tics, Proceedings of the Conference: Technical
Papers, pages 69–78.
Thomas Emerson. 2005. The second international
chinese word segmentation bakeoff. In Proceed-
ings of the fourth SIGHAN workshop on Chinese
language Processing, volume 133.
Mohit Iyyer, Jordan Boyd-Graber, Leonardo
Claudino, Richard Socher, and Hal Daum´e III.
2014. A neural network for factoid question an-
swering over paragraphs. In Proceedings of the
2014 Conference on Empirical Methods in Nat-
ural Language Processing, pages 633–644.
Rie Johnson and Tong Zhang. 2014. Effective use
of word order for text categorization with convo-
lutional neural networks. CoRR, abs/1412.1058.
Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1746–1751.
Mairgup Mansur, Wenzhe Pei, and Baobao Chang.
2013. Feature-based neural language model and
chinese word segmentation. In Sixth Interna-
tional Joint Conference on Natural Language
Processing, 2013, Nagoya, Japan, October 14-
18, 2013, pages 1271–1277.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gre-
gory S. Corrado, and Jeffrey Dean. 2013. Dis-
tributed representations of words and phrases
and their compositionality. In Advances in Neu-
ral Information Processing Systems, pages 3111–
3119.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gregoire Mesnil. 2014. A latent seman-
tic model with convolutional-pooling structure
for information retrieval. In Proceedings of the
23rd ACM International Conference on Confer-
ence on Information and Knowledge Manage-
ment, pages 101–110.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014. Sequence to sequence learning with neural
networks. In Advances in Neural Information
Processing Systems, pages 3104–3112.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew
Rabinovich. 2014. Going deeper with convolu-
tions. CoRR, abs/1409.4842.
Canhui Wang, Min Zhang, Shaoping Ma, and
Liyun Ru. 2008. Automatic online news is-
sue construction in web environment. In Pro-
ceedings of the 17th International Conference on
World Wide Web, pages 457–466.
Xiang Zhang and Yann LeCun. 2015. Text under-
standing from scratch. CoRR, abs/1502.01710.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmen-
tation and POS tagging. In Proceedings of the
2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 647–657.
</reference>
<page confidence="0.997022">
598
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.677197">
<title confidence="0.998178">Radical Embedding: Delving Deeper to Chinese Radicals</title>
<author confidence="0.990343">Xinlei Shi</author>
<author confidence="0.990343">Junjie Zhai</author>
<author confidence="0.990343">Xudong Yang</author>
<author confidence="0.990343">Zehua Xie</author>
<author confidence="0.990343">Chao</author>
<affiliation confidence="0.701955">Sogou Technology Inc., Beijing, China</affiliation>
<email confidence="0.978959">zhaijunjie,yangxudong,xiezehua,</email>
<abstract confidence="0.999524421052632">Languages using Chinese characters are mostly processed at word level. Inspired by recent success of deep learning, we delve deeper to character and radical levels for Chinese language processing. We propose a new deep learning technique, called “radical embedding”, with justifications based on Chinese linguistics, and validate its feasibility and utility through a set of three experiments: two in-house standard experiments on short-text categorization (STC) and Chinese word segmentation (CWS), and one in-field experiment on search ranking. We show that radical embedding achieves comparable, and sometimes even better, results than competing methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2055" citStr="Collobert et al., 2011" startWordPosition="322" endWordPosition="325">honetic Alphabet (IPA) for English. Each squared symbol is a distinct Chinese character, and there are no separators between characters calls for Chinese Word Segmentation (CWS) techniques to group adjacent characters into words. In most current applications (e.g., categorization and recommendation etc.), Chinese is a word a character Chinese: 7 )�/ )� &apos;�V A/ ffo Pinyin: jīn tiān/ tiān qì/ zhēn/ hǎo0 English: It is a nice day today. Figure 1: Illustration of Chinese Language represented at the word level. Inspired by recent success of delving deep (Szegedy et al., 2014; Zhang and LeCun, 2015; Collobert et al., 2011), an interesting question arises then: can we delve deeper than word level representation for better Chinese language processing? If the answer is yes, how deep can it be done for fun and for profit? Intuitively, the answer should be positive. Nevertheless, each Chinese character is semantically meaningful, thanks to its pictographic root from ancient Chinese as depicted in Figure 2. We could delve deeper by decomposing each character into character radicals. The right part of Figure 2 illustrates the decomposition. This Chinese character (meaning “morning”) is decomposed into 4 radicals that </context>
<context position="5189" citStr="Collobert et al., 2011" startWordPosition="816" endWordPosition="819">ural network components, which are combined and stacked to solve three application problems. Section 3 elaborates on the three applications and reports on the experiment results. With related work briefly discussed in Section 4, Section 5 concludes this study. For clarity, we limit the study to Simplified Chinese in this paper. 2 Deep Networks with Radical Embeddings This section presents the radical embedding technique, and the accompanying deep neural network components. These components are combined to solve the three applications in Section 3. Word embedding is a popular technique in NLP (Collobert et al., 2011). It maps words to vectors of real numbers in a relatively low dimensional space. It is shown that the proximity in this numeric space actually embodies algebraic semantic relationship, such as “Queen input output Convolution m V ∈ Rm+n−1 f ∈ i+n−1 k ∈ Rn s=i fs · ks−i 0 ≤ i ≤ m − n + 1 Max-pooling x ∈ Rd y = max(x) ∈ R Lookup M ∈ Rdx|D |vi = MIi ∈ Rd Table Ii ∈ R|D|x1 Tanh x ∈ Rd V ∈ Rd exi −e−xi yi = exi +e−xi 0 ≤ i ≤ d − 1 Linear x ∈ Rd V = x ∈ Rd ReLU x ∈ Rd V ∈ Rd yi = 0 if xi ≤ 0 yi = xi if xi &gt; 0 0 ≤ i ≤ d − 1 Softmax x ∈ Rd V ∈ Rd exi yi = j=1 exj 0 ≤ i ≤ d − 1 Concatenate xi ∈ Rd (x0,</context>
<context position="13608" citStr="Collobert et al., 2011" startWordPosition="2295" endWordPosition="2298">eresting to see that word embedding saturates using half of the data, while ranking with radical embedding catches up using the entire dataset, getting very close in accuracy (60.78% vs. 60.47%). Because no more data is available beyond the 95,640,311 triplets, unfortunately we cannot tell if radical embedding would eventually surpass word embedding with more data. 4 Related Work This paper presents the first piece of work on embedding radicals for fun and for profit, and we are mostly inspired by fellow researchers delving deeper in various domains (Zheng et al., 2013; Zhang and LeCun, 2015; Collobert et al., 2011; Kim, 2014; Johnson and Zhang, 2014; dos Santos and Gatti, 2014). For example, Huang et al.’s work (Huang et al., 2013) on DSSM uses letter trigram as the basic representation, which somehow resembles radicals. Zhang and Yann’s recent work (Zhang and LeCun, 2015) represents Chinese at PinYin level, thus taking Chinese as a western language. Although working at PinYin level might be a viable approach, using radicals should be more reasonable from a linguistic point of view. Nevertheless, PinYin only represents the pronunciation, which is arguably further away from semantics than radicals. 5 Co</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´ıcero Nogueira dos Santos</author>
<author>Maira Gatti</author>
</authors>
<title>Deep convolutional neural networks for sentiment analysis of short texts.</title>
<date>2014</date>
<booktitle>In 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers,</booktitle>
<pages>69--78</pages>
<contexts>
<context position="13673" citStr="Santos and Gatti, 2014" startWordPosition="2306" endWordPosition="2309">data, while ranking with radical embedding catches up using the entire dataset, getting very close in accuracy (60.78% vs. 60.47%). Because no more data is available beyond the 95,640,311 triplets, unfortunately we cannot tell if radical embedding would eventually surpass word embedding with more data. 4 Related Work This paper presents the first piece of work on embedding radicals for fun and for profit, and we are mostly inspired by fellow researchers delving deeper in various domains (Zheng et al., 2013; Zhang and LeCun, 2015; Collobert et al., 2011; Kim, 2014; Johnson and Zhang, 2014; dos Santos and Gatti, 2014). For example, Huang et al.’s work (Huang et al., 2013) on DSSM uses letter trigram as the basic representation, which somehow resembles radicals. Zhang and Yann’s recent work (Zhang and LeCun, 2015) represents Chinese at PinYin level, thus taking Chinese as a western language. Although working at PinYin level might be a viable approach, using radicals should be more reasonable from a linguistic point of view. Nevertheless, PinYin only represents the pronunciation, which is arguably further away from semantics than radicals. 5 Conclusion This study presents the first piece of evidence on the f</context>
</contexts>
<marker>Santos, Gatti, 2014</marker>
<rawString>C´ıcero Nogueira dos Santos and Maira Gatti. 2014. Deep convolutional neural networks for sentiment analysis of short texts. In 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, pages 69–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the fourth SIGHAN workshop on Chinese language Processing,</booktitle>
<volume>133</volume>
<contexts>
<context position="10150" citStr="Emerson, 2005" startWordPosition="1706" endWordPosition="1707">ding methods outperform competing LR and SVM algorithms uniformly, and the fusion of radicals with words and characters improves both. 3.2 Chinese Word Segmentation Figure 3(b) presents the CWS network architecture. It uses softmax as well because it essentially classifies whether each character should be a segmentation boundary. The input is firstly decomposed into a radical sequence, on which a sliding window of size 3 is applied to extract features, which are pipelined to downstream levels of the network. We evaluate the performance using two standard datasets: PKU and MSR, as provided by (Emerson, 2005). The PKU dataset contains 1.1M training words and 104K test words, and the MSR dataset contains 2.37M training words and 107K test words. We use the first 90% sentences for training and the rest 10% sentences for testing. We compare radical embedding with the CRF method2, FNLM (Mansur et al., 2013) and PSA (Zheng et al., 2013), and present the results in Table 3. Note that no dictionary is used in any of these algorithms. We see that the radical embedding (RdE) method, as the first attempt to segment words at radical level, actually achieves very competitive results. It outperforms both CRF a</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the fourth SIGHAN workshop on Chinese language Processing, volume 133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Iyyer</author>
<author>Jordan Boyd-Graber</author>
<author>Leonardo Claudino</author>
<author>Richard Socher</author>
<author>Hal Daum´e</author>
</authors>
<title>A neural network for factoid question answering over paragraphs.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>633--644</pages>
<marker>Iyyer, Boyd-Graber, Claudino, Socher, Daum´e, 2014</marker>
<rawString>Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III. 2014. A neural network for factoid question answering over paragraphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 633–644.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Johnson</author>
<author>Tong Zhang</author>
</authors>
<title>Effective use of word order for text categorization with convolutional neural networks.</title>
<date>2014</date>
<tech>CoRR, abs/1412.1058.</tech>
<contexts>
<context position="13644" citStr="Johnson and Zhang, 2014" startWordPosition="2301" endWordPosition="2304"> saturates using half of the data, while ranking with radical embedding catches up using the entire dataset, getting very close in accuracy (60.78% vs. 60.47%). Because no more data is available beyond the 95,640,311 triplets, unfortunately we cannot tell if radical embedding would eventually surpass word embedding with more data. 4 Related Work This paper presents the first piece of work on embedding radicals for fun and for profit, and we are mostly inspired by fellow researchers delving deeper in various domains (Zheng et al., 2013; Zhang and LeCun, 2015; Collobert et al., 2011; Kim, 2014; Johnson and Zhang, 2014; dos Santos and Gatti, 2014). For example, Huang et al.’s work (Huang et al., 2013) on DSSM uses letter trigram as the basic representation, which somehow resembles radicals. Zhang and Yann’s recent work (Zhang and LeCun, 2015) represents Chinese at PinYin level, thus taking Chinese as a western language. Although working at PinYin level might be a viable approach, using radicals should be more reasonable from a linguistic point of view. Nevertheless, PinYin only represents the pronunciation, which is arguably further away from semantics than radicals. 5 Conclusion This study presents the fir</context>
</contexts>
<marker>Johnson, Zhang, 2014</marker>
<rawString>Rie Johnson and Tong Zhang. 2014. Effective use of word order for text categorization with convolutional neural networks. CoRR, abs/1412.1058.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1746--1751</pages>
<contexts>
<context position="13619" citStr="Kim, 2014" startWordPosition="2299" endWordPosition="2300">d embedding saturates using half of the data, while ranking with radical embedding catches up using the entire dataset, getting very close in accuracy (60.78% vs. 60.47%). Because no more data is available beyond the 95,640,311 triplets, unfortunately we cannot tell if radical embedding would eventually surpass word embedding with more data. 4 Related Work This paper presents the first piece of work on embedding radicals for fun and for profit, and we are mostly inspired by fellow researchers delving deeper in various domains (Zheng et al., 2013; Zhang and LeCun, 2015; Collobert et al., 2011; Kim, 2014; Johnson and Zhang, 2014; dos Santos and Gatti, 2014). For example, Huang et al.’s work (Huang et al., 2013) on DSSM uses letter trigram as the basic representation, which somehow resembles radicals. Zhang and Yann’s recent work (Zhang and LeCun, 2015) represents Chinese at PinYin level, thus taking Chinese as a western language. Although working at PinYin level might be a viable approach, using radicals should be more reasonable from a linguistic point of view. Nevertheless, PinYin only represents the pronunciation, which is arguably further away from semantics than radicals. 5 Conclusion Th</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1746–1751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mairgup Mansur</author>
<author>Wenzhe Pei</author>
<author>Baobao Chang</author>
</authors>
<title>Feature-based neural language model and chinese word segmentation.</title>
<date>2013</date>
<booktitle>In Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>1271--1277</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="10450" citStr="Mansur et al., 2013" startWordPosition="1755" endWordPosition="1758">er should be a segmentation boundary. The input is firstly decomposed into a radical sequence, on which a sliding window of size 3 is applied to extract features, which are pipelined to downstream levels of the network. We evaluate the performance using two standard datasets: PKU and MSR, as provided by (Emerson, 2005). The PKU dataset contains 1.1M training words and 104K test words, and the MSR dataset contains 2.37M training words and 107K test words. We use the first 90% sentences for training and the rest 10% sentences for testing. We compare radical embedding with the CRF method2, FNLM (Mansur et al., 2013) and PSA (Zheng et al., 2013), and present the results in Table 3. Note that no dictionary is used in any of these algorithms. We see that the radical embedding (RdE) method, as the first attempt to segment words at radical level, actually achieves very competitive results. It outperforms both CRF and FNLM on both datasets, and is comparable with PSA. 2http://crfpp.googlecode.com/svn/trunk/doc/ index.html?source=navbar 596 Data Approach Precision Recall F1 CRF 88.1 86.2 87.1 PKU FNLM 87.1 87.9 87.5 PSA 92.8 92.0 92.4 RdE 92.6 92.1 92.3 CRF 89.3 87.5 88.4 MSR FNLM 92.3 92.2 92.2 PSA 92.9 93.6 9</context>
</contexts>
<marker>Mansur, Pei, Chang, 2013</marker>
<rawString>Mairgup Mansur, Wenzhe Pei, and Baobao Chang. 2013. Feature-based neural language model and chinese word segmentation. In Sixth International Joint Conference on Natural Language Processing, 2013, Nagoya, Japan, October 14-18, 2013, pages 1271–1277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="6087" citStr="Mikolov et al., 2013" startWordPosition="1038" endWordPosition="1041"> m − n + 1 Max-pooling x ∈ Rd y = max(x) ∈ R Lookup M ∈ Rdx|D |vi = MIi ∈ Rd Table Ii ∈ R|D|x1 Tanh x ∈ Rd V ∈ Rd exi −e−xi yi = exi +e−xi 0 ≤ i ≤ d − 1 Linear x ∈ Rd V = x ∈ Rd ReLU x ∈ Rd V ∈ Rd yi = 0 if xi ≤ 0 yi = xi if xi &gt; 0 0 ≤ i ≤ d − 1 Softmax x ∈ Rd V ∈ Rd exi yi = j=1 exj 0 ≤ i ≤ d − 1 Concatenate xi ∈ Rd (x0, x1 ..., xn−1) 0 ≤ i ≤ n − 1 E Rddxn D: radical vocabulary M: a matrix containing |D |columns, each column is a d-dimensional vector represent radical in D. Ii: a one hot vector stands for the ith radical in vocabulary Table 1: Neural Network Components − Woman + Man ≈ King” (Mikolov et al., 2013). As demonstrated in previous work, this numeric representation of words has led to big improvements in many NLP tasks such as machine translation (Sutskever et al., 2014), question answering (Iyyer et al., 2014) and document ranking (Shen et al., 2014). Radical embedding is similar to word embedding except that the embedding is at radical level. There are two ways of embedding: C13OW and skip-gram (Mikolov et al., 2013). We here use C13OW for radical embedding because the two methods exhibit few differences, and C13OW is slightly faster in experiments. Specifically, a sequence of Chinese char</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111– 3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yelong Shen</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
<author>Gregoire Mesnil</author>
</authors>
<title>A latent semantic model with convolutional-pooling structure for information retrieval.</title>
<date>2014</date>
<booktitle>In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,</booktitle>
<pages>101--110</pages>
<contexts>
<context position="6340" citStr="Shen et al., 2014" startWordPosition="1078" endWordPosition="1081">∈ Rd V ∈ Rd exi yi = j=1 exj 0 ≤ i ≤ d − 1 Concatenate xi ∈ Rd (x0, x1 ..., xn−1) 0 ≤ i ≤ n − 1 E Rddxn D: radical vocabulary M: a matrix containing |D |columns, each column is a d-dimensional vector represent radical in D. Ii: a one hot vector stands for the ith radical in vocabulary Table 1: Neural Network Components − Woman + Man ≈ King” (Mikolov et al., 2013). As demonstrated in previous work, this numeric representation of words has led to big improvements in many NLP tasks such as machine translation (Sutskever et al., 2014), question answering (Iyyer et al., 2014) and document ranking (Shen et al., 2014). Radical embedding is similar to word embedding except that the embedding is at radical level. There are two ways of embedding: C13OW and skip-gram (Mikolov et al., 2013). We here use C13OW for radical embedding because the two methods exhibit few differences, and C13OW is slightly faster in experiments. Specifically, a sequence of Chinese characters is decomposed into a sequence of radicals, to which C13OW is applied. We use the word2vec package (Mikolov et al., 2013) to train radical vectors, and then initialize the lookup table with these radical vectors. We list the network components in </context>
</contexts>
<marker>Shen, He, Gao, Deng, Mesnil, 2014</marker>
<rawString>Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gregoire Mesnil. 2014. A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 101–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="6258" citStr="Sutskever et al., 2014" startWordPosition="1065" endWordPosition="1068">= x ∈ Rd ReLU x ∈ Rd V ∈ Rd yi = 0 if xi ≤ 0 yi = xi if xi &gt; 0 0 ≤ i ≤ d − 1 Softmax x ∈ Rd V ∈ Rd exi yi = j=1 exj 0 ≤ i ≤ d − 1 Concatenate xi ∈ Rd (x0, x1 ..., xn−1) 0 ≤ i ≤ n − 1 E Rddxn D: radical vocabulary M: a matrix containing |D |columns, each column is a d-dimensional vector represent radical in D. Ii: a one hot vector stands for the ith radical in vocabulary Table 1: Neural Network Components − Woman + Man ≈ King” (Mikolov et al., 2013). As demonstrated in previous work, this numeric representation of words has led to big improvements in many NLP tasks such as machine translation (Sutskever et al., 2014), question answering (Iyyer et al., 2014) and document ranking (Shen et al., 2014). Radical embedding is similar to word embedding except that the embedding is at radical level. There are two ways of embedding: C13OW and skip-gram (Mikolov et al., 2013). We here use C13OW for radical embedding because the two methods exhibit few differences, and C13OW is slightly faster in experiments. Specifically, a sequence of Chinese characters is decomposed into a sequence of radicals, to which C13OW is applied. We use the word2vec package (Mikolov et al., 2013) to train radical vectors, and then initiali</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Szegedy</author>
<author>Wei Liu</author>
<author>Yangqing Jia</author>
<author>Pierre Sermanet</author>
<author>Scott Reed</author>
</authors>
<title>Dragomir Anguelov, Dumitru Erhan,</title>
<date>2014</date>
<journal>CoRR,</journal>
<pages>1409--4842</pages>
<location>Vincent Vanhoucke, and</location>
<contexts>
<context position="2007" citStr="Szegedy et al., 2014" startWordPosition="314" endWordPosition="317">on of Chinese, similar to the International Phonetic Alphabet (IPA) for English. Each squared symbol is a distinct Chinese character, and there are no separators between characters calls for Chinese Word Segmentation (CWS) techniques to group adjacent characters into words. In most current applications (e.g., categorization and recommendation etc.), Chinese is a word a character Chinese: 7 )�/ )� &apos;�V A/ ffo Pinyin: jīn tiān/ tiān qì/ zhēn/ hǎo0 English: It is a nice day today. Figure 1: Illustration of Chinese Language represented at the word level. Inspired by recent success of delving deep (Szegedy et al., 2014; Zhang and LeCun, 2015; Collobert et al., 2011), an interesting question arises then: can we delve deeper than word level representation for better Chinese language processing? If the answer is yes, how deep can it be done for fun and for profit? Intuitively, the answer should be positive. Nevertheless, each Chinese character is semantically meaningful, thanks to its pictographic root from ancient Chinese as depicted in Figure 2. We could delve deeper by decomposing each character into character radicals. The right part of Figure 2 illustrates the decomposition. This Chinese character (meanin</context>
</contexts>
<marker>Szegedy, Liu, Jia, Sermanet, Reed, 2014</marker>
<rawString>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2014. Going deeper with convolutions. CoRR, abs/1409.4842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canhui Wang</author>
<author>Min Zhang</author>
<author>Shaoping Ma</author>
<author>Liyun Ru</author>
</authors>
<title>Automatic online news issue construction in web environment.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on World Wide Web,</booktitle>
<pages>457--466</pages>
<contexts>
<context position="9208" citStr="Wang et al., 2008" startWordPosition="1555" endWordPosition="1558"> where the width of each layer is marked out as well. From the top down, a piece of short text, e.g., the title of a URL, is fed into the network, which goes through radical decomposition, table-lookup (i.e., locating the embedding vector corresponding to each radical), convolution, max pooling, two ReLU layers and one fully connected layer, all the way to the final softmax layer, where the loss is calculated against the given label. The standard back-propagation algorithm is used to fine tune all the parameters. The experiment uses the top-3 categories of the SogouCA and SogouCS news corpus (Wang et al., 2008). 100,000 samples of each category are randomly selected for training and 10,000 for testing. Hyper-parameters for SVM and LR are selected through crossvalidation. Table 2 presents the accuracy of different methods, where “wrd”, “chr”, and “rdc” denote word, character, and radical embedding, respectively. As can be seen, embedding methods outperform competing LR and SVM algorithms uniformly, and the fusion of radicals with words and characters improves both. 3.2 Chinese Word Segmentation Figure 3(b) presents the CWS network architecture. It uses softmax as well because it essentially classifie</context>
</contexts>
<marker>Wang, Zhang, Ma, Ru, 2008</marker>
<rawString>Canhui Wang, Min Zhang, Shaoping Ma, and Liyun Ru. 2008. Automatic online news issue construction in web environment. In Proceedings of the 17th International Conference on World Wide Web, pages 457–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Zhang</author>
<author>Yann LeCun</author>
</authors>
<title>Text understanding from scratch.</title>
<date>2015</date>
<location>CoRR, abs/1502.01710.</location>
<contexts>
<context position="2030" citStr="Zhang and LeCun, 2015" startWordPosition="318" endWordPosition="321"> to the International Phonetic Alphabet (IPA) for English. Each squared symbol is a distinct Chinese character, and there are no separators between characters calls for Chinese Word Segmentation (CWS) techniques to group adjacent characters into words. In most current applications (e.g., categorization and recommendation etc.), Chinese is a word a character Chinese: 7 )�/ )� &apos;�V A/ ffo Pinyin: jīn tiān/ tiān qì/ zhēn/ hǎo0 English: It is a nice day today. Figure 1: Illustration of Chinese Language represented at the word level. Inspired by recent success of delving deep (Szegedy et al., 2014; Zhang and LeCun, 2015; Collobert et al., 2011), an interesting question arises then: can we delve deeper than word level representation for better Chinese language processing? If the answer is yes, how deep can it be done for fun and for profit? Intuitively, the answer should be positive. Nevertheless, each Chinese character is semantically meaningful, thanks to its pictographic root from ancient Chinese as depicted in Figure 2. We could delve deeper by decomposing each character into character radicals. The right part of Figure 2 illustrates the decomposition. This Chinese character (meaning “morning”) is decompo</context>
<context position="13584" citStr="Zhang and LeCun, 2015" startWordPosition="2291" endWordPosition="2294">mbedding, but it is interesting to see that word embedding saturates using half of the data, while ranking with radical embedding catches up using the entire dataset, getting very close in accuracy (60.78% vs. 60.47%). Because no more data is available beyond the 95,640,311 triplets, unfortunately we cannot tell if radical embedding would eventually surpass word embedding with more data. 4 Related Work This paper presents the first piece of work on embedding radicals for fun and for profit, and we are mostly inspired by fellow researchers delving deeper in various domains (Zheng et al., 2013; Zhang and LeCun, 2015; Collobert et al., 2011; Kim, 2014; Johnson and Zhang, 2014; dos Santos and Gatti, 2014). For example, Huang et al.’s work (Huang et al., 2013) on DSSM uses letter trigram as the basic representation, which somehow resembles radicals. Zhang and Yann’s recent work (Zhang and LeCun, 2015) represents Chinese at PinYin level, thus taking Chinese as a western language. Although working at PinYin level might be a viable approach, using radicals should be more reasonable from a linguistic point of view. Nevertheless, PinYin only represents the pronunciation, which is arguably further away from seman</context>
</contexts>
<marker>Zhang, LeCun, 2015</marker>
<rawString>Xiang Zhang and Yann LeCun. 2015. Text understanding from scratch. CoRR, abs/1502.01710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqing Zheng</author>
<author>Hanyang Chen</author>
<author>Tianyu Xu</author>
</authors>
<title>Deep learning for chinese word segmentation and POS tagging.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>647--657</pages>
<contexts>
<context position="10479" citStr="Zheng et al., 2013" startWordPosition="1761" endWordPosition="1764">undary. The input is firstly decomposed into a radical sequence, on which a sliding window of size 3 is applied to extract features, which are pipelined to downstream levels of the network. We evaluate the performance using two standard datasets: PKU and MSR, as provided by (Emerson, 2005). The PKU dataset contains 1.1M training words and 104K test words, and the MSR dataset contains 2.37M training words and 107K test words. We use the first 90% sentences for training and the rest 10% sentences for testing. We compare radical embedding with the CRF method2, FNLM (Mansur et al., 2013) and PSA (Zheng et al., 2013), and present the results in Table 3. Note that no dictionary is used in any of these algorithms. We see that the radical embedding (RdE) method, as the first attempt to segment words at radical level, actually achieves very competitive results. It outperforms both CRF and FNLM on both datasets, and is comparable with PSA. 2http://crfpp.googlecode.com/svn/trunk/doc/ index.html?source=navbar 596 Data Approach Precision Recall F1 CRF 88.1 86.2 87.1 PKU FNLM 87.1 87.9 87.5 PSA 92.8 92.0 92.4 RdE 92.6 92.1 92.3 CRF 89.3 87.5 88.4 MSR FNLM 92.3 92.2 92.2 PSA 92.9 93.6 93.3 RdE 93.4 93.3 93.3 Table </context>
<context position="13561" citStr="Zheng et al., 2013" startWordPosition="2287" endWordPosition="2290">uperior to radical embedding, but it is interesting to see that word embedding saturates using half of the data, while ranking with radical embedding catches up using the entire dataset, getting very close in accuracy (60.78% vs. 60.47%). Because no more data is available beyond the 95,640,311 triplets, unfortunately we cannot tell if radical embedding would eventually surpass word embedding with more data. 4 Related Work This paper presents the first piece of work on embedding radicals for fun and for profit, and we are mostly inspired by fellow researchers delving deeper in various domains (Zheng et al., 2013; Zhang and LeCun, 2015; Collobert et al., 2011; Kim, 2014; Johnson and Zhang, 2014; dos Santos and Gatti, 2014). For example, Huang et al.’s work (Huang et al., 2013) on DSSM uses letter trigram as the basic representation, which somehow resembles radicals. Zhang and Yann’s recent work (Zhang and LeCun, 2015) represents Chinese at PinYin level, thus taking Chinese as a western language. Although working at PinYin level might be a viable approach, using radicals should be more reasonable from a linguistic point of view. Nevertheless, PinYin only represents the pronunciation, which is arguably </context>
</contexts>
<marker>Zheng, Chen, Xu, 2013</marker>
<rawString>Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for chinese word segmentation and POS tagging. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 647–657.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>