<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.986617">
SemanticNet-Perception of Human Pragmatics
</title>
<author confidence="0.998128">
Amitava Das1 and Sivaji Bandyopadhyay2
</author>
<affiliation confidence="0.9951755">
Department of Computer Science and Engineering
Jadavpur University
</affiliation>
<email confidence="0.968175">
amitava.santu@gmail.com1 sivaji_cse_ju@yahoo.com2
</email>
<sectionHeader confidence="0.993777" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999897225806452">
SemanticNet is a semantic network of
lexicons to hold human pragmatic
knowledge. So far Natural Language
Processing (NLP) research patronized
much of manually augmented lexicon
resources such as WordNet. But the
small set of semantic relations like
Hypernym, Holonym, Meronym and
Synonym etc are very narrow to cap-
ture the wide variations human cogni-
tive knowledge. But no such informa-
tion could be retrieved from available
lexicon resources. SemanticNet is the
attempt to capture wide range of con-
text dependent semantic inference
among various themes which human
beings perceive in their pragmatic
knowledge, learned by day to day cog-
nitive interactions with the surrounding
physical world. SemanticNet holds
human pragmatics with twenty well es-
tablished semantic relations for every
pair of lexemes. As every pair of rela-
tions cannot be defined by fixed num-
ber of certain semantic relation labels
thus additionally contextual semantic
affinity inference in SemanticNet could
be calculated by network distance and
represented as a probabilistic score.
SemanticNet is being presently devel-
oped for Bengali language.
</bodyText>
<sectionHeader confidence="0.98758" genericHeader="keywords">
1 Historical Motivation
</sectionHeader>
<bodyText confidence="0.953648666666667">
Semantics (from Greek &amp;quot;orivαvztxo(;&amp;quot; - seman-
tikos) is the study of meaning, usually in lan-
guage. The word &amp;quot;semantics&amp;quot; itself denotes a
range of ideas, from the popular to the highly
technical. It is often used in ordinary language
to denote a problem of understanding that
comes down to word selection or connotation.
We studied with various Psycholinguistics ex-
periments to understand how human natural
intelligence helps to understand general se-
mantic from nature. Our study was to under-
stand the human psychology about semantics
beyond language. We were haunting for the
intellectual structure of the psychological and
neurobiological factors that enable humans to
acquire, use, comprehend and produce natural
languages. Let’s come with an example of
simple conversation about movie between two
persons.
Person A: Have you seen the
movie ‘No Man&apos;s Land’? How
is it?
Person B: Although it is
good but you should see
</bodyText>
<subsubsectionHeader confidence="0.532393">
‘The Hurt Locker’?
</subsubsectionHeader>
<bodyText confidence="0.999757315789474">
May be the conversation looks very casual,
but our intension was to find out the direction
of the decision logic on the Person B’s brain.
We start digging to find out the nature of hu-
man intelligent thinking. A prolonged discus-
sion with Person B reveals that the decision
logic path to recommend a good movie was as
the Figure 1. The highlighted red paths are the
shortest semantic affinity distances of the hu-
man brain.
We call it semantic thinking. Although the
derivational path of semantic thinking is not
such easy as we portrait in Figure 1 but we
keep it easier for understandability. Actually a
human try to figure out the closest semantic
affinity node into his pragmatics knowledge by
natural intelligence. In the previous example
Person B find out with his intelligence that No
Man&apos;s Land is a war movie and got Oscar
</bodyText>
<page confidence="0.950982">
2
</page>
<note confidence="0.885804">
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 2–11,
Beijing, August 2010
</note>
<bodyText confidence="0.994441636363636">
award. Oscar award generally cracked by Hol-
lywood movies and thus Person B start search-
ing his pragmatics network to find out a movie
fall into war genre, from Hollywood and may
be got Oscar award. Person B finds out the
name of a movie The Hurt Locker at nearer
distance into his pragmatics knowledge net-
work which is an optimized recommendation
that satisfy all the criteria. Noticeably Person B
didn’t choice the other paths like Bollywood,
Foreign movie etc.
</bodyText>
<figureCaption confidence="0.99458">
Figure 1: Semantic Thinking
</figureCaption>
<bodyText confidence="0.999882157894737">
And thus our aim was to develop a computa-
tional lexicon structure for semantics as human
pragmatics knowledge. We spare long time to
find out the most robust structure to represent
pragmatics knowledge properly and it should
be easy understandable for next level of search
and usability.
We look into literature that probably direct
to the direction of our ideological thinking. We
found that in the year of 1996 Push Singh and
Marvin Minsky proposed the field has shat-
tered into subfields populated by researchers
with different goals and who speak very differ-
ent technical languages. Much has been
learned, and it is time to start integrating what
we&apos;ve learned, but few researchers are widely
versed enough to do so. They had a proposal
for how to do so in their ConceptNet work.
They developed lexicon resources like Con-
ceptNet (Liu and Singh, 2004). ConceptNet-
ConceptNet is a large-scale semantic network
(over 1.6 million links) relating a wide variety
of ordinary objects, events, places, actions, and
goals by only 20 different link types, mined
from corpus.
The present task of developing SemanticNet
is to capture semantic affinity knowledge of
human pragmatics as a lexicon database. We
extend our vision from the human common
sense (as in ConceptNet) to human pragmatics
and have proposed semantic relations for every
pair of lexemes that cannot be defined by fixed
number of certain semantic relation labels.
Contextual semantic affinity inference in Se-
manticNet could be calculated by network dis-
tance and represented as a probabilistic score.
SemanticNet is being presently developed for
Bengali language.
</bodyText>
<sectionHeader confidence="0.951336" genericHeader="introduction">
2 Semantic Roles
</sectionHeader>
<bodyText confidence="0.9999179">
The ideological study of semantic roles started
age old ago since Panini’s karaka theory that
assigns generic semantic roles to words in a
natural language sentence. Semantic roles are
generally domain specific in nature such as
FROM_DESTINATION,TO_DESTINATION,
DEPARTURE_TIME etc. Verb-specific se-
mantic roles have also been defined such as
EATER and EATEN for the verb eat. The
standard datasets that are used in various Eng-
lish SRL systems are: PropBank (Palmer et al.,
2005), FrameNet (Fillmore et al., 2003) and
VerbNet (Kipper et al., 2006). These collec-
tions contain manually developed well-trusted
gold reference annotations of both syntactic
and predicate-argument structures.
PropBank defines semantic roles for each
verb. The various semantic roles identified
(Dowty, 1991) are Agent, patient or theme etc.
In addition to verb-specific roles, PropBank
defines several more general roles that can ap-
ply to any verb (Palmer et al., 2005).
FrameNet is annotated with verb frame se-
mantics and supported by corpus evidence.
The frame-to-frame relations defined in Fra-
meNet are Inheritance, Perspective_on, Sub-
frame, Precedes, Inchoative_of, Causative_of
and Using. Frame development focuses on pa-
raphrasability (or near paraphrasability) of
words and multi-words.
VerbNet annotated with thematic roles refer
to the underlying semantic relationship be-
tween a predicate and its arguments. The se-
mantic tagset of VerbNet consists of tags as
agent, patient, theme, experiencer, stimulus,
instrument, location, source, goal, recipient,
benefactive etc.
It is evident from the above discussions that
no adequate semantic role set exists that can be
defines across various domains. Hence pro-
</bodyText>
<page confidence="0.997063">
3
</page>
<bodyText confidence="0.999618727272727">
Fortunately such corpus development could
be found in (Ekbal and Bandyopadhyay, 2008)
for Bengali. We obtained the corpus from the
authors. The Bengali NEWS corpus consisted
of consecutive 4 years of NEWS stories with
various sub domains as reported above. For the
present task we have used the Bengali NEWS
corpus, developed from the archive of a lead-
ing Bengali NEWS paper2 available on the
Web. The NEWS corpus is quite larger in size
as reported in Table 1.
</bodyText>
<sectionHeader confidence="0.975452" genericHeader="method">
4 Annotation
</sectionHeader>
<bodyText confidence="0.999438636363636">
From the collected document set 200 docu-
ments have been chosen randomly for the an-
notation task. Three annotators (Mr. X, Mr. Y
and Mr. Z) participated in the present task.
Annotators were asked to annotate the theme
words (topical expressions) which best de-
scribe the topical snapshot of the document.
The agreement of annotations among three
annotators has been evaluated. The agreements
of tag values at theme words level is reported
in Table 2.
</bodyText>
<table confidence="0.782719">
Annotators X vs. Y X Vs. Z Y Vs. Z Avg
Percentage 82.64% 71.78% 80.47% 78.3%
All Agree 75.45%
</table>
<tableCaption confidence="0.568657">
Table 2: Agreement of annotators at theme
words level
</tableCaption>
<sectionHeader confidence="0.44742" genericHeader="method">
5 Theme Identification
</sectionHeader>
<bodyText confidence="0.977386692307692">
Term Frequency (TF) plays a crucial role to
identify document relevance in Topic-Based
Information Retrieval. The motivation behind
developing Theme detection technique is that
in many documents relevant words may not
occur frequently or irrelevant words may occur
frequently. Moreover for the lexicon affinity
inference, topic or theme words are the only
strong clue to start with. The Theme detection
technique has been proposed to resolve these
issues to identify discourse level most relevant
thematic nodes in terms of word or lexicon
using a standard machine learning technique.
The machine learning technique used here is
Conditional Random Field (CRF)3. The theme
word detection has been defined as a sequence
posed SemanticNet does not only rely on fixed
type of semantics roles as ConceptNet. For
semantic relations we followed the 20 relations
defined in ConceptNet. Additionally we pro-
posed semantic relations for every pair of lex-
icons cannot be defined by exact semantic role
and thus we formulated a probabilistic score
based technique. Semantic affinity in Seman-
ticNet could be calculated by network distance.
Details could be found in relevant Section 8.
</bodyText>
<sectionHeader confidence="0.990341" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.890052142857143">
Present SemanticNet has been developed for
Bengali language. Resource acquisition is one
of the most challenging obstacles to work with
electronically resource constrained languages
like Bengali. Although Bengali is the sixth1
popular language in the World, second in India
and the national language in Bangladesh.
There was another issue drive us long way
to find out the proper corpus for the develop-
ment of SemanticNet. As the notion is to cap-
ture and store human pragmatic knowledge so
the hypothesis was chosen corpus should not
be biased towards any specific domain know-
ledge as human pragmatic knowledge is not
constricted to any domain rather it has a wide
spread range over anything related to universe
and life on earth. Additionally it must be larger
in size to cover mostly available general con-
cepts related to any topic. After a detail analy-
sis we decided it is better to choose NEWS
corpus as various domains knowledge like Pol-
itics, Sports, Entertainment, Social Issues,
Science, Arts and Culture, Tourism, Adver-
tisement, TV schedule, Tender, Comics and
Weather etc are could be found only in NEWS
corpus.
Statistics NEWS
Total no. of news documents in the 108,305
corpus
Total no. of sentences in the corpus 2,822,737
Avg no. of sentences in a document 27
Total no. of wordforms in the corpus 33,836,736
Avg. no. of wordforms in a document 313
Total no. of distinct wordforms in the 467,858
corpus
</bodyText>
<tableCaption confidence="0.986963">
Table 1: Bengali Corpus Statistics
</tableCaption>
<footnote confidence="0.780939333333333">
1
http://en.wikipedia.org/wiki/List_of_languages_by_ 2 http://www.anandabazar.com/
number_of_native_speakers 3 http://crfpp.sourceforge.net
</footnote>
<page confidence="0.994919">
4
</page>
<bodyText confidence="0.99969075">
labeling problem using various useful depend-
ing features. Depending upon the series of in-
put features, each word is tagged as either
Theme Word (TW) or Other (O).
</bodyText>
<subsectionHeader confidence="0.99494">
5.1 Feature Organization
</subsectionHeader>
<bodyText confidence="0.9998546">
The set of features used in the present task
have been categorized as Lexico-Syntactic,
Syntactic and Discourse level features. These
are listed in the Table 3 below and have been
described in the subsequent subsections.
</bodyText>
<table confidence="0.992633">
Types Features
Lexico-Syntactic POS
Frequency
Stemming
Syntactic Chunk Label
Dependency Parsing Depth
Discourse Level Title of the Document
First Paragraph
Term Distribution
Collocation
</table>
<tableCaption confidence="0.99415">
Table 3: Features
</tableCaption>
<subsectionHeader confidence="0.9993285">
5.2 Lexico-Syntactic Features
5.2.1 Part of Speech (POS)
</subsectionHeader>
<bodyText confidence="0.999918111111111">
It has been shown by Das and Bandyopadhyay,
(2009), that theme bearing words in sentences
are mainly adjective, adverb, noun and verbs
as other POS categories like pronoun, preposi-
tion, conjunct, article etc. have no relevance
towards thematic semantic of any document.
The detail of the POS tagging system chosen
for the present task could be found in (Das and
Bandyopadhyay 2009).
</bodyText>
<subsectionHeader confidence="0.994126">
5.3 Frequency
</subsectionHeader>
<bodyText confidence="0.999912666666667">
Frequency always plays a crucial role in identi-
fying the importance of a word in the docu-
ment or corpus. The system generates four
separate high frequent word lists after function
words are removed for four POS categories:
adjective, adverb, verb and noun. Word fre-
quency values are then effectively used as a
crucial feature in the Theme Detection tech-
nique.
</bodyText>
<subsectionHeader confidence="0.99725">
5.4 Stemming
</subsectionHeader>
<bodyText confidence="0.999989071428571">
Several words in a sentence that carry thematic
information may be present in inflected forms.
Stemming is necessary for such inflected
words before they can be searched in appropri-
ate lists. Due to non availability of good stem-
mers in Indian languages especially in Bengali,
a stemmer based on stemming cluster tech-
nique has been used as described in (Das and
Bandyopadhyay, 2010). This stemmer analyz-
es prefixes and suffixes of all the word forms
present in a particular document. Words that
are identified to have the same root form are
grouped in a finite number of clusters with the
identified root word as cluster center.
</bodyText>
<subsectionHeader confidence="0.992571">
5.5 Syntactic Features
5.5.1 Chunk Label
</subsectionHeader>
<bodyText confidence="0.999836333333333">
We found that Chunk level information is very
much effective to identify lexicon inference
affinity. As an example:
</bodyText>
<equation confidence="0.889261333333333">
(������ fid)/NP (Vmg
WftZfff)/NP (ZPTGT)/NP
(SiT�dq)/JJP (।)/SYM
</equation>
<bodyText confidence="0.9996148">
The movies released by Sa-
tyajit Roy are excellent.
In the above example two lexicons
“I&amp;/release” and “~~~/movie” are collo-
cated in a chunk and they are very much se-
mantically neighboring in human pragmatic
knowledge. Chunk feature effectively used in
supervised classifier. Chunk labels are defined
as B-X (Beginning), I-X (Intermediate) and E-
X (End), where X is the chunk label. In the
task of identification of Theme expressions,
chunk label markers play a crucial role. Fur-
ther details of development of chunking sys-
tem could be found in (Das and Bandyopad-
hyay 2009).
</bodyText>
<subsectionHeader confidence="0.672097">
5.5.2 Dependency Parser
</subsectionHeader>
<bodyText confidence="0.9999705">
Dependency depth feature is very useful to
identify Theme expressions. A particular
Theme word generally occurs within a particu-
lar range of depth in a dependency tree. Theme
expressions may be a Named Entity (NE: per-
son, organization or location names), a com-
mon noun (Ex: accident, bomb blast, strike etc)
or words of other POS categories. It has been
observed that depending upon the nature of
Theme expressions it can occur within a cer-
tain depth in the dependency tree in the sen-
tences. A statistical dependency parser has
</bodyText>
<page confidence="0.964997">
5
</page>
<bodyText confidence="0.993932">
been used for Bengali as described in (Ghosh
et al., 2009).
</bodyText>
<subsectionHeader confidence="0.976517">
5.6 Discourse Level Features
5.6.1 Positional Aspect
</subsectionHeader>
<bodyText confidence="0.999984916666667">
Depending upon the position of the thematic
clue, every document is divided into a number
of zones. The features considered for each
document are Title words of the document, the
first paragraph words and the words from the
last two sentences. A detailed study was done
on the Bengali news corpus to identify the
roles of the positional aspect features of a doc-
ument (first paragraph, last two sentences) in
the detection of theme words. The importance
of these positional features has been described
in the following section.
</bodyText>
<subsubsectionHeader confidence="0.711172">
5.6.2 Title Words
</subsubsectionHeader>
<bodyText confidence="0.9998968">
It has been observed that the Title words of a
document always carry some meaningful the-
matic information. The title word feature has
been used as a binary feature during CRF
based machine learning.
</bodyText>
<subsectionHeader confidence="0.568371">
5.6.3 First Paragraph Words
</subsectionHeader>
<bodyText confidence="0.999980625">
People usually give a brief idea of their beliefs
and speculations about any related topic or
theme in the first paragraph of the document
and subsequently elaborate or support their
ideas with relevant reasoning or factual infor-
mation. Hence first paragraph words are in-
formative in the detection of Thematic Expres-
sions.
</bodyText>
<subsectionHeader confidence="0.529797">
5.6.4 Words From Last Two Sentences
</subsectionHeader>
<bodyText confidence="0.9999925">
It is a general practice of writing style that
every document concludes with a summary of
the overall story expressed in the document.
We found that it is very obvious that every
document ended with dense theme/topic words
in the last two sentences.
</bodyText>
<sectionHeader confidence="0.674315" genericHeader="method">
5.6.5 Term Distribution Model
</sectionHeader>
<bodyText confidence="0.999960448275862">
An alternative to the classical TF-IDF weight-
ing mechanism of standard IR has been pro-
posed as a model for the distribution of a word.
The model characterizes and captures the in-
formativeness of a word by measuring how
regularly the word is distributed in a document.
Thus the objective is to estimate that measures
the distribution pattern of the k occurrences of
the word wi in a document d. Zipf&apos;s law de-
scribes distribution patterns of words in an en-
tire corpus. In contrast, term distribution mod-
els capture regularities of word occurrence in
subunits of a corpus (e.g., documents, para-
graphs or chapters of a book). A good under-
standing of the distribution patterns is useful to
assess the likelihood of occurrences of a theme
word in some specific positions (e.g., first pa-
ragraph or last two sentences) of a unit of text.
Most term distribution models try to character-
ize the informativeness of a word identified by
inverse document frequency (IDF). In the
present work, the distribution pattern of a word
within a document formalizes the notion of
theme inference informativeness. This is based
on the Poisson distribution. Significant Theme
words are identified using TF, Positional and
Distribution factor. The distribution function
for each theme word in a document is eva-
luated as follows:
</bodyText>
<equation confidence="0.84857325">
n n
f =I )/n
d (wa)
1 i=1
</equation>
<bodyText confidence="0.999968571428571">
where n=number of sentences in a document
with a particular theme word Si=sentence id of
the current sentence containing the theme word
and Si-1=sentence id of the previous sentence
containing the query term, TWi is the positional
id of current Theme word and TWi−1 is the posi-
tional id of the previous Theme word.
</bodyText>
<subsectionHeader confidence="0.486276">
5.6.6 Collocation
</subsectionHeader>
<bodyText confidence="0.9785008">
Collocation with other thematic
words/expressions is undoubtedly an important
clue for identification of theme sequence pat-
terns in a document. As we used chunk level
collocation to capture thematic words (as de-
scribed in 5.5.1) and in this section we are in-
troducing collocation feature as inter-chunk
collocation or discourse level collocation with
various granularity as sentence level, para-
graph level or discourse level.
</bodyText>
<sectionHeader confidence="0.992737" genericHeader="method">
6 Theme Clustering
</sectionHeader>
<bodyText confidence="0.9998874">
Theme clustering algorithms partition a set of
documents into finite number of topic based
groups or clusters in terms of theme
words/expressions. The task of document clus-
tering is to create a reasonable set of clusters
</bodyText>
<equation confidence="0.7535">
i=
</equation>
<page confidence="0.980644">
6
</page>
<bodyText confidence="0.963465435897436">
for a given set of documents. A reasonable
cluster is defined as the one that maximizes the
within-cluster document similarity and mini-
mizes between-cluster similarities. There are
two principal motivations for the use of this
technique in the theme clustering setting: effi-
ciency, and the cluster hypothesis.
The cluster hypothesis (Jardine and van
Rijsbergen, 1971) takes this argument a step
further by asserting that retrieval from a clus-
tered collection will not only be more efficient,
but will in fact improve retrieval performance
in terms of recall and precision. The basic no-
tion behind this hypothesis is that by separat-
ing documents according to topic, relevant
documents will be found together in the same
cluster, and non-relevant documents will be
avoided since they will reside in clusters that
are not used for retrieval. Despite the plausibil-
ity of this hypothesis, there is only mixed ex-
perimental support for it. Results vary consi-
derably based on the clustering algorithm and
document collection in use (Willett, 1988). We
employ the clustering hypothesis only to
measure inter-document level thematic affinity
inference on semantics.
Application of the clustering technique to
the three sample documents results in the fol-
lowing theme-by-document matrix, A, where
the rows represent various documents and the
columns represent the themes politics, sport,
and travel.
election cricket hotel 
A parliament sachin vacation 
=    governor soccer tourist  
The similarity between vectors is calculated
by assigning numerical weights to these words
and then using the cosine similarity measure as
specified in the following equation.
</bodyText>
<equation confidence="0.993228333333333">
→→ → → N
 )
S
qk,dj - gk.dj = ∑ wi,kw
×i,j---- (1)
i=1
</equation>
<bodyText confidence="0.999945681818182">
This equation specifies what is known as the
dot product between vectors. Now, in general,
the dot product between two vectors is not par-
ticularly useful as a similarity metric, since it is
too sensitive to the absolute magnitudes of the
various dimensions. However, the dot product
between vectors that have been length norma-
lized has a useful and intuitive interpretation: it
computes the cosine of the angle between the
two vectors. When two documents are identic-
al they will receive a cosine of one; when they
are orthogonal (share no common terms) they
will receive a cosine of zero. Note that if for
some reason the vectors are not stored in a
normalized form, then the normalization can
be incorporated directly into the similarity
measure as follows.
Of course, in situations where the document
collection is relatively static, it makes sense to
normalize the document vectors once and store
them, rather than include the normalization in
the similarity metric.
</bodyText>
<equation confidence="0.990850666666667">
N
w ×w
i=1 i, k i j
,
N 2 × N 2
∑i=1 w i,k ∑ i=1 wi,k
</equation>
<bodyText confidence="0.923349142857143">
Calculating the similarity measure and using
a predefined threshold value, documents are
classified using standard bottom-up soft clus-
tering k-means technique. The predefined thre-
shold value is experimentally set as 0.5 as
shown in Table 4.
→
</bodyText>
<tableCaption confidence="0.942248">
Table 4: Five cluster centroids (mean µj ).
</tableCaption>
<bodyText confidence="0.928826857142857">
A set of initial cluster centers is necessary in
the beginning. Each document is assigned to
the cluster whose center is closest to the doc-
ument. After all documents have been as-
signed, the center of each cluster is recom-
→ →
puted as the centroid or mean µ
</bodyText>
<figure confidence="0.993788">
----(2)
ID
Theme
1
2
3
(adm1
�stion)
intra
0.63
0.12
0.04
1
Tm~i�government)
(good1
0.58
0.11
0.06
~���
(society)
0.58
0.12
0.03
1
���
(law)
0.55
0.14
0.08
2
������
(research)
0.11
0.59
0.02
2
~���
(college)
0.15
0.55
0.01
2
WIlm
(higher study)
0.12
0.66
0.01
3)
Mug
(jehadi
0.13
0.05
0.58
3
�~���
(mosque)
0.05
0.01
0.86
3
������
(New Delhi)
0.12
0.04
0.65
3
wm���
(Kashmir)
0.03
0.01
0.93
is
(where µ
</figure>
<page confidence="0.736406">
7
</page>
<figure confidence="0.9088702">
the clustering coefficient) of its members that
isµ=( 1 / cj )∑
→ →
x c
E j
</figure>
<bodyText confidence="0.996223772727273">
is the cosine vector similarity function.
Table 4 gives an example of theme centroids
by the K-means clustering. Bold words in
Theme column are cluster centers. Cluster cen-
ters are assigned by maximum clustering coef-
ficient. For each theme word, the cluster from
Table 4 is still the dominating cluster. For ex-
ample, “391&apos;R W” has a higher membership
probability in cluster1 than in other clusters.
But each theme word also has some non-zero
membership in all other clusters. This is useful
for assessing the strength of association be-
tween a theme word and a topic. Comparing
two members of the cluster2, “~ ~~:q” and
“e����IM”, it is seen that “OR7MIM” is strongly
associated with cluster2 (p=0.65) but it has
some affinity with other clusters as well (e.g.,
p =0.12 with the cluster1). This is a good ex-
ample of the utility of soft clustering. These
non-zero values are still useful for calculating
vertex weight during Semantic Relational
Graph generation.
</bodyText>
<sectionHeader confidence="0.887792" genericHeader="method">
7 Semantic Relational Graph
</sectionHeader>
<bodyText confidence="0.9761228">
Representation of input text document(s) in the
form of graph is the key to our design prin-
ciple. The idea is to build a document graph
G=&lt;V,E&gt; from a given source document
d E D . At this preprocessing stage, text is
tokenized, stop words are eliminated, and
words are stemmed. Thus, the text in each
document is split into fragments and each
fragment is represented with a vector of consti-
tuent theme words. These text fragments be-
come the nodes V in the document graph.
The similarity between two nodes is ex-
pressed as the weight of each edge E of the
document graph. A weighted edge is added to
the document graph between two nodes if they
either correspond to adjacent text fragments in
the text or are semantically related by theme
words. The weight of an edge denotes the de-
gree of the semantic inference relationship.
The weighted edges not only denote document
level similarity between nodes but also inter
document level similarity between nodes. Thus
to build a document graph G, only the edges
with edge weight greater than some predefined
threshold value are added to G, which basical-
ly constitute edges E of the graph G.
The Cosine similarity measure has been
used here. In cosine similarity, each document
→
d is denoted by the vector V (d) derived from
d, with each component in the vector for each
Theme words. The cosine similarity between
two documents (nodes) d1 and d2 is computed
→
using their vector representations V(d1) and
→
V(d2) as equation (1) and (2) (Described in
Section 6). Only a slight change has been done
i.e. the dot product of two vec-
tors V (d1) V (d2)
</bodyText>
<listItem confidence="0.656847">
• is defined as
</listItem>
<bodyText confidence="0.984961">
The Euclidean length of d is defined to
documents in the corpus. Theme nodes within
a cluster are connected by vertex, weight is
calculated by clustering co-efficient of those
theme nodes. Additionally inter cluster vertex-
es are there. Cluster centers are interconnected
with weighted vertex. The weight is calculated
by cluster distance as measured by cosine simi-
larity measure as discussed earlier.
To better aid our understanding of the auto-
matically determined category relationships we
visualized this network using the Fruchterman-
Reingold force directed graph layout algorithm
(Fruchterman and Reingold, 1991) and the
NodeXL network analysis tool (Smith et al.,
2009)4. A theme relational model graph drawn
by NoddeXL is shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.978228" genericHeader="method">
8 Semantic Distance Measurement
</sectionHeader>
<bodyText confidence="0.999592444444445">
Finally generated semantic relational graph is
the desired SemanticNet that we proposed.
Generated Bengali SemanticNet consist of al-
most 90K high frequent Bengali lexicons. Only
four categories of POS (noun, adjective, ad-
verb and verb) considered for present genera-
tion as reported in Section 5.2.1. In the gener-
ated Bengali SemanticNet all the lexicons are
connected with weighted vertex either directly
</bodyText>
<figure confidence="0.765489083333333">
4 Available from
http://www.codeplex.com/NodeXL
M 2
∑V (d) where M is the total number of r
be
r
=
x. The distance function
M
∑V d V d
( 1) ( 2) .
r=1
</figure>
<page confidence="0.853744">
8
</page>
<figureCaption confidence="0.999792">
Figure 1: Semantic Relational Graph by NodeXL
</figureCaption>
<bodyText confidence="0.85343675">
or indirectly. Semantic lexicon inference could
be identified by network distance of any two
nodes by calculating the distance in terms of
weighted vertex. We computed the relevance
of semantic lexicon nodes by summing up the
edge scores of those edges connecting the node
with other nodes in the same cluster. As cluster
centers are also interconnected with weighted
vertex so inter-cluster relations could be also
calculated in terms of weighted network dis-
tance between two nodes within two separate
clusters. As an example:
</bodyText>
<figureCaption confidence="0.757322">
Figure 2: Semantic Affinity Graph
The lexicon semantic affinity inference from
Figure 2 could be calculated as follows:
</figureCaption>
<bodyText confidence="0.997487272727273">
where Sd (wi, wj) = semantic affinity dis-
tance between two lexicons wi and wj. Equa-
tion (1) and (2) are for intra-cluster and inter-
cluster semantic distance measure respectively.
k=number of weighted vertex between two
lexicons wi and wj. vk is the weighted vertex
between two lexicons. m=number of cluster
centers between two lexicons. lc is the distance
between cluster centers between two lexicons.
For illustration of present technique let take
an example:
</bodyText>
<equation confidence="0.688698">
+
(Argentina, goal)= 0.5 0.3 0.4
=
2
� 22
(Gun, goal)= 0.22 + 1 0.5 ×0.0 �
</equation>
<bodyText confidence="0.999667148148148">
It is evident from the previous example that
the score based semantic distance can better
illustrate lexicon affinity between Argentina
and goal but is no lexicon affinity relation be-
tween gun and goal.
Instead of giving only certain semantic rela-
tions like WordNet or ConceptNet the present
relative probabilistic score based lexicon affin-
ity distance based technique can represent best
acceptable solution for representing the human
pragmatic knowledge. Not only ideologically
rather the SemanticNet provide a good solution
to any type of NLP problem. A detail analysis
of Information retrieval system using Seman-
ticNet is detailed in evaluation section.
Although every lexicon pair cannot be la-
beled by exact semantic role but we try to keep
a few semantic roles to establish a crossroad
from previous computational lexicon tech-
niques to this new one. These semantic rela-
tions may be treated as a bridge to traverse
SemanticNet by gathering knowledge from
other resources WordNet and ConceptNet.
Approximately 22k (24% of overall Seman-
ticNet) lexicons are tagged with appropriate
semantic roles by two processes as described
below.
</bodyText>
<equation confidence="0.524937">
Sd ( , ) ----(1) or
w w =
i j k
E
n
k k
= 0
v
n
m k=0 vk × ∏ lc ---(2)
c
=0
= E
c =0 k
E
m
=0
</equation>
<page confidence="0.667526">
9
</page>
<sectionHeader confidence="0.544715" genericHeader="method">
10 Evaluation
</sectionHeader>
<bodyText confidence="0.998812806451613">
It is bit difficult to evaluate this type of lexicon
resources automatically. Manual validation
may be suggested as a better alternative but we
prefer for a practical implementation based
evaluation strategy.
For evaluation of Bengali SemanticNet it is
used in Information Retrieval task using cor-
pus from Forum for Information Retrieval
Evaluation (FIRE) 7 ad-hoc mono-lingual in-
formation retrieval task for Bengali language.
Two different strategies have been taken. First
a standard IR technique with TF-IDF, zonal
indexing and ranking based technique (Ban-
dyopadhyay et al., 2008) has been taken.
Second technique uses more or less same strat-
egy along with query expansion technique us-
ing SemanticNet (Although the term Seman-
ticNet was not mentioned there) as a resource
(Bhaskar et al., 2010).
Only the following evaluation metrics have
been listed for each run: mean average preci-
sion (MAP), Geometric Mean Average Preci-
sion (GM-AP), (document retrieved relevant
for the topic) R-Precision (R-Prec), Binary pre-
ferences (Bpref) and Reciprical rank of top
relevant document (Recip_Rank). The evalua-
tion strategy follows the global standard as
Text Retrieval Conference (TREC)8 metrics. It
is clearly evident from the system results as
reported in Table 6 that SemanticNet is a better
way to solve lexicon semantic affinity.
</bodyText>
<table confidence="0.999054">
Scores Bengali IR using
IR SemanticNet
MAP 0.0200 0.4002
GM_AP 0.0004 0.3185
R-Prec 0.0415 0.3894
Bpref 0.0583 0.3424
Recip_Rank 0.4432 0.6912
</table>
<tableCaption confidence="0.980448">
Table 6: Information Retrieval using Seman-
</tableCaption>
<bodyText confidence="0.7432">
ticNet
Evaluation result shows effectiveness of de-
veloped SemanticNet in IR. Further analysis
</bodyText>
<sectionHeader confidence="0.927554" genericHeader="method">
9 Semantic Role Assignment
</sectionHeader>
<bodyText confidence="0.9999625">
Two types of methods have been taken to as-
sign pair wise lexicon semantic affinity rela-
tions. First one is derived from ConceptNet. In
the second technique sub-graph is identified
consisting of a nearest verb and roles are as-
signed accordingly.
</bodyText>
<subsectionHeader confidence="0.996939">
9.1 Semantic Roles from ConceptNet
</subsectionHeader>
<bodyText confidence="0.9982559">
A ConceptNet API5 written in Java has been
used to extract pair wise relation from Con-
ceptNet. A Bengali-English dictionary (ap-
proximately 102119 entries) has been devel-
oped using the Samsad Bengali-English dictio-
nary6 used here for equivalent lookup of Eng-
lish meaning of each Bengali lexicon. Ob-
tained semantic relations from ConceptNet for
any lexicon English pair are assigned to source
Bengali pair lexicons. As an example:
</bodyText>
<figure confidence="0.4255696">
(“Tree”,”Gree”) (“5TT!”,”�)
OftenNear
PartOf
PropertyOf
IsA
</figure>
<subsectionHeader confidence="0.987068">
9.2 Verb Sub-Graph Identification
</subsectionHeader>
<bodyText confidence="0.981966833333333">
It is an automatic process using manually
augmented list of only 220 Bengali verbs. This
process starts from any arbitrary node of any
cluster and start finding any nearest verb with-
in the cluster. The system uses the manually
augmented list of verbs as partly reported in
</bodyText>
<tableCaption confidence="0.669213">
Table 5.
</tableCaption>
<table confidence="0.998817333333333">
Verb English Gloss Probable Relations
77 Be IsA
;3W Have CapableOf
&amp;quot;mg Have CapableOf
#wft Made MadeOf
:q i Live LocationOf
</table>
<tableCaption confidence="0.999194">
Table 5: Semantic Relations
</tableCaption>
<bodyText confidence="0.99970225">
The semantic relation labels attached with
every verb in the manually augmented list (as
reported in Table 5) is then automatically as-
signed between each pair of lexicons.
</bodyText>
<figure confidence="0.4335565">
5 http://web.media.mit.edu/~hugo/conceptnet/
6
</figure>
<footnote confidence="0.986809">
http://dsal.uchicago.edu/dictionaries/biswas_bengal 7 http://www.isical.ac.in/~clia/index.html
i/ 8 http://trec.nist.gov/
</footnote>
<page confidence="0.998364">
10
</page>
<bodyText confidence="0.9999598">
revealed that general query expansion tech-
nique generally used WordNet synonyms as a
resource. But in reality “TF-?T” and “&apos;fidT11”
could not be clustered in one cluster though
they represent same semantic of ‘heart’. First
one used in general context whereas the second
one used only in literature. If there is any
problem to understand Bengali let come with
an example of English. Conceptually &amp;quot;you&amp;quot;
and &amp;quot;thy&amp;quot; could be mapped in same cluster as
they both represent the semantic of 2nd person
but in reality &amp;quot;thy&amp;quot; simply refers to the
literature of the great English poet Shakes-
peare. Standard lexicons cannot discriminate
this type of fine-grained semantic differences.
</bodyText>
<sectionHeader confidence="0.986416" genericHeader="conclusions">
11 Conclusion and Future Task
</sectionHeader>
<bodyText confidence="0.999689">
Experimental result of Information Retrieval
using SemanticNet proves it is a better solution
rather than any existing lexicon resources. The
development strategy employs less human in-
terruption rather a general architecture of
Theme identification or Theme Clustering
technique using easily extractable linguistics
knowledge. The proposed technique could be
replicated for any new language.
SemanticNet could be useful any kind of In-
formation Retrieval technique, Information
Extraction technique, and topic based Summa-
rization and we hope for newly identified NLP
sub disciplines such as Stylometry or Author-
ship detection and plagiarism detection etc.
Our future task will be in the direction of
different experiments of NLP as mentioned
above to profoundly establish the efficiency of
SemanticNet. Furthermore we will try to de-
velop SemanticNet for many other languages.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999940650793651">
Bandhyopadhyay S., Das A., Bhaskar P.. English
Bengali Ad-hoc Monolingual Information Re-
trieval Task Result at FIRE 2008. In Working
Note of Forum for FIRE-2008.
Bhaskar P., Das A.,Pakray P.and Bandyopadhyay
S.(2010). Theme Based English and Bengali Ad-
hoc Monolingual Information Retrieval in FIRE
2010, In FIRE-2010.
Das A. and Bandyopadhyay S. (2009). Theme De-
tection an Exploration of Opinion Subjectivity.
In Proceeding of Affective Computing &amp; Intelli-
gent Interaction (ACII).
Das A. and Bandyopadhyay S. (2010). Morpholog-
ical Stemming Cluster Identification for Bangla,
In Knowledge Sharing Event-1: Task 3: Mor-
phological Analyzers and Generators, January,
2010, Mysore.
Ekbal A., Bandyopadhyay S (2008). A Web-based
Bengali News Corpus for Named Entity Recog-
nition. Language Resources and Evaluation
Journal. pages 173-182, 2008
Fillmore Charles J., Johnson Christopher R., and
Petruck Miriam R. L.. 2003. Background to
FrameNet. International Journal of Lexicogra-
phy, 16:235–250.
Fruchterman Thomas M. J. and Reingold Edward
M.(1991). Graph drawing by force-directed
placement. Software: Practice and Experience,
21(11):1129–1164.
Ghosh A., Das A., Bhaskar P., Bandyopadhyay
S.(2009). Dependency Parser for Bengali: the JU
System at ICON 2009. In NLP Tool Contest
ICON 2009, December 14th-17th, Hyderabad.
Jardine, N. and van Rijsbergen, C. J. (1971). The
use of hierarchic clustering in information re-
trieval. Information Storage and Retrieval, 7,
217-240.
Kipper Karin, Korhonen Anna, Ryant Neville, and
Palmer Martha. Extending VerbNet with Novel
Verb Classes. LREC 2006.
Liu Hugo and Singh Push (2004). ConceptNet: a
practical commonsense reasoning toolkit. BT
Technology Journal, 22(4):211-226.
Palmer Martha, Gildea Dan, Kingsbury Paul, The
Proposition Bank: A Corpus Annotated with
Semantic Roles, Computational Linguistics
Journal, 31:1, 2005.
Singh Push and Williams William (2003). LifeNet:
a propositional model of ordinary human activi-
ty. In the Proc. Of DC-KCAP 2003.
Singh Push, Barry Barbara, and Liu Hugo (2004).
Teaching machines about everyday life. BT
Technology Journal, 22(4):227-240.
Smith Marc, Ben Shneiderman, Natasa Milic-
Frayling, Eduarda Mendes Rodrigues, Vladimir
Barash, Cody Dunne, Tony Capone, Adam Per-
er, and Eric Gleave. 2009. Analyzing (social
media) networks with NodeXL. In C&amp;T ’09:
Proc. Fourth International Conference on Com-
munities and Technologies, LNCS. Springer.
Willerr, P. (1988). Recent trends in hierarchic doc-
ument clustering: A critical review. Information
Processing and Management, 24(5), 577-597.
</reference>
<page confidence="0.999485">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995138">SemanticNet-Perception of Human Pragmatics</title>
<author confidence="0.983135">Sivaji</author>
<affiliation confidence="0.9986225">Department of Computer Science and Engineering Jadavpur University</affiliation>
<abstract confidence="0.993990407258064">SemanticNet is a semantic network of lexicons to hold human pragmatic knowledge. So far Natural Language Processing (NLP) research patronized much of manually augmented lexicon resources such as WordNet. But the small set of semantic relations like Hypernym, Holonym, Meronym and Synonym etc are very narrow to capture the wide variations human cognitive knowledge. But no such information could be retrieved from available lexicon resources. SemanticNet is the attempt to capture wide range of context dependent semantic inference among various themes which human beings perceive in their pragmatic knowledge, learned by day to day cognitive interactions with the surrounding physical world. SemanticNet holds human pragmatics with twenty well established semantic relations for every pair of lexemes. As every pair of relations cannot be defined by fixed number of certain semantic relation labels thus additionally contextual semantic affinity inference in SemanticNet could be calculated by network distance and represented as a probabilistic score. SemanticNet is being presently developed for Bengali language. 1 Historical Motivation (from Greek semanis the study of meaning, usually in language. The word &amp;quot;semantics&amp;quot; itself denotes a range of ideas, from the popular to the highly technical. It is often used in ordinary language to denote a problem of understanding that comes down to word selection or connotation. We studied with various Psycholinguistics experiments to understand how human natural intelligence helps to understand general semantic from nature. Our study was to understand the human psychology about semantics beyond language. We were haunting for the intellectual structure of the psychological and neurobiological factors that enable humans to acquire, use, comprehend and produce natural languages. Let’s come with an example of simple conversation about movie between two persons. A: you seen the Man&apos;s Land’? is it? B: it is good but you should see Hurt Locker’? May be the conversation looks very casual, but our intension was to find out the direction of the decision logic on the Person B’s brain. We start digging to find out the nature of human intelligent thinking. A prolonged discussion with Person B reveals that the decision logic path to recommend a good movie was as the Figure 1. The highlighted red paths are the shortest semantic affinity distances of the human brain. We call it semantic thinking. Although the derivational path of semantic thinking is not such easy as we portrait in Figure 1 but we keep it easier for understandability. Actually a human try to figure out the closest semantic affinity node into his pragmatics knowledge by natural intelligence. In the previous example B find out with his intelligence that Land a war movie and got Oscar 2 of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex pages Beijing, August 2010 award. Oscar award generally cracked by Hollywood movies and thus Person B start searching his pragmatics network to find out a movie fall into war genre, from Hollywood and may be got Oscar award. Person B finds out the of a movie Hurt Locker nearer distance into his pragmatics knowledge network which is an optimized recommendation that satisfy all the criteria. Noticeably Person B didn’t choice the other paths like Bollywood, Foreign movie etc. Figure 1: Semantic Thinking And thus our aim was to develop a computational lexicon structure for semantics as human pragmatics knowledge. We spare long time to find out the most robust structure to represent pragmatics knowledge properly and it should be easy understandable for next level of search and usability. We look into literature that probably direct to the direction of our ideological thinking. We found that in the year of 1996 Push Singh and Marvin Minsky proposed the field has shattered into subfields populated by researchers with different goals and who speak very different technical languages. Much has been learned, and it is time to start integrating what we&apos;ve learned, but few researchers are widely versed enough to do so. They had a proposal for how to do so in their ConceptNet work. They developed lexicon resources like ConceptNet (Liu and Singh, 2004). ConceptNet- ConceptNet is a large-scale semantic network (over 1.6 million links) relating a wide variety of ordinary objects, events, places, actions, and goals by only 20 different link types, mined from corpus. The present task of developing SemanticNet is to capture semantic affinity knowledge of human pragmatics as a lexicon database. We extend our vision from the human common sense (as in ConceptNet) to human pragmatics and have proposed semantic relations for every pair of lexemes that cannot be defined by fixed number of certain semantic relation labels. Contextual semantic affinity inference in SemanticNet could be calculated by network distance and represented as a probabilistic score. SemanticNet is being presently developed for Bengali language. 2 Semantic Roles The ideological study of semantic roles started old ago since Panini’s that assigns generic semantic roles to words in a natural language sentence. Semantic roles are generally domain specific in nature such as FROM_DESTINATION,TO_DESTINATION, DEPARTURE_TIME etc. Verb-specific semantic roles have also been defined such as EATER and EATEN for the verb eat. The standard datasets that are used in various English SRL systems are: PropBank (Palmer et al., 2005), FrameNet (Fillmore et al., 2003) and VerbNet (Kipper et al., 2006). These collections contain manually developed well-trusted gold reference annotations of both syntactic and predicate-argument structures. PropBank defines semantic roles for each verb. The various semantic roles identified (Dowty, 1991) are Agent, patient or theme etc. In addition to verb-specific roles, PropBank defines several more general roles that can apply to any verb (Palmer et al., 2005). FrameNet is annotated with verb frame semantics and supported by corpus evidence. The frame-to-frame relations defined in FrameNet are Inheritance, Perspective_on, Subframe, Precedes, Inchoative_of, Causative_of and Using. Frame development focuses on paraphrasability (or near paraphrasability) of words and multi-words. VerbNet annotated with thematic roles refer to the underlying semantic relationship between a predicate and its arguments. The semantic tagset of VerbNet consists of tags as agent, patient, theme, experiencer, stimulus, instrument, location, source, goal, recipient, benefactive etc. It is evident from the above discussions that no adequate semantic role set exists that can be across various domains. Hence pro- 3 Fortunately such corpus development could be found in (Ekbal and Bandyopadhyay, 2008) for Bengali. We obtained the corpus from the authors. The Bengali NEWS corpus consisted of consecutive 4 years of NEWS stories with various sub domains as reported above. For the present task we have used the Bengali NEWS corpus, developed from the archive of a lead- Bengali NEWS available on the Web. The NEWS corpus is quite larger in size as reported in Table 1. 4 Annotation From the collected document set 200 documents have been chosen randomly for the annotation task. Three annotators (Mr. X, Mr. Y and Mr. Z) participated in the present task. Annotators were asked to annotate the theme words (topical expressions) which best describe the topical snapshot of the document. The agreement of annotations among three annotators has been evaluated. The agreements of tag values at theme words level is reported in Table 2. Annotators X vs. Y X Vs. Z Y Vs. Z Avg Percentage 82.64% 71.78% 80.47% 78.3% All Agree 75.45% Table 2: Agreement of annotators at theme words level 5 Theme Identification Term Frequency (TF) plays a crucial role to identify document relevance in Topic-Based Information Retrieval. The motivation behind developing Theme detection technique is that in many documents relevant words may not occur frequently or irrelevant words may occur frequently. Moreover for the lexicon affinity inference, topic or theme words are the only strong clue to start with. The Theme detection technique has been proposed to resolve these issues to identify discourse level most relevant thematic nodes in terms of word or lexicon using a standard machine learning technique. The machine learning technique used here is Random Field The theme word detection has been defined as a sequence posed SemanticNet does not only rely on fixed type of semantics roles as ConceptNet. For semantic relations we followed the 20 relations defined in ConceptNet. Additionally we proposed semantic relations for every pair of lexicons cannot be defined by exact semantic role and thus we formulated a probabilistic score based technique. Semantic affinity in SemanticNet could be calculated by network distance. Details could be found in relevant Section 8. 3 Corpus Present SemanticNet has been developed for Bengali language. Resource acquisition is one of the most challenging obstacles to work with electronically resource constrained languages Bengali. Although Bengali is the popular language in the World, second in India and the national language in Bangladesh. There was another issue drive us long way to find out the proper corpus for the development of SemanticNet. As the notion is to capture and store human pragmatic knowledge so the hypothesis was chosen corpus should not be biased towards any specific domain knowledge as human pragmatic knowledge is not constricted to any domain rather it has a wide spread range over anything related to universe and life on earth. Additionally it must be larger in size to cover mostly available general concepts related to any topic. After a detail analysis we decided it is better to choose NEWS</abstract>
<note confidence="0.772214588235294">corpus as various domains knowledge like Politics, Sports, Entertainment, Social Issues, Science, Arts and Culture, Tourism, Advertisement, TV schedule, Tender, Comics and Weather etc are could be found only in NEWS corpus. Statistics NEWS Total no. of news documents in the corpus 108,305 Total no. of sentences in the corpus 2,822,737 Avg no. of sentences in a document 27 Total no. of wordforms in the corpus 33,836,736 Avg. no. of wordforms in a document 313 Total no. of distinct wordforms in the corpus 467,858 Table 1: Bengali Corpus Statistics 1 2http://www.anandabazar.com/ 3http://crfpp.sourceforge.net</note>
<abstract confidence="0.981784909090909">4 labeling problem using various useful depending features. Depending upon the series of input features, each word is tagged as either Theme Word (TW) or Other (O). 5.1 Feature Organization The set of features used in the present task have been categorized as Lexico-Syntactic, Syntactic and Discourse level features. These are listed in the Table 3 below and have been described in the subsequent subsections.</abstract>
<title confidence="0.9906295">Types Features Lexico-Syntactic POS Frequency Stemming Syntactic Chunk Label Dependency Parsing Depth Discourse Level Title of the Document First Paragraph Term Distribution Collocation</title>
<note confidence="0.409282333333333">Table 3: Features 5.2 Lexico-Syntactic Features 5.2.1 Part of Speech (POS)</note>
<abstract confidence="0.971453455573505">It has been shown by Das and Bandyopadhyay, (2009), that theme bearing words in sentences are mainly adjective, adverb, noun and verbs as other POS categories like pronoun, preposition, conjunct, article etc. have no relevance towards thematic semantic of any document. The detail of the POS tagging system chosen for the present task could be found in (Das and Bandyopadhyay 2009). 5.3 Frequency Frequency always plays a crucial role in identifying the importance of a word in the document or corpus. The system generates four separate high frequent word lists after function words are removed for four POS categories: adjective, adverb, verb and noun. Word frequency values are then effectively used as a crucial feature in the Theme Detection technique. 5.4 Stemming Several words in a sentence that carry thematic information may be present in inflected forms. Stemming is necessary for such inflected words before they can be searched in approprilists. Due to non availability of good stemmers in Indian languages especially in Bengali, a stemmer based on stemming cluster technique has been used as described in (Das and Bandyopadhyay, 2010). This stemmer analyzes prefixes and suffixes of all the word forms present in a particular document. Words that are identified to have the same root form are grouped in a finite number of clusters with the identified root word as cluster center. 5.5 Syntactic Features 5.5.1 Chunk Label We found that Chunk level information is very much effective to identify lexicon inference affinity. As an example: released Satyajit Roy are excellent. In the above example two lexicons collocated in a chunk and they are very much semantically neighboring in human pragmatic knowledge. Chunk feature effectively used in supervised classifier. Chunk labels are defined as B-X (Beginning), I-X (Intermediate) and E- X (End), where X is the chunk label. In the task of identification of Theme expressions, chunk label markers play a crucial role. Further details of development of chunking system could be found in (Das and Bandyopadhyay 2009). 5.5.2 Dependency Parser Dependency depth feature is very useful to identify Theme expressions. A particular Theme word generally occurs within a particular range of depth in a dependency tree. Theme expressions may be a Named Entity (NE: person, organization or location names), a common noun (Ex: accident, bomb blast, strike etc) or words of other POS categories. It has been observed that depending upon the nature of Theme expressions it can occur within a certain depth in the dependency tree in the sentences. A statistical dependency parser has 5 been used for Bengali as described in (Ghosh et al., 2009). 5.6 Discourse Level Features 5.6.1 Positional Aspect Depending upon the position of the thematic clue, every document is divided into a number of zones. The features considered for each document are Title words of the document, the first paragraph words and the words from the last two sentences. A detailed study was done on the Bengali news corpus to identify the roles of the positional aspect features of a document (first paragraph, last two sentences) in the detection of theme words. The importance of these positional features has been described in the following section. 5.6.2 Title Words It has been observed that the Title words of a document always carry some meaningful thematic information. The title word feature has been used as a binary feature during CRF based machine learning. 5.6.3 First Paragraph Words People usually give a brief idea of their beliefs and speculations about any related topic or theme in the first paragraph of the document and subsequently elaborate or support their ideas with relevant reasoning or factual information. Hence first paragraph words are informative in the detection of Thematic Expressions. 5.6.4 Words From Last Two Sentences It is a general practice of writing style that every document concludes with a summary of the overall story expressed in the document. We found that it is very obvious that every document ended with dense theme/topic words in the last two sentences. 5.6.5 Term Distribution Model An alternative to the classical TF-IDF weighting mechanism of standard IR has been proposed as a model for the distribution of a word. The model characterizes and captures the informativeness of a word by measuring how regularly the word is distributed in a document. Thus the objective is to estimate that measures distribution pattern of the of word a document Zipf&apos;s law describes distribution patterns of words in an entire corpus. In contrast, term distribution models capture regularities of word occurrence in subunits of a corpus (e.g., documents, paragraphs or chapters of a book). A good understanding of the distribution patterns is useful to assess the likelihood of occurrences of a theme word in some specific positions (e.g., first paragraph or last two sentences) of a unit of text. Most term distribution models try to characterize the informativeness of a word identified by inverse document frequency (IDF). In the present work, the distribution pattern of a word within a document formalizes the notion of theme inference informativeness. This is based on the Poisson distribution. Significant Theme words are identified using TF, Positional and Distribution factor. The distribution function for each theme word in a document is evaluated as follows: n n 1 where n=number of sentences in a document a particular theme word id of the current sentence containing the theme word id of the previous sentence the query term, is the positional of current Theme word and is the positional id of the previous Theme word. 5.6.6 Collocation Collocation with other thematic words/expressions is undoubtedly an important clue for identification of theme sequence patterns in a document. As we used chunk level collocation to capture thematic words (as described in 5.5.1) and in this section we are introducing collocation feature as inter-chunk collocation or discourse level collocation with various granularity as sentence level, paragraph level or discourse level. 6 Theme Clustering Theme clustering algorithms partition a set of documents into finite number of topic based groups or clusters in terms of theme words/expressions. The task of document clustering is to create a reasonable set of clusters 6 for a given set of documents. A reasonable cluster is defined as the one that maximizes the within-cluster document similarity and minimizes between-cluster similarities. There are two principal motivations for the use of this technique in the theme clustering setting: effiand the hypothesis and van Rijsbergen, 1971) takes this argument a step further by asserting that retrieval from a clustered collection will not only be more efficient, but will in fact improve retrieval performance in terms of recall and precision. The basic notion behind this hypothesis is that by separating documents according to topic, relevant documents will be found together in the same cluster, and non-relevant documents will be avoided since they will reside in clusters that are not used for retrieval. Despite the plausibility of this hypothesis, there is only mixed experimental support for it. Results vary considerably based on the clustering algorithm and document collection in use (Willett, 1988). We the only to measure inter-document level thematic affinity inference on semantics. Application of the clustering technique to the three sample documents results in the following theme-by-document matrix, A, where the rows represent various documents and the columns represent the themes politics, sport, and travel. cricket hotel parliament sachin vacation  soccer tourist  The similarity between vectors is calculated by assigning numerical weights to these words and then using the cosine similarity measure as specified in the following equation. → →  ) S (1) This equation specifies what is known as the dot product between vectors. Now, in general, the dot product between two vectors is not particularly useful as a similarity metric, since it is too sensitive to the absolute magnitudes of the various dimensions. However, the dot product between vectors that have been length normalized has a useful and intuitive interpretation: it the the angle between the two vectors. When two documents are identical they will receive a cosine of one; when they are orthogonal (share no common terms) they will receive a cosine of zero. Note that if for some reason the vectors are not stored in a normalized form, then the normalization can be incorporated directly into the similarity measure as follows. Of course, in situations where the document collection is relatively static, it makes sense to normalize the document vectors once and store them, rather than include the normalization in the similarity metric. N i j , N Calculating the similarity measure and using a predefined threshold value, documents are classified using standard bottom-up soft clus- The predefined threshold value is experimentally set as 0.5 as shown in Table 4. 4: Five cluster centroids (mean A set of initial cluster centers is necessary in the beginning. Each document is assigned to the cluster whose center is closest to the document. After all documents have been asthe center of each cluster is recom- → → as the centroid or mean ----(2) ID Theme 1 2 3 (adm1 �stion) intra 0.63 0.12 0.04 1 Tm~i�government) (good1 0.58 0.11 0.06 ~��� (society) 0.58 0.12 0.03 1 ��� (law) 0.55 0.14 0.08 2 ������ (research) 0.11 0.59 0.02 2 ~��� (college) 0.15 0.55 0.01 2 WIlm (higher study) 0.12 0.66 0.01 3) Mug (jehadi 0.13 0.05 0.58 3 �~��� (mosque) 0.05 0.01 0.86 3 ������ (New Delhi) 0.12 0.04 0.65 3 wm��� (Kashmir) 0.03 0.01 0.93 is 7 the clustering coefficient) of its members that / → → x c the vector function. Table 4 gives an example of theme centroids the Bold words in Theme column are cluster centers. Cluster centers are assigned by maximum clustering coefficient. For each theme word, the cluster from Table 4 is still the dominating cluster. For ex- W” a higher membership probability in cluster1 than in other clusters. But each theme word also has some non-zero membership in all other clusters. This is useful for assessing the strength of association between a theme word and a topic. Comparing members of the cluster2, ~~:q” is seen that strongly associated with cluster2 (p=0.65) but it has some affinity with other clusters as well (e.g., p =0.12 with the cluster1). This is a good example of the utility of soft clustering. These non-zero values are still useful for calculating vertex weight during Semantic Relational Graph generation. 7 Semantic Relational Graph Representation of input text document(s) in the form of graph is the key to our design principle. The idea is to build a document graph a given source document At this preprocessing stage, text is tokenized, stop words are eliminated, and words are stemmed. Thus, the text in each document is split into fragments and each fragment is represented with a vector of constituent theme words. These text fragments become the nodes V in the document graph. The similarity between two nodes is exas the weight of each edge the document graph. A weighted edge is added to the document graph between two nodes if they either correspond to adjacent text fragments in the text or are semantically related by theme words. The weight of an edge denotes the degree of the semantic inference relationship. The weighted edges not only denote document level similarity between nodes but also inter document level similarity between nodes. Thus build a document graph only the edges with edge weight greater than some predefined value are added to which basicalconstitute edges the graph The Cosine similarity measure has been used here. In cosine similarity, each document → denoted by the vector from with each component in the vector for each Theme words. The cosine similarity between documents (nodes) computed → their vector representations → equation (1) and (2) (Described in Section 6). Only a slight change has been done i.e. the dot product of two vec- • is defined as Euclidean length of defined to documents in the corpus. Theme nodes within a cluster are connected by vertex, weight is calculated by clustering co-efficient of those theme nodes. Additionally inter cluster vertexes are there. Cluster centers are interconnected with weighted vertex. The weight is calculated by cluster distance as measured by cosine similarity measure as discussed earlier. To better aid our understanding of the automatically determined category relationships we visualized this network using the Fruchterman- Reingold force directed graph layout algorithm (Fruchterman and Reingold, 1991) and the NodeXL network analysis tool (Smith et al., A theme relational model graph drawn by NoddeXL is shown in Figure 1. 8 Semantic Distance Measurement Finally generated semantic relational graph is the desired SemanticNet that we proposed. Generated Bengali SemanticNet consist of almost 90K high frequent Bengali lexicons. Only four categories of POS (noun, adjective, adverb and verb) considered for present generation as reported in Section 5.2.1. In the generated Bengali SemanticNet all the lexicons are connected with weighted vertex either directly 4Available from http://www.codeplex.com/NodeXL the total number of be r = The distance function M d V d 1) ( 2) 8 Figure 1: Semantic Relational Graph by NodeXL or indirectly. Semantic lexicon inference could be identified by network distance of any two nodes by calculating the distance in terms of weighted vertex. We computed the relevance of semantic lexicon nodes by summing up the edge scores of those edges connecting the node with other nodes in the same cluster. As cluster centers are also interconnected with weighted vertex so inter-cluster relations could be also calculated in terms of weighted network distance between two nodes within two separate clusters. As an example: Figure 2: Semantic Affinity Graph The lexicon semantic affinity inference from Figure 2 could be calculated as follows: affinity disbetween two lexicons Equation (1) and (2) are for intra-cluster and intercluster semantic distance measure respectively. of weighted vertex between two the weighted vertex two lexicons. of cluster between two lexicons. is the distance between cluster centers between two lexicons. For illustration of present technique let take an example: + goal)= 0.3 0.4 2 goal)= 1 0.5 It is evident from the previous example that the score based semantic distance can better lexicon affinity between is no lexicon affinity relation be- Instead of giving only certain semantic relations like WordNet or ConceptNet the present relative probabilistic score based lexicon affinity distance based technique can represent best acceptable solution for representing the human pragmatic knowledge. Not only ideologically rather the SemanticNet provide a good solution to any type of NLP problem. A detail analysis of Information retrieval system using SemanticNet is detailed in evaluation section. Although every lexicon pair cannot be labeled by exact semantic role but we try to keep a few semantic roles to establish a crossroad from previous computational lexicon techniques to this new one. These semantic relations may be treated as a bridge to traverse SemanticNet by gathering knowledge from other resources WordNet and ConceptNet. Approximately 22k (24% of overall SemanticNet) lexicons are tagged with appropriate semantic roles by two processes as described below. , ) ----(1) or w jk E n k k v n ---(2) c E m =0 9 10 Evaluation It is bit difficult to evaluate this type of lexicon resources automatically. Manual validation may be suggested as a better alternative but we prefer for a practical implementation based evaluation strategy. For evaluation of Bengali SemanticNet it is used in Information Retrieval task using corpus from Forum for Information Retrieval (FIRE) 7ad-hoc mono-lingual information retrieval task for Bengali language. Two different strategies have been taken. First a standard IR technique with TF-IDF, zonal indexing and ranking based technique (Bandyopadhyay et al., 2008) has been taken. Second technique uses more or less same strategy along with query expansion technique using SemanticNet (Although the term SemanticNet was not mentioned there) as a resource (Bhaskar et al., 2010). Only the following evaluation metrics have been listed for each run: mean average precision (MAP), Geometric Mean Average Precision (GM-AP), (document retrieved relevant for the topic) R-Precision (R-Prec), Binary preferences (Bpref) and Reciprical rank of top relevant document (Recip_Rank). The evaluation strategy follows the global standard as Retrieval Conference metrics. It is clearly evident from the system results as reported in Table 6 that SemanticNet is a better way to solve lexicon semantic affinity. Scores Bengali IR using IR SemanticNet MAP 0.0200 0.4002 GM_AP 0.0004 0.3185 R-Prec 0.0415 0.3894 Bpref 0.0583 0.3424 Recip_Rank 0.4432 0.6912 6: Information Retrieval using SemanticNet Evaluation result shows effectiveness of developed SemanticNet in IR. Further analysis 9 Semantic Role Assignment Two types of methods have been taken to assign pair wise lexicon semantic affinity relations. First one is derived from ConceptNet. In the second technique sub-graph is identified of a nearest roles are assigned accordingly. 9.1 Semantic Roles from ConceptNet ConceptNet written in Java has been used to extract pair wise relation from ConceptNet. A Bengali-English dictionary (approximately 102119 entries) has been developed using the Samsad Bengali-English dictioused here for equivalent lookup of English meaning of each Bengali lexicon. Obtained semantic relations from ConceptNet for any lexicon English pair are assigned to source Bengali pair lexicons. As an example: OftenNear PartOf PropertyOf IsA 9.2 Verb Sub-Graph Identification It is an automatic process using manually augmented list of only 220 Bengali verbs. This process starts from any arbitrary node of any cluster and start finding any nearest verb within the cluster. The system uses the manually augmented list of verbs as partly reported in Table 5. Verb English Gloss Probable Relations 77 Be IsA ;3W Have CapableOf &amp;quot;mg Have CapableOf :q i Live LocationOf Table 5: Semantic Relations The semantic relation labels attached with every verb in the manually augmented list (as reported in Table 5) is then automatically assigned between each pair of lexicons. 5http://web.media.mit.edu/~hugo/conceptnet/ 6 7http://www.isical.ac.in/~clia/index.html 8http://trec.nist.gov/ 10 revealed that general query expansion technique generally used WordNet synonyms as a But in reality could not be clustered in one cluster though they represent same semantic of ‘heart’. First one used in general context whereas the second one used only in literature. If there is any problem to understand Bengali let come with an example of English. Conceptually &amp;quot;you&amp;quot; and &amp;quot;thy&amp;quot; could be mapped in same cluster as both represent the semantic of person but in reality &amp;quot;thy&amp;quot; simply refers to the literature of the great English poet Shakespeare. Standard lexicons cannot discriminate this type of fine-grained semantic differences. 11 Conclusion and Future Task Experimental result of Information Retrieval using SemanticNet proves it is a better solution rather than any existing lexicon resources. The development strategy employs less human interruption rather a general architecture of Theme identification or Theme Clustering technique using easily extractable linguistics knowledge. The proposed technique could be replicated for any new language. SemanticNet could be useful any kind of Information Retrieval technique, Information Extraction technique, and topic based Summarization and we hope for newly identified NLP sub disciplines such as Stylometry or Authorship detection and plagiarism detection etc. Our future task will be in the direction of different experiments of NLP as mentioned above to profoundly establish the efficiency of SemanticNet. Furthermore we will try to develop SemanticNet for many other languages.</abstract>
<note confidence="0.8281051875">References Bandhyopadhyay S., Das A., Bhaskar P.. English Bengali Ad-hoc Monolingual Information Retrieval Task Result at FIRE 2008. In Working Note of Forum for FIRE-2008. Bhaskar P., Das A.,Pakray P.and Bandyopadhyay S.(2010). Theme Based English and Bengali Adhoc Monolingual Information Retrieval in FIRE 2010, In FIRE-2010. Das A. and Bandyopadhyay S. (2009). Theme Detection an Exploration of Opinion Subjectivity. In Proceeding of Affective Computing &amp; Intelligent Interaction (ACII). Das A. and Bandyopadhyay S. (2010). Morphological Stemming Cluster Identification for Bangla, In Knowledge Sharing Event-1: Task 3: Morphological Analyzers and Generators, January, 2010, Mysore. Ekbal A., Bandyopadhyay S (2008). A Web-based Bengali News Corpus for Named Entity Recognition. Language Resources and Evaluation Journal. pages 173-182, 2008 Fillmore Charles J., Johnson Christopher R., and Petruck Miriam R. L.. 2003. Background to FrameNet. International Journal of Lexicography, 16:235–250. Fruchterman Thomas M. J. and Reingold Edward M.(1991). Graph drawing by force-directed placement. Software: Practice and Experience, 21(11):1129–1164. Ghosh A., Das A., Bhaskar P., Bandyopadhyay S.(2009). Dependency Parser for Bengali: the JU System at ICON 2009. In NLP Tool Contest ICON 2009, December 14th-17th, Hyderabad. Jardine, N. and van Rijsbergen, C. J. (1971). The use of hierarchic clustering in information retrieval. Information Storage and Retrieval, 7, 217-240. Kipper Karin, Korhonen Anna, Ryant Neville, and Palmer Martha. Extending VerbNet with Novel Verb Classes. LREC 2006. Liu Hugo and Singh Push (2004). ConceptNet: a practical commonsense reasoning toolkit. BT Technology Journal, 22(4):211-226. Palmer Martha, Gildea Dan, Kingsbury Paul, The Proposition Bank: A Corpus Annotated with Semantic Roles, Computational Linguistics Journal, 31:1, 2005. Singh Push and Williams William (2003). LifeNet: a propositional model of ordinary human activity. In the Proc. Of DC-KCAP 2003. Singh Push, Barry Barbara, and Liu Hugo (2004). Teaching machines about everyday life. BT Technology Journal, 22(4):227-240. Smith Marc, Ben Shneiderman, Natasa Milic- Frayling, Eduarda Mendes Rodrigues, Vladimir Barash, Cody Dunne, Tony Capone, Adam Perer, and Eric Gleave. 2009. Analyzing (social media) networks with NodeXL. In C&amp;T ’09: Proc. Fourth International Conference on Communities and Technologies, LNCS. Springer. Willerr, P. (1988). Recent trends in hierarchic document clustering: A critical review. Information Processing and Management, 24(5), 577-597.</note>
<intro confidence="0.547185">11</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>S Bandhyopadhyay</author>
<author>A Das</author>
<author>P Bhaskar</author>
</authors>
<title>English Bengali Ad-hoc Monolingual Information Retrieval Task Result at FIRE 2008. In Working Note of Forum for FIRE-2008.</title>
<marker>Bandhyopadhyay, Das, Bhaskar, </marker>
<rawString>Bandhyopadhyay S., Das A., Bhaskar P.. English Bengali Ad-hoc Monolingual Information Retrieval Task Result at FIRE 2008. In Working Note of Forum for FIRE-2008.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Bhaskar</author>
</authors>
<title>Das A.,Pakray P.and Bandyopadhyay S.(2010). Theme Based English and Bengali Adhoc Monolingual Information Retrieval</title>
<booktitle>in FIRE 2010, In FIRE-2010.</booktitle>
<marker>Bhaskar, </marker>
<rawString>Bhaskar P., Das A.,Pakray P.and Bandyopadhyay S.(2010). Theme Based English and Bengali Adhoc Monolingual Information Retrieval in FIRE 2010, In FIRE-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Das</author>
<author>S Bandyopadhyay</author>
</authors>
<title>Theme Detection an Exploration of Opinion Subjectivity.</title>
<date>2009</date>
<booktitle>In Proceeding of Affective Computing &amp; Intelligent Interaction (ACII).</booktitle>
<contexts>
<context position="11643" citStr="Das and Bandyopadhyay, (2009)" startWordPosition="1828" endWordPosition="1831">t features, each word is tagged as either Theme Word (TW) or Other (O). 5.1 Feature Organization The set of features used in the present task have been categorized as Lexico-Syntactic, Syntactic and Discourse level features. These are listed in the Table 3 below and have been described in the subsequent subsections. Types Features Lexico-Syntactic POS Frequency Stemming Syntactic Chunk Label Dependency Parsing Depth Discourse Level Title of the Document First Paragraph Term Distribution Collocation Table 3: Features 5.2 Lexico-Syntactic Features 5.2.1 Part of Speech (POS) It has been shown by Das and Bandyopadhyay, (2009), that theme bearing words in sentences are mainly adjective, adverb, noun and verbs as other POS categories like pronoun, preposition, conjunct, article etc. have no relevance towards thematic semantic of any document. The detail of the POS tagging system chosen for the present task could be found in (Das and Bandyopadhyay 2009). 5.3 Frequency Frequency always plays a crucial role in identifying the importance of a word in the document or corpus. The system generates four separate high frequent word lists after function words are removed for four POS categories: adjective, adverb, verb and no</context>
<context position="13783" citStr="Das and Bandyopadhyay 2009" startWordPosition="2177" endWordPosition="2181">fid)/NP (Vmg WftZfff)/NP (ZPTGT)/NP (SiT�dq)/JJP (।)/SYM The movies released by Satyajit Roy are excellent. In the above example two lexicons “I&amp;/release” and “~~~/movie” are collocated in a chunk and they are very much semantically neighboring in human pragmatic knowledge. Chunk feature effectively used in supervised classifier. Chunk labels are defined as B-X (Beginning), I-X (Intermediate) and EX (End), where X is the chunk label. In the task of identification of Theme expressions, chunk label markers play a crucial role. Further details of development of chunking system could be found in (Das and Bandyopadhyay 2009). 5.5.2 Dependency Parser Dependency depth feature is very useful to identify Theme expressions. A particular Theme word generally occurs within a particular range of depth in a dependency tree. Theme expressions may be a Named Entity (NE: person, organization or location names), a common noun (Ex: accident, bomb blast, strike etc) or words of other POS categories. It has been observed that depending upon the nature of Theme expressions it can occur within a certain depth in the dependency tree in the sentences. A statistical dependency parser has 5 been used for Bengali as described in (Ghosh</context>
</contexts>
<marker>Das, Bandyopadhyay, 2009</marker>
<rawString>Das A. and Bandyopadhyay S. (2009). Theme Detection an Exploration of Opinion Subjectivity. In Proceeding of Affective Computing &amp; Intelligent Interaction (ACII).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Das</author>
<author>S Bandyopadhyay</author>
</authors>
<title>Morphological Stemming Cluster Identification for Bangla,</title>
<date>2010</date>
<booktitle>In Knowledge Sharing Event-1: Task 3: Morphological Analyzers and Generators,</booktitle>
<location>Mysore.</location>
<contexts>
<context position="12744" citStr="Das and Bandyopadhyay, 2010" startWordPosition="2009" endWordPosition="2012">four separate high frequent word lists after function words are removed for four POS categories: adjective, adverb, verb and noun. Word frequency values are then effectively used as a crucial feature in the Theme Detection technique. 5.4 Stemming Several words in a sentence that carry thematic information may be present in inflected forms. Stemming is necessary for such inflected words before they can be searched in appropriate lists. Due to non availability of good stemmers in Indian languages especially in Bengali, a stemmer based on stemming cluster technique has been used as described in (Das and Bandyopadhyay, 2010). This stemmer analyzes prefixes and suffixes of all the word forms present in a particular document. Words that are identified to have the same root form are grouped in a finite number of clusters with the identified root word as cluster center. 5.5 Syntactic Features 5.5.1 Chunk Label We found that Chunk level information is very much effective to identify lexicon inference affinity. As an example: (������ fid)/NP (Vmg WftZfff)/NP (ZPTGT)/NP (SiT�dq)/JJP (।)/SYM The movies released by Satyajit Roy are excellent. In the above example two lexicons “I&amp;/release” and “~~~/movie” are collocated in</context>
</contexts>
<marker>Das, Bandyopadhyay, 2010</marker>
<rawString>Das A. and Bandyopadhyay S. (2010). Morphological Stemming Cluster Identification for Bangla, In Knowledge Sharing Event-1: Task 3: Morphological Analyzers and Generators, January, 2010, Mysore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ekbal</author>
<author>S Bandyopadhyay</author>
</authors>
<title>A Web-based Bengali News Corpus for Named Entity Recognition. Language Resources and Evaluation Journal.</title>
<date>2008</date>
<pages>173--182</pages>
<contexts>
<context position="7140" citStr="Ekbal and Bandyopadhyay, 2008" startWordPosition="1112" endWordPosition="1115">Causative_of and Using. Frame development focuses on paraphrasability (or near paraphrasability) of words and multi-words. VerbNet annotated with thematic roles refer to the underlying semantic relationship between a predicate and its arguments. The semantic tagset of VerbNet consists of tags as agent, patient, theme, experiencer, stimulus, instrument, location, source, goal, recipient, benefactive etc. It is evident from the above discussions that no adequate semantic role set exists that can be defines across various domains. Hence pro3 Fortunately such corpus development could be found in (Ekbal and Bandyopadhyay, 2008) for Bengali. We obtained the corpus from the authors. The Bengali NEWS corpus consisted of consecutive 4 years of NEWS stories with various sub domains as reported above. For the present task we have used the Bengali NEWS corpus, developed from the archive of a leading Bengali NEWS paper2 available on the Web. The NEWS corpus is quite larger in size as reported in Table 1. 4 Annotation From the collected document set 200 documents have been chosen randomly for the annotation task. Three annotators (Mr. X, Mr. Y and Mr. Z) participated in the present task. Annotators were asked to annotate the</context>
</contexts>
<marker>Ekbal, Bandyopadhyay, 2008</marker>
<rawString>Ekbal A., Bandyopadhyay S (2008). A Web-based Bengali News Corpus for Named Entity Recognition. Language Resources and Evaluation Journal. pages 173-182, 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fillmore Charles J</author>
<author>Johnson Christopher R</author>
<author>Petruck Miriam R L</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<pages>16--235</pages>
<marker>J, R, L, 2003</marker>
<rawString>Fillmore Charles J., Johnson Christopher R., and Petruck Miriam R. L.. 2003. Background to FrameNet. International Journal of Lexicography, 16:235–250.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fruchterman Thomas M J</author>
</authors>
<title>and Reingold Edward M.(1991). Graph drawing by force-directed placement.</title>
<journal>Software: Practice and Experience,</journal>
<volume>21</volume>
<issue>11</issue>
<marker>J, </marker>
<rawString>Fruchterman Thomas M. J. and Reingold Edward M.(1991). Graph drawing by force-directed placement. Software: Practice and Experience, 21(11):1129–1164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ghosh</author>
<author>A Das</author>
<author>P Bhaskar</author>
</authors>
<title>Bandyopadhyay S.(2009). Dependency Parser for Bengali: the JU System at ICON</title>
<date>2009</date>
<booktitle>In NLP Tool Contest ICON 2009, December 14th-17th,</booktitle>
<location>Hyderabad.</location>
<contexts>
<context position="14397" citStr="Ghosh et al., 2009" startWordPosition="2283" endWordPosition="2286">2009). 5.5.2 Dependency Parser Dependency depth feature is very useful to identify Theme expressions. A particular Theme word generally occurs within a particular range of depth in a dependency tree. Theme expressions may be a Named Entity (NE: person, organization or location names), a common noun (Ex: accident, bomb blast, strike etc) or words of other POS categories. It has been observed that depending upon the nature of Theme expressions it can occur within a certain depth in the dependency tree in the sentences. A statistical dependency parser has 5 been used for Bengali as described in (Ghosh et al., 2009). 5.6 Discourse Level Features 5.6.1 Positional Aspect Depending upon the position of the thematic clue, every document is divided into a number of zones. The features considered for each document are Title words of the document, the first paragraph words and the words from the last two sentences. A detailed study was done on the Bengali news corpus to identify the roles of the positional aspect features of a document (first paragraph, last two sentences) in the detection of theme words. The importance of these positional features has been described in the following section. 5.6.2 Title Words </context>
</contexts>
<marker>Ghosh, Das, Bhaskar, 2009</marker>
<rawString>Ghosh A., Das A., Bhaskar P., Bandyopadhyay S.(2009). Dependency Parser for Bengali: the JU System at ICON 2009. In NLP Tool Contest ICON 2009, December 14th-17th, Hyderabad.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Jardine</author>
<author>C J van Rijsbergen</author>
</authors>
<title>The use of hierarchic clustering in information retrieval.</title>
<date>1971</date>
<journal>Information Storage and Retrieval,</journal>
<volume>7</volume>
<pages>217--240</pages>
<marker>Jardine, van Rijsbergen, 1971</marker>
<rawString>Jardine, N. and van Rijsbergen, C. J. (1971). The use of hierarchic clustering in information retrieval. Information Storage and Retrieval, 7, 217-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kipper Karin</author>
<author>Korhonen Anna</author>
<author>Ryant Neville</author>
<author>Palmer Martha</author>
</authors>
<title>Extending VerbNet with Novel Verb Classes. LREC</title>
<date>2006</date>
<marker>Karin, Anna, Neville, Martha, 2006</marker>
<rawString>Kipper Karin, Korhonen Anna, Ryant Neville, and Palmer Martha. Extending VerbNet with Novel Verb Classes. LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liu Hugo</author>
<author>Singh Push</author>
</authors>
<title>ConceptNet: a practical commonsense reasoning toolkit.</title>
<date>2004</date>
<journal>BT Technology Journal,</journal>
<pages>22--4</pages>
<marker>Hugo, Push, 2004</marker>
<rawString>Liu Hugo and Singh Push (2004). ConceptNet: a practical commonsense reasoning toolkit. BT Technology Journal, 22(4):211-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Palmer Martha</author>
<author>Gildea Dan</author>
<author>Kingsbury Paul</author>
</authors>
<title>The Proposition Bank: A Corpus Annotated with Semantic Roles,</title>
<date>2005</date>
<journal>Computational Linguistics Journal,</journal>
<volume>31</volume>
<marker>Martha, Dan, Paul, 2005</marker>
<rawString>Palmer Martha, Gildea Dan, Kingsbury Paul, The Proposition Bank: A Corpus Annotated with Semantic Roles, Computational Linguistics Journal, 31:1, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Singh Push</author>
<author>Williams William</author>
</authors>
<title>LifeNet: a propositional model of ordinary human activity.</title>
<date>2003</date>
<booktitle>In the Proc. Of DC-KCAP</booktitle>
<marker>Push, William, 2003</marker>
<rawString>Singh Push and Williams William (2003). LifeNet: a propositional model of ordinary human activity. In the Proc. Of DC-KCAP 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Singh Push</author>
<author>Barry Barbara</author>
<author>Liu Hugo</author>
</authors>
<title>Teaching machines about everyday life.</title>
<date>2004</date>
<journal>BT Technology Journal,</journal>
<pages>22--4</pages>
<marker>Push, Barbara, Hugo, 2004</marker>
<rawString>Singh Push, Barry Barbara, and Liu Hugo (2004). Teaching machines about everyday life. BT Technology Journal, 22(4):227-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Smith Marc</author>
<author>Ben Shneiderman</author>
</authors>
<title>Natasa MilicFrayling, Eduarda Mendes Rodrigues, Vladimir Barash, Cody Dunne, Tony Capone, Adam Perer, and Eric Gleave.</title>
<date>2009</date>
<booktitle>In C&amp;T ’09: Proc. Fourth International Conference on Communities and Technologies, LNCS.</booktitle>
<publisher>Springer.</publisher>
<marker>Marc, Shneiderman, 2009</marker>
<rawString>Smith Marc, Ben Shneiderman, Natasa MilicFrayling, Eduarda Mendes Rodrigues, Vladimir Barash, Cody Dunne, Tony Capone, Adam Perer, and Eric Gleave. 2009. Analyzing (social media) networks with NodeXL. In C&amp;T ’09: Proc. Fourth International Conference on Communities and Technologies, LNCS. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Willerr</author>
</authors>
<title>Recent trends in hierarchic document clustering: A critical review.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>24</volume>
<issue>5</issue>
<pages>577--597</pages>
<marker>Willerr, 1988</marker>
<rawString>Willerr, P. (1988). Recent trends in hierarchic document clustering: A critical review. Information Processing and Management, 24(5), 577-597.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>