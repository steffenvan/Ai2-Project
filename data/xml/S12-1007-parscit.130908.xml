<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028202">
<title confidence="0.995799">
“Could you make me a favour and do coffee, please?”:
Implications for Automatic Error Correction in English and Dutch
</title>
<author confidence="0.99012">
Sophia Katrenko
</author>
<affiliation confidence="0.985313">
UiL-OTS
Utrecht University
</affiliation>
<email confidence="0.994901">
s.katrenko@uu.nl
</email>
<sectionHeader confidence="0.995598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998445">
The correct choice of words has proven chal-
lenging for learners of a second language and
errors of this kind form a separate category
in error typology. This paper focuses on one
known example of two verbs that are often
confused by non-native speakers of Germanic
languages, to make and to do. We conduct ex-
periments using syntactic information and im-
mediate context for Dutch and English. Our
results show that the methods exploiting syn-
tactic information and distributional similarity
yield the best results.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999864591836735">
When learning a second language, non-native speak-
ers make errors at all levels of linguistic analy-
sis, from pronunciation and intonation to language
use. Word choice errors form a substantial part
of all errors made by learners and may also be
observed in writing or speech of native speak-
ers. This category of errors includes homophones.
Some commonly known confusions in English are
accept-except, advice-advise, buy-by-bye, ate-eight,
to name but a few. Other errors can be explained
by a non-native speaker’s inability to distinguish be-
tween words because there exists only one corre-
sponding word in their native language. For ex-
ample, Portuguese and Spanish speakers have diffi-
culties to differentiate between te doen (to do) and
te maken (to make), and Turkish between kunnen
(can), weten (to know) and kennen (to know) in
Dutch (Coenen et al., 1979). Adopting terminol-
ogy from Golding and Roth (1999) and Rozovskaya
and Roth (2010), do/make and kunnen/kennen/weten
form two confusion sets. However, unlike the case
of kunnen/kennen/weten, where the correct choice is
often determined by syntactic context 1, the choice
between to make and to do can be motivated by
semantic factors. It has been argued in the litera-
ture that the correct use of these verbs depends on
what is being expressed: to do is used to refer to
daily routines and activities, while to make is used to
describe constructing or creating something. Since
word choice errors have different nature, we hypoth-
esize that there may exist no uniform approach to
correct them.
State-of-the-art spell-checkers are able to detect
spelling and agreement errors but fail to find words
used incorrectly, e.g. to distinguish to make from to
do. Motivated by the implications that the correct
prediction of two verbs of interest may have for au-
tomatic error correction, we model the problem of
choosing the correct verb in a similar vein to selec-
tional preferences. The latter has been considered
for a variety of applications, e. g. semantic role la-
beling (Zapirain et al., 2009). Words such as be or
do have been often excluded from consideration be-
cause they are highly polysemous and “do not select
strongly for their arguments” (McCarthy and Car-
roll, 2003). In this paper, we study whether semantic
classes of arguments may be used to determine the
correct predicate (e.g., to make or to do) and con-
sider the following research questions:
</bodyText>
<listItem confidence="0.765331">
1. Can information on semantic classes of direct
</listItem>
<footnote confidence="0.952407333333333">
1Kunnen is a modal verb followed by the main verb, kennen
takes a direct object as in, e.g., to know somebody, and weten is
often followed by a clause (as in I know that).
</footnote>
<page confidence="0.990007">
49
</page>
<note confidence="0.5301055">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 49–53,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.973870888888889">
objects potentially help to correct verb choice
errors?
2. How do approaches using contextual and syn-
tactic information compare when predicting to
make vs. to do?
The paper is organised as follows. Section 2.1
discusses the methods, followed by Section 2.2 on
data. The experimental findings are presented in
Section 2.3. We conclude in Section 3.
</bodyText>
<sectionHeader confidence="0.997354" genericHeader="introduction">
2 Experiments
</sectionHeader>
<bodyText confidence="0.999972142857143">
We re-examine several approaches to selectional
preferences in the context of error correction. Ex-
isting methods fall into one of two categories, either
those relying on information from WordNet (Mc-
Carthy and Carroll, 2003), or data-driven (Erk,
2007; Schulte im Walde, 2010; Pado et al., 2007).
For the purpose of our study, we focus on the latter.
</bodyText>
<subsectionHeader confidence="0.956476">
2.1 Methods
</subsectionHeader>
<bodyText confidence="0.9998769">
For each verb in question, we have a frequency-
based ranking list of nouns co-occurring with it
(verb-object pairs) which we use for the first two
methods.
Latent semantic clustering (LSC) Rooth et
al. (1999) have proposed a soft-clustering method to
determine selectional preferences, which models the
joint distribution of nouns n and verbs v by condi-
tioning them on a hidden class c. The probability of
a pair (v, n) then equals
</bodyText>
<equation confidence="0.968883">
P(v, n) _ � P(c)P(vJc)P(nJc) (1)
CEC
</equation>
<bodyText confidence="0.978109090909091">
Similarity-based method The next classifier we
use combines similarity between nouns with rank-
ing information and is a modification of the method
described in (Pado et al., 2007). First, for all words
nz on the ranking list their frequency scores are nor-
malised between 0 and 1, fz. Then, they are weighed
by the similarity score between a new noun nj and a
corresponding word on the ranking list, nz, and the
noun with the highest score (1-nearest neighbour) is
selected:
arg max fz x sim(nj, nz) (2)
nz
Finally, two highest scores for each verb’s ranking
list are compared and the verb with higher score is
selected as a preferred one.
In addition, if we sum over all seen words instead
of choosing the nearest neighbour, this will lead to
the original approach by Pado et al. (2007). In the
experimental part we consider both approaches (the
original method is referred to as SMP while the
nearest neighbour approach is marked by SMknn)
and study whether there is any difference between
the two when a verb that allows many different ar-
guments is considered (e.g., it may be better to use
the nearest neighbour approach for to do rather than
aggregating over all similarity scores).
Bag-of-words (BoW) approach This widely used
approach to document classification considers con-
textual words and their frequencies to represent doc-
uments (Zellig, 1954). We restrict the length of the
context around two verbs (within a window of f2
and f3 around the focus word, make or do) and
build a Naive Bayes classifier.
</bodyText>
<subsectionHeader confidence="0.995857">
2.2 Data
</subsectionHeader>
<bodyText confidence="0.999872285714286">
Both verbs, to make and to do, license complements
of various kinds, e. g. they can be mono-transitive,
ditransitive, and complex transitive (sentences 1, 2,
and 3, respectively). Furthermore, make can be part
of idiomatic ditransitives (e.g., make use of, make
fun of, make room for) and phrasal mono-transitives
(e.g., make up) .
</bodyText>
<listItem confidence="0.999181333333333">
1. Andrew made [a cake]dobj.
2. Andrew made [his mum]zobj [a cake]dobj.
3. Andrew made [his mum]dobj happy.
</listItem>
<bodyText confidence="0.999313538461538">
For English, we use one of the largest cor-
pora available, the PukWAC (over 2 billion words,
30GB) (Baroni et al., 2009), which has been parsed
by MaltParser (Nivre and Scholz, 2004). We extract
all sentences with to do or to make (based on lem-
mata). The verb to make occurs in 2,13% of sen-
tences, and the verb to do in 3,27% of sentences in
the PukWAC corpus. Next, we exclude from con-
sideration phrasal mono-transitives and select sen-
tences where verb complements are nouns (Table 1).
For experiments in Dutch, we use the “Wikipedia
Dump Of 2010” corpus, which is a part of Lassy
Large corpus (159 million tokens), and is parsed by
</bodyText>
<page confidence="0.880476">
50
</page>
<table confidence="0.983897666666667">
LANG # sent # dobj (to make) # dobj (to do)
EN 181,813,571 1,897,747 881,314
NL 8,639,837 15,510 6,197
</table>
<tableCaption confidence="0.954059">
Table 1: The number of sentences in English (EN) and Dutch (NL) corpora (the last two columns correspond to the
number of sentences where direct objects are nouns).
</tableCaption>
<bodyText confidence="0.999631772727273">
the Alpino parser (Bouma et al., 2001). Unlike in
English data, to make occurs here more often than
to do (3,3% vs. 1%). This difference can be ex-
plained by the fact that to do is also an auxiliary verb
in English which leads to more occurrences in to-
tal. Similarly to the English data set, phrasal mono-
transitives are filtered out. Finally, the sentences
that contain either to make or to do from wiki01 up
to wiki07 (19,847 sentences in total) have been se-
lected for training and wiki08 (1,769 sentences in
total) for testing. To be able to compare our results
against the performance on English data, we sample
a subset from PukWAC which is of the same size as
Dutch data set and is referred to as EN (sm).
To measure distributional similarity for the near-
est neighbour method, we use first-order and
second-order similarity based on Lin’s information
theoretic measure (Lin, 1998). For both languages,
similarity scores have been derived given a subset
of Wikipedia (276 million tokens for English and
114 million tokens for Dutch) using the DISCO
API (Kolb, 2009).
</bodyText>
<subsectionHeader confidence="0.682932">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.999739218181818">
Table 2 and Table 3 summarize our results. When re-
ferring to similarity-based methods, the symbols (f)
and (s) indicate first-order and second-order similar-
ity. For the BoW models, f2 and f3 corresponds
to the context length. The performance is measured
by true positive rate (TP) per class, overall accuracy
(Acc) and coverage (Cov). The former indicates in
how many cases the correct class label (make or do)
has been predicted, while the latter shows how many
examples a system was able to classify. Coverage is
especially indicative for LCS and semantic similar-
ity approaches because they may fail to yield pre-
dictions. For these methods, we provide two evalua-
tions. First, in order to be able to compare results
against the BoW approach, we measure accuracy
and coverage on all test examples. In such a case,
if some direct objects occur very often in the test set
and are classified correctly, accuracy scores will be
boosted. Therefore, we also provide the second eval-
uation where we measure accuracy and coverage on
(unique) test examples regardless of how frequent
they are. This evaluation will give us a better in-
sight into how well LCS and similarity-based meth-
ods work. Finally, we tested several settings for the
LSC method and the results presented here are ob-
tained for 20 clusters and 50 iterations. We remove
stop words 2 but do not take any other preprocessing
steps.
For both languages, it is more difficult to predict
to do than to make, although the differences in per-
formance on Dutch data (NL) are much smaller than
on English data (EN (sm)). An interesting obser-
vation is that using second-order similarity slightly
boosts performance for to make but is highly unde-
sirable for predicting to do (decrease in accuracy for
around 15%) in Dutch. This may be explained by the
fact that the objects of to do are already very generic.
Our findings on English data are that the similarity-
based approach is more sensitive to the choice of
aggregating over all words in the training set or se-
lecting the nearest neighbour. In particular, we ob-
tained better performance when choosing the nearest
neighbour for to do but aggregating over all scores
for to make. The results on Dutch and English data
are in general not always comparable. In addition
to the differences in performance of similarity-based
methods, the BoW models work better for predicting
to do in English but to make in Dutch.
As expected, similarity-based approaches yield
higher coverage than LSC, although the latter is su-
perior in terms of accuracy (in all cases but to do
in English). Since LSC turned out to be the most
computationally efficient method, we have also run
it on larger subsets of the PukWAC data set, up to
the entire corpus. We have not noticed any signifi-
</bodyText>
<footnote confidence="0.9851765">
2We use stop word lists for English and Dutch from http:
//snowball.tartarus.org/algorithms/.
</footnote>
<page confidence="0.992358">
51
</page>
<table confidence="0.9999425625">
LANG Method TP (to make) Cov (to make) TP (to do) Cov (to do) Acc (all) Cov (all)
EN (all) LSC 91.70 98.75 73.40 97.16 85.90 98.24
EN (sm) LSC 89.81 90.00 75.81 86.70 86.91 89.30
SMP (f) 84.89 98.82 69.89 95.14 81.78 98.03
SMP (s) 82.92 98.82 55.65 95.14 77.27 98.03
SMknn (f) 62.61 98.82 91.13 95.14 68.52 98.03
SMknn (s) 4.36 98.82 99.46 95.14 24.07 98.03
BoW f2 36.41 100 82.21 100 46.01 100
BoW f3 32.26 100 84.10 100 43.13 100
NL LSC 98.75 91.79 95.74 93.37 98.09 92.13
SMP (f) 95.64 95.82 92.97 98.14 95.06 96.32
SMP (s) 97.52 95.82 76.75 98.14 93.00 96.32
SMknn (f) 94.14 95.82 92.97 98.14 93.89 96.32
SMknn (s) 96.09 95.82 78.64 98.14 92.30 96.32
BoW f2 89.34 100 61.19 100 83.44 100
BoW f3 91.06 100 54.18 100 83.32 100
</table>
<tableCaption confidence="0.964151">
Table 2: True positive rate (TP, %), accuracy (Acc, %) and coverage (Cov, %) for the experiments on English (EN)
and Dutch (NL) data.
</tableCaption>
<table confidence="0.999936181818182">
LANG Method TP (to make) Cov (to make) TP (to do) Cov (to do) Acc (all) Cov (all)
EN (sm) LSC 80.88 77.12 52.60 74.76 73.73 76.51
SMP (f) 73.17 97.29 45.99 90.78 66.49 95.60
SMP (s) 77.00 97.29 33.69 90.78 66.36 95.60
SMknn (f) 31.18 97.29 82.35 90.78 43.76 95.60
SMknn (s) 4.36 98.82 98.93 90.78 25.76 95.60
NL LSC 94.85 63.40 86.59 76.64 92.39 66.83
SMP (f) 87.55 81.37 77.00 93.45 84.24 84.50
SMP (s) 91.16 81.37 54.00 93.45 80.52 84.50
SMknn (f) 80.72 81.37 76.00 93.45 79.66 84.50
SMknn (s) 85.54 81.37 55.00 93.45 76.79 84.50
</table>
<tableCaption confidence="0.991064">
Table 3: True positive rate (TP, %), accuracy (Acc, %) and coverage (Cov, %) for the experiments on English (EN)
and Dutch (NL) unique direct objects.
</tableCaption>
<bodyText confidence="0.870403272727273">
cant changes in performance; the results for the en-
tire data set, EN (all), are given in the first row of
Table 2. Table 3 shows the results for the methods
using direct object information on unique objects,
which gives a more realistic assessment of their per-
formance. At closer inspection, we noticed that
many non-classified cases in Dutch refer to com-
pounds. For instance, bluegrassmuziek (bluegrass
music) cannot be compared against known words in
the training set. In order to cover such cases, existing
methods may benefit from morphological analysis.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="conclusions">
3 Conclusions
</sectionHeader>
<bodyText confidence="0.999926076923077">
In order to predict the use of two often confused
verbs, to make and to do, we have compared two
methods to modeling selectional preferences against
the bag-of-words approach. The BoW method is al-
ways outperformed by LCS and similarity-based ap-
proaches, although the differences in performance
are much larger for to do in Dutch and for to make
in English. In this study, we do not use any corpus of
non-native speakers’ errors and explore how well it
is possible to predict one of two verbs provided that
the context words have been chosen correctly. In the
future work, we plan to label all incorrect uses of to
make and to do and to correct them.
</bodyText>
<sectionHeader confidence="0.99823" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.86831525">
The author thanks anonymous reviewers for their valu-
able comments. This work is supported by a VICI grant
number 277-80-002 by the Netherlands Organisation for
Scientific Research (NWO).
</bodyText>
<page confidence="0.997838">
52
</page>
<sectionHeader confidence="0.990169" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883322033898">
Marco Baroni and Silvia Bernardini and Adriano Fer-
raresi and Eros Zanchetta. 2009. The WaCky Wide
Web: A Collection of Very Large Linguistically Pro-
cessed Web-Crawled Corpora. Language Resources
and Evaluation 43(3), pp. 209-226.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage Computational Analysis
of Dutch. In Computational Linguistics in the Nether-
lands 2000. Enschede.
Jos´ee A. Coenen, W. van Wiggen, and R. Bok-Bennema.
1979. Leren van fouten: een analyse van de meest
voorkomende Nederlandse taalfouten, die gemaakt
worden door Marokkaanse, Turkse, Spaanse en Por-
tugese kinderen. Amsterdam: Stichting ABC, Contac-
torgaan voor de Innovatie van het Onderwijs.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of ACL 2007.
Prague, Czech Republic, 2007.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
Based Approach to Context-Sensitive Spelling Correc-
tion. Machine Learning 34(1-3), pp. 107-130.
Peter Kolb. 2009. Experiments on the difference be-
tween semantic similarity and relatedness. In Pro-
ceedings of the 17th Nordic Conference on Compu-
tational Linguistics - NODALIDA ’09, Odense, Den-
mark, May 2009.
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of COLING-ACL
1998, Montreal.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically
acquired selectional preferences. Computational Lin-
guistics, 29(4), pp. 639-654.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
COLING 04.
Sebastian Pad´o, Ulrike Pad´o and Katrin Erk. 2007. Flex-
ible, Corpus-Based Modelling of Human Plausibility
Judgements. In Proceedings of EMNLP/CoNLL 2007.
Prague, Czech Republic, pp. 400-409.
Mats Rooth, Stefan Riezler and Detlef Prescher. 1999.
Inducing a Semantically Annotated Lexicon via EM-
Based Clustering. In Proceedings of ACL 99.
Anna Rozovskaya and Dan Roth. 2010. Generating
Confusion Sets for Context-Sensitive Error Correction.
In Proceedings of EMNLP, pp. 961-970.
Sabine Schulte im Walde. 2010. Comparing Com-
putational Approaches to Selectional Preferences –
Second-Order Co-Occurrence vs. Latent Semantic
Clusters. In Proceedings of the 7th International Con-
ference on Language Resources and Evaluation, Val-
letta, Malta, pp. 1381–1388.
Be˜nat Zapirain, Eneko Agirre and Llu´ıs M`arquez. 2009.
Generalizing over Lexical Features: Selectional Pref-
erences for Semantic Role Classification. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers. Suntec, Singapore, pp. 73-76.
Harris Zellig. 1954. Distributional Structure. Word 10
(2/3), p. 146-62.
</reference>
<page confidence="0.999351">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.398757">
<title confidence="0.978584">you a favour and Implications for Automatic Error Correction in English and Dutch</title>
<author confidence="0.754629">Sophia</author>
<affiliation confidence="0.439201">Utrecht</affiliation>
<email confidence="0.991443">s.katrenko@uu.nl</email>
<abstract confidence="0.999199769230769">The correct choice of words has proven challenging for learners of a second language and errors of this kind form a separate category in error typology. This paper focuses on one known example of two verbs that are often confused by non-native speakers of Germanic make We conduct experiments using syntactic information and immediate context for Dutch and English. Our results show that the methods exploiting syntactic information and distributional similarity yield the best results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora.</title>
<date>2009</date>
<journal>Language Resources and Evaluation</journal>
<volume>43</volume>
<issue>3</issue>
<pages>209--226</pages>
<contexts>
<context position="6783" citStr="Baroni et al., 2009" startWordPosition="1120" endWordPosition="1123">rd, make or do) and build a Naive Bayes classifier. 2.2 Data Both verbs, to make and to do, license complements of various kinds, e. g. they can be mono-transitive, ditransitive, and complex transitive (sentences 1, 2, and 3, respectively). Furthermore, make can be part of idiomatic ditransitives (e.g., make use of, make fun of, make room for) and phrasal mono-transitives (e.g., make up) . 1. Andrew made [a cake]dobj. 2. Andrew made [his mum]zobj [a cake]dobj. 3. Andrew made [his mum]dobj happy. For English, we use one of the largest corpora available, the PukWAC (over 2 billion words, 30GB) (Baroni et al., 2009), which has been parsed by MaltParser (Nivre and Scholz, 2004). We extract all sentences with to do or to make (based on lemmata). The verb to make occurs in 2,13% of sentences, and the verb to do in 3,27% of sentences in the PukWAC corpus. Next, we exclude from consideration phrasal mono-transitives and select sentences where verb complements are nouns (Table 1). For experiments in Dutch, we use the “Wikipedia Dump Of 2010” corpus, which is a part of Lassy Large corpus (159 million tokens), and is parsed by 50 LANG # sent # dobj (to make) # dobj (to do) EN 181,813,571 1,897,747 881,314 NL 8,6</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni and Silvia Bernardini and Adriano Ferraresi and Eros Zanchetta. 2009. The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation 43(3), pp. 209-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Gertjan van Noord</author>
<author>Robert Malouf</author>
</authors>
<title>Alpino: Wide-coverage Computational Analysis of Dutch.</title>
<date>2001</date>
<booktitle>In Computational Linguistics in the Netherlands</booktitle>
<location>Enschede.</location>
<marker>Bouma, van Noord, Malouf, 2001</marker>
<rawString>Gosse Bouma, Gertjan van Noord, and Robert Malouf. 2001. Alpino: Wide-coverage Computational Analysis of Dutch. In Computational Linguistics in the Netherlands 2000. Enschede.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´ee A Coenen</author>
<author>W van Wiggen</author>
<author>R Bok-Bennema</author>
</authors>
<title>Leren van fouten: een analyse van de meest voorkomende Nederlandse taalfouten, die gemaakt worden door Marokkaanse, Turkse, Spaanse en Portugese kinderen. Amsterdam: Stichting ABC, Contactorgaan voor de Innovatie van het Onderwijs.</title>
<date>1979</date>
<marker>Coenen, van Wiggen, Bok-Bennema, 1979</marker>
<rawString>Jos´ee A. Coenen, W. van Wiggen, and R. Bok-Bennema. 1979. Leren van fouten: een analyse van de meest voorkomende Nederlandse taalfouten, die gemaakt worden door Marokkaanse, Turkse, Spaanse en Portugese kinderen. Amsterdam: Stichting ABC, Contactorgaan voor de Innovatie van het Onderwijs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007.</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4124" citStr="Erk, 2007" startWordPosition="667" endWordPosition="668">ects potentially help to correct verb choice errors? 2. How do approaches using contextual and syntactic information compare when predicting to make vs. to do? The paper is organised as follows. Section 2.1 discusses the methods, followed by Section 2.2 on data. The experimental findings are presented in Section 2.3. We conclude in Section 3. 2 Experiments We re-examine several approaches to selectional preferences in the context of error correction. Existing methods fall into one of two categories, either those relying on information from WordNet (McCarthy and Carroll, 2003), or data-driven (Erk, 2007; Schulte im Walde, 2010; Pado et al., 2007). For the purpose of our study, we focus on the latter. 2.1 Methods For each verb in question, we have a frequencybased ranking list of nouns co-occurring with it (verb-object pairs) which we use for the first two methods. Latent semantic clustering (LSC) Rooth et al. (1999) have proposed a soft-clustering method to determine selectional preferences, which models the joint distribution of nouns n and verbs v by conditioning them on a hidden class c. The probability of a pair (v, n) then equals P(v, n) _ � P(c)P(vJc)P(nJc) (1) CEC Similarity-based met</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of ACL 2007. Prague, Czech Republic, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Dan Roth</author>
</authors>
<title>A WinnowBased Approach to Context-Sensitive Spelling Correction.</title>
<date>1999</date>
<booktitle>Machine Learning 34(1-3),</booktitle>
<pages>107--130</pages>
<contexts>
<context position="1621" citStr="Golding and Roth (1999)" startWordPosition="255" endWordPosition="258">speakers. This category of errors includes homophones. Some commonly known confusions in English are accept-except, advice-advise, buy-by-bye, ate-eight, to name but a few. Other errors can be explained by a non-native speaker’s inability to distinguish between words because there exists only one corresponding word in their native language. For example, Portuguese and Spanish speakers have difficulties to differentiate between te doen (to do) and te maken (to make), and Turkish between kunnen (can), weten (to know) and kennen (to know) in Dutch (Coenen et al., 1979). Adopting terminology from Golding and Roth (1999) and Rozovskaya and Roth (2010), do/make and kunnen/kennen/weten form two confusion sets. However, unlike the case of kunnen/kennen/weten, where the correct choice is often determined by syntactic context 1, the choice between to make and to do can be motivated by semantic factors. It has been argued in the literature that the correct use of these verbs depends on what is being expressed: to do is used to refer to daily routines and activities, while to make is used to describe constructing or creating something. Since word choice errors have different nature, we hypothesize that there may exi</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>Andrew R. Golding and Dan Roth. 1999. A WinnowBased Approach to Context-Sensitive Spelling Correction. Machine Learning 34(1-3), pp. 107-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Kolb</author>
</authors>
<title>Experiments on the difference between semantic similarity and relatedness.</title>
<date>2009</date>
<booktitle>In Proceedings of the 17th Nordic Conference on Computational Linguistics - NODALIDA ’09,</booktitle>
<location>Odense, Denmark,</location>
<contexts>
<context position="8636" citStr="Kolb, 2009" startWordPosition="1449" endWordPosition="1450">elected for training and wiki08 (1,769 sentences in total) for testing. To be able to compare our results against the performance on English data, we sample a subset from PukWAC which is of the same size as Dutch data set and is referred to as EN (sm). To measure distributional similarity for the nearest neighbour method, we use first-order and second-order similarity based on Lin’s information theoretic measure (Lin, 1998). For both languages, similarity scores have been derived given a subset of Wikipedia (276 million tokens for English and 114 million tokens for Dutch) using the DISCO API (Kolb, 2009). 2.3 Results Table 2 and Table 3 summarize our results. When referring to similarity-based methods, the symbols (f) and (s) indicate first-order and second-order similarity. For the BoW models, f2 and f3 corresponds to the context length. The performance is measured by true positive rate (TP) per class, overall accuracy (Acc) and coverage (Cov). The former indicates in how many cases the correct class label (make or do) has been predicted, while the latter shows how many examples a system was able to classify. Coverage is especially indicative for LCS and semantic similarity approaches becaus</context>
</contexts>
<marker>Kolb, 2009</marker>
<rawString>Peter Kolb. 2009. Experiments on the difference between semantic similarity and relatedness. In Proceedings of the 17th Nordic Conference on Computational Linguistics - NODALIDA ’09, Odense, Denmark, May 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL 1998,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="8452" citStr="Lin, 1998" startWordPosition="1419" endWordPosition="1420">nglish data set, phrasal monotransitives are filtered out. Finally, the sentences that contain either to make or to do from wiki01 up to wiki07 (19,847 sentences in total) have been selected for training and wiki08 (1,769 sentences in total) for testing. To be able to compare our results against the performance on English data, we sample a subset from PukWAC which is of the same size as Dutch data set and is referred to as EN (sm). To measure distributional similarity for the nearest neighbour method, we use first-order and second-order similarity based on Lin’s information theoretic measure (Lin, 1998). For both languages, similarity scores have been derived given a subset of Wikipedia (276 million tokens for English and 114 million tokens for Dutch) using the DISCO API (Kolb, 2009). 2.3 Results Table 2 and Table 3 summarize our results. When referring to similarity-based methods, the symbols (f) and (s) indicate first-order and second-order similarity. For the BoW models, f2 and f3 corresponds to the context length. The performance is measured by true positive rate (TP) per class, overall accuracy (Acc) and coverage (Cov). The former indicates in how many cases the correct class label (mak</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of COLING-ACL 1998, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<pages>639--654</pages>
<contexts>
<context position="2938" citStr="McCarthy and Carroll, 2003" startWordPosition="473" endWordPosition="477"> spelling and agreement errors but fail to find words used incorrectly, e.g. to distinguish to make from to do. Motivated by the implications that the correct prediction of two verbs of interest may have for automatic error correction, we model the problem of choosing the correct verb in a similar vein to selectional preferences. The latter has been considered for a variety of applications, e. g. semantic role labeling (Zapirain et al., 2009). Words such as be or do have been often excluded from consideration because they are highly polysemous and “do not select strongly for their arguments” (McCarthy and Carroll, 2003). In this paper, we study whether semantic classes of arguments may be used to determine the correct predicate (e.g., to make or to do) and consider the following research questions: 1. Can information on semantic classes of direct 1Kunnen is a modal verb followed by the main verb, kennen takes a direct object as in, e.g., to know somebody, and weten is often followed by a clause (as in I know that). 49 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 49–53, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics objects potentially help t</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>Diana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4), pp. 639-654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 04.</booktitle>
<contexts>
<context position="6845" citStr="Nivre and Scholz, 2004" startWordPosition="1130" endWordPosition="1133">a Both verbs, to make and to do, license complements of various kinds, e. g. they can be mono-transitive, ditransitive, and complex transitive (sentences 1, 2, and 3, respectively). Furthermore, make can be part of idiomatic ditransitives (e.g., make use of, make fun of, make room for) and phrasal mono-transitives (e.g., make up) . 1. Andrew made [a cake]dobj. 2. Andrew made [his mum]zobj [a cake]dobj. 3. Andrew made [his mum]dobj happy. For English, we use one of the largest corpora available, the PukWAC (over 2 billion words, 30GB) (Baroni et al., 2009), which has been parsed by MaltParser (Nivre and Scholz, 2004). We extract all sentences with to do or to make (based on lemmata). The verb to make occurs in 2,13% of sentences, and the verb to do in 3,27% of sentences in the PukWAC corpus. Next, we exclude from consideration phrasal mono-transitives and select sentences where verb complements are nouns (Table 1). For experiments in Dutch, we use the “Wikipedia Dump Of 2010” corpus, which is a part of Lassy Large corpus (159 million tokens), and is parsed by 50 LANG # sent # dobj (to make) # dobj (to do) EN 181,813,571 1,897,747 881,314 NL 8,639,837 15,510 6,197 Table 1: The number of sentences in Englis</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of COLING 04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>Ulrike Pad´o and Katrin Erk.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP/CoNLL 2007.</booktitle>
<pages>400--409</pages>
<location>Prague, Czech Republic,</location>
<marker>Pad´o, 2007</marker>
<rawString>Sebastian Pad´o, Ulrike Pad´o and Katrin Erk. 2007. Flexible, Corpus-Based Modelling of Human Plausibility Judgements. In Proceedings of EMNLP/CoNLL 2007. Prague, Czech Republic, pp. 400-409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
</authors>
<title>Inducing a Semantically Annotated Lexicon via EMBased Clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL 99.</booktitle>
<contexts>
<context position="4443" citStr="Rooth et al. (1999)" startWordPosition="721" endWordPosition="724">tion 2.3. We conclude in Section 3. 2 Experiments We re-examine several approaches to selectional preferences in the context of error correction. Existing methods fall into one of two categories, either those relying on information from WordNet (McCarthy and Carroll, 2003), or data-driven (Erk, 2007; Schulte im Walde, 2010; Pado et al., 2007). For the purpose of our study, we focus on the latter. 2.1 Methods For each verb in question, we have a frequencybased ranking list of nouns co-occurring with it (verb-object pairs) which we use for the first two methods. Latent semantic clustering (LSC) Rooth et al. (1999) have proposed a soft-clustering method to determine selectional preferences, which models the joint distribution of nouns n and verbs v by conditioning them on a hidden class c. The probability of a pair (v, n) then equals P(v, n) _ � P(c)P(vJc)P(nJc) (1) CEC Similarity-based method The next classifier we use combines similarity between nouns with ranking information and is a modification of the method described in (Pado et al., 2007). First, for all words nz on the ranking list their frequency scores are normalised between 0 and 1, fz. Then, they are weighed by the similarity score between a</context>
</contexts>
<marker>Rooth, Riezler, Prescher, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler and Detlef Prescher. 1999. Inducing a Semantically Annotated Lexicon via EMBased Clustering. In Proceedings of ACL 99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Generating Confusion Sets for Context-Sensitive Error Correction.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>961--970</pages>
<contexts>
<context position="1652" citStr="Rozovskaya and Roth (2010)" startWordPosition="260" endWordPosition="263">rrors includes homophones. Some commonly known confusions in English are accept-except, advice-advise, buy-by-bye, ate-eight, to name but a few. Other errors can be explained by a non-native speaker’s inability to distinguish between words because there exists only one corresponding word in their native language. For example, Portuguese and Spanish speakers have difficulties to differentiate between te doen (to do) and te maken (to make), and Turkish between kunnen (can), weten (to know) and kennen (to know) in Dutch (Coenen et al., 1979). Adopting terminology from Golding and Roth (1999) and Rozovskaya and Roth (2010), do/make and kunnen/kennen/weten form two confusion sets. However, unlike the case of kunnen/kennen/weten, where the correct choice is often determined by syntactic context 1, the choice between to make and to do can be motivated by semantic factors. It has been argued in the literature that the correct use of these verbs depends on what is being expressed: to do is used to refer to daily routines and activities, while to make is used to describe constructing or creating something. Since word choice errors have different nature, we hypothesize that there may exist no uniform approach to corre</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>Anna Rozovskaya and Dan Roth. 2010. Generating Confusion Sets for Context-Sensitive Error Correction. In Proceedings of EMNLP, pp. 961-970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Comparing Computational Approaches to Selectional Preferences – Second-Order Co-Occurrence vs. Latent Semantic Clusters.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1381--1388</pages>
<location>Valletta, Malta,</location>
<contexts>
<context position="4148" citStr="Walde, 2010" startWordPosition="671" endWordPosition="672">to correct verb choice errors? 2. How do approaches using contextual and syntactic information compare when predicting to make vs. to do? The paper is organised as follows. Section 2.1 discusses the methods, followed by Section 2.2 on data. The experimental findings are presented in Section 2.3. We conclude in Section 3. 2 Experiments We re-examine several approaches to selectional preferences in the context of error correction. Existing methods fall into one of two categories, either those relying on information from WordNet (McCarthy and Carroll, 2003), or data-driven (Erk, 2007; Schulte im Walde, 2010; Pado et al., 2007). For the purpose of our study, we focus on the latter. 2.1 Methods For each verb in question, we have a frequencybased ranking list of nouns co-occurring with it (verb-object pairs) which we use for the first two methods. Latent semantic clustering (LSC) Rooth et al. (1999) have proposed a soft-clustering method to determine selectional preferences, which models the joint distribution of nouns n and verbs v by conditioning them on a hidden class c. The probability of a pair (v, n) then equals P(v, n) _ � P(c)P(vJc)P(nJc) (1) CEC Similarity-based method The next classifier </context>
</contexts>
<marker>Walde, 2010</marker>
<rawString>Sabine Schulte im Walde. 2010. Comparing Computational Approaches to Selectional Preferences – Second-Order Co-Occurrence vs. Latent Semantic Clusters. In Proceedings of the 7th International Conference on Language Resources and Evaluation, Valletta, Malta, pp. 1381–1388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Be˜nat Zapirain</author>
</authors>
<title>Eneko Agirre and Llu´ıs M`arquez.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers. Suntec, Singapore,</booktitle>
<pages>73--76</pages>
<marker>Zapirain, 2009</marker>
<rawString>Be˜nat Zapirain, Eneko Agirre and Llu´ıs M`arquez. 2009. Generalizing over Lexical Features: Selectional Preferences for Semantic Role Classification. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers. Suntec, Singapore, pp. 73-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harris Zellig</author>
</authors>
<date>1954</date>
<journal>Distributional Structure. Word</journal>
<volume>10</volume>
<issue>2</issue>
<pages>146--62</pages>
<contexts>
<context position="6057" citStr="Zellig, 1954" startWordPosition="997" endWordPosition="998">o the original approach by Pado et al. (2007). In the experimental part we consider both approaches (the original method is referred to as SMP while the nearest neighbour approach is marked by SMknn) and study whether there is any difference between the two when a verb that allows many different arguments is considered (e.g., it may be better to use the nearest neighbour approach for to do rather than aggregating over all similarity scores). Bag-of-words (BoW) approach This widely used approach to document classification considers contextual words and their frequencies to represent documents (Zellig, 1954). We restrict the length of the context around two verbs (within a window of f2 and f3 around the focus word, make or do) and build a Naive Bayes classifier. 2.2 Data Both verbs, to make and to do, license complements of various kinds, e. g. they can be mono-transitive, ditransitive, and complex transitive (sentences 1, 2, and 3, respectively). Furthermore, make can be part of idiomatic ditransitives (e.g., make use of, make fun of, make room for) and phrasal mono-transitives (e.g., make up) . 1. Andrew made [a cake]dobj. 2. Andrew made [his mum]zobj [a cake]dobj. 3. Andrew made [his mum]dobj </context>
</contexts>
<marker>Zellig, 1954</marker>
<rawString>Harris Zellig. 1954. Distributional Structure. Word 10 (2/3), p. 146-62.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>