<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003810">
<title confidence="0.9972095">
The Hinoki Sensebank
— A Large-Scale Word Sense Tagged Corpus of Japanese —
</title>
<author confidence="0.935123">
Takaaki Tanaka, Francis Bond and Sanae Fujita
</author>
<email confidence="0.917555">
{takaaki,bond,fujita}@cslab.kecl.ntt.co.jp
</email>
<note confidence="0.5386715">
NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
</note>
<sectionHeader confidence="0.966363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938176470588">
Semantic information is important for
precise word sense disambiguation system
and the kind of semantic analysis used in
sophisticated natural language processing
such as machine translation, question
answering, etc. There are at least two
kinds of semantic information: lexical
semantics for words and phrases and
structural semantics for phrases and
sentences.
We have built a Japanese corpus of over
three million words with both lexical
and structural semantic information. In
this paper, we focus on our method of
annotating the lexical semantics, that is
building a word sense tagged corpus and
its properties.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999509347826087">
While there has been considerable research on
both structural annotation (such as the Penn
Treebank (Taylor et al., 2003) or the Kyoto Corpus
(Kurohashi and Nagao, 2003)) and semantic
annotation (e.g. Senseval: Kilgariff and
Rosenzweig, 2000; Shirai, 2002), there are almost
no corpora that combine both. This makes it
difficult to carry out research on the interaction
between syntax and semantics.
Projects such as the Penn Propbank are adding
structural semantics (i.e. predicate argument
structure) to syntactically annotated corpora,
but not lexical semantic information (i.e. word
senses). Other corpora, such as the English
Redwoods Corpus (Oepen et al., 2002), combine
both syntactic and structural semantics in a
monostratal representation, but still have no
lexical semantics.
In this paper we discuss the (lexical) semantic
annotation for the Hinoki Corpus, which is
part of a larger project in psycho-linguistic
and computational linguistics ultimately aimed at
language understanding (Bond et al., 2004).
</bodyText>
<sectionHeader confidence="0.979787" genericHeader="method">
2 Corpus Design
</sectionHeader>
<bodyText confidence="0.999960666666667">
In this section we describe the overall design of
the corpus, and is constituent corpora. The basic
aim is to combine structural semantic and lexical
semantic markup in a single corpus. In order to
make the first phase self contained, we started
with dictionary definition and example sentences.
We are currently adding other genre, to make the
langauge description more general, starting with
newspaper text.
</bodyText>
<subsectionHeader confidence="0.958874">
2.1 Lexeed: A Japanese Basic Lexicon
</subsectionHeader>
<bodyText confidence="0.99962868">
We use word sense definitions from Lexeed:
A Japanese Semantic Lexicon (Kasahara et al.,
2004). It was built in a series of psycholinguistic
experiments where words from two existing
machine-readable dictionaries were presented to
subjects and they were asked to rank them on a
familiarity scale from one to seven, with seven
being the most familiar (Amano and Kondo,
1999). Lexeed consists of all words with a
familiarity greater than or equal to five. There
are 28,000 words in all. Many words have
multiple senses, there were 46,347 different
senses. Definition sentences for these sentences
were rewritten to use only the 28,000 familiar
words. In the final configuration, 16,900 different
words (60% of all possible words) were actually
used in the definition sentences. An example
entry for the wordF7��—doraib¯a “driver”
is given in Figure 1, with English glosses added.
This figure includes the sense annotation and
information derived from it that is described in this
paper.
Table 1 shows the relation between polysemy
and familiarity. The #WS column indicates the
average number of word senses that polysemous
</bodyText>
<page confidence="0.990375">
62
</page>
<note confidence="0.725931">
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 62–69,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<equation confidence="0.996841371428571">
⎡ ⎤
INDEX À{§�doraiba-
⎢⎥
⎢POS noun Lexical-Type noun-lex ⎥
⎢ ⎥
⎢ ⎥
⎢FAMILIARITY 6.5 [1–7] (&gt; 5) Frequency 37 Entropy 0.79 ⎥
⎢ ⎡ ⎤ ⎥
DEFINITIONF11/k/Z0 Ve1/89/,/�&amp;�&lt;1/8�/2d/��1/�
⎢ ⎥
⎢ ⎢ ⎢ ⎥
⎥
a tool for inserting and removing screws. ⎥
⎢ ⎢ ⎥
⎢ SENSE 1 ⎢ ⎥ ⎥ ⎥
EXAMPLE flyH100 À{§_2WaGF1kNZ8�
⎢ ⎢ ⎥
⎢⎢ ⎥ ⎥
⎢ (0.11) ⎢ ⎥
he used a small screwdriver to tighten the screws on his glasses. ⎥
⎢ ⎢ ⎥ ⎥
⎢ ⎢ ⎥ ⎥
HYPERNYM R1 equipment “tool”
⎢ ⎢ ⎥ ⎥
⎢ ⎢ ⎥ ⎥ ⎥
⎢ ⎣SEM. CLASS (942:tool/implement) (C (893:equipment)) ⎦ ⎥
⎢ ⎥
⎢WORDNET screwdriver1 ⎥
⎢ ⎥
⎢ ⎡ ⎤ ⎥
⎢ ⎥
DEFINITIONnR*1/k/AU1/2d/),,1/o
⎢ ⎥
⎢ ⎢ ⎥ ⎥
⎢Someone who drives a car.
SENSE 2⎢ ⎢ ⎥ ⎥
EXAMPLE XH&apos;(&apos;*P_C À{§—A0?*4i.e8�
⎢ ⎢ ⎥ ⎥
⎢⎢ ⎥ ⎥
⎢ (0.84) ⎢ ⎥
⎢ ⎢my father was given an award as a good driver. ⎥ ⎥ ⎥
⎢ ⎢ ⎥ ⎥
HYPERNYM),,1 hito “person”
⎢ ⎢ ⎥ ⎥
⎢ ⎢ ⎥ ⎥
⎢ ⎣SEM. CLASS (292:chauffeur/driver) (C (5:person)) ⎦ ⎥
⎢ ⎥
⎢ ⎥
⎢WORDNET driver1 ⎥
⎢ ⎡ ⎤ ⎥
⎢ ⎥
DEFINITION‹Â¬1/Z&apos;/,/AVF 1/m/G/†À­3/0��/}š/�
⎢ ⎥
⎢ ⎢ ⎥ ⎥
⎢ ⎢In golf, a long-distance club. A number one wood. ⎥ ⎥
⎢ ⎢ ⎥ ⎥
⎢SENSE 3⎢⎥⎥
EXAMPLE fly H À{ §— 2C Î Ë Ë »— AI 0 8 ⎢ ⎥ ⎥
⎢ ⎢ ⎥ ⎥
⎢he hit (it) 30 yards with the driver.
(0.05)⎢ ⎢ ⎥ ⎥
⎢ ⎢ ⎥
HYPERNYM†À-3 kurabu “club” ⎥
⎢ ⎢ ⎥ ⎥
⎢ ⎢
⎢ ⎢SEM. CLASS (921:leisure equipment) (C 921) ⎥ ⎥
⎥ ⎥
⎢ ⎢ ⎥ ⎥
⎣ ⎣WORDNET driver5 ⎦ ⎦
DOMAIN ‹Â¬1 gorufu “golf”
</equation>
<figureCaption confidence="0.994407">
Figure 1: Entry for the Word doraib¯a “driver” (with English glosses)
</figureCaption>
<bodyText confidence="0.9336928">
words have. Lower familiarity words tend to
have less ambiguity and 70 % of words with a
familiarity of less than 5.5 are monosemous. Most
polysemous words have only two or three senses
as seen in Table 2.
</bodyText>
<table confidence="0.996937666666667">
Fam #Words Poly- #WS #Mono-
semous semous(%)
6.5 - 368 182 4.0 186 (50.5)
6.0 - 4,445 1,902 3.4 2,543 (57.2)
5.5 - 9,814 3,502 2.7 6,312 (64.3)
5.0 - 11,430 3,457 2.5 7,973 (69.8)
</table>
<tableCaption confidence="0.999862">
Table 1: Familiarity vs Word Sense Ambiguity
</tableCaption>
<subsectionHeader confidence="0.983885">
2.2 Ontology
</subsectionHeader>
<bodyText confidence="0.9998956">
We also have an ontology built from the parse
results of definitions in Lexeed (Nichols and Bond,
2005). The ontology includes more than 50
thousand relationship between word senses, e.g.
synonym, hypernym, abbreviation, etc.
</bodyText>
<subsectionHeader confidence="0.994088">
2.3 Goi-Taikei
</subsectionHeader>
<bodyText confidence="0.895075">
As part of the ontology verification, all nominal
and most verbal word senses in Lexeed were
linked to semantic classes in the Japanese
thesaurus, Nihongo Goi-Taikei (Ikehara et al.,
1997). Common nouns are classified into about
2,700 semantic classes which are organized into a
</bodyText>
<figure confidence="0.998403954545454">
#WS #Words
1 18460
2 6212
3 2040
4 799
5 311
6 187
7 99
8 53
9 35
10 15
11 19
12 13
13 13
14 6
15 6
16 3
17 2
18 3
19 1
20 2
&gt; 21 19
</figure>
<tableCaption confidence="0.566751">
Table 2: Number of Word Senses
</tableCaption>
<page confidence="0.990098">
63
</page>
<bodyText confidence="0.94932">
semantic hierarchy.
</bodyText>
<subsectionHeader confidence="0.997476">
2.4 Hinoki Treebank
</subsectionHeader>
<bodyText confidence="0.999984">
Lexeed definition and example sentences are
syntactically and semantically parsed with HPSG
and correct results are manually selected (Tanaka
et al., 2005). The grammatical coverage over all
sentences is 86%. Around 12% of the parsed
sentences were rejected by the treebankers due
to an incomplete semantic representation. This
process had been done independently of word
sense annotation.
</bodyText>
<subsectionHeader confidence="0.992302">
2.5 Target Corpora
</subsectionHeader>
<bodyText confidence="0.999194044444444">
We chose two types of corpus to mark up: a
dictionary and two newspapers. Table 3 shows
basic statistics of the target corpora.
The dictionary Lexeed, which defined word
senses, is also used for a target for sense tagging.
Its definition (LXD-DEF) and example (LXD-EX)
sentences consist of basic words and function
words only, i.e. it is self-contained. Therefore,
all content words have headwords in Lexeed, and
all word senses appear in at least one example
sentence.
Both newspaper corpora where taken from the
Mainichi Daily News. One sample (Senseval2)
was the text used for the Japanese dictionary
task in Senseval-2 (Shirai, 2002), which has some
words marked up with word sense tags defined
in the Iwanami lexicon (Nishio et al., 1994).
The second sample was those sentences used in
the Kyoto Corpus (Kyoto), which is marked up
with dependency analyses (Kurohashi and Nagao,
2003). We chose these corpora so that we can
compare our annotation with existing annotation.
Both these corpora were thus already segmented
and annotated with parts-of-speech. However,
they used different morphological analyzers to
the one used in Lexeed, so we had to do some
remapping. E.g. in Kyoto the copula is not split
from nominal-adjectives, whereas in Lexeed it is:
TE%t�genkida “lively” vsT%t�genki da. This
could be done automatically after we had written
a few rules.
Although the newspapers contain many words
other than basic words, only basic words have
sense tags. Also, a word unit in the newspapers
does not necessarily coincide with the headword
in Lexeed since part-of-speech taggers used for
annotation are different. We do not adjust the word
segmentation and leave it untagged at this stage,
even if it is a part of a basic word or consists of
multiple basic words. For instance, Lexeed has the
compound entry`V%�kahei-kachi “monetary
value”, however, this word is split into two basic
words in the corpora. In this case, both two words
kahei “money” and%*kachi “value” are
tagged individually.
</bodyText>
<table confidence="0.999621666666667">
Corpus Tokens Content Basic %Mono-
Words Words semous
LXD-DEF 691,072 318,181 318,181 31.7
LXD-EX 498,977 221,224 221,224 30.5
Senseval2 888,000 692,069 391,010 39.3
Kyoto 969,558 526,760 472,419 36.3
</table>
<tableCaption confidence="0.998982">
Table 3: Corpus Statistics
</tableCaption>
<bodyText confidence="0.998200375">
The corpora are not fully balanced, but
allow some interesting comparisons. There are
effectively three genres: dictionary definitions,
which tend to be fragments and are often
syntactically highly ambiguous; dictionary
example sentences, which tend to be short
complete sentences, and are easy to parse; and
newspaper text from two different years.
</bodyText>
<sectionHeader confidence="0.997461" genericHeader="method">
3 Annotation
</sectionHeader>
<bodyText confidence="0.99995052631579">
Each word was annotated by five annotators.
We actually used 15 annotators, divided into 3
groups. None were professional linguists or
lexicographers. All of them had a score above
60 on a Chinese character based vocabulary test
(Amano and Kondo, 1998). We used multiple
annotators to measure the confidence of tags and
the degree of difficulty in identifying senses.
The target words for sense annotation are
the 9,835 headwords having multiple senses in
Lexeed (§ 2.1). They have 28,300 senses in
all. Monosemous words were not annotated.
Annotation was done word by word. Annotators
are presented multiple sentences (up to 50) that
contain the same target word, and they keep
tagging that word until occurrences are done. This
enables them to compare various contexts where
a target word appears and helps them to keep the
annotation consistent.
</bodyText>
<subsectionHeader confidence="0.895978">
3.1 Tool
</subsectionHeader>
<figureCaption confidence="0.8612106">
A screen shot of the annotation tool is given in
Figure 2. The interface uses frames on a browser,
with all information stored in SQL tables. The left
hand frame lists the words being annotated. Each
word is shown with some context: the surrounding
</figureCaption>
<page confidence="0.996205">
64
</page>
<bodyText confidence="0.999972705882353">
paragraph, and the headword for definition and
example sentences. These can be clicked on to
get more context. The word being annotated is
highlighted in red. For each word, the annotator
chooses its senses or one or more of the other tags
as clickable buttons. It is also possible to choose
one tag as the default for all entries on the screen.
The right hand side frame has the dictionary
definitions for the word being tagged in the top
frame, and a lower frame with instructions. A
single word may be annotated with senses from
more than one headword. For example§•is
divided into two headwords basu “bus” and basu
“bass”, both of which are presented.
As we used a tab-capable browser, it was easy
for the annotators to call up more information in
different tabs. This proved to be quite popular.
</bodyText>
<subsectionHeader confidence="0.984891">
3.2 Markup
</subsectionHeader>
<bodyText confidence="0.99925216">
Annotators choose the most suitable sense in the
given context from the senses that the word have
in lexicon. Preferably, they select a single sense
for a word, although they can mark up multiple
tags if the words have multiple meanings or are
truly ambiguous in the contexts.
When they cannot choose a sense in some
reasons, they choose one or more of the following
special tags.
o other sense: an appropriate sense is not found
in a lexicon. Relatively novel concepts (e.g.
À{§—doraib¯a “driver” for “software
driver”) are given this tag.
c multiword expressions (compound/ idiom): the
target word is a part of a non-compositional
compound or idiom.
p proper noun: the word is a proper noun.
z homonym: an appropriate entry is not found
in a lexicon, because a target is different
from head words in a lexicon (e.g. only a
headword§•bass “bus” is present in a
lexicon for§•basu “bass”).
e analysis error: the word segmentation or part-
of-speech is incorrect due to errors in pre-
annotation of the corpus.
</bodyText>
<subsectionHeader confidence="0.974927">
3.3 Feedback
</subsectionHeader>
<bodyText confidence="0.999929025641026">
One of the things that the annotators found hard
was not knowing how well they were doing. As
they were creating a gold standard, there was
initially no way of knowing how correct they were.
We also did not know at the start of the annotation
how fast senses could or should be annotated (a
test of the tool gave us an initial estimate of around
400 tokens/day).
To answer these questions, and to provide
feedback for the annotators, twice a day we
calculated and graphed the speed (in words/day)
and majority agreement (how often an annotator
agrees with the majority of annotators for each
token, measured over all words annotated so
far). Each annotator could see a graph with
their results labelled, and the other annotators
made anonymous. The results are grouped into
three groups of five annotators. Each group
is annotating a different set of words, but we
included them all in the feedback. The order
within each group is sorted by agreement, as we
wished to emphasise the importance of agreement
over speed. An example of a graph is given
in Figure 3. When this feedback was given,
this particular annotator has the second worst
agreement score in their subgroup (90.27%) and is
reasonably fast (1799 words/day) — they should
slow down and think more.
The annotators welcomed this feedback, and
complained when our script failed to produce it.
There was an enormous variation in speed: the
fastest annotator was 4 times as fast as the slowest,
with no appreciable difference in agreement.
After providing the feedback, the average speed
increased considerably, as the slowest annotators
agonized less over their decisions. The final
average speed was around 1,500 tokens/day, with
the fastest annotator still almost twice as fast as the
slowest.
</bodyText>
<sectionHeader confidence="0.999741" genericHeader="method">
4 Inter-Annotator Agreement
</sectionHeader>
<bodyText confidence="0.999541444444444">
We employ inter-annotator agreement as our core
measure of annotation consistency, in the same
way we did for treebank evaluation (Tanaka et al.,
2005). This agreement is calculated as the average
of pairwise agreement. Let wi be a word in a set of
content words W and wi, j be the jth occurrence of
a word wi. Average pairwise agreement between
the sense tags of wi, j each pair of annotators
marked up a(wi, j) is:
</bodyText>
<equation confidence="0.987108333333333">
∑ k (mi, j(sik)C2)
a(wi,j) � (1)
nwi,j C2
</equation>
<bodyText confidence="0.9999695">
where nwi,j(≥ 2) is the number of annotators that
tag the word wi,j, and mi,j(sik) is the number
</bodyText>
<page confidence="0.999556">
65
</page>
<figureCaption confidence="0.9999725">
Figure 2: Sense Annotation tool (wordM&lt;shibaraku “briefly”)
Figure 3: Sample feedback provided to an annotator
</figureCaption>
<bodyText confidence="0.963634333333333">
of sense tags sik for the word wi,j. Hence, the
agreement of the word wi is the average of awi,j
over all occurrences in a corpus:
</bodyText>
<equation confidence="0.956665666666667">
∑ j a(wi,j)
a(wi) � (2)
Nwi
</equation>
<page confidence="0.872055">
66
</page>
<bodyText confidence="0.9999696">
where NwZ is the frequency of the word wZ in a
corpus.
Table 4 shows statistics about the annotation
results. The average numbers of word senses in
the newspapers are lower than the ones in the
dictionary and, therefore, the token agreement
of the newspapers is higher than those of the
dictionary sentences. %Unanimous indicates the
ratio of tokens vs types for which all annotators
(normally five) choose the same sense. Snyder
and Palmer (2004) report 62% of all word types
on the English all-words task at SENSEVAL-3
were labelled unanimously. It is hard to directly
compare with our task since their corpus has only
2,212 words tagged by two or three annotators.
</bodyText>
<subsectionHeader confidence="0.936719">
4.1 Familiarity
</subsectionHeader>
<bodyText confidence="0.9997106">
As seen in Table 5, the agreement per type
does not vary much by familiarity. This was
an unexpected result. Even though the average
polysemy is high, there are still many highly
familiar words with very good agreement.
</bodyText>
<table confidence="0.999348">
Fam Agreement #WS %Monosem
token (type)
6.5 - .723 (.846) 7.00 22.6
6.0 - .780 (.846) 5.82 28.0
5.5 - .813 (.853) 3.79 42.4
5.0 - .821 (.850) 3.84 46.2
ALL .787 (.850) 5.18 34.5
</table>
<tableCaption confidence="0.998613">
Table 5: Inter-Annotator Agreement (LXD-DEF)
</tableCaption>
<subsectionHeader confidence="0.997098">
4.2 Part-of-Speech
</subsectionHeader>
<bodyText confidence="0.999990769230769">
Table 6 shows the agreement according to part of
speech. Nouns and verbal nouns (vn) have the
highest agreements, similar to the results for the
English all-words task at SENSEVAL-3 (Snyder
and Palmer, 2004). In contrast, adjectives have as
low agreement as verbs, although the agreement
of adjectives was the highest and that of verbs
was the lowest in English. This partly reflects
differences in the part of speech divisions between
Japanese and English. Adjectives in Japanese are
much close in behaviour to verbs (e.g. they can
head sentences) and includes many words that are
translated as verbs in English.
</bodyText>
<subsectionHeader confidence="0.968463">
4.3 Entropy
</subsectionHeader>
<bodyText confidence="0.9678515">
Entropy is directly related to the difficulty in
identifing senses as shown in Table 7.
</bodyText>
<table confidence="0.998036142857143">
POS Agreement (type) #WS %Monosemous
n .803 (.851) 2.86 62.9
v .772 (.844) 3.65 34.0
vn .849 (.865) 2.54 61.0
adj .770 (.810) 3.58 48.3
adv .648 (.833) 3.08 46.4
others .615 (.789) 3.19 50.8
</table>
<tableCaption confidence="0.8116165">
Table 6: POS vs Inter-Annotator Agreement (LXD-
DEF)
</tableCaption>
<table confidence="0.999771">
Entropy Agreement (type) #Words #WS
2 - .672 84 14.2
1 - .758 1096 4.38
0.5 - .809 1627 2.88
0.05 - .891 495 3.19
0 - .890 13778 2.56
</table>
<tableCaption confidence="0.999094">
Table 7: Entropy vs Agreement
</tableCaption>
<subsectionHeader confidence="0.997963">
4.4 Sense Lumping
</subsectionHeader>
<bodyText confidence="0.999985344827586">
Low agreement words have some senses that are
difficult to distinguish from each other: these
senses often have the same hypernyms. For
example, the agreement rate ofAI&apos;Lkusabana
“grass/flower” in LXD-DEF is only 33.7 %.
It has three senses whose semantic class is
similar: kusabana1 “flower that blooms in grass”,
kusabana2 “grass that has flowers” and souka1
“grass and flowers” (hypernyms flower1, grass1
and flower1 &amp; grass1 respectively).
In order to investigate the effect of semantic
similarity on agreement, we lumped similar word
senses based on hypernym and semantic class.
We use hypernyms from the ontology (§ 2.1) and
semantic classes in Goi-Taikei (§ 2.3), to regard
the word senses that have the same hypernyms or
belong to the same semantic classes as the same
senses.
Table 8 shows the distribution after sense
lumping. Table 9 shows the agreement with
lumped senses. Note that this was done with an
automatically derived ontology that has not been
fully hand corrected.
As is expected, the overall agreement increased,
from 0.787 to 0.829 using the ontology, and
to 0.835 using the coarse-grained Goi-Taikei
semantic classes. For many applications, we
expect that this level of disambiguation is all that
is required.
</bodyText>
<subsectionHeader confidence="0.976113">
4.5 Special Tags
</subsectionHeader>
<bodyText confidence="0.9631845">
Table 10 shows the ratio of special tags and
multiple tags to all tags. These results show
</bodyText>
<page confidence="0.99879">
67
</page>
<table confidence="0.999609333333333">
Annotated Agreement %Unanimous
Corpus Tokens #WS token (type) token (type) Kappa
LXD-DEF 199,268 5.18 .787 (.850) 62.8 (41.1) 0.58
LXD-EX 126,966 5.00 .820 (.871) 69.1 (53.2) 0.65
Senseval2 223,983 4.07 .832 (.833) 73.9 (45.8) 0.52
Kyoto 268,597 3.93 .833 (.828) 71.5 (46.1) 0.50
</table>
<tableCaption confidence="0.798397">
Table 4: Basic Annotation Statistics
</tableCaption>
<table confidence="0.9997414">
Corpus %Other Sense %MWE %Homonym %Proper Noun %Error %Multiple Tags
LXD-DEF 4.2 1.5 0.084 0.046 0.92 11.9
LXD-EX 2.3 0.44 0.035 0.0018 0.43 11.6
Senseval2 9.3 5.6 4.1 8.7 5.7 7.9
Kyoto 9.8 7.9 3.3 9.0 5.5 9.3
</table>
<tableCaption confidence="0.96814">
Table 10: Special Tags and Multiple Tags
</tableCaption>
<table confidence="0.999214611111111">
Fam Agreement #WS %Monosem
token (type)
6.5 - .772 (.863) 6.37 25.6
6.0 - .830 (.868) 5.16 31.5
5.5 - .836 (.872) 3.50 45.6
5.0 - .863 (.866) 3.76 58.7
ALL .829 (.869) 4.72 39.1
Lumping together Hypernyms
(4,380 senses compressed into 1,900 senses)
Fam Agreement #WS %Monosem
token (type)
6.5 - .775 (.890) 6.05 26.8
6.0 - .835 (.891) 4.94 36.4
5.5 - .855 (.894) 3.29 50.6
5.0 - .852 (.888) 3.46 49.9
ALL .835 (.891) 4.48 41.7
Lumping together Semantic Classes
(8,691 senses compressed into 4,030 senses)
</table>
<tableCaption confidence="0.941135">
Table 8: Sense Lumping Results (LXD-DEF)
</tableCaption>
<table confidence="0.9803721">
(LXD-DEF) Agreement #WS %Monosem
token (type)
no lumping .698 (.816) 8.81 0.0
lumping .811 (.910) 8.24 20.0
Hypernum Lumping
Agreement
(LXD-DEF) token (type) #WS %Monosem
no lumping .751 (.814) 7.09 0.0
lumping .840 (.925) 5.99 21.9
Semantic Class Lumping
</table>
<tableCaption confidence="0.994782">
Table 9: Lumped Sense Agreement (LXD-DEF)
</tableCaption>
<bodyText confidence="0.999805222222222">
the differences in corpus characteristics between
dictionary and newspaper. The higher ratios of
Other Sense and Homonym at newspapers indicate
that the words whose surface form is in a
dictionary are frequently used for the different
meanings in real text, e.g.�.Rgin “silver” is
used for the abbrebiation of€Rffginkou “bank”.
%Multiple Tags is the percentage of tokens for
which at least one annotator marked multiple tags.
</bodyText>
<sectionHeader confidence="0.999941" genericHeader="method">
5 Discussion
</sectionHeader>
<subsectionHeader confidence="0.998697">
5.1 Comparison with Senseval-2 corpus
</subsectionHeader>
<bodyText confidence="0.999934466666667">
The Senseval-2 Japanese dictionary task
annotation used senses from a different dictionary
(Shirai, 2002). In the evaluation, 100 test words
were selected from three groups with different
entropy bands (Kurohashi and Shirai, 2001). Da
is the highest entropy group, which contains the
most hard to tag words, and Dc is the lowest
entropy group.
We compare our results with theirs in Table 11.
The Senseval-2 agreement figures are slightly
higher than our overall. However, it is impossible
to make a direct comparison as the numbers of
annotators (two or three annotators in Senseval vs
more than 5 annotators in our work) and the sense
inventories are different.
</bodyText>
<subsectionHeader confidence="0.98469">
5.2 Problems
</subsectionHeader>
<bodyText confidence="0.999941294117647">
Two main problems came up when building
the corpora: word segmentation and sense
segmentation. Multiword expressions like
compounds and idioms are tied closely to both
problems.
The word segmentation is the problem of how
to determine an unit expressing a meaning. At
the present stage, it is based on headword in
Lexeed, in particular, only compounds in Lexeed
are recognized, we do not discriminate non-
decomposable compounds with decomposable
ones. However, if the headword unit in the
dictionary is inconsistent, word sense tagging
inherits this problem. For examples,-pfdichibu
has two main usage: one + classifier and a part
of something. Lexeed has an entry including both
two senses. However, the former is split into two
</bodyText>
<page confidence="0.998277">
68
</page>
<table confidence="0.999346625">
POS Da Db Dc Total
Hinoki Senseval Hinoki Senseval Hinoki Senseval Hinoki Senseval
noun .768 .809 .784 .786 .848 .957 .806 .859
14.4 13.1 5.0 4.1 3.1 3.8 5.9 5.1
verb .660 .699 .722 .896 .738 .867 .723 .867
16.7 21.8 10.3 9.3 5.2 5.9 9.6 10.9
total .710 .754 .760 .841 .831 .939 .768 .863
15.6 18.8 7.0 6.2 4.2 4.9 7.6 7.9
</table>
<tableCaption confidence="0.9396785">
Table 11: Comparison of Agreement for the Senseval-2 Lexical Sample Task Corpus ( upper row:
agreement, lower row: the number of word senses)
</tableCaption>
<bodyText confidence="0.99759775">
words by our morphological analyser in the same
way as other numeral + classifier.
The second problem is how to mark off
metaphorical meaning from literal meanings.
Currently, this also depends on the Lexeed
definition and it is not necessarily consistent
either. Some words in institutional idioms (Sag
et al., 2002) have the idiom sense in the lexicon
while most words do not. For instance,Yr
shippo “tail of animal”) has a sense for the reading
“weak point” in an idiom Y r � IN t shippo-o
tsukamu “lit. to grasp the tail, idiom. to find one’s
weak point”, while ff ase “sweat” does not have
a sense for the applicable meaning in the idiom ff
� j,41L�t ase-o nagasu “lit. to sweat, idiom, to work
hard”.
</bodyText>
<sectionHeader confidence="0.99973" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999893">
We built a corpus of over three million words
which has lexical semantic information. We are
currently using it to build a model for word sense
disambiguation.
</bodyText>
<sectionHeader confidence="0.983103" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9932445">
We thank the 15 sense annotators and 3 treebankers
(Takayuki Kuribayashi, Tomoko Hirata and Koji Yamashita).
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870411764706">
Anne Abeill´e, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer Academic Publishers.
Shigeaki Amano and Tadahisa Kondo. 1998. Estimation
of mental lexicon size with word familiarity database.
In International Conference on Spoken Language
Processing, volume 5, pages 2119–2122.
Shigeaki Amano and Tadahisa Kondo. 1999. Nihongo-no
Goi-Tokusei (Lexical properties ofJapanese). Sanseido.
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The
Hinoki treebank: A treebank for text understanding. In
Proceedings of the First International Joint Conference on
Natural Language Processing (IJCNLP-04), pages 554–
559. Hainan Island.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei
— A Japanese Lexicon. Iwanami Shoten, Tokyo. 5
volumes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic
lexicon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in
Japanese).
Adam Kilgariff and Joseph Rosenzweig. 2000. Framework
and results for English SENSEVAL. Computers and
the Humanities, 34(1–2):15–48. Special Issue on
SENSEVAL.
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus — while improving the parsing
system. In Abeill´e (2003), chapter 14, pages 249–260.
Sadao Kurohashi and Kiyoaki Shirai. 2001. SENSEVAL-2
Japanese task. SIG NLC 2001-10, IEICE. (in Japanese).
Eric Nichols and Francis Bond. 2005. Acquiring ontologies
using deep and shallow processing. In 11th Annual
Meeting of the Association for Natural Language
Processing, pages 494–498. Takamatsu.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami
Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo.
(in Japanese).
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christoper D. Manning, Dan Flickinger, and Thorsten
Brant. 2002. The LinGO redwoods treebank: Motivation
and preliminary applications. In 19th International
Conference on Computational Linguistics: COLING-
2002, pages 1253–7. Taipei, Taiwan.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions: A
pain in the neck for NLP. In Alexander Gelbuk,
editor, Computational Linguistics and Intelligent Text
Processing: Third International Conference: CICLing-
2002, pages 1–15. Springer-Verlag, Hiedelberg/Berlin.
Kiyoaki Shirai. 2002. Construction of a word sense tagged
corpus for SENSEVAL-2 Japanese dictionary task. In
Third International Conference on Language Resources
and Evaluation (LREC-2002), pages 605–608.
Benjamin Snyder and Martha Palmer. 2004. The English all-
words task. In Proceedings of Senseval-3, pages 41–44.
ACL, Barcelona.
Takaaki Tanaka, Francis Bond, Stephan Oepen, and Sanae
Fujita. 2005. High precision treebanking – blazing useful
trees using POS information. In ACL-2005, pages 330–
337.
Ann Taylor, Mitchel Marcus, and Beatrice Santorini. 2003.
The Penn treebank: an overview. In Abeill´e (2003),
chapter 1, pages 5–22.
</reference>
<page confidence="0.999316">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828367">
<title confidence="0.9874555">The Hinoki Sensebank — A Large-Scale Word Sense Tagged Corpus of Japanese —</title>
<author confidence="0.895518">Takaaki Tanaka</author>
<author confidence="0.895518">Francis Bond</author>
<author confidence="0.895518">Sanae</author>
<affiliation confidence="0.9034665">NTT Communication Science Nippon Telegraph and Telephone Corporation</affiliation>
<abstract confidence="0.999670277777778">Semantic information is important for precise word sense disambiguation system and the kind of semantic analysis used in sophisticated natural language processing such as machine translation, question answering, etc. There are at least two kinds of semantic information: lexical semantics for words and phrases and structural semantics for phrases and sentences. We have built a Japanese corpus of over three million words with both lexical and structural semantic information. In this paper, we focus on our method of annotating the lexical semantics, that is building a word sense tagged corpus and its properties.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Treebanks: Building and Using Parsed Corpora.</title>
<date>2003</date>
<editor>Anne Abeill´e, editor.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>2003</marker>
<rawString>Anne Abeill´e, editor. 2003. Treebanks: Building and Using Parsed Corpora. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shigeaki Amano</author>
<author>Tadahisa Kondo</author>
</authors>
<title>Estimation of mental lexicon size with word familiarity database.</title>
<date>1998</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<volume>5</volume>
<pages>2119--2122</pages>
<contexts>
<context position="9446" citStr="Amano and Kondo, 1998" startWordPosition="1581" endWordPosition="1584">istics The corpora are not fully balanced, but allow some interesting comparisons. There are effectively three genres: dictionary definitions, which tend to be fragments and are often syntactically highly ambiguous; dictionary example sentences, which tend to be short complete sentences, and are easy to parse; and newspaper text from two different years. 3 Annotation Each word was annotated by five annotators. We actually used 15 annotators, divided into 3 groups. None were professional linguists or lexicographers. All of them had a score above 60 on a Chinese character based vocabulary test (Amano and Kondo, 1998). We used multiple annotators to measure the confidence of tags and the degree of difficulty in identifying senses. The target words for sense annotation are the 9,835 headwords having multiple senses in Lexeed (§ 2.1). They have 28,300 senses in all. Monosemous words were not annotated. Annotation was done word by word. Annotators are presented multiple sentences (up to 50) that contain the same target word, and they keep tagging that word until occurrences are done. This enables them to compare various contexts where a target word appears and helps them to keep the annotation consistent. 3.1</context>
</contexts>
<marker>Amano, Kondo, 1998</marker>
<rawString>Shigeaki Amano and Tadahisa Kondo. 1998. Estimation of mental lexicon size with word familiarity database. In International Conference on Spoken Language Processing, volume 5, pages 2119–2122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shigeaki Amano</author>
<author>Tadahisa Kondo</author>
</authors>
<title>Nihongo-no Goi-Tokusei (Lexical properties ofJapanese).</title>
<date>1999</date>
<publisher>Sanseido.</publisher>
<contexts>
<context position="2743" citStr="Amano and Kondo, 1999" startWordPosition="405" endWordPosition="408">make the first phase self contained, we started with dictionary definition and example sentences. We are currently adding other genre, to make the langauge description more general, starting with newspaper text. 2.1 Lexeed: A Japanese Basic Lexicon We use word sense definitions from Lexeed: A Japanese Semantic Lexicon (Kasahara et al., 2004). It was built in a series of psycholinguistic experiments where words from two existing machine-readable dictionaries were presented to subjects and they were asked to rank them on a familiarity scale from one to seven, with seven being the most familiar (Amano and Kondo, 1999). Lexeed consists of all words with a familiarity greater than or equal to five. There are 28,000 words in all. Many words have multiple senses, there were 46,347 different senses. Definition sentences for these sentences were rewritten to use only the 28,000 familiar words. In the final configuration, 16,900 different words (60% of all possible words) were actually used in the definition sentences. An example entry for the wordF7��—doraib¯a “driver” is given in Figure 1, with English glosses added. This figure includes the sense annotation and information derived from it that is described in </context>
</contexts>
<marker>Amano, Kondo, 1999</marker>
<rawString>Shigeaki Amano and Tadahisa Kondo. 1999. Nihongo-no Goi-Tokusei (Lexical properties ofJapanese). Sanseido.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Sanae Fujita</author>
</authors>
<title>Chikara Hashimoto, Kaname Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani, Takaaki Tanaka, and Shigeaki Amano.</title>
<date>2004</date>
<booktitle>In Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP-04),</booktitle>
<pages>554--559</pages>
<location>Hainan Island.</location>
<marker>Bond, Fujita, 2004</marker>
<rawString>Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani, Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki treebank: A treebank for text understanding. In Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP-04), pages 554– 559. Hainan Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Masahiro Miyazaki</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1997</date>
<booktitle>Goi-Taikei — A Japanese Lexicon. Iwanami Shoten,</booktitle>
<volume>5</volume>
<pages>volumes/CDROM.</pages>
<location>Tokyo.</location>
<contexts>
<context position="5895" citStr="Ikehara et al., 1997" startWordPosition="1000" endWordPosition="1003">.5 - 368 182 4.0 186 (50.5) 6.0 - 4,445 1,902 3.4 2,543 (57.2) 5.5 - 9,814 3,502 2.7 6,312 (64.3) 5.0 - 11,430 3,457 2.5 7,973 (69.8) Table 1: Familiarity vs Word Sense Ambiguity 2.2 Ontology We also have an ontology built from the parse results of definitions in Lexeed (Nichols and Bond, 2005). The ontology includes more than 50 thousand relationship between word senses, e.g. synonym, hypernym, abbreviation, etc. 2.3 Goi-Taikei As part of the ontology verification, all nominal and most verbal word senses in Lexeed were linked to semantic classes in the Japanese thesaurus, Nihongo Goi-Taikei (Ikehara et al., 1997). Common nouns are classified into about 2,700 semantic classes which are organized into a #WS #Words 1 18460 2 6212 3 2040 4 799 5 311 6 187 7 99 8 53 9 35 10 15 11 19 12 13 13 13 14 6 15 6 16 3 17 2 18 3 19 1 20 2 &gt; 21 19 Table 2: Number of Word Senses 63 semantic hierarchy. 2.4 Hinoki Treebank Lexeed definition and example sentences are syntactically and semantically parsed with HPSG and correct results are manually selected (Tanaka et al., 2005). The grammatical coverage over all sentences is 86%. Around 12% of the parsed sentences were rejected by the treebankers due to an incomplete sema</context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei — A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 volumes/CDROM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaname Kasahara</author>
<author>Hiroshi Sato</author>
<author>Francis Bond</author>
</authors>
<title>Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki Amano.</title>
<date>2004</date>
<booktitle>Construction of a Japanese semantic lexicon: Lexeed. SIG NLC-159, IPSJ,</booktitle>
<location>Tokyo.</location>
<note>(in Japanese).</note>
<contexts>
<context position="2464" citStr="Kasahara et al., 2004" startWordPosition="360" endWordPosition="363"> ultimately aimed at language understanding (Bond et al., 2004). 2 Corpus Design In this section we describe the overall design of the corpus, and is constituent corpora. The basic aim is to combine structural semantic and lexical semantic markup in a single corpus. In order to make the first phase self contained, we started with dictionary definition and example sentences. We are currently adding other genre, to make the langauge description more general, starting with newspaper text. 2.1 Lexeed: A Japanese Basic Lexicon We use word sense definitions from Lexeed: A Japanese Semantic Lexicon (Kasahara et al., 2004). It was built in a series of psycholinguistic experiments where words from two existing machine-readable dictionaries were presented to subjects and they were asked to rank them on a familiarity scale from one to seven, with seven being the most familiar (Amano and Kondo, 1999). Lexeed consists of all words with a familiarity greater than or equal to five. There are 28,000 words in all. Many words have multiple senses, there were 46,347 different senses. Definition sentences for these sentences were rewritten to use only the 28,000 familiar words. In the final configuration, 16,900 different </context>
</contexts>
<marker>Kasahara, Sato, Bond, 2004</marker>
<rawString>Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki Amano. 2004. Construction of a Japanese semantic lexicon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgariff</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Framework and results for English</title>
<date>2000</date>
<booktitle>SENSEVAL. Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<note>Special Issue on SENSEVAL.</note>
<contexts>
<context position="1130" citStr="Kilgariff and Rosenzweig, 2000" startWordPosition="158" endWordPosition="161">ds of semantic information: lexical semantics for words and phrases and structural semantics for phrases and sentences. We have built a Japanese corpus of over three million words with both lexical and structural semantic information. In this paper, we focus on our method of annotating the lexical semantics, that is building a word sense tagged corpus and its properties. 1 Introduction While there has been considerable research on both structural annotation (such as the Penn Treebank (Taylor et al., 2003) or the Kyoto Corpus (Kurohashi and Nagao, 2003)) and semantic annotation (e.g. Senseval: Kilgariff and Rosenzweig, 2000; Shirai, 2002), there are almost no corpora that combine both. This makes it difficult to carry out research on the interaction between syntax and semantics. Projects such as the Penn Propbank are adding structural semantics (i.e. predicate argument structure) to syntactically annotated corpora, but not lexical semantic information (i.e. word senses). Other corpora, such as the English Redwoods Corpus (Oepen et al., 2002), combine both syntactic and structural semantics in a monostratal representation, but still have no lexical semantics. In this paper we discuss the (lexical) semantic annota</context>
</contexts>
<marker>Kilgariff, Rosenzweig, 2000</marker>
<rawString>Adam Kilgariff and Joseph Rosenzweig. 2000. Framework and results for English SENSEVAL. Computers and the Humanities, 34(1–2):15–48. Special Issue on SENSEVAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Building a Japanese parsed corpus — while improving the parsing system.</title>
<date>2003</date>
<booktitle>In Abeill´e (2003), chapter 14,</booktitle>
<pages>249--260</pages>
<contexts>
<context position="1058" citStr="Kurohashi and Nagao, 2003" startWordPosition="149" endWordPosition="152">ine translation, question answering, etc. There are at least two kinds of semantic information: lexical semantics for words and phrases and structural semantics for phrases and sentences. We have built a Japanese corpus of over three million words with both lexical and structural semantic information. In this paper, we focus on our method of annotating the lexical semantics, that is building a word sense tagged corpus and its properties. 1 Introduction While there has been considerable research on both structural annotation (such as the Penn Treebank (Taylor et al., 2003) or the Kyoto Corpus (Kurohashi and Nagao, 2003)) and semantic annotation (e.g. Senseval: Kilgariff and Rosenzweig, 2000; Shirai, 2002), there are almost no corpora that combine both. This makes it difficult to carry out research on the interaction between syntax and semantics. Projects such as the Penn Propbank are adding structural semantics (i.e. predicate argument structure) to syntactically annotated corpora, but not lexical semantic information (i.e. word senses). Other corpora, such as the English Redwoods Corpus (Oepen et al., 2002), combine both syntactic and structural semantics in a monostratal representation, but still have no l</context>
<context position="7490" citStr="Kurohashi and Nagao, 2003" startWordPosition="1276" endWordPosition="1279">XD-EX) sentences consist of basic words and function words only, i.e. it is self-contained. Therefore, all content words have headwords in Lexeed, and all word senses appear in at least one example sentence. Both newspaper corpora where taken from the Mainichi Daily News. One sample (Senseval2) was the text used for the Japanese dictionary task in Senseval-2 (Shirai, 2002), which has some words marked up with word sense tags defined in the Iwanami lexicon (Nishio et al., 1994). The second sample was those sentences used in the Kyoto Corpus (Kyoto), which is marked up with dependency analyses (Kurohashi and Nagao, 2003). We chose these corpora so that we can compare our annotation with existing annotation. Both these corpora were thus already segmented and annotated with parts-of-speech. However, they used different morphological analyzers to the one used in Lexeed, so we had to do some remapping. E.g. in Kyoto the copula is not split from nominal-adjectives, whereas in Lexeed it is: TE%t�genkida “lively” vsT%t�genki da. This could be done automatically after we had written a few rules. Although the newspapers contain many words other than basic words, only basic words have sense tags. Also, a word unit in t</context>
</contexts>
<marker>Kurohashi, Nagao, 2003</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 2003. Building a Japanese parsed corpus — while improving the parsing system. In Abeill´e (2003), chapter 14, pages 249–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Kiyoaki Shirai</author>
</authors>
<date>2001</date>
<booktitle>SENSEVAL-2 Japanese task. SIG NLC 2001-10, IEICE. (in Japanese).</booktitle>
<contexts>
<context position="20500" citStr="Kurohashi and Shirai, 2001" startWordPosition="3439" endWordPosition="3442">higher ratios of Other Sense and Homonym at newspapers indicate that the words whose surface form is in a dictionary are frequently used for the different meanings in real text, e.g.�.Rgin “silver” is used for the abbrebiation of€Rffginkou “bank”. %Multiple Tags is the percentage of tokens for which at least one annotator marked multiple tags. 5 Discussion 5.1 Comparison with Senseval-2 corpus The Senseval-2 Japanese dictionary task annotation used senses from a different dictionary (Shirai, 2002). In the evaluation, 100 test words were selected from three groups with different entropy bands (Kurohashi and Shirai, 2001). Da is the highest entropy group, which contains the most hard to tag words, and Dc is the lowest entropy group. We compare our results with theirs in Table 11. The Senseval-2 agreement figures are slightly higher than our overall. However, it is impossible to make a direct comparison as the numbers of annotators (two or three annotators in Senseval vs more than 5 annotators in our work) and the sense inventories are different. 5.2 Problems Two main problems came up when building the corpora: word segmentation and sense segmentation. Multiword expressions like compounds and idioms are tied cl</context>
</contexts>
<marker>Kurohashi, Shirai, 2001</marker>
<rawString>Sadao Kurohashi and Kiyoaki Shirai. 2001. SENSEVAL-2 Japanese task. SIG NLC 2001-10, IEICE. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Nichols</author>
<author>Francis Bond</author>
</authors>
<title>Acquiring ontologies using deep and shallow processing.</title>
<date>2005</date>
<booktitle>In 11th Annual Meeting of the Association for Natural Language Processing,</booktitle>
<pages>494--498</pages>
<location>Takamatsu.</location>
<contexts>
<context position="5569" citStr="Nichols and Bond, 2005" startWordPosition="952" endWordPosition="955"> “golf” Figure 1: Entry for the Word doraib¯a “driver” (with English glosses) words have. Lower familiarity words tend to have less ambiguity and 70 % of words with a familiarity of less than 5.5 are monosemous. Most polysemous words have only two or three senses as seen in Table 2. Fam #Words Poly- #WS #Monosemous semous(%) 6.5 - 368 182 4.0 186 (50.5) 6.0 - 4,445 1,902 3.4 2,543 (57.2) 5.5 - 9,814 3,502 2.7 6,312 (64.3) 5.0 - 11,430 3,457 2.5 7,973 (69.8) Table 1: Familiarity vs Word Sense Ambiguity 2.2 Ontology We also have an ontology built from the parse results of definitions in Lexeed (Nichols and Bond, 2005). The ontology includes more than 50 thousand relationship between word senses, e.g. synonym, hypernym, abbreviation, etc. 2.3 Goi-Taikei As part of the ontology verification, all nominal and most verbal word senses in Lexeed were linked to semantic classes in the Japanese thesaurus, Nihongo Goi-Taikei (Ikehara et al., 1997). Common nouns are classified into about 2,700 semantic classes which are organized into a #WS #Words 1 18460 2 6212 3 2040 4 799 5 311 6 187 7 99 8 53 9 35 10 15 11 19 12 13 13 13 14 6 15 6 16 3 17 2 18 3 19 1 20 2 &gt; 21 19 Table 2: Number of Word Senses 63 semantic hierarc</context>
</contexts>
<marker>Nichols, Bond, 2005</marker>
<rawString>Eric Nichols and Francis Bond. 2005. Acquiring ontologies using deep and shallow processing. In 11th Annual Meeting of the Association for Natural Language Processing, pages 494–498. Takamatsu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minoru Nishio</author>
<author>Etsutaro Iwabuchi</author>
<author>Shizuo Mizutani</author>
</authors>
<date>1994</date>
<booktitle>Iwanami Kokugo Jiten Dai Go Han [Iwanami Japanese Dictionary Edition 5]. Iwanami Shoten,</booktitle>
<location>Tokyo.</location>
<note>(in Japanese).</note>
<contexts>
<context position="7345" citStr="Nishio et al., 1994" startWordPosition="1253" endWordPosition="1256">ora. The dictionary Lexeed, which defined word senses, is also used for a target for sense tagging. Its definition (LXD-DEF) and example (LXD-EX) sentences consist of basic words and function words only, i.e. it is self-contained. Therefore, all content words have headwords in Lexeed, and all word senses appear in at least one example sentence. Both newspaper corpora where taken from the Mainichi Daily News. One sample (Senseval2) was the text used for the Japanese dictionary task in Senseval-2 (Shirai, 2002), which has some words marked up with word sense tags defined in the Iwanami lexicon (Nishio et al., 1994). The second sample was those sentences used in the Kyoto Corpus (Kyoto), which is marked up with dependency analyses (Kurohashi and Nagao, 2003). We chose these corpora so that we can compare our annotation with existing annotation. Both these corpora were thus already segmented and annotated with parts-of-speech. However, they used different morphological analyzers to the one used in Lexeed, so we had to do some remapping. E.g. in Kyoto the copula is not split from nominal-adjectives, whereas in Lexeed it is: TE%t�genkida “lively” vsT%t�genki da. This could be done automatically after we had</context>
</contexts>
<marker>Nishio, Iwabuchi, Mizutani, 1994</marker>
<rawString>Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani. 1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Christoper D Manning</author>
<author>Dan Flickinger</author>
<author>Thorsten Brant</author>
</authors>
<title>The LinGO redwoods treebank: Motivation and preliminary applications.</title>
<date>2002</date>
<booktitle>In 19th International Conference on Computational Linguistics: COLING2002,</booktitle>
<pages>1253--7</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1556" citStr="Oepen et al., 2002" startWordPosition="221" endWordPosition="224">th structural annotation (such as the Penn Treebank (Taylor et al., 2003) or the Kyoto Corpus (Kurohashi and Nagao, 2003)) and semantic annotation (e.g. Senseval: Kilgariff and Rosenzweig, 2000; Shirai, 2002), there are almost no corpora that combine both. This makes it difficult to carry out research on the interaction between syntax and semantics. Projects such as the Penn Propbank are adding structural semantics (i.e. predicate argument structure) to syntactically annotated corpora, but not lexical semantic information (i.e. word senses). Other corpora, such as the English Redwoods Corpus (Oepen et al., 2002), combine both syntactic and structural semantics in a monostratal representation, but still have no lexical semantics. In this paper we discuss the (lexical) semantic annotation for the Hinoki Corpus, which is part of a larger project in psycho-linguistic and computational linguistics ultimately aimed at language understanding (Bond et al., 2004). 2 Corpus Design In this section we describe the overall design of the corpus, and is constituent corpora. The basic aim is to combine structural semantic and lexical semantic markup in a single corpus. In order to make the first phase self contained</context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, Brant, 2002</marker>
<rawString>Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christoper D. Manning, Dan Flickinger, and Thorsten Brant. 2002. The LinGO redwoods treebank: Motivation and preliminary applications. In 19th International Conference on Computational Linguistics: COLING2002, pages 1253–7. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>Computational Linguistics and Intelligent Text Processing: Third International Conference: CICLing2002,</booktitle>
<pages>1--15</pages>
<editor>In Alexander Gelbuk, editor,</editor>
<publisher>Springer-Verlag, Hiedelberg/Berlin.</publisher>
<contexts>
<context position="22460" citStr="Sag et al., 2002" startWordPosition="3769" endWordPosition="3772">b .660 .699 .722 .896 .738 .867 .723 .867 16.7 21.8 10.3 9.3 5.2 5.9 9.6 10.9 total .710 .754 .760 .841 .831 .939 .768 .863 15.6 18.8 7.0 6.2 4.2 4.9 7.6 7.9 Table 11: Comparison of Agreement for the Senseval-2 Lexical Sample Task Corpus ( upper row: agreement, lower row: the number of word senses) words by our morphological analyser in the same way as other numeral + classifier. The second problem is how to mark off metaphorical meaning from literal meanings. Currently, this also depends on the Lexeed definition and it is not necessarily consistent either. Some words in institutional idioms (Sag et al., 2002) have the idiom sense in the lexicon while most words do not. For instance,Yr shippo “tail of animal”) has a sense for the reading “weak point” in an idiom Y r � IN t shippo-o tsukamu “lit. to grasp the tail, idiom. to find one’s weak point”, while ff ase “sweat” does not have a sense for the applicable meaning in the idiom ff � j,41L�t ase-o nagasu “lit. to sweat, idiom, to work hard”. 6 Conclusions We built a corpus of over three million words which has lexical semantic information. We are currently using it to build a model for word sense disambiguation. Acknowledgement We thank the 15 sens</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Alexander Gelbuk, editor, Computational Linguistics and Intelligent Text Processing: Third International Conference: CICLing2002, pages 1–15. Springer-Verlag, Hiedelberg/Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
</authors>
<title>Construction of a word sense tagged corpus for SENSEVAL-2 Japanese dictionary task.</title>
<date>2002</date>
<booktitle>In Third International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<pages>605--608</pages>
<contexts>
<context position="1145" citStr="Shirai, 2002" startWordPosition="162" endWordPosition="163">cal semantics for words and phrases and structural semantics for phrases and sentences. We have built a Japanese corpus of over three million words with both lexical and structural semantic information. In this paper, we focus on our method of annotating the lexical semantics, that is building a word sense tagged corpus and its properties. 1 Introduction While there has been considerable research on both structural annotation (such as the Penn Treebank (Taylor et al., 2003) or the Kyoto Corpus (Kurohashi and Nagao, 2003)) and semantic annotation (e.g. Senseval: Kilgariff and Rosenzweig, 2000; Shirai, 2002), there are almost no corpora that combine both. This makes it difficult to carry out research on the interaction between syntax and semantics. Projects such as the Penn Propbank are adding structural semantics (i.e. predicate argument structure) to syntactically annotated corpora, but not lexical semantic information (i.e. word senses). Other corpora, such as the English Redwoods Corpus (Oepen et al., 2002), combine both syntactic and structural semantics in a monostratal representation, but still have no lexical semantics. In this paper we discuss the (lexical) semantic annotation for the Hi</context>
<context position="7239" citStr="Shirai, 2002" startWordPosition="1236" endWordPosition="1237">rpus to mark up: a dictionary and two newspapers. Table 3 shows basic statistics of the target corpora. The dictionary Lexeed, which defined word senses, is also used for a target for sense tagging. Its definition (LXD-DEF) and example (LXD-EX) sentences consist of basic words and function words only, i.e. it is self-contained. Therefore, all content words have headwords in Lexeed, and all word senses appear in at least one example sentence. Both newspaper corpora where taken from the Mainichi Daily News. One sample (Senseval2) was the text used for the Japanese dictionary task in Senseval-2 (Shirai, 2002), which has some words marked up with word sense tags defined in the Iwanami lexicon (Nishio et al., 1994). The second sample was those sentences used in the Kyoto Corpus (Kyoto), which is marked up with dependency analyses (Kurohashi and Nagao, 2003). We chose these corpora so that we can compare our annotation with existing annotation. Both these corpora were thus already segmented and annotated with parts-of-speech. However, they used different morphological analyzers to the one used in Lexeed, so we had to do some remapping. E.g. in Kyoto the copula is not split from nominal-adjectives, wh</context>
<context position="20375" citStr="Shirai, 2002" startWordPosition="3422" endWordPosition="3423">mped Sense Agreement (LXD-DEF) the differences in corpus characteristics between dictionary and newspaper. The higher ratios of Other Sense and Homonym at newspapers indicate that the words whose surface form is in a dictionary are frequently used for the different meanings in real text, e.g.�.Rgin “silver” is used for the abbrebiation of€Rffginkou “bank”. %Multiple Tags is the percentage of tokens for which at least one annotator marked multiple tags. 5 Discussion 5.1 Comparison with Senseval-2 corpus The Senseval-2 Japanese dictionary task annotation used senses from a different dictionary (Shirai, 2002). In the evaluation, 100 test words were selected from three groups with different entropy bands (Kurohashi and Shirai, 2001). Da is the highest entropy group, which contains the most hard to tag words, and Dc is the lowest entropy group. We compare our results with theirs in Table 11. The Senseval-2 agreement figures are slightly higher than our overall. However, it is impossible to make a direct comparison as the numbers of annotators (two or three annotators in Senseval vs more than 5 annotators in our work) and the sense inventories are different. 5.2 Problems Two main problems came up whe</context>
</contexts>
<marker>Shirai, 2002</marker>
<rawString>Kiyoaki Shirai. 2002. Construction of a word sense tagged corpus for SENSEVAL-2 Japanese dictionary task. In Third International Conference on Language Resources and Evaluation (LREC-2002), pages 605–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English allwords task.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3,</booktitle>
<pages>41--44</pages>
<publisher>ACL,</publisher>
<location>Barcelona.</location>
<contexts>
<context position="15197" citStr="Snyder and Palmer (2004)" startWordPosition="2566" endWordPosition="2569">o an annotator of sense tags sik for the word wi,j. Hence, the agreement of the word wi is the average of awi,j over all occurrences in a corpus: ∑ j a(wi,j) a(wi) � (2) Nwi 66 where NwZ is the frequency of the word wZ in a corpus. Table 4 shows statistics about the annotation results. The average numbers of word senses in the newspapers are lower than the ones in the dictionary and, therefore, the token agreement of the newspapers is higher than those of the dictionary sentences. %Unanimous indicates the ratio of tokens vs types for which all annotators (normally five) choose the same sense. Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. It is hard to directly compare with our task since their corpus has only 2,212 words tagged by two or three annotators. 4.1 Familiarity As seen in Table 5, the agreement per type does not vary much by familiarity. This was an unexpected result. Even though the average polysemy is high, there are still many highly familiar words with very good agreement. Fam Agreement #WS %Monosem token (type) 6.5 - .723 (.846) 7.00 22.6 6.0 - .780 (.846) 5.82 28.0 5.5 - .813 (.853) 3.79 42.4 5.0 - .821 (.850) 3</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The English allwords task. In Proceedings of Senseval-3, pages 41–44. ACL, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Tanaka</author>
<author>Francis Bond</author>
<author>Stephan Oepen</author>
<author>Sanae Fujita</author>
</authors>
<title>High precision treebanking – blazing useful trees using POS information.</title>
<date>2005</date>
<booktitle>In ACL-2005,</booktitle>
<pages>330--337</pages>
<contexts>
<context position="6348" citStr="Tanaka et al., 2005" startWordPosition="1093" endWordPosition="1096">gy verification, all nominal and most verbal word senses in Lexeed were linked to semantic classes in the Japanese thesaurus, Nihongo Goi-Taikei (Ikehara et al., 1997). Common nouns are classified into about 2,700 semantic classes which are organized into a #WS #Words 1 18460 2 6212 3 2040 4 799 5 311 6 187 7 99 8 53 9 35 10 15 11 19 12 13 13 13 14 6 15 6 16 3 17 2 18 3 19 1 20 2 &gt; 21 19 Table 2: Number of Word Senses 63 semantic hierarchy. 2.4 Hinoki Treebank Lexeed definition and example sentences are syntactically and semantically parsed with HPSG and correct results are manually selected (Tanaka et al., 2005). The grammatical coverage over all sentences is 86%. Around 12% of the parsed sentences were rejected by the treebankers due to an incomplete semantic representation. This process had been done independently of word sense annotation. 2.5 Target Corpora We chose two types of corpus to mark up: a dictionary and two newspapers. Table 3 shows basic statistics of the target corpora. The dictionary Lexeed, which defined word senses, is also used for a target for sense tagging. Its definition (LXD-DEF) and example (LXD-EX) sentences consist of basic words and function words only, i.e. it is self-con</context>
<context position="14069" citStr="Tanaka et al., 2005" startWordPosition="2362" endWordPosition="2365"> when our script failed to produce it. There was an enormous variation in speed: the fastest annotator was 4 times as fast as the slowest, with no appreciable difference in agreement. After providing the feedback, the average speed increased considerably, as the slowest annotators agonized less over their decisions. The final average speed was around 1,500 tokens/day, with the fastest annotator still almost twice as fast as the slowest. 4 Inter-Annotator Agreement We employ inter-annotator agreement as our core measure of annotation consistency, in the same way we did for treebank evaluation (Tanaka et al., 2005). This agreement is calculated as the average of pairwise agreement. Let wi be a word in a set of content words W and wi, j be the jth occurrence of a word wi. Average pairwise agreement between the sense tags of wi, j each pair of annotators marked up a(wi, j) is: ∑ k (mi, j(sik)C2) a(wi,j) � (1) nwi,j C2 where nwi,j(≥ 2) is the number of annotators that tag the word wi,j, and mi,j(sik) is the number 65 Figure 2: Sense Annotation tool (wordM&lt;shibaraku “briefly”) Figure 3: Sample feedback provided to an annotator of sense tags sik for the word wi,j. Hence, the agreement of the word wi is the a</context>
</contexts>
<marker>Tanaka, Bond, Oepen, Fujita, 2005</marker>
<rawString>Takaaki Tanaka, Francis Bond, Stephan Oepen, and Sanae Fujita. 2005. High precision treebanking – blazing useful trees using POS information. In ACL-2005, pages 330– 337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Taylor</author>
<author>Mitchel Marcus</author>
<author>Beatrice Santorini</author>
</authors>
<title>The Penn treebank: an overview.</title>
<date>2003</date>
<booktitle>In Abeill´e (2003), chapter 1,</booktitle>
<pages>5--22</pages>
<contexts>
<context position="1010" citStr="Taylor et al., 2003" startWordPosition="141" endWordPosition="144">d natural language processing such as machine translation, question answering, etc. There are at least two kinds of semantic information: lexical semantics for words and phrases and structural semantics for phrases and sentences. We have built a Japanese corpus of over three million words with both lexical and structural semantic information. In this paper, we focus on our method of annotating the lexical semantics, that is building a word sense tagged corpus and its properties. 1 Introduction While there has been considerable research on both structural annotation (such as the Penn Treebank (Taylor et al., 2003) or the Kyoto Corpus (Kurohashi and Nagao, 2003)) and semantic annotation (e.g. Senseval: Kilgariff and Rosenzweig, 2000; Shirai, 2002), there are almost no corpora that combine both. This makes it difficult to carry out research on the interaction between syntax and semantics. Projects such as the Penn Propbank are adding structural semantics (i.e. predicate argument structure) to syntactically annotated corpora, but not lexical semantic information (i.e. word senses). Other corpora, such as the English Redwoods Corpus (Oepen et al., 2002), combine both syntactic and structural semantics in a</context>
</contexts>
<marker>Taylor, Marcus, Santorini, 2003</marker>
<rawString>Ann Taylor, Mitchel Marcus, and Beatrice Santorini. 2003. The Penn treebank: an overview. In Abeill´e (2003), chapter 1, pages 5–22.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>