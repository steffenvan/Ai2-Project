<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014873">
<title confidence="0.9929855">
DKPro WSD – A Generalized UIMA-based Framework
for Word Sense Disambiguation
</title>
<author confidence="0.782683">
Tristan Miller1 Nicolai Erbs1 Hans-Peter Zorn1 Torsten Zesch1,2 Iryna Gurevych1,2
(1) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
</author>
<affiliation confidence="0.600494">
Department of Computer Science, Technische Universit¨at Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
</affiliation>
<email confidence="0.824845">
http://www.ukp.tu-darmstadt.de/
</email>
<sectionHeader confidence="0.991004" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999303">
Implementations of word sense disam-
biguation (WSD) algorithms tend to be
tied to a particular test corpus format and
sense inventory. This makes it difficult to
test their performance on new data sets, or
to compare them against past algorithms
implemented for different data sets. In this
paper we present DKPro WSD, a freely
licensed, general-purpose framework for
WSD which is both modular and exten-
sible. DKPro WSD abstracts the WSD
process in such a way that test corpora,
sense inventories, and algorithms can be
freely swapped. Its UIMA-based architec-
ture makes it easy to add support for new
resources and algorithms. Related tasks
such as word sense induction and entity
linking are also supported.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999809316666667">
Word sense disambiguation, or WSD (Agirre and
Edmonds, 2006)—the task of determining which
of a word’s senses is the one intended in a par-
ticular context—has been a core research problem
in computational linguistics since the very incep-
tion of the field. Despite the task’s importance
and popularity as a subject of study, tools and re-
sources supporting WSD have seen relatively little
generalization and standardization. That is, most
prior implementations of WSD systems have been
hard-coded for particular algorithms, sense inven-
tories, and data sets. This makes it difficult to com-
pare systems or to adapt them to new scenarios
without extensive reimplementation.
In this paper we present DKPro WSD, a
general-purpose framework for word sense disam-
biguation which is both modular and extensible.
Its modularity means that it makes a logical sep-
aration between the data sets (e.g., the corpora
to be annotated, the answer keys, manually anno-
tated training examples, etc.), the sense invento-
ries (i.e., the lexical-semantic resources enumerat-
ing the senses to which words in the corpora are
assigned), and the algorithms (i.e., code which ac-
tually performs the sense assignments and prereq-
uisite linguistic annotations), and provides a stan-
dard interface for each of these component types.
Components which provide the same functional-
ity can be freely swapped, so that one can easily
run the same algorithm on different data sets (irre-
spective of which sense inventory they use), or test
several different algorithms on the same data set.
While DKPro WSD ships with support for a
number of common WSD algorithms, sense inven-
tories, and data set formats, its extensibility means
that it is easy to adapt to work with new meth-
ods and resources. The system is written in Java
and is based on UIMA (Lally et al., 2009), an
industry-standard architecture for analysis of un-
structured information. Support for new corpus
formats, sense inventories, and WSD algorithms
can be added by implementing new UIMA com-
ponents for them, or more conveniently by writing
UIMA wrappers around existing code. The frame-
work and all existing components are released un-
der the Apache License 2.0, a permissive free soft-
ware licence.
DKPro WSD was designed primarily to support
the needs of WSD researchers, who will appre-
ciate the convenience and flexibility it affords in
tuning and comparing algorithms and data sets.
However, as a general-purpose toolkit it could also
be used to implement a WSD module for a real-
world natural language processing application. Its
support for interactive visualization of the disam-
biguation process also makes it a powerful tool for
learning or teaching the principles of WSD.
The remainder of this paper is organized as fol-
lows: In §2 we review previous work in WSD file
formats and implementations. In §3 we describe
</bodyText>
<page confidence="0.993302">
37
</page>
<bodyText confidence="0.7197776">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 37–42,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
our system and further explain its capabilities and
advantages. Finally, in §4 we discuss our plans for
further development of the framework.
</bodyText>
<sectionHeader confidence="0.943533" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999974188405797">
In the early days of WSD research, electronic
dictionaries and sense-annotated corpora tended
to be small and hand-crafted on an ad-hoc ba-
sis. It was not until the growing availability of
large-scale lexical resources and corpora in the
1990s that the need to establish a common plat-
form for the evaluation of WSD systems was rec-
ognized. This led to the founding of the Sens-
eval (and later SemEval) series of competitions,
the first of which was held in 1998. Each com-
petition defined a number of tasks with prescribed
evaluation metrics, sense inventories, corpus file
formats, and human-annotated test sets. For each
task it was therefore possible to compare algo-
rithms against each other. However, sense inven-
tories and file formats still vary across tasks and
competitions. There are also a number of increas-
ingly popular resources used outside Senseval and
SemEval, each with their own formats and struc-
tures: examples of sense-annotated corpora in-
clude SemCor (Miller et al., 1994), MASC (Ide et
al., 2010), and WebCAGe (Henrich et al., 2012),
and sense inventories include VerbNet (Kipper et
al., 2008), FrameNet (Ruppenhofer et al., 2010),
DANTE (Kilgarriff, 2010), BabelNet (Navigli and
Ponzetto, 2012), and online community-produced
resources such as Wiktionary and Wikipedia. So
despite attempts at standardization, the canon of
WSD resources remains quite fragmented.
The few publically available implementa-
tions of individual disambiguation algorithms,
such as SenseLearner (Mihalcea and Csomai,
2005), SenseRelate::TargetWord (Patwardhan et
al., 2005), UKB (Agirre and Soroa, 2009), and
IMS (Zhong and Ng, 2010), are all tied to a partic-
ular corpus and/or sense inventory, or define their
own custom formats into which existing resources
must be converted. Furthermore, where the al-
gorithm depends on linguistic annotations such as
part-of-speech tags, the users are expected to sup-
ply these themselves, or else must use the anno-
tators built into the system (which may not always
be appropriate for the corpus language or domain).
One alternative to coding WSD algorithms from
scratch is to use general-purpose NLP toolkits
such as NLTK (Bird, 2006) or DKPro (Gurevych
et al., 2007). Such toolkits provide individual
components potentially useful for WSD, such as
WordNet-based measures of sense similarity and
readers for the odd corpus format. However, these
toolkits are not specifically geared towards devel-
opment and evaluation of WSD systems; there is
no unified type system or architecture which al-
lows WSD-specific components to be combined or
substituted orthogonally.
The only general-purpose dedicated WSD sys-
tem we are aware of is I Can Sense It (Joshi et al.,
2012), a Web-based interface for running and eval-
uating various WSD algorithms. It includes I/O
support for several corpus formats and implemen-
tations of a number of baseline and state-of-the-
art disambiguation algorithms. However, as with
previous single-algorithm systems, it is not possi-
ble to select the sense inventory, and the user is
responsible for pre-annotating the input text with
POS tags. The usability and extensibility of the
system are greatly restricted by the fact that it is a
proprietary, closed-source application fully hosted
by the developers.
</bodyText>
<sectionHeader confidence="0.994023" genericHeader="method">
3 DKPro WSD
</sectionHeader>
<bodyText confidence="0.999967230769231">
Our system, DKPro WSD, is implemented as a
framework of UIMA components (type systems,
collection readers, annotators, CAS consumers,
resources) which the user combines into a data
processing pipeline. We can best illustrate this
with an example: Figure 1 shows a pipeline for
running two disambiguation algorithms on the Es-
tonian all-words task from Senseval-2. UIMA
components are the solid, rounded boxes in the
lower half of the diagram, and the data and algo-
rithms they encapsulate are the light grey shapes
in the upper half. The first component of the
pipeline is a collection reader which reads the
text of the XML-formatted corpus into a CAS (a
UIMA data structure for storing layers of data
and stand-off annotations) and marks the words
to be disambiguated (the “instances”) with their
IDs. The next component is an annotator which
reads the answer key—a separate file which as-
sociates each instance ID with a sense ID from
the Estonian EuroWordNet—and adds the gold-
standard sense annotations to their respective in-
stances in the CAS. Processing then passes to
another annotator—in this case a UIMA wrap-
per for TreeTagger (Schmid, 1994)—which adds
POS and lemma annotations to the instances.
</bodyText>
<page confidence="0.988601">
38
</page>
<figure confidence="0.99973675">
Senseval-2
Estonian
all-words
test corpus
corpus
reader
answer key
annotator
Senseval-2
Estonian
all-words
answer key
Estonian
language
model
linguistic
annotator
Tree-
Tagger
simplified
Lesk
WSD
annotator
Estonian
Euro-
WordNet
sense
inventory
JMWNL
WSD
annotator
degree
centrality
evaluator
results and
statistics
</figure>
<figureCaption confidence="0.99999">
Figure 1: A sample DKPro WSD pipeline for the Estonian all-words data set from Senseval-2.
</figureCaption>
<bodyText confidence="0.998606373134328">
Then come the two disambiguation algorithms,
also modelled as UIMA annotators wrapping non-
UIMA-aware algorithms. Each WSD annotator it-
erates over the instances in the CAS and annotates
them with sense IDs from EuroWordNet. (Euro-
WordNet itself is accessed via a UIMA resource
which wraps JMWNL (Pazienza et al., 2008) and
which is bound to the two WSD annotators.) Fi-
nally, control passes to a CAS consumer which
compares the WSD algorithms’ sense annotations
against the gold-standard annotations produced by
the answer key annotator, and outputs these sense
annotations along with various evaluation metrics
(precision, recall, etc.).
A pipeline of this sort can be written with just
a few lines of code: one or two to declare each
component and if necessary bind it to the appro-
priate resources, and a final one to string the com-
ponents together into a pipeline. Moreover, once
such a pipeline is written it is simple to substitute
functionally equivalent components. For example,
with only a few small changes the same pipeline
could be used for Senseval-3’s English lexical
sample task, which uses a corpus and sense inven-
tory in a different format and language. Specif-
ically, we would substitute the collection reader
with one capable of reading the Senseval lexical
sample format, we would pass an English instead
of Estonian language model to TreeTagger, and
we would substitute the sense inventory resource
exposing the Estonian EuroWordNet with one for
WordNet 1.7.1. Crucially, none of the WSD algo-
rithms need to be changed.
The most important features of our system are
as follows:
Corpora and data sets. DKPro WSD currently
has collection readers for all Senseval and Sem-
Eval all-words and lexical sample tasks, the AIDA
CoNLL-YAGO data set (Hoffart et al., 2011), the
TAC KBP entity linking tasks (McNamee and
Dang, 2009), and the aforementioned MASC,
SemCor, and WebCAGe corpora. Our prepack-
aged corpus analysis modules can compute statis-
tics on monosemous terms, average polysemy,
terms absent from the sense inventory, etc.
Sense inventories. Sense inventories are ab-
stracted into a system of types and interfaces ac-
cording to the sort of lexical-semantic information
they provide. There is currently support for Word-
Net (Fellbaum, 1998), WordNet++ (Ponzetto and
Navigli, 2010), EuroWordNet (Vossen, 1998), the
Turk Bootstrap Word Sense Inventory (Biemann,
2013), and UBY (Gurevych et al., 2012), which
provides access to WordNet, Wikipedia, Wik-
tionary, GermaNet, VerbNet, FrameNet, Omega-
Wiki, and various alignments between them. The
system can automatically convert between vari-
ous versions of WordNet using the UPC mappings
(Daud´e et al., 2003).
Algorithms. As with sense inventories, WSD
algorithms have a type and interface hierarchy ac-
cording to what knowledge sources they require.
Algorithms and baselines already implemented in-
clude the analytically calculated random sense
baseline; the most frequent sense baseline; the
original, simplified, extended, and lexically ex-
panded Lesk variants (Miller et al., 2012); various
</bodyText>
<page confidence="0.998672">
39
</page>
<bodyText confidence="0.981724163265306">
graph connectivity approaches from Navigli and
Lapata (2010); Personalized PageRank (Agirre
and Soroa, 2009); the supervised TWSI system
(Biemann, 2013); and IMS (Zhong and Ng, 2010).
Our open API permits users to program support
for further knowledge-based and supervised algo-
rithms.
Linguistic annotators. Many WSD algorithms
require linguistic annotations from segmenters,
lemmatizers, POS taggers, parsers, etc. Off-the-
shelf UIMA components for producing such an-
notations, such as those provided by DKPro Core
(Gurevych et al., 2007), can be used in a DKPro
WSD pipeline with little or no adaptation.
Visualization tools. We have enhanced some
families of algorithms with animated, interactive
visualizations of the disambiguation process. For
example, Figure 2 shows part of a screenshot from
the interactive running of the degree centrality al-
gorithm (Navigli and Lapata, 2010). The system is
disambiguating the three content words in the sen-
tence “I drink milk with a straw.” Red, green, and
blue nodes represent senses (or more specifically,
WordNet sense keys) of the words drink, milk,
and straw, respectively; grey nodes are senses of
other words discovered by traversing semantic re-
lations (represented by arcs) in the sense inven-
tory. The current traversal (toast%2:34:00:: to
fuddle%2:34:00::) is drawn in a lighter colour.
Mouseover tooltips provide more detailed infor-
mation on senses. We have found such visualiza-
tions to be invaluable for understanding and de-
bugging algorithms.
Parameter sweeping. The behaviour of many
components (or entire pipelines) can be altered ac-
cording to various parameters. For example, for
the degree centrality algorithm one must specify
the maximum search depth, the minimum vertex
degree, and the context size. DKPro WSD can
perform a parameter sweep, automatically running
the pipeline once for every possible combination
of parameters in user-specified ranges and con-
catenating the results into a table from which the
optimal system configurations can be identified.
Reporting tools. There are several reporting
tools to support evaluation and error analysis. Raw
sense assignments can be output in a variety of for-
mats (XML, HTML, CSV, Senseval answer key,
etc.), some of which support colour-coding to
</bodyText>
<figureCaption confidence="0.8594005">
Figure 2: DKPro WSD’s interactive visualization
of a graph connectivity WSD algorithm.
</figureCaption>
<bodyText confidence="0.99943371875">
highlight correct and incorrect assignments. The
system can also compute common evaluation met-
rics (Agirre and Edmonds, 2006, pp. 76–80) and
plot precision–recall curves for each algorithm in
the pipeline, as well as produce confusion matri-
ces for algorithm pairs. Users can specify backoff
algorithms, and have the system compute results
with and without the backoff. Results can also be
broken down by part of speech. Figure 3 shows
an example of an HTML report produced by the
system—on the left is the sense assignment table,
in the upper right is a table of evaluation metrics,
and in the lower right is a precision–recall graph.
DKPro WSD also has support for tasks closely
related to word sense disambiguation:
Entity linking. Entity linking (EL) is the task of
linking a named entity in a text (e.g., Washington)
to its correct representation in some knowledge
base (e.g., either George Washington or Washing-
ton, D.C. depending on the context). EL is very
similar to WSD in that both tasks involve connect-
ing ambiguous words in a text to entries in some
inventory. DKPro WSD supports EL-specific
sense inventories such as the list of Wikipedia
articles used in the Knowledge Base Population
workshop of the Text Analysis Conference (TAC
KBP). This workshop, held annually since 2009,
provides a means for comparing different EL sys-
tems in a controlled setting. DKPro WSD contains
a reader for the TAC KBP data set, components
for mapping other sense inventories to the TAC
KBP inventory, and evaluation components for the
</bodyText>
<page confidence="0.99781">
40
</page>
<figureCaption confidence="0.998828">
Figure 3: An HTML report produced by DKPro WSD.
</figureCaption>
<bodyText confidence="0.99987625">
official metrics. Researchers can therefore miti-
gate the entry barrier for their first participation at
TAC KBP and experienced participants can extend
their systems by making use of further WSD algo-
rithms.
Word sense induction. WSD is usually per-
formed with respect to manually created sense in-
ventories such as WordNet. In word sense induc-
tion (WSI) a sense inventory for target words is
automatically constructed from an unlabelled cor-
pus. This can be useful for search result cluster-
ing, or for general applications of WSD for lan-
guages and domains for which a sense inventory
is not yet available. It is usually necessary to per-
form WSD at some point in the evaluation of WSI.
DKPro WSD supports WSI by providing state-of-
the art WSD algorithms capable of using arbitrary
sense inventories, including induced ones. It also
includes readers and writers for the SemEval-2007
and -2013 WSI data sets.
</bodyText>
<sectionHeader confidence="0.986347" genericHeader="conclusions">
4 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999738227272727">
In this paper we introduced DKPro WSD, a Java-
and UIMA-based framework for word sense dis-
ambiguation. Its primary advantages over exist-
ing tools are its modularity, its extensibility, and
its free licensing. By segregating and providing
layers of abstraction for code, data sets, and sense
inventories, DKPro WSD greatly simplifies the
comparison of WSD algorithms in heterogeneous
scenarios. Support for a wide variety of commonly
used algorithms, data sets, and sense inventories
has already been implemented.
The framework is under active development,
with work on several new features planned or in
progress. These include implementations or wrap-
pers for further algorithms and for the DANTE
and BabelNet sense inventories. A Web inter-
face is in the works and should be operational
by the time of publication. Source code, bi-
naries, documentation, tutorials, FAQs, an issue
tracker, and community mailing lists are avail-
able on the project’s website at https://code.
google.com/p/dkpro-wsd/.
</bodyText>
<sectionHeader confidence="0.998294" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.975601333333333">
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg Professor-
ship Program under grant No¯ I/82806.
</bodyText>
<page confidence="0.999285">
41
</page>
<sectionHeader confidence="0.985644" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999298876190476">
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Proc.
EACL, pages 33–41.
Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Lang. Resour. and Eval., 47(1):97–122.
Steven Bird. 2006. NLTK: The natural language
toolkit. In Proc. ACL-COLING (Interactive Presen-
tation Sessions), pages 69–72.
Jordi Daud´e, Llu´ıs Padr´o, and German Rigau. 2003.
Validation and tuning of WordNet mapping tech-
niques. In Proc. RANLP, pages 117–123.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Iryna Gurevych, Max M¨uhlh¨auser, Christof M¨uller,
J¨urgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt Knowledge Processing Reposi-
tory Based on UIMA. In Proc. UIMA Workshop at
GLDV.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. UBY – A large-scale unified
lexical-semantic resource. In Proc. EACL, pages
580–590.
Verena Henrich, Erhard Hinrichs, and Tatiana Vodola-
zova. 2012. WebCAGe – A Web-harvested corpus
annotated with GermaNet senses. In Proc. EACL,
pages 387–396.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F¨urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proc. EMNLP, pages 782–792.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The Manually Annotated
Sub-Corpus: A community resource for and by the
people. In Proc. ACL (Short Papers), pages 68–73.
Salil Joshi, Mitesh M. Khapra, and Pushpak Bhat-
tacharyya. 2012. I Can Sense It: A comprehensive
online system for WSD. In Proc. COLING (Demo
Papers), pages 247–254.
Adam Kilgarriff. 2010. A detailed, accurate, exten-
sive, available English lexical database. In Proc.
NAACL-HLT, pages 21–24.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification of
English verbs. Lang. Resour. and Eval., 42(1):21–
40.
Adam Lally, Karin Verspoor, and Eric Nyberg, editors.
2009. Unstructured Information Management Ar-
chitecture (UIMA) Version 1.0. OASIS.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track.
In Proc. TAC.
Rada Mihalcea and Andras Csomai. 2005. Sense-
Learner: Word sense disambiguation for all words
in unrestricted text. In Proc. ACL (System Demos),
pages 53–56.
George A. Miller, Martin Chodorow, Shari Landes,
Claudio Leacock, and Robert G. Thomas. 1994. Us-
ing a semantic concordance for sense identification.
In Proc. HLT, pages 240–243.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proc. COLING, pages
1781–1796.
Roberto Navigli and Mirella Lapata. 2010. An experi-
mental study of graph connectivity for unsupervised
word sense disambiguation. IEEE Trans. on Pattern
Anal. and Machine Intel., 32(4):678–692.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
An overview of BabelNet and its API for multilin-
gual language processing. In Iryna Gurevych and
Jungi Kim, editors, The People’s Web Meets NLP:
Collaboratively Constructed Language Resources.
Springer.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2005. SenseRelate::TargetWord – A gen-
eralized framework for word sense disambiguation.
In Proc. ACL (System Demos), pages 73–76.
Maria Teresa Pazienza, Armando Stellato, and Alexan-
dra Tudorache. 2008. JMWNL: An extensible mul-
tilingual library for accessing wordnets in different
languages. In Proc. LREC, pages 28–30.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proc. ACL, pages 1522–
1531.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2010. FrameNet II: Extended Theory and
Practice. International Computer Science Institute.
Helmud Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proc. NeMLaP.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Springer.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage word sense disambiguation system
for free text. In Proc. ACL (System Demos), pages
78–83.
</reference>
<page confidence="0.999296">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.863606">
<title confidence="0.9947815">DKPro WSD – A Generalized UIMA-based for Word Sense Disambiguation</title>
<author confidence="0.997942">Nicolai Hans-Peter Torsten Iryna</author>
<affiliation confidence="0.9695485">(1) Ubiquitous Knowledge Processing Lab Department of Computer Science, Technische Universit¨at Darmstadt (2) Ubiquitous Knowledge Processing Lab German Institute for Educational Research and Educational</affiliation>
<web confidence="0.999003">http://www.ukp.tu-darmstadt.de/</web>
<abstract confidence="0.999687631578947">Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Word Sense Disambiguation: Algorithms and Applications.</title>
<date>2006</date>
<editor>Eneko Agirre and Philip Edmonds, editors.</editor>
<publisher>Springer.</publisher>
<marker>2006</marker>
<rawString>Eneko Agirre and Philip Edmonds, editors. 2006. Word Sense Disambiguation: Algorithms and Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="5933" citStr="Agirre and Soroa, 2009" startWordPosition="917" endWordPosition="920">r et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych et al., 2007</context>
<context position="12371" citStr="Agirre and Soroa, 2009" startWordPosition="1925" endWordPosition="1928">ween them. The system can automatically convert between various versions of WordNet using the UPC mappings (Daud´e et al., 2003). Algorithms. As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require. Algorithms and baselines already implemented include the analytically calculated random sense baseline; the most frequent sense baseline; the original, simplified, extended, and lexically expanded Lesk variants (Miller et al., 2012); various 39 graph connectivity approaches from Navigli and Lapata (2010); Personalized PageRank (Agirre and Soroa, 2009); the supervised TWSI system (Biemann, 2013); and IMS (Zhong and Ng, 2010). Our open API permits users to program support for further knowledge-based and supervised algorithms. Linguistic annotators. Many WSD algorithms require linguistic annotations from segmenters, lemmatizers, POS taggers, parsers, etc. Off-theshelf UIMA components for producing such annotations, such as those provided by DKPro Core (Gurevych et al., 2007), can be used in a DKPro WSD pipeline with little or no adaptation. Visualization tools. We have enhanced some families of algorithms with animated, interactive visualizat</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for word sense disambiguation. In Proc. EACL, pages 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Creating a system for lexical substitutions from scratch using crowdsourcing.</title>
<date>2013</date>
<journal>Lang. Resour. and Eval.,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="11590" citStr="Biemann, 2013" startWordPosition="1815" endWordPosition="1816"> al., 2011), the TAC KBP entity linking tasks (McNamee and Dang, 2009), and the aforementioned MASC, SemCor, and WebCAGe corpora. Our prepackaged corpus analysis modules can compute statistics on monosemous terms, average polysemy, terms absent from the sense inventory, etc. Sense inventories. Sense inventories are abstracted into a system of types and interfaces according to the sort of lexical-semantic information they provide. There is currently support for WordNet (Fellbaum, 1998), WordNet++ (Ponzetto and Navigli, 2010), EuroWordNet (Vossen, 1998), the Turk Bootstrap Word Sense Inventory (Biemann, 2013), and UBY (Gurevych et al., 2012), which provides access to WordNet, Wikipedia, Wiktionary, GermaNet, VerbNet, FrameNet, OmegaWiki, and various alignments between them. The system can automatically convert between various versions of WordNet using the UPC mappings (Daud´e et al., 2003). Algorithms. As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require. Algorithms and baselines already implemented include the analytically calculated random sense baseline; the most frequent sense baseline; the original, simplified, extended</context>
</contexts>
<marker>Biemann, 2013</marker>
<rawString>Chris Biemann. 2013. Creating a system for lexical substitutions from scratch using crowdsourcing. Lang. Resour. and Eval., 47(1):97–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2006</date>
<booktitle>In Proc. ACL-COLING (Interactive Presentation Sessions),</booktitle>
<pages>69--72</pages>
<contexts>
<context position="6501" citStr="Bird, 2006" startWordPosition="1013" endWordPosition="1014">et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych et al., 2007). Such toolkits provide individual components potentially useful for WSD, such as WordNet-based measures of sense similarity and readers for the odd corpus format. However, these toolkits are not specifically geared towards development and evaluation of WSD systems; there is no unified type system or architecture which allows WSD-specific components to be combined or substituted orthogonally. The only general-purpose dedicated WSD system we are aware of is I Can Sense It (Joshi et al., 2012), a Web-based interface for running and evaluating various WSD algorith</context>
</contexts>
<marker>Bird, 2006</marker>
<rawString>Steven Bird. 2006. NLTK: The natural language toolkit. In Proc. ACL-COLING (Interactive Presentation Sessions), pages 69–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Daud´e</author>
<author>Llu´ıs Padr´o</author>
<author>German Rigau</author>
</authors>
<title>Validation and tuning of WordNet mapping techniques.</title>
<date>2003</date>
<booktitle>In Proc. RANLP,</booktitle>
<pages>117--123</pages>
<marker>Daud´e, Padr´o, Rigau, 2003</marker>
<rawString>Jordi Daud´e, Llu´ıs Padr´o, and German Rigau. 2003. Validation and tuning of WordNet mapping techniques. In Proc. RANLP, pages 117–123.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Max M¨uhlh¨auser</author>
<author>Christof M¨uller</author>
<author>J¨urgen Steimle</author>
<author>Markus Weimer</author>
<author>Torsten Zesch</author>
</authors>
<title>Darmstadt Knowledge Processing Repository Based on UIMA.</title>
<date>2007</date>
<booktitle>In Proc. UIMA Workshop at GLDV.</booktitle>
<marker>Gurevych, M¨uhlh¨auser, M¨uller, Steimle, Weimer, Zesch, 2007</marker>
<rawString>Iryna Gurevych, Max M¨uhlh¨auser, Christof M¨uller, J¨urgen Steimle, Markus Weimer, and Torsten Zesch. 2007. Darmstadt Knowledge Processing Repository Based on UIMA. In Proc. UIMA Workshop at GLDV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Judith Eckle-Kohler</author>
<author>Silvana Hartmann</author>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
<author>Christian Wirth</author>
</authors>
<title>UBY – A large-scale unified lexical-semantic resource.</title>
<date>2012</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>580--590</pages>
<contexts>
<context position="11623" citStr="Gurevych et al., 2012" startWordPosition="1819" endWordPosition="1822">entity linking tasks (McNamee and Dang, 2009), and the aforementioned MASC, SemCor, and WebCAGe corpora. Our prepackaged corpus analysis modules can compute statistics on monosemous terms, average polysemy, terms absent from the sense inventory, etc. Sense inventories. Sense inventories are abstracted into a system of types and interfaces according to the sort of lexical-semantic information they provide. There is currently support for WordNet (Fellbaum, 1998), WordNet++ (Ponzetto and Navigli, 2010), EuroWordNet (Vossen, 1998), the Turk Bootstrap Word Sense Inventory (Biemann, 2013), and UBY (Gurevych et al., 2012), which provides access to WordNet, Wikipedia, Wiktionary, GermaNet, VerbNet, FrameNet, OmegaWiki, and various alignments between them. The system can automatically convert between various versions of WordNet using the UPC mappings (Daud´e et al., 2003). Algorithms. As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require. Algorithms and baselines already implemented include the analytically calculated random sense baseline; the most frequent sense baseline; the original, simplified, extended, and lexically expanded Lesk var</context>
</contexts>
<marker>Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, Wirth, 2012</marker>
<rawString>Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann, Michael Matuschek, Christian M. Meyer, and Christian Wirth. 2012. UBY – A large-scale unified lexical-semantic resource. In Proc. EACL, pages 580–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Henrich</author>
<author>Erhard Hinrichs</author>
<author>Tatiana Vodolazova</author>
</authors>
<title>WebCAGe – A Web-harvested corpus annotated with GermaNet senses.</title>
<date>2012</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>387--396</pages>
<contexts>
<context position="5386" citStr="Henrich et al., 2012" startWordPosition="846" endWordPosition="849">first of which was held in 1998. Each competition defined a number of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a par</context>
</contexts>
<marker>Henrich, Hinrichs, Vodolazova, 2012</marker>
<rawString>Verena Henrich, Erhard Hinrichs, and Tatiana Vodolazova. 2012. WebCAGe – A Web-harvested corpus annotated with GermaNet senses. In Proc. EACL, pages 387–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Mohamed Amir Yosef</author>
<author>Ilaria Bordino</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
<author>Marc Spaniol</author>
<author>Bilyana Taneva</author>
<author>Stefan Thater</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>782--792</pages>
<marker>Hoffart, Yosef, Bordino, F¨urstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F¨urstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Proc. EMNLP, pages 782–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Christiane Fellbaum</author>
<author>Collin Baker</author>
<author>Rebecca Passonneau</author>
</authors>
<title>The Manually Annotated Sub-Corpus: A community resource for and by the people.</title>
<date>2010</date>
<booktitle>In Proc. ACL (Short Papers),</booktitle>
<pages>68--73</pages>
<contexts>
<context position="5350" citStr="Ide et al., 2010" startWordPosition="840" endWordPosition="843">al) series of competitions, the first of which was held in 1998. Each competition defined a number of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong </context>
</contexts>
<marker>Ide, Fellbaum, Baker, Passonneau, 2010</marker>
<rawString>Nancy Ide, Christiane Fellbaum, Collin Baker, and Rebecca Passonneau. 2010. The Manually Annotated Sub-Corpus: A community resource for and by the people. In Proc. ACL (Short Papers), pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salil Joshi</author>
<author>Mitesh M Khapra</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>I Can Sense It: A comprehensive online system for WSD.</title>
<date>2012</date>
<booktitle>In Proc. COLING (Demo Papers),</booktitle>
<pages>247--254</pages>
<contexts>
<context position="7030" citStr="Joshi et al., 2012" startWordPosition="1094" endWordPosition="1097">D algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych et al., 2007). Such toolkits provide individual components potentially useful for WSD, such as WordNet-based measures of sense similarity and readers for the odd corpus format. However, these toolkits are not specifically geared towards development and evaluation of WSD systems; there is no unified type system or architecture which allows WSD-specific components to be combined or substituted orthogonally. The only general-purpose dedicated WSD system we are aware of is I Can Sense It (Joshi et al., 2012), a Web-based interface for running and evaluating various WSD algorithms. It includes I/O support for several corpus formats and implementations of a number of baseline and state-of-theart disambiguation algorithms. However, as with previous single-algorithm systems, it is not possible to select the sense inventory, and the user is responsible for pre-annotating the input text with POS tags. The usability and extensibility of the system are greatly restricted by the fact that it is a proprietary, closed-source application fully hosted by the developers. 3 DKPro WSD Our system, DKPro WSD, is i</context>
</contexts>
<marker>Joshi, Khapra, Bhattacharyya, 2012</marker>
<rawString>Salil Joshi, Mitesh M. Khapra, and Pushpak Bhattacharyya. 2012. I Can Sense It: A comprehensive online system for WSD. In Proc. COLING (Demo Papers), pages 247–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>A detailed, accurate, extensive, available English lexical database.</title>
<date>2010</date>
<booktitle>In Proc. NAACL-HLT,</booktitle>
<pages>21--24</pages>
<contexts>
<context position="5510" citStr="Kilgarriff, 2010" startWordPosition="865" endWordPosition="866">s, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. F</context>
</contexts>
<marker>Kilgarriff, 2010</marker>
<rawString>Adam Kilgarriff. 2010. A detailed, accurate, extensive, available English lexical database. In Proc. NAACL-HLT, pages 21–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Anna Korhonen</author>
<author>Neville Ryant</author>
<author>Martha Palmer</author>
</authors>
<title>A large-scale classification of English verbs.</title>
<date>2008</date>
<journal>Lang. Resour. and Eval.,</journal>
<volume>42</volume>
<issue>1</issue>
<pages>40</pages>
<contexts>
<context position="5447" citStr="Kipper et al., 2008" startWordPosition="855" endWordPosition="858">mber of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own cu</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2008</marker>
<rawString>Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A large-scale classification of English verbs. Lang. Resour. and Eval., 42(1):21– 40.</rawString>
</citation>
<citation valid="true">
<date>2009</date>
<booktitle>Unstructured Information Management Architecture (UIMA) Version 1.0.</booktitle>
<editor>Adam Lally, Karin Verspoor, and Eric Nyberg, editors.</editor>
<publisher>OASIS.</publisher>
<marker>2009</marker>
<rawString>Adam Lally, Karin Verspoor, and Eric Nyberg, editors. 2009. Unstructured Information Management Architecture (UIMA) Version 1.0. OASIS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Trang Dang</author>
</authors>
<title>knowledge base population track.</title>
<date>2009</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proc. TAC.</booktitle>
<contexts>
<context position="11046" citStr="McNamee and Dang, 2009" startWordPosition="1734" endWordPosition="1737">tion reader with one capable of reading the Senseval lexical sample format, we would pass an English instead of Estonian language model to TreeTagger, and we would substitute the sense inventory resource exposing the Estonian EuroWordNet with one for WordNet 1.7.1. Crucially, none of the WSD algorithms need to be changed. The most important features of our system are as follows: Corpora and data sets. DKPro WSD currently has collection readers for all Senseval and SemEval all-words and lexical sample tasks, the AIDA CoNLL-YAGO data set (Hoffart et al., 2011), the TAC KBP entity linking tasks (McNamee and Dang, 2009), and the aforementioned MASC, SemCor, and WebCAGe corpora. Our prepackaged corpus analysis modules can compute statistics on monosemous terms, average polysemy, terms absent from the sense inventory, etc. Sense inventories. Sense inventories are abstracted into a system of types and interfaces according to the sort of lexical-semantic information they provide. There is currently support for WordNet (Fellbaum, 1998), WordNet++ (Ponzetto and Navigli, 2010), EuroWordNet (Vossen, 1998), the Turk Bootstrap Word Sense Inventory (Biemann, 2013), and UBY (Gurevych et al., 2012), which provides access</context>
</contexts>
<marker>McNamee, Dang, 2009</marker>
<rawString>Paul McNamee and Hoa Trang Dang. 2009. Overview of the TAC 2009 knowledge base population track. In Proc. TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>SenseLearner: Word sense disambiguation for all words in unrestricted text.</title>
<date>2005</date>
<booktitle>In Proc. ACL (System Demos),</booktitle>
<pages>53--56</pages>
<contexts>
<context position="5852" citStr="Mihalcea and Csomai, 2005" startWordPosition="907" endWordPosition="910">wn formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use gener</context>
</contexts>
<marker>Mihalcea, Csomai, 2005</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2005. SenseLearner: Word sense disambiguation for all words in unrestricted text. In Proc. ACL (System Demos), pages 53–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Martin Chodorow</author>
<author>Shari Landes</author>
<author>Claudio Leacock</author>
<author>Robert G Thomas</author>
</authors>
<title>Using a semantic concordance for sense identification.</title>
<date>1994</date>
<booktitle>In Proc. HLT,</booktitle>
<pages>240--243</pages>
<contexts>
<context position="5325" citStr="Miller et al., 1994" startWordPosition="835" endWordPosition="838">he Senseval (and later SemEval) series of competitions, the first of which was held in 1998. Each competition defined a number of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soro</context>
</contexts>
<marker>Miller, Chodorow, Landes, Leacock, Thomas, 1994</marker>
<rawString>George A. Miller, Martin Chodorow, Shari Landes, Claudio Leacock, and Robert G. Thomas. 1994. Using a semantic concordance for sense identification. In Proc. HLT, pages 240–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Chris Biemann</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>1781--1796</pages>
<contexts>
<context position="12250" citStr="Miller et al., 2012" startWordPosition="1909" endWordPosition="1912"> provides access to WordNet, Wikipedia, Wiktionary, GermaNet, VerbNet, FrameNet, OmegaWiki, and various alignments between them. The system can automatically convert between various versions of WordNet using the UPC mappings (Daud´e et al., 2003). Algorithms. As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require. Algorithms and baselines already implemented include the analytically calculated random sense baseline; the most frequent sense baseline; the original, simplified, extended, and lexically expanded Lesk variants (Miller et al., 2012); various 39 graph connectivity approaches from Navigli and Lapata (2010); Personalized PageRank (Agirre and Soroa, 2009); the supervised TWSI system (Biemann, 2013); and IMS (Zhong and Ng, 2010). Our open API permits users to program support for further knowledge-based and supervised algorithms. Linguistic annotators. Many WSD algorithms require linguistic annotations from segmenters, lemmatizers, POS taggers, parsers, etc. Off-theshelf UIMA components for producing such annotations, such as those provided by DKPro Core (Gurevych et al., 2007), can be used in a DKPro WSD pipeline with little </context>
</contexts>
<marker>Miller, Biemann, Zesch, Gurevych, 2012</marker>
<rawString>Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation. In Proc. COLING, pages 1781–1796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>An experimental study of graph connectivity for unsupervised word sense disambiguation.</title>
<date>2010</date>
<journal>IEEE Trans. on Pattern Anal. and Machine Intel.,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="12323" citStr="Navigli and Lapata (2010)" startWordPosition="1919" endWordPosition="1922">t, FrameNet, OmegaWiki, and various alignments between them. The system can automatically convert between various versions of WordNet using the UPC mappings (Daud´e et al., 2003). Algorithms. As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require. Algorithms and baselines already implemented include the analytically calculated random sense baseline; the most frequent sense baseline; the original, simplified, extended, and lexically expanded Lesk variants (Miller et al., 2012); various 39 graph connectivity approaches from Navigli and Lapata (2010); Personalized PageRank (Agirre and Soroa, 2009); the supervised TWSI system (Biemann, 2013); and IMS (Zhong and Ng, 2010). Our open API permits users to program support for further knowledge-based and supervised algorithms. Linguistic annotators. Many WSD algorithms require linguistic annotations from segmenters, lemmatizers, POS taggers, parsers, etc. Off-theshelf UIMA components for producing such annotations, such as those provided by DKPro Core (Gurevych et al., 2007), can be used in a DKPro WSD pipeline with little or no adaptation. Visualization tools. We have enhanced some families of </context>
</contexts>
<marker>Navigli, Lapata, 2010</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2010. An experimental study of graph connectivity for unsupervised word sense disambiguation. IEEE Trans. on Pattern Anal. and Machine Intel., 32(4):678–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>An overview of BabelNet and its API for multilingual language processing.</title>
<date>2012</date>
<booktitle>In Iryna Gurevych</booktitle>
<editor>and Jungi Kim, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="5549" citStr="Navigli and Ponzetto, 2012" startWordPosition="868" endWordPosition="871">uman-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. An overview of BabelNet and its API for multilingual language processing. In Iryna Gurevych and Jungi Kim, editors, The People’s Web Meets NLP: Collaboratively Constructed Language Resources. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>SenseRelate::TargetWord – A generalized framework for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proc. ACL (System Demos),</booktitle>
<pages>73--76</pages>
<contexts>
<context position="5903" citStr="Patwardhan et al., 2005" startWordPosition="912" endWordPosition="915">d corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) o</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2005</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2005. SenseRelate::TargetWord – A generalized framework for word sense disambiguation. In Proc. ACL (System Demos), pages 73–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Teresa Pazienza</author>
<author>Armando Stellato</author>
<author>Alexandra Tudorache</author>
</authors>
<title>JMWNL: An extensible multilingual library for accessing wordnets in different languages.</title>
<date>2008</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>28--30</pages>
<contexts>
<context position="9528" citStr="Pazienza et al., 2008" startWordPosition="1485" endWordPosition="1488">all-words answer key Estonian language model linguistic annotator TreeTagger simplified Lesk WSD annotator Estonian EuroWordNet sense inventory JMWNL WSD annotator degree centrality evaluator results and statistics Figure 1: A sample DKPro WSD pipeline for the Estonian all-words data set from Senseval-2. Then come the two disambiguation algorithms, also modelled as UIMA annotators wrapping nonUIMA-aware algorithms. Each WSD annotator iterates over the instances in the CAS and annotates them with sense IDs from EuroWordNet. (EuroWordNet itself is accessed via a UIMA resource which wraps JMWNL (Pazienza et al., 2008) and which is bound to the two WSD annotators.) Finally, control passes to a CAS consumer which compares the WSD algorithms’ sense annotations against the gold-standard annotations produced by the answer key annotator, and outputs these sense annotations along with various evaluation metrics (precision, recall, etc.). A pipeline of this sort can be written with just a few lines of code: one or two to declare each component and if necessary bind it to the appropriate resources, and a final one to string the components together into a pipeline. Moreover, once such a pipeline is written it is sim</context>
</contexts>
<marker>Pazienza, Stellato, Tudorache, 2008</marker>
<rawString>Maria Teresa Pazienza, Armando Stellato, and Alexandra Tudorache. 2008. JMWNL: An extensible multilingual library for accessing wordnets in different languages. In Proc. LREC, pages 28–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich word sense disambiguation rivaling supervised systems.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1522--1531</pages>
<contexts>
<context position="11505" citStr="Ponzetto and Navigli, 2010" startWordPosition="1802" endWordPosition="1805"> Senseval and SemEval all-words and lexical sample tasks, the AIDA CoNLL-YAGO data set (Hoffart et al., 2011), the TAC KBP entity linking tasks (McNamee and Dang, 2009), and the aforementioned MASC, SemCor, and WebCAGe corpora. Our prepackaged corpus analysis modules can compute statistics on monosemous terms, average polysemy, terms absent from the sense inventory, etc. Sense inventories. Sense inventories are abstracted into a system of types and interfaces according to the sort of lexical-semantic information they provide. There is currently support for WordNet (Fellbaum, 1998), WordNet++ (Ponzetto and Navigli, 2010), EuroWordNet (Vossen, 1998), the Turk Bootstrap Word Sense Inventory (Biemann, 2013), and UBY (Gurevych et al., 2012), which provides access to WordNet, Wikipedia, Wiktionary, GermaNet, VerbNet, FrameNet, OmegaWiki, and various alignments between them. The system can automatically convert between various versions of WordNet using the UPC mappings (Daud´e et al., 2003). Algorithms. As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require. Algorithms and baselines already implemented include the analytically calculated random</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich word sense disambiguation rivaling supervised systems. In Proc. ACL, pages 1522– 1531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Michael Ellsworth</author>
<author>Miriam R L Petruck</author>
<author>Christopher R Johnson</author>
<author>Jan Scheffczyk</author>
</authors>
<title>FrameNet II: Extended Theory and</title>
<date>2010</date>
<institution>Practice. International Computer Science Institute.</institution>
<contexts>
<context position="5484" citStr="Ruppenhofer et al., 2010" startWordPosition="860" endWordPosition="863">aluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing reso</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2010</marker>
<rawString>Josef Ruppenhofer, Michael Ellsworth, Miriam R. L. Petruck, Christopher R. Johnson, and Jan Scheffczyk. 2010. FrameNet II: Extended Theory and Practice. International Computer Science Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmud Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proc. NeMLaP.</booktitle>
<contexts>
<context position="8750" citStr="Schmid, 1994" startWordPosition="1373" endWordPosition="1374">e first component of the pipeline is a collection reader which reads the text of the XML-formatted corpus into a CAS (a UIMA data structure for storing layers of data and stand-off annotations) and marks the words to be disambiguated (the “instances”) with their IDs. The next component is an annotator which reads the answer key—a separate file which associates each instance ID with a sense ID from the Estonian EuroWordNet—and adds the goldstandard sense annotations to their respective instances in the CAS. Processing then passes to another annotator—in this case a UIMA wrapper for TreeTagger (Schmid, 1994)—which adds POS and lemma annotations to the instances. 38 Senseval-2 Estonian all-words test corpus corpus reader answer key annotator Senseval-2 Estonian all-words answer key Estonian language model linguistic annotator TreeTagger simplified Lesk WSD annotator Estonian EuroWordNet sense inventory JMWNL WSD annotator degree centrality evaluator results and statistics Figure 1: A sample DKPro WSD pipeline for the Estonian all-words data set from Senseval-2. Then come the two disambiguation algorithms, also modelled as UIMA annotators wrapping nonUIMA-aware algorithms. Each WSD annotator iterat</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmud Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proc. NeMLaP.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>EuroWordNet: A Multilingual Database with Lexical Semantic Networks.</booktitle>
<editor>Piek Vossen, editor.</editor>
<publisher>Springer.</publisher>
<marker>1998</marker>
<rawString>Piek Vossen, editor. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>It Makes Sense: A wide-coverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proc. ACL (System Demos),</booktitle>
<pages>78--83</pages>
<contexts>
<context position="5963" citStr="Zhong and Ng, 2010" startWordPosition="923" endWordPosition="926"> 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych et al., 2007). Such toolkits provide indiv</context>
<context position="12445" citStr="Zhong and Ng, 2010" startWordPosition="1937" endWordPosition="1940">rdNet using the UPC mappings (Daud´e et al., 2003). Algorithms. As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require. Algorithms and baselines already implemented include the analytically calculated random sense baseline; the most frequent sense baseline; the original, simplified, extended, and lexically expanded Lesk variants (Miller et al., 2012); various 39 graph connectivity approaches from Navigli and Lapata (2010); Personalized PageRank (Agirre and Soroa, 2009); the supervised TWSI system (Biemann, 2013); and IMS (Zhong and Ng, 2010). Our open API permits users to program support for further knowledge-based and supervised algorithms. Linguistic annotators. Many WSD algorithms require linguistic annotations from segmenters, lemmatizers, POS taggers, parsers, etc. Off-theshelf UIMA components for producing such annotations, such as those provided by DKPro Core (Gurevych et al., 2007), can be used in a DKPro WSD pipeline with little or no adaptation. Visualization tools. We have enhanced some families of algorithms with animated, interactive visualizations of the disambiguation process. For example, Figure 2 shows part of a </context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense: A wide-coverage word sense disambiguation system for free text. In Proc. ACL (System Demos), pages 78–83.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>