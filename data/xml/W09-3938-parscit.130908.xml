<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000759">
<title confidence="0.9978935">
k-Nearest Neighbor Monte-Carlo Control Algorithm
for POMDP-based Dialogue Systems
</title>
<author confidence="0.995359">
F. Lef`evre; M. Gaˇsi´c, F. Jurˇc´ıˇcek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. Young
</author>
<affiliation confidence="0.985921">
Spoken Dialogue Systems Group
Cambridge University Engineering Department
</affiliation>
<address confidence="0.718508">
Trumpington Street, Cambridge CB2 1PZ, UK
</address>
<email confidence="0.970247">
{frfl2, mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.uk
</email>
<sectionHeader confidence="0.996679" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918578947368">
In real-world applications, modelling di-
alogue as a POMDP requires the use of
a summary space for the dialogue state
representation to ensure tractability. Sub-
optimal estimation of the value func-
tion governing the selection of system re-
sponses can then be obtained using a grid-
based approach on the belief space. In
this work, the Monte-Carlo control tech-
nique is extended so as to reduce training
over-fitting and to improve robustness to
semantic noise in the user input. This tech-
nique uses a database of belief vector pro-
totypes to choose the optimal system ac-
tion. A locally weighted k-nearest neigh-
bor scheme is introduced to smooth the de-
cision process by interpolating the value
function, resulting in higher user simula-
tion performance.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999135733333333">
In the last decade dialogue modelling as a Partially
Observable Markov Decision Process (POMDP)
has been proposed as a convenient way to improve
spoken dialogue systems (SDS) trainability, nat-
uralness and robustness to input errors (Young et
al., 2009). The POMDP framework models dia-
logue flow as a sequence of unobserved dialogue
states following stochastic moves, and provides a
principled way to model uncertainty.
However, to deal with uncertainty, POMDPs
maintain distributions over all possible states. But
then training an optimal policy is an NP hard
problem and thus not tractable for any non-trivial
application. In recent works this issue is ad-
dressed by mapping the dialog state representation
</bodyText>
<affiliation confidence="0.4271395">
* Fabrice Lef`evre is currently on leave from the Univer-
sity of Avignon, France.
</affiliation>
<bodyText confidence="0.999934972222222">
space (the master space) into a smaller summary
space (Williams and Young, 2007). Even though
optimal policies remain out of reach, sub-optimal
solutions can be found by means of grid-based al-
gorithms.
Within the Hidden Information State (HIS)
framework (Young et al., 2009), policies are rep-
resented by a set of grid points in the summary be-
lief space. Beliefs in master space are first mapped
into summary space and then mapped into a sum-
mary action via the dialogue policy. The resulting
summary action is then mapped back into master
space and output to the user.
Methods which support interpolation between
points are generally required to scale well to large
state spaces (Pineau et al., 2003). In the current
version of the HIS framework, the policy chooses
the system action by associating each new belief
point with the single, closest, grid point. In the
present work, a k-nearest neighbour extension is
evaluated in which the policy decision is based on
a locally weighted regression over a subset of rep-
resentative grid points. This method thus lies be-
tween a strictly grid-based and a point-based value
iteration approach as it interpolates the value func-
tion around the queried belief point. It thus re-
duces the policy’s dependency on the belief grid
point selection and increases robustness to input
noise.
The next section gives an overview of the
CUED HIS POMDP dialogue system which we
extended for our experiments. In Section 3, the
grid-based approach to policy optimisation is in-
troduced followed by a presentation of the k-
nn Monte-Carlo policy optimization in Section 4,
along with an evaluation on a simulated user.
</bodyText>
<subsubsectionHeader confidence="0.642434">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 272–275,
</subsubsectionHeader>
<affiliation confidence="0.937837">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.995341">
272
</page>
<figure confidence="0.998743153846154">
Master Space Summary Space
Grnd affirm(...)
UInfo
...
Uinfo inform(...)
UInfo
...
b(1)
b(2)
h
status
p
status
last
uact
Policy
d i ems
om h p 1 fr o
Yes
Compatible
with hyp 1?
rt
Fi
y n xt
Tr
Ad
</figure>
<sectionHeader confidence="0.769089" genericHeader="method">
2 The CUED Spoken Dialogue System
</sectionHeader>
<subsectionHeader confidence="0.992098">
2.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.99990352">
The CUED HIS-based dialogue system pipelines
five modules: the ATK speech recogniser, an
SVM-based semantic tuple classifier, a POMDP
dialogue manager, a natural language generator,
and an HMM-based speech synthesiser. During
an interaction with the system, the user’s speech
is first decoded by the recogniser and an N-best
list of hypotheses is sent to the semantic classifier.
In turn the semantic classifier outputs an N-best
list of user dialogue acts. A dialogue act is a se-
mantic representation of the user action headed by
the user intention (such as inform, request,
etc) followed by a list of items (slot-value pairs
such as type=hotel, area=east etc). The
N-best list of dialogue acts is used by the dialogue
manager to update the dialogue state. Based on
the state hypotheses and the policy, a machine ac-
tion is determined, again in the form of a dialogue
act. The natural language generator translates the
machine action into a sentence, finally converted
into speech by the HMM synthesiser. The dia-
logue system is currently developed for a tourist
information domain (Towninfo). It is worth not-
ing that the dialogue manager does not contain any
domain-specific knowledge.
</bodyText>
<subsectionHeader confidence="0.996677">
2.2 HIS Dialogue Manager
</subsectionHeader>
<bodyText confidence="0.973874048387097">
The unobserved dialogue state of the HIS dialogue
manager consists of the user goal, the dialogue his-
tory and the user action. The user goal is repre-
sented by a partition which is a tree structure built
according to the domain ontology. The nodes in
the partition consist mainly of slots and values.
When querying the venue database using the par-
tition, a set of matching entities can be produced.
The dialogue history consists of the grounding
states of the nodes in the partition, generated us-
ing a finite state automaton and the previous user
and system action. A hypothesis in the HIS ap-
proach is then a triple combining a partition, a user
action and the respective set of grounding states.
The distribution over all hypotheses is maintained
throughout the dialogue (belief state monitoring).
Considering the ontology size for any real-world
problem, the so-defined state space is too large for
any POMDP learning algorithm. Hence to obtain a
tractable policy, the state/action space needs to be
reduced to a smaller scale summary space. The set
of possible machine dialogue acts is also reduced
in summary space. This is mainly achieved by re-
gure1: Master-summarySpaceMapping.
mo
vingal lac tit emsan dle avingon lyar e ducedse tof
di alogueac tty pes. Wh enma ppingba ckin toma
stersp ace, th ene cessaryit ems(i .e. sl ot-valuepa
irs)ar ein ferredby in spectingth emo st li kelydi
aloguest atehy potheses.T h
eop timalpo licy is ob tainedus ingre inforce-me
ntle arningin in teractionwi than ag endaba sedsi
mulatedus er (S chatzmannet al ., 20 07). At th een
dof ea chdi aloguea r e wardis gi vento th esy s-te
m:+2 0fo ras u ccessfulco mpletionan d-1 fo rea
chtu rn.A g r id-based op timisation is us ed to ob -ta
inth eop timalpo licy(s ee ne xtse ction).At ea chtu
rnth ebe lief is ma pped to as u mmarypo intfr omwh
ichas u mmaryac tionca n be de termined.Th esu
mmaryac tionis th enma ppedba ckto am a sterac
tionby ad dingth ere levantin formation.3G
r id-basedPo licy Op timisationIn
aP O MDP,th eop timalex actva luefu nctionca nbe
fo undit erativelyfr omth ete rminalst atein ap r
ocessca lled va lueit eration.At ea chit erationo,
po licy ve ctorsar ege neratedfo ral lpo ssibleac -ti
on/observationpa irsan dth eirco rrespondingva l-ue
sar eco mputedin te rmsof th epo licyve ctorsat
st epep t . Ho wever,ex actop timisationis no
ttr actablein pr actice, bu tap proximateso lutionsca
nst illpr ovide us efulpo licies. Re presentingaP O
MDPpo licy by ag r idof re presentativebe liefpo
intsyi elds an MD Pop timisation pr oblem fo rwh
ichma nytr actableso lutionsex ist, su ch as th eMo
nteCa rloCo ntrolal gorithm(S uttonan dBa rto,19
9 8)us ed he re.In
th ecu rrentHI Ssy stem,ea chsu mmarybe liefpo
intis a v e ctorco nsistingof th epr obabilitiesof th
eto ptw ohy potheses in ma stersp ace, tw odi s-cr
etest atusva riablessu mmarisingth est ateof th e27
Algorithm 1 Policy training with k-nn Monte
Carlo
</bodyText>
<listItem confidence="0.854005571428571">
1: Let Q(b, am) = expected reward on taking action am from belief point b�
2: Let N(�b, am) = number of times action am is taken from belief point b
3: Let B be a set of grid-points in belief space, {b} any subset of it
4: Let πknn : b —. am; Vb E B be a policy
5: repeat
6: t — 0
7: am,0 —initial greet action
</listItem>
<figure confidence="0.959684592592593">
8: b = b0 [= all states in single partition ]
Generate dialogue using e-greedy policy
9: repeat
10: t — t + 1
11: Get user turn a„,t and update belief state b
12: bt — SummaryState(b)
13: { bk}knn — k-Nearest(bt, B)
� RandomAction with probability �
�am,t — πknn(�bt) otherwise
15: record (bt, {bk}knn, am,t), T — t
16: until dialogue terminates with reward R from user simulator
Scan dialogue and update B, Q and N
17: for t = T downto 1 do
18: if 3b; E B, Ibt — b;I &lt; δ then — update nearest pt in B
19: for all 6k in {bk}knn do
20: w — �Nbt, bk) — �D weighting function
Q(�bk, �am,t) — Q(�bk,�am,t)�N(�bk,�am,t)+R�w
N(�bk,�am,t)+w
22: N(bk, am,t) — N(�bk, am,t) + w
23: end for
24: else — create new grid point
25: add bt to B
26: Q(bt, am,t) — R, N(bt, am,t) — 1
27: end if
28: R — γR — discount the reward
29: end for
30: until converged
</figure>
<bodyText confidence="0.9972006">
top hypothesis and its associated partition, and the
type of the last user act.
In order to use such a policy, a simple distance
metric in belief space is used to find the closest
grid point to a given arbitrary belief state:
</bodyText>
<equation confidence="0.9779636">
�
αd · (�bz(d) − bj(d))2
5
� E αd · (1 − δ( bz(d),bj(d)))(1)
d=3
</equation>
<bodyText confidence="0.983790375">
where the α’s are weights, d ranges over the 2 con-
tinuous and 3 discrete components of b and δ(x, y)
is 1 iff x = y and 0 otherwise.
Associated with each belief point is a function
Q( b, am) which records the expected reward of
taking summary action am when in belief state b.
Q is estimated by repeatedly executing dialogues
and recording the sequence of belief point-action
pairs (bt, Clm,t). At the end of each dialogue, each
Q( bt, Clm,t) estimate is updated with the actual dis-
counted reward. Dialogues are conducted using
the current policy π but to allow exploration of un-
visited regions of the state-action space, a random
action is selected with probability E.
Once the Q values have been estimated, the pol-
icy is found by setting
</bodyText>
<equation confidence="0.722546">
Q(b, bm), bb E B (2)
</equation>
<bodyText confidence="0.999931142857143">
Belief points are generated on demand during the
policy optimisation process. Starting from a sin-
gle belief point, every time a belief point is en-
countered which is sufficiently far from any ex-
isting point in the policy grid, it is added to the
grid as a new point. The inventory of grid points
is thus growing over time until a predefined maxi-
mum number of stored belief vectors is reached.
The training schedule adopted in this work is
comparable to the one presented in (Young et al.,
2009). Training starts in a noise free environment
using a small number of grid points and it con-
tinues until the performance of the policy asymp-
totes. The resulting policy is then taken as an ini-
tial policy for the next stage in which the noise
level is increased, the set of grid points is ex-
panded and the number of iterations is increased.
In practice a total of 750 to 1000 grid points have
been found to be sufficient and the total number of
simulated dialogues needed for training is around
100,000.
</bodyText>
<sectionHeader confidence="0.95457" genericHeader="method">
4 k-nn Monte-Carlo Policy Optimization
</sectionHeader>
<bodyText confidence="0.9999706">
In this work, we use the k nearest neighbor method
to obtain a better estimate of the value function,
represented by the belief points’ Q values. The al-
gorithm maintains a set of sample vectors b along
with their Q value vector Q(b, a). When a new
belief state b&apos; is encountered, its Q values are ob-
tained by looking up its k-nearest neighbours in
the database, then averaging their Q-values.
To obtain good estimates for the value func-
tion interpolation, local weights are used based
on the belief point distance. A Kullback-Leibler
(KL) divergence (relative entropy) could be used
as a distance function between the belief points.
However, while the KL-divergence between two
continuous distributions is well defined, this is
not the case for sample sets. In accordance with
the locally weighted learning theory (Atkeson et
al., 1997), a simple weighting scheme based on a
nearly Euclidean distance (eq. 1) is used to inter-
polate the policy over a set of points:
</bodyText>
<equation confidence="0.979285411764706">
πknn(
Eb) = argmax
am 141knn
In our experiments, we set the weighting co-
b2) =
e−161−6212.
2
j
bz −
E
d=1
�bjj =
b) = argmax
�
am
π(
Q(�bk, bm) X 4b(�bk, b)
</equation>
<bodyText confidence="0.853768">
efficients with the kernel function 4b(
</bodyText>
<page confidence="0.934849">
�b1,
274
</page>
<bodyText confidence="0.999961045454546">
Since it can be impossible to construct a full
system act from the best summary act, a back-off
strategy is used: an N-best list of summary acts,
ranked by their Q values, is scrolled through un-
til a feasible summary act is found. The resulting
overall process of mapping between master and
summary space and back is illustrated in Figure 1.
The complete k-nn version policy optimisation al-
gorithm is described in Algorithm 1.
The user simulator results for semantic error
rates ranging from 0 to 50% with a 5% step are
shown in Figure 2 for k E 11, 3, 5, 71, averaged
over 3000 dialogues. The results demonstrate that
the k-nn policies outperform the baseline 1-nn pol-
icy, especially on high noise levels. While our
initial expectations are met, increasing k above 3
does not improve performances. This is likely to
be due to the small size of the summary space as
well as the use of discrete dimensions. However
enlarging the summary space and the sample set is
conceivable with k-nn time-efficient optimisations
(as in (Lef`evre, 2003)).
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986916666667">
In this paper, an extension to a grid-based pol-
icy optimisation technique has been presented and
evaluated within the CUED HIS-based dialogue
system. The Monte-Carlo control policy optimi-
sation algorithm is complemented with a k-nearest
neighbour technique to ensure a better generaliza-
tion of the trained policy along with an increased
robustness to noise in the user input. Preliminary
results from an evaluation with a simulated user
confirm that the k-nn policies outperform the 1-nn
baseline on high noise, both in terms of successful
dialogue completion and accumulated reward.
</bodyText>
<sectionHeader confidence="0.994208" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999262">
This research was partly funded by the UK EP-
SRC under grant agreement EP/F013930/1 and
by the EU FP7 Programme under grant agree-
ment 216594 (CLASSIC project: www.classic-
project.org).
</bodyText>
<sectionHeader confidence="0.998474" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9924738">
C Atkeson, A Moore, and S Schaal. 1997. Locally
weighted learning. AI Review, 11:11–73, April.
F Lef`evre. 2003. Non-parametric probability estima-
tion for HMM-based automatic speech recognition.
Computer Speech &amp; Language, 17(2-3):113 – 136.
</reference>
<figure confidence="0.993566">
0 10 20 30 40 50
Semantic Error Rate
0 10 20 30 40 50
Semantic Error Rate
</figure>
<figureCaption confidence="0.9347485">
Figure 2: Comparison of the percentage of suc-
cessful simulated dialogues and the average re-
ward between the k-nn strategies on different error
rates.
</figureCaption>
<reference confidence="0.99865852631579">
J Pineau, G Gordon, and S Thrun. 2003. Point-based
value iteration: An anytime algorithm for POMDPs.
In Proc IJCAI, pages pp1025–1032, Mexico.
J Schatzmann, B Thomson, K Weilhammer, H Ye, and
SJ Young. 2007. Agenda-Based User Simulation
for Bootstrapping a POMDP Dialogue System. In
HLT/NAACL, Rochester, NY.
RS Sutton and AG Barto. 1998. Reinforcement Learn-
ing: An Introduction. MIT Press, Cambridge, Mass.
JD Williams and SJ Young. 2007. Scaling POMDPs
for Spoken Dialog Management. IEEE Audio,
Speech and Language Processing, 15(7):2116–
2129.
SJ Young, M Gaˇsi´c, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2009. The hid-
den information state model: A practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech &amp; Language, In Press, Un-
corrected Proof.
</reference>
<page confidence="0.788014">
98
</page>
<figure confidence="0.993968612903226">
96
94
92
90
88
86
84
82
80
78
1-nn
3-nn
5-nn
7-nn
Average Reward
14
13
12
11
10
9
7
6
5
4
8
1-nn
3-nn
5-nn
7-nn
Successful Completion Rate
</figure>
<page confidence="0.978141">
275
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.124034">
<title confidence="0.80135775">Neighbor Monte-Carlo Control for POMDP-based Dialogue Systems Gaˇsi´c, F. Jurˇc´ıˇcek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. Spoken Dialogue Systems</title>
<affiliation confidence="0.971851">Cambridge University Engineering</affiliation>
<address confidence="0.306429">Trumpington Street, Cambridge CB2 1PZ,</address>
<email confidence="0.501703">mg436,fj228,sk561,farm2,brmt2,ky219,</email>
<abstract confidence="0.99825615">In real-world applications, modelling dialogue as a POMDP requires the use of a summary space for the dialogue state representation to ensure tractability. Suboptimal estimation of the value function governing the selection of system responses can then be obtained using a gridbased approach on the belief space. In this work, the Monte-Carlo control technique is extended so as to reduce training over-fitting and to improve robustness to semantic noise in the user input. This technique uses a database of belief vector prototypes to choose the optimal system ac- A locally weighted neighbor scheme is introduced to smooth the decision process by interpolating the value function, resulting in higher user simulation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Atkeson</author>
<author>A Moore</author>
<author>S Schaal</author>
</authors>
<title>Locally weighted learning.</title>
<date>1997</date>
<journal>AI Review,</journal>
<pages>11--11</pages>
<contexts>
<context position="12255" citStr="Atkeson et al., 1997" startWordPosition="2124" endWordPosition="2127">value vector Q(b, a). When a new belief state b&apos; is encountered, its Q values are obtained by looking up its k-nearest neighbours in the database, then averaging their Q-values. To obtain good estimates for the value function interpolation, local weights are used based on the belief point distance. A Kullback-Leibler (KL) divergence (relative entropy) could be used as a distance function between the belief points. However, while the KL-divergence between two continuous distributions is well defined, this is not the case for sample sets. In accordance with the locally weighted learning theory (Atkeson et al., 1997), a simple weighting scheme based on a nearly Euclidean distance (eq. 1) is used to interpolate the policy over a set of points: πknn( Eb) = argmax am 141knn In our experiments, we set the weighting cob2) = e−161−6212. 2 j bz − E d=1 �bjj = b) = argmax � am π( Q(�bk, bm) X 4b(�bk, b) efficients with the kernel function 4b( �b1, 274 Since it can be impossible to construct a full system act from the best summary act, a back-off strategy is used: an N-best list of summary acts, ranked by their Q values, is scrolled through until a feasible summary act is found. The resulting overall process of ma</context>
</contexts>
<marker>Atkeson, Moore, Schaal, 1997</marker>
<rawString>C Atkeson, A Moore, and S Schaal. 1997. Locally weighted learning. AI Review, 11:11–73, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Lef`evre</author>
</authors>
<title>Non-parametric probability estimation for HMM-based automatic speech recognition.</title>
<date>2003</date>
<journal>Computer Speech &amp; Language, 17(2-3):113 –</journal>
<pages>136</pages>
<marker>Lef`evre, 2003</marker>
<rawString>F Lef`evre. 2003. Non-parametric probability estimation for HMM-based automatic speech recognition. Computer Speech &amp; Language, 17(2-3):113 – 136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pineau</author>
<author>G Gordon</author>
<author>S Thrun</author>
</authors>
<title>Point-based value iteration: An anytime algorithm for POMDPs.</title>
<date>2003</date>
<booktitle>In Proc IJCAI,</booktitle>
<pages>1025--1032</pages>
<contexts>
<context position="2620" citStr="Pineau et al., 2003" startWordPosition="411" endWordPosition="414">). Even though optimal policies remain out of reach, sub-optimal solutions can be found by means of grid-based algorithms. Within the Hidden Information State (HIS) framework (Young et al., 2009), policies are represented by a set of grid points in the summary belief space. Beliefs in master space are first mapped into summary space and then mapped into a summary action via the dialogue policy. The resulting summary action is then mapped back into master space and output to the user. Methods which support interpolation between points are generally required to scale well to large state spaces (Pineau et al., 2003). In the current version of the HIS framework, the policy chooses the system action by associating each new belief point with the single, closest, grid point. In the present work, a k-nearest neighbour extension is evaluated in which the policy decision is based on a locally weighted regression over a subset of representative grid points. This method thus lies between a strictly grid-based and a point-based value iteration approach as it interpolates the value function around the queried belief point. It thus reduces the policy’s dependency on the belief grid point selection and increases robu</context>
</contexts>
<marker>Pineau, Gordon, Thrun, 2003</marker>
<rawString>J Pineau, G Gordon, and S Thrun. 2003. Point-based value iteration: An anytime algorithm for POMDPs. In Proc IJCAI, pages pp1025–1032, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Weilhammer</author>
<author>H Ye</author>
<author>SJ Young</author>
</authors>
<title>Agenda-Based User Simulation for Bootstrapping a POMDP Dialogue System. In HLT/NAACL,</title>
<date>2007</date>
<location>Rochester, NY.</location>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>J Schatzmann, B Thomson, K Weilhammer, H Ye, and SJ Young. 2007. Agenda-Based User Simulation for Bootstrapping a POMDP Dialogue System. In HLT/NAACL, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RS Sutton</author>
<author>AG Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>Sutton, Barto, 1998</marker>
<rawString>RS Sutton and AG Barto. 1998. Reinforcement Learning: An Introduction. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Williams</author>
<author>SJ Young</author>
</authors>
<title>Scaling POMDPs for Spoken Dialog Management.</title>
<date>2007</date>
<journal>IEEE Audio, Speech and Language Processing,</journal>
<volume>15</volume>
<issue>7</issue>
<pages>2129</pages>
<contexts>
<context position="2001" citStr="Williams and Young, 2007" startWordPosition="307" endWordPosition="310">l., 2009). The POMDP framework models dialogue flow as a sequence of unobserved dialogue states following stochastic moves, and provides a principled way to model uncertainty. However, to deal with uncertainty, POMDPs maintain distributions over all possible states. But then training an optimal policy is an NP hard problem and thus not tractable for any non-trivial application. In recent works this issue is addressed by mapping the dialog state representation * Fabrice Lef`evre is currently on leave from the University of Avignon, France. space (the master space) into a smaller summary space (Williams and Young, 2007). Even though optimal policies remain out of reach, sub-optimal solutions can be found by means of grid-based algorithms. Within the Hidden Information State (HIS) framework (Young et al., 2009), policies are represented by a set of grid points in the summary belief space. Beliefs in master space are first mapped into summary space and then mapped into a summary action via the dialogue policy. The resulting summary action is then mapped back into master space and output to the user. Methods which support interpolation between points are generally required to scale well to large state spaces (P</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>JD Williams and SJ Young. 2007. Scaling POMDPs for Spoken Dialog Management. IEEE Audio, Speech and Language Processing, 15(7):2116– 2129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
<author>M Gaˇsi´c</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech &amp; Language, In Press, Uncorrected Proof.</title>
<date>2009</date>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2009</marker>
<rawString>SJ Young, M Gaˇsi´c, S Keizer, F Mairesse, J Schatzmann, B Thomson, and K Yu. 2009. The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech &amp; Language, In Press, Uncorrected Proof.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>