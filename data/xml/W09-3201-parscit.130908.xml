<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000083">
<title confidence="0.9163">
Social (distributed) language modeling, clustering and dialectometry
</title>
<author confidence="0.91604">
David Ellis
</author>
<affiliation confidence="0.823123">
Facebook
</affiliation>
<address confidence="0.619668">
Palo Alto, CA
</address>
<email confidence="0.976578">
dellis@facebook.com
</email>
<sectionHeader confidence="0.997169" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933133333333">
We present ongoing work in a scalable,
distributed implementation of over 200
million individual language models, each
capturing a single user’s dialect in a given
language (multilingual users have several
models). These have a variety of prac-
tical applications, ranging from spam de-
tection to speech recognition, and dialec-
tometrical methods on the social graph.
Users should be able to view any content
in their language (even if it is spoken by
a small population), and to browse our site
with appropriately translated interface (au-
tomatically generated, for locales with lit-
tle crowd-sourced community effort).
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999451333333333">
We approach several key questions from a data-
driven (statistical) perspective, drawing on large,
dynamic annotated corpora:
</bodyText>
<listItem confidence="0.8912281">
1. What social factors affect language change
(and evolution)? How?
2. How do individuals adjust their speech or
writing depending on context and audience?
(e.g., register, formality, humor, reference)
3. What are the minimum requirements for a
language (or dialect)?
(e.g., number of speakers, corpus size)
4. Is a common language necessary for commu-
nication?
</listItem>
<subsectionHeader confidence="0.541556">
Can a pidgin be predicted from its speaker-population?
</subsectionHeader>
<bodyText confidence="0.9990046">
To this end, we describe a framework for lan-
guage modeling on the social graph, which incor-
porates similarity clustering and lays the ground-
work for personalized (and multimodal) machine
translation.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999705260869565">
Research on large scale language model-
ing (Brants et al., 2007) has addressed sharding,
smoothing and integration with a machine transla-
tion pipeline. Our work takes a similar approach,
using Hadoop (Borthakur, 2007) and Hive to
query and process distributed data. Social annota-
tions enhanced smoothing for language modeling
in the context of information retrieval (Xu et
al., 2007), and hierarchical Bayesian networks
were used (Zhou et al., 2008) to incorporate user
domain interest in such models. Language models
are often used to detect spam, including in social
bookmarking (Bogers and van den Bosch, 2008).
Proposed scoring models for social
search (Schenkel et al., 2008) use friendship
strengths and an extension of term frequency1.
These could benefit from a deeper integration with
friends’ language models, perhaps to approximate
a user-specific inverse document frequency, rather
than treat each tag by a user as equally relevant to
all his friends of a given (relationship) strength.
Strehl et al. (2000) found that similarity clustering
perform best using weighted graph partitioning.
</bodyText>
<sectionHeader confidence="0.998554" genericHeader="method">
3 Language Model
</sectionHeader>
<bodyText confidence="0.999677583333333">
An individual’s language model is a mixture of
their locale (or another language they speak) and
token frequencies from the content they produce
(write) and consume (read). Since we have hun-
dreds of milliions of users, each of whose lan-
guage model can depend on a variety of data
sources, it is essential to distribute these counts
(and other figures derived from them) in a way that
optimizes the efficiency of our access patterns2.
We also tried clustering users, and represent-
ing the language of each as deviations from its
neighbors (or the norm of the cluster). However,
</bodyText>
<footnote confidence="0.852011666666667">
1Called “socially-enhanced tag frequency”.
2See Section 5 for discussion of a variety of use cases.
1
</footnote>
<note confidence="0.993288">
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 1–4,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999895944444444">
there are significantly more edges than nodes in
our graph (more friendships than people), so this
alternative is less efficient.
An individual’s language use varies greatly de-
pending on his interlocutor or audience3. Mes-
sages I send (privately) to a friend differ in style
from comments I make on a public photo of my
nephew, which in turn differ from my writing style
as realized in an academic or industry paper or ar-
ticle.
An obvious optimization is to describe a min-
imum spanning tree (MST) on the graph, where
each edge is weighted according to the similarity
of dialects associated with the nodes (individuals,
groups or other entities) it connects. Then, lan-
guage models of nodes connected by the MST can
depend on each other’s counts. Singletons default
to the general language model from their locale.
</bodyText>
<subsectionHeader confidence="0.999389">
3.1 Detecting Deviations
</subsectionHeader>
<bodyText confidence="0.9516385">
People who aren’t friends (and have no mutual
friends or other evident connection) may yet use
more similar language than siblings. This exam-
ple seems highly improbable or unnatural, and in
fact serves as a good heuristic for detecting com-
promised, spam-sending accounts (even if not or-
ganized in a botnet).
If a user sends a message with high perplexity:
</bodyText>
<listItem confidence="0.999603916666667">
1. Their account is compromised, and being
used to spam (or phish) their friends.
2. They are using a different language than
usual. Users are often bilingual (sometimes
multi)-, so we may not yet have realized they
are proficient in a given language.
3. There may be a problem with the language
model:
(a) large vocabulary (tends to inflate per-
plexity)
(b) genre mix (user interface v. user com-
munication)
</listItem>
<subsectionHeader confidence="0.999395">
3.2 Locale Induction
</subsectionHeader>
<bodyText confidence="0.97653">
A regional cluster of personal language models
can be combined to create a new locale. A crowd-
sourced translation process (Ellis, 2009) can thus
3This is not novel in or of itself, but the scale of our data
and experiments should lead to finer-grained understanding,
both of issues peculiar to a single language or its family, and
of language universals (or.patterns; priors likely intuitively
encoded).
be bootstrapped by indirect community contribu-
tions.
</bodyText>
<sectionHeader confidence="0.994011" genericHeader="method">
4 Machine Translation
</sectionHeader>
<bodyText confidence="0.9993912">
For an English-sepaking user, in order to opti-
mize the probability of the target (translated) sen-
tence given its source (Foreign), we follow Och
and Ney’s (2004) optimization of a set of feature
functions:
</bodyText>
<equation confidence="0.9540335">
eˆ = arg max
e
</equation>
<bodyText confidence="0.999914066666667">
It is thus easy for us to aggregate scores from
multiple language models (e.g., from individuals
comprising your network of friends or others you
interact with).
Our distributed, individual language models can
be a component of personalized machine transla-
tion, where the target language may be a penpal’s.
Either the decoder incorporates the counts from
user communications by supplementing the lan-
guage model used in its n-best candidate search,
or it uses the locale’s general language model and
factors in individual variance in a rescoring step.
We plan to offer inline statistical machine trans-
lation (SMT) of user-generated content, where the
translation model combines features from:
</bodyText>
<listItem confidence="0.996744555555556">
1. Our (interface) translations corpus for the
language pair
2. Related langauges or dialects4
3. Linguistic rules (Ellis, 2009), in some com-
bination of:
(a) Explicitly encoded
(b) Induced from training corpora
(c) Borrowed from related languages (esp.
for relatively minor or resource-poor)
</listItem>
<subsectionHeader confidence="0.998438">
4.1 Sparse Data
</subsectionHeader>
<bodyText confidence="0.999864375">
Data sparseness is clearly an issue for modeling
with this degree of specificity, so we explore a
range of possible smoothing techniques, as well
as methods for leveraging resources from related
languages (Genzel, 2005). If a user signed up for
Facebook last week, (s)he may not yet have con-
nected with many friends or shared much content
(which exacerbates the problem).
</bodyText>
<note confidence="0.490651">
4e.g. Spanish (Argentina, Spain), Chinese (Mandarin,
Cantonese (Hong Kong, Taiwan)), or Finnish and its neigh-
bors: inc. Estonian, S´ami, Komi
</note>
<figure confidence="0.460687666666667">
M
E λMhM(e, f)
M=1
</figure>
<page confidence="0.954854">
2
</page>
<bodyText confidence="0.999642166666667">
Domain adaptation is also important, since the
base corpus is for a user interface: usually more
formal, less varied than conversation. Ideally, we
would like to capture not only language change
(diversion, creolization) but an individual’s lin-
guistic evolution in a variety of contexts:
</bodyText>
<listItem confidence="0.991543642857143">
• She learns a language, practices its use, be-
comes increasingly accustomed to its twists
and turns (syntactic, lexical, morphological,
etc.)
• His mood shifts, he moves into a new apart-
ment or city, let alone grander (potentially
dynamic) features of context
• A startup company is suddently more visible
(e.g., resulting from press coverage, or a tech
blogger’s reference), and so an image (and
website design, copy) revamp is in order.
• Afflicted with post-traumatic stress, after
sensory deprivation, or in cases of neurologi-
cal disorders or brain damage.
</listItem>
<sectionHeader confidence="0.984752" genericHeader="method">
5 Similarity
</sectionHeader>
<bodyText confidence="0.987366">
We use a pipeline to cluster strings (to suggest
translations) and users (based on language use):
</bodyText>
<listItem confidence="0.998583857142857">
1. Preprocessing
• normalization (lowercasing)
• {segment,{lemmat,token}iz}ation
2. Similarity (pick one)
• fuzzy (hash) similarity5
• string edit distance
• phonetic (or phonological) edit distance
• language model perplexity
• KL-divergence (btn. language models)
3. Clustering (modular: select-an-algo)
• hierarchical (agglomerative or divisive)
• K-means (partitioning)
• graph-theoretic methods (cover as op-
posed to cluster)
</listItem>
<bodyText confidence="0.999520142857143">
This is architected for ease of experimentation
and modification, testing and tuning, so any com-
bination of the above should be functional. Some
applications of similarity require high accuracy
but can be processed offline, whereas others need
to be computed in less than ten milliseconds in re-
sponse to a live query.
</bodyText>
<footnote confidence="0.497007">
5i.e., Jaccard coefficient (Wikipedia, 2008)
</footnote>
<figureCaption confidence="0.760212">
Figure 1: Visualization of a user’s friends, where
the extent of each type of relationship or commu-
nication is indicated by saturation (shade of blue)
of the connection.
</figureCaption>
<sectionHeader confidence="0.995162" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999803357142857">
Although the components we use can be (and in
most cases, have been) thoroughly evaluated in
relative isolation, it is important to understand the
consequences of their use in concert. Improve-
ments to spam detection should be evident both in
tests on annotated6 data and in decreased reports
or complaints from users.
User-generated metadata, in some cases a sim-
ple report of offensive content or a friend’s com-
promised account, is a natural source of both la-
beled test data and training data. Our customer
service processes are thus tightly integrated with
machine learning efforts. See Figure 1 for commu-
nications in a small segment of the social graph.
</bodyText>
<sectionHeader confidence="0.992523" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9896756">
Preliminary experiments with user-initiated ma-
chine translation of friend-generated content sug-
gest it will soon be valuable. It is crucial to design
this in a scalable way, such that it extends to arbi-
trarily many languages7, both draws on and sup-
</bodyText>
<footnote confidence="0.9998">
6Either a binary classification (spam or non-spam) or a
gradient scale, possibly incorporating dimensions of phishi-
ness, spamminess, or other types of solicitousness.
7Including underrepresented ones like Oshindonga.
</footnote>
<page confidence="0.998144">
3
</page>
<bodyText confidence="0.999782166666667">
ports our internationalization efforts, and should
be useful on mobile devices (including in the spo-
ken modality).
Our introductory questions (from Section 1) are
far from fully answered, but we hope this work
might help to address them.
</bodyText>
<listItem confidence="0.991282833333333">
1. The number and strength of connections,
speed and frequency of communication, and
diversity of languages individuals are ex-
posed to all have strong influences on lan-
guage change.
2. Stylistic variations in an individual’s lan-
</listItem>
<bodyText confidence="0.613318923076923">
guage are evident in that it can be more accu-
rately captured as a mixture of models, each
of which is suited to a specific situation, style,
or set of interlocutors.
3. Two speakers is sufficient for a language. A
small model can adequately describe a lan-
guage, if each data point is a deviation from
another language.
4. A common language is far from necessary for
communication8. A set of arbitrary individu-
als’ language models can be combined (and
pruned, evolved) to derive the pidgin they
might speak.
</bodyText>
<subsectionHeader confidence="0.797912">
7.1 Future Work
</subsectionHeader>
<bodyText confidence="0.999976384615385">
Social natural language processing is (in a sense)
in its infancy. We hope to capture aspects of its
evolution, just as the field comes to better describe
and understand ongoing changes in human lan-
guages. We have not yet satisfactorily answered
our second question, but expect more fine-grained
analyses to follow, using our framework to com-
pare and contrast a variety of languages (from
Bantu to Balinese) and phenomena (inside jokes,
cross-linguistic usage of l33t and txt msg terms).
We hope to facilitate this by providing an API
to allow researchers access to anonymized9, ag-
gregated data.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992846428571429">
This technology is developed with support from
i18n team (engineers, language managers and oth-
ers) at Facebook, and all our international users.
8Photos, emoticons and tone of voice (for example) go
a long way. We hope personalized (including speech-to-
speech) translation will continue to bridge the language di-
vide.
</bodyText>
<subsectionHeader confidence="0.267451">
9Also honoring users’ privacy settings.
</subsectionHeader>
<bodyText confidence="0.999819333333333">
Thanks to our data scientists for the visualization
of a user’s friends, and the extent of communica-
tion connecting them.
</bodyText>
<sectionHeader confidence="0.995478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99903740425532">
Toine Bogers and Antal van den Bosch. 2008. Using
language models for spam detection in social book-
marking. In Proceedings of the ECML/PKDD Dis-
covery Challenge.
Dhruba Borthakur, 2007. The Hadoop Distributed File
System: Architecture and Design. The Apache Soft-
ware Foundation.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858–867.
David Ellis. 2009. A case study in community-
driven translation of a fast-changing website. In
Proceedings of the 13th International Conference on
Human-Computer Interaction HCII (to appear), San
Diego, California, USA.
Dmitriy Genzel. 2005. Creating Algorithms for
Parsers and Taggers for Resource-Poor Languages
Using a Related Resource-Rich Language. Ph.D.
thesis, Brown University.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417–449.
Ralf Schenkel, Tom Crecelius, Mouna Kacimi, Thomas
Neumann, Josiane Parreira, Marc Spaniol, and Ger-
hard Weikum. 2008. Social wisdom for search and
recommendation, June.
Er Strehl, Joydeep Ghosh, and Raymond Mooney.
2000. Impact of similarity measures on web-page
clustering. In In Workshop on Artificial Intelligence
for Web Search (AAAI 2000, pages 58–64. AAAI.
Wikipedia. 2008. Jaccard’s similarity coefficient.
Shengliang Xu, Shenghua Bao, Yunbo Cao, and Yong
Yu. 2007. Using social annotations to improve lan-
guage model for information retrieval. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, pages
1003–1006. CIKM.
Ding Zhou, Jiang Bian, Shuyi Zheng, Hongyuan Zha,
and Lee C. Giles. 2008. Exploring social an-
notations for information retrieval. In WWW ’08:
Proceeding of the 17th international conference on
World Wide Web, pages 715–724, New York, NY,
USA. ACM.
</reference>
<page confidence="0.996675">
4
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.745617">
<title confidence="0.991743">Social (distributed) language modeling, clustering and dialectometry</title>
<author confidence="0.894608">David Palo Alto</author>
<email confidence="0.999866">dellis@facebook.com</email>
<abstract confidence="0.9959219375">We present ongoing work in a scalable, distributed implementation of over 200 million individual language models, each capturing a single user’s dialect in a given language (multilingual users have several models). These have a variety of practical applications, ranging from spam detection to speech recognition, and dialectometrical methods on the social graph. Users should be able to view any content in their language (even if it is spoken by a small population), and to browse our site with appropriately translated interface (automatically generated, for locales with little crowd-sourced community effort).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Toine Bogers</author>
<author>Antal van den Bosch</author>
</authors>
<title>Using language models for spam detection in social bookmarking.</title>
<date>2008</date>
<booktitle>In Proceedings of the ECML/PKDD Discovery Challenge.</booktitle>
<marker>Bogers, van den Bosch, 2008</marker>
<rawString>Toine Bogers and Antal van den Bosch. 2008. Using language models for spam detection in social bookmarking. In Proceedings of the ECML/PKDD Discovery Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dhruba Borthakur</author>
</authors>
<title>The Hadoop Distributed File System: Architecture and Design. The Apache Software Foundation.</title>
<date>2007</date>
<contexts>
<context position="1734" citStr="Borthakur, 2007" startWordPosition="257" endWordPosition="258">rements for a language (or dialect)? (e.g., number of speakers, corpus size) 4. Is a common language necessary for communication? Can a pidgin be predicted from its speaker-population? To this end, we describe a framework for language modeling on the social graph, which incorporates similarity clustering and lays the groundwork for personalized (and multimodal) machine translation. 2 Related Work Research on large scale language modeling (Brants et al., 2007) has addressed sharding, smoothing and integration with a machine translation pipeline. Our work takes a similar approach, using Hadoop (Borthakur, 2007) and Hive to query and process distributed data. Social annotations enhanced smoothing for language modeling in the context of information retrieval (Xu et al., 2007), and hierarchical Bayesian networks were used (Zhou et al., 2008) to incorporate user domain interest in such models. Language models are often used to detect spam, including in social bookmarking (Bogers and van den Bosch, 2008). Proposed scoring models for social search (Schenkel et al., 2008) use friendship strengths and an extension of term frequency1. These could benefit from a deeper integration with friends’ language model</context>
</contexts>
<marker>Borthakur, 2007</marker>
<rawString>Dhruba Borthakur, 2007. The Hadoop Distributed File System: Architecture and Design. The Apache Software Foundation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>858--867</pages>
<contexts>
<context position="1581" citStr="Brants et al., 2007" startWordPosition="233" endWordPosition="236"> do individuals adjust their speech or writing depending on context and audience? (e.g., register, formality, humor, reference) 3. What are the minimum requirements for a language (or dialect)? (e.g., number of speakers, corpus size) 4. Is a common language necessary for communication? Can a pidgin be predicted from its speaker-population? To this end, we describe a framework for language modeling on the social graph, which incorporates similarity clustering and lays the groundwork for personalized (and multimodal) machine translation. 2 Related Work Research on large scale language modeling (Brants et al., 2007) has addressed sharding, smoothing and integration with a machine translation pipeline. Our work takes a similar approach, using Hadoop (Borthakur, 2007) and Hive to query and process distributed data. Social annotations enhanced smoothing for language modeling in the context of information retrieval (Xu et al., 2007), and hierarchical Bayesian networks were used (Zhou et al., 2008) to incorporate user domain interest in such models. Language models are often used to detect spam, including in social bookmarking (Bogers and van den Bosch, 2008). Proposed scoring models for social search (Schenk</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ellis</author>
</authors>
<title>A case study in communitydriven translation of a fast-changing website.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th International Conference on Human-Computer Interaction HCII (to appear),</booktitle>
<location>San Diego, California, USA.</location>
<contexts>
<context position="5241" citStr="Ellis, 2009" startWordPosition="828" endWordPosition="829"> If a user sends a message with high perplexity: 1. Their account is compromised, and being used to spam (or phish) their friends. 2. They are using a different language than usual. Users are often bilingual (sometimes multi)-, so we may not yet have realized they are proficient in a given language. 3. There may be a problem with the language model: (a) large vocabulary (tends to inflate perplexity) (b) genre mix (user interface v. user communication) 3.2 Locale Induction A regional cluster of personal language models can be combined to create a new locale. A crowdsourced translation process (Ellis, 2009) can thus 3This is not novel in or of itself, but the scale of our data and experiments should lead to finer-grained understanding, both of issues peculiar to a single language or its family, and of language universals (or.patterns; priors likely intuitively encoded). be bootstrapped by indirect community contributions. 4 Machine Translation For an English-sepaking user, in order to optimize the probability of the target (translated) sentence given its source (Foreign), we follow Och and Ney’s (2004) optimization of a set of feature functions: eˆ = arg max e It is thus easy for us to aggregate</context>
<context position="6626" citStr="Ellis, 2009" startWordPosition="1045" endWordPosition="1046">n be a component of personalized machine translation, where the target language may be a penpal’s. Either the decoder incorporates the counts from user communications by supplementing the language model used in its n-best candidate search, or it uses the locale’s general language model and factors in individual variance in a rescoring step. We plan to offer inline statistical machine translation (SMT) of user-generated content, where the translation model combines features from: 1. Our (interface) translations corpus for the language pair 2. Related langauges or dialects4 3. Linguistic rules (Ellis, 2009), in some combination of: (a) Explicitly encoded (b) Induced from training corpora (c) Borrowed from related languages (esp. for relatively minor or resource-poor) 4.1 Sparse Data Data sparseness is clearly an issue for modeling with this degree of specificity, so we explore a range of possible smoothing techniques, as well as methods for leveraging resources from related languages (Genzel, 2005). If a user signed up for Facebook last week, (s)he may not yet have connected with many friends or shared much content (which exacerbates the problem). 4e.g. Spanish (Argentina, Spain), Chinese (Manda</context>
</contexts>
<marker>Ellis, 2009</marker>
<rawString>David Ellis. 2009. A case study in communitydriven translation of a fast-changing website. In Proceedings of the 13th International Conference on Human-Computer Interaction HCII (to appear), San Diego, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Creating Algorithms for Parsers and Taggers for Resource-Poor Languages Using a Related Resource-Rich Language.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="7025" citStr="Genzel, 2005" startWordPosition="1106" endWordPosition="1107"> (SMT) of user-generated content, where the translation model combines features from: 1. Our (interface) translations corpus for the language pair 2. Related langauges or dialects4 3. Linguistic rules (Ellis, 2009), in some combination of: (a) Explicitly encoded (b) Induced from training corpora (c) Borrowed from related languages (esp. for relatively minor or resource-poor) 4.1 Sparse Data Data sparseness is clearly an issue for modeling with this degree of specificity, so we explore a range of possible smoothing techniques, as well as methods for leveraging resources from related languages (Genzel, 2005). If a user signed up for Facebook last week, (s)he may not yet have connected with many friends or shared much content (which exacerbates the problem). 4e.g. Spanish (Argentina, Spain), Chinese (Mandarin, Cantonese (Hong Kong, Taiwan)), or Finnish and its neighbors: inc. Estonian, S´ami, Komi M E λMhM(e, f) M=1 2 Domain adaptation is also important, since the base corpus is for a user interface: usually more formal, less varied than conversation. Ideally, we would like to capture not only language change (diversion, creolization) but an individual’s linguistic evolution in a variety of contex</context>
</contexts>
<marker>Genzel, 2005</marker>
<rawString>Dmitriy Genzel. 2005. Creating Algorithms for Parsers and Taggers for Resource-Poor Languages Using a Related Resource-Rich Language. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Comput. Linguist.,</journal>
<volume>30</volume>
<issue>4</issue>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Comput. Linguist., 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Schenkel</author>
<author>Tom Crecelius</author>
<author>Mouna Kacimi</author>
<author>Thomas Neumann</author>
<author>Josiane Parreira</author>
<author>Marc Spaniol</author>
<author>Gerhard Weikum</author>
</authors>
<title>Social wisdom for search and recommendation,</title>
<date>2008</date>
<contexts>
<context position="2197" citStr="Schenkel et al., 2008" startWordPosition="327" endWordPosition="330"> 2007) has addressed sharding, smoothing and integration with a machine translation pipeline. Our work takes a similar approach, using Hadoop (Borthakur, 2007) and Hive to query and process distributed data. Social annotations enhanced smoothing for language modeling in the context of information retrieval (Xu et al., 2007), and hierarchical Bayesian networks were used (Zhou et al., 2008) to incorporate user domain interest in such models. Language models are often used to detect spam, including in social bookmarking (Bogers and van den Bosch, 2008). Proposed scoring models for social search (Schenkel et al., 2008) use friendship strengths and an extension of term frequency1. These could benefit from a deeper integration with friends’ language models, perhaps to approximate a user-specific inverse document frequency, rather than treat each tag by a user as equally relevant to all his friends of a given (relationship) strength. Strehl et al. (2000) found that similarity clustering perform best using weighted graph partitioning. 3 Language Model An individual’s language model is a mixture of their locale (or another language they speak) and token frequencies from the content they produce (write) and consu</context>
</contexts>
<marker>Schenkel, Crecelius, Kacimi, Neumann, Parreira, Spaniol, Weikum, 2008</marker>
<rawString>Ralf Schenkel, Tom Crecelius, Mouna Kacimi, Thomas Neumann, Josiane Parreira, Marc Spaniol, and Gerhard Weikum. 2008. Social wisdom for search and recommendation, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Er Strehl</author>
<author>Joydeep Ghosh</author>
<author>Raymond Mooney</author>
</authors>
<title>Impact of similarity measures on web-page clustering.</title>
<date>2000</date>
<booktitle>In In Workshop on Artificial Intelligence for Web Search (AAAI</booktitle>
<pages>58--64</pages>
<contexts>
<context position="2536" citStr="Strehl et al. (2000)" startWordPosition="379" endWordPosition="382">al Bayesian networks were used (Zhou et al., 2008) to incorporate user domain interest in such models. Language models are often used to detect spam, including in social bookmarking (Bogers and van den Bosch, 2008). Proposed scoring models for social search (Schenkel et al., 2008) use friendship strengths and an extension of term frequency1. These could benefit from a deeper integration with friends’ language models, perhaps to approximate a user-specific inverse document frequency, rather than treat each tag by a user as equally relevant to all his friends of a given (relationship) strength. Strehl et al. (2000) found that similarity clustering perform best using weighted graph partitioning. 3 Language Model An individual’s language model is a mixture of their locale (or another language they speak) and token frequencies from the content they produce (write) and consume (read). Since we have hundreds of milliions of users, each of whose language model can depend on a variety of data sources, it is essential to distribute these counts (and other figures derived from them) in a way that optimizes the efficiency of our access patterns2. We also tried clustering users, and representing the language of ea</context>
</contexts>
<marker>Strehl, Ghosh, Mooney, 2000</marker>
<rawString>Er Strehl, Joydeep Ghosh, and Raymond Mooney. 2000. Impact of similarity measures on web-page clustering. In In Workshop on Artificial Intelligence for Web Search (AAAI 2000, pages 58–64. AAAI. Wikipedia. 2008. Jaccard’s similarity coefficient.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shengliang Xu</author>
<author>Shenghua Bao</author>
<author>Yunbo Cao</author>
<author>Yong Yu</author>
</authors>
<title>Using social annotations to improve language model for information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>1003--1006</pages>
<publisher>CIKM.</publisher>
<contexts>
<context position="1900" citStr="Xu et al., 2007" startWordPosition="281" endWordPosition="284">peaker-population? To this end, we describe a framework for language modeling on the social graph, which incorporates similarity clustering and lays the groundwork for personalized (and multimodal) machine translation. 2 Related Work Research on large scale language modeling (Brants et al., 2007) has addressed sharding, smoothing and integration with a machine translation pipeline. Our work takes a similar approach, using Hadoop (Borthakur, 2007) and Hive to query and process distributed data. Social annotations enhanced smoothing for language modeling in the context of information retrieval (Xu et al., 2007), and hierarchical Bayesian networks were used (Zhou et al., 2008) to incorporate user domain interest in such models. Language models are often used to detect spam, including in social bookmarking (Bogers and van den Bosch, 2008). Proposed scoring models for social search (Schenkel et al., 2008) use friendship strengths and an extension of term frequency1. These could benefit from a deeper integration with friends’ language models, perhaps to approximate a user-specific inverse document frequency, rather than treat each tag by a user as equally relevant to all his friends of a given (relation</context>
</contexts>
<marker>Xu, Bao, Cao, Yu, 2007</marker>
<rawString>Shengliang Xu, Shenghua Bao, Yunbo Cao, and Yong Yu. 2007. Using social annotations to improve language model for information retrieval. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 1003–1006. CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Zhou</author>
<author>Jiang Bian</author>
<author>Shuyi Zheng</author>
<author>Hongyuan Zha</author>
<author>Lee C Giles</author>
</authors>
<title>Exploring social annotations for information retrieval.</title>
<date>2008</date>
<booktitle>In WWW ’08: Proceeding of the 17th international conference on World Wide Web,</booktitle>
<pages>715--724</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1966" citStr="Zhou et al., 2008" startWordPosition="291" endWordPosition="294">uage modeling on the social graph, which incorporates similarity clustering and lays the groundwork for personalized (and multimodal) machine translation. 2 Related Work Research on large scale language modeling (Brants et al., 2007) has addressed sharding, smoothing and integration with a machine translation pipeline. Our work takes a similar approach, using Hadoop (Borthakur, 2007) and Hive to query and process distributed data. Social annotations enhanced smoothing for language modeling in the context of information retrieval (Xu et al., 2007), and hierarchical Bayesian networks were used (Zhou et al., 2008) to incorporate user domain interest in such models. Language models are often used to detect spam, including in social bookmarking (Bogers and van den Bosch, 2008). Proposed scoring models for social search (Schenkel et al., 2008) use friendship strengths and an extension of term frequency1. These could benefit from a deeper integration with friends’ language models, perhaps to approximate a user-specific inverse document frequency, rather than treat each tag by a user as equally relevant to all his friends of a given (relationship) strength. Strehl et al. (2000) found that similarity cluster</context>
</contexts>
<marker>Zhou, Bian, Zheng, Zha, Giles, 2008</marker>
<rawString>Ding Zhou, Jiang Bian, Shuyi Zheng, Hongyuan Zha, and Lee C. Giles. 2008. Exploring social annotations for information retrieval. In WWW ’08: Proceeding of the 17th international conference on World Wide Web, pages 715–724, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>