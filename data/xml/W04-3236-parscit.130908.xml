<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.998820333333333">
Chinese Part-of-Speech Tagging:
One-at-a-Time or All-at-Once?
Word-Based or Character-Based?
</title>
<author confidence="0.979196">
Hwee Tou Ng and Jin Kiat Low
</author>
<affiliation confidence="0.893699333333333">
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
</affiliation>
<email confidence="0.988976">
{nght, lowjinki}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.992355" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942419354839">
Chinese part-of-speech (POS) tagging
assigns one POS tag to each word in a
Chinese sentence. However, since words are
not demarcated in a Chinese sentence,
Chinese POS tagging requires word
segmentation as a prerequisite. We could
perform Chinese POS tagging strictly after
word segmentation (one-at-a-time
approach), or perform both word
segmentation and POS tagging in a
combined, single step simultaneously (all-at-
once approach). Also, we could choose to
assign POS tags on a word-by-word basis,
making use of word features in the
surrounding context (word-based), or on a
character-by-character basis with character
features (character-based). This paper
presents an in-depth study on such issues of
processing architecture and feature
representation for Chinese POS tagging,
within a maximum entropy framework. We
found that while the all-at-once, character-
based approach is the best, the one-at-a-time,
character-based approach is a worthwhile
compromise, performing only slightly worse
in terms of accuracy, but taking shorter time
to train and run. As part of our investigation,
we also built a state-of-the-art Chinese word
segmenter, which outperforms the best
SIGHAN 2003 word segmenters in the
closed track on 3 out of 4 test corpora.
</bodyText>
<sectionHeader confidence="0.999194" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999769421052632">
Most corpus-based language processing research
has focused on the English language.
Theoretically, we should be able to just port
corpus-based, machine learning techniques
across different languages since the techniques
are largely language independent. However, in
practice, the special characteristics of different
languages introduce complications. For Chinese
in particular, words are not demarcated in a
Chinese sentence. As such, we need to perform
word segmentation before we can proceed with
other tasks such as part-of-speech (POS) tagging
and parsing, since one POS tag is assigned to
each Chinese word (i.e., all characters in a
Chinese word have the same POS tag), and the
leaves of a parse tree for a Chinese sentence are
words.
To build a Chinese POS tagger, the following
questions naturally arise:
</bodyText>
<listItem confidence="0.924715818181818">
(1) Should we perform Chinese POS tagging
strictly after word segmentation in two separate
phases (one-at-a-time approach), or perform both
word segmentation and POS tagging in a
combined, single step simultaneously (all-at-once
approach)?
(2) Should we assign POS tags on a word-by-
word basis (like in English), making use of word
features in the surrounding context (word-based),
or on a character-by-character basis with
character features (character-based)?
</listItem>
<bodyText confidence="0.99974325">
This paper presents an in-depth study on such
issues of processing architecture and feature
representation for Chinese POS tagging, within a
maximum entropy framework. We analyze the
performance of the different approaches in our
attempt to find the best approach. To our
knowledge, our work is the first to systematically
investigate such issues in Chinese POS tagging.
</bodyText>
<sectionHeader confidence="0.982623" genericHeader="introduction">
2 Word Segmentation
</sectionHeader>
<bodyText confidence="0.999982863636364">
As a first step in our investigation, we built a
Chinese word segmenter capable of performing
word segmentation without using POS tag
information. Since errors in word segmentation
will propagate to the subsequent POS tagging
phase in the one-at-a-time approach, in order for
our study to give relevant findings, it is important
that the word segmenter we use gives state-of-
the-art accuracy.
The word segmenter we built is similar to the
maximum entropy word segmenter of (Xue and
Shen, 2003). Our word segmenter uses a
maximum entropy framework and is trained on
manually segmented sentences. It classifies each
Chinese character given the features derived
from its surrounding context. Each character can
be assigned one of 4 possible boundary tags: “b”
for a character that begins a word and is followed
by another character, “m” for a character that
occurs in the middle of a word, “e” for a
character that ends a word, and “s” for a
character that occurs as a single-character word.
</bodyText>
<subsectionHeader confidence="0.997696">
2.1 Word Segmenter Features
</subsectionHeader>
<bodyText confidence="0.986601882352941">
Besides implementing a subset of the features
described in (Xue and Shen, 2003), we also came
up with three additional types of features ((d) −
(f) below) which improved the accuracy of word
segmentation. The default feature, boundary tag
feature of the previous character, and boundary
tag feature of the character two before the current
character used in (Xue and Shen, 2003) were
dropped from our word segmenter, as they did
not improve word segmentation accuracy in our
experiments.
In the following feature templates used in our
word segmenter, C refers to a Chinese character
while W refers to a Chinese word. Templates (a)
− (c) refer to a context of five characters (the
current character and two characters to its left
and right). C0 denotes the current character, Cn
</bodyText>
<listItem confidence="0.5138718">
( C_n ) denotes the character n positions to the
right (left) of the current character.
(a) Cn (n = _2,_1,0,1,2)
(b) Cn Cn+1 (n = _2,_1,0,1)
(c) C_1C1
</listItem>
<equation confidence="0.60057725">
(d) W0 C0
(e) Pu(C0 )
(f) T(C )T(C )T(C )T(C )T(C )
_2 _1 0 1 2
</equation>
<bodyText confidence="0.9090992">
For example, given the character sequence
“新华社 记者”, when considering the character
“社”, template (a) results in the following
features C_2 =新 C_1 =华 C0 =社 C1 =记 C2 =者
to be set to 1, template (b) results in the features
</bodyText>
<equation confidence="0.931206">
C_2C_1 =新华 C_1C0 =华社 C0 C1 =社记
C1 C2 =记者 to be set to 1.
</equation>
<subsectionHeader confidence="0.99759">
2.2 Our Additional Features
</subsectionHeader>
<bodyText confidence="0.972789545454545">
W0 C0 :This feature captures the word
context in which the current character is found.
For example, the character “社” within the word
“新华社” will have the feature
W0 C0=新华社_社 set to 1. This feature helps in
recognizing seen words.
Pu( C0 ) :A punctuation symbol is usually a
good indication of a word boundary. This feature
checks whether the current character is a
punctuation symbol (such as “。”, “-”, “,”).
T(C )T(C )T(C )T( C )T(C ) :This
</bodyText>
<equation confidence="0.978963">
_2 _1 0 1 2
</equation>
<bodyText confidence="0.9859955">
feature is especially helpful in predicting the
word segmentation of dates and numbers, whose
exact characters may not have been seen in the
training text. Four type classes are defined:
numbers represent class 1, dates (“日”, “月”,
“年”, the Chinese character for “day”, “month”,
“year”, respectively) represent class 2, English
letters represent class 3, and other characters
represent class 4. For example, when considering
the character “年” in the character sequence
“九〇年代W”, the feature
T (C_2) ... T (C2) =11243 will be set to 1 ( “九”
and “〇” are the Chinese characters for “9” and
“0” respectively).
</bodyText>
<subsectionHeader confidence="0.999416">
2.3 Testing
</subsectionHeader>
<bodyText confidence="0.999767418604652">
During testing, the probability of a boundary tag
sequence assignment t1... tn given a character
sequence c1 ...cn is determined by using the
maximum entropy classifier to compute the
probability that a boundary tag ti is assigned to
each individual character ci. If we were to just
assign each character the boundary tag with the
highest probability, it is possible that the
classifier produces a sequence of invalid tags
(e.g., “m” followed by “s”). To eliminate such
possibilities, we implemented a dynamic
programming algorithm which considers only
valid boundary tag sequences given an input
character sequence. At each character position i,
the algorithm considers each last word candidate
ending at position i and consisting of K
characters in length (K = 1, ..., 20 in our
experiments). To determine the boundary tag
assignment to the last word W with K characters,
the first character of W is assigned boundary tag
“b”, the last character of W is assigned tag “e”,
and the intervening characters are assigned tag
“m”. (If W is a single-character word, then the
single character is assigned “s”.) In this way, the
dynamic programming algorithm only considers
valid tag sequences, and we are also able to make
use of the W0 C0 feature during testing.
After word segmentation is done by the
maximum entropy classifier, a post-processing
step is applied to correct inconsistently
segmented words made up of 3 or more
characters. A word W is defined to be
inconsistently segmented if the concatenation of
2 to 6 consecutive words elsewhere in the
segmented output document matches W. In the
post-processing step, the segmentation of the
characters of these consecutive words is changed
so that they are segmented as a single word. To
illustrate, if the concatenation of 2 consecutive
words “巴赛 罗纳” in the segmented output
document matches another word “巴赛罗纳”,
then “巴赛 罗纳” will be re-segmented as
“巴赛罗纳 ”.
</bodyText>
<subsectionHeader confidence="0.998876">
2.4 Word Segmenter Experimental Results
</subsectionHeader>
<bodyText confidence="0.999752545454545">
To evaluate the accuracy of our word segmenter,
we carried out 10-fold cross validation (CV) on
the 250K-word Penn Chinese Treebank (CTB)
(Xia et al., 2000) version 3.0. The Java opennlp
maximum entropy package from sourceforge1
was used in our implementation, and training
was done with a feature cutoff of 2 and 100
iterations.
The accuracy of word segmentation is
measured by recall (R), precision (P), and F-
measure (2RP /(R + P) ). Recall is the
proportion of correctly segmented words in the
gold-standard segmentation, and precision is the
proportion of correctly segmented words in word
segmenter’s output.
Figure 1 gives the word segmentation F-
measure of our word segmenter based on 10-fold
CV on the 250K-word CTB. Our word
segmenter achieves an average F-measure of
95.1%. This accuracy compares favorably with
(Luo, 2003), which reported 94.6% word
segmentation F-measure using his full parser
without additional lexical features, and about
94.9%2 word segmentation F-measure using only
word boundaries information, no POS tags or
constituent labels, but with lexical features
derived from a 58K-entry word list.
The average training time taken to train on
90% of the 250K-word CTB was 12 minutes,
while testing on 10% of CTB took about 1
minute. The running times reported in this paper
were all obtained on an Intel Xeon 2.4GHz
computer with 2GB RAM.
</bodyText>
<subsectionHeader confidence="0.670028">
Experiment Number
</subsectionHeader>
<bodyText confidence="0.978451384615385">
Figure 1: CTB 10-fold CV word segmentation F-
measure for our word segmenter
As further evaluation, we tested our word
segmenter on all the 4 test corpora (CTB,
Academia Sinica (AS), Hong Kong CityU (HK) ,
and Peking University (PK)) of the closed track
of the 2003 ACL-SIGHAN-sponsored First
International Chinese Word Segmentation
Bakeoff (Sproat and Emerson, 2003). For each of
the 4 corpora, we trained our word segmenter on
only the official released training data of that
corpus. Training was conducted with feature
cutoff of 2 and 100 iterations (these parameters
were obtained by cross validation on the training
set), except for the AS corpus where we used
cutoff 3 since the AS training corpus was too big
to train with cutoff 2.
Figure 2 shows our word segmenter’s F-
measure (based on the official word
segmentation scorer of 2003 SIGHAN bakeoff)
compared to those reported by all the 2003
SIGHAN participants in the four closed tracks
(ASc, HKc, PKc, CTBc). Our word segmenter
achieved higher F-measure than the best reported
F-measure in the SIGHAN bakeoff on the ASc,
HKc, and PKc corpus. For CTBc, due to the
</bodyText>
<figure confidence="0.9988843">
1 2 3 4 5 6 7 8 9 10
Word Seg F-Measure(%)
97.0
96.5
96.0
95.5
95.0
94.5
94.0
93.5
</figure>
<footnote confidence="0.813992333333333">
2 Based on visual inspection of Figure 3 of (Luo,
2003)
1 http://maxent.sourceforge.net
</footnote>
<bodyText confidence="0.999645833333333">
exceptionally high out-of-vocabulary (OOV) rate
of the test data (18.1%), our word segmenter’s F-
measure ranked in the third position. (Note that
the top participant of CTBc (Zhang et al., 2003)
used additional named entity knowledge/data in
their word segmenter).
</bodyText>
<page confidence="0.718209">
98
</page>
<figure confidence="0.989522277777778">
97
96
95
94
93
92
91
90
89
88
87
86
85
84
83
82
ASC1 HKC 2PKC CTBC CTBo
3 4 5
</figure>
<figureCaption confidence="0.9040485">
Figure 2: Comparison of word segmentation F-
measure for SIGHAN bakeoff3 tasks
</figureCaption>
<bodyText confidence="0.978687655172414">
We also compared the F-measure of our word
segmenter on CTBO, the open category of the
CTB corpus, where participants were free to use
any available resources and were not restricted to
only the official released training data of CTB.
On this CTBO task, we used as additional training
data the AS training corpus provided by
SIGHAN, after converting the AS training
corpus to GB encoding. We found that with this
additional AS training data added to the original
3 Last ranked participant of SIGHAN CTB (closed)
with F-measure 73.2% is not shown in Figure 2 due to
space constraint.
official released CTB training data of SIGHAN,
our word segmenter achieved an F-measure of
92.2%, higher than the best reported F-measure
in the CTB open task. With sufficient training
data, our word segmenter can perform very well.
In our evaluation, we also found that the
additional features we introduced in Section 2.2
and the post-processing step consistently
improved average word segmentation F-measure,
when evaluated on the 4 SIGHAN test corpora in
the closed track. The additional features
improved F-measure by an average of about
0.4%, and the post-processing step added on top
of the use of all features further improved F-
measure by 0.3% (i.e., for a cumulative total of
0.7% increase in F-measure).
</bodyText>
<sectionHeader confidence="0.971273" genericHeader="method">
3 One-at-a-Time, Word-Based POS Tagger
</sectionHeader>
<bodyText confidence="0.9997856">
Now that we have successfully built a state-of-
the-art Chinese word segmenter, we are ready to
explore issues of processing architecture and
feature representation for Chinese POS tagging.
An English POS tagger based on maximum
entropy modeling was built by (Ratnaparkhi,
1996). As a first attempt, we investigated
whether simply porting the method used by
(Ratnaparkhi, 1996) for English POS tagging
would work equally well for Chinese. Applying
it in the context of Chinese POS tagging,
Ratnaparkhi’s method assumes that words are
pre-segmented, and it assigns POS tags on a
word-by-word basis, making use of word
features in the surrounding context. This gives
rise to a one-at-a-time, word-based POS tagger.
Note that in a one-at-a-time approach, the
word-segmented input sentence given to the POS
tagger may contain word segmentation errors,
which can lower the POS tagging accuracy.
</bodyText>
<subsectionHeader confidence="0.926648">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.998481833333333">
The following feature templates were chosen.
W refers to a word while POS refers to the POS
tag assigned. The feature Pu(W0) checks if all
characters in the current word are punctuation
characters. Feature (e) encodes the class of
characters that constitute the surrounding words
(similar to feature (f) of the word segmenter in
Section 2.1). Four type classes are defined: a
word is of class 1 if it is a number; class 2 if the
word is made up of only numeric characters
followed by “H”, “月”,or “年”; class 3 when
the word is made up of only English characters
</bodyText>
<figure confidence="0.928250166666667">
Sighan Paticipants Our Word Segmenter
Word Seg F-Measure(%)
and optionally punctuation characters; class 4
otherwise.
(a) Wn (n = −2,−1,0,1,2 )
(b) WnWn+1 (n = −2,−1,0,1)
(c) W−1W1
(d) Pu(W0 )
(e) T(W )T(W )T(W )T(W )T(W )
− 2 − 1 0 1 2
(f) POS(W− 1 )
(g) POS(W− 2)POS(W− 1 )
</figure>
<subsectionHeader confidence="0.997963">
3.2 Testing
</subsectionHeader>
<bodyText confidence="0.999990571428571">
The testing procedure is similar to the beam
search algorithm of (Ratnaparkhi, 1996), which
tags each word one by one and maintains, as it
sees a new word, the N most probable POS tag
sequence candidates up to that point in the
sentence. For our experiment, we have chosen N
to be 3.
</bodyText>
<subsectionHeader confidence="0.996648">
3.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999862555555556">
The 250K-word CTB corpus, tagged with 32
different POS tags (such as “NR”, “PU”, etc)
was employed in our evaluation of POS taggers
in this study. We ran 10-fold CV on the CTB
corpus, using our word segmenter’s output for
each of the 10 runs as the input sentences to the
POS tagger. POS tagging accuracy is simply
calculated as (number of characters assigned
correct POS tag) / (total number of characters).
</bodyText>
<subsectionHeader confidence="0.761535">
Experiment Number
</subsectionHeader>
<bodyText confidence="0.9636244">
Figure 3: POS tagging accuracy using one-at-a-
time, word-based POS tagger
The POS tagging accuracy is plotted in Figure
3. The average POS tagging accuracy achieved
for the 10 experiments was only 84.1%, far lower
than the 96% achievable by English POS taggers
on the English Penn Treebank tag set. The
average training time was 25 minutes, while
testing took about 20 seconds. As an experiment,
we also conducted POS tagging using only the
features (a), (f), and (g) in Section 3.1, similar to
(Ratnaparkhi, 1996), and we obtained an average
POS tagging accuracy of 83.1% for that set of
features.
The features that worked well for English POS
tagging did not seem to apply to Chinese in the
maximum entropy framework. Language
differences between Chinese and English have
no doubt made the direct porting of an English
POS tagging method to Chinese ineffective.
</bodyText>
<sectionHeader confidence="0.9706785" genericHeader="method">
4 One-at-a-Time, Character-Based POS
Tagger
</sectionHeader>
<bodyText confidence="0.999926235294118">
Since one-at-a-time, word-based POS tagging
did not yield good accuracy, we proceeded to
investigate other combinations of processing
architecture and feature representation. We
observed that character features were
successfully used to build our word segmenter
and that of (Xue and Shen, 2003). Similarly,
character features were used to build a maximum
entropy Chinese parser by (Luo, 2003), where his
parser could perform word segmentation, POS
tagging, and parsing in an integrated, unified
approach. We hypothesized that assigning POS
tags on a character-by-character basis, making
use of character features in the surrounding
context may yield good accuracy. So we next
investigate such a one-at-a-time, character-based
POS tagger.
</bodyText>
<subsectionHeader confidence="0.79716">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.99962625">
The features that were used for our word
segmenter ((a) − (f)) in Section 2.1 were yet
again applied, with two additional features (g)
and (h) to aid POS tag prediction.
</bodyText>
<figure confidence="0.795815666666667">
(a) Cn (n = −2,−1,0,1,2)
(b) Cn Cn+1 (n = −2,−1,0,1)
(c) C−1C1
(d) W0 C0
(e) Pu(C0 )
(f) T(C )T(C )T(C )T(C )T(C )
</figure>
<equation confidence="0.918175">
−2 −1 0 1 2
(g) POS(C−1 W0 )
(h) POS(C−2W0 )POS(C−1W0 )
</equation>
<bodyText confidence="0.92398675">
: This feature refers to the
POS tag of the previous character before the
current word. For example, in the character
sequence “74 AL MA”, when considering the
</bodyText>
<figure confidence="0.927441928571428">
1 2 3 4 5 6 7 8 9 10
POS Accuracy(%)
89
88
87
86
85
84
83
82
81
80
79
(
</figure>
<equation confidence="0.7108175">
POS
C−1 W0 )
</equation>
<bodyText confidence="0.995534375">
character “A”, the feature POS(C−1W0 ) =PN is
set to 1 (assuming “k” was tagged as PN).
C−2W0 )POS(C−1W0 ) : For the same
example given above, when considering the
character “A”, the feature
POS(C−2W0 )POS(C−1W0 ) =P_PN is set to 1
(assuming “对” was tagged as P and “k” was
tagged as PN).
</bodyText>
<subsectionHeader confidence="0.998247">
4.2 Testing
</subsectionHeader>
<bodyText confidence="0.999818">
The testing algorithm is similar to that described
in Section 3.2, except that the probability of a
word being assigned a POS tag t is estimated by
the product of the probability of its individual
characters being assigned the same POS tag t.
For example, when estimating the probability of
“WTWU” being tagged NR, we find the product
of the probability of “WT” being tagged NR, “W”
being tagged NR, and “U” being tagged NR.
That is, we enforce the constraint that all
characters within a segmented word in the pre-
segmented input sentence must have the same
POS tag.
</bodyText>
<subsectionHeader confidence="0.999116">
4.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999793">
10-fold CV for CTB is repeated for this POS
tagger. Figure 4 shows the detailed POS tagging
accuracy. With a one-at-a-time, character-based
POS tagger, the average POS tagging accuracy
improved to 91.7%, 7.6% higher than that
achieved by the one-at-a-time, word-based POS
tagger. The average training timing was 55
minutes, while testing took about 50 seconds.
</bodyText>
<subsectionHeader confidence="0.910216">
Experiment Number
</subsectionHeader>
<bodyText confidence="0.958112666666667">
Figure 4: POS tagging accuracy using one-at-a-
time, character-based POS tagger
When a paired t-test was carried out to
compare character-based and word-based one-at-
a-time approaches, the character-based approach
was found to be significantly better than the
word-based approach, at the level of significance
0.01.
Assuming a one-at-a-time processing
architecture, Chinese POS tagging using a
character-based approach gives higher accuracy
compared to a word-based approach.
</bodyText>
<sectionHeader confidence="0.7755625" genericHeader="method">
5 All-at-Once, Character-Based POS
Tagger and Segmenter
</sectionHeader>
<bodyText confidence="0.999019">
Encouraged by the success of character features,
we next explored whether a change in processing
architecture, from one-at-a-time to all-at-once,
while still retaining the use of character features,
could give further improvement to POS tagging
accuracy. In this approach, both word
segmentation and POS tagging will be performed
in a combined, single step simultaneously. Each
character is assigned both a boundary tag and a
POS tag, for example “b_NN” (i.e., the first
character in a word with POS tag NN). Thus,
given 4 possible boundary tags and 32 unique
POS tags present in the training corpus, each
character can potentially be assigned one of
(4×32) classes.
</bodyText>
<subsectionHeader confidence="0.914342">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.9967104">
The features we used are identical to those
employed in the character-based POS tagger
described in section 4.1, except that features (g)
and (h) are replaced with those listed below. In
the following templates, B refers to the boundary
tag assigned. For example, given the character
sequence “V AL MA”, when considering the
character “A”, template (g) results in the feature
B(C−1W 0 )POS(C−1W 0 )=s_PN to be set to 1.
(assuming “k” was tagged as PN).
</bodyText>
<equation confidence="0.9691705">
(g) B(C−1W0 )POS(C−1W0 )
(h) B(C−2W0 )POS(C−2W0 )B(C−1W0 )POS(C 1W0 )
</equation>
<bodyText confidence="0.999345111111111">
Note that this approach is essentially that used
by (Luo, 2003), since his parser performs both
word segmentation and POS tagging (as well as
parsing) in one unified approach. The features we
used are similar to his tag features, except that
we did not use features with three consecutive
characters, since we found that the use of these
features did not improve accuracy. We also
added additional features (d) − (f).
</bodyText>
<figure confidence="0.976744090909091">
1 2 3 4 5 6 7 8 9 10
POS Accuracy(%)
95
94
93
92
91
90
89
(
POS
</figure>
<subsectionHeader confidence="0.997334">
5.2 Testing
</subsectionHeader>
<bodyText confidence="0.9998725">
Beam search algorithm is used with N = 3 during
the testing phase.
</bodyText>
<subsectionHeader confidence="0.998288">
5.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999106714285714">
10-fold CV on CTB was carried out again, using
unsegmented test sentences as input to the
program.
Figure 5 shows the word segmentation F-
measure, while Figure 6 shows the POS tagging
accuracy achieved by this approach. With an all-
at-once, character-based approach, an average
word segmentation F-measure of 95.2% and an
average POS tagging accuracy of 91.9% was
achieved. The average training timing was 3
hours, while testing took about 20 minutes.
There is a slight improvement in word
segmentation and POS tagging accuracy using
this approach, compared to the one-at-a-time,
character-based approach. When a paired t-test
was carried out at the level of significance 0.01,
the all-at-once approach was found to be
significantly better than the one-at-a-time
approach for POS tagging accuracy, although the
difference was insignificant for word
segmentation.
</bodyText>
<subsectionHeader confidence="0.565399">
Experiment Number
</subsectionHeader>
<figureCaption confidence="0.9590155">
Figure 5: CTB 10-fold CV word segmentation F-
measure using an all-at-once approach
</figureCaption>
<figure confidence="0.805188">
Experiment Number
</figure>
<figureCaption confidence="0.9812975">
Figure 6: CTB 10-fold CV POS tagging accuracy
using an all-at-once approach
</figureCaption>
<bodyText confidence="0.999725214285714">
However, the time required for training and
testing is increased significantly for the all-at-
once approach. When efficiency is a major
consideration, or if high quality hand-segmented
text is available, the one-at-a-time, character-
based approach could indeed be a worthwhile
compromise, performing only slightly worse than
the all-at-once approach. Table 1 summarizes the
methods investigated in this paper. Total testing
time includes both word segmentation and POS
tagging on 10% of CTB data. Note that an all-at-
once, word-based approach is not applicable as
word segmentation requires character features to
determine the word boundaries.
</bodyText>
<table confidence="0.998834555555556">
Method Word Seg POS Total
F-measure Accuracy Testing
(%) (%) Time
One-at-a-Time 95.1 84.1 1 min
Word-Based 20 secs
One-at-a-Time 95.1 91.7 1 min
Char-Based 50 secs
All-At-Once 95.2 91.9 20 mins
Char-Based
</table>
<tableCaption confidence="0.9859435">
Table 1: Summary table on the various methods
investigated for POS tagging
</tableCaption>
<sectionHeader confidence="0.998016" genericHeader="method">
6 Discussions
</sectionHeader>
<bodyText confidence="0.999908727272727">
Word-based or character-based? The findings
that a character-based approach is better than a
word-based approach for Chinese POS tagging is
not too surprising. Unlike in English where each
English letter by itself does not possess any
meaning, many Chinese characters have well
defined meanings. For example, the single
Chinese character “0” means “know”. And
when a character appears as part of a word, the
word derives part of its meaning from the
component characters. For example, “0V,”
means “knowledge”, “3&apos;u0” means “ignorant”,
“0-8” means “well-known”, etc. In addition,
since the out-of-vocabulary (OOV) rate for
Chinese words is much higher than the OOV rate
for Chinese characters, in the presence of an
unknown word, using the component characters
in the word to help predict the correct POS tag is
a good heuristic.
One-at-a-time or all-at-once? The all-at-once
approach, which considers all aspects of
available information in an integrated, unified
</bodyText>
<figure confidence="0.968625210526316">
1 2 3 4 5 6 7 8 9 10
Word Seg F-Measure(%)
97.0
96.5
96.0
95.5
95.0
94.5
94.0
93.5
93.0
1 2 3 4 5 6 7 8 9 10
POS Accuracy(%)
95
94
93
92
91
90
</figure>
<page confidence="0.998213">
89
</page>
<bodyText confidence="0.998298">
compared with the all-at-once, character-based
approach previously proposed.
framework, can make better informed decisions,
but incurs a higher computational cost.
</bodyText>
<sectionHeader confidence="0.999866" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999967594594594">
Much previous research on Chinese language
processing focused on word segmentation
(Sproat et al., 1996; Teahan et al., 2000; Sproat
and Emerson, 2003). Relatively less work has
been done on Chinese POS tagging. Kwong and
Tsou (2003) discussed the implications of POS
ambiguity in Chinese and the possible
approaches to tackle this problem when tagging a
corpus for NLP tasks. Zhou and Su (2003)
investigated an approach to build a Chinese
analyzer that integrated word segmentation, POS
tagging and parsing, based on a hidden Markov
model. Jing et al. (2003) focused on Chinese
named entity recognition, considering issues like
character-based versus word-based approaches.
To our knowledge, our work is the first to
systematically investigate issues of processing
architecture and feature representation for
Chinese POS tagging.
Our maximum entropy word segmenter is
similar to that of (Xue and Shen, 2003), but the
additional features we used and the post-
processing step gave improved word
segmentation accuracy.
The research most similar to ours is (Luo,
2003). Luo presented a maximum entropy
character-based parser, which as a consequence
of parsing also performed word segmentation
and POS tagging. The all-at-once, character-
based approach reported in this paper is
essentially the approach proposed by Luo. While
our investigation reveals that such an approach
gives good accuracy, our findings however
indicate that a one-at-a-time, character-based
approach to POS tagging gave quite comparable
accuracy, with the benefit of incurring much
reduced computational cost.
</bodyText>
<sectionHeader confidence="0.998187" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999966181818182">
Language differences between English and
Chinese have made direct porting of an English
POS tagging method to Chinese ineffective. In
Chinese, individual characters encode
information that aids in POS tagging. Using a
character-based approach for Chinese POS
tagging is more effective than a word-based
approach. Our study has also revealed that the
one-at-a-time, character-based approach gives
relatively good POS tagging accuracy with a
much improved training and testing time,
</bodyText>
<sectionHeader confidence="0.994022" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.916221">
This research is partially supported by a research
grant R252-000-125-112 from National
University of Singapore Academic Research
Fund.
</bodyText>
<sectionHeader confidence="0.983168" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999732605263158">
H. Jing, R. Florian, X. Luo, T. Zhang, and A.
Ittycheriah. 2003. HowtogetaChineseName
(Entity): segmentation and combination issues.
In Proc. of EMNLP.
O. Y. Kwong and B. K. Tsou. 2003. Categorial
fluidity in Chinese and its implications for
part-of-speech tagging. In Proc. of EACL.
X. Luo. 2003. A maximum entropy Chinese
character-based parser. In Proc. of EMNLP.
A. Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proc. of
EMNLP.
R. Sproat, C. Shih, W. Gale, and N. Chang.
1996. A stochastic finite-state word-
segmentation algorithm for Chinese.
Computational Linguistics, 22(3):377-404.
R. Sproat and T. Emerson. 2003. The first
international Chinese word segmentation
bakeoff. In Proc. of SIGHAN Workshop.
W. J. Teahan, Y. Wen, R. McNab, and I. H.
Witten. 2000. A compression-based algorithm
for Chinese word segmentation. Computational
Linguistics, 26(3): 375-393.
F. Xia, M. Palmer, N. Xue, M. E. Okurowski, J.
Kovarik, F-D Chiou, S. Huang, T. Kroch, and
M. Marcus. 2000. Developing guidelines and
ensuring consistency for Chinese text
annotation. In Proc. of LREC.
N. Xue and L. Shen. 2003. Chinese word
segmentation as LMR tagging. In Proc. of
SIGHAN Workshop.
H-P Zhang, H-K Yu, D-Y Xiong, and Q. Liu.
2003. HHMM-based Chinese lexical analyzer
ICTCLAS. In Proc. of SIGHAN Workshop.
G. Zhou and J. Su, 2003. A Chinese efficient
analyser integrating word segmentation, part-
of-speech tagging, partial parsing and full
parsing. In Proc. of SIGHAN Workshop.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.425390">
<title confidence="0.995377">Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once? Word-Based or Character-Based?</title>
<author confidence="0.997985">Tou Ng Kiat Low</author>
<affiliation confidence="0.999772">Department of Computer Science National University of Singapore</affiliation>
<address confidence="0.539261">3 Science Drive 2, Singapore 117543</address>
<email confidence="0.969439">nght@comp.nus.edu.sg</email>
<email confidence="0.969439">lowjinki@comp.nus.edu.sg</email>
<abstract confidence="0.99320896875">Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence. However, since words are not demarcated in a Chinese sentence, Chinese POS tagging requires word segmentation as a prerequisite. We could perform Chinese POS tagging strictly after word segmentation approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-atonce approach). Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based). This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework. We found that while the all-at-once, characterbased approach is the best, the one-at-a-time, character-based approach is a worthwhile compromise, performing only slightly worse in terms of accuracy, but taking shorter time to train and run. As part of our investigation, we also built a state-of-the-art Chinese word segmenter, which outperforms the best SIGHAN 2003 word segmenters in the closed track on 3 out of 4 test corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>R Florian</author>
<author>X Luo</author>
<author>T Zhang</author>
<author>A Ittycheriah</author>
</authors>
<title>HowtogetaChineseName (Entity): segmentation and combination issues.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="25115" citStr="Jing et al. (2003)" startWordPosition="4125" endWordPosition="4128">ns, but incurs a higher computational cost. 7 Related Work Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003). Relatively less work has been done on Chinese POS tagging. Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work is the first to systematically investigate issues of processing architecture and feature representation for Chinese POS tagging. Our maximum entropy word segmenter is similar to that of (Xue and Shen, 2003), but the additional features we used and the postprocessing step gave improved word segmentation accuracy. The research most similar to ours is (Luo, 2003). Luo presented a maximum entropy character-based parser, which as a consequence of parsing als</context>
</contexts>
<marker>Jing, Florian, Luo, Zhang, Ittycheriah, 2003</marker>
<rawString>H. Jing, R. Florian, X. Luo, T. Zhang, and A. Ittycheriah. 2003. HowtogetaChineseName (Entity): segmentation and combination issues. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Y Kwong</author>
<author>B K Tsou</author>
</authors>
<title>Categorial fluidity in Chinese and its implications for part-of-speech tagging.</title>
<date>2003</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="24789" citStr="Kwong and Tsou (2003)" startWordPosition="4072" endWordPosition="4075">all aspects of available information in an integrated, unified 1 2 3 4 5 6 7 8 9 10 Word Seg F-Measure(%) 97.0 96.5 96.0 95.5 95.0 94.5 94.0 93.5 93.0 1 2 3 4 5 6 7 8 9 10 POS Accuracy(%) 95 94 93 92 91 90 89 compared with the all-at-once, character-based approach previously proposed. framework, can make better informed decisions, but incurs a higher computational cost. 7 Related Work Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003). Relatively less work has been done on Chinese POS tagging. Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work is the first to systematically investigate issues of processing architecture and feature representation for Chinese POS tagging. Ou</context>
</contexts>
<marker>Kwong, Tsou, 2003</marker>
<rawString>O. Y. Kwong and B. K. Tsou. 2003. Categorial fluidity in Chinese and its implications for part-of-speech tagging. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>A maximum entropy Chinese character-based parser.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="9357" citStr="Luo, 2003" startWordPosition="1512" endWordPosition="1513">forge1 was used in our implementation, and training was done with a feature cutoff of 2 and 100 iterations. The accuracy of word segmentation is measured by recall (R), precision (P), and Fmeasure (2RP /(R + P) ). Recall is the proportion of correctly segmented words in the gold-standard segmentation, and precision is the proportion of correctly segmented words in word segmenter’s output. Figure 1 gives the word segmentation Fmeasure of our word segmenter based on 10-fold CV on the 250K-word CTB. Our word segmenter achieves an average F-measure of 95.1%. This accuracy compares favorably with (Luo, 2003), which reported 94.6% word segmentation F-measure using his full parser without additional lexical features, and about 94.9%2 word segmentation F-measure using only word boundaries information, no POS tags or constituent labels, but with lexical features derived from a 58K-entry word list. The average training time taken to train on 90% of the 250K-word CTB was 12 minutes, while testing on 10% of CTB took about 1 minute. The running times reported in this paper were all obtained on an Intel Xeon 2.4GHz computer with 2GB RAM. Experiment Number Figure 1: CTB 10-fold CV word segmentation Fmeasur</context>
<context position="11166" citStr="Luo, 2003" startWordPosition="1821" endWordPosition="1822"> where we used cutoff 3 since the AS training corpus was too big to train with cutoff 2. Figure 2 shows our word segmenter’s Fmeasure (based on the official word segmentation scorer of 2003 SIGHAN bakeoff) compared to those reported by all the 2003 SIGHAN participants in the four closed tracks (ASc, HKc, PKc, CTBc). Our word segmenter achieved higher F-measure than the best reported F-measure in the SIGHAN bakeoff on the ASc, HKc, and PKc corpus. For CTBc, due to the 1 2 3 4 5 6 7 8 9 10 Word Seg F-Measure(%) 97.0 96.5 96.0 95.5 95.0 94.5 94.0 93.5 2 Based on visual inspection of Figure 3 of (Luo, 2003) 1 http://maxent.sourceforge.net exceptionally high out-of-vocabulary (OOV) rate of the test data (18.1%), our word segmenter’s Fmeasure ranked in the third position. (Note that the top participant of CTBc (Zhang et al., 2003) used additional named entity knowledge/data in their word segmenter). 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 ASC1 HKC 2PKC CTBC CTBo 3 4 5 Figure 2: Comparison of word segmentation Fmeasure for SIGHAN bakeoff3 tasks We also compared the F-measure of our word segmenter on CTBO, the open category of the CTB corpus, where participants were free to use any availa</context>
<context position="16742" citStr="Luo, 2003" startWordPosition="2751" endWordPosition="2752">he maximum entropy framework. Language differences between Chinese and English have no doubt made the direct porting of an English POS tagging method to Chinese ineffective. 4 One-at-a-Time, Character-Based POS Tagger Since one-at-a-time, word-based POS tagging did not yield good accuracy, we proceeded to investigate other combinations of processing architecture and feature representation. We observed that character features were successfully used to build our word segmenter and that of (Xue and Shen, 2003). Similarly, character features were used to build a maximum entropy Chinese parser by (Luo, 2003), where his parser could perform word segmentation, POS tagging, and parsing in an integrated, unified approach. We hypothesized that assigning POS tags on a character-by-character basis, making use of character features in the surrounding context may yield good accuracy. So we next investigate such a one-at-a-time, character-based POS tagger. 4.1 Features The features that were used for our word segmenter ((a) − (f)) in Section 2.1 were yet again applied, with two additional features (g) and (h) to aid POS tag prediction. (a) Cn (n = −2,−1,0,1,2) (b) Cn Cn+1 (n = −2,−1,0,1) (c) C−1C1 (d) W0 C</context>
<context position="20754" citStr="Luo, 2003" startWordPosition="3424" endWordPosition="3425">) classes. 5.1 Features The features we used are identical to those employed in the character-based POS tagger described in section 4.1, except that features (g) and (h) are replaced with those listed below. In the following templates, B refers to the boundary tag assigned. For example, given the character sequence “V AL MA”, when considering the character “A”, template (g) results in the feature B(C−1W 0 )POS(C−1W 0 )=s_PN to be set to 1. (assuming “k” was tagged as PN). (g) B(C−1W0 )POS(C−1W0 ) (h) B(C−2W0 )POS(C−2W0 )B(C−1W0 )POS(C 1W0 ) Note that this approach is essentially that used by (Luo, 2003), since his parser performs both word segmentation and POS tagging (as well as parsing) in one unified approach. The features we used are similar to his tag features, except that we did not use features with three consecutive characters, since we found that the use of these features did not improve accuracy. We also added additional features (d) − (f). 1 2 3 4 5 6 7 8 9 10 POS Accuracy(%) 95 94 93 92 91 90 89 ( POS 5.2 Testing Beam search algorithm is used with N = 3 during the testing phase. 5.3 Experimental Results 10-fold CV on CTB was carried out again, using unsegmented test sentences as </context>
<context position="25620" citStr="Luo, 2003" startWordPosition="4201" endWordPosition="4202">integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work is the first to systematically investigate issues of processing architecture and feature representation for Chinese POS tagging. Our maximum entropy word segmenter is similar to that of (Xue and Shen, 2003), but the additional features we used and the postprocessing step gave improved word segmentation accuracy. The research most similar to ours is (Luo, 2003). Luo presented a maximum entropy character-based parser, which as a consequence of parsing also performed word segmentation and POS tagging. The all-at-once, characterbased approach reported in this paper is essentially the approach proposed by Luo. While our investigation reveals that such an approach gives good accuracy, our findings however indicate that a one-at-a-time, character-based approach to POS tagging gave quite comparable accuracy, with the benefit of incurring much reduced computational cost. 8 Conclusion Language differences between English and Chinese have made direct porting </context>
</contexts>
<marker>Luo, 2003</marker>
<rawString>X. Luo. 2003. A maximum entropy Chinese character-based parser. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="13233" citStr="Ratnaparkhi, 1996" startWordPosition="2161" endWordPosition="2162"> when evaluated on the 4 SIGHAN test corpora in the closed track. The additional features improved F-measure by an average of about 0.4%, and the post-processing step added on top of the use of all features further improved Fmeasure by 0.3% (i.e., for a cumulative total of 0.7% increase in F-measure). 3 One-at-a-Time, Word-Based POS Tagger Now that we have successfully built a state-ofthe-art Chinese word segmenter, we are ready to explore issues of processing architecture and feature representation for Chinese POS tagging. An English POS tagger based on maximum entropy modeling was built by (Ratnaparkhi, 1996). As a first attempt, we investigated whether simply porting the method used by (Ratnaparkhi, 1996) for English POS tagging would work equally well for Chinese. Applying it in the context of Chinese POS tagging, Ratnaparkhi’s method assumes that words are pre-segmented, and it assigns POS tags on a word-by-word basis, making use of word features in the surrounding context. This gives rise to a one-at-a-time, word-based POS tagger. Note that in a one-at-a-time approach, the word-segmented input sentence given to the POS tagger may contain word segmentation errors, which can lower the POS taggin</context>
<context position="14791" citStr="Ratnaparkhi, 1996" startWordPosition="2428" endWordPosition="2429"> segmenter in Section 2.1). Four type classes are defined: a word is of class 1 if it is a number; class 2 if the word is made up of only numeric characters followed by “H”, “月”,or “年”; class 3 when the word is made up of only English characters Sighan Paticipants Our Word Segmenter Word Seg F-Measure(%) and optionally punctuation characters; class 4 otherwise. (a) Wn (n = −2,−1,0,1,2 ) (b) WnWn+1 (n = −2,−1,0,1) (c) W−1W1 (d) Pu(W0 ) (e) T(W )T(W )T(W )T(W )T(W ) − 2 − 1 0 1 2 (f) POS(W− 1 ) (g) POS(W− 2)POS(W− 1 ) 3.2 Testing The testing procedure is similar to the beam search algorithm of (Ratnaparkhi, 1996), which tags each word one by one and maintains, as it sees a new word, the N most probable POS tag sequence candidates up to that point in the sentence. For our experiment, we have chosen N to be 3. 3.3 Experimental Results The 250K-word CTB corpus, tagged with 32 different POS tags (such as “NR”, “PU”, etc) was employed in our evaluation of POS taggers in this study. We ran 10-fold CV on the CTB corpus, using our word segmenter’s output for each of the 10 runs as the input sentences to the POS tagger. POS tagging accuracy is simply calculated as (number of characters assigned correct POS tag</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>C Shih</author>
<author>W Gale</author>
<author>N Chang</author>
</authors>
<title>A stochastic finite-state wordsegmentation algorithm for Chinese. Computational Linguistics,</title>
<date>1996</date>
<pages>22--3</pages>
<contexts>
<context position="24659" citStr="Sproat et al., 1996" startWordPosition="4050" endWordPosition="4053">o help predict the correct POS tag is a good heuristic. One-at-a-time or all-at-once? The all-at-once approach, which considers all aspects of available information in an integrated, unified 1 2 3 4 5 6 7 8 9 10 Word Seg F-Measure(%) 97.0 96.5 96.0 95.5 95.0 94.5 94.0 93.5 93.0 1 2 3 4 5 6 7 8 9 10 POS Accuracy(%) 95 94 93 92 91 90 89 compared with the all-at-once, character-based approach previously proposed. framework, can make better informed decisions, but incurs a higher computational cost. 7 Related Work Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003). Relatively less work has been done on Chinese POS tagging. Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work i</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>R. Sproat, C. Shih, W. Gale, and N. Chang. 1996. A stochastic finite-state wordsegmentation algorithm for Chinese. Computational Linguistics, 22(3):377-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>T Emerson</author>
</authors>
<title>The first international Chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In Proc. of SIGHAN Workshop.</booktitle>
<contexts>
<context position="10272" citStr="Sproat and Emerson, 2003" startWordPosition="1656" endWordPosition="1659">. The average training time taken to train on 90% of the 250K-word CTB was 12 minutes, while testing on 10% of CTB took about 1 minute. The running times reported in this paper were all obtained on an Intel Xeon 2.4GHz computer with 2GB RAM. Experiment Number Figure 1: CTB 10-fold CV word segmentation Fmeasure for our word segmenter As further evaluation, we tested our word segmenter on all the 4 test corpora (CTB, Academia Sinica (AS), Hong Kong CityU (HK) , and Peking University (PK)) of the closed track of the 2003 ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff (Sproat and Emerson, 2003). For each of the 4 corpora, we trained our word segmenter on only the official released training data of that corpus. Training was conducted with feature cutoff of 2 and 100 iterations (these parameters were obtained by cross validation on the training set), except for the AS corpus where we used cutoff 3 since the AS training corpus was too big to train with cutoff 2. Figure 2 shows our word segmenter’s Fmeasure (based on the official word segmentation scorer of 2003 SIGHAN bakeoff) compared to those reported by all the 2003 SIGHAN participants in the four closed tracks (ASc, HKc, PKc, CTBc)</context>
<context position="24707" citStr="Sproat and Emerson, 2003" startWordPosition="4058" endWordPosition="4061">od heuristic. One-at-a-time or all-at-once? The all-at-once approach, which considers all aspects of available information in an integrated, unified 1 2 3 4 5 6 7 8 9 10 Word Seg F-Measure(%) 97.0 96.5 96.0 95.5 95.0 94.5 94.0 93.5 93.0 1 2 3 4 5 6 7 8 9 10 POS Accuracy(%) 95 94 93 92 91 90 89 compared with the all-at-once, character-based approach previously proposed. framework, can make better informed decisions, but incurs a higher computational cost. 7 Related Work Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003). Relatively less work has been done on Chinese POS tagging. Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work is the first to systematically investigate issues</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>R. Sproat and T. Emerson. 2003. The first international Chinese word segmentation bakeoff. In Proc. of SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Teahan</author>
<author>Y Wen</author>
<author>R McNab</author>
<author>I H Witten</author>
</authors>
<title>A compression-based algorithm for Chinese word segmentation.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>375--393</pages>
<contexts>
<context position="24680" citStr="Teahan et al., 2000" startWordPosition="4054" endWordPosition="4057">rrect POS tag is a good heuristic. One-at-a-time or all-at-once? The all-at-once approach, which considers all aspects of available information in an integrated, unified 1 2 3 4 5 6 7 8 9 10 Word Seg F-Measure(%) 97.0 96.5 96.0 95.5 95.0 94.5 94.0 93.5 93.0 1 2 3 4 5 6 7 8 9 10 POS Accuracy(%) 95 94 93 92 91 90 89 compared with the all-at-once, character-based approach previously proposed. framework, can make better informed decisions, but incurs a higher computational cost. 7 Related Work Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003). Relatively less work has been done on Chinese POS tagging. Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work is the first to system</context>
</contexts>
<marker>Teahan, Wen, McNab, Witten, 2000</marker>
<rawString>W. J. Teahan, Y. Wen, R. McNab, and I. H. Witten. 2000. A compression-based algorithm for Chinese word segmentation. Computational Linguistics, 26(3): 375-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
<author>M Palmer</author>
<author>N Xue</author>
<author>M E Okurowski</author>
<author>J Kovarik</author>
<author>F-D Chiou</author>
<author>S Huang</author>
<author>T Kroch</author>
<author>M Marcus</author>
</authors>
<title>Developing guidelines and ensuring consistency for Chinese text annotation.</title>
<date>2000</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="8681" citStr="Xia et al., 2000" startWordPosition="1401" endWordPosition="1404">he concatenation of 2 to 6 consecutive words elsewhere in the segmented output document matches W. In the post-processing step, the segmentation of the characters of these consecutive words is changed so that they are segmented as a single word. To illustrate, if the concatenation of 2 consecutive words “巴赛 罗纳” in the segmented output document matches another word “巴赛罗纳”, then “巴赛 罗纳” will be re-segmented as “巴赛罗纳 ”. 2.4 Word Segmenter Experimental Results To evaluate the accuracy of our word segmenter, we carried out 10-fold cross validation (CV) on the 250K-word Penn Chinese Treebank (CTB) (Xia et al., 2000) version 3.0. The Java opennlp maximum entropy package from sourceforge1 was used in our implementation, and training was done with a feature cutoff of 2 and 100 iterations. The accuracy of word segmentation is measured by recall (R), precision (P), and Fmeasure (2RP /(R + P) ). Recall is the proportion of correctly segmented words in the gold-standard segmentation, and precision is the proportion of correctly segmented words in word segmenter’s output. Figure 1 gives the word segmentation Fmeasure of our word segmenter based on 10-fold CV on the 250K-word CTB. Our word segmenter achieves an a</context>
</contexts>
<marker>Xia, Palmer, Xue, Okurowski, Kovarik, Chiou, Huang, Kroch, Marcus, 2000</marker>
<rawString>F. Xia, M. Palmer, N. Xue, M. E. Okurowski, J. Kovarik, F-D Chiou, S. Huang, T. Kroch, and M. Marcus. 2000. Developing guidelines and ensuring consistency for Chinese text annotation. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>L Shen</author>
</authors>
<title>Chinese word segmentation as LMR tagging.</title>
<date>2003</date>
<booktitle>In Proc. of SIGHAN Workshop.</booktitle>
<contexts>
<context position="3684" citStr="Xue and Shen, 2003" startWordPosition="547" endWordPosition="550"> To our knowledge, our work is the first to systematically investigate such issues in Chinese POS tagging. 2 Word Segmentation As a first step in our investigation, we built a Chinese word segmenter capable of performing word segmentation without using POS tag information. Since errors in word segmentation will propagate to the subsequent POS tagging phase in the one-at-a-time approach, in order for our study to give relevant findings, it is important that the word segmenter we use gives state-ofthe-art accuracy. The word segmenter we built is similar to the maximum entropy word segmenter of (Xue and Shen, 2003). Our word segmenter uses a maximum entropy framework and is trained on manually segmented sentences. It classifies each Chinese character given the features derived from its surrounding context. Each character can be assigned one of 4 possible boundary tags: “b” for a character that begins a word and is followed by another character, “m” for a character that occurs in the middle of a word, “e” for a character that ends a word, and “s” for a character that occurs as a single-character word. 2.1 Word Segmenter Features Besides implementing a subset of the features described in (Xue and Shen, 20</context>
<context position="16644" citStr="Xue and Shen, 2003" startWordPosition="2734" endWordPosition="2737">et of features. The features that worked well for English POS tagging did not seem to apply to Chinese in the maximum entropy framework. Language differences between Chinese and English have no doubt made the direct porting of an English POS tagging method to Chinese ineffective. 4 One-at-a-Time, Character-Based POS Tagger Since one-at-a-time, word-based POS tagging did not yield good accuracy, we proceeded to investigate other combinations of processing architecture and feature representation. We observed that character features were successfully used to build our word segmenter and that of (Xue and Shen, 2003). Similarly, character features were used to build a maximum entropy Chinese parser by (Luo, 2003), where his parser could perform word segmentation, POS tagging, and parsing in an integrated, unified approach. We hypothesized that assigning POS tags on a character-by-character basis, making use of character features in the surrounding context may yield good accuracy. So we next investigate such a one-at-a-time, character-based POS tagger. 4.1 Features The features that were used for our word segmenter ((a) − (f)) in Section 2.1 were yet again applied, with two additional features (g) and (h) </context>
<context position="25464" citStr="Xue and Shen, 2003" startWordPosition="4174" endWordPosition="4177">and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work is the first to systematically investigate issues of processing architecture and feature representation for Chinese POS tagging. Our maximum entropy word segmenter is similar to that of (Xue and Shen, 2003), but the additional features we used and the postprocessing step gave improved word segmentation accuracy. The research most similar to ours is (Luo, 2003). Luo presented a maximum entropy character-based parser, which as a consequence of parsing also performed word segmentation and POS tagging. The all-at-once, characterbased approach reported in this paper is essentially the approach proposed by Luo. While our investigation reveals that such an approach gives good accuracy, our findings however indicate that a one-at-a-time, character-based approach to POS tagging gave quite comparable accu</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>N. Xue and L. Shen. 2003. Chinese word segmentation as LMR tagging. In Proc. of SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-P Zhang</author>
<author>H-K Yu</author>
<author>D-Y Xiong</author>
<author>Q Liu</author>
</authors>
<title>HHMM-based Chinese lexical analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>In Proc. of SIGHAN Workshop.</booktitle>
<contexts>
<context position="11392" citStr="Zhang et al., 2003" startWordPosition="1852" endWordPosition="1855"> those reported by all the 2003 SIGHAN participants in the four closed tracks (ASc, HKc, PKc, CTBc). Our word segmenter achieved higher F-measure than the best reported F-measure in the SIGHAN bakeoff on the ASc, HKc, and PKc corpus. For CTBc, due to the 1 2 3 4 5 6 7 8 9 10 Word Seg F-Measure(%) 97.0 96.5 96.0 95.5 95.0 94.5 94.0 93.5 2 Based on visual inspection of Figure 3 of (Luo, 2003) 1 http://maxent.sourceforge.net exceptionally high out-of-vocabulary (OOV) rate of the test data (18.1%), our word segmenter’s Fmeasure ranked in the third position. (Note that the top participant of CTBc (Zhang et al., 2003) used additional named entity knowledge/data in their word segmenter). 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 ASC1 HKC 2PKC CTBC CTBo 3 4 5 Figure 2: Comparison of word segmentation Fmeasure for SIGHAN bakeoff3 tasks We also compared the F-measure of our word segmenter on CTBO, the open category of the CTB corpus, where participants were free to use any available resources and were not restricted to only the official released training data of CTB. On this CTBO task, we used as additional training data the AS training corpus provided by SIGHAN, after converting the AS training corpu</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>H-P Zhang, H-K Yu, D-Y Xiong, and Q. Liu. 2003. HHMM-based Chinese lexical analyzer ICTCLAS. In Proc. of SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Su</author>
</authors>
<title>A Chinese efficient analyser integrating word segmentation, partof-speech tagging, partial parsing and full parsing.</title>
<date>2003</date>
<booktitle>In Proc. of SIGHAN Workshop.</booktitle>
<contexts>
<context position="24951" citStr="Zhou and Su (2003)" startWordPosition="4099" endWordPosition="4102">8 9 10 POS Accuracy(%) 95 94 93 92 91 90 89 compared with the all-at-once, character-based approach previously proposed. framework, can make better informed decisions, but incurs a higher computational cost. 7 Related Work Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003). Relatively less work has been done on Chinese POS tagging. Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work is the first to systematically investigate issues of processing architecture and feature representation for Chinese POS tagging. Our maximum entropy word segmenter is similar to that of (Xue and Shen, 2003), but the additional features we used and the postprocessing step gave improved word se</context>
</contexts>
<marker>Zhou, Su, 2003</marker>
<rawString>G. Zhou and J. Su, 2003. A Chinese efficient analyser integrating word segmentation, partof-speech tagging, partial parsing and full parsing. In Proc. of SIGHAN Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>